We present an algorithm for estimating scene flow using data-driven techniques. Our approach leverages the insight that many 3D scenes can be explained by a collection of agents moving as rigid bodies. The key component of our method is a deep architecture that can reason at the object-level by considering 3D scene flow in conjunction with other 3D tasks. This object-level abstraction allows us to relax the need for dense scene flow supervision and instead use simpler binary background segmentation masks and ego-motion annotations. This makes our method well-suited for large-scale datasets in autonomous driving, which often lack dense scene flow annotations. Our model outputs both low-level cues, such as pointwise flow, and higher-level cues, such as holistic scene understanding at the level of rigid objects. Additionally, we propose a test-time optimization technique to refine the predicted rigid scene flow. We demonstrate the effectiveness and generalization capacity of our method on four different autonomous driving datasets. To facilitate further research, we have made our source code and pre-trained models publicly available. They can be accessed at github.com/zgojcic/Rigid3DSceneFlow.