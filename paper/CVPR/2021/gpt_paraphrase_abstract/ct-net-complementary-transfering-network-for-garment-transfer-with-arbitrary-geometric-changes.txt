We propose a method called Complementary Transfering Network (CT-Net) for garment transfer between different people with heavy misalignments or severe occlusions. CT-Net consists of three modules: a complementary warping module, a layout prediction module, and a dynamic fusion module. The complementary warping module estimates two complementary warpings to transfer clothes at different granularities. The layout prediction module predicts the target layout to guide the preservation or generation of body parts in the synthesized images. The dynamic fusion module combines the advantages of the complementary warpings to produce high-quality garment transfer results. Experimental results on the DeepFashion dataset show that our network outperforms state-of-the-art methods in terms of quality and quantity. Our source code will be made available online.