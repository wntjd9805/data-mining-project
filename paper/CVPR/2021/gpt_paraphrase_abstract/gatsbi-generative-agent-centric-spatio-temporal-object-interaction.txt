We introduce GATSBI, a generative model that transforms raw observations into a structured latent representation, effectively capturing the spatial and temporal context of an agent's actions. In decision making scenarios that rely on visual information, agents encounter complex and high-dimensional observations involving multiple interacting entities. To make informed decisions, agents require a scene representation that identifies crucial elements and consistently propagates over time. Our approach, GATSBI, employs unsupervised learning to extract object-centric scene representations, distinguishing an active agent, static background, and passive objects. GATSBI further models the interactions between these decomposed entities, reflecting causal relationships, and predicts realistic future states. Our model is versatile, as it performs well in various environments with dynamic interactions between different types of robots and objects. Experimental results demonstrate that GATSBI outperforms state-of-the-art methods in scene decomposition and video prediction.