Current methods in few-shot learning (FSL) and self-supervised learning (SSL) have limitations in dealing with few labeled data. To address this, we propose a novel approach called Pareto self-supervised training (PSST) for FSL. PSST decomposes the few-shot auxiliary problem into multiple subproblems with different trade-off preferences and identifies a preference region where the main task performs best. We then use a preferred Pareto exploration to find optimal solutions within this region. Our approach achieves state-of-the-art performance on various benchmark datasets, demonstrating its effectiveness.