Learning based representation is crucial for the success of computer vision systems. However, representing a dynamically changing 3D object remains an unresolved issue. This paper presents a compositional representation for 4D captures, which involves a deforming 3D object over time. The representation disentangles shape, initial state, and motion into separate components, each represented by a latent code generated by a trained encoder. A neural Ordinary Differential Equation (ODE) is trained to model the motion, updating the initial state based on the learned motion code. A decoder reconstructs the 3D model at each time stamp using the shape code and the updated state code. An Identity Exchange Training (IET) strategy is proposed to encourage effective decoupling of each component in the network. Extensive experiments demonstrate that the proposed method outperforms existing deep learning based methods in 4D reconstruction and shows significant improvements in tasks such as motion transfer and completion.