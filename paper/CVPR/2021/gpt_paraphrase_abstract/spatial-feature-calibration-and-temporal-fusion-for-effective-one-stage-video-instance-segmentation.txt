Current one-stage video instance segmentation networks face two main challenges. First, the convolutional features are not properly aligned with anchor boxes or ground-truth bounding boxes, resulting in reduced sensitivity of the masks to spatial location. Secondly, these networks treat each frame of a video independently, failing to consider the temporal correlation between adjacent frames. To overcome these limitations, we propose a novel approach called STMask, which addresses these issues through spatial calibration and temporal fusion. For spatial feature calibration, we predict regressed bounding boxes around the ground-truth bounding boxes and extract features from them for frame-level instance segmentation. Additionally, we incorporate a temporal fusion module to capture the temporal correlation among video frames, enabling better handling of challenging scenarios such as motion blur, partial occlusion, and unusual object-to-camera poses. Our experiments on the YouTube-VIS valid set demonstrate the effectiveness of STMask with ResNet-50/-101 backbone, achieving mask average precision (AP) of 33.5%/36.8% and frame per second (FPS) of 28.6/23.4 in video instance segmentation. The code for our approach is publicly available at https://github.com/MinghanLi/STMask.