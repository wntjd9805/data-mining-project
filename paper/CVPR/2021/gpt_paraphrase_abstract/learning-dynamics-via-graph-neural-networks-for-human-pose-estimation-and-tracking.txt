This paper introduces a novel online approach for multi-person pose estimation and tracking in videos. Current state-of-the-art methods rely on first estimating poses in each frame and then performing data association and refinement. However, this strategy is prone to missed detections, especially in heavily cluttered scenes with occlusion. To address this issue, the proposed approach learns the dynamics of poses independently from pose detections in the current frame, making it robust in challenging scenarios. This is achieved through a graph neural network that incorporates spatial-temporal and visual information. The network takes historical pose tracklets as input and predicts the corresponding poses in the following frame for each tracklet. The predicted poses are then combined with the detected poses, if any, at the same frame to produce the final pose, potentially recovering occluded joints missed by the initial estimator. Experimental results on PoseTrack 2017 and PoseTrack 2018 datasets demonstrate that the proposed method outperforms state-of-the-art approaches in both human pose estimation and tracking tasks.