Quantization is crucial in deep neural network (DNN) hardware, and logarithmic quantization has advantages for DNN hardware implementations. However, its lower performance at high precision compared to linear quantization has been addressed by selective two-word logarithmic quantization (STLQ). Unfortunately, there is a lack of training methods for STLQ or logarithmic quantization in general. This paper introduces a new training method specifically designed for STLQ, which outperforms the previous state-of-the-art method. The results show that with this new training method, STLQ applied to weight parameters of ResNet-18 achieves the same performance as the state-of-the-art quantization method (APoT) at 3-bit precision. The method is also applied to various DNNs in image enhancement and semantic segmentation, yielding competitive results.