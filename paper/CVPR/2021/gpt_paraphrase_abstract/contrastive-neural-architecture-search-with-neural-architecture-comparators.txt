One of the main steps in Neural Architecture Search (NAS) is to evaluate the performance of potential architectures. Current approaches either use the validation performance directly or train a predictor to estimate performance. However, these methods can be computationally expensive or inaccurate, which can negatively impact search efficiency and performance. Additionally, accurately annotating architectures with performance data for specific tasks is challenging, making it difficult to train an effective performance predictor due to limited labeled data. This study suggests that estimating absolute performance may not be necessary for NAS. Instead, it may be sufficient to determine whether an architecture is better than a baseline. However, leveraging comparison information as the reward and effectively utilizing limited labeled data present significant challenges. To address these issues, the authors propose a new method called Contrastive Neural Architecture Search (CTNAS). CTNAS conducts architecture search by using the comparison results between architectures as the reward. The authors develop a Neural Architecture Comparator (NAC) to calculate the probability of candidate architectures being superior to a baseline. They also introduce a baseline updating scheme to iteratively improve the baseline in a curriculum learning manner. Importantly, the authors demonstrate that learning NAC is equivalent to optimizing the ranking of architectures. Extensive experiments in three search spaces validate the superiority of CTNAS compared to existing methods.