Motion style transfer is a significant problem in computer graphics and computer vision applications, such as human animation, games, and robotics. Most existing deep learning approaches for this problem are supervised and rely on registered motion pairs for training. However, these methods often produce deterministic outputs, limiting their flexibility. To address this, we propose an unsupervised approach using a generative flow model called M. M is trained to maximize the likelihood of a collection of unlabeled motions by synthesizing stylized motions autoregressively. Through invertible flow transformations, M efficiently infers latent codes that encode deep properties of motion styles. By combining these latent codes with an input style motion and an autoregressive context and control signal from an input content motion, M produces a stylized motion that transfers the style from the input style motion to the input content motion. Importantly, our model is probabilistic, allowing it to generate various plausible motions with a specific style. We evaluate our model on motion capture datasets with different human motion styles and demonstrate its superiority over state-of-the-art methods, despite not requiring manually labeled training data.