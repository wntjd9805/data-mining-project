Unsupervised contrastive learning has been highly successful, but little is known about the mechanism of contrastive loss. This study aims to understand the behavior of unsupervised contrastive loss and its relationship with uniformity and temperature. It is found that contrastive loss is a hardness-aware loss function, where the temperature controls the penalty strength on hard negative samples. While uniformity is important for contrastive learning to learn separable features, excessive pursuit of uniformity can hinder the tolerance towards semantically similar samples, damaging the formation of features useful for downstream tasks. This is due to the inherent flaw of the instance discrimination objective, which fails to consider the underlying relations between samples. Pushing semantically consistent samples apart does not benefit general downstream tasks. Therefore, a well-designed contrastive loss should balance uniformity and tolerance towards semantically similar samples. The choice of temperature plays a crucial role in achieving this balance, leading to improved feature qualities and downstream performances.