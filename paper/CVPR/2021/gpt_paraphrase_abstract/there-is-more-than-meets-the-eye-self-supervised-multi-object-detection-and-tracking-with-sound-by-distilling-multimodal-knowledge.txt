This abstract discusses the importance of sound attributes in object detection and tracking, as well as the potential use of audiovisual events to localize objects. However, previous methods have been limited to static camera scenarios and single object detection, relying heavily on RGB images that are susceptible to changes in illumination and weather. To address these limitations, the authors propose the MM-DistillNet framework, which incorporates multiple modalities (RGB, depth, and thermal images) to leverage complementary cues and distill knowledge into a single audio student network. They introduce a new MTA loss function for self-supervised distillation of information from multimodal teachers, and a self-supervised pretext task for the audio student that eliminates the need for manual annotations. The authors also present a large-scale multimodal dataset for evaluation. Experimental results demonstrate that their approach outperforms state-of-the-art methods, allowing for the detection of multiple objects using only sound during inference, even in moving scenarios.