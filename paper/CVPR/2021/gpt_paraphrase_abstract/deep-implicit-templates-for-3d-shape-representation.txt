Deep Implicit Templates (DITs) are a new type of 3D shape representation that aim to address the limitations of Deep Implicit Functions (DIFs). DIFs have gained popularity in the 3D vision community due to their compactness and strong representation power. However, they struggle with reasoning dense correspondences or semantic relationships across shapes, which restricts their applications in areas such as texture transfer and shape analysis. To overcome this limitation and enhance the interpretability of DIFs, the authors propose DITs, which allow for explicit correspondence reasoning in deep implicit representations. The key idea behind DITs is to formulate DIFs as conditional deformations of a template implicit function. To achieve this, the authors introduce the Spatial Warping LSTM, which decomposes the conditional spatial transformation into multiple point-wise transformations and ensures generalization capability. In addition, the authors carefully design the training loss to achieve high reconstruction accuracy while learning a plausible template with accurate correspondences in an unsupervised manner. Experimental results demonstrate that the proposed method can learn a common implicit template for a collection of shapes and establish dense correspondences across all shapes simultaneously, without the need for any supervision.