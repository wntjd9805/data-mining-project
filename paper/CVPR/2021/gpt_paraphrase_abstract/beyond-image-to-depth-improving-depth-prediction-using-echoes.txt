We present a solution to the problem of estimating depth using audiovisual data. Taking inspiration from animals like bats and dolphins that use echolocation to determine object distance, recent methods have utilized echoes for depth estimation. Our proposed approach is an end-to-end deep learning pipeline that combines RGB images, binaural echoes, and estimated material properties of objects within a scene. We argue that the relationship between images, echoes, and depth is influenced by the properties of scene elements, and a method designed to exploit this information can greatly improve depth estimation from audiovisual inputs. We introduce a novel fusion technique that explicitly incorporates material properties while combining audio and visual modalities to predict scene depth. Through experiments on the Replica dataset, we empirically demonstrate that our method outperforms the state-of-the-art audiovisual depth prediction method, achieving a 28% improvement in root mean square error (RMSE). Additionally, we showcase the effectiveness of our method on the larger Matterport3D dataset, proposing it as a new benchmark for multi-modal depth prediction with echoes. We conduct exhaustive ablation experiments and provide qualitative results to analyze the proposed method. The code and models are available at https://krantiparida.github.io/projects/bimgdepth.html.