Despite the advancements in generative adversarial networks (GANs) in creating realistic images, generating complex urban scenes remains a difficult task. Previous studies have divided scene generation into two phases: creating a semantic layout and then synthesizing an image based on that layout. In this research, we propose a method that conditions the generation of layouts to achieve higher semantic control. By providing a vector of class proportions, we generate layouts that match the desired composition. To achieve this, we introduce a conditional framework with innovative architecture designs and learning objectives, which effectively incorporate class proportions to guide the scene generation process. Our proposed architecture also allows for partial layout editing, which has interesting applications. By exercising semantic control, we are able to produce layouts that closely resemble the real distribution, thereby enhancing the overall scene generation process. Our models outperform existing benchmarks on various metrics and urban scene datasets. Additionally, we demonstrate the usefulness of our approach for data augmentation, as semantic segmenters trained on real layout-image pairs along with additional pairs generated by our method outperform models trained solely on real pairs.