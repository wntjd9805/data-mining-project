Shape modeling and reconstruction from raw point clouds of objects is a significant challenge in vision and graphics research. Traditional methods rely on analytic shape priors, but their performance suffers when the scanned points are not clean or complete. Recent advancements in data-driven approaches have shown promise by learning global and/or local models of implicit surface representations from auxiliary training shapes. Building on the observation that self-similar shape patterns repeat across an object's entire surface, we propose a novel approach to learn a local implicit surface network that can model the entire surface for direct surface reconstruction from raw point clouds. We also enhance the utilization of surface self-similarities by improving correlations among the optimized latent codes of individual surface patches. In situations where the orientations of raw points are uncertain or noisy, we incorporate sign-agnostic learning into our local implicit model, enabling the recovery of signed implicit fields of local surfaces from unsigned inputs. We refer to our framework as Sign-Agnostic Implicit Learning of Surface Self-Similarities (SAIL-S3). By applying a global post-optimization of local sign flipping, SAIL-S3 can directly model raw, unoriented point clouds and achieve high-quality object surface reconstruction. Experimental results demonstrate the superiority of our method over existing techniques.