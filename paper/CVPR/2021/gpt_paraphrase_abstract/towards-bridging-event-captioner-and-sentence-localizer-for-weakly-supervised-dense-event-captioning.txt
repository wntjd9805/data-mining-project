Previous methods in Dense Event Captioning (DEC) aim to localize and describe multiple events in untrimmed videos. Weakly Supervised Dense Event Captioning (WS-DEC) takes this a step further by not relying on human-annotated event boundaries. However, there are few methods addressing this task, and the connection between localization and description remains unresolved. This paper proposes that under weak supervision, the event captioning and localization modules should be closely linked to improve description performance. In contrast to previous approaches, our method involves the event captioner generating a sentence from a video segment and feeding it to the sentence localizer for segment reconstruction. The localizer produces word importance weights to guide the captioner in improving event descriptions. Additionally, a concept learner is incorporated as the basis of the sentence localizer to construct a set of concept features that enhance video features and improve the event captioner. Through experiments on the ActivityNet Captions dataset, our method outperforms state-of-the-art WS-DEC methods.