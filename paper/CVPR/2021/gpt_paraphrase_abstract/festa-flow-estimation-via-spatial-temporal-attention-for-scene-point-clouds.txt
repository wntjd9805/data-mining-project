The dynamics of a 3D scene, known as scene flow, are crucial for various applications such as autonomous driving, robot navigation, and AR/VR. Traditionally, scene flow is estimated from regular RGB video frames. However, with advancements in depth-sensing technologies, precise 3D measurements can now be obtained through point clouds, leading to new research in 3D scene flow. Extracting scene flow from point clouds is challenging due to their sparsity and irregular sampling patterns. One of the main challenges is the randomness in point set abstraction/feature extraction, which is a fundamental step in many flow estimation scenarios. To address this issue, we propose a novel Spatial Abstraction with Attention (SA2) layer that mitigates the problem of unstable abstraction. Additionally, we introduce a Temporal Abstraction with Attention (TA2) layer to improve attention in the temporal domain, resulting in better performance with a wider range of motions. Through extensive analysis and experiments, we validate the effectiveness of our method, called Flow Estimation via Spatial-Temporal Attention (FESTA), by comparing it to several state-of-the-art benchmarks for scene flow estimation.