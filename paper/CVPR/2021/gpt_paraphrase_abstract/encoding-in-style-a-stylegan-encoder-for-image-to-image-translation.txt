We introduce a new image-to-image translation framework called pixel2style2pixel (pSp). Our framework utilizes an innovative encoder network that generates a series of style vectors. These vectors are then fed into a pretrained Style-GAN generator, expanding the latent space known as W+. We demonstrate that our encoder can directly embed real images into W+ without the need for additional optimization.   Furthermore, we propose using our encoder to directly address image-to-image translation tasks by treating them as encoding problems from the input domain to the latent domain. Unlike previous StyleGAN encoders that follow an "invert first, edit later" approach, our method can handle various tasks even when the input image is not represented in the StyleGAN domain.   By leveraging StyleGAN for solving translation tasks, we simplify the training process significantly. Our approach does not require an adversary and provides better support for tasks that lack pixel-to-pixel correspondence. Additionally, it inherently supports multi-modal synthesis through style resampling.   We showcase the potential of our framework on different facial image-to-image translation tasks, surpassing state-of-the-art solutions designed specifically for a single task. Moreover, we demonstrate that our framework can be extended beyond human facial domains.   For further details and implementation, the code is available at https://github.com/eladrich/pixel2style2pixel.