This paper introduces a method called minimally invasive surgery for compressing neural networks. With the increasing depth and size of neural networks, there are challenges related to memory bandwidth, storage, latency, and throughput. Unlike traditional compression and knowledge distillation methods, the proposed approach adopts the principle of minimally invasive surgery. It involves learning principal features from a pair of dense and compressed models in a contrastive manner while optimizing the networks to meet hardware acceleration requirements. The method is evaluated through qualitative, quantitative, and ablation experiments, demonstrating excellent performance, acceleration, and generalization across different tasks.