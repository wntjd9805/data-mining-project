Bayesian neural networks are widely used due to their probabilistic representation framework. However, their ability to handle adversarial noises is limited. To address this issue, we propose Spectral Expectation Bound Regularization (SEBR) to enhance the robustness of Bayesian neural networks. Our theoretical analysis demonstrates that training with SEBR improves robustness to adversarial noises. Additionally, we prove that SEBR reduces the model's epistemic uncertainty, boosting confidence in predictions and further verifying its robustness. Experimental results on various Bayesian neural network structures and adversarial attacks confirm the validity of our theoretical findings and the effectiveness of SEBR.