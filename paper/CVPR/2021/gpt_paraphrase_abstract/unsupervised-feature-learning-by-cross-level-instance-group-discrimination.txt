Unsupervised feature learning has made significant progress through contrastive learning techniques, which rely on instance discrimination and invariant mapping. However, these methods struggle with natural data that is highly correlated and long-tail distributed. The similarity between instances conflicts with the goal of distinguishing individual instances, leading to unstable training and poor performance. To address this issue, we propose a novel approach called cross-level discrimination (CLD), which integrates between-instance similarity into contrastive learning. Unlike traditional instance grouping methods, CLD achieves this integration by discriminating between instances and local instance groups. By imposing attraction within augmented views of each instance and repulsion against instance groups, between-instance similarity naturally emerges. Our method also improves the positive/negative sample ratio of contrastive learning through batch-wise and cross-view comparisons, leading to better invariant mapping. To achieve both grouping and discrimination objectives, we apply them separately to features derived from a shared representation. Furthermore, we introduce normalized projection heads and unsupervised hyper-parameter tuning for the first time, enhancing the effectiveness of CLD. Extensive experimentation demonstrates that CLD is a powerful addition to existing methods, such as NPID, MoCo, InfoMin, and BYOL, particularly on highly correlated, long-tail, or balanced datasets. It outperforms MoCo v2 and SimCLR in every reported performance metric, even with significantly larger computational resources. CLD brings unsupervised learning closer to natural data and real-world applications. Our code is publicly available at: https://github.com/frank-xwang/CLD-UnsupervisedLearning.