This paper presents a new approach to addressing the challenge of transferring makeup from a misaligned reference image. Instead of computing pixel-wise correspondences between images, which is inaccurate and computationally expensive, the authors propose a two-step extraction-assignment process. They introduce a Style-based Controllable GAN model consisting of three components: target style-code encoding, face identity feature extraction, and makeup fusion. The Style Encoder encodes the makeup style of the reference image into a style-code that is invariant to spatial misalignment. This style-code, along with source identity features, is used by the Makeup Fusion Decoder to generate the final result. The proposed method allows for makeup removal, shade-controllable makeup transfer, and part-specific makeup transfer, even with large spatial misalignment. Extensive experiments demonstrate the superiority of this approach over existing methods. The code for this method is available at the provided GitHub link.