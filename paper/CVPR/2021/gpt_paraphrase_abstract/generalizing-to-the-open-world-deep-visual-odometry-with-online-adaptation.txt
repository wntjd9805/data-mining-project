This paper proposes an online adaptation framework for deep visual odometry (VO) that addresses the challenge of generalizing pretrained networks to unseen environments. The large domain gap between the training and testing data makes it difficult for pretrained networks to perform well in new scenes. To overcome this limitation, the authors propose a method that combines scene-agnostic geometric computations and Bayesian inference to adapt the deep VO network online. Instead of relying solely on learning-based pose estimation, the proposed method incorporates optical flow and depth information to solve for pose, with continuous improvement of single-view depth estimation using online learned uncertainties. Additionally, a differentiable Gauss-Newton layer utilizes an online learned photometric uncertainty for further depth and pose optimization. The proposed method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Experimental results on datasets such as Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that the proposed method achieves state-of-the-art generalization ability among self-supervised VO methods.