Deep neural networks are highly vulnerable to targeted attacks using imperceptible levels of adversarial noise. The underlying cause of this vulnerability is not well understood, but theoretical analyses can be simplified by considering each layer of a feed-forward network as an approximate solution to a sparse coding problem. Iterative solutions using basis pursuit are more stable and exhibit enhanced adversarial robustness. However, implementing layer-wise pursuit iteratively in cascades leads to error accumulation in deeper networks. In contrast, our novel method of deep pursuit approximates the activations of all layers as a single global optimization problem, enabling us to analyze deeper, real-world architectures with skip connections like residual networks. Our experimental results demonstrate improved resilience to adversarial noise.