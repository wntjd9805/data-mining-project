We introduce M3P, a unified model called Multitask Multilingual MultimodalPre-trained (M3P). This model combines multilingual pre-training and multimodal pre-training through multitask pre-training. Our objective is to learn universal representations that can map objects from different modalities or texts expressed in different languages into a shared semantic space. To further promote detailed alignment between images and non-English languages, we propose Multimodal Code-switched Training (MCT), which combines monolingual pre-training and multimodal pre-training using a code-switching strategy. We conduct experiments on the multilingual image retrieval task using two benchmark datasets, MSCOCO and Multi30K. M3P achieves comparable results for English and sets new state-of-the-art results for non-English languages.