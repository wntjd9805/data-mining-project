We introduce a method called Contrastive Video Representation Learning (CVRL) that allows us to learn visual representations from unlabeled videos. Our approach involves using a contrastive loss function, which pulls together augmented clips from the same video in the embedding space while pushing away clips from different videos. We investigate the importance of data augmentations for self-supervised video learning and find that both spatial and temporal information play a crucial role. To address this, we propose a temporally consistent spatial augmentation method that applies strong spatial augmentations to each frame while maintaining temporal consistency across frames. Additionally, we propose a sampling-based temporal augmentation method to avoid excessive enforcement of invariance on clips that are far apart in time. When trained on Kinetics-600, a linear classifier using CVRL representations achieves a top-1 accuracy of 70.4% with a 3D-ResNet-50 backbone, surpassing the performance of ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same R3D-50 model. Furthermore, the performance of CVRL can be enhanced to 72.9% with a larger R3D-152 (2x filters) backbone, significantly reducing the gap between unsupervised and supervised video representation learning. Our code and models will be made available at https://github.com/tensorflow/models/tree/master/official/.