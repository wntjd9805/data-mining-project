This abstract discusses the importance of accurate perception for autonomous robotic systems and self-driving cars. Specifically, it focuses on semantic segmentation, which provides semantic information about the surrounding environment. While previous methods for 3D LiDAR semantic segmentation have been introduced, they suffer from either high computational complexity or a lack of fine details for smaller objects. To address these issues, the authors propose (AF)2-S3Net, a CNN network that combines voxel-based and point-based learning methods. The network includes a multi-branch attentive feature fusion module in the encoder and an adaptive feature selection module with feature map re-weighting in the decoder. Experimental results demonstrate that the proposed method outperforms existing approaches on the nuScenes-lidarseg and SemanticKITTI benchmark datasets, ranking first in both competitions.