Current vision systems are proficient at recognizing patterns in visual stimuli, but their ability to contextualize these patterns within past or future events is lacking. This paper aims to address this limitation by studying long-form video understanding. The authors propose a framework for modeling long-form videos and establish evaluation protocols using extensive datasets. The research demonstrates that existing short-term models fall short when applied to long-form tasks. However, a new object-centric transformer-based video recognition architecture exhibits significantly improved performance across seven diverse tasks. Furthermore, this architecture outperforms comparable state-of-the-art methods on the AVA dataset.