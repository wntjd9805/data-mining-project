Human pose transfer is a challenging task in computer vision with various applications. While recent research has made progress in transferring the pose of a person's image from a source to a target pose, there is still room for improvement in capturing the semantic appearance accurately. The reconstructed results often lack consistency and realism in terms of textures. To address this issue, we propose a two-stage framework that focuses on both pose and appearance translation. In the first stage, we predict the target semantic parsing maps, which helps overcome the difficulties of pose transfer and facilitates the translation of per-region appearance style. This initial prediction serves as a foundation for the second stage. In the second stage, we introduce a new method for generating person images by incorporating region-adaptive normalization. This method utilizes per-region styles to guide the generation of the target appearance. Our proposed SPGNet (Semantic-Pose Guided Network) outperforms existing methods in terms of generating more semantic, consistent, and photo-realistic results. We have conducted extensive experiments and evaluated our method quantitatively and qualitatively, demonstrating its superiority. Additionally, we have made our source code and model available at https://github.com/cszy98/SPGNet.git.