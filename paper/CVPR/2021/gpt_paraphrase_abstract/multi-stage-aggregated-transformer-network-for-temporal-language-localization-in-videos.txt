We tackle the problem of localizing a specific moment in an untrimmed video using a natural language sentence query. Previous methods have not fully addressed two main challenges: effectively modeling the fine-grained visual-language alignment and accurately localizing the moment in the original video. To address these challenges, we propose a novel multi-stage aggregated transformer network for temporal language localization. Our approach includes a visual-language transformer backbone that allows iterations and alignments between visual and language sequences. Unlike previous methods, our backbone maintains a unified structure while being modality-specific. Additionally, we introduce a multi-stage aggregation module on top of the transformer backbone. This module computes stage-specific representations for different moment stages (starting, middle, and ending) for each video element. For a moment candidate, we concatenate the representations of its corresponding elements to form the final moment representation, which captures stage-specific information and enables accurate localization. Extensive experiments on ActivityNet Captions and TACoS datasets demonstrate the superior performance of our proposed method compared to other existing methods.