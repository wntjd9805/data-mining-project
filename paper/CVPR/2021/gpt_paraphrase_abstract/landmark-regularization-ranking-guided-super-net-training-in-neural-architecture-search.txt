Weight sharing is commonly used in neural architecture search (NAS) because it allows for efficient search on standard hardware. However, recent studies have revealed a discrepancy in performance rankings between stand-alone architectures and shared-weight networks. This undermines the fundamental assumption of weight-sharing NAS algorithms and hinders their effectiveness. To address this issue, we propose a regularization term that maximizes the correlation between the performance rankings of shared-weight networks and stand-alone architectures using a small set of landmark architectures. We integrate this regularization term into three different NAS algorithms and demonstrate its consistent performance improvement across various algorithms, search spaces, and tasks.