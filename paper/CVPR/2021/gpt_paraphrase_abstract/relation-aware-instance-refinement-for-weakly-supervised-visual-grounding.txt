Visual grounding refers to the process of connecting visual objects with their corresponding language entities, which is crucial for understanding cross-modal scenes. To achieve this, a popular approach is to use weak supervision from image-caption pairs. However, existing methods often suffer from inaccurate localization and ambiguous matching due to the lack of semantic relation constraints.In this paper, we propose a new context-aware weakly-supervised learning method that addresses these limitations. Our approach involves two stages: coarse-to-fine object refinement and entity relation modeling, both incorporated into a deep network. This allows us to generate more accurate object representations and improve matching accuracy.To effectively train our network, we introduce two loss functions. The first is a self-taught regression loss that helps improve the accuracy of proposal locations. The second is a classification loss that leverages parsed entity relations to enhance the matching process.We conducted extensive experiments on two publicly available benchmarks, namely Flickr30K Entities and Refer-ItGame. The results demonstrate the effectiveness of our weakly grounding framework, outperforming previous methods by a significant margin. Specifically, we achieved a top-1 accuracy of 59.27% in the Flickr30K Entities dataset and 37.68% in the ReferItGame dataset.Overall, our proposed method provides a novel and improved approach to weakly-supervised visual grounding, yielding more accurate results in cross-modal scene understanding tasks.