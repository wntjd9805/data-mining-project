This paper explores efficient training of neural networks on large-volume temporal data. The traditional method of backpropagation introduces high latency and limits real-time learning due to blocking computations. It also creates a coupling between consecutive layers, restricting model parallelism and increasing memory consumption. To address these issues, the authors propose building upon Side-ways, a method that propagates approximate gradients forward in time to avoid blocking. They also introduce different variants of skip connections for temporal integration of information. Additionally, the authors demonstrate how to decouple computation and distribute neural modules to different devices for parallel training. The proposed method, called Skip-Sideways, achieves low latency training, model parallelism, and the ability to extract temporal features. It improves performance on real-world action recognition video datasets, such as HMDB51, UCF101, and Kinetics-600. The authors also show that models trained with Skip-Sideways generate better future frames and can better utilize motion cues compared to Sideways models.