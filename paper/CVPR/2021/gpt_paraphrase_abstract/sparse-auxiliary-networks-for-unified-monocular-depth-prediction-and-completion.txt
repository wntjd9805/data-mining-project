This paper focuses on the importance of estimating scene geometry for robots and self-driving cars using cost-effective sensors. The specific problem addressed is predicting dense depth from a single RGB image, with the additional option of using sparse measurements from low-cost active depth sensors. To tackle this problem, the authors introduce Sparse Auxiliary Networks (SANs), a new module that allows monodepth networks to perform both depth prediction and completion tasks, depending on the availability of RGB images and sparse point clouds during inference. The authors propose two key methods: firstly, they separate the encoding stages of the image and depth map using sparse convolutions to process only valid depth map pixels. Secondly, when available, they integrate this information into the skip connections of the depth prediction network to enhance its features. The authors conducted extensive experiments on three benchmarks (NYUv2, KITTI, and DDAD) and demonstrate that their SAN architecture can simultaneously learn both tasks, achieving a significantly improved state-of-the-art performance in depth prediction.