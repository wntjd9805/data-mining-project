A new optimizer called AvaGrad is developed by analyzing adaptive methods. It is found that AvaGrad outperforms SGD on vision tasks when its adaptability is properly tuned. The success of AvaGrad is attributed to decoupling the learning rate and adaptability, which simplifies hyperparameter search. Surprisingly, it is discovered that Adam can also outperform SGD on vision tasks when the coupling between its learning rate and adaptability is considered. AvaGrad achieves the best results in terms of generalization accuracy compared to any existing optimizer (SGD or adaptive) across image classification and language modeling tasks. Additionally, AvaGrad shows improvements when used for training GANs.