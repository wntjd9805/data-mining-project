Scene text retrieval involves finding and searching for instances of text in an image gallery that are the same or similar to a given query text. This is typically done by matching the query text to recognized words outputted by a scene text spotter. In this paper, we propose a solution to this problem by directly learning a cross-modal similarity between the query text and each text instance in natural images. We accomplish this by developing an end-to-end trainable network that optimizes both scene text detection and cross-modal similarity learning. By ranking the detected text instances according to the learned similarity, scene text retrieval can be easily performed. Our method consistently outperforms state-of-the-art scene text spotting/retrieval approaches on three benchmark datasets. Notably, our framework of joint detection and similarity learning achieves significantly better performance than separate methods. The code for our approach is available at: https://github.com/lanfeng4659/STR-TDSL.