Recent advancements in Neural Architecture Search (NAS) have expanded the scope of research in the field, allowing for a wider range of vision tasks and more diverse search spaces. While previous NAS methods focused on designing architectures for single tasks, there is now a growing interest in developing algorithms that can efficiently and universally solve multiple tasks. These algorithms leverage transfer learning to preserve, reuse, and refine network design knowledge, thereby achieving higher efficiency in future tasks. However, the computational cost and complexity of cross-task NAS pose significant barriers to research in this direction. Existing NAS benchmarks primarily concentrate on classification tasks, neglecting other vision tasks. To address this limitation, we introduce TransNAS-Bench-101, a benchmark dataset that encompasses network performance across seven tasks, including classification, regression, pixel-level prediction, and self-supervised tasks. This diverse dataset enables the transfer of NAS methods across tasks and facilitates the evolution of complex transfer schemes. We explore two distinct types of search spaces: cell-level search space and macro-level search space. Our benchmark comprises 7,352 evaluated backbones on the seven tasks, resulting in 51,464 trained models with detailed training information. Through the introduction of TransNAS-Bench-101, we aim to stimulate the development of outstanding NAS algorithms that enhance cross-task search efficiency and generalizability. The dataset and code will be made publicly available at Mindspore1 and VEGA2.