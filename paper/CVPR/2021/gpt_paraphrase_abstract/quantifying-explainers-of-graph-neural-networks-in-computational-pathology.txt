Explainability is crucial for the clinical adoption of deep learning methods in digital pathology. However, current methods and explainability techniques often neglect the biological understanding of the images, making it difficult for pathologists to comprehend the results. In this study, we address this issue by using a biological entity-based graph processing approach and graph explainers, which provide explanations that are more accessible to pathologists. A major challenge in this context is to determine meaningful explainers in a standardized and quantifiable manner. To overcome this, we propose a set of novel quantitative metrics based on statistical analysis of class separability using pathologically measurable concepts. We apply these metrics to evaluate three types of graph explainers - layer-wise relevance propagation, gradient-based saliency, and graph pruning approaches - in explaining Cell-Graph representations for Breast Cancer Subtyping. These metrics can also be applied to other domains by utilizing domain-specific intuitive concepts. We validate our findings qualitatively and quantitatively using the BRACS dataset, a large cohort of breast cancer regions of interest, with the assistance of expert pathologists. The code, data, and models used in this study can be accessed [here](link).