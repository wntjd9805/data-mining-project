Companies often consider high-quality Machine Learning (ML) models as valuable intellectual property. However, adversaries with black-box access to these models can replicate their functionality through Model Stealing (MS) attacks. Existing MS attacks require access to the target dataset or a representative dataset to produce accurate clones. This paper introduces MAZE, a data-free model stealing attack that uses zeroth-order gradient estimation and synthetic data from a generative model. MAZE outperforms previous attacks, achieving a high clone accuracy range of 0.90 to 0.99 for four image classification models. Additionally, an extension of MAZE called MAZE-PD generates synthetic data closer to the target distribution, improving clone accuracy to 0.97 to 1.0 and reducing the required query budget by 2-24. This research demonstrates that preventing access to the target dataset is insufficient to protect ML models, highlighting the need for improved defenses against MS attacks.