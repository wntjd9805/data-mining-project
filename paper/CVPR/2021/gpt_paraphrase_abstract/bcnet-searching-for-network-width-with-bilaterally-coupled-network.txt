In recent times, there has been a focus on finding more compact network widths to address hardware limitations when deploying convolutional neural networks (CNNs). To achieve this, a one-shot supernet is commonly used to evaluate the performance of different network widths efficiently. However, existing methods often follow a unilaterally augmented principle, leading to unfair training of channels in the supernet. To tackle this issue, we propose a new supernet called Bilaterally Coupled Network (BCNet). In BCNet, each channel is trained fairly and is responsible for the same range of network widths, resulting in more accurate evaluation of each width. Additionally, we employ a stochastic complementary strategy to train BCNet and introduce a prior initial population sampling method to enhance the performance of the evolutionary search. Extensive experiments conducted on benchmark CIFAR-10 and ImageNet datasets demonstrate that our method achieves state-of-the-art or competitive performance compared to other baseline methods. Furthermore, our approach improves the performance of NAS models by refining their network widths. For instance, with the same FLOPs budget, our EfÔ¨ÅcientNet-B0 achieves a Top-1 accuracy of 77.36% on the ImageNet dataset, surpassing the performance of the original setting by 0.48%.