Controllable Image Captioning (CIC), which involves generating image descriptions based on specific control signals, has gained significant attention in recent years. Existing CIC studies have focused on objective control signals related to content and descriptive patterns, aiming to mimic human control over caption generation. However, these signals often overlook two crucial characteristics of an ideal control signal: 1) Event-compatibility, meaning that all visual contents mentioned in a sentence should be relevant to the described activity, and 2) Sample-suitability, indicating that control signals should be appropriate for a specific image sample.To address these limitations, we propose a new control signal called Verb-specific Semantic Roles (VSR) for CIC. VSR comprises a verb and associated semantic roles, representing a targeted activity and the entities involved in it. Our approach involves training a grounded semantic role labeling (GSRL) model to identify and ground entities for each role. We then utilize a semantic structure planner (SSP) to learn descriptive semantic structures similar to those produced by humans. Finally, a role-shift captioning model is used to generate captions.Extensive experiments and ablations demonstrate that our framework offers better controllability compared to several strong baselines on challenging CIC benchmarks. Additionally, our approach facilitates the generation of diverse captions at multiple levels. The code for our framework is available at: https://github.com/mad-red/VSR-guided-CIC.