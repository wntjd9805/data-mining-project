Text-video retrieval is a challenging task that aims to find relevant video contents based on natural language descriptions. Most existing methods only consider overall similarity between text and video, neglecting important local details. Some approaches incorporate local comparisons through complex operations, which introduce significant computation. In this paper, we propose an efficient global-local alignment method. We adaptively aggregate multi-modal video sequences and text features using shared semantic centers. Local cross-modal similarities are computed within each center, allowing for meticulous local comparisons while reducing computational costs. Additionally, we introduce a global alignment method that provides a complementary cross-modal measurement from a global perspective. The global aggregated visual features also offer additional supervision for optimizing the learnable semantic centers. Our proposed method achieves consistent improvements on three standard text-video retrieval benchmarks, surpassing the state-of-the-art by a significant margin.