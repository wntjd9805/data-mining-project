Creating benchmarks for human behavior understanding with unmanned aerial vehicles (UAVs) is crucial for various applications. However, current benchmarks have limitations in terms of data quantity, modalities, task categories, and subjects and environments diversity. To address this, we introduce UAV-Human, a new benchmark for UAV-based human behavior understanding. This benchmark includes a large dataset of 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. The dataset was collected over three months using a UAV, covering diverse urban and rural areas in both daytime and nighttime. This ensures variations in subjects, backgrounds, lighting, weather, occlusions, camera motion, and UAV flight angles. The comprehensive and challenging UAV-Human benchmark aims to advance research in UAV-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition. Additionally, we propose a fish-eye based action recognition method that minimizes distortions in fish-eye videos by learning unbounded transformations guided by RGB videos. Experimental results demonstrate the effectiveness of our method on the UAV-Human dataset.