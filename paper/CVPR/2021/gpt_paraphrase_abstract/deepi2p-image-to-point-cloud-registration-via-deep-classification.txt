This paper introduces DeepI2P, a new method for registering an image from an RGB camera with a point cloud. The goal is to estimate the transformation between the camera and Lidar coordinate frames when the data is captured at different locations in the same scene. Traditional methods for establishing correspondences between the two modalities are challenging due to the lack of appearance and geometric correlations. To overcome this challenge, we propose converting the registration problem into a classification and inverse camera projection optimization problem. We design a neural network that classifies whether each point in the point cloud is within or beyond the camera frustum. These labeled points are then used in an inverse camera projection solver to estimate the relative pose. We evaluate our approach on the Oxford Robotcar and KITTI datasets, showing its feasibility. The source code for our method is available at https://github.com/lijx10/DeepI2P.