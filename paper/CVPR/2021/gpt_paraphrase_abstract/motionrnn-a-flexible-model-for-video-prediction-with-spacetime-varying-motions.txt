This paper addresses the issue of predicting video frames with spacetime-varying motions that are constantly changing across both space and time. Previous methods focus on capturing temporal state transitions but overlook the complex spatiotemporal variations of the motion itself, making them less adaptable to changing motions. The authors observe that physical world motions can be divided into transient variation and motion trend, with the latter being the accumulation of previous motions. Therefore, simultaneously capturing the transient variation and motion trend is crucial for predicting spacetime-varying motions effectively. To achieve this, the authors propose the MotionRNN framework, which consists of the MotionGRU unit that can model both the transient variation and motion trend in a unified manner. Additionally, they introduce a Motion Highway that can be applied to RNN-based predictive models, improving the ability to predict changing motions and preventing motion vanishing in stacked multiple-layer predictive models. The MotionRNN framework is highly flexible and can be adapted to various models for deterministic spatiotemporal prediction. Experimental results show that MotionRNN significantly improves performance on three challenging benchmarks for video prediction with spacetime-varying motions.