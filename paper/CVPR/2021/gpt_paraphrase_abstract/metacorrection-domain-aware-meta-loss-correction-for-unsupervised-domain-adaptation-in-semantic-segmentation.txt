Unsupervised domain adaptation (UDA) is a technique that aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Existing UDA approaches use self-training and assign pseudo labels to the target data, treating them as ground truth labels to make the most of the unlabeled target data for model adaptation. However, these pseudo labels generated from the model optimized on the source domain often contain noise due to the domain gap. To address this issue, we propose a MetaCorrection framework for UDA semantic segmentation, which incorporates a Domain-aware Meta-learning strategy called Loss Correction (DMLC).   In our framework, we model the distribution of noise in the pseudo labels of the target domain by introducing a noise transition matrix (NTM). We also construct a meta dataset using domain-invariant source data to guide the estimation of the NTM. By minimizing the risk on the meta dataset, we optimize the NTM to correct the noisy pseudo labels and improve the generalization ability of the model on the target data. Additionally, considering the difference in capacity between shallow and deep features, we employ the DMLC strategy to provide appropriate supervision signals for different levels of features, ensuring effective deep adaptation.  We conducted extensive experiments and compared our method against existing state-of-the-art methods on three benchmark datasets. The results demonstrate the effectiveness of our approach, as it outperforms the existing methods in terms of performance.