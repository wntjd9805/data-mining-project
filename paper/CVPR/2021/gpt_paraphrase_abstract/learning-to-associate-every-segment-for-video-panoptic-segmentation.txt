The concept of temporal correspondence, which involves linking pixels or objects across frames in videos, is crucial for supervising video models. In order to achieve a comprehensive understanding of dynamic scenes, we expand this concept to encompass every segment. Our objective is to simultaneously learn coarse segment-level matching and fine pixel-level matching. To accomplish this, we propose two innovative learning objectives. To validate our proposals, we employ a deep siamese model and train it to learn temporal correspondence at both the segment and pixel levels, in addition to the target task. During inference, our model independently processes each frame without any additional computation or post-processing. Our per-frame inference model demonstrates superior performance on the Cityscapes-VPS and VIPER datasets, surpassing the previous state-of-the-art results. Furthermore, our model is highly efficient, running in a fraction of the time (3Ã—) compared to the previous leading approach.