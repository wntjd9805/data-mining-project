Catastrophic forgetting, where Deep Neural Networks (DNN) lose knowledge of earlier tasks when learning new ones, is a significant challenge. Existing methods, such as piggyback, use binary masks to address this issue but have limited modeling capacity. Another method, compress-grow-based, partially trains the backbone model but has high training cost. This study aims to achieve fast and accurate multi-task adaptation in continual learning. To achieve this, a new training method called Kernel-wise Soft Mask (KSM) is proposed. KSM learns a hybrid binary and real-value soft mask for each task, providing a richer representation capability without requiring low-level kernel support. Experimental results on benchmark datasets show that KSM outperforms state-of-the-art methods in terms of accuracy and training cost.