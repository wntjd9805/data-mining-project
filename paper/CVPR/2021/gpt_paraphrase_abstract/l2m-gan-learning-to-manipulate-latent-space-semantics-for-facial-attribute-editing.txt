A new facial attribute editing model, called L2M-GAN, is proposed to address the challenges of attribute correctness and irrelevance preservation. Existing methods rely on either spatial attention or latent space factorization, but they have limitations in effectively editing both local and global attributes. L2M-GAN overcomes these limitations by factorizing the latent space vector of the GAN into attribute-relevant and irrelevant codes, ensuring disentanglement through an orthogonality constraint. Additionally, an attribute-relevant code transformer is learned to manipulate the attribute value, subject to the same orthogonality constraint. This approach achieves a balance between attribute correctness and irrelevance preservation. Experimental results on CelebA-HQ demonstrate that L2M-GAN outperforms existing methods.