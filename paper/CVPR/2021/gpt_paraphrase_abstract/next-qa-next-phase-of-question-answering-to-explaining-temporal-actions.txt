We present NExT-QA, a meticulously designed benchmark for video question answering (VideoQA) that aims to enhance video comprehension by focusing on explaining temporal actions. Our benchmark includes multi-choice and open-ended QA tasks that target causal action reasoning, temporal action reasoning, and general scene comprehension. By analyzing various baselines and established VideoQA techniques, we discovered that the top-performing methods excel at providing shallow scene descriptions but struggle with causal and temporal action reasoning. Moreover, even models that perform well on multi-choice QA struggle to generalize their answers when applied to open-ended QA. This raises concerns about these models' ability to reason and suggests opportunities for improvement. We provide detailed results for different question types and offer heuristic observations for future research. With NExT-QA, we aim to guide the next generation of VQA research towards a deeper understanding of videos, moving beyond superficial descriptions. (The dataset and related resources can be accessed at https://github.com/doc-doc/NExT-QA.git). The answer format only consists of the abstract.