This paper introduces Panoptic FCN, a framework for panoptic segmentation that is simple, robust, and efficient. The goal is to accurately represent and predict both foreground objects and background elements in a unified fully convolutional pipeline. Panoptic FCN achieves this by assigning each object instance or background category to a specific kernel weight using a novel kernel generator. The prediction is then made by convolving the high-resolution feature directly. This approach allows for instance-aware and semantically consistent segmentation of objects and background without the need for additional localization or instance separation steps. The proposed approach outperforms previous models that rely on bounding boxes or do not use them, achieving high efficiency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. The code for Panoptic FCN is publicly available at https://github.com/Jia-Research-Lab/PanopticFCN.