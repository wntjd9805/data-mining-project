Adversarial attacks are crucial for understanding and enhancing the robustness of deep neural networks. Currently, attack methods focus on manipulating RGB images directly fed to convolutional neural network (CNN) classifiers, without considering the impact of camera optics and image processing pipeline (ISP) that generate the network inputs. However, these low-level pipelines can alter adversarial patterns, making them ineffective or even introducing new patterns that deceive downstream detectors. This research investigates and develops an attack that specifically deceives a particular camera ISP while leaving others unaffected, using the same downstream classifier. The attack is framed as a multi-task optimization problem, utilizing a differentiable approximation for the ISP. The proposed method is tested on state-of-the-art automotive hardware ISPs, achieving a 92% fooling rate against a specific ISP. Additionally, physical optics attacks are demonstrated with a 90% fooling rate for a specific camera lens.