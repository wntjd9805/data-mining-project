Recent semantic segmentation methods typically use a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder part of the network decreases the spatial resolution while learning abstract and semantic visual concepts with larger receptive fields. However, although context modeling is crucial for segmentation, efforts have mainly focused on increasing the receptive field by employing dilated/atrous convolutions or attention modules, while keeping the same encoder-decoder based FCN architecture. This paper proposes an alternative approach by treating semantic segmentation as a sequence-to-sequence prediction task. Instead of using convolutions and reducing resolution, a pure transformer is employed to encode an image as a sequence of patches. The transformer-based encoder captures global context information in every layer, enabling it to be combined with a simple decoder to create a powerful segmentation model known as SETR (SEgmentation TRansformer). Extensive experiments demonstrate that SETR achieves state-of-the-art performance on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU), and competitive results on Cityscapes. Notably, SETR secures the top position on the ADE20K test server leaderboard on the day of submission.