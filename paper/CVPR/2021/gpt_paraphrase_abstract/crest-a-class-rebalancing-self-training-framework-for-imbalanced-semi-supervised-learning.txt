Semi-supervised learning methods have not adequately addressed the issue of class imbalance in data. Although these methods perform poorly on minority classes, they still produce accurate pseudo-labels for these classes. In this study, we introduce Class-Rebalancing Self-Training (CReST), a straightforward yet effective approach to enhance existing semi-supervised learning methods on class-imbalanced data. CReST repeatedly trains a baseline SSL model using a labeled set that is expanded by incorporating pseudo-labeled samples from an unlabeled set. Notably, we select pseudo-labeled samples from minority classes more frequently based on an estimated class distribution. Additionally, we propose CReST+, which employs progressive distribution alignment to dynamically adjust the rebalancing strength. Our results demonstrate that CReST and CReST+ improve the performance of state-of-the-art SSL algorithms on various class-imbalanced datasets, consistently surpassing other popular rebalancing techniques. The code for CReST is available at https://github.com/google-research/crest.