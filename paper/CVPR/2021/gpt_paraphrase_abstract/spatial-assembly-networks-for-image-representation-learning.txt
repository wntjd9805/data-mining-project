Deep neural networks are known to be sensitive to changes in spatial configurations and scene structures. While image augmentations can enhance the robustness of neural networks under spatial transforms, they may still struggle with changes in object part configurations, spatial layout, and scene structures. To tackle this issue, we propose a learnable module called spatial assembly network (SAN). The SAN module reorganizes and assembles feature points from different spatial locations based on feature maps from previous network layers to maximize the discriminative power of the final feature representation. This differentiable module can be seamlessly integrated into existing network architectures, enhancing their ability to handle spatial variations and structural changes in images. Our experiments demonstrate that the SAN module significantly improves the performance of various metric and representation learning tasks, including image retrieval and classification, in both supervised and unsupervised learning scenarios.