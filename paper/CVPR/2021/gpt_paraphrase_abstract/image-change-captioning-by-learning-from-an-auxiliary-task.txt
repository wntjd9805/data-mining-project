We address the difficult task of generating captions that describe the subtle differences between two similar images. While existing methods primarily focus on developing new model architectures, we propose an alternative training approach. Drawing inspiration from multi-task learning, we introduce an auxiliary task to enhance the training of the change captioning network. We believe that the task of composed query image retrieval is suitable as the auxiliary task. The primary network generates a caption that describes the fine changes between the input images, and then the auxiliary network receives this caption along with one of the images and aims to select the second image from a set of candidates. This setup compels the primary network to produce detailed and precise captions by imposing an additional supervision loss from the auxiliary network. Additionally, we present a novel method for selecting a negative set of candidates for the retrieval task, which effectively enhances performance. Our proposed training strategy demonstrates strong performance on benchmark datasets for the task of change captioning.