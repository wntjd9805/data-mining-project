Human pose estimation from single images is a challenging problem in computer vision that requires a large amount of labeled training data. However, for activities like outdoor sports, such training data is often unavailable and difficult to obtain using traditional motion capture systems. In this study, we propose a self-supervised approach that learns a 3D pose estimator from unlabeled multi-view data. We achieve this by utilizing multi-view consistency constraints to separate the observed 2D pose into its underlying 3D pose and camera rotation. Unlike most existing methods, our approach does not require calibrated cameras and can learn from moving cameras. Additionally, we offer an optional extension for static camera setups, which includes constant relative camera rotations across multiple views. The success of our approach relies on new, unbiased reconstruction objectives that combine information from different views and training samples. We evaluate our method on two benchmark datasets (Human3.6M and MPII-INF-3DHP) as well as the SkiPose dataset, which contains real-world scenarios.