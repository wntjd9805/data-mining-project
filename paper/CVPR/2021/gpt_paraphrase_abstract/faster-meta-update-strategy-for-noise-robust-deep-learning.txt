Deep neural networks have been found to have a tendency to overfit when trained on biased data. To address this issue, meta-learning has been used to correct for training bias by employing a meta model. However, the main drawback of current meta learning approaches is the slow training process. In this study, we propose a new strategy called Faster Meta Update Strategy (FaMUS) to accelerate the meta gradient computation by replacing the most time-consuming step with a faster layer-wise approximation. Our empirical results demonstrate that FaMUS not only provides a reasonably accurate approximation of the meta gradient, but also reduces its variance. We extensively evaluate our method on two tasks and find that it can save two-thirds of the training time while still achieving comparable or even better generalization performance. Specifically, our method achieves state-of-the-art performance on both synthetic and realistic noisy labels, and shows promising results on long-tailed recognition using standard benchmarks. The code for our method is publicly available at https://github.com/youjiangxu/FaMUS.