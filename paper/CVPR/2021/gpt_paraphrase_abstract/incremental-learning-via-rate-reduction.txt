Current deep learning architectures struggle with catastrophic forgetting, which refers to their inability to retain knowledge of previously learned classes when trained on new classes. This is due to the black box nature of these models, making it challenging to adjust the model parameters effectively and preserve knowledge of previously seen data. To address this issue, we propose a white box architecture based on the principle of rate reduction. In this architecture, each layer of the network is explicitly computed without back propagation. By employing this approach, we demonstrate that, when given a pretrained network and new data classes, our method can construct a new network that emulates joint training with both past and new classes. Our experiments reveal that our learning algorithm exhibits significantly less decay in classification performance compared to state-of-the-art methods on MNIST and CIFAR-10 datasets. This substantial improvement justifies the use of white box algorithms for incremental learning, even with complex image data.