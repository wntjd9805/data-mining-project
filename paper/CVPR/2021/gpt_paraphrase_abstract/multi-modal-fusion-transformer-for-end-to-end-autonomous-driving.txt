This abstract discusses the integration of representations from complementary sensors for autonomous driving. While geometry-based sensor fusion has been successful for perception tasks, it may not be sufficient for the driving task, which requires considering the global context of the 3D scene. The authors demonstrate that imitation learning policies based on existing sensor fusion methods do not perform well in complex scenarios with a high density of dynamic agents, where global contextual reasoning is needed. To address this, they propose TransFuser, a Multi-Modal FusionTransformer that uses attention to integrate image and LiDAR representations. The efficacy of their approach is experimentally validated in urban settings using the CARLA urban driving simulator, achieving state-of-the-art driving performance and reducing collisions by 76% compared to geometry-based fusion.