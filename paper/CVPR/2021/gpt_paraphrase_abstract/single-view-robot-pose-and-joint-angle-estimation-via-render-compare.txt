We present RoboPose, a technique for estimating the joint angles and camera-to-robot pose of an articulated robot using a single RGB image. This is crucial for enabling mobile and itinerant autonomous systems to interact with other robots in non-instrumented environments, particularly in collaborative robotics. The challenge lies in the multiple degrees of freedom and infinite possible configurations of robots, leading to self-occlusions and depth ambiguities when captured by a single camera. Our work offers three key contributions. Firstly, we introduce a novel render & compare approach that can be trained on synthetic data, generalizes to unseen robot configurations during testing, and can be applied to various robots. Secondly, we emphasize the importance of robot parametrization for iterative pose updates and develop a parametrization strategy that is independent of the robot's structure. Lastly, we provide experimental results on benchmark datasets featuring four different robots, demonstrating that our method significantly outperforms the current state of the art. Code and pre-trained models can be accessed on our project webpage [1].