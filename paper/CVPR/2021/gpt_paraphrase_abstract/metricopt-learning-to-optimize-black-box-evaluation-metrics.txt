We investigate the problem of optimizing non-differentiable task evaluation metrics such as misclassification rate and recall directly. Our approach, called MetricOpt, operates in a black-box setting where the specific details of the target metric are unknown. To address this, we train a differentiable value function that maps compact model parameters to metric observations. This learned value function can be easily incorporated into existing optimizers like SGD and Adam, enabling efficient fine-tuning of pre-trained models. By providing effective metric supervision during fine-tuning, the value function helps improve the model's performance consistently, while also correcting potential biases introduced by relying solely on loss supervision. MetricOpt outperforms competing methods that involve complex loss design or adaptation and achieves state-of-the-art results in various tasks such as image classification, image retrieval, and object detection. Furthermore, MetricOpt demonstrates good generalization to new tasks and model architectures.