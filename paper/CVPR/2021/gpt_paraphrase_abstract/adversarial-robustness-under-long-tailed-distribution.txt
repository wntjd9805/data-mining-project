Adversarial robustness has been extensively studied in recent years, revealing the vulnerabilities and characteristics of deep networks. However, most existing research in this area focuses on balanced datasets, which do not accurately represent real-world data that often exhibits a long-tailed distribution. To address this limitation and make adversarial robustness more applicable to realistic scenarios, this study investigates the vulnerability and defense mechanisms under long-tailed distributions.The negative impacts of imbalanced data on both recognition performance and adversarial robustness are first identified, highlighting the inherent challenges of this problem. The study then systematically examines existing methods for long-tailed recognition in conjunction with the adversarial training framework, leading to several important observations. Firstly, improving natural accuracy is relatively easier compared to robust accuracy. Secondly, there is a tendency for fake gains of robust accuracy under unreliable evaluation. Lastly, the promotion of robustness is limited by boundary errors.Motivated by these findings, a novel framework called RoBal is proposed. This framework consists of two dedicated modules: a scale-invariant classifier and data rebalancing through margin engineering during training and boundary adjustment during inference. Extensive experiments demonstrate the superior performance of RoBal compared to other state-of-the-art defense methods.This study is the first to address adversarial robustness under long-tailed distributions, representing a significant step towards achieving real-world robustness. The code for the proposed framework is publicly available for further exploration and use.