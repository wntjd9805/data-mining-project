OCR-based image captioning is a method that automatically describes images by considering both visual objects and scene text. Unlike conventional image captioning, OCR-based image captioning requires the understanding of scene text, as the generated descriptions often consist of multiple OCR tokens. Previous approaches have attempted to achieve this by encoding the OCR tokens using visual and semantic representations. However, these limited representations may not establish strong correlations between OCR tokens. To address this issue, we propose enhancing the connections between OCR tokens by exploiting their geometric relationship. We consider various geometric properties such as height, width, distance, IoU, and orientation relations between OCR tokens to construct this relationship. To incorporate the learned relation, visual and semantic representations into a unified framework, we introduce the Long Short-Term Memory plus Relation-aware pointer network (LSTM-R) architecture. By leveraging the geometric relationship between OCR tokens, our LSTM-R utilizes a newly devised relation-aware pointer network to select OCR tokens from the scene text for OCR-based image captioning. Extensive experiments demonstrate the effectiveness of our LSTM-R, achieving state-of-the-art performance on TextCaps with a significant increase in the CIDEr-D score from 98.0% to 109.3%.