The aim of this study is to develop a language-based search system for large image and video datasets. One effective approach is to use dual encoders, which independently map text and vision to a joint embedding space. This approach is efficient for billions of images using approximate nearest neighbour search. However, using vision-text transformers with cross-attention has shown to provide better accuracy than joint embeddings. Unfortunately, the cost of cross-attention mechanisms makes this approach impractical for large-scale retrieval. To address this issue, this study combines the advantages of both approaches. Firstly, the researchers enhance transformer-based models with a new fine-grained cross-attention architecture, which significantly improves retrieval accuracy while maintaining scalability. Secondly, they propose a method to combine a fast dual encoder model with a slow but accurate transformer-based model through distillation and re-ranking. Finally, the researchers validate their approach using the Flickr30K image dataset, where they demonstrate a significant increase in inference speed while achieving competitive results compared to the state of the art. The method is also extended to the video domain, improving the state of the art on the VATEX dataset.