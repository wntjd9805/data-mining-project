The synthesis of three-dimensional (3D) human motion is crucial in graphics applications and understanding human activity. However, most existing approaches to generating realistic human motion overlook the importance of modeling human-scene interactions and affordance. Similarly, affordance reasoning, which considers actions that can be performed in a given environment, has mainly been studied with static human poses and gestures, rather than human motion. This paper aims to bridge the gap between human motion synthesis and scene affordance reasoning. The proposed approach is a hierarchical generative framework that synthesizes long-term 3D human motion based on the structure of the scene. Additionally, the framework incorporates multiple geometry constraints through optimization to enhance the realism of the generated motion. Experimental results demonstrate significant improvements compared to previous methods in generating natural and physically plausible human motion in a given scene.