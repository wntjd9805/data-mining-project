Recent advancements in semi-supervised learning have demonstrated significant progress in utilizing both labeled and unlabeled data. However, most of these studies assume a randomly initialized model. In this research, we propose a practical and competitive approach by combining semi-supervised learning with transfer learning. This approach allows us to leverage powerful pre-trained models from a source domain and labeled/unlabeled data from the target domain. To effectively utilize pre-trained weights and unlabeled target examples, we introduce adaptive consistency regularization, which includes two complementary components: Adaptive Knowledge Consistency (AKC) for examples between the source and target model, and Adaptive Representation Consistency (ARC) for the target model between labeled and unlabeled examples. We dynamically select examples for consistency regularization based on their potential contributions to the target task. Our experiments on popular benchmarks such as CIFAR-10, CUB-200, and MURA, using the fine-tuned ImageNet pre-trained ResNet-50 model, demonstrate that our proposed adaptive consistency regularization outperforms state-of-the-art semi-supervised learning techniques like Pseudo Label, Mean Teacher, and FixMatch. Additionally, our algorithm can be combined with existing methods, leading to further improvements on top of MixMatch and FixMatch. The code for our approach is available at https://github.com/Walleclipse/Semi-Supervised-Transfer-Learning-Paddle.