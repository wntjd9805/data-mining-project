UC2 is a novel framework that combines machine translation and vision-and-language pre-training to learn cross-lingual cross-modal representations. The scarcity of multilingual captions for image datasets is addressed by augmenting English-only datasets with other languages using machine translation. The framework extends the Masked Language Modeling and Image-Text Matching training objectives to a multilingual setting, capturing alignment between languages through shared visual context. Two new pre-training tasks, Masked Region-to-Token Modeling and Visual Translation Language Modeling, are proposed to facilitate the learning of a joint embedding space for images and multiple languages. Evaluation on multilingual image-text retrieval and visual question answering benchmarks shows that UC2 achieves state-of-the-art performance on non-English benchmarks while maintaining comparable performance to monolingual models on English tasks.