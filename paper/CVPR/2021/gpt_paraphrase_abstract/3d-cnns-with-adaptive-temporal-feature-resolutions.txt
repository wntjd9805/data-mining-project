State-of-the-art 3D Convolutional Neural Networks (CNN) have shown impressive results in action recognition datasets. However, they are computationally expensive and require significant GFLOPs. To address this issue, researchers have attempted to decrease the GFLOPs of 3D CNNs by reducing the temporal feature resolution within the network. However, finding an optimal setting for all input clips has proven challenging.  In this study, a new approach called Similarity Guided Sampling (SGS) module is introduced. This module is designed to be compatible with any existing 3D CNN architecture. The SGS module enhances the capabilities of 3D CNNs by learning the similarity of temporal features and grouping similar features together. This allows the temporal feature resolution to vary for each input video clip, rather than being static.  By integrating the SGS module as an additional layer within current 3D CNNs, the researchers were able to transform them into more efficient models with adaptive temporal feature resolutions (ATFR). Through evaluations, it was found that the proposed module significantly reduces the computational cost (GFLOPs) by half, while maintaining or even improving the accuracy of the models.  To validate the effectiveness of the SGS module, it was added to multiple state-of-the-art 3D CNNs and evaluated on various datasets including Kinetics-600, Kinetics-400, mini-Kinetics, Something-Something V2, UCF101, and HMDB51.  Overall, this study presents a novel approach to enhance the efficiency of 3D CNNs through the use of the SGS module, which adaptively adjusts the temporal feature resolution based on the similarity of features. The results demonstrate significant computational cost reduction without sacrificing accuracy, making it a valuable contribution to the field of action recognition.