The existing research on visual counting has mainly focused on counting specific categories such as people, animals, and cells. However, this paper aims to count objects from any category by using only a few annotated instances. The approach taken is to treat counting as a few-shot regression task. A novel method is proposed that takes a query image and a few exemplar objects from that image to predict a density map for the presence of all objects of interest. Additionally, a novel adaptation strategy is introduced to adapt the network to any novel visual category during testing, using only a few exemplar objects from that category. A dataset containing 147 object categories with over 6000 annotated images is also presented, which is suitable for developing few-shot counting models. Experimental results on this dataset demonstrate that the proposed method outperforms several state-of-the-art object detectors and few-shot counting approaches. The code and dataset can be accessed at https://github.com/cvlab- stonybrook/LearningToCountEverything.