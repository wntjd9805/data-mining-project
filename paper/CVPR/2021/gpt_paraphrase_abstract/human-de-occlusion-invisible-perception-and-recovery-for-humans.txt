This paper presents a solution to the problem of human de-occlusion, which involves determining the occluded segmentation masks and invisible appearance content of humans. The proposed approach consists of a two-stage framework. In the first stage, a stacked network structure is used to refine inaccurate masks obtained from a general instance segmentation model and predict integrated masks simultaneously. This stage also incorporates guidance from human parsing and typical pose masks to improve accuracy. In the second stage, a novel parsing guided attention module is employed to isolate body parts and capture context information across multiple scales to recover the content. To facilitate the research on human de-occlusion, the authors have created the Amodal Human Perception dataset (AHP), which offers annotations from real-world scenes and a larger number of humans compared to other amodal perception datasets. Experimental results using the AHP dataset demonstrate that the proposed method outperforms state-of-the-art techniques in both mask completion and content recovery tasks. The AHP dataset is publicly available at https://sydney0zq.github.io/ahp/.