Gradient compression is widely used in distributed training of deep neural networks to reduce communication overhead. While some methods treat all gradients equally, we believe that large and small gradients should be treated differently as they represent exploitation and exploration of gradient information, respectively. In this paper, we propose a novel gradient compressor called Gradient Sampling with Bayes Prior. We sample important/large gradients based on the global gradient distribution and periodically update it across multiple workers. We also introduce Bayes Prior into the distribution model to further explore the gradients. Our method is proven to converge for smooth non-convex problems in distributed systems. Compared to methods that prioritize high compression ratio at the expense of accuracy, we aim for no loss of accuracy and actual acceleration benefits in practice. Experimental comparisons in computer vision tasks and with various backbones demonstrate that our approach surpasses state-of-the-art techniques in terms of speed and accuracy, with a compression ratio limit of 100Ã—.