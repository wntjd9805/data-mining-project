We present a framework for acquiring strong visual representations that can adapt to different viewpoints, backgrounds, and scenes. Discriminative models often learn associations that occur naturally but are not applicable to images outside the training set. In this study, we demonstrate that we can guide generative models to produce alterations in features influenced by confounding factors. Through experiments, visualizations, and theoretical findings, we prove that this approach enables the learning of robust representations that align with the underlying causal relationships. Our method significantly enhances performance on various datasets requiring generalization beyond the training distribution. Additionally, we achieve state-of-the-art results in generalizing from ImageNet to ObjectNet dataset.