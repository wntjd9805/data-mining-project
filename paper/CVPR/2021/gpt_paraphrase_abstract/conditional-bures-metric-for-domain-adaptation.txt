Unsupervised domain adaptation (UDA) is a significant issue in classification-oriented transfer learning. While previous UDA methods have focused on the shift in marginal distributions between domains, they have neglected the discriminant information in label distributions, resulting in decreased classification performance in real-world applications. This study addresses the problem of conditional distribution shift, which is a key concern for current conditional invariant models. We propose a novel approach to tackle this problem by introducing a kernel covariance embedding for conditional distribution, which has not been explored before. We introduce the Conditional Kernel Bures (CKB) metric to measure the discrepancy between conditional distributions and develop an empirical estimation for the CKB metric without the need for an implicit kernel feature map. This approach provides a interpretable way to understand the knowledge transfer mechanism. Additionally, we establish a consistency theory for the empirical estimation, which guarantees convergence. We propose a conditional distribution matching network that learns both conditional invariant and discriminative features for UDA. Through extensive experiments and analysis, we demonstrate the superiority of our proposed model.