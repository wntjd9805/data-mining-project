The abstract discusses the need for benchmarks in developing computer vision models that can analyze and reason about compositional spatio-temporal events. Existing video question answering benchmarks are limited in their ability to pinpoint model weaknesses due to their conflation of multiple sources of error and biases that models can exploit. The authors introduce a new benchmark called Action Genome Question Answering (AGQA) which contains a large dataset of unbalanced question-answer pairs for videos. They also provide a balanced subset of question-answer pairs that minimizes bias and is three orders of magnitude larger than existing benchmarks. The evaluation of AGQA shows that while human evaluators marked a high percentage of question-answer pairs as correct, the best model achieved only a low accuracy rate. AGQA also introduces multiple training/test splits to evaluate different reasoning abilities, including generalization to novel compositions and indirect references. The evaluation of visual reasoning systems using AGQA demonstrates that the best models perform only slightly better than non-visual baselines that exploit linguistic biases, and none of the existing models generalize to novel compositions that were unseen during training. The abstract concludes by mentioning that the answer format outputs only the abstraction.