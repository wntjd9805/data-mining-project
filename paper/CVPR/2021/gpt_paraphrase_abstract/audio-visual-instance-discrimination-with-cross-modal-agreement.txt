We propose a self-supervised learning method for acquiring audio-visual representations from video and audio. Our approach utilizes contrastive learning to discriminate between video and audio in a cross-modal manner. We demonstrate that optimizing for cross-modal discrimination, rather than within-modal discrimination, is crucial for obtaining high-quality representations from both modalities. By adopting this simple yet effective strategy, our method achieves competitive performance when fine-tuned for action recognition tasks. Additionally, we extend the concept of positive and negative samples in contrastive learning by considering cross-modal agreement. Instead of treating individual instances as positives and negatives, we group multiple instances together based on their similarity in both the video and audio feature spaces. This approach improves the quality of positive and negative sets, enabling us to calibrate visual similarities through within-modal discrimination of positive instances. As a result, we observe significant improvements in downstream tasks.