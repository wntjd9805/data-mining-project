ImageNet is a widely used benchmark for image classification, but it suffers from label noise, as many samples actually contain multiple classes instead of just one. Previous studies proposed turning ImageNet into a multi-label task by providing exhaustive multi-label annotations for each image. However, these studies did not address the issue in the training set due to the high cost of annotation. In this paper, we argue that the mismatch between single-label annotations and multi-label images is problematic during training, especially when random crops are used. Random crops may contain different objects than the ground truth, resulting in noisy or incorrect supervision during training. To overcome this, we re-label the ImageNet training set with multi-labels. To tackle the annotation cost barrier, we use a strong image classifier trained on additional data to generate the multi-labels. We utilize the pixel-wise multi-label predictions before the final pooling layer to take advantage of location-specific supervision signals. Training on the re-labeled samples leads to improved model performance. With our localized multi-labels, ResNet-50 achieves a top-1 accuracy of 78.9% on ImageNet, which can be further improved to 80.2% with the CutMix regularization technique. We demonstrate that models trained with localized multi-labels also outperform baselines on transfer learning tasks and robustness benchmarks. The re-labeled ImageNet training set, pre-trained weights, and source code are available at https://github.com/naver-ai/relabel_imagenet.