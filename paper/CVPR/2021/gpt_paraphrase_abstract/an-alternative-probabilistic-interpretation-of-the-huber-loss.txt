The Huber loss is a popular loss function for regression tasks, but selecting the appropriate parameter for transitioning between quadratic and absolute value functions is challenging. The traditional probabilistic interpretation of the Huber loss fails to provide sufficient intuition for determining the transition point, often requiring a hyper-parameter search. In this study, we propose an alternative probabilistic interpretation that connects minimizing the Huber loss with minimizing an upper-bound on the Kullback-Leibler divergence between Laplace distributions. These distributions represent the noise in the ground-truth and predicted data. Furthermore, we establish a direct relationship between the parameters of the Laplace distributions and the transition point of the Huber loss. By analyzing a toy problem, we demonstrate that the optimal transition point is closely linked to the noise distribution in the ground-truth data. Our interpretation offers an intuitive way to identify suitable hyper-parameters by approximating the noise level in the data. We validate this approach through a case study and experimentation on the Faster R-CNN and RetinaNet object detectors.