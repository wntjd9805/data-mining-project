The increasing use of 3D sensors has sparked interest in analyzing point cloud data in more detail. However, this data is challenging for machines to perceive due to its raw nature. This study focuses on semantic segmentation, a visual task, for large-scale point cloud data collected in real-world scenarios. To improve accuracy, the study augments the local context of nearby points by using both geometric and semantic features. Additionally, it interprets the distinctness of points at multiple resolutions and employs an adaptive fusion method for feature representation at the point level. The study includes ablation studies and visualizations to validate the key modules. By comparing with state-of-the-art networks on three benchmarks, the effectiveness of the proposed network is demonstrated.