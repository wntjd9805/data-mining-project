Weight pruning is a technique used to reduce the size and inference time of deep neural networks in real-world applications. However, existing methods for weight pruning rely on manual tuning or heuristic rules to determine the appropriate pruning rates for each layer. This approach often leads to suboptimal performance. In this paper, we propose a new method called probabilistic masking (ProbMask) that directly works on the probability space. ProbMask uses probability as a global criterion to measure the importance of weights in all layers, allowing for automatic learning of weight redundancy. This eliminates the need for individually tuning pruning rates for different layers. Experimental results on CIFAR-10/100 and ImageNet datasets show that ProbMask outperforms previous state-of-the-art methods, especially in high pruning rate scenarios. The accuracy gap between ProbMask and existing methods can be as high as 10%. Additionally, ProbMask is effective in identifying supermasks, which are high-performance sub-networks within randomly weighted dense neural networks.