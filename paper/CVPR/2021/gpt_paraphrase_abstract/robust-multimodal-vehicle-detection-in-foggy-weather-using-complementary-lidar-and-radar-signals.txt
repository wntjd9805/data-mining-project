Vehicle detection for autonomous driving is crucial, but traditional methods relying on lidar or camera sensors suffer from reduced visibility in adverse weather conditions. To address this, we propose MVDNet, a two-stage deep fusion detector that utilizes radar data alongside lidar and camera inputs. MVDNet first generates proposals from the two sensors and then fuses region-wise features to enhance detection accuracy. We evaluate MVDNet using a training dataset created from raw lidar and radar signals collected from the Oxford Radar Robotcar. Our results demonstrate that MVDNet outperforms other state-of-the-art methods, particularly in adverse weather conditions, as measured by Average Precision (AP). The code and data for MVDNet are publicly available for further research.