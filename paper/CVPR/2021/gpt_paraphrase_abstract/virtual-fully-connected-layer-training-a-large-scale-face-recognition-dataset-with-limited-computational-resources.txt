Deep face recognition has made significant advancements thanks to Convolutional Neural Networks (CNNs) and large-scale datasets. However, training CNNs on a large-scale face recognition dataset with limited computational resources remains challenging. This is due to the need to train a fully-connected layer as the category classifier in the classification paradigm, which results in a high number of parameters (in the hundreds of millions) when the training dataset contains millions of identities. This poses a resource burden, particularly in terms of GPU memory. While the metric learning paradigm offers a more economical computation method, its performance lags behind that of the classification paradigm. To tackle this challenge, we propose a straightforward yet effective CNN layer called the Virtual fully-connected (Virtual FC) layer to reduce the computational requirements of the classification paradigm. Our Virtual FC layer significantly reduces the parameters (over 100 times fewer) compared to the fully-connected layer while achieving competitive performance on popular face recognition evaluation datasets. Additionally, our Virtual FC layer outperforms the metric learning paradigm by a significant margin on these evaluation datasets. We plan to release our code to facilitate the dissemination of our idea to other domains.