Multi-frame human pose estimation in complex situations is a challenging task. While current human joint detectors have achieved impressive results for static images, they fall short when applied to video sequences. This is mainly due to their inability to handle motion blur, video defocus, and pose occlusions, which stem from the lack of temporal dependency among video frames. On the other hand, using conventional recurrent neural networks to model spatial contexts, especially for pose occlusions, presents empirical difficulties. To address these issues, this paper presents a novel multi-frame human pose estimation framework that leverages temporal cues between video frames to improve keypoint detection. The framework consists of three modular components. The first component, called Pose Temporal Merger, encodes the spatiotemporal context of keypoints to generate effective searching scopes. The second component, Pose Residual Fusion, computes weighted pose residuals in dual directions. These residuals are then processed by the third component, the Pose Correction Network, which efficiently refines pose estimations. This proposed method outperforms others and achieves the top rank in the Multi-frame Person Pose Estimation Challenge on the benchmark datasets PoseTrack2017 and PoseTrack2018. The authors have made their code publicly available, hoping to inspire future research in this field.