Camera localization is the process of estimating the position and orientation of a camera based on RGB images. Traditional methods involve detecting and matching interest points between an image and a 3D model. Recently, learning-based approaches have been developed using convolutional neural networks (CNNs) to encode scene structures and predict dense coordinates from RGB images. However, these methods often require re-training or re-adaptation for new scenes and struggle with large-scale scenes due to limited network capacity.To address these limitations, we propose a new method called dense scene matching (DSM) for scene-agnostic camera localization. DSM involves constructing a cost volume between a query image and a scene, and then processing this volume and corresponding coordinates using a CNN to predict dense coordinates. Camera poses can be solved using PnP algorithms. Our method can also be extended to the temporal domain, providing additional performance improvements during testing.In our experiments, we found that our scene-agnostic approach achieved comparable accuracy to existing scene-specific approaches like KFNet on the 7scenes and Cambridge benchmark datasets. Furthermore, our method significantly outperformed the state-of-the-art scene-agnostic dense coordinate regression network SANet. The code for our method is available at the provided GitHub link.