Backdoor attacks in deep learning models involve hidden malicious behaviors that are triggered by specific "triggers" and cause misclassifications. Previous research focused on digital attacks using digitally generated triggers, but the question of whether physical objects can be used as triggers remains unanswered. In this study, we investigate the feasibility of physical backdoor attacks on facial recognition, a critical deep learning task. We use 7 physical objects as triggers and collect a dataset of 3205 images to explore the real-world conditions under which physical backdoor attacks can succeed. Our findings reveal that physical backdoor attacks can be highly successful if they are carefully configured to overcome the constraints imposed by physical objects. The placement of triggers depends on the target model's reliance on key facial features. Additionally, we find that four state-of-the-art defenses against digital backdoors are ineffective against physical backdoors because these defenses are based on assumptions that physical objects break. This study confirms that physical backdoor attacks are a serious real-world threat to critical classification tasks and calls for the development of new and more robust defenses against backdoors in the physical world.