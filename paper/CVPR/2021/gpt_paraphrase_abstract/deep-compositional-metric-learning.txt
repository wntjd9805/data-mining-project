We present a new framework called deep compositional metric learning (DCML) that aims to measure similarity between images effectively and in a way that can be generalized to new data. Traditional deep metric learning methods focus on minimizing a discriminative loss to increase the differences between different classes of images while reducing variations within the same class. However, this approach may not perform well when samples from the same class have diverse characteristics. To address this, we propose using an ensemble technique where multiple sub-embeddings are learned using different subtasks. However, most subtasks impose weaker or contradictory constraints, which compromises the discrimination ability of each sub-embedding to improve the generalization ability of their combination. To overcome this, we suggest separating the sub-embeddings from direct supervision and applying losses on different combinations of the sub-embeddings. We introduce learnable compositors to combine the sub-embeddings and utilize a self-reinforced loss to train the compositors. The compositors act as relays to distribute diverse training signals without compromising discrimination ability. Our framework outperforms existing methods on the CUB-200-2011, Cars196, and Stanford Online Products datasets, as shown by experimental results.