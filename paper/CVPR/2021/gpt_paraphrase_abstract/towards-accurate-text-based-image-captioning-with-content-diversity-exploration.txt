Text-based image captioning (TextCap) is crucial for machines to understand complex scenes by reading and reasoning images with texts. However, current methods that aim to describe the overall scene with a single global caption are insufficient for accurately capturing the complex texts and visual information present in an image. To address this challenge, we propose a novel method called Anchor-Captioner. Our method tackles three key challenges: determining which parts of the texts to copy or paraphrase, capturing the complex relationship between diverse texts, and generating multiple captions with diverse content. To overcome these challenges, we first identify important tokens as anchors and construct an anchor-centred graph (ACG) by grouping relevant texts for each anchor. Then, using different ACGs, we generate multiple captions to enhance the content diversity of the generated descriptions. Our experimental results demonstrate that our method achieves state-of-the-art performance and generates diverse captions that accurately describe images.