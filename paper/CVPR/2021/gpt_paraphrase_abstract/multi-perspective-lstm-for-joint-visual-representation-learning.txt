We introduce a novel LSTM cell design that can learn relationships within and across different perspectives in visual sequences. Our architecture incorporates additional gates and memories at the cell level, enabling more effective and comprehensive visual representations for recognition tasks. We evaluate our proposed architecture in lip reading and face recognition tasks using three relevant datasets. We compare our results with fusion strategies, other multi-input LSTM architectures, and alternative recognition methods. Our experiments demonstrate the superior performance of our solution in terms of accuracy and complexity. The code for our architecture is publicly accessible at https://github.com/arsm/MPLSTM.