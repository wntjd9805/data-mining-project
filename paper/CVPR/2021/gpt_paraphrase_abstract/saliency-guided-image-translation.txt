This paper introduces a new task called saliency-guided image translation, which aims to translate images based on user-specified saliency maps. To solve this problem, the authors propose a novel model called SalG-GAN, which is a Generative Adversarial Network (GAN) based model. SalG-GAN takes an original image and a target saliency map as input and generates a translated image that matches the target saliency map. The authors also propose a disentangled representation framework in SalG-GAN to encourage diverse translations for the same target saliency condition. Additionally, a saliency-based attention module is introduced to enhance the structures of the saliency-guided generator, saliency cue encoder, and saliency-guided global and local discriminators. To evaluate the model, the authors create both a synthetic dataset and a real-world dataset with labeled visual attention. The experimental results on these datasets demonstrate the effectiveness of the proposed SalG-GAN model for saliency-guided image translation.