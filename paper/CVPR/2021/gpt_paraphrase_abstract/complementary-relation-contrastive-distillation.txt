This study introduces a new method called Complementary Relation Contrastive Distillation (CRCD) for knowledge distillation. While previous approaches focused on individual representation distillation or inter-sample similarity preservation, this method emphasizes the importance of distilling inter-sample relations in a more effective way. CRCD estimates the mutual relation between samples in an anchor-based manner and transfers this structural knowledge from the teacher model to the student model. To enhance robustness, the mutual relations are modeled using both the feature and its gradient. Additionally, the method maximizes the lower bound of mutual information between the anchor-teacher relation distribution and the anchor-student relation distribution through relation contrastive loss. This allows for the distillation of both sample representation and inter-sample relations. Experimental results on various benchmarks demonstrate the effectiveness of CRCD in knowledge distillation.