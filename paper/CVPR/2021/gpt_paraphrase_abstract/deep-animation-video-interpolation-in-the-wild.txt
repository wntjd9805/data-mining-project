In the animation industry, creating cartoon videos at a high frame rate is expensive and time-consuming. Therefore, it is desirable to develop computational models that can automatically generate the missing frames. However, existing methods for video interpolation do not work well for animation videos. This is because animation videos have two unique characteristics that make frame interpolation challenging: 1) cartoons consist of lines and smooth color areas, which lack textures and make it difficult to accurately estimate motion; and 2) cartoons use exaggeration to convey stories, resulting in non-linear and extremely large motions.To address these challenges, we propose a framework called AnimeInterp. It consists of two modules that work in a coarse-to-fine manner. The first module, Segment-Guided Matching, tackles the lack of textures challenge by globally matching coherent color areas. The second module, Recurrent Flow Refinement, addresses the non-linear and large motion challenge by making recurrent predictions using a transformer-like architecture.To enable comprehensive training and evaluation, we create a large-scale animation dataset called ATD-12K, which contains 12,000 triplets with detailed annotations. Extensive experiments show that our approach outperforms existing state-of-the-art interpolation methods for animation videos. AnimeInterp produces high-quality results and is robust in various animation scenarios.The dataset and code for our proposed method are publicly available at https://github.com/lisiyao21/AnimeInterp/.