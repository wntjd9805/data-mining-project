The objective of compositional zero-shot learning is to identify new compositions (e.g. "old dog") based on observed visual primitives states (e.g. "old", "cute") and objects (e.g. "car", "dog") in the training data. This is challenging because the same state can have a different visual impact on a dog compared to a car. To address this, we introduce a new graph formulation called Compositional Graph Embedding (CGE). CGE learns image features, compositional classifiers, and latent representations of visual primitives simultaneously. Our approach leverages the relationship between states, objects, and compositions within a graph structure to facilitate knowledge transfer from seen to unseen compositions. By encoding semantic compatibility between concepts, our model enables generalization to unseen compositions without relying on external knowledge bases like WordNet. Our experimental results demonstrate that CGE outperforms the current state-of-the-art methods on MIT-States and UT-Zappos datasets in the challenging generalized compositional zero-shot setting. We also propose a new benchmark for this task based on the GQA dataset. The code for our approach is available at: https://github.com/ExplainableML/czsl.