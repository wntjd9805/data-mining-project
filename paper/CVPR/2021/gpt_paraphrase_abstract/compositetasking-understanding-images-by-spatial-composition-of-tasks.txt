We propose the concept of CompositeTasking, which involves combining multiple spatially distributed tasks in image understanding. This approach is motivated by the limited availability of labels across tasks and the need for a compact multi-tasking network. To enable CompositeTasking, we introduce a task conditioning model that uses a single encoder-decoder network to simultaneously perform multiple spatially varying tasks. The network takes an image and a set of dense task requests as inputs and generates predictions for each pixel based on the requested task. Additionally, we learn the task composition rules for CompositeTasking, including determining where each task should be applied. This approach not only provides a compact network for multi-tasking but also allows for task-editing. Furthermore, our method only requires sparse supervision for each task, yet achieves comparable results to baselines that use dense supervision and a multi-headed multi-tasking design. The source code for our method will be publicly available at www.github.com/nikola3794/composite-tasking.