The current evaluation method for long-tailed visual recognition trains the classification model on a long-tailed source label distribution but evaluates its performance on a uniform target label distribution. However, this protocol may not be practical as the target distribution may also be long-tailed. To address this, we propose treating long-tailed visual recognition as a label shift problem, where the source and target label distributions differ. A major challenge in dealing with this problem is the entanglement between the source label distribution and the model prediction. In this paper, we focus on disentangling the source label distribution from the model prediction. We first introduce a baseline method that matches the target label distribution by post-processing the model prediction trained using cross-entropy loss and the Softmax function. While this method outperforms existing methods on benchmark datasets, it can be further improved by directly disentangling the source label distribution from the model prediction during training. Therefore, we propose a novel method called LAbel distribution DisEntangling (LADE) loss, based on the optimal bound of Donsker-Varadhan representation. LADE achieves state-of-the-art performance on benchmark datasets and outperforms existing methods on various shifted target label distributions, demonstrating the general adaptability of our approach.