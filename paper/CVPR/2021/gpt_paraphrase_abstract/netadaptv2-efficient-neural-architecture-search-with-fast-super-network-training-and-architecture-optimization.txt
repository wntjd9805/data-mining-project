Neural architecture search (NAS) involves training a super-network, evaluating sampled deep neural networks (DNNs), and training the discovered DNN. Existing methods have focused on speeding up certain steps at the expense of others or sacrificing non-differentiable search metrics. This unbalanced approach limits the overall reduction in search time and the performance of discovered DNNs. This paper introduces NetAdaptV2, which addresses these limitations through three innovations. First, channel-level bypass connections are proposed to merge network depth and layer width, reducing the time for training and evaluating sampled DNNs. Second, ordered dropout allows for the training of multiple DNNs in a single pass, decreasing the time for training a super-network. Third, the multi-layer coordinate descent optimizer considers the interaction between multiple layers during optimization, improving the performance of discovered DNNs while supporting non-differentiable search metrics. With these innovations, NetAdaptV2 achieves a significant reduction in total search time on ImageNet and NYU Depth V2 datasets, while also discovering DNNs with better accuracy-latency/accuracy-MAC trade-offs compared to state-of-the-art NAS methods. Additionally, the discovered DNN outperforms NAS-discovered MobileNetV3 with a 1.8% higher top-1 accuracy at the same latency.