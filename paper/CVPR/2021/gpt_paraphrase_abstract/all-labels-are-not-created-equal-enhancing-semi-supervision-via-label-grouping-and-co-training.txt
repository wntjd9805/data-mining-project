Pseudo-labeling is a crucial aspect of semi-supervised learning (SSL) where the model generates artificial labels for unlabeled data. However, existing methods solely rely on the model's predictions without considering the visual similarity among classes, leading to poor representation of visually similar classes in the pseudo-labeled data. To address this issue, we propose SemCo, a method that incorporates label semantics and co-training. SemCo trains two classifiers with different views of class labels: one classifier uses the one-hot view disregarding class similarity, while the other utilizes a distributed view grouping similar classes. These classifiers are co-trained based on their disagreements. Our method outperforms existing approaches, achieving state-of-the-art results across various SSL tasks, including a 5.6% accuracy improvement on the Mini-ImageNet dataset with 1000 labeled examples. Furthermore, our method requires a smaller batch size and fewer training iterations to reach optimal performance. The code for SemCo is available at https://github.com/islam-nassar/semco.