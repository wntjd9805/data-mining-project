Despite the flexibility of deep neural networks (DNNs), they are susceptible to adversarial examples. Various defense strategies have been proposed but they often have limited practicality due to compromises in universality, effectiveness, or efficiency. In this study, we introduce a more practical approach called Lightweight Bayesian Refinement (LiBRe) that utilizes Bayesian neural networks (BNNs) for adversarial detection. By employing task and attack agnostic modeling under Bayes principle, LiBRe equips pre-trained task-dependent DNNs with the ability to defend against various types of adversarial attacks at a low cost. We enhance LiBRe's effectiveness and efficiency by developing a few-layer deep ensemble variational and utilizing pre-training and fine-tuning techniques. Additionally, we propose a novel method for adversarial detection-oriented uncertainty quantification without the need for inefficiently crafting adversarial examples during training. Extensive empirical studies across diverse scenarios demonstrate the practicality of LiBRe, and thorough ablation studies validate the effectiveness of our modeling and learning strategies.