In order to train deep neural networks (DNNs) that are robust and reliable, we conducted a systematic study on various approaches for modifying the target. These approaches include output regularization, self-label correction, and non-self label correction. During our study, we identified two key issues. Firstly, self-label correction is the most appealing approach as it utilizes the network's own knowledge without the need for additional models. However, the literature lacks a clear answer on how to automatically determine the trust level of a learner as the training progresses. Secondly, some methods penalize low-entropy predictions while others reward them. This raised the question of which approach is better. To address the first issue, we propose a novel end-to-end method called ProSelfLC. This method takes into account the learning time and entropy of a model's predictions. Specifically, for a given data point, we gradually increase trust in its predicted label distribution compared to its annotated label if the model has been trained for a sufficient amount of time and the prediction has low entropy (high confidence). Regarding the second issue, according to ProSelfLC, we empirically demonstrate that it is more effective to redefine a meaningful low-entropy status and optimize the learner towards it. This serves as a defense mechanism for entropy minimization. We conducted extensive experiments in both clean and noisy settings to demonstrate the effectiveness of ProSelfLC. The source code for ProSelfLC is available at https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.