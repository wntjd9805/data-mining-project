This paper introduces DRANet, a network architecture designed for unsupervised cross-domain adaptation by disentangling image representations and transferring visual attributes in a latent space. Unlike current domain adaptation methods that learn shared features across domains, DRANet preserves the distinct characteristics of each domain. The model encodes individual representations of content and style from both the source and target images, and incorporates the transferred style factor into the content factor using learnable weights specific to each domain. This approach enables bi-/multi-directional domain adaptation using a single encoder-decoder network and aligns their domain shift. Additionally, the paper proposes a content-adaptive domain transfer module that preserves scene structure while transferring style. Extensive experiments demonstrate that the model successfully separates content-style factors and synthesizes visually pleasing domain-transferred images. The proposed method achieves state-of-the-art performance on standard digit classification tasks and semantic segmentation tasks.