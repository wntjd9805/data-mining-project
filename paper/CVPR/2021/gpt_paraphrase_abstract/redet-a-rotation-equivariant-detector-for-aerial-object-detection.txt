Object detection in aerial images has become a prominent area of research in computer vision. Unlike objects in natural images, aerial objects often have arbitrary orientations, which necessitates additional parameters to encode orientation information. However, these additional parameters tend to be redundant and inefficient. Furthermore, conventional convolutional neural networks (CNNs) do not explicitly model orientation variation, requiring large amounts of rotation augmented data for accurate object detection. To address these challenges, this paper presents a Rotation-equivariant Detector (ReDet) that explicitly encodes rotation equivariance and invariance. The proposed ReDet incorporates rotation-equivariant networks into the detector to extract rotation-equivariant features, enabling accurate prediction of orientation and significantly reducing the model size. Additionally, the paper introduces Rotation-invariant RoI Align (RiRoIAlign), which adaptively extracts rotation-invariant features from the equivariant features based on the orientation of the region of interest (RoI). Extensive experiments conducted on challenging aerial image datasets, including DOTA-v1.0, DOTA-v1.5, and HRSC2016, demonstrate the superiority of the proposed method in aerial object detection. Compared to previous state-of-the-art results, the ReDet achieves 1.2, 3.5, and 2.6 mAP improvements on DOTA-v1.0, DOTA-v1.5, and HRSC2016, respectively, while reducing the number of parameters by 60% (from 313 Mb to 121 Mb). The code for the proposed method is available at: https://github.com/csuhan/ReDet.