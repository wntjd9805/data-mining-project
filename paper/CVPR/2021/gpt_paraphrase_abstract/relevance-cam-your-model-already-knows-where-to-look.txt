Explaining deep learning models has become increasingly important as neural networks are applied in various fields. Prior to practical applications, it is crucial to analyze a model's inference and the process of generating results. A commonly used explanation method is Class Activation Mapping (CAM), which focuses on understanding the last layer of convolutional neural networks in Computer Vision. This paper introduces a novel CAM method called Relevance-weighted Class Activation Mapping (Relevance-CAM) that utilizes Layer-wise Relevance Propagation to obtain weighting components. By addressing the shattered gradient problem, which causes noisy saliency maps for intermediate layers in gradient-based CAM methods, our proposed method provides a faithful and robust explanation map. This means it can correctly analyze both the intermediate layers and the last convolutional layer, resulting in better model explanations. The paper visualizes how each layer of popular image processing models extracts class-specific features using Relevance-CAM, evaluates its localization ability, and demonstrates why gradient-based CAM is inadequate for explaining intermediate layers. Through experimentation with weighting components, Relevance-CAM outperforms other CAM-based methods in recognition and localization evaluation across layers of any depth. The source code for Relevance-CAM is available at: https://github.com/mongeoroo/Relevance-CAM.