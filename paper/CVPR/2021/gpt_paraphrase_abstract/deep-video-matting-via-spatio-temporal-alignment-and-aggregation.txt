Deep learning has made significant progress in natural image matting, but there has been a lack of representative work on deep learning for video matting. This is due to the challenges of reasoning in the temporal domain and the absence of large-scale video matting datasets. In this paper, we propose a deep learning-based video matting framework that utilizes a novel spatio-temporal feature aggregation module (ST-FAM). Since optical flow estimation is unreliable within matting regions, ST-FAM is designed to align and aggregate information across different spatial scales and temporal frames within the network decoder. To eliminate the need for frame-by-frame trimap annotations, we introduce a lightweight interactive trimap propagation network. Additionally, we provide a large-scale video matting dataset with groundtruth alpha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation. Our experimental results, both quantitative and qualitative, demonstrate that our framework outperforms conventional video matting and deep image matting methods when considering multi-frame temporal information. Our dataset can be accessed at https://github.com/nowsyn/DVM.