Visual Question Answering (VQA) is a task where models tend to exploit biases in datasets instead of performing high-level reasoning. Traditional approaches address this issue by removing biases from training data or adding branches to detect and remove biases. However, we argue that uncertainty in vision is a major obstacle to learning reasoning in vision and language problems. To investigate this, we trained a visual oracle and conducted a large-scale study to demonstrate that it is less prone to exploiting dataset biases compared to standard models. We examine the attention mechanisms of the visual oracle and compare them with a state-of-the-art Transformer-based model. Through in-depth analysis and visualizations using an online tool, we gain insights into reasoning patterns, which we make publicly available. Leveraging these insights, we transfer reasoning patterns from the oracle to a Transformer-based VQA model by fine-tuning it with standard visual inputs. Our experiments show higher overall accuracy and improved generalization, particularly for infrequent answers, indicating a reduced reliance on dataset biases. The answer format only presents the abstraction.