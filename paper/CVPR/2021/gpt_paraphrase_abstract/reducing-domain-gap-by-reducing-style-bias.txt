Domain shift is a common problem faced by Convolutional Neural Networks (CNNs) when they are tested on new domains. This issue is caused by CNNs' strong inclination towards image styles (textures) rather than contents (shapes), making them sensitive to changes in the domain. To address this problem, we propose a solution called Style-Agnostic Networks (SagNets) that reduces the inherent style bias of CNNs and bridges the gap between domains. SagNets separate style encodings from class categories to avoid biased predictions based on style and instead focus more on the contents of the images. Through extensive experiments, we demonstrate that our method effectively reduces style bias and enhances the model's robustness in the face of domain shift. It achieves significant performance improvements across various cross-domain tasks, including domain generalization, unsupervised domain adaptation, and semi-supervised domain adaptation on multiple datasets.