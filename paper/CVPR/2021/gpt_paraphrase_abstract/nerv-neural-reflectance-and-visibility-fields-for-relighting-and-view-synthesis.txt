We introduce a technique that can generate a 3D representation of a scene using a set of images captured under diverse lighting conditions. This representation can be rendered from different viewpoints and under any lighting condition. Our approach utilizes a continuous volumetric function that is parameterized by multilayer perceptrons (MLPs). The MLPs take a 3D location as input and output various scene properties such as volume density, surface normal, material parameters, distance to the nearest surface intersection, and visibility of the external environment in any direction. These properties enable us to render novel views of the scene with realistic lighting effects, including indirect illumination. The accurate prediction of visibility and surface intersection fields is crucial for simulating direct and indirect illumination during training. Previous methods relying on brute-force techniques were limited to controlled lighting setups with a single light source, making them impractical for general lighting conditions. Our method outperforms alternative approaches for reconstructing relightable 3D scene representations and performs well even in complex lighting environments that have been challenging for previous methods.