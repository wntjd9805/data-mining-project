Video inpainting is the process of filling corrupted regions in videos with plausible content. This is typically done by finding correspondences between neighboring frames to accurately generate the missing content. Current methods employ attention, flow-based warping, or 3D temporal convolution to achieve this. However, flow-based warping can lead to artifacts when the optical flow is inaccurate, and temporal convolution may suffer from spatial misalignment. To address these issues, we propose the "Progressive Temporal Feature Alignment Network" (PTFAN). PTFAN progressively enriches the features extracted from the current frame by incorporating the warped features from neighboring frames using optical flow. This approach corrects spatial misalignment in the temporal feature propagation stage, resulting in improved visual quality and temporal consistency of the inpainted videos. Our experiments on the DAVIS and FVI datasets demonstrate that PTFAN outperforms existing deep learning approaches in terms of performance. The code for our proposed architecture is available at https://github.com/MaureenZOU/TSAM.