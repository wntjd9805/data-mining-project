In recent years, there has been significant progress in the field of 3D hand mesh recovery. However, the process of recovering camera-space 3D information from a single RGB image remains challenging due to the inherent ambiguity between 2D and 3D representations. To address this problem, we propose a two-step approach that involves root-relative mesh recovery and root recovery.In the first step, we extract joint landmarks and silhouette from the input image to provide 2D cues for the subsequent 3D tasks. Using these cues, we leverage the semantic relations among joints to generate a 3D mesh that is expressed relative to a root position, which is typically the wrist of the hand.In the second step, we register the root position to the camera space by aligning the generated 3D mesh with the 2D cues. This completes the process of recovering camera-space 3D mesh information. Our approach is unique in two ways: (1) it explicitly utilizes the known semantic relations among joints, and (2) it utilizes 1D projections of the silhouette and mesh to achieve robust registration.We have conducted extensive experiments on popular datasets such as FreiHAND, RHD, and Human3.6M to evaluate the performance of our approach. The results show that our method achieves state-of-the-art performance in both root-relative mesh recovery and root recovery. We have made our code publicly available at https://github.com/SeanChenxy/HandMesh.