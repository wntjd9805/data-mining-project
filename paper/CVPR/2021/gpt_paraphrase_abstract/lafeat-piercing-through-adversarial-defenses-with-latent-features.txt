Deep convolutional neural networks (CNNs) are vulnerable to adversarial attacks, as even a small perturbation in the input can easily mislead them to produce incorrect outputs. This poses a significant challenge in enhancing the robustness of CNNs against such attacks. Various defense techniques have been proposed to address this issue. However, this study reveals that certain "robust" models' latent features are unexpectedly susceptible to adversarial attacks. In response, the authors introduce a unified white-box attack algorithm called LAFEAT, which utilizes latent features in its gradient descent steps. Compared to existing methods, LAFEAT not only achieves higher computational efficiency in successful attacks but also emerges as a stronger adversary across different defense mechanisms. This suggests that the effectiveness of model robustness may rely on the strategic utilization of hidden components by defenders, rather than taking a holistic approach.