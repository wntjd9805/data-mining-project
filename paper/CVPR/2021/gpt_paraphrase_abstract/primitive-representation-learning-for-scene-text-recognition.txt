Scene text recognition is a difficult task due to the varied nature of text instances in natural scene images. Existing methods, such as CNN-RNN-CTC or encoder-decoder with attention, may not fully explore stable and efficient feature representations for multi-oriented scene texts. This paper proposes a novel approach to learn primitive representations of scene text images, aiming to exploit their intrinsic characteristics. The method models the elements in feature maps as nodes in an undirected graph. It introduces a pooling aggregator and a weighted aggregator to learn primitive representations, which are then transformed into high-level visual text representations using graph convolutional networks. A Primitive Representation learning Network (PREN) is constructed to utilize these visual text representations for parallel decoding. Additionally, the paper addresses the misalignment problem present in attention-based methods by integrating visual text representations into an encoder-decoder model with a 2D attention mechanism. This framework, called PREN2D, achieves state-of-the-art performance by alleviating the misalignment issue. Experimental results on both English and Chinese scene text recognition tasks demonstrate that PREN maintains a balance between accuracy and efficiency, while PREN2D outperforms other methods.