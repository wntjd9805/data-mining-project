Current computer vision methods struggle with view generalization problems, as they often fail to accurately predict the appearance of objects in novel views. This is due to the challenge of balancing single-view accuracy with novel view accuracy in machine learning. In order to address this issue, we propose a class of models that can easily control their geometric rigidity to strike a better balance between these two factors. We introduce a cycle consistency loss, which enhances view generalization by ensuring that a model can accurately predict the original view when presented with a generated view. Furthermore, to improve texture generalization, we employ a cycle consistency loss that promotes alignment of model textures, facilitating the sharing of texture information across different objects. Our method is compared to the state-of-the-art approach, and we demonstrate both qualitative and quantitative improvements in performance.