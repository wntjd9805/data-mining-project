We have created a new benchmark dataset called DroneCrowd to aid in the development of object detection, tracking, and counting algorithms for drone-captured videos. This dataset consists of 112 video clips with a total of 33,600 HD frames, covering various scenarios. We have annotated 20,800 people trajectories, including 4.8 million heads, and included several video-level attributes.   As a strong baseline, we have developed the Space-Time Neighbor-Aware Network (STNNet) to address the challenges of object detection, tracking, and counting in dense crowds. STNNet consists of a feature extraction module, density map estimation heads, and localization and association subnets.   To leverage the contextual information of neighboring objects, we have introduced the neighboring context loss, which helps train the association subnet. This loss ensures consistent relative positions of nearby objects in the temporal domain.   Through extensive experiments on the DroneCrowd dataset, we have demonstrated that STNNet outperforms existing state-of-the-art methods.