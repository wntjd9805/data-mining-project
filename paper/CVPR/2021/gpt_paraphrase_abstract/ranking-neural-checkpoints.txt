This paper focuses on ranking pre-trained deep neural networks (DNNs), known as checkpoints, for transfer learning to a downstream task. With the widespread use of DNNs, there is a large collection of checkpoints available from different sources. The main objective is to determine which checkpoints are most effective for the specific downstream task. To address this question comprehensively, the authors introduce a benchmark called Neural Checkpoint Ranking Benchmark (NeuCRaB) and examine several intuitive ranking measures. These measures are applicable to checkpoints with different output types, regardless of the pre-training datasets. Additionally, they are computationally efficient and have practical significance. The findings indicate that the linear separability of the features extracted by the checkpoints is a strong indication of transferability. The authors propose a new ranking measure called NLEEP, which demonstrates the best performance in the experiments conducted. The code for this research will be publicly available.