Efficiently representing camera pose is a crucial issue in 3D computer vision, specifically in tasks like camera pose regression and novel view synthesis. Traditionally, camera position is represented using Cartesian coordinates and orientation is represented using Euler angles or quaternions. However, these manual representations may not be the most effective for downstream tasks.In this study, we propose a method to learn neural representations of camera poses and 3D scenes, combined with neural representations of local camera movements. We represent the camera pose and 3D scene as vectors, and the local camera movement as a matrix that operates on the camera pose vector. Additionally, we parametrize the camera movement using a matrix Lie algebra, which forms the basis of a rotation system in the neural space. These vector representations are then concatenated and used to generate a posed 2D image through a decoder network.The model is trained solely on posed 2D images and corresponding camera poses, without using depth or shape information. We conducted extensive experiments on both synthetic and real datasets. The results demonstrate that our learned representation is more robust to noise in novel view synthesis and more effective in camera pose regression compared to other camera pose representations.Overall, our approach offers an innovative solution to the problem of effectively representing camera pose in 3D computer vision tasks.