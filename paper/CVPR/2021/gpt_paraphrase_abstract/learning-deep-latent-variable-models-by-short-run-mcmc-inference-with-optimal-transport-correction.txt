This study explores the use of deep top-down architectures for learning latent variable models. These models typically require inferring the latent variables for each training example using either time-consuming MCMC sampling or a separate inference model. The authors propose the use of a short-run MCMC, specifically short-run Langevin dynamics, as an approximate flow-based inference engine. The biased output distribution of the non-convergent short-run Langevin dynamics is corrected using optimal transport (OT), which aims to transform the biased distribution to the prior distribution with minimal transport cost. The experiments conducted in this study demonstrate the effectiveness of OT correction for short-run MCMC, and show that the proposed strategy outperforms variational auto-encoders (VAEs) in terms of image reconstruction/generation and anomaly detection in latent variable models.