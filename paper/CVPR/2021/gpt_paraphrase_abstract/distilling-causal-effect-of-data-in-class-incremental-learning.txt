We introduce a causal framework to explain the phenomenon of catastrophic forgetting in Class-Incremental Learning (CIL). We then propose a new distillation method that is different from existing techniques such as data replay and feature/label distillation. Firstly, we situate CIL within our framework and address the reason behind forgetting: the loss of the causal effect of old data during new training. Secondly, we explain how existing techniques mitigate forgetting by reintroducing the causal effect. Building on our causal framework, we suggest distilling the Colliding Effect between old and new data, which is essentially equivalent to the causal effect of data replay, but without the need for replay storage. By analyzing the causal effect, we can also identify the Incremental Momentum Effect of the data stream. Removing this effect helps to preserve the impact of old data, which would otherwise be overshadowed by new data, thereby reducing forgetting during testing. Extensive experiments conducted on three CIL benchmarks (CIFAR-100, ImageNet-Sub&Full) demonstrate that our proposed causal effect distillation significantly enhances various state-of-the-art CIL methods, resulting in improvements ranging from 0.72% to 9.06%.