Many adversarial attacks have been developed to investigate the security vulnerabilities of deep neural networks. In the black-box scenario, current model stealing attacks involve training a substitute model to imitate the behavior of the target model. However, this training process requires querying the target model, leading to high query complexity and vulnerability to defense mechanisms. This study aims to address these limitations by training a generalized substitute model called "Simulator" that can mimic the functionality of any unknown target model. To achieve this, we construct the training data using multiple tasks by collecting query sequences generated during attacks on various existing networks. The learning process incorporates a knowledge-distillation loss based on mean square error to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are computed and accumulated from multiple tasks to update the Simulator, improving its generalization capabilities.When attacking a target model that was not seen during training, the trained Simulator can accurately simulate its functionality with limited feedback. Consequently, a significant portion of queries can be transferred to the Simulator, reducing query complexity. Comprehensive experiments conducted on CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach significantly reduces query complexity compared to the baseline method, by several orders of magnitude. The implementation source code is available online1.