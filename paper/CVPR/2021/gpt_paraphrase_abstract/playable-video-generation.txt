This paper presents a new unsupervised learning problem called playable video generation (PVG). PVG aims to enable users to control the generated video by selecting discrete actions, similar to playing a video game. The challenge lies in learning meaningful actions and generating realistic videos based on user input. To address this, we propose a novel PVG framework trained in a self-supervised manner on a large dataset of unlabeled videos. Our framework uses an encoder-decoder architecture with predicted action labels as a bottleneck. The network is trained to learn a diverse action space by using a reconstruction loss on the generated video as the main driving force. We validate our approach on various datasets with different environments, demonstrating its effectiveness. More information, including code and examples, can be found on our project page at willi-menapace.github.io/playable-video-generation-website.