The precise estimation of optical flow, stereo depth, and camera motion is crucial for understanding real-world 3D scenes and visual perception. Previous studies have shown that jointly optimizing the geometric loss functions of individual networks can improve these tasks. However, in this paper, we propose a different approach by demonstrating that effective collaboration at the feature level can achieve even greater performance improvement for all three tasks compared to joint optimization at the loss level. We introduce a single network that combines and enhances the features of two consecutive stereo images, simultaneously estimating optical flow, stereo depth, and camera motion. The network consists of four parts: a feature-sharing encoder to enhance feature representation, a pooled decoder to estimate optical flow and stereo depth, a camera pose estimation module that fuses information from optical flow and stereo depth, and a cost volume complement module to improve the performance of optical flow in static and occluded regions. Our method achieves state-of-the-art performance in unsupervised optical flow estimation, stereo depth estimation, and camera motion estimation on benchmark datasets.