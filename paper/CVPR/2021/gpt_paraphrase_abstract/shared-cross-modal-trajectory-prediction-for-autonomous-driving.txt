Predicting the future paths of traffic agents in interactive environments is a critical and difficult task for ensuring the safety of autonomous driving systems. To address this challenge, we propose a Cross-Modal Embedding framework that leverages multiple sensor inputs, such as LiDAR, RGB camera, and radar. During training, our model learns to combine these different input modalities to create a shared latent space that captures complementary features. During testing, the model utilizes a single input modality, such as LiDAR data, to generate predictions while benefiting from the knowledge learned from multiple sensor modalities. We evaluate the effectiveness of our framework using two widely-used driving datasets and demonstrate its efficacy.