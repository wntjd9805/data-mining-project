Current methods for large vocabulary object detection often divide categories into groups and apply different strategies to each group. However, this approach leads to two problems: inconsistent training between similar-sized categories and a lack of discrimination for tail categories that are semantically similar to head categories. This paper proposes a novel Adaptive Class Suppression Loss (ACSL) to address these issues and improve the detection performance of tail categories. By analyzing the long-tail distribution from a statistic-free perspective, ACSL overcomes the limitations of manual grouping. It adjusts the suppression gradients for each sample of each class adaptively, ensuring training consistency and enhancing discrimination for rare categories. Experimental results on long-tail datasets LVIS and Open Images demonstrate that ACSL achieves significant improvements of 5.18% and 5.2% respectively with ResNet50-FPN, setting a new state of the art. The code and models for ACSL are available at https://github.com/CASIA-IVA-Lab/ACSL.