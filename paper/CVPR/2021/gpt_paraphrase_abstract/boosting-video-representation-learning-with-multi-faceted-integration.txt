The current datasets for video content only label one aspect of the content, such as objects or scenes. This biases the video representation towards that specific aspect. This paper introduces a new learning framework called MUFI, which integrates multiple facets of video content from different datasets to create a representation that encompasses the full spectrum of video content. MUFI utilizes visual-semantic embedding learning to map video representation into a semantic embedding space and optimizes the representation from two perspectives. One perspective is the intra-facet supervision, which captures the relationship between each video and its own label descriptions. The second perspective is the inter-facet supervision, which predicts the "semantic representation" of each video using facets from other datasets. Experimental results demonstrate that training a 3D CNN using the MUFI framework on a combination of four large-scale video datasets and two image datasets results in a superior video representation. The pre-trained 3D CNN with MUFI also outperforms other approaches in various video applications, achieving high accuracy in action recognition and video captioning tasks. Specifically, MUFI achieves 98.1% and 80.9% accuracy on UCF101 and HMDB51 datasets for action recognition, and a CIDEr-D score of 101.5% on MSVD dataset for video captioning.