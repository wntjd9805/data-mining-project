Generating accurate and realistic 3D point clouds is crucial for applications in autonomous driving and robotics. While deep learning has shown promise in representation learning, it remains challenging to synthesize high-fidelity point clouds due to difficulties in learning effective pointwise representations and generating complex distributions. In this study, we propose a dual-generators framework for point cloud generation, building upon the generative adversarial learning approach. Our framework consists of two generators: the first one learns point embeddings in a breadth-first manner, while the second refines the generated point cloud using a depth-first point embedding to create a robust and uniform point cloud. This progressive approach allows for the gradual learning of effective point embeddings. We evaluate our method on ShapeNet, a popular point cloud generation dataset, and demonstrate its superior performance in generating accurate point clouds for various object categories.