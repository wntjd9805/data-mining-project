We investigate the joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT), which aims to acquire cross-modal alignments from a large number of image-text pairs. Current state-of-the-art approaches extract significant regions from images and gradually align these regions with corresponding words. However, existing vision-language models face difficulties in fully comprehending the semantics of paired natural languages as region-based visual features typically only represent parts of an image. In this paper, we propose a method called SOHO ("Seeing Out of tHe bOx") that takes an entire image as input and learns vision-language representation in a seamless manner. Unlike region-based approaches, SOHO does not rely on bounding box annotations, resulting in a 10-fold increase in inference speed. SOHO learns to extract comprehensive yet concise image features through a visual dictionary (VD) that captures consistent visual abstractions of similar semantics. The VD is dynamically updated and utilized in our proposed pre-training task known as Masked Visual Modeling (MVM). We conduct experiments on four established vision-language tasks using standard VLPT settings. Notably, SOHO achieves significant improvements, including a 2.0% increase in R@1 score on the MSCOCO text retrieval 5k test split, a 1.5% increase in accuracy on the NLVR2 test-P split, and a 6.7% increase in accuracy on the SNLI-VE test split.