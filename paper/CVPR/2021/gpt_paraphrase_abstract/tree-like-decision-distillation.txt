Knowledge distillation aims to create a smaller but effective student network by leveraging the knowledge acquired by a larger teacher model. Previous approaches have focused on having the student imitate specific behaviors of the teacher, such as soft targets, features, or attention. However, this paper argues that the key aspect of distillation lies in capturing the problem-solving process of the teacher. By analyzing the decision-making process at different layers, it is observed that the teacher model follows a coarse-to-fine approach, where early layers focus on coarse-grained discrimination (e.g., distinguishing between animals and vehicles) and later layers handle fine-grained discrimination (e.g., distinguishing between specific animals or vehicles). Based on this observation, a new distillation method called Tree-like Decision Distillation (TDD) is proposed to equip the student with the same problem-solving mechanism as the teacher. Extensive experiments demonstrate that TDD achieves competitive performance compared to state-of-the-art methods. Additionally, TDD offers better interpretability due to its interpretable decision distillation approach, in contrast to traditional dark knowledge distillation methods.