The goal of disentanglement learning is to capture interpretable variations. However, interpretability has not been utilized to promote disentanglement in unsupervised settings. This paper explores the interpretability of disentangled representations by addressing two questions: where and what should be interpreted. To make a latent code easily interpretable, we propose learning a spatial mask that localizes the effect of each latent dimension on the resulting generated image. Furthermore, interpretability usually arises from latent dimensions that capture simple variations in data. To enforce encoding of simple variations, we introduce a perturbation on a specific dimension of the latent code and aim to identify this perturbation in the generated images. We also develop an unsupervised model selection method that measures perceptual distance scores along axes in the latent space. Our models successfully learn high-quality disentangled representations without supervision on various datasets. This demonstrates that our approach of incorporating interpretability is an effective method for achieving unsupervised disentanglement.