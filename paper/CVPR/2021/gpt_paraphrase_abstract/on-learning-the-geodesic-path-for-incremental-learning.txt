Neural networks are often plagued by a problem called catastrophic forgetting, where they forget past knowledge when acquiring new knowledge. Overcoming this issue is crucial for incremental learning, where the model learns from sequential experiences in an efficient and robust manner. Current techniques for incremental learning use knowledge distillation to prevent catastrophic forgetting. This involves updating the network while ensuring that its responses to previously seen concepts remain stable. Our research presents a new method for knowledge distillation. Unlike previous methods, we propose constructing low-dimensional manifolds for previous and current responses and minimizing the dissimilarity between the responses along the geodesic connecting the manifolds. This approach leads to more effective knowledge distillation with smooth properties that better preserves past knowledge, as confirmed by our comprehensive empirical study.