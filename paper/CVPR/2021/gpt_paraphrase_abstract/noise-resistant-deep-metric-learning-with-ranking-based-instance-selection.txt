Deep learning models are often affected by noisy labels in real-world data, resulting in decreased performance. While efforts have been made to address this issue in classification tasks, the problem of noisy labels in deep metric learning (DML) remains unresolved. This paper introduces a noise-resistant training technique for DML called Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM identifies noisy data within a minibatch by comparing their average similarity to image features extracted by previous versions of the neural network. These features are stored in a memory bank and can be retrieved as needed. To reduce the computational burden of the memory bank, an acceleration method is proposed that replaces individual data points with the class centers. Through extensive comparisons with 12 existing approaches using synthetic and real-world label noise, PRISM demonstrates superior performance, achieving up to 6.06% improvement in Precision@1.