This study introduces SLADE, a self-training framework aimed at enhancing retrieval performance by utilizing unlabeled data. Traditional distance metric learning techniques rely solely on labeled data to learn sample similarities in an embedding space. SLADE first trains a teacher model using labeled data and then utilizes it to generate pseudo labels for the unlabeled data. A student model is subsequently trained on both the labels and pseudo labels to generate final feature embeddings. The teacher model is initialized using self-supervised representation learning. To handle the presence of noisy pseudo labels, the student network incorporates a novel feature basis learning component. This component learns basis functions of feature representations for the unlabeled data, enabling the measurement of pairwise similarity using the learned basis vectors. These basis vectors are further employed to identify high-confidence samples for training the student network. The proposed method is evaluated on standard retrieval benchmarks (CUB-200, Cars-196, and In-shop), and the experimental results demonstrate that SLADE significantly outperforms state-of-the-art methods when additional unlabeled data is utilized.