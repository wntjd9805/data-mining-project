This paper introduces a new method called Discrepant Adversarial Training (DAT) to address the issue of degraded performance in deep networks when dealing with datasets containing noisy labels. Existing methods for learning with noisy labels have limitations, as they can only be applied to class-level noisy labels or perform well with synthetic noise but not with real-world noisy datasets. To overcome these limitations, DAT focuses on enforcing prominent feature extraction by matching the feature distribution between clean and noisy data. This allows the deep network to output the correct result under a noise-free feature representation. To measure the divergence between the noisy and clean distribution, a new metric is designed. By minimizing this metric through min-max training of discrepancy on classifiers and generators, DAT can align the feature space of noisy data with that of clean data.This paper is the first to address the noisy label problem from the perspective of feature distribution. Experimental results on both synthetic and real-world noisy datasets demonstrate that DAT consistently outperforms other state-of-the-art methods. The codes for implementing DAT are available at https://github.com/Tyqnn0323/DAT.