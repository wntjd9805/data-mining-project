Current one-shot neural architecture search (NAS) methods have been successful in reducing search costs by treating the entire search space as one network that only needs to be trained once. However, these methods often select each operation independently without considering previous layers and discard historical information obtained through computationally expensive processes. In this paper, we propose a novel sampling strategy based on Monte Carlo tree search (MCTS) that models the search space as a Monte Carlo tree (MCT) to capture dependencies among layers. The MCT stores intermediate results for future decisions and ensures a better balance between exploration and exploitation. Specifically, we update the MCT using the training loss as a reward for architecture performance evaluation. To accurately evaluate numerous nodes, we introduce node communication and hierarchical node selection methods in the training and search stages, respectively, to make better use of operation rewards and hierarchical information. Additionally, to enable fair comparisons between different NAS methods, we create an open-source NAS benchmark called NAS-Bench-Macro, which evaluates a macro search space on CIFAR-10. Extensive experiments on NAS-Bench-Macro and ImageNet demonstrate that our method significantly improves search efficiency and performance. For instance, by searching only 20 architectures, we achieve a top-1 accuracy of 78.0% with 442M FLOPs on ImageNet. Our code (Benchmark) is available at: https://github.com/xiusu/NAS-Bench-Macro.