Understanding how to interact with objects based on visual cues, known as visual affordance, is crucial for research in vision-guided robotics. Previous studies have focused on categorizing, segmenting, and reasoning about visual affordance in 2D and 2.5D image domains. However, a comprehensive understanding of object affordance requires learning and prediction in the 3D physical domain, which is currently lacking in the research community. To address this gap, we introduce the 3D AffordanceNet dataset, which consists of 23,000 shapes from 23 semantic object categories, annotated with 18 visual affordance categories. Using this dataset, we propose three benchmarking tasks to evaluate visual affordance understanding: full-shape, partial-view, and rotation-invariant affordance estimations. We evaluate three state-of-the-art point cloud deep learning networks on these tasks and also explore the potential benefits of semi-supervised learning using unlabeled data. Our comprehensive results on the dataset demonstrate the potential and challenges of visual affordance understanding as an important benchmark for research in this field.