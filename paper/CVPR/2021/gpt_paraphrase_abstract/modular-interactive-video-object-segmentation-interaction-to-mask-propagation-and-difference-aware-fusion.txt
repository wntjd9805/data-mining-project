We introduce the Modular interactive VOS (MiVOS) framework, which enhances generalizability and performance by separating the interaction-to-mask and mask propagation processes. The interaction module converts user interactions into an object mask, while the propagation module utilizes a unique top-k filtering strategy in the space-time memory to propagate the mask over time. To effectively capture the user's intent, we propose a difference-aware module that learns how to fuse the masks before and after each interaction, aligning them with the target frames using the space-time memory. Our method is evaluated both qualitatively and quantitatively on the DAVIS dataset with various forms of user interactions, such as scribbles and clicks. The results demonstrate that our approach surpasses existing state-of-the-art algorithms while requiring fewer frame interactions. Furthermore, our method exhibits better generalization to different types of user interactions. Additionally, we provide a large-scale synthetic VOS dataset with pixel-accurate segmentation of 4.8M frames, along with our source codes, to facilitate future research.