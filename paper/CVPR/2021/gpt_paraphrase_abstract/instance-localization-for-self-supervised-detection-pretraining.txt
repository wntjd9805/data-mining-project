Previous studies on self-supervised learning have made significant advancements in image classification; however, they often struggle with transferring these gains to object detection. This paper aims to enhance self-supervised pretrained models specifically for object detection. Recognizing the inherent differences between classification and detection, we propose a novel self-supervised pretext task called instance localization. By placing image instances at different positions and scales on background images, the pretext task involves predicting the instance category using the composite images and foreground bounding boxes. We demonstrate that incorporating bounding boxes into pretraining leads to improved alignment in both task and architecture for transfer learning. Additionally, we introduce an augmentation method for bounding boxes to further enhance feature alignment. Consequently, our model exhibits reduced performance in ImageNet semantic classification but excels in image patch localization, resulting in an overall stronger pretrained model for object detection. Experimental results indicate that our approach achieves state-of-the-art transfer learning results for object detection on PASCAL VOC and MSCOCO1 datasets.