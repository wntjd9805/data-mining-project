We present the "Incremental Implicitly-Refined Classification (IIRC)" setup, an extension to the class incremental learning setup that introduces two levels of granularity for incoming batches of classes. This means that each sample can have both a high-level (coarse) label, such as "bear," and a low-level (fine) label, such as "polar bear." The model is only provided with one label at a time and is required to infer the other label if it has already learned it. This setup reflects real-life scenarios where learners interact with the same group of entities multiple times, gaining more detailed knowledge about them while still retaining previous knowledge.Additionally, the IIRC setup allows for the evaluation of models in lifelong learning challenges that are not easily addressed in existing setups. These challenges can be exemplified by considering whether a model trained on the concept of "bear" in one task and "polar bear" in another task would forget the broader concept of "bear," correctly recognize a polar bear as a type of bear, and incorrectly associate the label of polar bear with other bear breeds.To facilitate the evaluation of models in the IIRC setup, we have developed a standardized benchmark. This benchmark enables the assessment of various state-of-the-art lifelong learning algorithms, highlighting their strengths and limitations. For example, distillation-based methods perform relatively well in this setup but are prone to making incorrect predictions by assigning too many labels to a single image.Overall, we believe that the proposed IIRC setup, along with the benchmark, provides a meaningful problem setting for practitioners to address important challenges in lifelong learning.