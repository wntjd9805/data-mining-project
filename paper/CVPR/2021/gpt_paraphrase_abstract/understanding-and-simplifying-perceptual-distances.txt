Perceptual metrics based on deep Convolutional Neural Networks (CNNs) have been highly successful in various computer vision tasks, surpassing traditional loss functions such as L1 or L2 in terms of pixel space. However, the reasons behind this success have remained somewhat unclear, as a good loss function does not rely on a specific CNN architecture or training method. This study demonstrates that similar success can be achieved using losses based on features from a deep CNN with random filters. By utilizing infinite CNNs, the authors establish an analytical representation for perceptual similarity in such networks. They further prove that the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distributions of small image patches in those images. Exploiting this equivalence, the authors propose a straightforward metric for comparing images, which directly calculates the MMD between the local patch distributions of the two images. This proposed metric is easy to comprehend, does not require deep networks, and exhibits comparable performance to perceptual metrics in various computer vision tasks.