We present an unsupervised approach for identifying and dividing sections of images that depict moving objects in live scenes. Our method involves initially dividing the motion field by minimizing mutual information between segments. These segments are then utilized to create object models that can be employed for object detection in static images. We train deep neural networks to represent both static and dynamic models using a bootstrapping technique, allowing for extrapolation to unseen objects. Although motion is required during training, the resulting object segmentation network can be applied to both static images and videos during inference. As more videos are observed, the detection of moving objects increases, which in turn aids in regularizing the identification of new objects. Our models are compared to state-of-the-art techniques in video object segmentation and salient object detection. In six benchmark datasets, our models perform favorably even when compared to methods that require manual annotation.