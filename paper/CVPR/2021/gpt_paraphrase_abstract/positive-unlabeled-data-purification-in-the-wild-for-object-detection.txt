Deep learning approaches for object detection have made significant progress thanks to large amounts of labeled images. However, the process of annotating images is still time-consuming, labor-intensive, and prone to errors. To enhance the performance of detectors, we aim to utilize all available labeled data and extract valuable samples from unlabeled images in the wild, a topic that has been seldom discussed. In this study, we propose a scheme based on positive-unlabeled learning to expand training data by identifying valuable images from unlabeled ones. The original training data is considered positive, while the unlabeled wild images serve as unlabeled data. To effectively use these purified data, we introduce a self-distillation algorithm that incorporates hint learning and ground truth bounded knowledge distillation. Experimental results demonstrate that our positive-unlabeled data purification method enhances the original detector by leveraging the massive pool of unlabeled data. Specifically, our approach improves the mean average precision (mAP) of FPN by 2.0% on the COCO benchmark.