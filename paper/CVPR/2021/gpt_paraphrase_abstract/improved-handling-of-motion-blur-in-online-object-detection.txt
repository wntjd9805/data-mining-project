We aim to detect specific objects in online vision systems that operate in real-world scenarios. Object detection is already challenging, and it becomes even more difficult when dealing with blurred images, such as those captured by car cameras or handheld phones. Most existing efforts have focused on sharp images with easily labeled ground truth, or they have treated motion blur as just one of many types of image corruptions. However, we specifically concentrate on the details of egomotion-induced blur. We explore five different approaches to address the performance gap between detecting objects in sharp and blurred images. These approaches include deblurring, multi-scale texture analysis, out-of-distribution testing, custom label generation, and conditioning the model based on the type of blur. Surprisingly, we find that custom label generation, which aims to resolve spatial ambiguity, significantly improves object detection compared to the other approaches. Additionally, contrary to classification findings, we observe a notable improvement by conditioning our model on specific categories of motion blur. We experimentally validate and combine these different approaches using blurred COCO images and real-world blur datasets, resulting in a practical and effective model with superior detection rates.