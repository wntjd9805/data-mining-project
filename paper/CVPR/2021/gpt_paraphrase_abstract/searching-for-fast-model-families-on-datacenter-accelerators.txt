Neural Architecture Search (NAS) and model scaling have made significant advancements in designing efficient and accurate convolutional architecture families. However, they fail to consider important hardware architecture details, limiting their utilization of emerging datacenter accelerators. This study aims to develop fast and accurate CNN models optimized for efficient inference on datacenter accelerators. Analysis of datacenter accelerators reveals that existing CNNs lack operational intensity, parallelism, and execution efficiency, and exhibit disproportionate FLOPs-latency relationship. To address these issues, a search space specific to datacenter accelerators is created, incorporating various optimized convolution structures and activation functions. Additionally, a novel multi-objective compound scaling method, called latency-aware compound scaling (LACS), is proposed to optimize both accuracy and latency. LACS determines that network depth should increase at a faster rate than image size and network width, contrary to previous observations. Applying LACS and the new search space, a new model series called EfﬁcientNet-X is developed. EfﬁcientNet-X achieves more than 2X speed improvement over EfﬁcientNet, a state-of-the-art model series, on TPUv3 and GPUv100, while maintaining comparable accuracy. Furthermore, EfﬁcientNet-X outperforms recent models such as RegNet and ResNeSt by up to 7X on TPUv3 and GPUv100. The source code for the models can be found at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/tpu.