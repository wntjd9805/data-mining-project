This paper addresses the issue of limited data available for training optical flow networks. It highlights the drawbacks of existing sources such as labeled synthetic datasets or unlabeled real videos. The paper introduces a framework that can generate accurate ground-truth optical flow annotations quickly and in large quantities using a single real picture. The approach involves using a monocular depth estimation network to create a plausible point cloud of the observed scene. By virtually moving the camera in the reconstructed environment with known motion vectors and rotation angles, both a new view and the corresponding optical flow field connecting each pixel in the original image to the new frame can be synthesized. Training optical flow networks with this generated data leads to better generalization on unseen real data compared to models trained on annotated synthetic datasets or unlabeled videos alone. Additionally, combining synthetic images with the generated data improves specialization.