We introduce ACON, a simple and effective activation function that learns to activate or deactivate neurons. We discover that Swish, a popular activation function, can be seen as a smooth approximation of ReLU. Similarly, we approximate the Maxout family with our ACON family, leading to significant performance improvements and making Swish a special case of ACON. Additionally, we propose meta-ACON, which optimizes the switching between nonlinear and linear parameters, providing a new design space. By using ACON, we achieve impressive results on both small and large models, improving the accuracy rate in ImageNet by 6.7% and 1.8% for MobileNet-0.25 and ResNet-152, respectively. Furthermore, ACON proves to be effective in object detection and semantic segmentation tasks, demonstrating its versatility. The code for ACON is available at https://github.com/nmaac/acon.