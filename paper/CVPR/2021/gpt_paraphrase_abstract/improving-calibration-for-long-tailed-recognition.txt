Two-stage methods have been used to improve the performance of deep neural networks when dealing with heavily class-imbalanced training datasets. However, a significant issue that still remains is miscalibration. In order to address this problem, we have developed two methods to enhance calibration and overall performance in such scenarios. Our first method, label-aware smoothing, takes into consideration the predicted probability distributions of classes, which are closely associated with the number of instances in each class. This approach helps to handle varying degrees of overconfidence for different classes and improves the learning of the classifier.Additionally, we have introduced shifted batch normalization in the decoupling framework to tackle dataset bias that may occur between the two stages of representation learning and classifier learning. This bias can arise due to the use of different samplers during these stages.The effectiveness of our proposed methods has been demonstrated through experiments on various popular long-tailed recognition benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. Our methods have achieved new state-of-the-art results on these datasets.