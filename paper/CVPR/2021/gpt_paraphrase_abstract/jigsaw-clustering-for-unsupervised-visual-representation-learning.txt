Unsupervised representation learning has been successful using contrastive learning. These methods typically duplicate each training batch to create contrastive pairs, resulting in additional computation. This paper introduces a new jigsaw clustering pretext task that only requires forwarding each training batch itself, reducing training costs. The proposed method utilizes information from both intra- and inter-images and significantly outperforms previous single-batch based approaches. It even performs comparably to contrastive learning methods when using only half of the training batches. This suggests that multiple batches during training are unnecessary, opening avenues for future research on single-batch unsupervised methods. The models trained on ImageNet datasets achieve state-of-the-art results with linear classification, surpassing previous single-batch methods by 2.6%. Moreover, the transferred models to COCO datasets outperform MoCo v2 by 0.4% using only half of the training batches. Additionally, the pretrained models outperform supervised ImageNet pretrained models on CIFAR-10 and CIFAR-100 datasets by 0.9% and 4.1%, respectively.