The alignment of partial views of a scene is crucial for understanding the environment and is important for various robotics tasks. Recent methods have introduced end-to-end systems that surpass traditional approaches by utilizing pose supervision. However, as cameras with depth sensors become more common, we can anticipate a new influx of raw RGB-D data that lacks the necessary annotations for supervision. To address this, we propose UnsupervisedR&R, a completely unsupervised approach for learning point cloud registration from raw RGB-D video. Our approach relies on differentiable alignment and rendering to ensure consistency in both photometric and geometric aspects between frames. We assess the effectiveness of our method using indoor scene datasets and discover that we outperform existing traditional approaches that employ classical and learned descriptors, while remaining competitive with supervised geometric point cloud registration methods.