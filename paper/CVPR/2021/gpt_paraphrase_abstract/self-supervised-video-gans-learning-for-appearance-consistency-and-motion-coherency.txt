This study introduces a novel method for video Generative Adversarial Networks (GANs) that focuses on achieving appearance consistency and motion coherency in videos. The proposed method utilizes self-supervised approaches, where dual discriminators for image and video are trained to solve their own specific tasks: appearance contrastive learning and temporal structure puzzle. These tasks allow the discriminators to learn representations of appearance and temporal context, while also compelling the generator to synthesize videos with consistent appearance and natural flow of motions. The effectiveness of the proposed method is demonstrated through extensive experiments on facial expression and human action benchmarks, which show superior performance compared to existing video GANs. Furthermore, the consistent improvements observed across different video GAN architectures validate the generality of the proposed framework.