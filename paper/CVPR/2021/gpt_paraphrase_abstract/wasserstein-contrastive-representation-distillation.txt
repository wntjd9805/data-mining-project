Knowledge distillation (KD) aims to transfer the knowledge of a teacher network to a smaller student network. However, existing methods like Kullback-Leibler divergence may not capture important structural knowledge and lack feature generalization, especially when the teacher and student networks have different classification tasks. To address this, we propose Wasserstein Contrastive Representation Distillation (WCoRD) which utilizes both primal and dual forms of Wasserstein distance for KD. The dual form transfers global knowledge, maximizing mutual information between the networks. The primal form facilitates local contrastive knowledge transfer within mini-batches, matching feature distributions. Experimental results demonstrate the superiority of WCoRD in privileged information distillation, model compression, and cross-modal transfer compared to state-of-the-art approaches.