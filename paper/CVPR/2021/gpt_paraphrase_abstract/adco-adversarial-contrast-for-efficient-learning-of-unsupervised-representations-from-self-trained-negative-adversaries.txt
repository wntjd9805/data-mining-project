Contrastive learning methods typically rely on generating difficult negative examples to train models to distinguish positive queries. However, existing methods have limitations in tracking changes in learned representations over iterations and utilizing useful information from past minibatches. In this study, we propose a new approach called Adversarial Contrastive (AdCo) that directly learns a set of negative adversaries to challenge the self-trained representation. The representation network and negative adversaries are alternately updated to obtain the most challenging negative examples for training the positive queries. We also demonstrate that the negative adversaries can track changes in representations over time by maximizing the adversarial contrastive loss. Experimental results show that the AdCo model achieves superior performance on ImageNet, with a top-1 accuracy of 73.2% over 200 epochs and 75.7% over 800 epochs in linear evaluation. Additionally, the AdCo model can be pre-trained more efficiently with shorter GPU time and fewer epochs. The source code for the AdCo model is available at https://github.com/maple-research-lab/AdCo.