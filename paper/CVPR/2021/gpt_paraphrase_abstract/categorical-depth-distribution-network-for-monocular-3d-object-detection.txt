Monocular 3D object detection is an important problem for autonomous vehicles, as it offers a simpler configuration compared to multi-sensor systems. The main challenge in monocular 3D detection is accurately predicting object depth, which is difficult due to the lack of direct range measurement. Many methods attempt to estimate depth directly, but they often have limited performance due to depth inaccuracy. Our proposed solution, the Categorical Depth Distribution Network (CaDDN), addresses this challenge by using a predicted categorical depth distribution for each pixel. This allows us to project contextual feature information to the appropriate depth interval in 3D space. We then use a computationally efficient bird's-eye-view projection and a single-stage detector to generate the final output detections. CaDDN is designed as a fully differentiable end-to-end approach for joint depth estimation and object detection. We evaluate our approach on the KITTI 3D object detection benchmark and achieve the top ranking among published monocular methods. Additionally, we provide the first monocular 3D detection results on the newly released Waymo Open Dataset. The code for CaDDN is also made available.