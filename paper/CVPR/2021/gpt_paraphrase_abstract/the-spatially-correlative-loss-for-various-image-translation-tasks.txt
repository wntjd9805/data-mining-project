We present a new approach for preserving scene structure consistency in unpaired image-to-image translation tasks. Existing methods rely on pixel-level or feature-level losses, but these are limited in their ability to handle large domain gaps. To overcome this challenge, we leverage the spatial patterns of self-similarity to define scene structure. Our proposed spatially-correlative loss focuses on capturing spatial relationships within an image rather than domain appearance. Additionally, we introduce a self-supervised learning method to explicitly learn spatially-correlative maps for each translation task. Our experiments demonstrate significant improvement over baseline models in various modes of unpaired image-to-image translation. Furthermore, our approach can easily be integrated into existing network architectures, making it widely applicable. The source code is available at https://github.com/lyndonzheng/F-LSeSim.