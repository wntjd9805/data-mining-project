We examine the task of quantizing N sorted, scalar data points using a fixed codebook with K entries that can be rescaled. The objective is to determine the optimal scaling factor α and assign the data points to the α-scaled codebook in order to minimize the squared error between the original and quantized points. Previous solutions to this problem only provided globally optimal algorithms for specific codebooks (binary and ternary) or assumed certain distributions (Gaussian, Laplacian). By analyzing the characteristics of the optimal quantizer, we develop an algorithm that has a time complexity of O(N K log K) and can find the optimal quantization parameters for any fixed codebook, regardless of the data distribution. We apply our algorithm to both synthetic and real-world neural network quantization problems and demonstrate its effectiveness.