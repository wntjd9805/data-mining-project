We introduce a new method for combining depth maps online, which utilizes a latent feature space to learn depth map fusion. Unlike previous fusion techniques that rely on explicit scene representations such as signed distance functions (SDFs), we propose a learned feature representation for fusion. Our approach involves separating the scene representation used for fusion from the output scene representation through an additional translator network. Our neural network architecture comprises a depth and feature fusion sub-network, followed by a translator sub-network that generates the final surface representation (e.g. TSDF) for visualization or other tasks. Our method operates in real-time, effectively handles high levels of noise, and is particularly adept at handling significant outliers commonly found in depth maps derived from photometric stereo. Experimental results using both real and synthetic data demonstrate superior performance compared to existing methods, especially in challenging scenarios with substantial noise and outliers. The source code will be provided at https://github.com/weders/NeuralFusion.