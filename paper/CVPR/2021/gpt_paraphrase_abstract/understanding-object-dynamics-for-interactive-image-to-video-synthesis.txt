This study explores the impact of locally manipulating a static scene and presents a novel approach for learning natural global articulations resulting from such local manipulations at the pixel level. The proposed method utilizes videos of moving objects without any knowledge of the underlying physical scene manipulation. Through a generative model, the approach learns to infer realistic object dynamics in response to user interaction and captures the interrelations between different regions of the object's body. Given a static image and a local poke at a pixel, the model predicts the object's deformation over time. Unlike existing video prediction techniques, this approach allows for interactive control of the deformation instead of synthesizing arbitrary realistic videos. The model is not limited to specific object categories and can transfer dynamics to unseen object instances. Extensive experiments using diverse objects demonstrate the effectiveness of the approach compared to common video prediction frameworks. For more details, please refer to the project page at https://bit.ly/3cxfA2L.