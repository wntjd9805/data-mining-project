Deep learning techniques have been successful in analyzing and understanding 3D point clouds. However, the process of annotating large-scale point sets is expensive, which highlights the need for generalizable representations that can be applied to different point sets. This paper introduces a novel problem called 3D Domain Generalization (3DDG), which aims to develop models that can generalize to unseen domains of point clouds without having access to them during training. This is a challenging task because there is a significant shift in geometry between simulated and real data, causing existing models to underperform due to overfitting. To address this problem, the paper proposes a solution called MetaSets, which involves meta-learning point cloud representations from transformed point sets with specific geometry priors. These learned representations are more adaptable to different geometries in unseen domains. The paper also presents two benchmarks for evaluating the transfer of 3D point clouds from simulated to real environments. Experimental results demonstrate that MetaSets outperforms existing deep learning methods by a significant margin.