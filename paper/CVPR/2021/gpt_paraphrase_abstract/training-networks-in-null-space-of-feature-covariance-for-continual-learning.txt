This paper introduces a new network training algorithm called Adam-NSCL to address the issue of catastrophic forgetting in continual learning. The algorithm aims to strike a balance between network stability and plasticity. The authors propose two mathematical conditions to achieve stability and plasticity in continual learning. The training process involves sequentially optimizing network parameters in the null space of all previous tasks. This is done by projecting the candidate parameter update into the approximate null space of previous tasks using singular value decomposition. The uncentered covariance matrix of input features is incrementally computed after each task for efficiency. The rationality of the approximate null space at each linear layer is empirically verified. The proposed approach is evaluated on CIFAR-100 and TinyImageNet datasets and outperforms or matches state-of-the-art continual learning approaches.