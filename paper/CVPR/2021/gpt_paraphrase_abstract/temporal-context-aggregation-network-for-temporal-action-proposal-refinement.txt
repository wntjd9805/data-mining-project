Temporal action proposal generation is a challenging task in video understanding. Current methods for generating proposals suffer from inaccurate temporal boundaries and low confidence. To address this, we propose a method called Temporal Context Aggregation Network (TCANet) that utilizes both local and global temporal context and refines boundaries progressively. Our approach includes a Local-Global Temporal Encoder (LGTE) that efficiently encodes the dependencies between local and global temporal information. We also utilize the boundary and internal context of proposals for frame-level and segment-level boundary regressions, respectively. The Temporal Boundary Regressor (TBR) combines these regression granularities to achieve precise boundaries and reliable confidence of proposals.We evaluate our method on three challenging datasets (HACS, ActivityNet-v1.3, and THUMOS-14) and demonstrate that TCANet generates proposals with high precision and recall. When combined with existing action classifiers, TCANet achieves remarkable temporal action detection performance compared to other methods. Notably, TCANet achieved first place in the CVPR 2020 - HACS challenge leaderboard for temporal action localization.In summary, our proposed TCANet utilizes temporal context aggregation and boundary refinement to generate high-quality action proposals. It achieves state-of-the-art performance on multiple datasets and outperforms other methods in temporal action detection.