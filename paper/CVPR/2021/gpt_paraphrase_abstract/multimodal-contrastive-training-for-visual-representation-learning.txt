We propose a novel approach to learning visual representations by incorporating multimodal data, utilizing both intra- and inter-modal similarity preservation objectives. Unlike existing methods that focus on single-domain proxy prediction tasks, our method leverages intrinsic data properties within each modality and semantic information from cross-modal correlation simultaneously. This leads to improved quality of learned visual representations. By combining multimodal training with various contrastive losses, our method can learn more powerful and generic visual features. We train our model on COCO and evaluate the learned visual representations on tasks such as image classification, object detection, and instance segmentation. Our method achieves state-of-the-art results on ImageNet classification, with a top-1 validation accuracy of 55.3% under the common transfer protocol, when pre-trained on COCO. We also demonstrate the effectiveness of our method on multi-label image tagging and cross-modal retrieval tasks using the large-scale Stock images dataset.