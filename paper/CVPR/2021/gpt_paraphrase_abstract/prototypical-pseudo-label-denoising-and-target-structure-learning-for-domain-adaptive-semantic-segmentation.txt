Self-training is a popular technique in domain adaptive segmentation, where a network is trained using pseudo labels in the target domain. However, this approach is prone to noisy pseudo labels and dispersed target features due to differences between the source and target domains. This paper proposes a method that utilizes representative prototypes, which are the centroids of classes, to address these challenges in unsupervised domain adaptation. Additionally, the paper explores the use of feature distances from prototypes to obtain richer information for estimating the likelihood of pseudo labels and facilitating online correction during training. The method also aligns prototypical assignments based on relative feature distances, resulting in a more compact target feature space. Furthermore, the paper finds that distilling the learned knowledge into a self-supervised pretrained model further enhances performance. Experimental results demonstrate significant improvements over existing methods. The code for this method is available at https://github.com/microsoft/ProDA.