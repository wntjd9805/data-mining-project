Appearance-based detectors perform well on typical scenes due to their high-capacity models and large annotated data. However, they struggle to work effectively in scenarios with limited training data. On the other hand, geometric motion segmentation algorithms can adapt to new scenes but have not yet reached the same level of performance as appearance-based detectors. This is mainly due to inaccurate motion estimations and motion configurations that are difficult to interpret. To combine the strengths of both approaches, we propose a modular network inspired by a geometric analysis of independent object motions that can be extracted from an egomotion field. This network takes two consecutive frames as input and predicts segmentation masks for the background and multiple rigidly moving objects, represented by 3D rigid transformations. Our method achieves state-of-the-art results in rigid motion segmentation on datasets such as KITTI and Sintel. Additionally, the inferred rigid motions significantly enhance depth and scene flow estimation. The code for our method is available at github.com/gengshan-y/rigidmask.