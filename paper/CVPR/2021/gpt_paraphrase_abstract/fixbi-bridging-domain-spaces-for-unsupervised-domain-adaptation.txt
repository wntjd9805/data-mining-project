Unsupervised domain adaptation (UDA) techniques have made significant progress in learning domain invariant representations. However, most studies have focused on direct adaptation from the source domain to the target domain, which leads to challenges due to large domain discrepancies. This paper presents a UDA method that effectively addresses these discrepancies. The proposed approach introduces a fixed ratio-based mixup to create multiple intermediate domains between the source and target domains. By training models on these augmented domains, we obtain a source-dominant model and a target-dominant model with complementary characteristics. Our confidence-based learning strategies, such as bidirectional matching with high-confidence predictions and self-penalization using low-confidence predictions, enable the models to learn from each other and their own results. This gradual transfer of domain knowledge from the source to the target domain is facilitated by our proposed methods. Extensive experiments conducted on three public benchmarks (Ofﬁce-31, Ofﬁce-Home, and VisDA-2017) demonstrate the superiority of our approach.