Our study explores the possibility of utilizing domain information from unseen test samples to improve domain generalization, which involves performing inference on data distributions that differ from the training data. We propose a two-step domain-adaptive approach: first, we learn a discriminative domain embedding from unsupervised training examples, and then we use this embedding as supplementary information to build a domain-adaptive model that considers both the input and its domain when making predictions. For unseen domains, our method uses a small number of unlabeled test examples to construct the domain embedding, allowing for adaptive classification in any unseen domain. Our approach achieves state-of-the-art performance on various domain generalization benchmarks. Additionally, we introduce the Geo-YFCC benchmark, a real-world, large-scale domain generalization benchmark that contains 1.1 million samples across 40 training, 7 validation, and 15 test domains, significantly larger than previous benchmarks. We demonstrate that existing approaches either do not scale to this dataset or underperform compared to a simple baseline of training a model on the union of data from all training domains. In contrast, our approach achieves a significant 1% improvement.