We present a new method called IMAGINE (IMAge-Guided model INvErsion) that can generate high-quality and diverse images using just one training sample. By utilizing the knowledge of image semantics from a pre-trained classifier, we can generate plausible images by matching multi-level feature representations in the classifier. We also incorporate adversarial training with an external discriminator to improve the quality of the generated images. IMAGINE allows for the enforcement of semantic specificity during the synthesis process, produces realistic images without the need for generator training, and provides users with intuitive control over the generation process. Through extensive experiments, we show that IMAGINE outperforms state-of-the-art GAN-based and inversion-based methods in three different image domains (objects, scenes, and textures). This research was conducted during an internship at Adobe Research.