Despite significant advancements in the application of neural networks (NN) across various domains, their lack of transparency and interpretability remains a major challenge. While recent developments in explainable artificial intelligence aim to address this issue by visualizing the relationship between input pixels and final outputs, these approaches are limited to explaining basic connections and do not offer insights on error correction. In this study, we propose a framework called VRX, which aims to interpret classification NNs using intuitive visual concepts with a structural perspective. Using a trained classification model, VRX extracts relevant visual concepts specific to each class and organizes them into structural concept graphs based on their pairwise relationships. Through knowledge distillation, we demonstrate that VRX can partially replicate the reasoning process of NNs and provide logical, concept-level explanations for the model's decisions. Through extensive experiments, we empirically demonstrate that VRX can effectively answer "why" and "why not" questions about predictions, offering easily understandable insights into the reasoning process. Furthermore, we show that these insights have the potential to guide improvements in NN performance.