We introduce a new method for learning audio and video representations using self-supervised learning. Previous methods rely on the natural correspondence between audio and video to train models to match representations from both modalities. However, this approach has two sources of training noise. Firstly, the audio and video signals can sometimes be uninformative of each other, leading to faulty positive correspondences. To mitigate the impact of these faulty positives, we propose a weighted contrastive learning loss that reduces their contribution to the overall loss. Secondly, the random sampling of negative instances in self-supervised contrastive learning can result in semantically similar instances being used as faulty negatives. To overcome this issue, we suggest optimizing an instance discrimination loss with a soft target distribution that estimates relationships between instances. We conduct extensive experiments on action recognition tasks to validate our approach and demonstrate that it addresses the challenges of audio-visual instance discrimination and improves transfer learning performance.