Enhancing low-light images is a crucial task with various practical applications. Existing methods that focus on single images perform well in static scenarios but struggle with temporal instability when applied to low-light videos. This issue arises because these methods are trained using single image pairs, lacking temporal information. Collecting real temporally consistent data for training is challenging due to the difficulty of obtaining large-scale and diverse low-light and normal-light video pairs with identical noise statistics in controlled environments. In this study, we propose a novel approach to address this problem by enforcing temporal stability in low-light video enhancement using static images. Our method involves learning and inferring motion fields (optical flow) from a single image to synthesize short-range video sequences. This strategy can be applied directly to large-scale datasets. Based on this idea, we present our method, which utilizes motion priors to enhance single-image low-light videos and ensure temporal consistency. Extensive experiments and user studies demonstrate the superior performance of our proposed method. The code and model for our approach will be made publicly available at https://github.com/zkawfanx/StableLLVE.