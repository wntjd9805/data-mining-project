We introduce a new method called exponential moving average normalization (EMAN) as a replacement for batch normalization (BN). EMAN improves the performance of existing self- and semi-supervised learning techniques that rely on student-teacher models. Unlike BN, which computes statistics within each batch, EMAN updates its statistics using exponential moving averages from the BN statistics of the student. This approach reduces the interdependence between samples in BN and enhances the generalization of the teacher. EMAN consistently outperforms strong baselines for self-supervised learning by 4-6/1-2 points and semi-supervised learning by about 7/2 points on ImageNet, even when only 1%/10% supervised labels are available. These improvements hold across different methods, network architectures, training durations, and datasets, demonstrating the general effectiveness of EMAN. The code for EMAN will be made available online.