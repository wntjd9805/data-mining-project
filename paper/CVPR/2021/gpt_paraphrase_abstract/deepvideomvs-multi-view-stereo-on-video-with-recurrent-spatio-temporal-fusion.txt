We present a novel method for predicting depth in online video streams using multiple views. Our approach efficiently and realistically propagates scene geometry information from previous time steps to the current time step. The foundation of our method is a lightweight encoder-decoder that can operate in real-time. To enhance its capabilities, we introduce a ConvLSTM cell at the bottleneck layer, allowing compression of a significant amount of past information in its states. The key innovation is the propagation of the hidden state of the cell while considering changes in viewpoint between time steps. At each time step, we warp the previous hidden state to the current camera plane using the previous depth prediction. This extension incurs minimal computational overhead and memory usage, while significantly improving the accuracy of depth predictions. In our evaluations on numerous indoor scenes, our method surpasses existing state-of-the-art multi-view stereo approaches across various metrics, all while maintaining real-time performance. Our code is available at: https://github.com/ardaduz/deep-video-mvs.