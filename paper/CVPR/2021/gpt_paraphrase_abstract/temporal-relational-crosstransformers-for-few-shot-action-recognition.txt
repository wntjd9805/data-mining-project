We present a new approach for few-shot action recognition that involves identifying corresponding frame tuples between the query and videos in the support set. Unlike previous methods, we use the CrossTransformer attention mechanism to create class prototypes, allowing us to observe relevant sub-sequences from all support videos. Instead of relying on class averages or single best matches, we form video representations using ordered tuples of frames with varying numbers. This enables us to compare sub-sequences of actions with different speeds and temporal offsets. Our proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art performance on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51, and UCF101 datasets. Significantly, our method outperforms prior work on SSv2 by a substantial margin (12%) due to its ability to model temporal relations. A detailed analysis highlights the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.