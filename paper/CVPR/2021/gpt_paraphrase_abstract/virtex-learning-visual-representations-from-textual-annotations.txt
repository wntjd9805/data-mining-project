Many vision tasks rely on pretrained visual representations, typically obtained through supervised training on ImageNet. However, recent approaches have explored unsupervised pretraining to handle large amounts of unlabeled images. In contrast, our goal is to learn high-quality visual representations using a smaller number of images. To achieve this, we propose VirTex, a pretraining method that utilizes semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions and then transfer them to various recognition tasks such as image classification, object detection, and instance segmentation. Remarkably, VirTex outperforms or matches features learned on ImageNet, whether supervised or unsupervised, despite using only a fraction of the images (up to ten times fewer).