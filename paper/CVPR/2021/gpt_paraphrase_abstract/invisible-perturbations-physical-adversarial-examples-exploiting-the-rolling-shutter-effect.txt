Previous methods for creating physical adversarial examples in computer vision relied on visible artifacts, such as stickers or colorful textures, to perturb the object being observed. This approach assumes that the perturbations must be visible for the camera to detect them. In contrast, we propose a new method for generating physical adversarial examples that are invisible to the human eye. Instead of modifying the object itself, we manipulate the light that illuminates the object. We show how an attacker can create a modulated light signal that adversarially illuminates a scene, leading to targeted misclassifications by a state-of-the-art deep learning model. We achieve this by exploiting the radiometric rolling shutter effect in standard cameras, which creates precise striping patterns in the captured images. To humans, it appears as if the object is properly illuminated, but the camera captures stripes that trick the machine learning models into outputting the desired classification chosen by the attacker. We perform a series of simulations and physical experiments using LEDs, achieving targeted attack success rates of up to 84%.