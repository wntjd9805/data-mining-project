We demonstrate that it is possible to remove the influence of a subset of training samples from the weights of a network trained on image classification tasks. This process, referred to as "forgetting," can be performed while still maintaining a significant amount of remaining information. We introduce a new concept of forgetting in a mixed-privacy setting, where a core subset of training samples is known not to require forgetting. This variation of the problem improves the accuracy and reliability of forgetting methods applied to vision classification tasks. Our approach enables efficient removal of all information from non-core data by setting a subset of weights to zero, with minimal impact on performance. We achieve these results by replacing a deep network with an appropriate linear approximation. By making specific modifications to the network architecture and training procedure, we show that this linear approximation can achieve comparable performance to the original network. The forgetting problem becomes quadratic and can be solved efficiently, even for large models. Unlike previous forgetting methods for deep networks, our approach can achieve close to state-of-the-art accuracy on large-scale vision tasks without sacrificing model accuracy.