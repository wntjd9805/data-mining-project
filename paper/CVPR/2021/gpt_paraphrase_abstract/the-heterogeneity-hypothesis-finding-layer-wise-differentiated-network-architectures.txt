This paper addresses the problem of designing convolutional neural networks (CNNs). Instead of focusing on the overall architecture, the study explores a neglected area of adjusting the channel configurations of predefined networks. The authors discover that by shrinking widened baseline networks, they can achieve superior performance. They propose the "heterogeneity hypothesis" which states that a layer-wise differentiated network architecture (LW-DNA) can outperform the original network with regular channel configurations, while having a lower model complexity. The LW-DNA models are identified without additional computational cost or training time compared to the original network, allowing for controlled experiments that emphasize the importance of layer-wise specific channel configurations. These LW-DNA models offer advantages in terms of overfitting and consistently outperform the baseline models in various experiments involving image classification, visual tracking, and image restoration. The code for the study is available at https://github.com/ofsoundof/Heterogeneity_Hypothesis.git.