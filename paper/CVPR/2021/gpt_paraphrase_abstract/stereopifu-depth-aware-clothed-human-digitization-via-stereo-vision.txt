This paper presents StereoPIFu, a method that combines the geometric constraints of stereo vision with the implicit function representation of PIFu to reconstruct the 3D shape of clothed humans using low-cost rectified images. The method utilizes voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. It also introduces a novel relative z-offset to associate predicted human depth and occupancy inference, enhancing surface detail restoration. The network structure is designed to fully utilize geometry information from the stereo images, resulting in improved human body reconstruction quality. StereoPIFu accurately infers the spatial location of the human body in camera space and maintains the correct relative position of different body parts, enabling the capture of human performance. Extensive experimental results demonstrate that StereoPIFu significantly improves the robustness, completeness, and accuracy of clothed human reconstruction compared to previous methods.