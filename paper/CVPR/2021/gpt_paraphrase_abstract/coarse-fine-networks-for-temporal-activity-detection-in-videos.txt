This paper introduces Coarse-Fine Networks, a two-stream architecture that improves video representations for long-term motion by leveraging different abstractions of temporal resolution. Traditional video models process inputs at fixed temporal resolutions without dynamic frame selection. However, the authors argue that processing multiple temporal resolutions and dynamically estimating the importance of each frame can greatly enhance video representations, particularly in temporal activity localization. To achieve this, the authors propose two components: 'Grid Pool', a learned temporal downsampling layer for extracting coarse features, and 'Multi-stage Fusion', a spatio-temporal attention mechanism for integrating fine-grained context with the coarse features. Experimental results demonstrate that their approach surpasses state-of-the-art methods in action detection on public datasets like Charades while significantly reducing computational and memory requirements. The code for their method is publicly available at https://github.com/kkahatapitiya/Coarse-Fine-Networks.