Action segmentation, which involves identifying boundaries between visually consistent concepts in videos, is crucial for various video understanding tasks. Although supervised approaches have shown promising results, they require extensive frame-level annotations. In this study, we propose an unsupervised method for segmenting actions in videos without any training. Our approach utilizes a temporally-weighted hierarchical clustering algorithm that effectively groups visually consistent frames. Our key finding is that creating a 1-nearest neighbor graph representation of the video, taking into account the temporal progression, is sufficient to form clusters of frames that are both visually and temporally consistent, where each cluster potentially represents an action. Moreover, we establish strong unsupervised baselines for action segmentation and demonstrate significant performance improvements compared to existing unsupervised methods on five challenging action segmentation datasets. Our code is available for access.