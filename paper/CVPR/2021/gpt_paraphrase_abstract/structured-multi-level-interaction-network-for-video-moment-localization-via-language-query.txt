We address the issue of localizing a specific moment described in a natural language query. Existing approaches either interact the query with a video frame or a moment proposal, but they overlook the inherent structure of moment construction, which is crucial for both cross-modal understanding and video content comprehension. In this study, we break down the activity moment into its boundary and content components. Utilizing this explored moment structure, we introduce a new approach called the Structured Multi-level Interaction Network (SMIN) to solve this problem through multi-level cross-modal interaction and content-boundary-moment interaction. Our method involves interacting the sentence-level query with the entire moment and the word-level query with the content and boundary, following a coarse-to-fine approach. Additionally, we capture the meaningful relationships between the boundary, content, and the whole moment proposal through content-boundary-moment interaction. Through these multi-level interactions, our model obtains robust cross-modal representations for accurate moment localization. Extensive experiments conducted on three benchmark datasets (Charades-STA, ActivityNet-Captions, and TACoS) demonstrate that our proposed approach outperforms state-of-the-art methods.