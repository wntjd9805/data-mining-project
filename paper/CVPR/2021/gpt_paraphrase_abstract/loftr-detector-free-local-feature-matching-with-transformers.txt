We introduce a new approach for matching local image features that deviates from the traditional sequential process of feature detection, description, and matching. Our method involves establishing dense pixel-wise matches at a coarse level and refining the good matches at a finer level. Unlike dense methods that rely on cost volumes for searching correspondences, we utilize self and cross attention layers in the Transformer model to generate feature descriptors that are conditioned on both images. The Transformer's global receptive field allows our method to generate dense matches even in low-texture areas where conventional feature detectors struggle to identify consistent interest points. Our experiments on various indoor and outdoor datasets demonstrate that our method, called LoFTR, significantly outperforms existing state-of-the-art methods. Furthermore, LoFTR achieves the top rank on two widely recognized benchmarks for visual localization among all published methods. The code for our method is available on our project page at https://zju3dv.github.io/loftr/.