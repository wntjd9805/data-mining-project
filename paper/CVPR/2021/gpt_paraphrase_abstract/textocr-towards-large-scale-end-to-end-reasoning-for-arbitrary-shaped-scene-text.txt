A crucial aspect of understanding scene text in TextVQA and TextCaps datasets is the detection and recognition of text using an OCR system. However, current systems face challenges due to the lack of ground truth text annotations for these datasets, as well as a shortage of scene text detection and recognition datasets on real images. This hinders progress in OCR and the evaluation of scene text-based reasoning independently from OCR systems. To address this, we introduce TextOCR, a system that detects and recognizes arbitrary-shaped scene text using 900k annotated words from real images in the TextVQA dataset. We demonstrate that existing state-of-the-art OCR models struggle to perform well on TextOCR, but training on TextOCR improves performance on multiple other OCR datasets. We utilize a TextOCR-trained OCR model to develop the PixelM4C model, which enables end-to-end scene text-based reasoning on images. This allows us to reconsider various design choices and achieve new state-of-the-art performance on the TextVQA dataset. The answer format output is limited to the abstract.