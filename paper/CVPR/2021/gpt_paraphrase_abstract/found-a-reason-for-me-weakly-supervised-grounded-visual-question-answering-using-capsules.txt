Recently, the research community has shown increased interest in addressing the problem of grounding visual question answering (VQA) tasks. Existing approaches have mostly focused on using pre-trained object detectors to solve this problem. However, these detectors rely on bounding box annotations, which may not always be available or practical for real-life large-scale applications. In this paper, we propose a different approach that tackles the grounding of visual entities in a weakly supervised manner, solely by training on the VQA task.  To tackle this problem, we introduce a visual capsule module that incorporates a query-based selection mechanism of capsule features. This allows the model to selectively focus on relevant regions based on textual cues from the question about visual information. By integrating this capsule module into existing VQA systems, we observe significant improvements in performance on the weakly supervised grounding task.  We demonstrate the effectiveness of our approach on two state-of-the-art VQA systems, namely stacked NMN and MAC, using two different evaluation datasets. The first dataset, CLEVR-Answers benchmark, consists of CLEVR scenes with groundtruth bounding boxes for objects relevant to the correct answer. The second dataset, GQA, is a real-world VQA dataset with compositional questions. In both cases, our systems with the proposed capsule module consistently outperform the respective baseline systems in terms of answer grounding, while achieving comparable performance on the VQA task.  Overall, our approach offers a more relaxed setting for grounding relevant visual entities in VQA tasks, eliminating the need for bounding box annotations. By leveraging the query-based selection mechanism of our visual capsule module, we enhance the performance of existing VQA systems, making them more effective in weakly supervised grounding scenarios.