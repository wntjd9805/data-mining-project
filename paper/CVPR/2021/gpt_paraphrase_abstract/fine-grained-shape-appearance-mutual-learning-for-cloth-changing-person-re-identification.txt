Recently, there have been significant advancements in person re-identification (Re-ID). However, current methods heavily rely on color appearance, which becomes unreliable when a person changes their clothes. Cloth-changing Re-ID is particularly challenging because images of pedestrians with different clothes exhibit large intra-class variation and small inter-class variation. There are important features for identification that are embedded in subtle body shape differences between pedestrians. In order to explore these body shape cues for cloth-changing Re-ID, we propose a Fine-grained Shape-Appearance Mutual learning framework (FSAM). This framework consists of two streams, one for learning fine-grained discriminative body shape knowledge and the other for transferring this knowledge to complement the appearance features. The shape stream of FSAM learns fine-grained discriminative masks with the guidance of identities and extracts detailed body shape features using a pose-specific multi-branch network. To complement the cloth-unrelated shape knowledge in the appearance stream, a dense interactive mutual learning process is performed between low-level and high-level features. This allows the appearance stream to be used independently without requiring additional computation for mask estimation. We evaluated our method on benchmark cloth-changing Re-ID datasets and achieved state-of-the-art performance.