Training Convolutional Neural Networks (CNNs) with data containing noisy labels is a difficult task. Directly providing labels to the data can lead to the CNNs memorizing incorrect labels. To address this issue, an indirect learning approach called Negative Learning for Noisy Labels (NLNL) has been effective in preventing overfitting by using complementary labels. However, NLNL requires a three-stage pipeline, which makes the filtering process time-consuming and increases training cost. To overcome these challenges, this study introduces a novel improvement to NLNL called Joint Negative and Positive Learning (JNPL). JNPL combines the filtering pipeline into a single stage, simplifying the training process. It trains CNNs using two loss functions: NL+ and PL+. NL+ is a modified version of NL that enhances convergence of noisy data by improving the gradient. PL+ is designed to enable faster convergence to clean data. By simultaneously training with NL+ and PL+, JNPL simplifies the pipeline and improves practical usability compared to NLNL. Additionally, when combined with a simple semi-supervised training technique, JNPL achieves state-of-the-art accuracy for classifying noisy data due to its superior filtering ability.