The group maximum differentiation competition (gMAD) has been used to enhance blind image quality assessment (BIQA) models by incorporating full-reference metrics. However, a significant challenge arises when applying this approach to troubleshoot existing BIQA models in real-world scenarios. Obtaining stronger competing models for effective failure-spotting is a complex task. Building upon recent discoveries that difficult samples of deep models can be revealed through network pruning, we introduce a solution called "self-competitors." These self-competitors are random ensembles of pruned versions of the target model that needs improvement. By employing self-gMAD competition, diverse failures can be efficiently identified. We then fine-tune both the target model and its pruned variants using the human-rated gMAD set. This enables all models to learn from their respective failures, thereby preparing them for the subsequent round of self-gMAD competition. Through experimental results, we demonstrate that our method effectively troubleshoots BIQA models in real-world scenarios and improves their generalizability.