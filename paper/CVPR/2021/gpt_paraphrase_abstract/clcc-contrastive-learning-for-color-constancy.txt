This paper introduces CLCC, a new framework for color constancy based on contrastive learning. Contrastive learning has been successful in generating high-quality visual representations for image classification. However, designing illuminant invariant augmentations, which are necessary for image classification, conflicts with the goal of color constancy, which is to estimate the illuminant from a raw image. To address this issue, CLCC proposes the use of effective contrastive pairs generated through a novel raw-domain color augmentation technique, enabling the learning of better illuminant-dependent features. Experimental results on the NUS-8 dataset demonstrate a 17.5% relative improvement over a strong baseline, achieving state-of-the-art performance without increasing model complexity. Additionally, CLCC achieves competitive performance on the Gehler dataset with three times fewer parameters compared to top-ranking deep learning methods. Importantly, CLCC exhibits increased robustness to variations in illuminant proximity, reducing the worst-case error by 28.7% in data-sparse regions. The code for CLCC is publicly available at https://github.com/howardyclo/clcc-cvpr21.