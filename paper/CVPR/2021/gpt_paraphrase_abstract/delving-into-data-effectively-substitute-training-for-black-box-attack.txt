Deep models have been shown to be susceptible to adversarial samples. When it comes to black-box attacks, where the architecture and weights of the targeted model are unknown, training a substitute model for adversarial attacks has gained significant attention. However, previous approaches to substitute training have focused on stealing knowledge from the target model based on real or synthetic data, without considering how the choice of data can enhance transferability between the substitute and target models. In this paper, we propose a new approach to substitute training that centers around designing the distribution of data used in the knowledge stealing process. Our approach involves a diverse data generation module that synthesizes large-scale data with a wide distribution. Additionally, we introduce an adversarial substitute training strategy that focuses on data located near the decision boundary. By combining these two modules, we are able to significantly improve the consistency between the substitute model and the target model, thereby enhancing the effectiveness of adversarial attacks. Extensive experiments demonstrate the effectiveness of our method compared to state-of-the-art competitors in both non-target and target attack scenarios. Furthermore, we provide detailed visualization and analysis to elucidate the advantages of our approach.