Knowledge distillation involves transferring knowledge from a complex pretrained teacher model to a smaller student model, allowing the student model to replace the teacher model during deployment. Recent literature has introduced self-knowledge distillation, which trains a student network to distill its own knowledge without relying on a pretrained teacher network. This approach can be divided into a data augmentation based approach and an auxiliary network based approach. However, the data augmentation approach loses local information during the augmentation process, limiting its applicability to diverse vision tasks like semantic segmentation. Additionally, existing knowledge distillation approaches do not consider refined feature maps commonly used in object detection and semantic segmentation. This paper proposes a new self-knowledge distillation method called Feature Refinement via Self-Knowledge Distillation (FRSKD). FRSKD uses an auxiliary self-teacher network to transfer refined knowledge to the classifier network. It can utilize both soft label and feature-map distillations, making it suitable for classification and semantic segmentation tasks that require preserving local information. The effectiveness of FRSKD is demonstrated through performance improvements on various tasks and benchmark datasets. The code implementation of FRSKD is available at https://github.com/MingiJi/FRSKD.