This paper introduces a innovative approach to transferring knowledge from one embedding model to another, known as transfer embedding. The method leverages the pairwise similarities between samples in the source embedding space as a form of knowledge and transfers this knowledge using a loss function for learning the target embedding models. The authors propose a new loss function called relaxed contrastive loss, which uses the pairwise similarities as relaxed labels for the relationships between samples. This loss function goes beyond class equivalence, allowing more important pairs to have a greater impact on the training process and does not impose any restrictions on the target embedding spaces. Experimental results on metric learning benchmarks demonstrate that the proposed method significantly improves performance and effectively reduces the sizes and output dimensions of the target models. The authors also show that their method can enhance the quality of self-supervised representation and the performance of classification models. Comparisons with existing embedding transfer techniques consistently demonstrate the superiority of the proposed method.