Reducing inconsistencies in the behavior of different versions of an AI system is crucial. In image classification, these inconsistencies manifest as "negative flips," where a new model incorrectly predicts the output for a test sample that was correctly classified by the old model. Positive-congruent (PC) training aims to minimize both error rate and negative flips, maximizing congruency with the reference model only on positive predictions. We propose Focal Distillation, a straightforward approach for PC training that emphasizes congruence by assigning greater weights to samples that were correctly classified. Additionally, we discovered that negative flips can be further reduced without compromising the new model's accuracy if the reference model is an ensemble of multiple deep neural networks.