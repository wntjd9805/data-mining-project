We propose a novel style-agnostic model for sketch-based image retrieval (SBIR) that addresses the challenge of style diversity in sketches. Unlike existing models, our model uses a cross-modal variational autoencoder (VAE) to separate the semantic content shared between sketches and corresponding photos from the unique style of the sketcher. Additionally, we introduce two style-adaptive components to meta-train our model, allowing it to dynamically adapt to unseen user styles. Our model achieves state-of-the-art performance in both category-level and instance-level SBIR, making it truly style-agnostic.