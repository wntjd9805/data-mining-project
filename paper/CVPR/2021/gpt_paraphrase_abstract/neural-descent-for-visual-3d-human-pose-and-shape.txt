We introduce a deep neural network method for reconstructing the 3D pose and shape of individuals, including hand gestures and facial expressions, using RGB images as input. Our approach utilizes a comprehensive statistical 3D human model called GHUM, which has been trained end-to-end. We employ a self-supervised learning regime to learn and optimize the pose and shape state of GHUM. Our methodology, known as HUman Neural Descent (HUND), incorporates a learning to learn and optimize approach that avoids second-order differentiation during model parameter training and eliminates the need for expensive state gradient descent during test time. Instead, we employ novel recurrent stages to update the pose and shape parameters, effectively minimizing losses while also meta-regularizing the process to ensure continuous progress. The symmetrical nature of HUND between training and testing enables it to support various operating regimes, including self-supervised ones, making it the first 3D human sensing architecture to do so. Through extensive testing, we demonstrate that HUND achieves highly competitive results on datasets like H3.6M and 3DPW, as well as producing high-quality 3D reconstructions for complex imagery captured in real-world settings.