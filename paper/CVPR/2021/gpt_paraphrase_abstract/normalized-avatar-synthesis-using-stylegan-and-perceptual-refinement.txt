We present a robust framework using Generative Adversarial Networks (GANs) to convert a normalized 3D avatar of a person from a single unconstrained photo. Our method can handle various input conditions, such as smiling faces or extreme lighting, and reliably generate high-quality textured models of neutral facial expressions and skin textures under diffuse lighting. Existing 3D face reconstruction methods combine non-linear morphable face models with GAN-based decoders to capture likeness and details but struggle to produce neutral head models with unshaded albedo textures, which are crucial for creating relightable and animation-friendly avatars in virtual environments. The main challenge is the lack of training and ground truth data for normalized 3D faces. To overcome this, we propose a two-stage approach. Firstly, we use a robust normalized 3D face generator by incorporating a non-linear morphable face model into a StyleGAN2 network, allowing us to generate detailed yet normalized facial assets. Then, we perform a perceptual refinement step using the generated assets as regularization to overcome the limited training samples of normalized faces. We also introduce a Normalized FaceDataset, consisting of photogrammetry scans, selected photographs, and generated neutral expressions in diffuse lighting. Although our dataset has significantly fewer subjects compared to state-of-the-art GAN-based 3D facial reconstruction methods, we demonstrate that it is possible to produce high-quality normalized face models for challenging unconstrained input images and achieve superior performance compared to existing methods.