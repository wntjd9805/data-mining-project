We address the issue of visual search in resource-constrained environments. Current systems use the same model to compute representations for query and gallery images, leading to a trade-off between accuracy and efficiency. To overcome this trade-off, we propose generating gallery embeddings from a large model and query embeddings from a compact model. To ensure compatibility between the two models, we modify the parameters and architectures used for embedding computation. Our approach, known as compatibility-aware neural architecture search (CMP-NAS), is tested on fashion and face image retrieval tasks. Compared to traditional visual search using the largest embedding model, CMP-NAS achieves a significant reduction in cost while maintaining high accuracy. Specifically, on the DeepFashion2 dataset, CMP-NAS reduces costs by 80-fold with a negligible decrease in accuracy. On the IJB-C dataset, CMP-NAS achieves a 23-fold cost reduction with a slight decrease in accuracy.