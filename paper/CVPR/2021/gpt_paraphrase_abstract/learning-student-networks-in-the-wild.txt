Data-free learning for student networks is a novel approach to address user concerns regarding privacy when using original training data. Due to the complexity of modern convolutional neural networks (CNNs), alternative images or meta-data generated by the teacher network often lack quality. As a result, the student network fails to achieve comparable performance, especially on large-scale image datasets. In contrast to previous methods, our approach aims to fully leverage the extensive unlabeled data available in the wild. We analyze the disparities between the teacher and student network outputs on the original data and develop a data collection technique. To improve the performance of the student network, we propose a noisy knowledge distillation algorithm. We train an adaptation matrix with the student network to correct the label noise produced by the teacher network on the collected unlabeled images. We validate the effectiveness of our Data-Free Noisy Distillation (DFND) method on multiple benchmarks, demonstrating its superiority over state-of-the-art data-free distillation methods. Our experiments on various datasets confirm that the student networks trained using our method can achieve comparable performance to those using the original dataset. The code for our approach is available at https://github.com/huawei-noah/Data-Efficient-Model-Compression.