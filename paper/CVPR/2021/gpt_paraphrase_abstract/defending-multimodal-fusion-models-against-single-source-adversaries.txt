This paper explores the resilience of multimodal neural networks to adversarial perturbations on a single modality. The study reveals that standard multimodal fusion models are susceptible to attacks on individual modalities, which can override correct information from other unperturbed modalities and lead to model failure. This vulnerability is observed across various multimodal tasks, highlighting the need for a solution. To address this, the authors propose an adversarially robust fusion strategy that trains the model to compare information from all input sources, identify inconsistencies in perturbed modalities compared to others, and only allow information from unperturbed modalities to be utilized. This approach significantly improves single-source robustness, achieving performance gains in action recognition, object detection, and sentiment analysis without compromising performance on clean data.