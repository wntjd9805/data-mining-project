Shadow detection in still images has been widely studied, but there is a lack of research on detecting shadows in dynamic scenes. This is mainly due to the absence of a reliable dataset with accurate annotations for video shadow detection. To address this issue, we have created a new dataset called ViSha, which includes 120 videos comprising 11,685 frames. These videos cover various object categories, lengths, and lighting conditions. Each frame in the dataset has been meticulously annotated with a high-quality pixel-level shadow mask. To the best of our knowledge, ViSha is the first dataset specifically designed for learning-based video shadow detection.In addition to creating ViSha, we have also developed a novel model called TVSD-Net (triple-cooperative video shadow detection network). This model employs three parallel networks that work collaboratively to learn discriminative representations at both intra-video and inter-video levels. Within the network, we have introduced a dual gated co-attention module to effectively capture features from neighboring frames within the same video. Furthermore, we have incorporated an auxiliary similarity loss to extract semantic information between different videos.To evaluate the performance of our model, we have conducted an extensive analysis using ViSha. We have compared our model with 12 state-of-the-art approaches, including single image shadow detectors, video object segmentation, and saliency detection methods. The experimental results demonstrate that our TVSD-Net outperforms the existing models in terms of video shadow detection accuracy.Overall, our work addresses the lack of research on shadow detection in dynamic scenes by creating the ViSha dataset and proposing the TVSD-Net model. Both the dataset and model contribute to advancing the field of video shadow detection and provide a solid foundation for future research in this area.