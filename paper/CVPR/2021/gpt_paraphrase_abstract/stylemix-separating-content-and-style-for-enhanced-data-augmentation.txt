Despite the success of deep neural networks in classification tasks, they are prone to overfitting and adversarial attacks. To address these issues, mixup based augmentation methods have been explored. However, these methods fail to differentiate between content and style features of images, instead opting to mix or cut-and-paste images. This study introduces StyleMix and StyleCutMix, the first mixup methods that separately manipulate the content and style information of input image pairs. By effectively blending the content and style, more diverse and resilient samples can be created, ultimately improving the generalization of model training. Additionally, an automatic scheme is developed to determine the degree of style mixing based on the pair's class distance, preventing the creation of chaotic mixed images from significantly different styled pairs. Experimental results on CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate that StyleMix achieves comparable or superior performance to state-of-the-art mixup methods, while also training classifiers that are more robust against adversarial attacks.