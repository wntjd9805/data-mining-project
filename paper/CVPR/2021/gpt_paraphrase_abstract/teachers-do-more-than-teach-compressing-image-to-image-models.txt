Generative Adversarial Networks (GANs) have been successful in generating high-quality images, but they are inefficient due to their high computational cost and memory usage. Previous attempts to compress GANs have resulted in smaller generators at the expense of image quality or through time-consuming searching processes. In this study, we propose a solution to these issues by introducing a teacher network that not only performs knowledge distillation but also provides a search space for finding efficient network architectures. To improve the efficiency of generative models, we incorporate an inception-based residual block into the generators. Additionally, we propose a one-step pruning algorithm that searches for a student architecture from the teacher model, significantly reducing the search cost. This algorithm does not require â„“1 sparsity regularization or its associated hyper-parameters, simplifying the training process. Furthermore, we introduce Global Kernel Alignment (GKA) as an index for distilling knowledge between the teacher and student networks. By maximizing the feature similarity between the two networks, our compressed networks achieve similar or even better image fidelity (measured by FID and mIoU) compared to the original models, while reducing computational cost (measured by MACs). The code for our approach will be made available at https://github.com/snap-research/CAT.