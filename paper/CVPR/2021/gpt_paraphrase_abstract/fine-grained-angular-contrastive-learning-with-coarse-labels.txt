Few-shot learning methods aim to optimize pre-training techniques that facilitate the adaptation of models to new classes, which were not seen during the training phase, using a limited number of examples. The ability to adapt to unseen classes is particularly important in practical applications where the label space of the pre-trained model needs to be flexible to effectively support new categories. However, the existing few-shot literature has overlooked the scenario of Coarse-to-Fine Few-Shot (C2FS) learning, where the training classes are more general than the target classes. An example of C2FS is when the target classes are sub-classes of the training classes. This scenario presents a challenge as the standard supervised pre-training tends to overlook intra-class variability, which is crucial for distinguishing between sub-classes.  To address this issue, this paper proposes a novel module called "Angular normalization" that combines supervised and self-supervised contrastive pre-training to effectively tackle the C2FS task. The effectiveness of this approach is demonstrated through extensive experiments on multiple baselines and datasets, resulting in significant improvements. The authors hope that this work will inspire further research in the field of C2FS classification, which is a challenging and highly practical topic.