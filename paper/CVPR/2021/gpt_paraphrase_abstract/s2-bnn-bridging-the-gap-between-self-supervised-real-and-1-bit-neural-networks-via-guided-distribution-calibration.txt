Previous research primarily focuses on self-supervised learning in real-valued networks, which has yielded promising results. However, the exploration of this task in the more challenging binary neural networks (BNNs) is still limited. This paper addresses the difficulty of learning networks with binary weights and activations, without any human-labeled data. It is observed that the commonly used contrastive objective is not effective for achieving competitive accuracy in BNNs, as the backbone network has limited capacity and representation ability. Consequently, instead of directly applying existing self-supervised methods, which lead to a significant decrease in performance, a novel guided learning paradigm is proposed. This paradigm aims to distill binary networks from real-valued ones by minimizing the loss and improving accuracy in the final prediction distribution. The proposed method increases the accuracy of the simple contrastive learning baseline by 5.5-15% in BNNs. Additionally, it is discovered that BNNs struggle to recover similar predictive distributions as real-valued models when trained without labels, highlighting the importance of calibration to address performance degradation. Extensive experiments conducted on the ImageNet dataset and downstream datasets demonstrate substantial improvement over the simple contrastive learning baseline. The proposed method is even comparable to many mainstream supervised BNN methods. The code for the method is available at https://github.com/szq0214/S2-BNN.