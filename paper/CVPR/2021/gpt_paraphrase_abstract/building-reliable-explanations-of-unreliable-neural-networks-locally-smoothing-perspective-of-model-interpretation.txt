We introduce a new approach to explaining the predictions made by neural networks. Our goal is to provide explanations that are reliable and accurately identify the input features that are important for the model's output. Our method is based on the assumption that the loss function of the model prediction exhibits a smooth landscape, characterized by locally consistent loss and gradient profiles. Through theoretical analysis, we establish that these locally smooth explanations can be learned by training the model on a batch of noisy copies of the input, using L1 regularization to generate a saliency map. Extensive experiments confirm our analysis, showing that our proposed saliency maps successfully identify the original classes of adversarial examples against both naturally and adversarially trained models, surpassing the performance of previous methods. Additionally, we demonstrate that this method's ability to accurately identify relevant input features of the input and neighboring data points is the reason for its strong performance, satisfying the requirements of a reliable explanation.