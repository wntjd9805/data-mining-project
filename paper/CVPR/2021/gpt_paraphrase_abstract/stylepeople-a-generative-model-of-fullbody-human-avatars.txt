We introduce a novel kind of human avatars that integrate a parametric mesh-based body model with a neural texture. These avatars successfully address the challenges of modeling clothing and hair, which are usually problematic for mesh-based methods. Moreover, we demonstrate the capability of generating these avatars from multiple video frames through backpropagation. To facilitate the creation process, we present a generative model that can be trained on datasets comprising images and videos of individuals. This model enables us to generate random avatars and create dressed avatars of people based on a single or a few images. The project's code is accessible at saic-violet.github.io/style-people.