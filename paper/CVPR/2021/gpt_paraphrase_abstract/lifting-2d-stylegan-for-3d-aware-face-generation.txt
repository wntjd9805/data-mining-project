We present LiftedGAN, a framework designed to enhance a pre-trained StyleGAN2 for generating 3D-aware faces. Our model is capable of disentangling the latent space of StyleGAN2 into texture, shape, viewpoint, and lighting, while also generating 3D components for synthetic image rendering. Unlike previous methods, our approach is entirely self-supervised, eliminating the need for manual annotation or a 3DMM model during training. Instead, our model learns to generate images and their corresponding 3D components by leveraging the prior knowledge encoded in StyleGAN2 with the help of a differentiable renderer. This enables our model to output both the 3D shape and texture, granting explicit control over pose and lighting for the generated images. Through qualitative and quantitative evaluations, we demonstrate that our approach outperforms existing methods in terms of content controllability and the generation of realistic, high-quality images in the realm of 3D-controllable GANs.