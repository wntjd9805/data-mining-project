Recent research has revealed that convolutional neural network classifiers have a tendency to prioritize texture over shape cues. In this study, we introduce a distinction between global image statistics and local image cues, as well as shape cues. Our approach, known as Permuted Adaptive Instance Normalization (pAdaIN), aims to reduce the reliance on global statistics in the hidden layers of image classifiers. To achieve this, pAdaIN randomly rearranges the samples in a given batch using a permutation Ï€. Adaptive Instance Normalization (AdaIN) is then applied between the activations of each sample and the corresponding activations of the permuted sample, effectively swapping statistics between the samples. This distortion of global image statistics forces the network to focus on cues such as shape or texture. By adjusting the probability p of selecting a random permutation, the strength of this effect can be controlled. Our method consistently outperforms baselines in various scenarios, including image classification, robustness, and domain adaptation. In image classification, our approach demonstrates improvements on both CIFAR100 and ImageNet datasets using multiple architectures. Additionally, our method enhances performance on the ImageNet-C and CIFAR-100-C datasets for multiple architectures in terms of robustness. Furthermore, in the domains of domain adaptation and domain generalization, our method achieves state-of-the-art results on the transfer learning task from GTAV to Cityscapes and on the PACS benchmark.