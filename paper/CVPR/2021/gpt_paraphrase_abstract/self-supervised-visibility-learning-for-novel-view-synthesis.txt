We propose a novel approach to address the problem of creating new views from a limited number of source images. Existing methods for image-based rendering estimate scene geometry and synthesize new views separately, which can result in errors that negatively impact the quality of the synthesized views. To overcome this issue, we introduce an end-to-end framework for novel view synthesis that eliminates error propagation. Specifically, we construct a volume for the target view and use a source-view visibility estimation module to determine the visibility of each voxel in the target view from the source views. By aggregating the visibility information from all source views, we create a consensus volume where each voxel represents the probability of surface existence. We then employ a soft ray-casting mechanism to find the most front surface (depth) in the target view. This involves traversing the consensus volume along viewing rays and estimating a depth probability distribution. Based on the estimated source-view visibility and target-view depth, we warp and aggregate the pixels from the source views to synthesize a new view. To train our network, we adopt an end-to-end self-supervised approach, which significantly reduces error accumulation during view synthesis. Experimental results demonstrate that our method outperforms state-of-the-art techniques by generating novel views of higher quality.