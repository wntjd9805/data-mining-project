We introduce a new attention mechanism called Causal Attention (CATT) to address the issue of confounding effects in existing attention-based vision-language models. These effects lead to biased attention, causing the model to focus on irrelevant correlations in the training data and reducing its generalization ability. Since the confounder is generally unknown, we employ front-door adjustment to perform causal intervention without any prior knowledge of the confounder. CATT combines In-Sample Attention (IS-ATT) and Cross-Sample Attention (CS-ATT), where CS-ATT brings other samples into every IS-ATT to mimic causal intervention. CATT follows the Q-K-V convention and can replace various attention modules like top-down attention and self-attention in Transformers. CATT significantly improves popular attention-based vision-language models, particularly in large-scale pre-training. For instance, it enhances the lighter LXMERT model, which requires less data and computational power, to perform similarly to the heavier UNITER model. The code for CATT is available at https://github.com/yangxuntu/lxmertcatt.