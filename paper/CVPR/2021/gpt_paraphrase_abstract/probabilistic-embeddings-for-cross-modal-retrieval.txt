Cross-modal retrieval methods aim to create a shared representation space for samples from different modalities, such as vision and language. However, the task becomes more challenging when dealing with images and their captions due to the multiple possible correspondences between them. This paper argues that deterministic functions are not powerful enough to capture the one-to-many relationships in these correspondences. Instead, the authors propose a Probabilistic Cross-Modal Embedding (PCME) approach, where samples from different modalities are represented as probabilistic distributions in a common embedding space.To evaluate the proposed method, the authors suggest using the CUB dataset in addition to common benchmarks like COCO. Unlike COCO, the CUB dataset provides exhaustive annotations for all possible image-caption pairs, making it a valuable resource for retrieval evaluation. The authors extensively analyze and test PCME, demonstrating that it not only improves retrieval performance compared to deterministic methods but also provides uncertainty estimates that enhance the interpretability of the embeddings. The code for PCME is available at a specified GitHub repository.