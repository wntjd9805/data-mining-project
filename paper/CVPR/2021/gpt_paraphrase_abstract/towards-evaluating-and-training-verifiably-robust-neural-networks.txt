Recent research has demonstrated that interval bound propagation (IBP) can be utilized to train neural networks that are robust and capable of verification. However, it has been observed that the bounding method CROWN, which is based on tight linear relaxation, often provides loose bounds on these IBP trained networks. Additionally, a significant number of neurons become inactive during the IBP training process, potentially affecting the network's representation capability. This paper investigates the relationship between IBP and CROWN and establishes that CROWN consistently produces tighter bounds than IBP when appropriate bounding lines are selected. To achieve lower verified errors than IBP, a relaxed version of CROWN called linear bound propagation (LBP) is proposed. Furthermore, a new activation function called the parameterized ramp function (ParamRamp) is introduced, which offers greater diversity in neuron status compared to the commonly used ReLU function. Extensive experiments conducted on MNIST, CIFAR-10, and Tiny-ImageNet datasets using ParamRamp activation demonstrate state-of-the-art verified robustness. The code for this research is available at https://github.com/ZhaoyangLyu/VerifiablyRobustNN.