Current deep learning-based methods for image deraining have shown promising results for synthetic rainy images. However, these methods struggle to perform well on real rainy images due to the significant differences between simulated and real rain. In this study, we argue that rain generation and removal should be closely linked as they are two sides of the same problem. To address this, we propose a unified disentangled image translation framework that jointly learns the generation and removal of real rain. Our approach involves a bidirectional disentangled translation network with two loops for rain generation and removal, both for real and synthetic rain images. We enforce the disentanglement strategy by decomposing the rainy image into a clean background and a rain layer, which helps preserve the identity of the background through cycle-consistency and adversarial losses. Additionally, this strategy facilitates the translation of the rain layer between real and synthetic rainy images. A counterpart composition with an entanglement strategy is applied for rain generation. Extensive experiments on synthetic and real-world rain datasets demonstrate the superiority of our proposed method compared to state-of-the-art approaches.