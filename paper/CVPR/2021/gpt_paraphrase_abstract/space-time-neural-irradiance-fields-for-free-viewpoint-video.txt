We propose a technique for learning a spatiotemporal neural ir-radiance field from a single video, allowing for free-viewpoint rendering. This approach builds upon recent advancements in implicit representations. Learning such a field from a single video is challenging due to the limited observations available. The 3D geometry of a scene can be represented in various ways, as changes in geometry can be explained by changes in appearance and vice versa. To address this ambiguity, we constrain the time-varying geometry of our dynamic scene representation by utilizing scene depth estimated from video depth estimation methods. By aggregating information from individual frames into a global representation, we mitigate the limitations of a single observation. Extensive quantitative evaluations demonstrate the effectiveness of our method, producing impressive free-viewpoint rendering outcomes.