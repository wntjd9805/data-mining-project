This paper introduces a new approach to video captioning called Open-book Video Captioning. Rather than relying solely on the video itself, this method generates captions based on relevant sentences from a training corpus. To tackle this problem, a Retrieve-Copy-Generate network is proposed, consisting of a video-to-text retriever that effectively retrieves hints from the training corpus, and a copy-mechanism generator that dynamically extracts expressions from the retrieved sentences. These two modules can be trained together or separately, providing flexibility and extensibility. The framework combines retrieval-based methods with encoder-decoder methods, allowing for diverse expressions from the retrieved sentences while generating accurate video content. Extensive experiments on benchmark datasets demonstrate that this approach outperforms existing methods, highlighting the effectiveness and promise of the proposed paradigm in video captioning.