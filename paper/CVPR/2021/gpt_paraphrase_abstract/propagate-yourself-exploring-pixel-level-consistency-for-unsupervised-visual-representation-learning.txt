Unsupervised visual representation learning using contrastive learning methods has achieved impressive transfer performance. However, the full potential of contrastive learning has not been fully realized because current methods only focus on instance-level pretext tasks, resulting in sub-optimal representations for dense pixel prediction tasks. This paper introduces pixel-level pretext tasks for learning dense feature representations. The first task applies contrastive learning directly at the pixel level. Additionally, a pixel-to-propagation consistency task is proposed, which surpasses the state-of-the-art approaches by a significant margin. The results show that this approach outperforms previous methods in Pascal VOC object detection, COCO object detection, and Cityscapes semantic segmentation. The pixel-level pretext tasks are effective for pre-training both regular backbone networks and head networks used for dense downstream tasks. They also complement instance-level contrastive methods. These findings highlight the potential of defining pretext tasks at the pixel level and suggest a new direction for unsupervised visual representation learning. The code for this work is available at https://github.com/zdaxie/PixPro.