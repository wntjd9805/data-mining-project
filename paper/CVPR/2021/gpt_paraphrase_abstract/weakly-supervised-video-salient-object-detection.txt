We have developed a weakly supervised video salient object detection model that reduces the need for time-consuming and expensive pixel-wise labeled training datasets. Instead, we use "fixation guided scribble annotations" to train our model. Our approach includes an "Appearance-motion fusion module" and a bidirectionalConvLSTM based framework to effectively learn from multiple modalities and capture long-term temporal context using these weak annotations. Additionally, we introduce a novel foreground-background similarity loss to enhance labeling similarity across frames. To improve our model's performance, we employ a weak annotation boosting strategy with a new pseudo-label generation technique. Through extensive experiments on six benchmark video saliency detection datasets, we demonstrate the effectiveness of our solution.