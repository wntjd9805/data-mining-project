This paper presents a framework for creating personalized 3D talking faces from audio using video-based learning. The authors propose two data normalizations during training that improve data efficiency. The first normalization separates and represents faces in a normalized space, which allows for separate regressions on the 3D face shape and 2D texture atlas. The second normalization leverages facial symmetry and approximate albedo consistency to remove lighting variations. These normalizations enable simple networks to generate high-quality lip-sync videos under different lighting conditions, using just a single speaker-specific video for training. The authors also introduce an auto-regressive approach to stabilize temporal dynamics by conditioning the model on its previous visual state. The method outperforms contemporary benchmarks in terms of realism, lip-sync accuracy, and visual quality, as demonstrated by human ratings and objective metrics. The paper also showcases several applications made possible by this framework.