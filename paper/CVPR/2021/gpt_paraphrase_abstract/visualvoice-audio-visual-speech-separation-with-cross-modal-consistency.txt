We present a novel method for separating speech from background sounds and other speakers in videos using audio-visual cues. Unlike existing approaches that focus on aligning lip movements with generated sounds, our method incorporates the visual appearance of the speaker's face as a prior to isolate the corresponding vocal qualities. By simultaneously learning audio-visual speech separation and cross-modal speaker embeddings from unlabeled video data, our approach achieves state-of-the-art performance on five benchmark datasets for audio-visual speech separation and enhancement. It also demonstrates good generalization to real-world videos with diverse scenarios. For more details and access to our code and video results, please visit our project website at http://vision.cs.utexas.edu/projects/VisualVoice/.