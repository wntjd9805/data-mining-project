We introduce a new dynamic network slimming method called DS-Net that aims to improve hardware efficiency by dynamically adjusting the number of filters in a network at test time based on different inputs. Unlike previous methods, DS-Net stores the filters statically and contiguously in hardware, eliminating the extra burden of indexing, weight-copying, or zero-masking. The dynamic inference capability of DS-Net is achieved through a double-headed dynamic gate consisting of an attention head and a slimming head, which predictively adjusts the network width with minimal additional computation cost. To ensure fairness and generality, we propose a two-stage training scheme inspired by one-shot NAS. In the first stage, we introduce a novel training technique called In-place Ensemble Bootstrapping for weight-sharing networks to enhance supernet training effectiveness. In the second stage, we propose SandwichGate Sparsification to assist gate training by identifying easy and hard samples in real-time. Extensive experiments demonstrate that DS-Net consistently outperforms both static and dynamic model compression methods by a significant margin (up to 5.9%). Specifically, DS-Net achieves 2-4× computation reduction and 1.62× real-world acceleration compared to ResNet-50 and MobileNet on ImageNet with minimal accuracy drops.