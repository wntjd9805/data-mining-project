We propose a novel method for channel pruning in Convolutional Neural Networks (CNNs) that addresses the problem of loss-metric mismatch. In channel pruning, the goal is to obtain a sub-network with high accuracy. However, the accuracy of a sub-network does not necessarily correspond to low classification loss. To overcome this issue, we train a separate neural network to predict the performance of sub-networks and use this prediction as a proxy for accuracy during pruning. Training such a network efficiently is challenging, as it can suffer from catastrophic forgetting and imbalance in the distribution of sub-networks. To address these challenges, we introduce an episodic memory to update and collect sub-networks during the pruning process. Our experiments demonstrate that the gradients from the performance prediction network and the classification loss have different directions. The proposed method achieves state-of-the-art performance with ResNet, MobileNetV2, and ShuffleNetV2+ on ImageNet and CIFAR-10 datasets.