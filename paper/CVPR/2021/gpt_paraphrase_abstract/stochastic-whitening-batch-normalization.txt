Batch Normalization (BN) is a widely used technique in Deep Neural Networks (DNNs) for normalizing activations within mini-batches, leading to faster convergence and improved generalization. The Iterative Normalization (IterNorm) method, which applies Newton's method for whitening activations, has shown further enhancements in these aspects. However, since IterNorm independently initializes the whitening matrix at each training step, there is no sharing of information between consecutive steps.To address this limitation, we propose the Stochastic Whitening Batch Normalization (SWBN) algorithm. Instead of exact computation of the whitening matrix at each step, SWBN gradually estimates it during training in an online manner. Our experiments demonstrate that SWBN not only improves convergence rate and generalization of DNNs but also incurs lower computational overhead compared to IterNorm. The high efficiency of SWBN makes it suitable for integration into DNN architectures with a large number of layers.We conducted comprehensive experiments and comparisons between BN, IterNorm, and SWBN layers in both conventional (many-shot) image classification and few-shot classification tasks. The results highlight the effectiveness of our proposed technique.