Active Learning for discriminative models has traditionally focused on individual samples, neglecting the distribution of classes and the difficulty of dealing with certain classes. This approach has proven to be detrimental. To address this issue, we propose a method that incorporates class imbalance into the Active Learning framework using Bayes' rule. We argue that three factors should be considered when estimating the probability of a classifier making a mistake for a given sample: the probability of mislabeling a class, the likelihood of the data given a predicted class, and the prior probability on the abundance of a predicted class. Implementing these factors requires a generative model and an intractable likelihood estimation. Therefore, we train a Variational Auto Encoder (VAE) for this purpose. To enhance VAE training and establish a connection with the classifier, we utilize the deep feature representations of the classifier as input to the VAE. By considering all three probabilities, particularly the data imbalance, our method significantly improves the effectiveness of existing methods when limited data is available. We demonstrate the applicability of our method on various classification tasks and datasets, including a real-world dataset with significant data imbalance, where it outperforms the state-of-the-art approaches. Figure 1 illustrates an example where existing methods fail to identify important samples for inclusion in the updated training set. We compare the importance metrics provided by different methods, including DBAL, CoreSet, Max. Entropy, and our method, for selected images in a dataset dominated by plane images. Existing methods heavily favor the plane class due to data imbalance, even when the results are correct. Additionally, these methods fail to recognize the importance of rare classes, resulting in suboptimal performance. In contrast, our method accurately identifies the importance of these samples by considering class difficulty and prior probabilities.