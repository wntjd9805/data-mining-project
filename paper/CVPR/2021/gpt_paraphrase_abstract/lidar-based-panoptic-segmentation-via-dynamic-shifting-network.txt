The development of autonomous driving technology has highlighted the need for a comprehensive 3D perception system. However, existing research focuses on parsing either objects or scenes from LiDAR sensor data. This study introduces the concept of LiDAR-based panoptic segmentation, which aims to parse both objects and scenes simultaneously. The Dynamic Shifting Network (DS-Net) is proposed as an effective framework for panoptic segmentation in the point cloud domain. DS-Net has three key features: a strong backbone design using cylinder convolution for LiDAR point clouds, a dynamic shifting module to handle complex point distributions, and a consensus-driven fusion approach to resolve discrepancies between semantic and instance predictions. To evaluate the performance of LiDAR-based panoptic segmentation, benchmarks are constructed using two large-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes. Extensive experiments demonstrate that DS-Net achieves superior accuracies compared to existing methods, achieving 1st place on the SemanticKITTI public leaderboard.