This paper presents a new approach to the problem of temporal sentence grounding (TSG), which involves identifying the temporal boundary of a specific segment in a video based on a sentence query. Previous methods either compare predefined candidate segments with the query or directly regress the boundary timestamps. In contrast, the proposed framework, called Context-aware Biaffine Localizing Network (CBLN), scores all pairs of start and end indices simultaneously using a biaffine mechanism. CBLN incorporates both local and global contexts to improve localization accuracy. Local contexts from adjacent frames help differentiate visually similar appearances, while global contexts from the entire video aid in reasoning about temporal relationships. Additionally, a multi-modal self-attention module is developed to enhance the query-guided video representation for the biaffine strategy. Experimental results on three public datasets (ActivityNet Captions, TACoS, and Charades-STA) demonstrate that CBLN outperforms existing methods, highlighting the effectiveness of the proposed localization framework. The code for CBLN is available at https://github.com/liudaizong/CBLN.