We present a new approach called multi-level out-of-distribution detection (MOOD) for efficiently detecting anomalous inputs. Traditional methods rely on the final layer outputs and require a full feedforward pass for each input. MOOD, on the other hand, utilizes intermediate classifier outputs to dynamically and efficiently infer out-of-distribution data. We establish a direct relationship between data complexity and optimal exit level, showing that easy out-of-distribution examples can be detected early without propagating to deeper layers. At each exit, we distinguish out-of-distribution examples using our proposed adjusted energy score, which is suitable for networks with multiple classifiers. We evaluate MOOD on 10 out-of-distribution datasets of varying complexities and find that it achieves up to 71.05% computational reduction in inference while maintaining competitive detection performance.