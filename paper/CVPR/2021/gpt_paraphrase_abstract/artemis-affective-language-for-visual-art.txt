We introduce a new extensive dataset and accompanying machine learning models that aim to explore the relationship between visual content, emotional impact, and language explanations. Unlike most existing computer vision annotation datasets, our focus is on the affective experience evoked by visual artworks. We ask annotators to indicate the dominant emotion felt when viewing an image and provide a grounded verbal explanation for their choice. This approach yields a diverse range of signals that capture both the objective content and emotional impact of an image, including associations with abstract concepts, visual similes and metaphors, and subjective references to personal experiences. Our dataset, ArtEmis, comprises 455K emotion attributions and explanations from humans, covering 80K artworks from WikiArt. Using this dataset, we develop captioning systems that can express and explain emotions elicited by visual stimuli. Impressively, the captions generated by these systems effectively reflect the semantic and abstract content of the images, surpassing systems trained on existing datasets. The dataset and methods are accessible at https://artemisdataset.org.