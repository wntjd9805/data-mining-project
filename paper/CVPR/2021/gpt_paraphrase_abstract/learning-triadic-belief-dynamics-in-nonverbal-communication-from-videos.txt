Humans have a unique ability to understand and interpret social cues, such as nonverbal communication. However, existing research on scene understanding often overlooks these important social characteristics. In this study, we aim to incorporate various nonverbal communication cues, including gaze, human poses, and gestures, to develop a model that can represent, learn, and infer the mental states of agents solely from visual inputs. Our model takes into account the beliefs of each agent, allowing it to accurately represent the true world state and infer the differing beliefs of each agent. By combining these beliefs and true world states, our model creates a "five minds" representation during agent interactions, which differs from previous approaches that rely on infinite recursion to infer beliefs. We propose a hierarchical energy-based model that can track and predict all five minds simultaneously. From this new perspective, social events are interpreted through a combination of nonverbal communication and belief dynamics, going beyond traditional keyframe video summaries. Experimental results demonstrate that our social account approach provides superior video summaries for videos with rich social interactions compared to state-of-the-art keyframe video summary methods.