Unsupervised video object segmentation (UVOS) is the process of automatically segmenting main objects in videos without any human intervention. The main challenge in UVOS is identifying the primary objects as there is limited prior knowledge about them. Previous methods have relied on optical flow to capture motion cues in videos and considered moving objects as primary ones. However, using only flow information is not sufficient to distinguish primary objects from background objects that move together. This is because combining noisy motion features with appearance features leads to incorrect localization of primary objects. To tackle this issue, we propose a new reciprocal transformation network that correlates three key factors: intra-frame contrast, motion cues, and temporal coherence of recurring objects. Each factor represents a type of primary object, and our reciprocal mechanism allows for effective removal of ambiguous distractions from videos. Additionally, our transformation module enhances motion features by reciprocally transforming appearance features, thereby excluding information from the moving background objects. This enables us to focus on moving objects with prominent appearances while removing co-moving outliers. Experimental results on public benchmarks demonstrate that our model outperforms state-of-the-art methods significantly. The code for our model is available at https://github.com/OliverRensu/RTNet.