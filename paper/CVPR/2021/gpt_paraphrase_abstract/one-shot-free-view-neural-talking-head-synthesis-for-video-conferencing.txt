We present a new model for synthesizing talking-head videos and its application in video conferencing. Our model is capable of generating a realistic talking-head video by using a source image of the person's appearance and a driving video that determines the motion. We encode the motion using a unique keypoint representation, which separates the identity-specific and motion-related information without supervision. Through extensive experiments, we demonstrate that our model outperforms other methods on standard datasets. Additionally, our compact keypoint representation allows for a video conferencing system that achieves the same visual quality as the H.264 standard while utilizing only a fraction of the bandwidth. Furthermore, our keypoint representation enables the user to rotate the head during synthesis, enhancing the simulation of face-to-face video conferencing experiences.