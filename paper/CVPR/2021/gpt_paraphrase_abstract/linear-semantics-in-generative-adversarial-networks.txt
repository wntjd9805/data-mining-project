This study focuses on understanding the semantic representation of Generative Adversarial Networks (GANs) in order to enable semantic control in the image generation process. The researchers discover that a well-trained GAN encodes image semantics in a simple manner, where a linear transformation of feature maps is sufficient to extract the generated image semantics. To validate this finding, extensive experiments are conducted on various GANs and datasets. Due to the simplicity of the encoding process, a semantic segmentation model can be learned for a trained GAN using only a small number of labeled images (e.g., 8). Building on these findings, two few-shot image editing techniques are proposed: Semantic-Conditional Sampling and Semantic Image Editing. By providing a trained GAN and as few as eight semantic annotations, users can generate diverse images based on a given semantic layout and have control over the synthesized image semantics. The code for these approaches has been made publicly available.