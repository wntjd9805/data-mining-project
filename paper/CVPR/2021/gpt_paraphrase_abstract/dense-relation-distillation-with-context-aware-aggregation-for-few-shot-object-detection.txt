Conventional deep learning methods for object detection require a large amount of expensive bounding box annotations for training. Few-shot object detection, which adapts to new classes with only a few annotated examples, is challenging because fine-grained features of novel objects can be easily overlooked with limited data. To address this, we propose DCNet, a method that aims to fully exploit annotated novel object features and capture fine-grained features of query objects. DCNet consists of a Dense Relation Distillation module and a Context-aware Aggregation module. The Dense Relation Distillation module matches support and query features in a feed-forward manner, covering all spatial locations to handle challenges like appearance changes and occlusions. The Context-aware Aggregation module adaptively combines features from different scales for a more comprehensive feature representation. Experimental results on PASCAL VOC and MS COCO datasets demonstrate that our approach achieves state-of-the-art performance. The code for our method is available at https://github.com/hzhupku/DCNet.