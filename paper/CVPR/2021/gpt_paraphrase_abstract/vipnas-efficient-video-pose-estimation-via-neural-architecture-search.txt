We propose a novel neural architecture search (NAS) method called ViP-NAS to improve the accuracy and efficiency of human pose estimation. Existing methods focus on accuracy but ignore real-time efficiency. Our approach searches for networks in both spatial and temporal levels to find the best trade-off. In the spatial level, we consider network depth, width, kernel size, group number, and attentions. In the temporal level, we search for optimal temporal feature fusions to enhance accuracy and speed across multiple video frames. Our method is the first to search for temporal feature fusion and automatic computation allocation in videos. Experimental results on COCO2017 and PoseTrack2018 datasets demonstrate the effectiveness of our approach. Our discovered model family, S-ViPNAS and T-ViPNAS, achieve significantly higher inference speed (CPU real-time) without sacrificing accuracy compared to previous state-of-the-art methods.