Extensive research has shown that pre-trained VGG networks are effective in capturing the visual style of an image for neural style transfer. However, the quality of stylization often degrades when applied to features from more advanced and lightweight networks like ResNet. Through experiments with different network architectures, it is found that the main architectural difference between VGG and ResNet - residual connections - produces feature maps with low entropy, which are not suitable for style transfer. To address this, a simple solution is proposed: applying a softmax transformation to the feature activations to enhance their entropy. Experimental results demonstrate that this approach significantly improves the quality of stylization results, even with networks that have random weights. This highlights the importance of the feature extraction architecture rather than the use of learned weights in style transfer tasks.