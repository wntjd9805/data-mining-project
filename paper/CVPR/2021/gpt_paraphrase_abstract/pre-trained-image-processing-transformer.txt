As the computing power of modern hardware continues to increase, pre-trained deep learning models such as BERT and GPT-3 have proven to be more effective than traditional methods. This progress is largely due to the representation ability of transformer architectures. In this study, we focus on low-level computer vision tasks such as denoising, super-resolution, and deraining, and introduce a new pre-trained model called image processing transformer (IPT). To fully utilize the capabilities of the transformer, we use the ImageNet benchmark to generate a large number of corrupted image pairs for training the IPT model with multiple heads and tails. Additionally, we incorporate contrastive learning to adapt to different image processing tasks. The pre-trained IPT model can be efficiently fine-tuned for specific tasks. With just one pre-trained model, IPT outperforms current state-of-the-art methods on various low-level benchmarks. The code for this model is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT.