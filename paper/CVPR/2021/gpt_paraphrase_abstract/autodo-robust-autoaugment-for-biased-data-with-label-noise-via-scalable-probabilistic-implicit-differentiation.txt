AutoAugment has generated interest in automated augmentation methods for deep learning models. These methods aim to improve the generalization of trained models to test data by estimating image transformation policies for the training dataset. However, recent papers have focused on reducing the complexity of policy search, but they are not robust when applied to biased and noisy data. In order to address these limitations, we propose a generalized automated dataset optimization (AutoDO) approach that minimizes the distribution shift between the test data and distorted training dataset. Our AutoDO model explicitly estimates a set of per-point hyperparameters to flexibly alter the distribution of the training data, including hyperparameters for augmentation, loss weights, and soft-labels. These hyperparameters are jointly estimated using implicit differentiation. We provide a theoretical probabilistic interpretation of this framework using Fisher information and demonstrate that its complexity scales linearly with the dataset size. Experimental results on SVHN, CIFAR-10/100, and ImageNet classification datasets show improvements of up to 9.3% for biased datasets with label noise compared to prior methods, and notably, gains of up to 36.6% for underrepresented SVHN classes.