Neural rendering techniques that combine machine learning with geometric reasoning have become a promising approach for generating new views of a scene using a limited number of images. One notable technique is Neural Radiance Fields (NeRF), which uses a deep network to map spatial coordinates and viewing directions to volume density and emitted radiance. However, NeRF can only be applied to static scenes where the same location can be observed from different images. In this study, we propose D-NeRF, a method that extends neural radiance fields to handle dynamic scenes. D-NeRF enables the reconstruction and rendering of new images of objects undergoing both rigid and non-rigid motions, using a single camera moving around the scene. To achieve this, we introduce time as an additional input to the system and divide the learning process into two stages. The first stage encodes the scene into a canonical space, while the second stage maps this canonical representation to the deformed scene at a specific time. Both mappings are learned simultaneously using fully-connected networks. Once the networks are trained, D-NeRF can generate novel images by controlling both the camera view and the time variable, thereby controlling the movement of the object. We demonstrate the effectiveness of our approach on scenes with objects undergoing rigid, articulated, and non-rigid motions. The code, model weights, and the dynamic scenes dataset will be made available.