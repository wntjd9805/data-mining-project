This paper proposes a new approach for image-to-image translation using an energy-based model (EBM) in the latent space of a pretrained autoencoder. The model, called LETIT1, assumes that two visual domains share the same latent space, which consists of a content code and a domain-specific style code. Unlike existing methods, LETIT1 does not explicitly extract and combine these codes but instead learns to transport the source style code to the target style code while preserving the content code. This simplifies the translation process and makes it more efficient in the one-sided unpaired image translation setting. The paper demonstrates through qualitative and quantitative comparisons that LETIT1 achieves superior translation quality and content preservation. Additionally, it claims to be the first model applicable to 1024Ã—1024-resolution unpaired image translation. The code for LETIT1 is available at https://github.com/YangNaruto/latent-energy-transport.