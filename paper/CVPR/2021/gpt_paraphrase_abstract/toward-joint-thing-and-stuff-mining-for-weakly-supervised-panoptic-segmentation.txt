This study introduces a novel framework called Jointly Thing-and-Stuff Mining (JTSM) for weakly supervised panoptic segmentation (WSPS). The goal of panoptic segmentation is to divide an image into object instances and semantic content for thing and stuff categories. Previous research has not explored the learning of WSPS using only image-level labels. The proposed JTSM framework utilizes a new technique called mask of interest pooling (MoIPool) to extract fixed-size pixel-accurate feature maps for arbitrary-shaped segmentations. This enables a panoptic mining branch to employ multiple instance learning (MIL) for recognizing things and stuff segmentation in a unified manner. The framework also refines segmentation masks through self-training using parallel instance and semantic segmentation branches. This collaboration combines the masks obtained from panoptic mining with bottom-up object evidence to improve spatial coherence and contour localization. Experimental results demonstrate the effectiveness of JTSM on PASCAL VOC and MS COCO datasets. Additionally, this work achieves competitive results for weakly supervised object detection and instance segmentation. This study represents an initial step towards addressing the challenge of panoptic segmentation with only image-level labels.