This study focuses on improving the performance of low-resolution models in instance-level detection tasks, such as object detection. While reducing input resolution can improve efficiency, it typically has a negative impact on detection performance. To address this issue, the researchers propose using knowledge distillation to transfer knowledge from a high- or multi-resolution model to a low-resolution student model. They introduce a method of spatially aligning feature maps between models of different input resolutions and implement aligned multi-scale training to train a multi-scale teacher model. They also propose feature-level fusion to dynamically combine the teacher's multi-resolution features to guide the student model. The low-resolution models trained using this approach perform competitively with high-resolution models trained using conventional multi-scale training, surpassing the performance of low-resolution models by 2.1% to 3.6% in terms of mAP (mean average precision) on various instance-level detection tasks and datasets. The code for this approach is publicly available at https://github.com/Jia-Research-Lab/MSAD.