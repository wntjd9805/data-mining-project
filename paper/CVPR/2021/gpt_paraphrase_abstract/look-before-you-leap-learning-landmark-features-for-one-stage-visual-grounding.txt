We propose an LBYL (‘Look Before You Leap’) Network for one-stage visual grounding. The LBYL-Net follows a language description to localize an object based on its spatial relation to landmarks. The network includes a landmark feature convolution module that encodes the spatial relations between the object and its context. We combine the contextual information with the object's visual features to ground it. To extract the landmark feature, we use a dynamic programming algorithm with low complexity. The LBYL-Net takes full consideration of contextual information and outperforms existing methods on ReferitGame, RefCOCO, and RefCOCO+ datasets. The code is available at https://github.com/svip-lab/LBYLNet.