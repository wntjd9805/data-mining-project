Current color-guided depth super-resolution (DSR) methods rely on paired RGB-D data for training, where the RGB image is used to recover the degraded depth map. However, collecting paired data can be challenging and expensive in real-world testing environments. In this study, we propose a novel approach to learn cross-modality knowledge during training using both RGB and depth modalities, but test on a target dataset that only contains single depth modality. Our main concept is to transfer the knowledge of scene structural guidance from the RGB modality to the single DSR task without modifying the network architecture. To achieve this, we create an auxiliary depth estimation (DE) task that takes an RGB image as input and estimates a depth map. We train both the DSR and DE tasks collaboratively to enhance the performance of DSR. A cross-task interaction module is introduced to facilitate bilateral cross-task knowledge transfer. Firstly, we devise a cross-task distillation scheme that promotes mutual learning between the DSR and DE networks in a teacher-student role-exchanging manner. Additionally, we introduce a structure prediction (SP) task that provides supplementary structure regularization to enable the DSR and DE networks to learn more informative structure representations for depth recovery. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance compared to other DSR methods.