Model quantization is a promising method for compressing deep neural networks and speeding up inference, allowing for deployment on mobile and edge devices. However, most existing quantization methods rely on access to training data to fine-tune the quantized model, which is not always possible in real situations due to privacy and security concerns. A small number of quantization methods attempt to achieve zero-shot model quantization without accessing training data, either through post-training quantization or batch normalization statistics-guided data generation. However, these methods suffer from low performance and efficiency. To address these issues, we propose a zero-shot adversarial quantization (ZAQ) framework that estimates discrepancies and transfers knowledge from a full-precision model to its quantized counterpart. Our framework includes a two-level discrepancy modeling approach that guides a generator to synthesize diverse and informative data examples for optimizing the quantized model through adversarial learning. We conducted extensive experiments on three vision tasks, demonstrating the superiority of ZAQ over strong zero-shot baselines and validating the effectiveness of its main components. The code for our framework is available at https://git.io/Jqc0y.