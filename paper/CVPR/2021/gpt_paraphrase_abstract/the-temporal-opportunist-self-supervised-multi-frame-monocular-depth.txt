Self-supervised monocular depth estimation networks are trained to predict the depth of a scene based on nearby frames during training. However, most of these networks do not utilize sequence information available at test time, which could improve the accuracy of depth prediction. Existing methods that do make use of sequence information either rely on computationally expensive techniques or off-the-shelf recurrent networks that indirectly utilize geometric information.To address this limitation, we propose ManyDepth, an adaptive approach to dense depth estimation that can leverage sequence information at test time. Inspired by multi-view stereo, we introduce a deep end-to-end cost volume based approach that is trained using self-supervision only. We also introduce a novel consistency loss that encourages the network to disregard the cost volume in unreliable situations, such as when there are moving objects, and an augmentation scheme to handle static cameras.Through extensive experiments on the KITTI and Cityscapes datasets, we demonstrate that ManyDepth outperforms all existing self-supervised baselines, including those that utilize single or multiple frames at test time. Our approach effectively utilizes sequence information to improve depth estimation accuracy, making it a promising solution for various applications.