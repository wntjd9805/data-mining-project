This study investigates the effectiveness of estimating 3D object poses using only RGB images and 2D object annotations, particularly in the absence of depth images. The researchers propose a two-step pose estimation framework that utilizes weakly-supervised segmentation and a dual-scale pose estimation network called DSC-PoseNet. In the first step, the framework learns to segment objects from real and synthetic data, providing a prior for pose estimation. In the second step, DSC-PoseNet predicts object poses by comparing segmentation masks and rendered visible object masks, considering both the original image scale and a resized fixed scale. This approach eliminates large scale variations and focuses on rotation estimation, improving pose estimation accuracy. Additionally, initial pose estimation is used to generate pseudo ground-truth for self-supervised training of DSC-PoseNet. Experimental results on popular benchmarks show that the proposed method significantly outperforms state-of-the-art models trained on synthetic data and is comparable to several fully-supervised methods.