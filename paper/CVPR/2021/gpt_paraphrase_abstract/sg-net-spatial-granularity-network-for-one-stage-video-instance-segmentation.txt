Video instance segmentation (VIS) is a critical task in computer vision. Most existing VIS methods extend Mask R-CNN by adding a tracking branch, but there is still room for improvement. In contrast, we propose a one-stage spatial granularity network (SG-Net) that approaches the VIS task from a new perspective. SG-Net has several advantages over conventional two-stage methods: 1) It has a compact architecture where each task head (detection, segmentation, and tracking) is crafted independently, allowing for effective feature sharing and joint optimization. 2) The mask prediction in SG-Net is dynamically performed on sub-regions of each detected instance, resulting in high-quality masks with fine granularity. 3) Our method avoids using expensive proposal-based RoI features for each task prediction, leading to reduced runtime complexity per instance. 4) SG-Net incorporates a tracking head that models objects' center movements, enhancing tracking robustness to different object appearances. We evaluate our approach on the YouTube-VIS dataset and achieve state-of-the-art results. Extensive experiments demonstrate that our compact one-stage method improves both accuracy and inference speed. We believe that SG-Net can serve as a strong and flexible baseline for the VIS task. Our code will be made available.