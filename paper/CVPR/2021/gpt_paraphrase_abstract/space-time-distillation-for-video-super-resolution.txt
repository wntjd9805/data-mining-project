We propose a method to enhance the performance of compact video super-resolution (VSR) networks without altering their original architectures. Our approach involves transferring knowledge from a complex VSR network to a compact one through a process called knowledge distillation. Specifically, we introduce a space-time distillation scheme that leverages both spatial and temporal knowledge in the VSR task. Spatial distillation involves extracting spatial attention maps from both networks to transfer spatial modeling capabilities. Time distillation reduces the performance gap between compact and complex models by distilling the feature similarity of temporal memory cells encoded from the sequence of feature maps using ConvLSTM. The proposed method can be easily incorporated into any network during the training process without modifying the original architecture. Experimental results on standard benchmarks demonstrate that our method significantly improves the performance of existing VSR networks in resource-constrained situations without increasing inference time.