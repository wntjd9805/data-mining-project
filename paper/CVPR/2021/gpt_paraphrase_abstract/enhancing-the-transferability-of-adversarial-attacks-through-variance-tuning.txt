Adversarial attacks can deceive deep neural networks by introducing imperceptible changes to input data. While these attacks have been successful in the white-box setting, their effectiveness is limited in the black-box setting, especially against models with defense mechanisms. This study proposes a new approach called variance tuning to enhance iterative gradient-based attacks and improve their transferability. Instead of using the current gradient directly for momentum accumulation, the method considers the gradient variance of the previous iteration to stabilize the update direction and avoid poor local optima. Experimental results on the ImageNet dataset show that variance tuning significantly improves the transferability of gradient-based adversarial attacks. Furthermore, the method can be combined with input transformations and used to attack ensemble models, achieving an average success rate of 90.1% against nine advanced defense methods, which is an 85.1% improvement over the current best attack performance. The code for this method is available at https://github.com/JHL-HUST/VT.