We introduce STaR, a new technique for tracking and reconstructing dynamic scenes with rigid motion in multi-view RGB videos without the need for manual annotation. Previous research has shown that neural networks are effective at compressing multiple views of a scene into a learned function that maps viewing rays to observed radiance values through volume rendering. However, these methods lose their predictive power when objects in the scene move. In this study, we explicitly model the rigid motion of objects using neural representations of radiance fields. We demonstrate that without any additional supervision, we can reconstruct a dynamic scene with a single moving rigid object by decomposing it into two parts and encoding each with its own neural representation. This is achieved by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses that align the two fields in each frame. Our method is capable of rendering photorealistic novel views in both synthetic and real-world datasets, with novelty measured in both spatial and temporal dimensions. Additionally, our factored representation allows for animation of previously unseen object motion.