One of the most difficult types of questions in visual question answering (VQA) is when the answer requires knowledge that is not present in the image. In this study, we examine open-domain knowledge, where the necessary knowledge to answer a question is not provided or annotated during training or testing. We utilize two types of knowledge representations and reasoning. The first is implicit knowledge, which can be effectively learned from unsupervised language pretraining and supervised training data using transformer-based models. The second is explicit, symbolic knowledge encoded in knowledge bases. Our approach combines both types of knowledge, leveraging the powerful implicit reasoning capabilities of transformer models for answer prediction and incorporating symbolic representations from a knowledge graph without losing their explicit semantics. We incorporate diverse sources of knowledge to address the wide range of knowledge required to solve questions based on knowledge. Our approach, called KRISP (Knowledge Reasoning with Implicit and Symbolic Representations), significantly outperforms the current state-of-the-art on OK-VQA, the largest available dataset for open-domain knowledge-based VQA. Through extensive analysis, we demonstrate that while our model effectively utilizes implicit knowledge reasoning, the symbolic answer module that explicitly connects the knowledge graph to the answer vocabulary is crucial for the performance of our method and its ability to generalize to uncommon answers. The answer format produced by our model only provides an abstraction of the answer.