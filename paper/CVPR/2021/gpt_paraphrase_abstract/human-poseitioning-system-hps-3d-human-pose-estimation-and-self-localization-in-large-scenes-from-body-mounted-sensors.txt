We present the Human POSEitioning System (HPS), a method that utilizes wearable sensors to recover the complete 3D pose of a human, aligned with a 3D scan of the surrounding environment. By incorporating inertial measurement units (IMUs) attached to the body limbs and a head-mounted camera facing outwards, HPS combines camera-based self-localization with IMU-based human body tracking. While the former offers accurate but noisy position and orientation estimations, the latter is precise in the short-term but prone to drift over extended periods. Our optimization-based integration leverages the advantages of both methods, resulting in a drift-free pose accuracy. Additionally, we incorporate 3D scene constraints into our optimization, such as foot contact with the ground, enabling physically plausible motion. HPS complements traditional third-person-based 3D pose estimation techniques, allowing for the capture of larger recording volumes and longer periods of motion. It has potential applications in virtual reality/augmented reality, where humans interact with the scene without requiring a direct line of sight with an external camera, or for training agents to navigate and interact with the environment based on first-person visual input, similar to real humans. To demonstrate the effectiveness of HPS, we recorded a dataset of humans interacting with large 3D scenes (ranging from 300 to 1000 m2), involving 7 subjects and over 3 hours of diverse motion. The dataset, code, and video can be accessed on the project page: http://virtualhumans.mpi-inf.mpg.de/hps/.