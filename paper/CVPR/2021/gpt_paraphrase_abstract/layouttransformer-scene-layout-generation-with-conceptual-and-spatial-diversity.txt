In existing methods for translating text inputs into layouts or images, explicit descriptions of each object in a scene, including their spatial information and associated relationships, are typically required. However, these methods do not effectively exploit the text input to infer implicit objects or relationships during layout generation. To address this limitation, we propose a LayoutTransformer Network (LT-Net) that can better utilize the text input. Our LT-Net encodes the semantic features of a scene-graph input, allowing for the exploitation of co-occurrences and implicit relationships. This enables the generation of conceptually diverse and plausible layout outputs. Furthermore, the decoder of our LT-Net translates the encoded contextual features into bounding boxes while preserving self-supervised relation consistency. By fitting the distributions of these bounding boxes to Gaussian mixture models, LT-Net can additionally produce spatially-diverse layouts. We evaluate our LT-Net extensively on the MS-COCO and Visual Genome datasets and demonstrate its effectiveness and plausibility compared to recent layout generation models. The codes for our LT-Net will be released at "LayoutTransformer".