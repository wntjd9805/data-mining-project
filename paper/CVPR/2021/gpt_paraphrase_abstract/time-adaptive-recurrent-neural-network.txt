We present a learning approach that adjusts the time constants of a continuous-time version of a vanilla RNN. These time constants are modified based on the current observation and hidden state. This method addresses the problems of RNN trainability by mitigating the issues of exploding and vanishing gradients. It achieves this by introducing new constraints on the parameter space and reducing noise in inputs by emphasizing informative inputs to enhance their impact on the hidden state. Consequently, our approach is computationally efficient, surpassing the limitations of existing methods aimed at improving RNN training. Despite being simpler and having a smaller memory footprint, our RNNs perform competitively compared to standard LSTMs and baseline RNN models on various benchmark datasets, including those that require long-term memory.