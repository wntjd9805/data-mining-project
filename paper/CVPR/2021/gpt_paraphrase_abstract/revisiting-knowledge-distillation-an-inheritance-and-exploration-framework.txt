Knowledge Distillation (KD) is a widely used technique for transferring knowledge from a teacher model to a student model. This is typically done by making the student model mimic the class distributions or intermediate feature representations of the teacher model. However, this approach limits the student model from learning new and undiscovered knowledge or features. In this paper, we propose a new framework called inheritance and exploration knowledge distillation (IE-KD) that addresses this limitation.In IE-KD, the student model is divided into two parts: inheritance and exploration. The inheritance part is trained to transfer the existing knowledge from the teacher model to the student model using a similarity loss. On the other hand, the exploration part is encouraged to learn representations that are different from the inherited ones using a dissimilarity loss. This framework can be easily combined with other distillation or mutual learning methods for training deep neural networks.Extensive experiments show that the inheritance and exploration parts of IE-KD work together to help the student model learn more diverse and effective representations. Our IE-KD framework can be a general technique to improve the performance of student networks and achieve state-of-the-art results. Furthermore, when applied to the training of two networks, IE-KD improves the performance of both networks compared to deep mutual learning.In summary, the proposed IE-KD framework enhances knowledge distillation by allowing the student model to inherit knowledge from the teacher model while also exploring new representations. This approach improves the diversity and effectiveness of the student model's learning, making it a valuable technique for improving the performance of deep neural networks.