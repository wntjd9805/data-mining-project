The problem of establishing dense correspondences across images depicting different instances under the same category is a challenging task in computer vision. This is due to the large variations within the same class and the lack of ground truth data. One popular approach is to learn correspondences from synthetic data, but this has limitations in handling real image pairs. To address this, we propose a novel approach called Probabilistic Model Distillation (PMD), which transfers knowledge learned from a probabilistic teacher model on synthetic data to a static student model using unlabeled real image pairs. We use a Probabilistic Supervision Reweighting (PSR) module and a Confidence-Aware Loss (CAL) to mine useful knowledge and reduce the impact of errors. Our experimental results on various benchmarks demonstrate that PMD achieves state-of-the-art performance. We also extend PMD to incorporate stronger supervision by training the probabilistic teacher with stronger key-point supervision, and again observe its superiority. The extensive experiments show that PMD can infer more reliable supervision signals and mitigate the influence of errors in pseudo labels. The code for PMD is available at https://github.com/fanyang587/PMD.