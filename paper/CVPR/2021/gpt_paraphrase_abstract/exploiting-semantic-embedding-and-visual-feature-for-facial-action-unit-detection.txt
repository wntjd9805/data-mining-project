Recent research on detecting facial action units (AU) has incorporated additional information such as facial landmarks, AU relationships, and web facial images to enhance AU detection accuracy. However, the utilization of semantic information for AUs has not been explored in this context. Semantic descriptions of AUs offer more comprehensive information than binary AU labels. In this study, we propose a novel approach called Semantic Embedding and Visual feature Network (SEV-Net) for AU detection. Our method involves obtaining AU semantic embeddings using Intra-AU and Inter-AU attention modules. The Intra-AU attention module captures the relationship between words within each sentence that describes a specific AU, while the Inter-AU attention module focuses on the relationship among these sentences. The learned AU semantic embeddings are then used to generate attention maps through a cross-modality attention network. These attention maps serve as weights for the aggregated features. Notably, our approach is the first to exploit semantic features in this manner. We evaluated our method on three public AU-coded facial expression databases, and it outperformed existing state-of-the-art methods.