Deep convolutional neural networks (CNNs) have greatly improved face recognition. These models are now being used for identity authentication in security-sensitive applications. However, they are vulnerable to adversarial patches, which are physical and stealthy, raising concerns about their real-world security. In this paper, we evaluate the robustness of face recognition models against adversarial patches using transferability, where the attacker has limited access to the target models. We extend existing transfer-based attack techniques to generate transferable adversarial patches, but we find that transferability is sensitive to initialization and decreases with larger perturbations, indicating overfitting to substitute models. To address this, we propose regularizing the adversarial patches on a low-dimensional data manifold. The manifold is represented by generative models trained on legitimate human face images. By using face-like features as adversarial perturbations and optimizing them on the manifold, we show that the gaps between substitute models and target models decrease significantly, resulting in better transferability. We conduct extensive digital world experiments to demonstrate the superiority of our proposed method in the black-box setting. We also apply our method in the physical world.