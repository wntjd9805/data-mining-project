The limited research on domain-robust visual question answering (VQA) methods has been inspired by the observation that computer vision methods tend to overfit to specific datasets. Unlike domain adaptation for object recognition, domain adaptation for VQA is more complex due to the handling of multimodal inputs, the presence of multiple steps with diverse modules, and the significant differences in answer spaces across datasets. To address these challenges, this study first quantifies the domain shifts between popular VQA datasets in both visual and textual spaces. Synthetic shifts are also created separately in the image and question domains to disentangle the shifts arising from different modalities. Second, the robustness of different families of VQA methods (classic two-stream, transformer, and neuro-symbolic methods) to these shifts is tested. Third, the applicability of existing domain adaptation methods is examined, and a new method is devised to bridge the domain gaps in VQA, specifically tailored to VQA models. To mimic real-world generalization, the focus is on unsupervised domain adaptation and the formulation of the open-ended classification task. The answer format only outputs the abstraction of the answer.