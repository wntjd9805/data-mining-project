Deep neural networks have proven to be effective in making predictions, but accurately assessing the reliability of these predictions remains a challenge. Deep Ensembles are considered one of the best methods for generating uncertainty estimates, but they are costly to train and evaluate. MC-Dropout is an alternative that is less expensive but also less reliable. Our main idea is that there is a range of ensemble-like models, with MC-Dropout and Deep Ensembles being at the extremes. MC-Dropout uses an infinite number of highly correlated models, while Deep Ensembles rely on a finite number of independent models. To combine the advantages of both approaches, we propose Masksembles. Instead of randomly dropping parts of the network like in MC-Dropout, Masksembles use a fixed number of binary masks. These masks are parameterized in a way that allows for controlling the correlations between individual models. By adjusting the overlap and size of the masks, we can choose the optimal configuration for the task. This results in a simple and easy-to-implement method that performs similarly to Ensembles but at a lower cost. We validate the effectiveness of Masksembles through experiments on two widely used datasets, CIFAR10 and ImageNet.