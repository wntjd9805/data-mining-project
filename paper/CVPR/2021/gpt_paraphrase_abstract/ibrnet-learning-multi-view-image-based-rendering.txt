We propose a method to generate new perspectives of complex scenes by interpolating a limited set of nearby views. Our approach consists of a network architecture that combines a multilayer perceptron and a ray transformer to estimate radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions). This allows us to incorporate appearance information from multiple source views in real-time rendering, reminiscent of traditional image-based rendering techniques. Unlike existing neural scene representation methods that optimize per-scene functions for rendering, we train a generic view interpolation function that can be applied to novel scenes. We employ differentiable classic volume rendering for training, using multi-view posed images as supervision. Experimental results demonstrate that our approach outperforms recent methods for novel view synthesis and, when fine-tuned for specific scenes, is on par with state-of-the-art single-scene neural rendering methods.