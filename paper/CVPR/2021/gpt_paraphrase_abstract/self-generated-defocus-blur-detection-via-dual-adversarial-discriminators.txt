This paper presents a method for training a deep defocus blur detection (DBD) model without the need for pixel-level annotation. The traditional approach of training deep models for DBD requires time-consuming and error-prone manual annotation. To address this issue, the authors propose a novel approach where a defocus blur region or focused clear area can be pasted into a realistic full blurred image or full clear image without affecting the judgment of the image. The authors train a generator using an adversarial approach against dual discriminators. The generator learns to produce a DBD mask that copies the focused area and unfocused region from a source image to another full clear image and full blurred image, creating a composite clear image and composite blurred image. By doing so, the discriminators are unable to distinguish the generated images from real full clear images and full blurred images, effectively achieving a self-generated DBD without the need for explicit annotation. Additionally, the authors propose a bilateral triplet-excavating constraint to prevent the degenerate problem caused by one discriminator defeating the other. The proposed approach is evaluated on two widely-used DBD datasets and demonstrates superior performance. Source codes for the approach are available at the provided GitHub link.