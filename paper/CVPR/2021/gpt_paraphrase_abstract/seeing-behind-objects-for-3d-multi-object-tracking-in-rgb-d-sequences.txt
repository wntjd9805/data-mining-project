Tracking objects in RGB-D video sequences is a difficult task due to factors such as changing viewpoints, motion, and occlusions. To overcome these challenges, we propose a method that combines object tracking and inferring the complete geometry of objects that are rigidly moving over time. Our key insight is that by inferring the complete geometry of objects, we can improve tracking performance. We achieve this by hallucinating unseen regions of objects, which allows us to establish additional correspondences between the same object instances. This approach ensures robust tracking even when there are significant changes in appearance. To implement our method, we first detect objects in each frame of the RGB-D sequence and learn to predict their complete object geometry as well as a dense correspondence mapping into a canonical space. This information enables us to derive the 6DoF poses for the objects in each frame and establish correspondences between frames, ensuring robust object tracking across the sequence. We conducted experiments using both synthetic and real-world RGB-D data, and our method achieved state-of-the-art performance in dynamic object tracking. Additionally, we demonstrate that our object completion technique significantly improves tracking, resulting in a mean MOTA improvement of 6.5%.