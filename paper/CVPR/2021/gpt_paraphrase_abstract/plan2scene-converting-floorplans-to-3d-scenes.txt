We introduce a method called Plan2Scene that aims to convert a floorplan and its associated photos into a textured 3D mesh model. Our system accomplishes this by: 1) converting the floorplan image into a 3D mesh model, 2) generating surface textures based on the input photos, and 3) inferring textures for surfaces that are not observed using a graph neural network architecture. To evaluate our system, we create indoor surface texture datasets and enhance an existing dataset with rectified surface crops and additional annotations. Our approach effectively deals with the challenge of generating tileable textures for dominant surfaces in the residence, such as floors, walls, and ceilings, even from a limited number of unaligned photos. Through qualitative and quantitative evaluations, we demonstrate that our system produces realistic 3D interior models, surpassing baseline methods in terms of texture quality metrics and as measured by a comprehensive user study.