Self-supervised learning has emerged as a promising approach to enhance the video representation capability of deep neural networks by utilizing supervision from the data itself. However, current methods often rely heavily on the background of the videos rather than focusing on the motion, which makes the models susceptible to changes in the background. To address this issue, we propose a solution called Background Erasing (BE), which aims to reduce the impact of the background by introducing additional background frames. In this method, a static frame is randomly selected and added to all other frames in the video, creating a distracting video sample. The model is then trained to bring the features extracted from the distracting video closer to the features extracted from the original video, thereby explicitly encouraging the resistance to background influence and emphasizing the changes in motion. It is important to note that our method is straightforward to implement and can be easily integrated into most state-of-the-art approaches with minimal effort. Experimental results demonstrate significant improvements using BE, with enhancements of 16.4% and 19.1% achieved on the heavily biased datasets UCF101 and HMDB51 when combined with MoCo, and a 14.5% improvement on the less biased dataset Diving48.