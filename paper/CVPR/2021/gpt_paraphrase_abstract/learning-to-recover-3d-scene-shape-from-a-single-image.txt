Despite recent advancements in estimating depth from a single image, current state-of-the-art methods fail to accurately reconstruct the 3D shape of a scene due to unknown depth shifts caused by shift-invariant reconstruction losses used in mixed-data depth prediction training, as well as potential unknown camera focal length. In this study, we thoroughly investigate this issue and propose a two-stage framework to address it. Firstly, we predict the depth up to an unknown scale and shift based on a single monocular image. Then, using 3D point cloud encoders, we predict the missing depth shift and focal length, allowing us to recover a more realistic 3D scene shape. Moreover, we introduce an image-level normalized regression loss and a normal-based geometry loss to improve the performance of depth prediction models trained on mixed datasets. To evaluate our approach, we test our depth model on nine unseen datasets and achieve state-of-the-art results in terms of zero-shot dataset generalization. The code for our method is available at the following link: https://git.io/Depth.