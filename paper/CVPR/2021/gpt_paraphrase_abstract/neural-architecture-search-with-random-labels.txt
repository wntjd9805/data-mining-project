This paper explores a new approach to neural architecture search (NAS) called searching with random labels (RLNAS). While most NAS algorithms rely on accurate labels for performance evaluation, RLNAS proposes a novel framework based on the ease-of-convergence hypothesis, using only random labels during the search process. The algorithm involves training a SuperNet with random labels and extracting the sub-network with the most significant weight changes. Extensive experiments on various datasets and search spaces demonstrate that RLNAS achieves comparable or even better results compared to state-of-the-art NAS methods that use ground truth labels. This finding challenges existing understandings of NAS and may inspire new insights into its essential aspects.