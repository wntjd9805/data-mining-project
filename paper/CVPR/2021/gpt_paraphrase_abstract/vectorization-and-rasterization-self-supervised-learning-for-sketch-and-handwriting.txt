Self-supervised learning has become popular for its ability to learn powerful representations from unlabeled data, leading to excellent performance on various challenging tasks. However, designing supervision-free pre-text tasks is difficult, especially since they are often specific to a particular modality. While there are self-supervised methods available for spatial or temporal data, there is a lack of a common pre-text task that benefits both modalities. This paper focuses on introducing a self-supervised pre-text task for sketches and handwriting data, which exist in both rasterized image and vector coordinate sequence forms. The paper proposes two innovative cross-modal translation pre-text tasks, namely Vectorization and Rasterization, that leverage the dual representation of the data. Vectorization learns to map image space to vector coordinates, while Rasterization maps vector coordinates to image space. The study demonstrates that the encoder modules trained using these pre-text tasks improve the analysis of hand-drawn data in both raster-based and vector-based downstream approaches. Empirical evidence indicates that the proposed pre-text tasks outperform existing single and multi-modal self-supervision methods.