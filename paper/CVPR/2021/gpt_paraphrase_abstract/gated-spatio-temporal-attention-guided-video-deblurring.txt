Video deblurring is a challenging task due to the complexity of varying blur. Existing methods rely on alignment for temporal information fusion, which is computationally expensive or leads to suboptimal results. In this study, we investigate two factors influencing deblurring quality: how to fuse spatio-temporal information and where to collect it. We propose a factorized gated spatio-temporal attention module that performs non-local operations across space and time to fully utilize available information without alignment. We aggregate spatial and temporal information and distribute global spatio-temporal information to each pixel. Our method outperforms existing techniques in terms of performance and efficiency. Additionally, we introduce a reinforcement learning-based framework for selecting keyframes with complementary and useful information. Our adaptive approach allows for flexible frame usage during inference. Extensive experiments on multiple datasets confirm the superiority of our method.