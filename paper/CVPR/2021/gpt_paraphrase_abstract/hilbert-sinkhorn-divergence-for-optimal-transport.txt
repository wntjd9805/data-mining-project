The Sinkhorn divergence is a popular metric used to compare probability distributions in optimal transport. However, most studies only apply the Sinkhorn divergence in Euclidean space, limiting its usefulness in analyzing complex data with nonlinear structures. Therefore, there is a need to enhance the Sinkhorn divergence's ability to capture nonlinear structures. To address this, we propose a theoretical and computational framework called the "Hilbert Sinkhorn divergence" (HSD), which extends the Sinkhorn divergence from Euclidean space to the reproducing kernel Hilbert space. Using kernel matrices, we derive a closed-form expression for the HSD, which is proven to be a tractable convex optimization problem. We also demonstrate several appealing statistical properties of the HSD, including strong consistency, asymptotic behavior, and sample complexity. Empirical experiments show that our method achieves state-of-the-art performance in image classification and topological data analysis.