Temporal action detection on unconstrained videos has made significant advancements in recent years, largely due to the success of deep learning techniques. However, the process of collecting large-scale temporal detection datasets is time-consuming and impractical. In light of this, we propose an improved model for temporal action localization that can effectively utilize limited labeled data. Our approach involves the design of two auxiliary tasks that reconstruct the available label information and aid in the learning process of the temporal action detection model. These tasks generate supervision signals by recycling the original annotations and are jointly trained with the temporal action detection model using a multi-task learning approach. It is worth noting that our proposed approach can be easily integrated into any region proposal based temporal action detection models. We have conducted extensive experiments on three benchmark datasets - THUMOS'14, Charades, and ActivityNet - and the results confirm the effectiveness of our model.