Recently, there has been a rise in the use of deep multimodal learning for cross-modal retrieval. However, collecting large-scale well-annotated data for unimodal data is costly and time-consuming. Additionally, dealing with multiple modalities adds further challenges. While crowd-sourcing annotation can help reduce labeling costs, it leads to noisy labels from non-expert annotators. To address this challenge, this paper introduces a Multi-modal Robust Learning (MRL) framework. The framework aims to learn with multimodal noisy labels, mitigating the impact of noisy samples and addressing the discrepancy between modalities. Specifically, the paper proposes a Robust Clustering loss (RC) that directs deep networks to focus on clean samples instead of noisy ones. Additionally, a simple yet effective multimodal loss function called Multimodal Contrastive loss (MC) is introduced to maximize the mutual information between different modalities, reducing the interference from noisy samples and cross-modal discrepancies. The proposed approach is evaluated through extensive experiments on four widely-used multimodal datasets. The results demonstrate the effectiveness of the MRL framework by comparing it to 14 state-of-the-art methods.