This study presents a new threat to robust machine learning models that emphasizes the importance of high-quality training data for achieving strong adversarial robustness. The authors propose a novel data poisoning attack that specifically targets certifiably robust classifiers. Unlike previous poisoning attacks that only reduce accuracy on a small set of target points, this attack reduces the average certified radius (ACR) of an entire target class in the dataset. The attack remains effective even when models are trained from scratch using state-of-the-art robust training methods. To make detection harder, the attack uses clean-label poisoning points with imperceptible distortions. The authors evaluate the effectiveness of the attack by poisoning the MNIST and CIFAR10 datasets and training deep neural networks using robust training methods. The ACR of the target class can be reduced by over 30% when models are trained on the poisoned data. Additionally, the poisoned data is transferable across different training methods and model architectures.