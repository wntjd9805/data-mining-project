We aim to solve the problem of uncertainty calibration, as deep neural networks often produce predictions that are not properly calibrated. While post-hoc calibration methods have been developed to address this issue, they have mainly focused on calibrating predictions within the same domain. Our contribution is twofold. Firstly, we discover that existing post-hoc calibration methods tend to generate overly confident predictions when faced with domain shift. Secondly, we propose a simple solution where we introduce perturbations to samples in the validation set prior to performing the post-hoc calibration process. Through extensive experiments, we demonstrate that this perturbation step significantly enhances calibration under domain shift across various architectures and modeling tasks.