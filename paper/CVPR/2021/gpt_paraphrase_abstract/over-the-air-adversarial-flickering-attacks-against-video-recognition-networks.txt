Deep neural networks used for video classification, similar to image classification networks, can be manipulated through adversarial attacks. While image classifiers solely rely on the visual information of an image, video classifiers also utilize the temporal information within a video. This study introduces a manipulation technique that fools video classifiers by introducing a flickering temporal perturbation, which can go unnoticed by human observers and can be implemented in the real world. By demonstrating the manipulation of action classification in individual videos, the authors extend the procedure to create a universal adversarial perturbation that achieves a high fooling ratio. Moreover, they develop a temporal-invariant perturbation that can be applied to videos without synchronizing it to the input. The attack is tested on multiple target models, and the transferability of the attack is demonstrated. These properties enable the bridging of the gap between simulated environments and real-world applications, as shown for the first time in an over-the-air flickering attack. The authors provide diagrams illustrating the attack in both digital and real-world settings, showcasing the addition of a uniform RGB perturbation to the attacked video in the digital domain and the transmission of the perturbation using a smart RGB LED bulb in the physical domain.