We present a novel learning framework that can generate detailed and spatially coherent reconstructions of multiple individuals from a single image. Existing methods for multi-person reconstruction have limitations, such as being model-based and unable to capture accurate 3D models of people with loose clothing and hair, or requiring manual intervention to handle occlusions or interactions. Our approach overcomes these limitations by introducing an end-to-end learning method that performs model-free implicit reconstruction for realistic 3D capture of multiple clothed individuals in various poses, even with occlusions, from a single image. Our network simultaneously estimates the 3D geometry of each person and their spatial locations to create a coherent multi-human reconstruction. Moreover, we have created a synthetic dataset that includes images with different numbers of inter-occluded humans and various clothing and hair styles. Our method produces robust and high-resolution reconstructions for images containing multiple humans with complex occlusions, loose clothing, and a wide range of poses and scenes. Our evaluation, using both synthetic and real-world datasets, demonstrates state-of-the-art performance, with significant improvements in the accuracy and completeness of the reconstructions compared to other approaches.