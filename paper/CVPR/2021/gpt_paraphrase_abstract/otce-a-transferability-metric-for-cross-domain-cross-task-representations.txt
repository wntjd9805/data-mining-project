Transfer learning across different domains and tasks is a complex problem that requires finding the optimal transfer strategy. While neural network-based feature transfer is commonly used, determining the best approach still involves time-consuming experiments and domain knowledge. To address this, we propose a transfer-ability metric called Optimal Transport based Conditional Entropy (OTCE) that predicts transfer performance in cross-domain and cross-task feature transfer scenarios. Our OTCE score considers both domain difference and task difference and evaluates them using optimal transport and the conditional entropy of the target task. We conducted experiments on the DomainNet and Office31 datasets, which demonstrated that OTCE significantly outperforms state-of-the-art methods, showing an average gain of 21% in correlation with ground truth transfer accuracy. Additionally, we explored two applications of the OTCE score: source model selection and multi-source feature fusion.