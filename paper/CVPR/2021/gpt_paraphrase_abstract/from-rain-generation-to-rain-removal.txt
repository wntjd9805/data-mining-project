Deep learning-based methods for single image rain removal (SIRR) are heavily influenced by the design of deraining models and the training datasets used. Most current state-of-the-art approaches focus on constructing powerful deep models to achieve better deraining results. In this paper, we propose a novel approach to improve the deraining performance by considering the training datasets. Specifically, we introduce a more efficient method for synthesizing rainy images. We develop a full Bayesian generative model for rainy images, where the rain layer is represented as a generator that takes latent variables (such as direction, scale, and thickness) representing the physical structural rain factors as input. To solve this model, we employ the variational inference framework to approximate the statistical distribution of rainy images in a data-driven manner. By learning this generator, we are able to automatically generate diverse and non-repetitive training pairs, which effectively enrich and augment existing benchmark datasets. To evaluate the realism of the generated rainy images, we conduct a user study that includes qualitative and quantitative assessments. Comprehensive experiments demonstrate that our proposed model can accurately capture the complex rain distribution, leading to significant improvements in deraining performance compared to current deep single image derainers. Moreover, our approach reduces the need for large pre-collected training samples for the SIRR task. The code for our model is available at https://github.com/hongwang01/VRGNet.