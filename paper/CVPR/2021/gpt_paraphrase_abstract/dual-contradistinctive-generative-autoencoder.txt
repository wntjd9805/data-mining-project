We introduce a novel generative autoencoder model, called dual contradistinctive generative autoencoder (DC-VAE), that combines two distinct losses to enhance the performance of generative autoencoders in both inference and synthesis tasks. The model incorporates an instance-level discriminative loss, which ensures fidelity at the instance level during reconstruction and synthesis, and a set-level adversarial loss, which promotes fidelity at the set level. These two contradistinctive losses work together seamlessly in the DC-VAE, resulting in significant improvements in qualitative and quantitative performance compared to baseline VAE models, without requiring any architectural changes. Our extensive experiments with DC-VAE demonstrate its effectiveness across various resolutions, ranging from 32×32 to 512×512. The results obtained with DC-VAE show state-of-the-art or competitive performance in image reconstruction, image synthesis, image interpolation, and representation learning tasks. Moreover, DC-VAE is a versatile VAE model that can be applied to a wide range of computer vision and machine learning tasks, making it suitable for various downstream applications.