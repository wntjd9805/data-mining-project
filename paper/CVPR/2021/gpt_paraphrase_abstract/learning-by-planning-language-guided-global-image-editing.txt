We present a method for language-guided image editing that addresses the limitations of previous approaches. Our approach uses a text-to-operation model to translate high-level editing language requests into a sequence of interpretable and differentiable editing operations. Unlike previous methods, our approach is not limited to domain-specific or low-resolution data, and it offers improved interpretability. We propose an operation planning algorithm to generate possible editing sequences using the target image as pseudo ground truth. We evaluate our method on two datasets and demonstrate its advantages over existing approaches. Our code is publicly available.