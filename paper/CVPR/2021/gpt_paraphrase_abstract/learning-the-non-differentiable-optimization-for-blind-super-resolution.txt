Blind super-resolution (SR) methods based on convolutional neural networks (CNN) typically use iterative optimization to approximate the ground-truth step-by-step, resulting in high computational costs and time-consuming inference. Most blind SR algorithms focus on obtaining high-fidelity results using L1 loss as the loss function. To enhance the visual quality of SR results, a perceptual metric like NIQE is necessary to guide network optimization. However, due to the non-differentiable nature of NIQE, it cannot be used as the loss function. To address these challenges, we propose an adaptive modulation network (AMNet) for blind SR with multiple degradations. AMNet includes an adaptive modulation layer (AMLayer) that efficiently fuses blur kernel and image features. We integrate a trainable blur kernel predictor into the blind SR model, eliminating the need for an iterative strategy. Furthermore, we incorporate deep reinforcement learning (AMNet-RL) to handle the non-differentiable optimization problem. In AMNet-RL, the blur kernel predictor acts as the actor to estimate the blur kernel from the input low-resolution image, and the reward is defined by a pre-defined differentiable or non-differentiable metric. Extensive experiments demonstrate that our model outperforms state-of-the-art methods in terms of fidelity and perceptual metrics.