Neural networks are being used more frequently in real-world applications, but there is a need to address issues like distributional shift and sequential task learning without forgetting. Network expansion methods have shown promise in solving these problems by adding model capacity for learning new tasks while avoiding catastrophic forgetting. However, these methods often become computationally expensive at larger scales due to the increase in additional parameters. To overcome this issue, we propose a simple and task-specific feature map transformation strategy called Efficient Feature Transformations (EFTs). EFTs allow for flexible learning of new tasks with minimal added parameters. Additionally, we suggest a feature distance maximization strategy that significantly improves task prediction in class incremental settings without requiring expensive generative models. To validate the effectiveness and efficiency of our approach, we conduct extensive experiments on discriminative and generative tasks using datasets like CIFAR-100, ImageNet-1K, LSUN, CUB-200, and Cats. Our results show that even with low single-digit parameter growth rates, EFTs outperform other continual learning methods in various settings.