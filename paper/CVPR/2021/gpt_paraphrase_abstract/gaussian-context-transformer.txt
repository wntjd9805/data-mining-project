Recently, many channel attention blocks have been proposed to enhance the representational power of deep convolutional neural networks (CNNs). These blocks typically learn the relationship between global contexts and attention activations through fully-connected layers or linear transformations. However, our empirical findings indicate that despite introducing numerous parameters, these attention blocks may not effectively learn this relationship. In this study, we propose a simple yet highly efficient channel attention block called Gaussian Context Transformer (GCT), based on the hypothesis that the relationship is predetermined. GCT achieves contextual feature excitation by utilizing a Gaussian function that satisfies the presupposed relationship. We develop two versions of GCT based on whether the standard deviation of the Gaussian function is learnable: GCT-B0 and GCT-B1. GCT-B0 is a parameter-free channel attention block that fixes the standard deviation and directly maps global contexts to attention activations without learning. On the other hand, GCT-B1 is a parameterized version that adaptively learns the standard deviation to enhance the mapping ability. Extensive experiments conducted on ImageNet and MS COCO benchmarks demonstrate that our GCTs consistently improve the performance of various deep CNNs and detectors. Our GCTs outperform other state-of-the-art channel attention blocks, such as SE and ECA, in terms of effectiveness and efficiency.