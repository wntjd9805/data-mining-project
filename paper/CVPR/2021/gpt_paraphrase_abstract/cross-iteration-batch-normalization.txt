Batch Normalization is a widely known technique that suffers from reduced effectiveness when applied to small mini-batch sizes. This occurs because the statistics used for normalization cannot be accurately estimated from the mini-batch during training iterations. To overcome this issue, we propose a solution called Cross-Iteration Batch Normalization (CBN), which leverages examples from multiple recent iterations to improve the estimation quality. However, computing statistics across iterations becomes challenging due to changes in network weights, making the network activations incomparable. To address this, we introduce a compensation technique based on Taylor polynomials that adjusts for the weight changes, allowing for accurate estimation of statistics and effective application of batch normalization. We evaluate CBN on object detection and image classification tasks with small mini-batch sizes and find that it outperforms the original batch normalization method and a direct calculation of statistics over previous iterations without the proposed compensation technique. The code for implementing CBN is available at https://aka.ms/cbn.