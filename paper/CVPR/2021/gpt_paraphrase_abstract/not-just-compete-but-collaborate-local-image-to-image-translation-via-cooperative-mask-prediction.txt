Facial attribute editing involves manipulating an image to change a specific attribute while maintaining the rest of the image details. Generative adversarial networks (GANs) with encoder-decoder architectures have been used for this purpose due to their ability to create realistic images. However, existing methods for unpaired datasets struggle to preserve attribute-irrelevant regions accurately without a ground truth image. This study introduces a new loss function called CAM-consistency loss, which enhances the consistency of an input image in image translation.While the cycle-consistency loss ensures that an image can be translated back, our approach goes further by preserving attribute-irrelevant regions even in a single translation to another domain. We achieve this by utilizing the Grad-CAM output computed from the discriminator. Our CAM-consistency loss directly optimizes this Grad-CAM output during training, allowing the generator to identify which local regions to change while leaving the other regions unchanged. This collaborative approach between the generator and discriminator improves the overall quality of image translation.We conducted experiments to validate the effectiveness and versatility of our proposed CAM-consistency loss. We applied it to various facial image editing models, including StarGAN, AttGAN, and STGAN. The results demonstrate the success and applicability of our approach.