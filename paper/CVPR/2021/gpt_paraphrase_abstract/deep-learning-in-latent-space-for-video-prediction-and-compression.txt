Learning-based video compression has made significant advancements in recent years, particularly through the use of deep neural networks (DNNs) to reduce spatial and temporal redundancies in video frames. In this study, we propose a novel framework that utilizes DNNs to predict and compress video sequences in a lower-dimensional latent vector space. Our approach involves learning an efficient representation of each video frame in the latent space and then performing inter-frame prediction within that domain. To achieve compression in the latent domain, we train a deep autoencoder using a generative adversarial network (GAN). Additionally, we leverage the temporal correlation within the video frame sequence by employing a convolutional long short-term memory (ConvLSTM) network to predict the latent vector representation of future frames. We demonstrate the effectiveness of our method in two applications: video compression and abnormal event detection, both of which utilize the same latent frame prediction network. Our proposed method outperforms or achieves comparable performance to state-of-the-art algorithms specifically designed for either video compression or anomaly detection.