Reconstructing a 3D hand from a single-view RGB image is difficult due to the various hand configurations and depth ambiguity. Most current methods rely on 3D annotations during training, but obtaining these annotations is costly. To address this issue, we propose S2HAND, a self-supervised 3D hand reconstruction network. S2HAND can estimate pose, shape, texture, and camera viewpoint without the need for labeled training data. We use 2D detected keypoints to obtain geometric cues from the input image. To train an accurate hand reconstruction model using these cues, we leverage the consistency between 2D and 3D representations and introduce novel losses to improve the network's output. This is the first time a reliable 3D hand reconstruction network has been trained without manual annotations. Our experiments demonstrate that our self-supervised method achieves comparable performance to fully-supervised approaches. The code for S2HAND is publicly available at https://github.com/TerenceCYJ/S2HAND.