Recent research has shown that using volumetric scene representations combined with differentiable volume rendering can achieve photo-realistic rendering for challenging scenes that cannot be reconstructed using meshes. However, current methods integrate geometry and appearance in a "black-box" volume that cannot be edited. In this study, we propose a new approach that explicitly separates geometry, represented as a continuous 3D volume, from appearance, represented as a continuous 2D texture map. To achieve this, we introduce a 3D-to-2D texture mapping network into volumetric representations. We constrain this network using a 2D-to-3D inverse mapping network and a cycle consistency loss, ensuring that 3D surface points map to 2D texture points that can be mapped back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and produces high-quality rendering results. Importantly, by separating geometry and texture, users can easily edit the appearance by editing the 2D texture maps.