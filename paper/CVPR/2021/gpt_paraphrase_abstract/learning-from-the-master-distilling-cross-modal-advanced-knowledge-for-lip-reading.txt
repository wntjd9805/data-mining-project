This paper introduces a new collaborative framework for lip reading, aiming to improve its performance by addressing two key issues. The first issue is the latent domain gap between the cross-modal data, which can result in learning ambiguity. To address this, the paper proposes a trainable "master" network that takes in both audio signals and silent lip videos, instead of relying on a pretrained teacher. The master network generates logits from three modalities: audio, video, and their combination. The second issue is the need for the teacher to adaptively adjust teaching contents as the student evolves. To address this, the paper incorporates task-specific feedback from the student to regularize the master network. Additionally, a couple of "tutor" networks are introduced to provide guidance and emphasize important knowledge. A curriculum learning design is also incorporated to ensure better convergence. Experimental results demonstrate that the proposed network outperforms existing methods on various benchmarks, including word-level and sentence-level scenarios.