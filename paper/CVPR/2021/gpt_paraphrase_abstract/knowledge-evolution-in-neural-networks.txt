The challenge of training deep networks with limited datasets is addressed in this study. The proposed approach, known as knowledge evolution (KE), involves splitting the deep network into two hypotheses: the fit-hypothesis and the reset-hypothesis. By iteratively evolving the knowledge within the fit-hypothesis through perturbations of the reset-hypothesis, the performance of the network is enhanced on small datasets. Additionally, KE reduces both overfitting and the need for extensive data collection. The effectiveness of KE is demonstrated through evaluations on various network architectures and loss functions, using small datasets and randomly initialized deep networks. The results show a significant improvement of 21% compared to a state-of-the-art baseline, accompanied by a 73% reduction in inference cost. KE also achieves state-of-the-art performance on classification and metric learning benchmarks. The code for implementing KE is available at http://bit.ly/3uLgwYb.