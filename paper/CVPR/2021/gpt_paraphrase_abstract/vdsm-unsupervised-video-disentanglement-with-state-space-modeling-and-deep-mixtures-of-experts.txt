Disentangling information in representations plays a crucial role in various tasks such as causal reasoning, generative modeling, and ensuring fairness in machine learning. However, achieving disentanglement typically requires supervision or inductive bias. Since supervision is often costly or unfeasible, we propose an unsupervised approach called Video Disentanglement State-Space Model (VDSM) that incorporates structural inductive bias. VDSM uses hierarchical structure, a dynamic prior, and a Mixture of Experts decoder to disentangle latent time-varying and dynamic factors. It learns separate representations for the object/person identity and the performed action in videos. We assess VDSM's performance through qualitative and quantitative tasks like identity and dynamics transfer, sequence generation, Fr√©chet Inception Distance, and factor classification. Remarkably, VDSM achieves state-of-the-art performance surpassing adversarial methods, even when those methods utilize additional supervision.