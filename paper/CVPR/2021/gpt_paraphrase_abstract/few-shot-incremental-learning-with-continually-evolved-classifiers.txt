Few-shot class-incremental learning (FSCIL) is a machine learning problem that involves learning new concepts from limited data points while retaining knowledge of old classes. This is challenging because the limited data leads to overfitting and catastrophic forgetting. Additionally, the learned classifier can only provide discriminative information in individual sessions, while FSCIL requires evaluation on all classes.   To address these challenges, we propose a two-fold approach. First, we use a decoupled learning strategy where only the classifiers are updated in each incremental session, preventing forgetting in the representations. We demonstrate that combining a pre-trained backbone with a non-parametric class mean classifier outperforms existing methods.   Second, we introduce a Continually Evolved Classifier (CEC) that uses a graph model to propagate context information between classifiers for adaptation. To enable learning of CEC, we design a pseudo incremental learning paradigm that constructs a pseudo incremental learning task to optimize the graph parameters by sampling data from the base dataset.   Experimental results on CIFAR100, miniImageNet, and CUB200 datasets show that our method significantly outperforms baselines and achieves state-of-the-art results with notable advantages.