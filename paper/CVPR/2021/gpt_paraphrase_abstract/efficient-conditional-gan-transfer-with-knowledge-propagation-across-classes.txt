Generative adversarial networks (GANs) have achieved impressive results in generating images both unconditionally and conditionally. Recent research has shown that pre-trained GANs, trained on different datasets, can be used to enhance image generation for a smaller target dataset. However, this transfer learning approach has not been extensively explored in the context of conditional GANs (cGANs), which presents new opportunities for knowledge transfer compared to unconditional setups. Specifically, in conditional GANs, new classes can benefit from knowledge borrowed from related old classes or shared knowledge among themselves to improve training. This motivates our study on efficient transfer learning in conditional GANs with knowledge propagation across classes.  To tackle this problem, we propose a novel method for GAN transfer that explicitly propagates knowledge from old classes to new classes. Our approach involves leveraging conditional batch normalization (BN), a commonly used technique, to learn class-specific information of the new classes based on that of the old classes. This allows for implicit knowledge sharing among the new classes and efficient knowledge propagation from the old classes to the new ones. Notably, the BN parameters increase linearly with the number of new classes.   Extensive evaluations demonstrate the substantial superiority of our proposed method over existing state-of-the-art approaches for efficient conditional GAN transfer tasks. For those interested, the code for our method can be found at the following GitHub repository: https://github.com/mshahbazi72/cGANTransfer.