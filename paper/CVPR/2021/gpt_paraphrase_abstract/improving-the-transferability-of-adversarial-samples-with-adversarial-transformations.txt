Deep neural networks (DNNs) have shown impressive performance in various vision tasks. However, they are highly vulnerable to adversarial examples, which are created by making slight modifications to benign samples in a way that is not detectable by humans. This raises concerns about the security of deploying DNNs in safety-critical domains. Transfer-based attacks have gained attention as a practical way to assess the robustness of DNNs. These attacks involve crafting adversarial samples using local models and then using these samples to attack a remote black-box model. However, existing transfer-based attacks often have low success rates due to overfitting to the local model. To address this, we propose enhancing the robustness of synthesized adversarial samples through adversarial transformations. We use an adversarial transformation network to model the most harmful distortions that can eliminate adversarial noise, and we require the synthesized samples to be resistant to these transformations. Extensive experiments on the ImageNet benchmark demonstrate that our method outperforms state-of-the-art techniques in attacking both undefended and defended models.