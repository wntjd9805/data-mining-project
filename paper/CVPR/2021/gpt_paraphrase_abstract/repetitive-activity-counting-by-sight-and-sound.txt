This paper aims to improve repetitive activity counting in videos by incorporating sound into the analysis. Unlike previous works that only focus on visual content, we introduce the use of corresponding sound to enhance accuracy in challenging visual conditions. We propose a model that separately analyzes the visual and sound streams and incorporates modules for cross-modal temporal interaction and reliability estimation. To facilitate learning and evaluation, we repurpose an existing dataset and introduce a variant for challenging vision conditions. Experimental results demonstrate the benefits of incorporating sound and the introduced modules for repetition counting. Our sight-only model already surpasses the state-of-the-art, and the addition of sound further improves performance, particularly in harsh visual conditions. The code and datasets can be found at https://github.com/xiaobai1217/RepetitionCounting.