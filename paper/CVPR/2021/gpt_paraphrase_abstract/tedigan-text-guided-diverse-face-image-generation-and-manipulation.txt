We introduce TediGAN, a new framework for generating and manipulating images using textual descriptions. Our approach includes three components: a StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity component learns how to match text and images by mapping them to a shared embedding space. We also employ instance-level optimization to preserve identity during image manipulation. Our model is capable of producing high-quality images at an unprecedented resolution of 10242. Additionally, our TediGAN framework supports multi-modal image synthesis, allowing for inputs such as sketches or semantic labels, with or without instance guidance. To enable text-guided multi-modal synthesis, we present the Multi-Modal CelebA-HQ dataset, which consists of real face images along with corresponding semantic segmentation maps, sketches, and textual descriptions. Through extensive experiments on this dataset, we demonstrate the superior performance of our proposed method. Code and data for our framework are available at https://github.com/weihaox/TediGAN.