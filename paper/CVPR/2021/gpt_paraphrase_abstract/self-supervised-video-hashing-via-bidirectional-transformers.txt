We propose a self-supervised video hashing method called Bidirectional Transformers (BTH) to improve unsupervised video retrieval. Unlike existing methods, BTH utilizes bidirectional models and reliable training objectives to maximize the use of frame correlations and video similarity structure. We introduce a visual cloze task to leverage bidirectional correlations between frames and a similarity reconstruction task to reveal the similarity structure in unlabeled video data. Additionally, we incorporate a cluster assignment task to enhance the learning of discriminative binary codes by exploiting the dataset's structural statistics. Experimental results on three benchmark datasets (FCVID, ActivityNet, and YFCC) demonstrate the effectiveness of our proposed approach.