This paper introduces MonoRec, a novel semi-supervised monocular dense reconstruction architecture for predicting depth maps in dynamic environments using a single moving camera. MonoRec is based on a multi-view stereo approach that incorporates information from multiple consecutive images into a cost volume. To handle the presence of moving objects in the scene, the paper proposes a MaskModule that predicts masks for these objects by leveraging the photometric inconsistencies captured in the cost volumes. Unlike other multi-view stereo methods, MonoRec is capable of reconstructing both static and moving objects by utilizing the predicted masks. Additionally, the paper presents a unique multi-stage training scheme with a semi-supervised loss formulation that eliminates the need for LiDAR depth values. The performance of MonoRec is extensively evaluated on the KITTI dataset, demonstrating its superiority over both multi-view and single-view methods. Furthermore, the generalizability of MonoRec is showcased by training it on the KITTI dataset and successfully applying it to the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded using a handheld camera. The code and related materials for MonoRec are available at https://vision.in.tum.de/research/monorec.