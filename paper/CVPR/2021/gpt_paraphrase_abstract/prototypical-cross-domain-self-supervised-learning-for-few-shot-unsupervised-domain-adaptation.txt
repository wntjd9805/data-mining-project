Previous research on unsupervised domain adaptation (UDA) has focused on transferring predictive models from a labeled source domain to an unlabeled target domain. However, in some cases, collecting labels in the source domain is expensive and impractical. To address this issue, recent work has utilized instance-wise cross-domain self-supervised learning, followed by fine-tuning. However, this approach only focuses on learning and aligning low-level discriminative features.In this paper, we propose an end-to-end framework called Prototypical Cross-domain Self-Supervised Learning (PCS) for Few-shot Unsupervised Domain Adaptation (FUDA). PCS not only performs cross-domain alignment of low-level features but also encodes and aligns semantic structures in the shared embedding space across domains. Our framework captures the semantic structures of the data by utilizing in-domain prototypical contrastive learning, and it aligns features through cross-domain prototypical self-supervision.Compared to state-of-the-art methods, PCS significantly improves the mean classification accuracy across different domain pairs in FUDA. Specifically, on datasets such as Office, Office-Home, VisDA-2017, and DomainNet, PCS achieves improvements of 10.5%, 3.5%, 9.0%, and 13.2% respectively.