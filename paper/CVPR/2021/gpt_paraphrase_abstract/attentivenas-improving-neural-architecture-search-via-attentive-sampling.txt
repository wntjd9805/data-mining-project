Neural architecture search (NAS) has shown promise in designing accurate and efficient models. Two-stage NAS, like BigNAS, separates model training and searching processes, resulting in efficient and accurate searches. However, the accuracy of final searched models is impacted by the sampling from the search space during training. While uniform sampling is commonly used for its simplicity, it ignores the model performance Pareto front, which is crucial in the search process and hinders further improvement in model accuracy. To address this, we propose AttentiveNAS, which focuses on improving the sampling strategy to achieve better performance Pareto. Additionally, we introduce algorithms that efficiently identify networks on the Pareto during training without the need for re-training or post-processing. Our method allows for the simultaneous generation of numerous networks across a wide range of FLOPs. The resulting AttentiveNAS models achieve top-1 accuracy ranging from 77.3% to 80.7% on ImageNet, surpassing SOTA models such as BigNAS, Once-for-All networks, and FBNetV3. Furthermore, we achieve an ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models can be found at the following link: https://github.com/facebookresearch/AttentiveNAS.