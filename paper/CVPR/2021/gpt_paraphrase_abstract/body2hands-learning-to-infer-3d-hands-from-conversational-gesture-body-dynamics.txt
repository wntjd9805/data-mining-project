We present a new approach to synthesizing and estimating 3D hand shapes in conversational gestures using a learned deep prior of body motion. Our model is based on the observation that body motion and hand gestures are closely related in non-verbal communication. To learn this prior, we train our model to predict 3D hand shape over time using only body motion input. By training our model with 3D pose estimations from a large dataset of internet videos, we are able to generate realistic 3D hand gestures based solely on the 3D motion of the speaker's arms. We evaluate the effectiveness of our method in synthesizing hand gestures from body motion input, as well as using it as a strong prior for estimating 3D hand poses from single-view images. Our results demonstrate that our method outperforms previous state-of-the-art approaches and can generalize to multi-person conversations, even beyond the training data.