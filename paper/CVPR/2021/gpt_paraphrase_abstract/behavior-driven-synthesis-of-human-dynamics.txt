Generating and representing human behavior in computer vision applications is crucial. However, current methods either represent behavior as sequences of postures or alter the appearance of individuals without control over their actual behavior. To address this limitation, we propose a model that can synthesize human behavior while maintaining control over their actions. Our model learns a dedicated representation of human dynamics that is independent of specific postures or appearances. By disentangling behavior from posture, we can change the behavior of a person in any given posture or transfer behavior from one video sequence to another. We introduce a conditional variational framework to achieve this disentanglement. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our approach in capturing, transferring, and generating diverse and detailed human behavior. More information about our project can be found at https://cutt.ly/5l7rXEp.