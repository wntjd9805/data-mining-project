The objective of learning disentanglement is to find a representation of observational data that captures multiple explanatory and generative factors in a low-dimensional space. The commonly used variational autoencoder (VAE) framework is effective in disentangling independent factors from the data. However, in real-world situations, factors with semantic meaning are often not independent but instead have a causal relationship. To address this, we propose a new framework called CausalVAE, which incorporates a Causal Layer to transform independent exogenous factors into causal endogenous factors that correspond to causally related concepts in the data. We also analyze the identifiability of the model and show that the proposed CausalVAE model can recover the true causal structure to a certain extent. We conduct experiments on various datasets, including synthetic and real-world benchmark CelebA, and demonstrate that the causal representations learned by CausalVAE are semantically interpretable. Additionally, we show that the causal relationship between the factors can be accurately identified as a Directed Acyclic Graph (DAG). Furthermore, we demonstrate that the proposed CausalVAE model is capable of generating counterfactual data by manipulating the causal factors through a "do-operation".