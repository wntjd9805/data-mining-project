Most perception methods in robotics rely on depth information from RGB-D cameras, but these cameras struggle to capture the depth of transparent objects due to the refraction and absorption of light. This paper presents a new approach for completing the depth of transparent objects using a single RGB-D image. The approach utilizes a local implicit neural representation based on ray-voxel pairs, which allows for generalization to unseen objects and fast inference. The paper introduces a framework that can fill in missing depth based on noisy RGB-D input and iteratively refines the depth estimation using a self-correcting refinement model. To train the system, a large-scale synthetic dataset with transparent objects is created. Experimental results show that the proposed method outperforms current state-of-the-art methods on both synthetic and real-world data. Additionally, the inference speed of the approach is improved by a factor of 20 compared to the previous best method, ClearGrasp. The code for the method will be made available at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.