This paper introduces a novel algorithm called LinE segment TRansformers (LETR) for end-to-end line segment detection. Unlike existing methods that rely on post-processing and heuristic-guided intermediate processing, LETR utilizes Transformers with integrated tokenized queries, self-attention mechanism, and encoding-decoding strategy. By skipping standard heuristic designs, LETR eliminates the need for edge/junction/region detection processes. Transformers are equipped with a multi-scale encoder/decoder strategy to achieve fine-grained line segment detection using a direct endpoint distance loss, which is more suitable for detecting geometric structures like line segments than standard bounding box representations. The Transformers progressively refine the line segments through layers of self-attention. Experimental results on Wireframe and YorkUrban benchmarks demonstrate that LETR achieves superior performance compared to existing methods.