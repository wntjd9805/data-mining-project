Many learning-based approaches struggle to generalize to new data because the learned prior is limited to the scale and variations of the training samples. This is especially true for 3D learning tasks due to the scarcity of available 3D datasets. To address this, we propose a new learning framework for 3D modeling and reconstruction that enhances the generalization ability of a deep generator. Our approach combines the strengths of learning-based and optimization-based methods. Unlike common practices that use fixed pre-trained priors during testing, we suggest further optimizing the learned prior and latent code based on input physical measurements after training. This strategy effectively overcomes the limitations imposed by pre-trained priors and enables high-quality adaptation to unseen data. We implement our framework using implicit surface representation and demonstrate its effectiveness in various challenging tasks that involve sparse or collapsed observations as input. Experimental results indicate that our approach outperforms state-of-the-art methods in terms of generality and accuracy.