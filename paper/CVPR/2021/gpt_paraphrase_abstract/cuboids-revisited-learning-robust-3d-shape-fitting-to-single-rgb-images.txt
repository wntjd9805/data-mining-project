Humans perceive and understand the world by simplifying it into basic parametric models. Man-made environments, for example, are often made up of simple shapes like cubes or cylinders. Being able to identify these shapes is important for creating abstract descriptions of scenes. Previous methods have tried to estimate the shape parameters directly from 2D or 3D inputs, but they can only handle simple objects and struggle with more complex scenes. In contrast, our approach uses a robust estimator to fit cuboids to 3D features, like depth maps, using a RANSAC estimator guided by a neural network. We condition the network on previously detected parts of the scene, parsing it one step at a time. To extract 3D features from a single RGB image, we optimize a feature extraction CNN in an end-to-end manner. However, simply minimizing the distance between points and cuboids can lead to large or incorrect cuboids blocking parts of the scene. Therefore, we propose an occlusion-aware distance metric that correctly handles opaque scenes. Our algorithm does not require labor-intensive labels, like cuboid annotations, for training. Results on the challenging NYU Depth v2 dataset show that our algorithm successfully abstracts cluttered real-world 3D scene layouts.