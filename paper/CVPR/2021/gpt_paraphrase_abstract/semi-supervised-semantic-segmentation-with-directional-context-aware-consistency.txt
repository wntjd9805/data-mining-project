The field of semantic segmentation has seen significant advancements in recent years. However, achieving high performance in this task often relies on a large number of pixel-level annotations. This paper focuses on the problem of semi-supervised segmentation, where only a small labeled dataset is available along with a much larger collection of unlabeled images. Due to the limited annotations, models tend to heavily depend on the contextual information present in the training data, resulting in poor generalization to unseen scenes. To address this, we propose a high-level representation that captures contextual information while maintaining self-awareness. This is achieved by ensuring context-aware consistency between features of the same identity but with different contexts, making the representations robust to varying environments. Furthermore, we introduce the Direc-tional Contrastive Loss (DC Loss) to enforce consistency in a pixel-to-pixel manner, where the feature with lower quality is aligned towards its counterpart. To improve the quality of samples, we present two sampling strategies to avoid false-negative samples and filter uncertain positive samples. Extensive experiments demonstrate that our simple yet effective method outperforms current state-of-the-art methods by a significant margin and generalizes well even with additional image-level annotations.