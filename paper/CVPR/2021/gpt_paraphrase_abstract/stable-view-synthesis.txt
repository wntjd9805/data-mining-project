We introduce Stable View Synthesis (SVS), a method for generating new views of a scene using a set of source images captured from different viewpoints. SVS utilizes a geometric scaffold obtained through structure-from-motion and multi-view stereo, where each point on the scaffold is associated with view rays and feature vectors that represent its appearance in the input images. The key component of SVS is the view-dependent on-surface feature aggregation, which processes the directional feature vectors at each 3D point to generate a new feature vector for a ray mapping that point to the target view. A convolutional network then renders the target view using a tensor of synthesized features for all pixels. The method is composed of differentiable modules and is trained end-to-end. It incorporates spatially-varying view-dependent importance weighting and feature transformation for each point, ensures spatial and temporal stability through smooth dependence on the target view, and enables synthesis of view-dependent effects like specular reflection. Experimental results demonstrate that SVS surpasses existing view synthesis methods in terms of both quantitative and qualitative performance on various real-world datasets, achieving unprecedented realism in free-viewpoint videos of complex scenes. The code for SVS is available at https://github.com/intel-isl/StableViewSynthesis.