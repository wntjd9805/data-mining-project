The progress of instance segmentation on class-balanced benchmarks has been impressive. However, these methods struggle to perform well in real-world scenarios where there is a long tail distribution of object categories. In these cases, instances of head classes dominate the dataset and serve as negative samples for tail categories. This imbalance leads to biased learning, causing objects of tail categories to be misclassified as background or head categories. To address this issue, we propose Seesaw Loss, a method that dynamically rebalances the gradients of positive and negative samples for each category. This is achieved through two complementary factors: the mitigation factor, which reduces the punishment for tail categories based on the ratio of cumulative training instances, and the compensation factor, which increases the penalty for misclassified instances to avoid false positives for tail categories. We conducted extensive experiments with Seesaw Loss using mainstream frameworks and different data sampling strategies. Through a simple end-to-end training pipeline, Seesaw Loss achieved significant improvements over Cross-Entropy Loss and attained state-of-the-art performance on the LVIS dataset without any additional complexities. The code for Seesaw Loss is available at https://github.com/open-mmlab/mmdetection.