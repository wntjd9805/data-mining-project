The Information Bottleneck (IB) is a principle for learning representations that retains relevant information for predicting labels while minimizing redundancy. However, optimizing the IB principle is challenging and relies on accurate estimation of mutual information. This paper introduces a new strategy called Variational Self-Distillation (VSD) that provides a scalable and flexible solution for fitting mutual information without explicitly estimating it. VSD enables the IB to capture the intrinsic correlation between representation and label in supervised training. Additionally, two other strategies, Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML), are introduced to improve the robustness of representation in multi-view learning by eliminating view-specific and task-irrelevant information. Experimental results in cross-modal person Re-ID demonstrate the superior performance of these strategies compared to state-of-the-art methods. These findings emphasize the need to reconsider the estimation of mutual information.