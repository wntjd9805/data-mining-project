In this study, we introduce a novel clustering method called CHiGac, which stands for Cluster-wise Hierarchical Generative Model for deep amortized clustering. Unlike traditional point-wise clustering approaches, CHiGac groups data points from a cluster-wise perspective, leading to a more efficient neural clustering architecture. CHiGac is capable of simultaneously learning the characteristics of a cluster, how to group data points into clusters, and how to dynamically control the number of clusters. By employing a dedicated cluster generative process, CHiGac effectively captures pairwise and higher-order interactions between data points within and between clusters, thus uncovering hidden structures in the data. To optimize the generalized lower bound of CHiGac, we propose an Ergodic Amortized Inference (EAI) strategy that considers the average behavior of a sequence of inner variational parameter trajectories. The theoretical analysis proves that this strategy reduces the amortization gap. We conduct experiments on both synthetic and real-world datasets to evaluate the performance of CHiGac. The results demonstrate that CHiGac efficiently and accurately clusters datasets, as measured by internal and external evaluation metrics (DBI and ACC).