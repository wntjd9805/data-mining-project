We introduce FACESEC, a framework that allows for comprehensive evaluation of the robustness of face recognition systems. This evaluation is conducted across four dimensions of adversarial modeling: the type of perturbation (such as pixel-level changes or the inclusion of face accessories), the attacker's knowledge of the system (including familiarity with the training data and learning architecture), the attacker's goals (such as avoiding detection or impersonating someone else), and the attacker's capability (whether attacks are tailored to individual inputs or applied to sets of inputs).  We utilize FACESEC to analyze five different face recognition systems in both closed-set (limited number of known individuals) and open-set (unlimited number of potential individuals) scenarios. Additionally, we assess the effectiveness of the current leading approach for defending against physically realizable attacks on these systems.  Our findings highlight the significance of accurate knowledge of the neural architecture in black-box attacks, as opposed to familiarity with the training data. Furthermore, we observe that open-set face recognition systems are more susceptible to various types of attacks compared to closed-set systems. However, the efficacy of attacks in other threat model variations depends heavily on the nature of the perturbation and the specific neural network architecture employed. For instance, attacks involving adversarial face masks tend to be more potent, even against models that have been adversarially trained. Additionally, the ArcFace architecture demonstrates greater robustness compared to other architectures.