Federated learning (FL) is a popular method for distributed learning that aims to protect privacy by not explicitly sharing private data. However, recent studies have highlighted the vulnerability of FL to inference attacks when model updates are shared. In this research, we make a key observation that the leakage of data representation from gradients is the main cause of privacy breaches in FL. We further analyze this observation to explain how the leakage occurs. Building upon this insight, we propose a defense mechanism called Soteria to counter model inversion attacks in FL. Our defense strategy involves learning to perturb data representation in a way that severely degrades the quality of reconstructed data, while maintaining FL performance. Additionally, we provide certified robustness and convergence guarantees for FL and FedAvg algorithms after implementing our defense. To evaluate the effectiveness of our defense, we conduct experiments on MNIST and CIFAR10 datasets, focusing on defending against the DLG and GS attacks. The results demonstrate that our proposed defense significantly increases the mean squared error between reconstructed and raw data by up to 160 times compared to baseline defense methods, without sacrificing accuracy. This substantial improvement in privacy protection enhances the security of the FL system. The code for our defense mechanism is available at https://github.com/jeremy313/Soteria.