The problem of reconstructing a complete 3D scene with accurate semantic information from a single-view depth or RGBD image is crucial but challenging for indoor scene understanding. In this study, we introduce a new framework called Scene-Instance-Scene Network (SISNet) that combines instance and scene level semantic information. Our approach can infer detailed shape information and identify nearby objects with easily confused semantic categories. The key idea is to separate instances from a roughly completed semantic scene instead of using the raw input image, which helps guide the reconstruction of instances and the overall scene. SISNet performs iterative scene-to-instance (SI) and instance-to-scene (IS) semantic completion. The SI step encodes the surrounding context of objects to effectively separate instances from the scene, allowing each instance to be voxelized at a higher resolution for capturing finer details. The IS step integrates the fine-grained instance information back into the 3D scene, resulting in more accurate semantic scene completion. Through extensive experiments on real NYU, NYUCAD, and synthetic SUNCG-RGBD datasets, we demonstrate that our proposed method consistently outperforms state-of-the-art approaches. The code and supplementary material for our method will be available at https://github.com/yjcaimeow/SISNet.