Estimating the 3D poses of hands and objects from a single image is a difficult task due to self-occlusion and limited ground-truth annotations. To address these challenges, we propose a unified framework that utilizes semi-supervised learning to estimate the 3D poses of hands and objects. Our approach incorporates explicit contextual reasoning between hand and object representations, allowing for better estimation accuracy. Instead of relying solely on limited 3D annotations, we leverage the spatial-temporal consistency present in large-scale hand-object videos to generate pseudo labels for training. This semi-supervised learning approach not only improves hand pose estimation in real-world datasets but also enhances object pose estimation, which typically lacks sufficient ground-truth data. Furthermore, by training our model with diverse videos, it demonstrates better generalization performance across multiple out-of-domain datasets. To access our project page and code, please visit https://stevenlsw.github.io/Semi-Hand-Object.