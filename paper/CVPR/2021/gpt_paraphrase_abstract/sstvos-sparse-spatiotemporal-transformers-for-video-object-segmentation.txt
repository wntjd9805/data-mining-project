This paper presents a new approach to video object segmentation (VOS) using a Transformer-based method called Sparse Spatiotemporal Transformers (SST). To overcome the limitations of previous approaches, we propose a scalable and end-to-end solution for VOS. SST utilizes sparse attention to extract per-pixel representations for each object in a video based on spatiotemporal features. By employing attention mechanisms, our method allows the model to learn to focus on multiple frames in the video history, enabling it to perform correspondence-like computations necessary for motion segmentation. We demonstrate the superiority of attention-based methods over recurrent networks in the spatiotemporal domain. Our approach achieves competitive results on popular benchmark datasets, YouTube-VOS and DAVIS 2017, while improving scalability and robustness to occlusions compared to state-of-the-art techniques. The code for our method is available at https://github.com/dukebw/SSTVOS.