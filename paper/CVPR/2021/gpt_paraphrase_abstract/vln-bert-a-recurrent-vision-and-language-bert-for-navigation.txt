The accuracy of visiolinguistic tasks has greatly improved with the use of vision-and-language (V&L) BERT. However, its application in vision-and-language navigation (VLN) is limited due to the challenge of adapting the BERT architecture to the partially observable Markov decision process in VLN. This requires history-dependent attention and decision making. To address this, we propose a time-aware recurrent BERT model for VLN. Our model incorporates a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE datasets, we demonstrate that our model outperforms more complex encoder-decoder models, achieving state-of-the-art results. Additionally, our approach can be applied to other transformer-based architectures, supports pre-training, and can simultaneously solve navigation and referring expression tasks.