We present SelfDoc, a versatile pre-training framework for understanding document images. Our framework takes into account the multimodal nature of documents and their sequential reading nature. It utilizes the positional, textual, and visual information of each semantically meaningful component in a document, while also modeling the contextualization between content blocks. Unlike existing models, our approach takes a coarse-grained approach instead of treating individual words as input, avoiding excessive contextualization. Additionally, we incorporate cross-modal learning during pre-training to make full use of the multimodal information in unlabeled documents. For downstream tasks, we propose a novel modality-adaptive attention mechanism that adjusts the emphasis on language and vision signals for effective multimodal feature fusion. Our framework benefits from self-supervised pre-training on documents without the need for annotations, using a feature masking training strategy. It outperforms previous models on multiple downstream tasks, while requiring significantly fewer document images in the pre-training stage.