Unsupervised representation learning is crucial for detecting and locating anomalies in images. However, there are two main challenges in learning such representations. Firstly, the limited sample size hinders the ability to learn a comprehensive and transferable representation using traditional techniques. Secondly, the learned features should be able to distinguish normal and anomalous samples despite being trained only on normal samples. To address these challenges, we propose a method that involves distilling features from an expert network, pre-trained on ImageNet, into a simpler cloner network.By comparing the intermediate activation values of the expert and cloner networks for a given input sample, we are able to detect and localize anomalies. Our method improves upon previous approaches by considering multiple intermediate hints in the distillation process, leading to better utilization of the expert's knowledge and a more distinct discrepancy between the two networks. Unlike other methods that struggle with precise anomaly localization or require expensive region-based training, our framework incorporates interpretability algorithms without the need for any specialized or intensive training procedures.Despite the significant differences between some test datasets and ImageNet, our proposed method achieves competitive or even superior results compared to the state-of-the-art on various datasets, including MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two other medical datasets. These results are achieved in both anomaly detection and localization tasks.