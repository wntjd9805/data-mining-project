Developing instance segmentation models that are both efficient with data and capable of handling uncommon object categories is a significant challenge in the field of computer vision. One promising approach to tackle this challenge is by utilizing data augmentations. In this study, we thoroughly investigate the effectiveness of the Copy-Paste augmentation technique for instance segmentation. Previous studies on Copy-Paste relied on complex modeling of the surrounding visual context when pasting objects, but our findings indicate that a simple random pasting mechanism is sufficient and can yield substantial improvements over strong baselines. Additionally, we demonstrate that the Copy-Paste augmentation is compatible with semi-supervised methods that leverage extra data through pseudo labeling, such as self-training. On the COCO instance segmentation dataset, our approach achieves a mask Average Precision (maskAP) of 49.1 and a box Average Precision (box AP) of 57.3. This represents a notable improvement of 0.6 mask AP and 1.5 box AP over the previous state-of-the-art. Furthermore, we showcase the significant enhancements that Copy-Paste can bring to the LVIS benchmark, with our baseline model surpassing the winner of the LVIS 2020 Challenge by 3.6 mask AP in the rare object category.