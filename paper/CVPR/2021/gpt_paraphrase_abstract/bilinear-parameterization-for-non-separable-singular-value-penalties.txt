Low rank inducing penalties have been proven effective in uncovering important structures in computer vision and machine learning. However, these methods often result in non-convex optimization problems, which can be challenging to solve. Typically, standard splitting schemes like Alternating Direction Methods of Multipliers (ADMM) or subgradient methods are used, but these approaches have slow convergence near local minima. To address this, we propose a new method using second-order methods, specifically the variable projection method (VarPro). Instead of non-convex penalties, we introduce a surrogate that converts the original objectives into differentiable equivalents, enabling faster convergence. This approach is compatible with a wide range of regularizers within the bilinear framework. We demonstrate the benefits of our method on real datasets for both rigid and non-rigid structure from motion tasks. Our results show qualitative improvements in reconstructions, indicating that popular non-convex objectives can transition advantageously into our proposed framework.