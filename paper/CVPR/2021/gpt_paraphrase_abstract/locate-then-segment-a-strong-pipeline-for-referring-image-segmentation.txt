The traditional approach to referring image segmentation involves combining visual and linguistic features to directly generate a segmentation mask without explicitly considering the localization information of the referred objects. In this study, we propose a new approach called "Locate-Then-Segment" (LTS), where attention is first applied to the target image regions based on a language expression, followed by generating a refined segmentation mask considering the object's context. The LTS extracts and combines visual and textual features to create a cross-modal representation, then uses cross-model interaction to locate the referred object using position prior, and finally generates the segmentation result using a lightweight segmentation network. Our LTS method surpasses previous state-of-the-art methods by a significant margin on three benchmark datasets (+3.2% on RefCOCO+ and +3.4% on RefCOCOg). Additionally, our model provides explicit object localization, enhancing interpretability as demonstrated by visualization experiments. We believe that our LTS framework has the potential to serve as a strong baseline for referring image segmentation.