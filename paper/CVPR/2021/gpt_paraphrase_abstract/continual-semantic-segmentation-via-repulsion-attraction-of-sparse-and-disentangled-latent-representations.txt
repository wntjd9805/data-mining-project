Deep neural networks face a significant challenge known as catastrophic forgetting, where they forget previously learned tasks when learning new ones. This paper focuses on addressing this issue in the context of class incremental continual learning for semantic segmentation. The goal is to learn new categories over time without retaining previous training data. To achieve this, the proposed learning scheme shapes the latent space to minimize forgetting while improving recognition of novel classes. The framework consists of three novel components that can be easily combined with existing techniques. Firstly, prototypes matching ensures consistency in the latent space for old classes, making the encoder produce similar representations for previously seen classes in subsequent steps. Secondly, features sparsification creates space in the latent space to accommodate novel classes. Finally, contrastive learning is used to cluster features based on their semantics while separating those from different classes. The effectiveness of the approach is extensively evaluated on the Pascal VOC2012 and ADE20K datasets, demonstrating superior performance compared to state-of-the-art methods.