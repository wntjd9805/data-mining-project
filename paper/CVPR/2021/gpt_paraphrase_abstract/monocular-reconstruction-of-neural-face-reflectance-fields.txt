The reflectance field of a face refers to the properties that determine how light interacts with the face, including diffuse, specular, inter-reflection, and self-shadowing effects. Most current methods for estimating face reflectance from a single image only consider diffuse properties, with a few including a specular component. However, these methods overlook important perceptual aspects such as higher-order global illumination effects and self-shadowing. In this study, we propose a novel neural representation for face reflectance that can estimate all components responsible for the final appearance from a single image. Instead of modeling each component separately, our neural representation generates a basis set of faces in a space that is invariant to geometric deformations and parametrized by the input light direction, viewpoint, and face geometry. We train our method using a dataset of 300 individuals illuminated with 150 different light conditions from 8 viewpoints. Our results demonstrate that our method outperforms existing monocular reflectance reconstruction methods by better capturing physical effects like sub-surface scattering, specularities, self-shadows, and other higher-order effects. By reconstructing the reflectance field, we can render the face from any viewpoint and under any lighting condition.