GuessWhat?! is a game where a Questioner agent asks questions and an Oracle agent answers about an object in an image. A Guesser agent then makes a final guess based on the dialog history. Previous work focused on optimizing dialogue policies and fusing visual-linguistic information, but without shared knowledge of vision-linguistic representation. This paper proposes new models for the Oracle, Guesser, and Questioner that utilize a pretrained vision-linguistic model called VilBERT. The Oracle model incorporates a fusion mechanism to understand both intra and inter-object questions. The Guesser model introduces a state-estimator that leverages VilBERT's strength in single-turn expression comprehension. The Questioner shares the state-estimator from the pretrained Guesser to guide the question generator. Experimental results show that our models outperform existing models by a significant margin in terms of Oracle, Guesser, and End-to-End Questioner performance. The answer format only provides the abstraction.