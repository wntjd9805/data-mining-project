We propose SOLID-Net, a neural network that accurately estimates outdoor lighting variations for any pixel location in a single outdoor image. Instead of using a unified sky environment map like previous approaches, we generate local lighting environment maps that vary spatially. This is achieved by combining a global sky environment map with image information that has been warped based on estimated geometric details from intrinsics. Since there is no readily available outdoor dataset with image and local lighting ground truth, we introduce the SOLID-Img dataset. This dataset consists of physically-based rendered images along with corresponding intrinsic and lighting information. To train our neural network, we use physically-based constraints to regress intrinsic cues and then utilize them for both global and local lighting estimation. Experimental results on synthetic and real datasets demonstrate that SOLID-Net outperforms previous methods significantly.