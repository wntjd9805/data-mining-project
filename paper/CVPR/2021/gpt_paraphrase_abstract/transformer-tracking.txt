This study addresses the limitations of correlation as a feature fusion method in tracking algorithms. While correlation is commonly used in Siamese-based trackers, it has drawbacks such as losing semantic information and easily falling into local optima. To overcome these limitations, the authors propose an attention-based feature fusion network inspired by the Transformer model. The network includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. The resulting Transformer tracking (TransT) method combines these modules with a Siamese-like feature extraction backbone, a classification and regression head, and attention-based fusion mechanisms. Experimental results on six challenging datasets, including LaSOT, TrackingNet, and GOT-10k benchmarks, demonstrate the promising performance of TransT. The tracker achieves efficient real-time performance, running at approximately 50 fps on a GPU. The code and models for TransT are available on GitHub at https://github.com/chenxin-dlut/TransT.