This paper focuses on the challenge of separating individual speech signals from videos using audio-visual neural processing. Traditional methods rely on frame-wise matching criteria, which can be limited by the accuracy of audio-visual synchronization and the effectiveness of their representations. To address the issue of frame discontinuity caused by transmission delays or jitter, the authors propose a cross-modal affinity network (CaffNet) that learns global correspondence and locally varying affinities between audio and visual streams. The global term provides stability over a temporal sequence, resolving the problem of inconsistent assignments. By extending the cross-modal affinity to a complex network, the authors further enhance the separation performance in the complex spectral domain. Experimental results demonstrate that the proposed methods outperform conventional approaches on various datasets, showcasing their advantages in real-world scenarios.