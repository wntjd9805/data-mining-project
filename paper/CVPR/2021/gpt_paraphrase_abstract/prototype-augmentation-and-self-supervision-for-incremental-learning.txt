Despite their impressive performance in individual tasks, deep neural networks struggle with catastrophic forgetting when learning new tasks incrementally. Various incremental learning methods have been proposed recently, with some relying on stored data or complex generative models to achieve acceptable performance. However, storing data from previous tasks is limited by memory or privacy concerns, and generative models are often unstable and inefficient in training. In this paper, we propose a simple non-exemplar based method called PASS to address the problem of catastrophic forgetting in incremental learning.Our approach involves two key strategies. Firstly, we suggest memorizing one class-representative prototype for each old class and utilizing prototype augmentation (protoAug) in the deep feature space. This helps to maintain the decision boundary of previous tasks. Secondly, we incorporate self-supervised learning (SSL) to learn more generalizable and transferable features for other tasks. This demonstrates the effectiveness of SSL in incremental learning.Through experiments conducted on benchmark datasets, we demonstrate that our approach outperforms non-exemplar based methods significantly. Additionally, our approach achieves comparable performance to exemplar based approaches.