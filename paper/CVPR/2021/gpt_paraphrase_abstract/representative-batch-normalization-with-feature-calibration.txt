Batch Normalization (BatchNorm) has become a standard component in modern neural networks for stabilizing training. It achieves this by standardizing features over the batch dimension using centering and scaling operations and statistics of mean and variance. Although BatchNorm enables stable training and improved network representation, it overlooks the differences in representations among individual instances. To address this limitation, we propose a feature calibration scheme that enhances instance-specific representations with minimal computational cost. The centering calibration strengthens informative features and reduces noisy ones, while the scaling calibration ensures a more stable feature distribution. Our proposed variant, Representative Batch-Norm, can be easily incorporated into existing methods to enhance performance in tasks such as classification, detection, and segmentation. The source code can be found at http://mmcheng.net/rbn.