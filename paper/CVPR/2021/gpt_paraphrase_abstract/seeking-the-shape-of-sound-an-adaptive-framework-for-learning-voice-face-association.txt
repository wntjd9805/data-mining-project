In recent years, there has been a growing interest in learning the connection between voice and face automatically in the field of computer vision. However, previous studies in this area have primarily focused on using local information for aligning the two modalities and have not considered the variation in learning difficulty across different individuals. This paper presents a new approach to address these limitations. Firstly, we propose a two-level modality alignment loss that incorporates both global and local information. This includes introducing a global loss driven by identity classification, which ensures that the embeddings of different identities are maximally distant from each other while embeddings of the same identity are minimally distant. Secondly, we introduce a dynamic reweighting scheme to effectively handle hard-to-learn identities and filter out unlearnable ones. Experimental results demonstrate that our proposed method outperforms existing approaches in various scenarios, such as voice-face matching, verification, and retrieval.