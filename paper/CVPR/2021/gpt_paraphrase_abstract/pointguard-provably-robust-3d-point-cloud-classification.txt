The vulnerability of 3D point cloud classification to adversarial attacks has been demonstrated in various studies, which has implications for safety-critical applications like autonomous driving and robotic grasping. These attacks involve modifying, adding, or deleting a small number of points in order to make a classifier predict an incorrect label for a point cloud. Randomized smoothing is a commonly used technique to build robust 2D image classifiers, but it falls short when applied to 3D point cloud classification as it can only certify robustness against adversarially modified points. This research introduces PointGuard, the first defense mechanism that guarantees provable robustness against adversarially modified, added, and/or deleted points. PointGuard achieves this by creating multiple subsampled point clouds from the original point cloud, each containing a random subset of points, and predicting the label of the original point cloud based on the majority vote among the labels of the subsampled point clouds predicted by the classifier. The main theoretical contributions of this work include proving that PointGuard consistently predicts the same label for a 3D point cloud when the number of adversarially modified, added, and/or deleted points is limited, and demonstrating the tightness of the derived bound without making assumptions about the point cloud classifier. Additionally, an efficient algorithm is designed to compute the certified robustness guarantees offered by PointGuard. The effectiveness of PointGuard is evaluated through empirical analysis on benchmark datasets like ModelNet40 and ScanNet.