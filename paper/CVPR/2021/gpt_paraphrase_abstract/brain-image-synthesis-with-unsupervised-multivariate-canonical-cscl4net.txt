Advances in neuroscience have demonstrated the usefulness of multi-modal medical data for investigating certain diseases and understanding human cognition. However, obtaining complete sets of different modalities is hindered by factors such as long acquisition times, high costs, and artifacts. Additionally, the complexity, high dimensionality, and heterogeneity of neuroimaging data pose challenges in effectively utilizing randomized scans, as data from the same modality is often measured differently by different machines. Therefore, there is a need to move beyond traditional imaging-dependent processes and generate anatomically specific target-modality data from a source input. To address this, we propose a novel approach called CSCℓ4Net, which learns dedicated features that can account for both inter- and intra-modal variations. By unifying intra-modal data in feature maps and using multivariate canonical adaptation, CSCℓ4Net enables feature-level mutual transformation. The inclusion of a positive definite Riemannian manifold-penalized data fidelity term allows CSCℓ4Net to reconstruct missing measurements based on transformed features. Moreover, the maximization of the ℓ4-norm simplifies into a computationally efficient optimization problem. Extensive experiments demonstrate the ability and robustness of CSCℓ4Net compared to state-of-the-art methods across multiple datasets.