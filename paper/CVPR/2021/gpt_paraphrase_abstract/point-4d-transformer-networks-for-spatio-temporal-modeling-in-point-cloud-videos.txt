Point cloud videos are characterized by irregularities and lack of order in the spatial dimension, making it challenging to track the movement of points accurately. Existing methods rely on point tracking, which may fail to handle colorless point clouds. In this study, we propose a novel approach called the Point 4D Transformer (P4Transformer) network to model raw point cloud videos. The P4Transformer network consists of a point 4D convolution and a transformer. The convolution embeds the spatio-temporal local structures of the point cloud video, while the transformer captures the appearance and motion information by performing self-attention on the embedded features. This approach allows for merging related or similar local areas based on attention weight instead of explicit tracking. We conducted extensive experiments on 3D action recognition and 4D semantic segmentation benchmarks, and the results demonstrate the effectiveness of our P4Transformer network for point cloud video modeling. Figure 1 illustrates the process of point cloud video modeling using our P4Transformer network, where color represents depth. A point cloud video is a sequence of irregular and unordered 3D coordinate sets, and our P4Transformer network encodes this video into a coordinate tensor using a point 4D convolution. The transformer then performs self-attention on the embedded tensors to capture the global spatio-temporal structure of the entire point cloud video. Overall, our P4Transformer network offers a new approach to modeling point cloud videos without relying on point tracking.