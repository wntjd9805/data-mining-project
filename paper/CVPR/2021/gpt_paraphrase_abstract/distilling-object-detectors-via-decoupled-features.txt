This study introduces a novel distillation algorithm called DeFeat for object detection. Knowledge distillation is a popular technique for transferring information from a complex teacher network to a smaller student network while maintaining performance. However, existing approaches often overlook the importance of features derived from regions that do not contain objects. The study highlights the significance of these features and emphasizes that different regions should be assigned different importance during distillation. DeFeat processes two levels of decoupled features, namely decoupled features from the neck and decoupled proposals from the classification head, to embed useful information into the student detector. The proposed DeFeat algorithm outperforms state-of-the-art distillation methods for object detection across various detectors with different backbones. For instance, DeFeat improves the mAP of ResNet50 based Faster R-CNN from 37.4% to 40.9% and ResNet50 based RetinaNet from 36.5% to 39.7% on the COCO benchmark. The code for DeFeat will be made available to the public.