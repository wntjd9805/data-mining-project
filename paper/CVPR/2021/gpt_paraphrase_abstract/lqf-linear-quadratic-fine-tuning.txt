Linear classifiers that optimize convex loss functions exhibit predictable behavior with respect to changes in training data, initial conditions, and optimization. However, deep neural networks (DNNs) trained through non-linear fine-tuning lack these desirable properties. Previous attempts to linearize DNNs have provided theoretical insights but lacked practical impact due to significant performance gaps compared to non-linear optimization. In this study, we propose a method, called LQF, for linearizing pre-trained models that achieves comparable performance to non-linear fine-tuning in real-world image classification tasks. This approach allows for the interpretability of linear models without sacrificing performance. LQF involves simple modifications to the architecture, loss function, and optimization used in classification: replacing ReLU with Leaky-ReLU, using mean squared loss instead of cross-entropy, and pre-conditioning with Kronecker factorization. While each of these changes alone is insufficient to match the performance of non-linear fine-tuning, their combination enables comparable performance and even superior results in low-data scenarios. This approach offers the simplicity, robustness, and interpretability of linear-quadratic optimization.