We explore the challenge of generating realistic traffic scenes automatically. Current methods rely on predefined rules and heuristics to insert actors into the scene, but they struggle to accurately represent the complexity and diversity of real traffic scenes. This creates a gap between synthesized and real traffic scenes, making it difficult to train and test self-driving vehicles effectively. To overcome this limitation, we introduce SceneGen, a neural autoregressive model that eliminates the need for rules and heuristics. By using the ego-vehicle state and a detailed map of the surrounding area, SceneGen inserts actors of different types into the scene and generates their sizes, orientations, and velocities. We demonstrate the effectiveness of SceneGen on two large-scale datasets, proving its ability to accurately model the distributions of real traffic scenes. Additionally, we show that when combined with sensor simulation, SceneGen can be used to train perception models that can generalize to real-world scenarios.