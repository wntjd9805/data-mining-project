The Visual Semantic Embedding (VSE) approach is widely used for vision-language retrieval, aiming to create a deep embedding space where visual data is closely aligned with semantic text labels or descriptions. While recent VSE models employ complex methods to contextualize and aggregate multi-modal features, our research reveals that simple global pooling functions, such as max pooling, outperform these complex models across different feature extractors. However, finding the best pooling function for different data modalities and feature extractors can be time-consuming and tedious. To address this, we propose the Generalized Pooling Operator (GPO), which automatically adapts to the optimal pooling strategy for different features without requiring manual tuning. We integrate GPO into the VSE model, resulting in VSE∞. Despite its simplicity, VSE∞ significantly outperforms previous VSE methods in image-text retrieval benchmarks using popular feature extractors. With minor adjustments, variants of VSE∞ achieve state-of-the-art performance on two video-text retrieval datasets. Through comprehensive experiments and visualizations, we demonstrate that GPO consistently discovers the best pooling strategy and can serve as a plug-and-play feature aggregation module for standard VSE models. The code and pre-trained models are available at http://jcchen.me/vse_infty/.