This work aims to annotate sign instances in continuous sign language using a Transformer model. By training the model on a large collection of signing footage with weakly-aligned sub-titles, it becomes capable of attending to a wide range of sign instances and localizing them. The contributions of this study include: (1) demonstrating the use of continuous signing videos with weakly-aligned subtitles to localize signs in continuous sign language, (2) generating hundreds of thousands of annotations for a large sign vocabulary using the learned attention, (3) collecting 37K manually verified sign instances across 950 sign classes to support sign language recognition research, and (4) surpassing the previous state-of-the-art performance on the BSL-1K sign language recognition benchmark by training on the newly annotated data from our method.