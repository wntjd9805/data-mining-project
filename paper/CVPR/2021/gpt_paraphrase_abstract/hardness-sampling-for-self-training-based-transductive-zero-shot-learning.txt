Transductive zero-shot learning (T-ZSL) has gained attention as a solution to the domain shift problem in existing zero-shot learning (ZSL) methods. However, a challenge in T-ZSL is effectively utilizing unseen-class samples for training. To address this, we analyze the roles of unseen-class samples with different levels of difficulty in the training process and make three observations based on the uneven prediction phenomenon in ZSL methods. We propose two hardness sampling approaches to select diverse and difficult samples from an unseen-class dataset. The first approach identifies samples based on the class-level frequency of model predictions, while the second approach enhances this by normalizing the class frequency using an approximate class prior estimated by a prior estimation algorithm. We introduce a new Self-Training framework with Hardness Sampling for T-ZSL (STHS), where any inductive ZSL method can be embedded and iteratively trained with unseen-class samples selected using the hardness sampling approach. We evaluate two typical ZSL methods within the STHS framework and demonstrate superior performance compared to state-of-the-art methods on three public benchmarks. Additionally, we highlight that some existing transductive generalized ZSL (T-GZSL) methods separately use the unseen-class dataset for training, which is not ideal for a generalized ZSL task. Therefore, we propose a stricter T-GZSL data setting and establish a competitive baseline by incorporating the STHS framework into T-GZSL.