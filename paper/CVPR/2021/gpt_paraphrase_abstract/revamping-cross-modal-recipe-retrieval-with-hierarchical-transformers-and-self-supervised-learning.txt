Cross-modal recipe retrieval has become increasingly popular due to the significance of food in people's lives and the abundance of digital cooking recipes and food images. This study examines existing approaches for cross-modal recipe retrieval and proposes a simplified end-to-end model that utilizes established and high-performing encoders for text and images. The model introduced in this work is a hierarchical recipe Transformer that attentively encodes individual recipe components such as titles, ingredients, and instructions. Additionally, a self-supervised loss function is proposed, which is computed on pairs of individual recipe components and leverages semantic relationships within recipes. This loss function allows for training using both image-recipe and recipe-only samples. The design choices of the proposed method are thoroughly analyzed and validated through ablation studies. The results show that the proposed method achieves state-of-the-art performance in cross-modal recipe retrieval on the Recipe1M dataset. The code and models used in this study are publicly available for further research and development.