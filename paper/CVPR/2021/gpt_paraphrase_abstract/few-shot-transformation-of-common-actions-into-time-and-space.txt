This paper presents a novel approach for few-shot common action localization in time and space. The goal is to locate a specific action within a long untrimmed video using only a few trimmed support videos that contain the same action. Unlike previous methods, our approach does not rely on class labels, interval bounds, or bounding boxes. Instead, we introduce a new few-shot transformer architecture that combines joint commonality learning and localization prediction without the need for proposals. We evaluate our approach on re-organized datasets and demonstrate its effectiveness, even when the support videos are noisy. Additionally, we compare favorably against state-of-the-art methods in the few-shot and one-shot settings for common localization in time. Furthermore, we extend our few-shot transformer to enable common action localization per pixel. Overall, our approach offers a promising solution for few-shot common action localization in both time and space.