Deep Neural Networks (DNNs) often require large amounts of GPU memory for training on image/video databases. However, the finite physical memory of GPUs limits the potential image resolutions and batch sizes that can be used, thus affecting DNN performance. To address this issue without requiring GPU upgrades, the Gradient Check-Pointing (GCP) training method trades computation for additional memory beyond the existing GPU hardware. During the forward pass, GCP only stores a subset of intermediate tensors called Gradient Checkpoints (GCs). Then, during the backward pass, extra local forwards are performed to compute the missing tensors. The total memory cost of training is the sum of the memory cost of the gradient checkpoints and the maximum memory cost of local forwards. To achieve the highest possible memory cut-offs, optimal algorithms are needed for selecting GCs.Existing GCP approaches either rely on manually specifying GCs or use heuristics-based GC search on Linear Computation Graphs (LCGs). However, these approaches are not applicable to Arbitrary Computation Graphs (ACGs). In this paper, we propose theories and optimal algorithms for GC selection that can be applied to both LCGs and ACGs, achieving the maximum memory cut-offs. Extensive experiments demonstrate that our approach outperforms existing methods (only applicable to LCGs) and can be used with a wide range of network architectures, including popular ones like Alexnet, VGG, ResNet, Densenet, Inception Net, and even highly complex DNNs generated through Network Architecture Search. Our work enables GCP training on ACGs and reduces training memory usage by up to 80% with a reasonable increase in time overhead (30%-50%). The codes for our approach are available.