We assess the effectiveness of semi-supervised learning (SSL) on a realistic benchmark that involves imbalanced data and includes images from new classes. Our benchmark comprises two fine-grained classification datasets sampled from Aves and Fungi taxonomy. We discover that recently proposed SSL methods offer significant advantages and can effectively utilize out-of-class data to enhance performance when training deep networks from the beginning. However, their performance falls short compared to a transfer learning baseline, an alternative approach for learning from a limited number of examples. Moreover, in the transfer learning scenario, while existing SSL methods show improvements, the presence of out-of-class data often has a negative impact. In this scenario, the most robust strategy is standard fine-tuning followed by distillation-based self-training. Our findings indicate that semi-supervised learning with experts on realistic datasets may require different strategies than those currently prevalent in the literature. The graph in Figure 1 illustrates the accuracy of SSL algorithms on the Semi-Aves and Semi-Fungi datasets using different pre-trained models and in-class and out-of-class unlabeled data. It also includes the performances of the supervised baseline and supervised oracle. Transfer learning from experts proves to be far more effective than SSL from scratch, while SSL provides only modest gains in the transfer setting. Although out-of-class data is valuable when training from scratch, this is not the case when training from experts. For more details, refer to Tables 2 and 3.