Panoptic segmentation is a difficult task that involves simultaneously segmenting objects at the instance level and background contents at the semantic level. Current methods typically use a two-stage detection network for instance segmentation and a fully convolutional network for semantic segmentation. However, these methods require post-processing or additional modules to handle conflicts between the outputs of these networks, resulting in low efficiency, high memory consumption, and complexity in implementation. In order to simplify the process and reduce computation and memory costs, we propose a one-stage approach called Lightweight Panoptic Segmentation Network (LPSNet). Unlike existing methods, LPSNet does not involve a proposal, anchor, or mask head. Instead, it predicts a bounding box and semantic category for each pixel based on the feature map produced by an augmented feature pyramid. We also design a parameter-free head to merge the per-pixel bounding box and semantic prediction, resulting in panoptic segmentation output. LPSNet is both efficient in computation and memory and accurate in panoptic segmentation. Extensive experiments on COCO, Cityscapes, and Mapillary Vistas datasets demonstrate the promising effectiveness and efficiency of the proposed LPSNet.