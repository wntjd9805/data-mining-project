In a common classification task scenario, there is often a large amount of training data available, but only a small portion of it is labeled with class labels. The objective of semi-supervised training is to improve classification accuracy by utilizing not only the labeled data but also a large amount of unlabeled data. Previous studies have made significant advancements by considering the consistency between differently augmented labeled and unlabeled data. Building upon this, we propose a new unsupervised objective that focuses on the relationship between high confidence unlabeled data that share similarities. Our proposed Pair Loss minimizes the statistical distance between high confidence pseudo labels that have a similarity above a certain threshold. By combining the Pair Loss with techniques developed by the MixMatch family, our SimPLE algorithm achieves significant performance improvements compared to previous algorithms on CIFAR-100 and Mini-ImageNet, and performs comparably to state-of-the-art methods on CIFAR-10 and SVHN. Additionally, SimPLE outperforms state-of-the-art methods in transfer learning scenarios where models are initialized with weights pre-trained on ImageNet or DomainNet-Real. The code for SimPLE is publicly available on GitHub at github.com/zijian-hu/SimPLE.