We present Mask-ToF, a technique for reducing the occurrence of flying pixels (FP) in time-of-flight (ToF) depth captures. FPs are artifacts commonly found near depth edges, where the integration of light paths from both the object and its background leads to mixed light at a sensor pixel, resulting in inaccurate depth estimates. These inaccuracies can negatively impact 3D vision tasks downstream. Mask-ToF addresses this issue by learning an occlusion mask at the microlens level, creating a customized sub-aperture for each sensor pixel. This mask controls the mixture of foreground and background light on a per-pixel basis, encoding scene geometry directly into the ToF measurements. We develop a differentiable ToF simulator to train a convolutional neural network (CNN) to decode this information and generate high-quality, low-FP depth reconstructions. We evaluate Mask-ToF using a simulated light field dataset and validate the method with an experimental prototype. We fabricate the learned amplitude mask and design an optical relay system to virtually place it on a high-resolution ToF sensor. Our results show that Mask-ToF performs well on real data without the need for retraining, reducing FP counts by 50%.