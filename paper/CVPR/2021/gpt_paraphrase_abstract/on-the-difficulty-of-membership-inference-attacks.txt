Membership inference attacks on deep models have gained attention in recent studies, aiming to determine if a sample was used in the training process. However, these studies have primarily focused on reporting the accuracy, precision, and recall of the positive class (member class), neglecting the performance of these attacks on the negative class (non-member class). This paper highlights the misleading nature of the current reporting practices, as they fail to address the high false positive rate or false alarm rate (FAR) associated with these attacks. FAR represents the frequency at which the attack model misclassifies non-training samples as training ones, rendering MI attacks impractical. This impracticality is particularly significant in membership inference tasks where the majority of samples belong to the negative (non-training) class. Additionally, the paper reveals that existing MI attack models can only identify membership of misclassified samples with mediocre accuracy, which constitutes a small portion of training samples. The study explores previously unexplored features for membership inference, such as distance to the decision boundary and gradient norms, and demonstrates that deep models' responses are largely similar between train and non-train samples. Experimental analysis on various image classification tasks and model architectures illustrates that current state-of-the-art MI attacks cannot simultaneously achieve high accuracy and low FAR, even with multiple advantages given to the attacker. The source code for the study is available at https://github.com/shrezaei/MI-Attack.