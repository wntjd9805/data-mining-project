Most existing models for RGB-T semantic segmentation use primitive fusion strategies to combine features from RGB and thermal images. However, these strategies overlook the differences between the modalities and result in reduced discriminability of the fused features. To address this issue, we propose a novel approach called Adaptive-weighted Bi-directional Modality Difference Reduction Network (ABMDRNet). This approach first reduces the modality differences using a bi-directional image-to-image translation method and then selectively fuses the discriminative multi-modality features for semantic segmentation. We also introduce the Multi-Scale Spatial Context (MSC) and Multi-Scale Channel Context (MCC) modules to incorporate contextual information from both spatial and channel dimensions. Experimental results on the MFNet dataset demonstrate that our method outperforms existing approaches in terms of semantic segmentation accuracy.