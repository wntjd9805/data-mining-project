The generative adversarial network (GAN) is a crucial model for unsupervised machine learning. To train GAN's discriminators, various discriminator loss functions have been developed, all following a common structure of a sum of real and fake losses based on actual and generated data. However, this equally weighted sum of losses can lead to instability and mode collapse as it may favor one loss over the other. In this study, we propose a new type of discriminator loss function called adaptive weighted loss functions (aw-loss functions). By utilizing the gradients of the real and fake parts of the loss, we can dynamically adjust the weights to train the discriminator in a way that enhances GAN's stability. Our approach is applicable to any discriminator model with a loss composed of real and fake parts. Experimental results demonstrate the effectiveness of our loss functions in improving the performance of unconditional and conditional image generation tasks. In particular, our approach significantly outperforms baseline results in terms of Inception Scores (IS) and Fr√©chet Inception Distance (FID) metrics on CIFAR-10, STL-10, and CIFAR-100 datasets.