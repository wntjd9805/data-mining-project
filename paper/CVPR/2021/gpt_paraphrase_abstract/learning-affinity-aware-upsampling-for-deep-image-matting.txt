We demonstrate that learning affinity during the upsampling process is an effective and efficient method for utilizing pairwise interactions in deep networks. Traditionally, second-order features are used to establish adjacent relationships in dense prediction tasks by incorporating a learnable module after upsampling, such as non-local blocks. However, this approach requires additional propagation layers and can result in larger models. To address these limitations, we propose Affinity-Aware Upsampling (A2U), which leverages a unified mathematical framework to generalize existing upsampling operators into a second-order form. A2U generates upsampling kernels using a lightweight low-rank bilinear model and conditions them on second-order features. Moreover, our upsampling operator can also be applied to downsampling.We explore various implementations of A2U and evaluate their effectiveness on two detail-sensitive tasks: image reconstruction on a toy dataset and large-scale image matting. In the case of image matting, where affinity-based techniques are commonly used, our results on the Composition-1k matting dataset demonstrate a 14% relative improvement in the SAD metric compared to a strong baseline, while the number of parameters increases negligibly (< 0.5%). Furthermore, when compared to the state-of-the-art matting network, our approach achieves an 8% higher performance with only 40% of the model complexity.