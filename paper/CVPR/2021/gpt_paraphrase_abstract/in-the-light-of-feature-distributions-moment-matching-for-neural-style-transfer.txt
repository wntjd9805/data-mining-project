Style transfer is a technique used to transform the visual style of an image to match that of another image. Neural Style Transfer (NST) is a popular method that interprets style as a distribution in the feature space of a Convolutional Neural Network (CNN). However, current implementations of NST only partially align the feature distributions, leading to limitations in achieving the desired style. To address this, we propose a new approach that more accurately matches the distributions, resulting in a faithful reproduction of the desired style while remaining computationally efficient.Our approach involves adapting the dual form of Central Moment Discrepancy (CMD), which is commonly used in domain adaptation, to minimize the difference between the target style and the feature distribution of the output image. By considering higher-order centralized moments, our method extends existing NST techniques that only consider the first and second moments. This dual interpretation allows for a more precise matching of the style distribution.Through experiments, we demonstrate that our approach not only has strong theoretical properties but also produces visually superior style transfer results. Additionally, our method better disentangles style from semantic image content. Overall, our findings highlight the potential of more accurately matching feature distributions in achieving high-quality style transfer.