Siamese networks have become widely used in recent models for unsupervised visual representation learning. These networks aim to maximize the similarity between two augmented versions of an image while avoiding collapsing solutions. Surprisingly, our research shows that even simple Siamese networks can learn meaningful representations without the need for negative sample pairs, large batches, or momentum encoders. Although collapsing solutions can occur, we found that a stop-gradient operation is crucial in preventing this. We propose a hypothesis on the implications of the stop-gradient operation and provide proof-of-concept experiments to support it. Our approach, called "SimSiam," achieves competitive results on ImageNet and downstream tasks. By sharing our code, we hope to inspire others to reconsider the role of Siamese architectures in unsupervised representation learning.