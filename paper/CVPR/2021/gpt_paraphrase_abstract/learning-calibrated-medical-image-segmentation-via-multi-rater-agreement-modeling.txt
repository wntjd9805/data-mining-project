In the field of medical image analysis, it is common to gather multiple annotations from different clinical experts or raters to reduce diagnostic errors. However, computer vision practitioners often rely on ground-truth labels obtained from a majority vote or a single preferred rater, disregarding the valuable information contained in the raw multi-rater annotations. To address this, we propose MR-Net, a method that explicitly models multi-rater agreement or disagreement. MR-Net includes an expertise-aware inferring module (EIM) that incorporates the expertise level of individual raters as prior knowledge to generate high-level semantic features. Additionally, our approach can reconstruct multi-rater gradings from coarse predictions and leverages the multi-rater agreement cues to improve segmentation performance. To the best of our knowledge, our work is the first to produce calibrated predictions for medical image segmentation considering different expertise levels. We conducted extensive experiments on five medical segmentation tasks involving diverse imaging modalities, and our MR-Net outperformed state-of-the-art methods, demonstrating its effectiveness and applicability across various medical segmentation tasks. The source code for MR-Net is publicly available.