We explore the task of weakly-supervised audio-visual video parsing, where the goal is to segment a video into temporal event segments and predict the corresponding audible or visible event categories. This task is challenging because the training data only provides video-level event labels without specifying the temporal boundaries or modalities. Previous approaches have used these overall event labels to supervise both the audio and visual models, but we believe that this approach is problematic due to the asynchrony between audio and visual signals. For instance, in a basketball video, commentators may be speaking, but their visual presence may not be apparent. In this paper, we address this issue by leveraging the cross-modal correspondence between audio and visual signals. We generate reliable event labels for each modality by swapping audio and visual tracks with unrelated videos. If the original audio or visual data contain event cues, the event prediction based on the newly assembled data would still be highly confident. This approach helps protect our models from being misled by ambiguous event labels. Additionally, we propose using cross-modal audio-visual contrastive learning to induce temporal differences in attention models within videos. This encourages the model to select the current temporal segment from all possible context candidates. Our experiments demonstrate that our approach outperforms state-of-the-art methods by a significant margin.