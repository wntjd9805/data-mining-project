The goal of language-queried video actor segmentation is to accurately identify the actor performing specific actions described in a natural language query. Current methods use 3D CNNs to extract spatio-temporal features from video clips, but this approach introduces spatial misalignment from neighboring frames, leading to inaccurate segmentation. To address this, we propose a collaborative spatial-temporal encoder-decoder framework. Our framework includes a 3D temporal encoder to recognize the queried actions and a 2D spatial encoder to accurately segment the actors in the target frame. We introduce a Language-Guided Feature Selection (LGFS) module in the decoder to integrate spatial and temporal features. Additionally, we propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically combine linguistic features for effective feature interaction. Our method achieves superior performance on two benchmark datasets while requiring less computational resources than previous approaches.