Current deep learning techniques that estimate the relative camera pose between two images struggle to handle difficult scenarios like large camera movements, occlusions, and significant changes in perspective. These models still face challenges despite being trained on large supervised datasets. To overcome these limitations, we draw inspiration from methods that improve the regression of 2D and 3D keypoint locations by predicting a discrete distribution over these locations. Similarly, in this study, we propose improving camera pose regression by predicting a discrete distribution over camera poses instead. To accomplish this, we introduce DirectionNet, a model that estimates discrete distributions over the 5D relative pose space using a new parameterization to simplify the estimation problem. Specifically, DirectionNet decomposes the relative camera pose into a set of 3D direction vectors, which can be represented as points on a sphere. Consequently, DirectionNet predicts discrete distributions on the sphere as its output. We assess the performance of our model on challenging synthetic and real pose estimation datasets derived from Matterport3D and InteriorNet. The results are promising, demonstrating an approximate 50% reduction in error compared to direct regression methods.