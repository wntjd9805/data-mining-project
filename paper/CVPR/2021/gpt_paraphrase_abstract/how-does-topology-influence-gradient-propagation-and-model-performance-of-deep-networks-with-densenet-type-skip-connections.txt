DenseNets have been successful in achieving high accuracy in computer vision tasks due to their concatenation-type skip connections. This paper explores the relationship between the topology of these skip connections and gradient propagation, which ultimately affects the test performance of deep neural networks (DNNs) in a predictable manner. A new metric called NN-Mass is introduced to measure the efficiency of information flow in DNNs. The study also demonstrates that NN-Mass can be applied to other types of skip connections, such as those found in ResNets, Wide-ResNets (WRNs), and MobileNets. Therefore, NN-Mass can identify models with comparable accuracy despite having different size and computational requirements. The effectiveness of NN-Mass is validated through experiments on synthetic and real datasets, including MNIST, CIFAR-10, CIFAR-100, and ImageNet. Additionally, the closed-form equation of NN-Mass allows for the design of significantly compressed DenseNets (for CIFAR-10) and MobileNets (for ImageNet) without the need for time-consuming training or searching.