Predicting 3D human motion is crucial for understanding human behavior and has various applications in fields such as human tracking, HCI, and graphics. Previous research has focused on predicting future 3D joint locations based on past joint sequences, which performs better than predicting joint rotations. However, joint locations alone do not fully determine 3D human pose, leaving room for undefined degrees of freedom. We propose a different approach by predicting sparse locations on the body surface that correspond to motion capture markers, which provide additional information about human movement. To achieve this, we train a novel variational autoencoder called MOJO (More than OurJOints) using the AMASS dataset. MOJO generates motions based on latent frequencies in a DCT space, preserving the temporal resolution of the input motion and introducing high-frequency components. To address the accumulation of errors in motion prediction, we fit the SMPL-X body model to the predictions at each time step and project the solution back onto the space of valid bodies. Our approach produces state-of-the-art and realistic 3D body animations, as demonstrated by quantitative and qualitative experiments. The code for our method is available for research purposes at https://yz-cnsdqz.github.io/MOJO/MOJO.html.