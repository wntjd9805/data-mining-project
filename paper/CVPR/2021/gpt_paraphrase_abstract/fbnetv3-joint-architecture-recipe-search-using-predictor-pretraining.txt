Neural Architecture Search (NAS) has been successful in producing neural networks that outperform manually-designed ones. However, previous NAS methods only consider one set of training hyper-parameters, ignoring the potential for better combinations of architecture and training recipe. To address this limitation, we propose Neural Architecture-Recipe Search (NARS), which simultaneously searches for both architectures and their corresponding training recipes. NARS incorporates an accuracy predictor that evaluates architecture-recipe pairs, helping in sample selection and ranking. Additionally, we utilize architecture statistics, such as FLOP count, to pretrain the predictor, enhancing its efficiency and prediction reliability. After training the predictor, we perform fast evolutionary searches to generate architecture-recipe pairs for different resource constraints, resulting in FBNetV3. FBNetV3 is a family of compact neural networks that surpass the performance of both automatically and manually-designed competitors. For instance, FBNetV3 achieves the same accuracy as EfﬁcientNet and ResNeSt on ImageNet with significantly fewer FLOPs (2.0× and 7.1× less, respectively). Moreover, FBNetV3 demonstrates notable improvements in object detection tasks, achieving higher mAP despite having 18% fewer FLOPs and 34% fewer parameters compared to EfﬁcientNet-based equivalents.