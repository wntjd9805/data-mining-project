Video-based unsupervised domain adaptation (UDA) is more comprehensive than image-based UDA as it addresses domain shifts in both spatial representation and temporal dynamics. Existing approaches mainly focus on short-term modeling and alignment using frame-level or clip-level features, which may not be discriminative enough for video-based UDA tasks. To overcome these limitations, this paper introduces a self-supervised contrastive framework called spatio-temporal contrastive domain adaptation (STCDA), which establishes cross-modal domain alignment. STCDA learns joint clip-level and video-level representation alignment by leveraging effective representations obtained through self-supervised learning (SSL). Additionally, the proposed method incorporates spatio-temporal contrastive learning (STCL) to capture useful long-term feature representations for classification. STCL is trained in a self-supervised manner using contrastive clip/video pairs with positive or negative properties. Furthermore, the paper introduces a novel domain metric scheme called video-based contrastive alignment (VCA) to optimize category-aware video-level alignment and generalization between the source and target domains. Experimental results demonstrate that the proposed STCDA achieves state-of-the-art performance on several UDA benchmarks for action recognition.