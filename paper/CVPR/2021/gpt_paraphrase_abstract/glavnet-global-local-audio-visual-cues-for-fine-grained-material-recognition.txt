This paper focuses on the identification of materials using both auditory and visual perception. To achieve this, a new dataset called GLAudio is created, which includes the geometry of the object being struck and the corresponding sound captured through sound synthesis or real measurements. Unlike existing datasets, GLAudio also considers local geometries around different hitpoints, which are found to have a greater impact on sound and provide more cues for material recognition compared to global geometries. To effectively extract features from different modalities and combine them, a deep neural network called GLAVNet is proposed. GLAVNet consists of multiple branches and a fusion module designed for proper fusion of the extracted features. After training on GLAudio, GLAVNet achieves state-of-the-art performance in material identification and supports fine-grained material categorization.