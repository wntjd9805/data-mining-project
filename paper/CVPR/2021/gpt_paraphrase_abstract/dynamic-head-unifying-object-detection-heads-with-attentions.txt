The development of methods that combine object localization and classification in object detection has been extensive due to its complex nature. However, previous works have focused on improving performance in specific object detection heads without presenting a unified perspective. This paper introduces a novel dynamic head framework that integrates object detection heads with attention mechanisms. By incorporating multiple self-attention mechanisms, including scale-awareness, spatial-awareness, and task-awareness, the proposed approach enhances the representation ability of object detection heads without increasing computational load. Experimental results on the COCO benchmark demonstrate the effectiveness and efficiency of the dynamic head. Using a standard ResNeXt-101-DCN backbone, the proposed approach achieves a new state-of-the-art performance with an average precision of 54.0 AP, surpassing popular object detectors. The code for the proposed dynamic head framework is available at https://github.com/microsoft/DynamicHead.