We introduce DeepSurfels, a new hybrid scene representation that combines explicit and neural components to encode both geometry and appearance information. Unlike traditional representations, DeepSurfels can accurately capture high-frequency textures, facilitate real-time updates of appearance information, and seamlessly integrate with machine learning techniques. We also present an online appearance fusion pipeline that can be trained end-to-end, leveraging self-supervision through reprojection error from input images. Our method outperforms traditional texture mapping approaches and recent learning-based methods, offering faster runtime, improved generalization capabilities, and better scalability for larger scenes.