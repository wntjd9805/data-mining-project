Parsing indoor scenes from RGB images is a difficult task due to challenges such as occlusions, object distortion, and variations in viewpoint. This paper presents a new approach called the 3D-to-2D distillation framework, which addresses these challenges by leveraging 3D features extracted from large-scale 3D data repositories. The framework has three key contributions. Firstly, it distills 3D knowledge from a pretrained 3D network to supervise a 2D network during training. This allows the 2D network to learn simulated 3D features from 2D features without the need for actual 3D data. Secondly, a two-stage dimension normalization scheme is designed to calibrate the 2D and 3D features, enabling better integration between them. This helps improve the overall performance of the system. Lastly, a semantic-aware adversarial training model is proposed to extend the framework for training with unpaired 3D data. This enhances the versatility of the approach and makes it applicable to a wider range of datasets.Extensive experiments on different datasets including ScanNet-V2, S3DIS, and NYU-v2 demonstrate the superiority of the proposed approach. The results also indicate that the 3D-to-2D distillation technique leads to improved generalization of the model. Overall, this work presents a novel and effective solution for indoor scene semantic parsing from RGB images.