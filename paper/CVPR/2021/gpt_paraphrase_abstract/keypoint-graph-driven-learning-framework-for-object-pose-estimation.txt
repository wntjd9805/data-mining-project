Many recent methods for estimating the 6D pose of objects have used 3D models to generate synthetic images for training. However, because of the differences between real and synthetic images, networks trained only on synthetic data struggle to accurately estimate pose in real images. To address this, we propose a solution that focuses on making the network robust to different domains, rather than trying to make synthetic images more similar to real images. We introduce a Domain Adaptive Keypoints Detection Network (DAKDN) that includes a domain adaptation layer to minimize the differences in deep features between synthetic and real images. One challenge is the lack of ground truth labels for real images, but we leverage the invariant geometry structure of keypoints to optimize DAKDN for 6D pose estimation across domains. DAKDN uses a Graph Convolutional Network (GCN) block to learn the geometry structure from synthetic images and employs the GCN to guide the training for real images. The 6D poses of objects are calculated using the Perspective-n-Point (PnP) algorithm based on the predicted keypoints. Our experiments demonstrate that our method outperforms existing approaches without the need for manual pose labels and performs comparably to approaches that use manual pose labels.