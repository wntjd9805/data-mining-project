This paper presents a deep learning model that utilizes self-supervision to complete 3D point clouds by estimating missing parts and their surrounding context. The model combines local and global information in an embedding. A denoising pretext task provides local cues, detached from high-level semantics and shared across multiple classes. Contrastive learning is used to maximize agreement between different variants of the same shape with missing portions, generating a representation that captures the global appearance of the shape. This combined embedding has category-agnostic properties, allowing for better generalization to unseen categories during training. Additionally, when decoding the joint representation, the model blends the reconstructed missing part with the partial shape by considering the known surrounding region and reconstructing it as an auxiliary objective. Extensive experiments on the ShapeNet dataset demonstrate the effectiveness of each component of the method, resulting in state-of-the-art results. The approach is able to work on novel categories without relying on classification, shape symmetry priors, or adversarial training procedures.