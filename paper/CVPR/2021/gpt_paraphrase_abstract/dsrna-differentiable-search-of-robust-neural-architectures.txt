In deep learning, the design of deep neural network architectures is crucial for achieving high accuracy. Many methods have been proposed to automatically search for high-performance architectures. However, these searched architectures are vulnerable to adversarial attacks, where slight changes to input data can significantly impact prediction outcomes. To address this problem, we propose methods for conducting differentiable search for robust neural architectures. Our methods define two differentiable metrics to measure the robustness of architectures, based on certified lower bound and Jacobian norm bound. We then search for robust architectures by maximizing these metrics. Unlike previous approaches that improve robustness implicitly through adversarial training and random noise injection, our methods explicitly and directly maximize robustness metrics to obtain robust architectures. We evaluate and verify the robustness of our methods on CIFAR-10, ImageNet, and MNIST datasets using game-based and verification-based evaluation. Experimental results demonstrate that our methods are more robust to norm-bound attacks compared to several robust NAS baselines, more accurate than baselines in the absence of attacks, and have significantly higher certified lower bounds than baselines.