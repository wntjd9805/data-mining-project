Although deep learning methods for video deraining have made significant progress, they still have two main limitations. Firstly, they struggle to accurately represent the characteristics of rain layers in rainy videos. Rain layers have distinct visual properties in terms of direction, scale, and thickness in the spatial dimension, as well as causal properties such as velocity and acceleration in the temporal dimension. These properties can be effectively captured using spatial-temporal processes in statistics. Secondly, existing deep learning methods heavily rely on labeled training data, where the rain layers are artificially generated. This mismatch between synthetic and real data leads to poor performance when applied to real-world scenarios. To address these issues, this paper introduces a novel semi-supervised video deraining method. The proposed method utilizes a dynamic rain generator to accurately depict the intrinsic characteristics of rain layers. The dynamic generator comprises an emission model and a transition model, both parameterized by deep neural networks. The emission model captures the spatial appearance of rain streaks, while the transition model captures their temporal dynamics. Furthermore, different prior formats are designed for the labeled synthetic data and unlabeled real data to maximize the utilization of their shared knowledge. To learn the model, a MonteCarlo-based Expectation-Maximization (EM) algorithm is employed. Extensive experiments are conducted to validate the effectiveness of the proposed semi-supervised deraining model.