Network quantization is a technique that allows deep neural networks to be run on edge devices using low-precision arithmetic, improving inference efficiency. However, using aggressively low-bit quantization schemes, such as 2-bit, for complex tasks like object detection is still challenging due to significant performance degradation and unverifiable efficiency on common hardware. In this study, we propose a solution called AQD (Accurate Quantized object Detection) that eliminates floating-point computation by utilizing fixed-point operations in all layers, including convolutional layers, normalization layers, and skip connections. This enables inference to be performed using integer-only arithmetic. To demonstrate the trade-off between latency and accuracy, we apply our proposed methods on RetinaNet and FCOS. Experimental results using the MS-COCO dataset show that AQD achieves comparable or even better performance than the full-precision counterpart under extremely low-bit schemes. This practical value makes our approach significant. The source code and models for AQD are available at: https://github.com/aim-uofa/model-quantization.