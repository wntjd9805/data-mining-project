Our objective is to understand how humans interact with their surroundings and use that knowledge to enable virtual characters to interact in a similar manner. To achieve this, we introduce a new model called POSA (Pose with prOximitieS and contActs) that captures the physical relationships between humans and scenes. POSA is designed to generalize to different scenes and represents interactions from a body-centric perspective. It enhances the SMPL-X parametric human body model by encoding the probability of contact between each vertex and the scene surface, as well as the corresponding semantic label of the scene. We train POSA using a Variational Autoencoder (VAE) conditioned on the SMPL-X vertices, using the PROX dataset that contains SMPL-X meshes of people interacting with 3D scenes, along with the scene semantics from the PROX-E dataset. We demonstrate the effectiveness of POSA through two applications. Firstly, we use it to automatically position 3D scans of people in scenes. By fitting a SMPL-X model to the scan and leveraging POSA, we identify the most suitable placement in the 3D scene based on the expected contact relationships. Our perceptual study shows significant improvement compared to existing methods in this task. Secondly, we show that POSA's learned representation of body-scene interaction enhances monocular human pose estimation, resulting in more accurate estimations that align with the 3D scene. Our model and code are publicly available for research purposes at https://posa.is.tue.mpg.de.