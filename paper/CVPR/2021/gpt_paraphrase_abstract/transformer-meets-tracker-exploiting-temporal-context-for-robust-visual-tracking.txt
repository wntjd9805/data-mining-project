This paper addresses the issue of overlooking temporal contexts in video object tracking and proposes a transformer architecture to bridge the gap between individual frames. Unlike traditional use of transformers in natural language processing, the encoder and decoder are separated into two parallel branches and integrated into Siamese-like tracking pipelines. The transformer encoder enhances target templates through attention-based feature reinforcement, improving the quality of the tracking model. The transformer decoder propagates tracking cues from previous templates to the current frame, aiding in the object searching process. The proposed transformer-assisted tracking framework is trained end-to-end and achieves superior performance compared to existing trackers. When combined with the discriminative tracking pipeline, the method sets new state-of-the-art records on popular tracking benchmarks.