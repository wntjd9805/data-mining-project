Despite significant progress in neural architecture search (NAS), most methods focus on finding a single accurate and robust architecture. However, model ensembles have been shown to have better generalization capability and performance compared to standalone models. In this paper, we propose a novel approach called one-shot neural ensemble architecture search (NEAS) to simultaneously search for multiple diverse models. This approach faces two challenges: an enlarged search space and increased complexity. To address the first challenge, we introduce a diversity-based metric that considers both potentiality and diversity of candidate operators to guide search space shrinking. For the second challenge, we enable a new search dimension to learn layer sharing among different models for efficiency purposes. Our experiments on ImageNet demonstrate that NEAS improves the supernet's capacity of ranking ensemble architectures and leads to better search results. The discovered architectures outperform state-of-the-art models such as MobileNetV3 and EfficientNet families under aligned settings. Additionally, we evaluate the generalization ability and robustness of our searched architecture on the COCO detection benchmark and achieve a 3.1% improvement on average precision compared to MobileNetV3. The codes and models are available for further exploration.