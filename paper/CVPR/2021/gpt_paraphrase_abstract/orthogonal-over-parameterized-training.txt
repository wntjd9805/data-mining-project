The architecture and training algorithm of a neural network determine its inductive bias. Effectively training a neural network is crucial for achieving good generalization. We introduce a new framework called orthogonal over-parameterized training (OPT) that minimizes the hyperspherical energy, which represents the diversity of neurons on a hypersphere. OPT maintains the minimum hyperspherical energy during training, resulting in improved empirical generalization. Instead of changing the randomly initialized weights of neurons, OPT learns an orthogonal transformation that is applied to these neurons. We explore different methods for learning this orthogonal transformation, such as unrolling orthogonalization algorithms, using orthogonal parameterization, and designing orthogonality-preserving gradient descent. To enhance scalability, we propose stochasticOPT, which performs the orthogonal transformation stochastically on some dimensions of neurons. Interestingly, OPT highlights the importance of learning an appropriate coordinate system for neurons in achieving better generalization. We provide insights into why OPT leads to improved generalization and conduct extensive experiments that demonstrate the superiority of OPT compared to standard training methods.