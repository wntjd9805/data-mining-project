Quantization is a popular method for compressing and accelerating neural networks. Data-free quantization, which uses synthetic data for calibration, has shown promise but suffers from homogenization issues that result in reduced performance. To address this problem, we propose the DiverseSample Generation (DSG) scheme. DSG relaxes the alignment of feature statistics at the distribution level and enhances specific layers for different data samples. Our scheme can be applied to various quantization methods, including AdaRound. We evaluate DSG on large-scale image classification tasks and consistently achieve significant improvements across different network architectures and quantization methods, especially at lower bit levels. Additionally, models calibrated with synthetic data perform similarly to those calibrated with real data and even outperform them in some cases.