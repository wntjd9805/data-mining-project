We examine and analyze the latent style space of Style-GAN2, an advanced framework for generating images, by utilizing models pretrained on various datasets. Firstly, we demonstrate that the StyleSpace, which consists of channel-wise style parameters, is considerably more disentangled compared to other intermediate latent spaces explored in previous studies. Secondly, we present a technique for discovering numerous style channels, each of which independently controls a specific visual attribute in a localized and disentangled manner. Thirdly, we propose a straightforward approach for identifying style channels that govern a particular attribute, using either a pretrained classifier or a small set of example images. Manipulating visual attributes through these StyleSpace controls is shown to be more disentangled than the approaches proposed in earlier research, supported by the newly introduced Attribute Dependency metric. Finally, we showcase the practicality of using StyleSpace controls for manipulating real images. These findings pave the way for semantically meaningful and well-disentangled image manipulations through simple and intuitive interfaces.