We have developed a real-time method for estimating a high dynamic range (HDR) environment map from a low dynamic range (LDR) camera image with a narrow field of view. This technique allows for realistic reflections and shading on virtual objects of various material finishes, such as mirrors or diffuse surfaces, when rendered into a real environment using augmented reality. Our approach utilizes a convolutional neural network called EnvMapNet, which has been trained end-to-end using two new loss functions: ProjectionLoss for the generated image and ClusterLoss for adversarial training. By comparing our results to other state-of-the-art methods both qualitatively and quantitatively, we demonstrate that our algorithm significantly reduces the directional error of estimated light sources by over 50% and achieves a 3.7 times lower Frechet Inception Distance (FID). Additionally, we have developed a mobile application that can run our neural network model in less than 9 milliseconds on an iPhone XS. This application is capable of rendering visually coherent virtual objects in real-time within previously unseen real-world environments.