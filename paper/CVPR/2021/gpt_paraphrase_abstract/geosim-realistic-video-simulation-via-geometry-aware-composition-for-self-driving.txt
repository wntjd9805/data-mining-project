Scalable simulation of sensors is a challenging problem in safety-critical domains like self-driving. Current image simulation methods lack photorealism and fail to model the 3D environment and dynamic objects accurately, leading to a loss of high-level control and physical realism. In this study, we introduce GeoSim, a geometry-aware image composition process that generates new urban driving scenarios by incorporating dynamic objects from different scenes into existing images and rendering them at different angles. To achieve this, we first create a diverse collection of 3D objects with realistic geometry and appearance using sensor data. During the simulation, we employ a novel geometry-aware simulation-by-composition technique that proposes realistic object placements, generates new views of dynamic objects from the object collection, and blends the rendered segments into the image. The resulting synthetic images are realistic, traffic-aware, and geometrically consistent, enabling our approach to handle complex scenarios. We demonstrate the effectiveness of our method in two important applications: realistic video simulation across multiple camera sensors and synthetic data generation for enhancing segmentation tasks. For high-resolution video results, please refer to the link provided: https://tmux.top/publication/geosim/.