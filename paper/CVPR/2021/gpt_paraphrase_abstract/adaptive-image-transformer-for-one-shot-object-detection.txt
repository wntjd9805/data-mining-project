One-shot object detection involves identifying all instances of a specific class within a target image, using a query image patch. The challenge arises when the class label of the query patch and its examples are not available in the training data. We propose using language translation as a means to enhance metric-learning-based detection methods. By emulating the process of language translation, we can adaptively translate the feature of each object proposal to better correlate with the given query feature, aiding in discriminating the class-similarity among proposal-query pairs. To achieve this, we introduce the Adaptive Image Transformer (AIT) module, which utilizes an attention-based encoder-decoder architecture to explore both intra-coder and inter-coder attention simultaneously. The adaptability of our design proves to be effective in addressing the challenges of one-shot learning. With the informative attention cues provided by our model, it performs exceptionally well in predicting the class-similarity between target image proposals and the query image patch. Despite its simplicity in concept, our model significantly outperforms a state-of-the-art technique, achieving improved unseen-class object classification results on the PASCAL-VOC and MS-COCO benchmark datasets. Specifically, the mean average precision (mAP) increases from 63.8 to 72.2, and the average precision at 50% intersection over union (AP50) increases from 22.0 to 24.3.