This paper introduces a new framework called Neighborhood Contrastive Learning (NCL) to address the task of Novel Class Discovery (NCD). NCD involves discovering new classes in unlabeled samples using a labeled dataset with known classes. The authors exploit the characteristics of NCD to develop NCL, which focuses on learning discriminative representations that improve clustering performance. The authors make two main contributions. Firstly, they observe that a feature extractor trained on the labeled set produces representations where a generic query sample and its neighbors are likely to belong to the same class. Based on this observation, they use contrastive learning to retrieve and aggregate pseudo-positive pairs, encouraging the model to learn more discriminative representations. Secondly, they find that most instances are easily discriminated by the network, leading to a lower contrastive loss. To address this issue, they propose generating hard negatives by combining labeled and unlabeled samples in the feature space. Experimental results demonstrate that these two approaches significantly enhance clustering performance, resulting in the model outperforming state-of-the-art methods by a large margin. For example, the model achieves a clustering accuracy improvement of 13% on CIFAR-100 and 8% on ImageNet.