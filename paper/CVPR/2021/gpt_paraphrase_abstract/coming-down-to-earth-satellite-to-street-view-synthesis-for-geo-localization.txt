The objective of cross-view image based geo-localization is to determine the location of a street view image by comparing it to a collection of geo-tagged satellite images. This task is known for being difficult due to the significant differences in perspective and appearance between the two domains. We demonstrate that this disparity can be explicitly addressed by training the model to generate realistic street views from satellite inputs. Based on this insight, we propose a new multi-task architecture that combines image synthesis and retrieval. This approach allows the network to learn latent feature representations that are beneficial for retrieval by utilizing them to generate images in both input domains. To our knowledge, our method is the first to generate realistic street views from satellite images and simultaneously localize the corresponding query street view in an end-to-end manner. In our experiments, we achieve state-of-the-art performance on the CVUSA and CVACT benchmarks. Additionally, we provide compelling qualitative results for satellite-to-street view synthesis.