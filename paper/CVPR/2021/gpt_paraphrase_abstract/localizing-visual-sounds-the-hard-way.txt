This study aims to localize sound sources in videos without the need for manual annotations. The main contribution of this work is the discovery that training a network to distinguish challenging image fragments, even when the object producing the sound is present in the image, significantly improves localization performance. This is achieved by incorporating a mechanism that automatically mines hard samples and incorporates them into a contrastive learning formulation. The effectiveness of the proposed algorithm is demonstrated by achieving state-of-the-art performance on the widely used Flickr SoundNet dataset. Additionally, a new benchmark called VGG-Sound Source (VGG-SS) is introduced, which provides bounding box annotations for sound sources in the recently-introduced VGG-Sound dataset. VGG-SS is a larger dataset compared to existing ones, containing 5,000 videos across 200 categories and being video-based. The algorithm developed in this study also outperforms several baselines on the VGG-SS benchmark. All code and datasets related to this research can be accessed at http://www.robots.ox.ac.uk/Ëœvgg/research/lvs/.