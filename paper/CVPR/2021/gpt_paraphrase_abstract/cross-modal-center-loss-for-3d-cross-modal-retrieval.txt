This paper introduces a novel approach to cross-modal retrieval, which aims to learn features that are discriminative and invariant to different data modalities. Unlike existing methods that rely on features extracted by offline networks, the proposed approach jointly trains the components of the retrieval framework with metadata to find optimal features. The framework is updated using three loss functions: a cross-modal center loss to reduce cross-modal discrepancies, a cross-entropy loss to maximize inter-class variations, and a mean-square-error loss to minimize modality variations. The proposed cross-modal center loss minimizes the distances between features of objects belonging to the same class across all modalities. The approach is evaluated on retrieval tasks involving 2D images, 3D point clouds, and mesh data, and it outperforms state-of-the-art methods on both cross-modal and in-domain retrieval for 3D objects on the ModelNet10 and ModelNet40 datasets.