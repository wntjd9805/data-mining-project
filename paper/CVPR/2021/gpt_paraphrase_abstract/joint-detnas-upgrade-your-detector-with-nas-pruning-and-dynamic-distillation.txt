We introduce Joint-DetNAS, a unified framework for object detection that combines Neural Architecture Search (NAS), pruning, and Knowledge Distillation (KD). Unlike traditional approaches that use these techniques sequentially, Joint-DetNAS optimizes them together. The algorithm consists of two main processes: student morphism and dynamic distillation. Student morphism optimizes the student's architecture and eliminates redundant parameters by adopting a weight inheritance strategy, which allows the student to update its architecture while leveraging the predecessor's weights. This accelerates the search process significantly. In dynamic distillation, an elastic teacher pool is trained using a progressive shrinking strategy, enabling the sampling of teacher detectors without additional cost in subsequent searches. Our algorithm directly outputs a high-performance student detector without the need for additional training, using a base detector as input. Experimental results show that Joint-DetNAS outperforms the traditional sequential approach by a large margin. When given a classic R101-FPN as the base detector, Joint-DetNAS increases its mAP from 41.4 to 43.9 on MSCOCO and reduces latency by 47%. This performance is comparable to the state-of-the-art EfÔ¨ÅcientDet while requiring less search cost. We believe that our approach can offer the community a new method for jointly optimizing NAS, KD, and pruning.