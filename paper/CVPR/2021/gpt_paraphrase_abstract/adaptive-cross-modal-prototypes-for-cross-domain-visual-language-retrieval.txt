This paper addresses the problem of visual-text retrieval in a practical scenario where labeled visual data with text descriptions are available in one domain, but only unlabeled visual data without text descriptions are available in another domain. The authors propose a framework called ADAPTIVE CROSS-MODAL PROTOTYPES to enable retrieval in the target domain by learning cross-modal visual-text representations while minimizing distribution shifts between the two domains. The framework is based on two main ideas: encoding the inductive bias that the learned representations should be compositional with respect to concepts in each modality, achieved through clustering pretrained features and a regularization scheme, and maximizing mutual information between cross-modal representations in the source and target domains during learning. The proposed approach outperforms existing methods for cross-domain visual-text retrieval in both images and videos.