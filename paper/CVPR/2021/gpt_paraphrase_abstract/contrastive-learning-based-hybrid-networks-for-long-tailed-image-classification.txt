Learning discriminative image representations is crucial for long-tailed image classification, as it facilitates the learning process for imbalanced cases. This study explores effective supervised contrastive learning strategies to improve image representations from imbalanced data and enhance classification accuracy. The proposed approach involves a hybrid network structure comprising a supervised contrastive loss for feature learning and a cross-entropy loss for classifier learning. The learning process transitions progressively from feature learning to classifier learning, based on the belief that better features lead to better classifiers. Two variants of contrastive loss are examined for feature learning: supervised contrastive (SC) loss and prototypical supervised contrastive (PSC) learning. Both variants aim to bring samples from the same class closer together in the normalized embedding space while pushing samples from different classes further apart. SC loss incorporates positive samples from the same class into the state-of-the-art unsupervised contrastive loss, while PSC learning addresses memory consumption limitations in SC loss. Extensive experiments on three long-tailed classification datasets demonstrate the advantages of the proposed contrastive learning-based hybrid networks for long-tailed classification.