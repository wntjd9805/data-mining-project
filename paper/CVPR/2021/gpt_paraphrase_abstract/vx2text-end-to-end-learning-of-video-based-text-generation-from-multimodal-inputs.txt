We introduce VX2TEXT, a framework that generates text from multimodal inputs such as video, text, speech, or audio. To utilize transformer networks, which are known for their language modeling capabilities, each modality is first converted into language embeddings using a trainable tokenizer. This allows our approach to perform multimodal fusion in the language space, eliminating the need for ad-hoc cross-modal fusion modules. To overcome the non-differentiability of tokenization on continuous inputs like video or audio, we employ a relaxation scheme that enables end-to-end training. Unlike previous models that only have encoders, our network includes a decoder that generates open-ended text by fusing the multimodal embeddings from the language encoder. This makes our approach fully generative and applicable to various "video+x to text" tasks without requiring specialized network heads for each task. The proposed framework is not only conceptually straightforward but also remarkably effective. Experimental results demonstrate that our single architecture outperforms the state-of-the-art on three video-based text-generation tasks: captioning, question answering, and audio-visual scene-aware dialog.