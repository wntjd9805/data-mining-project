Our research explores the learning of a depth-specific structural representation from sketches rather than realistic images. This representation captures essential depth estimation features while disregarding irrelevant style information. We introduce S2R-DepthNet, which can effectively generalize to unseen real-world data, despite being trained only on synthetic data. S2R-DepthNet comprises three modules: 1) Structure Extraction (STE) module, which extracts a domain-invariant structural representation by separating an image into structure and style components.2) Depth-specific Attention (DSA) module, which learns task-specific knowledge to suppress depth-irrelevant structures for improved depth estimation and generalization.3) Depth Prediction (DP) module, which predicts depth from the depth-specific representation.Remarkably, our method outperforms state-of-the-art unsupervised domain adaptation methods that utilize real-world images for training, even without access to real-world images. Furthermore, when provided with a small amount of labeled real-world data, our approach achieves state-of-the-art performance in the semi-supervised setting.