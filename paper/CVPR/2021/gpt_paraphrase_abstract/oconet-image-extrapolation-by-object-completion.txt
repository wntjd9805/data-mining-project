This study addresses the challenge of extrapolating images beyond their original field of view. Current methods struggle to extrapolate images with prominent foreground objects, and are often limited to specific objects like humans. However, they perform well on indoor/outdoor scenes. To overcome these limitations, the researchers propose a new method called OCONet (Object COmpletion Networks). OCONet utilizes an encoder-decoder architecture trained with adversarial loss to predict the texture and extent of foreground objects. This is achieved by conditioning the object completion network on the object's class. The predicted extent is represented as a signed-distance field. In a separate step, the background is extended, and the object is composited on top using the predicted mask. The researchers demonstrate through both qualitative and quantitative evaluations that their method outperforms existing state-of-the-art image extrapolation techniques, particularly in challenging scenarios.