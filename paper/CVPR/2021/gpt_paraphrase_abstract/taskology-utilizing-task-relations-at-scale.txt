This abstract discusses the problem of scene understanding in computer vision tasks, such as object classification, detection, scene segmentation, and depth estimation. The authors propose leveraging the relationships between these tasks to improve their performance and reduce the need for labeled data. By training the tasks jointly and supervising each other through consistency losses, they can achieve better results and incorporate unsupervised or simulated data. The authors also introduce a distributed joint training algorithm that allows learning across multiple tasks or with large amounts of input data. They demonstrate the effectiveness of their framework on various tasks, including depth prediction, semantic segmentation, motion estimation, and object tracking. Overall, their approach shows improved performance, particularly in scenarios with limited labeled data.