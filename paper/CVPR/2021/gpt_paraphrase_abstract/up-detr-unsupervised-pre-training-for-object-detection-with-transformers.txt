Object detection with transformers (DETR) has achieved comparable performance to Faster R-CNN by utilizing a transformer encoder-decoder architecture. Building upon the success of pre-training transformers in natural language processing, we propose a pretext task called random query patch detection to pre-train DETR in an unsupervised manner, resulting in UP-DETR. In this approach, we randomly extract patches from the input image and use them as queries for the decoder. The model is trained to detect these query patches within the original image. To address key challenges during pre-training, namely multi-task learning and multi-query localization, we employ two strategies. Firstly, to balance the preferences between classification and localization in the pretext task, we freeze the CNN backbone and introduce a patch feature reconstruction branch that is jointly optimized with patch detection. Secondly, we extend UP-DETR from single-query patch to multi-query patches by incorporating object query shuffling and attention masking for multi-query localization. Experimental results demonstrate that UP-DETR significantly improves the performance of DETR, achieving faster convergence and higher average precision in tasks such as object detection, one-shot detection, and panoptic segmentation. The code and pre-training models are available at https://github.com/dddzg/up-detr.