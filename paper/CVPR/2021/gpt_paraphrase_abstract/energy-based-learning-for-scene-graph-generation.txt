Conventional methods for generating scene graphs in computer vision rely on cross-entropy losses, which treat objects and relationships as separate entities. However, this approach overlooks the inherent structure in the output space, which is crucial in structured prediction problems. In this study, we propose a new energy-based learning framework for scene graph generation. This framework effectively incorporates the structure of scene graphs in the output space, providing an inductive bias that enables models to learn efficiently with limited labeled data. By training existing state-of-the-art models using our energy-based framework, we achieve significant performance improvements of up to 21% and 27% on the Visual Genome and GQA benchmark datasets, respectively. Additionally, we demonstrate the efficiency of our proposed framework by showcasing superior performance in zero- and few-shot settings where data is scarce.