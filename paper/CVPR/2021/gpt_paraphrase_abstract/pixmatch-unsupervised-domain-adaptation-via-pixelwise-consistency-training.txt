Unsupervised domain adaptation is a promising technique for computer vision tasks like semantic segmentation, which require costly and time-consuming data annotation. The idea is to train models on annotated images from a simulated domain and deploy them on real domains. In this study, we propose a new framework for unsupervised domain adaptation based on target-domain consistency training. Essentially, we believe that for a model to perform well on the target domain, its output should remain consistent when the input undergoes small changes. To achieve this, we introduce a new loss term that enforces pixelwise consistency between the model's predictions on a target image and a perturbed version of the same image. Our approach is simpler, easier to implement, and more memory-efficient during training compared to popular adversarial adaptation methods. Through experiments and extensive analysis, we demonstrate that our straightforward approach achieves excellent results on two challenging synthetic-to-real benchmarks: GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes.