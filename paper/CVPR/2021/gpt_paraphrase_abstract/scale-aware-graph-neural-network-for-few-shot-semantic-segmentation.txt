Few-shot semantic segmentation (FSS) aims to segment objects from unseen classes using only a few annotated support images. Existing FSS methods utilize support prototypes or rely on heuristic multi-scale feature fusion to identify query objects. However, these methods do not effectively utilize the relationships between multi-scale features, resulting in inaccurate localization of query objects. To address this challenge, we propose an end-to-end scale-aware graph neural network (SAGNN) for FSS. SAGNN builds a scale-aware graph by considering support-induced multi-scale query features as nodes and modeling each edge as the interaction between connected nodes. Through progressive message passing, SAGNN captures cross-scale relations and handles object variations, leading to more precise node embeddings and accurate foreground object prediction. Additionally, we introduce a self-node collaboration mechanism to leverage location relations across scales, enabling SAGNN to perceive different resolutions of the same objects. Experimental results on PASCAL-5i and COCO-20i datasets demonstrate that SAGNN achieves state-of-the-art performance in FSS.