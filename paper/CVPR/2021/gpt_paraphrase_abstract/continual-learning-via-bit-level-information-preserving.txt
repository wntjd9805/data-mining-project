Continual learning, which involves learning different tasks sequentially, still faces challenges such as significant forgetting and high memory cost. In this study, we examine the process of continual learning from an information theory perspective and identify that forgetting occurs when a model loses information gain on its parameters from previous tasks while learning new ones. Based on this insight, we propose a new approach called Bit-Level Information Preserving (BLIP) that preserves information gain on model parameters by updating them at the bit level using parameter quantization. BLIP trains a neural network with weight quantization on new tasks and determines which bits to freeze based on the estimated information gain from task data, thus preventing forgetting. We conduct extensive experiments across various tasks, including classification and reinforcement learning, and our results demonstrate that BLIP outperforms previous state-of-the-art methods or achieves comparable performance. Notably, BLIP achieves nearly zero forgetting while maintaining constant memory overhead throughout continual learning.