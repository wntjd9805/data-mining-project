Many existing explanation methods in deep learning focus on mapping importance estimates for a model's prediction back to the original input space. However, these visual explanations often fail to fully capture the model's underlying concept. Additionally, without understanding the semantic concept of the model, it is challenging to intervene and modify the model's behavior based on its explanations, which is known as Explanatory Interactive Learning. To address these limitations, we propose intervening on a Neuro-Symbolic scene representation, which allows for revisions to be made on the semantic level. For example, instead of focusing on the color of an object, the model can be instructed to consider other factors in decision-making. We have created a new dataset called CLEVR-Hans, which contains complex compositions of various objects to better capture real-world scenarios. Our experiments on CLEVR-Hans demonstrate that our semantic explanations, provided at a per-object level, can identify confounding factors that cannot be identified using visual explanations alone. Importantly, feedback on the semantic level enables us to revise the model's focus and reduce dependencies on these factors.