Convolutional video models have much higher computational complexity compared to image-level models. Due to limited computational resources, it is currently not possible to train long video sequences end-to-end. Instead, videos are typically split into clips, resulting in incomplete temporal information flow. Taking inspiration from natural language processing techniques for dealing with long sentences, we propose a method that treats videos as sequential fragments satisfying the Markov property. This method, called progressive training (PGT), trains the video as a whole by progressively propagating information through the temporal dimension in multiple steps. PGT enables the end-to-end training of long videos with limited resources and ensures effective information transmission. We empirically demonstrate that PGT is a general and robust training method, leading to significant performance improvements on various models and datasets. For instance, the proposed method improves the SlowOnly network by 3.7 mAP on Cha-rades and 1.9 top-1 accuracy on Kinetics, with minimal additional parameters and computation requirements. The code for PGT is available at: https://github.com/BoPang1996/PGT.