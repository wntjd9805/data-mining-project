In recent years, knowledge distillation has proven to be an effective method for compressing models. By using this approach, lightweight student models are able to acquire knowledge from larger teacher models. However, existing distillation methods for detection tasks have limitations in terms of generalization and reliance on ground truth. These methods overlook the valuable relationship information between instances. To address this issue, we propose a new distillation method called general instance distillation (GID) that focuses on discriminative instances without considering their positive or negative distinction based on ground truth. Our approach incorporates a general instance selection module (GISM) that utilizes feature-based, relation-based, and response-based knowledge for distillation. Extensive results demonstrate that our student model achieves a significant improvement in average precision (AP) and even surpasses the performance of the teacher model in various detection frameworks. Specifically, when applied to the COCO dataset, our GID approach combined with RetinaNet and ResNet-50 achieves a mAP of 39.1%, surpassing the baseline of 36.2% by 2.9%. It even outperforms the ResNet-101 based teacher model, which achieves an AP of 38.1%.