We aim to improve the accuracy of dense depth map estimation from a single RGB input image. Our approach involves incorporating global information processing into an encoder-decoder convolutional neural network architecture. We introduce a transformer-based architecture block called AdaBins, which divides the depth range into bins and adaptively estimates their center values for each image. The final depth values are obtained through linear combinations of the bin centers. Our experiments demonstrate significant enhancements in depth estimation compared to existing methods on popular depth datasets, across all evaluation metrics. Furthermore, we validate the effectiveness of AdaBins through an ablation study and provide the code and pre-trained weights of our state-of-the-art model.