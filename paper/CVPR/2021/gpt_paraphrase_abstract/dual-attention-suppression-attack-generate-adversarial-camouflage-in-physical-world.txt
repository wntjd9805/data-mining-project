Adversarial examples pose a threat to deep learning models, and physical adversarial examples have gained significant attention as they are more dangerous for practical systems. However, existing methods for generating physical adversarial examples have limitations in terms of attacking different models and appearing visually suspicious. This paper introduces the DualAttention Suppression (DAS) attack, which aims to generate visually-natural physical adversarial camouflages with strong transferability by suppressing both model and human attention. The DAS attack achieves this by distracting the model's attention patterns and evading human-specific attention, resulting in camouflages that are visually-natural and contextually relevant. The proposed method is evaluated extensively in both digital and physical settings, demonstrating superior performance compared to state-of-the-art methods on classification and detection tasks using up-to-date models such as Yolo-V5.