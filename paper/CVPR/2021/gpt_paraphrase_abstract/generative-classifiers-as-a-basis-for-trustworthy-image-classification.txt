With the advancement of deep learning systems, the importance of trustworthiness in evaluating models has grown. Trustworthiness is defined as the combination of explainability and robustness. Generative classifiers (GCs) have been identified as a promising model type that naturally possesses these qualities. However, previous studies have mainly focused on simple datasets like MNIST and CIFAR. In this study, we propose an architecture and training approach that enables GCs to operate on a more complex level, specifically the ImageNet challenge. Our findings demonstrate the significant potential of GCs in achieving trustworthy image classification. Even when applied in a naive manner, GCs exhibit improved explainability and certain aspects of robustness compared to feed-forward models. While not all trustworthiness issues are fully resolved, we observe that GCs provide a promising foundation for the development of future algorithms and modifications. To facilitate further generative classification tasks, we have made our trained model available for download, aiming to serve as a starting point similar to pre-trained ResNet architectures for discriminative classification. The code for our work can be found at github.com/VLL-HD/trustworthy GCs.