Novel view synthesis is a challenging problem in computer graphics where the goal is to generate new views of a scene from existing images. Recent advancements in neural rendering techniques have made it possible to achieve photorealistic image quality for this task. However, the state-of-the-art neural volume rendering approaches are slow to train and require a significant amount of time for rendering high-resolution images.To address these limitations, we propose a novel approach that combines high-capacity neural scene representations with periodic activations. This allows us to optimize both the implicit surface and the radiance field of a scene using only posed 2D images for supervision. By doing so, we significantly accelerate the training process for neural volume rendering, reducing the rendering time by approximately two orders of magnitude.One unique aspect of our approach is that our implicit surface representation allows us to export a mesh with view-dependent texture information. This makes our representation compatible with traditional graphics pipelines, enabling real-time rendering rates. Moreover, our approach achieves unprecedented image quality compared to other surface methods.We evaluate the quality of our approach using existing datasets as well as high-quality 3D face data obtained from a custom multi-camera rig. Our results demonstrate the effectiveness of our approach in generating realistic and high-quality novel views of scenes.