This paper presents a systematic study on the ability of machines to perceive multisensory information when subjected to attacks. The authors investigate the robustness of audio-visual learning by conducting adversarial attacks on audio, visual, and both modalities. They examine whether audio-visual integration enhances perception and explore the impact of different fusion mechanisms on the resilience of audio-visual models. To understand the interactions between modalities during attacks, the authors develop a weakly-supervised model that localizes sound sources in videos. To counter multimodal attacks, they propose an audio-visual defense approach that incorporates an audio-visual dissimilarity constraint and external feature memory banks. Extensive experiments reveal that audio-visual models are vulnerable to multimodal adversarial attacks, and audio-visual integration can actually decrease model robustness in the presence of such attacks. Furthermore, even a weakly-supervised sound source localization model can be deceived. However, the authors' defense method effectively enhances the resilience of audio-visual networks without significantly compromising their performance on clean data. The source code and pre-trained models associated with this study are publicly available on GitHub (https://github.com/YapengTian/AV-Robustness-CVPR21).