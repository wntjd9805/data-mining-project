We propose a novel generative model for 3D garment deformations that allows us to develop a data-driven approach for virtual try-on, specifically focusing on addressing collisions between garments and the body. Unlike existing methods that require post-processing to fix interpenetrations between garments and the body during testing, our approach directly generates 3D garment configurations that do not collide with the underlying body. The key to our success is a new canonical space for garments that eliminates pose and shape deformations already captured by a new diffused human body model. This model extrapolates body surface properties, such as skinning weights and blendshapes, to any 3D point. We utilize this representation to train a generative model with a unique self-supervised collision term that effectively resolves interpenetrations between garments and the body. Through extensive evaluation and comparison with other data-driven methods, we demonstrate that our approach is the first to successfully handle garment-body contact in unseen body shapes and motions without sacrificing realism and detail.