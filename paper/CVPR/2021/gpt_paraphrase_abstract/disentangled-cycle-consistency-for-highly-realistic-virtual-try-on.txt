This study introduces a Disentangled Cycle-consistency Try-On Network (DCTON) for virtual try-on, which aims to replace a person's clothes with desired in-shop clothes images. Existing methods for virtual try-on use in-painting or cycle consistency techniques, which reconstruct the input image in a self-supervised manner. However, these methods do not distinguish between clothing and non-clothing regions, leading to a decrease in virtual try-on quality. DCTON addresses this issue by disentangling important components of virtual try-on, such as clothes warping, skin synthesis, and image composition. It can be trained in a self-supervised manner through cycle consistency learning. Experimental results demonstrate that DCTON outperforms state-of-the-art approaches on challenging benchmarks.