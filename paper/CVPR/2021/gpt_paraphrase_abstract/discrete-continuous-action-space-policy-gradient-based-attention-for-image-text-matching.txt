The task of image-text matching is crucial and has numerous applications. Its objective is to match images and text based on similar semantic information. Existing methods do not explicitly transform the different modalities into a shared space. Additionally, the attention mechanism used in image-text matching models lacks supervision. To address these issues, we propose a new attention scheme that projects image and text embeddings into a common space and optimizes the attention weights directly based on evaluation metrics. This attention scheme can be seen as supervised attention and does not require additional annotations. We train it using a novel Discrete-continuous action space policy gradient algorithm, which is more effective in modeling complex action spaces compared to previous continuous action space policy gradient methods. We evaluate our proposed methods on two well-known benchmark datasets, Flickr30k and MS-COCO, and achieve significantly better results than previous approaches.