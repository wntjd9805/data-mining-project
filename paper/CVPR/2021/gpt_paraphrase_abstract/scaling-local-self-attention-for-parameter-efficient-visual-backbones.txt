Self-attention is a promising technique for enhancing computer vision systems by offering parameter-independent scaling and content-dependent interactions. This is in contrast to convolutions, which rely on parameter-dependent scaling and content-independent interactions. Recent studies have shown that self-attention models exhibit better accuracy-parameter trade-offs compared to conventional convolutional models like ResNet-50.This research aims to develop self-attention models that not only outperform baseline models but also surpass high-performing convolutional models. To achieve this, two extensions to self-attention are proposed, along with a more efficient implementation. These enhancements contribute to improved speed, memory usage, and accuracy of the models. Consequently, a new family of self-attention models, called HaloNets, is introduced, achieving state-of-the-art accuracies on the parameter-limited ImageNet classification benchmark.Preliminary transfer learning experiments demonstrate that HaloNet models outperform much larger models and exhibit superior inference performance. Additionally, when applied to more challenging tasks such as object detection and instance segmentation, simple local self-attention and convolutional hybrids show improvements over strong baseline models. These findings highlight the effectiveness of self-attention models in domains typically dominated by convolutions.