Dependency relations among visual entities are widespread because objects and scenes have a high level of structure. These relations provide prior knowledge about the real world that can enhance the generalization ability of deep learning methods. Instead of focusing on contextual reasoning that aggregates features in the spatial domain, visual dependency reasoning explicitly models the dependency relations among visual entities. This paper presents a new network architecture called DependencyNet for semantic segmentation, which integrates dependency reasoning at three semantic levels.  At the intra-class level, DependencyNet separates the representations of different object categories and updates them based on the internal object structures. Inter-class reasoning then performs spatial and semantic reasoning by considering the dependency relations among different object categories. The paper explores how to discover the dependency graph from training annotations in detail. Additionally, global dependency reasoning refines the representations of each object category using global scene information.  Through extensive experiments with controlled model size and network depth, the paper demonstrates that each individual dependency reasoning component benefits semantic segmentation, and their combination significantly improves the performance of the base network. Experimental results on two benchmark datasets show that DependencyNet achieves comparable performance to the most recent state-of-the-art methods.