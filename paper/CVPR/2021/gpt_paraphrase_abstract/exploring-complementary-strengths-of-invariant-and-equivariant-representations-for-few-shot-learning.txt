The problem of collecting a large number of labeled samples is often impractical in real-world scenarios. To overcome this challenge, few-shot learning (FSL) has emerged as the leading approach, aiming to quickly adapt to new categories with limited samples. FSL has traditionally relied on gradient-based meta-learning and metric learning techniques. However, recent studies have highlighted the importance of powerful feature representations achieved through a simple embedding network, which can outperform complex FSL algorithms. Building on this insight, we propose a novel training mechanism that simultaneously enforces both equivariance and invariance to a wide range of geometric transformations. While previous works have separately employed equivariance or invariance, our approach combines them for the first time. By optimizing for these contrasting objectives together, our model learns features that are not only independent of input transformations but also encode the structure of geometric transformations. This combination of features enables effective generalization to novel classes with limited data samples. Additionally, we introduce a self-supervised distillation objective that further enhances our method's performance. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art FSL methods on five popular benchmark datasets, even without the use of knowledge distillation.