The effectiveness of modern deep convolutional networks (CNNs) in handling distribution shifts and adapting to new tasks with limited training examples has been questioned. However, recent advancements in transfer learning suggest that these networks can indeed cope with severe distribution shifts. In this study, we explore the relationship between out-of-distribution and transfer performance of image classification CNNs, which has not been previously investigated. We examine how the size of the pre-training data, the scale of the model, and the data preprocessing pipeline impact the performance. Our findings demonstrate that increasing both the training set and model sizes significantly enhances the ability to handle distributional shifts. Surprisingly, we also discover that simple modifications in the preprocessing, such as adjusting the image resolution, can greatly alleviate robustness issues in certain cases. Moreover, we identify limitations in existing evaluation datasets for robustness and introduce a synthetic dataset called SI-SCORE for a systematic analysis of common factors of variation in visual data, including object size and position.