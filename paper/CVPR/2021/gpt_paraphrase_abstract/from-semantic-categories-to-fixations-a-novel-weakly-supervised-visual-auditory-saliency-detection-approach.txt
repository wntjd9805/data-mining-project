The improvement of video saliency detection models has been significant due to advancements in deep learning techniques and the availability of large-scale training sets. However, visual-audio fixation prediction using deep learning is still in its early stages. Currently, only a few visual-audio sequences have real fixations recorded in a real visual-audio environment. It is therefore unnecessary and inefficient to collect real fixations in the same circumstances. This paper proposes a novel weakly-supervised approach to address this issue by using video category tags. The approach, called selective class activation mapping (SCAM), selects the most discriminative regions in the spatial-temporal-audio context using a coarse-to-fine strategy. These regions show high consistency with real human-eye fixations and can be used as pseudo ground truths to train a new spatial-temporal-audio (STA) network. Without relying on real fixations, the performance of our STA network is comparable to fully supervised models. Our code and results can be found at https://github.com/guotaowang/STANet.