We present a novel approach for detecting human-object interactions (HOIs) that addresses the limitations of existing CNN-based methods. These methods struggle to leverage image-wide features due to the locality of CNNs, rely on manually defined regions of interest that may not capture important context, and mix up features for closely located HOI instances. To overcome these limitations, we propose a transformer-based feature extractor that incorporates an attention mechanism and query-based detection. The attention mechanism effectively aggregates contextually important information across the entire image, while the queries are designed to capture individual human-object pairs, preventing feature mixing. The transformer-based feature extractor generates highly effective embeddings, allowing for simple and intuitive subsequent detection heads. Extensive analysis demonstrates that our proposed method successfully extracts contextually important features, resulting in significant performance improvements over existing methods (5.37 mAP on HICO-DET and 5.6 mAP on V-COCO). The source codes for our method can be found at https://github.com/hitachi-rd-cv/qpic.