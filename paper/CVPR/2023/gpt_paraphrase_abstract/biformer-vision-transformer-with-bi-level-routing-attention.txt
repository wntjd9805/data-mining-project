Attention is a crucial component of vision transformers, but it comes with a high computational cost and memory usage due to pairwise token interactions. Previous approaches have tried to address this by introducing handcrafted sparsity, limiting attention to local windows or stripes. In contrast, our proposed dynamic sparse attention via bi-level routing allows for more flexible computation allocation with content awareness. We filter out irrelevant key-value pairs at a coarse region level and then apply fine-grained token-to-token attention in the remaining candidate regions. Our implementation saves computation and memory by utilizing sparsity and only involving GPU-friendly dense matrix multiplications. We introduce a new vision transformer called BiFormer, which benefits from attending to a small subset of relevant tokens in a query-adaptive manner, leading to improved performance and computational efficiency, particularly in dense prediction tasks. Empirical results across various computer vision tasks validate the effectiveness of our design. The code is available at https://github.com/rayleizhu/BiFormer.