Class-incremental semantic segmentation is a challenging task that involves learning new classes while maintaining the ability to segment old ones. However, existing methods based on convolutional networks suffer from catastrophic forgetting due to the unavailability of old-class labels. These methods typically use knowledge distillation to prevent forgetting but have limitations. Firstly, they require adding additional convolutional layers to predict new classes. Secondly, they fail to distinguish between different regions corresponding to old and new classes during knowledge distillation, resulting in limited learning of new classes.To address these limitations, we propose a novel transformer framework called Incrementer for class-incremental semantic segmentation. Unlike existing methods, Incrementer only requires adding new class tokens to the transformer decoder for learning new classes. Additionally, we introduce a new knowledge distillation scheme that focuses on distilling information from the old-class regions. This reduces the constraints imposed by the old model on the learning of new classes, thereby enhancing plasticity. Moreover, we present a class de-confusion strategy to mitigate overfitting to new classes and confusion between similar classes.Our approach is both simple and effective, and extensive experiments demonstrate its superiority over state-of-the-art methods. On benchmark datasets such as Pascal VOC and ADE20k, our Incrementer achieves significant performance improvements of 5âˆ¼15 absolute points. We believe that our Incrementer can serve as a robust pipeline for class-incremental semantic segmentation.