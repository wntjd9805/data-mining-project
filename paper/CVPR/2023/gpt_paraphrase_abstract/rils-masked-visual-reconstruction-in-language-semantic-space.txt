This study explores the combination of masked image modeling (MIM) and natural language supervision to enhance transferable visual pre-training. The authors propose a new pre-training framework called masked visual Reconstruction in Language semantic Space (RILS). In RILS, sentence representations generated by a text encoder are used as prototypes to transform vision-only signals into semantically meaningful MIM reconstruction targets. This allows the vision models to capture structured information and predict the proper semantic of masked tokens. The improved visual representations can then enhance the text encoder through image-text alignment, which is crucial for effective MIM target transformation. Experimental results demonstrate that RILS outperforms previous MIM and CLIP methods and achieves better performance on various tasks, particularly in low-shot scenarios. The RILS code is available at https://github.com/hustvl/RILS.