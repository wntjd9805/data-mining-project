Dataset Distillation (DD) is a newly emerging field that focuses on generating smaller yet effective synthetic training datasets from large ones. Current DD methods, which rely on gradient matching, have shown promising results. However, these methods are computationally intensive as they require continuous optimization of a dataset using numerous randomly initialized models. This paper proposes two model augmentation techniques, namely using early-stage models and parameter perturbation, to create an informative synthetic dataset at a significantly reduced training cost. Extensive experiments demonstrate that our method achieves a speedup of up to 20 times compared to state-of-the-art methods, while still maintaining comparable performance.