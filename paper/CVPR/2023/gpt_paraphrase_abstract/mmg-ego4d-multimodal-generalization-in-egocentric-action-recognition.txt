This paper addresses the problem of egocentric action recognition in a novel way, termed "Multimodal Generalization" (MMG). MMG examines how systems can generalize when data from certain modalities is limited or missing. The study focuses on both supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two scenarios: missing modality generalization, where some modalities present during training are missing during inference, and cross-modal zero-shot generalization, where the modalities present during training and inference are different. To investigate this problem, the authors create a new dataset called MMG-Ego4D, which includes video, audio, and inertial motion sensor (IMU) modalities. They evaluate various models on MMG-Ego4D and propose new methods to improve generalization. These methods include a fusion module with modality dropout training, contrastive-based alignment training, and a novel cross-modal prototypical loss for better few-shot performance. The authors hope that this study will serve as a benchmark and guide for future research in multimodal generalization problems. The dataset and code for this study can be found at https://github.com/facebookresearch/MMG Ego4D.