We propose a new framework called A5 (Adversarial Augmentation Against Adversarial Attacks) that provides a certified preemptive defense against adversarial attacks. Instead of relying on post-attack countermeasures, A5 creates a defensive perturbation that guarantees the failure of any attack up to a certain magnitude. We utilize existing automatic perturbation analysis tools for neural networks to achieve this. We investigate the conditions for effectively applying A5, examine the significance of classifier robustness, and analyze the appearance of robustified images. Through our experiments, we demonstrate the effectiveness of on-the-fly defensive augmentation using a robustifier network that disregards the ground truth label. We also highlight the advantages of co-training the robustifier and classifier. A5 consistently outperforms state-of-the-art certified defenses on various datasets such as MNIST, CIFAR10, FashionMNIST, and Tinyimagenet. Furthermore, we showcase the application of A5 in creating certifiably robust physical objects. Our code repository provides the opportunity to experiment with different scenarios, including physical attacks, beyond the tested man-in-the-middle attack.