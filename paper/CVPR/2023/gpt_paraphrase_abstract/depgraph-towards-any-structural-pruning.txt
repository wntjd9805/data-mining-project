Model acceleration through structural pruning involves removing parameter groups from neural networks. However, the patterns of parameter grouping differ across models, making architecture-specific pruners ineffective for new architectures. This study focuses on the challenging task of general structural pruning for various architectures like CNNs, RNNs, GNNs, and Transformers. The main challenge lies in the structural coupling, which requires pruning multiple layers simultaneously while ensuring consistent removal of unimportant parameters to avoid structural issues and performance degradation. To overcome this problem, the authors propose a fully automatic method called Dependency Graph (DepGraph) that models layer dependencies and groups coupled parameters for pruning. The proposed method is evaluated extensively on different architectures and tasks, such as image recognition, graph analysis, 3D point cloud processing, and language understanding. The results demonstrate that even with a simple norm-based criterion, the proposed method consistently achieves satisfactory performance.