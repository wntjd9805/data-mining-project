Foundation models have demonstrated outstanding performance and generalization capabilities across different domains. However, current fine-tuning methods for these models only focus on minimizing a single task-specific loss, neglecting the potential benefits of leveraging other losses. To address this limitation, we propose MELTR, a plug-in module that combines multiple loss functions to aid in learning the target task through auxiliary learning. We formulate this auxiliary learning as a bi-level optimization problem and present an efficient optimization algorithm based on Approximate Implicit Differentiation (AID). We evaluate our framework on various video foundation models and observe significant performance improvements across four downstream tasks: text-to-video retrieval, video question answering, video captioning, and multi-modal sentiment analysis. Our qualitative analyses demonstrate that MELTR effectively transforms individual loss functions and integrates them into a unified and effective loss. The code for MELTR is available at https://github.com/mlvlab/MELTR.