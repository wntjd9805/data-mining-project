The use of Contrastive Language-Image Pre-training (CLIP) has shown promising results in 2D zero-shot and few-shot learning. However, its application in 3D scene understanding has not been explored. This paper presents the first attempt to investigate how CLIP knowledge can benefit 3D scene understanding. We introduce CLIP2Scene, a straightforward yet effective framework that transfers CLIP knowledge from 2D image-text models to a 3D point cloud network. Our experiments demonstrate that the pre-trained 3D network achieves impressive performance in various downstream tasks, such as annotation-free and fine-tuning with labelled data for semantic segmentation. To accomplish this, we design a Semantic-driven Cross-modal Contrastive Learning framework that pre-trains the 3D network by leveraging semantic and spatial-temporal consistency regularization. By incorporating CLIP's text semantics, we select positive and negative point samples and employ contrastive loss to train the 3D network. Additionally, we enforce consistency between temporally coherent point cloud features and their corresponding image features. We evaluate our approach on SemanticKITTI, nuScenes, and ScanNet datasets. For the first time, our pre-trained network achieves annotation-free 3D semantic segmentation with 20.8% and 25.08% mIoU on nuScenes and ScanNet respectively. When fine-tuned with 1% or 100% labelled data, our method outperforms other self-supervised methods with improvements of 8% and 1% mIoU respectively. Furthermore, we demonstrate the generalizability of our approach in handling cross-domain datasets. The code is publicly available.