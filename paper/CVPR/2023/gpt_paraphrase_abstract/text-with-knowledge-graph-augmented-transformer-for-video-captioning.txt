Video captioning is the process of describing the content of videos using natural language. While there has been progress in this field, there is still room for improvement, particularly in real-world applications, due to the challenge of long-tail words. In this study, we introduce a new approach called TextKG, which utilizes a two-stream transformer model. The external stream incorporates additional knowledge, such as a pre-built knowledge graph, to better understand the content of videos and address the long-tail words challenge. The internal stream focuses on utilizing the multi-modality information present in videos, such as video frames, speech transcripts, and video captions, to generate high-quality captions. The two streams share information through a cross attention mechanism, allowing them to enhance each other's performance. Extensive experiments conducted on challenging video captioning datasets demonstrate that TextKG outperforms state-of-the-art methods. For example, on the YouCookII dataset, TextKG improves absolute CIDEr scores by 18.7% compared to the best published results.