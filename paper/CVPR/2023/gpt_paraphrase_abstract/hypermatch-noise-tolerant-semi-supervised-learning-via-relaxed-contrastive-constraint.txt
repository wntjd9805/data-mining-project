Recent advancements in Semi-Supervised Learning (SSL) have shown significant progress through the use of Contrastive Learning. This approach has proven to be effective in learning cluster representations and leveraging large amounts of unlabeled data. However, SSL has been plagued by the issue of mismatched instance pairs caused by inaccurate pseudo labels, leading to a confirmation bias. To address this problem, we propose a new SSL method called HyperMatch, which can be integrated into existing SSL frameworks to handle noise in unlabeled data. HyperMatch combines confidence predictions with semantic similarities to create a more objective class distribution. A Gaussian Mixture Model is then used to separate pseudo labels into a "confident" subset and a "less confident" subset. Additionally, we introduce the Relaxed Contrastive Loss, which assigns the "less confident" samples to a hyper-class consisting of the top-K nearest classes. This regularization technique effectively reduces the influence of incorrect pseudo labels and increases the probability of correctly classifying the "less confident" samples. Experimental results demonstrate that HyperMatch outperforms the state-of-the-art Fix-Match method on CIFAR100 with 400 and 2500 labeled samples by 11.86% and 4.88% respectively.