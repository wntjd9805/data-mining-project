Prompt tuning is an effective method for adapting pretrained visual-language models (VLM) to specific tasks by incorporating task-related textual tokens. However, existing approaches that combine learnable textual tokens with class tokens to obtain specific textual knowledge suffer from poor generalization to unseen classes, as they forget the essential general textual knowledge that enables strong generalization. To address this issue, we propose a novel approach called Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. KgCoOp aims to reduce the discrepancy between the learnable prompt and the hand-crafted prompt, as this discrepancy leads to forgetting essential knowledge. Specifically, KgCoOp minimizes the discrepancy between the embeddings generated by the learned prompts and the hand-crafted prompts. By incorporating KgCoOp alongside the contrastive loss, we can create a discriminative prompt that performs well on both seen and unseen tasks. Extensive evaluations on various benchmarks demonstrate that our proposed Knowledge-guided Context Optimization significantly improves prompt tuning performance and achieves better results with less training time.