The process of quantifying movement in three dimensions (3D) is crucial for studying human and animal behavior. However, manually annotating poses is a laborious and costly task. To address this, self-supervised keypoint discovery has emerged as a promising approach for estimating 3D poses without annotations. Nevertheless, existing methods typically operate on single 2D views and lack 3D capabilities. In this study, we propose a novel technique called BKinD-3D, which performs self-supervised keypoint discovery in 3D using multi-view videos of behaving agents, without any supervision in terms of keypoints or bounding boxes in 2D or 3D. Our method employs an encoder-decoder architecture with a 3D volumetric heatmap. It is trained to reconstruct spatiotemporal differences across multiple views and incorporates joint length constraints on a learned 3D skeleton of the subject. By doing so, we can discover keypoints without the need for manual supervision in videos of both humans and rats. This highlights the potential of 3D keypoint discovery in the study of behavior.