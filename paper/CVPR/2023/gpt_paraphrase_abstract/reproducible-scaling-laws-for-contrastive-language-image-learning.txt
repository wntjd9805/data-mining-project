Scaling up neural networks has resulted in impressive performance in various tasks, with performance often following consistent scaling laws based on training set size, model size, and compute. However, previous research on scaling laws has been limited to private data and models or focused on specific language or vision learning. In this study, we aim to address these limitations by investigating scaling laws for contrastive language-image pre-training (CLIP) using the public LAION dataset and the open-source OpenCLIP repository. Through large-scale experiments involving models trained on up to two billion image-text pairs, we observe power law scaling in multiple downstream tasks such as zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. Interestingly, we find that the training distribution plays a crucial role in scaling laws, as the OpenAI and OpenCLIP models exhibit different scaling behaviors despite having identical model architectures and similar training approaches. To promote reproducibility and accessibility of scaling laws research, we provide our evaluation workflow and all models, including the largest public CLIP models, as open-source. The source code and instructions to replicate this study can be found at https://github.com/LAION-AI/scaling-laws-openclip.