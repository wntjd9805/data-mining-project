We propose a method for recommending music for a given video based on user-guided music selection using natural language. The main challenge is the lack of text descriptions for the music in existing datasets. To address this, we present three contributions. Firstly, we introduce a text-synthesis approach that utilizes a large-scale language model (BLOOM-176B) to generate natural language music descriptions by combining pre-trained music tagger outputs and a small number of human text descriptions. Secondly, we train a new trimodal model using these synthesized music descriptions, which combines text and video representations to query music samples. We incorporate a text dropout regularization mechanism during training, which is crucial for model performance. Our model ensures that the retrieved music audio aligns with the visual style of the video and the musical genre, mood, or instrumentation described in the natural language query. Thirdly, we create a testing dataset by annotating a subset of 4k clips from the YT8M-MusicVideo dataset with natural language music descriptions, which is publicly available. We demonstrate that our approach achieves comparable or superior performance to previous methods in video-to-music retrieval, while significantly improving retrieval accuracy when guided by text. Our ViML model combines video and language representations to retrieve a suitable music track from a database, ensuring that the audio matches both the video and language content. The music style can be adjusted to match the language query, and the vocalist can be changed to match the video content. For a comprehensive understanding of our results, please refer to the companion video on our website.