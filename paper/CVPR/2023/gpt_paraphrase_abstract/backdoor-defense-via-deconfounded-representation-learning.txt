Deep neural networks (DNNs) can be susceptible to backdoor attacks, where hidden backdoors are inserted into the DNN model by adding poisoned examples to the training dataset. Despite efforts to detect and remove backdoors, it is unclear whether a clean model can be directly obtained from the poisoned dataset. This paper presents a causal graph to model the generation process of poisoned data and identifies the backdoor attack as a confounder that introduces spurious associations between input images and target labels, compromising the reliability of model predictions. Building on this causal understanding, the authors propose a defense mechanism called Causality-inspired Backdoor Defense (CBD). CBD involves training a backdoored model to capture the confounding effects and a clean model to capture the desired causal effects. The clean model achieves this by minimizing mutual information with the confounding representations from the backdoored model and using a sample-wise re-weighting scheme. Extensive experiments on multiple benchmark datasets demonstrate that CBD effectively reduces backdoor threats while maintaining high accuracy in predicting benign samples. The defense method is also shown to resist potential adaptive attacks. The code for CBD is available at https://github.com/zaixizhang/CBD.