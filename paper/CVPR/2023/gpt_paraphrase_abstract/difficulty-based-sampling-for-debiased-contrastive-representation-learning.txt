Contrastive learning is a popular method for self-supervised representation learning, but it faces challenges with false negative samples that degrade its performance. These false negatives are negative samples that are mistakenly labeled as different from the anchor, when in fact they have the same label. To address this problem, it is important to distinguish between true and false negatives, as well as easy and hard negatives. Previous approaches have used statistical methods and hyperparameter tuning, but in this paper, we propose a novel debiased contrastive learning method that explores the connection between hard negative samples and data bias. We introduce a triplet loss to train an encoder that focuses more on easy negative samples, and we demonstrate theoretically that this loss amplifies bias in self-supervised representation learning. Through empirical experiments, we show that our proposed method improves downstream classification performance.