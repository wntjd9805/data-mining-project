We propose a novel approach called EC2 (Emergent Communication for Embodied Control) for pre-training video-language representations to improve few-shot embodied control. Unlike previous methods that use contrastive learning to align visual and language modalities, we focus on modeling their complementary differences. We learn an unsupervised "language" of videos through emergent communication, linking video semantics with natural language structures. We utilize a language model to learn embodied representations of video trajectories, emergent language, and natural language, which are then used to fine-tune a lightweight policy network for downstream control. Our experiments on Meta-world and Franka Kitchen benchmarks consistently show that EC2 outperforms previous contrastive learning methods for both video and language inputs. Ablation studies demonstrate the importance of emergent language, which improves both video and language learning and outperforms using pre-trained video captions. We also provide a quantitative and qualitative analysis of the emergent language and discuss future directions for leveraging emergent communication in embodied tasks.