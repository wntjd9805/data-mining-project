The recent advancements in Vision Transformer (ViT) have surpassed traditional CNN-based models in various vision tasks. However, recent research in the CNN field has shown that carefully tuned pure CNN models can achieve comparable performance to ViT models. Despite this, designing high-performance CNN models is difficult and requires significant prior knowledge of network design. To address this challenge, a novel framework called Mathematical Architecture Design for Deep CNN (Deep-MAD1) is introduced. DeepMAD treats a CNN network as an information processing system and formulates its expressiveness and effectiveness through structural parameters. These parameters are optimized using a constrained mathematical programming (MP) problem, which can be easily solved by off-the-shelf MP solvers on CPUs with low memory usage. Importantly, DeepMAD is a purely mathematical framework that does not require GPUs or training data during network design. The efficacy of DeepMAD is demonstrated on multiple large-scale computer vision benchmark datasets. Notably, on ImageNet-1k, DeepMAD achieves higher top-1 accuracy than ConvNeXt and Swin models, with improvements of 0.7% and 1.5% on the Tiny level and 0.8% and 0.9% on the Small level, solely using conventional convolutional layers.