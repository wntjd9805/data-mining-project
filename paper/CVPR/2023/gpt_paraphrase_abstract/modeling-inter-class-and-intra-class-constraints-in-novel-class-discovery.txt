The objective of novel class discovery (NCD) is to develop a model that can transfer common knowledge from a labeled dataset to an unlabeled dataset while also discovering new classes within the unlabeled dataset. Although various methods and training pipelines have been proposed to improve NCD performance, existing methods do not fully utilize the essence of the NCD setting. In this paper, we propose a new approach that incorporates both inter-class and intra-class constraints in NCD using the symmetric Kullback-Leibler divergence (sKLD). The inter-class sKLD constraint leverages the disjoint relationship between labeled and unlabeled classes to enhance separability in the embedding space. Additionally, the intra-class sKLD constraint ensures stability in the training process by explicitly constraining the relationship between a sample and its augmentations. We conducted extensive experiments on popular benchmarks and achieved state-of-the-art results, including significant improvements in clustering accuracy. The code for our method is available at https://github.com/FanZhichen/NCD-IIC.