This study addresses the challenge of Long-Tailed Recognition (LTR) in deep neural networks, specifically focusing on the performance of Vision Transformers (ViTs). The imbalance in real-world data poses a significant obstacle to LTR, and existing methods rarely train ViTs with Long-Tailed (LT) data, leading to unfair comparisons. To address this issue, the authors propose LiVT, a novel approach that trains ViTs from scratch using only LT data. The authors observe that ViTs face more severe LTR problems and introduce Masked Generative Pretraining (MGP) as a method to learn generalized features. Through ample evidence, they demonstrate that MGP is more robust than supervised approaches. Additionally, they find that Binary Cross Entropy (BCE) loss, although effective with ViTs, struggles with LTR tasks. To mitigate this, they propose balanced BCE, which provides theoretical foundations by deriving the unbiased extension of Sigmoid and compensating for extra logit margins. Bal-BCE enables ViTs to converge quickly in just a few epochs. Extensive experiments validate the effectiveness of LiVT with MGP and Bal-BCE, showing that it successfully trains ViTs without additional data and significantly outperforms comparable state-of-the-art methods. For example, their ViT-B achieves 81.0% Top-1 accuracy in the iNaturalist 2018 dataset without any additional enhancements. The authors provide the code for LiVT at https://github.com/XuZhengzhuo/LiVT.