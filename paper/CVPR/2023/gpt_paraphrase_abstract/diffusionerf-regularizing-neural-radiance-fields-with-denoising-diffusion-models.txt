Neural Radiance Fields (NeRFs) have achieved impressive results in generating novel views of scenes. However, they can suffer from artifacts when trained with limited input views. To address this issue, we propose using a denoising diffusion model (DDM) to learn a prior over scene geometry and color. The DDM is trained on RGBD patches from the synthetic Hypersim dataset and can predict the gradient of the logarithm of a joint probability distribution of color and depth patches. This gradient is used to regularize the geometry and color of the scene during NeRF training. Our experiments on the LLFF dataset demonstrate that our learned prior improves the quality of reconstructed geometry and enhances generalization to novel views. Additionally, evaluations on the DTU dataset show improved reconstruction quality compared to other NeRF methods.