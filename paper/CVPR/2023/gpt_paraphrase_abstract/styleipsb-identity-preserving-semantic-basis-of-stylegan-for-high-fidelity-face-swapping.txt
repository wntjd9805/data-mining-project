Recent studies have shown that StyleGAN, a generative model, is capable of producing highly realistic images. This has inspired researchers to utilize pre-trained StyleGAN models for high-fidelity face swapping. However, existing methods have failed to meet expectations in two crucial aspects: the generated images lack pore-level details and do not adequately preserve the identity of challenging face-swapping cases. To address these limitations, we propose a novel approach called StyleIPSB (StyleGAN Identity-Preserving Semantic Bases). StyleIPSB consists of a set of semantic bases that control specific attributes such as pose, expression, and illumination, while disentangling them from one another. By constraining the style code within the W+ subspace, StyleIPSB preserves pore-level details and becomes a valuable tool for high-fidelity face swapping. Our proposed framework for face swapping with StyleIPSB follows a three-stage process. Firstly, we transform the target facial attributes to align with the source image by mapping 3D Morphable Model (3DMM) parameters, which capture significant semantic variations, to the coordinates of StyleIPSB that exhibit higher identity preservation and fidelity. Secondly, we learn the residual attribute between the reenacted face and the target face to transform detailed attributes that are not captured by 3DMM. Finally, we blend the transformed face into the background of the target image. Extensive experiments and comparisons demonstrate that StyleIPSB effectively preserves identity and pore-level details, resulting in state-of-the-art performance in face swapping. We will make our code available at https://github.com/a686432/StyleIPSB.