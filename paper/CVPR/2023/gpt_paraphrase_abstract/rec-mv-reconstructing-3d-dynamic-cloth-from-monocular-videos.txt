Developing a practical and cost-effective method for reconstructing dynamic 3D garment surfaces from monocular videos is a significant challenge in digitizing clothing. While recent neural rendering techniques have achieved impressive results in reconstructing clothed human bodies from monocular videos, they fail to separate the garment surface from the body. Similarly, existing garment reconstruction methods based on feature curves struggle to generate temporally consistent surfaces for video inputs.  To overcome these limitations, this paper presents a novel approach called REC-MV, which formulates the problem of reconstructing 3D garment feature curves and surfaces from monocular video as an optimization task. By jointly optimizing explicit feature curves and implicit signed distance fields (SDF) of the garments, REC-MV enables the extraction of open garment meshes through garment template registration in a canonical space.  The effectiveness of REC-MV is demonstrated through experiments on multiple casually captured datasets, where it outperforms existing methods and produces high-quality dynamic garment surfaces. The source code for REC-MV is publicly available at https://github.com/GAP-LAB-CUHK-SZ/REC-MV.