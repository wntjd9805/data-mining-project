Contrastive learning-based approaches like CLIP have shown impressive performance in video-language representation learning by focusing on semantic interaction between predefined video-text pairs. However, to further improve this interaction, we need to address the challenge of fine-grained cross-modal learning. In this study, we propose a novel approach that models video-text as game players using multivariate cooperative game theory. This allows us to handle uncertainty in fine-grained semantic interaction with various levels of granularity, flexible combinations, and vague intensity. Our method, called Hierarchical Banzhaf Interaction (HBI), evaluates the correspondence between video frames and text words to achieve sensitive and explainable cross-modal contrast. To efficiently implement this cooperative game, we cluster the original video frames (text words) and compute the Banzhaf Interaction between the merged tokens. By using token merge modules, we can conduct cooperative games at different semantic levels. Our extensive experiments on text-video retrieval and video-question answering benchmarks demonstrate the effectiveness of HBI, with superior performance compared to existing methods. Additionally, HBI can also serve as a visualization tool to enhance the understanding of cross-modal interaction, making it highly valuable for the research community.