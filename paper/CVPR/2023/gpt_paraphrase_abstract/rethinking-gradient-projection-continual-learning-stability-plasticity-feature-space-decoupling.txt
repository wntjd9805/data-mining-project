Continual learning is a process of learning new information while retaining previously learned knowledge. Recent research has shown that learning can retain knowledge if the updated gradient is orthogonal to the feature space. However, existing methods require the gradient to be fully orthogonal to the entire feature space, which limits the adaptability of the learning process as the feature space expands with each new task. To address this issue, this paper introduces the space decoupling (SD) algorithm. SD divides the feature space into two complementary subspaces: the stability space (I) and the plasticity space (R). The stability space is created by intersecting the historical and current feature spaces, providing a collection of shared bases for multiple tasks. The plasticity space, on the other hand, is constructed by finding the orthogonal complement of the stability space, containing task-specific bases. By imposing distinct constraints on the stability and plasticity spaces, the proposed method achieves a better balance between stability and adaptability. The effectiveness of the SD algorithm is demonstrated through extensive experiments using gradient projection baselines, and the results show that SD is applicable to different models and achieves state-of-the-art performance on publicly available datasets.