This paper introduces Re-GAN, a novel method for training Generative Adversarial Networks (GANs) that enables efficient training with limited data. Traditional GAN training on high-fidelity images typically requires a large number of training images. However, recent research has shown that dense GAN models contain sparse sub-networks, known as "lottery tickets," which produce better results when trained separately with limited data. The process of finding these lottery tickets involves a costly train-prune-retrain process.  In contrast, Re-GAN dynamically reconfigures the GAN architecture during training to explore different sub-network structures. It repeatedly prunes unimportant connections to regularize the GAN network, reducing the risk of prematurely pruning important connections. Additionally, it regrows these connections to maintain stability. This approach stabilizes GAN models with less data and offers an alternative to existing GAN ticket and progressive growing methods.  Re-GAN demonstrates its effectiveness on datasets of varying sizes, domains, and resolutions, including CIFAR-10, Tiny-ImageNet, and multiple few-shot generation datasets. It also performs well with different GAN architectures such as SNGAN, ProGAN, StyleGAN2, and AutoGAN. Furthermore, Re-GAN shows improved performance when combined with recent augmentation approaches.  Notably, Re-GAN requires fewer floating-point operations (FLOPs) and less training time by removing unimportant connections during GAN training, while still generating comparable or even higher-quality samples. In comparison to the state-of-the-art StyleGAN2, Re-GAN outperforms without the need for any additional fine-tuning steps.  The code for Re-GAN can be accessed at the following link: https://github.com/IntellicentAI-Lab/Re-GAN.