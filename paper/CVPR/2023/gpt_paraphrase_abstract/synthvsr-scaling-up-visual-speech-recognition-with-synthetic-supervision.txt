This paper explores the potential of using synthetic visual data for visual speech recognition (VSR), as current state-of-the-art methods rely on limited publicly available transcribed video datasets. The proposed method, called SynthVSR, improves VSR performance by generating synthetic lip movements using a speech-driven lip animation model. This model is trained on an unlabeled audio-visual dataset and can be further optimized with labeled videos. By leveraging abundant acoustic data and face images, large-scale synthetic data is generated for semi-supervised VSR training. The performance of SynthVSR is evaluated on the Lip Reading Sentences 3 (LRS3) benchmark dataset, achieving a word error rate (WER) of 43.3% with only 30 hours of real labeled data. When using all 438 hours of labeled data from LRS3, the WER is reduced to 27.9%, comparable to the state-of-the-art self-supervised AV-HuBERT method. Additionally, when combined with large-scale pseudo-labeled audio-visual data, SynthVSR achieves a new state-of-the-art WER of 16.9% using only publicly available data, surpassing approaches trained with significantly more non-public machine-transcribed video data. Extensive ablation studies are conducted to analyze the impact of each component in the proposed method.