Computational pathology has the potential to save lives, but the need for annotated data and the high cost of annotating pathology images pose challenges. Self-supervised learning (SSL) has proven effective in utilizing unlabeled data and could greatly benefit pathology research. However, there is a lack of comprehensive studies comparing SSL methods and exploring their adaptation for pathology. To fill this gap, we conducted the largest-scale study to date on SSL pre-training using pathology image data. We evaluated four representative SSL methods on various downstream tasks and found that domain-aligned pre-training in pathology consistently outperformed ImageNet pre-training in standard SSL settings, including linear and fine-tuning evaluations, as well as in low-label scenarios. Additionally, we propose domain-specific techniques that we experimentally demonstrate enhance performance. Furthermore, we applied SSL for the first time to the challenging task of nuclei instance segmentation and observed significant and consistent improvements in performance. We have made our pre-trained model weights publicly available.