Recent studies on video moment retrieval have primarily focused on improving accuracy, efficiency, and robustness by using high-quality annotations. However, the process of annotating precise frame-level information is time-consuming and costly. Little attention has been given to the labeling process itself. In this study, we propose a new interactive approach to facilitate human-in-the-loop annotation for video moment retrieval. The main challenge is selecting "ambiguous" frames and videos for binary annotations, which will aid in network training. We introduce a hierarchical uncertainty-based modeling method that explicitly considers the uncertainty of each frame within the entire video sequence corresponding to the query description. The frame with the highest uncertainty is selected for annotation by human experts, significantly reducing their workload. By obtaining a small number of labels from experts, we demonstrate that it is possible to learn a competitive video moment retrieval model in such a challenging environment. Additionally, we treat the uncertainty score of frames in a video as a whole and estimate the difficulty of each video, further reducing the burden of video selection. Our active learning strategy works not only at the frame level but also at the sequence level. Experimental results on two public datasets confirm the effectiveness of our proposed method. The code for our approach is available at https://github.com/renjie-liang/HUAL. Figure 1 illustrates our interactive method, named HUAL, which requires only binary annotations, thus reducing the cost of annotation. In each round, the user (student) selects a frame with the highest uncertainty, and the expert (teacher) provides binary feedback for that frame. With more labels provided, the video moment retrieval model is retrained, and the entire process can be conducted in a human-in-the-loop manner.