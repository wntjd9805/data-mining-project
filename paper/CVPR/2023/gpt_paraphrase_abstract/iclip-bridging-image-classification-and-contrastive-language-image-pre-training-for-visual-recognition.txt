This paper introduces iCLIP, a method that effectively combines image classification and contrastive language-image pre-training. Instead of using separate heads for each task, iCLIP fuses the two tasks by adapting the image classification to share the same formula and model weights as the language-image pre-training. To further connect the tasks, iCLIP enhances category names in image classification using external knowledge, such as dictionary descriptions. Experimental results demonstrate that iCLIP successfully combines the strengths of both tasks: the strong discrimination ability of image classification and the good zero-shot ability of CLIP tasks. Notably, iCLIP achieves an 82.9% top-1 accuracy on IN-1K and outperforms CLIP by 1.8% on zero-shot recognition of the Kornblith 12-dataset benchmark, using a similar model size. The code and models for iCLIP are publicly available at https://github.com/weiyx16/iCLIP.