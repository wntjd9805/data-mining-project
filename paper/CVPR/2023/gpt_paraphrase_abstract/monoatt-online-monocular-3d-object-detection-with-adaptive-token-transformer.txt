Mono3D is a challenging task in mobile object detection, and existing transformer-based models have limitations due to coarse tokens and limited computational power. In this paper, we propose MonoATT, an online Mono3D framework that uses a novel vision transformer with varying shapes and sizes of tokens to improve mobile Mono3D. MonoATT adaptively assigns finer tokens to more significant areas and utilizes a transformer to enhance Mono3D. We design a scoring network to select important image areas and a token clustering and merging network with attention mechanism to merge tokens around selected areas. We reconstruct a pixel-level feature map from heterogeneous tokens and use a state-of-the-art Mono3D detector for detection. Experimental results on the KITTI dataset show that MonoATT significantly improves Mono3D accuracy for near and far objects with low latency. MonoATT outperforms other methods by a large margin and ranks first on the KITTI 3D benchmark.