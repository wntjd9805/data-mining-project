To train point cloud reconstruction networks, reconstruction losses are commonly used to evaluate the differences between reconstructed results and ground truths. However, existing methods that measure the training loss using point-to-point distance may introduce additional flaws due to predefined matching rules that may not accurately capture the shape differences. Although some learning-based approaches have been proposed to address this issue, they still measure shape differences in 3D Euclidean space, which may limit their ability to capture defects in reconstructed shapes. In this study, we propose a learning-based method called Contrastive Adversarial Loss (CALoss) to dynamically measure the point cloud reconstruction loss in a non-linear representation space. CALoss combines the contrastive constraint with an adversarial strategy to achieve this. The contrastive constraint helps CALoss learn a representation space with shape similarity, while the adversarial strategy helps CALoss identify differences between reconstructed results and ground truths. Experimental results on reconstruction-related tasks demonstrate that CALoss can improve reconstruction performances and learn more representative representations. Our method measures shape differences with distances between learned global representations in the constructed representation space, unlike matching-based losses that measure distances between points matched by predefined rules. For example, PCLoss learns to extract descriptors in 3D Euclidean space by linearly weighting coordinates based on their distances to predicted center points. On the other hand, our method dynamically measures shape differences using distances between learned global representations. The proposed CALoss is optimized using the adversarial loss (Ladv) to maximize representation distances between ground truths and reconstructed results, and the representation distances (Lp) between positive samples (Sp) and ground truths (Sg). The representation distances (Lr) between ground truths (Sg) and reconstructed results (So) are used to train the task network. Figure 1 illustrates the differences between our method and existing approaches.