This paper investigates the use of masked autoencoder (MAE) pre-training on videos for matching-based tasks such as visual object tracking (VOT) and video object segmentation (VOS). The authors find that a basic approach of randomly masking frame patches in videos and reconstructing the pixels does not effectively capture temporal relations, resulting in suboptimal representations for VOT and VOS. To address this issue, they propose DropMAE, which incorporates spatial-attention dropout during frame reconstruction to enhance temporal correspondence learning in videos. DropMAE proves to be a powerful and efficient temporal matching learner, outperforming the ImageNet-based MAE in terms of fine-tuning results and pre-training speed. The authors also discover that motion diversity in pre-training videos is more crucial than scene diversity for improving performance in VOT and VOS. The pre-trained DropMAE model can be directly used for fine-tuning in existing ViT-based trackers without any modifications. Notably, DropMAE achieves state-of-the-art performance on 8 out of 9 highly competitive video tracking and segmentation datasets. The code and pre-trained models are accessible at https://github.com/jimmy-dq/DropMAE.git.