Efforts are being made to obtain reliable uncertainty estimates from deterministic single-forward pass models due to the high computational cost associated with conventional uncertainty quantification methods. This study investigates two complex single-forward pass uncertainty approaches, DUQ and SNGP, to determine if they heavily rely on a well-regularized feature space. Interestingly, without utilizing the more complex uncertainty estimation techniques of DUQ and SNGP, it is discovered that a single softmax neural network with a regularized feature space achieved through residual connections and spectral normalization outperforms DUQ and SNGP in predicting epistemic uncertainty. This is achieved by using simple Gaussian Discriminant Analysis post-training as a separate feature-space density estimator, without the need for fine-tuning on out-of-distribution (OoD) data, feature ensembling, or input pre-processing. The proposed Deep Deterministic Uncertainty (DDU) baseline, which is conceptually straightforward, is also capable of distinguishing between aleatoric and epistemic uncertainty. It performs on par with Deep Ensembles, the current state-of-the-art method for uncertainty prediction, across various OoD benchmarks (CIFAR-10/100 vs SVHN/Tiny-ImageNet, ImageNet vs ImageNet-O), active learning scenarios with different model architectures, and large-scale vision tasks like semantic segmentation. Furthermore, the DDU baseline is computationally more efficient.