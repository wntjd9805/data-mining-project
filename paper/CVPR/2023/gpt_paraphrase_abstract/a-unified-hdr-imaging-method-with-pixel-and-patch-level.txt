Generating high-quality HDR images from low dynamic range (LDR) images with different exposures is challenging due to ghosting caused by object motion or camera shaking. While Deep Neural Networks (DNNs) have been proposed to address this issue, they fail to produce satisfactory results in the presence of motion and saturation. In this study, we propose a hybrid HDR deghosting network called HyHDRNet, which aims to learn the complex relationship between reference and non-reference images to generate visually pleasing HDR images in various scenarios. HyHDRNet consists of a content alignment subnetwork and a Transformer-based fusion subnetwork. The content alignment subnetwork utilizes patch aggregation and ghost attention to integrate similar content from non-reference images at the patch level while suppressing undesired components at the pixel level to effectively eliminate ghosting. To enable mutual guidance between patch-level and pixel-level information, a gating module is employed to swap useful information in ghosted and saturated regions. Additionally, the Transformer-based fusion subnetwork employs a Residual Deformable Transformer Block (RDTB) to adaptively merge information from different exposed regions, resulting in high-quality HDR images. The proposed method was evaluated on four widely used public HDR image deghosting datasets, and experimental results demonstrate that HyHDRNet outperforms state-of-the-art methods in terms of both quantitative and qualitative evaluations, achieving appealing HDR visualization with consistent textures and colors.