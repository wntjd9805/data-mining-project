The issue of missing modalities is a significant problem in multi-modal models, and current methods either address missing modalities during evaluation or train separate models for specific missing modality scenarios. However, these models are designed for specific tasks and cannot easily be adapted to different tasks. This paper introduces the Shared-Specific Feature Modelling (ShaSpec) method, which is simpler and more effective than other approaches. ShaSpec utilizes all available input modalities by learning shared and specific features to better represent the data. This is achieved through auxiliary tasks, distribution alignment, domain classification, and a residual feature fusion procedure. Furthermore, ShaSpec's design simplicity allows for easy adaptation to various tasks. Experimental results on medical image segmentation and computer vision classification demonstrate that ShaSpec significantly outperforms other methods. For example, on BraTS2018, ShaSpec improves the state-of-the-art results by over 3% for enhancing tumor, 5% for tumor core, and 3% for the entire tumor.