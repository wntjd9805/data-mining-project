This paper presents a new method for representing volumetric videos in real-time view synthesis of dynamic scenes. While neural scene representations have shown great potential in modeling and rendering complex static scenes, extending them to dynamic scenes is challenging due to slow rendering speed and high storage requirements. To address this issue, the authors propose representing the radiance field of each frame using shallow MLP networks stored in 2D grids called MLP maps. These maps are dynamically predicted by a shared 2D CNN decoder, eliminating the need for explicit storage of MLP parameters and reducing storage cost. The use of shallow MLPs significantly improves rendering speed for 3D scenes. Experimental results demonstrate that the proposed approach achieves state-of-the-art rendering quality on the NHR and ZJU-MoCap datasets, while maintaining real-time rendering efficiency with a speed of 41.7 fps for 512 Ã— 512 images on an RTX 3090 GPU. The code for this approach is available at https://zju3dv.github.io/mlp maps/.