Unsupervised multimodal domain adaptation poses challenges in both domain adaptation and modality alignment. Previous studies have addressed these challenges in separate stages, either by aligning modalities first and then adapting domains, or vice versa. However, these studies have not considered the association between domains and modalities, thus failing to leverage their complementary information. In this research, we propose a unified approach that simultaneously aligns domains and modalities. Our model introduces a tensor-based alignment module (TAL) to explore the relationship between domains and modalities, allowing for effective interaction and utilization of complementary information. Additionally, we propose a dynamic domain generator (DDG) module to bridge the gap between domains by generating transitional samples through self-supervised mixing of shared information from two domains. This approach facilitates the learning of a domain-invariant common representation space. Extensive experiments demonstrate the superior performance of our method in two real-world applications. The code for our approach will be made publicly available.