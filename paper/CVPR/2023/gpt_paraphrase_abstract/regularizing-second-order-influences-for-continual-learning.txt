Continual learning aims to acquire knowledge from data streams that change over time without forgetting previously learned information. Existing methods use replay-based techniques, where a small buffer of seen data is rehearsed. However, these methods often overlook the interference between successive rounds of selecting data from the buffer. In this study, we analyze the interaction of sequential selection steps using influence functions and discover a new type of influence that gradually amplifies bias in the replay buffer, compromising the selection process. To address this, we propose a novel selection objective that regularizes these second-order effects and is also connected to commonly used criteria. We also provide an efficient implementation for optimizing this criterion. Experimental results on various continual learning benchmarks demonstrate that our approach outperforms state-of-the-art methods. The code for our approach is available at https://github.com/feifeiobama/InfluenceCL.