Knowledge distillation (KD) is a training strategy that enhances lightweight student models with the guidance of more complex teacher models. However, the significant architectural differences between teacher-student pairs limit the effectiveness of distillation. Instead of previous adaptive distillation methods, we propose a novel training-free framework to search for the optimal student architecture given a teacher. Our findings indicate that the best model under standard training is not the most suitable for distillation. We also discover that the similarity in feature semantics and sample relations between randomly initialized teacher-student networks correlates with the final distillation performance. As a result, we efficiently measure similarity matrices based on semantic activation maps to select the optimal student architecture using an evolutionary algorithm. This approach, called Distillation WithOut Training (DisWOT), significantly improves model performance in the distillation stage and accelerates training by at least 180Ã—. Furthermore, we extend the similarity metrics in DisWOT to introduce new distillers and KD-based zero-proxies. Our experiments on CIFAR, ImageNet, and NAS-Bench-201 demonstrate that our technique achieves state-of-the-art results in different search spaces. For more information, please visit our project and access the code at https://lilujunai.github.io/DisWOT-CVPR2023/.