The problem of training a model on one domain to generalize to any unseen domain is addressed in Single Domain Generalization (SDG). While there is extensive research on SDG for image classification, there is a lack of literature on SDG object detection. To overcome the challenges of learning object localization and representation, we propose using a pre-trained vision-language model to incorporate semantic domain concepts through textual prompts. Our approach involves augmenting the features extracted by the detector backbone and employing a text-based classification loss. Our experiments demonstrate the advantages of our method, as it outperforms the existing SDG object detection method, Single-DGOD, by 10% on a diverse weather-driving benchmark.