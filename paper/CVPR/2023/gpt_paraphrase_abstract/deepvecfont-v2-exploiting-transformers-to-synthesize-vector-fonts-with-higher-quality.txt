The task of vector font synthesis is a challenging problem in the fields of Computer Vision and Computer Graphics. The existing method, DeepVecFont, achieves state-of-the-art performance by utilizing information from both image and sequence data. However, it has limitations in handling long sequences and relies heavily on image-guided post-processing, resulting in synthesized glyphs with distortions and artifacts that are inferior to human-designed fonts. To address these issues, this paper proposes an enhanced version of DeepVecFont with three novel contributions. Firstly, the paper replaces recurrent neural networks (RNNs) with Transformers to process sequential data, and introduces a relaxation representation for vector outlines, significantly improving the model's ability to synthesize long and complex outlines. Secondly, the paper suggests sampling auxiliary points in addition to control points to accurately align the generated and target BÃ©zier curves or lines. Lastly, to reduce error accumulation in the sequential generation process, a context-based self-refinement module based on another Transformer-based decoder is developed to remove artifacts in the initially synthesized glyphs. The proposed method effectively resolves the problems of the original DeepVecFont and outperforms existing approaches in generating English and Chinese vector fonts with complex structures and diverse styles. The paper presents qualitative and quantitative results to support these claims. The research was supported by various organizations and institutions. The paper includes a visualization of the vector glyphs synthesized by DeepVecFont and the proposed method, demonstrating that the proposed method generates visually-pleasing results with compact and coordinated outlines.