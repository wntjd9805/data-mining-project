The straight-through estimator (STE) is commonly used in quantization-aware training (QAT) to enable gradient flow over non-differentiable functions through approximation. However, STE often leads to unstable convergence during QAT, resulting in significant quality degradation in low precision. To address this issue, a new approach called pseudo-quantization training has been proposed, which updates learnable parameters using pseudo-quantization noise instead of STE. This study introduces a novel method called noise proxy-based integrated pseudo-quantization (NIPQ) that supports pseudo-quantization for both activation and weight by integrating truncation into the pseudo-quantization framework. NIPQ updates all quantization parameters, such as bit-width and truncation boundary, as well as network parameters using gradient descent without encountering the instability associated with STE. Extensive experiments demonstrate that NIPQ significantly outperforms existing quantization algorithms in various vision and language applications.