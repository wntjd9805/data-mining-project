We have extended the neural radiance fields (NeRFs) framework to dynamic large-scale urban scenes. Previous methods focused on reconstructing short video clips and faced challenges in scalability and supervision. To address these issues, we introduce two innovations. Firstly, we divide the scene into three separate hash table data structures to efficiently encode static, dynamic, and far-field radiance fields. Secondly, we utilize unlabeled target signals such as RGB images, sparse LiDAR, self-supervised 2D descriptors, and 2D optical flow. By employing photometric, geometric, and feature-metric reconstruction losses, our approach, called SUDS, decomposes dynamic scenes into the static background, individual objects, and their motions. We achieve impressive results by scaling our reconstructions to thousands of objects across millions of frames from numerous videos covering large geospatial areas. Additionally, we demonstrate the effectiveness of our approach through tasks like novel-view synthesis, unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. We compare our method with previous work on KITTI and Virtual KITTI 2 datasets, outperforming state-of-the-art approaches while significantly reducing training time by a factor of 10. Overall, our work represents the largest dynamic NeRF built to date and showcases the potential of our approach in various applications.