In recent years, a number of image compression neural networks have emerged as leaders in rate-distortion performance. These networks excel in nonlinear analysis and synthesis transforms facilitated by deep neural networks. However, most of these methods use uniform scalar quantizers for simplicity, despite vector quantizers being theoretically optimal. This paper introduces a new approach called Lattice Vector Quantization with spatially Adaptive Companding (LVQAC) mapping. LVQAC takes advantage of inter-feature dependencies better than scalar uniform quantization while remaining computationally efficient. Additionally, LVQAC incorporates a spatially adaptive companding (AC) mapping to enhance adaptability to source statistics. The LVQAC design can be easily integrated into any end-to-end optimized image compression system. Extensive experiments show that replacing uniform quantizers with LVQAC improves rate-distortion performance in CNN image compression models without significantly increasing model complexity.