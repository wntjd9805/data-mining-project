This paper introduces a novel neural mapping technique for camera localization. The method involves encoding an entire scene into a grid of latent codes and then using a Transformer-based auto-decoder to regress the 3D coordinates of query pixels. Traditional camera localization methods typically require storing scenes as 3D point clouds with per-point features, resulting in large storage requirements. While compression is possible, it often leads to a significant decrease in performance. However, the proposed NeuMap approach achieves high compression rates with minimal performance drop by utilizing learnable latent codes to store scene information and a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. The scene-agnostic network design also learns robust matching priors through training with large-scale data and allows for quick optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks demonstrate that NeuMap outperforms other coordinate regression methods significantly and achieves similar performance to feature matching methods while requiring a much smaller scene representation size. For instance, in the Aachen night benchmark, NeuMap achieves 39.1% accuracy with only 6MB of data compared to other methods that require 100MB or even several gigabytes and fail under high compression settings. The codes for NeuMap are available at https://github.com/Tangshitao/NeuMap.