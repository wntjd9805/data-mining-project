We introduce a real-time method that can track the movement of an unknown object in 6 degrees of freedom (6-DoF) using a monocular RGBD video sequence. Simultaneously, our method reconstructs the object in a neural 3D representation. Our technique is applicable to any rigid object, even when there is minimal visual texture present. The object segmentation is assumed to be available only in the first frame, and no additional information or assumptions about the interaction agent are needed. The core of our method lies in a Neural Object Field that is learned alongside a pose graph optimization process. This combination allows us to accumulate information effectively and create a consistent 3D representation that captures both the geometry and appearance of the object. To facilitate communication between different threads, we automatically maintain a dynamic pool of posed memory frames. Our approach is capable of handling challenging sequences with significant changes in pose, partial or full occlusion, untextured surfaces, and specular highlights. We demonstrate the superior performance of our method compared to existing approaches using the HO3D, YCBInEOAT, and BEHAVE datasets. For more information, please visit our project page at https://bundlesdf.github.io/.