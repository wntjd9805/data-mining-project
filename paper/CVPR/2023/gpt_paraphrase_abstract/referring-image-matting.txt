In this paper, we introduce a new task called Referring Image Matting (RIM) that aims to extract the precise alpha matte of a specific object based on a given natural language description. Unlike conventional image matting techniques that require user-defined scribbles or trimap, or indiscriminately extract all foreground objects, RIM allows for a more natural and simpler instruction for image matting.   To facilitate research in this area, we create a large-scale challenging dataset called RefMatte. This dataset is generated using an image composition and expression generation engine, which automatically produces high-quality images with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we create a real-world test set with 100 high-resolution natural images and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods.   Furthermore, we propose a novel baseline method called CLIPMat for RIM. CLIPMat includes a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Through extensive experiments conducted on RefMatte in both keyword and expression settings, we demonstrate the superior performance of CLIPMat compared to representative methods.   We believe that this work provides valuable insights into image matting and encourages further research in this field. The dataset, code, and models used in this study are available at https://github.com/JizhiziLi/RIM.