Recently, there has been progress in CLIP-guided image synthesis, which involves adapting a pre-trained generator to a new domain using only textual labels. This method is efficient and does not require target-domain samples. However, existing approaches still have limitations in the quality of generated images and can suffer from mode collapse. This is because a fixed adaptation direction is used for all cross-domain image pairs, resulting in identical supervision signals. To overcome this issue, we propose a method called Image-specific Prompt Learning (IPL). IPL learns specific prompt vectors for each source-domain image, providing a more precise adaptation direction for each cross-domain image pair. This greatly enhances the flexibility of the target-domain generator. Our evaluations on different domains show that IPL effectively improves the quality and diversity of synthesized images and reduces mode collapse. Furthermore, IPL is independent of the generative model's structure, such as generative adversarial networks or diffusion models. The code for IPL is available at a specified GitHub repository.