Modern machine learning pipelines face limitations in terms of data availability, storage quotas, privacy regulations, and expensive annotation processes. These constraints pose challenges in training and updating large-scale models on dynamic annotated sets. Continual learning aims to address this issue by enabling deep neural networks to learn new patterns for unseen classes without significantly impacting their performance on previously learned ones. This paper focuses on continual learning for video data and proposes a novel method called PIVOT. PIVOT leverages knowledge from pre-trained models in the image domain to reduce the number of trainable parameters and minimize forgetting. Unlike previous approaches, our method effectively utilizes prompting mechanisms for continual learning without any in-domain pre-training. Experimental results demonstrate that PIVOT outperforms state-of-the-art methods by a significant 27% on the 20-task ActivityNet setup.