This study focuses on talking face generation, which involves reconstructing facial movements of lips based on speech input. Previous research has emphasized the importance of synchronizing lip movements with speech and improving visual quality. However, they have largely overlooked the intelligibility of the spoken words, which is a crucial aspect of generating high-quality results. To address this issue, we propose utilizing a lip-reading expert to penalize incorrect lip region generation and enhance visual intelligibility. Additionally, to overcome limited data availability, we train the lip-reading expert in a self-supervised manner using both auditory and visual information. Our approach incorporates contrastive learning to improve lip-speech synchronization and a transformer model to encode audio and video simultaneously, considering the overall temporal dependency of the audio. To evaluate the performance, we introduce a new strategy employing two different lip-reading experts to measure the intelligibility of the generated videos. Through rigorous experiments, we demonstrate that our proposed method outperforms other state-of-the-art approaches, such as Wav2Lip, in terms of reading intelligibility, achieving over 38% Word Error Rate (WER) on the LRS2 dataset and 27.8% accuracy on the LRW dataset. Furthermore, we achieve state-of-the-art performance in lip-speech synchronization and comparable results in visual quality.