Fine-tuning large-scale pre-trained vision models is a common method for achieving top performance on computer vision benchmarks. However, this approach can be inefficient as it requires duplicating the entire model for each task. In this study, we propose a new method called LoRand, which aims to find a better balance between task performance and the number of trainable parameters. LoRand generates small adapter structures with low-rank synthesis, while keeping the original backbone parameters fixed, resulting in greater parameter sharing. We conducted extensive experiments on object detection, semantic segmentation, and instance segmentation tasks to demonstrate the effectiveness of LoRand. By training only a small percentage (1% to 3%) of the pre-trained backbone parameters, LoRand achieves comparable performance to standard fine-tuning on COCO and ADE20K datasets, and even outperforms fine-tuning on the low-resource PASCAL VOC dataset.