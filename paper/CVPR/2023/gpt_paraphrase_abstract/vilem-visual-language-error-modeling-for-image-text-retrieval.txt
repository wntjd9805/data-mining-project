Dominant pre-training methods for image-text retrieval typically use a "dual-encoder" architecture, which involves two encoders to extract image and text representations. However, these methods often overlook detailed semantic associations between images and text. In this study, we propose a new approach called Visual-Language Error Modeling (ViLEM) to address this issue. ViLEM introduces a proxy task that "proofreads" each word in the text against the corresponding image, injecting detailed image-text association into the dual-encoder model. To generate diverse plausible negative texts, we edit the image-paired text using pre-trained language models. ViLEM then trains the model to discriminate the correctness of each word in the negative texts and corrects any wrong words using image information. Additionally, we introduce a multi-granularity interaction framework that combines global and local image features with text features, enabling the association of local text semantics with high-level visual context and multi-level local visual information. Our method outperforms state-of-the-art dual-encoder methods in image-text retrieval and significantly enhances the discriminativeness of local textual semantics. Furthermore, our model demonstrates good generalization to video-text retrieval.