Visible-modal object tracking leads to various multi-modal tracking tasks. Currently, the common approach for multi-modal tracking involves fully fine-tuning the RGB-based parameters of the foundation model. However, this method is not the most optimal due to limited downstream data and poor transferability. Inspired by the success of prompt learning in language models, this paper introduces Visual Prompt multi-modal Tracking (ViPT). ViPT learns modal-relevant prompts to adapt the frozen pre-trained foundation model to different multi-modal tracking tasks. ViPT effectively utilizes the knowledge of the RGB-based model, which is pre-trained at scale, while introducing minimal trainable parameters (less than 1% of model parameters). ViPT outperforms the full fine-tuning approach in multiple downstream tracking tasks, including RGB+Depth, RGB+Thermal, and RGB+Event tracking. Extensive experiments demonstrate the potential of visual prompt learning for multi-modal tracking, with ViPT achieving state-of-the-art performance while maintaining parameter efficiency. The code and models for ViPT are available at https://github.com/jiawen-zhu/ViPT.