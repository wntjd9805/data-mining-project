Current evaluation methods for domain generalization (DG) in deep neural networks are limited by the use of the leave-one-out strategy and a small number of domains. To address this, we propose NICO++, a large-scale benchmark with numerous labeled domains, and introduce rational evaluation methods for assessing DG algorithms comprehensively. We also propose two metrics to measure covariate shift and concept shift in DG datasets. Additionally, we present two novel generalization bounds that demonstrate how limited concept shift and significant covariate shift enhance the evaluation capability for generalization. Through extensive experiments, we demonstrate that NICO++ outperforms existing DG datasets in terms of evaluation capability and its ability to mitigate unfairness resulting from the lack of oracle knowledge in model selection. The data and code for the NICO++ benchmark can be found at https://github.com/xxgege/NICO-plus.