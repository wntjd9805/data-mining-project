This paper focuses on addressing the issue of robust deep single-image reflection removal (SIRR) against adversarial attacks. Current SIRR methods based on deep learning have shown a significant decrease in performance due to imperceptible distortions and perturbations on input images. To comprehensively study the robustness of SIRR, we conduct various adversarial attacks specifically designed for the SIRR problem, targeting different regions and objectives. To tackle this problem, we propose a robust SIRR model that incorporates three key components: the cross-scale attention module, the multi-scale fusion module, and the adversarial image discriminator. The model leverages the multi-scale mechanism to minimize the disparity between features extracted from clean and adversarial images. Additionally, the image discriminator adaptively distinguishes between clean and noisy inputs, enhancing the robustness of the model. We extensively evaluate our model on Nature, SIR2, and Real datasets, demonstrating its significant improvement in the robustness of SIRR across various scenes.