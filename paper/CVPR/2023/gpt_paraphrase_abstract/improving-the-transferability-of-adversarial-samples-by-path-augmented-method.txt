Deep neural networks have made great progress in various vision tasks, but they are susceptible to imperceptible adversarial noise, which poses a challenge for their deployment in real-world scenarios, particularly security-related ones. Transfer-based attacks, which create adversarial samples using a local model, have gained attention due to their efficiency in evaluating the robustness of target models. However, current transfer-based attacks rely on data augmentation methods that select image augmentation paths heuristically, potentially including semantics-inconsistent images that reduce the transferability of the generated adversarial samples. To address this issue, we propose the Path-Augmented Method (PAM). PAM involves constructing a pool of candidate augmentation paths and using a greedy search to determine the employed augmentation paths during adversarial sample generation. Additionally, we train a Semantics Predictor (SP) to limit the length of the augmentation path and avoid augmenting semantics-inconsistent images. Extensive experiments show that PAM outperforms state-of-the-art baselines by an average improvement of over 4.8% in terms of attack success rates.