This paper presents a new approach to capturing the performance of clothed humans in sparse-view or monocular videos. Existing methods either use personalized templates for full human performance capture or recover garments from a single frame with static poses. However, these methods have limitations such as the inconvenience of extracting cloth semantics with a one-piece template and unstable tracking in single frame-based methods. To overcome these challenges, we propose a novel method that tracks clothing and human body motion separately using double-layer neural radiance fields (NeRFs). Our approach involves using double-layer NeRFs for the body and garments and optimizing the deformation fields and canonical double-layer NeRFs to track the densely deforming template of the clothing and body. We also introduce a physics-aware cloth simulation network to generate realistic cloth dynamics and body-cloth interactions. Compared to existing methods, our approach is fully differentiable and robustly captures both body and clothing motion in dynamic videos. Additionally, we represent clothing with an independent NeRFs, allowing us to model implicit fields of general clothes effectively. Experimental evaluations on real multi-view and monocular videos validate the effectiveness of our method.