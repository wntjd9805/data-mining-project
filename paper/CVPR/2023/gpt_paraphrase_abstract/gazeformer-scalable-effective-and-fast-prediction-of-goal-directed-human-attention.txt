Predicting human gaze is crucial in the field of Human-Computer Interaction (HCI). However, for gaze prediction models to be practically useful in HCI applications, they need to be scalable, fast, and accurate in predicting spatial and temporal gaze patterns. Existing models for scanpath prediction primarily focus on goal-directed attention, which limits their applicability due to their reliance on trained target detectors for all possible objects and the need for human gaze data for training, both of which are not scalable. To address this limitation, we introduce a new task called ZeroGaze, which involves predicting gaze for objects that have never been searched before. We propose a novel model called Gazeformer to solve the ZeroGaze problem. Unlike existing methods that use object detector modules, Gazeformer encodes the target using a natural language model, thereby leveraging semantic similarities in scanpath prediction. We adopt a transformer-based encoder-decoder architecture for Gazeformer, as transformers are particularly effective in generating contextual representations. Our experiments demonstrate that Gazeformer outperforms other models by a significant margin (19%-70%) in the ZeroGaze setting. It also surpasses existing target-detection models in standard gaze prediction tasks for both target-present and target-absent searches. Additionally, Gazeformer is more than five times faster than the current state-of-the-art target-present visual search model. The code for Gazeformer can be accessed at https://github.com/cvlab-stonybrook/Gazeformer/.