Reconstructing and rendering large-scale indoor scenes in real time has been a challenge. While SLAM-based methods can reconstruct scene geometry progressively, they lack photorealistic rendering capabilities. On the other hand, NeRF-based methods can produce high-quality novel views but are computationally expensive and cannot handle online input efficiently. To address these limitations, we propose SurfelNeRF, which combines the advantages of classical 3D reconstruction and NeRF. SurfelNeRF uses a flexible neural surfel representation to store geometric attributes and appearance features extracted from input images. We also introduce a surfel-based fusion scheme to integrate incoming frames into the global neural scene representation. Additionally, we develop a differentiable rasterization scheme for rendering neural surfel radiance fields, resulting in significant speed improvements during training and inference. Experimental results demonstrate that SurfelNeRF achieves state-of-the-art performance on ScanNet, with PSNR values of 23.82 and 29.58 in feedforward inference and per-scene optimization settings, respectively.