Video object detection often faces feature degradation issues that are not commonly seen in the image domain. One approach to address this problem is to utilize temporal information and combine features from neighboring frames. While Transformer-based object detectors have shown improved performance in image domain tasks, recent studies have attempted to extend these methods to video object detection. However, existing Transformer-based video object detectors still follow the same pipeline as classical object detectors, focusing on enhancing object feature representations through aggregation. In this study, we take a different approach to video object detection. Specifically, we enhance the quality of queries for Transformer-based models through aggregation. To achieve this, we propose a vanilla query aggregation module that calculates weighted averages of queries based on neighboring frame features. We then expand this module to a more practical version that generates and aggregates queries based on input frame features. Extensive experiments confirm the effectiveness of our proposed methods. When integrated with our modules, the current state-of-the-art Transformer-based object detectors show improvements of over 2.4% in mAP and 4.2% in AP50 on the challenging ImageNet VID benchmark. The code for our methods is available at https://github.com/YimingCuiCuiCui/FAQ.