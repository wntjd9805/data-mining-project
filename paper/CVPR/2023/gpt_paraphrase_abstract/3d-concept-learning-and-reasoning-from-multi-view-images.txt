Humans have the ability to reason accurately in three dimensions by gathering multiple viewpoints of the environment. To mimic this ability, we have created a new benchmark called 3D multi-view visual question answering (3DMV-VQA). This benchmark consists of a dataset collected by an agent that actively moves and captures RGB images in an environment using the Habitat simulator. The dataset includes approximately 5,000 scenes, 600,000 images, and 50,000 paired questions. We have evaluated various state-of-the-art models for visual reasoning on this benchmark and found that their performance is poor. We propose a principled approach for 3D reasoning, which involves inferring a concise 3D representation of the world from the multi-view images, grounding it on open-vocabulary semantic concepts, and then conducting reasoning on these representations. As a first step towards this approach, we introduce a novel framework called 3D concept learning and reasoning (3D-CLR), which combines neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Experimental results show that our framework significantly outperforms baseline models, but the challenge of 3D reasoning remains largely unsolved. We also analyze the challenges in depth and suggest potential future directions.