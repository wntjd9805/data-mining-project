We introduce Region-aware Open-vocabulary VisionTransformers (RO-ViT), a new approach for image-text pretraining that aims to bridge the gap between image-level pretraining and open-vocabulary object detection. In our method, we modify the pretraining phase by randomly cropping and resizing regions of positional embeddings instead of using the entire image embeddings. This adjustment aligns better with the use of positional embeddings at the region level during the detection finetuning phase. Additionally, we replace the commonly used softmax cross entropy loss in contrastive learning with focal loss to improve the learning of informative yet challenging examples. Furthermore, we leverage recent advancements in novel object proposals to enhance open-vocabulary detection finetuning.  We evaluate our complete model on the LVIS and COCO open-vocabulary detection benchmarks, as well as zero-shot transfer. The results demonstrate that RO-ViT achieves a state-of-the-art average precision (APr) score of 32.1 on LVIS, surpassing the best existing approach by 5.8 points and achieving competitive performance in zero-shot transfer detection. Surprisingly, RO-ViT also enhances image-level representation and sets the state-of-the-art on 9 out of 12 metrics in COCO and Flickr image-text retrieval benchmarks, outperforming competitive approaches that employ larger models.