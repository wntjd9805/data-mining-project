Understanding the movement patterns in dynamic environments is crucial for autonomous driving. This has led to a growing interest in predicting motion in LiDAR point clouds without considering specific object classes. In outdoor scenes, it is often possible to separate the scene into mobile foregrounds and static backgrounds, allowing us to associate motion understanding with scene parsing. Building on this observation, we propose a new weakly supervised approach for motion prediction. Instead of relying on expensive motion annotations, we use partially or fully annotated foreground/background binary masks for supervision (1% or 0.1% annotations). Our approach consists of two stages: in Stage 1, we train a segmentation model using the incomplete binary masks, and in Stage 2, we use this model for self-supervised learning of the motion prediction network by estimating potential moving foregrounds in advance. To enhance the robustness of self-supervised motion learning, we introduce a Consistency-aware Chamfer Distance loss that leverages multi-frame information and effectively suppresses outliers. Extensive experiments demonstrate that our weakly supervised models outperform self-supervised models by a significant margin and perform on par with some supervised models. This highlights the effectiveness of our approach in striking a balance between annotation effort and performance.