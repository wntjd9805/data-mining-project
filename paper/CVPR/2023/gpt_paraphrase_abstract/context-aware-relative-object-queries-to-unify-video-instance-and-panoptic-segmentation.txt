Object queries have become a valuable tool for representing object proposals. However, their application in tasks involving video segmentation raises two important questions. Firstly, how can we process frames in a sequential manner while seamlessly propagating object queries across frames? Using independent object queries for each frame does not allow for tracking and requires additional post-processing. Secondly, how can we generate temporally consistent yet expressive object queries that capture both appearance and position changes? Utilizing the entire video at once fails to capture position changes and is not scalable for long videos. To address these challenges, we propose the use of "context-aware relative object queries" that are continuously propagated frame-by-frame. This approach enables seamless object tracking and handles occlusion and object reappearance without the need for post-processing. Additionally, we demonstrate that context-aware relative object queries effectively capture position changes of moving objects. We evaluate our proposed approach across three difficult tasks: video instance segmentation, multi-object tracking and segmentation, and video panoptic segmentation. By employing the same approach and architecture, we achieve comparable or superior results to state-of-the-art methods on various challenging datasets including OVIS, Youtube-VIS, Cityscapes-VPS, MOTS 2020, and KITTI-MOTS.