This study focuses on the translation of spoken languages into Sign languages to facilitate communication between the hearing and hearing-impaired communities. The researchers propose a novel method for animating written text in HamNoSys, a universal lexical Sign language notation, into signed pose sequences. The method utilizes transformer encoders to generate pose predictions, taking into account the spatial and temporal information of the text and poses. The training process employs weak supervision and demonstrates the ability to learn from incomplete and inaccurate data. The researchers also introduce a new distance measurement, DTW-MJE, which considers missing keypoints to accurately measure the distance between pose sequences. The validity of this measurement is confirmed using the AUTSL Sign language dataset, and it is utilized to evaluate the quality of the generated pose sequences. The code for data pre-processing, the model, and the distance measurement is made publicly available for future research purposes.