Adversarial training is an effective method for defending against adversarial examples. However, it often struggles with a large gap in robustness when faced with unseen adversaries during testing, known as the adversarially robust generalization problem. The relationship between adversarially robust generalization and architectural design has not been well understood. To address this gap, this study systematically explores this relationship for the first time. The researchers evaluate 20 different adversarially trained architectures on ImageNette and CIFAR-10 datasets using various â„“p-norm adversarial attacks. The findings reveal that Vision Transformers, such as PVT and CoAtNet, generally exhibit better adversarially robust generalization compared to CNNs, which tend to overfit on specific attacks and struggle to generalize to multiple adversaries. The researchers conduct a theoretical analysis using Rademacher complexity and discover that higher weight sparsity, achieved through specially-designed attention blocks, significantly contributes to the better adversarially robust generalization of Transformers. The paper aims to enhance the understanding of mechanisms for designing robust deep neural networks. The model weights used in the study can be accessed at http://robust.art.