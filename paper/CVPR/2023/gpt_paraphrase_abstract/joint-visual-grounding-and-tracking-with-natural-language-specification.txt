The aim of tracking by natural language specification is to locate a target in a sequence based on a natural language description. Current algorithms tackle this problem in two separate steps: visual grounding and tracking. This approach overlooks the connection between visual grounding and tracking, which is that natural language descriptions provide semantic cues for localizing the target in both steps. Additionally, the separate framework is difficult to train end-to-end. To address these issues, we propose a joint visual grounding and tracking framework that combines grounding and tracking into a single task: localizing the target based on visual-language references. Our framework includes a multi-source relation modeling module to establish the relationship between visual-language references and the test image. We also introduce a temporal modeling module that utilizes global semantic information to provide temporal clues, enhancing our model's ability to adapt to target appearance variations. Extensive experiments on TNL2K, LaSOT, OTB99, and RefCOCOg datasets demonstrate that our method outperforms state-of-the-art algorithms in both tracking and grounding tasks. The code for our approach is available at https://github.com/lizhou-cs/JointNLT. Figure 1 illustrates the difference between the separated visual grounding and tracking framework and our proposed joint visual grounding and tracking framework.