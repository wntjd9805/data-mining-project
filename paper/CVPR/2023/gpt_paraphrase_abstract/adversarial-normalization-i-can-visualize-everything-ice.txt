The effectiveness of vision transformers in predicting image classes and their explainability visualization has been studied. However, visualizing the explainability of vision transformers is challenging due to various factors such as skip connections, attention operators, and limited reflection of self-attention scores. In this paper, we propose a novel method called ICE (Adversarial Normalization: ICan visualize Everything) to improve the visualization of a vision transformer's explainability. ICE enables the model to directly predict a class for each patch in an image, distinguishing between background and foreground regions. We evaluated ICE on the ImageNet-Segmentation dataset and it outperformed other visualization methods in terms of explainability for different model sizes. We also conducted analyses on weakly-supervised object localization and unsupervised object discovery tasks, and ICE achieved comparable performance to state-of-the-art methods on the CUB-200-2011 and PASCALVOC07/12 datasets. By incorporating ICE into the encoder of the DeiT-S model, we improved efficiency by 44.01% on the ImageNet dataset compared to the original model. Additionally, ICE demonstrated comparable accuracy and efficiency to the state-of-the-art pruning model EViT. The code for ICE is available at https://github.com/Hanyang-HCC-Lab/ICE.