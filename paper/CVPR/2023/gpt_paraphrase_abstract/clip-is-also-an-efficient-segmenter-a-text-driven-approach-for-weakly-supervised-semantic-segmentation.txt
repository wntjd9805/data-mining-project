We address the challenging task of weakly supervised semantic segmentation (WSSS) using image-level labels. Existing approaches have high training costs and follow a multi-stage framework. To overcome these limitations, we investigate the potential of Contrastive Language-Image Pre-training models (CLIP) for localizing categories without additional training. We propose a novel WSSS framework called CLIP-ES that enhances all three stages of WSSS by leveraging CLIP. Firstly, we introduce the softmax function into GradCAM and utilize the zero-shot ability of CLIP to reduce confusion caused by non-target classes and backgrounds. Additionally, we explore text inputs in the WSSS setting and employ two text-driven strategies: sharpness-based prompt selection and synonym fusion. Secondly, we simplify the CAM refinement stage by introducing a real-time class-aware attention-based affinity (CAA) module based on the inherent multi-head self-attention (MHSA) in CLIP-ViTs. Lastly, when training the final segmentation model with masks generated by CLIP, we incorporate a confidence-guided loss (CGL) that focuses on confident regions. Our CLIP-ES achieves state-of-the-art performance on Pascal VOC 2012 and MS COCO 2014 datasets, while significantly reducing the time required for pseudo mask generation compared to previous methods. The code for CLIP-ES is available at https://github.com/linyq2117/CLIP-ES.