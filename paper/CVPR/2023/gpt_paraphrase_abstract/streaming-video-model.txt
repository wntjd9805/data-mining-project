The traditional approach to video understanding tasks involves using separate architectures for sequence-based tasks (such as action recognition) and frame-based tasks (such as multiple object tracking). However, we propose a new approach called Streaming Vision Transformer (S-ViT), which combines both types of tasks into a single architecture. S-ViT utilizes a memory-enabled spatial encoder to extract frame-level features for frame-based tasks, and then uses a task-specific temporal decoder to obtain spatiotemporal features for sequence-based tasks. Our experiments show that S-ViT achieves state-of-the-art accuracy in sequence-based action recognition and outperforms conventional architectures in frame-based multiple object tracking. We believe that S-ViT represents a significant step towards a unified deep learning architecture for video understanding. The code for S-ViT is available at https://github.com/yuzhms/Streaming-Video-Model.