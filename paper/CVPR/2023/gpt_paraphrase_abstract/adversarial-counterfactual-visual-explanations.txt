Counterfactual explanations aim to change the output labels of a model with minimal perturbations, but adversarial attacks are not suitable for this purpose as the perturbations are seen as noise rather than actionable modifications. This paper proposes a method to convert adversarial attacks into meaningful perturbations without modifying the classifiers being explained. The approach suggests that Denoising Diffusion Probabilistic Models can effectively regulate the generation of adversarial attacks to avoid high-frequency and out-of-distribution perturbations. The key idea is to use a diffusion model to refine the attacks, enabling the study of the target model regardless of its level of robustness. Extensive experimentation demonstrates the advantages of this counterfactual explanation approach compared to the current State-of-the-Art in various testbeds.