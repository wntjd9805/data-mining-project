Deep learning in computer vision has made significant progress by using large amounts of labeled training data. However, labeling all data for every task and domain is costly and often inaccurate. Additionally, the collection of data can result in non-IID training and test data with duplicate samples, making it difficult to verify theories and discover new findings. To address these challenges, one approach is to generate synthetic data through 3D rendering with domain randomization. In this study, we extensively explore supervised learning and domain adaptation using synthetic data. By leveraging the controlled environment provided by 3D rendering, we validate important learning insights and uncover new principles related to data regimes and network architectures in generalization. We also investigate how factors such as object scale, material texture, illumination, camera viewpoint, and background in a 3D scene affect generalization. Furthermore, we compare the transferability of synthetic and real data for pre-training through simulation-to-reality adaptation, showing the potential of synthetic data pre-training to improve real-world test results. Lastly, we introduce a new benchmark called S2RDA, which offers significant challenges for transferring from simulation to reality in image classification.