Image-text pretrained models like CLIP have gained attention for their ability to learn multi-modal knowledge from image-text data pairs. These models have the potential to improve visual representation learning in the video domain. This paper focuses on temporal modeling for transferring image-to-video knowledge using the CLIP model. The current temporal modeling mechanisms are specialized for either high-level semantic tasks or low-level visual pattern tasks, but they struggle to address both simultaneously. The challenge lies in modeling temporal dependency while leveraging both high-level and low-level knowledge in the CLIP model. To address this, the paper introduces the Spatial-Temporal Auxiliary Network (STAN), a simple and effective mechanism that extends the CLIP model for various video tasks. STAN utilizes a branch structure with spatial-temporal modules to contextualize multi-level CLIP features. The proposed method is evaluated on two video tasks: Video-Text Retrieval and Video Recognition. Extensive experiments demonstrate the superiority of the STAN model compared to state-of-the-art methods on different datasets. The code for STAN is available at https://github.com/farewellthree/STAN.