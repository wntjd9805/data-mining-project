Understanding dynamic hand motions and actions in egocentric RGB videos is a difficult task due to self-occlusion and uncertainty. To overcome these challenges, we propose a transformer-based framework that leverages temporal information for accurate estimation. Recognizing the distinct temporal characteristics and semantic correlation between hand pose estimation and action recognition, our framework consists of two cascaded transformer encoders. The first encoder focuses on short-term temporal cues to estimate hand poses, while the second encoder aggregates pose and object information over a longer time span to recognize actions. Our approach yields competitive results on two popular first-person hand action benchmarks, FPHA and H2O. Extensive ablation studies confirm the effectiveness of our design choices.