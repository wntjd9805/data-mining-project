Video moment retrieval and highlight detection (MR/HD) have gained attention due to the increasing demand for video understanding. The main goal of MR/HD is to locate specific moments in videos and estimate their relevance to a given text query. However, existing transformer-based models fail to fully utilize the information provided by the query. They often overlook the relationship between the query and video contents when predicting moments and their saliency. To address this issue, we propose Query-Dependent DETR (QD-DETR), a detection transformer designed specifically for MR/HD. Our encoding module incorporates cross-attention layers to explicitly incorporate the context of the text query into video representation. Additionally, we introduce negative (irrelevant) video-query pairs during training, aiming to produce low saliency scores for these pairs. This encourages the model to accurately estimate the relevance between query-video pairs. Furthermore, we introduce an input-adaptive saliency predictor that dynamically defines the criteria for saliency scores based on the given video-query pairs. Extensive experiments demonstrate that building query-dependent representations is crucial for MR/HD. QD-DETR surpasses state-of-the-art methods on QVHighlights, TVSum, and Charades-STA datasets. The source code is available on GitHub at github.com/wjun0830/QD-DETR.