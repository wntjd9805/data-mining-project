Recent advancements in transformer-based methods have shown significant success in 3D human pose estimation. However, the computational cost of calculating the joint-to-joint affinity matrix increases quadratically with the number of joints. This limitation becomes even more problematic for pose estimation in video sequences, which require spatio-temporal correlation across the entire video. To address this issue, we propose a novel approach called Spatio-Temporal Criss-cross attention (STC). The STC block decomposes correlation learning into space and time by dividing the input feature into two partitions along the channel dimension. Spatial and temporal attention is then applied to each partition separately. The STC block models interactions between joints within the same frame and along the same trajectory by combining the outputs from the attention layers. Based on this concept, we develop STCFormer by stacking multiple STC blocks and incorporating a new Structure-enhanced Positional Embedding (SPE) that considers the structure of the human body. The SPE consists of two components: spatio-temporal convolution for capturing local structure around neighboring joints and part-aware embedding for indicating the belonging of each joint to a specific body part. Extensive experiments conducted on benchmark datasets, Human3.6M and MPI-INF-3DHP, demonstrate superior results compared to state-of-the-art approaches. Notably, STCFormer achieves the best published performance to date, with a P1 error of 40.5mm on the challenging Human3.6M dataset.