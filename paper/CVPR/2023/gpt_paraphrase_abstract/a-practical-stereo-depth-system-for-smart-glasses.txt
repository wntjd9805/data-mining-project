We describe the design of a stereo depth sensing system that performs pre-processing, on-line stereo rectification, and stereo depth estimation. It also has the capability to fallback to monocular depth estimation when rectification is unreliable. The resulting depth data is then used in a unique view generation pipeline to create 3D computational photography effects using images captured by smart glasses. Our system is designed to run on a mobile phone with limited computational resources, making it necessary for the design to be general and not dependent on specific hardware or ML accelerators. While each step of the system has been studied individually, there is a lack of practical system descriptions. In our work, we demonstrate how all the steps of the system work together and handle failures or suboptimal input data. We also address challenges such as calibration changes due to heat and ensure that the system meets memory and latency constraints for a smooth user experience. Our trained models are efficient and run in less than 1 second on a six-year-old Samsung Galaxy S8 phone's CPU. We also show that our models perform well on unseen data, achieving good results on benchmark datasets and real-world images captured from the smart glasses.