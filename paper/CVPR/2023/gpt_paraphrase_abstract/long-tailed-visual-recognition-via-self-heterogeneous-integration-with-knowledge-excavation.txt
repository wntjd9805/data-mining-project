Deep neural networks have made significant advancements in recent years. However, they tend to be biased towards majority classes when faced with real-world data that follows a long-tailed distribution. State-of-the-art methods address this issue by using a mixture of experts (MoE) to focus on different parts of the distribution. However, these methods assume that all classes have the same preferences for models with the same depth. In this study, we propose a new MoE-based approach called Self-Heterogeneous Integration with Knowledge Excavation (SHIKE). We introduce Depth-wise Knowledge Fusion (DKF) to combine features from different shallow and deep parts within each expert's network, making the representation more diverse. Additionally, we propose Dynamic Knowledge Transfer (DKT) based on DKF to minimize the influence of the hardest negative class, which has a significant impact on the tail classes in our MoE framework. As a result, SHIKE significantly improves the classification accuracy of long-tailed data, particularly for the tail classes. Our method achieves state-of-the-art performance on CIFAR100-LT, ImageNet-LT, iNaturalist 2018, and Places-LT datasets, with accuracy rates of 56.3%, 60.3%, 75.4%, and 41.9%, respectively. The source code for SHIKE is available at https://github.com/jinyan-06/SHIKE.