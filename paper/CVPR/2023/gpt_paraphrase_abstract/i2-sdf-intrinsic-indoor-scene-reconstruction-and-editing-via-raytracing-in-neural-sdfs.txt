This study introduces a novel method called I2-SDF for indoor scene reconstruction and editing using differentiable Monte Carlo raytracing on neural signed distance fields (SDFs). The proposed framework recovers shapes, incident radiance, and materials from multiple views. A new bubble loss is introduced to enhance the reconstruction of small objects, and an error-guided adaptive sampling scheme is implemented to improve the quality of reconstruction for large-scale indoor scenes. Additionally, the study suggests decomposing the neural radiance field into spatially-varying materials through surface-based, differentiable Monte Carlo raytracing and emitter semantic segmentations. This enables physically based and photorealistic scene relighting and editing. Comparative experiments demonstrate the superior quality of the proposed method in indoor scene reconstruction, novel view synthesis, and scene editing when compared to existing techniques. More information can be found on the project page: https://jingsenzhu.github.io/i2-sdf.