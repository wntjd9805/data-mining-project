This paper introduces a novel approach to generative modeling of multimodal image-text data. While previous methods focused on generating one modality conditioned on the other, this paper proposes a unified generative vision-and-language (VL) model that can produce both images and text sequences. The proposed model, called MAGVLT, is based on a non-autoregressive mask prediction and offers several advantages over the autoregressive approach. These advantages include bidirectional context encoding, fast decoding through parallel token predictions, and extended editing capabilities. To train MAGVLT, the authors combine image-to-text, text-to-image, and joint image-and-text mask prediction tasks. They also introduce two additional tasks to enhance training. Experimental results demonstrate that MAGVLT outperforms previous models, including ARGVLT, in various VL generation tasks. MAGVLT achieves competitive results in zero-shot image-to-text and text-to-image generation without the need for monomodal data and networks.