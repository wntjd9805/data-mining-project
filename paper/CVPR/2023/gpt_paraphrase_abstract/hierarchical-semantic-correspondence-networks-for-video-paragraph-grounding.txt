Video Paragraph Grounding (VPG) is a challenging task in understanding vision and language, where the goal is to locate multiple events in an untrimmed video based on a paragraph query description. The main challenge lies in understanding the complex semantic relations between visual and textual modalities. Existing methods focus on modeling contextual information at the sentence level, overlooking the rich correspondence relations at different semantic levels, such as video-word and video-paragraph correspondence.In this study, we propose a novel approach called the Hierarchical Semantic Correspondence Network (HSCNet) to address this limitation. HSCNet aims to explore multi-level visual-textual correspondence by learning hierarchical semantic alignment and using dense supervision to ground diverse levels of queries. To achieve this, we develop a hierarchical encoder that encodes the multi-modal inputs into aligned representations at different levels. This encoding captures the semantic relations between visual and textual information.To enable multi-level supervision based on the hierarchical semantic correspondence, we design a hierarchical decoder. This decoder progressively performs finer grounding for lower-level queries, taking into account the higher-level semantics. By leveraging the hierarchical structure of the encoder-decoder architecture, our method effectively grounds events in videos based on paragraph queries.Extensive experiments conducted on two challenging benchmarks, namely ActivityNet-Captions and TACoS, demonstrate the effectiveness of HSCNet. Our method outperforms state-of-the-art approaches, showcasing its superiority in video paragraph grounding.