Current deep neural network (DNN) based methods for video anomaly detection (VAD) focus on frame reconstruction or prediction, but they lack the ability to mine higher-level visual features and temporal context relationships. To address this limitation, we propose a new approach inspired by video codec theory. Our method introduces the task of video event restoration based on keyframes, where the DNN is trained to infer missing frames and restore the video event. This encourages the DNN to learn and extract higher-level visual features and comprehensive temporal context relationships. We present a novel U-shaped SwinTransformer Network with Dual Skip Connections (USTN-DSC) for video event restoration. The USTN-DSC incorporates a cross-attention mechanism and a temporal upsampling residual skip connection to enhance the restoration of complex static and dynamic motion object features in the video. Additionally, we propose an adjacent frame difference loss to ensure motion consistency in the video sequence. Experimental results on benchmark datasets demonstrate that USTN-DSC outperforms existing methods, indicating the effectiveness of our approach.