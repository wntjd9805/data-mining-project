This abstract discusses the challenges of training generative models and representation learning models independently in computer vision tasks. It proposes a framework called MAsked Generative Encoder (MAGE) that integrates state-of-the-art image generation and self-supervised representation learning. MAGE utilizes variable masking ratios during pre-training to enable both generative training and representation learning within the same framework. It incorporates semantic tokens learned by a vector-quantized GAN and masking techniques. Additionally, a contrastive loss is added to enhance the representation produced by the encoder. The performance of MAGE is extensively evaluated on ImageNet-1K dataset, where a single MAGE ViT-L model achieves impressive results in both image generation and representation learning tasks. The abstract concludes by providing a link to the available code for MAGE implementation.