Traditional methods for understanding 3D scenes rely on labeled datasets to train models for specific tasks with supervision. In contrast, we propose OpenScene, a different approach that utilizes a model to predict dense features for 3D scene points. These features are co-embedded with text and image pixels in the CLIP feature space. This novel zero-shot approach allows for task-agnostic training and open-vocabulary queries.By employing OpenScene, we can achieve state-of-the-art zero-shot 3D semantic segmentation. The model first infers CLIP features for each 3D point and then classifies them based on similarities to embeddings of arbitrary class labels. This methodology not only enables zero-shot segmentation but also opens up a range of scene understanding applications that were previously unexplored.One of the remarkable benefits of OpenScene is the ability for users to enter any text query and visualize which parts of a scene correspond to that query through a heat map. This functionality is particularly useful for identifying objects, materials, affordances, activities, and room types in complex 3D scenes. Importantly, all of this is accomplished using a single model that is trained without any labeled 3D data.In summary, our proposed OpenScene approach revolutionizes 3D scene understanding by offering task-agnostic training and open-vocabulary queries. It allows for zero-shot 3D semantic segmentation and enables various scene understanding applications that were previously unattainable. Our approach is effective in identifying various scene components in complex 3D scenes, all accomplished through a single model trained without labeled 3D data.