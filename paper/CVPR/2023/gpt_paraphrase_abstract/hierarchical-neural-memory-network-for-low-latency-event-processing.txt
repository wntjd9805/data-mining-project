This abstract presents a novel neural network architecture designed for event-based dense prediction tasks with low latency. Unlike traditional architectures that encode scene contents at a fixed rate, regardless of their temporal characteristics, the proposed network adjusts the temporal scale of encoding based on the movement speed. This is achieved by constructing a temporal hierarchy using stacked latent memories operating at different rates. By gradually propagating information from fast to slow memory modules, the architecture extracts dynamic to static scene contents from low latency event streams. This approach reduces redundancy in conventional architectures and leverages long-term dependencies. Additionally, an attention-based event representation efficiently encodes sparse event streams into memory cells. The proposed approach is evaluated on three event-based dense prediction tasks, where it outperforms existing methods in terms of accuracy and latency while demonstrating effective event and image fusion capabilities. The code for this architecture is available at https://hamarh.github.io/ hmnet/.