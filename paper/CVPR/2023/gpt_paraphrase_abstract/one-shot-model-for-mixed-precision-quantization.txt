Neural network quantization is a popular technique for compressing models, but finding the optimal bit width is challenging. Most existing methods find a single mixed-precision architecture, requiring multiple search restarts to find a suitable architecture. This study focuses on gradient-based optimization methods for finding tensor bit width. The authors derive several theoretical methods and propose a novel One-Shot method that efficiently finds a diverse set of Pareto-front architectures. The proposed method is 5 times more efficient than existing methods for large models. The authors verify the method on classification and super-resolution models, achieving a correlation score above 0.93 between predicted and actual model performance. The Pareto-front architecture selection is straightforward, requiring only 20 to 40 supernet evaluations, which is a new state-of-the-art result.