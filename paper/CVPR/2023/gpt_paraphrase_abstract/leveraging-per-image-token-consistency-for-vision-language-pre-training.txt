Current approaches to vision-language pre-training (VLP) typically use cross-modal masked language modeling (CMLM) to learn the association between vision and language. However, we have observed that CMLM has limitations in achieving this goal. Firstly, there is a modality bias where a significant number of masked tokens can be accurately predicted using only language information, neglecting the visual inputs. Secondly, CMLM primarily focuses on the masked tokens, failing to effectively utilize the unmasked tokens for learning vision-language associations.To address these issues, we propose a new method called EPIC (Leveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, we employ a saliency-based masking strategy to mask tokens in an image-sentence pair that are relevant to the image. These masked tokens are then replaced with alternative options generated by a language model through an inconsistent token generation procedure. The model is then tasked with determining the consistency between each token in the sentence and the image through an Image-Token Consistency Task.EPIC can be easily integrated with existing pre-training methods. Through extensive experiments, we demonstrate that combining EPIC with state-of-the-art pre-training approaches such as ViLT, ALBEF, METER, and X-VLM results in significant improvements in downstream tasks. We have made our code available at https://github.com/gyhdog99/epic.