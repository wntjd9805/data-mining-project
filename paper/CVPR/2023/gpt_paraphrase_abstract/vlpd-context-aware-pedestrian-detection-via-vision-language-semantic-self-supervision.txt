Accurately detecting pedestrians in urban environments is crucial for applications such as autonomous driving and video surveillance. However, the presence of objects resembling humans often leads to incorrect detections, and pedestrians that are small or heavily occluded are easily missed due to their unusual appearances. Simply relying on object regions is insufficient, so the challenge lies in effectively utilizing explicit and semantic contexts. Existing context-aware pedestrian detectors either learn latent contexts using visual clues alone or require time-consuming annotations to obtain explicit and semantic contexts. To address this issue, we propose a novel approach called Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD). Our method models semantic contexts explicitly without the need for additional annotations. Firstly, we introduce a self-supervised Vision-Language Semantic (VLS) segmentation method. This method learns both fully-supervised pedestrian detection and contextual segmentation by using self-generated explicit labels of semantic classes through vision-language models. Additionally, we propose a self-supervised Prototypical Semantic Contrastive (PSC) learning method. This method better discriminates pedestrians from other classes by leveraging the more explicit and semantic contexts obtained from VLS. Extensive experiments on widely-used benchmarks demonstrate that our proposed VLPD outperforms previous state-of-the-art methods, especially in challenging scenarios like small-scale objects and heavy occlusion. The code for our approach is publicly available at https://github.com/lmy98129/VLPD.