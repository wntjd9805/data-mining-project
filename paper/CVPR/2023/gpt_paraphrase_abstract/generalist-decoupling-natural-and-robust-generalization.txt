Deep neural networks trained using standard methods often struggle with adversarial examples. While adversarial training can defend against these examples, it typically results in a decrease in natural generalization. To address this problem, we propose a new approach called Generalist, which separates natural and robust generalization and uses different training strategies for each. Instead of minimizing a global loss, Generalist trains base learners with task-specific strategies so they can specialize in their respective fields. The parameters of the base learners are periodically combined to create a global learner, which is then used to continue training the base learners. Theoretical analysis shows that the risks of Generalist decrease as the base learners improve. Extensive experiments demonstrate that Generalist achieves high accuracy on natural examples while maintaining considerable robustness to adversarial examples. Notably, Generalist outperforms other advanced adversarial training methods in terms of both clean accuracy and robust accuracy. Importantly, Generalist achieves these improvements without significant computational overhead or increasing the model size. The code for Generalist is available at https://github.com/PKU-ML/Generalist.