Zero-shot quantization (ZSQ) is a promising technique for compressing and speeding up deep neural networks in scenarios where training data for full-precision models is not available. ZSQ achieves network quantization by using synthetic samples, but the performance of quantized models heavily relies on the quality of these synthetic samples. However, existing ZSQ methods suffer from a critical drawback: the synthetic samples can be easily fitted by models, leading to significant performance degradation on challenging samples. To overcome this issue, we propose a new method called HArd sample Synthesizing and Training (HAST). HAST focuses on synthesizing hard samples and ensures that these samples are difficult to fit during the training of quantized models. Additionally, HAST aligns the features extracted by full-precision and quantized models to guarantee similarity. Our extensive experiments demonstrate that HAST outperforms existing ZSQ methods and achieves performance comparable to models quantized with real data.