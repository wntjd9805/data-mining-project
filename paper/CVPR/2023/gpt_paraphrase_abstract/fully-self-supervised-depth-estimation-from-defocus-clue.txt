Depth-from-defocus (DFD) techniques have shown great potential in estimating depth from defocus patterns in images. However, existing self-supervised methods for DFD rely on all-in-focus (AIF) images, which are not practical to capture in real-world scenarios. This limitation hinders the application of DFD methods. To address this issue, we propose a fully self-supervised framework that can estimate depth solely from a sparse focal stack. Our framework eliminates the need for depth and AIF image ground-truth and produces superior depth predictions. This allows DFD methods to be more applicable in real-world settings. We introduce a more realistic setting for DFD tasks where no depth or AIF image ground-truth is available and present a novel self-supervision framework that generates reliable depth and AIF image predictions under this challenging setting. Our framework combines a neural model for depth and AIF image prediction with an optical model for validation and refinement. We evaluate our framework on three benchmark datasets, including both rendered and real focal stacks. The qualitative and quantitative results demonstrate that our method establishes a strong baseline for self-supervised DFD tasks. The source code for our framework is publicly available at https://github.com/Ehzoahis/DEReD.