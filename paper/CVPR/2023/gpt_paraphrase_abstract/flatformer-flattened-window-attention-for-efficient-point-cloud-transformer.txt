The Transformer model has been successful in various domains, but its application to 3D point cloud data has been hindered by high latency compared to sparse convolution-based models. This is because transformers are designed for dense and regular data, while point clouds are sparse and irregular. To address this issue, this paper introduces FlatFormer, which sacrifices spatial proximity for better computational regularity. The point cloud is flattened using window-based sorting and grouped into equal-sized partitions. This approach eliminates the need for expensive structuring and padding operations. Self-attention is then applied within the groups to extract local features, alternate sorting axes are used to gather features from different directions, and windows are shifted to exchange features across groups. FlatFormer achieves state-of-the-art accuracy on the Waymo Open Dataset, with a 4.6x speedup over the transformer-based SST model and a 1.4x speedup over the sparse convolutional CenterPoint model. This is the first point cloud transformer to achieve real-time performance on edge GPUs and outperforms sparse convolutional methods in terms of speed while matching or surpassing their accuracy on large-scale benchmarks.