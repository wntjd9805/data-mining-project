We present a regularization technique called submodel co-training, which is similar to co-training, self-distillation, and stochastic depth. In this method, we create two modified versions of the neural network, referred to as "sub-models", by activating only a subset of the layers in each network. These sub-models act as soft teachers for each other, providing additional loss functions to complement the regular loss from the one-hot labels. Our approach, known as "co-sub", utilizes a single set of weights and does not require a pre-trained external model or temporal averaging.Through experiments, we demonstrate that submodel co-training is effective for training recognition backbones in tasks such as image classification and semantic segmentation. Our technique is compatible with various architectures, including RegNet, ViT, PiT, XCiT, Swin, and ConvNext. We observe improvements in the results of these architectures under comparable conditions. For example, a ViT-B model pre-trained with cosub on ImageNet-21k achieves a top-1 accuracy of 87.4% @448 on the ImageNet validation set.