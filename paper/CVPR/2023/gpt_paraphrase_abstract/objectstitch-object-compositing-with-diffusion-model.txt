This study focuses on the challenging task of object compositing using 2D images. Traditionally, this process involves multiple stages such as color matching, geometry correction, and shadow generation to achieve realistic results. However, the manual annotation of training data pairs for compositing is labor-intensive and not scalable. To address this, we propose a self-supervised framework that leverages conditional diffusion models to generate composite objects without the need for manual labeling. Our framework can transform the viewpoint, geometry, color, and shadow of the generated object while preserving its original characteristics using a content adaptor. Additionally, we employ data augmentation techniques to enhance the fidelity of the generator. Through a user study on real-world images, our method surpasses other baselines in terms of realism and faithfulness of the synthesized results.