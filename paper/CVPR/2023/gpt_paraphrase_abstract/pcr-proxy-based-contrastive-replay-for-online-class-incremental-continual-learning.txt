Online class-incremental continual learning is a specific task within continual learning where the goal is to continuously learn new classes from a data stream. The challenge in this task is the issue of catastrophic forgetting, which refers to the forgetting of historical knowledge of old classes. Existing methods address this issue by replaying old data in a proxy-based or contrastive-based manner. However, these methods have their limitations. Proxy-based replay tends to favor new classes due to class imbalance issues, while contrastive-based replay is unstable and difficult to converge due to limited samples. In this study, we thoroughly analyze these two replay methods and discover that they can complement each other. Building on this finding, we propose a new replay-based method called proxy-based contrastive replay (PCR). The key idea of PCR is to replace the contrastive samples with corresponding proxies in a contrastive-based manner. This approach effectively addresses the issue of catastrophic forgetting by mitigating class imbalance and ensures faster convergence of the model. We conduct extensive experiments on three real-world benchmark datasets and consistently demonstrate the superiority of PCR over various state-of-the-art methods. Overall, our findings highlight the importance of combining proxy-based and contrastive-based replay methods to improve online class-incremental continual learning.