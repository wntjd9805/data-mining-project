This paper aims to develop an automated model for generating text-based Audio Description (AD) for movies. However, generating high-quality AD is challenging due to the reliance on contextual information and the limited training data available. To overcome these challenges, we utilize pretrained foundation models like GPT and CLIP and focus on training a mapping network that connects these models for generating visually-conditioned text. Our contributions to achieving high-quality AD include: (i) incorporating context from the movie clip, previous AD, and subtitles; (ii) addressing the lack of training data by pretraining on large-scale datasets that lack visual or contextual information; (iii) enhancing existing AD datasets by removing label noise in the MAD dataset and adding character naming information; and (iv) achieving promising results in the movie AD task compared to previous methods.