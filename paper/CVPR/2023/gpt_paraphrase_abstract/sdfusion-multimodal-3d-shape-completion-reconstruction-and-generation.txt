This study introduces a new framework designed to simplify the process of creating 3D assets for novice users. The framework allows for interactive generation by supporting various input types, such as images, text, partially observed shapes, and combinations of these inputs, with the ability to adjust the influence of each input. The core of the approach involves an encoder-decoder system that compresses 3D shapes into a compact representation, which is then utilized in a diffusion model. To accommodate multi-modal inputs, task-specific encoders with dropout and cross-attention mechanisms are employed. The model's flexibility enables it to handle a range of tasks, outperforming previous methods in shape completion, image-based 3D reconstruction, and text-to-3D. Notably, the model can integrate all these tasks into a single tool, allowing users to generate shapes using incomplete shapes, images, and textual descriptions simultaneously. Users can assign relative weights to each input and interact with the system. Additionally, the study demonstrates an efficient method to apply textures to the generated shapes using large-scale text-to-image models.