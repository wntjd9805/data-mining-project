Understanding scenes in movies is essential for various applications, including video moderation, search, and recommendation. However, labeling individual scenes is a time-consuming process. On the other hand, movie-level metadata, such as genre and synopsis, is readily available as part of the film production process. In this study, we propose a novel approach to learning a versatile scene representation using movie metadata. We utilize movie metadata to determine the similarity between movies and leverage it in contrastive learning to focus our search for positive scene pairs within similar movies. Our learned scene representation consistently outperforms state-of-the-art methods on diverse tasks evaluated using multiple benchmark datasets. Specifically, our representation shows an average improvement of 7.9% on seven classification tasks and a 9.7% improvement on two regression tasks in the LVU dataset. Additionally, we evaluate our scene representation on a newly collected movie dataset, demonstrating its generalizability to previously unexplored video moderation tasks.