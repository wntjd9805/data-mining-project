Nowadays, mobile devices commonly have an asymmetric dual-lens configuration that captures wide-angle and telephoto images of the same scene, enabling realistic super-resolution (SR). However, the degradation for modeling realistic SR is specific to each image due to unknown factors like camera motion. In this study, we introduce ZeDuSR, a zero-shot solution for dual-lens SR that learns an image-specific SR model using only the dual-lens pair at test time. ZeDuSR adapts to the current scene without requiring external training data, eliminating generalization difficulties. However, achieving this goal presents two main challenges: aligning the dual-lens pair while preserving realistic degradation and effectively utilizing limited training data. To overcome these challenges, we propose a degradation-invariant alignment method and a degradation-aware training strategy that maximizes the information within a single dual-lens pair. Extensive experiments demonstrate the superiority of ZeDuSR compared to existing solutions using both synthesized and real-world dual-lens datasets. The implementation code can be found at https://github.com/XrKang/ZeDuSR.