Recently, there have been significant advancements in text-to-image generation models, resulting in high-quality outputs. However, these models lack fine-grained control over the shapes and layout of different regions or objects in the generated images. Previous attempts to address this issue relied on fixed labels, limiting their effectiveness. This paper introduces SpaText, a novel approach for text-to-image generation that allows open-vocabulary scene control. In addition to a global text prompt describing the overall scene, the user provides a segmentation map annotated with free-form natural language descriptions for each region of interest. To overcome the lack of large-scale datasets with detailed textual descriptions for image regions, the authors leverage existing text-to-image datasets and propose a novel spatio-textual representation based on CLIP. The effectiveness of this approach is demonstrated on two state-of-the-art diffusion models: pixel-based and latent-based. The paper also extends the classifier-free guidance method in diffusion models to the multi-conditional case and presents an alternative accelerated inference algorithm. Furthermore, the authors propose several automatic evaluation metrics, including FID scores and a user study, to evaluate their method's performance. The results indicate that the proposed approach achieves state-of-the-art results in generating images with fine-grained textual scene control.