The existing approach to 3D dense captioning involves a complex process of detecting objects and then generating captions for them, using many manually designed components. However, this approach is not ideal for handling crowded scenes and variations in object distributions across different scenarios. In this paper, we propose a straightforward yet effective framework called Vote2Cap-DETR, based on the recent popular DETR (DEtection TRansformer) model. Our framework offers several advantages over previous methods: 1) It eliminates the need for numerous hand-crafted components by employing a transformer encoder-decoder architecture with a learnable vote query driven object decoder and a caption decoder that generates dense captions in a set-prediction manner. 2) Unlike the two-stage approach, our method performs object detection and captioning in a single stage. 3) Through extensive experiments on two widely used datasets, ScanRefer and Nr3D, we demonstrate that our Vote2Cap-DETR outperforms current state-of-the-art methods by 11.13% and 7.11% in CIDEr@0.5IoU, respectively. The code for our framework will be made available soon.