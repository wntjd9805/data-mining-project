A lightweight and efficient model called POTTER (POoling aTtention TransformER) is proposed in this paper for the task of human mesh recovery (HMR) from single images. While transformer architectures have achieved state-of-the-art performance in HMR, they come with high memory and computational overhead. To address this issue, POTTER introduces an efficient pooling attention module that significantly reduces memory and computational costs without compromising performance. Additionally, a new transformer architecture is designed by integrating a High-Resolution (HR) stream, which leverages high-resolution local and global features to improve the accuracy of human mesh recovery. The performance of POTTER is compared to the SOTA method METRO, and it is found that POTTER requires only 7% of total parameters and 14% of Multiply-Accumulate Operations on the Human3.6M and 3DPW datasets. The project webpage for POTTER can be found at https://zczcwh.github.io/potter_page/.