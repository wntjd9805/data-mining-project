Upcycling deep neural networks (DNNs) to adapt them to new tasks in the presence of domain and category shifts is a challenging problem. Unsupervised domain adaptation (UDA), particularly Source-free Domain Adaptation (SFDA), has emerged as a promising solution. However, existing SFDA methods are limited to closed-set settings where the source and target domains share the same label space. This paper proposes Source-free Universal Domain Adaptation (SF-UniDA) to address this limitation. SF-UniDA aims to identify "known" data samples under both domain and category shifts and reject "unknown" data samples using only the knowledge from a pre-trained source model. To achieve this, the paper introduces a novel global and local clustering learning technique (GLC). GLC includes an adaptive one-vs-all global clustering algorithm to distinguish between different target classes and a local k-NN clustering strategy to mitigate negative transfer. The superiority of GLC is evaluated on multiple benchmarks with various category shift scenarios, including partial-set, open-set, and open-partial-set domain adaptation. Notably, in the challenging open-partial-set scenario, GLC outperforms UMAD by 14.8% on the VisDA benchmark. The code for GLC is available at https://github.com/ispc-lab/GLC.