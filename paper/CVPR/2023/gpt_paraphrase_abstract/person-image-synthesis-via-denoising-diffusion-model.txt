This study focuses on the generation of photorealistic images of human beings in various poses. Existing methods using generative adversarial networks struggle with maintaining realistic textures and handling complex deformations and occlusions. The authors propose a Person Image Diffusion Model (PIDM) that breaks down the transfer problem into simpler forward-backward denoising steps. This approach enables the learning of plausible source-to-target transformation trajectories that preserve faithful textures and undistorted appearance details. To accurately model the correspondences between appearance and pose information, a texture diffusion module based on cross-attention is introduced. Additionally, disentangled classifier-free guidance is proposed to ensure the synthesized output closely resembles the conditional inputs in terms of both pose and appearance information. The proposed approach is evaluated on two large-scale benchmarks and through a user study, demonstrating its photorealism under challenging scenarios. The authors also highlight how the generated images can be useful in downstream tasks. The code for implementing the PIDM model is available at https://github.com/ankanbhunia/PIDM.