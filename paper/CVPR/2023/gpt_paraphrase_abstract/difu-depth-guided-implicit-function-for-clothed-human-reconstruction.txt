Recently, there has been a growing interest in function-based methods for reconstructing clothed human figures using a single image. These methods typically rely on a 3D embedding branch, such as the skinned multi-person linear (SMPL) model, to compensate for the limited information available in a single image. However, in this paper, we propose a new method called DIFu that goes beyond the SMPL model by incorporating both parametric and non-parametric 3D information from a projected depth prior. DIFu consists of a generator, an occupancy prediction network, and a texture prediction network. The generator takes a front-side RGB image of the human as input and generates a back-side image. Depth maps for both the front and back images are then estimated and projected into a 3D volume space. The occupancy prediction network extracts features from the front and back images using both 2D and 3D encoders, and uses these features to estimate occupancy. Notably, the voxel-aligned features obtained from the projected depth maps contain detailed 3D information such as hair and clothing. The texture inference branch also estimates the colors of each query point. We demonstrate the effectiveness of DIFu by comparing it to other recent function-based models, both quantitatively and qualitatively.