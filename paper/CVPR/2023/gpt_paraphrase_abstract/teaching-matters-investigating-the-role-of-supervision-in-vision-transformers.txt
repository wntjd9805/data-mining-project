In recent years, Vision Transformers (ViTs) have become increasingly popular and have been applied in various fields. However, their behavior under different learning approaches has not been thoroughly investigated. This study compares ViTs trained using different methods of supervision and demonstrates that they exhibit a wide range of behaviors in terms of attention, representations, and downstream performance. Interestingly, a consistent behavior observed across supervision methods is the emergence of Offset Local Attention Heads, which are self-attention heads that focus on a nearby token with a fixed directional offset. This phenomenon has not been previously highlighted in any published work. The analysis reveals that ViTs possess high flexibility and adapt their processing of local and global information based on the training method employed. Furthermore, it is found that contrastive self-supervised methods achieve competitive features compared to explicitly supervised features and can even outperform them in certain tasks focused on specific parts. Additionally, the representations of reconstruction-based models exhibit noteworthy similarities to contrastive self-supervised models.