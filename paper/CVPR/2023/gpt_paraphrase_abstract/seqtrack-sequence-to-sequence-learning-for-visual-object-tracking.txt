This paper introduces a novel approach to visual tracking called SeqTrack, which treats tracking as a sequence generation task. Unlike previous methods that rely on complex head networks, SeqTrack employs a simple encoder-decoder transformer architecture. The encoder extracts visual features using a bidirectional transformer, while the decoder generates bounding box values sequentially using a causal transformer. The loss function used is cross-entropy. This sequence learning paradigm simplifies the tracking framework and achieves competitive performance on benchmarks. In particular, SeqTrack achieves a state-of-the-art performance of 72.5% AUC on the LaSOT benchmark. The code and models for SeqTrack can be found at https://github.com/microsoft/VideoX.