We propose a straightforward yet efficient method for self-supervised pre-training in image harmonization, utilizing large-scale unannotated image datasets. Our approach involves generating pre-training data using the Label-Efficient Masked Region Transform (LEMaRT) pipeline. This pipeline generates a foreground mask and applies transformations to alter various visual attributes, such as defocus blur, contrast, and saturation, within the specified mask region. Our pre-training process involves training image harmonization models to reconstruct the original image from the perturbed image. Additionally, we introduce a novel image harmonization model called SwinIH, which enhances the Swin Transformer with a combination of local and global self-attention mechanisms. Pre-training SwinIH using LEMaRT leads to a new state-of-the-art performance in image harmonization. Importantly, our method is label-efficient, requiring less annotated data for fine-tuning compared to existing approaches. Notably, on the iHarmony4 dataset, SwinIH outperforms the current state-of-the-art method, SCS-Co, by 0.4 dB when fine-tuned with only 50% of the training data and by 1.0 dB when trained on the full dataset.