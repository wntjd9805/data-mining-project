This paper examines the challenge of learning multi-layer generator models. These models involve building multiple layers of latent variables as a prior model on top of the generator, which helps in learning complex data distributions and hierarchical representations. However, the conventional approach of modeling inter-layer relations between latent variables using non-informative Gaussian distributions can limit the model's expressivity. To address this limitation and improve the prior models' expressiveness, the paper proposes an energy-based model (EBM) that operates on the joint latent space across all layers of latent variables, with the multi-layer generator as its foundation. This joint latent space EBM prior model captures the contextual relations within each layer using layer-wise energy terms, and it corrects latent variables across different layers jointly. The paper introduces a joint training scheme based on maximum likelihood estimation (MLE), which involves Markov Chain Monte Carlo (MCMC) sampling for both the prior and posterior distributions of the latent variables from different layers. To ensure efficient inference and learning, a variational training scheme is also proposed, utilizing an inference model to expedite the costly posterior MCMC sampling. Experimental results demonstrate that the learned model is capable of generating high-quality images and capturing hierarchical features, leading to improved outlier detection.