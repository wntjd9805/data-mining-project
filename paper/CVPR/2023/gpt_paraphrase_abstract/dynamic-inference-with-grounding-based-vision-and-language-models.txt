Recently, transformers have been successfully used for tasks involving vision and language. Models with over 200M parameters have been proposed to learn visual grounding and have shown impressive results on various vision and language tasks. However, these large models suffer from computational redundancy, which affects their runtime efficiency. To tackle this issue, we present a solution called dynamic inference for grounding-based vision and language models, which is conditioned on input image-text pairs. Our approach involves dynamically skipping certain layers in the model and removing redundant tokens at different levels of the backbone. We also fuse image tokens with language tokens in an adaptive manner. To learn the policies for dynamic inference, we leverage reinforcement learning to train agents. We apply this method to a recent model called ViT-MDETR, where we replace the CNN backbone with a vision transformer, resulting in D-ViT-MDETR. We conduct experiments on image-language tasks and observe that our approach significantly improves the runtime efficiency of state-of-the-art models MDETR and GLIP, with up to a 50% reduction in runtime. Furthermore, there is only a minimal accuracy drop of approximately 0.3% on tasks such as Referring Expression Comprehension, Segmentation, and VQA.