Recent advancements in deterministic prompt learning have shown promise as an alternative method for various vision-related tasks. These techniques utilize pre-trained vision-language models to acquire robust visual representations. However, this approach falls short when it comes to dense prediction tasks that involve complex and diverse objects. This is because a single deterministic description is inadequate for representing the entirety of an image. To address this limitation, we introduce a new approach called probabilistic prompt learning, which aims to leverage vision-language knowledge in dense prediction tasks. Our method involves the use of learnable attribute prompts that describe universal attributes across different object classes. These attributes, along with class information and visual-context knowledge, are combined to create class-specific textual distributions. By sampling text representations and employing a probabilistic pixel-text matching loss, we guide the dense prediction task, resulting in improved stability and generalization capabilities. We conducted extensive experiments and ablation studies on various dense prediction tasks, and the results demonstrate the effectiveness of our proposed method.