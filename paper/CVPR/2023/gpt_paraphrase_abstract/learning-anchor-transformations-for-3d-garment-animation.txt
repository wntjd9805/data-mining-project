This paper introduces a novel model called AnchorDEF, which aims to predict 3D garment animation based on body motion sequences. The model utilizes a combination of rigid transformations and nonlinear displacements to deform a garment mesh template. Anchors are strategically placed around the mesh surface to guide the learning process of rigid transformation matrices. Once the anchor transformations are determined, the model regresses per-vertex nonlinear displacements in a canonical space, simplifying the complexity of deformation space learning. The consistency of position, normal, and direction of the transformed anchors is explicitly maintained to ensure the physical meaning of learned anchor transformations and improve generalization. In addition, the model proposes an adaptive anchor updating technique that optimizes the anchor position by considering local mesh topology, enabling the learning of representative anchor transformations. The effectiveness of AnchorDEF is demonstrated through qualitative and quantitative experiments on various types of garments, showcasing its superior performance in 3D garment deformation prediction during motion, particularly for loose-fitting garments.