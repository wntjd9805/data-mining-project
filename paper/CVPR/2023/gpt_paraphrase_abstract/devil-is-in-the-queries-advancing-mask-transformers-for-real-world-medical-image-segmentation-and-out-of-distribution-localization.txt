Real-world medical image segmentation is complex and involves a wide range of objects, including rare conditions that are clinically significant. It is crucial for medical AI algorithms to effectively handle these rare conditions to avoid potentially harmful outcomes. In this study, we propose a method called MaxQuery, which uses object queries in Mask Transformers to perform semantic segmentation. During training, the queries are fitted to the cluster centers of the inliers, allowing for the detection and localization of out-of-distribution (OOD) regions in real-world medical images. Additionally, we address the challenge of distinguishing between foreground objects (both OOD and inliers) and the background, which can cause the queries to focus excessively on the background. To overcome this, we introduce a query-distribution (QD) loss that enforces clear boundaries between segmentation targets and other regions at the query level, thereby improving both inlier segmentation and OOD indication. We evaluate our framework on two real-world segmentation tasks involving pancreatic and liver tumors, and our results show significant improvements over previous state-of-the-art algorithms. Specifically, our framework achieves an average improvement of 7.39% on AUROC, 14.69% on AUPR, and 13.79% on FPR95 for OOD localization. Furthermore, our framework outperforms the leading baseline nnUNet by an average of 5.27% on DSC for inlier segmentation.