This study addresses the issue of open-vocabulary semantic segmentation (OVS), which involves segmenting objects from various classes that are not predefined. The authors propose a transformer-based model called OVSegmentor, which is trained using web-crawled image-text pairs without mask annotations. OVSegmentor uses a slot-attention based binding module to group image pixels into learnable tokens, which are then aligned with corresponding caption embeddings. Two proxy tasks, masked entity completion and cross-image mask consistency, are introduced for training. Masked entity completion infers masked entities in the caption using group tokens, allowing the model to learn fine-grained alignment between visual groups and text entities. Cross-image mask consistency ensures consistent mask predictions for images with shared entities, promoting visual invariance learning. A new dataset called CC4M is constructed for pre-training by filtering CC12M based on frequently appearing entities, resulting in improved training efficiency. The performance of OVSegmentor is evaluated on four benchmark datasets, namely PASCAL VOC, PASCAL Context, COCO Object, and ADE20K, through zero-shot transfer. The results demonstrate that OVSegmentor outperforms existing approaches on PASCAL VOC using only 3% of the data (4M vs 134M) for pre-training.