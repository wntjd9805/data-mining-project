Existing large-scale text-to-image diffusion models have made significant progress in generating images from text descriptions. However, these models often rely solely on text inputs, limiting their controllability. To address this limitation, we propose a novel approach called GLIGEN (Grounded-Language-to-Image Generation) that enhances the functionality of pre-trained text-to-image diffusion models by incorporating grounding inputs. Our method maintains the extensive concept knowledge of the pre-trained model by freezing its weights and introducing new trainable layers with a gated mechanism to incorporate grounding information. By conditioning on caption and bounding box inputs, our model achieves open-world grounded text-to-image generation. Importantly, GLIGEN demonstrates strong generalization capabilities to novel spatial configurations and concepts. In evaluations on COCO and LVIS datasets, our model significantly outperforms existing supervised layout-to-image baselines in terms of zero-shot performance. This work was conducted in collaboration with Microsoft, and the authors are co-senior contributors.