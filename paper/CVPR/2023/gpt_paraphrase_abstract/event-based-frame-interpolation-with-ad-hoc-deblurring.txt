The performance of video frame interpolation is closely linked to how well it handles motion in the input scene. While previous studies acknowledge the usefulness of asynchronous event information for this task, they overlook the fact that motion can result in either blurred or sharp input videos, depending on exposure time and motion speed. They typically assume either sharp input videos and focus solely on frame interpolation, or blurry input videos and include a separate deblurring stage before interpolation. In contrast, we propose a versatile approach for event-based frame interpolation that performs deblurring on-the-fly, allowing it to work effectively on both sharp and blurry input videos. Our method employs a bidirectional recurrent network that naturally incorporates the temporal aspect of interpolation and adaptively combines information from input frames and events based on their temporal proximity. Additionally, we introduce a new high-resolution dataset called HighREV, which includes events and color videos from real-world scenarios. This dataset provides a challenging evaluation environment for the task at hand. Through extensive experiments on the GoPro benchmark and our dataset, we demonstrate that our network consistently outperforms previous state-of-the-art methods in frame interpolation, single image deblurring, and the joint task of interpolation and deblurring. Our code and dataset can be accessed at https://github.com/AHupuJR/REFID.