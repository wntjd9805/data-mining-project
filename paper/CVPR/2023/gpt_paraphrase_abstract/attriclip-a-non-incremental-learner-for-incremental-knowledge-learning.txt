This paper presents AttriCLIP, a learner designed for continual learning, which allows a model to incrementally learn knowledge from sequentially arrived data. Unlike previous approaches that use a conventional classification architecture, AttriCLIP is built upon the pre-trained visual-language model CLIP. It has a fixed image encoder and text encoder to extract features from both images and text. The text includes a category name and a fixed number of learnable parameters that serve as attributes. By computing visual and textual similarity for classification, AttriCLIP avoids the need for incremental expansion of parameters and the use of memory to store rehearsal data. Instead, it utilizes attribute prompts to encode common knowledge useful for classification, effectively mitigating catastrophic forgetting. The performance of AttriCLIP is evaluated against CLIP-based and previous state-of-the-art continual learning methods in realistic settings with domain-shift and long-sequence learning. The results demonstrate that AttriCLIP outperforms previous methods. The implementation code is available at the provided URL.