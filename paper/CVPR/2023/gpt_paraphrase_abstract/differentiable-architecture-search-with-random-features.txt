DARTS, a technique for architecture search, has been influential in the development of NAS techniques due to its efficient and effective search process. However, it suffers from performance collapse. This paper addresses the performance collapse problem in DARTS from two perspectives. First, the expressive power of the supernet in DARTS is investigated, leading to a new setup of the DARTS paradigm with only training Batch-Norm. Second, it is theoretically found that random features dilute the auxiliary connection role of skip-connection in supernet optimization, allowing the search algorithm to focus on fairer operation selection and thus solving the performance collapse issue. The improved versions of DARTS and PC-DARTS, named RF-DARTS and RF-PCDARTS respectively, are instantiated with random features. Experimental results demonstrate that RF-DARTS achieves high test accuracy on CIFAR-10 and achieves state-of-the-art performance on ImageNet when transferred from CIFAR-10. Moreover, RF-DARTS performs consistently well across different datasets and search spaces. RF-PCDARTS achieves even better results on ImageNet, surpassing representative methods directly searched on ImageNet.