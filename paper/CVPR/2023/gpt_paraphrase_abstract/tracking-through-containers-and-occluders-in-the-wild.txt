Tracking objects in cluttered and dynamic environments is a challenging task for computer vision systems. This paper introduces TCOW, a new benchmark and model for visual tracking in scenarios with heavy occlusion and containment. The objective is to segment both the target object and its surrounding container or occluder in a given video sequence. To facilitate the study of this task, a combination of synthetic and annotated real datasets is created, enabling supervised learning and structured evaluation of model performance across different variations, such as moving or nested containment. Two transformer-based video models are evaluated, revealing their surprising capability in tracking targets under certain task variations. However, there is still a significant performance gap that needs to be bridged before claiming that a tracking model has truly acquired object permanence.