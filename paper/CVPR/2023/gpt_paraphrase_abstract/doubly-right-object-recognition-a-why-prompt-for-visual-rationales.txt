Many visual recognition models are typically evaluated based solely on their accuracy in classifying images, a metric in which they excel. This study aims to explore whether computer vision models can also provide accurate explanations for their predictions. To achieve this, the authors propose a new benchmark called "doubly right" object recognition, which evaluates models based on their ability to generate both correct labels and rationales for their predictions. The authors discover that even state-of-the-art models like CLIP often produce incorrect rationales for their categorical predictions. However, through a customized dataset that transfers rationales from language models to visual representations, the authors demonstrate the effectiveness of a "why prompt" that enables large visual representations to generate accurate rationales. The authors provide visualizations and empirical experiments to support their findings, revealing that the inclusion of these prompts significantly enhances performance not only in doubly right object recognition but also in the transfer of knowledge to unseen tasks and datasets.