Neural network generalization is important for safe deployment in real-world scenarios. Traditional training strategies, such as data augmentation, ensembling, and model averaging, are used to improve generalization. This study introduces a simple but effective benchmark for generalization, which incorporates diverse augmentations in the training process and promotes a more balanced distribution of features. Additionally, the authors propose a strategy called Diversify-Aggregate-Repeat Training (DART), which trains multiple models using different augmentations to explore the loss basin. The weights of these models are then aggregated to combine their expertise and enhance generalization. Repeating the aggregation step throughout training improves the optimization trajectory and ensures that the individual models have low loss barriers, leading to improved generalization when combined. The proposed approach is theoretically justified and shown to have better generalization performance. The method also achieves state-of-the-art results in both in-domain and domain generalization benchmarks. It is a versatile technique that can be easily integrated with various base training algorithms to achieve performance gains. The code for this study is available at: https://github.com/val-iisc/DART.