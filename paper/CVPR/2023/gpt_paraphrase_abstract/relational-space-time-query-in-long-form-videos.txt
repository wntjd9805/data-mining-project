This paper focuses on the understanding of egocentric videos, which are uncurated long videos capturing daily life activities from the wearer's perspective. Existing video benchmarks study the problems of activity, object, and interaction reasoning independently and in short curated clips. However, real-world applications like AR assistants require addressing these problems together. To address this, the paper proposes an integrated framework called Relational Space-Time Query (ReST) that evaluates video understanding models through templated spatiotemporal queries. Additionally, two new benchmarks, ReST-ADL and ReST-Ego4D, are introduced to enhance existing egocentric video datasets with query annotations generated by the ReST framework. Baseline models and in-depth analysis on the benchmarks are provided, offering insights into the query tasks. The integrated framework and benchmarks aim to advance comprehensive, multi-step reasoning in long videos and support the development of future video understanding models.