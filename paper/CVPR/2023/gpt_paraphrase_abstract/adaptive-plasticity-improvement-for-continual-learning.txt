Numerous attempts have been made to address the problem of catastrophic forgetting in lifelong learning. However, prioritizing non-forgetting on old tasks may hinder the model's ability to learn new tasks. Although some approaches have aimed to strike a balance between stability and plasticity, none have focused on evaluating and enhancing a model's plasticity specifically for new tasks. This study introduces a novel method called adaptive plasticity improvement (API) for continual learning. API not only effectively handles catastrophic forgetting on old tasks but also assesses the model's plasticity and adapts it accordingly for learning new tasks. Experimental results on various real datasets demonstrate that API surpasses other state-of-the-art methods in terms of both accuracy and memory usage.