Visual object tracking relies heavily on visual representation. However, there is a lack of research on methods specifically tailored for tracking representation learning. Most trackers utilize pre-trained representations from ImageNet. In this study, we introduce a straightforward yet powerful representation learning method called masked appearance transfer. This method is based on an encoder-decoder architecture. Initially, we encode the visual appearances of both the template and the search region together, and then we decode them separately. During the decoding process, we reconstruct the original search region image. However, for the template, we instruct the decoder to reconstruct the target appearance within the search region. This transfer of target appearance allows for the learning of tracking-specific representations. To enhance discriminability, we randomly mask out certain inputs. To evaluate the effectiveness of our approach, we devise a lightweight and simple tracker that can assess the representation for both target localization and box regression. Extensive experiments demonstrate the efficacy of our proposed method, as the learned representations enable our simple tracker to achieve state-of-the-art performance on six datasets. The proposed method is available at https://github.com/difhnp/MAT.