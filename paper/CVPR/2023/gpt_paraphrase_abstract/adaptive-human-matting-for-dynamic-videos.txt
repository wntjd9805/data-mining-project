Recent advancements in video matting have aimed to overcome the reliance on trimap annotations, as they are costly and unsuitable for real-time applications. However, existing trimap-free methods struggle to handle diverse and unstructured videos effectively. To address this issue, we propose a novel framework called Adaptive Matting for Dynamic Videos (AdaM). AdaM simultaneously distinguishes foregrounds from backgrounds and captures detailed alpha mattes of human subjects in the foreground. Our approach consists of two interconnected network designs: (1) an encoder-decoder network that generates alpha mattes and intermediate masks to guide the transformer in adaptively decoding foregrounds and backgrounds, and (2) a transformer network that combines long- and short-term attention to preserve spatial and temporal contexts, facilitating accurate foreground decoding. We evaluate our method on recently introduced datasets and demonstrate significant improvements in matting realism and temporal coherence in complex real-world videos. Our model achieves state-of-the-art generalizability. For more information and examples, please visit our GitHub repository at https://github.com/microsoft/AdaM.