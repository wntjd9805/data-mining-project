Integrated Gradients (IG) and its variations are widely used techniques for interpreting deep neural network decisions. However, these methods often introduce noise into their explanation saliency maps, which hinders their interpretability. In this study, we analyze the source of this noise and propose a new approach, the Important Direction Gradient Integration (IDGI) framework, to reduce it based on our analytical findings. IDGI can be easily incorporated into any IG-based method that utilizes the Reimann Integration for gradient computation. Through extensive experiments with three IG-based methods, we demonstrate that IDGI significantly enhances interpretability across various metrics. The source code for IDGI is available at https://github.com/yangruo1226/IDGI.