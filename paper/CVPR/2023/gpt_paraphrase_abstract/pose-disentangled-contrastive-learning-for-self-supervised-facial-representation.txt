Recently, self-supervised facial representation has gained attention for its ability to understand faces without relying on large annotated datasets. However, current contrastive-based self-supervised learning (SSL) still performs poorly in learning facial representation. This is because existing contrastive learning tends to learn features that are invariant to pose, resulting in a loss of pose details and compromising learning performance. To overcome this limitation, we propose a novel method called Pose-disentangled Contrastive Learning (PCL) for general self-supervised facial representation. Our PCL method includes a pose-disentangled decoder (PDD) with an orthogonalizing regulation to separate pose-related features from face-aware features. This allows pose-related and pose-unrelated facial information to be processed independently without interfering with each other's training. Additionally, we introduce a pose-related contrastive learning scheme that learns pose-related information through data augmentation of the same image. This approach produces more effective face-aware representations for various downstream tasks. We evaluated PCL on four challenging facial understanding tasks, including facial expression recognition, face recognition, AU detection, and head pose estimation. Experimental results show that PCL outperforms state-of-the-art SSL methods. Our code is available at https://github.com/DreamMr/PCL.