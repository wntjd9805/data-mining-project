The development of Referring Expression Comprehension (REC) is hindered by the expensive annotations required for grounding the referent based on an expression. Existing weakly supervised methods rely on computationally expensive two-stage detection networks. In this paper, we propose a novel weakly supervised model called RefCLIP that utilizes an efficient one-stage detector. RefCLIP defines weakly supervised REC as an anchor-text matching problem, eliminating the need for complex post-processing. We introduce anchor-based contrastive loss to optimize RefCLIP through numerous anchor-text pairs. Additionally, we propose a model-agnostic weakly supervised training scheme using RefCLIP as a teacher to generate pseudo-labels for existing REC models. Our scheme improves weakly supervised performance compared to RefCLIP itself. We validate our approaches through extensive experiments on four REC benchmarks, namely RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame. The results demonstrate significant performance gains over existing weakly supervised models, such as a +24.87% improvement on RefCOCO, while also achieving 5 times faster inference speed. More information about the project can be found at https://refclip.github.io.