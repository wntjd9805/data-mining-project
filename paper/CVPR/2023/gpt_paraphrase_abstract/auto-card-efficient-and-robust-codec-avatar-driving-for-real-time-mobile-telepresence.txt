Real-time and robust photorealistic avatars are in high demand for immersive telepresence in augmented reality/virtual reality (AR/VR). However, the computational expense required to accurately infer facial expressions from headset-mounted cameras remains a major obstacle in achieving realistic avatars. In this study, we introduce a framework called Auto-CARD that allows real-time and robust driving of Codec Avatars using only on-device computing resources. This is accomplished by reducing redundancy in two ways. Firstly, we develop a neural architecture search technique called AVE-NAS, which enhances the robustness of searched architectures in the presence of extreme facial expressions and ensures compatibility with fast-evolving AR/VR headsets. Secondly, we exploit temporal redundancy in consecutively captured images during continuous rendering and introduce a mechanism called LATEX to skip redundant frame computations. By leveraging the linearity of the latent space derived by the avatar decoder, we propose adaptive latent extrapolation for redundant frames. Our evaluation demonstrates the effectiveness of the Auto-CARD framework in real-time Codec Avatar driving scenarios, achieving a 5.05Ã— speed-up on Meta Quest 2 while maintaining comparable or superior animation quality compared to state-of-the-art avatar encoder designs.