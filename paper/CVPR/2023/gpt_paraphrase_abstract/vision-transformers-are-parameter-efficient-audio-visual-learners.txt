This study explores the capability of frozen Vision Transformers (ViTs) to generalize to audio-visual data without fine-tuning. The researchers propose a Latent Audio-Visual Hybrid (LAVISH) adapter that adds a small number of trainable parameters to each layer of a frozen ViT. The LAVISH adapter efficiently combines visual and audio cues using a set of latent tokens, reducing the computational cost. Compared to existing audio-visual methods, the LAVISH approach achieves competitive or superior performance on various tasks while using fewer tunable parameters and without relying on costly audio pretraining or external audio encoders. The code for this research is available at the provided link.