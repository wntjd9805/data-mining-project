As the demand for Deep Neural Networks (DNNs) increases, organizations are outsourcing the training process. However, there is a risk of backdoor attacks on externally trained DNNs. It is essential to defend against these attacks by mitigating the backdoor behavior while maintaining the network's normal prediction power on clean inputs. Existing methods rely on labeled clean samples to remove abnormal backdoor behavior, but this requirement is often unrealistic as end users may not have access to the training data. In this study, we propose a new defense method that does not require training labels. By carefully re-initializing the weights at each layer and employing knowledge distillation, our method effectively removes backdoor behaviors from a suspicious network without compromising its normal behavior significantly. Experimental results demonstrate that our label-free method performs comparably to state-of-the-art defense methods trained with labels. Furthermore, our method shows promising defense results even on out-of-distribution data, making it highly practical. The code for our method is available at the following GitHub link: https://github.com/luluppang/BCU.