The understanding of multimodal semantics often involves uncertainty, leading to messages that can refer to multiple targets. This uncertainty poses challenges for interpretation, both within and across different modalities. Previous research has not adequately addressed the modeling of this uncertainty, especially in pre-training on unlabeled datasets and fine-tuning on specific tasks. In this study, we propose a Probability Distribution Encoder (PDE) that represents all modalities as probabilistic distributions, capturing sequence-level interactions. This approach, compared to deterministic methods, allows for richer semantics and more complex relationships. We also integrate uncertainty modeling into popular pre-training frameworks and suggest appropriate pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM). By fine-tuning these models, we achieve state-of-the-art results in challenging downstream tasks such as image-text retrieval, visual question answering, visual reasoning, and visual entailment. Figure 1 illustrates multimodal uncertainties and provides an example of language uncertainty, showing the difference between point representations and distribution representations. The images and text used in the figure are from the MSCOCO dataset [30]. The output of the answer format only provides abstraction.