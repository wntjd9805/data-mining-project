Creating realistic and editable models of clothed humans from 3D scans and poses is a difficult task. Current methods for reposing models have limitations in terms of expressiveness and require time-consuming mesh extraction for each new pose. Additionally, these methods often fail to maintain surface correspondences across different poses. To address these issues, we propose a new approach called Invertible Neural Skinning (INS). Our method incorporates a Pose-conditioned Invertible Network (PIN) architecture to extend the Linear Blend Skinning (LBS) process and learn pose-dependent deformations, thus preserving correspondences. We combine PIN with a differentiable LBS module to create an efficient and end-to-end INS pipeline. Our experiments demonstrate that our method outperforms state-of-the-art reposing techniques for clothed humans, while also preserving surface correspondences and being significantly faster. We also conduct an ablation study to validate the effectiveness of our pose-conditioning formulation, and our qualitative results show that INS effectively corrects artefacts introduced by LBS.