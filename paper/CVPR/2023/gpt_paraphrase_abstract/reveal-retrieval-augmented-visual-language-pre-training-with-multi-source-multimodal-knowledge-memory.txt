This paper introduces a Retrieval-Augmented Visual Language Model called REV EAL that is capable of encoding world knowledge into a large-scale memory and retrieving information from it to answer knowledge-intensive queries. The REV EAL model consists of four main components: the memory, the encoder, the retriever, and the generator. The large-scale memory encodes various multimodal sources of world knowledge using a unified encoder. The retriever then identifies the most relevant knowledge entries in the memory, while the generator combines the retrieved knowledge with the input query to generate the output. A key innovation of our approach is that all the components (memory, encoder, retriever, and generator) are pre-trained end-to-end on a massive amount of data. Additionally, our approach can leverage a diverse range of multimodal knowledge sources, resulting in significant improvements. The experimental results demonstrate that REV EAL achieves state-of-the-art performance in visual question answering and image captioning tasks. Further details about this work can be found on the project page: reveal.github.io. The output format only includes the abstract.