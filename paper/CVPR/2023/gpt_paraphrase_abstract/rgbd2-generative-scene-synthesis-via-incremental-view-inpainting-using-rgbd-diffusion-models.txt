We propose a novel solution, RGBD2, to recover scene geometry and colors from sparse RGBD view observations. Our approach involves generating new RGBD views along a camera trajectory and fusing them to obtain the scene geometry. We use an intermediate surface mesh for rendering new views and then complete it using an inpainting network. Each rendered RGBD view is back-projected as a partial surface and added to the intermediate mesh. This approach helps address the challenge of multi-view inconsistency. We modify a versatile RGBD diffusion model for inpainting to enable our use in this context. We evaluate our approach on the task of synthesizing 3D scenes from sparse RGBD inputs and demonstrate its superiority over existing methods through extensive experiments using the ScanNet dataset. More information can be found on our project page: https://jblei.site/proj/rgbd-diffusion.