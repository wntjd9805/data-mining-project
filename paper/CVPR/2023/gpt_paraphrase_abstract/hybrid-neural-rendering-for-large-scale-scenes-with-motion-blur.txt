Rendering high-quality and view-consistent novel views of large-scale scenes from real-world images is a challenging task. Existing methods often struggle with artifacts such as motion blur, which degrade the fidelity of the rendered images. In this study, we propose a hybrid neural rendering model that combines image-based representation and neural 3D representation to improve the quality of rendered images. Additionally, we address the issue of motion blur by simulating blur effects on the rendered images and reducing their importance during training using precomputed quality-aware weights. Our model outperforms state-of-the-art point-based methods for synthesizing novel views, as demonstrated through extensive experiments on both real and synthetic data. The code for our model is publicly available at https://daipengwa.github.io/Hybrid-Rendering-ProjectPage/.