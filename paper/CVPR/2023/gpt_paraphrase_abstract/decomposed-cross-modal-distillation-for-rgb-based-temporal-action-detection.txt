Temporal action detection aims to predict the time intervals and classes of action instances in videos. Existing two-stream models, while showing promising performance, have slow inference speed due to their reliance on computationally expensive optical flow. In this study, we present a decomposed cross-modal distillation framework that enhances the performance of RGB-based detectors by transferring knowledge from the motion modality. Instead of directly distilling the knowledge, we propose separately learning RGB and motion representations, which are then combined for action localization. The dual-branch design and asymmetric training objectives facilitate effective motion knowledge transfer while preserving RGB information. Additionally, we introduce a local attentive fusion approach to better exploit the complementarity of multimodal features. This fusion method preserves the local discriminability of features, which is crucial for action localization. Extensive experiments on benchmarks confirm the effectiveness of our proposed method in improving RGB-based action detectors. Importantly, our framework is compatible with various backbones and detection heads, consistently yielding improvements across different model combinations.