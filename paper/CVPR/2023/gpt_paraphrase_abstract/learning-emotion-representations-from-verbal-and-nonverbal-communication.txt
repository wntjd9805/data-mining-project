Emotion comprehension is a crucial yet complex aspect of creating artificial general intelligence. However, the lack of extensive annotated datasets has hindered advancements in this field. To overcome this challenge, we introduce Emotion-CLIP, a novel pre-training approach that extracts visual emotion representations from both verbal and nonverbal communication using uncurated data. Unlike previous methods that rely on numerical labels or descriptions, communication inherently contains emotion information. Moreover, acquiring emotion representations from communication aligns better with the human learning process. We guide EmotionCLIP to focus on nonverbal emotion cues through subject-aware context encoding and verbal emotion cues through sentiment-guided contrastive learning. Through extensive experiments, we demonstrate the effectiveness and transferability of EmotionCLIP. Using a linear-probe evaluation protocol, EmotionCLIP outperforms state-of-the-art supervised visual emotion recognition techniques and competes with multimodal approaches across various benchmarks. We believe that the emergence of EmotionCLIP will address the existing challenge of limited data availability in emotion understanding, thereby facilitating progress in related domains. The code and pre-trained models are accessible at https://github.com/Xeaver/EmotionCLIP.