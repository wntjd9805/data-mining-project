This paper addresses the problem of simultaneously detecting objects and inferring their visual attributes in an image, even without manual annotations during training. The authors propose a two-stage approach called CLIP-Attr, where candidate objects are first proposed using an offline RPN and then classified for their semantic category and attributes. To improve the model's performance, the authors employ a federated strategy to fine-tune the CLIP model by combining multiple datasets and aligning visual representations with attributes. They also explore the use of freely available online image-caption pairs for weakly supervised learning. Additionally, the authors develop an efficient Faster-RCNN model that performs object proposals and classification on semantic categories and attributes using classifiers generated from a text encoder. Through extensive experiments on various datasets, including VAW, MS-COCO, LSA, and OVAD, the authors demonstrate that jointly training object detection and attribute prediction significantly outperforms existing approaches that treat these tasks independently. The proposed method exhibits strong generalization ability to novel attributes and categories.