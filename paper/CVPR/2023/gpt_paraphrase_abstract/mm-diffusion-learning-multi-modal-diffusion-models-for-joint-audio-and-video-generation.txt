We propose a novel framework for generating high-quality realistic videos with synchronized audio. Our Multi-Modal Diffusion model (MM-Diffusion) utilizes two-coupled denoising autoencoders to generate joint audio-video pairs. Unlike existing models, MM-Diffusion employs a sequential multi-modal U-Net for a joint denoising process. The audio and video subnets in our model gradually generate aligned audio-video pairs from Gaussian noises while ensuring semantic consistency across modalities. We introduce a random-shift based attention block to efficiently align the two subnets and enhance the fidelity of the audio-video generation. Our extensive experiments demonstrate superior results in both unconditional audio-video generation and zero-shot conditional tasks. We achieve the best performance on Landscape and AIST++ dancing datasets, as measured by FVD and FAD metrics. Additionally, a Turing test with 10k votes confirms the preference for our model. The code and pre-trained models are available for download at https://github.com/researchmm/MM-Diffusion.