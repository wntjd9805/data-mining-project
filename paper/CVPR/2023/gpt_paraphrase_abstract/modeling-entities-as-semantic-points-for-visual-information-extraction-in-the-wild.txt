Visual Information Extraction (VIE) is gaining importance in academia and industry due to its applications in the real world. However, existing benchmarks used to evaluate VIE methods lack complexity and fail to accurately represent real-world scenarios. To address this, we have created a new dataset for VIE that includes challenging document images taken from real applications, featuring common difficulties like blur, occlusion, and printing shift. Our first contribution is the curation and release of this dataset. We then propose an alternative approach to accurately and robustly extract key information from document images under these tough conditions. Unlike previous methods that incorporate visual information into a multi-modal architecture or train text spotting and information extraction in an end-to-end manner, we model entities as semantic points. These points contain enriched semantic information about the attributes and relationships of different entities, resulting in improved entity labeling and linking. Through extensive experiments on standard benchmarks and our proposed dataset, we demonstrate that our method achieves significantly enhanced performance compared to previous state-of-the-art models. The dataset can be accessed at https://www.modelscope.cn/datasets/damo/SIBR/summary.