The development of generative models capable of producing highly realistic synthetic face images has raised concerns regarding security and ethics. To counter these fake faces, deep learning forensic classifiers have been created as a first line of defense. While these classifiers can accurately detect if a face image is synthetic or real, they are susceptible to adversarial attacks. These attacks can successfully evade detection, but they introduce visible patterns of noise that can be detected by careful human observation. Moreover, these attacks assume access to the target model, which may not always be the case. Efforts have been made to directly manipulate the latent space of generative adversarial networks (GANs) to generate adversarial fake faces that can bypass forensic classifiers. In this study, we take it a step further and demonstrate the possibility of generating adversarial fake faces with specific attributes, such as hair color, eye size, race, and gender. We achieve this by utilizing StyleGAN, a state-of-the-art generative model with disentangled representations that allow for modifications within the range of natural images. We propose a framework for searching adversarial latent codes within the feature space of StyleGAN, guided by either a text prompt or a reference image. Additionally, we introduce a meta-learning based optimization strategy to ensure transferable performance on unknown target models. Extensive experiments confirm that our approach can produce semantically manipulated adversarial fake faces that accurately represent the specified attributes and successfully deceive forensic face classifiers, while remaining undetectable to humans. The code for our work is available at: https://github.com/koushiksrivats/face_attribute_attack.