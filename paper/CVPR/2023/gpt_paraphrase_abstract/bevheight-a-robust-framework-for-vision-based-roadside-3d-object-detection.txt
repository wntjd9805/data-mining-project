The recent focus in autonomous driving systems has been on developing perception methods for sensors on the ego-vehicle, but there is a potential alternative approach that has been overlooked. We have found that current bird's eye view detection methods, which are vision-centric, do not perform well on roadside cameras. This is because these methods primarily focus on recovering depth in relation to the camera center, which is not effective when the distance between the car and the ground increases. In this paper, we propose a new approach called BEVHeight, which addresses this issue. Instead of predicting pixel-wise depth, we regress the height to the ground, allowing for a distance-agnostic formulation that makes it easier to optimize camera-only perception methods. Our method outperforms previous vision-centric methods by a significant margin on popular 3D detection benchmarks for roadside cameras. The code for our approach is available at https://github.com/ADLab-AutoDrive/BEVHeight.