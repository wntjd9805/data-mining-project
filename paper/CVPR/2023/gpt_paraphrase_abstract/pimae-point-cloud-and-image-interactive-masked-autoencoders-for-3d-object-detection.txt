Masked Autoencoders (MAEs) have been successful in learning powerful visual representations and achieving state-of-the-art results in various modalities. However, their performance in multi-modality settings has not been extensively explored. This study focuses on the interaction between point cloud and RGB image data, two modalities commonly encountered in the real world. To enhance the synergy between these modalities, a self-supervised pre-training framework called Pi-MAE is proposed. Pi-MAE promotes 3D and 2D interaction through three key aspects. Firstly, the importance of masking strategies between the two modalities is recognized, and a projection module is used to align the mask and visible tokens of both modalities. Secondly, a well-designed two-branch MAE pipeline with a novel shared decoder is utilized to facilitate cross-modality interaction in the mask tokens. Lastly, a unique cross-modal reconstruction module is developed to improve representation learning for both modalities. Extensive experiments conducted on large-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2) reveal the challenges in learning interactive point-image features. The proposed Pi-MAE framework significantly enhances the performance of multiple 3D detectors, 2D detectors, and few-shot classifiers by 2.9%, 6.7%, and 2.4%, respectively. The code for Pi-MAE is available at https://github.com/BLVLab/PiMAE.