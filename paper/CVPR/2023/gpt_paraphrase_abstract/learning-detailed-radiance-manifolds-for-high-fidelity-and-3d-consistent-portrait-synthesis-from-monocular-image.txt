A major challenge in generating new views of monocular portrait images is maintaining 3D consistency when the pose of the subject changes. Existing methods often rely on 2D generative models, resulting in noticeable inconsistencies in the 3D representation. This study proposes a novel approach for generating new views of monocular portrait images that maintains 3D consistency. The method is based on a recently developed 3D-aware GAN called Generative Radiance Manifolds (GRAM), which has shown strong 3D consistency in generating multiview images of virtual subjects. However, simply mapping a real image into the latent space of GRAM only produces coarse representations without fine details, and improving the fidelity of the reconstructions through instance-specific optimization is time-consuming. To address this limitation, the study introduces a novel method for reconstructing fine details on the radiance manifolds from monocular images. These fine details are then combined with the coarse representations to achieve high-fidelity reconstruction. The study also incorporates 3D priors derived from the coarse representations to regulate the learned details and ensure reasonable results in the synthesized images. The proposed method is trained on a dataset of 2D images and achieves high-fidelity and 3D-consistent portrait synthesis, outperforming previous approaches.