Research on adversarial attacks has primarily focused on classification tasks, neglecting denser prediction tasks like semantic segmentation. Existing methods for adversarial segmentation do not accurately solve the problem and overestimate the perturbations needed to deceive models. In this study, we propose a white-box attack for these models using proximal splitting to generate adversarial perturbations with smaller ℓ∞ norms. Our attack can handle numerous constraints within a nonconvex minimization framework, incorporating augmented Lagrangian techniques, adaptive constraint scaling, and masking strategies. We demonstrate that our attack outperforms previous approaches and adapted classification attacks for segmentation, providing a comprehensive benchmark for this dense task.