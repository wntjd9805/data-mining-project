The original Neural Radiance Fields (NeRF) have been successful in modeling scene appearance but lack real-time rendering capability. Recent approaches have addressed this by either incorporating NeRF outputs into a data structure or using explicit feature grids. However, these strategies increase memory usage, making them unsuitable for bandwidth-constrained applications. This paper proposes a grid-based approach that achieves real-time view synthesis at over 150 FPS using a lightweight model. The key contribution is a novel architecture that splits the density field into regions and models density using different decoders that share the same feature grid. This reduces the grid size and encourages compact feature representation across different parts of the scene. The final model size is further reduced by symmetrically disposing of features in each region, facilitating feature pruning and enabling smooth gradient transitions between neighboring voxels. Comprehensive evaluation shows that our method achieves real-time performance and comparable quality to state-of-the-art, with a more than 2Ã— improvement in the FPS/MB ratio.