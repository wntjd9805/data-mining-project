Recent advancements in hand pose estimation have revealed the potential of using synthetic data to train neural networks. However, this approach has limitations in terms of generalizing to real-world data due to differences in domains. To address this issue, we introduce a framework for cross-domain semi-supervised hand pose estimation. Our focus is on the challenging task of training models using labeled multi-modal synthetic data and unlabeled real-world data.To tackle this problem, we propose a dual-modality network that leverages synthetic RGB and synthetic depth images. In the pre-training phase, our network employs multi-modal contrastive learning and attention-fused supervision to acquire effective representations of RGB images. To further refine the network, we integrate a novel self-distillation technique during fine-tuning, which helps reduce the noise caused by pseudo-labels.Experimental results demonstrate that our proposed method significantly enhances the accuracy of 3D hand pose estimation and 2D keypoint detection on standard benchmarks.