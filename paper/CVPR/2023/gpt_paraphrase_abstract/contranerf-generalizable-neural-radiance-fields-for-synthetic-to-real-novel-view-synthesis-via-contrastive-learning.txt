This study explores the generalization of NeRF-based novel view synthesis for unseen scenes, specifically focusing on the synthetic-to-real transferability that is important in practical applications. The researchers discover that models trained with synthetic data tend to generate volume densities that are sharper but less accurate, leading to either fine-grained details or severe artifacts depending on the correctness of the volume densities. To address this issue, the study proposes the use of geometry-aware contrastive learning to learn multi-view consistent features with geometric constraints. Additionally, cross-view attention is employed to enhance the geometry perception of features by querying features across input views. Experimental results demonstrate that the proposed method outperforms existing novel view synthesis approaches in terms of image quality and fine-grained details, as measured by PSNR, SSIM, and LPIPS. Furthermore, even when trained on real data, the method achieves state-of-the-art results. More details can be found at the following link: https://haoy945.github.io/contranerf/.