Large-scale multi-modal training is effective in imparting strong generalization to the CLIP model when using image-text pairs. However, training on a similar scale for videos is not feasible, so recent approaches focus on transferring image-based CLIP to the video domain. This is achieved by adding new parametric modules to learn temporal information and inter-frame relationships, which require careful design efforts. However, these models tend to overfit on the given task distribution and lack generalization when learned on videos. To address this, the question arises of how to effectively transfer image-level CLIP representations to videos.In this study, we demonstrate that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis shows that processing frames at the image level using the CLIP image-encoder, followed by feature pooling and similarity matching with text embeddings, implicitly models the temporal cues within ViFi-CLIP. This fine-tuning enables the model to focus on scene dynamics, moving objects, and inter-object relationships.In scenarios with limited data, where full fine-tuning is not feasible, we propose a "bridge and prompt" approach. This approach involves using fine-tuning to bridge the domain gap initially and then learning prompts on both the language and vision sides to adapt CLIP representations. We extensively evaluate this simple yet strong baseline on zero-shot, base-to-novel generalization, few-shot, and fully supervised settings across five video benchmarks.To facilitate further research, we provide our code and pre-trained models on the GitHub repository https://github.com/muzairkhattak/ViFi-CLIP.