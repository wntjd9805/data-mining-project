We propose NeRDi, a framework for 2D-to-3D reconstruction that utilizes human-like prior knowledge of the 3D world. By treating single-view reconstruction as a 3D generation problem, we optimize NeRF representations using a diffusion loss and pretrained image diffusion models. We enhance the coherence of multiview content by incorporating language guidance from vision-language models and conditioning the diffusion model on semantic and visual features of the input image. Furthermore, we introduce a geometric loss based on estimated depth maps to regularize the NeRF's 3D geometry. Experimental results on the DTU MVS dataset demonstrate that our method produces higher-quality novel views compared to existing approaches trained on the same dataset. We also showcase the generalizability of our framework for zero-shot NeRF synthesis on diverse real-world images.