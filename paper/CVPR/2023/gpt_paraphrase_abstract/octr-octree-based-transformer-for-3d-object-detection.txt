This paper addresses the challenge of capturing sufficient features from large-scale 3D scenes for LiDAR-based 3D object detection, particularly for distant or occluded objects. While recent attempts using Transformers have shown promise in modeling long sequences, they struggle to balance accuracy and efficiency due to inadequate receptive fields and coarse-grained correlations. To overcome this issue, the authors propose an Octree-based Transformer called OcTr. OcTr constructs a dynamic octree on the hierarchical feature pyramid by performing self-attention on the top level and recursively propagating to lower levels, capturing rich global context in a coarse-to-fine manner while controlling computational complexity. Additionally, to improve foreground perception, a hybrid positional embedding is introduced, combining semantic-aware positional embedding and attention mask to leverage both semantic and geometry clues. The proposed approach is evaluated on the Waymo Open Dataset and KITTI Dataset, achieving new state-of-the-art results.