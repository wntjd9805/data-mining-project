We present VDN-NeRF, a technique for training neural radiance fields (NeRFs) that can handle non-Lambertian surfaces and dynamic lighting conditions. These conditions lead to variations in the radiance of a point when viewed from different angles. Instead of explicitly modeling the complex underlying factors that cause this view-dependent phenomenon, we propose a simple and effective approach that normalizes the view-dependence by extracting invariant information already present in the learned NeRFs. By jointly training NeRFs for view synthesis with this normalization, we achieve improved geometric accuracy. Our experiments demonstrate that our normalization technique reduces the impact of shape-radiance ambiguity on geometry, aligning the optimal capacity for explaining view-dependent variations. Importantly, our method can be applied to different baselines and enhances geometry without altering the volume rendering pipeline, even when the data is captured under a moving light source. The code for our method is available at: https://github.com/BoifZ/VDN-NeRF.