Visual dynamics models that learn from experience have been successful in robotic manipulation tasks. However, the best way to represent scenes with multiple interacting objects is still uncertain. Current methods break down scenes into separate objects, but struggle with accurately modeling and manipulating them under challenging lighting conditions, as they only encode appearance tied to specific lighting conditions. In this study, we suggest using object-centric neural scattering functions (OSFs) as representations of objects within a model-predictive control framework. OSFs can model the per-object light transport, allowing for compositional scene re-rendering when objects are rearranged and lighting conditions change. By combining this approach with inverse parameter estimation and graph-based neural dynamics models, we demonstrate improved performance and generalization in model-predictive control in multi-object environments. This improvement is observed even in previously unseen scenarios with difficult lighting conditions.