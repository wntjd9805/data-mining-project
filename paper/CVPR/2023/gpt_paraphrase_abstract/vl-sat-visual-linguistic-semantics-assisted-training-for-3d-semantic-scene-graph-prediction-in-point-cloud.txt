Predicting 3D semantic scene graphs (3DSSG) in point clouds is challenging due to limited semantic information compared to 2D images and the presence of long-tailed relation distributions. To address these challenges, we propose the Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme, which leverages rich semantics from 2D images and linguistic information to enhance 3DSSG prediction models. The scheme involves training a multi-modal oracle model that learns reliable structural representations based on vision, language, and 3D geometry. This oracle model's knowledge is then transferred to the 3D model during training, improving its performance in predicting long-tailed and ambiguous semantic relations. By effectively utilizing visual-linguistic semantics, VL-SAT significantly improves the performance of common 3DSSG prediction models, even with only 3D inputs during inference, particularly in dealing with tail relation triplets. Extensive evaluations and ablation studies on the 3DSSG dataset confirm the effectiveness of our proposed scheme. The code for VL-SAT is available at https://github.com/wz7in/CVPR2023-VLSAT.