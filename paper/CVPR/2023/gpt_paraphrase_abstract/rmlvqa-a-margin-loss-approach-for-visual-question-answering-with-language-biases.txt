Visual Question Answering (VQA) models often exhibit language biases, learning to correlate questions with answers while disregarding the image. Previous attempts to address this bias involved using question-only models or data augmentations. In this study, we propose a novel approach called Robust Margin Loss for Visual Question Answering (RMLVQA), which consists of two components. The first component takes into account the frequency of answers within a question type in the training data, aiming to mitigate the bias caused by class imbalance. However, it fails to consider the difficulty of answering each sample, which can affect the learning process. To address this, the second component of our approach learns instance-specific margins, enabling the model to differentiate between samples of varying complexity.To incorporate these components, we introduce a bias-injecting component to our model and calculate the instance-specific margins based on the confidence of this component. By combining these with estimated margins, our training loss considers both answer-frequency and task-complexity. We demonstrate that our approach, RMLVQA, surpasses existing state-of-the-art methods that do not rely on data augmentation when evaluated on benchmark VQA datasets with language biases. Additionally, our method maintains competitive performance on in-distribution (id) data, making it the most robust among comparable methods. In summary, we propose RMLVQA, which improves VQA performance on biased datasets, while also achieving competitive results on id data.