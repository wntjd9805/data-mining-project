FlexNeRF is a new method for generating realistic images of humans in motion from single videos. It is designed to handle challenging scenarios where the subject is moving quickly or in complex ways, even with limited camera views. The approach combines a canonical time and pose configuration with pose-dependent motion fields and pose-independent temporal deformations. By incorporating temporal and cyclic consistency constraints, as well as additional losses on intermediate representation, such as segmentation, the method produces high-quality outputs even with sparse views. Experimental results show that our approach outperforms existing techniques on public benchmark datasets and a self-captured fashion dataset. More information about the project can be found at https://flex-nerf.github.io/.