Federated learning-based semantic segmentation (FSS) has gained significant attention for its decentralized training on local clients. However, existing FSS models assume fixed categories, which leads to forgetting of old categories in practical applications where new categories are introduced incrementally. This issue is further exacerbated when new clients with novel classes join the training. To overcome these challenges, we propose a Forgetting-Balanced Learning (FBL) model that addresses forgetting on old classes from both intra-client and inter-client perspectives. Through the use of adaptive class-balanced pseudo labeling, we develop a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to address intra-client forgetting. Additionally, we introduce a task transition monitor to handle inter-client forgetting by identifying new classes and storing the latest old global model for relation distillation. Our qualitative experiments demonstrate significant improvement compared to other methods. The code for our model is available at https://github.com/JiahuaDong/FISS.