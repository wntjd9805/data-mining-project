There is a growing interest in developing unlearnable examples (UEs) to protect visual privacy on the Internet. UEs are training samples that contain invisible noise, making it impossible for unauthorized users to learn from them using machine learning models. Existing methods for generating UEs assume a label-consistency setting, where hackers and protectors have the same labels for the samples. However, in a more practical label-agnostic setting, hackers may exploit the protected data differently. This renders existing UE generation methods ineffective. To address this challenge, we propose a new technique called Unlearnable Clusters (UCs) that generates label-agnostic UEs using cluster-wise perturbations. We also suggest using Vision-and-Language Pre-trained Models (VLPMs) like CLIP as the surrogate model to improve the transferability of the crafted UCs across different domains. We empirically validate the effectiveness of our approach using various datasets, target models, and commercial platforms. The code is available at https://github.com/jiamingzhang94/Unlearnable-Clusters.