We present a novel framework that allows for the synthesis of images based on semantic layouts of varying precision levels. These layouts can range from simple text descriptions to detailed 2D semantic canvases with precise shapes. The input layout comprises one or more semantic regions with adjustable precision levels, offering users control over the output. Our framework encompasses both text-to-image (T2I) synthesis at the lowest precision level and segmentation-to-image (S2I) synthesis at the highest level. By supporting intermediate precision levels, our framework caters to users with different drawing skills and creative workflows. To address the challenges associated with this new setup, we introduce several innovative techniques. This includes developing a pipeline for collecting training data, creating a precision-encoded mask pyramid and a text feature map representation to encode precision level, semantics, and composition information together, and implementing a multi-scale guided diffusion model for image synthesis. To assess the effectiveness of our proposed method, we construct a test dataset featuring user-drawn layouts with diverse scenes and styles. Experimental results demonstrate that our method can generate high-quality images that adhere to the specified layout precision, outperforming existing methods. For more information, please visit our project page at https://zengxianyu.github.io/scenec/.