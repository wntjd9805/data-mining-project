We propose an extension to masked autoencoders (MAE) that enhances the learned representations by explicitly promoting the learning of higher-level scene features. This is accomplished by introducing a perceptual similarity term between generated and real images, and incorporating techniques from adversarial training literature such as multi-scale training and adaptive discriminator augmentation. The combination of these approaches not only improves pixel reconstruction but also captures higher-level details in images. Our method, called Perceptual MAE, demonstrates superior performance in downstream tasks compared to previous methods. On ImageNet-1K, we achieve a top-1 accuracy of 78.1% with linear probing and up to 88.1% with fine-tuning, along with similar results in other downstream tasks, all without the need for additional pre-trained models or data.