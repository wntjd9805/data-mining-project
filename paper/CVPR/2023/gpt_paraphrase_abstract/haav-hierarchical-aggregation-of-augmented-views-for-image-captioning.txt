Considerable advancements have been made in the field of image captioning, mainly focusing on how to encode the image using pre-trained models. These encodings can be visual, such as image grid features or detected objects, or more recently, textual, such as image tags or text descriptions of image regions. With the availability of more advanced encodings, it becomes essential to find efficient and effective ways to leverage this diverse set of encodings. In this study, we propose a novel approach that considers the encodings as augmented views of the input image. Our image captioning model efficiently encodes each view independently using a shared encoder. To further enhance the quality of the representations and improve data efficiency, we introduce a contrastive loss that incorporates information from all the encoded views in a unique manner. To determine the most effective views for caption generation, our proposed hierarchical decoder dynamically weighs the encoded views. This is achieved by first aggregating information within each view at the token level and then across views at the view level. Through extensive experiments, we demonstrate significant performance improvements compared to state-of-the-art methods. Specifically, we achieve a +5.6% CIDEr score improvement on MS-COCO and a +12.9% CIDEr score improvement on Flickr30k. Additionally, we conduct thorough analyses to highlight the importance of each component in our design.