This study addresses the issue of sub-optimal 3D object detection techniques that focus solely on positional information and neglect object-level shape information. To overcome this limitation, the authors propose AShapeFormer, a module that utilizes multi-head attention to encode object shape information. They also introduce shape tokens and object-scene positional encoding to fully exploit shape information. Additionally, a semantic guidance sub-module is introduced to improve object shape perception by sampling more foreground points and reducing the influence of background points. The authors demonstrate the effectiveness of AShapeFormer by enhancing multiple existing methods, achieving significant improvements of up to 8.1% on the SUN RGB-D and ScanNetV2 datasets. The code for AShapeFormer is available at https://github.com/ZechuanLi/AShapeFormer.