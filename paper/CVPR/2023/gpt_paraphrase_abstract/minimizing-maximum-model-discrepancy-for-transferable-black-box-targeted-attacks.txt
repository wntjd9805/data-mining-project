This study examines the problem of targeted attacks on black-box models from the perspective of model discrepancy. The researchers provide a theoretical analysis by presenting a generalization error bound for these attacks, ensuring their success. They discover that the attack error on a target model primarily depends on the empirical attack error on the substitute model and the maximum model discrepancy among substitute models. To address this problem, the researchers propose a new algorithm for black-box targeted attacks based on their theoretical analysis. In this algorithm, they minimize the maximum model discrepancy (M3D) of the substitute models during the training of the generator for generating adversarial examples. This approach allows their model to create highly transferable adversarial examples that are robust to model variation, thereby increasing the success rate of attacking black-box models. The researchers conducted extensive experiments on the ImageNet dataset using various classification models. Their proposed approach significantly outperforms existing state-of-the-art methods. The code for their approach is available at https://github.com/Asteriajojo/M3D.