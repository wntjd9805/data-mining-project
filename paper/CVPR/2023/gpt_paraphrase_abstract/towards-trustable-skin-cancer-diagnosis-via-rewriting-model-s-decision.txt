Deep neural networks have shown promise in image recognition tasks, but they can rely on irrelevant artifacts or dataset bias to improve performance. This can result in untrustworthy models with catastrophic outcomes in real-world scenarios. To address this issue in skin cancer diagnosis, we propose a human-in-the-loop framework for model training. Our approach automatically detects confounding factors by analyzing sample co-occurrence behavior and learns these concepts using easily obtained exemplars. By mapping the model's features onto an explainable concept space, users can interpret and intervene using first-order logic instructions. We evaluate our method on controlled skin lesion datasets and public datasets, demonstrating its effectiveness in detecting and removing confounding factors without prior category distribution knowledge or fully annotated concept labels. Additionally, our method enables the model to focus on clinically relevant concepts, improving its performance and trustworthiness during inference.