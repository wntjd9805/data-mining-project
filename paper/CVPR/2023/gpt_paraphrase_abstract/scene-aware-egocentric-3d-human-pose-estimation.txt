Egocentric 3D human pose estimation using a single head-mounted fisheye camera is gaining attention for its applications in virtual and augmented reality. However, existing methods struggle with challenging poses that involve occlusion or close interaction with the scene. In response, we propose a scene-aware egocentric pose estimation method that considers scene constraints. Our approach includes an egocentric depth estimation network that predicts the depth map from a wide-view fisheye camera and handles occlusion using a depth-inpainting network. We also introduce a scene-aware pose estimation network that projects 2D image features and the estimated depth map into a voxel space, enabling the regression of 3D pose using a V2V network. This voxel-based feature representation establishes a geometric connection between image features and scene geometry, allowing the V2V network to constrain the predicted pose based on the estimated scene geometry. To train our networks, we generated synthetic datasets (EgoGTA) and an in-the-wild dataset based on EgoPW (EgoPW-Scene). Our experimental results demonstrate that our method outperforms existing techniques in terms of accuracy and physical plausibility of human-scene interaction.