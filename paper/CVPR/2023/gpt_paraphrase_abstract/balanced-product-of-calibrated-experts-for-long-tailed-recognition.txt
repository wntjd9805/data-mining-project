Recognition problems in the real world often have label distributions that are skewed towards a few dominant classes, making it difficult to learn representations that generalize well to the less frequent classes. When the distribution of the test data differs from that of the training data, such as when the test distribution is uniform while the training distribution is long-tailed, the problem of distribution shift must be addressed. Recent research suggests using multiple diverse experts to tackle this issue, where each expert specializes in different classes. In this study, we propose a new approach called Balanced Product of Experts (BalPoE) that extends the concept of logit adjustment to ensembles. BalPoE combines a family of experts with different target distributions at test time, offering a more generalized solution compared to previous methods. We demonstrate how to define these distributions and combine the experts to make unbiased predictions by proving that the ensemble is Fisher-consistent for minimizing balanced error. Our theoretical analysis reveals that our balanced ensemble requires calibrated experts, which we achieve using mixup in practice. We conduct extensive experiments and our approach achieves state-of-the-art performance on three long-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. The code for our method is available at https://github.com/emasa/BalPoE-CalibratedLT.