Token Turing Machines (TTM) are proposed as a novel model for real-world sequential visual understanding. Drawing inspiration from the Neural Turing Machine, TTM incorporates an external memory composed of tokens that summarize previous history or frames. This memory is efficiently accessed, read, and written using a Transformer as the processing unit/controller at each step. By utilizing the memory module, TTM can process new observations using only the contents of the memory rather than the entire history. This allows for efficient processing of long sequences without incurring excessive computational costs at each step. Experimental results demonstrate that TTM outperforms other models, including Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential visual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning. The code for TTM is publicly available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token turing.