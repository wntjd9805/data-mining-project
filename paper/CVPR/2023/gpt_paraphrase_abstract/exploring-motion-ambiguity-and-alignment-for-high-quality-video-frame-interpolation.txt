Existing deep-learning-based approaches for video frame interpolation (VFI) heavily rely on ground-truth (GT) intermediate frames, which may not accurately represent the motion between adjacent frames. Consequently, these methods often produce blurred results due to averaging. To address this issue, we propose relaxing the requirement of reconstructing an intermediate frame close to the GT. Instead, we introduce a texture consistency loss (TCL) that ensures the interpolated content maintains similar structures to the given frames, even if it differs from the GT. Our plug-and-play TCL consistently improves the performance of existing VFI frameworks. Prior methods typically utilize cost volume or correlation maps for accurate image or feature warping. However, the computational complexity of O(N^2) (N being the pixel count) makes it impractical for high-resolution videos. To overcome this limitation, we introduce a simple yet powerful guided cross-scale pyramid alignment (GCSPA) module with an efficient O(N) complexity. This module effectively leverages multi-scale information. Extensive experiments demonstrate the efficiency and effectiveness of our proposed strategy.