Mixup training has been proven to enhance predictive accuracy by combining samples and their labels. However, recent findings suggest that mixup training may negatively affect uncertainty calibration, making models less reliable in estimating uncertainty. This study examines the mixup process and identifies the data transformation's confidence penalty as the cause of calibration degradation. To address this issue, the study explores mixup inference strategies and discovers that although they improve calibration, they do not surpass simple ensembles. Consequently, the study proposes a new approach called mixup inference in training, which employs a decoupling principle to recover the outputs of raw samples during the forward network pass. By incorporating mixup inference, models can be trained using the original one-hot labels, eliminating the negative impact of confidence penalty and effectively resolving mixup's calibration problem. Experimental results demonstrate that this strategy successfully addresses mixup's calibration issue without compromising predictive performance, and even enhances accuracy compared to vanilla mixup training.