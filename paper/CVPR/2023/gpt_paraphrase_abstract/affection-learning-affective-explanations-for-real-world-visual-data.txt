This study examines the range of emotional reactions that real-world images can evoke. The researchers created a large dataset consisting of 85,007 images, each accompanied by categorical emotional reactions and textual explanations. A total of 6,283 annotators participated in the analysis, providing 526,749 responses. Despite the subjectivity of emotional reactions, the researchers found common ground among the annotators, suggesting that emotional responses can be captured to a significant extent. With this in mind, the researchers aim to address several key questions: Can neural networks be developed to generate plausible emotional responses to visual data, explained through language? Can these methods be guided to produce explanations with varying degrees of pragmatism, grounding emotional reactions in the visual stimulus? How can the performance of such methods be evaluated for this novel task? This work represents the initial steps towards more human-centric and emotionally-aware image analysis systems. The code and data from this study are publicly available at https://affective-explanations.org.