There is increasing interest in searching for information in large video collections. Previous studies have focused on specific tasks like text-based video retrieval, moment retrieval, video summarization, and video captioning individually, without considering an end-to-end system that can search and generate summaries simultaneously. This integrated setup would enable various applications, such as searching for relevant videos in a collection using text queries, extracting the most relevant moments from those videos, and segmenting the moments into important steps with captions. To address this gap, we introduce the HIREST dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization in instructional videos. The HIREST dataset consists of 3.4K pairs of text and video from an instructional video dataset. Out of these, 1.1K videos have annotations for relevant moments and breakdown of each moment into key instruction steps with captions and timestamps, resulting in a total of 8.6K step captions. Our benchmark includes tasks for video retrieval, moment retrieval, moment segmentation, and step captioning. In moment segmentation, models are tasked with breaking down a video moment into instruction steps and identifying the start and end boundaries. In step captioning, models generate a textual summary for each step. We also provide baseline models for both task-specific and end-to-end joint approaches. Although the baseline models show promise, there is still ample room for improvement, and we encourage the community to contribute to this area of research.