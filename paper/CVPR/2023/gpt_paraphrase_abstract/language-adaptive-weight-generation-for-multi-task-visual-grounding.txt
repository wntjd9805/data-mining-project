Current approaches in visual grounding passively use visual backbones to extract features without considering expression-related hints. This passive perception can result in mismatches and limit performance improvement. Ideally, the visual backbone should actively extract visual features based on expressions, which serve as blueprints for desired visual features. By using expressions as priors, active perception can extract relevant visual features and effectively address mismatches. To achieve this, we propose a framework called VG-LAW (Visual Grounding based on Language-Adaptive Weights), which utilizes a language-aware visual backbone to dynamically generate weights for different expressions. VG-LAW does not require additional modules for cross-modal interaction and, with a multi-task head, can effectively comprehend referring expressions and perform segmentation. We conducted extensive experiments on four datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame), which demonstrated the effectiveness of VG-LAW and its state-of-the-art performance.