This study aims to understand how actions are performed in videos by predicting adverbs that modify the actions. The problem is approached as a regression task, where textual relationships between verbs and adverbs are used to generate a regression target for learning action changes. The proposed approach achieves state-of-the-art results in adverb prediction and antonym classification, surpassing previous work even when action labels are not available during testing and adverbs are not paired as antonyms. Existing datasets for adverb recognition are either noisy or lack adverb-influenced actions, leading to learning difficulties and unreliable evaluation. To address this, a new high-quality dataset called Adverbs in Recipes (AIR) is created, focusing on instructional recipe videos with actions that exhibit visual changes when performed differently. The videos in AIR are carefully curated, trimmed, and manually reviewed by multiple annotators to ensure label quality. Results show that models improve their learning from AIR due to cleaner videos, but adverb prediction on AIR remains challenging, indicating room for further improvement.