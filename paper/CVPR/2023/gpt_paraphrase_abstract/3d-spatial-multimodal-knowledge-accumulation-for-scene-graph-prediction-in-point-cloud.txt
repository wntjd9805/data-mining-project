Understanding a 3D scene involves more than just recognizing objects; it requires inferring relationships and interactions among them. However, existing methods struggle with the complex nature of 3D scenes, which include partially scanned objects, dense placement, changing sizes, and challenging relationships. In this study, we leverage the hierarchical structures of physical space in 3D scenes to improve predictions. These structures help associate semantic and spatial arrangements, leading to less ambiguous predictions and better handling of variations within scene categories. To achieve this, we integrate the structural cues of 3D physical spaces into deep neural networks for scene graph prediction. We use an external knowledge base to create a 3D spatial multimodal knowledge graph, incorporating both visual content and textual facts. Additionally, we propose a knowledge-enabled scene graph prediction module that utilizes 3D spatial knowledge to regularize semantic relationships. Extensive experiments demonstrate that our method outperforms current state-of-the-art competitors. The code for our method is available at https://github.com/HHrEtvP/SMKA.