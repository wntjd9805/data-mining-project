Semantic segmentation is the process of classifying pixels into specific object or stuff classes. However, this task becomes more challenging when dealing with blur, especially motion blur that is inevitable with lightweight and compact cameras. Previous research has mostly focused on improving segmentation performance for clear images, neglecting motion blur. This study aims to address the issue of motion blur in semantic segmentation by proposing a Class-Centric Motion-Blur Augmentation (CCMBA) strategy. By utilizing segmentation annotations, synthetic space-variant blur is generated only for specific regions corresponding to selected semantic classes. This approach allows the network to learn semantic segmentation for clean images, as well as those with egomotion blur and dynamic scene blur. The effectiveness of the proposed method is demonstrated on various datasets using both CNN and Vision Transformer-based semantic segmentation networks. Additionally, the generalizability of the approach is illustrated by evaluating its performance on real-world blur datasets.