Recent studies have shown that diffusion models have a strong ability to generate images based on conditioning, such as text-guided image synthesis. This success has motivated researchers to explore the use of large-scale pre-trained diffusion models for addressing the challenging task of real image editing. However, existing approaches in this area typically rely on learning a textual token that corresponds to multiple images of the same object. In many cases, only a single image is available, which poses a problem when fine-tuning pre-trained diffusion models as it leads to severe overfitting. The leakage of information from the pre-trained models hinders the ability to maintain the original content of the given image while incorporating new features guided by language. To overcome this issue, we propose a novel model-based guidance approach that builds upon classifier-free guidance. This allows us to distill the knowledge from a model trained on a single image into the pre-trained diffusion model, enabling content creation even with only one input image. Additionally, we introduce a patch-based fine-tuning technique that effectively facilitates the generation of images with arbitrary resolutions. Through extensive experiments, we validate the design choices of our approach and demonstrate its promising editing capabilities, including style changes, content addition, and object manipulation. The code for our method is publicly available.