The growing trend of gathering large and unfiltered datasets for training vision-and-language models has raised concerns about the fairness of the representations they produce. Even manually annotated datasets like MSCOCO, though small, are known to be influenced by societal biases. This issue, which is far from being resolved, may be exacerbated by the use of uncontrolled data collected from the Internet. Furthermore, the lack of tools to analyze societal bias in large image collections makes it extremely difficult to address the problem effectively.In our study, we make three significant contributions. Firstly, we annotate a portion of the widely utilized Google Conceptual Captions dataset, which is commonly used for training vision-and-language models, with four demographic attributes and two contextual attributes. This annotation allows us to gain insights into how different demographic groups are represented in the dataset. Secondly, we conduct a comprehensive analysis of these annotations, focusing on the representation of various demographic groups. This analysis helps us understand the extent and nature of societal bias present in the dataset.Lastly, we evaluate three popular tasks in image captioning and text-image vision-and-language models: CLIP embeddings, text-to-image generation, and image captioning. Our evaluation reveals that societal bias remains a persistent problem across all three tasks, highlighting the ongoing challenges in achieving fair representations in vision-and-language models.To access the data and findings from our study, please visit our GitHub repository at https://github.com/noagarcia/phase.