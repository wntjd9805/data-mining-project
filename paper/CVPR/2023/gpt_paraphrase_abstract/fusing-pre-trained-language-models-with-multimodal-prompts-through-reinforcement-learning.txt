Language models have the ability to reason using common sense, and while specialized models can learn from explicit knowledge, larger models like GPT-3 show a broad capacity for commonsense reasoning. This study aims to determine if this knowledge can be extended to multimodal inputs, such as images and audio, without the need for paired domain data. To achieve this, the researchers propose a method called ESPER (Extending Sensory PErception with Reinforcement learning), which allows text-only pre-trained models to handle tasks involving visual commonsense reasoning. The key innovation of ESPER is the use of reinforcement learning to align multimodal inputs with language model generations, eliminating the need for direct supervision. The reward optimization in ESPER relies solely on cosine similarity derived from CLIP, without requiring additional paired (image, text) data. Experimental results demonstrate that ESPER outperforms baseline models and previous approaches in various multimodal text generation tasks, including captioning and commonsense reasoning. The researchers also introduce a new benchmark dataset called the ESP dataset, which challenges models to generate text for different domains based on given images. The code and data for ESPER are publicly available on GitHub at https://github.com/JiwanChung/esper.