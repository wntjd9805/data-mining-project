Deep artificial neural networks (DNNs) trained with mam-backpropagation effectively model the visual system of mammals, accurately capturing neural responses from primary visual cortex to inferior temporal cortex. However, these networks have limited ability to explain representations in higher cortical areas, particularly in circuits related to the transformation from egocentric to allocentric perspectives. To address this, we propose a new benchmark for scene perception that is inspired by a hippocampal dependent task. This benchmark tests the ability of DNNs to transform scenes viewed from different egocentric perspectives. We utilize a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus and train DNNs using a triplet loss. By enforcing a factorized latent space, we divide information propagation into "what" and "where" pathways, which allows us to reconstruct the input. As a result, we surpass the current state-of-the-art for unsupervised object segmentation on the CATER and MOVi-A,B,C benchmarks.