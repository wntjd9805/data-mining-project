The importance of human evaluation in validating text-to-image generative models cannot be overstated. However, a survey of 37 recent papers reveals that many of them either rely solely on automatic measures or poorly describe human evaluations that lack reliability and repeatability. To address this issue, this paper introduces a standardized and well-defined human evaluation protocol that aims to enable verifiable and reproducible evaluations in future research. In our initial data collection, we conducted experiments that demonstrate the incompatibility between current automatic measures and human perception when evaluating the performance of text-to-image generation results. Additionally, we offer valuable insights for the design of reliable and conclusive human evaluation experiments. Finally, we have made several resources publicly accessible to facilitate easy and efficient implementation of our proposed protocol.