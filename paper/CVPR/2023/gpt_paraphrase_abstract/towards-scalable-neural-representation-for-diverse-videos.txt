The use of implicit neural representations (INR) in encoding 3D scenes and images has gained attention. However, existing methods are limited to encoding a small number of short videos with redundant content, making them inefficient for encoding a large number of diverse videos. This paper proposes a new framework called D-NeRV, which jointly encodes long and diverse videos using a unified model. D-NeRV decouples clip-specific visual content from motion information, incorporates temporal reasoning into the neural network, and reduces spatial redundancies. The results show that D-NeRV outperforms previous methods and traditional video compression techniques on the UCF101 and UVG datasets. Additionally, D-NeRV achieves higher accuracy than NeRV on action recognition tasks when used as a data-loader.