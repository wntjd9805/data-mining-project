We introduce Equiangular Basis Vectors (EBVs) for classification tasks in deep neural networks. Traditionally, models use a fully connected layer with softmax for classification, aiming to map feature representations to label space. In contrast, metric learning focuses on transforming data points to a new space where similar points are closer and dissimilar points are farther apart. Our approach differs from previous methods by generating normalized vector embeddings as "predefined classifiers" that are both equal in status and as orthogonal as possible. During training, we minimize the spherical distance between the input embedding and its corresponding categorical EBV. During inference, predictions are made by identifying the categorical EBV with the smallest distance. Experimental results on the ImageNet-1K dataset and other tasks demonstrate that our method outperforms general fully connected classifiers without significantly increasing computation. We achieved first place in the 2022 DIGIXGlobal AI Challenge with our EBVs, and our code is open-source and available at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors.