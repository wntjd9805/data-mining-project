In environments with multiple speakers, such as dinner parties, people often focus their auditory attention on a specific speaker while ignoring others. Identifying who someone is listening to is crucial for developing technologies that can understand social behavior and devices that can enhance human hearing by amplifying specific sound sources. The computer vision and audio research communities have made significant progress in recognizing sound sources and speakers in various scenarios. This study goes a step further by addressing the challenge of localizing auditory attention targets in egocentric video, specifically detecting who a camera wearer is listening to within their field of view. To address this novel problem, we propose an end-to-end deep learning approach that utilizes egocentric video and multichannel audio to predict the heatmap of the camera wearer's auditory attention. Our approach incorporates spatiotemporal audiovisual features and holistic scene analysis to make accurate predictions, outperforming a range of baseline models on a complex multi-speaker conversation dataset. For more information, please visit our project page: https://fkryan.github.io/saal.