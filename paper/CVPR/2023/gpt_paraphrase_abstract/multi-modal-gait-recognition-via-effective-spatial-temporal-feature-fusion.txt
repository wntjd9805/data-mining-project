Gait recognition is a biometric technology that identifies individuals based on their walking patterns. The two main approaches for gait recognition are silhouette-based and skeleton-based methods. However, silhouette data can be problematic due to clothing occlusion, while skeleton data lack body shape information. To address these limitations and improve gait recognition, we propose a transformer-based framework called MMGaitFormer. This framework effectively combines spatial-temporal information from both skeletons and silhouettes. The proposed Spatial Fusion Module (SFM) performs detailed fusion of body parts by using an attention mechanism to align silhouettes and skeletons. The Temporal Fusion Module (TFM) models temporal information through Cycle Position Embedding (CPE) and fuses temporal information from both modalities. Experimental results demonstrate that our MMGaitFormer achieves superior performance on popular gait datasets. Notably, in the challenging "CL" condition of walking in different clothes in CASIA-B dataset, our method achieves a rank-1 accuracy of 94.8%, outperforming state-of-the-art single-modal methods by a significant margin.