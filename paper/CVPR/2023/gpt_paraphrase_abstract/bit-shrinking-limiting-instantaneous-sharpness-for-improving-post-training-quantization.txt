Post-training quantization (PTQ) is a compression method that reduces model size and computational cost. However, quantizing a model to a low-bit representation, such as less than 4 bits, is challenging and often leads to significant performance degradation. In this study, we investigate the loss landscapes of quantized networks with different bit-widths and find that networks with more irregular loss surfaces are more prone to getting stuck in suboptimal solutions, particularly in low-bit quantization. Our analysis reveals that the irregular surfaces are caused by excessive quantization noise. To address this issue, we introduce a sharpness term that quantifies the impact of quantization noise and propose a strategy to smooth the rugged loss surface. Instead of directly optimizing the target bit network, we employ a self-adapted shrinking scheduler that gradually reduces the bit-width from a high value to the target value while limiting the increase in the sharpness term to a suitable range. This approach can be seen as iteratively introducing small "instant" quantization noise and adjusting the network to mitigate its effects. Extensive experiments on classification and detection tasks demonstrate the effectiveness of our Bit-shrinking strategy in PTQ. For Vision Transformer models, our INT8 and INT6 models only suffer a drop of 0.5% and 1.5% in Top-1 accuracy, respectively. On traditional CNN networks, our INT4 quantized models achieve state-of-the-art performance with a drop in Top-1 accuracy of only 1.3% and 3.5% on ResNet18 and MobileNetV2, respectively, without fine-tuning.