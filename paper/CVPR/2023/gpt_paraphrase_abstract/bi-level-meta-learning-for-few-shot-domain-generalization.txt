Few-shot learning aims to generalize knowledge from limited samples to unseen data. While previous methods focus on generalization within specific domains, practical scenarios may require generalization across domains. This paper introduces the concept of few-shot domain generalization (FSDG), a more challenging variant of few-shot classification. FSDG involves bridging a larger gap between seen and unseen domains. To tackle this problem, the authors propose a meta-learning approach that leverages two levels of meta-knowledge. The lower-level meta-knowledge consists of domain-specific embedding spaces, enabling intra-domain generalization. The upper-level meta-knowledge includes a base space and a prior subspace for inter-domain generalization. The authors formulate the learning problem with bi-level optimization and develop an optimization algorithm that does not rely on higher-order derivative information. Evaluation on the Meta-Dataset benchmark demonstrates the superiority of their method compared to previous works.