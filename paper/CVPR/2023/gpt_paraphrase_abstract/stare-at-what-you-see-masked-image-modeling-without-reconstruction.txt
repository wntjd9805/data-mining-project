The prevailing training method for large-scale vision representation is Masked Autoencoders (MAE). MAE reconstructs masked image patches from visible regions, forcing the model to infer semantic correlation within an image. Recent approaches have used semantic-rich teacher models to extract image features as the reconstruction target, resulting in improved performance. However, we argue that the features extracted by powerful teacher models already encode rich semantic correlation across regions in an intact image, unlike low-level features like pixel values. This raises the question of whether reconstruction is necessary in MaskedImage Modeling (MIM) with a teacher model. In this paper, we propose an efficient MIM paradigm called MaskAlign. MaskAlign focuses on learning the consistency between visible patch features extracted by the student model and intact image features extracted by the teacher model. To address the issue of input inconsistency between the student and teacher model, we introduce a Dynamic Alignment (DA) module with learnable alignment. Our experimental results show that masked modeling remains effective even without reconstruction on masked regions. When combined with Dynamic Alignment, MaskAlign achieves state-of-the-art performance with significantly higher efficiency. The code and models for MaskAlign are available at https://github.com/OpenPerceptionX/maskalign.