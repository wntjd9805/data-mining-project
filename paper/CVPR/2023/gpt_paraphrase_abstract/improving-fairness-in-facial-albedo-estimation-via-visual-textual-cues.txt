Recent advancements in 3D face reconstruction have improved geometry prediction, but the enhancement of cosmetic appearance is still limited due to the challenge of inferring albedo. Existing methods that consider illumination to improve albedo estimation tend to produce a bias towards light skin tones, as they rely on racially biased albedo models and limited light constraints. In this paper, we propose a novel approach called ID2Albedo that directly estimates albedo without constraining illumination. We establish a relationship between albedo and intrinsic semantic attributes such as race, skin color, and age, which can effectively constrain the albedo map. We introduce visual-textual cues and a semantic loss to supervise facial albedo estimation. By pre-defining text labels for attributes like race, skin color, age, and wrinkles, we use the CLIP model to compute the similarity between the text and input image, assigning a pseudo-label to each facial image. During training, we ensure that the generated albedos have the same attributes as the inputs. Additionally, we train an unbiased facial albedo generator and utilize the semantic loss to learn the mapping from illumination-robust identity features to albedo latent codes. Our ID2Albedo model is trained in a self-supervised manner and achieves superior accuracy and fidelity compared to state-of-the-art albedo estimation methods. Importantly, our approach demonstrates excellent generalizability and fairness, particularly when applied to in-the-wild data.