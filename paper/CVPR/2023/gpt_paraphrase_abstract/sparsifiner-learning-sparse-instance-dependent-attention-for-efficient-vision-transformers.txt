Vision Transformers (ViT) have demonstrated better performance than Convolutional Neural Networks (CNNs), but they often require significant computational resources. Previous methods have attempted to address this issue by using fixed attention patterns that limit the number of spatially nearby tokens considered during multi-head self-attention (MHSA) operations. However, these structured attention patterns only capture spatial relevance and ignore learned semantic connections from a full attention mask.In this study, we propose a new approach to learn instance-dependent attention patterns. We introduce a lightweight connectivity predictor module that estimates the connectivity score between each pair of tokens. Tokens with high connectivity scores indicate that their features are relevant both spatially and semantically. By restricting each token to attend to only a small number of other tokens, the resulting binarized connectivity masks are naturally sparse, allowing for efficient computations through sparse operations. Our method, called Sparsifiner, learns unstructured attention patterns and achieves a superior trade-off between FLOPs (floating-point operations) and top-1 accuracy on the ImageNet dataset compared to token sparsity. Sparsifiner reduces MHSA FLOPs by 48% to 69% while maintaining an accuracy drop of less than 0.4%. Furthermore, we demonstrate that combining attention and token sparsity can reduce ViT FLOPs by over 60%.Overall, our approach enables ViT models to achieve comparable performance to CNNs while significantly reducing computational costs.