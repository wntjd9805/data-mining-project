The primary challenge in domain generalization (DG) is handling the difference in data distribution between training and test data. Recent studies propose test-time training (TTT) as a potential solution, which adapts the learned model with test data. However, the success of TTT depends on two main factors: selecting an appropriate auxiliary TTT task and identifying reliable parameters to update during the test phase. Previous research and our experiments indicate that TTT may not improve the model if these factors are not properly considered. To address this, we propose an Improved Test-Time Adaptation (ITTA) method. Instead of heuristically defining an auxiliary objective, we introduce a learnable consistency loss for the TTT task, which includes adjustable parameters for better alignment with the main prediction task. Additionally, we introduce adaptive parameters for the trained model and suggest updating them only during the test phase. Extensive experiments demonstrate that these strategies benefit the learned model, and ITTA outperforms current state-of-the-art methods on various DG benchmarks. The code for ITTA is available at https://github.com/liangchen527/ITTA.