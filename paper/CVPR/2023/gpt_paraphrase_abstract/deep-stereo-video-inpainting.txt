Stereo video inpainting involves filling missing regions in both the left and right views of a stereo video simultaneously. While single video inpainting has achieved promising results using deep convolutional neural networks, the task of inpainting missing regions in stereo videos has not been extensively explored. In addition to achieving spatial and temporal consistency, stereo video inpainting also faces the challenge of maintaining stereo consistency between the left and right views, reducing 3D fatigue for viewers. This paper introduces SVINet, a novel deep stereo video inpainting network that utilizes deep convolutional neural networks for the first time in this task. SVINet employs a self-supervised flow-guided deformable temporal alignment module to align features in the left and right view branches. These aligned features are then fed into a shared adaptive feature aggregation module to generate missing content for each branch. A parallax attention module (PAM) is introduced to fuse the completed features of the left and right views by considering significant stereo correlation using cross-view information. Additionally, a stereo consistency loss is developed to regularize the trained parameters, ensuring high-quality stereo video inpainting results with improved stereo consistency. Experimental results show that SVINet outperforms state-of-the-art single video inpainting models.