Open-set Unsupervised Video Domain Adaptation (OU-VDA) involves adapting an action recognition model from a labeled source domain to an unlabeled target domain that includes categories not present in the source. Previous approaches focused on training specialized open-set classifiers or using weighted adversarial learning. However, in this study, we propose a different method using pre-trained Language and Vision Models (CLIP) for OU-VDA. CLIP is well-suited for this task due to its comprehensive representation and zero-shot recognition capabilities. However, using CLIP's zero-shot protocol to reject target-private instances requires knowledge of the target-private label names, which is often not available. To overcome this challenge, we introduce AutoLabel, a method that automatically discovers and generates object-centric compositional candidate target-private class names. Despite its simplicity, we demonstrate that when combined with AutoLabel, CLIP can effectively reject target-private instances, thereby improving alignment between the shared classes in the two domains. The code for this approach is publicly available.