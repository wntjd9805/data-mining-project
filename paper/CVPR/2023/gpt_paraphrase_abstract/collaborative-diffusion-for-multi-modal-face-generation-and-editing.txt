Recent advancements in diffusion models have proven their effectiveness as a generative tool. However, existing diffusion models have primarily focused on uni-modal control, where the diffusion process is driven by only one condition. To enhance user creativity, it is desirable for diffusion models to be controllable by multiple modalities simultaneously. For example, generating and editing faces by describing the age (text-driven) while drawing the face shape (mask-driven). In this study, we propose Collaborative Diffusion, a method that enables multi-modal face generation and editing without the need for re-training. Our approach leverages pre-trained uni-modal diffusion models that collaborate to achieve the desired results. We observe that diffusion models driven by different modalities are inherently complementary in terms of the latent denoising steps. This allows for the establishment of bilateral connections. To achieve this, we introduce the dynamic diffuser, a meta-network that adaptively hallucinates multi-modal denoising steps by predicting influence functions for each pre-trained uni-modal model. Collaborative Diffusion not only enables collaboration between uni-modal diffusion models for generation, but also integrates multiple uni-modal manipulations for multi-modal editing. Through extensive qualitative and quantitative experiments, we demonstrate the superiority of our framework in terms of image quality and condition consistency.