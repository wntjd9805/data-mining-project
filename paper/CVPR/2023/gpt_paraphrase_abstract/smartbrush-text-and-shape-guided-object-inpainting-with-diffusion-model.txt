This study introduces a new diffusion-based model called SmartBrush for completing missing regions in images using both text and shape guidance. Unlike generic image inpainting methods, this approach allows for more flexible and precise control over the inpainted content. The model incorporates both text and shape guidance to generate more accurate results and preserve the background better. To achieve this, a novel training and sampling strategy is proposed by combining the diffusionU-net with object-mask prediction. Additionally, a multi-task training strategy is introduced by jointly training inpainting with text-to-image generation, which helps leverage more training data. Experimental results demonstrate that the proposed model outperforms existing methods in terms of visual quality, mask controllability, and background preservation.