The use of foundation models in task-specific fine-tuning has been successful but inconsistent with the goal of general perception modeling. To address this inconsistency, we propose Uni-Perceiver v2, the first generalist model capable of handling large-scale vision and vision-language tasks with competitive performance. Our approach encodes images as general region proposals and texts using a Transformer-based language model. The encoded representations are transformed by a task-agnostic decoder, and different tasks are formulated as a unified maximum likelihood estimation problem. We also introduce Task-Balanced Gradient Normalization, an effective optimization technique that ensures stable multi-task learning with an unmixed sampling strategy. Uni-Perceiver v2 is jointly trained on various tasks and can directly handle downstream tasks without task-specific adaptation. Our results demonstrate that Uni-Perceiver v2 outperforms existing generalist models in terms of versatility and performance. Additionally, compared to strong baselines that require task-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance across a wide range of vision and vision-language tasks.