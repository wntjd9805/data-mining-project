Previous methods of knowledge distillation have been successful in compressing models, but they lack an explanation of how the transferred knowledge improves the performance of the student network. This study aims to propose a knowledge distillation method that is both highly interpretable and competitively performing. The researchers revisit the structure of mainstream CNN models and discover that the ability to identify class discriminative regions is crucial for classification. They demonstrate that this ability can be obtained and enhanced by transferring class activation maps. Based on these findings, they introduce a new method called class attention transfer based knowledge distillation (CAT-KD). Unlike previous methods, CAT-KD explores and presents several properties of the transferred knowledge, improving its interpretability and contributing to a better understanding of CNNs. Despite its high interpretability, CAT-KD achieves state-of-the-art performance on multiple benchmarks. The code for CAT-KD is available at: https://github.com/GzyAftermath/CAT-KD.