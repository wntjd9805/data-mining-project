We investigate the challenging task of generating realistic human motion sequences based on different conditional inputs, such as action classes or textual descriptors. However, human motions are highly diverse and have different distributions compared to conditional modalities like textual descriptors. This makes it difficult to learn a probabilistic mapping from the desired conditional input to the human motion sequences. Additionally, raw motion data from motion capture systems may contain redundancy and noise, making it computationally expensive and prone to artifacts to model the joint distribution of raw motion sequences and conditional inputs. To address these challenges, we propose a method that first uses a Variational AutoEncoder (VAE) to obtain a low-dimensional latent code that represents the human motion sequence. Instead of directly establishing connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. This Motion Latent-based Diffusion model (MLD) generates realistic motion sequences based on the given conditional inputs while reducing computational overhead in both training and inference stages.Extensive experiments on various human motion generation tasks demonstrate that our MLD outperforms state-of-the-art methods, achieving significant improvements and being two orders of magnitude faster than previous diffusion models on raw motion sequences.