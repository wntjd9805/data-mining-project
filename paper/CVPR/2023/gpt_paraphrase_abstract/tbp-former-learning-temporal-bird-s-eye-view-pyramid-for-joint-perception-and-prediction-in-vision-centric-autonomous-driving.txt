The use of vision-based joint perception and prediction (PnP) is a growing trend in autonomous driving research. This involves predicting the future behavior of traffic participants based on raw RGB images. However, there are challenges in synchronizing features from multiple camera views and timestamps, as well as utilizing these spatial-temporal features effectively. To address this, we present a solution called the temporal bird's-eye-view pyramid transformer (TBP-Former) for vision-centric PnP. This solution consists of two key components. Firstly, we introduce a pose-synchronized bird's-eye-view (BEV) encoder that maps raw image inputs with any camera pose and time to a shared and synchronized BEV space, improving spatial-temporal synchronization. Secondly, we propose a spatial-temporal pyramid transformer that comprehensively extracts multi-scale BEV features and predicts future BEV states using spatial priors. Extensive experiments on the nuScenes dataset demonstrate that our framework outperforms existing vision-based prediction methods. The code for our framework is available at the following GitHub link: https://github.com/MediaBrain-SJTU/TBP-Former.