We introduce Deformer, a new method for reconstructing 3D human mesh from monocular images. Deformer utilizes a vertex-based approach and employs a transformer decoder with specialized attention modules to align a body mesh model with the input image. These attention modules include body sparse self-attention and deformable mesh cross-attention, which allow Deformer to effectively utilize high-resolution image features and a dense mesh model. Unlike previous approaches, Deformer avoids the computational burden associated with these components. Experimental results demonstrate that Deformer achieves superior performance on benchmark datasets such as Human3.6M and 3DPW. Additionally, an ablation study confirms the effectiveness of Deformer's model designs in leveraging multi-scale feature maps. The code for Deformer is publicly available at https://github.com/yusukey03012/DeFormer.