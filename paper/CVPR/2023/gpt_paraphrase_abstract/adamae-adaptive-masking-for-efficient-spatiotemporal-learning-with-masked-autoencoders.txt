This paper introduces AdaMAE, an adaptive masking strategy for Masked Autoencoders (MAEs) that enables the learning of generalizable representations for various types of data, including images, text, audio, and videos. Unlike existing MAE approaches, which use random patch, tube, or frame based masking strategies, AdaMAE utilizes an end-to-end trainable auxiliary sampling network to select visible tokens based on the semantic context. This network estimates a categorical distribution over spacetime-patch tokens and rewards and selects tokens that increase the expected reconstruction error, inspired by the policy gradient algorithm in reinforcement learning. By sampling more tokens from high spatiotemporal information regions, AdaMAE is able to mask 95% of tokens, resulting in lower memory requirements and faster pre-training. The efficacy of the adaptive sampling approach is demonstrated through ablation studies on the Something-Something v2 (SSv2) dataset, achieving state-of-the-art results of 70.0% and 81.7% in top-1 accuracy on SSv2 and Kinetics-400 action classification datasets, respectively, using a ViT-Base backbone and 800 pre-training epochs. The code and pre-trained models for AdaMAE are available at: https://github.com/wgcban/adamae.git.