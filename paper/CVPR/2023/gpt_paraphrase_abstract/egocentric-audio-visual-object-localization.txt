This paper examines the challenging task of ego-centric audio-visual object localization. It identifies two key issues: the presence of egomotion in first-person recordings and the creation of out-of-view sound components when attention shifts. To address the first problem, the paper proposes a geometry-aware temporal aggregation module that explicitly handles egomotion by estimating temporal geometry transformations and updating visual representations accordingly. To overcome the second issue, a cascaded feature enhancement module is introduced, which improves localization robustness by disentangling visually-indicated audio representation. The paper leverages naturally occurring audio-visual temporal synchronization during training as a form of self-supervision to avoid costly labeling. Additionally, the authors annotate and create the Epic SoundingObject dataset for evaluation purposes. The experimental results demonstrate that the proposed method achieves state-of-the-art localization performance in egocentric videos and can be applied to various audio-visual scenes. The code for the method is available at https://github.com/WikiChao/Ego-AV-Loc.