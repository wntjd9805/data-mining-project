Unsupervised stereo matching algorithms have gained significant attention as they allow for the estimation of disparity without the need for ground-truth data. However, these algorithms often assume that the left and right images possess consistent visual properties and struggle when faced with asymmetric stereo images. This paper introduces a new approach called spatially-adaptive self-similarity (SASS) for unsupervised asymmetric stereo matching. SASS builds upon the concept of self-similarity and generates deep features that are resilient to asymmetries. To effectively capture diverse patterns, the sampling patterns used to calculate self-similarities are adaptively generated across different regions of the image. In order to learn the optimal sampling patterns, a contrastive similarity loss is designed with positive and negative weights. This encourages SASS to encode features that are agnostic to asymmetry while still maintaining the distinctiveness required for stereo correspondence. The proposed method is extensively evaluated through various experiments, including ablation studies and comparisons with other techniques. The results demonstrate the effectiveness of the SASS approach in handling resolution and noise asymmetries.