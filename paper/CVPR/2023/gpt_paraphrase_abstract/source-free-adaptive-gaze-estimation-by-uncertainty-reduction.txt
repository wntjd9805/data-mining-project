Gaze estimation across different domains has gained attention recently due to the disparity between the controlled conditions in which training data is collected and the varied environments in which the trained estimators are used. However, obtaining simultaneous access to annotated source data and target data for prediction can be challenging due to privacy and efficiency concerns. To address this issue, we propose an unsupervised source-free domain adaptation approach for gaze estimation called Uncertainty Reduction Gaze Adaptation (UnReGA). UnReGA adapts a source-trained gaze estimator to unlabeled target domains without relying on source data. The framework achieves adaptation by reducing both sample uncertainty and model uncertainty. Sample uncertainty is mitigated by improving image quality and making them more suitable for gaze estimation, while model uncertainty is reduced by minimizing prediction variance on the same inputs. We conduct extensive experiments on six cross-domain tasks and demonstrate the effectiveness of UnReGA and its components. The results show that UnReGA outperforms other state-of-the-art cross-domain gaze estimation methods, both with and without source data. The code for UnReGA is available at https://github.com/caixin1998/UnReGA.