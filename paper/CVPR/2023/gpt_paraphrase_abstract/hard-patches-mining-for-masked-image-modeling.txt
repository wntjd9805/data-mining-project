Masked image modeling (MIM) has garnered significant attention in research for its potential in learning scalable visual representations. Current approaches in MIM primarily focus on predicting specific contents of masked patches, with their performance being closely tied to pre-defined mask strategies. This approach can be likened to training a student (the model) to solve given problems (predict masked patches). However, we argue that the model should not only focus on solving given problems but also act as a teacher by generating more challenging problems on its own.To address this, we propose a new framework called Hard Patches Mining (HPM) for MIM pre-training. We observe that the reconstruction loss can naturally serve as a metric for the difficulty of the pre-training task. Therefore, we introduce an auxiliary loss predictor that first predicts patch-wise losses and determines where to apply the next mask. This predictor utilizes a relative relationship learning strategy to prevent overfitting to exact reconstruction loss values. Our experiments conducted under various settings demonstrate the effectiveness of HPM in constructing masked images.Additionally, we find through empirical evidence that solely introducing the loss prediction objective leads to powerful representations. This verifies the efficacy of the model's ability to identify areas that are difficult to reconstruct.In summary, we propose a novel framework, HPM, for MIM pre-training. By considering the reconstruction loss as a metric for difficulty, we introduce an auxiliary loss predictor to guide the generation of masked images. Our experiments show the effectiveness of HPM in constructing masked images, and we also observe that the model's ability to predict loss values leads to powerful representations.