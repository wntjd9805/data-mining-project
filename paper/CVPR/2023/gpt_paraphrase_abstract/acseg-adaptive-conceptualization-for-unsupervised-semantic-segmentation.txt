Recently, there has been significant progress in the development of unsupervised dense prediction tasks, such as unsupervised semantic segmentation (USS), through the use of self-supervised large-scale visual pre-training models. These models have proven effective in representing pixel-level semantic relationships, which contain valuable class-aware information. However, utilizing these models to identify semantically consistent pixel groups or regions in an image is challenging due to issues like over/under-clustering. In this study, we focus on the pixel-level semantic aggregation in self-supervised ViT pre-trained models for image segmentation and propose a new approach called ACSeg (AdaptiveConceptualization) for USS. Our approach involves encoding concepts into learnable prototypes and using the Adaptive Concept Generator (ACG) to map these prototypes to informative concepts for each image. Additionally, we consider the complexity of different images and introduce the modularity loss to optimize ACG based on the estimation of pixel pair intensities within the same concept. Finally, we transform the USS task into an unsupervised classification problem by classifying the discovered concepts. Our extensive experiments demonstrate the effectiveness of ACSeg, with state-of-the-art results.