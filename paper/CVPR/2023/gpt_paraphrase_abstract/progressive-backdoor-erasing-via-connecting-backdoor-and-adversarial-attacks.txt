DNNs are vulnerable to backdoor and adversarial attacks. Although these attacks are usually considered separate problems, this study uncovers a connection between them. It is found that models with backdoors exhibit similar behaviors in their adversarial examples and triggered images, activating the same subset of DNN neurons. This suggests that backdoors significantly impact the model's adversarial examples. To address this, a novel algorithm called Progressive Backdoor Erasing (PBE) is proposed. PBE leverages untargeted adversarial attacks to gradually remove the backdoor from the model. Unlike previous methods, PBE can erase the backdoor even without access to a clean extra dataset. Experimental results demonstrate the effectiveness of PBE against five state-of-the-art backdoor attacks, showing no significant performance degradation on clean samples and outperforming existing defense methods.