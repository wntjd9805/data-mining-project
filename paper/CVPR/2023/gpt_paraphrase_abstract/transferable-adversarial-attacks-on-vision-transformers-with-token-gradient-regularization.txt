Vision transformers (ViTs) have achieved success in various computer vision tasks, yet they remain susceptible to adversarial samples. Transfer-based attacks pose a significant security threat to ViT-based applications by employing a local model to generate adversarial samples and directly launching attacks on a target black-box model. To mitigate this threat, it is crucial to develop effective transfer-based attacks that can identify the vulnerabilities of ViTs in security-sensitive scenarios. Existing approaches mainly focus on regularizing input gradients to stabilize the direction of updated adversarial samples. However, these approaches fail to address the issue of large variances in back-propagated gradients within intermediate blocks of ViTs. This can lead to adversarial samples getting stuck in poor local optima and emphasizing model-specific features. To overcome these limitations, we propose a method called Token Gradient Regularization (TGR). Leveraging the structural characteristics of ViTs, TGR reduces the variance of back-propagated gradients in each internal block of ViTs in a token-wise manner and utilizes the regularized gradient to generate adversarial samples. Through extensive experiments attacking both ViTs and CNNs, we demonstrate the superiority of our approach. Notably, compared to state-of-the-art transfer-based attacks, our TGR method achieves an average performance improvement of 8.8%.