The performance of modern object detectors decreases when the test distribution differs from the training distribution. Many existing methods focus on addressing changes in object appearance caused by factors like varying illumination or disparities between synthetic and real images. In contrast, our approach tackles geometric shifts that arise from variations in the image capture process or environmental constraints, resulting in differences in the apparent geometry of the objects. We propose a self-training technique that learns a set of geometric transformations to minimize these shifts without utilizing any labeled data or camera information from the new domain. We evaluate our method on two specific shifts: changes in a camera's field of view (FoV) and changes in viewpoint. Our results demonstrate that learning geometric transformations significantly improves the performance of object detectors in the target domains.