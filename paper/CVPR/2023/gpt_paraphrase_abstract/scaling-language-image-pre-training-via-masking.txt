We introduce Fast Language-Image Pre-training (FLIP), a straightforward and more efficient approach for training CLIP. In our method, we randomly mask and remove a significant portion of image patches during training. This masking technique allows us to learn from a larger number of image-text pairs within the same time frame, enabling us to compare more samples per iteration while utilizing a similar amount of memory. Consequently, this approach strikes a favorable balance between accuracy and training time. In our extensive experiments involving 400 million image-text pairs, FLIP demonstrates notable enhancements in both accuracy and speed compared to the baseline approach without masking. Moreover, FLIP consistently outperforms the CLIP counterparts trained on the same data across a wide range of downstream tasks. Leveraging the speedup achieved by FLIP, we further explore the scalability of increasing the model size, data size, and training duration, reporting promising findings and comparisons. We anticipate that our research will encourage future investigations into scaling vision-language learning.