Domain adaptive semantic segmentation methods often use stage-wise training, including a warm-up and self-training stage. However, these methods face challenges in both stages. In the warm-up stage, adversarial training is commonly used but has limited performance improvement due to blind feature alignment. In the self-training stage, finding proper categorical thresholds is difficult. To address these issues, we propose a new approach. In the warm-up stage, we replace adversarial training with a symmetric knowledge distillation module that only uses source domain data. This domain generalizable warm-up model significantly improves performance. We also introduce a cross-domain mixture data augmentation technique to further enhance performance. In the self-training stage, we propose a threshold-free dynamic pseudo-label selection mechanism to overcome the threshold problem and better adapt the model to the target domain. Extensive experiments show that our framework consistently outperforms prior methods on popular benchmarks. The codes and models are available at https://github.com/fy-vision/DiGA.