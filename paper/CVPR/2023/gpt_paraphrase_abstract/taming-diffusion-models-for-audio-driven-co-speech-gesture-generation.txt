This abstract discusses the challenges faced in generating accurate co-speech gestures for virtual avatars in human-machine interaction using existing methods. It introduces a new framework called DiffGesture that utilizes diffusion-based techniques to capture audio-to-gesture associations and maintain temporal coherence. The framework includes a diffusion-conditional generation process, a Diffusion Audio-Gesture Transformer to handle multiple modalities and long-term temporal dependencies, and a Diffusion Gesture Stabilizer to eliminate temporal inconsistencies. The authors also incorporate implicit classifier-free guidance to balance gesture diversity and quality. Experimental results show that DiffGesture outperforms existing methods in generating coherent gestures with improved mode coverage and stronger audio correlations. The code for DiffGesture is available on GitHub.