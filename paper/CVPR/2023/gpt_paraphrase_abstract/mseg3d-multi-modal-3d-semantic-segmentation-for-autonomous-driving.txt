In the context of autonomous driving, LiDAR and camera are two methods commonly used for 3D semantic segmentation. However, LiDAR-only approaches often struggle with accurately segmenting small and distant objects due to limited laser points. On the other hand, the potential of multi-modal solutions remains largely unexplored. In this study, we address three key challenges in multi-modal segmentation: modality heterogeneity, field of view intersection, and data augmentation. To mitigate the modality heterogeneity, we propose a multi-modal 3D semantic segmentation model called MSeg3D. This model combines intra-modal feature extraction and inter-modal feature fusion. The fusion process includes geometry-based feature fusion (GF-Phase), cross-modal feature completion, and semantic-based feature fusion (SF-Phase). By integrating these techniques, MSeg3D can effectively handle the differences between LiDAR and camera data.Additionally, we enhance the multi-modal data augmentation by applying asymmetric transformations individually to the LiDAR point cloud and multi-camera images. This approach diversifies the augmentation transformations, leading to improved model training. We evaluate the performance of MSeg3D on the nuScenes, Waymo, and SemanticKITTI datasets, and it achieves state-of-the-art results. Even when faced with malfunctioning multi-camera input or multi-frame point cloud input, MSeg3D demonstrates robustness and outperforms the LiDAR-only baseline. Our code for MSeg3D is publicly available at https://github.com/jialeli1/lidarseg3d.