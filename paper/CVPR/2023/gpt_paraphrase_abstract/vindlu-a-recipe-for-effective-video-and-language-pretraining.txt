In recent years, significant progress has been made in the field of video-and-language (VidL) understanding. However, many current VidL approaches utilize complex and specialized model architectures and intricate pretraining protocols, which makes reproducing, analyzing, and comparing these frameworks challenging. Instead of introducing a new VidL model, this study aims to comprehensively explore and explain the key factors in VidL model design through empirical research. The factors investigated include the design of spatiotemporal architecture, multimodal fusion schemes, pretraining objectives, choice of pretraining data, pretraining and finetuning protocols, and dataset and model scaling. The empirical study reveals that the most crucial design factors are temporal modeling, video-to-text multimodal fusion, masked modeling objectives, and joint training on images and videos. Based on these empirical insights, a step-by-step recipe called VINDLU is developed for effective VidL pretraining. The final model trained using this recipe achieves comparable or superior results to state-of-the-art approaches on various VidL tasks, without relying on external CLIP pretraining. For instance, on the text-to-video retrieval task, our approach achieves 61.2% on DiDeMo and 55.0% on ActivityNet, surpassing the current state-of-the-art by 7.8% and 6.1% respectively. Additionally, our model also achieves state-of-the-art results in video question-answering on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC, and TVQA. The code and pretrained models for our approach are publicly available at: https://github.com/klauscc/VindLU.