This study focuses on few-shot image classification in a cross-domain scenario, where the difference between the training and testing domains affects classification accuracy. To address this issue, we propose a method called prompting-to-disentangle (ProD) that utilizes a novel exploration with the prompting mechanism. ProD follows the common approach of multi-domain training and extracts the backbone feature using a standard Convolutional Neural Network. The key aspect of ProD is the use of the prompting mechanism in the transformer to separate the domain-general (DG) and domain-specific (DS) knowledge from the backbone feature. Specifically, ProD combines a DG prompt, which is shared across all training domains and is learnable, with a DS prompt generated on-the-fly from the domain-of-interest. These prompts are then fed into a lightweight transformer, which outputs DG and DS features concurrently, resulting in the disentangling effect. Our experiments demonstrate the following: 1) Sharing a single DG prompt among all training domains already enhances generalization towards new test domains. 2) The cross-domain generalization can be further improved by making the DG prompt neutral towards the training domains. 3) During inference, the DS prompt is generated from the support samples and can capture knowledge specific to the test domain through the prompting mechanism. By combining these three benefits, ProD significantly enhances cross-domain few-shot classification. For example, on the CUB dataset, ProD improves the accuracy of 5-way 5-shot classification from 73.56% (baseline) to 79.19%, setting a new state-of-the-art result.