Mobile robots, such as autonomous vehicles, heavily rely on electromagnetic radiation sensors like lidars, radars, and cameras for perception. However, these sensors can be unreliable in unfavorable environmental conditions, such as low-light scenarios and adverse weather, and they can only detect obstacles within their direct line-of-sight. On the other hand, audible sound from other road users carries information even in challenging scenarios. Despite being overlooked due to their low spatial resolution and lack of directional information, sound waves can serve as a complementary sensing modality to traditional sensors. In this study, we propose long-range acoustic beamforming of sound generated by road users in natural environments as an additional sensing approach. To support our proposal and foster further research in this area, we introduce the first-ever multimodal long-range acoustic beamforming dataset. We also present a neural aperture expansion method for beamforming and demonstrate its effectiveness in multimodal automotive object detection when combined with RGB images in challenging automotive scenarios. This approach outperforms camera-only methods and offers ultra-fast acoustic sensing sampling rates. For access to the dataset and code, please refer to [link].