Continuous sign language recognition (CSLR) is the process of recognizing glosses in a sign language video. Current methods for CSLR consist of two modules: a spatial perception module and a temporal aggregation module, both of which are learned together. However, previous research has shown that the spatial perception module, responsible for extracting spatial features, is not adequately trained. In this study, we conducted empirical studies and found that using a shallow temporal aggregation module improves the training of the spatial perception module. However, this shallow temporal aggregation module is unable to effectively capture both local and global temporal context information in sign language. To overcome this challenge, we propose a cross-temporal context aggregation (CTCA) model. This model consists of a dual-path network with separate branches for perceiving local and global temporal context. We also introduce a cross-context knowledge distillation learning objective to combine the two types of context and linguistic prior. Through knowledge distillation, the resulting one-branch temporal aggregation module can perceive both local-global temporal and semantic context. This structure of a shallow temporal perception module facilitates the learning of the spatial perception module. Our method outperforms existing approaches on challenging CSLR benchmarks, as demonstrated by extensive experiments.