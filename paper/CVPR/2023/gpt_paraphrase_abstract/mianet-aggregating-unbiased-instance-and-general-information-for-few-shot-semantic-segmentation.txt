Existing few-shot segmentation methods rely on meta-learning and extract instance knowledge from a support set to segment target objects in a query set. However, this extracted knowledge is insufficient to handle the varying differences within the same class due to the limited number of samples in the support set. In order to address this limitation, we propose a multi-information aggregation network (MI-ANet) that effectively combines general knowledge, represented by semantic word embeddings, with instance information to improve segmentation accuracy.Our approach introduces a general information module (GIM) that extracts a general class prototype from word embeddings as a complement to instance information. To achieve this, we design a triplet loss that utilizes the general class prototype as an anchor and samples positive-negative pairs from local features in the support set. This triplet loss helps transfer semantic similarities from the word embedding space to the visual representation space. To overcome model bias towards the seen training classes and to capture multi-scale information, we further introduce a non-parametric hierarchical prior module (HPM) that generates unbiased instance-level information by calculating pixel-level similarity between the support and query image features.Finally, we propose an information fusion module (IFM) that combines the general and instance information to make predictions for the query image. Through extensive experiments on PASCAL-5i and COCO-20i datasets, we demonstrate that MI-ANet outperforms existing methods, setting a new state-of-the-art in few-shot segmentation. The code for our approach is available at github.com/Aldrich2y/MIANet.