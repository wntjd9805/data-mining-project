Recently, there has been a growing interest in automatically predicting the emotions of user-generated videos (UGVs). However, current methods primarily focus on a small number of key visual frames, which may restrict their ability to capture the context that conveys the intended emotions. To address this limitation, this study introduces a cross-modal temporal erasing network that identifies not only keyframes, but also context and audio-related information in a weakly-supervised manner. By leveraging the relationships within and between different segments, the network iteratively erases keyframes to encourage the model to pay more attention to the context that contains complementary information. Extensive experiments on three challenging video emotion benchmarks demonstrate that our proposed method outperforms state-of-the-art approaches. The code for our method is publicly available at https://github.com/nku-zhichengzhang/WECL. In Figure 1, we provide an illustration of keyframes detected by an off-the-shelf method on the Ekman-6 dataset. It is important to note that the deeper color indicates a greater impact on the overall video emotion, and "GT" represents the category label from the ground truth. The orange-colored texts indicate the categorical prediction results. The answer format provides only the abstract.