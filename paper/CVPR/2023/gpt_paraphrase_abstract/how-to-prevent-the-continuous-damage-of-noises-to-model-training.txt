Deep learning with noisy labels is a challenging problem that occurs in various situations. Current methods attempt to mitigate the impact of mislabeled samples by adjusting loss weights or screening techniques, relying on the model's ability to identify mislabeled samples. However, during training, the imperfect model may incorrectly predict some mislabeled samples, leading to continuous harm to the training process. Consequently, there is a significant performance gap between existing anti-noise models trained with noisy samples and models trained with clean samples. To address this issue, we propose a Gradient Switching Strategy (GSS) to prevent the continuous damage caused by mislabeled samples. Theoretical analysis reveals that the harm stems from the misleading gradient direction computed from these mislabeled samples. Under the influence of the accumulated misleading gradients, the trainee model deviates from the correct optimization direction. The GSS tackles this problem by switching the gradient direction of each sample based on a gradient direction pool containing all-class gradient directions with different probabilities. Throughout training, the gradient direction pool is iteratively updated, assigning higher probabilities to potential principal directions for high-confidence samples. On the other hand, uncertain samples are compelled to explore different directions instead of misleading the model in a fixed direction. Extensive experiments demonstrate that GSS achieves comparable performance to a model trained with clean data. Furthermore, GSS can be easily integrated into existing frameworks. This approach of switching gradient directions offers a fresh perspective for future research in the field of noisy-label learning.