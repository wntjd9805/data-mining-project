This study focuses on controlling the content generated by pre-trained generative models based on ordinary differential equations (ODEs). The authors propose a method called FlowGrad, which optimizes the output of ODE models based on a guidance function to achieve controllable generation. They demonstrate that gradients can be efficiently back-propagated from the output to intermediate time steps on the ODE trajectory by decomposing the back-propagation and computing vector-Jacobian products. To speed up the computation, a non-uniform discretization technique is introduced to approximate the ODE trajectory, where the straight parts are grouped together in one discretization step. This approach reduces the back-propagation time by approximately 90% with negligible error. FlowGrad outperforms existing baselines in text-guided image manipulation tasks. Additionally, it allows the discovery of global semantic directions in frozen ODE-based generative models, enabling the manipulation of new images without additional optimization.