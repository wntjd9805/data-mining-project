Recently, the effectiveness of flat minima for improving generalization has been demonstrated, with sharpness-aware minimization (SAM) achieving state-of-the-art performance. However, the current definition of flatness used in SAM and related studies is limited to zeroth-order flatness, which considers only the worst-case loss within a perturbation radius. This definition proves inadequate in distinguishing between minima with low generalization error and those with high generalization error, whether there is a single minimum or multiple minima within the perturbation radius. To address this limitation, we propose first-order flatness, a stronger measure that focuses on the maximal gradient norm within the perturbation radius. This measure bounds both the maximal eigenvalue of the Hessian at local minima and the regularization function of SAM. Additionally, we introduce a novel training procedure called Gradient norm Aware Minimization (GAM) to find minima with uniformly small curvature in all directions. Experimental results demonstrate that GAM enhances the generalization of models trained using popular optimizers like SGD and AdamW across various datasets and networks. Furthermore, GAM complements SAM by aiding in the discovery of flatter minima and improving generalization. The code for GAM is available at https://github.com/xxgege/GAM.