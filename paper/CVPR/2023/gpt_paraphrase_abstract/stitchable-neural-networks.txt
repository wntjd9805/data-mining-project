The availability of powerful pretrained model families in public model zoos, such as ResNet and DeiT, has greatly contributed to the success of deep learning. However, the diverse scales within each model family raise the question of how to efficiently combine these models for dynamic accuracy-efficiency trade-offs during model deployment. In this study, we propose Stitchable Neural Networks (SN-Net), a scalable and efficient framework for model deployment. SN-Net splits pretrained models, or anchors, across blocks or layers and uses stitching layers to map activations between anchors. With minimal training, SN-Net can interpolate between anchors of different scales, allowing it to adapt to resource constraints at runtime. Experimental results on ImageNet classification show that SN-Net performs on par with or even better than individually trained networks, supporting various deployment scenarios. For example, by stitching Swin Transformers, we can challenge numerous models in the Timm model zoo with just one network. We believe that this elastic model framework can serve as a strong foundation for further research in wider communities.