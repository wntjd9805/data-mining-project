This study proposes a scalable multi-dataset detector called ScaleDet, which allows for the utilization of diverse large-scale datasets without incurring additional annotation costs. Unlike existing methods that rely on manual relabeling or complex optimization techniques to unify labels, ScaleDet introduces a straightforward and scalable approach to create a unified semantic label space for multi-dataset training. The model is trained using visual-textual alignment to learn label assignments based on semantic similarities across datasets. Once trained, ScaleDet demonstrates strong generalization capabilities on both seen and unseen classes in any given upstream and downstream datasets. The effectiveness of ScaleDet is evaluated through extensive experiments using various datasets, including LVIS, COCO, Objects365, OpenImages, and ODinW. The results show that ScaleDet outperforms state-of-the-art detectors with the same backbone, achieving impressive mean average precision (mAP) scores of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on OpenImages, and 71.8 on ODinW.