The self-attention mechanism has played a crucial role in the recent advancements of Vision Transformer (ViT) by allowing adaptive feature extraction from global contexts. However, current self-attention methods either use sparse global attention or window attention to reduce computation complexity, which may compromise local feature learning or rely on handcrafted designs. In contrast, local attention restricts the receptive field of each query to neighboring pixels, offering the benefits of both convolution and self-attention, such as local inductive bias and dynamic feature selection. However, existing local attention modules either employ inefficient Im2Col functions or rely on specific CUDA kernels that cannot be generalized to devices without CUDA support. In this study, we propose a novel local attention module called Slide Attention, which utilizes common convolution operations for high efficiency, flexibility, and generalizability. We reinterpret the column-based Im2Col function from a new row-based perspective and substitute it with efficient Depthwise Convolution. Additionally, we introduce a deformed shifting module based on re-parameterization techniques, allowing for the relaxation of fixed key/value positions to deformed features in the local region. This enables our module to achieve local attention in an efficient and flexible manner. Extensive experiments demonstrate that our slide attention module is applicable to various advanced Vision Transformer models and compatible with different hardware devices. Moreover, it consistently improves performance on comprehensive benchmarks.