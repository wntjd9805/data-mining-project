In recent times, the rise of Metaverse and AI-generated content has led to increased interest in cross-modal retrieval involving 2D and 3D data. However, this task is challenging due to the heterogeneous nature and semantic differences between the data. Additionally, the presence of imperfect annotations is common due to the ambiguity of the 2D and 3D content, resulting in noisy labels that negatively impact learning performance. To address this problem, this study presents a robust 2D-3D retrieval framework called RONO, which aims to learn from multimodal data with noise. The framework introduces a novel mechanism called Robust Discriminative Center Learning (RDCL) that adaptively distinguishes clean and noisy samples, providing them with positive and negative optimization directions respectively to mitigate the negative effects of noisy labels. Furthermore, a Shared Space Consistency Learning (SSCL) mechanism is proposed to capture intrinsic information within the noisy data by minimizing cross-modal and semantic discrepancies between common and label spaces simultaneously. The paper provides comprehensive mathematical analyses to theoretically demonstrate the noise tolerance of the proposed method. The effectiveness of the approach is validated through extensive experiments conducted on four 3D-model multimodal datasets, comparing it with 15 state-of-the-art methods. The code for the proposed method is available at https://github.com/penghu-cs/RONO.