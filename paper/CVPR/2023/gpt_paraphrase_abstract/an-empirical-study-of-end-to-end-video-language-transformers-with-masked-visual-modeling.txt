Masked visual modeling (MVM) has been proven to be effective in visual pre-training. However, previous studies on re-constructive objectives in video-language (VidL) pre-training have not found a truly effective MVM strategy that significantly improves downstream performance. In this study, we investigate the potential of MVM in VidL learning using the VIdeO-LanguagE Transformer (VIOLET) model. We explore eight different reconstructive targets for MVM, ranging from low-level pixel values to high-level visual features. Through comprehensive experiments, we identify the factors that contribute to effective MVM training and present an enhanced version of VIOLET, called VIOLETv2. Empirically, we demonstrate that VIOLETv2, pre-trained with the MVM objective, achieves notable improvements on 13 VidL benchmarks, including video question answering, video captioning, and text-to-video retrieval.