We propose VoxFormer, an AI framework called Transformer-based semantic scene completion, that can generate complete 3D volumetric semantics from 2D images. Our framework consists of two stages: initial estimation of visible and occupied voxel queries using depth estimation, followed by densification to generate dense 3D voxels from the sparse ones. The key idea behind our design is that the visual features in 2D images represent only the visible scene structures, not the occluded or empty spaces. Therefore, focusing on featurizing and predicting the visible structures is more reliable. To propagate information to all voxels, we use a masked autoencoder design with self-attention. Our experiments on SemanticKITTI dataset demonstrate that VoxFormer outperforms the current state-of-the-art, showing a relative improvement of 20.0% in geometry and 18.1% in semantics. Furthermore, our framework reduces GPU memory usage during training to less than 16GB. The code for VoxFormer is available on GitHub at https://github.com/NVlabs/VoxFormer.