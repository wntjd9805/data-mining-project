Domain generalization (DG) aims to improve the generalization ability of deep neural networks by training models with multiple source domains. One common approach to DG is domain augmentation, which assumes that increasing the diversity of source domains will enhance out-of-distribution generalization. However, these claims have been based on intuition rather than rigorous mathematical analysis. Our empirical investigations reveal that the correlation between model generalization and domain diversity may not always be strictly positive, limiting the effectiveness of domain augmentation. In order to address this limitation, our work proposes a new perspective on DG that frames it as a convex game between domains. We introduce a regularization term based on supermodularity to encourage each diversified domain to enhance model generalization. Additionally, we develop a sample filter to eliminate low-quality samples and mitigate the influence of potentially harmful information. Our framework offers a novel approach for the formal analysis of DG, and our heuristic analysis and extensive experiments demonstrate its rationality and effectiveness.