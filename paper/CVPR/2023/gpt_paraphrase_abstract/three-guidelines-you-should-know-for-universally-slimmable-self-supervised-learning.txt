We present a new approach called universally slimmable self-supervised learning (US3L) that aims to improve the accuracy-efficiency trade-offs in deploying self-supervised models on different devices. We found that directly applying self-supervised learning to universally slimmable networks often leads to training failures. However, we discovered that maintaining temporal consistency in the guidance is crucial for the success of self-supervised learning in universally slimmable networks. To ensure this consistency, we propose three guidelines for loss design from a unified gradient perspective. Additionally, we introduce dynamic sampling and group regularization strategies to enhance both training efficiency and accuracy. We empirically validate our US3L method on convolutional neural networks and vision transformers. Remarkably, our method surpasses various state-of-the-art methods, including individually trained models, on recognition, object detection, and instance segmentation benchmarks. With just a single training session and one set of weights, our approach demonstrates superior performance.