Decision forests are highly accurate models in machine learning, despite their heuristic training process that does not optimize any well-defined loss. While mechanisms like bagging or boosting have been crucial to the success of forests, we believe that improving the optimization process could eliminate the need for ensembling heuristics. However, optimizing forests or trees is challenging due to their non-differentiable nature. For the first time, we demonstrate that it is possible to learn a forest by jointly optimizing a desired loss and regularization across all trees and parameters. Our Forest Alternating Optimization algorithm defines a forest as a parametric model with a fixed number of trees and structure, and iteratively updates each tree to decrease the objective function. The algorithm is highly effective at optimization but tends to overfit, which can be corrected by averaging. As a result, our forest consistently outperforms the state-of-the-art models in terms of accuracy while utilizing fewer and smaller trees.