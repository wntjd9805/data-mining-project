In this study, we introduce CLIP-S4, a novel approach to semantic segmentation that overcomes the limitations of existing methods. Traditionally, semantic segmentation requires expensive pixel-wise annotations and predefined classes. However, CLIP-S4 utilizes self-supervised pixel representation learning and vision-language models to perform various semantic segmentation tasks without the need for human annotations or knowledge of unknown classes.To achieve this, we first train the model to learn pixel embeddings through pixel-segment contrastive learning using different views of augmented images. Additionally, we enhance the pixel embeddings and enable language-driven semantic segmentation by incorporating two types of consistency guided by vision-language models. The first type is embedding consistency, which aligns our pixel embeddings with the joint feature space of a pre-trained vision-language model called CLIP. The second type is semantic consistency, which ensures that our model makes the same predictions as CLIP for a set of target classes that include both known and unknown prototypes.By adopting CLIP-S4, we introduce a class-free semantic segmentation task that eliminates the need for unknown class information during training. Our approach consistently outperforms state-of-the-art unsupervised and language-driven semantic segmentation methods on four popular benchmarks. Notably, our method significantly surpasses these methods in terms of unknown class recognition.