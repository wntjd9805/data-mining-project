To ensure the usefulness of robots, they need to be able to locate objects based on human descriptions without extensive training on specific data. This concept is referred to as language-driven zero-shot object navigation (L-ZSON). In this study, we propose a framework called CLIP on Wheels (CoW) that adapts open-vocabulary models to L-ZSON without fine-tuning. To evaluate the performance of L-ZSON, we introduce the PASTURE benchmark, which includes scenarios involving uncommon objects, objects described by attributes, and hidden objects relative to visible ones. We conduct a comprehensive empirical study by deploying 22 CoW baselines across HABITAT, ROBOTHOR, and PASTURE. Our evaluation, which consists of over 90k navigation episodes, reveals that the CoW baselines struggle to utilize language descriptions effectively but excel at finding uncommon objects. Surprisingly, a simple CoW model, equipped with CLIP-based object localization and classical exploration techniques without any additional training, achieves the same navigation efficiency as a state-of-the-art ZSON method trained for 500M steps on HABITAT MP3D data. Furthermore, this CoW model outperforms a state-of-the-art ROBOTHOR ZSON model by 15.6 percentage points in terms of success rate.