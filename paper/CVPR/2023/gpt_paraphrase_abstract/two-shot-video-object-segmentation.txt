Previous research on video object segmentation (VOS) has primarily focused on training models using densely annotated videos. However, this approach is expensive and time-consuming due to the need for pixel-level annotations. In this study, we propose a new training paradigm called two-shot video object segmentation (two-shot VOS) that demonstrates the feasibility of training a satisfactory VOS model using only two labeled frames per training video. Our approach involves generating pseudo labels for unlabeled frames during training and optimizing the model using a combination of labeled and pseudo-labeled data. This simple approach can be easily applied to existing frameworks. We first pre-train a VOS model on sparsely annotated videos in a semi-supervised manner, with the first frame always being labeled. Then, we use the pre-trained model to generate pseudo labels for all unlabeled frames, which are stored in a pseudo-label bank. Finally, we retrain a VOS model using both labeled and pseudo-labeled data, without any restrictions on the first frame. This is the first general method proposed for training VOS models on two-shot VOS datasets. Our approach achieves comparable results to models trained on fully labeled datasets, using only a small percentage (7.3% and 2.9%) of labeled data from the YouTube-VOS and DAVIS benchmarks. The code and models for our approach are available at https://github.com/yk-pku/Two-shot-Video-Object-Segmentation. Figure 1 shows the problem formulation and a comparison of our approach with other STCN variants on various datasets.