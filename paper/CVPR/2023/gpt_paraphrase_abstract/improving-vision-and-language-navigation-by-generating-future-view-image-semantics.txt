This paper focuses on the task of Vision-and-Language Navigation (VLN), which involves an agent navigating an environment based on natural language instructions. The authors aim to investigate whether the agent can benefit from generating potential future views during navigation. Humans typically have expectations of what the future environment will look like, based on instructions and surrounding views, which aids in navigation. To equip the agent with the ability to generate future navigation views, the authors propose three proxy tasks during pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These tasks teach the model to predict missing views in a panorama, missing steps in the trajectory, and generate the next view based on the instruction and navigation history, respectively. The agent is then fine-tuned on the VLN task with an auxiliary loss that minimizes the difference between the agent-generated view semantics and the ground truth view semantics. Empirically, the authors' VLN-SIG achieves state-of-the-art results on the Room-to-Room dataset and CVDN dataset. They also demonstrate that the agent learns to fill in missing patches in future views, improving interpretability of predicted actions. Additionally, learning to predict future view semantics leads to better performance on longer paths.