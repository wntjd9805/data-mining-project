Multi-task learning (MTL) aims to train one model to perform multiple tasks by utilizing shared information among the tasks. However, existing MTL models often suffer from negative interference between tasks. Previous attempts to address this interference have focused on balancing loss/gradient or using partial overlaps in parameter partitioning among tasks. In this study, we propose ETR-NLP, which combines non-learnable primitives (NLPs) and explicit task routing (ETR) to mitigate task interference. Our approach involves using NLPs to extract task-agnostic features, which are then combined in a shared branch for all tasks and task-specific branches for each individual task. By explicitly decoupling the learnable parameters into shared and task-specific ones, along with the use of non-learnable primitives, we allow for flexibility in minimizing task interference. We evaluate the effectiveness of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Our experimental results demonstrate that ETR-NLP outperforms state-of-the-art baselines with fewer learnable parameters and comparable computational complexity across all datasets. The code for ETR-NLP is available at the following URL.