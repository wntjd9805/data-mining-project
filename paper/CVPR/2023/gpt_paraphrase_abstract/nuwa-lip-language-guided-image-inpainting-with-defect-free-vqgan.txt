This study addresses the challenge of language-guided image inpainting, which involves filling in defective regions of an image using text guidance while preserving the non-defective regions. Existing methods often result in distorted structures on the non-defective parts due to the direct encoding of defective images. To overcome this limitation, the authors propose a new approach called N Â¨UWA-LIP, which combines defect-free VQGAN (DF-VQGAN) and a multi-perspective sequence-to-sequence module (MP-S2S).DF-VQGAN introduces relative estimation to control the spread of information and symmetrical connections to preserve structure details in the inpainting process. MP-S2S is employed to effectively incorporate text guidance into the locally defective regions by aggregating different perspectives from low-level pixels, high-level tokens, and text descriptions.Experimental results demonstrate that DF-VQGAN significantly improves the inpainting process without causing unexpected changes in non-defective regions. The proposed method outperforms state-of-the-art approaches on three open-domain benchmarks. The authors plan to make their code, datasets, and model publicly available.