Current methods for recovering 3D interacting hands have primarily been tested in controlled motion capture environments, rather than real-world scenarios. This is because it is extremely difficult to collect data on 3D interacting hands in the wild, even in 2D form. To address this challenge, we propose InterWild, a method that combines motion capture and in-the-wild samples to enable robust 3D interacting hands recovery using a limited amount of in-the-wild 2D/3D data. The recovery process involves two sub-problems: recovering the 3D position of each hand, and determining the relative translation between the two hands. To solve the first sub-problem, we bring together motion capture and in-the-wild samples in a shared 2D scale space. While in-the-wild datasets provide limited 2D/3D interacting hands data, they contain a significant amount of large-scale 2D single hand data. Taking advantage of this, we use a single hand image as input for the first sub-problem, regardless of whether two hands are interacting. This allows us to align the interacting hands from motion capture datasets with the single hands from in-the-wild datasets in the shared 2D scale space.For the second sub-problem, we bring motion capture and in-the-wild samples to a shared appearance-invariant space. Unlike the first sub-problem, the 2D labels from in-the-wild datasets are not useful due to the ambiguity of 3D translations. Instead, we enhance the generalizability of motion capture samples by using only geometric features, without relying on images, as input for the second sub-problem. Since geometric features are invariant to appearances, motion capture and in-the-wild samples do not suffer from a significant appearance gap between the two datasets. Our code for InterWild is publicly available, allowing researchers to utilize our method for 3D interacting hands recovery in the wild.