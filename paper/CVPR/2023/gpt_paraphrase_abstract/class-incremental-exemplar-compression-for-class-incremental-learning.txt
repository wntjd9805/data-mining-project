This study proposes an approach to address the limited memory budget in exemplar-based class-incremental learning (CIL) by compressing exemplars through downsampling non-discriminative pixels and saving them in memory. The compression is achieved by generating 0-1 masks on discriminative pixels using class activation maps (CAM). A model called class-incremental masking (CIM) is introduced to overcome two challenges in using CAM: the trade-off between coverage on discriminative pixels and the quantity of exemplars, and the variation of optimal thresholds for different object classes. The CIM model is optimized alternately with the conventional CIL model using bilevel optimization. Extensive experiments on high-resolution CIL benchmarks demonstrate the effectiveness of using compressed exemplars by CIM, achieving a state-of-the-art CIL accuracy. The code for this approach is available at the provided GitHub link.