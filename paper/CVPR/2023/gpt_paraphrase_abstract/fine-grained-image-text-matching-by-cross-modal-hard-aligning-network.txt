Current image-text matching methods align visual and semantic fragments and use a cross-attention mechanism to find detailed cross-modal semantic connections. However, this mechanism can lead to unnecessary or irrelevant alignments, which reduces retrieval accuracy and efficiency. Although some progress has been made in finding meaningful alignments and improving accuracy, the efficiency issue remains unresolved. This study proposes a coding framework to explain the alignment process and offers a new perspective on the cross-attention mechanism and the problem of redundant alignments. Using this framework, the researchers design a Cross-modal HardAligning Network (CHAN) that focuses on the most relevant region-word pairs and eliminates all other alignments. Experiments on two public datasets confirm that the relevance of the most associated pairs is a reliable indicator of image-text similarity, with superior accuracy and efficiency compared to existing approaches. The code for the proposed method is available at https://github.com/ppanzx/CHAN. The paper includes a figure illustrating different semantic corresponding methods, highlighting the improvement made by CHAN in attending to the most relevant region while neglecting misalignments.