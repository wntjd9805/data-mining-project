This study focuses on Binary Neural Networks (BNNs), where convolution weights are represented by 1-bit values to improve storage and computation efficiency. Previous research has shown that successful BNNs have binary kernels that follow a power-law distribution, meaning that their values are mostly concentrated in a small number of codewords. Building on this observation, the goal of this paper is to compact typical BNNs and achieve similar performance by learning non-repetitive kernels within a binary kernel subspace. This is done by treating the binarization process as kernel grouping using a binary codebook, and the task is to learn how to select a smaller subset of codewords from the full codebook. The Gumbel-Sinkhorn technique is used to approximate the codeword selection process, and a method called Permutation Straight-Through Estimator (PSTE) is developed to optimize this selection process while maintaining the non-repetitive occupancy of selected codewords. Experimental results demonstrate that this approach reduces both the model size and computational costs, and also achieves better accuracy compared to state-of-the-art BNNs with similar resource budgets.