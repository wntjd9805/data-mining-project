This study aims to explore an essential question in self-supervised learning of point clouds: what is an effective signal that can be utilized to learn features from point clouds without annotations? In order to address this question, a framework for point cloud representation learning is introduced, which is based on geometric feature reconstruction. Unlike previous studies that employ masked autoencoders (MAE) and solely predict original coordinates or occupancy from masked point clouds, our approach considers the distinctions between images and point clouds and identifies three self-supervised learning objectives specific to point clouds: centroid prediction, normal estimation, and curvature prediction. When combined, these three objectives create a challenging self-supervised learning task that mutually enhances models' ability to understand the detailed geometry of point clouds. The pipeline of our method is conceptually straightforward and consists of two main steps. First, groups of points are randomly masked out, followed by encoding using a Transformer-based point cloud encoder. Second, a lightweight Transformer decoder predicts centroid, normal, and curvature for points within each voxel. The pre-trained Transformer encoder is then transferred to a downstream perception model. Upon evaluation on the nuScene Dataset, our model demonstrates a noteworthy improvement of 3.38 mAP for object detection, 2.1 mIoU gain for segmentation, and 1.7 AMOTA gain for multi-object tracking. Additionally, experiments conducted on the Waymo OpenDataset also yield significant performance enhancements compared to baseline models.