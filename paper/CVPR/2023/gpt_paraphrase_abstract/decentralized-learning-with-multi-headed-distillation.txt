We propose a new method for decentralized learning with private data in the field of machine learning. This technique allows multiple agents with their own private and non-iid data to learn from each other without sharing their data, weights, or weight updates. Our approach is efficient in terms of communication, as it utilizes an unlabeled public dataset and incorporates multiple auxiliary heads for each client. This greatly enhances training efficiency, especially when dealing with heterogeneous data. Our method enables individual models to maintain and improve performance on their private tasks while also significantly enhancing performance on the global aggregated data distribution. We investigate the impact of data and model architecture heterogeneity, as well as the influence of the communication graph topology on learning efficiency. Our results demonstrate that our agents can substantially improve their performance compared to learning in isolation.