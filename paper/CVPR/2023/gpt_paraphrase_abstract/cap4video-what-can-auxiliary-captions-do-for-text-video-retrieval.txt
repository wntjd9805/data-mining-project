Most current methods for text-video retrieval focus on matching the visual content of videos with textual query sentences. However, in real-world situations, online videos often have accompanying text information like titles, tags, and subtitles that can be used for matching textual queries. With this in mind, we propose a new approach to text-video retrieval that generates captions directly from videos using zero-shot video captioning with the help of web-scale pre-trained models (such as CLIP and GPT-2). We introduce Cap4Video, a framework that utilizes captions in three ways: 1) video-caption pairs can be used to augment the training data, 2) cross-modal feature interaction between the video and caption enhances video representations, and 3) the Query-Caption matching branch complements the original Query-Video matching branch for text-video retrieval. We conduct comprehensive ablation studies to demonstrate the effectiveness of our approach. Cap4Video achieves state-of-the-art performance on four standard text-video retrieval benchmarks (MSR-VTT, VATEX, MSVD, and DiDeMo) without any post-processing. The code is available at https://github.com/whwu95/Cap4Video.