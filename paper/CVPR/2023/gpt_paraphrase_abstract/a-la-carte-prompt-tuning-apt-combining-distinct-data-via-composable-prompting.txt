We present A-la-carte Prompt Tuning (APT), a transformer-based method that allows for the customization of prompts on specific data, enabling them to be combined flexibly during inference. Each prompt can be trained independently, potentially on different devices, at different times, and on different data distributions or domains. Additionally, each prompt only contains information about the subset of data it was trained on. This approach, which we refer to as "a-la-carte learning," allows for the creation of personalized models tailored to each user's access rights and preferences. By adding or removing prompts, we can modify the information contained in the model without the need for retraining from scratch. Our experiments show that models built using a-la-carte learning achieve accuracy levels within 5% of models trained on the combined data sources, while incurring similar training and inference time costs. In the Split CIFAR-100 and CORe50 continual learning benchmarks, we achieve state-of-the-art performance.