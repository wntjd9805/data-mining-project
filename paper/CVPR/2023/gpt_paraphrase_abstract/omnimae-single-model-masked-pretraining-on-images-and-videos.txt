A new benchmark has been set by Transformer-based architectures, which have proven to be competitive in various visual domains such as images and videos. Previous studies have focused on these modalities individually, but the use of a common architecture suggests the possibility of training a single model for multiple visual modalities. However, previous attempts at unified modeling either employed vision-specific architectures or yielded inferior performance compared to single-modality models. In this research, we demonstrate that a simple VisionTransformer can be trained on images and videos using masked autoencoding, without the need for labeled data. This single model learns visual representations that are on par with or surpass single-modality representations on image and video benchmarks, all while utilizing a less complex architecture. Additionally, this model can be trained rapidly by discarding a significant portion of image and video patches, making it feasible to train large model architectures at a fast pace. Notably, our single ViT-Huge model achieves 86.6% accuracy on ImageNet and 75.5% accuracy on the challenging Something Something-v2 video dataset through fine-tuning.