Existing state-of-the-art video models process videos by treating them as a sequence of spatio-temporal tokens, without explicitly modeling objects and their interactions. This paper aims to enhance video models by leveraging object knowledge to reduce the number of tokens processed and improve recognition accuracy. Unlike previous approaches that either sacrifice accuracy by dropping tokens or increase computational requirements for improved accuracy, we propose two methods. First, we introduce an object-guided token sampling strategy that retains a small portion of input tokens without significant accuracy loss. Second, we propose an object-aware attention module that enhances feature representation with object information, leading to improved accuracy. Our model, ObjectViViT, outperforms strong baselines using fewer tokens. Specifically, we achieve comparable results to the baseline using only 30%, 40%, and 60% of input tokens on SomethingElse, Something-something v2, and Epic-Kitchens, respectively. When processing the same number of tokens as the baseline, Object-ViViT improves performance by 0.6 to 4.2 points on these datasets.