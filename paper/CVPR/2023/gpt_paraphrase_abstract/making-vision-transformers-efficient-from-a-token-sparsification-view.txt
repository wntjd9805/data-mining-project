The practical applications of Vision Transformers (ViTs) are limited due to their quadratic computational complexity. Previous methods that prune redundant tokens to improve efficiency have drawbacks such as significant accuracy drops, difficulty in applying them to local vision transformers, and lack of general-purpose networks for downstream tasks. To address these issues, we propose a novel approach called Semantic Token ViT (STViT) that enables efficient global and local vision transformers and can be adapted as a backbone for downstream tasks. In STViT, semantic tokens represent cluster centers and are initialized by pooling image tokens in space and recovered through attention, allowing them to adaptively capture global or local semantic information. The cluster properties of semantic tokens enable a few tokens to achieve the same effect as a large number of image tokens in both global and local vision transformers. For example, using only 16 semantic tokens on DeiT-(Tiny,Small,Base) leads to the same accuracy as a larger number of tokens but with more than 100% faster inference speed and nearly 60% reduction in FLOPs. Similarly, employing 16 semantic tokens in each window of Swin-(Tiny,Small,Base) accelerates the model by around 20% with a slight increase in accuracy. Our method is not limited to image classification and is extended to video recognition as well. Additionally, we introduce a STViT-Recovery network that restores detailed spatial information based on STViT, making it suitable for downstream tasks where previous token sparsification methods fail. Experimental results show that our method achieves competitive results compared to the original networks in object detection and instance segmentation, while reducing backbone FLOPs by over 30%.