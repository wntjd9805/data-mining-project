This paper introduces PointClustering, an unsupervised representation learning approach that utilizes transformation invariance for pre-training on point clouds. The method views transformation invariance as a form of self-supervision for learning representations. PointClustering formulates the pretext task as deep clustering and incorporates transformation invariance as an inductive bias. The approach optimizes feature clusters and the backbone iteratively, considering both point level and instance level invariance learning. Point-level invariance learning preserves local geometric properties by aggregating point features from one instance across transformations, while instance-level invariance learning examines cluster semantics across the entire dataset. PointClustering is compatible with various backbone architectures, including MLP-based, CNN-based, and Transformer-based models. Experimental results on the ScanNet dataset demonstrate that models pre-trained using PointClustering outperform other methods on six benchmark tasks, including classification and segmentation. Notably, PointClustering achieves a 94.5% accuracy on ModelNet40 when using a Transformer backbone. The source code for PointClustering is available at https://github.com/FuchenUSTC/PointClustering.