A video forensics method using anomaly detection is proposed to identify subtle inconsistencies between visual and audio signals in manipulated videos. The method is trained solely on real, unlabeled data using an autoregressive model that generates audio-visual feature sequences. These feature sets capture the temporal synchronization between video frames and sound. During testing, videos with low probability assigned by the model are flagged as potentially manipulated. Remarkably, the model achieves high performance in detecting manipulated speech videos despite being trained entirely on real videos. The project site for this method can be found at https://cfeng16.github.io/audio-visual-forensics.