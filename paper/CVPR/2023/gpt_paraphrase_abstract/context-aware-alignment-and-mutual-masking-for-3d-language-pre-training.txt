The current methods for 3D visual reasoning in human-computer interaction are limited to specific tasks and do not have pre-training methods to learn general representations that can be applied to different tasks. The lack of paired data for 3D-language pre-training, along with the complex structure of point clouds and spatial relationship ambiguities, make this a challenging problem. This paper introduces a generic 3D-language pre-training approach that addresses multiple aspects of 3D-language reasoning by learning universal representations. The learning objective consists of two main parts: 1) Context aware spatial-semantic alignment, which aligns point clouds and texts to reduce relational ambiguities, and 2) Mutual 3D-Language Masked modeling, which enables information exchange between modalities. Instead of reconstructing sparse 3D points, the proposed method uses masked proposal reasoning to learn semantic class and mask-invariant representations. When adapted to various downstream tasks such as 3D visual grounding, 3D dense captioning, and 3D question answering, the proposed method achieves promising results. The code for this method is available at the provided GitHub link.