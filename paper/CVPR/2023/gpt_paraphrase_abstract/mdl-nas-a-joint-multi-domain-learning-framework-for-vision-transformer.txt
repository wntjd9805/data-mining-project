This study introduces MDL-NAS, a comprehensive framework that combines multiple vision tasks into a manageable supernet and optimizes them collectively across different dataset domains. MDL-NAS achieves storage efficiency by consolidating multiple models into a single one through shared parameters. The framework utilizes a coarse-to-fine search space, where the coarse search space offers optimal architectures for different tasks, while the fine search space addresses the challenges of multi-domain learning through fine-grained parameter sharing. Two parameter sharing policies, sequential sharing and mask sharing, are proposed in the fine search space, allowing for partial sharing and non-sharing of parameters at each layer of the network. The study also presents a joint-subnet search algorithm that identifies the optimal architecture and sharing parameters for each task while considering resource constraints. This challenges the conventional approach of using backbone networks designed for image classification in downstream vision tasks. Experimental results demonstrate that both non-hierarchical and hierarchical transformers within the MDL-NAS framework achieve competitive performance compared to state-of-the-art methods, while maintaining efficient storage deployment and computation. The study also shows that MDL-NAS enables incremental learning and avoids catastrophic forgetting when generalizing to new tasks.