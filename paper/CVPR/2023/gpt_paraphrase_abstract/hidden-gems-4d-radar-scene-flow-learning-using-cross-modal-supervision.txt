This study presents a new method for estimating 4D radar-based scene flow by utilizing cross-modal learning. The approach is motivated by the redundant sensing capabilities of autonomous vehicles, which provide various cues for supervising radar scene flow estimation. The authors propose a multi-task model architecture and loss functions to effectively train the model using multiple cross-modal constraints. Extensive experiments demonstrate the superior performance of the proposed method and its ability to accurately infer 4D radar scene flow. The study also showcases the usefulness of cross-modal supervised learning for motion segmentation and ego-motion estimation. The source code for this work will be made available on https://github.com/Toytiny/CMFlow.