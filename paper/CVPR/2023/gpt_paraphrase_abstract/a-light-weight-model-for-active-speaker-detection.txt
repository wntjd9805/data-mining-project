In audio-visual scenarios, active speaker detection is a difficult task that aims to identify who is speaking in scenarios with one or more speakers. This task is important in many applications and previous studies have tried to enhance performance by using multiple candidate information and complex models. However, these methods have high memory and computational requirements, making them unsuitable for resource-limited scenarios. Therefore, this study proposes a lightweight active speaker detection architecture that reduces the number of input candidates, uses separate convolutions for audio-visual feature extraction, and employs gated recurrent units with low computational complexity for cross-modal modeling. Experimental results on the AVA-ActiveSpeaker dataset show that the proposed framework achieves competitive mAP performance while having significantly lower resource costs compared to the state-of-the-art method. Specifically, the proposed framework has fewer model parameters and FLOPs, making it more efficient. The framework also performs well on the Columbia dataset, demonstrating its robustness. The code and model weights for the framework are available at the provided GitHub link.