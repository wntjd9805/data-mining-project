A crucial aspect of digital content creation is having a fully automated pipeline for object reconstruction. Although significant advancements have been made in 3D reconstruction, the process of removing the background to obtain a clean object model still requires manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this study, we propose a new framework called AutoRecon that automates the discovery and reconstruction of objects from multi-view images. We demonstrate that foreground objects can be accurately located and segmented from SfM point clouds by utilizing self-supervised 2D vision transformer features. Subsequently, we reconstruct decomposed neural scene representations using dense supervision from the decomposed point clouds, resulting in precise object reconstruction and segmentation. Our experiments on various datasets show the effectiveness and robustness of AutoRecon. More details, including the code and supplementary material, can be found on the project page: https://zju3dv.github.io/autorecon/. Figure 1 provides an overview of our fully automated pipeline and the resulting outcomes. When given a video focused on a specific object, we achieve a rough decomposition by segmenting the prominent foreground object from a semi-dense SfM point cloud, using pointwise-aggregated 2D DINO features. We then train a decomposed neural scene representation using multi-view images, aided by the coarse decomposition results, to reconstruct foreground objects and generate high-quality foreground masks that are consistent across multiple views.