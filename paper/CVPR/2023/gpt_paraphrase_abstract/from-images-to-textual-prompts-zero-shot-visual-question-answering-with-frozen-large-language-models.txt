Large language models (LLMs) have shown impressive ability to generalize to new language tasks without any specific training. However, applying LLMs to zero-shot visual question-answering (VQA) poses challenges due to the disconnect between the modalities (language and visual) and the tasks themselves. While end-to-end training on multimodal data can bridge these gaps, it is inflexible and computationally expensive. To overcome this issue, we propose a module called Img2LLM, which acts as a plug-and-play solution that provides LLM prompts for zero-shot VQA without the need for end-to-end training. Our approach involves developing LLM-agnostic models that describe image content using question-answer pairs, which serve as effective prompts for LLMs. The benefits of Img2LLM include achieving comparable or better performance than methods relying on end-to-end training, offering flexibility to work with various LLMs for VQA, and eliminating the need for specialized LLMs and costly end-to-end finetuning. The code for implementing Img2LLM is available through the LAVIS framework at the provided GitHub link. The answer format focuses solely on the abstraction.