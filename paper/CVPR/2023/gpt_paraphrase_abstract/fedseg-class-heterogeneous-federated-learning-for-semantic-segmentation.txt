Federated Learning (FL) is a distributed learning method that enables multiple clients to collaboratively learn a global model while preserving data privacy. While there have been numerous FL algorithms developed for classification tasks, there is limited focus on the more complex semantic segmentation tasks, particularly in situations where the classes are heterogeneous among clients. Semantic segmentation in heterogeneous FL poses greater challenges compared to classification. Firstly, the non-IID distribution leads to inconsistencies in foreground-background classes among clients, resulting in divergent local updates. Secondly, the presence of class heterogeneity in dense prediction tasks makes it difficult for clients to reach the global optimum.To address these challenges, we propose a basic FL approach called FedSeg for class-heterogeneous semantic segmentation. Our method tackles the local optimization issue and addresses the foreground-background inconsistency problem by introducing a modified cross-entropy loss. Additionally, we incorporate pixel-level contrastive learning to ensure that local pixel embeddings align with the global semantic space. Through extensive experiments on four benchmark datasets for semantic segmentation (Cityscapes, CamVID, PascalVOC, and ADE20k), we demonstrate the effectiveness of FedSeg. We hope that our work will attract more attention from the FL community towards the demanding field of semantic segmentation in federated learning.