Recent popular Role-Playing Games (RPGs) have seen the successful implementation of character auto-creation systems. These systems utilize a bone-driven face model that allows users to personalize and customize in-game characters using both continuous parameters (such as bone positions) and discrete parameters (such as hairstyles). Previous auto-creation systems relied on image-driven approaches, optimizing facial parameters to resemble a reference face photo. This paper introduces a new method called text-to-parameter translation (T2P) that enables text-driven game character auto-creation without the need for reference photos or manual parameter editing. T2P leverages the power of large-scale pre-trained multi-modal CLIP and neural rendering to search for both continuous and discrete facial parameters in a unified framework. Unlike previous methods, T2P effectively learns discrete facial parameters despite their discontinuous representation. Experimental results demonstrate that T2P can generate high-quality and vivid game characters based on text prompts, outperforming other state-of-the-art text-to-3D generation methods in both objective and subjective evaluations.