We present a new approach for single-image super-resolution (SISR) that addresses the limitations of using a single perceptual loss in generating high-resolution outputs. While networks trained with perceptual and adversarial losses produce high-contrast results, they often fail to accurately restore locally varying shapes, resulting in artifacts and unnatural details. Previous attempts at combining various losses have been challenging to optimize. To overcome this, we propose a two-model framework: a predictive model that determines the optimal objective map for a given low-resolution input, and a generative model that applies this objective map to produce the corresponding high-resolution output. The generative model is trained using our proposed objective trajectory, which represents a range of essential objectives, allowing the network to learn different SR results based on combined losses. The predictive model is trained using LR images and corresponding optimal objective maps from the trajectory. Experimental results on five benchmarks demonstrate that our method outperforms state-of-the-art perception-driven SR methods in various metrics, such as LPIPS, DISTS, PSNR, and SSIM. The visual results also showcase the superiority of our method in perception-oriented reconstruction. The code for our method is available at https://github.com/seungho-snu/SROOE.