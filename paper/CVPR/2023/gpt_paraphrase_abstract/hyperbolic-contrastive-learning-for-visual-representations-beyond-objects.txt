The focus of this paper is on learning representations for objects and scenes that maintain their inherent structure. Current self-/un-supervised methods treat objects and scenes equally, but we argue that they should follow a hierarchical structure based on their compositionality. To achieve this, we propose a contrastive learning framework that uses a Euclidean loss to learn object representations and a hyperbolic loss to encourage scene representations to be close to their constituent objects in a hyperbolic space. This hyperbolic objective promotes the scene-object relationship in the representations by optimizing their norms. Our experiments on the COCO and OpenImages datasets demonstrate that the hyperbolic loss enhances the performance of various baselines in image classification, object detection, and semantic segmentation tasks. Additionally, the learned representations enable us to solve vision tasks involving the interaction between scenes and objects in a zero-shot manner. The representation space learned by our models shows that object images of the same class gather near the center in similar directions, while scene images are located further away in these directions with larger norms.