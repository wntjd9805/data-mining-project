Continuous convolution has become popular in handling irregularly sampled data and modeling long-term dependency. The use of large convolutional kernels has further boosted its development by efficiently constructing these kernels. However, the prevalent approach of leveraging neural networks, particularly multilayer perceptrons (MLPs), has drawbacks such as high computational costs, complex hyperparameter tuning, and limited descriptive power of filters. To address these issues, this paper proposes an alternative method for continuous convolution that does not rely on neural networks. This approach is more computationally efficient and offers improved performance. The authors introduce self-moving point representations where weight parameters can freely move, and interpolation schemes are employed to implement continuous functions. Experimental results demonstrate that this alternative approach enhances the performance of continuous convolution with drop-in replacement in existing frameworks. Notably, the lightweight structure of this method allows for its successful application in a large-scale setting, such as ImageNet, and outperforms previous techniques. The code for this approach is available at the provided GitHub link.