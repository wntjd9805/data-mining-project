This paper introduces SPoTr, a Self-Positioning point-based Transformer that addresses the challenge of applying Transformers to point clouds. While Transformers have been successful in computer vision tasks, their quadratic cost in the number of points makes them unsuitable for point cloud data. SPoTr overcomes this limitation by incorporating local self-attention and self-positioning point-based global cross-attention. The self-positioning points, located based on the input shape, capture both spatial and semantic information to enhance expressive power. Additionally, SPoTr introduces a novel global cross-attention mechanism that improves the scalability of global self-attention by using a small set of self-positioning points to compute attention weights. Experimental results demonstrate the effectiveness of SPoTr on shape classification, part segmentation, and scene segmentation tasks. Notably, SPoTr achieves a 2.6% accuracy gain over previous models on shape classification with ScanObjectNN. The interpretability of self-positioning points is also demonstrated through qualitative analyses. The code for SPoTr is available at https://github.com/mlvlab/SPoTr.