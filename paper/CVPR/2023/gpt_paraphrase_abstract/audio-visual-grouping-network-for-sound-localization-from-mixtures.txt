This paper introduces a novel audio-visual grouping network called AVGN for sound source localization in videos. While previous methods focused on single-source localization using audio-visual association, this work aims to localize multiple sources simultaneously. Existing approaches are limited in their ability to handle a fixed number of sources and create compact class-aware representations for each source. To address these limitations, AVGN utilizes learnable audio-visual class tokens to aggregate class-aware source features. These features are then used to guide the localization of corresponding visual regions. The proposed framework outperforms existing methods in terms of localizing a flexible number of sources and disentangling category-aware audio-visual representations for individual sound sources. Extensive experiments conducted on various benchmarks demonstrate the state-of-the-art performance of AVGN in both single-source and multi-source scenarios. The code for AVGN implementation is available at https://github.com/stoneMo/AVGN.