This study focuses on the generation of synthetic talking videos using one-shot video-driven talking face generation. The goal is to transfer facial motion from a video to a portrait image while simultaneously considering head pose and facial expression. However, this entanglement of pose and expression creates a challenge for video portrait editing, specifically when modifications are required only in expression while keeping the pose unchanged. The main obstacle in decoupling pose and expression is the lack of paired data with the same pose but different expressions. While some methods have attempted to address this challenge using 3D Morphable Models (3DMMs) for explicit disentanglement, the limited number of Blend-shapes in 3DMMs results in inaccurate facial detail capture, which affects motion transfer. To overcome these limitations, this paper introduces a novel self-supervised disentanglement framework that does not rely on 3DMMs or paired data. The framework consists of a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where pose and expression motion can be disentangled, allowing for convenient pose or expression transfer through addition. The two generators then render the modified latent codes into images. To ensure disentanglement, a bidirectional cyclic training strategy with carefully designed constraints is proposed. Evaluations demonstrate that the method can independently control pose or expression and can be utilized for general video editing. The code for this method is available at https://github.com/Carlyx/DPE.