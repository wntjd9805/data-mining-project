Large text-to-image models have made significant advancements in AI by generating high-quality and diverse images based on text prompts. However, these models lack the ability to mimic the appearance of subjects from a given reference set and create new versions of them in different contexts. This study introduces a novel technique called "personalization" for text-to-image diffusion models. By fine-tuning a pretrained model with a few images of a subject, the model learns to associate a unique identifier with that specific subject. Using this identifier, the model can then generate new photorealistic images of the subject in various scenes, poses, views, and lighting conditions that are not present in the reference images. The technique utilizes the model's semantic prior and incorporates a new autogenous class-specific prior preservation loss to preserve the key features of the subject. The approach is applied to challenging tasks such as subject recontextualization, text-guided view synthesis, and artistic rendering, while maintaining the subject's essential characteristics. Additionally, a new dataset and evaluation protocol are provided for the subject-driven generation task.