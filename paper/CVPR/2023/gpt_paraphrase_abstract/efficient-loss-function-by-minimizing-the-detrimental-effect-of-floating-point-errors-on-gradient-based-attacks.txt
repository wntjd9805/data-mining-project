This study focuses on the vulnerability and weak robustness of current deep-learning networks when faced with human-imperceptive perturbations added to their input data. Various attack techniques have been proposed to assess the robustness of these models, but gradient-based attacks have been found to overestimate this robustness significantly. The reason behind this failure is identified as the relative error in calculated gradients, which is caused by floating-point errors such as underflow and rounding errors. Although it is challenging to eliminate this relative error, its impact on gradient-based attacks can be controlled. To address this issue, the authors propose an efficient loss function that minimizes the detrimental effect of floating-point errors on the attacks. Experimental results demonstrate that this loss function outperforms other alternatives in terms of efficiency and reliability when tested against a wide range of defense mechanisms.