Due to the growing popularity of edge devices, there is a need to implement real-time segmentation on the edge for various applications, including autonomous driving. Vision Transformers (ViTs) have shown promising results for vision tasks, but their full-attention mechanism requires significant computational resources, making real-time inference challenging on edge devices. This paper aims to develop ViTs with reduced computations and faster inference speed to enable dense semantic segmentation on edge devices. To achieve this, a pruning parameterization method is proposed to address the pruning problem in semantic segmentation. Additionally, a bi-level optimization method is employed, leveraging implicit gradients to solve the problem. Experimental results indicate that the proposed approach achieves a mIoU of 38.9 on ADE20K val while operating at a speed of 56.5FPS on Samsung S21, which represents the highest mIoU under the same computation constraint for real-time inference.