Pre-training deep convolutional neural networks (DC-NNs) is crucial for visual sentiment analysis (VSA). Most existing methods use pre-trained backbones from large-scale object classification datasets like ImageNet. While this improves performance compared to random initialization, we argue that DC-NNs pre-trained on ImageNet may focus too much on object recognition and lack high-level sentiment understanding. To address this issue, we propose a sentiment-oriented pre-training method based on human visual sentiment perception (VSP). We break down the VSP process into three steps: stimuli taking, holistic organizing, and high-level perceiving. We train three separate models to imitate each VSP step using sentiment-aware tasks that extract sentiment-discriminated representations. Through a multi-model amalgamation strategy, we transfer the knowledge learned from each perception step into a single target model, resulting in significant performance improvements. We validate the effectiveness of our method through extensive experiments covering various VSA tasks, including single-label learning, multi-label learning, and label distribution learning. The experimental results demonstrate consistent enhancements across these tasks. Our code is available on https://github.com/tinglyfeng/sentiment_pretraining.