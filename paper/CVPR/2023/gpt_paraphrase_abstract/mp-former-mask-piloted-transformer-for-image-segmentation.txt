We introduce a mask-piloted Transformer that enhances the masked-attention mechanism in Mask2Former for image segmentation. We address the issue of inconsistent mask predictions in Mask2Former, which result in inconsistent optimization objectives and limited utilization of decoder queries. To tackle this problem, we propose a mask-piloted training approach that incorporates noised ground-truth masks in masked-attention and trains the model to reconstruct the original masks. By utilizing ground-truth masks as a guide, our approach mitigates the negative impact of inaccurate mask predictions in Mask2Former. Our MP-Former achieves significant performance improvements across three image segmentation tasks (instance, panoptic, and semantic), with a ResNet-50 backbone leading to +2.3AP and +1.6mIoU on Cityscapes instance and semantic segmentation tasks. Additionally, our method accelerates training, surpassing Mask2Former with half the number of training epochs on ADE20K using both ResNet-50 and Swin-L backbones. Moreover, our method introduces minimal computation during training and no additional computation during inference. The source code for our method is available at https://github.com/IDEA-Research/MP-Former.