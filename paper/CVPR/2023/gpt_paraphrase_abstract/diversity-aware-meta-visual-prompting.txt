We introduce DAM-VP, a prompting method for transferring pre-trained models to downstream tasks with a fixed backbone. The problem with visual prompting is that image datasets often have diverse data, making it difficult for a generic prompt to handle the distribution shift towards the original pretraining data. To overcome this, we propose a Diversity-Aware prompting strategy that uses a Meta-prompt for initialization. We cluster the downstream dataset into subsets, each with its own optimized prompt. This divide-and-conquer approach reduces optimization difficulty and improves prompting performance. Additionally, all prompts are initialized with a meta-prompt learned from multiple datasets. This bootstrapped paradigm leverages prompting knowledge from previous datasets to speed up convergence and enhance performance on new datasets. During inference, we dynamically select a prompt based on the feature distance between the input and each subset. Our DAM-VP outperforms previous prompting methods on various downstream datasets and pretraining models, demonstrating superior efficiency and effectiveness. The code for DAM-VP is available at: https://github.com/shikiw/DAM-VP.