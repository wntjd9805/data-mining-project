Despite significant progress, deep learning has been found to be vulnerable to adversarial attacks. Various methods have been proposed to train robust networks, but most of them only defend against a single type of attack. Recent work has started to address multiple attacks, but there is still room for improvement. In this study, we approach the problem of multi-target robustness as a bargaining game, where different adversaries negotiate to agree on updating the model's parameters. We observe a phenomenon called player domination, where existing max-based approaches fail to converge. Through theoretical analysis, we introduce a new framework that adjusts the budgets of adversaries to prevent player dominance. Experimental results on standard benchmarks demonstrate that incorporating our framework into existing approaches significantly enhances multi-target robustness.