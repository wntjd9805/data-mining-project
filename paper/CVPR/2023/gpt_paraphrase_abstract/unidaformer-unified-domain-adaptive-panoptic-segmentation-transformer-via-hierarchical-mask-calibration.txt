Domain adaptive panoptic segmentation addresses the challenge of annotating data by utilizing pre-annotated data from related domains. However, existing methods use separate networks for instance segmentation and semantic segmentation, resulting in a large number of network parameters and complex training and inference processes. To overcome these limitations, we propose UniDAformer, a unified domain adaptive panoptic segmentation transformer. UniDAformer introduces Hierarchical Mask Calibration (HMC), which corrects inaccurate predictions at different levels (regions, superpixels, and pixels) through online self-training. It offers three key advantages: 1) enabling unified domain adaptive panoptic adaptation, 2) improving domain adaptive panoptic segmentation by reducing false predictions, and 3) being end-to-end trainable with a simpler training and inference pipeline. Extensive experiments on public benchmarks demonstrate that UniDAformer outperforms state-of-the-art methods in domain adaptive panoptic segmentation. Unlike existing approaches that use separate networks for things and stuff, UniDAformer utilizes a single unified network for joint adaptation, resulting in fewer parameters and a simplified training and inference pipeline.