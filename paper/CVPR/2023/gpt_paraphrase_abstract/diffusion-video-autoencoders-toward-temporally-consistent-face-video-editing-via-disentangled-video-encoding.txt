Several studies have attempted to extend face image editing methods to face video editing. However, achieving temporal consistency among edited frames remains a challenge. In this study, we propose a new face video editing framework using diffusion autoencoders. Our framework successfully extracts decomposed features of identity and motion from a given video, allowing for easy manipulation of temporally invariant features to ensure consistency. Additionally, our model, based on diffusion models, provides both reconstruction and editing capabilities simultaneously. Unlike existing GAN-based methods, our model is robust to corner cases in wild face videos, such as occluded faces.