Recently, numerous methods have been developed to explain deep neural networks by estimating the importance of individual image pixels in driving the model's decision. These methods use various types of perturbations, such as occlusion, blur-ring, and masking. However, the space of possible perturbations is vast, and current attribution methods require significant computation time to sample the space accurately and provide high-quality explanations. In this study, we introduce EVA (Explaining using Verified Perturbation Analysis), which is the first explainability method with guarantees that it has exhaustively searched through an entire set of possible perturbations. We utilize advancements in verified perturbation analysis methods to propagate bounds through a neural network and comprehensively explore a potentially infinite set of perturbations in a single forward pass. Our approach takes advantage of the time efficiency and guaranteed complete coverage of the perturbation space provided by verified perturbation analysis. This allows us to identify the image pixels that strongly influence the model's decision. We systematically evaluate EVA and demonstrate its state-of-the-art performance on multiple benchmarks. Our code is freely available on GitHub: github.com/deel-ai/formal-explainability.