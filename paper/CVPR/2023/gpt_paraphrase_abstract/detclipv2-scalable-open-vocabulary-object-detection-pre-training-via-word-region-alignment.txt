This paper introduces DetCLIPv2, a training framework designed to achieve open-vocabulary object detection (OVD) by incorporating large-scale image-text pairs. Unlike existing OVD frameworks that rely on pre-trained vision-language models or use pseudo labeling, DetCLIPv2 directly learns the alignment between words and image regions. The framework utilizes a maximum word-region similarity to guide the contrastive objective and is trained with a combination of supervision from detection, grounding, and image-text pair data. Through joint training and the use of low-resolution input for image-text pairs, DetCLIPv2 efficiently exploits image-text pair data. It outperforms previous works, such as GLIP, GLIPv2, and DetCLIP, with a substantial improvement in zero-shot AP on the LVIS benchmark. DetCLIPv2 demonstrates superior performance even compared to its fully-supervised counterpart.