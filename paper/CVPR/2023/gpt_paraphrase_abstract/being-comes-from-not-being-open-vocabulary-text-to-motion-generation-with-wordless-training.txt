Text-to-motion generation is a complex task that involves creating motion sequences that accurately represent the semantics of a given input text. However, existing approaches are limited by the availability of diverse labeled training data and often require online optimizations during inference, which can be inefficient and unstable. In this study, we propose an offline open-vocabulary text-to-motion generation method that does not rely on paired training data or online adaptations for unseen texts. Drawing inspiration from prompt learning in natural language processing, we pretrain a motion generator to reconstruct full motion from masked motion. During inference, instead of modifying the motion generator, we reframe the input text as a masked motion prompt for the generator to "reconstruct" the motion. To create the prompt, we use a text-to-pose generator to synthesize unmasked poses. We introduce a text-pose alignment model to supervise the optimization of the text-to-pose generator, measuring the alignment between texts and 3D poses. To prevent overfitting, we propose a wordless training mechanism that optimizes the text-to-pose generator without any training texts. Our extensive experiments demonstrate the superiority of our method compared to baseline approaches. The code for our method is available at https://github.com/junfanlin/oohmg.