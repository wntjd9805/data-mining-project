Semi-supervised action recognition is a difficult and important task that is hindered by the expense of annotating videos. While most current approaches use convolutional neural networks, the potential of vision transformer models has been largely unexplored. In this study, we investigate the use of transformer models in semi-supervised learning (SSL) for action recognition. We propose SVFormer, a framework that utilizes a steady pseudo-labeling technique called EMA-Teacher to handle unlabeled video samples. While data augmentations have been effective in semi-supervised image classification, they have limited impact on video recognition. Thus, we introduce a unique augmentation strategy called Tube Token-Mix, specifically designed for video data, where video clips are mixed using a mask with consistent masked tokens along the temporal axis. Additionally, we propose a temporal warping augmentation to account for complex temporal variations in videos by stretching selected frames to different durations within the clip. Extensive experiments on three datasets, Kinetics-400, UCF-101, and HMDB-51, demonstrate the superiority of SVFormer. Notably, SVFormer achieves a 31.5% improvement over the state-of-the-art with fewer training epochs at a 1% labeling rate on Kinetics-400. We believe our method can serve as a robust benchmark and inspire further research in semi-supervised action recognition using Transformer networks. The code is available at https://github.com/ChenHsing/SVFormer.