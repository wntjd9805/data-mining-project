This paper introduces a new approach called HNeRV (Hybrid Neural Representation for Videos) for storing and reconstructing videos using neural networks. Unlike previous methods that use fixed and content-agnostic embeddings, HNeRV utilizes a learnable encoder to generate content-adaptive embeddings. Additionally, HNeRV incorporates HNeRV blocks that distribute model parameters evenly throughout the network, allowing higher layers to store high-resolution content and video details. The experimental results demonstrate that HNeRV outperforms implicit methods in terms of video reconstruction quality (+4.7 PSNR) and convergence speed (16Ã— faster), while also exhibiting better internal generalization. Furthermore, HNeRV offers advantages in terms of decoding speed, flexibility, and deployment compared to traditional codecs (H.264, H.265) and learning-based compression methods. The effectiveness of HNeRV is also explored in downstream tasks such as video compression and video inpainting.