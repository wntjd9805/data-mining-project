Learning to incorporate geometric information, such as distance and shape, is crucial for comprehending the real world and various vision-related tasks. To address this, we propose a method called Learning by Sketching (LBS), inspired by human behavior, which converts an image into a set of colored strokes that explicitly represent the scene's geometric details. Unlike existing approaches, LBS achieves this in a single inference step without relying on a sketch dataset. The strokes are then used to generate a sketch, and a CLIP-based perceptual loss ensures semantic similarity between the sketch and the image. We demonstrate that sketching is equivariant to arbitrary affine transformations, preserving geometric information. Through experiments, we show that LBS significantly enhances object attribute classification on the unlabeled CLEVR dataset, facilitates domain transfer between CLEVR and STL-10 datasets, and improves performance on various downstream tasks. These results confirm that LBS effectively captures rich geometric information.