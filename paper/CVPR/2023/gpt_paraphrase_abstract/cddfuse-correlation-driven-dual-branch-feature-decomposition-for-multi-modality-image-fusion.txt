We propose a novel network called CDDFuse for multi-modality image fusion, which aims to combine different modalities while preserving their unique features. Our approach involves using Restormer blocks to extract shallow cross-modality features. We then introduce a dual-branch feature extractor that combines Transformer-CNN with Lite Transformer blocks for handling global features and Invertible Neural Networks blocks for extracting local information. To ensure the quality of the fused image, we propose a correlation-driven loss that encourages low-frequency features to be correlated and high-frequency features to be uncorrelated. The fused image is generated using global fusion and local fusion layers. Our experiments demonstrate that CDDFuse achieves excellent results in infrared-visible image fusion and medical image fusion tasks. Moreover, we show that CDDFuse improves performance in downstream tasks such as infrared-visible semantic segmentation and object detection. The code for our approach is available at https://github.com/Zhaozixiang1228/MMIF-CDDFuse.