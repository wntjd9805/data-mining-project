This paper introduces a new framework called TriDet for temporal action detection. Current methods often struggle with imprecise boundary predictions due to the unclear boundaries of actions in videos. To address this issue, the authors propose a novel Trident-head that models the action boundary using a relative probability distribution. Additionally, they propose a Scalable-Granularity Perception (SGP) layer in the feature pyramid of TriDet to mitigate the rank loss problem of self-attention and aggregate information across different temporal granularities. By incorporating the Trident-head and SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks (THUMOS14, HACS, and EPIC-KITCHEN 100) while requiring lower computational costs compared to previous methods. For instance, TriDet achieves an average mAP of 69.3% on THUMOS14, surpassing the previous best by 2.5% with only 74.6% of its latency. The code for TriDet is available at https://github.com/dingfengshi/TriDet.