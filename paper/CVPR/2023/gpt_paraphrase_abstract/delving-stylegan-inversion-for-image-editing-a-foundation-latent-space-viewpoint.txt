In this study, we propose a method for improving the inversion and editing of Generative Adversarial Network (GAN) through StyleGAN. GAN inversion involves mapping an input image to embedding spaces (W, W+, and F) in order to maintain image fidelity and enable meaningful manipulation. Previous methods have focused on exploring the extended latent space W+ and feature space F, rather than the foundation latent space W, to improve reconstruction fidelity while maintaining editability. However, since W+ and F are derived from W, we believe that focusing on W could lead to improvements in these methods. Therefore, we first obtain the proper latent code in the foundation latent space W by using contrastive learning to align W with the image space. We then utilize a cross-attention encoder to transform the obtained latent code in W into W+ and F. Our experiments demonstrate that our exploration of the foundation latent space W enhances the representation ability of latent codes in W+ and features in F, resulting in state-of-the-art reconstruction fidelity and editability results on standard benchmarks. The details of our method can be found on our project page: https://kumapowerliu.github.io/CLCAE.