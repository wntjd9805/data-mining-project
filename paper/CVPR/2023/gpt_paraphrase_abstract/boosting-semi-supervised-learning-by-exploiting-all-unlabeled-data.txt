Semi-supervised learning (SSL) has gained significant attention as it has the potential to reduce the reliance on large labeled datasets. Recent methods, such as FixMatch, have shown promising results by combining consistency regularization and pseudo-labeling. However, these methods suffer from discarding complex examples, as all pseudo-labels need to be selected using a high threshold to filter out noise. Consequently, examples with uncertain predictions are not utilized during training. To address this limitation, we propose two novel techniques: Entropy Meaning Loss (EML) and Adaptive Negative Learning (ANL). EML incorporates the prediction distribution of non-target classes into the optimization objective to avoid competition with the target class. This generates more high-confidence predictions for selecting pseudo-labels. ANL introduces an additional negative pseudo-label for all unlabeled data, enabling the utilization of low-confidence examples. The allocation of this label is dynamically evaluated based on the top-k performance of the model. Importantly, EML and ANL do not require any additional parameters or hyperparameters. We integrate these techniques with FixMatch to create a simple yet powerful framework called FullMatch. Extensive experiments on various SSL benchmarks (CIFAR-10/100, SVHN, STL-10, and ImageNet) demonstrate that FullMatch outperforms FixMatch by a significant margin. When combined with FlexMatch, an advanced FixMatch-based framework, we achieve state-of-the-art performance. The source code for FullMatch is available at https://github.com/megvii-research/FullMatch.