This paper focuses on weakly-supervised audio-visual event perception (WS-AVEP), which aims to identify and categorize events in both audio and visual modalities using only video-level annotations. While previous approaches have either ignored the unsynchronized nature of audio-visual tracks or disregarded the complementary modality for enhancing event perception, the authors argue that each modality should provide evidence of an event's presence or absence. To address this, they propose a unified framework called Cross-Modal Presence-Absence Evidence (CMPAE) that collects evidence using subjective logic theory. They also introduce a joint-modal mutual learning (JML) process to calibrate the evidence for different types of events dynamically. Experimental results demonstrate that their method outperforms existing approaches, achieving significant improvements in visual and audio metrics at the event level. The code for their approach is available on GitHub.