This paper aims to develop a semantic radiance field that is accurate, efficient, and generalizable by learning from multiple scenes. While most existing NeRFs focus on tasks such as neural scene rendering, image synthesis, and multi-view reconstruction, there have been limited attempts to incorporate high-level semantic understanding into the NeRF structure. One such attempt, Semantic-NeRF, learns both color and semantic labels from a single ray with multiple heads. However, this approach lacks rich semantic information due to the limitations of a single ray and requires training a specific model for each scene. To overcome these limitations, we propose SemanticRay (S-Ray), which leverages semantic information from the ray direction through multi-view reprojections. To avoid the computational cost of performing dense attention over multi-view reprojected rays, we introduce a Cross-Reprojection Attention module that utilizes intra-view radial and cross-view sparse attentions to decompose contextual information along reprojected rays and across multiple views. The modules are stacked to collect dense connections. Experimental results demonstrate that S-Ray can learn from multiple scenes and exhibits strong generalization ability for unseen scenes. More information about the project can be found at https://liuff19.github.io/S-Ray/.