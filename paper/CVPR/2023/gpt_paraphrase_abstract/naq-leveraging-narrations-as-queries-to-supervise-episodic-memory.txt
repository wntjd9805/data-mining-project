This abstract discusses the challenge of searching long egocentric videos using natural language queries (NLQ) and proposes a data augmentation strategy called Narrations-as-Queries (NaQ) to address this challenge. The goal of this research is to create a fluid index of everything a person has seen before in order to augment human memory and provide relevant information on demand, particularly in augmented reality and robotics applications.The structured nature of the learning problem, with free-form text query inputs and localized video temporal window outputs, makes it difficult and costly to supervise. Therefore, the NaQ strategy transforms standard video-text narrations into training data for a video query localization model.The effectiveness of NaQ is validated on the Ego4D benchmark, where it significantly improves the accuracy of multiple top models, even doubling their performance. It also achieves the best results to date in the Ego4DNLQ challenge, outperforming all winners in the CVPR and ECCV 2022 competitions and ranking first on the current public leaderboard.In addition to achieving state-of-the-art performance for NLQ, the authors demonstrate unique properties of their approach. They show that the NaQ strategy enables zero-shot and few-shot NLQ, meaning it can perform well even with limited training data. Furthermore, it improves the performance of queries related to long-tail object categories, which are less commonly seen.The code and models for this research can be found at http://vision.cs.utexas.edu/projects/naq.