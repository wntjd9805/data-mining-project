Monocular 3D human pose and shape estimation is a challenging problem as there can be multiple 3D solutions that can explain a 2D image. Recent methods address this by predicting a probability distribution of plausible 3D pose and shape parameters based on the image. However, these approaches have a trade-off between three important properties: accuracy, sample-input consistency, and sample diversity.In this study, we introduce a new method called HuManiFlow which aims to simultaneously achieve accurate, consistent, and diverse distributions. To achieve this, we leverage the human kinematic tree to decompose the full body pose into per-body-part pose distributions conditioned on their ancestors in an autoregressive manner. These per-body-part distributions are implemented using normalizing flows that respect the structure of the Lie group SO(3) for per-body-part poses.We demonstrate that the commonly used 3D point estimate losses, although widely used, reduce the diversity of samples. Instead, we only employ probabilistic training losses to preserve sample diversity. Our experimental results on the 3DPW and SSP-3D datasets show that HuManiFlow outperforms state-of-the-art probabilistic approaches.Overall, our method addresses the limitations of existing approaches by simultaneously achieving accuracy, sample-input consistency, and sample diversity in monocular 3D human pose and shape estimation.