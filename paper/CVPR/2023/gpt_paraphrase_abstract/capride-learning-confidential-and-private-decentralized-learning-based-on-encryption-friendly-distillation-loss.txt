This abstract discusses the challenge of training accurate deep neural networks (DNNs) when large volumes of data are not available to a single entity. Privacy concerns often prevent data sharing between entities, leading to the development of cross-silo federated learning (FL). However, most existing cross-silo FL algorithms have a trade-off between utility and privacy. To address this, the authors propose a framework called Confidential and PrivateDecentralized (CaPriDe) learning, which utilizes fully homomorphic encryption (FHE) to enable collaborative learning while maintaining data confidentiality and privacy. In CaPriDe learning, participants release their private data in an encrypted form, allowing other participants to perform inference in the encrypted domain. The key aspect of CaPriDe learning is mutual knowledge distillation between multiple local models through a novel distillation loss. This loss approximates the Kullback-Leibler (KL) divergence between local predictions and encrypted inferences of other participants on the same data, which can be computed in the encrypted domain. Experimental results demonstrate that CaPriDe learning improves the accuracy of local models without central coordination, ensures data confidentiality and privacy, and can handle statistical heterogeneity. However, there are limitations in terms of model architecture constraints, limited scalability, and computational complexity of encrypted domain inference. The code for CaPriDe learning can be found at a specific GitHub repository.