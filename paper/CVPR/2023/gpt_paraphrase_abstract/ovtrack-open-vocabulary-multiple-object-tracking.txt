Recognizing, localizing, and tracking dynamic objects in a scene is crucial for various real-world applications like self-driving cars and robots. However, conventional multiple object tracking (MOT) benchmarks focus on a limited number of object categories that do not adequately represent the diverse range of objects encountered in real-world scenarios. Consequently, current MOT methods are constrained to tracking only pre-defined object categories. To overcome this limitation, we propose a new task called open-vocabulary MOT, which aims to assess tracking performance beyond pre-defined training categories. To address this task, we introduce OVTrack, an open-vocabulary tracker capable of tracking arbitrary object classes. OVTrack leverages vision-language models for both classification and association using knowledge distillation. Additionally, we employ a data hallucination strategy to enhance appearance feature learning through denoising diffusion probabilistic models. As a result, OVTrack achieves state-of-the-art performance on the large-scale, large-vocabulary TAO benchmark while being trained solely on static images.