In recent years, there has been a growing interest in self-supervised monocular depth estimation methods that do not rely on ground truth data for training. The development of lightweight yet effective models that can be deployed on edge devices has become a significant area of focus. Existing architectures often utilize heavier backbones, leading to larger model sizes. However, this paper introduces a lightweight architecture called Lite-Mono that achieves comparable results. The study explores the efficient combination of Convolutional Neural Networks (CNNs) and Transformers, presenting two novel modules: Consecutive Dilated Convolutions (CDC) and Local-Global Features Interaction (LGFI). The CDC module extracts rich multi-scale local features, while the LGFI module utilizes self-attention to encode long-range global information into the features. Experimental results demonstrate that Lite-Mono significantly outperforms Monodepth2 in terms of accuracy, while requiring approximately 80% fewer trainable parameters. The source code and models for Lite-Mono are publicly available at https://github.com/noahzn/Lite-Mono.