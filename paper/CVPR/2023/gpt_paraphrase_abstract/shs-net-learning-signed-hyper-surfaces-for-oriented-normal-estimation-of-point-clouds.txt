We present a new approach called SHS-Net for estimating oriented normals of point clouds by learning signed hyper surfaces. Existing methods typically follow a two-stage pipeline, involving unoriented normal estimation and normal orientation, using separate algorithms for each step. However, these methods are sensitive to parameter settings, leading to subpar results when dealing with noisy, density variant, or complex point clouds. To address this, we introduce signed hyper surfaces (SHS) parameterized by multi-layer perceptron (MLP) layers, enabling us to estimate oriented normals in an end-to-end manner. The SHS are learned implicitly in a high-dimensional feature space that aggregates local and global information. We incorporate a patch encoding module and a shape encoding module to encode the 3D point cloud into local and global latent codes, respectively. A decoder module, called attention-weighted normal prediction module, takes the latent codes as input to predict oriented normals. Our experimental results demonstrate that SHS-Net outperforms state-of-the-art methods in both unoriented and oriented normal estimation on widely used benchmarks. The code, data, and pretrained models are available at the provided GitHub link. This work was supported by various funding sources. Figure 1 illustrates the proposed SHS-Net for direct estimation of oriented normals from point clouds. In contrast, previous studies typically utilize a two-stage approach using different algorithms for unoriented normal estimation and normal orientation.