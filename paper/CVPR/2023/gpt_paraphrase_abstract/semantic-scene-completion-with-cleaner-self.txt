Semantic Scene Completion (SSC) is a challenging task that involves converting 2D pixel images with depth information into 3D voxel representations with predicted semantic labels. This problem is considered ill-posed because the prediction model must imagine what lies behind the visible surface, typically represented by a Truncated Signed Distance Function (TSDF). However, existing methods that rely on TSDF estimated from depth values often produce incomplete volumetric predictions and confused semantic labels due to the imperfections in depth camera sensors.To address these limitations, we propose a novel approach called TSDF-CAD. We generate a ground-truth 3D voxel representation of a perfect visible surface and use it to train a "cleaner" SSC model. This noise-free model can focus more on the imagination of unseen voxels. We then distill the intermediate "cleaner" knowledge into another model that takes noisy TSDF input. This transfer of knowledge is achieved by using the 3D occupancy feature and the semantic relations of the "cleaner self" to supervise the predictions of the "noisy self" and correct the issues of incomplete volumetric predictions and confused semantic labels.Our experimental results demonstrate the effectiveness of our method. We observe improvements of 3.1% IoU and 2.2% mIoU in measuring scene completion and SSC, respectively, compared to the noisy counterparts. Additionally, our approach achieves state-of-the-art accuracy on the widely used NYU dataset. The code for our method is publicly available at https://github.com/fereenwong/CleanerS.