Audio-driven facial reenactment is an important technique used in various fields such as film-making, virtual avatars, and video conferences. Current methods either use explicit representations of the face (such as 2D facial landmarks or 3D face models) or implicit representations (such as Neural Radiance Fields). However, these methods face a trade-off between interpretability and expressive power, as well as controllability and quality of results. In this study, we propose a new parametric implicit face representation that overcomes these trade-offs and introduces a novel audio-driven facial reenactment framework capable of generating high-quality talking heads while maintaining control. Our parametric implicit representation combines the best aspects of explicit and implicit methods by using interpretable parameters from 3D face models. Additionally, we introduce several techniques to enhance different components of our framework, including incorporating contextual information into the encoding of audio-to-expression parameters, using conditional image synthesis for parameterizing the implicit representation, and formulating facial reenactment as a conditional image inpainting problem. We also propose a novel data augmentation technique to improve the generalizability of our model. Through extensive experiments, we demonstrate that our method produces more realistic results compared to previous methods, capturing the identities and talking styles of speakers with greater fidelity.