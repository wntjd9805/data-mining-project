This paper presents a method for creating animatable avatars from static scans by modeling clothing deformations in different poses. Existing learning-based methods have limitations in capturing details or hindering end-to-end learning. To address these limitations, the authors propose a point-based approach that decomposes explicit garment-related templates and adds pose-dependent wrinkles to them. This disentangles the clothing deformations, allowing for better learning and application of pose-dependent wrinkles to unseen poses. Additionally, the authors propose learning point features on a body surface to capture fine-grained and pose-dependent clothing geometry and address seam artifact issues. To support further research in this field, the authors introduce a high-quality scan dataset of humans in real-world clothing. The proposed approach is validated on existing datasets and the newly introduced dataset, demonstrating improved clothing deformation results in unseen poses. The project page with code and dataset can be accessed at https://www.liuyebin.com/closet.