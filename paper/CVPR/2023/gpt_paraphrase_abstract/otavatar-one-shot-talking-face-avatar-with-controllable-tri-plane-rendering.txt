The primary goals of creating face avatars using neural implicit field representations are controllability, generalizability, and efficiency. However, existing methods have failed to meet all three requirements simultaneously. Some methods focus on static portraits or limit the representation to specific subjects, while others have high computational costs, reducing flexibility. This paper presents a solution called One-shot Talking face Avatar (OTAvatar) that addresses these limitations. OTAvatar constructs personalized face avatars using a generalized controllable tri-plane rendering approach, requiring only one reference portrait. The method first converts a portrait image into a motion-free identity code. Then, the identity code and a motion code are used to modulate an efficient CNN, generating a tri-plane volume that encodes the desired motion for the subject. Volume rendering is applied to generate images from any viewpoint. The key aspect of the proposed solution is a decoupling-by-inverting strategy that separates identity and motion in the latent code through optimization-based inversion. By utilizing the efficient tri-plane representation, OTAvatar achieves controllable rendering of generalized face avatars at a speed of 35 FPS on A100. Experimental results demonstrate promising performance in cross-identity reenactment on subjects not included in the training set and improved 3D consistency. The code for OTAvatar is publicly available at https://github.com/theEricMa/OTAvatar.