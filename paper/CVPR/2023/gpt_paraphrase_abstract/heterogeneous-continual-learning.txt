We present a novel framework and solution for addressing the continual learning problem when dealing with changing network architectures. While most existing methods focus on adapting a single architecture to new tasks by modifying weights, we recognize the need to adapt existing solutions to novel architectures due to rapid progress in architecture design. To overcome this limitation, we propose Heterogeneous Continual Learning (HCL), which involves the continual emergence of a wide range of evolving network architectures alongside new data and tasks. Our solution involves building upon the distillation family of techniques, modifying it to a new setting where a weaker model acts as a teacher and a stronger architecture serves as a student. Additionally, we consider a scenario with limited access to previous data and introduce Quick Deep Inversion (QDI) as a means to recover prior task visual features and facilitate knowledge transfer. QDI offers significant computational cost reduction compared to previous methods and improves overall performance. In summary, we propose a new approach for continual learning that incorporates a modified knowledge distillation paradigm and a quick data inversion method to enhance distillation. Our evaluation on various benchmarks demonstrates a notable increase in accuracy compared to state-of-the-art methods across different network architectures.