Generating a coherent video sequence from a few initial static frames is a difficult task. In addition to predicting future frames, the ability to rewind or fill in missing frames is also important but has been largely unexplored in video completion. Given the potential for different outcomes based on limited information, we propose a system that can generate videos based on natural language instructions, introducing a new task called text-guided video completion (TVC). To address this task, we propose Multi-modal Masked Video Generation (MMVG), which discretizes video frames into visual tokens and masks some of them during training to complete videos from any time point. At inference, a single MMVG model can handle all three cases of TVC - prediction, rewind, and infilling - by applying appropriate masking conditions. We evaluate MMVG in various video scenarios and find that it is effective in generating high-quality videos with text guidance for TVC.