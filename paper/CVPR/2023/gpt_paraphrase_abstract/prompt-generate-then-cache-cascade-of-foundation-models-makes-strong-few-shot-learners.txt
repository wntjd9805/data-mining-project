To improve visual recognition in low-data scenarios, deep neural networks need to learn generalized representations from limited training samples. Recent advancements in CLIP-based methods have shown promising few-shot performance by leveraging contrastive language-image pre-training. However, there is a question of whether incorporating more diverse pre-training knowledge can further enhance few-shot representation learning. In this study, we propose CaFo, a Cascade of Foundation models that integrates various pre-training paradigms to facilitate better few-shot learning. Our CaFo knowledge incorporates the vision-contrastive knowledge of CLIP, the vision-generative knowledge of DALL-E, and the language-generative knowledge of GPT-3. The CaFo framework operates in a 'Prompt, Generate, then Cache' manner. Firstly, we utilize GPT-3 to generate textual prompts that provide rich downstream linguistic semantics for CLIP. Next, we use DALL-E to generate synthetic images, thereby expanding the few-shot training data without requiring manual effort. Finally, we introduce a learnable cache model to intelligently blend the predictions from CLIP and DINO. By collaborating in this way, CaFo maximizes the potential of different pre-training methods and achieves state-of-the-art performance in few-shot classification. The code for CaFo is available at https://github.com/ZrrSkywalker/CaFo.