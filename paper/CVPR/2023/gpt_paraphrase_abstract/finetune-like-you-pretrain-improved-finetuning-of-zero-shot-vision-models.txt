Finetuning image-text models, such as CLIP, has achieved impressive results on various benchmarks. However, recent research has demonstrated that even minor differences in the finetuning process can significantly impact the final performance, both for data within the distribution and data outside the distribution. In this study, we propose a straightforward approach of mimicking contrastive pretraining, which consistently outperforms alternative finetuning methods. We accomplish this by treating downstream class labels as text prompts and optimizing the contrastive loss between image embeddings and prompt embeddings (contrastive finetuning). Our approach surpasses baseline methods across multiple benchmarks, including distribution shift, transfer learning, and few-shot learning tasks. Notably, on the WILDS-iWILDCam benchmark, our proposed approach, FLYP, achieves a 2.3% improvement in accuracy for in-distribution data and a 2.7% improvement for out-of-distribution data, outperforming the current leaderboard. On seven out-of-distribution datasets, FLYP outperforms standard finetuning by 4.2% and the current state-of-the-art (LP-FT) by over 1% for both in-distribution and out-of-distribution data. Additionally, on three few-shot learning benchmarks, FLYP achieves gains of up to 4.6% over standard finetuning and 4.4% over the state-of-the-art. Overall, our proposed contrastive finetuning method establishes itself as a simple and intuitive state-of-the-art approach for supervised finetuning of image-text models like CLIP. The code for our method is available at https://github.com/locuslab/FLYP.