Iterative learning methods have been introduced to enhance generalization, often requiring longer training periods. LLF is a leading method that strengthens learning in early layers by periodically re-initializing the last few layers. This study proposes a novel approach called SEAL, which utilizes Simulated annealing in EArly Layers instead of later layer re-initialization. While later layers follow the usual gradient descent process, early layers experience short periods of gradient ascent followed by gradient descent. Extensive experiments on the Tiny-ImageNet dataset and various transfer learning and few-shot learning tasks demonstrate that SEAL outperforms LLF significantly. Additionally, it is found that LLF features, while improving the target task, negatively impact transfer learning performance across all explored datasets. In contrast, SEAL surpasses LLF by a large margin on the same target datasets. Furthermore, the prediction depth of SEAL is considerably lower than that of LLF and normal training, indicating superior prediction performance on average.