We propose a method for inserting people into scenes while considering the scene's affordances. Our model is able to infer realistic poses based on the scene context and re-pose the reference person accordingly. We achieve this by training a large-scale diffusion model on a dataset of 2.4M video clips, which allows us to generate diverse and plausible poses while respecting the scene context. Our model also enables the synthesis of realistic people and scenes without conditioning, as well as interactive editing. Our method outperforms prior work in terms of synthesizing realistic human appearance and natural human-scene interactions, as shown in quantitative evaluations.