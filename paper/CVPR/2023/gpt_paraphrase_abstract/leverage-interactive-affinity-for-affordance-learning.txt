Learning the potential actions that can be performed with objects in images and understanding the interactive functionalities of objects through human demonstrations is a difficult task due to the wide range of human-object interactions. Existing algorithms for learning these affordances often rely on assigning labels to specific regions and assume a singular relationship between functional regions and affordance labels. However, these algorithms perform poorly when faced with new environments that have significant appearance variations.In this study, we propose a new approach to affordance learning by leveraging interactive affinity. Interactive affinity is extracted from human-object interactions and transferred to non-interactive objects. It represents the connections between different parts of the human body and specific regions of the target object, providing valuable information about the interconnectivity between humans and objects. By incorporating interactive affinity, we aim to reduce the ambiguity in perceiving action possibilities.To implement this approach, we introduce a pose-aided interactive affinity learning framework that uses human pose as a guide for the network to learn interactive affinity from human-object interactions. We also develop a keypoint heuristic perception scheme to address uncertainties caused by interaction diversities and contact occlusions. Additionally, we create a contact-driven affordance learning dataset consisting of over 5,000 labeled images.Our experimental results demonstrate that our method outperforms existing models in terms of objective metrics and visual quality. We provide the code and dataset for our approach on GitHub (github.com/lhc1224/PIAL-Net).