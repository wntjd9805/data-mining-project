We present a new approach for recognizing human actions using motion capture (MoCap) sequences. Instead of using manual steps to derive standardized skeleton representations, we introduce a Spatial-Temporal Mesh Transformer (STMT) that directly models the mesh sequences. Our model utilizes a hierarchical transformer with intra-frame offset attention and inter-frame self-attention. This attention mechanism allows the model to focus on non-local relationships in the spatial-temporal domain by attending between any two vertex patches. To fully activate the bi-directional and auto-regressive attention in our hierarchical transformer, we employ masked vertex modeling and future frame prediction as self-supervised tasks. Our proposed method achieves superior performance compared to skeleton-based and point-cloud-based models on popular MoCap benchmarks. The code for our approach is available at https://github.com/zgzxy001/STMT.