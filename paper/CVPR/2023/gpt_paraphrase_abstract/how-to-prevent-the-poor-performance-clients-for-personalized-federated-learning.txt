Personalized federated learning (pFL) is a method that trains personalized models based on heterogeneous distributed local data. While previous studies have focused on improving performance through averaging or top perspective, they have not adequately addressed the issue of poor-performing clients. These clients may have biased universal information shared with others. To tackle this problem, we propose a new pFL strategy called Personalize Locally, Generalize Universally (PLGU). PLGU addresses biased performance by generalizing fine-grained universal information and implementing a Layer-Wised Sharpness-Aware Minimization (LWSAM) algorithm while maintaining local personalization. We integrate PLGU into two pFL schemes: one with a global model and one without. We provide detailed training procedures for both schemes and demonstrate through extensive experimentation that the proposed PLGU-based algorithms achieve state-of-the-art performance and competitive generalization bounds.