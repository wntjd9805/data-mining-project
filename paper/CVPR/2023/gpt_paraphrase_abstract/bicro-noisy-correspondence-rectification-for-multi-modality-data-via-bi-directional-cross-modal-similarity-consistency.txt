Cross-modal matching is a fundamental technique in multi-modal learning that aims to align different sensory modalities in a shared feature space. However, collecting and annotating multi-modal datasets accurately is challenging, leading researchers to use co-occurred data pairs from the internet. Unfortunately, these datasets often contain mismatched pairs that can harm model performance. To address this issue, we propose a framework called BiCro (Bidirectional Cross-modal similarity consistency) that can be easily integrated into existing cross-modal matching models to improve their robustness against noisy data. BiCro estimates soft labels for noisy data pairs to reflect their true correspondence degree. The basic idea behind BiCro is that similar images should have similar textual descriptions and vice versa, and the consistency of these similarities can be used as soft labels to train the matching model. Experimental results on three popular cross-modal matching datasets show that our method significantly improves the noise-robustness of various matching models, surpassing the state-of-the-art by a clear margin. The code for BiCro is available at https://github.com/xu5zhao/BiCro.