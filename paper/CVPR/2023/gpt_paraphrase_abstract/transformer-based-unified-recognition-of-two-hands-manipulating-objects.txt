Recently, there has been significant interest in understanding hand-object interactions in egocentric videos. Most existing approaches utilize convolutional neural network (CNN) features combined with long short-term memory (LSTM) or graph convolution network (GCN) to achieve a comprehensive understanding of two hands, an object, and their interactions. In this study, we introduce a novel unified framework based on Transformers, which enhances the understanding of two hands manipulating objects. Our framework takes the entire image containing two hands, an object, and their interactions as input and simultaneously estimates three key information from each frame: the poses of the two hands, the pose of the object, and the object type. Subsequently, the action class, determined by the hand-object interactions, is predicted for the entire video using the estimated information, along with a contact map that encodes the interaction between the hands and the object. We conduct experiments on benchmark datasets (H2O and FPHA), and our method achieves state-of-the-art accuracy, demonstrating its superiority. Ablative studies further validate the effectiveness of each proposed module.