Vision Transformers (ViTs) have achieved impressive performance in computer vision tasks. However, their high computational cost remains a challenge compared to convolutional neural networks (CNNs). This is mainly due to ViTs' attention mechanism, which measures global similarities and has a quadratic complexity with the number of input tokens. Existing efficient ViTs sacrifice either global or local context by adopting local or linear attention. In this study, we aim to address this limitation and propose a framework called Castling-ViT. The Castling-ViT framework combines linear-angular attention and masked softmax-based quadratic attention during training, but switches to using only linear-angular attention during inference to improve efficiency. We leverage angular kernels to measure similarities between queries and keys using spectral angles. To simplify the framework, we decompose angular kernels into linear terms and high-order residuals, retaining only the linear terms. We also adopt two parameterized modules, a depthwise convolution and an auxiliary masked softmax attention, to approximate high-order residuals and capture global and local information. The masks for softmax attention are gradually regularized to become zeros, resulting in no overhead during inference. Extensive experiments demonstrate the effectiveness of Castling-ViT, achieving higher accuracy and reduced MACs (multiply-accumulate operations) in classification tasks, as well as higher mAP (mean Average Precision) in object detection tasks compared to ViTs with vanilla softmax-based attentions. The project page is available for further information.