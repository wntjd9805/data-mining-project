The potential of vision transformers (ViT) in various vision tasks has been observed, while the U-Net, based on a convolutional neural network (CNN), remains the dominant model for diffusion tasks. To address this, we introduce a simple and versatile ViT-based architecture called U-ViT for image generation using diffusion models. U-ViT treats all inputs, including time, condition, and noisy image patches, as tokens and incorporates long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks. U-ViT performs comparably, if not better, than a CNN-based U-Net of similar size. Notably, U-ViT achieves record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256 and 5.48 in text-to-image generation on MS-COCO, without relying on large external datasets during the training of generative models. Our findings suggest that for diffusion-based image modeling, long skip connections are crucial, while the downsampling and upsampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide valuable insights for future research on backbones in diffusion models and enhance generative modeling on large-scale cross-modality datasets.