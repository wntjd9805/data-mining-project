This study demonstrates the feasibility of learning models for monocular 3D reconstruction of articulated objects, such as horses, cows, and sheep, using a small number of labeled images with 2D keypoints. The proposed approach involves training category-specific keypoint estimators, generating 2D keypoint pseudo-labels on unlabeled web images, and utilizing both labeled and self-labeled sets to train 3D reconstruction models. The approach is based on two important observations: (1) 2D keypoint estimation networks trained on a limited number of images of a specific object category have good generalization capabilities and can produce reliable pseudo-labels, and (2) a data selection mechanism can automatically curate a subset of unlabeled web images suitable for training. The combination of these insights enables the training of models that effectively utilize web images, leading to improved 3D reconstruction performance for various articulated object categories compared to the fully-supervised baseline. The proposed approach can quickly initiate model training and only requires a small number of images labeled with 2D keypoints, which can be easily obtained for any new object category. To demonstrate the practicality of the approach, the authors annotated 2D keypoints on 250 giraffe and bear images from COCO in just 2.5 hours per category.