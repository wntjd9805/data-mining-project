This paper addresses the challenge of generalizable 3D part segmentation in vision and robotics. Traditional supervised methods for training deep models in this task require large-scale 3D datasets with detailed part annotations, which are expensive to obtain. To overcome this limitation, the paper proposes a novel approach for low-shot part segmentation of 3D point clouds. This approach leverages a pretrained image-language model called GLIP, which has demonstrated superior performance in 2D detection tasks with open-vocabulary. The authors transfer the knowledge learned by GLIP from 2D to 3D by utilizing GLIP-based part detection on point cloud rendering and a new 2D-to-3D label lifting algorithm. Additionally, they incorporate multi-view 3D priors and few-shot prompt tuning techniques to further enhance performance. The proposed method is extensively evaluated on the PartNet and PartNet-Mobility datasets, showcasing its ability to achieve excellent zero-shot 3D part segmentation. Furthermore, the few-shot version of the method outperforms existing approaches by a large margin and achieves competitive results compared to fully supervised methods. The authors also demonstrate the applicability of their method to iPhone-scanned point clouds without significant domain gaps.