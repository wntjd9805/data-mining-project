In recent years, there has been significant progress in large-scale vision transformers (ViTs), while large-scale models based on convolutional neural networks (CNNs) are still in their early stages. This research introduces a new large-scale CNN-based foundation model called InternImage. InternImage is designed to benefit from increased parameters and training data, similar to ViTs. Unlike recent CNNs that focus on large dense kernels, InternImage utilizes deformable convolution as its core operator. This approach enables the model to have both a large effective receptive field necessary for downstream tasks like detection and segmentation, as well as adaptive spatial aggregation that is conditioned by input and task information. Consequently, InternImage reduces the strict biases commonly found in traditional CNNs, allowing it to learn stronger and more robust patterns using large-scale parameters and massive amounts of data, similar to ViTs. The effectiveness of InternImage is demonstrated on challenging benchmarks such as ImageNet, COCO, and ADE20K. Notably, InternImage-H achieves a new record with a 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, surpassing current leading CNNs and ViTs.