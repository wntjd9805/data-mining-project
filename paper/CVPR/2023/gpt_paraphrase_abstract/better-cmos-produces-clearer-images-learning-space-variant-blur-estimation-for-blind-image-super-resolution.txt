Existing blind image Super-Resolution (SR) methods assume that blur kernels are space-invariant, but in real applications, blur is often space-variant due to factors like object motion and out-of-focus. This significantly affects the performance of advanced SR methods. To address this issue, we introduce two new datasets, NYUv2-BSR and Cityscapes-BSR, which contain images with out-of-focus blur. These datasets can be used for further research on blind SR with space-variant blur. Using these datasets, we propose a novel Cross-MOdal fuSion network (CMOS) that simultaneously estimates blur and semantics, resulting in improved SR results. Our method includes a feature Grouping Interactive Attention (GIA) module to enhance the interaction between the two modalities and avoid inconsistency. The structure of GIA is universal and can be applied to the interaction of other features as well. Through qualitative and quantitative experiments comparing our method with state-of-the-art approaches on the mentioned datasets and real-world images, we demonstrate the superiority of our method. For example, our method achieves a higher PSNR/SSIM (+1.91↑/+0.0048↑) on NYUv2-BSR compared to MANet1.