Class incremental semantic segmentation (CISS) aims to address the problem of catastrophic forgetting and enhance discrimination. Previous approaches have primarily relied on regularization techniques like knowledge distillation to preserve previous knowledge in the current model. However, distillation alone often leads to limited improvements as it only ensures consistency between the representations of old and new models.This paper introduces a straightforward yet efficient method called Endpoints Weight Fusion (EWF) to create a model with a robust memory of old knowledge. EWF dynamically fuses the model containing old knowledge with the model retaining new knowledge, thereby reinforcing the memory of old classes in evolving distributions. Additionally, we analyze the relationship between our fusion strategy and the popular moving average technique EMA, illustrating why our method is better suited for class-incremental learning. To facilitate parameter fusion with a closer distance in the parameter space, we employ distillation to enhance the optimization process.Furthermore, extensive experiments are conducted on two widely used datasets, resulting in state-of-the-art performance.