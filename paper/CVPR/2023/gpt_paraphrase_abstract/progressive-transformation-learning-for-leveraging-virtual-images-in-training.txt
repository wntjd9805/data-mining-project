In order to effectively analyze images taken by unmanned aerial vehicles (UAVs) and detect specific objects, such as humans, it is necessary to have access to large-scale datasets that contain diverse instances of humans in different poses and viewing angles. Instead of manually curating such datasets, we propose a method called Progressive Transformation Learning (PTL) that gradually enhances a training dataset by adding virtual images that are transformed to have a more realistic appearance. Typically, when there is a significant difference between real and virtual images, the quality of a virtual-to-real transformation generator in a conditional Generative Adversarial Network (GAN) framework is compromised. To address this domain gap issue, PTL takes a unique approach by iteratively performing three steps: 1) selecting a subset of virtual images based on the domain gap, 2) transforming the selected virtual images to improve their realism, and 3) incorporating the transformed virtual images into the training set while removing them from the pool. Accurately measuring the domain gap is crucial in PTL, and we demonstrate through theoretical analysis that the feature representation space of an object detector can be modeled as a multivariate Gaussian distribution. This allows us to compute the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space. Experimental results confirm that PTL significantly improves performance compared to the baseline, especially when dealing with limited data and cross-domain scenarios.