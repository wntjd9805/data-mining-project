Efficient neural networks for mobile devices are typically optimized based on metrics like FLOPs or parameter count. However, these metrics may not accurately reflect the network's latency when deployed on a mobile device. Therefore, we conducted a thorough analysis of various metrics by deploying several mobile-friendly networks on a mobile device. Through this analysis, we identified and examined architectural and optimization bottlenecks in recent efficient neural networks, and proposed strategies to address these bottlenecks.As part of our research, we developed an efficient backbone called MobileOne. We created different variants of MobileOne that achieved an inference time of less than 1 ms on an iPhone 12, while still maintaining a top-1 accuracy of 75.9% on the ImageNet dataset. Our experiments demonstrated that MobileOne outperformed other efficient architectures in terms of both speed and accuracy on mobile devices. In fact, our best model was 38 times faster than MobileFormer while achieving similar performance on ImageNet. Additionally, our model achieved a 2.3% higher top-1 accuracy on ImageNet compared to EfficientNet, all at similar latency.Furthermore, we tested the generalizability of our model across multiple tasks, including image classification, object detection, and semantic segmentation. We observed significant improvements in both latency and accuracy compared to existing efficient architectures when our model was deployed on a mobile device.For those interested in replicating our work, we have made the code and models available on our GitHub repository at https://github.com/apple/ml-mobileone.