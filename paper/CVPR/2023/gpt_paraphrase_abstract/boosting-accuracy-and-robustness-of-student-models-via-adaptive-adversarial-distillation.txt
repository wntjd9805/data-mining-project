Distilled student models in teacher-student architectures are commonly used for efficient deployment in real-time applications and edge devices. However, these student models are more vulnerable to adversarial attacks at the edge. Existing methods like adversarial training have limited effectiveness on compressed networks. To address this, recent studies have focused on adversarial distillation (AD) which aims to inherit both prediction accuracy and adversarial robustness from a robust teacher model through robust optimization. However, existing AD methods often overcorrect towards model smoothness due to the use of fixed supervision information from the teacher model in the min-max framework of AD. In this paper, we propose an adaptive adversarial distillation (AdaAD) approach that involves the teacher model in the knowledge optimization process, allowing it to interact with the student model and adaptively search for better results. Compared to state-of-the-art methods, AdaAD significantly improves both the prediction accuracy and adversarial robustness of student models in various scenarios. Specifically, our AdaAD-trained ResNet-18 model achieves top-ranking performance (54.23% robust accuracy) on RobustBench under AutoAttack.