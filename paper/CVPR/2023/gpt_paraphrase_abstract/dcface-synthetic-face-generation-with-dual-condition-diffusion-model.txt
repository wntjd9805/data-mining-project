Creating synthetic datasets for training face recognition models is challenging because it involves generating multiple images of the same subjects under various factors such as pose, illumination, expression, aging, and occlusion. Previous studies have explored the use of GAN or 3D models for synthetic dataset generation. In this research, we tackle this problem by combining subject appearance (ID) and external factors (style) conditions. These two conditions allow us to control both inter-class and intra-class variations. To achieve this, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our innovative Patch-wise style extractor and Time-step dependent ID loss enable DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition models trained on synthetic images generated by DCFace exhibit higher verification accuracies compared to previous methods, achieving an average improvement of 6.11% on four out of five test datasets: LFW, CFP-FP, CPLFW, AgeDB, and CALFW. To access the code, please refer to the provided code link.