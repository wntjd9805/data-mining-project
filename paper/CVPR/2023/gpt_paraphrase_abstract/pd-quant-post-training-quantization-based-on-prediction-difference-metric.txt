Post-training quantization (PTQ) is a method of compressing neural networks by converting them from full-precision models to quantized models using lower-precision data types. While PTQ can reduce the size and computational cost of deep neural networks, it can also introduce quantization noise and decrease prediction accuracy, particularly in extremely low-bit scenarios. The main challenge currently is determining the appropriate quantization parameters, such as scaling factors and weight rounding. Existing approaches aim to minimize the difference between features before and after quantization to determine these parameters. However, this approach only considers local information and may not yield the most optimal quantization parameters. To address this limitation, we propose PD-Quant, a method that takes into account global information. PD-Quant determines quantization parameters by utilizing the differences between network predictions before and after quantization. Furthermore, PD-Quant mitigates the overfitting problem in PTQ caused by the limited number of calibration sets by adjusting the activation distribution. Experimental results demonstrate that PD-Quant yields better quantization parameters and improves prediction accuracy in quantized models, particularly in low-bit scenarios. For instance, PD-Quant achieves an accuracy of 53.14% for ResNet-18 and 40.67% for RegNetX-600MF in a 2-bit weight and 2-bit activation setting. The code for PD-Quant is available at https://github.com/hustvl/PD-Quant.