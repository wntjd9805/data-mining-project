Data-free Knowledge Distillation (DFKD) has become popular for transferring knowledge from a Teacher neural network to a Student neural network without using training data. However, in the Adversarial DFKD framework, the accuracy of the student network is negatively affected by the non-stationary distribution of pseudo-samples during generator updates. To address this, we propose a meta-learning framework called Learning to Retain while Acquiring. In this framework, we treat the tasks of Knowledge-Acquisition (learning from newly generated samples) and Knowledge-Retention (retaining knowledge on previously encountered samples) as meta-train and meta-test, respectively. This approach ensures that the student network maintains performance on previously seen examples while acquiring knowledge from current samples. We also discover that there is an implicit alignment between the Knowledge-Retention and Knowledge-Acquisition tasks, indicating that the proposed student update strategy enforces a common gradient direction for both tasks. This helps alleviate interference between the two objectives. We validate our hypothesis through extensive evaluation and comparison with prior methods on multiple datasets.