Training Generative Adversarial Networks (GANs) is notoriously challenging, particularly for complex distributions and limited data. As a result, there is a need for tools that can audit trained networks in a way that is understandable to humans, such as identifying biases or ensuring fairness. However, existing GAN audit tools only allow for coarse-grained comparisons based on summary statistics like FID or recall.In this paper, we propose a different approach called Cross-GAN Auditing (xGA). Instead of comparing a GAN to data or models, xGA compares a newly developed GAN to a previously established "reference" GAN. By doing so, xGA can identify intelligible attributes that are common to both GANs, unique to the new GAN, or missing from the new GAN. This provides users and developers with an intuitive assessment of the similarities and differences between GANs.To evaluate attribute-based GAN auditing approaches like xGA, we introduce novel metrics. Our quantitative analysis demonstrates that xGA outperforms baseline methods. Additionally, we present qualitative results that illustrate the common, novel, and missing attributes identified by xGA in GANs trained on various image datasets.Overall, our approach of comparing GANs using attribute-based auditing provides a more comprehensive and informative assessment of the models, surpassing previous techniques.