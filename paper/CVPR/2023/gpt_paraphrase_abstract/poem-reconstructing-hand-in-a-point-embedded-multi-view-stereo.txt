Enabling neural networks to effectively capture 3D geometric-aware features is crucial in tasks related to vision based on multiple views. Previous methods have typically encoded 3D information by converting it into 2D features through multi-view stereo. In this study, we propose a new approach called POEM, which directly operates on the 3D points embedded in the multi-view stereo to reconstruct hand meshes. Points naturally represent 3D information and are ideal for combining features from different views, as they have distinct projections on each view. Our method is based on a simple yet effective idea that a complex 3D hand mesh can be represented by a set of 3D points that are embedded in the multi-view stereo, carry features from the multi-view images, and surround the hand. To leverage the potential of points, we introduce two operations: point-based feature fusion and a cross-set point attention mechanism. Evaluation on three challenging multi-view datasets demonstrates that POEM outperforms state-of-the-art methods in hand mesh reconstruction. The code and models are available for research purposes at github.com/lixiny/POEM.