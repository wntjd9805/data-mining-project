Diffusion models have shown great potential in text-conditioned image synthesis. There is a growing interest in personalizing these models to generate specialized target objects or styles. This paper aims to learn an unseen style by fine-tuning a pre-trained diffusion model with a small number of images (less than 10). The goal is to enable the fine-tuned model to generate high-quality images of arbitrary objects in this specific style. The authors propose a novel toolkit of fine-tuning techniques, including customized data augmentations for text-to-image conversion, a content loss to separate content and style, and sparse updating focusing on a few time steps. This framework, called Specialist Diffusion, is compatible with existing diffusion model backbones and other personalization techniques. Experimental results demonstrate that Specialist Diffusion outperforms recent few-shot personalization methods such as Textual Inversion and DreamBooth in terms of learning sophisticated styles with minimal training data. Furthermore, it is shown that Specialist Diffusion can be integrated with textual inversion to further enhance performance, even for highly unusual styles. The source code for Specialist Diffusion is available at the provided GitHub repository.