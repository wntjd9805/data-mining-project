We present the OBJECTFOLDER BENCHMARK, a collection of 10 tasks that focus on multisensory learning related to object recognition, reconstruction, and manipulation using sight, sound, and touch. Additionally, we introduce the OBJECTFOLDER REAL dataset, which encompasses multisensory measurements for 100 real-world household objects. This dataset is obtained through a novel pipeline that collects 3D meshes, videos, impact sounds, and tactile readings of these objects. Our assessment involves systematic benchmarking of the 1,000 multisensory neural objects from OBJECTFOLDER, as well as the real multisensory data from OBJECTFOLDER REAL. The results highlight the significance of multisensory perception and shed light on the roles of vision, audio, and touch in various object-centric learning tasks. By making our dataset and benchmark suite publicly available, we aim to encourage and facilitate further research in multisensory object-centric learning across fields such as computer vision and robotics. For more information, please visit our project page: https://objectfolder.stanford.edu.