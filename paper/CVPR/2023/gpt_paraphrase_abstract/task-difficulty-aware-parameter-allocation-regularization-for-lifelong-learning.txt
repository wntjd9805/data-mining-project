Parameter regularization and allocation methods are effective in addressing the issue of catastrophic forgetting in lifelong learning. However, these methods treat all tasks in a sequence uniformly and fail to consider the varying difficulty levels of different tasks. As a result, parameter regularization methods often struggle when learning a new task that is very different from the previously learned tasks, while parameter allocation methods waste parameters when learning simple tasks. To address these limitations, this paper introduces a novel approach called ParameterAllocation & Regularization (PAR) that dynamically selects the most suitable strategy for each task based on its learning difficulty. The proposed method utilizes a divergence estimation technique, specifically the Nearest-Prototype distance, to measure the relatedness between tasks using only the features of the new task. Additionally, this paper presents a time-efficient architecture search strategy that considers task-relatedness to reduce the parameter overhead in allocation. Experimental results on multiple benchmarks demonstrate that our method outperforms state-of-the-art approaches, showing scalability and significant reduction in model redundancy while improving performance. Furthermore, qualitative analysis confirms that PAR achieves reasonable task-relatedness.