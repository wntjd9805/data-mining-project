Previous research has shown that deep neural networks (DNNs) are capable of memorizing label noise. However, we have observed that the strength of memorization varies for each instance, and this can be measured by the confidence value, which increases during training. Based on this observation, we propose a method called Dynamic Instance-specific Selection and Correction (DISC) for learning from noisy labels (LNL). Our approach involves using a two-view-based backbone for image classification to obtain confidence values for each image. We then introduce a dynamic threshold strategy that uses the momentum of each instance's memorization strength in previous epochs to select and correct noisy labeled data. By combining the dynamic threshold strategy with two-view learning, we are able to categorize each instance into one of three subsets (clean, hard, purified) based on the prediction consistency and discrepancy between the two views at each epoch. Finally, we apply different regularization strategies to improve the robustness of the network for subsets with different degrees of label noise. We conducted comprehensive evaluations on three controlled and four real-world LNL benchmarks, and our method outperformed the current state-of-the-art methods in leveraging useful information from noisy data while reducing the impact of label noise. The code for our method is available at https://github.com/JackYFL/DISC.