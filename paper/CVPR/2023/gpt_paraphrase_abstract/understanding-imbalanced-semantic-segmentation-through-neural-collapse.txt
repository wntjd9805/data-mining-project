A recent study has discovered a phenomenon known as neural collapse, where the means of features within a class and the weight vectors of classifiers converge to specific points in a geometric structure called a simplex equiangular tight frame during the final phase of classification training. In this paper, we investigate how this phenomenon applies to semantic segmentation, specifically examining the structures of the last-layer feature centers and classifiers. Through empirical and theoretical analysis, we find that semantic segmentation introduces contextual correlation and imbalanced distribution among classes, disrupting the equiangular and maximally separated structure observed in neural collapse. However, this symmetric structure proves advantageous for discriminating against minor classes. To preserve these benefits, we propose a regularizer that encourages the network to learn features closer to the desirable structure in imbalanced semantic segmentation. Experimental results demonstrate significant improvements in both 2D and 3D semantic segmentation benchmarks using our method. Additionally, our approach achieves first place and sets a new record (+6.8% mIoU) on the ScanNet200 test leaderboard.