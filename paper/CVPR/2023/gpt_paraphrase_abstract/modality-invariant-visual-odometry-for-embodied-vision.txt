Localizing an agent in a noisy environment is crucial for many tasks involving vision. Visual Odometry (VO) is a practical alternative to unreliable GPS and compass sensors, particularly in indoor settings. While SLAM-based methods perform well with minimal data requirements, they are less adaptable and robust when it comes to noise and changes in sensor equipment compared to learning-based approaches. Recent deep VO models, however, have limitations as they are trained on a fixed set of input modalities, such as RGB and depth, using millions of samples. These models fail catastrophically when sensors malfunction, sensor suites change, or modalities are intentionally excluded due to resource constraints, like power consumption. Additionally, training these models from scratch is even more costly without access to simulators or existing models for fine-tuning. These scenarios, although often overlooked in simulations, significantly hinder the reusability of models in real-world applications. To address this, we propose a Transformer-based VO approach that is invariant to the modality of input sensors and can handle diverse or changing sensor suites for navigation agents. Our model surpasses previous methods while being trained on only a fraction of the data. We believe this approach will expand the range of real-world applications that can benefit from flexible and learned VO models.