We propose a novel framework for estimating the motion between consecutive frames of point clouds in 3D scenes. Existing methods use superpoints, which are points with similar geometric features, to capture local motion. However, current superpoint generation methods are not effective for complex scenes, leading to inaccurate motion estimation. To address this, our framework includes an iterative end-to-end approach that dynamically updates the superpoints to guide the flow prediction at the point level. Our framework consists of a flow-guided superpoint generation module and a superpoint-guided flow refinement module. In the superpoint generation module, we use bidirectional flow information to associate points with superpoint centers, generating superpoints for pairwise point clouds. With these superpoints, we reconstruct the flow for each point by aggregating the superpoint-level flow and encode the consistency between the flow of pairwise point clouds. Finally, we refine the point-level flow using a GRU network with the consistency encoding and the reconstructed flow. Our method achieves promising performance according to extensive experiments on various datasets. The code for our method is available at the provided GitHub link.