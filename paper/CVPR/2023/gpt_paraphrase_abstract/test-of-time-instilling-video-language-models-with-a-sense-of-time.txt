Modelling and understanding time in video analysis is a challenging task for current models. Language has emerged as a crucial factor for effective generalization, making it essential for video-language models to incorporate temporal understanding. This study focuses on the consistency of time order, specifically the before/after relations. We demonstrate that seven existing video-language models struggle to comprehend even basic temporal relationships. The question arises as to whether it is possible to enhance these foundational models with temporal awareness without retraining them entirely. To address this, we propose a temporal adaptation approach using VideoCLIP as a base model, which involves post-pretraining on a small amount of video-text data. We evaluate the adapted models on six datasets for three different downstream tasks that require varying levels of time understanding. Encouraging performance improvements are observed, particularly for tasks that demand higher time awareness. This research serves as an initial step in investigating and instilling temporal understanding in existing video-language models without the necessity of extensive training from scratch.