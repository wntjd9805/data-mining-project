Vision-language pre-training has gained attention for its flexibility and transferability in various visual tasks. However, VL-driven segmentation has not been explored extensively, and existing approaches require additional training images or segmentation annotations to adapt a VL model for downstream segmentation tasks. This paper introduces a novel image-free segmentation task where semantic segmentation is performed with only a set of target semantic categories, without task-specific images or annotations. The proposed method, called IFSeg, generates artificial image-segmentation pairs using a pre-trained VL model. Artificial training data is created by generating a 2D map of random semantic categories and their corresponding word tokens. By leveraging the common space in which the VL model projects visual and text tokens, the artificially generated word map replaces real image inputs for the VL model. Extensive experiments show that IFSeg establishes an effective baseline for this task and outperforms existing methods that rely on stronger supervision like task-specific images and segmentation masks. The code for IFSeg is available at https://github.com/alinlab/ifseg.