We introduce a new method called Iterative Vision-and-Language Navigation (IVLN) to evaluate language-guided agents navigating in a persistent environment over time. Existing benchmarks for Vision-and-Language Navigation (VLN) reset the agent's memory at the start of each episode, testing its ability to navigate without prior information. However, real-world robots operate in the same environment for extended periods. The IVLN approach addresses this discrepancy by training and evaluating VLN agents that retain memory across multiple tours of scenes, each consisting of a series of instruction-following Room-to-Room (R2R) episodes. We have created discrete and continuous Iterative Room-to-Room (IR2R) benchmarks, comprising approximately 400 tours in 80 indoor scenes. Our findings indicate that simply extending the implicit memory of high-performing transformer VLN agents is insufficient for IVLN. Instead, agents that can build maps benefit from environment persistence, suggesting a renewed focus on map-building agents in VLN.