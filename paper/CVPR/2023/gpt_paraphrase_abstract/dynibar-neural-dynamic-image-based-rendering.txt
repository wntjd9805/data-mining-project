We present a new solution to the problem of generating new perspectives from a single video of a complex and dynamic scene. Current methods using Neural Radiance Fields (NeRFs) that vary over time have shown impressive results for this task. However, when dealing with long videos that involve intricate object movements and unpredictable camera paths, these methods often produce blurry or inaccurate renderings, limiting their practical applications. To overcome these limitations, we propose a different approach that utilizes a volumetric image-based rendering framework. This framework synthesizes new viewpoints by aggregating features from neighboring views in a way that is aware of the scene's motion. Our system retains the strengths of previous methods in modeling complex scenes and view-dependent effects while also allowing for the generation of photo-realistic new views from long videos that showcase complex scene dynamics with unrestricted camera trajectories. We demonstrate significant improvements over state-of-the-art techniques on datasets containing dynamic scenes. Additionally, we apply our approach to real-world videos with challenging camera and object motion, where existing methods struggle to produce high-quality renderings.