Deep hashing methods for large-scale image retrieval have gained popularity. However, current supervised approaches have limitations in terms of training efficiency, data distribution coverage, and pair imbalance issues. A recent method called central similarity quantization (CSQ) addresses some of these problems by using hash centers as a global similarity metric. CSQ performs well but lacks a worst-case guarantee on the minimal distance between hash centers. To overcome this, we propose an optimization method that finds hash centers with a constraint on their minimal distance, despite the problem's non-convex nature. We utilize the Gilbert-Varshamov bound from coding theory to achieve a large minimal distance while ensuring empirical feasibility. Each clearly-separated hash center represents an image class, and we introduce effective loss functions to train deep hashing networks. Experimental results on three datasets demonstrate that our method outperforms state-of-the-art deep hashing methods in image retrieval.