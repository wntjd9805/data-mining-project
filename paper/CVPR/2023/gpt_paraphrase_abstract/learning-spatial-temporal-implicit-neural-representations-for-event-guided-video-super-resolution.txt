This study focuses on utilizing event cameras, which capture intensity changes asynchronously, to address the challenging task of video super-resolution (VSR) at random scales. The high temporal resolution of event cameras presents difficulties in representing the spatial-temporal information needed for guiding VSR. To overcome this, the authors propose a novel framework that incorporates the spatial-temporal interpolation of events into VSR. Their method consists of three modules: the Spatial-Temporal Fusion (STF) module learns 3D features from events and RGB frames, the Temporal Filter (TF) module extracts explicit motion information from events near the queried timestamp, and the Spatial-Temporal Implicit Representation (STIR) module generates the super-resolved frame. The authors also introduce a real-world dataset with spatially aligned events and RGB frames. Extensive experiments demonstrate that their method outperforms previous approaches, achieving VSR at random scales. The code and dataset are available at the provided link.