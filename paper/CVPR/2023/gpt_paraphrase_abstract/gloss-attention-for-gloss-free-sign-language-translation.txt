Current sign language translation (SLT) methods often rely on gloss annotations to aid in the translation process. However, acquiring these gloss annotations can be challenging. In order to address this issue, we conducted an analysis of existing models to understand how gloss annotations contribute to SLT. Our findings revealed that gloss annotations provide two valuable pieces of information to the model: 1) they help the model implicitly learn the location of semantic boundaries in continuous sign language videos, and 2) they assist the model in comprehending sign language videos on a global scale. Based on these insights, we propose a new approach called gloss attention, which allows the model to focus its attention on video segments that share similar semantics, similar to how gloss annotations aid existing models. Additionally, we incorporate knowledge from natural language models regarding sentence-to-sentence similarity into our gloss attention SLT network (GASLT) to improve its understanding of sign language videos at the sentence level. Our experimental results, obtained from multiple large-scale sign language datasets, demonstrate the superior performance of our GASLT model compared to existing methods. To facilitate further research and implementation, we have made our code accessible at https://github.com/YinAoXiong/GASLT.