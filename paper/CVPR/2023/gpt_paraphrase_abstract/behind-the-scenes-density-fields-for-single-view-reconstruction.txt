Inferring a meaningful representation of a geometric scene from a single image is a key problem in computer vision. Traditional methods that predict depth maps can only reason about visible areas in the image. However, neural radiance fields (NeRFs) can capture true 3D information, including color, but are too complex to generate from a single image. As an alternative, we propose predicting an implicit density field from a single image. This density field maps every location in the image's frustum to volumetric density. Instead of storing color in the density field, we directly sample color from available views, resulting in a less complex scene representation compared to NeRFs. Our method can be predicted in a single forward pass by a neural network and is trained using self-supervision from video data. Our formulation enables depth prediction and novel view synthesis through volume rendering. Experimental results demonstrate that our method can predict meaningful geometry for occluded regions in the input image. Additionally, we showcase the potential of our approach on three datasets for depth prediction and novel-view synthesis.