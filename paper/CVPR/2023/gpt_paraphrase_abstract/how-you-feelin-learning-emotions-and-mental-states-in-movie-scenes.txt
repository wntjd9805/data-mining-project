The abstract discusses the importance of understanding characters' emotions and mental states in movie story analysis. The authors propose a method called EmoTx, which is a multimodal Transformer-based architecture that uses videos, multiple characters, and dialog utterances to predict a diverse and multi-label set of emotions for each character at the level of a movie scene. The predictions include classic emotions such as happiness and anger, as well as other mental states like honesty and helpfulness. The authors leverage annotations from the MovieGraphs dataset to train the model. They conduct experiments using the 10 most frequently occurring labels, the 25 most frequently occurring labels, and a mapping that clusters 181 labels into 26. Ablation studies and comparisons with other state-of-the-art emotion recognition approaches demonstrate the effectiveness of EmoTx. Analysis of EmoTx's self-attention scores reveals that expressive emotions are often associated with character tokens, while other mental states rely more on video and dialog cues.