Graphs are a versatile tool for data processing. Spectral graph theory, which utilizes linear algebra, offers powerful algorithms for designing deep networks with graph characteristics. One application is the creation of optimal graphs for specific tasks or obtaining low-dimensional embeddings of data. Previous approaches used Rayleigh-quotient type losses to minimize the problem, but we propose a different method of directly learning the graph's eigenspace. However, a challenge with this approach is the inconsistent mapping of features to eigenspace coordinates across different batches. We examine the degrees of freedom involved in learning this task using batches and propose a stable alignment mechanism that can handle batch and graph-metric changes. Our experimental results demonstrate that our learned spectral embedding outperforms the state-of-the-art in terms of various metrics such as NMI, ACC, Grassman distance, orthogonality, and classification accuracy. Furthermore, our learning approach is more stable compared to existing methods.