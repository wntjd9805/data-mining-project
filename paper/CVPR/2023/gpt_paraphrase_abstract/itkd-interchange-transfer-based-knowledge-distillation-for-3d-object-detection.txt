Recent advancements in point-cloud based 3D object detectors have shown significant progress. However, most research in this area has focused solely on improving accuracy, neglecting computational efficiency. In this study, we propose a novel autoencoder-style framework that incorporates channel-wise compression and decompression using interchange transfer-based knowledge distillation. By passing features from both a teacher and a student network through a shared autoencoder, we aim to learn the map-view feature of the teacher network. To regularize the compressed representation, we introduce a compressed representation loss that combines the channel-wise compression knowledge from both networks. Furthermore, we transfer the decompressed features in opposite directions to minimize the gap in the interchange reconstructions. Additionally, we introduce a head attention loss to align the 3D object detection information obtained through the multi-head self-attention mechanism. Extensive experiments demonstrate that our approach can effectively train a lightweight model that is well-suited for 3D point cloud detection tasks. We validate the superiority of our method using popular public datasets such as Waymo and nuScenes.