Our novel approach, Semantic Image Editing by Disentangling Object and Background (SIEDOB), addresses the limitations of previous methods in processing content-rich images and generating realistic objects and texture-consistent backgrounds. SIEDOB utilizes heterogeneous subnetworks to handle foreground objects and backgrounds separately. It disassembles the edited input into background regions and instance-level objects, which are then fed into dedicated generators. The synthesized parts are embedded in their original locations and harmonized using a fusion network. To ensure high-quality edited images, we propose innovative designs such as the Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch Discriminator, and Style-Diversity Object Generator, which are integrated into SIEDOB. Extensive experiments on Cityscapes and ADE20K-Room datasets demonstrate that our method significantly outperforms previous methods, particularly in synthesizing realistic and diverse objects and texture-consistent backgrounds. The code for our method is available at https://github.com/WuyangLuo/SIEDOB.