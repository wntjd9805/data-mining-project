This paper introduces a practical solution for the challenging problem of inverse rendering in real-world settings with unknown ambient lighting. The proposed system utilizes multi-view images captured by a smartphone to recover scene geometry and reflectance. The main idea is to leverage the smartphone's built-in flashlight as a minimally controlled light source and decompose image intensities into two photometric components: a static appearance corresponding to ambient flux, and a dynamic reflection induced by the moving flashlight. Unlike traditional methods, our approach does not require flash/non-flash images to be captured in pairs. By combining neural light fields and off-the-shelf methods, we capture ambient reflections and use the flashlight component to decouple reflectance and illumination accurately. Our setup is suitable for non-darkroom environments and avoids the challenges associated with explicitly solving ambient reflections. Extensive experiments demonstrate that our method is easy to implement, simple to set up, and consistently outperforms existing inverse rendering techniques in real-world scenarios. Additionally, our neural reconstruction can be easily exported to a physically-based rendering (PBR) textured triangle mesh, making it suitable for industrial applications. The source code and data for our method are publicly available on GitHub.