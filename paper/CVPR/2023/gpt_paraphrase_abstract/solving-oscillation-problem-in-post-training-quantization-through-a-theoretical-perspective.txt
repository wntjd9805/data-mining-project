Post-training quantization (PTQ) is a widely acknowledged compression technique due to its efficiency, data privacy, and low computational requirements. However, an issue of oscillation in PTQ methods has been overlooked. This paper aims to address this problem by presenting a theoretical explanation and proposing a principled and generalized framework to mitigate it. The oscillation in PTQ is attributed to the differences in module capacity, which we define as the module capacity (ModCap). We measure the degree of oscillation using differentials between adjacent modules. To resolve this problem, we select the top-k differentials and jointly optimize and quantize the corresponding modules. Extensive experiments validate the effectiveness of our approach, which not only reduces performance degradation but also generalizes well to various neural networks and PTQ methods. For instance, when quantizing ResNet-50 with 2/4 bits, our method outperforms the previous state-of-the-art by 1.9%. Notably, our method exhibits even greater improvements in small model quantization, surpassing the BRECQ method by 6.61% on MobileNetV2 Ã—0.5.