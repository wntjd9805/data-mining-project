This research focuses on the challenge of generating realistic and diverse 3D body movements based on human speech. The study begins by creating a high-quality dataset of 3D body meshes synchronized with speech recordings. A novel framework is then developed to convert speech into body poses, hand gestures, and facial expressions, with separate models for each component. This separation is based on the observation that facial articulation is strongly linked to speech, while body poses and hand gestures have weaker correlations. The framework utilizes an autoencoder for face motions and a compositional vector-quantized variational autoencoder (VQ-VAE) for body and hand motions, enabling the generation of diverse results. A cross-conditional autoregressive model is also proposed to generate coherent and realistic body poses and hand gestures. Extensive experiments and user studies confirm that this approach achieves superior performance both qualitatively and quantitatively. The dataset and code are publicly available for research purposes at https://talkshow.is.tue.mpg.de/.