We introduce a novel 3D generative adversarial network (GAN) inversion framework that produces realistic new views while preserving specific details of the input image. The challenge in achieving high-fidelity 3D GAN inversion lies in the trade-off between geometry and texture, as overfitting to a single view can harm the accuracy of the estimated geometry. To address this issue, we propose a unique pipeline that combines pseudo-multi-view estimation with visibility analysis. This pipeline retains the original textures for visible areas and employs generative priors for occluded regions. Through extensive experiments, we demonstrate that our approach outperforms previous methods in terms of reconstruction quality and synthesis of new views, even for images with textures outside the training distribution. Moreover, our pipeline enables editing of image attributes using the inverted latent code and 3D-aware texture modification. This advancement in high-fidelity 3D rendering from a single image holds promise for various applications involving AI-generated 3D content. The source code for our approach is available at https://github.com/jiaxinxie97/HFGI3D/.