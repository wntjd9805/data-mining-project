The lack of depth information makes monocular 3D lane detection a challenging task. One common approach is to convert front-viewed (FV) images or features into bird-eye-view (BEV) space using inverse perspective mapping (IPM) and detect lanes from BEV features. However, IPM relies on the assumption of a flat ground and loses context information, resulting in inaccurate 3D lane restoration. Another attempt to predict 3D lanes directly from FV representations falls short due to the absence of structured representation for 3D lanes. To address these limitations, we introduce a BEV-free method called Anchor3DLane, which defines 3D lane anchors in the 3D space and predicts 3D lanes directly from FV representations. By projecting the 3D lane anchors onto the FV features, we extract their features, which contain both structural and context information for accurate predictions. We also propose a global optimization method that utilizes the equal-width property between lanes to reduce lateral prediction errors. Extensive experiments on three widely used 3D lane detection benchmarks demonstrate that Anchor3DLane surpasses previous BEV-based methods and achieves state-of-the-art performance. The code for Anchor3DLane is available at: https://github.com/tusen-ai/Anchor3DLane.