We introduce CrowdCLIP, an unsupervised framework for crowd counting that reduces the reliance on costly manual labeling. Our approach leverages the recent success of the vision-language model CLIP and the natural mapping between crowd patches and count text. In the training stage, we use a multi-modal ranking loss with ranking text prompts to guide the image encoder learning. In the testing stage, we employ a progressive filtering strategy to select potential crowd patches and map them into the language space with various counting intervals. Extensive experiments on challenging datasets demonstrate that CrowdCLIP outperforms previous unsupervised counting methods and even surpasses some fully-supervised methods in the cross-dataset setting. The source code for CrowdCLIP will be made available at https://github.com/dk-liang/CrowdCLIP.