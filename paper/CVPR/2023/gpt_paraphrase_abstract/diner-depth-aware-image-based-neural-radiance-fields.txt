We introduce DINER, a Depth-aware Image-based NEural RadianceÔ¨Åelds. By utilizing a set of sparse RGB input views, DINER predicts depth and feature maps to guide the reconstruction of a volumetric scene representation. This enables the rendering of 3D objects from new viewpoints. Our approach incorporates depth information into feature fusion and scene sampling, resulting in better synthesis quality and the ability to process input views with larger disparities. This allows us to capture scenes more comprehensively without the need for changes in hardware requirements, and it facilitates larger viewpoint changes during novel view synthesis. We evaluate DINER by synthesizing novel views for both human heads and general objects, and we observe significant improvements in qualitative results and perceptual metrics compared to the previous state of the art. The code for DINER is publicly available through the Project Webpage.