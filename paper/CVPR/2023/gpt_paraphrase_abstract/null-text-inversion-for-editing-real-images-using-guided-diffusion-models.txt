This abstract discusses the development of a technique for editing real images using text prompts. It highlights the use of large-scale text-guided diffusion models for generating images and the current focus on enabling text-based editing tools. To edit a real image, the image must first be transformed into the model's domain using a meaningful text prompt. The proposed technique involves two key components: pivotal inversion for diffusion models and null-text optimization. Pivotal inversion uses a single noise vector for each timestamp to optimize the inversion process. This approach improves upon the limitations of direct inversion methods. Null-text optimization focuses on modifying the unconditional textual embedding used for classifier-free guidance, rather than the input text embedding. This allows for editing images based on prompts without the need to adjust the model's weights. The technique is evaluated using the Stable Diffusion model on various images.