Sketches have been primarily used for image retrieval, but this paper introduces a new approach by leveraging the expressiveness of sketches for object detection. The proposed framework enables detection based on specific sketches, such as identifying a particular zebra in a herd or focusing on a specific part of the zebra. Notably, this model works without prior knowledge of the expected category during testing and does not require additional bounding boxes or class labels. Instead of developing a new model, the paper demonstrates the synergy between existing foundation models like CLIP and sketch models designed for sketch-based image retrieval (SBIR). By combining CLIP's generalization capabilities with SBIR's ability to bridge the gap between sketches and photos, the framework achieves accurate object detection. The approach involves prompting the sketch and photo branches of an SBIR model independently to create generalizable encoders. These encoders are then adapted for object detection, ensuring alignment between the region embeddings of detected boxes and the sketch and photo embeddings from SBIR. Experimental evaluations on standard object detection datasets demonstrate that the proposed framework outperforms both supervised and weakly-supervised object detectors in zero-shot setups. The project page for more information is available at: https://pinakinathc.github.io/sketch-detect.