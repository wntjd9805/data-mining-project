Many perception systems in fields such as mobile computing, autonomous navigation, and AR/VR face computational limitations when processing high-resolution input images. Previous research has proposed nonuniform downsamplers that learn to focus on important regions of an image, reducing computational load while still retaining relevant information for the task at hand. However, these downsampling techniques can negatively affect tasks that require spatial labels, such as 2D/3D object detection and semantic segmentation. In this study, we introduce a method called "learn to zoom" (LZU) that addresses this issue. LZU first zooms in on the input image to compute spatial features and then reverts any distortions caused by the zooming process. We achieve efficient and differentiable unzooming by approximating the zooming warp with a piecewise bilinear mapping that can be inverted. LZU can be applied to any task that involves 2D spatial input and any model that produces 2D spatial features. We demonstrate the versatility of LZU by evaluating its performance on various tasks and datasets, including object detection on Argoverse-HD, semantic segmentation on Cityscapes, and monocular 3D object detection on nuScenes. Interestingly, we observe performance improvements even when high-resolution sensor data is not available, suggesting that LZU can also be used to "learn to up-sample." For code and additional visuals related to LZU, please visit https://tchittesh.github.io/lzu/.