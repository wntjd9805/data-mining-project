The current approach to weakly supervised dense object localization (WSDOL) using Class Activation Mapping (CAM) is limited in its ability to properly associate pixel features, resulting in inaccurate localization maps. To address this issue, we propose a new method that leverages Contrastive Language-Image Pre-training (CLIP) to construct multi-modal class representations. Our approach involves a unified transformer framework that learns class-specific visual and textual tokens, capturing semantics from visual data and class-related language priors from CLIP. Additionally, we enrich these class-specific tokens with sample-specific contexts, including visual and image-language contexts, to enhance class representation learning and facilitate dense localization. Experimental results on multiple datasets demonstrate the superiority of our proposed method for WSDOL, as well as its ability to achieve state-of-the-art results in weakly supervised semantic segmentation (WSSS).