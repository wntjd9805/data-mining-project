Self-supervised learning in vision-language processing (VLP) involves using the alignment between images and text to train models. However, previous work in biomedical VLP has focused on aligning single image and report pairs, ignoring the fact that clinical notes often refer to prior images. This lack of consideration not only leads to poor alignment between modalities but also misses the opportunity to leverage temporal information in the data for self-supervision. To address this limitation, we propose a new approach called BioViL-T. This approach incorporates prior images and reports during both training and fine-tuning stages. BioViL-T utilizes a CNN-Transformer hybrid multi-image encoder that is trained jointly with a text model. It is designed to handle challenges such as pose variations and missing input images over time. The resulting model performs exceptionally well on various downstream tasks, including progression classification, phrase grounding, and report generation. It also consistently improves disease classification and sentence-similarity tasks. To evaluate the quality of vision-language representations in terms of temporal semantics, we introduce a new benchmark dataset called MS-CXR-T. Our experimental results demonstrate the advantages of incorporating prior images and reports, allowing us to make the most of the available data. By considering a series of images and their correlation with reports, our approach achieves state-of-the-art performance and offers greater generalization across a wider range of tasks. In conclusion, our work highlights the importance of leveraging temporal information in biomedical VLP and provides a novel approach, BioViL-T, that effectively utilizes prior images and reports for improved model performance. We also contribute a new benchmark dataset, MS-CXR-T, to facilitate further research in this area.