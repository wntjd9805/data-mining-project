This study focuses on exploring the freestyle capability of layout-to-image synthesis (LIS) models, which typically generate images for a predefined set of semantic classes. The aim is to investigate how well these models can generate unseen semantics, such as new classes, attributes, and styles, given a specific layout. This task is referred to as FreestyleLIS (FLIS). The authors propose leveraging large-scale pre-trained text-to-image diffusion models to achieve the generation of unseen semantics. The main challenge is enabling the diffusion model to synthesize images from layouts that violate its pre-learned knowledge. To address this, a new module called Rectified Cross-Attention (RCA) is introduced. RCA integrates semantic masks into the diffusion model by rectifying the attention maps between image and text tokens. This allows for the incorporation of various semantics from pre-trained knowledge onto a specific layout. The experiments demonstrate that the proposed diffusion network produces realistic and freestyle layout-to-image generation results with diverse text inputs, opening up possibilities for interesting applications. The code for the proposed method is available at https://github.com/essunny310/FreestyleNet.