The recent detection transformer (DETR) has significantly improved object detection, but its use on devices with limited resources requires a large amount of computation and memory. One solution is to use quantization, which represents the network using low-bit parameters and operations. However, existing quantization methods result in a noticeable decrease in performance when applied to low-bit quantized DETR (Q-DETR). Our empirical analysis reveals that this performance drop is mainly due to distortion in the query information. To address this issue, we propose a solution called distribution rectification distillation (DRD). We formulate DRD as a bi-level optimization problem, extending the information bottleneck (IB) principle to the learning of Q-DETR. In the inner level, we align the distributions of the queries to maximize self-information entropy. In the upper level, we introduce a foreground-aware query matching scheme to effectively transfer teacher information and minimize conditional information entropy in distillation-desired features. Extensive experiments demonstrate that our method outperforms previous approaches. For instance, using 4-bit Q-DETR with a ResNet-50 backbone can theoretically speed up DETR by 6.6 times and achieve a 39.4% AP, with only a 2.6% performance gap compared to its real-valued counterpart on the COCO dataset.