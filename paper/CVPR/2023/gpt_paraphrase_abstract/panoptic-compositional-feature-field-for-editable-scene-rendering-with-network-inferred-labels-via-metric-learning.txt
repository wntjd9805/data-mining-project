The challenge of decomposing neural implicit representations into objects for editing at the instance level is still difficult, despite their impressive view synthesis capabilities. Previous approaches have achieved promising results by using ground truth annotations to supervise the learning of object-compositional representations. However, the manual labeling of ground truth annotations is expensive and impractical for real-world scenes. In this study, we propose a new framework called Panoptic Compositional Feature Field (PCFF) that learns an object-compositional neural implicit representation for editable scene rendering using labels inferred from off-the-shelf 2D panoptic segmentation networks instead of ground truth annotations. Our approach introduces an instance quadruplet metric learning to create a discriminating panoptic feature space, enabling reliable scene editing. We also propose semantic-related strategies to exploit correlations between semantic and appearance attributes, leading to improved rendering results. Experimental results on various scene datasets, including ScanNet, Replica, and ToyDesk, demonstrate that our method achieves superior performance in novel view synthesis and produces convincing real-world scene editing results.