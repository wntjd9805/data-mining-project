3D single object tracking is a crucial task in various applications, such as autonomous driving. However, it is a challenging problem due to factors like appearance variation, occlusion, and limited sensor capabilities. Existing methods often overlook important contextual information, resulting in inadequate use of knowledge for effective tracking. To address this issue, we propose CXTrack, a novel transformer-based network for 3D object tracking. CXTrack utilizes contextual information to enhance tracking results. Our approach involves a target-centric transformer network that takes point features from consecutive frames and the previous bounding box as input. This enables the exploration of contextual information and implicit propagation of target cues. To ensure accurate localization for objects of all sizes, we introduce a transformer-based localization head with a unique center embedding module that distinguishes the target from distractors. Extensive experiments on three large-scale datasets, namely KITTI, nuScenes, and WaymoOpen Dataset, demonstrate that CXTrack achieves state-of-the-art tracking performance. Moreover, the proposed method operates at a high frame rate of 34 FPS.