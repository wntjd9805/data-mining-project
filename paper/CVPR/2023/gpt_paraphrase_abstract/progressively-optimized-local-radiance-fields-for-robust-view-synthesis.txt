We introduce an algorithm that can reconstruct the radiance field of a large-scale scene using a single video. This task presents two main challenges. Firstly, existing methods rely on accurate camera poses obtained from Structure-from-Motion algorithms, which often fail on videos recorded in natural environments. Secondly, using a single global radiance field with limited representational capacity is not suitable for longer trajectories in an unbounded scene. To address these challenges, we propose a progressive approach that jointly estimates camera poses and the radiance field. This approach significantly improves the robustness of the reconstruction, especially when dealing with unknown poses. Additionally, we dynamically allocate new local radiance fields trained with frames within a temporal window to handle large unbounded scenes. This further enhances robustness and enables scalability. Our evaluation on the TANKS AND TEMPLES dataset and our own outdoor dataset, STATIC HIKES, demonstrates that our approach outperforms state-of-the-art methods.