We introduce X-Avatar, an innovative model for avatars that can accurately replicate the appearance and expressions of digital humans, enabling realistic experiences in telepresence, AR/VR, and other applications. Our approach comprehensively models the body, hands, facial expressions, and appearance, and can be trained using either full 3D scans or RGB-D data. To achieve this, we propose a part-aware learned forward skinning module that can be controlled using the parameter space of SMPL-X, allowing for expressive animation of X-Avatars. To improve the fidelity of the results, especially for smaller body parts, we propose new part-aware sampling and initialization strategies. Despite the increased number of articulated bones, our training method remains efficient. To capture fine details in the avatar's appearance, we extend the geometry and deformation fields with a texture network that is conditioned on various factors such as pose, facial expression, geometry, and surface normals. Through experimental evaluations, we demonstrate that our method surpasses existing baseline approaches in both quantitative and qualitative measures for animation tasks. Additionally, we contribute a new dataset called X-Humans, which comprises 233 sequences of high-quality textured scans from 20 participants, totaling 35,500 data frames. This dataset aims to facilitate further research on expressive avatars. More information can be found on our project page: https://ait.ethz.ch/X-Avatar.