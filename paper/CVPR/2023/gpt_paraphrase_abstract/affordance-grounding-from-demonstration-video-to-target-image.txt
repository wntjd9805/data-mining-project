To enable intelligent robots and AR glasses to learn from expert demonstrations and apply that knowledge to solve problems, it is crucial to ground human hand interactions from demonstration videos and apply them to a user's AR glass view. However, this task is challenging due to the need to predict detailed affordances and limited training data that fails to account for video-image discrepancies. To address these challenges, we propose a method called Affordance Transformer (Afformer) that utilizes a transformer-based decoder to refine affordance grounding. Additionally, we introduce Mask Affordance Hand (MaskAHand), a self-supervised pre-training technique that synthesizes video-image data and simulates context changes to improve affordance grounding across video-image discrepancies. Our approach, Afformer with MaskAHand pre-training, achieves state-of-the-art performance on multiple benchmarks, including a significant 37% improvement on the OPRA dataset. The code for our method is available at https://github.com/showlab/afformer.