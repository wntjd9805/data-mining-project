Text-supervised semantic segmentation is a new area of research that involves using image-text contrasting to generate semantic segments. However, current methods may rely on specific network architectures. This study demonstrates that a basic contrastive language-image pre-training (CLIP) model can effectively serve as a text-supervised semantic segmentor on its own. Firstly, it is found that a vanilla CLIP model is not as effective as localization and segmentation methods due to its optimization process being focused on aligning visual and language representations densely. To overcome this limitation, the locality-driven alignment (LoDA) method is proposed, which drives CLIP optimization through sparsely aligning local representations. Additionally, a simple segmentation (SimSeg) framework is introduced. By combining LoDA and SimSeg, a vanilla CLIP model is significantly improved, resulting in impressive semantic segmentation outcomes. The proposed method surpasses previous state-of-the-art approaches on the PASCAL VOC 2012, PASCAL Context, and COCO datasets by a large margin. The code and models for this research can be accessed at github.com/muyangyi/SimSeg.