The robustness of deep neural networks has become a significant concern, as they can be sensitive to corruption during testing, even if trained on high-quality images. To address this issue, researchers have explored methods such as data augmentation and feature distillation. Inspired by sparse representation in image restoration, we propose a simple approach to learning image-quality-independent feature representation. We achieve this by introducing discrete vector quantization (VQ) to remove redundancy in recognition models. Our method involves adding a codebook module to quantize deep features, concatenating them, and using a self-attention module to enhance representation. During training, we enforce quantization of features from both clean and corrupted images in the same discrete embedding space. This allows us to learn an invariant quality-independent feature representation, resulting in improved recognition robustness for low-quality images. Experimental results demonstrate the effectiveness of our method, achieving a state-of-the-art result on ImageNet-C and an accuracy improvement on other robustness benchmark datasets. The source code for our method is available at the provided URL.