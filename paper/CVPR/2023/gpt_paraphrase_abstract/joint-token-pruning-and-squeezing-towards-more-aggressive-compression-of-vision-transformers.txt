Vision transformers (ViTs) have shown promising results in computer vision tasks, but their high computational cost limits their practical use. Previous methods that prune redundant tokens have achieved a good balance between performance and computation costs. However, these pruning strategies can result in significant information loss. Our experiments show that the impact of pruned tokens on performance is noticeable. To address this issue, we propose a new module called Token Pruning & Squeezing (TPS) that efficiently compresses vision transformers. TPS first uses pruning to identify reserved and pruned subsets of tokens. Then, it squeezes the information from pruned tokens into the reserved tokens using unidirectional nearest-neighbor matching and similarity-based fusing. Our approach outperforms state-of-the-art methods in terms of token pruning intensities. Specifically, when reducing the computational budgets of DeiT-tiny&small to 35%, our method improves accuracy by 1%-6% compared to baselines in ImageNet classification. Furthermore, our proposed method accelerates the throughput of DeiT-small beyond DeiT-tiny while achieving 4.78% higher accuracy than DeiT-tiny. Experiments on various transformers demonstrate the effectiveness of our method, and analysis experiments confirm its robustness to errors in the token pruning policy. The code for our method is publicly available at https://github.com/megvii-research/TPS-CVPR2023.