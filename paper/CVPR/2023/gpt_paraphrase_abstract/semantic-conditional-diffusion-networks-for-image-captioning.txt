Recent advancements in text-to-image generation have seen the emergence of diffusion models as powerful generative models. However, effectively utilizing these latent variable models to capture the relationship between discrete words and achieve complex visual-language alignment in image captioning is not straightforward. This paper challenges the conventional approach of learning Transformer-based encoder-decoder models and proposes a new paradigm called Semantic-Conditional Diffusion Networks (SCD-Net) specifically designed for image captioning. In SCD-Net, we first employ a cross-modal retrieval model to identify semantically relevant sentences that convey comprehensive semantic information for each input image. These rich semantics are then used as semantic priors to guide the learning process of the Diffusion Transformer, which generates the output sentence through a diffusion process. Multiple Diffusion Transformer structures are stacked in SCD-Net to progressively enhance the output sentence, improving the alignment between vision and language and ensuring linguistic coherence in a cascaded manner. To stabilize the diffusion process, we introduce a novel self-critical sequence training strategy that leverages the knowledge of a standard autoregressive Transformer model to guide the learning of SCD-Net. Extensive experiments conducted on the COCO dataset demonstrate the promising potential of diffusion models in tackling the challenging task of image captioning. The source code for SCD-Net is available at https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet.