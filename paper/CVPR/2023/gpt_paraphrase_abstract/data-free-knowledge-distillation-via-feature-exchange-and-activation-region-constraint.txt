This paper introduces a new approach to data-free knowledge distillation called Spaceship-Net, which addresses the limitations of current methods in generating diverse and efficient synthetic data. The proposed method utilizes channel-wise feature exchange (CFE) and multi-scale spatial activation region consistency (mSARC) constraint. CFE enables the generative network to sample from the feature space and synthesize diverse images for the student network's learning. However, using CFE alone can introduce unwanted noises in the synthesized images, which can hinder distillation learning. To overcome this, mSARC ensures that the student network imitates not only the logit output but also the spatial activation region of the teacher network, reducing the impact of unwanted noises. Extensive experiments on various datasets demonstrate the effectiveness of the proposed method compared to state-of-the-art data-free knowledge distillation methods. The code for Spaceship-Net is available on GitHub. This research was supported by the National Key R&D Program of China and the National Natural Science Foundation of China.