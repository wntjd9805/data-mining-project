Virtual reality and augmented reality have led to an increased demand for 3D content creation. However, this process typically requires laborious effort from human experts. In this study, we explore the challenging task of converting a single image into a 3D object and present the first successful generation of a realistic 3D object with 360-degree views that closely align with the given reference image. By leveraging the reference image, our model addresses the desire to generate new perspectives of objects from images. This advancement offers a promising approach to simplify the workflows of 3D artists and XR designers. We propose a unique framework called NeuralLift-360, which incorporates a depth-aware neural radiance representation (NeRF) and learns to construct the scene using denoising diffusion models. By incorporating a ranking loss, our NeuralLift-360 can utilize rough depth estimation for guidance in real-world scenarios. Additionally, we employ a CLIP-guided sampling strategy to ensure coherent guidance during the diffusion process. Extensive experiments demonstrate the superior performance of our NeuralLift-360 compared to existing state-of-the-art methods. For more information, please visit our project page at https://vita-group.github.io/NeuralLift-360/.