Most neural video codecs currently have a one-way flow of information, with motion coding providing motion vectors for frame coding. This paper suggests that by introducing bi-directional information interactions, the synergy between motion coding and frame coding can be improved. The authors propose a Motion Information Propagation technique which allows for the exchange of information between motion coding and frame coding. The high-dimension motion feature from the motion decoder is used as motion guidance during frame coding to reduce alignment errors. Additionally, the feature from context generation is propagated as motion condition for coding subsequent motion latent, creating a cycle of interactions that enhances feature propagation in motion coding and allows for better exploitation of long-range temporal correlation. The authors also propose a hybrid context generation method to leverage multi-scale context features and improve motion condition. Experimental results demonstrate that their approach achieves a 12.9% reduction in bit rate compared to the state-of-the-art neural video codec.