This paper addresses the issue of Audio-Visual Speech Recognition (AVSR) in scenarios where both audio and visual inputs are corrupted. Previous research has primarily focused on complementing corrupted audio inputs with clean visual inputs, assuming that clean visual inputs are always available. However, in real-life situations, clean visual inputs may not be accessible and can also be corrupted by factors like occluded lip regions or noise. Consequently, we analyze the lack of robustness in previous AVSR models when dealing with corrupted multimodal input streams. To address this, we propose a multimodal input corruption modeling approach to develop robust AVSR models. Additionally, we introduce a novel AVSR framework, called AV-RelScore, which is resilient to corrupted multimodal inputs. The AV-RelScore module can determine the reliability of each input modal stream for prediction and can leverage the more reliable streams for improved prediction accuracy. We evaluate the effectiveness of our proposed method through comprehensive experiments on widely used benchmark databases, LRS2 and LRS3. Furthermore, we demonstrate that the reliability scores obtained by AV-RelScore accurately reflect the degree of corruption, enabling the proposed model to focus on reliable multimodal representations.