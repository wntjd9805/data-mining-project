Vision-language modeling has allowed for the development of open-vocabulary tasks, which allow predictions to be made based on any given text prompt without prior training. While current open-vocabulary tasks primarily focus on object classes, there is limited research on object attributes due to the absence of a reliable attribute evaluation benchmark. This research paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and its corresponding benchmark. The goal of this task and benchmark is to assess the understanding of object-level attribute information by vision-language models. In order to accomplish this, a meticulously annotated test set has been created, covering 117 attribute classes across 80 object classes from MSCOCO. This test set includes both positive and negative annotations, enabling open-vocabulary evaluation. In total, the benchmark consists of 1.4 million annotations. The paper also provides a baseline method for open-vocabulary attribute detection for reference. Additionally, the benchmark's significance is demonstrated through the examination of attribute detection performance by various foundation models.