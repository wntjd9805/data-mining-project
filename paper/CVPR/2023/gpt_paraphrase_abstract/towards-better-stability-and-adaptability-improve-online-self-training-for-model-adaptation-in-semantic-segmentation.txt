Unsupervised domain adaptation (UDA) in semantic segmentation aims to enhance the adaptability of segmentation models in target domains by transferring knowledge from source domains. However, UDA requires access to labeled source data, which can be problematic in scenarios involving privacy, property rights protection, and confidentiality. This paper focuses on unsupervised model adaptation (UMA), also known as source-free domain adaptation, which adapts a source-trained model to the target domain without using source data. The study explores the potential of online self-training for UMA, but identifies that the absence of source domain loss weakens the method's stability and adaptability. The research identifies two reasons for the degradation of online self-training - untimely updates of the teacher model and biased knowledge obtained from the source-trained model. To address these issues, the paper proposes a dynamic teacher update mechanism and a training-consistency based resampling strategy to improve the stability and adaptability of online self-training. Experimental results on multiple model adaptation benchmarks demonstrate that the proposed method achieves new state-of-the-art performance, comparable or even superior to existing UDA methods. The code for the method is available at https://github.com/DZhaoXd/DT-ST.