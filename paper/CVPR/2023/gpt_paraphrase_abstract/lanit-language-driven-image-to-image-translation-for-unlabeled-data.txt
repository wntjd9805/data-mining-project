Current methods for translating images into different styles have two main problems: they heavily rely on manually labeling each image with its style, and they struggle to handle images with multiple style attributes. Recent unsupervised methods solve the first problem by using clustering techniques to assign labels to images, but they still struggle with images that have multiple attributes. Additionally, the labels generated by these methods are not easily understandable by humans. To address these issues, we propose a language-driven image-to-image translation model called LANIT. We utilize textual descriptions of attributes associated with images in the dataset to generate labels. The similarity between images and attributes is used to determine the labels, allowing for the possibility of multiple attributes per image. Users can specify the desired style by providing a set of attributes in natural language. To account for inaccuracies in the initial prompts, we introduce prompt learning. We also introduce a domain regularization loss to ensure that translated images are mapped to the correct style. Our experiments on standard benchmarks demonstrate that LANIT achieves comparable or better performance than existing models. The code for LANIT is available at github.com/KU-CVLAB/LANIT.