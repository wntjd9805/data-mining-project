In this study, we introduce a new approach called PlaneDepth for self-supervised monocular depth estimation (MDE) in autonomous driving. While existing depth representations based on near frontal-parallel planes have shown impressive results, they suffer from the issue of discontinuity in the ground representation. To address this, PlaneDepth utilizes orthogonal planes, including vertical planes and ground planes, to estimate the depth distribution using a Laplacian Mixture Model. These planes are used to synthesize a reference view for self-supervision. We also identify that commonly used data augmentation techniques such as resizing and cropping can negatively affect the accuracy of plane predictions by breaking the orthogonality assumptions. To overcome this, we propose a method to rectify the predefined planes and predicted camera pose during the resizing and cropping transformation. Additionally, we introduce an augmented self-distillation loss supervised with a bilateral occlusion mask to enhance the robustness of the orthogonal planes representation for occlusions. Our approach enables the unsupervised extraction of the ground plane, which is crucial for autonomous driving. Extensive experiments conducted on the KITTI dataset validate the effectiveness and efficiency of our proposed method. The code is publicly available for further reference.