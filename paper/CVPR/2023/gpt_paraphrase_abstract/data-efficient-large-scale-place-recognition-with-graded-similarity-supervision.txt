Visual place recognition (VPR) is a crucial task in computer vision for accurately determining the location of images. Current methods for training VPR models rely on binary labels that indicate whether two images depict the same place or not. However, this binary approach fails to consider the continuous similarity between images taken from different positions due to camera pose variations. This binary labeling introduces noise into the training process, causing VPR models to get stuck in local minima and necessitating expensive algorithms for hard mining to ensure convergence.To address this issue, we propose an automatic re-annotation strategy to re-label VPR datasets. Instead of binary labels, we assign graded similarity labels to image pairs based on available localization metadata. This allows us to capture the partial sharing of visual cues between images of the same place, considering the differences in camera pose. Additionally, we introduce a new training loss called Generalized Contrastive Loss (GCL) that utilizes these graded similarity labels to train contrastive networks.By using the new labels and GCL, we eliminate the need for hard-pair mining and achieve improved performance in VPR through nearest neighbor search using image descriptors. Our approach yields comparable or superior results compared to methods that rely on expensive hard-pair mining and re-ranking techniques.In summary, we propose a novel re-annotation strategy and a Generalized Contrastive Loss to enhance the training of VPR models. By considering the continuous similarity between images and utilizing graded similarity labels, we eliminate the need for costly hard mining algorithms and achieve superior performance in visual place recognition.