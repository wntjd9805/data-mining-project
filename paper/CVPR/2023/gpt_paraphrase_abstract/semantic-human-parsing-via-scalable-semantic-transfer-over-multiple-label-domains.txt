This paper introduces Scalable Semantic Transfer (SST), a new training approach that explores the benefits of using data from different label domains to train a powerful human parsing network. The paper addresses two common scenarios: universal parsing and dedicated parsing. Universal parsing focuses on learning homogeneous human representations from multiple label domains and making predictions by using different segmentation heads. Dedicated parsing aims to learn specific domain predictions while incorporating semantic knowledge from other domains. SST offers several advantages. Firstly, it serves as an effective training scheme by embedding semantic associations of human body parts from multiple label domains into the learning process. Secondly, it is an extensible semantic transfer framework that allows for the continuous addition of human parsing datasets to enhance training. Thirdly, the relevant modules are only used for auxiliary training and can be removed during inference, reducing reasoning costs. Experimental results demonstrate that SST achieves promising universal human parsing performance and shows significant improvements compared to other methods on three human parsing benchmarks: PASCAL-Person-Part, ATR, and CIHP. The code for SST is available at https://github.com/yangjie-cv/SST.