Video matting research has primarily focused on improving temporal coherence through neural networks. However, the reliance on user-annotated trimaps to estimate alpha values has posed a labor-intensive challenge. While some recent studies have attempted to address this issue by utilizing video object segmentation methods for trimap propagation, they have yielded inconsistent results. In this study, we propose a more robust and faster end-to-end video matting model called FTP-VM (FastTrimap Propagation - Video Matting) that incorporates trimap propagation. The FTP-VM integrates trimap propagation and video matting into a single model, replacing the additional backbone in memory matching with a lightweight trimap fusion module. To enhance temporal coherence, we incorporate the segmentation consistency loss from automotive segmentation and employ a Recurrent Neural Network (RNN) for collaboration in trimap segmentation. Experimental results demonstrate that the FTP-VM performs competitively in both composited and real videos, even with only a few given trimaps. Additionally, the FTP-VM exhibits a significantly higher efficiency, being eight times faster than state-of-the-art methods, thus confirming its robustness and applicability in real-time scenarios. The code for FTP-VM is publicly available at https://github.com/csvt32745/FTP-VM.