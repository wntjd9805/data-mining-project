This paper addresses the challenge of generating controllable 3D textured shapes based on textual descriptions. Existing methods require either labeled captions or extensive optimization time. To overcome these limitations, the authors propose a new framework called TAPS3D. This framework trains a text-guided 3D shape generator using pseudo captions. By extracting relevant words from the CLIP vocabulary and constructing captions using templates, the authors provide high-level semantic supervision for the generated shapes. Additionally, they incorporate low-level image regularization to enhance texture details and increase geometry diversity. During the inference phase, the proposed model can generate 3D textured shapes directly from text input without the need for additional optimization. Extensive experiments are conducted to analyze each component of the framework, demonstrating its effectiveness in producing high-fidelity, text-relevant 3D shapes. The code for TAPS3D is available at https://github.com/plusmultiply/TAPS3D.