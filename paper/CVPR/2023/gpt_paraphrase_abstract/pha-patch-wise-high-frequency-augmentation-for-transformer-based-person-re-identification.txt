Recent studies have demonstrated that incorporating Convolutional Neural Networks (CNNs) into Vision Transformers (ViTs) can enhance person re-identification performance. However, the underlying reasoning behind this improvement remains unclear. We investigate this issue from a frequency perspective and find that ViTs are less capable than CNNs in preserving crucial high-frequency components such as clothing texture details. This deficiency arises due to the inherent Self-Attention mechanism in ViTs, which dilutes high-frequency components with low-frequency ones. To address this limitation, we propose a Patch-wise High-frequency Augmentation (PHA) method consisting of two primary designs. Firstly, we employ the Discrete Haar Wavelet Transform to split patches containing high-frequency components, enabling the ViT to consider these split patches as auxiliary input and enhance its feature representation ability for high-frequency components. Secondly, we introduce a novel patch-wise contrastive loss to prevent the dilution of high-frequency components by low-frequency ones during network optimization. This loss acts as an implicit augmentation during gradient optimization, improving the representation capacity of key high-frequency components. As a result, the ViT becomes more adept at capturing these components and extracting discriminative person representations. PHA is essential during training but can be omitted during inference, without adding extra complexity. Extensive experiments conducted on widely-used person re-identification datasets validate the effectiveness of our approach.