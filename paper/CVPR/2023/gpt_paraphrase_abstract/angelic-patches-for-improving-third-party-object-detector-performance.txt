Deep learning models have proven to be highly vulnerable to distribution shifts, such as synthetic perturbations and spatial transformations. In this study, we investigate whether the characteristics of adversarial attack methods can be utilized to enhance the robustness of object detection to these distribution shifts. We focus on a specific type of object detection scenario where the target objects have control over their appearance.To address this, we propose a reversed version of the Fast Gradient Sign Method (FGSM) called the reversed FGSM. This method allows us to generate "angelic patches" that significantly improve the detection probability of objects, even without prior knowledge of the perturbations. These patches are applied simultaneously to each object instance, enhancing both classification accuracy and bounding box precision.Experimental results demonstrate the effectiveness of the partial-covering patch in solving complex bounding box problems. Furthermore, this performance is transferable to different detection models, even when subjected to severe affine transformations and deformable shapes. Notably, our approach is the first object detection patch that achieves both cross-model efficacy and utilizes multiple patches.The real-world experiments show an average improvement in accuracy of 30%. For those interested, the code for our method is available at: https://github.com/averysi224/angelic_patches.