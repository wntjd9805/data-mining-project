Self-supervised video representation learning has made significant progress using masked visual modeling. However, current methods focus on reconstructing low-level features from raw pixel values to learn representations from scratch. In this paper, we propose a two-stage framework called masked video distillation (MVD) for video representation learning. In the first stage, we pretrain an image or video model by recovering low-level features of masked patches. In the second stage, we use these features as targets for masked feature modeling. We find that students taught by video teachers perform better on temporally-heavy video tasks, while students taught by image teachers transfer stronger spatial representations for spatially-heavy video tasks. To take advantage of different teachers, we design a spatial-temporal co-teaching method for MVD. This method distills student models from both video and image teachers through masked feature modeling. Extensive experiments show that video transformers pretrained with spatial-temporal co-teaching outperform models distilled with a single teacher on various video datasets. Our MVD with vanilla ViT achieves state-of-the-art performance compared to previous methods on challenging video downstream tasks. For instance, with the ViT-Large model, our MVD achieves 86.4% and 76.7% Top-1 accuracy on Kinetics-400 and Something-Something-v2, respectively, surpassing VideoMAE by 1.2% and 2.4%. When using a larger ViT-Huge model, MVD achieves state-of-the-art performance with 77.3% Top-1 accuracy on Something-Something-v2. The code for MVD is available at https://github.com/ruiwang2021/mvd.