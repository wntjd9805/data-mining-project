Capturing the interactions between humans and their environment in 3D is crucial for various applications in robotics, graphics, and vision. However, existing methods for reconstructing 3D human and object from a single RGB image have limitations. These methods lack consistent relative translation across frames due to assuming a fixed depth, and their performance degrades when dealing with occlusions. To address these issues, we propose a new approach to track the 3D human, object, contacts, and relative translation across frames using a single RGB camera, while being robust to heavy occlusions. Our method is based on two key insights. Firstly, we enhance the accuracy of neural field reconstructions by conditioning them on per-frame SMPL model estimates, which are obtained by pre-fitting SMPL to a video sequence. This allows for more precise neural reconstruction and generates consistent relative translation across frames. Secondly, we leverage the information provided by visible frames of human and object motion to infer the appearance of occluded objects. We introduce a novel transformer-based neural network that explicitly utilizes object visibility and human motion to make predictions for the occluded frames, by leveraging neighboring frames. By incorporating these insights, our method demonstrates robust tracking of both human and object, even in the presence of occlusions. We conducted experiments on two datasets, which showed that our method outperforms state-of-the-art approaches significantly. To facilitate further research, we have made our code and pre-trained models available at: https://virtualhumans.mpi-inf.mpg.de/VisTracker.