Federated Domain Generalization (FedDG) aims to learn a global model that can generalize well to new clients with potential differences in data distribution. Previous approaches have focused on unbiased training strategies within each domain, but they struggle to ensure generalization across domains. To address this challenge, we propose a new global objective that incorporates a variance reduction regularizer to promote fairness. We introduce a FL-friendly method called Generalization Adjustment (GA) to optimize this objective by dynamically adjusting aggregation weights. Theoretical analysis shows that GA can achieve a tighter generalization bound by explicitly re-weighting the aggregation, instead of relying on implicit data sharing. The proposed algorithm is versatile and can be combined with various local client training-based methods. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, consistently outperforming other FedDG algorithms when used together. The source code is available at https://github.com/MediaBrain-SJTU/FedDG-GA.