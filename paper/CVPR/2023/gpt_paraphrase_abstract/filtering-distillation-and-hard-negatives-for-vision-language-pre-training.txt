Vision-language models trained using contrastive learning on large-scale noisy data are gaining popularity for zero-shot recognition problems. This paper aims to enhance the contrastive pre-training pipeline by addressing three key aspects: dataset noise, model initialization, and the training objective. Firstly, a straightforward filtering strategy called Complexity, Action, and Text-spotting (CAT) is proposed, which significantly reduces the dataset size while improving performance in zero-shot vision-language tasks. Secondly, an approach known as Concept Distillation is introduced to capitalize on strong unimodal representations for contrastive training, without increasing training complexity, while outperforming previous methods. Lastly, the traditional contrastive alignment objective is modified, and an importance-sampling technique is proposed to enhance the significance of hard-negatives without introducing additional complexity. On a comprehensive zero-shot benchmark consisting of 29 tasks, the Distilled and Hard-negative Training (DiHT) approach demonstrates improvement in 20 tasks compared to the baseline. Moreover, a novel approach is presented to bridge the gap between zero-shot and few-shot performance in the context of few-shot linear probing, resulting in substantial improvements over previous techniques. The models can be accessed at github.com/facebookresearch/diht.