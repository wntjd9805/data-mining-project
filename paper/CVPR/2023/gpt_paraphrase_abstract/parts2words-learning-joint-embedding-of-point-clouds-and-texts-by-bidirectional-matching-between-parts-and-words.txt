Shape-Text matching plays a crucial role in understanding shapes at a higher level. Current approaches typically represent 3D shapes as multiple 2D rendered views, but this method fails to accurately interpret the shape due to structural ambiguity caused by self-occlusion in the limited number of views. To address this problem, we propose a new approach that directly represents 3D shapes as point clouds. Our method focuses on learning a joint representation of point clouds and texts by bidirectional matching between shape parts and words. First, we divide the point clouds into parts and then utilize optimal transport method to match these parts with words in an optimized feature space. Each part is represented by aggregating features of all the points within it, while each word is abstracted based on its contextual information. We optimize the feature space to increase the similarities between the paired training samples and simultaneously maximize the margin between the unpaired ones. Experimental results demonstrate that our proposed method significantly improves accuracy compared to the state-of-the-art methods on multi-modal retrieval tasks using the Text2Shape dataset. The codes for our method are available at [insert link].