Finetuning a large vision language model (VLM) on a small-scale visual question answering (VQA) dataset is a common approach. However, specialized VQA datasets are much smaller than general-purpose ones, making it challenging to collect additional labels. To address this, we propose SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA leverages the VLM and target dataset to create a teacher model that generates question-answer pseudolabels based on images alone. This allows us to pseudolabel unlabeled images. The initial VLM is then finetuned using the augmented dataset. Our experiments demonstrate that SelTDA enhances robustness to adversarial questions, counterfactual examples, and rephrasings. It also improves domain generalization and preserves numerical reasoning skills. Importantly, SelTDA does not require additional annotations or architectural modifications and can be used with any modern encoder-decoder multimodal transformer. The code for SelTDA is available at https://github.com/codezakh/SelTDA.