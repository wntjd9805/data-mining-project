The inverted index is a commonly used data structure that improves retrieval speed by dividing the database into smaller sets and limiting distance computation. It also enhances search quality by allowing quantizers to take advantage of the condensed distribution of residual vector space. However, we have identified an issue where current deep learning-based quantizers do not benefit from the residual vector space as effectively as traditional shallow quantizers. To address this problem, we propose a novel approach called disentangled representation learning for unsupervised neural quantization. This method separates the inverted index information from the vectors, leading to a more compact latent space. Our experiments on large-scale datasets demonstrate that our approach significantly outperforms existing retrieval systems.