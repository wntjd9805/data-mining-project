Deep learning has seen significant advancements in recent years, thanks to advanced neural network structures and large-scale human-annotated datasets. However, accurately and efficiently annotating these datasets, particularly in specialized domains that require detailed labels, is often expensive and challenging. In such cases, acquiring coarse labels is much easier as they do not require expert knowledge. This study introduces a contrastive learning approach called masked contrastive learning (MaskCon) to address the under-explored problem of learning with a coarse-labelled dataset to tackle a finer labelling problem. Within the contrastive learning framework, MaskCon generates soft-labels for each sample using coarse labels against other samples and an augmented view of the sample in question. Unlike self-supervised contrastive learning, where only the sample's augmentations are considered hard positives, and supervised contrastive learning, where only samples with the same coarse labels are considered hard positives, our proposed method utilizes both inter-sample relations and coarse labels by incorporating soft labels based on sample distances masked by coarse labels. Our method demonstrates its versatility by encompassing many existing state-of-the-art works and providing tighter bounds on generalization error. Experimental results show that our method outperforms current state-of-the-art approaches on various datasets, including CIFAR10, CIFAR100, ImageNet-1K, StanfordOnline Products, and Stanford Cars196 datasets. The code and annotations for our method are available at https://github.com/MrChenFeng/MaskCon_CVPR2023.