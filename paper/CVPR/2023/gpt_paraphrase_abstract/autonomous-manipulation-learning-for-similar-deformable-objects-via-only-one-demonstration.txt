Deformable objects are commonly found in real-life scenarios, but they receive less attention compared to rigid objects in the field of 3D object recognition and manipulation. Existing methods for manipulating deformable objects face two challenges: the need for a large number of demonstrations to train models for specific instances, and the lack of generalization when transferring learned skills to similar or new instances within the same category. To address these issues, we propose a framework for category-level deformable 3D object manipulation. Our framework requires only one demonstration to manipulate deformable 3D objects and can generalize the learned skills to new instances without the need for re-training. The framework consists of two modules: the Nocs State Transform (NST) module, which transforms observed point clouds of the target into a unified pose state called the Nocs state, serving as the foundation for category-level manipulation learning; and the Neural Spatial Encoding (NSE) module, which encodes category-level spatial information to guide the expected grasping point without re-training, enabling the generalization of learned skills to novel instances. The framework plans a relative motion path for autonomous manipulation. Both simulated results using our Cap40 dataset and real robotic experiments confirm the effectiveness of our framework.