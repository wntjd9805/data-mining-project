Catastrophic forgetting poses a significant challenge in continual learning, as models need to learn new tasks without access to previous task data. Existing methods based on knowledge distillation in feature space have been effective in reducing forgetting. However, these methods often overlook the importance of plasticity by directly constraining new features to match old ones. To address this, we propose a method called Backward Feature Projection (BFP) that allows new features to change within a learnable linear transformation of old features. BFP maintains the linear separability of old classes while enabling the emergence of new feature directions for new classes. BFP can be combined with experience replay methods and greatly improve performance. Furthermore, we show that BFP helps learn a better representation space, where linear separability is preserved during continual learning and linear probing achieves high classification accuracy.