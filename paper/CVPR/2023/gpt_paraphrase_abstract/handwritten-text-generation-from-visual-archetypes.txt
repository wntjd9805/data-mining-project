Generating realistic images of handwritten text in a specific writer's style is a difficult task, especially when dealing with new styles and unfamiliar characters that were not encountered during training. While there have been recent advancements in generative models for emulating a writer's style, they have not addressed the challenge of handling rare characters. In this study, we introduce a Transformer-based model for generating few-shot styled handwritten text, with a focus on obtaining a strong and informative representation of both the text and the style. Specifically, we propose a novel approach where the textual content is represented as a sequence of dense vectors derived from images of symbols written in standard GNU Unifont glyphs, which serve as visual archetypes. This approach is more effective in generating characters that are rarely seen during training but may share visual similarities with more commonly observed characters. For the style aspect, we obtain a robust representation of unseen writers' calligraphy by leveraging specific pre-training on a large synthetic dataset. Our proposed method demonstrates its effectiveness through quantitative and qualitative evaluations, showing that it can generate words in unseen styles and with rare characters more faithfully than existing approaches that rely on independent one-hot encodings of the characters.