The objective of domain generalization (DG) is to enhance the ability of a model to generalize from a source domain to unseen domains. The Sharpness-Aware Minimization (SAM) method has been developed to achieve this goal by minimizing the sharpness measure of the loss landscape. However, SAM and its variants may not always converge to a desired flat region with a low loss value. This paper introduces two conditions that ensure convergence to a flat minimum with a small loss, and presents an algorithm called Sharpness-Aware Gradient Matching (SAGM) to meet these conditions and improve model generalization. SAGM simultaneously minimizes the empirical risk, the perturbed loss (the maximum loss within a nearby parameter space), and the gap between them. By aligning the gradient directions between the empirical risk and the perturbed loss, SAGM enhances generalization capability without increasing computational cost. Extensive experiments demonstrate that SAGM consistently outperforms state-of-the-art methods on five DG benchmarks. The code for SAGM is available at https://github.com/Wang-pengfei/SAGM.