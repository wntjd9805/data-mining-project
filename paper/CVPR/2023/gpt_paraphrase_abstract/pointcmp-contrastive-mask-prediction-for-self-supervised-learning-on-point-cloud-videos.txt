This paper introduces the PointCMP framework, a self-supervised learning approach for point cloud videos. The framework utilizes a two-branch structure to simultaneously learn local and global spatio-temporal information. Additionally, a mutual similarity based augmentation module is implemented to generate hard samples for improved representation learning. The proposed PointCMP achieves superior performance compared to existing fully-supervised methods on benchmark datasets. Furthermore, transfer learning experiments demonstrate the effectiveness of the learned representations across different datasets and tasks.