In recent years, pre-trained models have become increasingly important in deep learning research and applications. However, the defense against adversarial examples has mainly been studied in the context of training from random initialization on simple classification tasks. This paper aims to explore the fine-tuning of adversarially pre-trained models for various classification tasks in order to fully utilize their potential in adversarial robustness. Previous studies have shown that the key challenge lies in maintaining the robustness of the pre-trained model during downstream task learning. The existing model-based and data-based approaches do not effectively improve both generalization and adversarial robustness. Therefore, a novel statistics-based approach called Two-Wing Normalization (TWINS) fine-tuning framework is proposed. This framework consists of two neural networks, where one network preserves the statistical characteristics of the pre-training data in the batch normalization layers. TWINS not only transfers the robust information from the pre-trained model, but also enhances the effective learning rate without compromising training stability. This is achieved by breaking the relationship between weight norm and gradient norm in the standard batch normalization layer, enabling faster escape from sub-optimal initialization and reducing robust overfitting. Experimental results demonstrate the effectiveness of TWINS on various image classification datasets, achieving improvements in both generalization and robustness.