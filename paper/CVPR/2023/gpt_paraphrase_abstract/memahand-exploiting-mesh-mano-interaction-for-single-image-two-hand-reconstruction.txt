Current methods for hand reconstruction typically involve using a generic 3D hand model or directly predicting the positions of hand meshes. Parametric representations, which include hand shapes and rotational poses, are more stable but less accurate. Non-parametric methods, on the other hand, can predict more precise mesh positions. This paper proposes a novel approach that simultaneously reconstructs meshes and estimates MANO parameters for both hands from a single RGB image. This allows us to take advantage of the strengths of both types of hand representations. To achieve this, we introduce Mesh-Mano interaction blocks (MMIBs) that utilize mesh vertices positions and MANO parameters as query tokens. MMIB consists of a graph residual block to aggregate local information and two transformer encoders to model long-range dependencies. The transformer encoders use different attention masks to model intra-hand and inter-hand attention. Additionally, a mesh alignment refinement module is introduced to further improve the alignment between the mesh and the image. Extensive experiments on the InterHand2.6M benchmark demonstrate promising results compared to state-of-the-art hand reconstruction methods.