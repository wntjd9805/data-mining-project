Vision-Language Pre-Training (VLP) has shown promise in aligning image and text pairs, benefiting various cross-modal learning tasks. However, VLP models often lack the crucial ability to localize visual elements, limiting their effectiveness in tasks like visual reasoning. To address this, we propose a novel approach called Position-guided Text Prompt (PTP) to enhance the visual grounding capability of VLP models. In the VLP phase, PTP divides the image into blocks and identifies objects within each block using a widely used object detector. It then reformulates the visual grounding task as a fill-in-the-blank problem, prompting the model to predict objects in the given blocks or regress the blocks of a given object. By incorporating PTP into state-of-the-art VLP frameworks, we consistently observe significant improvements across different cross-modal learning architectures and benchmarks. For example, the average recall@1 in zero-shot Flickr30KRetrieval increases by 4.8 for the ViLT baseline, and the CIDEr score in COCO Captioning improves by 5.3 for the SOTABLIP baseline. Additionally, PTP achieves comparable results to object-detector based methods while offering faster inference speed since it discards the object detector during inference, unlike the latter.