Transformer-based methods have achieved impressive results in low-level vision tasks, such as image super-resolution. However, these networks have limitations in utilizing the full spatial range of input information. To address this issue, we propose a novel approach called Hybrid Attention Transformer (HAT). HAT combines channel attention and window-based self-attention schemes, leveraging their respective strengths in utilizing global statistics and local fitting capability. Additionally, we introduce an overlapping cross-attention module to enhance interaction between neighboring window features and improve cross-window information aggregation. In the training stage, we employ a same-task pre-training strategy to further leverage the model's potential. Extensive experiments demonstrate the effectiveness of our proposed modules, and we scale up the model to achieve significant performance improvements compared to state-of-the-art methods, surpassing them by more than 1dB.