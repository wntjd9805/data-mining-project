In the current era, dense prediction networks are commonly used to address visual scene understanding problems. However, obtaining pixel-wise dense annotations can be costly or even impossible for certain tasks, such as scene parsing or intrinsic image decomposition. To overcome this limitation, we propose a new approach that leverages cheap point-level weak supervision. Unlike existing methods that use the same architecture for full supervision, our method, called dense prediction fields (DPFs), introduces a paradigm that generates predictions for point coordinate queries. This is inspired by the success of implicit representations like distance or radiance fields. DPFs generate intermediate features for continuous sub-pixel locations, enabling outputs of any desired resolution. Additionally, DPFs are naturally compatible with point-level supervision. We demonstrate the effectiveness of DPFs through two different tasks: high-level semantic parsing and low-level intrinsic image decomposition. For these tasks, supervision is provided in the form of single-point semantic category and two-point relative reflectance, respectively. Our experiments on three large-scale public datasets (PASCALContext, ADE20K, and IIW) show that DPFs achieve state-of-the-art performance with significant improvements. The code for DPFs can be accessed at https://github.com/cxx226/DPF.