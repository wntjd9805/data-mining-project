The popularity of Contrastive Language-Image Pre-training (CLIP) has grown due to its impressive zero-shot recognition performance across various downstream tasks. However, CLIP requires a large amount of image-text pairs for training, making it data-intensive and necessitating the memorization of different semantic concepts. To address this, our paper introduces a new and efficient framework called Retrieval Augmented Contrastive Language-Image Pre-training (RA-CLIP) that enhances embeddings through online retrieval. We achieve this by selecting a subset of image-text data as a reference set. When presented with an input image, relevant image-text pairs are retrieved from the reference set to enrich the representation of the input image. This process can be likened to an open-book exam, where the reference set serves as a cheat sheet, eliminating the need to memorize all visual concepts in the training data. We explore how the correspondence between images and texts in the cheat sheet can be leveraged to recognize visual concepts. Through comprehensive experiments, we demonstrate the effectiveness of RA-CLIP. It surpasses the performance of the vanilla CLIP baseline by a significant margin across multiple tasks, including zero-shot image classification (+12.7%), linear probe image classification (+6.9%), and zero-shot ROI classification (+2.8%).