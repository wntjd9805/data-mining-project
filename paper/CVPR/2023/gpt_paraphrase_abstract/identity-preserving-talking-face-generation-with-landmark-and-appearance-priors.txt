Generating realistic and lip-synced talking face videos from audio is a challenging task. While person-specific methods can produce vivid videos, they require the target speaker's videos for training. On the other hand, existing person-generic methods struggle to generate realistic videos while preserving the speaker's identity. To address these issues, we propose a two-stage framework. In the first stage, we introduce a novel Transformer-based landmark generator that uses the audio to infer lip and jaw landmarks. To ensure that the generated landmarks match the speaker's facial outline, we leverage prior landmark characteristics of the speaker's face. In the second stage, we develop a video rendering model that translates the generated landmarks into face images. To generate realistic and identity-preserving visual content, we extract prior appearance information from the lower-half occluded target face and static reference images. We align the static reference images with the target face's pose and expression using motion fields to effectively utilize the prior information. Additionally, we reuse auditory features to ensure synchronization between the generated face images and the audio. Extensive experiments demonstrate that our method outperforms existing person-generic talking face generation methods. Our approach produces more realistic, lip-synced, and identity-preserving videos.