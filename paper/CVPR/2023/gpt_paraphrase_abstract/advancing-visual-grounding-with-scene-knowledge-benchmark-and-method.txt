Visual grounding (VG) is a field that aims to establish a strong connection between visual information and language. It serves as a testbed for evaluating the understanding and reasoning abilities of vision-and-language models in the context of images and texts. However, existing VG datasets often lack the need for complex reasoning, as they are constructed using simple description texts. This was proven in a recent study, where a basic LSTM-based text encoder achieved impressive results without pretraining on mainstream VG datasets. To address this limitation, this paper introduces a new benchmark called Scene Knowledge-guided Visual Grounding (SK-VG). In SK-VG, the image content and referring expressions alone are insufficient to ground the target objects, requiring the models to exhibit reasoning abilities based on extensive scene knowledge. Two approaches are proposed to tackle this task: one involves embedding knowledge into the image features before interaction with the image-query, while the other utilizes linguistic structure to aid in computing image-text matching. Extensive experiments are conducted to evaluate these approaches, which show promising results but also highlight areas for improvement, such as performance and interpretability. The dataset and code for SK-VG can be accessed at https://github.com/zhjohnchan/SK-VG.