Current object detection methods that use pretrained vision-language models struggle to detect Mobile User Interface (MUI) elements due to the presence of additional OCR information, which is often overlooked. To address this issue, this study introduces a new dataset called MUI-zh for MUI element detection and proposes an Adaptively Prompt Tuning (APT) module that leverages OCR information effectively. APT is a lightweight module that optimizes category prompts across different modalities. By encoding visual features and OCR descriptions of each element, APT dynamically adjusts the representation of frozen category prompts. The effectiveness of APT is evaluated on existing CLIP-based detectors for both standard and open-vocabulary MUI element detection. Extensive experiments demonstrate significant improvements achieved by the proposed method on two datasets. The MUI-zh dataset can be accessed at github.com/antmachineintelligence/MUI-zh.