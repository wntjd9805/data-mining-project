Many existing trackers that use RGB-based cameras are designed for low frame rate benchmarks, typically around 30 frames per second. However, this limitation hinders their effectiveness in real-world scenarios, especially when tracking fast-moving objects. On the other hand, event-based cameras, which are inspired by biological sensors, have the potential to track objects at high frame rates due to their superior temporal resolution. However, event-based cameras lack detailed texture information compared to conventional cameras. This unique combination of strengths and weaknesses between the two camera types has motivated us to propose a novel approach for high frame rate object tracking. In this paper, we present an end-to-end network that integrates both conventional frame-based data and event-based data. Our network includes modules for alignment and fusion of the two modalities, which effectively combine meaningful information from each modality. The alignment module ensures that the different measurement rates and styles of the frame and event modalities are properly synchronized, guided by the cues provided by the event data. On the other hand, the fusion module emphasizes valuable features and suppresses noise by exploiting the complementary aspects of both modalities. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. We have evaluated our approach using the FE240hz dataset and achieved impressive results, tracking objects at frame rates of up to 240Hz.