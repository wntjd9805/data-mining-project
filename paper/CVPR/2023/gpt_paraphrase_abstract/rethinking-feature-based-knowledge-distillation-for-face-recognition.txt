The expansion of face datasets has led to the prevalent use of feature-based distillation for large-scale face recognition. This study aims to eliminate the need for identity supervision in student training to save GPU memory. However, simply removing identity supervision leads to poorer distillation results. The authors investigate this performance degradation and attribute it to the intrinsic dimension and capacity gap problem. By limiting the teacher's search space through reverse distillation, the authors narrow the intrinsic gap and enable more effective feature-only distillation. This reverse distillation method creates a universally student-friendly teacher that significantly improves student performance. The authors also design a student proxy to better bridge the intrinsic gap, further enhancing the effectiveness of their proposed method. As a result, their approach outperforms state-of-the-art distillation techniques with identity supervision on various face recognition benchmarks. These improvements are consistent across different teacher-student pairs.