This study focuses on weakly supervised sequential video understanding, where there is no accurate time-stamp level text-video alignment provided. The researchers propose a solution inspired by CLIP, utilizing a transformer to aggregate frame-level features for video representation and a pre-trained text encoder to encode corresponding texts. To establish the correspondence between text and video, they introduce a multiple granularity loss. This includes a video-paragraph contrastive loss that matches the entire video with the complete script, and a fine-grained frame-sentence contrastive loss that matches each action with its description. Since frame-sentence correspondence data is unavailable, the researchers generate pseudo frame-sentence correspondence by considering the sequential nature of video actions in the temporal domain. The network training is supervised using these pseudo labels. Through extensive experiments on video sequence verification and text-to-video matching, the proposed method demonstrates superior performance compared to baseline approaches, confirming its effectiveness. The code for this approach can be found at https://github.com/svip-lab/WeakSVR.