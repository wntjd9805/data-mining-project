Real-world images often have imbalanced content density, with some areas being uniform and others containing many small objects. However, traditional convolutional deep networks treat all areas equally during downsampling. This leads to a limited representation of small objects, resulting in poorer performance in tasks like segmentation. To address this issue, we propose AutoFocusFormer (AFF), a local-attention transformer image recognition backbone that adapts downsampling by learning to retain important pixels. Instead of using the traditional grid structure, we introduce a point-based local attention block, supported by a balanced clustering module and a learnable neighborhood merging module. This approach generates representations for our point-based versions of state-of-the-art segmentation heads. Experimental results demonstrate that AFF significantly improves over baseline models of similar sizes.