This paper investigates Blind Image Decomposition (BID), which aims to remove multiple types of degradation simultaneously without prior knowledge of the noise type. However, there are two practical challenges that need to be addressed. Firstly, existing methods often require extensive data supervision, which makes them impractical for real-world scenarios. Secondly, the conventional approach focuses on identifying abnormal patterns in the image to separate the noise, which conflicts with the primary task of image restoration, compromising efficiency and authenticity. To tackle both challenges, we propose a simplified and efficient paradigm called Context-aware Pretraining (CP). This paradigm involves two pretext tasks: mixed image separation and masked image reconstruction. By doing so, we reduce the need for extensive annotation and facilitate context-aware feature learning. We also introduce a Context-aware Pretrained network (CPNet) that follows a structure-to-texture restoration process. CPNet consists of two transformer-based parallel encoders, an information fusion module, and a multi-head prediction module. The information fusion module leverages the correlation between spatial and channel dimensions, while the multi-head prediction module enables texture-guided appearance flow. Additionally, we introduce a sampling loss and attribute label constraint to utilize spatial context, resulting in high-fidelity image restoration.Extensive experiments conducted on both real and synthetic benchmarks demonstrate the competitive performance of our method in various BID tasks.