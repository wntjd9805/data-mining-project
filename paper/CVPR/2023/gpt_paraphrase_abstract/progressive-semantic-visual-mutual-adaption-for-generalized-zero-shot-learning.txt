This study focuses on Generalized Zero-Shot Learning (GZSL), a method that uses knowledge from known categories to identify unseen categories. Previous approaches have primarily relied on identifying shared attributes, but this can lead to semantic ambiguity when multiple visual appearances correspond to the same attribute. To address this issue, the authors propose a dual semantic-visual transformer module (DSVTM) that progressively models the correspondences between attribute prototypes and visual features. This module is integrated into a progressive semantic-visual mutual adaption (PSVMA) network, which aims to disambiguate semantics and improve knowledge transferability. The DSVTM includes an instance-motivated semantic encoder that learns prototypes specific to each image, allowing unmatched semantic-visual pairs to be recast into matched ones. Additionally, a semantic-motivated instance decoder enhances cross-domain interactions for semantic-related instance adaptation, promoting the generation of unambiguous visual representations. To address bias towards seen classes in GZSL, a debiasing loss function is introduced to ensure consistency between predictions for seen and unseen categories. Experimental results show that the PSVMA approach consistently outperforms other state-of-the-art methods in GZSL. The code for this study is available at: https://github.com/ManLiuCoder/PSVMA.