Computer vision models excel at identifying and utilizing patterns in the world. However, it is computationally expensive to learn these patterns from scratch, which poses a challenge for low-parameter models operating on edge devices such as smartphones. This raises the question of whether the performance of models with limited representational power can be enhanced by supplementing training with additional information about statistical regularities. To investigate this, we focus on action recognition and action anticipation, capitalizing on the observation that actions typically occur in stereotypical sequences. We introduce the Event-Transition Matrix (ETM), which is derived from action labels in a dataset of untrimmed videos. The ETM captures the temporal context of a specific action by quantifying the likelihood of its occurrence before or after each other action in the dataset. By incorporating information from the ETM into the training process, we demonstrate improved performance in action recognition and anticipation across various egocentric video datasets. Through ablation and control studies, we establish that the coherent sequence of information captured by the ETM plays a crucial role in this enhancement, and we find that smaller models benefit the most from this explicit representation of temporal context. For access to the code, matrices, and models related to our project, please visit our project page at https://camilofosco.com/etm_website.