We present a novel approach to learning visual-linguistic representations in a self-supervised learning framework. Our method introduces a new operation, loss function, and data augmentation strategy. Instead of removing regions from an image that are relevant to a certain word in the corresponding caption, we generate diverse features for the image-text matching task by soft-masking these regions. We achieve this by computing word-conditional visual attention using a multi-modal encoder. To address overfitting and bias issues, we propose a focal loss for the image-text contrastive learning objective, which encourages the model to focus on hard but diverse examples. Additionally, we perform data augmentations by masking texts and introducing distortions to images in order to enhance self-supervised learning. Our approach, which combines these three innovations, proves to be effective in learning a pretrained model and achieves impressive performance on various vision-language tasks.