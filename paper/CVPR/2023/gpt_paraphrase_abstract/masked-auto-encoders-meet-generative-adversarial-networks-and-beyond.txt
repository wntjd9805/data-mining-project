The Masked Auto-Encoder (MAE) pretraining method involves masking image patches and training a vision Transformer to reconstruct the original pixels. This method has shown impressive performance in vision tasks but requires a lot of training resources. This paper introduces a new framework called GAN-MAE, which is similar to Generative Adversarial Networks. GAN-MAE uses a generator to generate masked patches based on visible patches and a discriminator to determine if a patch was synthesized by the generator. This framework improves representation learning by distinguishing between predicted and original image patches. Additionally, the parameters of the vision Transformer backbone are shared between the generator and discriminator. Extensive experiments show that the adversarial training of GAN-MAE is more efficient and outperforms the standard MAE in terms of model size, training data, and computation resources. The gains are consistent across different model sizes and datasets. For example, a ViT-B model trained with GAN-MAE for 200 epochs achieves higher fine-tuning top-1 accuracy on ImageNet-1k with fewer FLOPs compared to MAE trained for 1600 epochs. GAN-MAE also performs well in transferring downstream tasks.