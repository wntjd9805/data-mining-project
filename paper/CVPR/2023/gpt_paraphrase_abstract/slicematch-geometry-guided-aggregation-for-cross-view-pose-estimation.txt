This study focuses on estimating the camera pose of a ground-level image in relation to an aerial image of the local area. The proposed method, called SliceMatch, includes ground and aerial feature extractors, feature aggregators, and a pose predictor. The feature extractors capture dense features from the ground and aerial images. The feature aggregators generate a single ground descriptor and multiple pose-dependent aerial descriptors using a set of candidate camera poses. The aerial feature aggregator incorporates a cross-view attention module for selecting aerial features guided by the ground view and uses the geometric projection of the ground camera's viewing frustum on the aerial image to pool features. Precomputed masks enable efficient construction of aerial descriptors. SliceMatch is trained using contrastive learning, and pose estimation is achieved by comparing the similarity between the ground descriptor and the aerial descriptors. In comparison to existing approaches, SliceMatch achieves a 19% lower median localization error on the VIGOR benchmark when using the same VGG16 backbone at a speed of 150 frames per second. Furthermore, when using a ResNet50 backbone, SliceMatch demonstrates a 50% lower error.