MLP-based neural radiance fields (NeRF-based methods) often struggle to accurately render large-scale scenes due to limited model capacity, resulting in blurry images. Some recent approaches address this issue by dividing the scene into regions and using multiple sub-NeRFs to model each region individually. However, this linearly increases training costs and the number of sub-NeRFs as the scene size grows. Another solution is to use a feature grid representation, which is computationally efficient and can scale to large scenes with higher grid resolutions. However, the feature grid often produces suboptimal results and noisy artifacts, particularly in complex regions with intricate geometry and texture. In this study, we introduce a new framework that achieves high-fidelity rendering on large urban scenes while maintaining computational efficiency. We propose using a compact multi-resolution ground feature plane representation to capture the scene's overall characteristics and complement it with positional encoding inputs through another NeRF branch for rendering. Our approach combines the advantages of both solutions: a lightweight NeRF, guided by the feature grid representation, can produce photorealistic views with fine details, while the jointly optimized ground feature planes can further refine the output, resulting in more accurate and natural renderings.