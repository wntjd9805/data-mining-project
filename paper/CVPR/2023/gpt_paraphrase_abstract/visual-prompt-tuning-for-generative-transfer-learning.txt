Efficiently learning generative image models across different domains requires transferring knowledge from a large dataset to a new model. In this study, we propose a method for learning vision transformers through generative knowledge transfer. Our approach is based on generative vision transformers that represent images as sequences of visual tokens using autoregressive or non-autoregressive transformers. To adapt to a new domain, we utilize prompt tuning, which involves adding learnable tokens called prompts to the image token sequence and developing a new prompt design specifically for our task. We conduct experiments on various visual domains with different amounts of training data and demonstrate the effectiveness of knowledge transfer, resulting in significantly improved image generation quality.