Our argument is that there are various interpretations of "similarity" and that models should be capable of adapting to these interpretations dynamically, similar to how humans do. This is in contrast to most representation learning methods, whether supervised or self-supervised, which learn a fixed embedding function and therefore implicitly assume a single interpretation of similarity. For example, models trained on ImageNet are biased towards object categories, while a user may prefer the model to focus on colors, textures, or specific elements in a scene. To address this, we introduce the GeneCIS (Genesis) benchmark, which evaluates models' ability to adapt to different interpretations of similarity. Unlike previous benchmarks, our evaluation is designed specifically for zero-shot learning and considers a wide range of similarity conditions. We observe that powerful CLIP models struggle on the GeneCIS benchmark and that performance on this benchmark is only weakly correlated with accuracy on ImageNet, indicating that simply scaling existing methods is not effective. To overcome these challenges, we propose a simple and scalable solution that involves mining information from existing image-caption datasets. We find that our method significantly improves performance on the GeneCIS benchmark compared to baselines and also enhances zero-shot performance on related image retrieval benchmarks. In fact, although our model is evaluated using zero-shot learning, it outperforms state-of-the-art supervised models on the MIT-States benchmark. In conclusion, as the creators of machine learning models, we must determine in advance what constitutes the "world" of the model and define what is considered "similar" or "equal," as Karl Popper stated in 1963.