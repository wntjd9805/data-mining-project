This study focuses on the task of sign language retrieval, which involves two sub-tasks: text-to-sign-video retrieval and sign-video-to-text retrieval. Unlike traditional video-text retrieval, sign language videos not only convey visual signals but also carry rich semantic meanings because sign languages are natural languages. To address this unique characteristic, the authors approach sign language retrieval as a cross-lingual retrieval problem and a video-text retrieval task. They consider the linguistic properties of both sign languages and natural languages and simultaneously identify the fine-grained cross-lingual mappings between signs and words by contrasting the texts and sign videos in a joint embedding space. This process is referred to as cross-lingual contrastive learning. Another challenge faced is the scarcity of sign language datasets compared to speech recognition datasets. To overcome this, the authors utilize a domain-agnostic sign encoder that is pre-trained on large-scale sign videos and adapt it to the target domain using pseudo-labeling. Their framework, called CiCo (Cross-lingual Contrastive learning for domain-aware sign language retrieval), outperforms existing methods by a significant margin on various datasets. For example, it achieves improvements of +22.4 T2V and +28.0 V2T R@1 on the How2Sign dataset and +13.7 T2V and +17.1 V2T R@1 on the PHOENIX-2014T dataset. The code and models for this work are available at the GitHub repository: https://github.com/FangyunWei/SLRT.