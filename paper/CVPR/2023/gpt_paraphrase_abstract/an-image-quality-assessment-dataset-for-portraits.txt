The demand for high-quality smartphone photos, particularly in portrait photography, is increasing. Manufacturers use perceptual quality criteria during the development of smartphone cameras, but this process is expensive. Automated learning-based methods for image quality assessment (IQA) can partially replace this costly procedure. However, because IQA is subjective, it is important to estimate and ensure the consistency of the IQA process, which is lacking in the widely used mean opinion scores (MOS) for crowdsourcing IQA. Existing blind IQA (BIQA) datasets do not adequately address the difficulty of cross-content assessment, which may impact the quality of annotations. This paper introduces PIQ23, a portrait-specific IQA dataset consisting of 5116 images from 50 different scenarios captured by 100 smartphones. The dataset encompasses a wide range of brands, models, and use cases, and includes individuals of various genders and ethnicities who have given consent for their photos to be used in public research. The dataset is annotated using pairwise comparisons by over 30 image quality experts for three image attributes: face detail preservation, face target exposure, and overall image quality. A thorough statistical analysis of these annotations demonstrates their consistency across PIQ23. Additionally, the paper demonstrates that IQA predictions can be improved by incorporating semantic information, such as image context. The PIQ23 dataset, statistical analysis, and BIQA algorithms are available at https://github.com/DXOMARK-Research/PIQ2023.