Vision and Language (VL) models have shown impressive performance in various tasks without any prior training. However, there are still challenges in understanding complex language. To address this, we introduce the concept of Structured Vision & Language Concepts (SVLC), which includes object attributes, relations, and states present in both text and image. Recent studies have revealed that even the best VL models struggle with SVLC. One solution is to collect dedicated datasets for each SVLC type, but this can be costly and time-consuming. Instead, we propose a more efficient data-driven approach to enhance VL models' understanding of SVLCs using existing pre-training datasets and without the need for additional data. While automatic understanding of image structure remains a challenge, language structure is better modeled and understood, making it useful for training VL models. In this paper, we suggest various techniques based on language structure understanding to manipulate the textual part of paired VL datasets. The proposed techniques result in a significant improvement of up to 15% in SVLC understanding for VL models trained with the updated data, with only a slight decrease in their zero-shot capabilities. This improvement is observed whether training from scratch or fine-tuning a pre-trained model. The code and pretrained models can be accessed at: [link].