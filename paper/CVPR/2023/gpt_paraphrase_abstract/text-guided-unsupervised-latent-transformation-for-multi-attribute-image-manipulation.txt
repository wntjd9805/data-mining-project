We propose a model called Text-guided Unsupervised StyleGAN Latent Transformation (TUSLT) that addresses the limitation of existing approaches in StyleGAN-based image editing. These approaches typically focus on supervised learning for individual attributes and do not allow for simultaneous manipulation of multiple attributes. TUSLT overcomes this limitation by adaptively inferring a single transformation step in the latent space of StyleGAN to manipulate multiple attributes. To achieve this, we use a two-stage architecture for a latent mapping network. In the first stage, the network learns a diverse set of semantic directions tailored to the input image. In the second stage, it nonlinearly fuses the directions associated with the target attributes to infer a residual vector. This two-stage architecture allows for handling diverse attribute combinations effectively.To provide semantic guidance, we leverage the cross-modal text-image representation of CLIP. We use the semantic similarity between preset attribute text descriptions and training images to perform pseudo annotations. Additionally, we jointly train an auxiliary attribute classifier with the latent mapping network. We conducted extensive experiments to demonstrate the superior performance of TUSLT. The adopted strategies, including the two-stage architecture and the use of CLIP for semantic guidance, contribute to its effectiveness in manipulating multiple attributes simultaneously.