Multimodal fusion is a technique that enhances the accuracy of semantic segmentation. However, there is limited exploration on fusing multiple modalities. In order to address this issue, we have developed the DELIVER benchmark for arbitrary-modal segmentation. This benchmark includes Depth, LiDAR, multiple Views, Events, and RGB modalities. Additionally, we have included severe weather conditions and sensor failure cases in the dataset to leverage the complementary nature of different modalities and address partial outages. To achieve this, we have introduced the CMNEXT model, which is a cross-modal segmentation model. The CMNEXT model includes a Self-Query Hub (SQ-Hub) that extracts valuable information from any modality and combines it with the RGB representation. Importantly, the addition of new modalities to the CMNEXT model only introduces a minimal increase in parameters. Furthermore, we have introduced the Parallel Pooling Mixer (PPX) to efficiently utilize cues from auxiliary modalities. Through extensive experiments on six benchmarks, including DELIVER, KITTI-360, MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets, CMNEXT has achieved state-of-the-art performance. It can scale from 1 to 81 modalities, and on the DELIVER dataset, it has achieved a mIoU of 66.30%, which is a significant improvement of 9.10% compared to the mono-modal baseline.