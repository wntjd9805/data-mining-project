We present a new method for estimating the number of samples needed for a model to achieve a desired level of performance. The commonly used power law approach is found to be inaccurate when extrapolating from small datasets. We observe that the error in performance estimation follows a nonlinear progression for few-shot scenarios and a linear progression for high-shot scenarios. To address this, we introduce a novel piecewise power law (PPL) that handles these two data regimes differently. We train a random forest regressor using meta learning to estimate the parameters of the PPL, which can generalize across different tasks, architectures, and initializations. Compared to the power law, the PPL improves performance estimation by an average of 37% for classification and 33% for detection across multiple datasets. Additionally, we extend the PPL to provide a confidence bound and use it to limit the prediction horizon, reducing overestimation of data by 76% for classification and 91% for detection datasets.