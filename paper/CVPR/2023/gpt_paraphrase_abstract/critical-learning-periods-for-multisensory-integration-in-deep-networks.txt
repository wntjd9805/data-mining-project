This study demonstrates that the ability of a neural network to integrate information from different sources depends on receiving properly correlated signals during the early stages of training. Interfering with this learning process during the initial phase can permanently impair the development of a skill, both in artificial and biological systems, which is known as a critical learning period. The authors assert that critical periods arise from the complex and unstable dynamics in the early stages of training, which significantly impact the final performance and learned representations of the system. This challenges the prevailing belief, based on the analysis of wide and shallow networks, that the early learning dynamics of neural networks are simple and similar to those of linear models. The study also reveals that even deep linear networks exhibit critical learning periods for multi-source integration, while shallow networks do not. To investigate how internal representations change in response to disturbances or sensory deficits, the authors introduce a new measure of source sensitivity, which enables the tracking of source inhibition and integration during training. The analysis of inhibition suggests that cross-source reconstruction can serve as a natural auxiliary training objective, and the authors demonstrate that architectures trained with cross-sensor reconstruction objectives are more resilient to critical periods. These findings suggest that the recent success of self-supervised multi-modal training is not solely due to improved architectures and more data, but also to more robust learning dynamics.