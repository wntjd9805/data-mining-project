Generating videos with diverse and coherent motions is a challenging task due to the complexity of space and time. Existing methods either generate videos in an autoregressive manner or treat time as a continuous signal, but they struggle to synthesize detailed and diverse motions while maintaining temporal consistency. This study argues that using a single time-agnostic latent vector in a style-based generator is not enough to model various and temporally-consistent motions. To address this limitation, the researchers propose incorporating additional time-dependent motion styles to capture diverse motion patterns. They also introduce a Motion Style Attention (MoStAtt) mechanism, which enhances frames with dynamic dynamics for each layer by assigning attention scores to different motion styles. This mechanism allows for weight modulation and soft attention to different motion styles. Experimental results demonstrate that the proposed model achieves state-of-the-art performance on four unconditional video synthesis benchmarks, where the model is trained with only three frames per clip. The model also produces visually superior results in terms of dynamic motions. The code and videos are available at the provided GitHub link.