We present a new approach called Panoptic Lifting for generating 3D volumetric representations of real-world scenes from images. Our model can generate color images and 3D-consistent panoptic segmentation from different viewpoints. Unlike existing methods, our approach only requires 2D panoptic segmentation masks generated by a pre-trained network. Our main contribution is a panoptic lifting scheme based on a neural field representation that creates a unified and multi-view consistent 3D representation of the scene. To handle inconsistencies in 2D instance identifiers across views, we use a linear assignment with a cost based on the model's predictions and the generated segmentation masks, ensuring consistent 2D-to-3D lifting. We also propose and evaluate techniques to improve the robustness of our method to noisy labels, including confidence estimation, segment consistency loss, bounded segmentation fields, and gradient stopping. Experimental results on challenging datasets demonstrate the effectiveness of our approach, with improvements of 8.4%, 13.8%, and 10.6% in scene-level PQ over the state of the art methods in the Hypersim, Replica, and ScanNet datasets.