Recent advancements in vision-based reinforcement learning (RL) have shown that extracting high-level features from raw pixels using self-supervised learning is effective for learning policies. However, these methods primarily focus on learning global representations of images and do not consider the local spatial structures present in consecutive frames. This paper proposes a new approach called self-supervised Paired Similarity Representation Learning (PSRL) to encode spatial structures in an unsupervised manner. The method generates individual latent volumes using an encoder to capture the variance in local spatial structures, such as correspondence maps among multiple frames. This provides ample fine-grained samples for training the deep RL encoder. Additionally, the paper attempts to learn global semantic representations in the action aware transform module, which predicts future state representations using action vectors. The proposed method imposes similarity constraints on three latent volumes: transformed query representations using estimated pixel-wise correspondence, predicted query representations from the action aware transform model, and target representations of future state. These constraints guide the action aware transform with a locality-inherent volume. Experimental results on complex tasks in Atari Games and DeepMind Control Suite demonstrate that RL methods benefit significantly from the proposed self-supervised learning of paired similarity representations.