Diffusion models have been successful in various tasks, but their reliance on large-scale training data from diverse sources raises concerns about the trustworthiness of the data. This study aims to investigate the vulnerabilities of diffusion models to potential manipulations of training data. The focus is on Trojan attacks, where the goal is to determine how difficult it is to perform such attacks on well-trained diffusion models and what adversarial targets can be achieved. The researchers propose a Trojan attack called TrojDiff, which optimizes the Trojan diffusion and generative processes during training. They introduce novel transitions in the Trojan diffusion process to distribute adversarial targets into a biased Gaussian distribution and propose a new parameterization of the Trojan generative process to effectively train the attack. Three types of adversarial targets are considered: forcing the Trojaned diffusion models to output instances from a specific class in the in-domain distribution, the out-of-domain distribution, or a particular instance. The evaluation is conducted on CIFAR-10 and CelebA datasets using DDPM and DDIM diffusion models. The results demonstrate that TrojDiff consistently achieves high attack performance for different adversarial targets using various triggers, while maintaining performance in benign environments. The code for TrojDiff is available at the provided GitHub link.