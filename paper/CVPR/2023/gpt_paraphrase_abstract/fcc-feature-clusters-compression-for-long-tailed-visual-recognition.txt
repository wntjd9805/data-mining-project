Deep Neural Networks (DNNs) often struggle with long-tailed data because they tend to under-represent minority classes. Previous solutions have addressed this issue from various angles, but they have not considered the impact of the density of Backbone Features (BFs). By using representation learning, DNNs can organize BFs into dense clusters in feature space, while the features of minority classes typically form sparse clusters. In practical applications, these features are discretely mapped or may even cross the decision boundary, leading to misclassification. Based on this observation, we propose a straightforward and versatile method called Feature Clusters Compression (FCC) to increase the density of BFs by compressing the clusters. FCC achieves this by multiplying the original BFs by a scaling factor during the training phase, establishing a linear compression relationship between the original features and the multiplied ones, and compelling DNNs to map the former into denser clusters. During the testing phase, we directly feed the original features to the classifier without multiplying them, resulting in BFs of test samples being mapped closer together and less likely to cross the decision boundary. Furthermore, FCC can be easily combined with existing long-tailed methods to enhance their performance. We apply FCC to several state-of-the-art methods and evaluate them on widely used long-tailed benchmark datasets. Extensive experiments confirm the effectiveness and versatility of our approach. The source code is available at https://github.com/lijian16/FCC.