Previous methods for Under-Display Camera (UDC) image restoration have struggled to obtain large-scale and perfectly aligned paired training data, resulting in the use of monitor-based or simulation-based methods that sacrifice realism and introduce domain gaps. In this study, we propose a new approach that revisits the classic stereo setup for data collection, capturing two images of the same scene using a UDC and a standard camera. Our key idea is to extract details from a high-quality reference image and apply them to the UDC image, enabling the generation of real training pairs. However, this approach is susceptible to spatial misalignment caused by perspective and depth of field changes, as well as the significant domain discrepancy between UDC and normal images. To address these challenges, we introduce a novel Transformer-based framework that incorporates two components: the Domain Alignment Module (DAM) and the Geometric Alignment Module (GAM). These components facilitate the accurate alignment of UDC and normal views by encouraging robust correspondence discovery. Our extensive experiments demonstrate that generating high-quality and well-aligned pseudo UDC training pairs is advantageous for training a reliable restoration network. The code and dataset for our approach are available at https://github.com/jnjaby/AlignFormer.