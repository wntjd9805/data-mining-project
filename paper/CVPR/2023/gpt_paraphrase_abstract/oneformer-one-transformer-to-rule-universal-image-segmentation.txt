Universal Image Segmentation has been explored in the past through various approaches such as scene parsing, panoptic segmentation, and new panoptic architectures. However, these architectures do not truly unify image segmentation as they require individual training for each segmentation task. In order to address this limitation, we propose OneFormer, a universal image segmentation framework that combines segmentation tasks through a multi-task train-once approach. To achieve this, we introduce a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Additionally, we incorporate a task token to dynamically adapt our model to the specific task at hand, allowing for multi-task training and inference. Furthermore, we propose the use of a query-text contrastive loss during training to improve the distinctions between tasks and classes. Notably, our OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, Cityscapes, and COCO datasets, despite the latter being trained individually for each task. We believe that OneFormer represents a significant advancement towards achieving a more universal and accessible image segmentation framework.