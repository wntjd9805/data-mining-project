The Contrastive Language-Image Pre-training (CLIP) model has shown promise in various tasks by leveraging pretrained vision and language knowledge. This paper proposes a new method called TCM, which focuses on using CLIP directly for text detection without pretraining. The advantages of TCM are: (1) It can improve existing scene text detectors. (2) It enables few-shot training, leading to a significant performance boost with limited labeled data. (3) By incorporating CLIP into scene text detection methods, it demonstrates promising domain adaptation ability. The code for TCM will be publicly available at https://github.com/wenwenyu/TCM.