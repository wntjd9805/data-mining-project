Deep neural networks are known to be vulnerable to adversarial examples, which are modified inputs that can cause incorrect predictions. While these adversarial examples can be transferred between models, targeted attacks still have lower success rates due to variations in decision boundaries. In order to improve the transferability of targeted adversarial examples, the authors propose introducing competition into the optimization process. They suggest crafting adversarial perturbations in the presence of two new types of competitor noises: adversarial perturbations towards different target classes and friendly perturbations towards the correct class. By including these competitors, adversarial examples can be suppressed if they deceive the network by extracting specific features leading to the target class. This competition forces adversarial examples to adopt different attack strategies by leveraging more diverse features, thus increasing their transferability to different models. To efficiently simulate this competition, the authors introduce a method called Clean Feature Mixup (CFM), which randomly mixes up stored clean features during model inference. Experimental results on the ImageNet-Compatible and CIFAR-10 datasets demonstrate that CFM outperforms existing baselines. The code for CFM is available at the provided GitHub link.