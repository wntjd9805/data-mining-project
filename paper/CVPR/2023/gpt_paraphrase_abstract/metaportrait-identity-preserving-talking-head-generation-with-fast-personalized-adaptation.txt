This study introduces a novel framework for generating talking head videos while preserving the identity of the person. The framework improves upon previous methods in two ways. Firstly, it emphasizes the importance of dense landmarks for accurately capturing the geometry of the face. Secondly, it incorporates adaptive fusion techniques inspired by face-swapping methods to better preserve the key characteristics of the person's image portrait during synthesis. Although the proposed model outperforms existing benchmarks in terms of generation fidelity, personalized fine-tuning is still required for real-world usage. However, this fine-tuning process is computationally demanding and not feasible for standard users. To address this, the study proposes a fast adaptation model using a meta-learning approach, which can quickly adapt the learned model to a high-quality personalized model in as little as 30 seconds. Additionally, a spatial-temporal enhancement module is introduced to improve fine details while maintaining temporal coherency. Extensive experiments demonstrate the significant superiority of the proposed approach compared to state-of-the-art methods in both one-shot and personalized settings.