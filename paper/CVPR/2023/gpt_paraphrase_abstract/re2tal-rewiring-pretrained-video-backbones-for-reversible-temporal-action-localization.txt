Temporal action localization (TAL) is a challenging task that involves predicting actions of varying durations and complexity in videos. However, training TAL models end to end on long videos is difficult due to limited GPU memory. Existing methods typically rely on pre-extracted features, which hinders their ability to optimize for localization performance. To address this limitation, we propose a novel end-to-end method called Re2TAL. This method rewires pretrained video backbones to enable reversible TAL. Re2TAL utilizes reversible modules in the backbone, allowing the input to be recovered from the output and clearing bulky intermediate activations from memory during training. We introduce a network rewiring mechanism that transforms any module with a residual connection into a reversible module without changing parameters. This approach offers the advantage of obtaining a variety of reversible networks from existing and future model designs, while also requiring less training effort by reusing pre-trained parameters. In our experiments, Re2TAL achieves impressive results on ActivityNet-v1.3 and THUMOS-14 datasets, surpassing the performance of other RGB-only methods. We provide the code for Re2TAL on our GitHub repository.