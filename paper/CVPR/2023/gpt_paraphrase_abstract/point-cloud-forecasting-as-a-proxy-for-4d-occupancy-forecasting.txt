Predicting future developments is crucial for the motion planning of autonomous systems. However, traditional methods are limited as they rely on expensive human annotations and are difficult to scale to large unlabeled datasets. One promising approach is 3D point cloud forecasting from unannotated LiDAR sequences. This task requires algorithms to capture sensor extrinsics, sensor intrinsics, and the shape and motion of objects in the scene. However, the focus should be on predicting the world, not the sensors themselves. In order to address this, we reframe the task as spacetime occupancy forecasting, which removes the need to consider sensor extrinsics and intrinsics. However, obtaining ground-truth 4D occupancy data is costly, so we render point cloud data from 4D occupancy predictions. This allows us to train and test occupancy algorithms using unannotated LiDAR sequences and evaluate and compare forecasting algorithms across different datasets, sensors, and vehicles.