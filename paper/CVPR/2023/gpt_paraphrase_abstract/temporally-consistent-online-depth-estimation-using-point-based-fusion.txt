This paper focuses on the problem of depth estimation in computer vision tasks such as 3D reconstruction and computational photography. Most existing methods only estimate depth from single frames, leading to inconsistencies and artifacts when applied to videos. The goal of this paper is to develop a method that can estimate depth maps in a temporally consistent manner for video streams in real-time. This is a challenging task as future frames are unknown, requiring the method to balance enforcing consistency and correcting errors from previous estimations. Additionally, the presence of dynamic objects further complicates the problem. To address these challenges, the proposed method utilizes a global point cloud that is updated dynamically for each frame, combined with a learned fusion approach in image space. This approach encourages consistency while also allowing for updates to handle errors and dynamic objects. The results, both qualitatively and quantitatively, demonstrate that the proposed method achieves state-of-the-art quality in consistent video depth estimation.