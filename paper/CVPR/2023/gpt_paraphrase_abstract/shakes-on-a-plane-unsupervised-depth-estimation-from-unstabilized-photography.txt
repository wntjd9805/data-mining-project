Current mobile burst photography processes capture and combine a short series of frames to produce an improved image, but often overlook the three-dimensional (3D) aspects of the scene they are capturing. They treat the motion of pixels between images as a two-dimensional (2D) problem. However, our research shows that even with just the natural hand tremor present in a "long-burst" sequence of forty-two 12-megapixel RAW frames captured within two seconds, there is enough parallax information to accurately determine the depth of the scene. To address this, we have developed a test-time optimization technique that utilizes a neural RGB-D representation to analyze long-burst data and simultaneously estimate both scene depth and camera motion. Our model, which combines planes and depth, has been trained end-to-end and incorporates a coarse-to-fine refinement process that controls the access of the network to multi-resolution volume features at different stages of training. We have experimentally validated our method and successfully demonstrated the generation of geometrically precise depth reconstructions without requiring any additional hardware or separate preprocessing steps for the data or pose estimation.