This work challenges the design philosophy of the Vision Transformer (ViT) model by redistributing parameters across transformer blocks and different structures within the block. By using a novel Hessian-based pruning criteria and latency-aware regularization, the authors develop a new architecture called NViT (Novel ViT) that achieves significant reductions in FLOPs, parameters, and runtime speed compared to the DeiT-Base model on ImageNet-1K. Smaller NViT variants also outperform the DeiT Small/Tiny variants and SWIN-Small model in terms of accuracy and parameter reduction. The authors provide analysis on the parameter redistribution insights of NViT, highlighting the high prunability of ViT models and unique parameter distribution trends across stacked ViT blocks. These insights suggest a simple and effective parameter redistribution rule for more efficient ViTs and improved performance.