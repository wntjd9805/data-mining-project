We introduce SLOPER4D, a unique dataset collected in urban environments to support the study of global human pose estimation (GHPE) with human-scene interaction in real-world settings. Our dataset features recordings of 12 individuals engaged in various activities across 10 diverse urban scenes, captured from a first-person perspective using a head-mounted device with integrated LiDAR and camera technologies. The dataset includes frame-wise annotations for 2D key points, 3D pose parameters, global translations, and reconstructed scene point clouds. To achieve accurate 3D ground truth in large dynamic scenes, we propose a joint optimization method that fits local SMPL meshes to the scene and fine-tunes camera calibration on a per-frame basis during dynamic motions. This approach results in plausible and scene-realistic 3D human poses. SLOPER4D comprises 15 sequences of human motions, with trajectory lengths ranging from 200 to 1,300 meters and covering areas from 200 to 30,000 square meters. The dataset includes over 100,000 LiDAR frames, 300,000 video frames, and 500,000 IMU-based motion frames. With SLOPER4D, we conduct a detailed analysis of two critical tasks: camera-based 3D HPE and LiDAR-based 3D HPE in urban environments. Additionally, we introduce a new task, GHPE, and benchmark its performance using SLOPER4D. Our in-depth analysis demonstrates that SLOPER4D presents significant challenges to existing methods while offering abundant research opportunities. The dataset and code are publicly available at http://www.lidarhumanmotion.net/sloper4d/.