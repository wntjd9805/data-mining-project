Estimating the 3D pose of interacting hands from a single RGB image is a difficult task due to challenges such as self-occlusion and inter-occlusion, confusion between similar appearance patterns of hands, and mapping joint positions from 2D to 3D. In this study, we propose an extension of A2J, a depth-based 3D hand pose estimation method, to the RGB domain under interacting hand conditions. Our approach, called A2J-Transformer, enhances A2J by incorporating a local-global aware mechanism to capture fine details of interacting hands and articulated clues among joints. A2J-Transformer offers three main advantages over A2J. First, it utilizes self-attention across local anchor points to improve the awareness of global spatial context and better capture joint articulation clues, thus resisting occlusion. Secondly, each anchor point is treated as a learnable query with adaptive feature learning, enhancing the capacity for pattern fitting. Lastly, anchor points are located in 3D space instead of 2D, allowing for leveraging 3D pose prediction. Experimental results on the challenging InterHand 2.6M dataset demonstrate that A2J-Transformer achieves state-of-the-art performance in model-free hand pose estimation, with significant advancements in the case of two hands (3.38mm MPJPE improvement). Furthermore, A2J-Transformer shows strong generalization and can also be applied to the depth domain. The code for A2J-Transformer is available at https://github.com/ChanglongJiangGit/A2J-Transformer.