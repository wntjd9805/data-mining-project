Federated learning, a technique for collaborative learning while preserving privacy, has shown promise. However, existing solutions primarily focus on data from the same domain, posing a challenge when dealing with distributed data from diverse domains. In such cases, the private model performs poorly on other domains due to domain shift. To address this, we propose Federated Prototypes Learning (FPL) for federated learning under domain shift. Our approach involves constructing cluster prototypes and unbiased prototypes to provide domain knowledge and ensure fair convergence. We aim to bring sample embeddings closer to cluster prototypes of the same semantics while aligning local instances with the respective unbiased prototype through consistency regularization. Experimental results on Digits and OfÔ¨ÅceCaltech tasks demonstrate the effectiveness of our proposed solution and the efficiency of crucial modules.