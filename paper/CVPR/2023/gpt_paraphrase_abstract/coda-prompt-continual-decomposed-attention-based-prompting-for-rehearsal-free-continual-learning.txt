Computer vision models often experience catastrophic forgetting when learning new concepts from continuously changing training data. Traditional solutions to this problem involve rehearsing previously seen data, but this approach increases memory costs and may compromise data privacy. However, the emergence of large-scale pre-trained vision transformer models has introduced prompting approaches as an alternative to data rehearsal. These approaches utilize a key-query mechanism to generate prompts and have proven to be highly resistant to catastrophic forgetting in continual learning without rehearsal. Nonetheless, the key mechanism of these methods is not trained end-to-end with the task sequence. Consequently, their plasticity is reduced, resulting in lower accuracy for new tasks and an inability to take advantage of expanded parameter capacity. To address this issue, we propose learning a set of prompt components that are combined with input-conditioned weights to generate input-conditioned prompts. This novel attention-based end-to-end key-query scheme outperforms the current state-of-the-art method, DualPrompt, by up to 4.5% in average final accuracy on established benchmarks. Additionally, our approach surpasses the state-of-the-art by up to 4.4% accuracy on a continual learning benchmark that includes both class-incremental and domain-incremental task shifts, representing various practical scenarios. The code for our method is available at https://github.com/GT-RIPL/CODA-Prompt.