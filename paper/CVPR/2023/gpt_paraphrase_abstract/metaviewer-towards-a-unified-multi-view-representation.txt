This study proposes a new approach to multi-view representation learning that overcomes the limitations of existing methods. Current methods typically involve extracting features from each view and then manually fusing or aligning them to create a unified representation. However, this manual manipulation can reduce the quality of the representation. To address this issue, the authors introduce a meta-learning perspective, where a meta-learner called MetaViewer automatically derives the unified representation without the need for manual fusion functions or aligning criteria. The extraction and fusion of view-specific features are formulated as a nested optimization problem and solved using a bi-level optimization scheme. This allows MetaViewer to automatically fuse view-specific features and learn the optimal fusion scheme by observing reconstruction processes from the unified to the specific across all views. The proposed method is evaluated through extensive experiments on classification and clustering tasks, demonstrating its efficiency and effectiveness.