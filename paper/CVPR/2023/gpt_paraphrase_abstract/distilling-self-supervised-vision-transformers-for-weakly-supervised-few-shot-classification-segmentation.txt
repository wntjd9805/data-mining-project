We present a method for weakly-supervised few-shot image classification and segmentation using a self-supervised VisionTransformer (ViT). By utilizing token representations from the self-supervised ViT and their correlations through self-attention, our approach generates classification and segmentation predictions. Our model effectively learns these tasks without pixel-level labels during training, relying solely on image-level labels. To achieve this, we use attention maps created from tokens produced by the self-supervised ViT backbone as pixel-level pseudo-labels. Additionally, we explore a practical scenario where a small subset of training images has ground-truth pixel-level labels while the rest only have image-level labels. In this mixed setup, we propose enhancing the pseudo-labels by training a pseudo-label enhancer with the available ground-truth pixel-level labels. Experimental results on Pascal-5i and COCO-20i datasets demonstrate significant performance improvements across various supervision settings, especially when there is limited or no access to pixel-level labels.