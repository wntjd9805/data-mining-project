This study investigates the possibility of using conversational videos captured from multiple perspectives to efficiently map a scene. The researchers propose a new problem of building a map of a previously unseen 3D environment by leveraging the shared information in the audio-visual observations of participants engaged in a natural conversation. The hypothesis is that as individuals interact and move within a scene, their audio-visual cues can help reveal hidden areas. To address the high processing cost of egocentric visual streams, the researchers explore how to coordinate the sampling of visual information to minimize redundancy and reduce power consumption. They introduce an audio-visual deep reinforcement learning approach that collaborates with a shared scene mapper to selectively activate the camera and efficiently map out the space. The approach is evaluated using both a state-of-the-art audio-visual simulator and real-world video, outperforming previous mapping methods and achieving a favorable balance between cost and accuracy. More information about the project can be found at http://vision.cs.utexas.edu/projects/chat2map.