Recent advancements in diffusion models have greatly impacted text-to-image generation technology. Although existing approaches have been successful in producing high-quality, realistic images based on text input, there are still unresolved issues that hinder further improvement in terms of image fidelity and text relevance. To address these challenges, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model. This model enhances the quality of generated images by incorporating detailed textual and visual knowledge of key elements in the scene and utilizing different denoising experts at various denoising stages. Through these mechanisms, ERNIE-ViLG 2.0 achieves state-of-the-art performance on the MS-COCO dataset, with a zero-shot FID-30k score of 6.75. Additionally, it surpasses recent models in terms of image fidelity and image-text alignment, as evidenced by a side-by-side human evaluation using the bilingual prompt set ViLG-300.