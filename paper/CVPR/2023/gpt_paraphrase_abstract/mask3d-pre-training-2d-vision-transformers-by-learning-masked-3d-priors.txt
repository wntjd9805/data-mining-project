We propose a method called Mask3D that leverages large-scale RGB-D data to embed 3D structural priors into 2D backbones used in computer vision. Unlike traditional 3D contrastive learning approaches, our method simplifies the process by formulating a pretext reconstruction task that involves masking RGB and depth patches in individual RGB-D frames. We demonstrate the effectiveness of Mask3D in enhancing representation learning for various scene understanding tasks, including semantic segmentation, instance segmentation, and object detection. Experimental results on ScanNet, NYUv2, and Cityscapes datasets show that Mask3D outperforms existing self-supervised 3D pre-training methods, achieving a significant improvement of +6.5% mIoU compared to the state-of-the-art Pri3D on ScanNet image semantic segmentation.