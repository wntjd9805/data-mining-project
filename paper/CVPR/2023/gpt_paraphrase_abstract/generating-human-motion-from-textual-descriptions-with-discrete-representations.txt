This study explores a straightforward and well-known method for generating human motion from textual descriptions using a combination of Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT). The researchers demonstrate that a simple CNN-based VQ-VAE, trained using commonly used techniques such as EMA and Code Reset, yields high-quality discrete representations. To enhance the training of GPT, the researchers introduce a simple corruption strategy to address the discrepancy between training and testing. Surprisingly, the proposed T2M-GPT model outperforms competing methods, including recent diffusion-based approaches. The researchers achieve comparable performance on the consistency between text and generated motion (R-Precision) on the largest dataset, HumanML3D, while significantly outperforming MotionDiffuse in terms of FID (0.116 vs. 0.630). However, the study also highlights that the dataset size poses limitations to their approach. Overall, this work suggests that VQ-VAE remains a strong contender for human motion generation. The implementation of the proposed method is accessible on the project page: https://mael-zys.github.io/T2M-GPT/.