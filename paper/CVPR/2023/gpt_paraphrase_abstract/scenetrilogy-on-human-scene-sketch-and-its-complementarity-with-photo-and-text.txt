This paper introduces an extension of scene understanding that incorporates human sketches into the existing modalities of photos and text. The goal is to create a flexible joint embedding that allows for the utilization of any combination of modalities for various tasks, such as retrieval or captioning. The proposed approach involves disentangling modality-specific components from modality-agnostic ones using information-bottleneck and conditional invertible neural networks. The modality-agnostic instances from sketches, photos, and text are then combined using a modified cross-attention mechanism. The resulting embedding can be used for a range of scene-related tasks without requiring task-specific modifications. The aim is to provide end-users with the flexibility to leverage the strengths of each modality, hence the proposal of a trilogy of sketch, photo, and text representations. More information about the project can be found at the provided project page.