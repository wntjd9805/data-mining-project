Training embodied agents in simulation is a common practice in the field of embodied AI. However, these agents often struggle to perform well in real-world settings due to their lack of generalization abilities. This paper introduces Phone2Proc, a method that addresses this issue by utilizing a 10-minute phone scan and conditional procedural generation. Phone2Proc creates a collection of training scenes that closely resemble the target environment in terms of semantics. The generated scenes take into account the wall layout and arrangement of large objects from the phone scan, as well as sample lighting, clutter, surface textures, and instances of smaller objects with randomized placement and materials. By utilizing a simple RGB camera, training with Phone2Proc significantly improves sim-to-real ObjectNav performance. The success rate increases from 34.7% to 70.7% across a diverse range of real-world environments, including homes, offices, and RoboTHOR. Moreover, Phone2Proc's diverse distribution of generated scenes enables agents to adapt robustly to real-world changes, such as human movement, object rearrangement, lighting variations, and clutter.