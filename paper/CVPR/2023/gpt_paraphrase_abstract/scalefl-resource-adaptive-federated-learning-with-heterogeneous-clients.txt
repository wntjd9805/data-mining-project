Federated learning (FL) is a distributed learning method that supports continuous learning and client privacy. However, some clients may have limited resources and can only train a smaller local model. This paper introduces ScaleFL, a novel FL approach that addresses resource heterogeneity and provides a fair FL framework for all clients. ScaleFL scales down the deep neural network (DNN) model by adjusting its width and depth using early exits, enabling efficient training on distributed clients without sacrificing important features. Additionally, ScaleFL utilizes self-distillation to improve aggregation by transferring knowledge among subnetworks. Experimental results on benchmark datasets demonstrate that ScaleFL outperforms existing methods in terms of global and local model performance, while also achieving better inference efficiency with reduced latency and model size.