Adversarial attacks have become a major concern in evaluating the robustness of machine learning models. Targeted attacks, in particular, aim to deceive models into producing specific predictions that are more challenging and dangerous than untargeted attacks. Existing targeted attacks can be categorized into two types: instance-specific attacks and instance-agnostic attacks.Instance-specific attacks create adversarial examples by iteratively updating the gradients of a specific instance. On the other hand, instance-agnostic attacks rely on a universal perturbation or a generative model trained on a global dataset to launch attacks. However, these approaches heavily depend on the classification boundary of substitute models and overlook the realistic distribution of the target class, resulting in limited performance in targeted attacks. Furthermore, they fail to consider the combination of specific instance information and global dataset information.To overcome these limitations, we analyze the problem using a causal graph and propose a method to craft transferable targeted adversarial examples by injecting target patterns. Building upon this analysis, we introduce a generative attack model consisting of a cross-attention guided convolution module and a pattern injection module. The cross-attention guided convolution module leverages both dynamic and static convolution kernels to capture the advantages of both instance-specific and instance-agnostic attacks. The pattern injection module utilizes a pattern prototype to encode target patterns, guiding the generation of targeted adversarial examples. Additionally, we provide rigorous theoretical analysis to ensure the effectiveness of our method.Extensive experiments demonstrate that our method outperforms 10 existing adversarial attacks when tested against 13 different models.