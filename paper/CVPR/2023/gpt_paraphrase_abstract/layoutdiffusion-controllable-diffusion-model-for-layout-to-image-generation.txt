We present LayoutDiffusion, a diffusion model for layout-to-image generation that offers improved generation quality and controllability compared to previous methods. Our approach addresses the challenge of controlling both the global layout map and individual objects in complex scenes. To overcome the difficulty of fusing image and layout information, we propose constructing a structural image patch with region information and transforming it into a special layout for unified fusion. We also introduce the Layout Fusion Module (LFM) and Object-aware Cross Attention (OaCA), which model the relationships among multiple objects in a spatially precise and object-aware manner. Extensive experiments demonstrate that LayoutDiffusion outperforms state-of-the-art methods on FID, CAS, COCO-stuff, and VG datasets. Our code is publicly available at https://github.com/ZGCTroy/LayoutDiffusion.