The authors of this paper introduce a new task called referring multi-object tracking (RMOT), which aims to use language expressions to guide the prediction of multiple objects in videos. This is the first work to achieve predictions for an arbitrary number of referred objects in videos. To support RMOT, the authors create a benchmark dataset called Refer-KITTI, which consists of 18 videos with 818 expressions. Each expression is annotated with an average of 10.7 objects. The authors also propose a transformer-based architecture called TransRMOT to address the new task in an online manner, which achieves impressive detection performance and outperforms other approaches. The Refer-KITTI dataset and code are publicly available at https://referringmot.github.io.