We present PAniC-3D, a system designed to reconstruct stylized 3D character heads directly from illustrated portraits of anime characters. This task poses unique challenges due to the complex geometry of hair and accessories, as well as the non-photorealistic contour lines used in character illustrations. Furthermore, there is a lack of suitable data to train and evaluate this stylized reconstruction task. To address these challenges, our proposed PAniC-3D architecture utilizes a line-filling model to bridge the gap between illustrations and 3D models, and represents complex geometries using a volumetric radiance field. We train our system using two large datasets, consisting of 11.2k Vroid 3D models and 1k Vtuber portrait illustrations, and evaluate its performance on a novel AnimeRecon benchmark. Our results demonstrate that PAniC-3D outperforms baseline methods and establishes the task of stylized reconstruction from portrait illustrations. Our contributions include the development of PAniC-3D, the creation of the Vroid 3D dataset, which provides 3D assets with multiview renders in the anime style, the compilation of the Vtuber dataset, which bridges the gap between illustrations and renders through the removal of lines from drawings, and the creation of the AnimeRecon benchmark, which enables quantitative evaluation of both image and geometry metrics for stylized reconstruction.