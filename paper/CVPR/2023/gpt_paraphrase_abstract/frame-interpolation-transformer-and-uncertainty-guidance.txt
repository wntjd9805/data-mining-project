Recent advancements in video frame interpolation have been made possible through various developments in different directions. Some studies have focused on improving optical flow methods and splatting strategies, while others have explored alternative approaches such as direct predictions and transformers. However, challenges still exist in more difficult conditions like complex lighting and large motion.To address these challenges, this work presents a novel transformer-based interpolation network architecture for video production. This architecture not only estimates the interpolated frame but also predicts the expected error associated with it. This approach offers several advantages for frame interpolation applications. Firstly, it achieves improved visual quality across multiple datasets, as demonstrated by a user study. Secondly, it generates error maps for the interpolated frames, which are crucial for identifying problematic frames in longer video sequences. Lastly, for rendered content, a partial rendering pass of the intermediate frame can be guided by the predicted error, resulting in a new frame of superior quality. By estimating errors, our method can produce higher-quality intermediate frames in a fraction of the time required for full rendering.In summary, this work introduces a transformer-based interpolation network architecture that addresses the challenges of video frame interpolation in complex conditions. The proposed method improves visual quality, provides error estimation for real-life applications, and enables the generation of high-quality frames with reduced rendering time.