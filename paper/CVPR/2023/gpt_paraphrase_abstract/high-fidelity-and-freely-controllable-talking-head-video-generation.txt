Current methods of generating talking head videos face challenges that affect the quality and control of the generated videos. These challenges include unexpected deformation and distortions in the generated face, the lack of explicit disentanglement of movement-relevant information in the driving image, and flickering artifacts due to inconsistent landmarks between frames. In this paper, we propose a new model that addresses these challenges and allows for free control over head pose and expression in high-fidelity talking head videos. Our method utilizes self-supervised learned landmarks and 3D face model-based landmarks to model the motion. We also introduce a motion-aware multi-scale feature alignment module to transfer motion without distorting the face, as well as a feature context adaptation and propagation module to enhance the smoothness of the synthesized videos. We evaluate our model on challenging datasets and demonstrate its superior performance compared to existing methods. More information can be found at https://yuegao.me/PECHead.