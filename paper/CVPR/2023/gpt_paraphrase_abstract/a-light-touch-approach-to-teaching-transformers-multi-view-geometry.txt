Transformers are highly adaptable learners when it comes to visual tasks, primarily because they lack manually-specified prior knowledge. However, this flexibility becomes challenging in tasks involving multiple-view geometry, where there are countless variations in 3D shapes and viewpoints, and strict rules of projective geometry must be followed. To address this issue, we propose a solution that strikes a balance between guiding visual Transformers to learn multiple-view geometry and allowing them to deviate when necessary. We accomplish this by utilizing epipolar lines to direct the Transformer's cross-attention maps during training. We penalize attention values outside the epipolar lines and encourage higher attention along these lines, as they contain plausible geometric matches. Notably, our approach does not rely on camera pose information during testing. Our focus is on pose-invariant object instance retrieval, a task where traditional Transformer networks struggle due to significant viewpoint differences between query and retrieved images. Through experiments, we demonstrate that our method outperforms state-of-the-art techniques in object retrieval without requiring pose information during testing.