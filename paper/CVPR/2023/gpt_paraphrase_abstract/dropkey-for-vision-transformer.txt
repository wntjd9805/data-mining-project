This paper focuses on analyzing and enhancing the dropout technique for self-attention layers in the Vision Transformer. Prior works have surprisingly overlooked this important aspect. The study addresses three key questions: 1. What should be dropped in self-attention layers? Instead of dropping attention weights, the paper proposes a novel approach called dropout-before-softmax, where dropout operations are applied to the Key before attention matrix calculation. Theoretical analysis confirms that this scheme helps with regularization and preserves the probability features of attention weights, mitigating overfitting issues and improving the model's ability to capture crucial information globally.2. How should the drop ratio be scheduled in consecutive layers? Instead of using a constant drop ratio for all layers, the paper introduces a decreasing schedule that gradually reduces the drop ratio as the self-attention layers stack up. Experimental results demonstrate that this schedule prevents overfitting in low-level features and avoids missing important high-level semantics, thereby enhancing the robustness and stability of model training.3. Is structured dropout operation necessary, similar to CNNs? The paper explores a patch-based block-version of dropout operation and finds that this technique, commonly used in CNNs, is not essential for the Vision Transformer (ViT).Based on the research conducted on these three questions, the paper introduces a new method called Drop-Key. This method considers the Key as the dropout unit and utilizes a decreasing schedule for the drop ratio, providing a general improvement for ViTs. Comprehensive experiments validate the effectiveness of Drop-Key across various ViT architectures and vision tasks, including image classification, object detection, human-object interaction detection, and human body shape recovery.