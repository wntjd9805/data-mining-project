Learning diverse modality-shared semantic concepts for visible-infrared person re-identification (VI-ReID) is a challenging task due to the modality gap and high visual ambiguity between visible and infrared images. Body shape is an important modality-shared cue for VI-ReID. In order to discover more diverse modality-shared cues, we propose a shape-erased feature learning paradigm that aims to remove body shape information from the learned features. By doing so, the ReID model is forced to extract other modality-shared features for identification. This paradigm decorrelates modality-shared features in two orthogonal subspaces. Specifically, we jointly learn shape-related features in one subspace and shape-erased features in the orthogonal complement. This approach maximizes the conditional mutual information between shape-erased features and identity while discarding body shape information, thereby explicitly enhancing the diversity of the learned representation. We evaluate our method on three benchmark datasets (SYSU-MM01, RegDB, and HITSZ-VCM) and demonstrate its effectiveness.