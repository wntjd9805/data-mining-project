The issue of effectively modeling complex spatiotemporal dependencies in long-form videos is still unresolved. The Structured State-Space Sequence (S4) model, which has linear complexity, has shown promise in this area. However, we have found that treating all image-tokens equally, as the S4 model does, can negatively impact its efficiency and accuracy. To overcome this limitation, we propose a new model called Selective S4 (S5) that incorporates a lightweight mask generator to selectively choose informative image tokens. This approach leads to more efficient and accurate modeling of long-term spatiotemporal dependencies in videos. Unlike previous token reduction methods, our S5 model avoids dense self-attention calculations by leveraging the guidance of the momentum-updated S4 model. This allows our model to efficiently discard less informative tokens and adapt better to various long-form video understanding tasks. However, like other token reduction methods, there is a risk of incorrectly dropping informative image tokens. To enhance the robustness and temporal context of our model, we introduce a novel approach called long-short masked contrastive learning (LSMCL), which enables our model to predict longer temporal context using shorter input videos. Our extensive comparative analysis using three challenging long-form video understanding datasets (LVU, COIN, and Breakfast) demonstrates that our approach consistently outperforms the previous state-of-the-art S4 model, achieving up to 9.6% higher accuracy while reducing memory usage by 23%.