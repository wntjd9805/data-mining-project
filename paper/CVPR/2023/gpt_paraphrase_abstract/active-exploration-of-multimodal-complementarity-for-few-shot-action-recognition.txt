Recently, there has been an increasing focus on few-shot action recognition and significant progress has been made. However, previous methods have primarily relied on limited unimodal data, such as RGB frames, and have not fully explored the potential of multimodal information. To address this gap, this paper presents a novel framework called Active Multimodal Few-shot Action Recognition (AMFAR). The framework actively identifies the most reliable modality for each sample based on task-specific context information, thereby enhancing the few-shot reasoning process. During meta-training, an Active Sample Selection (ASS) module is employed to group query samples into different categories based on the reliability of their modalities. This is done by utilizing modality-specific posterior distributions. Additionally, an Active Mutual Distillation (AMD) technique is developed to extract task-specific knowledge from the reliable modality and improve the representation learning of the unreliable modalities using bidirectional knowledge distillation.During meta-test, an Adaptive Multimodal Inference (AMI) method is used to fuse the modality-specific posterior distributions, giving more weight to the reliable modality. Through extensive experiments conducted on four public benchmarks, our model demonstrates significant improvements over existing unimodal and multimodal approaches.In summary, this paper introduces the AMFAR framework for active multimodal few-shot action recognition. By actively identifying the reliable modality for each sample and leveraging task-specific knowledge, our model achieves superior performance compared to previous methods.