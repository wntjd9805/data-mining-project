This study introduces the concept of compressed-domain temporal sentence grounding (TSG), which aims to locate a specific moment in an untrimmed video based on a sentence query. Previous works in this field have focused on high-level visual features extracted from consecutive decoded frames, but they struggle to handle compressed videos for query modeling due to limited representation capability and high computational complexity. To address this issue, the authors propose a novel framework called Three-branch Compressed-domain Spatial-temporal Fusion (TCSF). This framework utilizes compressed videos directly as the visual input and extracts and aggregates three types of low-level visual features (I-frame, motion vector, and residual features) for efficient and effective grounding. Unlike previous methods, which encode the entire decoded frames, TCSF only learns the appearance representation from the I-frame feature to reduce delay. Additionally, motion information is explored through both the motion vector feature and the residual feature, which captures the relations between neighboring frames. A three-branch spatial-temporal attention layer with an adaptive motion-appearance fusion module is designed to extract and aggregate both appearance and motion information for the final grounding. Experimental results on three challenging datasets demonstrate that TCSF outperforms other state-of-the-art methods in terms of performance while maintaining lower complexity.