The abstract discusses the effectiveness of using projection-based methods for semantic segmentation of outdoor LiDAR point clouds. These methods, which convert the 3D problem into a 2D problem through range projection, are popular due to their fast computation and ability to achieve state-of-the-art results when combined with other point cloud representations. However, recent advancements in computer vision have shown that vision transformers (ViTs) have achieved superior results in image-based benchmarks. The study aims to investigate if projection-based methods can benefit from these improvements by incorporating ViTs. The researchers found that by using the same backbone architecture as RGB images, they could leverage the knowledge gained from training on large image datasets. This approach overcomes the challenge of training ViTs, which typically require a lot of training data. Additionally, a tailored convolutional stem is used to compensate for ViTs' lack of inductive bias. The study also introduces a convolutional decoder and a skip connection to refine pixel-wise predictions by combining low-level features with high-level predictions. The proposed method, called RangeViT, outperforms existing projection-based methods on nuScenes and SemanticKITTI datasets. The code for RangeViT is available at https://github.com/valeoai/rangevit.