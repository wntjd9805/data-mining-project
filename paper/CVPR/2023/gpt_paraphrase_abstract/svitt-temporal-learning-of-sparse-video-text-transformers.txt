Despite the capacity and abundance of training data, recent research has shown that video-text transformers tend to focus more on spatial representations rather than temporal relationships across frames. This paper identifies several challenges in the temporal learning of video-text transformers, including the trade-off between spatial and temporal information due to limited network size, the curse of dimensionality in multi-frame modeling, and the decreasing returns of semantic information with longer clips. Based on these findings, the authors propose SViTT, a sparse video-text architecture that achieves multi-frame reasoning at a lower computational cost compared to dense transformers. SViTT employs two forms of sparsity: edge sparsity, which limits communications between tokens in self-attention, and node sparsity, which discards uninformative visual tokens. The model is trained using a curriculum that increases sparsity as the clip length increases. Experimental results demonstrate that SViTT outperforms dense transformer baselines on various video-text retrieval and question answering benchmarks, while requiring less computational resources. More information about the project can be found at http://svcl.ucsd.edu/projects/svitt.