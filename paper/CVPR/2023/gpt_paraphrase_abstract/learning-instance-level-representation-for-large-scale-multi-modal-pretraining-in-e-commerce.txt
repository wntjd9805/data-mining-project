This study aims to develop a versatile multi-modal foundation model that can be applied to various applications in E-commerce. While recent vision-language pretraining approaches have made significant progress in the general domain, directly applying these frameworks to E-commerce, which involves both natural and product images, may not be optimal. To address this issue, we propose a new multi-modal pretraining paradigm called ECLIP. This paradigm includes a decoder architecture that incorporates learnable instance queries to capture instance-level semantics. Additionally, we introduce two pretext tasks that allow the model to focus on specific product instances without the need for manual annotations. By training on a large dataset of E-commerce-related data, ECLIP is able to extract more generic, semantic-rich, and robust representations. Experimental results demonstrate that ECLIP outperforms existing methods by a significant margin in various downstream tasks, showcasing its strong applicability in real-world E-commerce scenarios.