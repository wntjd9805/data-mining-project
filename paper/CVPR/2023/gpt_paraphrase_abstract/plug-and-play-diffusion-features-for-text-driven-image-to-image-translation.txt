Large-scale text-to-image generative models have revolutionized generative AI by synthesizing complex images. However, a major challenge is providing users with control over the generated content. This paper introduces a framework that extends text-to-image synthesis to image-to-image translation. By inputting a guidance image and a target text prompt, our method utilizes a pre-trained text-to-image diffusion model to generate an image that aligns with the target text while preserving the layout of the guidance image. We demonstrate that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention within the model. This approach allows for direct injection of features from the guidance image into the translation process without requiring additional training or fine-tuning. Our framework produces high-quality results in various text-guided image translation tasks, including transforming sketches, rough drawings, and animations into realistic images while changing their class and appearance.