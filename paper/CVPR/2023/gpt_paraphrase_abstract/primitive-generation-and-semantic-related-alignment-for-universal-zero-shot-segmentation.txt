This study focuses on universal zero-shot segmentation, which aims to perform panoptic, instance, and semantic segmentation for new categories without any training samples. To achieve this, we leverage the inter-class relationships in the semantic space to transfer visual knowledge from seen categories to unseen ones. Our approach involves bridging the semantic-visual spaces and utilizing semantic relationships for visual feature learning. We introduce a generative model that synthesizes features for unseen categories, addressing the lack of training data for these categories. To address the domain gap between semantic and visual spaces, we enhance the generator by incorporating learned primitives that contain fine-grained attributes related to categories. These primitives are selectively assembled to synthesize unseen features. Additionally, we disentangle the visual feature into semantic-related and semantic-unrelated parts. The semantic-related part aligns with the inter-class relationships in the semantic space, facilitating the transfer of semantic knowledge to visual feature learning. Our proposed approach achieves impressive state-of-the-art performance in zero-shot panoptic segmentation, instance segmentation, and semantic segmentation.