To ensure the robustness of neural network perception systems used in safety critical applications, it is necessary to have assurance of their reliability. However, guaranteeing network robustness requires formal verification, which is typically done by analyzing invariance to analytically defined transformations. These approaches do not account for the various real-world changes that can occur, such as changes in object pose, scene viewpoint, occlusions, etc. In this study, we propose an efficient method for verifying specifications that capture these diverse changes using Latent Variable Models. Our approach involves adding an invertible encoding head to the network being verified, allowing for the verification of latent space sets with minimal reconstruction overhead. We conducted verification experiments on three different classes of latent space specifications, each capturing different types of realistic input variations. Unlike previous work in this field, our approach is independent of input dimensionality and can scale to a wide range of deep networks and real-world datasets. It mitigates the inefficiency and dependence on decoder expressivity found in current state-of-the-art approaches.