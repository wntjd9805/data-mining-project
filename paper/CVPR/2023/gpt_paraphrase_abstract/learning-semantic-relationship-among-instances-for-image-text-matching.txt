Image-text matching is an important task that aims to align images and language. Previous studies have focused on capturing fragment-level relations within a single modality, such as salient regions in an image or words in a sentence. However, they have overlooked instance-level interactions among multiple samples and modalities. This paper argues that understanding sample relations can help learn subtle differences for difficult instances and transfer shared knowledge for infrequent samples, leading to better overall embeddings. To address this, the authors propose a hierarchical relation modeling framework (HREM) that captures both fragment-level and instance-level relations to learn discriminative and robust cross-modal embeddings. Experimental results on the Flickr30K and MS-COCO datasets demonstrate that the proposed method outperforms state-of-the-art approaches by 4%-10% in terms of rSum. The code for the proposed method is available at https://github.com/CrossmodalGroup/HREM.