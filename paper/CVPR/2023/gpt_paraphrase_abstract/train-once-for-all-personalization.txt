We investigate the problem of training a model that can be easily personalized for different end-users without requiring additional data. One common method is to train a generic model for classifying various objects and then select the appropriate classes. However, we have found that this approach is not optimal, possibly because the model's weights are fixed and not personalized. To overcome this limitation, we propose TAPER, a framework that is trained once and can be customized for different end-users based on their task descriptions. TAPER learns a set of basis models and a mixer predictor, allowing the weights of the basis models to be dynamically combined into a personalized model based on the task description. Through extensive experiments on multiple recognition tasks, we demonstrate that TAPER consistently outperforms baseline methods in achieving higher personalized accuracy. Additionally, we show that TAPER can synthesize a much smaller model while maintaining comparable performance to a larger generic model, making it more suitable for resource-limited devices. Interestingly, even without explicit task descriptions, TAPER can adapt to the deployed context based on past predictions, further enhancing its personalization capabilities.