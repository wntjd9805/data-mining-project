This study focuses on large pre-trained vision-language models (VLMs) and the biases they exhibit due to imbalances in the training data. These biases result in unequal representation of different identity groups in the model's image and text representations, limiting their usefulness in real-world applications. To address this issue, the authors propose a new debiasing method called DEAR (Debiasing with Additive Residuals). DEAR learns additive residual image representations that offset the original representations, ensuring fair output representations that do not distinguish between different identity groups. The authors also highlight the limitations of current fairness tests, which are based on limited face image datasets and fail to indicate the applicability of specific text concepts to different identity groups. To address this gap, the authors introduce the PROTECTED ATTRIBUTE TAG ASSOCIATION (PATA) dataset, a context-based bias benchmarking dataset that provides visual context for a diverse human population in various scenarios. Experimental results demonstrate the effectiveness of the proposed framework in terms of fairness and preserving zero-shot performance across multiple datasets. The PATA dataset is also made available to the public.