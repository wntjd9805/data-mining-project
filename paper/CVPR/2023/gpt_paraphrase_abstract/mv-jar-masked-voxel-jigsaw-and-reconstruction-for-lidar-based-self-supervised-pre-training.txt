This paper presents a new method called the Masked Voxel Jigsaw and Reconstruction (MV-JAR) for self-supervised pre-training using LiDAR data. The method takes into account the voxel distributions in the scene and the local point distributions within the voxel, inspired by the hierarchy in downstream 3D object detectors. The authors address the uneven distribution of LiDAR points by employing a Reversed-Furthest-Voxel-Sampling strategy. They combine two techniques to model the distributions and achieve superior performance. The paper also introduces a new benchmark for 3D object detection on the Waymo dataset, addressing limitations in previous experiments that uniformly sample fine-tuning splits with varying data proportions. The new benchmark samples scene sequences for diverse fine-tuning splits, ensuring model convergence and accurate evaluation of pre-training methods. Experimental results on the Waymo benchmark and the KITTI dataset show that MV-JAR consistently improves 3D detection performance across different data scales, achieving up to a 6.3% increase in mAPH compared to training from scratch. The codes and benchmark are available at https://github.com/SmartBot-PJLab/MV-JAR.