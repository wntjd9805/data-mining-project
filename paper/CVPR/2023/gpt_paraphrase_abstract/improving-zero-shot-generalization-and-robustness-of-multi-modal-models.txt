Multi-modal image-text models like CLIP and LiT have achieved impressive results in image classification benchmarks, particularly in their ability to generalize to unseen classes. However, while these models have high zero-shot accuracies in the top-5 predictions, their top-1 accuracies are significantly lower, with some cases showing a gap of over 25%. This study aims to understand the reasons behind this performance gap and identifies ambiguity in text prompts as a major factor. To address this issue, the researchers propose a simple and efficient post-hoc method to identify images where the top-1 prediction is likely to be incorrect. This method measures the consistency of predictions across multiple prompts and image transformations. The results show that this procedure outperforms the popular max logit baseline in selective prediction tasks, providing better predictions for potential mistakes. Additionally, the researchers suggest a straightforward approach to improve accuracy for uncertain images by leveraging the WordNet hierarchy. They augment the original class by including its parent and children from the semantic label hierarchy and incorporate this augmentation into text prompts. Experiments are conducted on both CLIP and LiT models using five different ImageNet-based datasets. The proposed method significantly improves the top-1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set for CLIP. Furthermore, it demonstrates improvements across ImageNet shifted datasets, four other datasets, and other model architectures like LiT. Importantly, the proposed method is hyperparameter-free, requires no additional model training, and can be easily applied to other large multi-modal architectures. The code for this method is available at https://github.com/gyhandy/Hierarchy-CLIP.