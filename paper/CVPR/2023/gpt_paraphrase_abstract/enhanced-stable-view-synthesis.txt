We present a new approach to improve the synthesis of novel views from images captured by a moving camera. Our focus is on outdoor scenes, which pose challenges in accurately recovering the geometric structure and camera pose, leading to poor results with existing methods. These methods rely too heavily on multiview stereo (MVS) for geometric reconstruction, which is limited in accuracy, and assume that the camera poses computed by COLMAP are the best estimates, despite known limitations in accuracy. Our approach proposes a principled solution that combines MVS and monocular depth to improve scene depth estimation for nearby and distant points. Additionally, we refine camera poses through graph optimization, enhancing the view-dependent on-surface feature aggregation of the entire scene. Extensive evaluation on benchmark datasets, including Tanks and Temples, demonstrates significant improvements in view synthesis compared to previous methods, with a 1.5 dB increase in PSNR on Tanks and Temples. Similar improvements are observed on other benchmark datasets such as FVS, Mip-NeRF 360, and DTU.