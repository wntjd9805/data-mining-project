We present a technique for generating new views using a single wide-baseline stereo image pair. Traditional methods struggle in this scenario because they cannot accurately reconstruct the 3D geometry and are computationally expensive. To address these issues, we propose a multi-view transformer encoder, an efficient image-space sampling scheme, and a lightweight cross-attention-based renderer. These advancements allow us to train our method on a large-scale dataset of indoor and outdoor scenes. Our approach learns effective multi-view geometry priors and reduces rendering time. We extensively compare our method to previous approaches on different datasets and demonstrate superior performance in novel view synthesis and maintaining consistency across multiple views.