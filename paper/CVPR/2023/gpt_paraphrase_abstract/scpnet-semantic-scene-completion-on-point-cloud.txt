Training deep models for semantic scene completion (SSC) is difficult due to sparse and incomplete input, diverse object scales, and label noise for moving objects. To address these challenges, we propose three solutions. Firstly, we redesign the completion sub-network by introducing Multi-Path Blocks (MPBs) that aggregate multi-scale features without lossy downsampling operations. Secondly, we develop a novel knowledge distillation objective called Dense-to-Sparse Knowledge Distillation (DSKD) to transfer dense, relation-based semantic knowledge from a multi-frame teacher model to a single-frame student model. This significantly improves the representation learning of the single-frame model. Lastly, we propose a label rectification strategy that utilizes panoptic segmentation labels to remove traces of dynamic objects in completion labels, leading to improved performance for deep models, especially for moving objects. We conduct extensive experiments on two public SSC benchmarks, SemanticKITTI and SemanticPOSS. Our SCPNet achieves first place in the SemanticKITTI semantic scene completion challenge, outperforming the competitive S3CNet by 7.2 mIoU. SCP-Net also surpasses previous completion algorithms on the SemanticPOSS dataset. Additionally, our method achieves competitive results in SemanticKITTI semantic segmentation tasks, demonstrating the benefits of knowledge learned in scene completion for the segmentation task.