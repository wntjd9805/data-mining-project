This study focuses on improving the accuracy of Unsupervised Salient Object Detection (USOD) by addressing the issue of noisy labels. Current USOD methods rely on pseudo labels generated from traditional methods or pre-trained networks, which can be unreliable. Some existing methods only focus on easy samples with reliable labels, ignoring the valuable knowledge in hard samples. To overcome this limitation, the proposed method aims to extract rich and accurate saliency knowledge from both easy and hard samples. Firstly, a Confidence-aware Saliency Distilling (CSD) strategy is introduced, which assigns scores to samples based on their confidences. This strategy helps the model distill saliency knowledge progressively from easy samples to hard samples. Secondly, a Boundary-aware Texture Matching (BTM) strategy is proposed to refine the boundaries of the noisy labels. This is achieved by matching the textures around the predicted boundaries. Extensive experiments conducted on various benchmarks including RGB, RGB-D, RGB-T, and video SOD demonstrate that the proposed method achieves state-of-the-art performance in USOD. The code for implementing this method is available at www.github.com/moothes/A2S-v2.