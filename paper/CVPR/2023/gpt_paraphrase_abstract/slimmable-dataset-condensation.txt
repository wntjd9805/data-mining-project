This paper addresses the problem of dataset distillation, which involves compressing a large dataset into a smaller synthetic one. Traditional methods for dataset condensation rely on a fixed storage or transmission budget, requiring the repetition of the synthesis process when the budget changes. This can be impractical or even impossible. The authors propose a new approach called slimmable dataset condensation that allows for the extraction of a smaller synthetic dataset using only previous condensation results. They identify two key challenges in this successive compression setting: the inconsistency of neural networks across different compression times and the underdetermined solution space for synthetic data. To address these challenges, they introduce a novel training objective that explicitly considers both factors. Additionally, their method incorporates significance-aware parameterization for the synthetic datasets. Theoretical analysis shows that by discarding minor components without training, an upper-bounded error can be achieved. Alternatively, if training is allowed, this strategy can serve as a strong initialization for fast convergence. The proposed solution is evaluated through extensive comparisons and ablations on multiple benchmarks, demonstrating its superiority over existing methods.