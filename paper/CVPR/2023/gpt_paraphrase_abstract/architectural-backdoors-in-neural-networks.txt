Machine learning is susceptible to manipulation by attackers, who can manipulate data and data sampling procedures during the training stage to control the behavior of the model. One common goal of these attacks is to plant backdoors, which force the victim model to recognize a trigger known only by the adversary. This paper introduces a new type of backdoor attack that hides within the model architectures themselves, specifically in the inductive bias of the functions used for training. These backdoors are easy to implement, such as by releasing open-source code for a backdoored model architecture that others unknowingly reuse. The study shows that model architectural backdoors pose a significant threat and can survive a complete re-training of the model. The paper formalizes the key principles behind these backdoors, such as the connection between the input and output, and suggests potential protections against them. The attacks are evaluated on computer vision benchmarks of various sizes, revealing that the vulnerability is widespread across common training settings.