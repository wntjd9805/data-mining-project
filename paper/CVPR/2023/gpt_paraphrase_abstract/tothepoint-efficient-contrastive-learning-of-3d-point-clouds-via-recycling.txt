In recent years, there have been significant advancements in the field of point cloud processing, specifically in classification and segmentation. However, traditional supervised learning methods require a large amount of labeled data for training, which is time-consuming and labor-intensive to annotate. In contrast, self-supervised learning utilizes unlabeled data and pre-trains a backbone model using a pretext task to extract latent representations that can be used for downstream tasks. While self-supervised learning has been extensively explored in 2D image processing, its application in 3D point cloud data is relatively unexplored.Existing models for self-supervised learning of 3D point clouds heavily rely on a large number of data samples and demand substantial computational resources and training time. To address this challenge, we propose a novel contrastive learning approach called ToThePoint. Unlike traditional contrastive learning methods that focus on maximizing agreement between features derived from pairs of point clouds with different types of augmentation, ToThePoint also emphasizes maximizing agreement between permutation invariant features and features discarded after max pooling. We first perform self-supervised learning on the ShapeNet dataset and then evaluate the network's performance on various downstream tasks.In our experiments on the ModelNet40, ModelNet40C, Scanob-jectNN, and ShapeNet-Part datasets, ToThePoint achieves competitive or even superior results compared to state-of-the-art baselines in terms of performance. Importantly, our approach accomplishes this with significantly less training time, being 200 times faster than the baselines.In summary, our proposed method, ToThePoint, introduces a new contrastive learning approach for self-supervised learning of 3D point clouds. It demonstrates impressive performance in downstream tasks and offers a substantial reduction in training time compared to existing baselines.