Vision-language models (VLMs) that have been pre-trained on large collections of image-text pairs have shown impressive transferability across different visual tasks. This makes them a promising avenue for developing effective video recognition models. However, the current research in this area is still limited. We believe that the main value of pre-trained VLMs lies in bridging the gap between visual and textual domains. In this study, we propose a new framework called BIKE, which utilizes this cross-modal bridge to explore bidirectional knowledge. Firstly, we introduce the Video Attribute Association mechanism, which uses knowledge from videos to generate textual auxiliary attributes that complement video recognition. Secondly, we present a Temporal Concept Spotting mechanism that leverages textual expertise to capture temporal saliency in videos without the need for additional parameters, resulting in improved video representation. We conducted extensive experiments on six popular video datasets, including Kinetics-400 & 600, UCF-101, HMDB-51, ActivityNet, and Charades. The results demonstrate that our approach achieves state-of-the-art performance in various recognition scenarios, including general, zero-shot, and few-shot video recognition. Our best model achieves an accuracy of 88.6% on the challenging Kinetics-400 dataset using the CLIP model that has been released. The code for our framework is available at https://github.com/whwu95/BIKE.