We introduce a novel approach to estimate dense depth from a combination of camera images and radar point clouds. The formation process of mmWave radar point clouds is explained, highlighting the challenges posed by ambiguous elevation and noisy depth and azimuth components, which often lead to incorrect positions when projected onto the image. Existing methods have failed to address these intricacies in camera-radar fusion. Our approach is motivated by these challenges, leading us to develop a network that maps each radar point to potential surfaces in the image plane onto which it may project. Unlike previous methods, we do not treat the raw radar point cloud as a flawed depth map. Instead, we independently process each raw point to associate it with likely pixels in the image, resulting in a semi-dense radar depth map. To combine radar depth with the image, we propose a gated fusion scheme that considers the confidence scores of the correspondences. This allows us to selectively merge radar and camera embeddings, resulting in a dense depth map. We evaluate our method using the NuScenes benchmark and demonstrate a significant improvement of 10.3% in mean absolute error and 9.1% in root-mean-square error compared to the best existing method. The code for our approach is available at https://github.com/nesl/radar-camera-fusion-depth.