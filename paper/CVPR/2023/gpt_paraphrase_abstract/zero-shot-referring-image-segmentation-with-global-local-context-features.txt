We present a novel approach to referring image segmentation (RIS) that addresses the issue of expensive and labor-intensive data labeling. Our method leverages pre-trained cross-modal knowledge from CLIP to achieve zero-shot referring image segmentation. To obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures both global and local contextual information. By utilizing instance masks from off-the-shelf techniques, our method can segment detailed instance-level groundings. Additionally, we introduce a global-local text encoder that captures both sentence-level semantics and focuses on the target noun phrase. In our experiments, our approach outperforms several zero-shot baselines and even a weakly supervised method by significant margins. Our code is available at the provided GitHub link.