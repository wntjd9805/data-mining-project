The detection of 3D objects from point clouds is crucial for autonomous driving safety. While many studies have made significant progress in this area, they often face challenges such as expensive annotation costs and limited applicability to unknown data due to differences between domains. Some recent works have attempted to address domain gaps, but they have not effectively adapted to the variations in beam densities between domains, which is important for overcoming differences in LiDAR collectors. In this study, we propose a density-insensitive domain adaptation framework to address the domain gap induced by varying beam densities. Firstly, we introduce Random Beam Re-Sampling (RBRS) to improve the robustness of 3D detectors trained on the source domain against varying beam densities. We then use this pre-trained detector as the backbone model and employ a task-specific teacher-student framework to generate high-quality pseudo labels for unlabeled target domain data. To further adapt to density differences in the target domain, we feed the teacher and student branches with the same samples of different densities and utilize an Object Graph Alignment (OGA) module to establish object graphs between the two branches, ensuring consistency in attribute and relation of objects with different densities. Experimental results on three commonly used 3D object detection datasets demonstrate that our proposed domain adaptation method outperforms state-of-the-art methods, particularly for varying-density data. The code for our method is available at https://github.com/WoodwindHu/DTS.