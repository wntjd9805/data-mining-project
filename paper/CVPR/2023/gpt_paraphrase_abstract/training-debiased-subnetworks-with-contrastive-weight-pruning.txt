Neural networks often exhibit bias towards features that are falsely correlated, leading to misleading statistical evidence that cannot be generalized. This raises the question of whether an unbiased subnetwork can exist within a biased network, and if so, how to extract it. Previous observations of unbiased subnetworks have been based on ground-truth unbiased samples, but it remains unexplored how to discover optimal subnetworks with biased training datasets in practice. To address this, we highlight potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We also emphasize the importance of bias-conflicting samples in structure learning. Based on these insights, we propose the Debiased Contrastive Weight Pruning (DCWP) algorithm, which identifies unbiased subnetworks without requiring expensive group annotations. Experimental results demonstrate that our approach outperforms state-of-the-art debiasing methods, despite significantly reducing the number of parameters.