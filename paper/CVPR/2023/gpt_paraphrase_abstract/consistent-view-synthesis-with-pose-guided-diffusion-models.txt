We introduce a pose-guided diffusion model for generating consistent and high-quality novel views from a single image, addressing limitations in existing techniques. This model utilizes an attention layer that incorporates epipolar lines as constraints, enabling effective association between different viewpoints. Experimental results on both synthetic and real-world datasets validate the effectiveness of our proposed diffusion model. Our approach outperforms state-of-the-art transformer-based and GAN-based methods in terms of qualitative results, and more information can be found at https://poseguided-diffusion.github.io/. This work is significant for Virtual Reality applications, as it tackles the challenge of synthesizing novel views with a wide range of camera motion and ensures consistent and immersive experiences. The proposed technique shows promising potential for enhancing VR applications by providing a consistent long-term video of novel views from a single image.