Deciphering visual information from brain recordings is crucial for advancing our knowledge of the human visual system and establishing a strong foundation for connecting human and computer vision through the Brain-Computer Interface. However, the task of reconstructing high-quality images with accurate meaning from brain recordings is challenging due to the complex nature of brain signals and the limited availability of data annotations. In this study, we introduce MinD-Vis, a novel approach called SparseMasked Brain Modeling with Double-Conditioned LatentDiffusion Model for Human Vision Decoding. Firstly, we develop an effective self-supervised representation of fMRI data by employing mask modeling in a large latent space, which is inspired by the concept of sparse coding in the primary visual cortex. Through the augmentation of a latent diffusion model with double-conditioning, our approach demonstrates the ability to reconstruct highly plausible images from brain recordings with semantically meaningful details, even when only a small number of paired annotations are available. We conducted extensive qualitative and quantitative evaluations of our model, which revealed that it outperformed state-of-the-art methods in both semantic mapping (100-way semantic classification) and image generation quality (FID), achieving improvements of 66% and 41% respectively. Furthermore, we conducted a comprehensive ablation study to thoroughly analyze the effectiveness of our framework.