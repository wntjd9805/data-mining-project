The problem of class incremental learning (CIL) is addressed in this study. Current approaches utilize network expansion (NE) to add a task expert per task, but this leads to models that grow quickly with the number of tasks. To address this, a new method called dense network expansion (DNE) is proposed. DNE introduces dense connections between intermediate layers of task expert networks, allowing for knowledge transfer via feature sharing and reusing. This is achieved through a cross-task attention mechanism called task attention block (TAB), which fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the feature mixing level and is separate from spatial attentions. The DNE approach maintains the feature space of old classes while growing the network and feature scale at a slower rate than previous methods. It outperforms state-of-the-art methods by 4% in accuracy, with similar or smaller model scale.