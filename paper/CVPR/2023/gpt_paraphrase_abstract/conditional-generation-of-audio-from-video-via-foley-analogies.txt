Designers add sound effects to videos to create a specific artistic effect, which may differ from the actual sound in the scene. To address the challenge of creating a soundtrack that matches the on-screen actions but deviates from the true sound, we introduce the concept of conditional Foley. We propose a pretext task to train our model in predicting sound for a video clip by using a conditional audio-visual clip from the same source video. Additionally, we present a model that generates a soundtrack for a silent video based on a user-provided example of how the video should sound. Our model successfully generates sound from videos and adapts its output based on the supplied example, as demonstrated through human studies and automated evaluation metrics. More information can be found on our project site: https://xypb.github.io/CondFoleyGen.