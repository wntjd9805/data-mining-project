As the use of black-box models becomes more prevalent in high-stakes applications, various data-driven explanation methods have been introduced. However, machine learning models face constant challenges due to changes in data distribution. This raises the question of whether data-driven explanations are reliable when faced with out-of-distribution data. Our empirical findings reveal that even though models may predict accurately, their explanations can still be unreliable under distributional shifts. To tackle this issue, we propose a model-agnostic learning framework called Distributionally Robust Explanations (DRE). Inspired by self-supervised learning, our approach leverages inter-distribution information to provide supervisory signals for learning explanations without human annotation. We investigate whether robust explanations can enhance the model's generalization capability through extensive experiments across various tasks and data types, including image classification, regression, and scientific tabular data. Our results demonstrate that the proposed method significantly improves the model's performance in terms of explanation and prediction robustness against distributional shifts.