The recent popularity of 4D point cloud sequences has sparked interest in finding efficient ways to utilize unlabeled data due to the expensive and labor-intensive nature of obtaining labeled 4D datasets. Existing self-supervised point cloud representation learning methods focus solely on static snapshots, disregarding the potential for comprehensive geometric details revealed through sequential observations of dynamic scenes. To address this limitation, this study introduces a novel 4D self-supervised pre-training method called Complete-to-Partial 4D Distillation. The proposed approach treats 4D representation learning as a teacher-student knowledge distillation framework, enabling the student to acquire valuable 4D representations under the guidance of the teacher. Experimental results demonstrate that this method outperforms previous pre-training approaches across various tasks related to understanding 4D point cloud sequences. The code for this method is available at: https://github.com/dongyh20/C2P.