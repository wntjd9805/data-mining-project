This paper demonstrates that fine-tuning a neural captioning model with a self-supervised discriminative communication objective improves the generation of informative and visually descriptive captions. The goal is to train the model to produce captions that allow a text-conditioned image retriever to accurately identify the corresponding image from a set of candidates. The popular ClipCap captioner is used for experiments, with similar results replicated using BLIP. While the discriminatively fine-tuned captions may slightly lag behind non-fine-tuned captions in similarity to human references when trained and tested on the same dataset, they outperform the non-fine-tuned captions when generating captions for out-of-domain datasets. Additionally, on the Conceptual Captions dataset, the discriminatively fine-tuned captions are more helpful for human annotators in an image discrimination task compared to both vanilla ClipCap captions and ground-truth captions.