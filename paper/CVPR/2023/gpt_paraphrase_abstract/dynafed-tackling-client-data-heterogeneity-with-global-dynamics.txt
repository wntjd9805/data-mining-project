The Federated Learning (FL) paradigm encounters challenges when dealing with different client data. Training on non-iid distributed data leads to local models deviating from each other and degrading the performance of the global model. One solution is to gather all client data on the server, but this compromises privacy and contradicts the purpose of FL. This paper proposes a method called DYNAFED to collect and utilize global knowledge without compromising data privacy. The method involves reserving a short trajectory of global model snapshots on the server and synthesizing a small pseudo dataset that mimics the dynamics of the reserved trajectory. This synthesized data is then used to aggregate the deflected clients into the global model. DYNAFED has several advantages: it does not require an external on-server dataset, it can take effect early in the training process, and the synthesized data only needs to be generated once and can be used in subsequent rounds. Extensive experiments demonstrate the effectiveness of DYNAFED, and the underlying mechanism of the method is also explored.