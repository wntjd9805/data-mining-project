This study focuses on human multimodal emotion recognition (MER), which involves perceiving human emotions through language, visual, and acoustic cues. While previous MER approaches have shown impressive performance, the inherent differences between modalities still pose challenges, and the contribution of each modality varies significantly. To address this issue, the researchers propose a decoupled multimodal distillation (DMD) approach that enables flexible and adaptive crossmodal knowledge distillation to enhance the discriminative features of each modality.  In the DMD approach, the representation of each modality is separated into two parts: modality-irrelevant and modality-exclusive spaces. This separation is done in a self-regression manner. Each part undergoes a graph distillation unit (GD-Unit) to perform specialized and effective knowledge distillation. The GD-Unit consists of a dynamic graph, where each vertex represents a modality and each edge indicates dynamic knowledge distillation. This approach allows for flexible knowledge transfer patterns, with distillation weights automatically learned.  Experimental results demonstrate that the DMD approach consistently outperforms state-of-the-art MER methods. Visualization results show that the graph edges in DMD exhibit meaningful distributional patterns in relation to the modality-irrelevant and modality-exclusive feature spaces. The researchers have released the codes for DMD on GitHub.  Figure 1 illustrates the significant discrepancies in emotion recognition when using unimodality, the conventional cross-modal distillation approach, and the proposed DMD method. The DMD method includes two graph distillation units: homogeneous GD and heterogeneous GD. The decoupled GD paradigm reduces the burden of absorbing knowledge from the heterogeneous data and allows for specialized and effective knowledge distillation.