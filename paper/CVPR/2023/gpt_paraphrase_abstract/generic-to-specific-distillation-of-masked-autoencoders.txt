Large vision Transformers (ViTs) have made remarkable progress through self-supervised pre-training. However, lightweight ViT models, limited by their lower capacity, can benefit from training mechanisms. Knowledge distillation offers a way to transfer knowledge from large teacher models to smaller student models. Nonetheless, conventional single-stage distillation struggles to retain task-agnostic knowledge crucial for model generalization.To address this limitation, we propose a new approach called generic-to-specific distillation (G2SD) to unlock the potential of small ViT models. G2SD leverages large models pre-trained by masked autoencoders to supervise the training of small models. In the generic distillation phase, the decoder of the small model is encouraged to align its feature predictions with the hidden representations of the large model. This facilitates the transfer of task-agnostic knowledge. In the specific distillation phase, the predictions of the small model are constrained to be consistent with those of the large model, enabling the transfer of task-specific features that ensure task performance.Using G2SD, the vanilla ViT-Small model achieves impressive results. It achieves 98.7%, 98.1%, and 99.3% of the performance of its teacher model (ViT-Base) in image classification, object detection, and semantic segmentation tasks, respectively. This sets a strong baseline for two-stage vision distillation. The code for G2SD can be accessed at https://github.com/pengzhiliang/G2SD.