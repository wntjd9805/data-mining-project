Current diffusion models are highly effective for generating images, both conditionally and unconditionally. However, these models lack the capability to support tasks related to 3D understanding, such as generating 3D objects that are consistent from different viewpoints or reconstructing objects from a single view. This paper introduces RenderDiffusion, the first diffusion model designed specifically for 3D generation and inference. The model is trained using only 2D supervision from monocular images. The key component of our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This ensures a robust and coherent 3D representation throughout the diffusion process, despite the use of 2D supervision. The resulting 3D representation can be rendered from any viewpoint. We evaluate the performance of RenderDiffusion on various datasets, including FFHQ, AFHQ, ShapeNet, and CLEVR. Our results demonstrate competitive performance in generating and inferring 3D scenes from 2D images. Furthermore, our diffusion-based approach enables us to utilize 2D inpainting for editing 3D scenes. For more information, please refer to our project page: https://github.com/Anciukevicius/RenderDiffusion.