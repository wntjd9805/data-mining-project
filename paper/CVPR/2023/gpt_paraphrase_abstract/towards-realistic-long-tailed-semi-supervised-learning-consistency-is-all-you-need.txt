Existing long-tailed semi-supervised learning (LTSSL) algorithms assume that the class distributions of labeled and unlabeled data are similar, which can lead to poor performance when the distributions are mismatched. To address this issue, we propose a new method called adaptive consistency regularizer (ACR) that effectively utilizes unlabeled data with unknown class distributions. ACR dynamically refines pseudo-labels based on estimated true class distributions of unlabeled data. Despite its simplicity, ACR achieves state-of-the-art performance on standard LTSSL benchmarks, showing a significant increase in test accuracy when the class distributions are mismatched. Even when the class distributions are identical, ACR outperforms sophisticated LTSSL algorithms consistently. We conducted extensive ablation studies to identify the key factors contributing to ACR's success. The source code for ACR is available at https://github.com/Gank0078/ACR.