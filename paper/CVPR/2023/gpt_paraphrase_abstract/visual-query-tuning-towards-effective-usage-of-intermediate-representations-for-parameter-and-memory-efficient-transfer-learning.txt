Visual query tuning (VQT) is a proposed method for effectively utilizing the intermediate features of Vision Transformers in downstream tasks. By introducing a small number of learnable "query" tokens to each layer, VQT leverages the inner workings of Transformers to summarize the rich intermediate features of each layer. These summarized features can then be used to train the prediction heads of downstream tasks. Unlike other fine-tuning approaches that require back-propagation through the entire backbone, VQT only learns to combine the intermediate features, resulting in memory efficiency during training. VQT complements other parameter-efficient fine-tuning approaches in transfer learning. In empirical evaluations, VQT consistently outperforms the current state-of-the-art approach that uses intermediate features for transfer learning and performs better than full fine-tuning in many cases. Furthermore, VQT achieves higher accuracy under memory constraints compared to approaches that adapt features. Importantly, VQT is compatible with these approaches, allowing for an additional boost in transfer learning. The code for VQT is available at https://github.com/andytu28/VQT.