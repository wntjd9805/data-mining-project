Contrastive Language-Image Pre-training has been successful in open-world vision tasks using unlabeled text-image pairs. However, applying this approach to 3D space is challenging due to limited Text-3D data pairs. Existing methods use intermediate 2D representations for 3D data, but this sacrifices 3D geometry information. To address this, we propose Contrastive Language-Image-Point Cloud Pretraining (CLIP2) that directly learns transferable 3D point cloud representation in realistic scenarios. We use correspondences in 2D and 3D scenarios to build well-aligned text-image-point proxies. A cross-modal contrastive objective is used to learn semantic and instance-level aligned point cloud representation. Experimental results demonstrate the transferability of our learned 3D representation in indoor and outdoor scenarios, improving state-of-the-art methods in zero-shot and few-shot 3D recognition. We also analyze different representations in real scenarios and present an optional ensemble scheme.