The objective of text-to-image person retrieval is to identify a specific person based on a textual description query. The main challenge lies in mapping visual and textual information into a shared latent space. Previous approaches have utilized separately pre-trained models for visual and textual features extraction, but they lack the necessary alignment capabilities for effective multimodal data matching. Additionally, these approaches rely on prior information to explore explicit part alignments, which can distort intra-modality information. To address these issues, we propose IRRA, a cross-modal framework that learns relations between local visual-textual tokens and improves global image-text matching without requiring additional supervision. Firstly, we introduce an ImplicitRelation Reasoning module that integrates visual cues into textual tokens using a cross-modal multimodal interaction encoder. This allows for cross-modal interaction and enhances alignment. Secondly, we propose Similarity Distribution Matching to globally align visual and textual embeddings by minimizing the KL divergence between image-text similarity distributions and normalized label matching distributions. Our method achieves state-of-the-art results on all three public datasets, outperforming previous methods by a significant margin of 3%-9% in Rank-1 accuracy. Figure 1 illustrates the evolution of text-to-image person retrieval paradigms, highlighting the progression from early global matching methods to recent explicit local matching methods, and finally to our implicit relation reasoning aided matching paradigm.