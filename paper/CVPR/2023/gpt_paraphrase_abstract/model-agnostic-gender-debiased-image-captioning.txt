This study focuses on addressing gender bias in image captioning models, which have been found to perpetuate and amplify societal biases present in the training data. Previous approaches have attempted to reduce gender misclassification by emphasizing the importance of people in the images, but this has led to the generation of gender-stereotypical words at the expense of accurately predicting gender. Based on this observation, the researchers propose that there are two types of gender bias affecting image captioning models: 1) bias that relies on context to predict gender, and 2) bias in the probability of generating certain gender-related words, often stereotypical ones. To mitigate both types of bias, the researchers introduce a framework called LIBRA. This framework learns from artificially biased samples to decrease both types of biases, correcting gender misclassification and replacing gender-stereotypical words with more neutral alternatives.