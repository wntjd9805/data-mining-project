3D semantic segmentation on large-scale point clouds is crucial for autonomous systems. Unlike single-scan segmentation, this task requires identifying both the semantic categories and motion states of points. However, existing methods designed for single-scan segmentation perform poorly on multi-scan tasks due to the lack of effective temporal information integration. To address this, we propose MarS3D, a motion-aware module that can be added to single-scan models to enable them with multi-scan perception capabilities. MarS3D consists of two key components: the Cross-Frame Feature Embedding module for enhanced representation learning and the Motion-Aware Feature Learning module for improved motion awareness. Extensive experiments demonstrate that MarS3D significantly enhances the performance of the baseline model. The code for MarS3D is publicly available at https://github.com/CVMI-Lab/MarS3D.