Autoregressive transformers have been successful in generating videos, but their ability to learn long-term dependencies is limited due to the quadratic complexity of self-attention. They also suffer from slow inference time and error propagation. To address these issues, we propose a new approach called Memory-efficient Bidirectional Transformer (MeBT) that allows for end-to-end learning of long-term dependencies in videos and faster inference. MeBT is based on recent advancements in bidirectional transformers and can decode the entire spatio-temporal volume of a video in parallel from partially observed patches. By projecting observable context tokens into a fixed number of latent tokens and using cross-attention to decode the masked tokens, MeBT achieves linear time complexity in both encoding and decoding. With its linear complexity and bidirectional modeling, our method significantly improves upon autoregressive transformers in terms of generating moderately long videos in terms of quality and speed. For more information, including videos and code, please visit https://sites.google.com/view/mebt-cvpr2023.