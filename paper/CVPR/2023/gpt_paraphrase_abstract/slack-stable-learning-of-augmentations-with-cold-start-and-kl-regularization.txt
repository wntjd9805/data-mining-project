Automatic data augmentation is a technique that improves the performance of neural networks by applying a set of transformations to the input data. However, the selection of these transformations is typically done manually. This paper introduces a new approach to automatic data augmentation that does not rely on prior knowledge or pretraining. Instead, the augmentation policy is learned directly, which presents challenges due to the larger search space and instability of optimization algorithms. To address these challenges, the authors propose a successive cold-start strategy with Kullback-Leibler regularization and parameterize magnitudes as continuous distributions. Despite the more difficult setting, their approach achieves competitive results on standard benchmarks and can be applied beyond natural images.