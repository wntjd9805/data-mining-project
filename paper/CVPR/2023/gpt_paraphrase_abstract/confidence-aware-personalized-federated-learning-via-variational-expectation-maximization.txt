Federated Learning (FL) is a distributed learning method that trains a shared model across multiple clients. A common challenge in FL is that clients may have different and non-identically distributed data sets. Personalized Federated Learning (PFL) addresses this challenge by using locally adapted models. In this study, we propose a new framework for PFL using hierarchical Bayesian modeling and variational inference. We introduce a global model as a latent variable to capture the common trends among clients' parameters. We optimize the model by maximizing the marginal likelihood through variational expectation maximization. Our algorithm provides a confidence value that represents the uncertainty of clients' parameters and local model deviations from the global model. This confidence value is used to weigh clients' parameters during aggregation and adjust the regularization effect of the global model. We conducted extensive empirical studies on multiple datasets to evaluate our method. The results show that our approach performs well in mildly heterogeneous circumstances and significantly outperforms other state-of-the-art PFL frameworks in highly heterogeneous settings.