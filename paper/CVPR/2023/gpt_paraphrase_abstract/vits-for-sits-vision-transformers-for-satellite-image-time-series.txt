This paper introduces the Temporo-Spatial VisionTransformer (TSViT), a fully-attentional model designed for processing general Satellite Image Time Series (SITS) using the Vision Transformer (ViT) framework. TSViT divides a SITS record into non-overlapping patches in both spatial and temporal dimensions. These patches are tokenized and then processed using a factorized temporo-spatial encoder. The paper argues that a temporal-then-spatial factorization is more intuitive for SITS processing compared to natural images, and provides experimental evidence to support this claim. Furthermore, the model's discriminative power is enhanced by introducing two new mechanisms: acquisition-time-specific temporal positional encodings and multiple learnable class tokens. An extensive ablation study is conducted to evaluate the impact of these novel design choices. The proposed architecture achieves state-of-the-art performance, outperforming previous approaches by a significant margin on three publicly available SITS semantic segmentation and classification datasets. The model, training, and evaluation codes can be accessed at https://github.com/michaeltrs/DeepSatModels.