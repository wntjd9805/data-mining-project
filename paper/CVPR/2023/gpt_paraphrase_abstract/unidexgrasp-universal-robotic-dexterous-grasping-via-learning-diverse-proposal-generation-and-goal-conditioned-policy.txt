This study addresses the issue of learning universal robotic dexterous grasping in a table-top environment using point cloud observations. The aim is to grasp and lift objects in various ways across multiple categories, including unseen objects. The approach is inspired by successful parallel gripper grasping pipelines and involves two stages: grasp proposal generation and goal-conditioned grasp execution. In the first stage, a novel probabilistic model is proposed to determine grasp poses based on the point cloud observation, considering rotation, translation, and articulation. This model is trained on a large-scale dexterous grasp dataset, allowing for the generation of diverse and high-quality grasp poses for object point clouds. In the second stage, motion planning is replaced with a goal-conditioned grasp policy to handle the complexity of dexterous grasp execution. Learning this generalizable grasp policy without oracle states is challenging, so several innovations are proposed, including state canonicalization, object curriculum, and teacher-student distillation. By integrating these stages, the final pipeline achieves universal generalization for dexterous grasping, with an average success rate of over 60% on thousands of object instances. This performance significantly surpasses all baselines while exhibiting only minimal generalization gap.