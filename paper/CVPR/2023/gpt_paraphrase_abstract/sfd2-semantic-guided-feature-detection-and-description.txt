Visual localization is a crucial task for autonomous driving and robotics. Current methods extract a large number of locally reliable features, which leads to limited efficiency and accuracy in challenging environments. Instead, we propose a method that extracts globally reliable features by incorporating high-level semantics into both the detection and description processes. Our semantic-aware detector can identify keypoints in reliable regions (such as buildings and traffic lanes) and suppress unreliable areas (such as the sky and cars) without the need for explicit semantic labels. This improves the accuracy of keypoint matching by reducing the impact of appearance changes and eliminates the need for additional segmentation networks during testing. Additionally, our descriptors are enriched with semantics, making them more discriminative and resulting in more consistent matches during testing. Experimental results on the Aachen Day-Night and RobotCar-Seasons datasets demonstrate that our model outperforms previous local features and achieves competitive accuracy compared to advanced matchers. Furthermore, our model is significantly faster, performing 2 to 3 times faster when using 2k and 4k keypoints, respectively. The code for our model is available at https://github.com/feixue94/sfd2.