The effectiveness of formula-driven supervised learning (FDSL) for pre-training vision transformers has been demonstrated, with ExFractalDB-21k surpassing the pre-training impact of ImageNet-21k. Previous studies have suggested that contours are more important than textures in pre-training vision transformers. However, the lack of a systematic exploration into why contour-oriented synthetic datasets can achieve comparable accuracy to real datasets leaves room for doubt. In this study, we introduce a new approach based on circular harmonics to systematically investigate the design space of contour-oriented synthetic datasets. This enables us to efficiently search for optimal FDSL parameters and maximize the diversity of synthetic images in the dataset, which we found to be crucial. When we utilize the resulting dataset, VisualAtom-21k, to pre-train ViT-Base, we achieve a top-1 accuracy of 83.7% when fine-tuning on ImageNet-1k. This is only a 0.5% difference from the top-1 accuracy (84.2%) achieved by JFT-300M pre-training, despite the images in VisualAtom-21k being 1/14th the scale. Unlike JFT-300M, which is a static dataset, the quality of synthetic datasets will continue to improve, and our work demonstrates the potential for this. FDSL also circumvents common issues associated with real images, such as privacy/copyright concerns, labeling costs/errors, and ethical biases.