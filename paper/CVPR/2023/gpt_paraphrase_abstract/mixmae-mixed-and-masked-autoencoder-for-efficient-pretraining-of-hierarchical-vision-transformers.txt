This paper introduces MixMAE, a pretraining method suitable for hierarchical Vision Transformers. While existing masked image modeling (MIM) methods replace input tokens with a [MASK] symbol to reconstruct the original image, this approach slows down training and creates inconsistencies. On the other hand, MAE doesn't use [MASK] tokens but isn't compatible with hierarchical Vision Transformers. To address these issues and speed up pretraining, MixMAE replaces masked tokens with visible tokens from another image, creating a mixed image. Dual reconstruction is then performed to reconstruct the original images from the mixed input, leading to improved efficiency. The effectiveness of MixMAE is demonstrated using the Swin Transformer with a large window size and up to 600M parameters. Experimental results show that MixMAE achieves 85.1% top-1 accuracy on ImageNet-1K after pretraining for 600 epochs. Additionally, its transfer performance on 6 other datasets indicates that MixMAE offers a better tradeoff between FLOPs and performance compared to previous MIM methods.