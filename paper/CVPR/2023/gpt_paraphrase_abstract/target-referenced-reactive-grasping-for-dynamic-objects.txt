Reactive grasping, the ability of robots to successfully grasp moving objects, is a topic of great interest in robotics. Current approaches primarily focus on the smoothness of predicted grasp poses over time, but neglect the semantic consistency of these poses. As a result, the predicted grasps may not be consistent with the same object, especially in cluttered environments. In this study, we propose a solution to reactive grasping by utilizing target-referenced tracking through generated grasp spaces. Our method consists of two stages: 1) discovering correspondences between grasp poses using an attentional graph neural network and selecting the pose with the highest similarity to the target pose, and 2) refining the selected grasp poses based on target and historical information. We evaluate our method using the GraspNet-1Billion benchmark and conduct real robot experiments with 30 dynamic object scenes. The results demonstrate that our method outperforms other approaches, with an average success rate of over 80 percent. Code and demos can be accessed at https://graspnet.net/reactive.