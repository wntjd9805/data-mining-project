Dual-pixel photography is a type of photography that uses an ultra-high resolution to capture both RGB and depth information. However, there are challenges in fully utilizing this technology. Unlike traditional stereo photography, dual-pixel photography has a bidirectional disparity that includes positive and negative values depending on the depth of focus. Additionally, capturing a wide range of dual-pixel disparity results in a severely blurred image, which affects the accuracy of depth estimation. To address these challenges, data-driven approaches have been proposed, but they often estimate either inverse depth or blurriness maps due to the lack of ground-truth datasets. In this study, we propose a self-supervised learning method that leverages the anisotropic blur kernels present in dual-pixel photography. By exploiting the reflective-symmetric nature of the dual-pixel left/right images, we can estimate bidirectional disparity without the need for a training dataset. Our method uses a novel kernel-split symmetry loss to account for this phenomenon and outperforms existing dual-pixel methods in generating accurate disparity maps based on focus-plane depth.