Pruning, a popular method of compressing neural network models by setting a subset of parameters to zero, has been found to potentially introduce or worsen bias in the model's output. However, the relationship between pruning and induced bias is not well understood. This study aims to systematically investigate and characterize this phenomenon in Convolutional Neural Networks used in computer vision. The researchers demonstrate that it is possible to obtain highly sparse models with less than 10% remaining weights that maintain accuracy and minimize bias compared to dense models. However, they also find that as sparsity increases, pruned models exhibit higher uncertainty in their outputs and increased correlations, directly leading to increased bias. The study introduces criteria that can predict whether bias will increase with pruning based solely on the uncompressed model and identifies the samples most susceptible to biased predictions after compression. The code for this research can be accessed at https://github.com/IST-DASLab/pruned-vision-model-bias.