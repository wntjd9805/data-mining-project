REACT is a framework that aims to build customized visual models for specific domains by acquiring relevant web knowledge. Unlike models like CLIP, which rely on a web-scale data collection process and expensive pre-training to incorporate broad concept coverage, REACT retrieves a small portion of image-text pairs from the web-scale database as external knowledge. The model is then customized by training new modularized blocks while keeping the original weights frozen. Extensive experiments on various tasks, including classification, retrieval, detection, and segmentation, were conducted to demonstrate the effectiveness of REACT. In zero-shot classification, REACT outperformed CLIP by achieving a 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark (20 datasets).