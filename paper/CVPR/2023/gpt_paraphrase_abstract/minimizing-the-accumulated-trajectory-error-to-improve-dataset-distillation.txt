The success of model-based deep learning relies on large-scale real-world data, but processing such data comes with high computational, storage, and training costs. To address this, dataset distillation has emerged as a method to distill information from large datasets into smaller synthetic datasets that can achieve similar performance. Current methods match gradients between real and synthetic data but suffer from accumulated trajectory error during evaluation. To mitigate this, we propose a novel approach called Flat Trajectory Distillation (FTD) that encourages the optimization algorithm to seek a flat trajectory. We show that weights trained on synthetic data are robust against accumulated errors with the regularization towards a flat trajectory. FTD improves the performance of gradient-matching methods by up to 4.7% on a subset of high-resolution images from the ImageNet dataset. We also demonstrate the effectiveness and generalizability of FTD on datasets of different resolutions and its applicability to neural architecture search. Code for FTD is available at https://github.com/AngusDujw/FTD-distillation.