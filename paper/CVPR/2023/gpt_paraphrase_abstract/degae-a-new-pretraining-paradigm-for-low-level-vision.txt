The effectiveness of self-supervised pretraining in high-level vision has been widely acknowledged, but its application in low-level vision is still uncertain and lacks a solid foundation. This paper aims to address the fundamental questions regarding the purpose and challenges of pretraining in low-level vision. The authors examine previous pretraining methods in both high-level and low-level vision and classify current low-level vision tasks into two categories based on the difficulty of acquiring data: low-cost and high-cost tasks. Previous research has predominantly focused on pretraining for low-cost tasks, which has resulted in limited performance improvements. However, the authors argue that pretraining is more crucial for high-cost tasks, which involve more challenging data acquisition. To enhance the performance of various low-level vision tasks, the authors propose a novel pretraining approach called degradation autoencoder (DegAE). DegAE is specifically designed for low-level vision and follows the philosophy of creating pretext tasks for self-supervised pretraining. By utilizing DegAE pretraining, SwinIR achieves a 6.88dB improvement in image dehaze task, while Uformer demonstrates performance gains of 3.22dB and 0.54dB in dehaze and derain tasks, respectively.