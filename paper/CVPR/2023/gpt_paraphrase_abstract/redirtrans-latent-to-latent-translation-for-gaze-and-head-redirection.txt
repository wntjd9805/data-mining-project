Learning-based gaze estimation methods require a large amount of training data with accurate gaze annotations. Several image synthesis methods have been proposed to address the challenges of collecting and annotating gaze data. However, these methods have mainly focused on changing gaze directions in images that only include eyes or restricted ranges of faces with low resolution. This limits their applicability in various scenarios. To overcome this limitation, we propose a portable network called ReDirTrans, which achieves latent-to-latent translation for redirecting gaze directions and head orientations in an interpretable manner. ReDirTrans projects input latent vectors into aimed-attribute embeddings and redirects these embeddings with assigned pitch and yaw values. The initial and edited embeddings are then projected back to the initial latent space as residuals to modify the input latent vectors, representing the removal of the old status and addition of the new status. This approach mitigates impacts on other attributes and the distribution of latent vectors. By combining ReDirTrans with a pretrained fixed e4e-StyleGAN pair, we create ReDirTrans-GAN, which accurately redirects gaze in full-face images with high resolution while preserving other attributes such as identity, expression, and hairstyle. Additionally, we present improvements for the downstream learning-based gaze estimation task by using redirected samples as dataset augmentation.