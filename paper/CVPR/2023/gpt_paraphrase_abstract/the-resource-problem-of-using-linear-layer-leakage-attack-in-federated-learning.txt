Secure aggregation in federated learning aims to protect privacy by ensuring that a server can only access a decrypted combined update. Existing linear layer leakage methods are the only data reconstruction attacks that can scale and achieve a high leakage rate, regardless of the number of clients or batch size. However, these methods require injecting a large fully-connected layer, resulting in increased resource overhead as the number of clients increases. We argue that this resource overhead is due to a flawed perspective in previous work, which treats attacking an aggregate update the same as attacking an individual update with a larger batch size. Instead, we propose approaching the attack from the viewpoint of combining multiple individual updates, allowing the use of sparsity to reduce resource overhead. Our findings demonstrate that employing sparsity can decrease model size overhead by over 327 times and computation time by 3.34 times compared to the state-of-the-art, while maintaining an equivalent total leakage rate of 77% even with 1000 clients in aggregation.