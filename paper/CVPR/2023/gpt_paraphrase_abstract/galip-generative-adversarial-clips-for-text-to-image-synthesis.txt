Generating high-quality, complex images from text is a difficult task. Existing models, such as autoregressive and diffusion models, have made progress in synthesizing realistic images through extensive pretraining. However, these models suffer from three main drawbacks. Firstly, they require a large amount of training data and parameters to achieve good performance. Secondly, their multi-step generation process significantly slows down image synthesis. Lastly, controlling the synthesized visual features is challenging and necessitates carefully designed prompts.To address these limitations and enable efficient, fast, and controllable text-to-image synthesis, we propose a novel approach called Generative Adversarial CLIPs (GALIP). GALIP leverages the power of the pretrained CLIP model in both the discriminator and generator components. Specifically, we introduce a CLIP-based discriminator that utilizes CLIP's strong scene understanding capabilities to accurately evaluate image quality. Additionally, we propose a CLIP-empowered generator that incorporates visual concepts from CLIP through bridge features and prompts. By integrating CLIP into both the generator and discriminator, our model achieves improved training efficiency, requiring only 3% of the training data and 6% of the learnable parameters compared to large pretrained autoregressive and diffusion models. Despite this reduction, GALIP achieves comparable results in terms of image quality. Furthermore, our model achieves approximately 120 times faster synthesis speed and inherits the smooth latent space characteristic of GANs.Extensive experimental results demonstrate the outstanding performance of GALIP. The code for our model is publicly available at https://github.com/tobran/GALIP.