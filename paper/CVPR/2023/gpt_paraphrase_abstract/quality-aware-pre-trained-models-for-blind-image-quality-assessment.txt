Blind image quality assessment (BIQA) has seen advancements in recent years with deep learning-based methods improving the performance of automatically evaluating the perceived quality of a single image. However, the limited availability of labeled data hampers the full potential of deep learning-based BIQA methods. To address this issue, we propose a self-supervised learning approach with a pretext task customized for BIQA, allowing for learning representations from a significantly larger amount of data. To guide the learning process, we introduce a quality-aware contrastive loss that assumes patches from a distorted image should have similar quality but differ from patches with different degradations and from patches of different images. Additionally, we enhance the existing degradation process to create a degradation space with a large size. By pre-training our method on ImageNet, the models become more sensitive to image quality and show significant improvements in downstream BIQA tasks. Experimental results demonstrate the remarkable enhancements achieved by our method on popular BIQA datasets.