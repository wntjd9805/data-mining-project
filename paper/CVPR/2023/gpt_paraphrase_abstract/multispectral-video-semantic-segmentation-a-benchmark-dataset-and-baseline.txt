Semantic segmentation is crucial for applications like autonomous driving and nighttime rescue. Most existing approaches use RGB images as input, but they struggle in adverse weather conditions. To address this, researchers have started investigating multispectral semantic segmentation, which combines RGB and thermal infrared images. However, current methods focus on single RGBT image input and struggle with dynamic real-world scenes. In this paper, we introduce a new task called Multispectral Video Semantic Segmentation (MVSS) and create a dataset called MVSeg consisting of RGB and thermal videos with pixel-level annotations. We also propose a baseline model called MVNet that learns semantic representations from multispectral and temporal contexts. We conduct comprehensive experiments on the MVSeg dataset and show that using multispectral video input significantly improves semantic segmentation, and our MVNet baseline is effective.