In this study, we address the limitations of existing audio-visual event localization (AVE) methods, which only work with manually trimmed videos containing a single event. We propose a new approach called dense-localizing audio-visual events that aims to identify and recognize all audio-visual events in untrimmed videos. This task is challenging as it requires a detailed understanding of audio-visual scenes and context. To overcome this challenge, we introduce the Untrimmed Audio-Visual (UnAV-100) dataset, which consists of 10,000 untrimmed videos containing over 30,000 audio-visual events. On average, each video has 2.8 events, and these events are often related and can co-occur. We develop a learning-based framework that combines both audio and visual information to accurately localize events of varying lengths and capture dependencies between them. Our experiments demonstrate the effectiveness of our method, highlighting the importance of multi-scale cross-modal perception and dependency modeling. The UnAV-100 dataset and code are publicly available at https://unav100.github.io.