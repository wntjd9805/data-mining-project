Recent studies have demonstrated the potential of utilizing unstructured text data from online sources as auxiliary information for zero-shot image classification. However, these methods typically rely on high-quality sources like Wikipedia and are limited to a single source of information. On the other hand, Large Language Models (LLM) trained on vast amounts of web text have exhibited impressive capabilities in repurposing their acquired knowledge for various tasks.In this research, we propose a novel approach that leverages an LLM to provide text supervision for a zero-shot image classification model. Our method involves training the LLM using a few text descriptions provided by different annotators as examples. By conditioning the LLM on these examples, we enable it to generate multiple text descriptions (referred to as views) for each class. Our proposed model, called I2MVFormer, learns multi-view semantic embeddings for zero-shot image classification based on these class views.We demonstrate that each text view of a class offers complementary information, which enables the model to learn highly discriminative class embeddings. Furthermore, our experiments show that I2MVFormer outperforms baseline models in effectively utilizing the multi-view text supervision from the LLM. In fact, I2MVFormer achieves a new state-of-the-art performance on three widely-used benchmark datasets for zero-shot image classification with unsupervised semantic embeddings.For those interested, the code implementation of I2MVFormer is available at https://github.com/ferjad/I2DFormer.