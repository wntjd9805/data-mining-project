Batch Normalization (BN) has been extensively studied in computer vision tasks, but little research has focused on its impact on continual learning. In this study, we propose a new update patch for BN specifically designed for exemplar-based class-incremental learning (CIL). The main challenge of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, leading to biased mean, variance, and affine transformation parameters towards the current task and causing forgetting of past tasks. While a recent BN variant has been developed for "online" CIL, our experiments show that it is not effective for "offline" CIL, where multiple epochs are used on imbalanced training data. This ineffectiveness arises from not fully addressing the data imbalance issue, particularly in computing gradients for learning the affine transformation parameters of BN. To address this, we propose a hyperparameter-free variant called Task-Balanced BN (TBBN), which resolves the imbalance issue by creating a horizontally-concatenated task-balanced batch using reshape and repeat operations during training. Our experiments on CIFAR-100, ImageNet-100, and five dissimilar task datasets demonstrate that TBBN, which functions the same as vanilla BN during inference, can be easily applied to existing exemplar-based offline CIL algorithms and consistently outperforms other BN variants.