Deep Stereo Matching (SM) networks have gained attention in computer vision for their impressive performance. However, these networks often learn dataset-specific shortcuts and struggle to generalize to unseen realistic datasets. This paper focuses on training robust models for the domain generalized SM task by learning shortcut-invariant representations from synthetic data to address domain shifts. The proposed Hierarchical Visual Transformation (HVT) network transforms training samples into new domains with diverse distributions at three levels: Global, Local, and Pixel. It then maximizes the visual discrepancy between the source domain and new domains while minimizing cross-domain feature inconsistency to capture domain-invariant features. This prevents the model from exploiting artifacts in synthetic stereo images as shortcut features, resulting in more effective estimation of disparity maps based on robust and shortcut-invariant representations. The HVT network is integrated with state-of-the-art SM networks and evaluated on public benchmark datasets. Extensive experiments demonstrate that the HVT network significantly improves the performance of existing SM networks in synthetic-to-realistic domain generalization.