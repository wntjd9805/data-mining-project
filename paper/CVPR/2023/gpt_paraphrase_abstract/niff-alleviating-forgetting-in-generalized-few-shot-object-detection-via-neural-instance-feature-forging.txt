Privacy and memory are significant concerns in the discussion surrounding the societal impact of artificial intelligence (AI). These concerns stem from the necessity of vast amounts of data to train deep neural networks. Generalized Few-shot Object Detection (G-FSOD) is an AI learning paradigm that aims to address these concerns by utilizing prior knowledge from existing classes (base classes) to detect new classes, thus reducing the need for extensive training samples. However, current approaches assume that the base images are accessible, which may not be the case when data sharing and storage present challenges. To overcome this limitation, we propose the first data-free knowledge distillation (DFKD) approach for G-FSOD. This approach leverages the statistics of the region of interest (RoI) features from the base model to generate instance-level features without accessing the base images. Our approach involves designing a lightweight generator with class-wise heads to generate and replay diverse instance-level base features to the RoI head while fine-tuning on the new data. This differs from standard DFKD approaches in image classification, which invert the entire network to generate base images. Additionally, we carefully design the novel fine-tuning pipeline to regularize the model. Our experiments demonstrate that our approach significantly reduces the memory requirements for base images while achieving state-of-the-art performance on the challenging MS-COCO and PASCAL-VOC benchmarks for G-FSOD.