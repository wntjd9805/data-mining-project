Efforts to enhance the ability of convolutional neural networks (CNNs) to withstand adversarial attacks have mainly focused on improving adversarial training methods. However, little attention has been given to analyzing the impact of architectural elements, such as topology, depth, and width, on adversarial robustness. This study aims to fill this gap by conducting a comprehensive examination of the influence of architectural design on adversarial robustness, specifically focusing on residual networks. The analysis encompasses both block-level and network scaling-level architectural designs. Through systematic experiments, valuable insights are gained. Subsequently, a robust residual block named RobustResBlock and a compound scaling rule named RobustScaling are devised to allocate depth and width according to the desired FLOP count. The combination of RobustResBlock and RobustScaling results in a collection of adversarially robust residual networks called RobustResNets, which cover a wide range of model capacities. Experimental evaluations conducted on multiple datasets and adversarial attacks consistently demonstrate that RobustResNets outperform both the standard WRNs and other existing robust architectures. They achieve a state-of-the-art AutoAttack robust accuracy of 63.7% with the inclusion of 500K external data while being 2 more compact in terms of parameters. The code for these networks is available at a specified URL.