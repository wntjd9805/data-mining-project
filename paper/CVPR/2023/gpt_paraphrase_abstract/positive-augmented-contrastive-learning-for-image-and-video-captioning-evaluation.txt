The CLIP model has been shown to be highly effective for various cross-modal tasks, including evaluating captions generated by vision-and-language architectures. In this study, we introduce a new method called Positive-Augmented Contrastive learningScore (PAC-S) to assess image captioning. PAC-S combines contrastive visual-semantic learning with generated images and text using curated data. Our experiments across multiple datasets demonstrate that PAC-S achieves the strongest correlation with human judgments for both images and videos. It outperforms existing metrics such as CIDEr and SPICE, as well as reference-free metrics like CLIP-Score. Additionally, we evaluate the system-level correlation of PAC-S when applied to popular image captioning approaches and explore the impact of using different cross-modal features. The source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.