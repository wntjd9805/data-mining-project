Current methods for syncing lip movements with audio still struggle to find a balance between generation quality and the model's ability to generalize. Previous studies either require extensive training data or produce low-quality results with a similar movement pattern for all subjects. This paper introduces StyleSync, a framework that achieves high-fidelity lip synchronization. The researchers propose using a style-based generator to achieve this property in both one-shot and few-shot scenarios. They design a module that encodes spatial information using masks to preserve facial details, and modify mouth shapes accurately using audio through modulated convolutions. Additionally, the framework enables personalized lip-sync by introducing style space and generator refinement on limited frames, ensuring accurate preservation of a target person's identity and talking style. The researchers conduct extensive experiments that demonstrate the effectiveness of their method in producing high-fidelity results across various scenes. More information and resources can be found at the given URL.