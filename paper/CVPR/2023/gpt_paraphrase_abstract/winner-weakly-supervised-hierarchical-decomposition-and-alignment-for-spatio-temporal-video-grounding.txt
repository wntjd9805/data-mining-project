Spatio-temporal video grounding is the process of localizing the visual tube that corresponds to a given language query. Current techniques rely on dense boundary and bounding box annotations, which can be costly. To address this issue, we explore the weakly-supervised setting, where models learn from video-language data without annotations. We find that spurious correlations within the data can be reduced by capturing the decomposed structures of video and language data. Based on this insight, we propose a new framework called WINNER for hierarchical video-text understanding. WINNER constructs a language decomposition tree and applies a structural attention mechanism and top-down feature backtracking to build a multi-modal decomposition tree. This allows for a hierarchical understanding of unstructured videos. The multi-modal decomposition tree forms the foundation for multi-hierarchy language-tube matching. We introduce a hierarchical contrastive learning objective to align and distinguish the decomposition structures of video and text within and across samples. This achieves alignment of the video-language decomposition structure. Extensive experiments demonstrate the validity and effectiveness of our design, surpassing state-of-the-art weakly supervised methods and even some supervised methods.