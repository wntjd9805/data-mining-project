This study introduces SPARTN, a method for improving robot policies that use eye-in-hand cameras by augmenting expert demonstrations with synthetic perturbations. SPARTN leverages neural radiance fields (NeRFs) to inject corrective noise into visual demonstrations, generating perturbed viewpoints and calculating corrective actions. This approach does not require additional expert supervision or environment interaction, and distills the geometric information in NeRFs into a real-time reactive RGB-only policy. In simulated 6-DoF visual grasping benchmarks, SPARTN improves success rates significantly compared to imitation learning without augmentations and even surpasses some methods with online supervision. It also bridges the gap between RGB-only and RGB-D success rates, eliminating the need for depth sensors. In real-world 6-DoF robotic grasping experiments, SPARTN improves absolute success rates by an average of 22.5%, even for traditionally challenging objects for depth-based methods. Video results can be found at the provided website.