The abstract discusses Visual Place Recognition (VPR) and the limitations of conventional methods in matching and ranking query images. The authors propose a new framework called R2Former that incorporates a transformer model for both retrieval and reranking. This framework considers various factors such as feature correlation, attention value, and coordinates to determine if image pairs belong to the same location. R2Former outperforms existing methods in terms of accuracy, speed, and memory consumption. It also achieves state-of-the-art results on the MSLS challenge set and can be applied to large-scale real-world scenarios. Additionally, experiments show that vision transformer tokens are comparable, and sometimes superior, to CNN local features for local matching. The code for R2Former is available on GitHub at https://github.com/Jeff-Zilence/R2Former.