Current methods for separating audio and visual information typically use a standard design where an audio encoder-decoder network is combined with visual encoding features. However, this approach makes it difficult to learn both multi-modal feature encoding and robust sound decoding for audio separation. To address this issue, we propose a new approach called Instruments as Queries (iQuery), which incorporates a flexible query expansion mechanism. Our method ensures consistency between different modalities and disentangles different musical instruments. We use visually named queries to initiate the learning of audio queries and employ cross-modal attention to remove potential sound source interference. To adapt to new instruments or event classes, we introduce additional audio prompts while freezing the attention mechanism. Experimental results on three benchmark datasets demonstrate that our iQuery approach improves the performance of audio-visual sound source separation. The code for our method is available at the following GitHub repository: https://github.com/JiabenChen/iQuery.