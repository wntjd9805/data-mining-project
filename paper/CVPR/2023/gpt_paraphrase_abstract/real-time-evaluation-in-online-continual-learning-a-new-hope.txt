Current assessments of Continual Learning (CL) techniques typically assume unlimited training time and computation resources, which is not realistic in real-world scenarios. Therefore, we propose a practical real-time evaluation of CL that does not wait for the model to complete training before predicting the next data. To achieve this, we assess the computational costs of current CL methods. We conduct extensive experiments on a large-scale dataset called CLOC, containing 39 million time-stamped images with geolocation labels. Surprisingly, our simple baseline outperforms state-of-the-art CL methods in this evaluation, raising doubts about the applicability of existing methods in realistic settings. We also examine various CL components commonly used in the literature, such as memory sampling strategies and regularization approaches. Our findings indicate that all the methods considered are not competitive compared to our simple baseline. This suggests that the majority of existing CL research is focused on a specific type of stream that is not practical. Our hope is that this evaluation serves as the initial step towards a paradigm shift in considering computational costs when developing online continual learning methods.