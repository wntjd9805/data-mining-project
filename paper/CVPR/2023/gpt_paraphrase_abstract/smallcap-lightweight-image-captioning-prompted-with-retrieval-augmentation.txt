Recent advancements in the field of image captioning have primarily focused on increasing the size of both the data and the model, resulting in a significant rise in the cost of pre-training and fine-tuning. In contrast, we propose an alternative approach called SMALLCAP, which generates captions based on an input image and associated captions retrieved from a datastore. Our model is designed to be lightweight and quick to train, as it only involves learning parameters in newly introduced cross-attention layers that connect a pre-trained CLIP encoder with a GPT-2 decoder.One key advantage of SMALLCAP is its ability to transfer to new domains without requiring additional fine-tuning. This is made possible by the fact that the contents of the datastore can be easily replaced, allowing for the exploitation of large-scale data without the need for training. Our experiments demonstrate that SMALLCAP, trained solely on the COCO dataset, performs competitively on this benchmark. Moreover, it successfully transfers to other domains without any retraining, solely by retrieving relevant data from the target domain.Furthermore, we achieve further improvements in performance by leveraging diverse human-labeled and web data in a training-free manner. This approach proves to be effective across various domains, including the nocaps benchmark, which specifically evaluates the model's ability to generalize to unseen visual concepts.In summary, our SMALLCAP model offers a lightweight and efficient solution for image captioning. It demonstrates competitive performance on benchmark datasets and showcases the potential for training-free exploitation of large-scale data in various domains.