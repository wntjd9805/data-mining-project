Current attention algorithms highlight all salient objects in an image, but humans often focus their attention based on the task at hand, selectively attending to task-relevant objects. This task-guided top-down attention allows for task-adaptive representation and improves the model's generalization to different tasks. To achieve this, we propose an Analysis-by-Synthesis Vision Transformer (AbSViT) model that leverages a classic Analysis-by-Synthesis (AbS) perspective. Previous research has shown a functional equivalence between visual attention and sparse reconstruction, and we demonstrate that an AbS visual system optimized with a similar sparse reconstruction objective, modulated by a goal-directed top-down signal, naturally simulates top-down attention. AbSViT achieves controllable top-down attention by approximating AbS variationally. In various vision-language tasks such as VQA and zero-shot retrieval, where language guides top-down attention, AbSViT consistently outperforms baselines. Additionally, AbSViT can be used as a general backbone, enhancing performance in tasks like classification, semantic segmentation, and model robustness. More details about AbSViT can be found on the project page: https://sites.google.com/view/absvit.