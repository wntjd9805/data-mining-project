Traditional video action detectors typically follow a two-stage process, where a person detector is used to identify actors and then 3D RoIAlign is employed to extract actor-specific features for classification. However, this approach has limitations as it requires multi-stage training and inference and fails to capture context information beyond the bounding box. Some recent query-based action detectors attempt to predict action instances in an end-to-end manner but struggle with feature sampling and decoding, resulting in subpar performance or slow convergence. To address these issues, we propose a new one-stage sparse action detector called STMixer. STMixer incorporates two key designs. Firstly, we introduce a query-based adaptive feature sampling module, which enables our model to extract discriminative features from the entire spatiotemporal domain, enhancing its adaptability. Secondly, we devise a dual-branch feature mixing module that allows our model to dynamically attend to and combine video features along the spatial and temporal dimensions. This improves feature decoding and overall performance. By integrating these designs with a video backbone, our STMixer achieves efficient end-to-end action detection. Despite its simplicity, STMixer achieves state-of-the-art results on AVA, UCF101-24, and JHMDB datasets.