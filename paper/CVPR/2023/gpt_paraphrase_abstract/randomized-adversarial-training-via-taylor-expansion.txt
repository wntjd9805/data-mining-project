There has been a significant increase in research focused on developing stronger deep neural networks that can withstand adversarial examples. Adversarial training has proven to be one of the most effective methods in this regard. Many studies have attempted to find a balance between robustness against adversarial examples and accuracy over clean examples by improving adversarial training techniques. Building upon previous research that suggests smoothing updates on weights during training can lead to better generalization and finding flat minima, we propose a new approach to reconcile the trade-off between robustness and accuracy. Our method involves adding random noise to deterministic weights, which allows us to design a novel adversarial training technique using Taylor expansion of a small Gaussian noise. We demonstrate through extensive experiments using various attack methods that our approach improves upon existing state-of-the-art adversarial training methods, enhancing both robustness and clean accuracy. The code for our method is available at https://github.com/Alexkael/Randomized-Adversarial-Training.