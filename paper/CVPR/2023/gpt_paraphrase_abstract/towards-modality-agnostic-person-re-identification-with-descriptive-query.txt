Person re-identification (ReID) using descriptive queries, such as text or sketches, is an important addition to existing image-image paradigms. Typically, this field focuses on matching different modalities, like text-to-image or sketch-to-photo. However, in practical scenarios, it is uncertain whether these queries are available. This has led to the exploration of a new and challenging problem known as modality-agnostic person re-identification.To address this problem, we propose a unified person re-identification (UNIReID) architecture that can effectively adapt to cross-modality and multi-modality tasks. UNIReID incorporates a simple dual-encoder with task-specific modality learning, allowing us to mine and fuse information from both visual and textual modalities. To overcome the imbalanced training problem in UNIReID, where different tasks have varying difficulties, we introduce a task-aware dynamic training strategy. This strategy adjusts the training focus based on the difficulty of each task, ensuring better performance. Additionally, we have created three multi-modal ReID datasets by collecting corresponding sketches from photos, which serve as valuable resources for this challenging study.Experimental results on these multi-modal ReID datasets demonstrate that UNIReID significantly improves retrieval accuracy and generalization ability across different tasks and unseen scenarios.