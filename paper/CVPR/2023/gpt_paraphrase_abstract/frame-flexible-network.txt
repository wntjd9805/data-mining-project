Existing video recognition algorithms typically require different training pipelines for inputs with varying numbers of frames. This leads to repetitive training operations and increased storage costs. Additionally, when evaluating the model using frames that were not used in training, a significant drop in performance is observed, known as the Temporal Frequency Deviation phenomenon. To address this issue, we propose a solution called Frame Flexible Network (FFN). FFN allows the model to be evaluated at different frames, enabling adjustments in computation. It also reduces the memory costs associated with storing multiple models. FFN incorporates multiple sets of training sequences, utilizes Multi-Frequency Alignment (MFAL) to learn representations that are invariant to temporal frequency, and leverages Multi-Frequency Adaptation (MFAD) to enhance the representation abilities further. Through comprehensive empirical validations using different architectures and popular benchmarks, we demonstrate the effectiveness and generalization of FFN. For example, on the Something-Something V1 dataset, FFN achieves performance gains of 7.08%, 5.15%, and 2.17% at Frame 4, 8, and 16, respectively, compared to Uniformer. The code for FFN is available at https://github.com/BeSpontaneous/FFN.