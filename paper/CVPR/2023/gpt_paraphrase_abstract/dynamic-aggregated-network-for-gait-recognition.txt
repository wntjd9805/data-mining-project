Gait recognition is widely used in various applications such as video surveillance, crime scene investigation, and social security. However, it faces challenges in real-life scenarios, including different carrying conditions, wearing overcoats, and varying viewing angles. Although deep learning-based gait recognition methods have shown promise, they often focus on extracting a single salient feature using fixed-weighted convolutional networks. They also fail to consider the relationship between gait features in key regions and neglect the aggregation of complete motion patterns. To address these limitations, we propose a new perspective that emphasizes the inclusion of global motion patterns in multiple key regions, where each global motion pattern consists of a series of local motion patterns. To achieve this, we introduce a Dynamic Aggregation Network (DANet) that facilitates the learning of more discriminative gait features. The DANet incorporates a dynamic attention mechanism that adapts to focus on key regions and generates expressive local motion patterns. Additionally, we employ a self-attention mechanism to select representative local motion patterns and enhance the learning of robust global motion patterns. Extensive experiments conducted on three popular public gait datasets (CASIA-B, OUMVLP, and Gait3D) demonstrate that our proposed method surpasses the current state-of-the-art methods and significantly improves gait recognition performance.