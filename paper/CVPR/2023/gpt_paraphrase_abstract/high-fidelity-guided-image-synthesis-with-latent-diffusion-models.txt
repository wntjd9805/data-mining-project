Recent advancements in text-conditioned latent diffusion models have sparked significant interest in controllable image synthesis using user scribbles. While user scribbles control the color composition and text prompts provide control over the overall image semantics, previous works in this area have suffered from a domain shift problem, resulting in generated outputs lacking details and appearing simplistic. To address this issue, we propose a novel guided image synthesis framework that treats the output image as the solution to a constrained optimization problem. While finding an exact solution is infeasible, we demonstrate that an approximation can be achieved with just one pass of the reverse diffusion process. Furthermore, by establishing a cross-attention-based correspondence between the input text tokens and the user stroke-painting, we enable the user to control the semantics of different painted regions without the need for conditional training or fine-tuning. Our approach surpasses the previous state-of-the-art by 85.32% in terms of overall user satisfaction, as demonstrated by a human user study. For more information, please visit our project page at https://1jsingh.github.io/gradop.