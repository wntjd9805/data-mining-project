Deep learning-based medical image segmentation studies have made significant progress in achieving human-level performance. However, the process of acquiring pixel-level expert annotations for training data is expensive and time-consuming in the field of medical imaging. To address this issue, unsupervised domain adaptation (UDA) can be utilized, allowing the use of annotated data from one imaging modality to train a network that can accurately perform segmentation on a target modality with no labels. In this study, we propose SDC-UDA, a straightforward yet effective framework for volumetric UDA in cross-modality medical image segmentation. Our approach incorporates intra- and inter-slice self-attentive image translation, uncertainty-constrained pseudo-label refinement, and volumetric self-training. What sets our method apart from previous UDA methods is its ability to achieve continuous segmentation in the slice direction, resulting in higher accuracy and potential for clinical use. We evaluate the performance of SDC-UDA using multiple publicly available cross-modality medical image segmentation datasets and demonstrate state-of-the-art segmentation results. Furthermore, our method exhibits superior slice-direction continuity compared to previous studies.