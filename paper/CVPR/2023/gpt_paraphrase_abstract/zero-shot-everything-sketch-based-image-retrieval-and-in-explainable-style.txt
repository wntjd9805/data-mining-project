This study focuses on zero-shot sketch-based image retrieval (ZS-SBIR) and introduces two important innovations compared to previous research. Firstly, the authors address all variants of ZS-SBIR (inter-category, intra-category, and cross datasets) using a single network. Secondly, they aim to understand the mechanism behind sketch-photo matching by making it explainable. The authors propose a transformer-based cross-modal network that utilizes three novel components. These components include a self-attention module with a learnable tokenizer to generate visual tokens representing informative local regions, a cross-attention module to establish local correspondences between visual tokens in different modalities, and a kernel-based relation network to combine local matches and compute an overall similarity metric for sketch-photo pairs. Experimental results demonstrate the superior performance of the proposed approach in all ZS-SBIR scenarios. The authors achieve the goal of explainability by visualizing cross-modal token correspondences and by synthesizing sketches into photos through the replacement of matched photo patches. The code and model of the proposed method are available at https://github.com/buptLinfy/ZSE-SBIR.