This study addresses the problem of depth completion, which involves predicting dense depth values for an image based on sparse depth measurements. While deep-learning-based methods have made significant progress in this area, they struggle to model long-range relationships between pixels due to the local nature of convolutional layers or graph models. Although fully Transformer-based architectures have shown promising results with global receptive fields, they still lag behind CNN models in terms of performance and efficiency due to the loss of local feature details. To overcome these limitations, the authors propose a novel approach called Joint Convolutional Attention and Transformer block (JCAT). This approach combines the strengths of convolutional attention layers and VisionTransformer into a single block, forming the basic unit of their depth completion model. By integrating both local connectivity and global context, the proposed hybrid architecture, named Completion-Former, outperforms state-of-the-art CNN-based methods on benchmark datasets like KITTI Depth Completion and NYUv2. Additionally, it achieves significantly higher efficiency, reducing the number of floating-point operations (FLOPs) by nearly one-third compared to pure Transformer-based methods. The code for the proposed model is publicly available on GitHub at the given link.