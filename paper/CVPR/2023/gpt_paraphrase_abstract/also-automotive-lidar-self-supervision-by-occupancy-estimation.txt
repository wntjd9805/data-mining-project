We introduce a novel self-supervised technique for pre-training deep perception models that operate on point clouds. Our approach involves training the model to reconstruct the surface from which the 3D points were sampled, and utilizing the latent vectors as input for perception tasks. The underlying assumption is that if the network can successfully reconstruct the scene surface using sparse input points, it likely captures some semantic information that can enhance actual perception tasks. This formulation is straightforward to implement and applicable to a wide range of 3D sensors and deep networks used in semantic segmentation or object detection. Unlike contrastive learning approaches, our method supports a single-stream pipeline, enabling training with limited resources. We conducted extensive experiments using various lidars on different autonomous driving datasets for semantic segmentation and object detection. The results demonstrate the efficacy of our annotation-free approach in learning valuable representations compared to existing methods. The code for our method is available at github.com/valeoai/ALSO.