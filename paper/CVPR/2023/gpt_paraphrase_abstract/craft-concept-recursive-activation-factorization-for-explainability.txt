Attribution methods, which utilize heatmaps to identify the most influential regions of an image that impact model decisions, have become popular as a means of explaining model behavior. However, recent research has revealed the limited practicality of these methods, partly due to their narrow focus on the most prominent regions of an image. While they reveal "where" the model looks, they fail to elucidate "what" the model sees in those areas. This study aims to address this gap with CRAFT, a novel approach that identifies both "what" and "where" by generating concept-based explanations. The authors introduce three new elements to the automatic concept extraction literature: (i) a recursive strategy to detect and break down concepts across layers, (ii) a novel method for estimating concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. The authors conduct experiments in both human and computer vision to demonstrate the benefits of their approach. They show that their technique for estimating concept importance is more accurate than previous methods. When evaluating the usefulness of the method for human experimenters on a human-centered utility benchmark, the authors find that their approach significantly improves performance in two out of three test scenarios.