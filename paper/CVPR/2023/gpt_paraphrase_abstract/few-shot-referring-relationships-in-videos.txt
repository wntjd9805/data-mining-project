The task of comprehensively understanding videos involves interpreting visual relationships. This involves identifying and localizing the subjects and objects that are connected by a predicate in a given video. While modern visio-lingual understanding capabilities make solving this problem possible, it requires a large number of annotated training examples. However, annotating every combination of subject, object, and predicate is time-consuming, expensive, and may not be feasible. Therefore, there is a need for models that can learn to localize subjects and objects connected by an unseen predicate using only a few support set videos that share the same predicate. This problem is referred to as few-shot referring relationships in videos.To address this challenge, we propose a solution that formulates the problem as a minimization of an objective function defined over a random field with multiple vertices representing candidate bounding boxes for the subject and object. The objective function consists of frame-level and visual relationship similarity potentials. To learn these potentials, we utilize a relation network that takes query-conditioned translational relationship embedding as inputs and is meta-trained using support set videos in an episodic manner. We use belief propagation-based message passing on the random field to minimize the objective function and obtain the spatiotemporal localization of subject and object trajectories.We conducted extensive experiments on two public benchmarks, ImageNet-VidVRD and VidOR, to evaluate the effectiveness of our proposed approach. We compared it with competitive baselines and assessed its efficacy.