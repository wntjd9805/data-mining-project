Addressing data heterogeneity in federated learning is a significant challenge. Previous methods either align client and server models or use control variates to correct client model drift. However, these approaches are not effective in over-parameterized models like deep neural networks. This study aims to improve the widely used FedAvg algorithm in deep neural networks by understanding the impact of data heterogeneity on gradient updates across network layers. The authors find that while FedAvg efficiently learns feature extraction layers, the significant diversity in the final classification layers hampers performance. Based on this observation, they propose correcting model drift by reducing variance only in the final layers. The authors demonstrate that their approach outperforms existing benchmarks with similar or lower communication costs. They also provide proof of convergence rates for their algorithm.