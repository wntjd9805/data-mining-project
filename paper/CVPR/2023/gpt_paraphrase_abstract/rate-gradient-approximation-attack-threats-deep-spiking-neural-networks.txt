Spiking Neural Networks (SNNs) have gained significant attention due to their energy-efficient properties and potential use in neuromorphic hardware. Currently, SNNs consist mostly of simple Leaky Integrate-and-Fire (LIF) neurons and have shown comparable performance to Artificial Neural Networks (ANNs) in image classification tasks on large datasets. However, the robustness of deep SNNs has not been fully explored. This study demonstrates that communication between layers in these SNNs mostly relies on rate coding. Based on this observation, a novel attack method called Rate Gradient Approximation Attack (RGA) is developed, specifically targeting rate coding SNNs. The RGA attack is generalized to SNNs with different leaky parameters and input encoding by designing surrogate gradients. Additionally, a time-extended enhancement is introduced to generate more effective adversarial examples. Experimental results demonstrate that the proposed RGA attack is more effective than previous methods and less sensitive to neuron hyperparameters. Moreover, the study concludes that rate-coded SNNs composed of LIF neurons are not secure, highlighting the need to explore training methods for SNNs with complex neurons and other neuronal codings. The code for the RGA attack is available at https://github.com/putshua/SNN-attack-RGA.