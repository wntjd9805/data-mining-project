Monocular 3D object detection has become a popular method in autonomous driving due to its ease of implementation. One of its advantages is that it does not require Li-DAR point clouds for inference. However, current methods still rely on 3D point cloud data for labeling the ground truths during training. This inconsistency between training and inference makes it difficult to utilize large-scale feedback data and increases data collection expenses. To address this issue, we propose a new weakly supervised monocular 3D object detection method that can train the model using only 2D labels on images. Specifically, we investigate three types of consistency - projection, multi-view, and direction consistency - and design a weakly supervised architecture based on these consistencies. Additionally, we introduce a new 2D direction labeling method to accurately predict rotation direction. Experimental results demonstrate that our weakly supervised method achieves comparable performance to some fully supervised methods. Moreover, when used as a pre-training method, our model significantly outperforms the corresponding fully supervised baseline with only one-third of the 3D labels.