Human-centric tasks such as pose estimation, human parsing, pedestrian detection, and person re-identification are crucial in various industrial applications of visual models. Although these tasks have their own specific semantic aspects, they also share a common underlying structure related to the human body. However, there have been limited efforts to exploit this homogeneity and develop a general-purpose model for human-centric tasks.In this study, we propose UniHCP, which stands for Unified Model for Human-Centric Perceptions, to address this gap. UniHCP aims to unify a wide range of human-centric tasks in a simplified and end-to-end manner using the plain vision transformer architecture. To train UniHCP, we leverage a large-scale dataset comprising 33 human-centric datasets, allowing us to achieve better performance than strong baselines on various in-domain and downstream tasks through direct evaluation.Furthermore, when UniHCP is adapted to specific tasks, it surpasses specialized models tailored for each task, setting new state-of-the-art results. For instance, UniHCP achieves a mean Intersection over Union (mIoU) of 69.8 on the CIHP dataset for human parsing, an attribute prediction mean Average Precision (mA) of 86.18 on PA-100K, a mean Average Precision (mAP) of 90.3 on Market1501 for person re-identification (ReID), and a Jaccard Index (JI) of 85.8 on CrowdHuman for pedestrian detection.We have made the code and pretrained model of UniHCP available on GitHub at https://github.com/OpenGVLab/UniHCP.