Improving the generalization ability of Deep Neural Networks (DNNs) is crucial for their practical applications and has been a long-standing challenge. Previous theoretical studies have revealed that DNNs exhibit preferences for certain frequency components during the learning process, which can impact the robustness of the learned features. In this study, we introduce Deep Frequency Filtering (DFF) as a novel approach to learn domain-generalizable features. DFF is the first attempt to explicitly modulate the frequency components of varying transfer difficulties across domains in the latent space during training. To accomplish this, we utilize Fast Fourier Transform (FFT) to analyze the feature maps at different layers. Subsequently, we employ a lightweight module to learn attention masks from the frequency representations obtained through FFT. These attention masks enhance the transferable components while suppressing the components that hinder generalization. Furthermore, we conduct empirical comparisons to evaluate the efficacy of different attention designs for implementing DFF. Extensive experiments validate the effectiveness of our proposed DFF and demonstrate its superiority over state-of-the-art methods in various domain generalization tasks, including close-set classification and open-set retrieval.