This paper focuses on the issue of detecting 3D objects from multi-view images. Existing methods for this task use global 3D position embeddings (PE) to establish the geometric correspondence between images and 3D space. However, we argue that directly combining 2D image features with global 3D PE can make it challenging to learn view transformations due to variations in camera extrinsics. To address this, we propose a new approach called CAPE (Camera view Position Embedding). Instead of using the global coordinate system, we generate 3D position embeddings under the local camera-view coordinate system. This allows the 3D position embeddings to be independent of encoding camera extrinsic parameters. Additionally, we extend CAPE to incorporate temporal modeling by leveraging object queries from previous frames and encoding ego motion to enhance 3D object detection. Our CAPE method achieves state-of-the-art performance (61.0% NDS and 52.5% mAP) among LiDAR-free methods on the nuScenes dataset. The codes and models for our approach are available.