Most existing image restoration methods rely on the posterior distribution of natural images. However, these methods have limitations as they assume known degradation and require supervised training, making them less adaptable to complex real-world applications. In this study, we introduce the Generative Diffusion Prior (GDP) as an unsupervised sampling approach to effectively model posterior distributions. GDP utilizes a pre-trained denoising diffusion generative model (DDPM) to address linear inverse, non-linear, and blind problems. It employs a protocol of conditional guidance, which proves to be more practical than commonly used methods. Additionally, GDP excels at optimizing degradation model parameters during the denoising process, enabling blind image restoration. We also develop hierarchical guidance and patch-based techniques that allow GDP to generate images of any resolution. Experimental results demonstrate the versatility of GDP across various image datasets, including linear problems like super-resolution, deblurring, inpainting, and colorization, as well as non-linear and blind issues such as low-light enhancement and HDR image recovery. GDP surpasses current unsupervised methods in terms of reconstruction quality and perceptual quality on diverse benchmarks. Furthermore, GDP generalizes well for natural or synthesized images of arbitrary sizes from tasks beyond the distribution of the ImageNet training set. For more information, please refer to the project page: https://generativediffusionprior.github.io/.