Combining multiple datasets in computer vision tasks can improve performance. However, this trend is not observed in object detection due to two inconsistencies: taxonomy difference and domain gap. To address these challenges, we propose a new design called Detection Hub, which is dataset-aware and category-aligned. This design helps mitigate dataset inconsistencies and provides coherent guidance for the detector to learn across multiple datasets. The dataset-aware design is achieved by learning a dataset embedding that adapts object queries and convolutional kernels in detection heads. We align the categories across datasets into a unified space using word embeddings and leverage the semantic coherence of language embedding. By using Detection Hub, we can benefit from large data in object detection. Experimental results show that joint training on multiple datasets significantly improves performance compared to training on individual datasets. Detection Hub also achieves state-of-the-art performance on the UODB benchmark, which consists of a wide variety of datasets.