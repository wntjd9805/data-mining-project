Model-agnostic meta-learning (MAML) is a successful algorithm in the field of meta-learning. It consists of an outer-loop process that learns a shared initialization and an inner-loop process that optimizes task-specific weights. While MAML uses standard gradient descent in the inner-loop, recent studies have shown that using a meta-learned preconditioner to control the gradient descent can be advantageous. However, existing preconditioners lack the ability to adapt in a task-specific and path-dependent manner. They also fail to satisfy the Riemannian metric condition, which is important for steepest descent learning. To address these limitations, we propose a new approach called Geometry-Adaptive Pre-conditioned gradient descent (GAP). GAP can efficiently meta-learn a preconditioner that depends on task-specific parameters and satisfies the Riemannian metric condition. Our experiments demonstrate that GAP outperforms both the state-of-the-art MAML family and the preconditioned gradient descent-MAML (PGD-MAML) family in various few-shot learning tasks. The code for GAP is available at: https://github.com/Suhyun777/CVPR23-GAP.