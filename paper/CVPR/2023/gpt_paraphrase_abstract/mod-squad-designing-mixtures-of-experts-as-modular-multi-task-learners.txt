Multi-task learning (MTL) poses greater challenges in optimization compared to single-task learning (STL) due to conflicting gradients from different tasks. Sharing parameters among related tasks can be beneficial for cooperation, but some tasks require specialized parameters for specific data types or discrimination. To tackle this challenge, we propose Mod-Squad, a new model that modularizes into groups of experts called a 'Squad'. This modular structure allows us to formalize cooperation and specialization by matching experts with tasks. We optimize this matching process during model training by incorporating mixture of experts (MoE) layers into a transformer model, using a new loss that considers the mutual dependence between tasks and experts. Consequently, only a small set of experts is activated for each task, preventing the sharing of the entire backbone model among all tasks. This strengthens the model, particularly with larger training sets and increasing task numbers. Notably, we can extract the small set of experts for each task as a standalone model while maintaining the same performance as the large model. Extensive experiments conducted on the Taskonomy dataset (13 vision tasks) and the PASCAL-Context dataset (5 vision tasks) demonstrate the superiority of our approach. The project page is available at https://vis-www.cs.umass.edu/mod-squad.