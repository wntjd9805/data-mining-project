Predicting an accurate depth map from a single image is a difficult task due to the infinite ways in which a 2D scene can be projected into a 3D scene. Previous studies have used multi-head attention (MHA) modules to improve depth map regression by capturing long-range information and generating attention maps based on pixel relationships. However, the quadratic complexity of MHA limits its ability to compute high-resolution depth features efficiently. In this paper, we propose a new approach that utilizes depth-wise convolution to obtain long-range information and introduces a novel trap attention mechanism. This mechanism sets traps on an extended space for each pixel and determines the attention mechanism based on the feature retention ratio of a convolution window, effectively converting the quadratic computational complexity to a linear form. We also introduce an encoder-decoder trap depth estimation network, where a vision transformer serves as the encoder and the trap attention is used in the decoder to estimate depth from a single image. Extensive experiments on the NYU Depth-v2 and KITTI datasets demonstrate that our network outperforms state-of-the-art methods in monocular depth estimation while reducing the number of parameters. The code for our proposed network is available at: https://github.com/ICSResearch/TrapAttention.