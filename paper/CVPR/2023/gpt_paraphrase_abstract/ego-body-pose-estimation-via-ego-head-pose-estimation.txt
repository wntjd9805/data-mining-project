Estimating 3D human motion from egocentric video sequences is crucial for understanding human behavior and has applications in VR/AR. However, it is challenging to learn a mapping between egocentric videos and human motions because the user's body is often not visible to the front-facing camera. Additionally, collecting high-quality datasets with paired egocentric videos and 3D human motions is difficult due to the need for accurate motion capture devices, which limits the variety of scenes captured. To address these challenges, we propose a new method called Ego-Body Pose Estimation via Ego-Head Pose Estimation (EgoEgo). This method decomposes the problem into two stages, connected by the head motion as an intermediate representation. First, EgoEgo integrates SLAM and a learning approach to accurately estimate head motion. Then, leveraging the estimated head pose, EgoEgo generates multiple plausible full-body motions using conditional diffusion. This disentanglement of head and body pose eliminates the need for paired training datasets, allowing us to use large-scale egocentric video datasets and motion capture datasets separately. To benchmark our method, we develop a synthetic dataset called AMASS-Replica-Ego-Syn (ARES) with paired egocentric videos and human motions. Our EgoEgo model outperforms current state-of-the-art methods on both ARES and real-world data.