The field of emotional talking face generation has gained significant attention lately. However, current methods are limited in their ability to control emotions and handle unseen emotion styles. They either overlook the one-shot setting or produce low-quality generated faces. To address these limitations, we propose a more flexible and generalized framework. Our approach involves incorporating emotion style in text prompts and using an Aligned Multi-modal Emotion encoder to embed emotion from text, image, and audio into a unified space. This approach allows for arbitrary emotion modality during testing and generalization to unseen emotion styles. Additionally, we introduce an Emotion-aware Audio-to-3DMM Convertor to connect emotion conditions and audio sequences to structural representation. This is followed by a style-based High-fidelity Emotional Face generator that can generate high-resolution realistic identities. Our texture generator utilizes hierarchical learning to create flow fields and animated faces in a residual manner. Through extensive experiments, we demonstrate the flexibility and generalization of our method in emotion control and the effectiveness of high-quality face synthesis.