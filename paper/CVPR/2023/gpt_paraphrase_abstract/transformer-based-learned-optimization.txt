We propose a novel method for learned optimization by using a neural network to represent the computation of an optimizer's update step. The parameters of the optimizer are learned through training on a set of optimization tasks, with the goal of efficiently performing minimization. Our innovation is a new neural network architecture called Opti-mus, which is inspired by the BFGS algorithm. Similar to BFGS, we estimate a preconditioning matrix by summing rank-one updates, but we use a Transformer-based neural network to predict these updates along with the step length and direction. Unlike recent learned optimization approaches, our formulation allows for conditioning across the dimensions of the parameter space without requiring retraining for optimization tasks of varying dimensionality. We demonstrate the advantages of our approach on a benchmark of objective functions commonly used for evaluating optimization algorithms, as well as on the real-world task of physics-based visual reconstruction of articulated 3D human motion.