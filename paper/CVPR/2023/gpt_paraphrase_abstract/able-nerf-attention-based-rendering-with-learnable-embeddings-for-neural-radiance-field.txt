Neural Radiance Field (NeRF) is a widely used method for representing 3D scenes by optimizing a continuous volumetric scene function. However, NeRF struggles with producing accurate view-dependent effects for glossy and transparent surfaces, resulting in murky appearances. One approach to mitigate these issues is to exclude volumes with back-facing normals from the volumetric rendering equation. While this helps with glossy surfaces, it still fails to properly represent translucent objects. This paper proposes an alternative framework called ABLE-NeRF that incorporates self-attention-based techniques on volumes along a ray. Additionally, inspired by modern game engines, Learnable Embeddings are used to capture view-dependent effects within the scene. ABLE-NeRF significantly improves the rendering of glossy surfaces, reducing blurriness, and produces realistic translucent surfaces that previous methods lack. Experimental results on the Blender dataset demonstrate that ABLE-NeRF achieves state-of-the-art performance, outperforming Ref-NeRF across all three image quality metrics (PSNR, SSIM, LPIPS).