Both human vision and natural language share the characteristic of being compositional. However, despite advancements in large-scale pretraining for vision and language, they struggle with compositionality. To address this, we introduce a new evaluation benchmark called CREPE, which assesses systematicity and productivity. For systematicity, we use a test dataset with over 370K image-text pairs and three different seen-unseen splits based on popular training datasets. Additionally, we generate hard negative captions for a subset of the pairs. For productivity, CREPE includes 17K image-text pairs with varying complexities and 278K hard negative captions with different foils. We generate these datasets using Visual Genome scene graphs, region descriptions, handcrafted templates, and GPT-3. Our findings show that model performance decreases when novel compositions dominate the retrieval set, and retrieval success declines as complexity increases. These results are consistent across different models and training dataset sizes.