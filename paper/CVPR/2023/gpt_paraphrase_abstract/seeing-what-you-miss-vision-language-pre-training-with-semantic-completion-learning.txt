To enhance the cross-modal alignment in vision-language pre-training (VLP) models, previous approaches have focused on reconstructing masked tokens based on visible context. However, they often neglect the global semantic features of the masked data, limiting the models' ability to align global representations. In this study, we introduce a novel task called Semantic Completion Learning (SCL) to address this issue. SCL complements the missing semantics of masked data by incorporating information from the other modality, facilitating the learning of more representative global features. Additionally, we propose a flexible vision encoder that enables our model to perform image-text and video-text multimodal tasks simultaneously. Experimental results demonstrate that our approach achieves state-of-the-art performance on various vision-language benchmarks, including visual question answering, image-text retrieval, and video-text retrieval.