Recently, large-scale pre-trained models in Vision-and-Language (VL) have shown impressive performance in zero-shot downstream tasks, even with minimal text prompts. However, these models still struggle with Structured VLConcept (SVLC) reasoning, such as recognizing object attributes, states, and inter-object relations. This leads to errors in reasoning that need to be addressed by teaching the models the missing SVLC skills. This often requires using private data, resulting in a data-free continual learning setting without task identification.To address this challenge, we present the ConStruct-VL benchmark, which is difficult for existing data-free continual learning strategies. We propose a data-free method called Adversarial Pseudo-Replay (APR), which generates adversarial reminders of past tasks using previous task models. To efficiently utilize this method, we introduce a continual parameter-efficient neural architecture called Layered-LoRA (LaLo), which allows access to all past models during training without memory cost. Our approach outperforms all other data-free methods by approximately 7% and even matches some levels of experience-replay, making it suitable for applications where data privacy is crucial.