We present a novel type of deep neural networks called Integral Neural Networks (INNs) that deviate from the traditional representation of network layers as weight tensors. Instead, INNs use continuous layer representations along the filter and channel dimensions. In INNs, the weights are expressed as continuous functions defined on hypercubes, and discrete transformations of inputs are replaced by continuous integration operations. During inference, the continuous layers can be converted back to the conventional tensor representation using numerical integral quadratures. This representation allows for arbitrary network discretization with various intervals for the integral kernels. By applying this approach, we can prune the model directly on an edge device with minimal performance loss, even at high rates of structural pruning, without any fine-tuning. To evaluate the effectiveness of our approach, we conducted experiments using different neural network architectures on multiple tasks. The results demonstrate that INNs achieve comparable performance to their discrete counterparts, while also preserving similar performance levels (with only a 2% accuracy loss for ResNet18 on Imagenet) at high rates of structural pruning (up to 30%) without fine-tuning. In comparison, conventional pruning methods result in a 65% accuracy loss under the same conditions. The code for our approach is available at gitee.