This paper introduces JAWS, a method for transferring visual cinematic features from a reference video clip to a newly generated clip. The approach utilizes an implicit-neural-representation (INR) to compute a clip that shares the same cinematic features as the reference clip. The authors propose a general formulation of a camera optimization problem within the INR framework, which computes extrinsic and intrinsic camera parameters as well as timing. By leveraging the differentiability of neural representations, they are able to back-propagate cinematic losses through a NeRF network to directly optimize the cinematic parameters. The authors also incorporate guidance maps to enhance the quality and efficiency of the system. Experimental results demonstrate that the system can successfully replicate famous camera sequences from movies by adapting the framing, camera parameters, and timing of the generated video clip to closely resemble the reference clip.