Object detectors often struggle with the differences between the training domain and real-world applications. Self-training using mean-teacher is a popular method for adapting to new domains, but it is hindered by low-quality pseudo-labels. In this study, we discover a connection between mean-teacher self-training and contrastive learning and propose a unified framework called Contrastive MeanTeacher (CMT) that combines these two paradigms. Instead of using pseudo-labels only for predictions, our approach utilizes them to extract object-level features and optimize them through contrastive learning, without relying on labels in the target domain. When combined with mean-teacher self-training, CMT achieves state-of-the-art performance on Foggy Cityscapes, surpassing previous methods by 2.1% mAP. Importantly, CMT is able to maintain performance and achieve greater improvements as the noise in pseudo-labels increases.