The Masked Autoencoder (MAE) is a powerful technique used in various vision tasks that involves randomly masking image patches and reconstructing them. However, there is still a lack of effective data augmentation strategies for MAE, unlike contrastive learning which has well-established methods. This study focuses on exploring the prevalent mixing augmentation for MAE. The researchers find that simply mixing patches can actually harm the performance of the model by increasing mutual information (MI) between the patches. To address this issue, they propose a new approach called homologous recognition, which is an auxiliary pretext task. This task not only reduces the increase in MI by explicitly requiring each patch to recognize its homologous patches, but also improves the model's ability to perceive objects in a self-supervised pre-training phase. Through extensive experiments, the researchers demonstrate that their proposed Mixed Autoencoder (MixedAE) achieves state-of-the-art transfer results in masked image modeling (MIM) augmentations across different downstream tasks, while maintaining high efficiency. MixedAE outperforms MAE in terms of accuracy, mIoU, and AP on datasets like ImageNet-1K, ADE20K, and COCO with a standard ViT-Base architecture. Furthermore, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while also speeding up the training process by 2 times. This study is the first of its kind to consider mixing for MIM from the perspective of pretext task design. The code for the proposed method will be made publicly available.