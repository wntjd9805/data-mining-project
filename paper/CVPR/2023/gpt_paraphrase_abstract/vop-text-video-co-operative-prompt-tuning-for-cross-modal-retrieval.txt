The pre-trained CLIP has been widely used in text-video cross-modal retrieval studies. However, these studies often require heavy modules and extensive parameter tuning, resulting in increased computational burden and knowledge forgetting. To address this issue, we propose the VoP framework for efficient tuning in text-video retrieval tasks. VoP is an end-to-end framework that incorporates video and text prompts, serving as a powerful baseline with minimal trainable parameters. Additionally, we introduce three novel video prompt mechanisms that leverage the spatio-temporal characteristics of videos to enhance performance. These mechanisms focus on modeling frame position, frame context, and layer function using specific trainable prompts. Experimental results demonstrate that VoP outperforms full fine-tuning, achieving a 1.4% average R@1 improvement across five text-video retrieval benchmarks with significantly fewer parameters. The code for VoP is available at https://github.com/bighuang624/VoP.