Accelerating networks with small training sets is crucial due to data privacy concerns. Previous approaches have focused on pruning filters to achieve this acceleration. However, this paper introduces the concept of dropping blocks as a superior method in this scenario. Dropping blocks provides a higher acceleration ratio and achieves better latency-accuracy performance in the few-shot setting. To determine which blocks to drop, the paper proposes the concept of recoverability, which measures the ease of recovering the compressed network. The proposed recoverability concept is efficient and effective in selecting blocks to drop. Furthermore, an algorithm called PRACTISE is introduced to accelerate networks using only a small number of training images. PRACTISE outperforms previous methods significantly, surpassing them by an average of 7% on ImageNet-1k for a 22% reduction in latency. It also demonstrates high generalization ability, performing well in data-free or out-of-domain data settings. The code for PRACTISE is available at https://github.com/DoctorKey/Practise.