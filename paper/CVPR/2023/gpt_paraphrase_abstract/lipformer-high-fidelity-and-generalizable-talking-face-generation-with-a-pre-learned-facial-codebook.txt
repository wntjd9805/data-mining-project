This abstract discusses the challenge of generating realistic talking face videos and proposes a solution called LipFormer. The existing methods either lack fine facial details or require training a model for each identity. The authors suggest using a pre-learned codebook from high-quality face images as a prior to achieve high-fidelity and generalizable talking head synthesis. They propose a Transformer-based framework that models audio-visual coherence and predicts lip-codes based on input audio features. Additionally, they introduce an adaptive face warping module to handle lip-code prediction under different poses. The experiments demonstrate that LipFormer can generate more realistic talking face videos and faithfully generalize to unseen identities.