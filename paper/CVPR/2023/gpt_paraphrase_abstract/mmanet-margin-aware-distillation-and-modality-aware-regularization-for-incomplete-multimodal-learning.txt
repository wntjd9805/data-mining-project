Multimodal learning has gained attention recently due to its potential in various applications. However, a common challenge in multimodal learning is the presence of missing data, which leads to a significant decrease in performance. In order to address this issue, we propose a general framework called MMANet that aims to assist incomplete multimodal learning. MMANet consists of three components: a deployment network for making inferences, a teacher network for transferring comprehensive multimodal information to the deployment network, and a regularization network for guiding the deployment network in balancing weak modality combinations. To facilitate the transfer of information, we introduce a novel technique called margin-aware distillation (MAD), which takes into account the uncertainty of classification to weigh the contribution of each sample. This encourages the deployment network to focus on samples near decision boundaries and improve the inter-class margin. Furthermore, we develop a modality-aware regularization (MAR) algorithm to identify weak modality combinations and guide the regularization network in calculating the prediction loss for these combinations. This prompts the deployment network to enhance its representation ability for weak modality combinations in an adaptive manner. Extensive experiments on multimodal classification and segmentation tasks demonstrate that our MMANet outperforms existing state-of-the-art methods significantly. The code for MMANet is available at the following GitHub repository: https://github.com/shicaiwei123/MMANet.