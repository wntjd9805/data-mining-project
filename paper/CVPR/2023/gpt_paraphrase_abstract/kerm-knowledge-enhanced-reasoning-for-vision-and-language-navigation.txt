The task of vision-and-language navigation (VLN) involves enabling an embodied agent to navigate to a remote location using natural language instructions in real scenes. Previous approaches have used entire features or object-centric features to represent possible navigation options, but these representations are not efficient enough for successful navigation. To address this, we propose a Knowledge Enhanced Reasoning Model (KERM) that leverages knowledge to improve the agent's navigation ability. The KERM retrieves facts from a constructed knowledge base, ranging from properties of single objects to relationships between objects, which provide crucial information for VLN. We also introduce the purification, fact-aware interaction, and instruction-guided aggregation modules in KERM to integrate visual, history, instruction, and fact features. This allows the model to automatically select and gather relevant cues, resulting in more accurate action prediction. Experimental results on the REVERIE, R2R, and SOON datasets validate the effectiveness of our method. The source code for the proposed approach is available at the specified GitHub repository.