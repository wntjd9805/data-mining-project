While action recognition systems perform well on test points within a specific distribution, they struggle when faced with unexpected shifts in test data. Currently, there is a lack of demonstrated methods for adapting video action recognition models during testing. To address this issue, we propose a spatio-temporal approach that enables adaptation on a single video sample. This approach involves aligning the feature distribution of online estimates with the training statistics. Additionally, we enforce prediction consistency across temporally augmented views of the same test video sample. Our technique is not dependent on architecture and significantly improves performance on both the TANet convolutional architecture and the Video Swin Transformer, as demonstrated through evaluations on three benchmark action recognition datasets. In fact, our method outperforms existing test-time adaptation approaches in scenarios involving both single distribution shifts and random distribution shifts. For those interested, the code for our proposed method will be available at https://github.com/wlin-at/ViTTA.