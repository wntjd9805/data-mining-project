Image-text retrieval is a challenging task that aims to connect visual information with natural language by aligning regions in images with corresponding words. One of the main difficulties in this task is the existence of one-to-many correspondence, where a single query can have multiple matches in the other modality. Although previous approaches such as multi-point mapping, probabilistic distribution, and geometric embedding have shown promising results, the problem of one-to-many correspondence remains largely unexplored. In this study, we propose a novel method called Multilateral Semantic Relations Modeling (MSRM) to address this issue. MSRM utilizes a hypergraph model to capture the one-to-many correspondence between multiple samples and a given query. Specifically, we first map the query to a probabilistic embedding, which allows us to learn its true semantic distribution using Mahalanobis distance. Then, each candidate instance in a mini-batch is treated as a hypergraph node with its mean semantics, while the query is modeled as a hyperedge to capture the semantic correlations beyond pairwise relationships. We conduct comprehensive experiments on two commonly used datasets, and the results demonstrate that our MSRM method outperforms state-of-the-art approaches in handling multiple matches, while still achieving comparable performance in instance-level matching.