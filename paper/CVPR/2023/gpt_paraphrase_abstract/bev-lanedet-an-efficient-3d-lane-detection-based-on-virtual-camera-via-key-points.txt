This abstract discusses the development of autonomous driving systems, specifically focusing on the topic of 3D lane detection. Previous methods for detecting 3D lanes have been impractical due to complex spatial transformations and inflexible representations. To address these issues, the authors propose a new approach called BEV-LaneDet. This approach has three main contributions. Firstly, they introduce a Virtual Camera that ensures consistency in the spatial relationship among cameras mounted on different vehicles, improving the learning process. Secondly, they propose a simple and efficient 3D lane representation called Key-Points Representation, which is better suited for representing diverse and complicated lane structures. Lastly, they present a lightweight and chip-friendly spatial transformation module called Spatial Transformation Pyramid, which transforms front-view features into bird's-eye-view (BEV) features. Experimental results demonstrate that BEV-LaneDet outperforms other approaches in terms of F-Score, achieving higher accuracy on two datasets and operating at a speed of 185 FPS. The code for BEV-LaneDet is available at the given GitHub link.