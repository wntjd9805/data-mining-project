This paper presents Content-aware Token Sharing (CTS), a method to enhance the computational efficiency of semantic segmentation networks that utilize Vision Transformers (ViTs). While previous approaches have proposed token reduction techniques for ViT-based image classification networks, they are not directly applicable to semantic segmentation. In this study, we address this limitation by introducing CTS, which exploits the fact that multiple image patches with the same semantic class can share a token due to redundant information. To achieve this, we employ a class-agnostic policy network that predicts whether image patches contain the same semantic class, allowing them to share a token if they do. Through experiments, we investigate the key design choices of CTS and demonstrate its effectiveness on various datasets (ADE20K, Pascal Context, and Cityscapes), ViT backbones, and different segmentation decoders. By implementing Content-aware Token Sharing, we can reduce the number of processed tokens by up to 44% without compromising the quality of the segmentation results.