Despite the significant advancements in deep learning for computer vision tasks, they remain susceptible to adversarial examples. Adversarial training methods have proven to be effective in defending against such examples. However, some of these methods may have a negative impact when misclassifying a natural example. To address this issue, we propose a new adversarial training scheme that encourages the model to produce similar output probabilities for both an adversarial example and its "inverse adversarial" counterpart. The counterpart is generated by maximizing the likelihood in the vicinity of the natural example. Our extensive experiments on various vision datasets and architectures demonstrate that our training method achieves state-of-the-art robustness and natural accuracy compared to other robust models. Additionally, by utilizing a universal version of inverse adversarial examples, we enhance the performance of single-step adversarial training techniques at a low computational cost.