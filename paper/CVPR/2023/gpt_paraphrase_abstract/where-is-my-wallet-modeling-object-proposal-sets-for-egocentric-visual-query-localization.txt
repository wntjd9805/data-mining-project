This paper addresses the issue of localizing objects in image and video datasets using visual exemplars, with a particular focus on egocentric visual query localization. The study identifies biases in current query-conditioned model design and visual query datasets. To address these biases, the paper proposes expanding limited annotations and dynamically dropping object proposals during training. Additionally, a novel transformer-based module called Conditioned Contextual Transformer (CocoFormer) is introduced to incorporate query information and object-proposal set context. Experimental results demonstrate that these adaptations significantly improve egocentric query detection and enhance visual query localization in both 2D and 3D configurations. The frame-level detection performance is improved from 26.28% to 31.26% in average precision (AP), leading to substantial improvements in VQ2D and VQ3D localization scores. In fact, the proposed context-aware query object detector achieved first and second rankings in the VQ2D and VQ3D tasks, respectively, in the 2nd Ego4D challenge. Moreover, the relevance of the proposed model in the Few-Shot Detection (FSD) task is demonstrated with state-of-the-art results. The code for the proposed model is available at https://github.com/facebookresearch/vq2d_cvpr.