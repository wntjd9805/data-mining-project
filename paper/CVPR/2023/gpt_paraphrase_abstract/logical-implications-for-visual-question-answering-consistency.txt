Despite recent advancements in Visual Question Answering (VQA) models, doubts about their reasoning capabilities persist due to inconsistent or contradictory answers. Existing methods address this issue through indirect strategies or strong assumptions on question-answer pairs to ensure model consistency. In contrast, we propose a novel approach focused on directly reducing logical inconsistencies to enhance model performance. Our method introduces a new consistency loss term that can be applied to various VQA models, relying on knowledge of the logical relationship between question-answer pairs. Although VQA datasets typically lack this information, we suggest inferring logical relations using a dedicated language model and incorporating them into our proposed consistency loss function. We extensively evaluate our approach on the VQA Introspect and DME datasets, demonstrating its effectiveness in improving state-of-the-art VQA models across different architectures and settings. Our method primarily outputs the abstraction of the answer format.