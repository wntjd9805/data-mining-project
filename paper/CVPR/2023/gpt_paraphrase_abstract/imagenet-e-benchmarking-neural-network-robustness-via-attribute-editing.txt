Recent research has shown that higher accuracy on ImageNet typically results in better resilience against various corruptions. In this study, we deviate from the conventional approach of investigating novel out-of-distribution corruptions or perturbations that deep models may encounter. Instead, we focus on model debugging within in-distribution data to determine the object attributes that a model may be sensitive to. To accomplish this, we develop an object editing toolkit that allows us to control backgrounds, sizes, positions, and directions. Additionally, we create a comprehensive benchmark called ImageNet-E (Editing) to assess image classifier robustness in terms of object attributes. Using ImageNet-E, we evaluate the performance of current deep learning models, including convolutional neural networks and vision transformers. Our findings reveal that most models exhibit significant sensitivity to attribute changes. Even a minor alteration in the background can lead to an average drop of 9.23% in top-1 accuracy. Furthermore, we assess the robustness of some models, including adversarially trained models and other robust trained models, and observe that certain models demonstrate poorer resilience against attribute changes compared to vanilla models. Building on these insights, we propose methods to enhance attribute robustness through preprocessing, architectural design, and training strategies. Our work aims to provide valuable insights to the research community and pave the way for further exploration in robust computer vision. The code and dataset for this study can be accessed at https://github.com/alibaba/easyrobust.