BEVGuide is a new framework for autonomous driving that addresses the challenges of integrating multiple sensors and performing diverse tasks in a single algorithm. It introduces a Bird's Eye-View (BEV) representation learning method that unifies various sensors, such as cameras, lidar, and radar, in an end-to-end manner. The framework utilizes a transformer backbone to extract BEV feature embeddings and a BEV-guided multi-sensor attention block to learn the BEV representation from sensor-specific features. BEVGuide is efficient and flexible, supporting different sensor configurations. Extensive experiments demonstrate its exceptional performance in BEV perception tasks. More information can be found on the project page at https://yunzeman.github.io/BEVGuide.