This paper introduces a novel framework called Side Adapter Network (SAN) for open-vocabulary semantic segmentation using a pre-trained vision-language model. The proposed approach treats semantic segmentation as a region recognition problem and incorporates a side network into a frozen CLIP model. The side network consists of two branches: one for predicting mask proposals and the other for predicting attention bias, which is applied in the CLIP model to identify the class of masks. This decoupled design leverages the advantages of CLIP in recognizing mask classes and allows for a lightweight side network that can reuse CLIP features. The entire network can be trained end-to-end, enabling the side network to adapt to the frozen CLIP model and make the predicted mask proposals CLIP-aware. The approach is characterized by its speed, accuracy, and minimal increase in trainable parameters. Experimental evaluations on multiple semantic segmentation benchmarks demonstrate that our method outperforms existing approaches, achieving up to 18 times fewer trainable parameters and 19 times faster inference speed. The paper also includes visualization results on ImageNet, showcasing the effectiveness of the proposed approach. The authors hope that their framework will serve as a strong baseline and facilitate future research in open-vocabulary semantic segmentation.