To generate realistic images based on a semantic segmentation map, semantic image synthesis requires large datasets with per-pixel label annotations. However, obtaining such datasets is time-consuming and costly. To address this issue, we propose a transfer method that utilizes a model trained on a large dataset to enhance learning on smaller datasets by estimating pairwise relationships between source and target classes. By introducing a class affinity matrix as the first layer to the source model and fine-tuning it for the target domain, we make the model compatible with the target label maps. We explore various approaches to estimate class affinities, including leveraging prior knowledge such as semantic segmentation, textual label embeddings, and self-supervised vision features. Our approach is applied to GAN-based and diffusion-based architectures for semantic synthesis. Experimental results demonstrate the effectiveness of combining different methods to estimate class affinity and show that our approach significantly outperforms existing state-of-the-art transfer techniques for generative image models.