We introduce Recurrent Vision Transformers (RVTs) as a new framework for object detection using event cameras. Event cameras offer rapid visual information with low latency and are resistant to motion blur, making them ideal for time-critical scenarios. However, previous approaches in event-based vision have achieved excellent detection performance at the expense of long inference times, often exceeding 40 milliseconds. In our research, we reimagine the design of recurrent vision backbones to significantly reduce inference time by six-fold while maintaining similar performance levels. We accomplish this by employing a multi-stage approach that incorporates three key concepts in each stage. First, we utilize a convolutional prior, acting as a conditional positional embedding. Second, we incorporate local and dilated global self-attention mechanisms to enable spatial feature interaction. Lastly, we employ recurrent temporal feature aggregation to minimize latency while preserving temporal information. Our RVTs can be trained from scratch and achieve state-of-the-art performance on event-based object detection, with an mAP (mean average precision) of 47.2% on the Gen1 automotive dataset. Furthermore, RVTs offer fast inference times, taking less than 12 ms on a T4 GPU, and have favorable parameter efficiency, requiring five times fewer parameters compared to previous methods. This study provides valuable insights into effective design choices that can benefit research in event-based vision. The code for our work is available at https://github.com/uzh-rpg/RVT.