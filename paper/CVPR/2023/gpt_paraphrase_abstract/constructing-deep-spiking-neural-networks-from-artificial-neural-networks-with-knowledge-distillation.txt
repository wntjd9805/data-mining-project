Spiking neural networks (SNNs) are known for their high computing efficiency as they use spikes as information units, similar to biological neural systems. However, the current network structures and training methods limit the performance of SNNs. Unlike artificial neural networks (ANNs), SNNs cannot directly apply gradient descent rules for parameter adjustment due to their discrete signals. To address this limitation, we propose a new method for constructing deep SNN models using knowledge distillation (KD), where an ANN serves as the teacher model and the SNN as the student model. Through joint training, the SNN learns rich feature information from the ANN through KD, enabling efficient deep spiking structures without the need to train from scratch. Our method also requires fewer time steps for training compared to direct training or ANN to SNN approaches. Moreover, it exhibits excellent noise immunity for various types of artificial and natural signals. This novel method offers an efficient way to improve SNN performance by constructing deeper structures, making it suitable for practical scenarios that require light and efficient brain-inspired computing.