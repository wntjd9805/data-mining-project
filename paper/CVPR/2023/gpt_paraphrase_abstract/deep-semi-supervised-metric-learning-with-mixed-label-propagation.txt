Metric learning is a challenging task, especially when working with unlabeled data. Typically, pairs of data are considered similar if they are close together. However, this assumption makes it difficult to identify far-apart similar pairs and close dissimilar pairs during training. In this study, we propose a new approach to metric learning that overcomes this issue.Our method focuses on identifying hard negative pairs, which are pairs of data that receive dissimilar labels through label propagation (LP) when the edge connecting them is removed in the affinity matrix. By doing so, we are able to identify negative pairs even if they are close to each other. This information is then used to enhance LP's ability to identify far-apart positive pairs and close negative pairs.The effectiveness of our approach is demonstrated through improved performance metrics, such as recall, precision, and Normalized Mutual Information (NMI), in Content-based Information Retrieval (CBIR) applications. These results indicate that our semi-supervised metric learning method is capable of significantly enhancing performance in real-world scenarios.