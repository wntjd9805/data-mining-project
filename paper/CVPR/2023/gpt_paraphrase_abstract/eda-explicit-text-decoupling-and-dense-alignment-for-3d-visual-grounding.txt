We present a novel approach called EDA for 3D visual grounding, which involves finding objects in point clouds based on natural language descriptions. Existing methods either consider all words together or focus solely on object names, resulting in a loss of word-level information or neglect of other attributes. To address these limitations, we introduce a text decoupling module that generates textual features for each semantic component. Additionally, we propose two losses, namely position alignment loss and semantic alignment loss, to supervise the dense matching between language and point cloud objects. We also introduce a new visual grounding task that involves locating objects without object names, allowing for a comprehensive evaluation of the model's dense alignment capacity. Through experiments, we demonstrate that our approach achieves state-of-the-art performance on widely-adopted 3D visual grounding datasets, namely Scan-Refer and SR3D/NR3D, and outperforms other methods on our newly-proposed task. The source code for our approach is available at https://github.com/yanmin-wu/EDA.