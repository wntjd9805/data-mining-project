Learning inter-image similarity is crucial for self-supervised pre-training of 3D medical images, as they often share similar semantic regions. However, the lack of semantic prior in metrics and the semantic-independent variation in these images make it difficult to measure inter-image similarity reliably. This hinders the learning of consistent representations for the same semantic features. To address this challenge, we propose a novel paradigm called Geometric Visual Similarity Learning. This paradigm incorporates the prior of topological invariance into the measurement of inter-image similarity, enabling consistent representation of semantic regions. We also introduce a new geometric matching head, the Z-matching head, which facilitates the learning of global and local similarity of semantic regions, aiding in the efficient representation learning across different scale-level inter-image semantic features. Our experiments demonstrate that pre-training with our inter-image similarity learning approach improves inner-scene, inter-scene, and global-local transferring abilities in four challenging 3D medical image tasks. We will make our codes and pre-trained models publicly available.