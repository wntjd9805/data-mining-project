Recent research has revealed that deep vision models often rely too heavily on low-level or "texture" features, resulting in poor generalization. To address this issue, several data augmentation techniques have been proposed to mitigate the texture bias in deep neural networks (DNNs). In this study, we introduce a straightforward and lightweight adversarial augmentation method that explicitly encourages the network to learn holistic shapes for accurate object classification.Our augmentation approach involves overlaying edgemaps from one image onto another image with shuffled patches. The mixing proportion for this overlay is randomly determined, and the edgemap image is assigned the same label as the original image. By classifying these augmented images, the model is required to not only detect and focus on edges but also differentiate between relevant and irrelevant edges.Experimental results demonstrate that our augmentations significantly enhance classification accuracy and robustness across various datasets and neural architectures. For instance, when applied to ViT-S, we achieve absolute gains in classification accuracy of up to 6%. Moreover, we observe gains of up to 28% and 8.5% on natural adversarial and out-of-distribution datasets (such as ImageNet-A and ImageNet-R) for ViT-B and ViT-S, respectively. Through analysis using diverse probe datasets, we find that our trained models exhibit substantially increased sensitivity to shapes, which accounts for the observed improvements in robustness and classification accuracy.