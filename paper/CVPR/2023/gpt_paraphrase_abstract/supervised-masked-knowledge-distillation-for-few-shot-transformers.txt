Vision Transformers (ViTs) have shown impressive performance on computer vision tasks that have abundant data by capturing dependencies among local features. However, when it comes to few-shot learning (FSL) on small datasets with limited labeled data, ViTs tend to overfit and experience significant degradation in performance due to the absence of CNN-like inductive bias. Previous approaches in FSL have addressed this issue either by incorporating self-supervised auxiliary losses or leveraging label information in supervised settings. However, there is still a gap between self-supervised and supervised few-shot Transformers. Drawing inspiration from recent advancements in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel approach called Supervised Masked Knowledge Distillation (SMKD) for few-shot Transformers. Our model integrates label information into self-distillation frameworks. Unlike previous self-supervised methods, our approach allows intra-class knowledge distillation on both class and patch tokens and introduces the challenging task of reconstructing masked patch tokens across intra-class images. Experimental results on four benchmark datasets for few-shot classification demonstrate that our method, despite its simple design, outperforms previous approaches by a significant margin and achieves a new state-of-the-art performance. Detailed analyses further confirm the effectiveness of each component in our model. The code for our approach can be found at: https://github.com/HL-hanlin/SMKD.