We address the challenge of open-world semantic segmentation, which involves segmenting various visual concepts in images without dense annotations. Current methods in this field have made significant progress by using contrastive learning (CL) to learn different visual concepts and transferring this understanding to the segmentation task. However, these CL-based methods face a problem during testing, as they only consider alignment between images and text during training, while segmentation requires alignment between regions and text. To overcome this limitation, we propose a new framework called Text-grounded Contrastive Learning (TCL), which allows a model to learn region-text alignment directly. Our approach generates a segmentation mask based on a given text, extracts text-grounded image embeddings from the masked region, and aligns them with text embeddings using TCL. By learning region-text alignment directly, our framework encourages the model to improve the quality of generated segmentation masks. Furthermore, to ensure a fair and comprehensive evaluation, we present a unified evaluation protocol using widely used semantic segmentation datasets. TCL achieves state-of-the-art zero-shot segmentation performance with significant improvements across all datasets. The code for our framework is available at https://github.com/kakaobrain/tcl.