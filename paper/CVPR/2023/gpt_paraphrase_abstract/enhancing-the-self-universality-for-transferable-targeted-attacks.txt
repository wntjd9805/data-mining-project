We present a new method for targeted attacks called Self-Universality (SU) attack. Unlike existing methods, our approach optimizes adversarial perturbations without requiring additional training efforts on auxiliary networks. We observe that highly universal perturbations are more transferable for targeted attacks. To achieve self-universality, we make the perturbation agnostic to different local regions within an image, eliminating the need for extra data. We introduce a feature similarity loss that maximizes the similarity between adversarial perturbed global images and randomly cropped local regions, promoting universality. By making the features from adversarial perturbations more dominant than benign images, our method improves targeted transferability. Extensive experiments on an ImageNet-compatible dataset demonstrate that SU achieves high success rates, outperforming existing state-of-the-art methods by 12%. The code for our method is available at https://github.com/zhipeng-wei/Self-Universality.