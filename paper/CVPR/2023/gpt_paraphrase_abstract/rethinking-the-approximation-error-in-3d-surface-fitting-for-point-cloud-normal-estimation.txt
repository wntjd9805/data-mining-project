Current methods for estimating normal vectors in point clouds typically involve fitting a geometric surface locally and calculating the normal from the fitted surface. Recently, learning-based methods have been used to predict weights for each point, which are then used to solve the weighted least-squares surface fitting problem. However, these methods often overlook the approximation error of the fitting problem, resulting in less accurate surface fits. In this paper, we conduct a thorough analysis of the approximation error in the surface fitting problem. To improve the accuracy of estimated surface normals, we propose two design principles. Firstly, we apply the Z-direction Transform to rotate local patches, which leads to better surface fitting with reduced approximation error. Secondly, we model the error of normal estimation as a learnable term. We implement these principles using deep neural networks and integrate them with state-of-the-art normal estimation methods. Extensive experiments demonstrate that our approaches bring significant benefits to point cloud normal estimation, pushing the boundaries of state-of-the-art performance on both synthetic and real-world datasets. The code for our methods is available at https://github.com/hikvision-research/3DVision.