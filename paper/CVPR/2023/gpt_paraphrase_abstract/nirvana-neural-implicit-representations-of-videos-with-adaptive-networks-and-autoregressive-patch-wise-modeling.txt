The use of Implicit Neural Representations (INR) has proven effective for high-quality video compression. However, current approaches have limitations as they fail to utilize the temporal redundancy in videos, resulting in lengthy encoding times. Moreover, these methods have fixed architectures that are not suitable for longer videos or higher resolutions. To address these issues, we propose NIRVANA, an approach that treats videos as groups of frames and employs separate networks for each group to perform patch-wise prediction. The video representation is autoregressive, with networks trained on a current group initialized using weights from the previous group's model. In order to improve efficiency, we quantize the parameters during training, eliminating the need for post-hoc pruning or quantization. When compared to previous methods on the UVG dataset, NIRVANA enhances encoding quality from 37.36 to 37.70 (measured in PSNR) and boosts encoding speed by 12 times, while maintaining the same compression rate. Unlike previous video INR approaches that struggle with larger resolutions and longer videos, our algorithm naturally scales due to its patch-wise and autoregressive design. Additionally, our method achieves variable bitrate compression by adapting to videos with varying inter-frame motion. NIRVANA also achieves 6 times faster decoding speed, scaling well with additional GPUs, making it suitable for various deployment scenarios.