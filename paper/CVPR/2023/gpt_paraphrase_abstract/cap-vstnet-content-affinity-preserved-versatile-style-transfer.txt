This paper introduces a new framework called CAP-VSTNet to address the problem of content affinity loss in photorealistic and video style transfer. The framework includes a reversible residual network and an unbiased linear transform module. Unlike traditional reversible networks, the reversible residual network in CAP-VSTNet preserves content affinity without introducing redundant information, leading to improved stylization. Additionally, the framework utilizes Matting Laplacian training loss to tackle the pixel affinity loss problem caused by the linear transform. The experiments demonstrate that CAP-VSTNet outperforms the state-of-the-art methods in terms of qualitative and quantitative results.