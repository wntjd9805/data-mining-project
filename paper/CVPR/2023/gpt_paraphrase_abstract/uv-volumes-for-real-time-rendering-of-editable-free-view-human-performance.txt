In immersive VR/AR applications, neural volume rendering is used to create realistic renderings of human performers from any viewpoint. However, the rendering process is computationally expensive, limiting its practicality. To address this issue, we propose a new approach called UV Volumes, which can generate real-time editable free-view videos of human performers. This approach separates the high-frequency details of the human appearance from the 3D volume and encodes them into 2D neural texture stacks (NTS). The use of smooth UV volumes allows for smaller and shallower neural networks to process the 3D coordinates and detailed appearance in the 2D NTS. The mapping between the parameterized human model and the smooth texture coordinates allows for better generalization to novel poses and shapes, making the rendered videos editable. Additionally, the use of NTS enables applications such as retexturing. Extensive experiments conducted on CMUPanoptic, ZJU Mocap, and H36M datasets demonstrate that our model can render high-resolution images at an average of 30 frames per second, achieving comparable photo-realism to state-of-the-art methods. More information about the project and supplementary materials can be found at https://fanegg.github.io/UV-Volumes.