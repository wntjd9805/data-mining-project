This work focuses on improving gait recognition in real-world, 3D scenarios. While video-based gait recognition has shown success in controlled environments, it lacks the ability to capture human 3D structure information. To address this limitation, the authors propose a 3D gait recognition framework called LidarGait that extracts precise 3D gait features from point clouds. The approach involves projecting sparse point clouds into depth maps to learn representations with 3D geometry information. The authors demonstrate that this method outperforms existing point-wise and camera-based methods by a significant margin. To support their research, the authors create the SUSTech1K dataset, which is the first large-scale LiDAR-based gait recognition dataset. This dataset contains a diverse range of sequences from multiple subjects and covers various variations such as visibility, occlusions, clothing, carrying, and scenes. The experiments conducted show that 3D structure information plays a crucial role in gait recognition, LidarGait outperforms other methods, and LiDAR sensors are superior to RGB cameras for outdoor gait recognition. The source code and dataset are publicly available at https://lidargait.github.io.