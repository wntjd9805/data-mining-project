ClassiÔ¨Åcation datasets in practical settings are often labeled by humans, resulting in potentially noisy labels due to individual annotators disagreeing on the assigned labels. The inter-rater agreement can be measured, but the underlying noise distribution of the labels is usually unknown. This study aims to address this issue by: (i) demonstrating how inter-annotator statistics can be used to estimate the noise distribution of the labels, (ii) proposing methods that utilize the estimated noise distribution to learn from the noisy dataset, and (iii) establishing generalization bounds in the empirical risk minimization framework based on the estimated quantities. The paper concludes with experimental results that support the findings.