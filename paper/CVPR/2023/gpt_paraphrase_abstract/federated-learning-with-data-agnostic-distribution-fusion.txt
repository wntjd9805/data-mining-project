Federated learning is a distributed machine learning approach that aims to protect data privacy. However, a major challenge in federated learning is the presence of non-independent and non-identically distributed (non-IID) data samples across clients, which leads to slow convergence and a drop in performance of the aggregated global model. To address this issue, we propose a novel method called FedFusion. FedFusion is a data-agnostic distribution fusion-based model aggregation approach that allows for the optimization of federated learning with non-IID local datasets. It achieves this by representing the heterogeneous clients' data distributions using a global fusion component with different virtual parameters and weights. We utilize a Variational AutoEncoder (VAE) method to learn the optimal parameters of the distribution fusion components based on limited statistical information from the local models. This derived distribution fusion model is then applied to optimize federated model aggregation with non-IID data. Through extensive experiments using real-world datasets in various federated learning scenarios, we demonstrate that FedFusion significantly improves performance compared to the current state-of-the-art methods.