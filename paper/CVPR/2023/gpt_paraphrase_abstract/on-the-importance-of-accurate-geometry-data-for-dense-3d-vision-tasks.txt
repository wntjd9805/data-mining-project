Learning-based methods for solving dense 3D vision problems typically rely on training with 3D sensor data. However, the advantages and drawbacks of the distance measurement principle used in these methods are often not compared or discussed in the literature due to a lack of multi-modal datasets. Certain challenges arise when dealing with texture-less regions, reflective materials, and translucent objects, as current hardware struggles to accurately measure distances in these cases. Training models on inaccurate or corrupted data leads to biased models and limits their ability to generalize. These effects often go unnoticed when the sensor measurement is considered as ground truth during evaluation. This paper explores the impact of sensor errors on two specific dense 3D vision tasks: depth estimation and reconstruction. We demonstrate the significant influence of sensor characteristics on learned predictions and identify generalization issues that arise in real-world household environments due to different technologies. To evaluate our findings, we introduce a meticulously designed dataset consisting of measurements from various commodity sensors, including D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the substantial impact of sensor noise and lays the foundation for improved dense vision estimates and targeted data fusion.