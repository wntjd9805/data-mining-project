Current state-of-the-art methods for video object segmentation (VOS) involve matching-based approaches, where the segmentation mask for each frame is determined based on its similarity to previously processed and annotated frames. These methods only utilize the groundtruth masks for learning mask prediction and do not consider the important aspect of space-time correspondence matching, which is fundamental to this approach. To address this issue, we propose a correspondence-aware training framework that enhances matching-based VOS methods by explicitly promoting robust correspondence matching during network learning. By leveraging the inherent coherence in videos at both pixel and object levels, our algorithm reinforces the supervised training of mask segmentation with label-free, contrastive correspondence learning. Our approach does not require additional annotation cost or architectural modifications, and does not impact deployment speed. Experimental results on widely used benchmarks (DAVIS2016&2017 and YouTube-VOS2018&2019) demonstrate the effectiveness of our algorithm, outperforming existing matching-based VOS solutions.