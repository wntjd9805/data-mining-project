The study explores the potential use of human attention information as a form of weak supervision for self-supervised representation learning. However, collecting large datasets with human attention labels is costly. To address this challenge, the researchers create an auxiliary teacher model trained on a small labeled dataset to predict human attention. This teacher model is then used to generate pseudo attention labels for ImageNet. The researchers train a model with a primary contrastive objective and incorporate a simple output head to predict attention maps using the pseudo labels from the teacher model. The quality of the learned representations is evaluated based on classification performance and image retrieval tasks. The results show that the contrastive model trained with teacher guidance produces spatial-attention maps that align better with human attention compared to vanilla contrastive models. Additionally, the approach improves classification accuracy and robustness on ImageNet and ImageNet-C datasets. The model representations also demonstrate improved performance on image retrieval tasks across multiple datasets.