Semi-supervised learning is a technique used to improve the efficiency of deep models by using unlabeled data to reduce the dependency on labeled samples. While previous approaches have focused on pixel-wise consistency using convolutional neural networks (CNNs), they have not effectively addressed global learning capabilities and class-level features for unlabeled data. Recent research has shown that Transformer models achieve superior performance across various tasks. In this paper, we propose a new algorithm called SemiCVT that combines the strengths of CNNs and Transformers in a comprehensive manner. We unify existing Mean-Teacher approaches by integrating intra-model and inter-model properties for semi-supervised segmentation. Our approach involves designing a parallel CNN-Transformer architecture (CVT) with an intra-model local-global interaction schema (LGI) in Fourier domain. We also introduce inter-model class-wise consistency to complement the class-level statistics of CNNs and Transformers through cross-teaching. Extensive empirical evidence demonstrates that SemiCVT outperforms state-of-the-art methods in two public benchmarks.