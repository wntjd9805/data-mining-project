Multimodal models, like the Transformer architecture, have become more effective but still rely on specific components and training procedures for different tasks and modalities. For instance, CLIP trains separate text and image towers using contrastive loss. We propose a unified approach called CLIP-Pixels Only (CLIPPO), where a single encoder handles regular images and text rendered as images. CLIPPO achieves comparable performance to CLIP-style models in image-based tasks with fewer parameters and no text-specific components. By training CLIPPO jointly using contrastive learning, it performs well in natural language understanding tasks without word-level loss, outperforming previous pixel-based methods. Surprisingly, CLIPPO achieves good accuracy in visual question answering by rendering the question and image together. Additionally, CLIPPO demonstrates strong performance in multilingual multimodal retrieval without modifications, as it does not require tokenization.