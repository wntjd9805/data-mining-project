Current federated learning algorithms for channel-sparsity in client models do not account for the diverse features in client data, hindering collaborative training of channel neurons. To address this, we propose Flado, which customizes the sparsity of individual neurons for each client to enhance alignment of model update trajectories. Empirical results demonstrate that Flado achieves the highest task accuracies across various datasets with an unlimited budget, while also significantly reducing the floating-point operations (FLOPs) needed for training by more than 10 under the same communication budget. Additionally, Flado outperforms competing federated learning algorithms by pushing the boundaries of communication/computation trade-off further on the Pareto frontier.