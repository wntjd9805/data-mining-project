Diffusion-based generative models have achieved remarkable success in generating images. These models allow external models to control the generation process without needing to fine-tune the diffusion model. However, using off-the-shelf models for guidance is ineffective due to their poor performance on noisy inputs. The current approach is to fine-tune the guidance models using labeled data corrupted with noises. This approach has limitations in handling inputs with various noises and collecting labeled datasets for scalability.To address these limitations, we propose a novel strategy that utilizes multiple experts, each specialized in a specific noise range, to guide the reverse process of diffusion at corresponding timesteps. However, managing multiple networks and using labeled data is impractical. Therefore, we introduce a practical guidance framework called Practical Plug-And-Play (PPAP) that utilizes parameter-efficient fine-tuning and data-free knowledge transfer. We extensively conduct ImageNet class conditional generation experiments to demonstrate that our method can successfully guide diffusion with small trainable parameters and no labeled data.Furthermore, we show that image classifiers, depth estimators, and semantic segmentation models can guide publicly available GLIDE through our framework in a plug-and-play manner. The code for our framework is available at https://github.com/riiid/PPAP.