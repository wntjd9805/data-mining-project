We introduce a straightforward method to transform a ViT encoder into a highly efficient video model that can handle image and video inputs seamlessly. By selectively sampling the inputs, the model can perform training and inference for both input types. This approach allows for easy scalability and adaptation to large-scale pre-trained ViTs without the need for complete fine-tuning. Importantly, our model achieves state-of-the-art results.