We demonstrate, for the first time, that neural networks trained exclusively on synthetic data achieve state-of-the-art accuracy in estimating 3D human pose and shape from real images. Previous synthetic datasets were either small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is challenging, but we successfully accomplish this for full-body motion using our BED-LAM dataset. BED-LAM consists of monocular RGB videos with accurately annotated 3D body shapes in SMPL-X format, encompassing a wide range of body shapes, motions, skin tones, hair, and clothing that is realistically simulated using commercial clothing physics simulation. We generate diverse scenes with varying lighting, camera motions, and numbers of people. By training various HPS regressors on BED-LAM, we achieve state-of-the-art accuracy on benchmarks with real images, surpassing the performance of models trained with synthetic data. Through BED-LAM, we gain insights into the important design choices that contribute to accuracy. With high-quality synthetic training data, we find that a basic approach like HMR can match the accuracy of the current leading method (CLIFF). BED-LAM is valuable for various tasks, and we provide access to all images, ground truth bodies, 3D clothing, support code, and more for research purposes. Furthermore, we offer detailed information about our synthetic data generation pipeline, enabling others to create their own datasets. Please visit our project page for more details: https://bedlam.is.tue.mpg.de/.