This work presents an architecture for Video Question Answering (VQA) that focuses on predicting situation hyper-graphs in order to answer questions about video content. The proposed method trains a situation hyper-graph decoder to identify graph representations with actions and object/human-object relationships from the input video clip. It also utilizes cross-attention between the predicted situation hyper-graphs and the question embedding to predict the correct answer. The architecture is trained in an end-to-end manner and optimized using a VQA loss with the cross-entropy function and a Hungarian matching loss for the situation graph prediction. The effectiveness of the proposed approach is evaluated on two challenging benchmarks (AGQA and STAR), demonstrating significant performance improvement in video question-answering tasks. The answer format outputs only the abstraction.