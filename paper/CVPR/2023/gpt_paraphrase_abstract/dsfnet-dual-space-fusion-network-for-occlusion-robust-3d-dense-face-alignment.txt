Existing monocular 3D dense face alignment methods are limited in their usage scenarios due to their sensitivity to severe occlusion and large view angles. The current state-of-the-art method, which is based on 3D Morphable Models (3DMM), only regresses the model's coefficients and does not fully utilize the 2D spatial and semantic information available, which can provide valuable cues for face shape and orientation.   In this study, we propose a solution to the occlusion and view angle problems by jointly modeling 3D facial geometry in both image and model space. Instead of directly predicting the entire face, we first regress image space features in the visible facial region using dense prediction. Then, we use the regressed features of the visible regions to predict our model's coefficients, taking advantage of the prior knowledge of the whole face geometry from the morphable models to complete the invisible regions.   To achieve high robustness and accuracy in unconstrained scenarios, we introduce a fusion network that combines the strengths of both image and model space predictions. Our fusion module makes our method robust to occlusion and large pitch and roll view angles, as well as noise and large yaw angles. Through comprehensive evaluations, we demonstrate that our method outperforms state-of-the-art methods on the 3D dense face alignment task, achieving a Normalized Mean Error (NME) of 3.80% on the AFLW2000-3D dataset, which is a 5.5% improvement. The code for our method is available at https://github.com/lhyfst/DSFNet.