Video-text retrieval is a growing field in computer vision and natural language processing, focused on finding relevant videos based on text queries. This paper addresses the challenging task of Unsupervised Domain Adaptation Video-text Retrieval (UDAVR), where training and testing data come from different distributions. Previous approaches have mainly focused on reducing the domain shift, but have overlooked the issue of misalignment between target videos and texts. To address this, the authors propose a novel method called Dual Alignment Domain Adaptation (DADA). The DADA method first introduces cross-modal semantic embedding to generate discriminative source features in a joint embedding space. Additionally, video and text domain adaptations are utilized to balance the minimization of domain shifts. To address the misalignment in the target domain, the authors propose the Dual Alignment Consistency (DAC) approach, which leverages the semantic information from both modalities. DAC adaptively aligns video-text pairs that are more likely to be relevant in the target domain, gradually increasing the number of positive pairs while potentially aligning noisy pairs in later stages. This ensures that more truly aligned target pairs are generated and maintains the discriminability of target features. Experimental results demonstrate that DADA outperforms state-of-the-art methods, achieving significant improvements of 20.18% and 18.61% in R@1 under the TGIF→MSR-VTT and TGIF→MSVD settings, respectively. This highlights the superiority of the proposed method.