Multimodal learning (MML) seeks to overcome the limitations of individual modalities by leveraging their shared characteristics. However, existing MML methods often face the challenge of "modality imbalance" due to optimizing a uniform objective for different modalities, leading to suboptimal performance. To address this issue, some approaches adjust the learning pace based on the fused modality, which is dominated by the stronger modality, resulting in limited improvement for the weaker modality.   To better exploit the multimodal features, we propose Prototypical Modality Rebalance (PMR) which focuses on enhancing the learning of the slower modality without interference from other modalities. Our approach involves using prototypes that represent general features for each class and constructing non-parametric classifiers for evaluating the performance of each modality individually. We then aim to accelerate the learning of the slower modality by improving its clustering towards the prototypes.   Additionally, to mitigate the suppression caused by the dominant modality, we introduce a prototype-based entropy regularization term during the early stages of training to prevent premature convergence. Importantly, our method is independent of specific model structures and fusion methods, relying solely on the representations of each modality. This makes it highly applicable in various scenarios. The source code for our method is available for reference.