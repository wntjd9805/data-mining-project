Unsupervised semantic segmentation has emerged as a solution to the high demand for manpower in labeling pixel-level annotations. While previous studies using the vision transformer (ViT) backbone have shown promising results, there are still shortcomings in terms of task-specific training guidance and local semantic consistency. To address these issues, we propose a method that utilizes contrastive learning to uncover hidden positives and learn semantic relationships for improved consistency in local regions. This involves identifying two types of global hidden positives, namely task-agnostic and task-specific, for each anchor, based on feature similarities derived from a pre-trained backbone and a segmentation head-in-training. By gradually increasing the contribution of task-specific hidden positives, the model becomes more adept at capturing task-specific semantic features. Additionally, we introduce a gradient propagation strategy to learn semantic consistency between neighboring patches, operating under the assumption that nearby patches are likely to share the same semantics. This strategy involves adding loss functions to propagate to local hidden positives and semantically similar nearby patches, with the magnitude determined by predefined similarity scores. Through these training schemes, our proposed method achieves state-of-the-art results in various datasets including COCO-stuff, Cityscapes, and Potsdam-3. The code for our method is publicly available at the following GitHub repository: https://github.com/hynnsk/HP.