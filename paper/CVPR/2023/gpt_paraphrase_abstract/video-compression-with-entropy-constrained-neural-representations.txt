The use of neural networks to encode videos has been proposed as a new approach for video processing. However, traditional techniques still perform better than neural video representation (NVR) methods in video compression. The reason for this performance gap is that current NVR methods do not efficiently capture both temporal and spatial information, and they minimize rate and distortion separately. To address this, we propose a new convolutional architecture for video representation that better captures spatio-temporal information. We also introduce a training strategy that optimizes rate and distortion together. Unlike previous methods, our approach learns all network and quantization parameters jointly, eliminating the need for post-training operations. We evaluate our method on the UVG dataset and achieve state-of-the-art results in video compression with NVRs. Additionally, our method surpasses the commonly used HEVC benchmark, closing the gap to autoencoder-based video compression techniques. Our approach produces sharper frames at lower bits per pixel (bpp), as shown in Figure 1.