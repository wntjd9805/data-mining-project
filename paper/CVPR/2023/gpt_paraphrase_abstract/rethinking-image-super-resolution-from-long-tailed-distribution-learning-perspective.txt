Prior research has observed that enhancing the resolution of low-frequency regions is easier than that of high-frequency regions. Although efforts have been made to address this issue, there is limited understanding of its underlying causes. This paper aims to provide a plausible explanation from a machine learning perspective, specifically the twin fitting problem resulting from the distribution of pixels in natural images. The authors propose redefining image super resolution (SR) as a long-tailed distribution learning problem and bridging the gap between low and high-level vision tasks. They introduce a solution that balances the gradients from pixels in the low and high-frequency regions by incorporating a static and learnable structure prior. The proposed SR model achieves better balance in fitting both regions, resulting in improved overall performance. Experimental evaluations on various SR models and datasets validate the effectiveness of the proposed solution.