A new approach is proposed for learning 3D representations for perception tasks in autonomous driving using self-supervised image features. The existing framework faces challenges due to self-similarity and class imbalance in the datasets. To address the self-similarity problem, the proposed method introduces a semantically tolerant image-to-point contrastive loss that considers the semantic distance between positive and negative image regions. This helps to maintain the local semantic structure of the learned representations. Additionally, a class-agnostic balanced loss is designed to tackle the issue of class imbalance by approximating the degree of imbalance through a semantic similarity measure. Experimental results demonstrate that the proposed approach significantly improves 2D-to-3D representation learning for 3D semantic segmentation compared to state-of-the-art frameworks. It consistently outperforms existing methods across various 2D self-supervised pretrained models.