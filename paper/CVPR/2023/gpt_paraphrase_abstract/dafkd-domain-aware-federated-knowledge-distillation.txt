Federated Distillation (FD) has gained attention for its efficiency in combining diverse models trained on distributed client data. However, existing FD methods overlook the diversity of these models, leading to reduced performance when some models lack knowledge about certain samples. To address this, we propose DaFKD, a new approach that treats the local data in each client as a specific domain. DaFKD leverages a domain discriminator for each client to identify the correlation between a sample and its domain. To optimize the ensemble of soft predictions from diverse models, we share partial parameters of the domain discriminator with the classification model. Extensive experiments demonstrate that our method improves model accuracy by up to 6.02% compared to state-of-the-art baselines.