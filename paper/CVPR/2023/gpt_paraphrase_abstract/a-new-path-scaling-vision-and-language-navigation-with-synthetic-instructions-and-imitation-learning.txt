Recent studies have focused on training RL agents in Vision-and-Language Navigation (VLN) to understand and execute human instructions in realistic environments. However, due to limited human instruction data and training environment diversity, these agents still struggle with language grounding and spatial understanding. Although pretraining on large text and image-text datasets has been explored, the improvements have been minimal. To address this, we investigate the use of large-scale augmentation with synthetic instructions. We use over 500 indoor environments captured in densely-sampled 360-degree panoramas and generate navigation trajectories and visually-grounded instructions for each trajectory using a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. This results in a dataset of 4.2 million instruction-trajectory pairs, which is significantly larger than existing human-annotated datasets and includes a wider variety of environments and viewpoints. To effectively utilize this large-scale data, we train a simple transformer agent using imitation learning. Our approach outperforms existing RL agents on the challenging RxR dataset, improving the state-of-the-art NDTW scores in both seen and unseen environments. This work suggests a new approach to improving instruction-following agents by emphasizing large-scale training on high-quality synthetic instructions.