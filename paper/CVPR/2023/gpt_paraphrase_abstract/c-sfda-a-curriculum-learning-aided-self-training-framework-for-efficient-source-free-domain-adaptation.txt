Unsupervised domain adaptation (UDA) methods aim to adapt models trained on labeled data from one domain to unlabeled data from another domain. In contrast, source-free domain adaptation (SFDA) is a more practical approach as it does not require access to the source data during adaptation. However, current SFDA methods primarily rely on pseudo-label refinement through self-training, which has two main issues: 1) the presence of noisy pseudo-labels can lead to early memorization during training, and 2) maintaining a memory bank for the refinement process is resource-intensive. To address these concerns, we propose C-SFDA, a curriculum learning aided self-training framework for SFDA that efficiently and reliably adapts to domain changes using selective pseudo-labeling. We incorporate a curriculum learning scheme, which promotes learning from a limited number of reliable pseudo-labels. This simple yet effective step prevents the propagation of label noise during different stages of adaptation and eliminates the need for a memory-bank based refinement process. Through extensive experiments on image recognition and semantic segmentation tasks, we confirm the effectiveness of our method. Additionally, C-SFDA can also be applied to online test-time domain adaptation and outperforms previous state-of-the-art methods in this task.