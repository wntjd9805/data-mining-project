Multi-task learning is a useful approach for solving multiple tasks simultaneously, but existing algorithms are not well-suited for evaluating multi-task methods in autonomous driving. To address this, we extensively evaluate popular multi-task methods on a large-scale driving dataset, focusing on object detection, semantic segmentation, drivable area segmentation, and lane detection. Our analysis shows that current methods have made progress but still lag behind single-task baselines. To overcome this, we propose a new multi-task framework called VE-Prompt. VE-Prompt incorporates visual exemplars through task-specific prompts to guide the model in learning high-quality task-specific representations. We generate visual exemplars based on bounding boxes and color-based markers to improve the accuracy of target category appearances and reduce the performance gap. Additionally, we combine transformer-based encoders and convolutional layers to enable efficient and accurate perception in autonomous driving. Experimental results on the BDD100K dataset demonstrate that VE-Prompt outperforms the multi-task baseline and even surpasses single-task models.