Recent advancements in 3D object detection (3DOD) have yielded impressive results for LiDAR-based models. However, surround-view 3DOD models that rely on multiple camera images perform poorly due to the need for transforming features from a perspective view to a 3D world representation, which is challenging due to the lack of depth information. This study presents X3KD, a comprehensive framework for knowledge distillation in multi-camera 3DOD, encompassing different modalities, tasks, and stages. The proposed framework involves cross-task distillation from an instance segmentation teacher (X-IS) in the perspective view feature extraction stage, allowing for supervision without the ambiguity caused by error backpropagation through view transformation. Additionally, cross-modal feature distillation (X-FD) and adversarial training (X-AT) are employed to enhance the 3D world representation of multi-camera features using information from a LiDAR-based 3DOD teacher. Furthermore, the teacher model is utilized for cross-modal output distillation (X-OD), providing dense supervision during the prediction stage. A thorough analysis of knowledge distillation at various stages of multi-camera 3DOD is conducted, and the final X3KD model surpasses previous state-of-the-art approaches on the nuScenes and Waymo datasets, while also demonstrating generalizability to RADAR-based 3DOD. For qualitative results, a video can be viewed at https://youtu.be/1do9DPFmr38.