Recently, pretraining visual models on large-scale image-caption datasets has become a popular alternative to traditional pretraining on image classification data. These image-caption datasets are considered more "open-domain" as they encompass a wider range of scene types and vocabulary words, resulting in models that perform well in tasks involving few- and zero-shot recognition.  However, while image-caption datasets offer broader coverage, large-scale classification datasets offer more detailed categories with a balanced distribution of labels. To take advantage of the benefits of both types of datasets, this study explores a pretraining strategy that combines classification and caption datasets.  Initially, the researchers discovered that naively merging the datasets leads to suboptimal performance in downstream zero-shot recognition tasks. This occurs because the model is influenced by dataset bias, where the coverage of image domains and vocabulary words differs between the two datasets.  To address this issue, the researchers propose a method called "Prefix Conditioning." This simple yet effective technique helps to separate dataset biases from visual concepts by introducing prefix tokens during training. These prefix tokens inform the language encoder about the type of input data (classification or caption), allowing the language encoder to learn from both datasets and customize feature extraction for each dataset.  The beauty of prefix conditioning lies in its versatility, as it can be easily integrated into existing visual-linguistic pretraining objectives such as CLIP or UniCL. Through experiments, the researchers demonstrate that prefix conditioning enhances zero-shot image recognition and improves the model's robustness to distribution shifts at the image level.  In summary, this study introduces a pretraining strategy that combines classification and caption datasets by using prefix conditioning to mitigate dataset biases. The results show improved performance in zero-shot image recognition tasks and increased resilience to image-level distribution shifts.