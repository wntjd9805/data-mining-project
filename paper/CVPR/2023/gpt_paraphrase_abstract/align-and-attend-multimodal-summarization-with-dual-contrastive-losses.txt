Multimodal summarization aims to extract key information from different sources to create comprehensive summaries. Unlike unimodal summarization, multimodal summarization utilizes cross-modal information to improve the quality of summaries. However, existing methods do not consider the temporal correspondence between modalities and the correlation between different samples. To address this, we propose Align and Attend Multimodal Summarization (A2Summ), a unified transformer-based model that effectively aligns and attends to multimodal input. Additionally, we introduce two contrastive losses to capture inter-sample and intra-sample correlations. Through extensive experiments on standard video summarization datasets (TVSum and SumMe) and multimodal summarization datasets (Daily Mail and CNN), we show that A2Summ outperforms existing methods, achieving state-of-the-art performance on all datasets. Furthermore, we have created a large-scale multimodal summarization dataset called BLiSS, which includes livestream videos, transcribed texts, and annotated summaries. The code and dataset for A2Summ are publicly available at https://boheumd.github.io/A2Summ/.