Open-vocabulary detection (OVD) is a task that involves detecting objects from categories that are not included in the base categories on which the detector is trained. Recent OVD methods utilize large-scale visual-language pre-trained models, like CLIP, to recognize these novel objects. However, there are two main challenges in incorporating these models into detector training: (1) the distribution mismatch that occurs when applying a VL-model trained on entire images to region recognition tasks, and (2) the difficulty of localizing objects from unseen classes. To address these challenges, we propose CORA, a framework inspired by DETR, which adapts CLIP for Open-vocabulary detection through Region prompting and Anchor pre-matching. Region prompting helps bridge the gap between the whole-to-region distribution by prompting the region features of the CLIP-based region classifier. Anchor pre-matching aids in learning generalizable object localization by employing a class-aware matching mechanism. We evaluate the performance of CORA on the COCO OVD benchmark and achieve an AP50 of 41.7 on novel classes, surpassing the previous state-of-the-art by 2.4 AP50 even without using additional training data. When extra training data is available, we train CORA+ using both ground-truth base-category annotations and additional pseudo bounding box labels computed by CORA. CORA+ achieves an AP50 of 43.1 on the COCO OVD benchmark and a box APr of 28.1 on the LVIS OVD benchmark. The code for CORA is publicly available at https://github.com/tgxs002/CORA.