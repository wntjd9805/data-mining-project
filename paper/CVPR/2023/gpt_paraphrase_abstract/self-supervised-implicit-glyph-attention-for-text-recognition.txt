The attention mechanism is widely used in scene text recognition (STR) methods because it can extract character-level representations. There are two types of attention mechanisms: implicit attention and supervised attention. Implicit attention is learned from sequence-level text annotations and can suffer from alignment-drifted issues. Supervised attention, on the other hand, requires character-level bounding box annotations and is memory-intensive for languages with larger character categories. To address these issues, we propose a novel attention mechanism called self-supervised implicit glyph attention (SIGA). SIGA combines self-supervised text segmentation and implicit attention alignment to improve attention correctness without the need for extra character-level annotations. Experimental results show that SIGA outperforms previous attention-based STR methods in terms of attention correctness and recognition performance on various benchmarks.