We present a solution to the problem of reconstructing clothed humans from a single image or multiple uncalibrated views. Existing methods struggle with accurately capturing the detailed geometry of clothed humans and often require a calibrated setup for multi-view reconstruction. Our proposed framework overcomes these limitations by utilizing the parametric SMPL-X model and allowing for an arbitrary number of input images. The key component of our framework is the self-evolved signed distance field (SeSDF) module, which enables the learning of deformations in the signed distance field derived from the fitted SMPL-X model. This allows for the encoding of detailed geometry that closely resembles the actual clothed human, resulting in improved reconstruction quality. Additionally, we introduce a simple method for self-calibrating multi-view images using the fitted SMPL-X parameters, eliminating the need for manual calibration and enhancing the flexibility of our approach. We also incorporate an occlusion-aware feature fusion strategy to effectively integrate the most relevant features for human model reconstruction. Through comprehensive evaluations on public benchmarks, we demonstrate the superior performance of our framework in terms of both qualitative and quantitative measures.