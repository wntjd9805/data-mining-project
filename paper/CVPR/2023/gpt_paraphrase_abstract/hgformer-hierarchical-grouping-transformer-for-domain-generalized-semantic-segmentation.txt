Current semantic segmentation models have been successful in scenarios where the test data is similar to the training data. However, in real-world applications, the test data may come from a different domain than the training data, making it necessary to improve the model's ability to handle domain differences. This study focuses on semantic segmentation under the domain generalization setting, where the model is trained on the source domain and tested on an unseen target domain. Previous research has shown that Vision Transformers are more robust than CNNs due to their self-attention's visual grouping property. In this work, we propose a novel hierarchical grouping transformer (HGFormer) that explicitly groups pixels into part-level masks and then whole-level masks. These masks at different scales aim to segment both parts and the whole of classes. HGFormer combines mask classification results from both scales to predict class labels. We conduct experiments using seven public semantic segmentation datasets to evaluate HGFormer's performance in various cross-domain settings. The results demonstrate that HGFormer outperforms previous methods significantly, yielding more robust semantic segmentation results compared to per-pixel classification methods and flat-grouping transformers. The code for HGFormer can be accessed at https://github.com/dingjiansw101/HGFormer.Figure 1 illustrates the concept of semantic segmentation, which involves dividing an image into classification units (regions) and then classifying these units. These units can range from pixels to large masks. Mask classification, which aggregates features over large image regions of the same class, is intuitively more robust than per-pixel classification. However, directly grouping pixels into whole-level masks is challenging, especially when there is a distribution shift (e.g., Gaussian noise). To address this problem, we propose a hierarchical grouping approach that first groups pixels into part-level masks and then groups these part-level masks into whole-level masks to obtain reliable masks. We combine the classification of both part-level and whole-level masks for robust semantic segmentation, as the masks at the two levels capture complementary information.