Current Weakly-supervised Temporal Action Localization (WTAL) methods face challenges due to the absence of temporal annotation. This paper aims to improve WTAL by utilizing text information. Two approaches are proposed: (a) a discriminative objective to increase the difference between action classes and reduce over-completeness, and (b) a generative objective to enhance intra-class integrity and identify more complete temporal boundaries. For the discriminative objective, a Text-Segment Mining (TSM) mechanism is introduced. TSM constructs a text description based on the action class label and uses it to mine class-related segments. Without temporal annotation, TSM compares the text query with the entire video dataset to find the best matching segments while ignoring irrelevant ones. However, TSM alone is too restrictive as it neglects semantic-related segments shared by different video categories, leading to incomplete localization. To address this, a generative objective called Video-text Language Completion (VLC) is introduced. VLC focuses on semantic-related segments in videos to complete the text sentence. The proposed method achieves state-of-the-art performance on THUMOS14 and ActivityNet1.3 datasets. Surprisingly, it can also be seamlessly applied to existing methods, significantly improving their performance. The code is available at https://github.com/lgzlIlIlI/Boosting-WTAL.