Few-shot class-incremental learning (FSCIL) aims to continuously learn to classify new classes with limited samples without forgetting the old classes. The conventional approach in FSCIL involves training with the cross-entropy (CE) loss in the base session and freezing the feature extractor for adapting to new classes. However, we have discovered that the CE loss is not optimal for base session training as it fails to achieve good class separation in representations, leading to a degradation in generalization to novel classes. One possible method to address this issue is to incorporate naive supervised contrastive learning (SCL) in the base session. Unfortunately, although SCL improves representation separation among different base classes to some extent, it still struggles to separate base classes from new classes.Motivated by these findings, we propose a novel approach called Semantic-Aware Virtual Contrastive model (SAVC) to enhance the separation between new classes and base classes by introducing virtual classes to SCL. These virtual classes are generated using predefined transformations and serve as placeholders for unseen classes in the representation space, providing diverse semantic information. By learning to recognize and contrast in the fantasy space created by virtual classes, our SAVC significantly improves base class separation and generalization to novel classes, achieving state-of-the-art performance on three widely-used FSCIL benchmark datasets. The code for SAVC is available at: https://github.com/zysong0113/SAVC.