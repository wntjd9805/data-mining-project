Convolutional neural networks (CNNs) have achieved impressive results in various tasks, but their decision-making process lacks transparency and interpretability, hindering further performance improvement. To address this, there has been a growing interest in explainable artificial intelligence (XAI) for CNNs. While there have been successful attempts to explain 2D image classification CNNs, the explanation of 3D video recognition CNNs is less explored, and none have provided high-level explanations. This paper proposes a spatial-temporal concept-based explanation (STCE) framework for interpreting 3D CNNs. The approach represents videos using high-level supervoxels, clustering similar supervoxels as concepts that are understandable to humans. The framework assigns a score to each concept, indicating its significance in the CNN decision process. Experiments on diverse 3D CNNs demonstrate that the proposed method can identify global concepts with varying levels of importance, enabling a deeper understanding of their impact on tasks like action recognition. The source codes for this approach are publicly available at https://github.com/yingji425/STCE.