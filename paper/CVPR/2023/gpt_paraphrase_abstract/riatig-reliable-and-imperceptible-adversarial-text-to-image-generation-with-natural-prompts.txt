The text-to-image generation field has made significant progress in creating realistic images, but there is a concern about the security risks associated with this technology. However, there has been limited research on the robustness of these models from an adversarial perspective. Previous studies have focused on untargeted attacks and have not considered reliability and imperceptibility. In this paper, we propose RIATIG, an adversarial attack that generates reliable and imperceptible prompts for text-to-image models using inconspicuous examples. Our attack formulates the prompt crafting as an optimization process and solves it using a genetic-based method. We evaluate the effectiveness and stealthiness of our attack on six popular text-to-image models in both white-box and black-box settings. We have made the artifacts available for further research and development.