Sign languages are visual forms of communication that use handshape, facial expression, and body movement. However, there are many visually similar signs in sign languages, known as VISigns, which make it difficult for vision neural networks to differentiate between them. To address this issue, we propose the Natural Language-Assisted Sign Language Recognition (NLA-SLR) framework. This framework utilizes the semantic information contained in glosses, or sign labels, to improve recognition accuracy. Firstly, for VISigns with similar meanings, we introduce a language-aware label smoothing technique. This involves generating soft labels for each training sign based on the normalized semantic similarities between glosses. These soft labels assist in training the neural network by providing more context and reducing confusion between visually similar signs. Secondly, for VISigns with distinct meanings, we employ an inter-modality mixup technique. This technique combines both visual and gloss features to enhance the separability of different signs. By blending the features and supervising the learning process with blended labels, the network is better able to distinguish between visually similar signs with different meanings. Additionally, we introduce a new backbone network called the video-keypoint network. This network not only models RGB videos and human body keypoints but also learns from sign videos with varying temporal receptive fields. This allows the network to capture more comprehensive information and improve recognition performance. Through empirical evaluation, our proposed method achieves state-of-the-art results on three widely-used benchmarks for sign language recognition: MSASL, WLASL, and NMFs-CSL. The code for our method is available at https://github.com/FangyunWei/SLRT.