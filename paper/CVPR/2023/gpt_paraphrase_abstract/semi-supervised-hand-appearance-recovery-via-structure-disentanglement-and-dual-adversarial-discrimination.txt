We collected a large dataset of hand images with accurate annotations using marker-based motion capture technology. However, the presence of markers limits the usefulness of these images for reconstructing hand appearance. To address this issue, we propose a novel approach that involves disentangling the structure of the bare hand from the degraded images and then mapping the appearance onto this structure using a dual adversarial discrimination scheme. Our method takes advantage of both the modeling capabilities of Vision Transformer (ViT) for structure disentanglement and the dual discrimination on translation processes and results for enhancing the image-to-image translation. Through comprehensive evaluations, we demonstrate that our framework can effectively recover realistic hand appearance from various datasets containing markers and even objects occluding the hand. This approach opens up new possibilities for acquiring bare hand appearance data to address other related learning problems.