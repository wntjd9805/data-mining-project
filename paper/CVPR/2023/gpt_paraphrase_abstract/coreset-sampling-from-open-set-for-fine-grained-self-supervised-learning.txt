Deep learning has been widely used in various domains, but it faces challenges when applied to fine-grained tasks. These challenges include the need for expert knowledge for annotation and the requirement for a versatile model that can be used for different tasks within a specific domain. However, recent advancements in self-supervised learning (SSL) offer a promising solution for pretraining models without annotations, which can then be used for downstream tasks. SSL utilizes large-scale unlabeled datasets, known as open-sets, and does not rely on annotations. In this study, we propose a novel problem called Open-Set Self-Supervised Learning, assuming the availability of both a large-scale unlabeled open-set dataset and a fine-grained target dataset during the pretraining phase. To address the distribution mismatch between the open-set and target dataset, we introduce the SimCore algorithm, which samples a coreset from the open-set that has the closest similarity to the target dataset in the latent space. Through extensive experiments on eleven fine-grained datasets and seven open-sets, we demonstrate that SimCore significantly improves representation learning performance in various downstream tasks.