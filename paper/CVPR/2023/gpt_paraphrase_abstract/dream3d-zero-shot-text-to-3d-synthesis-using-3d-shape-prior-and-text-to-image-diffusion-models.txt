Recent advancements in CLIP-guided 3D optimization techniques, such as DreamFields and PureCLIPNeRF, have yielded impressive outcomes in text-to-3D synthesis without any prior knowledge. However, these methods often struggle to produce precise and faithful 3D structures that align with the input text due to their reliance on scratch training and random initialization. In this study, we propose the incorporation of explicit 3D shape priors into the CLIP-guided 3D optimization process. Initially, we generate a high-quality 3D shape as a prior based on the input text in the text-to-shape stage. This shape serves as the initialization for a neural radiance field, which is then optimized using the complete prompt. To tackle the challenging task of text-to-shape generation, we introduce a straightforward yet effective approach that directly connects the text and image modalities through a robust text-to-image diffusion model. To bridge the style gap between images synthesized by the text-to-image diffusion model and shape renderings used for training the image-to-shape generator, we propose joint optimization of a learnable text prompt and fine-tuning of the text-to-image diffusion model for rendering-style image generation. Our method, Dream3D, surpasses state-of-the-art techniques by generating imaginative 3D content with superior visual quality and shape accuracy. Further details and results can be found on our project page at https://bluestyle97.github.io/dream3d/.