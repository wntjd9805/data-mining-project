Deep neural networks are vulnerable to adversarial attacks because of the accumulation of perturbations in the feature level. Previous research has focused on deactivating non-robust feature activations to enhance model robustness. However, we argue that these malicious activations still contain valuable information for accurate predictions. Therefore, we propose a new approach called Feature Separation and Recalibration (FSR) to recalibrate these non-robust activations and improve the robustness of feature maps. FSR disentangles the input feature map into robust features that aid in correct predictions and non-robust features that contribute to mispredictions during adversarial attacks. The recalibration step adjusts the non-robust activations to restore potentially useful cues for model predictions. Extensive experiments demonstrate that FSR outperforms traditional deactivation techniques and enhances the robustness of existing adversarial training methods by up to 8.57% with minimal computational overhead. The code for FSR is available at https://github.com/wkim97/FSR.