Combining LiDAR and camera data is crucial for accurate 3D object detection in autonomous driving systems. However, this task is challenging because these modalities have different features and levels of granularity. Previous approaches have explored the semantic information of camera features by converting 2D points into 3D space and incorporating 2D semantics through cross-modal interaction or fusion techniques. However, these approaches overlook the depth information when converting points into 3D space, which leads to unreliable fusion of 2D semantics with 3D points. Additionally, their fusion strategies, such as concatenation or attention, are ineffective in fusing 2D and 3D information or lack fine-grained interactions in the voxel space.To address these shortcomings, we propose a novel framework that better utilizes depth information and enables fine-grained cross-modal interaction between LiDAR and camera. Our framework consists of two key components. First, we use a Multi-Depth Unprojection (MDU) method to improve the quality of depth information for lifted points at each interaction level. Second, we apply a Gated Modality-Aware Convolution (GMA-Conv) block to finely modulate voxels associated with the camera modality and aggregate multi-modal features into a unified space. Together, these components provide the detection head with more comprehensive features from both LiDAR and camera.On the nuScenes test benchmark, our proposed method, called MSMD-Fusion, achieves state-of-the-art results in 3D object detection and tracking tasks without relying on test-time augmentation or ensemble techniques. The code for our method is available at the provided GitHub link.