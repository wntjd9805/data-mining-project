Transformers-based methods have been successful in image deraining by incorporating non-local information for high-quality image reconstruction. However, existing Transformers use all similarities between query and key tokens for feature aggregation, which can introduce interference and hinder clear image restoration. To address this issue, we introduce Sparse Transformer (DRSformer), an adaptive network that selectively retains the most useful self-attention values for feature aggregation. We employ a learnable top-k selection operator to retain crucial attention scores from the keys for each query, improving feature aggregation. Additionally, we develop a mixed-scale feed-forward network to capture multi-scale information essential for latent clear image restoration. To enhance feature learning, we integrate a mixture of experts feature compensator that combines local context from CNN operators. Extensive experiments on benchmark datasets demonstrate that our proposed method outperforms state-of-the-art approaches. The source code and trained models can be found at https://github.com/cschenxiang/DRSformer.