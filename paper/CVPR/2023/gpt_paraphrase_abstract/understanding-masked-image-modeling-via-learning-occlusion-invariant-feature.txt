Recently, Masked Image Modeling (MIM) has achieved significant success in self-supervised visual recognition. However, the workings of MIM remain unclear, as it differs greatly from previously studied siamese approaches like contrastive learning. In this study, we propose a new perspective: MIM implicitly learns features that are invariant to occlusion, similar to siamese methods that learn other types of invariance. By reformulating MIM into an equivalent siamese form, we can interpret MIM methods within a unified framework alongside conventional methods, with the only differences being the choice of data transformations (i.e., what invariance to learn) and similarity measurements. Additionally, using MAE as an example of MIM, we empirically demonstrate that the success of MIM models is not heavily reliant on the choice of similarity functions, but rather on the learned occlusion invariant feature introduced by masked images. This feature, despite being less semantic, proves to be a favorable initialization for vision transformers. Our findings aim to inspire researchers in the computer vision community to develop more powerful self-supervised methods.