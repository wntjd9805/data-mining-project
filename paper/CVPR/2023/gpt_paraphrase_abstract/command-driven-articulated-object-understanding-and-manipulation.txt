We introduce Cart, a novel method for manipulating articulated objects based on human commands. In addition to previous research that focuses on inferring articulation structures, Cart goes further by enabling the manipulation of articulated shapes to align them according to simple command templates. The key feature of Cart is its ability to use predicted object structures to connect visual observations with user commands, resulting in effective manipulations. This is achieved through the encoding of command messages for motion prediction and a test-time adaptation that adjusts the movement based solely on command supervision. Cart demonstrates accurate manipulation of object shapes across a wide range of object categories, surpassing the performance of existing approaches in understanding the inherent articulation structures. It also exhibits strong generalization capabilities to previously unseen object categories and real-world objects. We believe that Cart has the potential to pave the way for instructing machines in operating articulated objects. The code for Cart can be accessed at https://github.com/dvlab-research/Cart.