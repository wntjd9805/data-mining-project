This study focuses on open-vocabulary detection, which aims to identify new objects based on text descriptions. The authors propose a strategy for open-vocabulary 3D point-cloud detection that involves two main steps. Firstly, they develop a point-cloud detector that can accurately localize various objects. Secondly, they establish a connection between textual and point-cloud representations to enable the detector to classify novel object categories. To achieve this, the authors utilize rich image pre-trained models and employ predicted 2D bounding boxes from 2D pre-trained detectors to supervise the learning process of the point-cloud detector. Additionally, they introduce a novel de-biased triplet cross-modal contrastive learning method to connect image, point-cloud, and text modalities, leveraging vision-language pre-trained models like CLIP. The use of image and vision-language pre-trained models allows for open-vocabulary 3D object detection without the need for 3D annotations. Experimental results on the ScanNet and SUN RGB-D datasets demonstrate that the proposed method outperforms various baselines, with improvements of at least 3.03 points and 7.47 points, respectively. The authors also provide a detailed analysis to explain the effectiveness of their approach. The code for this study is available at https://github.com/lyhdet/OV-3DET.