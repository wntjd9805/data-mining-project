Neural Radiance Fields (NeRF) have gained significant attention recently for their impressive ability to reconstruct 3D scenes and synthesize new views from a set of camera images. However, previous methods assume high-quality input images, whereas real-world scenarios often involve image degradation due to factors like motion blur and low-light conditions. This degradation negatively impacts the rendering quality of NeRF. In this study, we propose a novel approach called Bundle Adjusted Deblur Neural Radiance Fields (BAD-NeRF) that can handle severe motion blur and inaccurate camera poses. Our method models the physical image formation process of a motion-blurred image and simultaneously learns the parameters of NeRF while recovering camera motion trajectories during exposure. Experimental results demonstrate that by directly modeling the real physical image formation process, BAD-NeRF outperforms existing methods on both synthetic and real datasets. The code and data for BAD-NeRF are available at https://github.com/WU-CVGL/BAD-NeRF.