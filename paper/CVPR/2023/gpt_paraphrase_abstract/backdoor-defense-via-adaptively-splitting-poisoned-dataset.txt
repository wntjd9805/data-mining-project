Backdoor defenses for deep neural networks (DNNs) aim to protect against malicious alterations through backdoor attacks. As DNNs often use external training data from untrusted sources, it is crucial to have a robust defense strategy during the training phase. The key aspect of training-time defense is the proper selection and handling of poisoned samples. In this study, we present a unified framework that summarizes training-time defenses by splitting the poisoned dataset into two pools of data. Within this framework, we propose an adaptively splitting dataset-based defense (ASD). ASD employs loss-guided split and meta-learning-inspired split techniques to dynamically update the two data pools. By utilizing a split clean data pool and a polluted data pool, ASD effectively defends against backdoor attacks during training. Our extensive experiments on various benchmark datasets and DNN models, against six state-of-the-art backdoor attacks, demonstrate the superiority of ASD. The code for our defense strategy can be found at https://github.com/KuofengGao/ASD.