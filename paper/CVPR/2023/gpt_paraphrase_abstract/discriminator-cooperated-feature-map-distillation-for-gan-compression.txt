Despite their excellent performance in generating images, Generative Adversarial Networks (GANs) have a reputation for requiring large storage and intensive computation. Knowledge distillation, a technique that has shown promise in reducing the cost of GANs, is explored in this paper. The authors focus on the importance of the teacher discriminator and propose a new method called Discriminator-Cooperated Distillation (DCD) to improve the feature maps generated by the GAN. Unlike traditional methods that match feature maps pixel-to-pixel, DCD uses the teacher discriminator as a transformation to guide the student generator towards producing outputs that are perceptually similar to those of the teacher generator. Additionally, to address the issue of mode collapse in GAN compression, the authors introduce a collaborative adversarial training paradigm where the teacher discriminator is trained from scratch alongside the student generator using DCD. The results of their approach outperform existing GAN compression methods, such as CycleGAN, reducing the FID metric from 61.53 to 48.24 compared to the state-of-the-art method's score of 51.92. The source code for this work is available at https://github.com/poopit/DCD-official.