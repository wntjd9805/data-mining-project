Diffusion models have made significant advancements in generating high-quality images, particularly when guided by external information. However, this guidance relies on a large amount of annotated images for training, which can be limited and prone to errors. In this study, we propose a novel approach for self-guided diffusion models that eliminates the need for image annotation. Instead, we leverage self-supervision signals to create a framework that provides guidance signals at different levels of image granularity, ranging from holistic images to object boxes and segmentation masks. Our experiments on both single-label and multi-label image datasets demonstrate that our self-labeled guidance consistently outperforms diffusion models without guidance, and in some cases, even surpasses guidance based on ground-truth labels. Additionally, by incorporating self-supervised box or mask proposals, our method generates visually diverse yet semantically coherent images without requiring any class, box, or segment label annotations. The self-guided diffusion approach we propose is straightforward, adaptable, and has the potential to be successfully implemented at scale.