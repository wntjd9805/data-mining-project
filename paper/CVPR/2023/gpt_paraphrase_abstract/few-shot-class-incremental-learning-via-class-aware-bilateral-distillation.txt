Few-Shot Class-Incremental Learning (FSCIL) is a challenging task that involves learning new classes with limited training samples. This task is more difficult than Class-Incremental Learning (CIL) due to the scarcity of data. While knowledge distillation, a common technique used in CIL, can help prevent forgetting of older classes by aligning outputs between current and previous models, it does not address the risk of overfitting to new classes in FSCIL. To address this issue, we propose a novel distillation structure for FSCIL that takes into account the challenge of overfitting. Our approach involves leveraging knowledge from two teachers. The first teacher is a model trained on abundant data from base classes, which has rich general knowledge that can be used to mitigate the overfitting of current novel classes. The second teacher is the updated model from the previous incremental session, which contains the adapted knowledge of previous novel classes and helps prevent forgetting. We introduce an adaptive strategy based on class-wise semantic similarities to combine the guidance from these two teachers. To preserve base class knowledge while accommodating novel concepts, we use a two-branch network with an attention-based aggregation module. This allows us to dynamically merge predictions from the two complementary branches. Our method has been extensively evaluated on three popular FSCIL datasets (mini-ImageNet, CIFAR100, and CUB200), and the results demonstrate its effectiveness, surpassing existing works by a significant margin. The code for our method is available at https://github.com/LinglanZhao/BiDistFSCIL.