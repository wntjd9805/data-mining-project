Deep neural networks (DNNs) have made significant advancements in vision-based problems, but their predictions tend to be overconfident and poorly calibrated. Most research on calibration focuses on classification tasks within the same domain, while little progress has been made on calibrating DNN-based object detection models, which are crucial for safety-critical applications. This paper introduces a novel approach inspired by train-time calibration methods to align the confidence of bounding box predictions with their accuracy. The proposed loss formulation, which depends on true positives and false positives, is made differentiable to be used alongside other application-specific loss functions during training. Extensive experiments on various benchmark datasets demonstrate the effectiveness of the proposed method in reducing calibration errors for both in-domain and out-domain scenarios. The source code and pre-trained models are available at the provided GitHub link.