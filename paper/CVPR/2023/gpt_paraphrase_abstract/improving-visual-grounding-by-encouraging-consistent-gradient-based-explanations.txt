We propose a new method called Attention Mask Consistency (AMC) to improve joint vision-language models by aligning their gradient-based explanations with human-provided region-level annotations. This approach outperforms previous methods that rely on using vision-language models to score object detectors' outputs. By incorporating AMC into standard vision-language modeling objectives, our model achieves a state-of-the-art accuracy of 86.49% in the Flickr30k visual grounding benchmark, a significant improvement of 5.38% compared to the previous best model trained with the same level of supervision. Additionally, our approach achieves excellent results on referring expression comprehension benchmarks, with an accuracy of 80.34% in the easy test of RefCOCO+ and 64.55% in the difficult split. AMC is easy to implement, effective, and applicable to any vision-language model using various types of region annotations.