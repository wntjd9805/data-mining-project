Learning an accurate entropy model is crucial for reducing redundancy in point cloud compression. A recently developed octree-based auto-regressive entropy model utilizes self-attention to identify dependencies in a broader context and shows promise. However, the model's heavy global attention computations and auto-regressive contexts are not practical for real-world applications. To address this, we propose a hierarchical attention structure that maintains a global receptive field while having a linear complexity to the context scale. Additionally, we introduce a grouped context structure to overcome the serial decoding issue caused by auto-regression without compromising compression performance. Experimental results demonstrate that our proposed entropy model outperforms the state-of-the-art large-scale auto-regressive entropy model in terms of rate-distortion performance and decoding latency reduction.