Dataset Distillation is a technique that aims to condense the knowledge contained in a dataset into a small number of synthetic images. The objective is to generate a limited set of artificial data points that, when used as training data for a learning algorithm, produce a model that closely approximates one trained on the original dataset. However, current dataset distillation methods face limitations in their ability to adapt to new architectures and handle high-resolution datasets. In order to address these challenges, we propose leveraging the learned prior from pre-trained deep generative models to synthesize the distilled data. Our approach involves developing a novel optimization algorithm that distills a large number of images into a few intermediate feature vectors in the latent space of the generative model. By integrating our method with existing techniques, we significantly enhance the generalization across different architectures in all scenarios.