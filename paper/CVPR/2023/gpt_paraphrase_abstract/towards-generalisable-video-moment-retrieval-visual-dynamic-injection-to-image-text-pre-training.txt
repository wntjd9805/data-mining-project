Existing methods for video moment retrieval (VMR) rely heavily on separate pre-training feature extractors for visual and textual understanding. However, without enough temporal boundary annotations, it is challenging to learn universal video-text alignments. In this study, we aim to improve generalizable VMR by exploring multi-modal correlations derived from large-scale image-text data. To address the limitations of image-text pre-training models in capturing video changes, we propose a method called Visual-Dynamic Injection (VDI) that enhances the model's understanding of video moments. While existing VMR methods focus on building temporal-aware video features, they often overlook the importance of text descriptions of temporal changes during pre-training. To overcome this, we extract visual context and spatial dynamic information from video frames and explicitly align them with phrases that describe video changes, such as verbs. This alignment allows us to encode relevant visual and motion patterns in the text embeddings, resulting in more accurate video-text alignments. We evaluate our approach on two VMR benchmark datasets (Charades-STA and ActivityNet-Captions) and achieve state-of-the-art performance. Particularly, VDI demonstrates significant advantages when tested on out-of-distribution splits that involve novel scenes and vocabulary.