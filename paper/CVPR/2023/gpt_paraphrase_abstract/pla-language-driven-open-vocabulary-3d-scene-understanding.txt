The aim of open-vocabulary scene understanding is to identify and locate categories that go beyond the annotated label space. Recent advancements in 2D open-vocabulary perception have been driven by large-scale image-text data from the internet, which includes a wide range of vocabulary concepts. However, this progress cannot be directly applied to 3D scenarios due to the lack of accessible 3D-text pairs on a large scale. To address this, we propose a method that distills knowledge from pre-trained vision-language models by generating captions for multi-view images in 3D. This enables explicit associations between 3D scenes and semantic-rich captions. Additionally, we design hierarchical 3D-caption pairs to facilitate coarse-to-fine visual-semantic representation learning, leveraging geometric constraints between 3D scenes and multi-view images. By employing contrastive learning, our model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method significantly outperforms baseline approaches in open-vocabulary semantic and instance segmentation, achieving improvements of 25.8% to 44.7% in hIoU and 14.5% to 50.4% in hAP50. Furthermore, our method demonstrates robust transferability in challenging zero-shot domain transfer tasks. For more information, please visit our project website at https://dingry.github.io/projects/PLA.