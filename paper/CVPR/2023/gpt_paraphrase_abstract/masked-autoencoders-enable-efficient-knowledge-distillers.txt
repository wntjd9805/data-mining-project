This study examines the potential of distilling knowledge from pre-trained models, specifically focusing on Masked Autoencoders. The authors propose a simple approach that involves optimizing the pixel reconstruction loss on masked inputs and minimizing the distance between the intermediate feature maps of the teacher and student models. This design allows for a computationally efficient knowledge distillation framework, as it only requires a small visible subset of patches and partial execution of the teacher model. Compared to directly distilling fine-tuned models, distilling pre-trained models significantly enhances downstream performance. For instance, by distilling knowledge from a pre-trained ViT-L into a ViT-B using our method, the ImageNet top-1 accuracy improves to 84.0%, surpassing the baseline of directly distilling a fine-tuned ViT-L by 1.2%. Interestingly, our method demonstrates robust knowledge distillation even with high masking ratios, achieving competitive ImageNet accuracy of 83.6% with a 95% masking ratio and 82.4% accuracy with just four visible patches (98% masking ratio). The source code and models are publicly available at https://github.com/UCSC-VLAA/DMAE.