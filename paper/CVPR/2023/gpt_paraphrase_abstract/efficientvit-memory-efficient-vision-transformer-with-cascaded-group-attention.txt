EfficientViT is a new family of high-speed vision transformers that address the heavy computation costs associated with existing models. The speed of current transformer models is limited by memory inefficient operations, particularly tensor reshaping and element-wise functions in the multi-head self-attention (MHSA) mechanism. To overcome this, EfficientViT introduces a new building block with a sandwich layout, incorporating a single memory-bound MHSA between efficient feed-forward neural (FFN) layers. This design improves memory efficiency and enhances channel communication.Additionally, EfficientViT identifies computational redundancy in attention maps across heads. To tackle this issue, a cascaded group attention module is introduced, which feeds attention heads with different splits of the full feature. This not only reduces computation cost but also improves attention diversity.Comprehensive experiments demonstrate that EfficientViT outperforms existing efficient models, achieving a favorable balance between speed and accuracy. For example, EfficientViT-M5 surpasses MobileNetV3-Large by 1.9% in accuracy while achieving significantly higher throughput on Nvidia V100GPU and Intel Xeon CPU. Compared to MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy while running 5.8×/3.7× faster on the GPU/CPU and 7.4× faster when converted to ONNX format.The code and models for EfficientViT are available for access.