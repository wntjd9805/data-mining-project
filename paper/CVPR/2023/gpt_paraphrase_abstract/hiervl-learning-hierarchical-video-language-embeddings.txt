Video-language embeddings have shown promise in adding meaning to visual representations, but current methods only capture short-term connections between video clips and accompanying text. To address this limitation, we propose HierVL, a new hierarchical video-language embedding that considers both long-term and short-term associations. We utilize training data consisting of videos with timestamped text descriptions of human actions, as well as a high-level text summary of the entire video. Our approach introduces a hierarchical contrastive training objective that encourages alignment between text and visuals at both the clip and video levels. The clip-level constraints capture the instantaneous actions described in the step-by-step descriptions, while the video-level constraints capture the broader context and intent of the activity as expressed in the summary text. By employing this hierarchical scheme, we achieve better performance in representing individual video clips compared to single-level embeddings, and our long-term video representation achieves state-of-the-art results in tasks that require modeling extended video sequences. Furthermore, HierVL demonstrates successful transferability to various challenging downstream tasks, including those in EPIC-KITCHENS-100, Charades-Ego, and HowTo100M datasets, both in zero-shot and fine-tuned settings.