Learning discriminative video representation from unlabeled videos is a challenging task in video analysis. Previous approaches have focused on predicting appearance contents in masked regions to learn a representation model. However, this method may not effectively capture temporal clues, as appearance contents can be easily reconstructed from a single frame. To address this limitation, we propose a new pre-training paradigm called Masked Motion Encoding (MME). MME aims to reconstruct both appearance and motion information to explore temporal clues. We tackle two critical challenges in improving representation performance: representing long-term motion across multiple frames and obtaining fine-grained temporal clues from sparsely sampled videos. Inspired by human perception, which recognizes actions by tracking object position and shape changes, we reconstruct a motion trajectory that represents these changes in the masked regions. Additionally, we enforce the model to reconstruct dense motion trajectories in spatial and temporal dimensions, considering the sparse video input. Pre-training the model with our MME paradigm enables it to anticipate detailed long-term motion. The code for our approach is available at https://github.com/XinyuSun/MME.