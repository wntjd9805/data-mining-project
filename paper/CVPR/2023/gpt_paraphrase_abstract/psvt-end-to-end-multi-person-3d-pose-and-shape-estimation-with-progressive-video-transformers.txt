Current methods for estimating the 3D pose and shape of multiple individuals in video typically involve a two-stage process: detecting human instances in each frame and then performing single-person pose and shape estimation using a temporal model. However, this approach fails to capture the global spatio-temporal context among the spatial instances.   To address this limitation, we propose a new end-to-end framework called PSVT (Progressive Video Transformer) for multi-person 3D pose and shape estimation. In PSVT, we introduce a spatio-temporal encoder (STE) that captures the global dependencies between spatial objects. Additionally, we utilize spatio-temporal pose and shape decoders (STPD and STSD) to capture the global dependencies between pose/shape queries and feature tokens.   To handle the variations of objects over time, we employ a progressive decoding scheme that updates pose and shape queries at each frame. Furthermore, we introduce a novel pose-guided attention (PGA) mechanism for the shape decoder, which improves the prediction of shape parameters. These components enhance the decoder of PSVT and lead to improved performance.  Extensive experiments conducted on four datasets demonstrate that PSVT achieves state-of-the-art results. Figure 1 provides a visual comparison between the traditional multi-stage approach and our end-to-end framework. Existing video-based methods perform single-person pose and shape estimation using temporal modeling on cropped areas, while PSVT achieves end-to-end multi-person pose and shape estimation in video using a spatial-temporal encoder and decoder.