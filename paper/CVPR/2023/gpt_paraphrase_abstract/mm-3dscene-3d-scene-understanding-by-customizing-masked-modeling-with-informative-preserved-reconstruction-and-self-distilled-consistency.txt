Masked Modeling (MM) has been successful in various vision challenges by reconstructing masked visual patches. However, applying MM to large-scale 3D scenes is still a challenge due to sparse data and scene complexity. The conventional random masking used in 2D images often leads to ambiguity when recovering the masked region of 3D scenes. In this study, we propose a novel approach that preserves informative reconstruction by utilizing local statistics to identify and preserve representative structured points in order to enhance the masking task for 3D scene understanding. Our method combines this informative-preserved reconstruction with a progressive reconstruction approach, allowing us to focus on modeling regional geometry and reducing ambiguity in masked reconstruction. Additionally, scenes with progressive masking ratios can also be used to learn consistent representations from unmasked areas, further improving spatial consistency. By combining informative-preserved reconstruction on masked areas and consistency self-distillation from unmasked areas, we introduce a unified framework called MM-3DScene. We evaluate our approach through comprehensive experiments on various downstream tasks, and the results show consistent improvement in performance, such as a 6.1% increase in mean average precision at 0.5 intersection over union (mAP@0.5) for object detection and a 2.2% increase in mean intersection over union (mIoU) for semantic segmentation.