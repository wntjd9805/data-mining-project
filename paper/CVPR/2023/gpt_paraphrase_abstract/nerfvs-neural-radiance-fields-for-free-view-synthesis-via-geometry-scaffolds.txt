We introduce NeRFVS, a new approach based on neural radiance fields (NeRF) that allows for unrestricted navigation within a room. While NeRF is effective at rendering images for new viewpoints similar to the training data, it struggles with views that are substantially different. To address this limitation, we utilize holistic prior information, such as pseudo depth maps and view coverage data, obtained from neural reconstruction to guide the learning of implicit neural representations of 3D indoor scenes. Specifically, we use an existing neural reconstruction method to generate a geometric scaffold, and then propose two loss functions based on the holistic priors to improve NeRF learning. The first loss function, a robust depth loss, allows for tolerance of errors in the pseudo depth map and guides the geometry learning of NeRF. The second loss function, a variance loss, regularizes the variance of implicit neural representations to reduce ambiguity in geometry and color during the learning process. These loss functions are adjusted during NeRF optimization based on the view coverage information to mitigate the negative effects of imbalanced view coverage. Extensive results demonstrate that our NeRFVS method outperforms existing view synthesis methods both quantitatively and qualitatively in indoor scenes, achieving high-quality free navigation outcomes.