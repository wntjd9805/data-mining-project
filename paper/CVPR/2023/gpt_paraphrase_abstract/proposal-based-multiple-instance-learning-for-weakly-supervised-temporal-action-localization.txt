This abstract discusses the problem of weakly-supervised temporal action localization, which involves localizing and recognizing actions in untrimmed videos using only video-level category labels for training. Existing methods typically use the Segment-based Multiple Instance Learning (S-MIL) framework, where segment predictions are supervised by video labels. However, this approach leads to suboptimal results because the objective of acquiring segment-level scores during training is different from the objective of acquiring proposal-level scores during testing. To address this issue, the authors propose a new framework called Proposal-based Multiple Instance Learning (P-MIL). P-MIL directly classifies candidate proposals in both training and testing stages, and it includes three key designs: 1) a surrounding contrastive feature extraction module to consider surrounding contrastive information and suppress discriminatory short proposals, 2) a proposal completeness evaluation module to eliminate low-quality proposals using completeness pseudo labels, and 3) an instance-level rank consistency loss that leverages the complementarity of RGB and FLOW modalities for robust detection. The authors conduct extensive experiments on two challenging benchmarks (THUMOS14 and ActivityNet) and demonstrate the superior performance of their method. The code for their method is available on GitHub.