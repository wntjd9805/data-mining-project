We propose a method called Image-to-Point Masked Autoencoders (I2P-MAE) to obtain high-quality 3D representations from pre-trained 2D models. Due to the lack of 3D datasets, learning for 3D features is hindered. Our approach leverages the knowledge learned from 2D pre-training to guide the 3D masked autoencoding process. We use off-the-shelf 2D models to extract visual features from input point clouds and then employ two types of image-to-point learning schemes. Firstly, we introduce a 2D-guided masking strategy that preserves important point tokens. This allows the network to focus on significant 3D structures with spatial cues. Secondly, we enforce the visible tokens to reconstruct multi-view 2D features. This enables the network to inherit high-level 2D semantics for discriminative 3D modeling. Our pre-trained I2P-MAE achieves competitive accuracy on ModelNet40 without fine-tuning. After fine-tuning on ScanObjectNN's hardest split, I2P-MAE achieves state-of-the-art accuracy, demonstrating its superior transferable capacity. Our code is available at https://github.com/ZrrSkywalker/I2P-MAE.