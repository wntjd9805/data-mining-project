We introduce RAFTformer, a transformer-based model for real-time action forecasting in real-world scenarios. RAFTformer consists of two stages: a video transformer backbone that operates on high-resolution, short-range clips, and a head transformer encoder that aggregates information from multiple clips to predict long-term actions. To enhance forecast accuracy, we propose a novel self-supervised shuffled causal masking scheme as a model-level augmentation. Additionally, we propose a new real-time evaluation setting that considers the trade-off between forecast accuracy and model inference latency. Our network design significantly reduces inference latency by 9Ã— compared to previous works while achieving the same forecasting accuracy. Moreover, RAFTformer requires 94% less training compute and 90% fewer training parameters than prior state-of-the-art models, outperforming them by 4.9 points on EGTEA Gaze+ and 1.4 points on EPIC-Kitchens-100 validation set, measured by Top-5 recall (T5R) in offline settings. In the real-time setting, RAFTformer outperforms previous models by an even larger margin of up to 4.4 T5R points on the EPIC-Kitchens-100 dataset. For more information, please visit our project webpage: https://karttikeya.github. io/publication/RAFTformer/.