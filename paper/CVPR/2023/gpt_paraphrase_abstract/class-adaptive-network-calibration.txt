Recent research has emphasized the importance of calibration in training deep neural networks, in addition to conventional accuracy. Existing methods for addressing miscalibration during training have used penalty functions alongside a classification loss, with a fixed scalar balancing weight that applies to all classes. However, these methods have two main limitations: 1) the inability to address different difficulties or imbalances among classes due to the uniform balancing weight, and 2) the lack of adaptability in the balancing weight, which hinders finding the optimal compromise between accuracy and calibration and requires hyper-parameter search for each application.To overcome these limitations, we propose a novel approach called Class Adaptive Label Smoothing (CALS) for calibrating deep networks. CALS allows for learning class-wise multipliers during training, providing a powerful alternative to common label smoothing penalties. Our method is based on the well-established Augmented Lagrangian technique in constrained optimization, but we have made several modifications to tailor it specifically for large-scale, class-adaptive training. We conducted comprehensive evaluations and multiple comparisons on various benchmarks, including image classification, semantic segmentation, and text classification, and our proposed method demonstrated superior performance. The code for CALS is available at https://github.com/by-liu/CALS.