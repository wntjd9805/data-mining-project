Scene graph generation (SGG) involves representing an image as a graph with objects as nodes and their relationships as edges. However, current SGG methods face two challenges in real-world scenarios: 1) requiring time-consuming annotations for training and 2) being limited to recognizing only known objects. To address these issues, we propose a novel approach that utilizes a pre-trained visual-semantic space (VSS) for language-supervised and open-vocabulary SGG. We obtain scene graph supervision data by parsing image language descriptions into semantic graphs, and then ground the noun phrases in these graphs to image regions using the pre-trained VSS. This enables open-vocabulary object detection by grounding object category names with a text prompt in the VSS. We also build relation representations based on visually-grounded objects for relation recognition, achieving open-vocabulary SGG. Extensive experiments on the Visual Genome benchmark across different SGG scenarios validate our approach, consistently outperforming existing methods and showcasing the potential of utilizing pre-trained VSS for more practical SGG scenarios.