Zero-shot quantization is a promising technique for developing lightweight deep neural networks when data is inaccessible due to reasons such as cost and privacy concerns. It involves using the learned parameters of batch normalization layers in a pre-trained model to generate synthetic data. This synthetic data is then used to distill knowledge from the pre-trained model to a quantized model, enabling optimization with the synthetic dataset. However, current discussions on zero-shot quantization mainly focus on quantization-aware training methods, which require task-specific losses and long optimization processes. In this study, we propose a post-training quantization scheme for zero-shot quantization that can produce high-quality quantized networks within a few hours. We also introduce a framework called GENIE that generates data specifically suited for quantization. This synthetic data generated by GENIE allows us to create robust quantized models without the need for real datasets, making it comparable to few-shot quantization. Additionally, we propose a post-training quantization algorithm to further enhance the performance of quantized models. By combining these approaches, we can bridge the gap between zero-shot and few-shot quantization, significantly improving the quantization performance compared to existing methods. Our proposed approach represents a unique state-of-the-art zero-shot quantization method. The code for our approach is available at https://github.com/SamsungLabs/Genie.