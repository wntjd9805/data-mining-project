Current approaches to Continual Learning (CL) struggle to effectively learn a long sequence of diverse and difficult tasks without significant performance degradation. These approaches often have limitations such as high memory cost, long training times, or being restricted to a single device. To address these challenges, we propose the Batch Model Consolidation (BMC) method, inspired by mini-batch training. During the regularization phase, BMC trains multiple expert models simultaneously on different tasks. Each expert model maintains weight similarity to a base model through a stability loss and creates a buffer using a portion of the task's data. In the consolidation phase, we combine the knowledge learned by the expert models using a batched consolidation loss on memory data that aggregates all buffers. We extensively evaluate each component of BMC through an ablation study and demonstrate its effectiveness on benchmark datasets, including Split-CIFAR-100, Tiny-ImageNet, and the Stream dataset consisting of 71 image classification tasks from various domains and difficulties. Our method surpasses the performance of the next best CL approach by 70% and is the only approach capable of maintaining performance across all 71 tasks.