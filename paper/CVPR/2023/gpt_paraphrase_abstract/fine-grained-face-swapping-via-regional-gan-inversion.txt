We introduce a new approach to high-fidelity face swapping that accurately preserves subtle details in geometry and texture. Instead of traditional face swapping methods, we propose an "editing for swapping" (E4S) framework that disentangles the shape and texture of facial components. This framework allows for both global and local swapping of facial features, as well as user-defined control over the extent of swapping. Additionally, our E4S paradigm effectively handles facial occlusions using facial masks. Our system is built on a Regional GAN Inversion (RGI) method that explicitly separates shape and texture, enabling face swapping in the latent space of Style-GAN. We develop a multi-scale mask-guided encoder to project the texture of each facial component into regional style codes, and a mask-guided injection module to manipulate feature maps with style codes. By disentangling shape and texture, face swapping is simplified as a problem of style and mask swapping. Through extensive experiments and comparisons with state-of-the-art methods, our approach demonstrates superior preservation of texture and shape details, particularly with high-resolution images. For more information, please visit our project page at https://e4s2022.github.io.