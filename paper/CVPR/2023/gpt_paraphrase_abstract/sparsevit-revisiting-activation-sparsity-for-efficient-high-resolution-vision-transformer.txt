Neural networks benefit from high-resolution images but face increased computational complexity, limiting their use in time-sensitive applications. To address this, we propose SparseViT, a method for reducing computation in vision transformers. By selectively skipping computations for less important regions, we achieve significant latency reduction without sacrificing accuracy. We introduce sparsity-aware adaptation and evolutionary search to find the optimal pruning ratios for different layers. In experiments, SparseViT achieves speedups of 1.5×, 1.4×, and 1.3× in 3D object detection, 2D instance segmentation, and 2D semantic segmentation, respectively, with minimal accuracy loss.