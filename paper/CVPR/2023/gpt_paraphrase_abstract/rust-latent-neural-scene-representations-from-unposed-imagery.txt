Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recent advancements in neural scene representations have had a significant impact and have been applied in various applications. However, a major challenge remains: training a single model that can provide generalizable latent representations across different scenes. The Scene Representation Transformer (SRT) has shown promise in this regard, but scaling it to a larger and more diverse set of scenes is difficult and requires accurate ground truth data. To tackle this issue, we propose a novel approach called RUST (Really Unposed Scene representation Transformer), which trains a pose-free model for synthesizing new views using only RGB images. Our key insight is training a Pose Encoder that examines the target image and learns a latent pose embedding, which is then used by the decoder for view synthesis. Through empirical analysis, we demonstrate that the learned latent pose structure enables meaningful camera transformations during testing and accurate pose estimations. Surprisingly, RUST achieves similar quality to methods that have access to perfect camera pose, opening up possibilities for large-scale training of neural scene representations without relying on precise pose information.