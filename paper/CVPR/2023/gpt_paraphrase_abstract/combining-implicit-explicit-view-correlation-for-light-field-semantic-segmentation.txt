The use of light field technology, which captures both spatial and angular information of light rays, has potential applications in various fields, including semantic segmentation. However, the high-dimensional nature of light field data presents challenges in fully utilizing the relationships among views while maintaining contextual information in a single view, particularly when working with limited memory. In this study, we introduce a new network called LF-IENet for light field semantic segmentation. This network incorporates two methods to extract complementary information from surrounding views and segment the central view. The first method, implicit feature integration, utilizes an attention mechanism to calculate inter-view and intra-view similarity and modulates the features of the central view. The second method, explicit feature propagation, directly warps features from other views to the central view based on disparity information. These two methods work together to fuse complementary information across views in the light field. Experimental results on both real-world and synthetic light field datasets demonstrate the superior performance of our proposed method, highlighting the effectiveness of this new architecture.