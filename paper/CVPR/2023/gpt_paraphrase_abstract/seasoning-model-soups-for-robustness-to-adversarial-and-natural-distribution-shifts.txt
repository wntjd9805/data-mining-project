Adversarial training is commonly used to make classifiers resistant to specific threats or adversaries. However, existing methods for training classifiers to be robust against multiple threats require knowledge of all attacks during training and are still vulnerable to unseen distribution shifts. This study introduces a novel approach to obtaining adversarially-robust model soups, which are linear combinations of parameters that smoothly balance robustness to different adversaries. These model soups provide control over the level and type of robustness, and can achieve robustness to all threats without the need to train on each individually. In certain cases, the resulting model soups exhibit greater robustness to a particular adversary compared to models specifically trained against that adversary. Furthermore, the study demonstrates that adversarially-robust model soups can effectively adapt to distribution shifts with only a few examples.