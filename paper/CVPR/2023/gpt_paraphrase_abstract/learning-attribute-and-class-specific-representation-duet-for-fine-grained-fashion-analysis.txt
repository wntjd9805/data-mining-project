Fashion representation learning involves the analysis and understanding of visual elements in fashion images, considering their relationships and dependencies. However, existing methods focus on learning fine-grained representations at the attribute level, without considering the connections between different classes. To address this, we propose a method that learns attribute and class-specific fashion representations, leveraging prior knowledge about the taxonomy of fashion attributes and classes. Our approach utilizes two sub-networks to progressively refine the visual representation of a fashion image, improving its robustness for fashion retrieval. We introduce a multi-granularity loss that incorporates attribute-level and class-level losses, enabling learning across different levels of granularity. Experimental results on three benchmark datasets demonstrate the effectiveness of our method, surpassing state-of-the-art approaches by a significant margin.