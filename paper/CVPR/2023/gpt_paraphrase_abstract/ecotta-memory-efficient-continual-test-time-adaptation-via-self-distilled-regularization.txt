This paper introduces a straightforward and efficient approach to improving continual test-time adaptation (TTA) while minimizing memory usage. TTA is typically performed on edge devices with limited memory, but previous studies have overlooked the importance of reducing memory requirements. Long-term adaptation often leads to catastrophic forgetting and error accumulation, which limits the practicality of TTA in real-world scenarios. To address these challenges, our approach consists of two components. First, we propose lightweight meta networks that can adapt frozen original networks to the target domain. This architecture reduces memory consumption by minimizing the size of intermediate activations needed for backpropagation. Second, we introduce a novel self-distilled regularization technique that ensures the output of the meta networks does not deviate significantly from the output of the frozen original networks. This regularization preserves well-trained knowledge from the source domain and prevents error accumulation and catastrophic forgetting without the need for additional memory. Experimental results show that our simple yet effective strategy outperforms other state-of-the-art methods on various image classification and semantic segmentation benchmarks. Notably, our proposed method with ResNet-50 and WideResNet-40 achieves memory savings of 86% and 80% respectively compared to the recent state-of-the-art method, CoTTA.