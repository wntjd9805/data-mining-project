Deep learning models that have flatter minima in their loss landscape tend to generalize better. However, this property has not been thoroughly explored in deep long-tailed recognition (DLTR), a practical problem where the model needs to generalize equally well across all classes despite imbalanced label distribution. This paper presents empirical evidence showing that deep long-tailed models often have sharp minima, and incorporating existing flattening operations into long-tailed learning algorithms does not significantly improve performance. Instead, the paper proposes a two-stage optimization approach called Class-Conditional Sharpness-Aware Minimization (CC-SAM) based on the decoupling paradigm in DLTR. In the first stage, both the feature extractor and classifier are trained with parameter perturbations at a class-conditioned scale, which is theoretically motivated by the characteristic radius of flat minima under the PAC-Bayesian framework. In the second stage, adversarial features are generated using class-balanced sampling to enhance the robustness of the classifier with the backbone frozen. Extensive experiments on various long-tailed visual recognition benchmarks demonstrate that CC-SAM achieves competitive performance compared to state-of-the-art methods. The code for CC-SAM is available at https://github.com/zzpustc/CC-SAM.