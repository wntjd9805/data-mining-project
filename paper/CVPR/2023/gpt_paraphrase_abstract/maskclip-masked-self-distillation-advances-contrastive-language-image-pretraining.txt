This paper introduces a framework called MaskCLIP, which incorporates a newly proposed technique called masked self-distillation into contrastive language-image pretraining. The main idea behind masked self-distillation is to distill representation from a complete image to the representation predicted from a masked image. This incorporation provides two important benefits. First, masked self-distillation focuses on learning local patch representations, which complements the vision-language contrastive approach that focuses on text-related representations. Second, masked self-distillation aligns with vision-language contrastive by utilizing the visual encoder for feature alignment and learning local semantics with indirect supervision from the language. The paper presents carefully designed experiments and comprehensive analysis to validate these two benefits. Additionally, the paper introduces local semantic supervision into the text branch, further enhancing the pretraining performance. Through extensive experiments, it is demonstrated that MaskCLIP achieves superior results in linear probing, finetuning, and zero-shot performance for various challenging downstream tasks when guided by the language encoder. The code for MaskCLIP can be accessed at https://github.com/LightDXY/MaskCLIP.