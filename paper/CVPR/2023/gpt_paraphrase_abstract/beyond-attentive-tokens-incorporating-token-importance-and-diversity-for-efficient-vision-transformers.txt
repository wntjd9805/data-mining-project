Vision transformers have made significant progress in various vision tasks, but their computational efficiency is hindered by the quadratic interactions between tokens. To address this issue, several pruning methods have been proposed to remove redundant tokens. However, these methods primarily focus on preserving local attentive tokens while neglecting the importance of global token diversity. In this study, we highlight the significance of diverse global semantics and propose an efficient token decoupling and merging approach that considers both token importance and diversity for pruning. By analyzing class token attention, we separate attentive and inattentive tokens. In addition to preserving the most discriminative local tokens, we merge similar inattentive tokens and match homogeneous attentive tokens to maximize token diversity. Despite its simplicity, our method achieves a promising balance between model complexity and classification accuracy. For instance, on DeiT-S, our method reduces FLOPs by 35% with only a 0.2% drop in accuracy. Furthermore, thanks to the maintenance of token diversity, our method even improves the accuracy of DeiT-T by 0.1% after reducing FLOPs by 40%.