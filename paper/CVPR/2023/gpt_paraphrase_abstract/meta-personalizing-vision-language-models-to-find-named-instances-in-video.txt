Large-scale vision-language models (VLM) have achieved impressive results in language-guided search applications. However, these models currently struggle with personalized searches for specific objects in videos. To address this problem, we propose three contributions. Firstly, we introduce a method to meta-personalize a pre-trained VLM by learning how to personalize it at test time for video search. This involves extending the VLM's token vocabulary to include novel word embeddings for each specific instance. We represent each instance embedding as a combination of shared and learned global category features to capture only instance-specific characteristics. Secondly, we propose learning this personalization without explicit human supervision. Our approach automatically identifies moments of named visual instances in videos using transcripts and vision-language similarity in the VLM's embedding space. Finally, we introduce This-Is-My, a benchmark for personal video instance retrieval. We evaluate our approach on This-Is-My and Deep-Fashion2 datasets and demonstrate a 15% relative improvement over the state of the art on the latter dataset.