We introduce Video Localized Narratives, a novel approach to multimodal video annotations that bridge the gap between vision and language. Whereas the previous method, Localized Narratives, relied on annotators speaking and moving their mouse simultaneously on an image to ground each word, this method faces challenges when applied to videos. Our proposed protocol enables annotators to narrate the story of a video using Localized Narratives, allowing for the annotation of complex events involving multiple actors and passive objects. To demonstrate the effectiveness of our approach, we annotated a substantial number of videos from various datasets, resulting in 1.7 million words. Additionally, we created new benchmarks for video narrative grounding and video question answering tasks and present results from strong baseline models. To access our annotations, please visit https://google.github.io/video-localized-narratives/.