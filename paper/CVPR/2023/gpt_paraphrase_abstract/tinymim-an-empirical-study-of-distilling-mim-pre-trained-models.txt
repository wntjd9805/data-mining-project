This paper investigates the use of distillation techniques to transfer the success of large masked image modeling (MIM) pre-trained models to smaller models. The authors systematically explore various options within the distillation framework, such as distilling targets, losses, input, network regularization, and sequential distillation. The findings reveal that distilling token relations is more effective than distilling CLS token and feature-based information. Additionally, using an intermediate layer of the teacher network as a target performs better when the student's depth differs from that of the teacher. The authors also find that weak regularization is preferred. Based on these findings, the authors achieve significant improvements in fine-tuning accuracy compared to scratch MIM pre-training on ImageNet-1K classification for ViT-Tiny, ViT-Small, and ViT-base models, with gains of +4.2%, +2.4%, and +1.4% respectively. The authors also introduce the TinyMIM model, which achieves a mIoU of 52.2 in AE20K semantic segmentation, surpassing the MAE baseline by 4.1. Furthermore, the TinyMIM model achieves a top-1 accuracy of 79.6% on ImageNet-1K image classification, setting a new record for small vision models of the same size and computation budget. These impressive results suggest that better training methods can be explored to develop small vision Transformer models, rather than relying solely on introducing inductive biases into architectures as done in previous works. The code for the TinyMIM model is available at https://github.com/OliverRensu/TinyMIM.