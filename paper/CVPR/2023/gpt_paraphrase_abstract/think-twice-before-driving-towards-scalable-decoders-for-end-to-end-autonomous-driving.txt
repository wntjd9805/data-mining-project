Recent advancements in end-to-end autonomous driving have shown significant progress. However, existing methods typically follow a separated encoder-decoder approach, where the encoder extracts hidden features from sensor data and the decoder predicts the future actions or trajectories of the ego-vehicle. This paradigm poses challenges as the encoder lacks knowledge of the intended behavior of the ego agent, making it difficult to identify safety-critical regions and infer future situations. Additionally, the decoder is often composed of simple MLPs or GRUs, while the encoder is intricately designed, causing an imbalance in resource allocation and hindering the learning process.To address these issues, our work focuses on two principles: fully utilizing the encoder's capacity and increasing the decoder's capacity. Firstly, we predict a coarse-grained future position and action using the encoder features. Then, based on this prediction, we imagine the future scene to assess the potential consequences of driving accordingly. We also retrieve encoder features around the predicted coordinate to gather detailed information about safety-critical regions. Finally, we refine the coarse-grained position and action by predicting the offset from the ground truth, incorporating the retrieved salient feature. This refinement module can be stacked in a cascaded manner, enhancing the decoder's capacity with spatial-temporal knowledge about the conditioned future. We conducted experiments on the CARLA simulator, achieving state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies further validated the effectiveness of each proposed module.