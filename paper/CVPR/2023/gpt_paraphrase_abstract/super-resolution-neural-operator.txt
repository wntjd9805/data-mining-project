We propose the Super-resolution Neural Operator (SRNO), a deep learning framework that can generate high-resolution (HR) images at various scales from low-resolution (LR) counterparts. SRNO treats LR-HR image pairs as continuous functions with different grid sizes and learns the mapping between these function spaces. The framework first transforms the LR input into a higher-dimensional latent representation space to capture sufficient basis functions. It then approximates the implicit image function iteratively using a kernel integral mechanism, followed by dimensionality reduction to generate the RGB representation at the desired coordinates. SRNO stands out from previous continuous super-resolution (SR) methods in two ways. Firstly, it efficiently implements the kernel integral using Galerkin-type attention, which possesses non-local properties in the spatial domain and benefits the grid-free continuum. Secondly, its multilayer attention architecture allows for dynamic latent basis updates, crucial for "hallucinating" high-frequency information from the LR image. Experimental results demonstrate that SRNO outperforms existing continuous SR methods in terms of accuracy and running time. The code for SRNO can be found at the following GitHub repository: https://github.com/2y7c3/Super-Resolution-Neural-Operator.