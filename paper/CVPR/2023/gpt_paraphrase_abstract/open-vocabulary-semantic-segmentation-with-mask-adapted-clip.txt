Open-vocabulary semantic segmentation is a task that involves dividing an image into different regions based on text descriptions, even if those descriptions were not included in the training data. Current approaches use a two-stage process, where class-agnostic mask proposals are generated first, and then pre-trained vision-language models like CLIP are used to classify the masked regions. However, we have identified that the performance bottleneck lies in the pre-trained CLIP model, as it does not work well with masked images.To overcome this limitation, we propose a solution that involves fine-tuning the CLIP model on a dataset of masked image regions and their corresponding text descriptions. We gather training data by mining an existing dataset of image captions, such as COCO Captions, and use CLIP to match the masked image regions with nouns in the captions. Although our dataset is noisy and less precise compared to manually annotated segmentation labels with fixed classes like COCO-Stuff, we find that it preserves CLIP's ability to generalize better due to its diversity.In addition to fine-tuning the entire model, we introduce a technique called "mask prompt tuning" that takes advantage of the "blank" areas in masked images. This method does not require modifying any weights of CLIP but still brings significant improvement. Furthermore, it can enhance the performance of a fully fine-tuned model. Our best model, trained on COCO and evaluated on ADE20K-150, achieves a mean intersection over union (mIoU) of 29.6%, which is 8.5% higher than the previous state-of-the-art. This is the first time that open-vocabulary generalist models match the performance of supervised specialist models from 2017 without any specific adaptations for the dataset.