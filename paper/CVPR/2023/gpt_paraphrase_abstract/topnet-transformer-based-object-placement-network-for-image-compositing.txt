We explore the problem of automatically placing an object into a background image for image compositing. The objective is to train a model that can predict suitable placements for the object, considering both its location and scale. The quality of the composite image heavily relies on the accuracy of these predictions. Existing approaches either generate potential bounding boxes or use global representations from object and background images with sliding-window search, but they fail to incorporate local information present in the background images. However, local clues in the background are crucial for determining the compatibility of object placements at specific locations and scales. This paper proposes a transformer module to learn the correlation between object features and all local background features. By doing so, detailed information can be provided regarding all possible location and scale configurations. Additionally, we introduce a sparse contrastive loss to train our model with sparse supervision. Our new formulation allows for the generation of a 3D heatmap that indicates the plausibility of different location and scale combinations in a single network forward pass, which is more than 10 times faster than the previous sliding-window method. Furthermore, our approach supports interactive search, allowing users to provide pre-defined locations or scales. The proposed method can be trained using explicit annotations or in a self-supervised manner using an off-the-shelf inpainting model. It outperforms state-of-the-art methods significantly and generalizes well to real-world images with diverse challenging scenes and object categories, as demonstrated by a user study.