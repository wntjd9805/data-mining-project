We propose a method for learning audio source separation in videos using natural language queries and unlabeled video and audio pairs as training data. The challenge is to associate linguistic descriptions of sound-emitting objects with their visual and audio components without annotations. To overcome this, we modify existing vision-language models to provide pseudo-target supervision through new loss functions, promoting better alignment between audio, visual, and natural language. Our approach can separate sounds using text, video, and audio input, or text and audio input alone. We demonstrate the effectiveness of our self-supervised method on three datasets, outperforming supervised approaches without object detectors or text labels. Our project page with code can be found at https://cs-people.bu.edu/rxtan/projects/VAST.