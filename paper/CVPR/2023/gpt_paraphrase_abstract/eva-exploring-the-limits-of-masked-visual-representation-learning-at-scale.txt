We introduce EVA, a vision-based model that explores the boundaries of visual representation using publicly available data. EVA is a pre-trained Vanilla ViT model that reconstructs masked image-text aligned vision features based on visible image patches. By utilizing this method, we are able to efficiently scale up EVA to one billion parameters and achieve impressive results on various vision-related tasks including image recognition, video action recognition, object detection, instance segmentation, and semantic segmentation, all without extensive supervised training. Furthermore, we have discovered that quantitative changes in scaling EVA lead to qualitative changes in transfer learning performance, distinguishing it from other models. Notably, EVA excels in the challenging task of large vocabulary instance segmentation, achieving state-of-the-art results on datasets like LVIS and COCO. In addition to serving as a vision encoder, EVA also functions as a vision-centric, multi-modal connector between images and text. We have found that initializing the vision tower of a larger CLIP model with EVA significantly improves training stability and outperforms training from scratch with fewer samples and less computational resources. This discovery opens up new avenues for scaling and accelerating the training of multi-modal foundation models.