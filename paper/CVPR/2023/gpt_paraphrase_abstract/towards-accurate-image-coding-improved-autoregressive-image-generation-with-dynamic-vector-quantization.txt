Current autoregressive models based on vector quantization (VQ) follow a two-stage process where images are encoded into fixed-length codes using a codebook and then generated based on these codes. However, this approach fails to account for the varying information densities in different image regions, leading to inadequate representation of important regions and redundancy in unimportant ones. This ultimately degrades the quality and speed of generation. Additionally, the fixed-length coding results in an unnatural raster-scan autoregressive generation. To overcome these issues, we propose a new two-stage framework consisting of the Dynamic-Quantization VAE (DQ-VAE) and the DQ-Transformer. The DQ-VAE encodes image regions into variable-length codes based on their information densities, resulting in a more accurate and compact code representation. The DQ-Transformer then generates images in an autoregressive manner, starting from coarse-grained regions with fewer codes and progressing to fine-grained regions with more codes. This is achieved through a novel stacked-transformer architecture and the use of shared-content and non-shared position input layers. Our approach has been validated through comprehensive experiments on various generation tasks, demonstrating its effectiveness and efficiency. The code for our framework will be made available at the given GitHub repository. Figure 1 illustrates the motivation behind our approach. Existing fixed-length coding fails to consider information densities, leading to poor details and inconsistent structure. In contrast, our information-density-based variable-length coding accurately encodes images, resulting in rich details and consistent structure. Additionally, our approach improves the autoregressive generation order by moving from coarse-to-fine, which is more natural and effective compared to the existing unnatural raster-scan order. The error map shows the l1 loss between original images and reconstructions, with higher values indicating worse performance. The examples shown are from a previous study.