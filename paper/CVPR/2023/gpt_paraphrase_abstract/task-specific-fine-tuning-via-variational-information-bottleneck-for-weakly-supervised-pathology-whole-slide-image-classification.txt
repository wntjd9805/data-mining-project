Multiple Instance Learning (MIL) has shown promise in analyzing digital Pathology Whole Slide Images (WSIs). However, it faces challenges in performance and generalization due to high computational costs and limited supervision. Previous methods have used a pretrained model from ImageNet, but this approach may result in the loss of important information and hinder generalization without image-level training-time augmentation. Self-supervised Learning (SSL) offers potential representation learning schemes, but it has not explored task-specific features through partial label tuning. To address this issue, we propose an efficient WSI fine-tuning framework inspired by the Information Bottleneck theory. This framework helps us find the minimal sufficient statistics of WSI, allowing us to fine-tune the backbone into a task-specific representation based only on weak WSI-level labels. We analyze the WSI-MIL problem and provide theoretical deductions for our fine-tuning method. We evaluate our method on five pathological WSI datasets with various WSI heads and demonstrate significant improvements in accuracy and generalization compared to previous approaches. The source code is available at https://github.com/invoker-LL/WSI-finetuning.