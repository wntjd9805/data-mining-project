We introduce an efficient multimodal fusion method called PMF for combining pre-trained transformers in computer vision and natural language processing. Our approach offers flexibility and encourages interactions among different modalities. We also propose disentangling prompts into three types to optimize multimodal learning. Notably, we suggest adding prompt vectors only on deep layers of unimodal transformers to reduce training memory usage. Experimental results demonstrate that our method achieves similar performance to other multimodal finetuning methods while using fewer trainable parameters and saving up to 66% of training memory.