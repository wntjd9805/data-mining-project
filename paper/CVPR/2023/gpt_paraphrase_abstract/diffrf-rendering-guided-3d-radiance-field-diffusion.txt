We present DiffRF, a new technique for synthesizing 3D radiance fields using denoising diffusion probabilistic models. Unlike previous methods that work with images, latent codes, or point cloud data, we directly generate volumetric radiance fields. To achieve this, we propose a 3D denoising model that operates on an explicit voxel grid representation. However, obtaining ground truth radiance field samples is challenging due to ambiguity and artifacts in the fields generated from a set of posed images. To overcome this, we combine the denoising formulation with a rendering loss, allowing our model to learn a prior that prioritizes good image quality over reproducing fitting errors like floating artifacts. Our model learns multi-view consistent priors, enabling free-view synthesis and accurate shape generation, unlike 2D diffusion models. Compared to 3D GANs, our diffusion-based approach naturally supports conditional generation, such as masked completion or single-view 3D synthesis during inference.