VQA models often struggle with out-of-distribution data and domain generalization. To better understand these challenges, we introduce Super-CLEVR, a virtual benchmark that allows us to isolate different factors in VQA domain shifts and study their effects independently. We consider four factors: visual complexity, question redundancy, concept distribution, and concept compositionality. By generating controlled data, Super-CLEVR enables us to test VQA methods in scenarios where the test data differs from the training data along each of these factors. We evaluate four existing methods (NSCL, NSVQA, FiLM, and mDETR) and propose a new method called P-NSVQA, which extends NSVQA with uncertainty reasoning. P-NSVQA outperforms the other methods on three out of the four domain shift factors. Our results indicate that a VQA model that disentangles reasoning and perception, combined with probabilistic uncertainty, is more robust to domain shifts. The dataset and code for Super-CLEVR are available at https://github.com/Lizw14/Super-CLEVR. The answer format of Super-CLEVR focuses on abstraction.