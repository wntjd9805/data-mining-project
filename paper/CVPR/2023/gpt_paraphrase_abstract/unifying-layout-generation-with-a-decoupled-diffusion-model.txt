Layout generation is an important task in graphic design that involves creating realistic scenes with different elements. This task is particularly useful for reducing the workload in creating formatted scenes for publications, documents, and user interfaces. However, the diverse application scenarios make it challenging to unify the various subtasks of layout generation, including conditional and unconditional generation. To address this challenge, we propose a Layout Diffusion Generative Model (LDGM) that can unify layout generation using a single decoupled diffusion model. LDGM considers a layout with missing or coarse attributes as an intermediate diffusion status from a completed layout. Since different attributes have their own meanings and characteristics, we decouple the diffusion processes for each attribute to enhance the diversity of training samples. Additionally, we learn the reverse process jointly to take advantage of global-scope contexts and facilitate generation. As a result, our LDGM can generate layouts from scratch or based on available attributes. Through extensive qualitative and quantitative experiments, we demonstrate that our proposed LDGM outperforms existing layout generation models in terms of functionality and performance.