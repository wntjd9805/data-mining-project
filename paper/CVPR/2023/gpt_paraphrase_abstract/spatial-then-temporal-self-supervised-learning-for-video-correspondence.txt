This study focuses on the importance of effective representations in low-level video analysis for deriving correspondences between video frames. Previous research has primarily focused on either spatial-discriminative or temporal-repetitive features, neglecting the synergy between spatial and temporal cues. To address this issue, the authors propose a spatial-then-temporal self-supervised learning method. Firstly, spatial features are extracted from unlabeled images using contrastive learning. Secondly, temporal cues in unlabeled videos are utilized to enhance these features through reconstructive learning. To prevent the forgetting of spatial cues and combat temporal discontinuity, the authors introduce a global correlation distillation loss and a local correlation distillation loss, respectively. Experimental results demonstrate that the proposed method surpasses the current state-of-the-art self-supervised methods in correspondence-based video analysis tasks. Ablation studies are also conducted to confirm the effectiveness of the two-step design and the distillation losses.