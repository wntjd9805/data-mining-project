Vision Transformers (ViTs) have emerged as a formidable competitor to convolutional neural networks (CNNs) in the field of computer vision. However, the industrialization of ViTs has introduced new security challenges in the form of backdoor attacks. This is due to the self-attention mechanism that contributes to the success of ViTs. Unlike CNNs, ViTs are more sensitive to triggers within patches, which can manipulate the model's attention and lead to misclassification. To exploit this vulnerability, we have developed a new backdoor attack framework called BadViT. BadViT uses a universal patch-wise trigger to divert the model's attention from relevant patches to trigger-containing ones, thereby confusing the ViT and compromising its performance. To enhance the stealthiness of the attack, we propose invisible variants of BadViT that limit the strength of trigger perturbations. Extensive experiments demonstrate that BadViT is an effective backdoor attack method against ViTs, requiring fewer poisoned samples and exhibiting satisfactory convergence. Additionally, the attack is transferable to downstream tasks. We also investigate the vulnerabilities of ViTs to backdoor attacks from the perspective of existing defense schemes.