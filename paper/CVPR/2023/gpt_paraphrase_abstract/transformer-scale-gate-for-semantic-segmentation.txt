Accurately encoding contextual information at multiple scales is crucial for semantic segmentation. Many existing segmentation models based on transformers combine features across scales without considering their quality, leading to potential degradation in segmentation results. In this study, we leverage the inherent properties of Vision Transformers and introduce a module called Transformer Scale Gate (TSG) to effectively combine multi-scale features. TSG utilizes self and cross attentions in Vision Transformers to determine the optimal scales for feature selection. This module is highly flexible and can be easily integrated into any encoder-decoder-based hierarchical vision Transformer. Through extensive experiments on Pascal Context, ADE20K, and Cityscapes datasets, we demonstrate that our proposed feature selection strategy consistently improves segmentation performance.