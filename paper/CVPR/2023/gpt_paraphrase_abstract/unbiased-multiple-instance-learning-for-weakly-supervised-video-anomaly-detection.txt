Weakly Supervised Video Anomaly Detection (WSVAD) is a challenging task as it involves predicting anomalies at the snippet-level even though anomaly labels are only provided at the video-level. Multiple Instance Learning (MIL) has been commonly used for WSVAD, but it often leads to numerous false alarms due to biases in the snippet-level detector. These biases cause the detector to favor abnormal snippets with simple context, become confused by normal snippets exhibiting the same bias, and miss anomalies with different patterns. To address this issue, we propose a new MIL framework called Unbiased MIL (UMIL) that aims to learn unbiased anomaly features to enhance WSVAD. In each training iteration of UMIL, we utilize the current detector to divide the samples into two groups: the most confident abnormal/normal snippets and the remaining ambiguous ones. By identifying the invariant features across these two groups, we can eliminate the context biases that vary between them. Extensive experiments conducted on benchmark datasets UCF-Crime and TAD demonstrate the effectiveness of our UMIL framework. Furthermore, we have provided our code for UMIL on GitHub at https://github.com/ktr-hubrt/UMIL.