We propose a method to learn video representation for understanding procedural activities using instructional videos and their narrations from the internet. Our approach does not rely on human annotations and instead learns to encode both action steps and their temporal ordering from a large-scale dataset. We use a deep probabilistic model to capture the temporal dependencies and individual variations in step ordering. Our model not only improves procedure reasoning capabilities but also enhances step recognition. We achieve state-of-the-art results in step classification and forecasting tasks, with significant performance gains. Additionally, our model performs well in zero-shot inference and predicting plausible steps for incomplete procedures. Our code is publicly available at https://github.com/facebookresearch/ProcedureVRL.