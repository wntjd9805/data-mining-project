AV-ASR, or audiovisual automatic speech recognition, aims to enhance the reliability of speech recognition systems by incorporating visual information. However, training fully supervised multimodal models from scratch is limited by the requirement of large labeled audiovisual datasets for each specific domain. In this study, we propose AVFormer, a straightforward approach to augment audio-only models with visual information and perform lightweight domain adaptation. This is achieved by injecting visual embeddings into a frozen ASR model using lightweight trainable adaptors. We demonstrate that these adaptors can be trained on a small amount of weakly labeled video data with minimal additional training time and parameters. Additionally, we introduce a curriculum scheme during training, which is crucial for enabling the model to effectively process both audio and visual information. Our model achieves state-of-the-art zero-shot results on three different AV-ASR benchmarks (How2, VisSpeech, and Ego4D) while maintaining decent performance on traditional audio-only speech recognition benchmarks (LibriSpeech). Qualitative results highlight that our model effectively utilizes visual information for robust speech recognition.