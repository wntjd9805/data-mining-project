This paper presents a scalable and general self-supervised pre-training method called Video Masked Autoencoder (VideoMAE) for building video foundation models. The authors address the challenge of training video foundation models with billions of parameters by scaling the VideoMAE in both model and data. They propose a dual masking strategy where an encoder operates on a subset of video tokens and a decoder processes another subset of tokens, resulting in an efficient pre-training process. The masking decoder further reduces computational cost, enabling the pre-training of billion-level models in video. The authors also use a progressive training paradigm involving initial pre-training on a diverse unlabeled dataset and post-pre-training on a mixed labeled dataset. They successfully train a video ViT model with a billion parameters, achieving state-of-the-art performance on Kinetics and Something-Something datasets. Additionally, the pre-trained video ViT models are extensively evaluated on various downstream tasks, demonstrating their effectiveness as general video representation learners.