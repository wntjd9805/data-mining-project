We propose a novel approach for conditional image-to-video (cI2V) generation, which involves generating a plausible video based on an image and a condition. The challenge lies in generating both spatial appearance and temporal dynamics that correspond to the given image and condition. Our approach, called latent flow diffusion models (LFDM), addresses this challenge by synthesizing an optical flow sequence in the latent space and warping the given image based on the generated flow. Unlike previous methods, LFDM fully utilizes the spatial content of the image to generate spatial details and temporal motion. The training of LFDM involves two stages: an unsupervised learning stage to train a latent flow auto-encoder for spatial content generation, and a conditional learning stage to train a diffusion model (DM) for temporal latent flow generation. The DM in our LFDM only needs to learn a low-dimensional latent flow space, making it more computationally efficient. We conducted experiments on multiple datasets and demonstrated that LFDM outperforms previous methods consistently. Additionally, LFDM can be easily adapted to new domains by finetuning the image decoder. Our code is available at the provided GitHub link.