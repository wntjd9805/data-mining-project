We present Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP's contrastive loss. The aim is to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. This alignment enables the model to identify image regions corresponding to a given text input, allowing seamless transfer to open vocabulary semantic segmentation without the need for segmentation annotations during training. By incorporating PACL into pre-trained CLIP encoders, we achieve state-of-the-art performance on four different segmentation benchmarks: Pascal VOC, PascalContext, COCO Stuff, and ADE20K. Additionally, we demonstrate that PACL can also improve zero-shot classification accuracy for image-level predictions when used with a CLIP backbone. In fact, across a set of 12 image classification datasets, PACL consistently outperforms CLIP in zero-shot classification accuracy.