We present the SceneDiffuser, a novel model for 3D scene understanding. Unlike previous approaches, SceneDiffuser is both aware of the scene and guided by physics, while also being goal-oriented. By employing an iterative sampling strategy, SceneDiffuser integrates scene-aware generation, physics-based optimization, and goal-oriented planning using a differentiable diffusion-based denoising process. This design overcomes the limitations of previous scene-conditioned generative models, such as discrepancies among modules and posterior collapse. We evaluate SceneDiffuser on various tasks, including human pose and motion generation, dexterous grasp generation, 3D navigation path planning, and robot arm motion planning. The results demonstrate significant improvements compared to previous models, highlighting the vast potential of SceneDiffuser for the wider 3D scene understanding community.