Many researchers have focused on reducing the number of floating-point operations (FLOPs) to design faster neural networks. However, we have found that reducing FLOPs does not necessarily result in a corresponding reduction in latency. This is because the low floating-point operations per second (FLOPS) are inefficient. We have identified that the main cause of this inefficiency is the frequent memory access of operators, particularly the depthwise convolution. In response, we propose a new approach called partial convolution (PConv) that improves the extraction of spatial features by reducing redundant computation and memory access. Based on PConv, we introduce FasterNet, a new family of neural networks that achieves significantly higher running speeds without sacrificing accuracy in various vision tasks. For example, our tiny FasterNet-T0 is 2.8×, 3.3×, and 2.4× faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while maintaining 2.9% higher accuracy on ImageNet-1k. Additionally, our large FasterNet-L achieves an impressive 83.5% top-1 accuracy, comparable to Swin-B, while providing 36% higher inference throughput on GPU and saving 37% compute time on CPU. Our code is available at https://github.com/JierunChen/FasterNet.