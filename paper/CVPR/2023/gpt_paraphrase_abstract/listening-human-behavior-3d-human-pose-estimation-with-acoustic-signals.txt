This study investigates the extent to which behavior of humans can be inferred solely from acoustic signals, without any high-level information such as voices or sounds of scenes/actions. Existing methods that rely on human speech or specific actions suffer from privacy issues. To overcome this challenge, the researchers propose a novel approach using active acoustic sensing with a single pair of microphones and loudspeakers. The diffraction of sound makes it difficult to capture detailed pose information, but the researchers introduce a framework that encodes multichannel audio features into 3D human poses. By extracting phase features along with typical spectrum features, they aim to capture subtle sound changes. It is observed that reflected or diffracted sounds are influenced by physical differences among subjects, which negatively affects prediction accuracy. To address this, a subject discriminator is employed to improve accuracy. Experimental results demonstrate that their method outperforms baseline methods using only low-dimensional acoustic information. The datasets and codes used in this project will be made publicly available.