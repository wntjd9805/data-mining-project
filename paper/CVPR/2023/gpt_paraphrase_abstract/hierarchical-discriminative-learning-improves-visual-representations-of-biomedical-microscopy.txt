Learning high-quality, self-supervised, visual representations is crucial for advancing computer vision in biomedical microscopy and clinical medicine. Previous studies have focused on self-supervised representation learning (SSL) methods for instance discrimination and applied them directly to image patches or fields-of-view from gigapixel whole-slide images (WSIs) used in cancer diagnosis. However, this approach has limitations. Firstly, it assumes that patches from the same patient are independent. Secondly, it overlooks the patient-slide-patch hierarchy in clinical biomedical microscopy. Thirdly, it requires strong data augmentations that may negatively impact downstream performance. It is important to note that sampled patches from WSIs of a patient's tumor encompass a diverse range of image examples that represent the same cancer diagnosis. To address these limitations, we propose HiDisc, a data-driven method that leverages the inherent patient-slide-patch hierarchy in clinical biomedical microscopy. HiDisc defines a hierarchical discriminative learning task that implicitly learns features of the underlying diagnosis. Our approach uses a self-supervised contrastive learning framework, where positive patch pairs are defined based on their common ancestry in the data hierarchy. Furthermore, we employ a unified patch, slide, and patient discriminative learning objective for visual SSL. We evaluate HiDisc on two vision tasks using two biomedical microscopy datasets and demonstrate that (1) HiDisc pretraining outperforms current state-of-the-art self-supervised pretraining methods for cancer diagnosis and genetic mutation prediction, and (2) HiDisc learns high-quality visual representations without relying on strong data augmentations. Figure 1 illustrates the hierarchical self-supervised discriminative learning approach adopted by HiDisc, which integrates patch, slide, and patient discrimination into a unified self-supervised learning task.