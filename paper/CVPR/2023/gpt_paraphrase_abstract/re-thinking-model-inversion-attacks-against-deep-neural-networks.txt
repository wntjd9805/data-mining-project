Model inversion attacks are a concern due to the potential leakage of private training data through unauthorized access to a model. Recent advancements in model inversion algorithms have improved attack performance. In this study, we address two fundamental issues in state-of-the-art model inversion algorithms and propose solutions that significantly enhance attack performance. Firstly, we analyze the sub-optimal optimization objective of current algorithms and introduce an improved objective that enhances attack performance. Secondly, we identify the problem of "MI overfitting" which hinders the reconstruction of meaningful training data and propose a novel concept of "model augmentation" to overcome this issue. Our proposed solutions are straightforward yet highly effective, resulting in a substantial 11.8% improvement in accuracy on the standard CelebA benchmark, achieving over 90% attack accuracy for the first time. These findings highlight the clear risk of sensitive information leakage from deep learning models, emphasizing the importance of privacy considerations. Our code, demo, and models can be accessed at https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/.