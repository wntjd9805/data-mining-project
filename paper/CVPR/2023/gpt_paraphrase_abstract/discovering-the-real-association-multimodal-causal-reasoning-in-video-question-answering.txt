Video Question Answering (VideoQA) is a challenging task that involves accurately correlating information from different modalities. Previous methods have focused on addressing specific challenges, such as extracting features from multiple modalities, aligning video and text, and fusing information. However, these methods often rely on statistical evidence, ignoring potential biases in the multimodal data. In this study, we adopt a causal representation perspective to investigate the relational structure of multimodal data and propose a new inference framework.In the visual modality, there may be objects that are irrelevant to the question but establish simple associations with the answer. On the other hand, in the textual modality, the model tends to prefer local phrase semantics, which may deviate from the global semantics in long sentences. To improve the model's generalization, we explicitly capture visual features that are causally related to the question semantics and reduce the impact of local language semantics on question answering.We evaluate our proposed framework on two large causal VideoQA datasets and observe the following benefits: 1) it enhances the accuracy of the existing VideoQA backbone, and 2) it demonstrates robustness in handling complex scenes and questions. The code for our framework will be made available at https://github.com/Chuanqi-Zang/Discovering-the-Real-Association.