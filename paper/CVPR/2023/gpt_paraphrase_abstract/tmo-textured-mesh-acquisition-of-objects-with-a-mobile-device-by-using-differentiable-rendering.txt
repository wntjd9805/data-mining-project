We propose a novel pipeline for capturing detailed 3D models using a single smartphone in real-world environments. Our pipeline provides access to images, depth maps, and accurate camera poses. Initially, we utilize an RGBD-aided structure from motion approach to obtain filtered depth maps and refine camera poses using depth information. Next, we employ a neural implicit surface reconstruction technique to generate high-quality meshes. We also introduce a new training process that incorporates regularization from classical multi-view stereo methods. Additionally, we employ differentiable rendering to enhance incomplete texture maps and produce visually realistic textures that closely resemble the original scene. The versatility of our pipeline allows for capturing common objects without the need for controlled settings or precise mask images. We demonstrate the effectiveness of our approach by capturing objects with complex shapes and validate our results through quantitative comparisons with existing 3D reconstruction and texture mapping methods.