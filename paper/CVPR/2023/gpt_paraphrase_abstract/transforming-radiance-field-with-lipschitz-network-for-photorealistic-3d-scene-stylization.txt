Advancements in 3D scene representation and novel view synthesis have resulted in the emergence of Neural Radiance Fields (NeRFs). However, utilizing NeRF for the task of photorealistic 3D scene stylization is not straightforward. The goal of this task is to generate visually consistent and photorealistic stylized scenes from new viewpoints. Combining NeRF with photorealistic style transfer (PST) alone leads to inconsistencies and degradation in the synthesis of stylized views. After thorough analysis, we propose a new approach to simplify this complex task. We demonstrate that by transforming the appearance representation of a pre-trained NeRF using Lipschitz mapping, the consistency and photorealism across source views can be seamlessly encoded into the syntheses. This insight motivates the development of a concise and flexible learning framework called LipRF, which enhances arbitrary 2D PST methods with Lipschitz mapping tailored for 3D scenes. The technical process of LipRF involves pre-training a radiance field to reconstruct the 3D scene and then applying 2D PST to emulate the style on each view as a prior for learning a Lipschitz network to stylize the pre-trained appearance. To address the impact of Lipschitz condition on the expressivity of the neural network, we introduce an adaptive regularization technique to balance the reconstruction and stylization. Additionally, a gradual gradient aggregation strategy is employed to optimize LipRF in a cost-efficient manner. We conducted extensive experiments to evaluate LipRF's performance in photorealistic 3D stylization and object appearance editing. The results demonstrate the high quality and robustness of LipRF in both tasks.