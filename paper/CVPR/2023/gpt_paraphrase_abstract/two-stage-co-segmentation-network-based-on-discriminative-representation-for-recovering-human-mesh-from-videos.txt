Recent advancements in recovering 3D human mesh from videos have shown promising progress. However, existing methods primarily concentrate on maintaining temporal consistency in videos, neglecting the importance of spatial representation in complex scenes. As a result, these methods struggle to generate a reasonable and smooth sequence of human meshes in challenging conditions such as extreme lighting and chaotic backgrounds. To address this issue, we propose a two-stage co-segmentation network that leverages discriminative representation to recover human body meshes from videos.In the first stage of our network, we focus on segmenting the spatial domain of the video to extract fine-grained spatial information. To enhance the discriminative representation within each frame, we employ a dual-excitation mechanism and a frequency domain enhancement module. Simultaneously, we suppress irrelevant information, such as the background, to improve the quality of the recovered human mesh sequence. Moving to the second stage, we shift our attention to the temporal context of the video by segmenting its temporal domain. Here, we develop a dynamic integration strategy to model inter-frame discriminative representation. This allows us to capture the temporal dependencies and further improve the quality of the recovered human meshes.To ensure the generation of plausible human actions, we introduce a landmark anchor area loss that constrains the variation of the human motion area. This loss function plays a crucial role in efficiently generating realistic and discriminative human actions.We conducted extensive experiments on large publicly available datasets and our method showcased superior performance compared to most state-of-the-art approaches. We plan to make our code publicly available to facilitate further research and advancements in this field.