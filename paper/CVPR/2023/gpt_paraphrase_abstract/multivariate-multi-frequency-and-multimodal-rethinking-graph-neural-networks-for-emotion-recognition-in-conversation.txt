The Emotion Recognition in Conversation (ERC) task poses a challenge due to the complex relationships across modalities and contexts. Previous approaches have loosely represented these relationships, potentially hindering relationship modeling. Graph Neural Networks (GNNs) have emerged as a solution for capturing data relations, but existing GNN-based ERC models have limitations, such as assuming pairwise relationships and disregarding high-frequency signals. To address these limitations, we propose a GNN-based model that considers multivariate relationships and values multi-frequency signals to capture the varying importance of emotion discrepancy and commonality. Our model enhances the ability of GNNs to capture relationships among utterances and improves multimodal and contextual modeling. Experimental results demonstrate that our method surpasses previous state-of-the-art approaches on two widely used multimodal ERC datasets.