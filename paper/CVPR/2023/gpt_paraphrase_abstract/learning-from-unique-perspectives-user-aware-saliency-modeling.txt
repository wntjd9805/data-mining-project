This paper focuses on the importance of visual preferences in attention modeling and introduces the concept of user-aware saliency modeling. The authors propose a new model that can adaptively predict personalized attention, user group attention, and general saliency using a single model. They also propose a learning method to understand visual attention in a progressive manner by incorporating knowledge about the composition of attention from different users. The authors conduct extensive analyses on publicly available saliency datasets to examine the roles of visual preferences. Experimental results demonstrate the effectiveness of their method in capturing the distinct visual behaviors of different users and the general saliency of visual stimuli.