We introduce LIRF (Local Implicit Ray Function), a novel neural rendering technique that addresses the issue of blurred or aliased views in current generalizable neural radiance fields (NeRF) methods. These methods typically sample a scene with a single ray per pixel, which can lead to inconsistencies when the input and rendered views have different resolutions. To overcome this challenge, LIRF utilizes conical frustums to aggregate information and construct a ray. By considering 3D positions within these frustums, LIRF takes the 3D coordinates and features as inputs and predicts a local volumetric radiance field. The continuous nature of the coordinates enables LIRF to render high-quality novel views with a continuously-valued scale using volume rendering. Additionally, our approach employs transformer-based feature matching to predict visible weights for each input view, improving performance in occluded areas. Experimental results on real-world scenes demonstrate that LIRF surpasses state-of-the-art methods in rendering novel views of unseen scenes at arbitrary scales.