The conventional approach to video understanding tasks treats them in isolation and uses distinct datasets for each task. However, wearable cameras provide an immersive egocentric perspective that presents a connected network of video understanding tasks. These tasks, such as hand-object manipulations, navigation, and human-human interactions, continuously unfold based on the person's goals. This interconnectedness calls for a more unified approach. We propose EgoTask Translation (EgoT2), which utilizes a collection of models optimized for separate tasks and learns to translate their outputs to improve performance across all tasks simultaneously. Unlike traditional transfer or multi-task learning, EgoT2 employs separate task-specific backbones and a shared task translator. This design captures synergies between diverse tasks and mitigates task competition. We demonstrate the effectiveness of our model on various video tasks from the Ego4D dataset, surpassing existing transfer paradigms and achieving top-ranked results in four Ego4D 2022 benchmark challenges.