Multi-frame depth estimation methods often achieve high accuracy by relying on multi-view geometric consistency. However, in dynamic scenes such as autonomous driving, this consistency is often violated in the dynamic areas, resulting in inaccurate estimations. Many existing methods address this issue by identifying dynamic areas with explicit masks and compensating for the multi-view cues using local monocular depth or features. However, these methods have limitations due to the quality of the masks and the underutilization of the fusion of the two types of cues.To overcome these limitations, we propose a novel method that learns to fuse multi-view and monocular cues encoded as volumes without the need for manually created masks. Our analysis reveals that multi-view cues capture more accurate geometric information in static areas, while monocular cues capture more useful contexts in dynamic areas. To leverage the geometric perception learned from multi-view cues in static areas and enhance the representation of the multi-view cost volume with monocular cues in dynamic areas, we introduce a cross-cue fusion (CCF) module. This module includes a cross-cue attention (CCA) mechanism that encodes spatially non-local relative intra-relations from each source to enhance the representation of the other.Experimental results on real-world datasets demonstrate the significant effectiveness and generalization ability of our proposed method.