Large-scale vision-language models (VLMs) that are pre-trained on extensive data have acquired comprehensive visual representations and broad visual concepts. It is important to appropriately transfer the learned knowledge structure of these VLMs to downstream tasks with limited data. However, existing efficient transfer learning approaches for VLMs either compromise the prior knowledge or exhibit excessive bias towards it. For instance, prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one, while adapter-style tuning (AT) solely relies on the pre-trained features. To overcome these limitations, we propose a new efficient tuning approach called Task Residual Tuning (TaskRes). TaskRes operates directly on the text-based classifier and explicitly separates the prior knowledge of the pre-trained models from new knowledge related to a target task. TaskRes retains the original classifier weights from the VLMs and generates a new classifier for the target task by fine-tuning a set of prior-independent parameters as a residual to the original classifier. This approach ensures reliable preservation of prior knowledge and allows for flexible exploration of task-specific knowledge. TaskRes is a simple yet effective method that outperforms previous efficient transfer learning methods (e.g., PT and AT) on 11 benchmark datasets, while requiring minimal implementation effort. The code for TaskRes is available at https://github.com/geekyutao/TaskRes.