This paper proposes a novel approach called OKDPH (Online Knowledge Distillation via Parameter Hybridization) for online knowledge distillation (OKD) in order to improve students' generalization ability. Unlike existing techniques that rely on sophisticated modules, OKDPH leverages multi-model settings to achieve a distillation effect with excellent generalization performance. The approach involves constructing a Hybrid-Weight Model (HWM) by linearly weighting the parameters of multiple student models in each training batch. The supervision loss of HWM is used to estimate the curvature of the loss landscape and measure generalization explicitly. By integrating HWM's loss into students' training, OKDPH promotes flatter minima and obtains robust solutions. To address the redundancy of parameters and prevent the collapse of HWM, a fusion operation is introduced to maintain the high similarity of students. Experimental results demonstrate that OKDPH outperforms state-of-the-art OKD methods and methods that seek flat minima in terms of performance, parameter efficiency, lightweightness, and robustness. The code for OKDPH is publicly available at https://github.com/tianlizhang/OKDPH.