Large-scale training of deep learning models relies heavily on publicly available data from the internet, raising concerns about data privacy. To address this issue, previous methods have added small noises to make the data unlearnable for models. However, these methods are susceptible to adversarial training and can be computationally intensive. In this study, we propose a novel technique called Convolution-based UnlearnableDAtaset (CUDA) generation. CUDA is created using controlled class-wise convolutions with randomly generated filters based on a private key. This encourages the network to learn the relationship between filters and labels rather than informative features for classification. Theoretical analysis demonstrates that CUDA effectively poisons Gaussian mixture data, reducing the performance of the optimal Bayes classifier on clean data. We also empirically validate the effectiveness of CUDA on various datasets and architectures. Our experiments show that CUDA remains robust to different data augmentations, training approaches, and adaptive defenses. For example, training a ResNet-18 on ImageNet-100 CUDA achieves only low clean test accuracies with different training methods compared to ERM on clean data. Even when only a fraction of the training dataset is perturbed, CUDA still exhibits the unlearnability effect with ERM. Overall, our findings highlight CUDA as a robust solution for protecting data privacy in deep learning training.