The problem of converting images into discrete representations for generative modeling has been a longstanding challenge. Current methods either determine the discrete representation deterministically by selecting the best-matching token or stochastically by sampling from a predicted distribution. However, deterministic quantization often leads to codebook collapse and misalignment during inference, while stochastic quantization suffers from low codebook utilization and perturbed reconstruction. To address these issues, this paper introduces a regularized vector quantization framework that incorporates two types of regularization. The first is prior distribution regularization, which compares the predicted token distribution to a prior distribution to prevent codebook collapse and low utilization. The second is stochastic mask regularization, which introduces randomness during quantization to strike a balance between inference misalignment and unperturbed reconstruction. Additionally, a probabilistic contrastive loss is designed to mitigate the perturbed reconstruction objective. Extensive experiments demonstrate that this proposed framework consistently outperforms existing vector quantization methods in various generative models, including auto-regressive and diffusion models.