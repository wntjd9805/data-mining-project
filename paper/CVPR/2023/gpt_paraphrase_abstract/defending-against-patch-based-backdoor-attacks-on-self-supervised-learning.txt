Recently, there has been evidence that self-supervised learning (SSL) is susceptible to patch-based data poisoning backdoor attacks. These attacks involve an adversary poisoning a portion of the unlabeled data, resulting in a final SSL model that contains a backdoor that the adversary can exploit. This research aims to protect self-supervised learning against such attacks by proposing a three-step defense pipeline.In the first step, a model is trained on the poisoned data. Then, in the second step, a defense algorithm called PatchSearch is introduced. PatchSearch utilizes the trained model to search for poisoned samples within the training data and removes them from the set. Finally, in the third step, a final model is trained using the cleaned-up training set.The results of this study demonstrate that PatchSearch is a highly effective defense mechanism. For instance, it significantly improves the accuracy of a model on images containing the trigger from 38.2% to 63.7%, which is comparable to the accuracy of a clean model (64.6%). Moreover, PatchSearch outperforms other baseline approaches and state-of-the-art defense methods, including those that employ additional clean, trusted data. The code for PatchSearch is available at https://github.com/UCDvision/PatchSearch.