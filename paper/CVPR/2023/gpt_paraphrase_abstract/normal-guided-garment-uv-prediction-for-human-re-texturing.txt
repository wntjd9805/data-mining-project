This study explores the challenge of editing human videos in a realistic manner by considering the complex geometric deformations of clothes. To achieve this, a texture map needs to account for both the garment transformation caused by body movements and the fine-grained 3D surface geometry. Typically, 3D reconstruction of dynamic clothes from images or videos is required. However, this paper presents a novel approach that allows for editing dressed human images and videos without the need for 3D reconstruction. Instead, a geometry-aware texture map is estimated between the garment region in an image and the texture space (UV map). This UV map preserves isometry with respect to the underlying 3D surface by utilizing predicted 3D surface normals from the image. The proposed method captures the garment's geometry in a self-supervised manner, eliminating the need for annotated UV maps, and can be extended to predict temporally coherent UV maps. Experimental results demonstrate the superiority of the proposed method compared to existing approaches in estimating human UV maps using both real and synthetic data.