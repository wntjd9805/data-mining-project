This study focuses on the problem of estimating the pose of a robot from a sequence of single-view images taken by an online camera. This is an important task for robots to interact with their environment. The main challenges in this task are the robot's self-occlusions and the ambiguity present in single-view images. The authors propose a method that utilizes temporal information and the prior knowledge of the robot's structure to address these challenges. By considering successive frames and the robot's joint configuration, the method learns to accurately determine the 2D coordinates of predefined keypoints on the robot, such as its joints. Using the known camera intrinsic parameters and the robot's joint status, the camera-to-robot pose is estimated using a Perspective-n-point (PnP) solver. The estimate is further refined iteratively using the robot structure prior. To train the proposed method, a large-scale synthetic dataset is created with domain randomization to bridge the gap between simulation and reality. Extensive experiments conducted on both synthetic and real-world datasets, as well as a robotic grasping task, demonstrate that the proposed method achieves state-of-the-art performance and outperforms traditional hand-eye calibration algorithms in real-time (36 FPS). The project page provides access to the code and data: https://sites.google.com/view/sgtapose.