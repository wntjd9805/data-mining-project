This study introduces a multimodal approach to address the challenge of reconstructing 4D faces from monocular videos. Traditional methods for 3D face reconstruction from 2D images face limitations due to depth ambiguity. While existing techniques rely on either visual information or audio-based 3D mesh animation, our proposed method, AV-Face, combines both modalities to accurately reconstruct facial and lip motion in 4D without the need for 3D ground truth during training. Our approach involves a coarse stage that estimates per-frame parameters of a 3D morphable model, followed by lip refinement and a fine stage that recovers facial geometric details. By leveraging transformer-based modules to capture temporal audio and video information, our method remains robust even in cases of insufficient modalities or face occlusions. Through extensive qualitative and quantitative evaluation, our method outperforms the current state-of-the-art techniques.