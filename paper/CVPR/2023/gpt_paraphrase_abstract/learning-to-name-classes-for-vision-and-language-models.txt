Large-scale vision and language models have demonstrated impressive zero-shot recognition performance by mapping class-specific text queries to image content. However, two challenges persist: the models are highly sensitive to hand-crafted class names used as queries, and they struggle to adapt to new, smaller datasets. To address these issues, we propose utilizing available data to learn an optimal word embedding for each class based on its visual content. By training new word embeddings on a frozen model, we can maintain zero-shot capabilities for new classes, easily adjust models to new datasets, and correct potentially inaccurate or ambiguous class names. Our approach can be seamlessly integrated into image classification and object detection pipelines, resulting in significant performance improvements across various scenarios. Additionally, it provides valuable insights into model biases and labeling errors.