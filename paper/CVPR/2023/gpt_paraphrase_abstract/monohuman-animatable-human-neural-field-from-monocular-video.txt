The ability to animate virtual avatars with free-view control is crucial for applications such as virtual reality and digital entertainment. Previous studies have used neural radiance field (NeRF) to reconstruct the human body from monocular videos. Some recent works have incorporated a deformation network into NeRF to model the dynamics of human motion. However, these approaches either rely on pose-dependent representations or struggle with motion coherency, making it challenging to realistically generalize to unseen pose sequences.In this paper, we propose a novel framework called MonoHuman, which addresses these limitations and generates high-quality avatars under arbitrary novel poses. Our approach involves modeling the deformation field using bi-directional constraints and leveraging keyframe information for coherent results. Specifically, we introduce a Shared Bidirectional Deformation module that creates a pose-independent deformation field by separating backward and forward deformation correspondences. We also devise a Forward Correspondence Search module that uses keyframe features to guide the rendering network. As a result, our rendered avatars are view-consistent and have high fidelity, even in challenging pose settings.We conducted extensive experiments to evaluate the performance of our MonoHuman framework against state-of-the-art methods. The results demonstrate the superiority of our approach in terms of rendering realistic avatars under various pose conditions.