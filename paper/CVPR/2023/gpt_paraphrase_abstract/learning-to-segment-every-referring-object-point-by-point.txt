Referring Expression Segmentation (RES) aims to align visual and linguistic information at the pixel level. However, existing RES methods require extensive and costly pixel-level annotations. This paper proposes a new approach for RES training, which uses abundant referring bounding boxes and only a small percentage of pixel-level masks (e.g., 1%). To improve transferability from the REC model, the proposed approach adopts a point-based sequence prediction model. The co-content teacher-forcing method is introduced to explicitly associate point coordinates with referred spatial features, reducing exposure bias caused by limited segmentation masks. Additionally, the resampling pseudo points strategy is proposed to select more accurate pseudo-points as supervision, making the most of referring bounding box annotations. Experimental results demonstrate that the proposed model achieves 52.06% accuracy on Re-fCOCO+@testA, using only 1% of the mask annotations, compared to 58.93% in a fully supervised setting. The code for the proposed model is available at https://github.com/qumengxue/Partial-RES.git.