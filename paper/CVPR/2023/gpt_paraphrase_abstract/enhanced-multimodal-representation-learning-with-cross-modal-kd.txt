This paper investigates the use of auxiliary modalities available during training to enhance the learning of multimodal representations. The commonly used objective of maximizing mutual information often leads to a weak solution, where the teacher model is made as weak as the student model to achieve maximum mutual information. To avoid this weak solution, we introduce an additional objective term that considers the mutual information between the teacher and the auxiliary modality model. Additionally, we propose minimizing the conditional entropy of the teacher given the student to reduce the information gap between them. We design novel training schemes based on contrastive learning and adversarial learning to optimize mutual information and conditional entropy, respectively. Experimental results on three popular multimodal benchmark datasets demonstrate that our proposed method outperforms several state-of-the-art approaches in video recognition, video retrieval, and emotion classification.