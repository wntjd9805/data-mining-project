We propose a method called Text2Scene that automatically creates realistic textures for virtual scenes consisting of multiple objects. Our approach uses a reference image and text descriptions to guide the process of adding detailed textures to labeled 3D geometries in the scene. The generated colors respect the hierarchical structure and semantic parts of the objects, which are often made of similar materials. Instead of applying a flat stylization to the entire scene at once, we use weak semantic cues from geometric segmentation and assign initial colors to segmented parts. Then, we add texture details to individual objects in a way that aligns their projections on the image space with the input's feature embedding. This decomposition allows our pipeline to be computationally tractable and memory-efficient. Unlike other methods, our framework leverages existing image and text embedding resources and does not require dedicated datasets with high-quality textures created by skilled artists. To the best of our knowledge, this is the first practical and scalable approach that achieves these results.