Unsupervised domain adaptive object detection (UDA-OD) is the task of training a detector that can generalize knowledge from a labeled source domain to an unlabeled target domain. Existing graph-based methods for UDA-OD perform well in some cases, but they struggle to learn a suitable node set for the graph. Furthermore, these methods only consider visual features and overlook the linguistic knowledge carried by semantic prototypes such as dataset labels.   To address these issues, we propose a method called cross-modality graph reasoning adaptation (CIGAR) that leverages both visual and linguistic knowledge. Our approach involves performing cross-modality graph reasoning between linguistic and visual modality graphs to enhance their representations. We also introduce a discriminative feature selector that identifies the most discriminative features and uses them as nodes in the visual graph, ensuring both efficiency and effectiveness. Additionally, we incorporate a linguistic graph matching loss to regulate the update of linguistic graphs and maintain their semantic representation during training.   Through comprehensive experiments, we demonstrate the effectiveness of our proposed CIGAR method.