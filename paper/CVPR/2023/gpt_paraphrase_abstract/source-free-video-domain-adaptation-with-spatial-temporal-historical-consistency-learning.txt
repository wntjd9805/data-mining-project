Source-free domain adaptation (SFDA) is a growing area of research that focuses on adapting a pretrained model using unlabeled target data, without relying on labeled source data. This is particularly advantageous in real-world scenarios where access to source data is limited. While SFDA has been explored for images, there has been little attention given to videos. Simply applying image-based SFDA methods to videos often produces unsatisfactory outcomes as videos have unique characteristics. In this study, we propose a straightforward and adaptable approach for Source-Free Video Domain Adaptation (SFVDA) that leverages consistency learning from spatial, temporal, and historical perspectives. Our approach assumes that videos of the same action category share a low-dimensional space, regardless of the spatio-temporal variations that cause domain shifts in the high-dimensional space. To address domain shifts, we simulate spatio-temporal variations by applying spatial and temporal augmentations to the target videos and encourage the model to make consistent predictions across the original and augmented versions. Our method is simple and can be applied to various SFVDA scenarios, and experimental results demonstrate its state-of-the-art performance across all settings.