We introduce a new implicit representation scheme called Multiplicative Fourier Level of Detail (MFLOD) that is inspired by the success of multiplicative filter networks. This scheme operates on a multi-resolution feature grid or volume, such as a sparse voxel octree. At each level, the features are first modulated by a sinusoidal function and then element-wise multiplied by a linear transformation of the previous layer's representation in a recursive manner. This process generates scale-aggregated encodings, which are then fed into a simple linear forward pass to obtain the final output.Unlike previous hybrid representations that rely on interleaved multilevel fusion and nonlinear activation-based decoding, MFLOD can be described as a linear combination of sine basis functions with varying amplitude, frequency, and phase applied to the learned multilevel features. This characteristic makes it highly suitable for Fourier analysis.We conducted comprehensive experiments on various tasks related to implicit neural representation learning, including image fitting, 3D shape representation, and neural radiance fields. The results demonstrate that the proposed MFLOD scheme achieves superior quality and generalizability.