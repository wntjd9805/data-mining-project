Federated Learning with Model Distillation (FedMD) is a new collaborative learning approach that aims to address privacy risks in federated learning. Instead of sharing private model parameters, FedMD only transmits output logits of public datasets as distilled knowledge. However, our research reveals that even this method is not entirely secure, as carefully designed attacks can still lead to data exposure. We discovered that a malicious server can perform a Paired-Logits Inversion (PLI) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between server and client models. Our experiments using facial recognition datasets demonstrate that, using paired server-client logits of public datasets, the malicious server can successfully reconstruct private images with a high success rate. These findings highlight the need for improved security measures in FedMD-like schemes.