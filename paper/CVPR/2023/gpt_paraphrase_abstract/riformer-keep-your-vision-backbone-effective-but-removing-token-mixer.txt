This study focuses on maintaining the effectiveness of a vision backbone while removing token mixers in its basic building blocks. Token mixers, which are used for information communication in vision transformers (ViTs), are computationally expensive and cause latency issues. However, completely removing token mixers leads to an incomplete model structure and a significant drop in accuracy. To address this, the researchers develop a RepIdentityFormer model architecture based on the re-parameterizing concept, which eliminates the need for token mixers. They also explore an improved learning paradigm to enhance the performance of the token mixer-free backbone. Through extensive experiments and analysis, the researchers demonstrate that the network architecture's inductive bias can be incorporated into a simplified network structure with the right optimization strategy. The proposed optimization strategy enables the creation of a highly efficient vision backbone with promising performance. The findings of this study can serve as a starting point for further exploration of optimization-driven efficient network design.