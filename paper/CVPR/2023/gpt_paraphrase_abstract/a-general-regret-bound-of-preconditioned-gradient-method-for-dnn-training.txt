The use of adaptive learning rate methods, such as Adam, has greatly improved the optimization of Deep Neural Networks (DNNs). However, these methods only consider the diagonal elements of the preconditioned matrix, limiting their effectiveness. Despite full-matrix preconditioned gradient methods theoretically having a lower regret bound, they are not practical for training DNNs due to their high complexity. In this study, we introduce a general regret bound with a constrained full-matrix preconditioned gradient. We demonstrate that the updating formula for the preconditioner can be derived by solving a cone-constrained optimization problem. By incorporating block-diagonal and Kronecker-factorized constraints, we obtain a specific guide function. We develop a new DNN optimizer called AdaBK by minimizing the upper bound of the guide function. To make AdaBK efficient and effective, we implement techniques such as statistics updating, dampening, efficient matrix inverse root computation, and gradient amplitude preservation. AdaBK can be easily integrated into existing DNN optimizers like SGDM and AdamW, resulting in significant improvements on benchmark vision tasks such as image classification, object detection, and segmentation. The code for AdaBK is publicly available at https://github.com/Yonghongwei/AdaBK.