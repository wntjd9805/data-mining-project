Creating a universal Video-Language model that can effectively solve various video understanding tasks has been a challenge for the machine learning field. Previous approaches have used uni-modal and cross-modal feature encoders, trained with pair-wise contrastive pretext tasks. However, these models often have to make trade-offs between efficiency and performance and require different architectures for different tasks. This is because the pair-wise training method struggles to align and fuse features from different modalities. To address this issue, we propose Clover, a Correlated Video-Language pre-training method that aims to develop a universal Video-Language model capable of solving multiple video understanding tasks without compromising performance or efficiency. Clover improves cross-modal feature alignment and fusion by introducing a novel tri-modal alignment pre-training task. We further enhance this alignment by incorporating learning from semantic masked samples and a new pair-wise ranking loss. Clover achieves state-of-the-art results on several downstream tasks, including zero-shot and fine-tuning retrieval tasks, as well as video question answering tasks. The codes and pre-trained models for Clover can be accessed at https://github.com/LeeYN-43/Clover.