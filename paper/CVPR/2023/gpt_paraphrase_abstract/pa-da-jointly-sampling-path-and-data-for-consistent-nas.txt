One-shot Neural Architecture Search (NAS) methods train a supernet and then use the pre-trained weights to evaluate sub-models, reducing the search cost. However, it has been observed that the shared weights have different gradient descent directions during training, leading to large gradient variance in the supernet. This gradient variance affects the consistency of the supernet's ranking. To address this issue, we propose a method that minimizes the gradient variance by optimizing the sampling distributions of path and data (PA&DA). We derive a theoretical relationship between the gradient variance and the sampling distributions, showing that the optimal sampling probability is proportional to the normalized gradient norm of path and training data. Using the normalized gradient norm as an importance indicator, we adopt an importance sampling strategy for the supernet training. Our method requires minimal computation cost for optimizing the sampling distributions, but effectively reduces gradient variance and improves the generalization performance of the supernet. We compare our method with other approaches in different search spaces and demonstrate its superiority in terms of reliable ranking performance and accuracy of searched architectures. The code for our method is available at https://github.com/ShunLu91/PA-DA.