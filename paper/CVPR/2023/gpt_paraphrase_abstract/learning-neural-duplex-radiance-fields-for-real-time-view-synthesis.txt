Neural radiance fields (NeRFs) are capable of generating high-quality synthetic images from novel viewpoints. However, this method requires a large number of computations for each pixel, making real-time rendering impractical. In this study, we propose a new technique to convert NeRFs into mesh-based representations that can be efficiently rendered using parallel graphics processing. Our approach encodes scenes as neural radiance features on a two-layer mesh, which addresses inaccuracies in 3D surface reconstruction by learning radiance information from reliable ray-surface intersections. Instead of using time-consuming multilayer perceptrons (MLPs), we employ screen-space convolutions to capture local geometric relationships between pixels, resulting in visually appealing images. Additionally, we enhance the overall performance of our framework through a novel multi-view distillation optimization strategy. Extensive experiments on various datasets demonstrate the effectiveness and superiority of our approach.