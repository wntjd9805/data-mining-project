An efficient unsupervised 3D representation learning method is crucial for annotating the 3D LiDAR data of autonomous driving. This paper presents the Triangle Constrained Contrast (TriCC) framework, specifically designed for autonomous driving scenes, which learns 3D unsupervised representations by incorporating multimodal information and temporal sequences. The framework treats a triplet consisting of one camera image and two LiDAR point clouds with different timestamps as input. The key aspect of the TriCC framework is the consistent constraint that automatically identifies matching relationships within the triplet using a "self-cycle" approach, enabling representation learning. By leveraging matching relations across temporal dimensions and modalities, the framework conducts triplet contrast to enhance learning efficiency. TriCC is the first framework to unify both temporal and multimodal semantics, utilizing comprehensive information in autonomous driving scenes. Unlike previous contrastive methods that rely on handcrafted pairs, TriCC autonomously discovers contrasting pairs with higher difficulty. Extensive experiments are conducted on various semantic segmentation and 3D detection datasets using Minkowski-UNet and VoxelNet. Results demonstrate that TriCC learns effective representations with significantly fewer training iterations and greatly improves state-of-the-art results in all downstream tasks. The code and models for TriCC can be accessed at https://bopang1996.github.io/.