Dataset Condensation is a technique used in deep learning applications to reduce the size of a large dataset while still being able to train a high-performing model. Traditional methods for dataset condensation involve optimization-based approaches, where the dataset is condensed by matching gradients or parameters during model optimization. However, these methods are computationally intensive, even on small datasets and models. To address this issue, this paper proposes a new dataset condensation method based on distribution matching, which is more efficient and promising. The authors identify two important shortcomings of naive distribution matching, namely imbalanced feature numbers and unvalidated embeddings for distance computation. They introduce three novel techniques to overcome these shortcomings: partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization. The proposed method is simple yet effective and outperforms most previous optimization-based methods while using fewer computational resources. This allows for scalability of data condensation to larger datasets and models. Extensive experiments are conducted to demonstrate the effectiveness of the method. The source code for the method is available at https://github.com/uitrbn/IDM.