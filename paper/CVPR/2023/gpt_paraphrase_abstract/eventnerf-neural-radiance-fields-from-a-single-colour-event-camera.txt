Event cameras have gained popularity in various applications due to their unique features such as high dynamic range, low motion blur, low latency, and low data bandwidth. While existing event-based 3D reconstruction approaches have been successful in recovering sparse point clouds, the sparsity remains a limitation in computer vision and graphics. This paper introduces a novel approach for dense and photorealistic 3D reconstruction using a single color event stream. The approach utilizes a neural radiance field trained in a self-supervised manner from events, preserving the original resolution of the color event channels. Additionally, a specialized ray sampling strategy tailored to events enables efficient training. The proposed method achieves unprecedented quality in rendering results in the RGB space. Evaluation on synthetic and real scenes demonstrates significantly denser and visually appealing renderings compared to existing methods. The method also exhibits robustness in challenging scenarios with fast motion and low lighting conditions. To facilitate further research, the authors provide a newly recorded dataset and the source code for their approach.