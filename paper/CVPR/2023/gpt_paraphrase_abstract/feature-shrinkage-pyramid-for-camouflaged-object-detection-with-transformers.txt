We present a novel approach called FeatureShrinkage Pyramid Network (FSPNet) to address the limitations of vision transformers in camouflaged object detection. Vision transformers have shown strong global context modeling capabilities but lack effective locality modeling and feature aggregation in decoders. These limitations hinder the detection of camouflaged objects that rely on subtle cues from indistinguishable backgrounds. To overcome these limitations, we propose the use of a non-local token enhancement module (NL-TEM) that leverages the non-local mechanism to enhance local representations of transformers by interacting neighboring tokens and exploring high-order relations within tokens. Additionally, we introduce a feature shrinkage decoder (FSD) with adjacent interaction modules (AIM) that progressively aggregates adjacent transformer features through a layer-by-layer shrinkage pyramid. This allows us to accumulate imperceptible but effective cues for object information decoding.Our proposed model, FSPNet, outperforms 24 existing competitors on three challenging camouflaged object detection benchmark datasets using six widely-used evaluation metrics. We have made our code publicly available at https://github.com/ZhouHuang23/FSPNet.