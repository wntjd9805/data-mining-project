Event cameras have been proven to be effective for low-latency and low-bandwidth feature tracking in challenging scenarios due to their high temporal resolution and resilience to motion blur. However, existing feature tracking methods for event cameras have limitations such as extensive parameter tuning, sensitivity to noise, and lack of generalizability. To address these issues, we propose the first data-driven feature tracker for event cameras that utilizes low-latency events to track features detected in a grayscale frame. Our approach incorporates a novel frame attention module that enables information sharing across feature tracks, resulting in robust performance. Through the direct transfer of knowledge from synthetic to real data, our data-driven tracker surpasses existing methods in terms of relative feature age by up to 120% while maintaining the lowest latency. Furthermore, by employing a self-supervision strategy to adapt our tracker to real data, the performance gap is further increased to 130%. For additional details, please refer to the provided video and code links.