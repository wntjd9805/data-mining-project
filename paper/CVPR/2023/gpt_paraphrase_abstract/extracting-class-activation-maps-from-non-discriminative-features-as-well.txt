The current method of extracting class activation maps (CAM) from a classification model often fails to accurately identify foreground objects. It only recognizes the discriminative region of an object, such as the head of a sheep, while mistaking the rest as background, like the leg of a sheep. This is because the weight of the classifier used to compute CAM only captures the discriminative features of objects. To address this issue, we propose a new computation method for CAM that explicitly captures both discriminative and non-discriminative features, resulting in CAM that covers the whole object. Our approach involves omitting the last pooling layer of the classification model and performing clustering on all local features of an object class. Local features refer to features at a specific spatial pixel position. We refer to the resulting cluster centers as local prototypes, which represent local semantics such as the head, leg, and body of a sheep. When presented with a new image of the class, we compare its unpooled features to each prototype, derive similarity matrices, and aggregate them into a heatmap, which serves as our CAM. This enables our CAM to capture all local features of the class without discrimination. We evaluate the effectiveness of our method in the challenging tasks of weakly-supervised semantic segmentation (WSSS). We integrate our approach into existing state-of-the-art WSSS methods, including MCTformer and AMN, by replacing their original CAM with ours. Our extensive experiments on standard WSSS benchmarks, such as PASCAL VOC and MS COCO, demonstrate the superiority of our method, with consistent improvements and minimal computational overhead. Our code is available at https://github.com/zhaozhengChen/LPCAM.