The aim of this study is to make the process of animating virtual characters based on real performances more accessible and affordable. Currently, this process requires expensive motion capture setups and expert animators, limiting its availability to large production houses. To address this issue, the researchers have developed a cost-effective alternative called "Transfer4D" that utilizes commodity depth sensors and automates the rigging and animation transfer process, reducing the effort required by animators. Unlike previous methods that assume the source material is noise-free and watertight, this approach can transfer motion from an incomplete, single-view depth video to a semantically similar target mesh. To handle the challenges posed by sparse and incomplete depth videos, as well as variations between source and target objects, the researchers propose using skeletons as an intermediary representation. They have developed an unsupervised skeleton extraction pipeline that incorporates additional geometric information, resulting in superior performance in motion reconstruction and transfer compared to contemporary methods. This approach is also applicable to a wide range of scenarios. The researchers track motion from the depth sequence using non-rigid reconstruction and rig the source object using skinning decomposition. Ultimately, their approach enables motion retargeting using affordable depth sensors, streamlining the animation process and making it more accessible to a broader audience.