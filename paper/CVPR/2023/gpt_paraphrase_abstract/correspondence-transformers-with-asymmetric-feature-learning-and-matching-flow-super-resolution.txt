This paper addresses the challenge of learning dense visual correspondences between different object instances of the same category using sparse annotations. The approach proposed in this paper involves breaking down the problem into two simpler tasks. Firstly, the local feature descriptors of the source and target images are mapped into shared semantic spaces to obtain coarse matching flows. This step involves utilizing an asymmetric feature learning module that employs a biased cross-attention mechanism to encode token features of the source images in relation to their target counterparts. Secondly, the matching flows in low resolution are refined to generate accurate point-to-point matching results. This is achieved by employing a super-resolution network that enhances the matching flow in low resolutions to obtain precise correspondences. The proposed method is built upon vision transformers and can be trained in an end-to-end manner. The effectiveness of the method is demonstrated through extensive experiments on popular benchmarks, including PF-PASCAL, PF-WILLOW, and SPair-71K. The experimental results show that the proposed method efficiently captures subtle semantic differences in pixels. The code for implementing the method is available at https://github.com/YXSUNMADMAX/ACTR.