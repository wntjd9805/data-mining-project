Accurately predicting the locations of human actions in future frames is crucial for applications such as human-robot collaboration. Although some progress has been made in predicting human actions using computer vision, accurately localizing these actions in future frames still needs improvement. To address this, we introduce a new task called spatial action localization in the future (SALF) that aims to predict action locations in both observed and future frames. SALF is challenging as it requires understanding the underlying physics of video observations to accurately predict future action locations. To tackle this task, we utilize the concept of NeuralODE, which models the latent dynamics of sequential data by solving ordinary differential equations (ODE) with neural networks. We propose a novel architecture called AdamsFormer, which extends observed frame features to future time horizons by modeling continuous temporal dynamics through ODE solving. Specifically, we employ the Adams method, a multi-step approach that efficiently utilizes information from previous steps. Through extensive experiments on UCF101-24 and JHMDB-21 datasets, our proposed model demonstrates superior performance compared to existing long-range temporal modeling methods in terms of frame-mAP.