The abstract discusses the challenges of multi-channel video-language retrieval and the need for models to understand information from different channels in order to link videos with textual responses or queries. It mentions the effectiveness of contrastive multimodal models in aligning entities in images/videos and text, but highlights the lack of a clear way to adapt these models to multi-channel video-language retrieval with limited data and resources.The paper proposes a model design space with two axes: how to represent videos and how to fuse video and text information. It explores the options of representing videos using continuous feature vectors or discrete text tokens, and the fusion methods of using a multimodal transformer or a pretrained contrastive text model. The four combinations are extensively evaluated on five video-language datasets.Surprisingly, the study finds that using discrete text tokens coupled with a pretrained contrastive text model yields the best performance. This approach even outperforms state-of-the-art methods on the iVQA and How2QA datasets without the need for additional training on large amounts of video-text data. The analysis suggests that representing videos as text tokens captures important visual information and naturally aligns with text models that are strong retrievers after the contrastive pretraining process.The findings of this study establish a solid foundation for future research on affordable and upgradable multimodal intelligence.