Current methods for video-based 3D human pose and shape estimations rely on intra-frame accuracy and inter-frame smoothness metrics. However, these methods treat these metrics as a single problem and use monotonous modeling structures in their networks. This approach makes it difficult to balance short-term and long-term temporal correlations, leading to issues such as global location shift, temporal inconsistency, and insufficient local details. To address these problems, we propose a novel end-to-end framework called Global-to-Local Transformer (GLoT), which structurally decouples the modeling of long-term and short-term correlations. GLoT consists of a global transformer that incorporates a Masked Pose and Shape Estimation strategy to learn inter-frame correlations, and a local transformer that focuses on capturing local details on the human mesh while interacting with the global transformer through cross-attention. Additionally, we introduce a Hierarchical Spatial Correlation Regressor to refine intra-frame estimations using decoupled global-local representation and implicit kinematic constraints. Our GLoT outperforms previous state-of-the-art methods on popular benchmarks including 3DPW, MPI-INF-3DHP, and Human3.6M, while utilizing the lowest number of model parameters. The source code for GLoT is available at https://github.com/sxl142/GLoT.