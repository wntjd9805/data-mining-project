Text-conditioned image editing has gained significant attention recently. However, existing methods have limitations, such as being restricted to specific editing types, using synthetic images, or requiring multiple input images of the same object. This paper introduces a novel approach that enables complex text-based semantic edits on a single real image. Unlike previous methods, our proposed method only requires one input image and a target text, without any additional inputs or views of the object. We leverage a pre-trained text-to-image diffusion model called Imagic, which produces a text embedding that aligns with both the input image and the target text. Imagic fine-tunes the diffusion model to capture the image-specific appearance. We demonstrate the effectiveness of our approach on various inputs from different domains, showcasing a wide range of high-quality complex semantic image edits within a unified framework. To evaluate performance, we introduce TEdBench, a challenging image editing benchmark. A user study confirms that Imagic outperforms previous leading editing methods on TEdBench.