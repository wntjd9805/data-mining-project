We propose an all-in-one Transformer model for Video-Language Pre-training (VLP) that combines raw video and textual signals into joint representations using a unified backbone architecture. Unlike existing models, our approach addresses the challenge of incorporating temporal information from video data by introducing a novel token rolling operation. This allows us to encode temporal representations in a non-parametric manner. By fine-tuning our pre-trained model on various downstream video-text tasks, we achieve state-of-the-art performance with minimal model FLOPs on ten datasets. Our method outperforms competitive counterparts, demonstrating its superiority. The code and pretrained models are available at https://github.com/showlab/all-in-one.