Although federated learning has made remarkable progress, most studies have assumed that the data provided by each client are fully labeled. However, in real-world scenarios, clients often have a significant amount of unlabeled data. To address this issue, a promising solution called federated active learning has emerged. In this approach, there are two types of query selector models available in the decentralized setting: 'global' and 'local-only' models. However, there is limited literature discussing the performance dominance of these models and the reasons behind it.  In this study, we demonstrate that the superiority of the two selector models depends on the diversity between classes at both the global and local levels. Additionally, we observe that these models play a crucial role in resolving the imbalance between the two sides. Building on these findings, we propose a robust FAL (federated active learning) sampling strategy called LoGo. This strategy integrates both the global and local-only models through a two-step active selection scheme. Through extensive experimentation, we show that LoGo consistently outperforms six other active learning strategies across 38 different experimental settings.  For further details and implementation, the code for LoGo is available at: https://github.com/raymin0223/LoGo.