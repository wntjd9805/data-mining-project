This study explores the process of dataset vectorization for two tasks related to datasets: evaluating the suitability of a training set for a specific domain, and analyzing the difficulty of a test set for a trained model. The key aspect of these tasks is measuring the relationship between datasets. To achieve this, an effective dataset vectorization scheme is needed, which should retain as much informative dataset information as possible, enabling the distance between dataset vectors to reflect their similarity. The proposed solution is a bag-of-prototypes (BoP) representation at the dataset level, which builds upon the image-level bag-of-patches descriptor by incorporating semantic prototypes. The approach involves creating a codebook of K prototypes derived from a reference dataset. When encoding a target dataset, each image feature is quantized to a corresponding prototype in the codebook, resulting in a K-dimensional histogram. Importantly, the BoP representation captures the semantic distribution of the dataset without relying on labels. Additionally, BoP representations work effectively with Jensen-Shannon divergence for measuring dataset similarity. Despite its simplicity, BoP consistently outperforms existing representations in various benchmark tests for the two dataset-level tasks.