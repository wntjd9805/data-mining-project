Careful evaluation of explanations is crucial for reliable models, but it is important to have a clear understanding of the metrics used. Model randomization testing, while valuable, should not be the sole determinant for selecting or dismissing explanation methods. In this study, we highlight a disparity between randomization-based sanity checks and measures of model output faithfulness in ranking explanation methods. We identify limitations of model-randomization-based sanity checks, showing that attribution maps with zero pixel-wise covariance can achieve high scores, thus rendering the checks uninformative. Additionally, we demonstrate that top-down model randomization maintains the scales of forward pass activations, meaning that channels with large activations still contribute strongly to the output even after randomization. Consequently, explanations after randomization are expected to only differ to a certain extent, which explains the observed gap in rankings. In conclusion, these findings illustrate the inadequacy of using model-randomization-based sanity checks as the sole criterion for ranking attribution methods.