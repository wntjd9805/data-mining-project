The performance of Small Object Detection (SOD) is currently not satisfactory compared to general object detection. This paper introduces a new approach called Scale-aware Knowledge Distillation (ScaleKD) to address this issue. ScaleKD transfers knowledge from a complex teacher model to a compact student model, aiming to balance inference speed and SOD performance. The proposed method includes two modules: a scale-decoupled feature distillation module that allows the student model to mimic the teacher's features specifically for small objects, and a cross-scale assistant module that improves the student model's bounding box predictions by capturing multi-scale semantic information. The effectiveness of ScaleKD is evaluated using COCO and VisDrone datasets, with various types of models. The results show that ScaleKD outperforms other methods in both general detection performance and SOD performance.