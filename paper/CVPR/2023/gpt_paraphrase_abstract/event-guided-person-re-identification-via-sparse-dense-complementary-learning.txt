Video-based person re-identification (Re-ID) is a popular field in computer vision that has numerous applications in video surveillance. Existing methods in this field utilize spatial and temporal correlations in frame sequences to extract person features. However, these methods are limited by issues such as motion blur in frames, which leads to the loss of identity-discriminating cues. The emergence of a new bio-inspired sensor called event camera, which can asynchronously record intensity changes, has brought new possibilities to the Re-ID task. This sensor has microsecond resolution and low latency, allowing it to accurately capture pedestrian movements even in degraded environments.In this study, we propose the Sparse-Dense Complementary Learning (SDCL) Framework, which effectively extracts identity features by utilizing both dense frames and sparse events. For frames, we use a CNN-based module to aggregate the dense features of pedestrian appearance. For event streams, we design a bio-inspired spiking neural network (SNN) backbone that encodes event signals into sparse feature maps in a spiking form to capture the dynamic motion cues of pedestrians. We also incorporate a cross feature alignment module to fuse motion information from events and appearance cues from frames, enhancing identity representation learning.Experimental results on various benchmarks demonstrate that our method, which incorporates events and SNN into Re-ID, outperforms other competitive methods. The code for our method is available at the given GitHub link. This work was supported by the National Key R&D Program of China, the National Natural Science Foundation of China, and the University Synergy Innovation Program of Anhui Province. The abstract also includes visual examples of learned feature maps, showcasing the effectiveness of our approach.Note: The abstract has been paraphrased while maintaining the core meaning and reducing the character count to under 4000 characters.