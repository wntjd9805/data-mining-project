This paper examines the issue of temporal video grounding (TVG), which involves predicting the start and end times of moments described in a text sentence within a long untrimmed video. Recent advancements in TVG techniques have been made possible by fine-grained 3D visual features. However, the use of 3D convolutional neural networks (CNNs) to extract these features is time-consuming and requires significant memory and computing resources. To address this, the authors propose a novel framework called text-visual prompting (TVP), which incorporates optimized perturbation patterns (referred to as 'prompts') into both the visual inputs and textual features of a TVG model. Unlike 3D CNNs, TVP allows for effective co-training of vision and language encoders in a 2D TVG model, enhancing the performance of cross-modal feature fusion with low-complexity sparse 2D visual features. Additionally, the authors introduce a Temporal-DistanceIoU (TDIoU) loss for efficient learning of TVG. Experimental results on two benchmark datasets, Charades-STA and ActivityNet Captions, demonstrate that TVP significantly improves the performance of 2D TVG models (e.g., a 9.79% improvement on Charades-STA and a 30.77% improvement on ActivityNet Captions) while achieving 5Ã— faster inference than TVG using 3D visual features. The code for this research is available at Open.Intel.