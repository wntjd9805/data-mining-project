Pre-trained vision-language models have sparked significant interest in few-shot learning. However, when faced with a limited number of training images, two main challenges arise: (1) the presence of distracting information in the visual features of images and (2) the difficulty in aligning the visual and language feature distributions that are irrelevant to specific classes. To address the distraction problem, we propose a Selective Attack module that utilizes trainable adapters to generate spatial attention maps. These maps guide attacks on areas of images that are not relevant to the class, thereby capturing critical features and calibrating the visual feature distributions. In order to achieve better alignment between the visual and language feature distributions, which describe the same object class, we introduce a cross-modal distribution alignment module. This module incorporates a vision-language prototype for each class to align the distributions and employs the Earth Mover's Distance (EMD) to optimize the prototypes. To enhance computational efficiency, we also propose an augmentation strategy that increases the diversity of both the images and text prompts, thereby reducing overfitting to the few-shot training images. Through extensive experiments on 11 datasets, we consistently outperform previous approaches in few-shot learning. The implementation code for our method can be found at https://gitee.com/mindspore/models/tree/master/research/cv/SADA. Additionally, we derive the upper bound of EMD for further reference.