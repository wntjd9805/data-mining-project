Contrastive language-image pre-training (CLIP) is commonly used to align images and texts, but it struggles with the loose correlation between web-crawled data, making it inefficient and requiring a large training batch size. This study explores non-contrastive language-image pre-training (nCLIP) to see if it can produce similar results. The empirical findings show that while nCLIP is effective for representation learning, it underperforms in zero-shot recognition. To address this, a new framework called xCLIP is introduced, which combines CLIP and nCLIP. It is observed that nCLIP improves feature semantics in CLIP, resulting in superior performance in both zero-shot transfer and representation learning. The effectiveness of xCLIP is validated through systematic evaluation across various downstream tasks, including classification, retrieval, and representation learning. The code and pre-trained models for xCLIP will be publicly available at https://github.com/shallowtoil/xclip.