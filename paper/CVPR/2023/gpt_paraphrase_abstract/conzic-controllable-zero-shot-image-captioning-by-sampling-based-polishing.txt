Zero-shot capability in deep learning, which allows machines to perform tasks without curated training data, has been hailed as a revolutionary advancement. ZeroCap, the only existing solution for zero-shot image captioning (IC), has achieved success by employing large-scale pre-trained models to sequentially search for each word in the caption. However, ZeroCap has limitations in terms of caption diversity and inference speed due to its autoregressive generation and gradient-directed searching mechanism, respectively. Additionally, ZeroCap fails to address the issue of controllability in zero-shot IC. To address these challenges, we propose a framework called ConZIC for Controllable Zero-shot IC. The core component of ConZIC is a novel non-autoregressive language model called Gibbs-BERT, which utilizes sampling techniques to generate and refine each word in a continuous manner. Through extensive quantitative and qualitative evaluations, we demonstrate that ConZIC outperforms ZeroCap in both zero-shot IC and controllable zero-shot IC. Notably, ConZIC achieves approximately 5 times faster generation speed compared to ZeroCap and exhibits 1.5 times higher diversity scores while accurately generating captions based on different control signals. The code for ConZIC is publicly available at https://github.com/joeyz0z/ConZIC.