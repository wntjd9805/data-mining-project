Current text recognition systems have relied on image synthesis and augmentation to overcome the lack of real-world complexity and diversity in training data. This paper introduces a method called Conditional Text Image Generation with Diffusion Models (CTIG-DM) that leverages the capabilities of Diffusion Models to generate photo-realistic and diverse text images. The method utilizes three conditions: image condition, text condition, and style condition, to control attributes, contents, and styles of the generated images. By combining and configuring these conditions, four text image generation modes are derived: synthesis mode, augmentation mode, recovery mode, and imitation mode. Extensive experiments on handwritten and scene text demonstrate that CTIG-DM can generate image samples that mimic real-world complexity and diversity, thereby enhancing the performance of existing text recognizers. Additionally, CTIG-DM shows promise in domain adaptation and generating images containing Out-Of-Vocabulary (OOV) words.