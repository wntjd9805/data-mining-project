Unsupervised domain adaptation (UDA) involves adapting a model trained on one type of data to another type of data without access to target annotations. Previous UDA methods have struggled with classes that visually resemble each other in the target domain, as there is no ground truth available to learn the subtle appearance differences. To address this issue, we propose the Masked Image Consistency (MIC) module, which enhances UDA by learning the spatial context relations of the target domain as additional clues for robust visual recognition. MIC enforces consistency between predictions of masked target images and pseudo-labels generated based on the complete image. This is achieved using an exponential moving average teacher. By minimizing the consistency loss, the network learns to infer predictions of the masked regions from their context. MIC can be integrated into various UDA methods for different visual recognition tasks such as image classification, semantic segmentation, and object detection. MIC significantly improves the state-of-the-art performance across these recognition tasks in synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA scenarios. For example, MIC achieves unprecedented UDA performances of 75.9 mIoU and 92.8% on GTAâ†’Cityscapes and VisDA-2017, respectively. These results correspond to improvements of +2.1 and +3.0 percentage points over the previous state of the art. The implementation of MIC is available at https://github.com/lhoyer/MIC.