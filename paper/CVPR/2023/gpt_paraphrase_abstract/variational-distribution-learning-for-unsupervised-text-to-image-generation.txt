This abstract introduces a novel algorithm for text-to-image generation using deep neural networks. Unlike previous methods that rely on text captions for training, we utilize a pretrained CLIP model. The CLIP model effectively aligns image and text embeddings in a shared space and performs well on zero-shot recognition tasks. Our approach involves optimizing a text-to-image generation model by maximizing the data log-likelihood based on pairs of image-text CLIP embeddings. To enhance alignment between the two domains, we adopt a principled approach using variational inference to estimate an approximate posterior of the hidden text embedding given an image and its CLIP feature. Through experiments, we demonstrate that our proposed framework significantly outperforms existing methods in unsupervised and semi-supervised text-to-image generation scenarios.