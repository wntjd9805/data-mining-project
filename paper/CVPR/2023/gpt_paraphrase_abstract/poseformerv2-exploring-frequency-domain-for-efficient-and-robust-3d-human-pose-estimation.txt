Transformer-based methods have been successful in 2D-to-3D human pose estimation. However, the performance of these methods, such as PoseFormer, is limited by the length of the input joint sequence and the quality of 2D joint detection. Existing methods use self-attention on all frames, which is computationally expensive and not robust to noise from 2D joint detectors. To address these issues, we propose PoseFormerV2, which represents skeleton sequences in the frequency domain to enhance the receptive field and improve robustness to noisy 2D joint detection. With minimal modifications to PoseFormer, our method effectively combines features from both the time and frequency domains, achieving a better balance between speed and accuracy. Extensive experiments on benchmark datasets demonstrate that our approach outperforms PoseFormer and other transformer-based variants. The code is available at https://github.com/QitaoZhao/PoseFormerV2.