In order to create Video Question Answering (VideoQA) systems that can assist humans in daily activities, it is necessary to be able to extract answers from long-form videos that contain diverse and complex events. Currently, multi-modal VQA models perform well on images or short video clips, especially with the use of large-scale multi-modal pre-training. However, when these methods are applied to long-form videos, new challenges emerge. One challenge is that using a dense video sampling strategy requires too much computational power. Another challenge is that methods relying on sparse sampling struggle in scenarios where there are multiple events and levels of visual reasoning required.To address these challenges, we propose a new model called Multi-modal Iterative Spatial-temporal Transformer (MIST) that is designed to better adapt pre-trained models for long-form VideoQA. MIST achieves this by breaking down traditional dense spatial-temporal self-attention into segment and region selection modules. These modules selectively choose frames and image regions that are most relevant to the question being asked. The model also efficiently processes visual concepts at different levels of granularity through an attention module. Additionally, MIST iteratively conducts selection and attention over multiple layers to support reasoning over multiple events.We conducted experiments on four VideoQA datasets (AGQA, NExT-QA, STAR, and Env-QA) and found that MIST outperforms other models, achieving state-of-the-art performance while maintaining efficiency. The code for implementing MIST is available on GitHub at github.com/showlab/mist.In summary, we propose a new model, MIST, that improves the performance of pre-trained models for long-form VideoQA. It accomplishes this by selectively choosing relevant frames and image regions and efficiently processing visual concepts at different levels of granularity. Experimental results demonstrate that MIST achieves state-of-the-art performance and is highly efficient.