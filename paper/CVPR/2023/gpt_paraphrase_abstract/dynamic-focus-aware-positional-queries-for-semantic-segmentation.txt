The recent advancements in semantic segmentation have been driven by DETR-like segmentors, which train a set of queries representing class prototypes or target segments. A new approach called masked attention has been proposed to limit each query's attention to foreground regions predicted by the preceding decoder block, making optimization easier. However, this approach relies on learnable positional queries that encode dataset statistics, resulting in inaccurate localization for individual queries. This paper introduces a new query design called Dynamic Focus-aware Positional Queries (DFPQ) for semantic segmentation. DFPQ dynamically generates positional queries based on cross-attention scores from the preceding decoder block and positional encodings for corresponding image features. This preserves rich localization information for target segments and provides accurate positional priors. Additionally, the paper proposes a method to efficiently handle high-resolution cross-attention by aggregating contextual tokens based on low-resolution cross-attention scores for local relation aggregation. Experimental results on ADE20K and Cityscapes datasets demonstrate that our framework, with two modifications to Mask2former, achieves state-of-the-art performance, surpassing Mask2former by significant margins in single-scale mIoU with different backbone architectures. The source code for our framework is available at https://github.com/ziplab/FASeg.