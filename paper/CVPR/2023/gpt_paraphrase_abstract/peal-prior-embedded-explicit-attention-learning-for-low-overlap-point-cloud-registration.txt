The incorporation of the Transformer model into point cloud feature representation has been successful in low-overlap point cloud registration. However, the use of global correlations in feature space can lead to ambiguity in indoor low-overlap scenarios. To address this, we propose PEAL, a Prior-embedded Explicit Attention Learning model. PEAL divides the points into two parts based on their location in the overlapping or non-overlapping region and explicitly learns one-way attention with the overlapping points. This approach significantly improves feature distinctiveness and achieves state-of-the-art performance in various metrics on benchmark datasets.