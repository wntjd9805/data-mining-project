Graph Neural Networks (GNNs) face challenges in generalizing to out-of-distribution (OOD) scenarios. Existing approaches use graph editions to create augmented environments and learn GNNs that can handle generalization. However, these editions often lead to label shifts, causing inconsistent predictive relationships among augmented environments and hindering generalization. To overcome this issue, we propose a method called LiSA. LiSA generates label-invariant augmentations by extracting predictive patterns from training graphs and constructing multiple label-invariant subgraphs. These subgraphs are then used to build different augmented environments. To enhance diversity among the environments, LiSA incorporates energy-based regularization to increase the distances between their distributions. This results in diverse augmented environments with consistent predictive relationships, enabling effective learning of invariant GNNs. Extensive experiments on node-level and graph-level OOD benchmarks demonstrate that LiSA achieves impressive generalization performance with various GNN backbones. The code for LiSA is available at https://github.com/Samyu0304/LiSA.