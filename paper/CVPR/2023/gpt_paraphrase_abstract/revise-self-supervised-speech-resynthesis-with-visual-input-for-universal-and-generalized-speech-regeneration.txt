This paper proposes a new approach called Generalized Speech Regeneration to improve speech quality using visual input. Unlike previous studies that focus on specific auditory distortions, this approach aims to enhance certain aspects of speech, such as intelligibility, quality, and video synchronization, without necessarily reconstructing the exact clean signal. The problem is framed as audio-visual speech resynthesis, consisting of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). These steps are connected using discrete units derived from a self-supervised speech model, and the self-supervised audio-visual speech model is used to initialize P-AVSR. The proposed model, named ReVISE, is the first high-quality model for video-to-speech synthesis in real-world scenarios. It achieves superior performance on various audio-visual regeneration tasks and has been evaluated on a challenging benchmark dataset, EasyCom, with limited training data. ReVISE effectively reduces noise and improves speech quality. Further details can be found on the project page: https://wnhsu.github.io/ReVISE/.