Existing deep saliency prediction algorithms enhance object recognition by incorporating additional information such as scene context, semantic relationships, gaze direction, and object dissimilarity. However, these models overlook the temporal aspect of gaze shifts during image observation. To address this limitation, we propose a novel saliency prediction model that leverages human temporal attention patterns to generate saliency maps in sequential time intervals. Our approach locally adjusts the saliency predictions by combining the acquired temporal maps. Through experiments on the SALICON benchmark and CodeCharts1k dataset, we demonstrate that our method surpasses state-of-the-art models, including a multi-duration saliency model. We have made our code publicly available on GitHub1.