In recent times, the prevalent approach to achieving high performance in Visual Question Answering (VQA) is by fine-tuning pretrained vision-language models (VLMs). However, as these models become larger, it becomes computationally expensive, inefficient in terms of storage, and susceptible to overfitting when tuning all the parameters for a specific task in low-resource scenarios. Although current methods for parameter-efficient tuning have reduced the number of tunable parameters, there still remains a significant performance gap compared to full finetuning. This paper introduces MixPHM, a redundancy-aware method for parameter-efficient tuning that outperforms full finetuning in low-resource VQA. MixPHM is a lightweight module that consists of multiple PHM-experts implemented in a mixture-of-experts manner. To reduce redundancy in parameters, we reparameterize expert weights in a low-rank subspace and share a portion of the weights both within and across MixPHM. Additionally, we propose Redundancy Regularization based on quantitative analysis of representation redundancy, which helps MixPHM reduce irrelevant redundancy while promoting relevant correlation. Experimental results on VQA v2, GQA, and OK-VQA datasets under different low-resource settings demonstrate that MixPHM surpasses state-of-the-art parameter-efficient methods and consistently outperforms full finetuning.