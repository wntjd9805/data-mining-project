Non-line-of-sight (NLOS) imaging is a technique that uses indirect reflections to capture hidden objects in 3D. However, reconstructing images from NLOS measurements is difficult, especially for complex scenes. To address this challenge, we introduce NLOST, the first transformer-based neural network for NLOS reconstruction. Our approach involves extracting shallow features using physics-based priors and using spatial-temporal self attention encoders to analyze local and global correlations in the 3D NLOS data. By splitting or downsampling the features into different scales, we explore various levels of detail. Additionally, we employ a spatial-temporal cross attention decoder to combine local and global features in the token space of the transformer. This results in deep features with strong representation capabilities. Finally, we fuse the deep and shallow features to reconstruct the 3D volume of hidden scenes. Our method outperforms existing solutions on synthetic and real-world data captured by different NLOS imaging systems, as demonstrated by extensive experimental results.