This paper presents a method called Image-based Joint-Embedding Predictive Architecture (I-JEPA) for self-supervised learning from images. The approach aims to learn semantic image representations without the need for manually created data augmentations. The main idea behind I-JEPA is to predict the representations of different target blocks within the same image using a single context block. To ensure the production of semantic representations, the paper highlights the importance of sampling target blocks with large-scale semantic information and using informative and spatially distributed context blocks. The empirical results show that when combined with Vision Transformers, I-JEPA is highly scalable. The authors trained a ViT-Huge/14 model on ImageNet using 16 A100 GPUs in less than 72 hours, achieving strong performance on various tasks such as linear classification, object counting, and depth prediction.