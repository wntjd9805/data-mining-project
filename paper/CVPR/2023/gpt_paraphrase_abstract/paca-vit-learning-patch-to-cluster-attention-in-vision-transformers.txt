This paper introduces a new approach called Patch-to-Cluster attention (PaCa) in Vision Transformers (ViTs) to address certain limitations. ViTs treat image patches as "visual tokens" and learn patch-to-patch attention. However, the patch embedding-based tokenizer in ViTs has a semantic gap compared to the textual tokenizer. Additionally, the quadratic complexity of patch-to-patch attention makes it difficult to explain the learned ViTs. To overcome these issues, the proposed PaCa-ViT learns patch-to-cluster attention. In this approach, queries start with patches, while keys and values are based on clustering with a predefined number of clusters. The clusters are learned end-to-end, which improves tokenizers and enables joint clustering-for-attention and attention-for-clustering, leading to better and interpretable models. Moreover, the quadratic complexity is reduced to linear complexity.The PaCa module is utilized in designing efficient and interpretable ViT backbones and semantic segmentation head networks. Experimental tests on ImageNet-1k image classification, MS-COCO object detection and instance segmentation, and MIT-ADE20k semantic segmentation demonstrate that the proposed methods outperform the prior art, such as SWin and PVTs, by significant margins in ImageNet-1k and MIT-ADE20k. Furthermore, the PaCa models are more efficient than PVT models in MS-COCO and MIT-ADE20k due to their linear complexity. The clusters learned by PaCa-ViT have semantic meaning.For access to the code and model checkpoints, they are available at https://github.com/iVMCL/PaCaViT.