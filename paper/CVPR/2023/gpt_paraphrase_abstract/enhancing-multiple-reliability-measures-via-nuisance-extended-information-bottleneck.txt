In situations where there is limited training data, the presence of biases in the data acquisition process can lead to predictive signals that are not generalizable. This can cause a model to become fragile and vulnerable to distribution shifts. To address this issue, we propose an adversarial threat model with a mutual information constraint that allows for a wider range of perturbations during training. We extend the standard information bottleneck framework to include the modeling of nuisance information. Our approach utilizes autoencoder-based training and practical encoder designs for both convolutional and Transformer-based architectures. Experimental results demonstrate that our method improves the robustness of learned representations without relying on domain-specific knowledge. We achieve significant improvements in reliability measures, such as a 78.4% to 87.2% increase in AUROC on a challenging OBJECTS benchmark for novelty detection, as well as enhanced corruption, background, and adversarial robustness. The code for our proposed approach is available at https://github.com/jh-jeong/nuisance_ib.