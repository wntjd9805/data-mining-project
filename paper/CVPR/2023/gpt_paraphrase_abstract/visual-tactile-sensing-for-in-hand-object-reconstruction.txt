Tactile sensing is an essential way for humans to perceive the world, working alongside vision to understand local geometry, measure deformation, and determine hand-object contact. The availability of open-source tactile sensors like DIGIT has made visual-tactile learning more accessible for research and reproducibility. In this study, we propose a new framework called VTacO, which uses the DIGIT sensor for visual-tactile in-hand object reconstruction. We also extend this framework to VTacOH for hand-object reconstruction. Since our method can handle both rigid and deformable objects, existing benchmarks are not suitable. To address this, we introduce VT-Sim, a simulation environment that generates hand-object interactions for both types of objects. Using VT-Sim, we create a large-scale training dataset and evaluate our method on it. Through extensive experiments, we demonstrate that our proposed method outperforms previous baseline methods both qualitatively and quantitatively. Additionally, we apply our simulation-trained model directly to real-world test cases, providing qualitative results. The codes, models, simulation environment, and datasets are available at https://sites.google.com/view/vtaco/.