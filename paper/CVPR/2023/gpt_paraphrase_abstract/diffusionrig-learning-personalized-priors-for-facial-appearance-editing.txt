We present a solution to the problem of learning individual-specific facial characteristics from a limited number of portrait photos of the same person. This allows us to modify the facial appearance of a specific person, such as their expression and lighting, while maintaining their identity and fine facial details. Our approach, called DiffusionRig, utilizes a diffusion model that is conditioned on 3D face models estimated from single in-the-wild images. These models are obtained using a readily available estimator. In essence, DiffusionRig learns to transform simplistic renderings of 3D face models into realistic photos of a particular individual. The training of DiffusionRig occurs in two stages: first, it learns generic facial characteristics from a large-scale face dataset, and then it learns person-specific characteristics from a small collection of portrait photos of the person of interest. By learning the mapping from computer-generated imagery to real photos using personalized priors, DiffusionRig can modify the lighting, expression, head pose, and other aspects of a portrait photo, relying solely on coarse 3D models, while still preserving the person's identity and other fine details. Through qualitative and quantitative experiments, we demonstrate that DiffusionRig outperforms existing methods in terms of both identity preservation and photorealism. For additional information, including supplementary material, video, code, and data, please visit our project website at https://diffusionrig.github.io.