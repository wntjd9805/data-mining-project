To address the issue of video semantic segmentation with limited annotations, researchers have explored methods to utilize unlabeled frames by assigning pseudo labels or improving features. This study introduces a novel feature enhancement network that models both short- and long-term temporal correlations. Unlike previous approaches that only consider short-term correspondence, this method incorporates long-term temporal correlation from distant frames to expand the temporal perception field and provide richer contextual information. By modeling adjacent and distant frames together, the risk of overfitting is reduced, resulting in high-quality feature representation for unlabeled frames in the training set and unseen videos in the testing set. The proposed method, called SSLTM (Simultaneously Short- and Long-Term Temporal Modeling), outperforms state-of-the-art techniques by 2% to 3% mIoU on the challenging VSPW dataset when only one frame is annotated per video. Additionally, when combined with a pseudo label based method like MeanTeacher, the final model achieves a mIoU that is only 0.13% less than the performance ceiling of manually annotating all frames.