We present LAVILA, a novel method for learning video-language representations using Large Language Models (LLMs). Our approach involves repurposing pre-trained LLMs to be conditioned on visual input and fine-tuning them to generate automatic video narrations. These generated narrations offer several advantages, such as comprehensive coverage of lengthy videos, improved temporal synchronization between visual information and text, and increased textual diversity.The video-language embedding obtained through contrastive learning using these narrations surpasses the previous state-of-the-art in various first-person and third-person video tasks, both in zero-shot and fine-tuned scenarios. Particularly noteworthy is LAVILA's impressive performance, achieving an absolute improvement of 10.1% in EGTEA classification and 5.9% in Epic-Kitchens-100 multi-instance retrieval benchmarks.Moreover, our experiments demonstrate that LAVILA outperforms models trained on the full set of narrations when trained with only half of the narrations from the Ego4D dataset. Additionally, we observe positive scaling behavior in terms of increasing pre-training data and model size.In summary, our proposed LAVILA approach leverages Large Language Models to learn video-language representations, enhancing the performance of various video tasks and exhibiting promising scalability.