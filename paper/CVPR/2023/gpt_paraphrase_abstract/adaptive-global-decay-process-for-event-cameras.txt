In event-based vision problems, selecting the most relevant events is crucial. Current strategies include constant temporal decay, constant number of events, or flow-based lifetime of events. However, these strategies have limitations. We propose a new decay process for event cameras that adapts to the scene dynamics with minimal latency. Our method uses event activity to encode the global scene dynamics. We evaluate our approach in various vision problems and datasets, consistently outperforming existing methods. We believe our method will have a significant impact on event-based research. Code is available at https://github.com/neuromorphic-paris/event batch.