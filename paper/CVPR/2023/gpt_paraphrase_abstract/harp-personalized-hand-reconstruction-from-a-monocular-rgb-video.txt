We introduce HARP (HAnd Reconstruction and Personalization), a method for creating personalized hand avatars. Using a short video of a human hand, HARP reconstructs a hand avatar with high fidelity in appearance and geometry. Unlike current trends in neural implicit representations, HARP models the hand using a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without neural components. This explicit representation allows for scalability, robustness, and efficiency in creating hand avatars, as demonstrated by our experiments. HARP is optimized using gradient descent from a video captured by a mobile phone, making it suitable for real-time rendering in AR/VR applications. To achieve this, we have designed a shadow-aware differentiable rendering scheme that handles high degree articulations, self-shadowing, and challenging lighting conditions. The scheme also generalizes to unseen poses and views, producing realistic renderings of hand animations. Additionally, the learned HARP representation can enhance the quality of 3D hand pose estimation in challenging viewpoints. Our extensive analysis on appearance reconstruction, novel view and pose synthesis, and 3D hand pose refinement confirms the key advantages of HARP. It is a personalized hand representation that is ready for AR/VR applications, offering superior fidelity and scalability.