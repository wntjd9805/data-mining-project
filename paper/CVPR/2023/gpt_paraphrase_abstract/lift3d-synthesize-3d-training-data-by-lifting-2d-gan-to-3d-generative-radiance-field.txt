This study investigates the use of 3D generative models for creating synthetic training data for 3D vision tasks. The generative models need to meet two main requirements: generating photorealistic data that resembles real-world scenarios, and aligning the 3D attributes with given sampling labels. However, current 3D GANs based on NeRF (Neural Radiance Fields) fail to meet these requirements due to their generation pipeline and the lack of explicit 3D supervision. To address this, we propose Lift3D, a novel framework that generates 3D data by inverting a 2D-to-3D generation process. Lift3D offers several advantages over previous methods: (1) Unlike other 3D GANs, Lift3D can adapt to different camera resolutions and produce higher resolution and more realistic output. (2) By leveraging a disentangled 2D GAN to generate 3D objects, Lift3D provides explicit 3D information and accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting datasets for autonomous driving. Experimental results demonstrate that our data generation framework significantly improves the performance of 3D object detectors. The code for our framework can be found at len-li.github.io/lift3d-web.