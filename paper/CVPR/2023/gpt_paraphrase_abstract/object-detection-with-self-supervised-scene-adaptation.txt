This paper suggests a new approach to enhance the performance of a trained object detector in scenes with fixed camera perspectives. The proposed method involves adapting the trained detector using pseudo-ground truth labels generated by the detector itself and an object tracker in a cross-teaching manner. When the camera perspective remains unchanged, our method takes advantage of background equivariance by employing artifact-free object mixup for data augmentation and accurate background extraction as an additional input modality. We also introduce a comprehensive and diverse dataset for the development and evaluation of scene-adaptive object detection. Experiments conducted on this dataset demonstrate that our method significantly improves the average precision of the original detector, surpassing the performance of previous state-of-the-art self-supervised domain adaptive object detection methods. The dataset and code associated with our research can be accessed at https://github.com/cvlab-stonybrook/scenes100.