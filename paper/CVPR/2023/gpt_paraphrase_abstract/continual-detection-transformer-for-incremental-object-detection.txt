This paper introduces a new method called ContinuaL DEtection TRansformer (CL-DETR) to address the issue of catastrophic forgetting in incremental object detection (IOD). IOD involves training an object detector in multiple phases, each with annotations for new object categories. Existing techniques like knowledge distillation (KD) and exemplar replay (ER) are not effective when directly applied to transformer-based object detectors like Deformable DETR and UP-DETR. CL-DETR solves this problem by incorporating KD and ER in a transformer-based IOD framework. The paper proposes a Detector Knowledge Distillation (DKD) loss, which focuses on the most informative and reliable predictions from previous versions of the model, while ignoring redundant background predictions. This loss function ensures compatibility with the available ground-truth labels. Additionally, the paper improves the exemplar replay (ER) technique by introducing a calibration strategy that preserves the label distribution of the training set, leading to better matching of training and testing statistics.The effectiveness of CL-DETR is validated through extensive experiments on the COCO 2017 dataset. The results demonstrate that CL-DETR achieves state-of-the-art performance in the IOD setting.