This paper introduces DiffSwap, a framework for high-quality and adjustable face swapping. Unlike previous approaches that rely on specific network architectures and loss functions to combine information from source and target faces, we propose a new approach that treats face swapping as a conditional inpainting task. This is achieved by utilizing a diffusion model guided by desired face attributes such as identity and landmarks. One challenge of applying diffusion models to face swapping is the time-consuming multi-step sampling required to generate images during training. To address this, we propose a mid-point estimation method that efficiently produces a reasonable diffusion result with only 2 steps, allowing us to incorporate identity constraints for improved face swapping quality. Our framework offers several advantages over previous methods: 1) Controllability, as our method allows for full customization and control of the mask and conditions in the latent space. 2) High-fidelity, as the conditional inpainting formulation effectively leverages the generative capability of diffusion models and minimizes artifacts in the background of target images. 3) Shape preservation, as the controllability of our method enables the use of 3D-aware landmarks as conditions during generation, ensuring the preservation of the source face's shape. Extensive experiments on FF++ and FFHQ datasets demonstrate that our approach achieves state-of-the-art face swapping results in terms of both quality and quantity.