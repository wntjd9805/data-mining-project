In this study, we propose a novel approach to visual representation learning by utilizing language similarity to sample semantically similar image pairs for contrastive learning. Unlike traditional methods that rely on hand-crafted augmentations or learned clusters, our approach uses language similarity to select view pairs. Additionally, our method differs from image-text contrastive learning as we employ pre-trained language models to guide the learning process instead of minimizing a cross-modal loss. Through a series of experiments, we demonstrate that our language-guided learning approach produces superior features compared to image-based and image-text representation learning methods.