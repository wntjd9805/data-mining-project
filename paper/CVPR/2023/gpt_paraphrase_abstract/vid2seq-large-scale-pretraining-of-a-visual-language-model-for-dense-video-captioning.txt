This study presents Vid2Seq, a single-stage dense event captioning model that is pretrained on narrated videos available at a large scale. The architecture of Vid2Seq enhances a language model by incorporating special time tokens, enabling it to predict event boundaries and textual descriptions in a unified output sequence. Since annotated datasets lack the necessary training data for such a model, this research demonstrates the ability to utilize unlabeled narrated videos by converting transcribed speech sentence boundaries into pseudo event boundaries and using the transcribed speech sentences as pseudo event captions. By pretraining the Vid2Seq model on the YT-Temporal-1B dataset, it achieves superior performance compared to existing methods on various dense video captioning benchmarks, including YouCook2, ViTT, and ActivityNet Captions. Furthermore, Vid2Seq exhibits strong generalization to video paragraph captioning, video clip captioning, and few-shot settings. The publicly available code can be found at [1].