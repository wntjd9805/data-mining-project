We propose a new efficient image restoration Transformer that addresses the limitation of Vision Transformers in capturing global dependencies among pixels. Our method employs a coarse-to-fine approach by first capturing superpixel-wise global dependencies and then transferring them to each pixel. This is achieved through two neural blocks, namely condensed attention (CA) and dual adaptive (DA) blocks. The CA block efficiently captures global dependencies at the superpixel level using feature aggregation, attention computation, and feature recovery. The DA block adaptsively encapsulates globality from superpixels into pixels using a novel dual-way structure. Our method achieves comparable performance to SwinIR while requiring only approximately 6% FLOPs.