In recent years, there has been significant advancement in DeepFake technology, which has led to the development of many DeepFake detection techniques. These techniques typically rely on identifying statistical differences between real and DeepFake-generated images in both spatial and frequency domains. However, in this study, we propose a new approach called StatAttack, which aims to minimize these statistical differences in order to evade current DeepFake detectors. StatAttack consists of two main parts: firstly, we introduce statistical-sensitive natural degradations such as exposure, blur, and noise to the fake images in an adversarial manner. Secondly, we observe that the statistical differences between natural and DeepFake images are related to the distribution shift between the two types of images. To address this, we use a distribution-aware loss to guide the optimization of different degradations. As a result, the feature distributions of the generated adversarial examples closely resemble those of natural images. Additionally, we enhance StatAttack by introducing MStatAttack, which incorporates multi-layer degradations and jointly tunes the combination weights using a loss function. Our proposed attack method is evaluated on four spatial-based detectors and two frequency-based detectors using four datasets. The experimental results demonstrate the effectiveness of our approach in both white-box and black-box settings. Figure 1 illustrates the principle of our method, showing how it maps fake samples to the common regions of different detectors, enabling it to fool both detectors.