This study investigates the use of cross-modal training with 2D-3D paired datasets to improve 2D scene understanding. These datasets consist of multi-view images and 3D scene scans, which introduce geometric and view-invariance priors into 2D features. However, the scalability and potential for further improvements are hindered by the need for large-scale scene datasets. To address this, the authors propose an alternative learning method that utilizes lightweight and publicly available 3D data in the form of CAD models. The authors create a geometric-aware alignment in a 3D space, where the similarity between CAD models is reflected by the Chamfer distance. They then transfer the acquired geometric-aware properties into 2D features, resulting in improved performance on downstream tasks compared to existing RGB-CAD approaches. Importantly, this technique is not limited to paired RGB-CAD datasets. Instead, the authors exclusively train on pseudo pairs generated from CAD-based reconstruction methods. This approach enhances the performance of state-of-the-art 2D pre-trained models that utilize ResNet-50 or ViT-B backbones on various 2D understanding tasks. The authors demonstrate comparable results to state-of-the-art methods trained on scene scans across four tasks in NYUv2, SUNRGB-D, indoor ADE20k, and indoor/outdoor COCO datasets, despite using lightweight CAD models or pseudo data. Interested readers are encouraged to visit the authors' webpage for more information.