In recent years, significant progress has been made in video action recognition. Various models, including convolutional neural networks (CNN) and transformer-based approaches, have achieved impressive performance on existing benchmarks. In this study, we conduct a comprehensive analysis of the robustness of these models for video action recognition, specifically focusing on real-world distribution shift perturbations rather than adversarial perturbations. To perform this analysis, we introduce four benchmark datasets: HMDB51-P, UCF101-P, Kinetics400-P, and SSv2-P. We evaluate the robustness of six state-of-the-art action recognition models against 90 different perturbations. Our findings reveal several interesting observations. Firstly, transformer-based models consistently exhibit greater robustness compared to CNN-based models. Secondly, pretraining enhances the robustness of transformer-based models more significantly than CNN-based models. Thirdly, all studied models demonstrate robustness to temporal perturbations across all datasets except for SSv2, suggesting that the importance of temporal information in action recognition varies depending on the dataset and activities. Additionally, we investigate the impact of augmentations on model robustness and introduce a real-world dataset, UCF101-DS, which contains realistic distribution shifts to further validate our findings. We believe that this study serves as a benchmark for future research in the field of robust video action recognition.