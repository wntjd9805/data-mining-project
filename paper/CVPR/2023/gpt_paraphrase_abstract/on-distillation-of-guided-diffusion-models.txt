Classiﬁer-free guided diffusion models have proven to be highly effective for high-resolution image generation. However, these models are computationally expensive during inference due to the need to evaluate two diffusion models multiple times. To address this issue, we propose a method to distill classiﬁer-free guided diffusion models into faster sampling models. We accomplish this by training a single model to match the output of the combined conditional and unconditional models, and then progressively distilling that model to a diffusion model that requires fewer sampling steps. Using our approach, we are able to generate visually comparable images to the original model with as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10 datasets for standard diffusion models trained on the pixel-space. This achieves FID/IS scores comparable to the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space, such as Stable Diffusion, our approach generates high-ﬁdelity images using only 1 to 4 denoising steps, resulting in at least a 10-fold acceleration in inference compared to existing methods on ImageNet 256x256 and LAION datasets. Additionally, we demonstrate the effectiveness of our approach in text-guided image editing and inpainting tasks. Our distilled model generates high-quality results using as few as 2-4 denoising steps. Overall, our approach significantly reduces the computational cost of classiﬁer-free guided diffusion models without compromising image quality or performance in various image generation tasks.