Current state-of-the-art approaches for few-shot action recognition achieve promising results by matching frames using learned visual features. However, they have two limitations: first, the matching between local frames is often inaccurate due to the lack of guidance for long-range temporal perception; second, explicit motion learning is usually overlooked, leading to incomplete information. To address these issues, we propose a method called Motion-augmented Long-short Contrastive Learning (MoLo). MoLo consists of two key components: a long-short contrastive objective and a motion autodecoder. The long-short contrastive objective aims to give local frame features a better understanding of long-form temporal context by maximizing their agreement with the global token of videos from the same class. The motion autodecoder is a lightweight architecture that reconstructs pixel motions from differential features, explicitly incorporating motion dynamics into the network. By doing so, MoLo can simultaneously learn long-range temporal context and motion cues for comprehensive few-shot matching. We evaluate the effectiveness of MoLo on five standard benchmarks and the results demonstrate that it outperforms recent advanced methods. The source code for MoLo is available at https://github.com/alibaba-mmai-research/MoLo.