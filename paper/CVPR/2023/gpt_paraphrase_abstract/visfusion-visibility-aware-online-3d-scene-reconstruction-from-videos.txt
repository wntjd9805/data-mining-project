We present VisFusion, an innovative method for reconstructing 3D scenes in real-time using monocular videos. Our approach focuses on capturing volumetric features to accurately represent the scene. Unlike existing methods that aggregate features without considering visibility, we enhance feature fusion by inferring visibility using a similarity matrix derived from projected features in each image pair. Our pipeline follows a coarse-to-fine approach, incorporating a volume sparsification process. In contrast to previous works that globally sparsify voxels with a fixed occupancy threshold, we locally sparsify voxels along each visual ray to maintain finer details. The sparse local volume is then fused with a global volume for online reconstruction. To improve the accuracy of our method, we propose predicting the truncated signed distance function (TSDF) in a coarse-to-fine manner by learning its residuals across multiple scales. Experimental results on benchmark datasets demonstrate that our approach outperforms existing methods in capturing scene details. The code for our method is publicly available at https://github.com/huiyu-gao/VisFusion.