This study explores the combination of deep reinforcement learning (DRL) and representation learning to improve agent performance. It is theorized that the representation of the 𝑄-network and its target 𝑄-network should have a distinguishable property, where the similarity between the value functions of adjacent time steps is bounded. However, empirical experiments reveal that DRL agents may violate this property, leading to sub-optimal policies. To address this issue, the authors propose a simple regularization technique called Policy Evaluation with Easy Regularization on Representation (PEER), which enforces a distinguishable representation property through explicit regularization on internal representations. The convergence rate of PEER is also guaranteed. Implementing PEER only requires adding one line of code. Experimental results demonstrate that incorporating PEER into DRL significantly improves performance and sample efficiency. PEER achieves state-of-the-art results on various environments in PyBullet, DM-Control, and Atari games. This work is the first to study the inherent representation property of 𝑄-network and its target. The code for PEER is available at the provided link.