The use of generative models in computer vision has been extensively researched. Recently, diffusion models have gained significant attention for their ability to generate high-quality images. One important characteristic of image generative models is their capacity to disentangle different attributes, allowing for style modification without altering the semantic content. Previous studies have demonstrated that generative adversarial networks possess inherent disentanglement capabilities, enabling them to perform image editing without re-training the network. This study aims to investigate whether diffusion models also possess this innate capability. The findings reveal that stable diffusion models can be modified towards a target style by partially changing the input text embedding while keeping the introduced random noises constant. This modification does not affect the semantic content of the generated images. Building upon this discovery, the authors propose a lightweight image editing algorithm that optimizes the mixing weights of two text embeddings for style matching and content preservation. This process only requires optimization of approximately 50 parameters and does not involve fine-tuning the diffusion model itself. Experimental results demonstrate that the proposed method outperforms diffusion-model-based image-editing algorithms that require fine-tuning, as it can modify a wide range of attributes. Furthermore, the optimized weights generalize well to different images. The code for this algorithm is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.