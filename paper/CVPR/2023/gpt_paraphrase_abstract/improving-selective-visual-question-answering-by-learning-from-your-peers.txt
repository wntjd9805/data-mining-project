Despite advancements in Visual Question Answering (VQA), there has been limited exploration of models' ability to assess their own accuracy. Recent research has revealed that VQA models often struggle to abstain from answering when they are incorrect. The option to abstain, known as Selective Prediction, is particularly important when deploying VQA systems to users who rely on the system's output, such as VQA assistants for visually impaired users. In such cases, abstention becomes crucial as users may provide inputs that are out-of-distribution or adversarial, increasing the likelihood of incorrect answers. This study investigates Selective VQA in both in-distribution (ID) and out-of-distribution (OOD) scenarios, where models are presented with a combination of ID and OOD data. The objective is to maximize the number of questions answered while minimizing the risk of error on those questions. To achieve this, a simple yet effective approach called Learning from Your Peers (LYP) is proposed. LYP trains multimodal selection functions for making abstention decisions by utilizing predictions from models trained on different subsets of the training data as targets. Importantly, this approach does not require additional manual labels or held-out data and provides a signal for identifying examples that are easy or difficult to generalize to.Extensive evaluations demonstrate the benefits of LYP across various models with different architectures and scales. For in-distribution scenarios, the selective prediction metric coverage at a 1% risk of error (C@1%) reaches 32.92%, doubling the previous best coverage of 15.79% for this task. In mixed in-distribution/out-of-distribution scenarios, using models' softmax confidences for abstention decisions performs poorly, answering less than 5% of questions at a 1% risk of error even with only 10% OOD examples. However, incorporating a learned selection function with LYP increases the coverage to 25.38% C@1%.In summary, this study addresses the under-explored aspect of models' ability to assess their own correctness in VQA. The proposed LYP approach effectively trains multimodal selection functions without the need for additional data or manual labels. The results demonstrate significant improvements in selective prediction coverage for both in-distribution and mixed in-distribution/out-of-distribution scenarios. The findings have implications for developing more reliable and trustworthy VQA systems, particularly in situations where users heavily rely on the system's output.