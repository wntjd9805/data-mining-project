Multimodal learning has shown success in cross-modal retrieval tasks, but it heavily relies on accurate correspondence between different types of media. However, obtaining such ideal data is expensive and time-consuming. As a result, most commonly used datasets are collected from the internet and inevitably contain mismatched pairs. Training on these noisy datasets leads to performance degradation as cross-modal retrieval methods incorrectly enforce similarity between mismatched data. To address this issue, we propose a MetaSimilarity Correction Network (MSCN) that provides reliable similarity scores. We treat binary classification as a meta-process, where the MSCN learns discrimination from positive and negative meta-data. Additionally, we develop a data purification strategy that uses meta-data as prior knowledge to remove noisy samples and reduce the impact of noise. We conduct extensive experiments to demonstrate the effectiveness of our method in both synthetic and real-world noisy datasets, including Flickr30K, MS-COCO, and Conceptual Captions. Our code is publicly available.