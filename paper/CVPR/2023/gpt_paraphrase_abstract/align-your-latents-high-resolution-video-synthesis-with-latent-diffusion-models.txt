Latent Diffusion Models (LDMs) offer a solution for generating high-quality images while reducing computational requirements. This study explores the application of LDMs to the generation of high-resolution videos, which is a resource-intensive task. The authors first pre-train an LDM on images and then extend it to generate videos by introducing a temporal dimension to the latent space diffusion model and fine-tuning it on encoded image sequences. They also align diffusion model upsamplers to ensure temporal consistency in video super resolution. The study focuses on two real-world applications: simulating driving data and creating content using text-to-video conversion. The Video LDM achieves state-of-the-art performance in modeling real driving videos of high resolution. Additionally, the authors demonstrate that their approach can leverage pre-trained image LDMs by training a temporal alignment model, thereby transforming a text-to-image LDM into an efficient text-to-video model. The temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. The authors showcase the first results for personalized text-to-video generation, offering promising prospects for future content creation. The project page provides further details.