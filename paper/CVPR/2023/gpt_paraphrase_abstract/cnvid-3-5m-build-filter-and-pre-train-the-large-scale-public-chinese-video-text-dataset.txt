In recent years, there has been significant progress in video-text pre-training due to the availability of well-designed large-scale datasets. However, most of these datasets are only available in English, limiting research and applications in Chinese video-text pre-training. While some methods have explored Chinese video-text pre-training, they have relied on private datasets that are not publicly accessible. This lack of large-scale public datasets and benchmarks in Chinese poses a challenge for further advancements in this field.To address this issue, we introduce CNVid-3.5M, a large-scale public dataset consisting of over 3.5 million Chinese video-text pairs. Our contributions can be summarized through three actions: "Build," "Filter," and "Pre-train." Firstly, we collect more than 4.5 million videos from Chinese websites to create a public Chinese video-text dataset. Secondly, we propose a novel filtering method to enhance the data quality, resulting in the CNVid-3.5M dataset by eliminating 1 million weakly-paired videos. Lastly, we benchmark CNVid-3.5M using three popular pixel-level pre-training architectures. Additionally, we propose the Hard Sample Curriculum Learning strategy to enhance the pre-training performance.To the best of our knowledge, CNVid-3.5M is currently the largest publicly available video-text dataset in Chinese. Furthermore, we provide the first set of pixel-level benchmarks for Chinese video-text pre-training. The dataset, codebase, and pre-trained models can be accessed at https://github.com/CNVid/CNVid-3.5M.