This study addresses the challenge of balancing the complexity and accuracy of multi-modal and single-modal 3D object detection methods for autonomous driving. To enhance the performance of single-modal detectors, the authors propose a universal cross-modality knowledge distillation framework called UniDistill. During training, UniDistill maps the features of both the teacher and student detectors to the Bird's-Eye-View (BEV) representation, which is suitable for various modalities. Three distillation losses are then computed to align foreground features and facilitate knowledge transfer from teacher to student without increasing inference cost. UniDistill supports different distillation paths, such as LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR, and fusion-to-camera, by leveraging the similar detection paradigm in BEV. Additionally, the three distillation losses mitigate the impact of misaligned background information and account for object size variations, leading to improved distillation effectiveness. The authors validate the performance of UniDistill through extensive experiments on the nuScenes dataset, demonstrating a significant increase in mAP and NDS (Normalized Detection Score) for student detectors ranging from 2.0% to 3.2%.