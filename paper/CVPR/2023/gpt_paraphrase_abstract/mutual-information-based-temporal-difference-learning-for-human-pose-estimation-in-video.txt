Temporal modeling plays a crucial role in estimating human pose across multiple frames. Existing methods often rely on full-optical flow or deformable convolution to predict motion fields, but this approach can introduce irrelevant cues and yield suboptimal results in complex spatio-temporal interactions. Alternatively, the temporal difference, which encodes representative motion information, has not been fully utilized in pose estimation. In this study, we propose a novel framework for multi-frame human pose estimation that leverages temporal differences to model dynamic contexts and utilizes mutual information to extract useful motion information. Our framework includes a multi-stage Temporal Difference Encoder that incrementally learns from feature difference sequences to derive informative motion representations. We also introduce a Representation Disentanglement module that defines and minimizes the mutual information between useful and noisy constituents of raw motion features, allowing us to capture discriminative motion signals relevant to the task. Our approach achieves impressive results, ranking first in the Crowd Pose Estimation in Complex Events Challenge on the HiEve benchmark dataset, and attaining state-of-the-art performance on three other benchmarks: PoseTrack2017, PoseTrack2018, and PoseTrack21.