This paper introduces human sketches to the field of Explainable Artificial Intelligence (XAI) for the first time. The authors argue that sketches, as a "human-centered" form of data, provide a natural interface for studying explainability. They focus on developing designs that specifically address the explainability of sketches. They start by recognizing strokes as a unique element that allows for flexible object construction and manipulation, which is not possible with photos. They then design a simple sketch encoder that is friendly to explainability, taking into account the intrinsic properties of strokes such as shape, location, and order. The authors define a new XAI task for sketches called stroke location inversion (SLI), which aims to determine how well a network can recover stroke locations in an unseen sketch. They provide qualitative results in the form of snapshots and GIFs to illustrate the SLI process. Interestingly, their sketch encoder also achieves the highest accuracy in sketch recognition to date with the fewest number of parameters. The code for their work is available at https://sketchxai.github.io.