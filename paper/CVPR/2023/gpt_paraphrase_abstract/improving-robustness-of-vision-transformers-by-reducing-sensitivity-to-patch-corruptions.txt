Despite their success, vision transformers still remain susceptible to image corruptions like noise or blur. The vulnerability primarily arises from the unstable self-attention mechanism, which relies on patch-based inputs and often becomes excessively sensitive to corruptions across patches. For instance, occluding a small percentage of patches with random noise (e.g., 10%) leads to significant drops in accuracy and disrupts intermediate attention layers. To tackle this issue, we propose a novel training approach called reducing sensitivity to patch corruptions (RSPC) that enhances the robustness of transformers. Our method involves identifying and occluding/corrupting the most vulnerable patches, and then mitigating sensitivity to these patches by aligning the intermediate features between clean and corrupted examples. Notably, the creation of patch corruptions is learned adversarially to the subsequent feature alignment process, which sets our approach apart from existing methods. Experimental results demonstrate that our RSPC significantly enhances the stability of attention layers and consistently improves robustness across various benchmark datasets, including CIFAR-10/100-C, ImageNet-A, ImageNet-C, and ImageNet-P.