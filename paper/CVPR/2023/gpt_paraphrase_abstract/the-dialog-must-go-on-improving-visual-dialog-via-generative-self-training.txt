This paper introduces a novel approach called Generative Self-Training (GST) for visually-grounded dialog, specifically the task of Visual dialog (VisDial). VisDial involves answering a series of questions based on an image, using the dialog history as context. Previous methods have primarily trained dialog agents using supervised learning on VisDial data or by leveraging pre-training on related vision-and-language datasets. GST aims to enhance the training process by incorporating unlabeled images from the web. It starts by retrieving relevant images within the same domain using out-of-distribution detection. It then generates synthetic dialogs related to these images using multimodal conditional text generation. The dialog agent is subsequently trained using both the original VisDial data and the synthetic dialogs generated by GST. This approach allows for a significant increase in the amount of training data, scaling up from 1.2 million to 12.9 million QA data instances.To ensure the quality of the synthetic dialogs during training, the paper proposes two additional techniques. Firstly, perplexity-based data selection is used to filter out low-quality synthetic dialogs. Secondly, multimodal consistency regularization is employed to maintain coherence between the visual and textual components of the dialogs.The performance of GST is evaluated on the Vis-Dial v1.0 and v0.9 datasets, and it achieves state-of-the-art results on both. Additionally, GST demonstrates robustness against visual and textual adversarial attacks. Furthermore, GST proves particularly effective in scenarios where limited training data is available.The code for implementing GST is made publicly available at https://github.com/gicheonkang/gst-visdial.