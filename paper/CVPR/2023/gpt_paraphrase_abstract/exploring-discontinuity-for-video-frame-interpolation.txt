This study focuses on video frame interpolation (VFI), which involves generating intermediate frames between two consecutive frames. Previous research has mainly focused on refining the warped frames using frame warping operations, but these methods are limited to natural videos with continuous motions. However, many practical videos contain unnatural objects with discontinuous motions, such as logos, user interfaces, and subtitles. To address this issue, the authors propose three techniques to enhance the robustness of deep learning-based VFI architectures. First, they introduce a novel data augmentation strategy called figure-text mixing (FTM), which allows the models to learn discontinuous motions during the training stage without requiring any additional datasets. Second, they propose a module that predicts a map called the discontinuity map (D-map), which distinguishes between areas of continuous and discontinuous motions. Lastly, they propose loss functions to provide supervision for the discontinuous motion areas, which can be used in conjunction with FTM and D-map. To evaluate their approach, the authors create a special test benchmark called the Graphical Discontinuous Motion (GDM) dataset, which includes mobile games and chatting videos. When applied to state-of-the-art VFI networks, their method significantly improves the interpolation quality not only on the GDM dataset, but also on existing benchmarks that only contain continuous motions, such as Vimeo90K, UCF101, and DAVIS.