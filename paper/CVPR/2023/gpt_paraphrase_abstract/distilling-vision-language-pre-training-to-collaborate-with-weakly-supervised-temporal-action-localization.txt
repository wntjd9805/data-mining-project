Weakly-supervised temporal action localization (WTAL) aims to detect and classify action instances using only category labels. Most existing methods rely on Classification-Based Pre-training (CBP) to generate video features for action localization. However, the discrepancy between the optimization objectives of classification and localization leads to incomplete temporal localization results. To address this issue without additional annotations, this study proposes to distill action knowledge from Vision-Language Pre-training (VLP). Surprisingly, the localization results obtained from vanilla VLP exhibit an over-complete issue, which complements the results obtained from CBP. To leverage this complementarity, a novel distillation-collaboration framework is proposed, consisting of two branches for CBP and VLP, respectively. The framework is optimized using a dual-branch alternate training strategy. In the B step, confident background pseudo-labels are distilled from the CBP branch, while in the F step, confident foreground pseudo-labels are distilled from the VLP branch. This effectively fuses the complementarity of the dual branches to form a strong alliance. Extensive experiments and ablation studies conducted on THUMOS14 and ActivityNet1.2 datasets demonstrate that our method outperforms state-of-the-art approaches.