We have developed a new approach for eliminating injected backdoors in deep learning models. Our method involves creating a new model with the same structure as the trojaned model and training it from scratch using a small subset of samples. The goal is to minimize a cloning loss that measures the differences in neuron activations between the two models. The set of important neurons varies for each input based on their activation magnitude and impact on the classification outcome. Theoretically, our method is proven to better recover the benign functions of the backdoor model and effectively remove backdoors compared to fine-tuning. Our experiments demonstrate that our technique successfully eliminates nine different types of backdoors with minimal loss in benign accuracy, surpassing existing state-of-the-art techniques based on fine-tuning, knowledge distillation, and neuron pruning.