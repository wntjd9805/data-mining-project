We propose a Memory-friendly Scalable Super-Resolution (SR) framework (MSSR) to address the need for scalable deep SR models that can be customized to the computational resources of the platform. Existing dynamic scalable SR methods are not memory-friendly as they require multiple fixed-size models. Inspired by the Lottery Tickets Hypothesis (LTH), we explore the existence of unstructured scalable SR models, or "winning tickets," which are sub-networks of extreme sparsity. MSSR consists of forward and backward stages. In the forward stage, we use LTH to progressively shrink the SR model and prune-out masks to form nested sets. Stochastic self-distillation (SSD) is used to boost sub-network performance. In the backward stage, the smaller SR model can be expanded by recovering and fine-tuning the pruned parameters. Extensive experiments demonstrate the effectiveness of MSSR, with the smallest-scale sub-network achieving 94% sparsity and outperforming lightweight SR methods. The scalable SR network flowchart shows the adjustable nature of the model to memory resource allocation.