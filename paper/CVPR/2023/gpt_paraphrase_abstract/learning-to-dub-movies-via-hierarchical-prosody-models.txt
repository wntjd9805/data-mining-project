The movie dubbing task, also known as visual voice clone (V2C), involves generating speeches that match the speaker's emotion in a video by using a desired speaker voice as a reference. This task is more challenging than traditional text-to-speech tasks because it requires the generated speech to accurately match the varying emotions and speaking speed in the video. In this study, we propose a new movie dubbing architecture that addresses these challenges through hierarchical prosody modeling. We connect visual information to speech prosody by aligning lip movements to speech duration, conveying facial expressions to speech energy and pitch using an attention mechanism based on valence and arousal representations inspired by psychology findings. Additionally, we incorporate an emotion booster to capture the atmosphere from global video scenes. All of these embeddings are combined to generate a mel-spectrogram, which is then converted into speech waves using an existing vocoder. Our experimental results on the V2C and Chem benchmark datasets demonstrate the favorable performance of our proposed method. The code and trained models can be accessed at https://github.com/GalaxyCong/HPMDubbing.