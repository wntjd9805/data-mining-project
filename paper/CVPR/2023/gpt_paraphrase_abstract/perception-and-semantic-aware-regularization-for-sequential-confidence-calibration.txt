Deep sequence recognition (DSR) models are gaining popularity for their effectiveness in various applications. However, most DSR models only consider the target sequences in their training, which can lead to overconfidence in their predictions. Label smoothing is commonly used to address this issue by redistributing a small value from each token to other tokens. However, this approach neglects the correlations between tokens and sequences, which could provide more valuable information for regularization and improve performance.In this study, we propose a new framework called Perception and Semantic aware Sequence Regularization (PSSR) to leverage the correlations between tokens and sequences for more effective regularization. We identify tokens and sequences that have high perception and semantic correlations with the target sequences, as these contain more relevant information. To achieve this, we employ a semantic context-free recognition and a language model to find similar sequences based on perception and semantic similarity, respectively.Additionally, we recognize that the level of overconfidence varies across samples based on their difficulty. To address this, we introduce an adaptive calibration intensity module that computes a difficulty score for each sample, allowing for finer-grained regularization.We conducted extensive experiments on canonical sequence recognition tasks, including scene text and speech recognition, and our method outperformed existing approaches, setting new state-of-the-art results. The code for our method is available at https://github.com/husterpzh/PSSR.