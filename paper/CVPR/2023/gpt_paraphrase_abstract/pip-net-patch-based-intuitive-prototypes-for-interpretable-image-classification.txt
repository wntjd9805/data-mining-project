Prototypical patch-based methods aim to explain their reasoning to humans by recognizing different components in an image. However, current prototype-based methods often learn prototypes that do not align with human visual perception. This means that the same prototype can represent different concepts in the real world, making interpretation difficult. To address this issue, we propose PIP-Net (Patch-based Intuitive Prototypes Network), an interpretable image classification model that learns prototypical parts in a self-supervised manner, which better correlate with human vision.PIP-Net can be understood as a sparse scoring sheet, where the presence of a prototypical part in an image provides evidence for a specific class. The model is also capable of abstaining from making a decision when encountering out-of-distribution data by indicating that it has not seen it before. Importantly, PIP-Net only requires image-level labels and does not rely on any part annotations.One of the key advantages of PIP-Net is its global interpretability. The set of learned prototypes provides a comprehensive understanding of the model's reasoning process. Additionally, PIP-Net can also provide a smaller local explanation by identifying the relevant prototypes within a single image. We demonstrate that our prototypes align well with ground-truth object parts, which indicates that PIP-Net bridges the "semantic gap" between the latent space and pixel space.Overall, our PIP-Net with interpretable prototypes allows users to interpret the decision-making process in an intuitive, faithful, and semantically meaningful way. Interested parties can find the code for PIP-Net at the following link: https://github.com/M-Nauta/PIPNet.