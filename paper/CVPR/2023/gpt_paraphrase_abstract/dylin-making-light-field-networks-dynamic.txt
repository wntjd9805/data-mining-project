Light Field Networks, which represent radiance fields as oriented rays, are significantly faster and more accurate than coordinate network counterparts in capturing 3D structures from 2D observations. However, they are limited to static scenes. To address this limitation, we propose the Dynamic Light Field Network (DyLiN) method, capable of handling non-rigid deformations and topological changes. DyLiN learns a deformation field from input rays to canonical rays and handles discontinuities in a higher-dimensional space. Additionally, we introduce CoDyLiN, which enhances DyLiN with controllable attribute inputs. Both models are trained using knowledge distillation from pretrained dynamic radiance fields. We evaluated DyLiN using synthetic and real-world datasets containing various non-rigid deformations. DyLiN outperformed existing methods both qualitatively and quantitatively in terms of visual fidelity, while being 25-71 times faster computationally. We also tested CoDyLiN on attribute annotated data, where it surpassed its teacher model. More information about the project can be found at https://dylin2023.github.io.