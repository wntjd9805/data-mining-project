Scene graphs provide a structured representation of a scene by encoding objects and their spatial relationships. Current approaches generate scene graphs by labeling all possible edges between objects, which is computationally expensive. This study presents a transformer-based approach that generates scene graphs using two components. Firstly, a possible scene graph structure is sampled from detected objects and their visual features. Then, predicate classification is performed on the sampled edges to generate the final scene graph. This approach efficiently generates scene graphs with minimal inference overhead. Experiments on the Visual Genome dataset demonstrate the effectiveness of the proposed approach, achieving an average mean recall of 20.7% across different settings for scene graph generation. This outperforms state-of-the-art approaches while offering competitive performance to unbiased approaches.