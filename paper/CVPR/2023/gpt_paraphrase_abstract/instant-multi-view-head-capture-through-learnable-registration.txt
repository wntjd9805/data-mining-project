Current methods for capturing datasets of 3D heads in dense semantic correspondence are slow and involve two separate steps: multi-view stereo (MVS) reconstruction and non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads), which directly infers 3D heads in dense correspondence from calibrated multi-view images. In addition, we propose jointly registering a 3D head dataset while training TEMPEH, using a geometric loss as a regularizer. Our multi-view head inference utilizes a volumetric feature representation that combines features from each view based on camera calibration information. To handle partial occlusions and a large capture volume, we employ view- and surface-aware feature fusion and a spatial transformer-based head localization module. During training, we use raw MVS scans as supervision, but once trained, TEMPEH can predict 3D heads in dense correspondence without the need for scans. The prediction time per head is about 0.3 seconds, with a median reconstruction error of 0.26 mm, which is 64% lower than the current state-of-the-art. This allows for efficient capture of large datasets with multiple individuals and diverse facial motions. Our code, model, and data are publicly available at https://tempeh.is.tue.mpg.de.