Recent advancements in denoising diffusion generative models have led to impressive results in generating realistic and diverse data. However, the generation process of these models is slow due to lengthy iterative noise estimations that rely on complex neural networks. This hinders the widespread deployment of diffusion models, particularly on edge devices. Previous approaches have attempted to speed up the generation process by finding shorter sampling trajectories, but they fail to consider the computational cost of noise estimation with a heavy network in each iteration.   In this study, we propose a method to accelerate generation by compressing the noise estimation network. Since retraining diffusion models is challenging, we exclude mainstream training-aware compression methods and instead introduce post-training quantization (PTQ) into the acceleration process. However, the output distributions of noise estimation networks change over time, which renders previous PTQ methods ineffective for diffusion models designed for multiple time steps. To address this, we develop a DM-specific PTQ method that focuses on quantized operations, calibration dataset, and calibration metric. We formulate our method based on comprehensive investigations and observations, specifically targeting the unique multi-time-step structure of diffusion models.   Experimental results demonstrate that our method can quantize full-precision diffusion models into 8-bit models without compromising or even improving their performance in a training-free manner. Furthermore, our method can be easily integrated as a plug-and-play module with other fast-sampling methods, such as DDIM. The code for our method is available at the provided GitHub link.