This paper proposes a new approach to efficiently utilize the powerful representation capabilities of vision foundation models, such as ViT and Swin, for video understanding. Previous methods that combined spatial and temporal modeling in a unified module did not fully exploit the potential of image transformers. To address this issue, the authors suggest using a dual-path architecture in video models. They introduce a novel DUALPATH adaptation that separates spatial and temporal adaptation paths, employing a lightweight bottleneck adapter in each transformer block. For temporal dynamic modeling, consecutive frames are incorporated into a grid-like frameset to mimic the ability of vision transformers to extrapolate relationships between tokens. The authors also investigate multiple baselines in video understanding and compare them with the proposed DUALPATH approach. Experimental results on four action recognition benchmarks demonstrate that pretrained image transformers with DUALPATH can effectively generalize beyond the data domain.