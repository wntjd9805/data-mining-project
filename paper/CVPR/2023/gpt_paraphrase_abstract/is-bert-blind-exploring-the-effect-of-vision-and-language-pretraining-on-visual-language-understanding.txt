This study explores the potential benefits of incorporating visual and language pretraining techniques to improve the performance of text-only tasks that involve implicit visual reasoning. The researchers propose a set of tasks to evaluate the visual reasoning abilities of text encoder models, as well as non-visual tasks for comparison. They also introduce a novel zero-shot knowledge probing method called Stroop probing, which allows models like CLIP to perform text-only tasks without requiring a specific prediction head. The results show that state-of-the-art text encoders trained with multimodal data outperform those trained with unimodal data on visual language understanding tasks, but underperform on non-visual language understanding tasks. This sheds light on the mixed results previously observed in the capabilities of multimodal models for language understanding. The study concludes that exposure to images during pretraining provides inherent visual reasoning knowledge that benefits language-only tasks requiring implicit visual reasoning. These findings offer valuable insights for the selection of text encoders in multimodal learning contexts.