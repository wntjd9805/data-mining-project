The absence of a generic 3D GAN inversion framework has limited the applications of 3D face reconstruction and semantic editing, despite the progress made by StyleGAN in 2D face reconstruction and editing. This paper addresses the challenge of 3D GAN inversion by predicting a latent code from a single face image to accurately recover its 3D shapes and textures. The problem is complex due to the infinite possibilities of shape and texture compositions in the image and the limited capacity of a global latent code. To overcome this, the paper introduces a self-training scheme that learns the inversion process efficiently using proxy samples generated from a 3DGAN, without the need for real-world 2D-3D training pairs. Additionally, the paper enhances the generation network by adding a local branch that reconstructs face details using pixel-aligned features. A new pipeline for 3D view-consistent editing is also proposed. Extensive experiments demonstrate that the method surpasses existing inversion methods in terms of shape and texture reconstruction quality.