This paper addresses the issue of catastrophic forgetting in continual learning, where a model needs to learn a sequence of tasks and perform well on all of them. The authors introduce the concept of task-induced bias, which arises in continual learning. They analyze the causal framework of continual learning and identify two mechanisms that naturally reduce task-induced bias in task and domain incremental learning. However, these mechanisms are not present in class incremental learning (CIL), where each task consists of a unique subset of classes. To overcome task-induced bias in CIL, the authors propose a causal intervention operation to break the causal path that leads to bias. They implement this operation as a causal debias module that transforms biased features into unbiased ones. They also present a training pipeline to integrate this module into existing methods and optimize the entire architecture. Importantly, their approach does not rely on data replay and can be easily incorporated into existing methods. Through extensive experiments on CIFAR-100 and ImageNet datasets, the authors demonstrate that their approach significantly improves accuracy and reduces forgetting compared to well-established methods.