We examine ObjectGoal Navigation, a task where a virtual robot must navigate to a specific object in a new environment. Previous research has shown that imitation learning using behavior cloning on human demonstrations produces promising results. However, this approach has limitations, such as poor generalization to new states and the high cost of collecting demonstrations. In contrast, reinforcement learning is easily scalable but requires careful reward engineering. To address these challenges, we propose PIRLNav, a two-stage learning scheme that combines behavior cloning pretraining with reinforcement learning finetuning. Our approach achieves a success rate of 65.0% on OBJECTNAV, which is a 5.0% improvement over the previous state-of-the-art. We conduct an empirical analysis to investigate design choices. First, we compare the performance of BC→RL training using human demonstrations versus automatically generated demonstrations, such as shortest paths or task-agnostic frontier exploration trajectories. We find that BC→RL on human demonstrations outperforms other sources, even when accounting for BC-pretraining success. We also analyze the impact of the size of the BC pretraining dataset on RL-finetuning performance. We observe that as the size of the BC dataset increases and achieves high accuracy, the improvements from RL-finetuning become smaller. We show that 90% of the performance of our best BC→RL policy can be achieved with less than half the number of BC demonstrations. Finally, we examine the failure modes of our OBJECTNAV policies and provide guidelines for further improvement. More information about our project can be found on our project page: ram81.github.io/projects/pirlnav.