Existing image-based methods face challenges in animal pose estimation due to limited training data and variations within and between species. This study proposes the use of pre-trained language models, such as CLIP, to enhance animal pose estimation by providing textual descriptions of animal keypoints. However, connecting language models with visual keypoints proves to be difficult due to the disparity between text-based descriptions and keypoint-based visual features. To address this, the authors introduce CLAMP, a novel prompt-based Contrastive learning scheme that effectively bridges this gap. CLAMP adapts text prompts to animal keypoints during network training through spatial-aware and feature-aware processes, incorporating two contrastive losses. Experimental results demonstrate that CLAMP achieves state-of-the-art performance in supervised, few-shot, and zero-shot settings, surpassing image-based methods significantly. The code for CLAMP is available at https://github.com/xuzhang1199/CLAMP.