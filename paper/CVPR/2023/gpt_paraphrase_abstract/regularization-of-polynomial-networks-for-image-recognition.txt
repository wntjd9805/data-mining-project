Deep Neural Networks (DNNs) have achieved remarkable performance in various tasks; however, their lack of interpretability makes them difficult to analyze theoretically. Polynomial Networks (PNs) have emerged as an alternative approach with improved interpretability, but they have not yet matched the performance of DNNs. This study aims to bridge this performance gap by introducing a new class of PNs that can achieve the same performance as ResNet, a powerful DNN, across six different benchmarks. The researchers emphasize the importance of strong regularization and extensively investigate the specific regularization schemes necessary to match the performance. To further enhance the regularization schemes, they propose D-PolyNets, which achieve a higher degree of expansion compared to previous polynomial networks while being more parameter-efficient and maintaining similar performance. The authors expect that their new models can contribute to understanding the role of elementwise activation functions, which are no longer necessary for training PNs. The source code for the proposed networks is available at a provided GitHub link. Figure 1 illustrates how the proposed R-PolyNets and D-PolyNets enable polynomial networks to reach the performance levels of powerful neural networks in a variety of tasks.