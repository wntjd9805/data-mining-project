We propose a two-stage framework called MOtion, Scene and Object decomposition (MOSO) for video prediction. The first stage, MOSO-VQVAE, decomposes a previous video clip into motion, scene, and object components and represents them as discrete tokens. In the second stage, MOSO-Transformer predicts the object and scene tokens of the subsequent video clip based on the previous tokens and adds dynamic motion at the token level. Our framework can be extended to unconditional video generation and video frame interpolation tasks. Experimental results show that our method achieves state-of-the-art performance on challenging benchmarks for video prediction and unconditional video generation: BAIR, RoboNet, KTH, KITTI, and UCF101. Additionally, MOSO can create realistic videos by combining objects and scenes from different videos.