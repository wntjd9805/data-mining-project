We present a novel technique for generating comprehensive 3D reconstructions from a single kaleidoscopic image using a neural surface representation. Full-surround 3D reconstruction is crucial for various applications, including augmented and virtual reality. A kaleidoscope, which employs a single camera and multiple mirrors, offers a practical solution for achieving full-surround coverage by capturing multiple viewpoints in a single image through the redistribution of light directions. This enables efficient and dynamic single-shot 3D reconstruction. However, employing a kaleidoscopic image for multi-view stereo poses challenges as it requires decomposing the image into multiple view images by determining the correspondence between pixels and virtual cameras, a process referred to as labeling. To overcome this challenge, our approach eliminates the need for explicit label estimation and instead utilizes silhouette, background, foreground, and texture information present in the kaleidoscopic image to shape a neural surface representation. We showcase the benefits of our method through a series of simulated and real-world experiments, encompassing both static and dynamic scenes.