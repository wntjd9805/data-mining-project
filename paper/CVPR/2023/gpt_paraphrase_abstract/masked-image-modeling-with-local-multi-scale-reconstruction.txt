Masked Image Modeling (MIM) has been successful in self-supervised representation learning. However, MIM models have limitations in terms of computational burden and slow learning process, making them less suitable for industrial applications. Current MIM models focus on the top layer of the encoder for reconstruction tasks, neglecting the explicit guidance and interaction among the lower layers. To address this, we propose conducting the reconstruction task on multiple local layers, including both lower and upper layers. Additionally, we introduce local multi-scale reconstruction to enable the learning of information at different scales. The lower layers focus on fine-scale supervision signals, while the upper layers handle coarse-scale signals. This approach not only speeds up representation learning by explicitly guiding multiple layers but also enhances the understanding of input through multi-scale semantics. Our experiments demonstrate that our model achieves comparable or better performance on classification, detection, and segmentation tasks compared to existing MIM models, while requiring significantly less pre-training burden. The code for our model is available in both MindSpore and PyTorch.