Pre-training on large-scale video data has become a common approach for learning transferable spatiotemporal representations. However, existing methods are mostly limited to specific datasets and do not provide satisfactory representations without additional modifications. This limitation arises from their focus on capturing pixel-level information rather than spatiotemporal semantics, which hampers further advancements in video understanding.Drawing inspiration from the success of image-text pre-training methods like CLIP, we propose a novel approach to enhance transferable spatiotemporal representation learning by incorporating language semantics. We introduce a new pretext task called Turning to Video for Transcript Sorting (TVTS), where we sort shuffled Automatic Speech Recognition (ASR) scripts by attending to learned video representations. Unlike relying on descriptive captions, our method learns purely from video data and utilizes transcribed speech knowledge to provide noisy but informative semantics over time. By contextualizing the temporal aspects of the video, our approach enables the reorganization of narrative transcripts, making it applicable to large-scale uncurated video data in real-world scenarios.Our method showcases impressive out-of-the-box performance in generating spatiotemporal representations on diverse benchmarks. For instance, we achieve a +13.6% improvement over VideoMAE on the SSV2 benchmark through linear probing. The code for our method is publicly available at https://github.com/TencentARC/TVTS.