This study explores the application of knowledge distillation in image-based 6D object pose estimation. While knowledge distillation has been successful in various tasks using deep teacher networks to train compact student networks, its effectiveness in 6D pose estimation has not been investigated. The researchers observed that modern 6D pose estimation frameworks produce local predictions, such as sparse 2D keypoints or dense representations, which the compact student network struggles to predict accurately. Instead of directly supervising the student network's predictions with the teacher network, the researchers propose distilling the teacher's distribution of local predictions into the student network to enhance its training. Experimental results on multiple benchmarks demonstrate that this distillation method achieves state-of-the-art performance with different compact student models and for both keypoint-based and dense prediction-based architectures.