In recent times, open-vocabulary learning has become popular for segmenting text-based descriptions into arbitrary categories. However, current methods focus on designing specialized architectures and parameters for specific segmentation tasks, resulting in fragmentation between different tasks and hindering the uniformity of segmentation models. To address this issue, we present FreeSeg, a versatile framework that achieves Unified, Universal, and Open-Vocabulary Image Segmentation. FreeSeg utilizes an all-in-one network trained in a single shot and applies the same architecture and parameters to seamlessly handle diverse segmentation tasks during the inference process. Furthermore, adaptive prompt learning aids the unified model in capturing task-aware and category-sensitive concepts, thereby enhancing model robustness in multi-task and varied scenarios. Extensive experiments demonstrate that FreeSeg surpasses the performance and generalization of the best task-specific architectures by a significant margin in three segmentation tasks: semantic segmentation, instance segmentation, and panoptic segmentation. Notably, FreeSeg achieves a 5.5% increase in mIoU for semantic segmentation, a 17.6% increase in mAP for instance segmentation, and a 20.1% increase in PQ for panoptic segmentation of unseen classes on the COCO dataset. For more information, please visit our project page: https://FreeSeg.github.io.