Historically, scene graph generation (SGG) methods have faced challenges such as long-tail bias and slow inference speed. However, humans are capable of analyzing object relationships based solely on context descriptions, guided by their experience. For instance, given descriptions of a cup and a table along with their spatial locations, humans can speculate possible relationships such as < cup, on, table > or < table, near, cup >. Some impossible predicates like flying in and looking at can be excluded empirically, even without visual appearance information.To address these challenges, we propose a contextual scene graph generation (C-SGG) method that does not rely on visual information. We also introduce a context augmentation method to generate diverse context descriptions by making slight perturbations in the position and size of objects. We believe that these perturbations do not significantly affect the relationship between objects at the context level. By using these diverse context descriptions, we can train the C-SGG model in an unbiased manner, thereby alleviating long-tail bias.Furthermore, we introduce a context guided visual scene graph generation (CV-SGG) method that builds upon the experience gained from C-SGG. This approach allows vision to focus on possible predicates, achieving a trade-off between common predicates and tail predicates. Extensive experiments on publicly available datasets demonstrate that C-SGG mitigates long-tail bias and enables real-time SGG without the need for computationally expensive visual feature extraction.In summary, our proposed methods, C-SGG and CV-SGG, leverage context descriptions to overcome long-tail bias and improve inference speed in scene graph generation tasks.