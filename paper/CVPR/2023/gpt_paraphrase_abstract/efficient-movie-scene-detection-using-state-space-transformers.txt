Detecting different movie scenes is crucial for understanding the plot of a movie. However, accurately identifying these scenes is challenging due to the need to analyze long segments of the film. Existing video recognition models are not suitable for this task as they are designed for short-range analysis. To address this issue, we propose a State-Space Transformer model called TranS4mer. This model efficiently captures dependencies in long movie videos to accurately detect movie scenes. TranS4mer utilizes a novel building block called S4A, which combines structured state-space sequence (S4) and self-attention (A) layers. The S4A block employs self-attention to capture short-range dependencies within each movie shot, which are uninterrupted periods where the camera position remains unchanged. Additionally, the state-space operation in the S4A block aggregates long-range cues between shots. The final TranS4mer model is obtained by stacking multiple S4A blocks and can be trained end-to-end. Our proposed TranS4mer outperforms previous methods in three movie scene detection datasets (MovieNet, BBC, and OVSD). Furthermore, it is twice as fast and requires three times less GPU memory than standard Transformer models. We will make our code and models publicly available.