This study focuses on the challenges of improving the quality of natural videos captured by consumer cameras, which often suffer from low framerate and motion blur. Existing computational methods that address these issues assume known and fixed exposure time, which is unrealistic. The goal of this work is to develop a more realistic approach for joint video multi-frame interpolation and deblurring under unknown exposure time.To achieve this, the researchers propose a method that utilizes a variant of supervised contrastive learning to create an exposure-aware representation from input blurred frames. They train two U-Nets for intra-motion and inter-motion analysis, adapting to the learned exposure representation through gain tuning. The video reconstruction network is then built upon the exposure and motion representation using progressive exposure-adaptive convolution and motion refinement.Extensive experiments are conducted on both simulated and real-world datasets. The optimized method shows significant performance improvements compared to the state-of-the-art approaches for joint video ×8 interpolation and deblurring task. Furthermore, the proposed method outperforms existing methods by more than 1.5 dB in terms of PSNR for the seemingly implausible ×16 interpolation task.