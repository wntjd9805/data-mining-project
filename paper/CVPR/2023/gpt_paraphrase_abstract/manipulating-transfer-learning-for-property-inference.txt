Transfer learning is a popular technique used to adapt pre-trained models for different tasks when there is limited data and computational resources. In this study, we investigate how an adversary who controls the upstream model can launch property inference attacks on a victim's tuned downstream model. These attacks aim to determine if specific images of an individual are present in the training set of the downstream model. We demonstrate that the adversary can manipulate the upstream model to effectively and specifically infer properties (with an AUC score > 0.9), while still maintaining good performance on the main task. The manipulation involves making the upstream model generate different distributions of activations for samples with and without the target property. This allows the adversary to easily distinguish between downstream models trained with and without examples containing the target property. Our code for these attacks is publicly available.