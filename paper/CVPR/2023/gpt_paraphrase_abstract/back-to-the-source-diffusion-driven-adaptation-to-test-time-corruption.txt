Test-time adaptation improves the accuracy of a model trained on one set of data when tested on a different set of data. Most methods update the model by re-training it on the new data, but this approach is sensitive to the amount and order of the data, as well as the optimization parameters. In this study, we propose a different approach called diffusion-driven adaptation (DDA), where instead of updating the model, we update the target data and project all test inputs towards the original data using a generative diffusion model. Our DDA method shares its models for classification and generation across all domains, training them on the original data and then freezing them for all target domains to avoid the costly re-training process. We also incorporate image guidance and classifier self-ensembling to automatically determine the extent of adaptation needed. Our experiments on the ImageNet-C benchmark show that DDA's input adaptation is more robust compared to model adaptation, particularly in scenarios with limited data, dependent data, or mixed data.