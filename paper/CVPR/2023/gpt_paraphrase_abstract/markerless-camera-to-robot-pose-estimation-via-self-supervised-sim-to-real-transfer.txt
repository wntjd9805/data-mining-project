The camera-to-robot pose is a crucial aspect of vision-based robot control, requiring accuracy and effort. Traditional methods involve modifying the robot with markers, while newer deep learning approaches use markerless feature extraction. However, these methods rely on synthetic data and domain randomization to bridge the gap between simulated and real-world environments, as acquiring 3D annotations is time-consuming. This study introduces an end-to-end pose estimation framework that overcomes the limitations of 3D annotations in real-world data. The framework combines deep learning and geometric vision, with a fully differentiable pipeline. To train the Camera-to-Robot Pose Estimation Network (CtRNet), the researchers utilize foreground segmentation and differentiable rendering for self-supervision at the image level. The predicted pose is visualized and the image loss is back-propagated to train the neural network. Experimental results on two public real datasets demonstrate the effectiveness of the proposed approach compared to existing methods. The framework is also integrated into a visual servoing system to showcase its potential for real-time precise robot pose estimation in automation tasks.