This paper introduces a new approach called LayoutDM for automatic layout generation in graphic design. The existing methods using generative models like GANs and VAEs have made progress but still have limitations in terms of quality and diversity of the generated layouts. Inspired by the success of diffusion models in generating high-quality images, this paper explores their potential for layout generation and proposes a transformer-based layout diffusion model. Instead of using convolutional neural networks, a transformer-based conditional Layout Denoiser is proposed to learn the reverse diffusion process for generating samples from noised layout data. The LayoutDM combines the strengths of both transformer and DDPM, resulting in high-quality generation, strong sample diversity, faithful distribution coverage, and stationary training. Experimental results demonstrate that LayoutDM outperforms state-of-the-art generative models in terms of quality and diversity.