Adversarial attacks can deceive deep neural networks (DNNs) by introducing subtle changes to innocent samples. These attacks can be transferred to attack DNNs with unknown architectures or parameters, posing a threat to real-world applications. However, existing transferable attacks fail to differentiate between style and content features during optimization, limiting their effectiveness. To address this limitation, we propose a new attack method called StyLess, which focuses on perturbing style features. Instead of using a regular network as the surrogate model, we suggest using stylized networks that alter style features through adaptive instance normalization. By preventing adversarial examples from relying on non-robust style features, our method generates more transferable perturbations. Extensive experiments demonstrate that our approach significantly enhances the transferability of adversarial examples. Moreover, when combined with other attack techniques, our generic approach outperforms state-of-the-art transferable attacks.