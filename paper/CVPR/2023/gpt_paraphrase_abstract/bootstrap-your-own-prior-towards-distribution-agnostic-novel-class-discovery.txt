Novel Class Discovery (NCD) is a method used to identify unknown classes without any prior annotation, utilizing knowledge gained from a set of known classes. However, existing approaches make the impractical assumption that the distribution of novel classes is uniform, disregarding the imbalanced nature of real-world data. To overcome this limitation, we propose a new task called distribution-agnostic NCD, which allows for data drawn from arbitrary unknown class distributions. This renders existing methods ineffective or even harmful. To address this challenge, we introduce a new technique called "Boot-strapping Your Own Prior (BYOP)". BYOP estimates the class prior iteratively based on the model's predictions. We employ a dynamic temperature technique that improves the estimation of the class prior by encouraging sharper predictions for samples with lower confidence. As a result, BYOP achieves more accurate pseudo-labels for novel samples, which benefit subsequent training iterations. Through extensive experiments, we demonstrate that existing methods struggle with imbalanced class distributions, while BYOP consistently outperforms them across various distribution scenarios, proving its effectiveness.