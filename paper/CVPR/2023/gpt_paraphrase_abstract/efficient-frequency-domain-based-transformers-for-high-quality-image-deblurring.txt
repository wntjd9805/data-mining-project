We introduce a novel method for high-quality image deblurring using Transformers in the frequency domain. Inspired by the convolution theorem, we develop a frequency domain-based self-attention solver (FSAS) that replaces matrix multiplication with element-wise product operations. We also address the limitations of using the naive feed-forward network (FFN) in Transformers by proposing a discriminative frequency domain-based FFN (DFFN) that incorporates a gated mechanism based on the JPEG compression algorithm. Our approach combines FSAS and DFFN in an encoder-decoder architecture, with FSAS specifically applied in the decoder module for improved deblurring. Experimental results demonstrate that our method outperforms state-of-the-art approaches.