This study introduces the concept of N-Gram context to low-level vision using Transformers, specifically for single image super-resolution (SR). The Swin Transformer (Swin) with window self-attention (WSA) has been proven suitable for SR, but the plain WSA neglects broad regions and has a limited receptive field. Additionally, existing deep learning SR methods are computationally intensive. To overcome these issues, the N-Gram context is defined as neighboring local windows in Swin, allowing for interaction through sliding-WSA to expand the regions seen and restore degraded pixels. The proposed NGswin is an efficient SR network with a SCDP bottleneck that leverages multi-scale outputs from the hierarchical encoder. Experimental results demonstrate that NGswin achieves competitive performance while maintaining an efficient structure compared to previous methods. Furthermore, the N-Gram context is applied to other Swin-based SR methods, resulting in an enhanced model called SwinIR-NG. SwinIR-NG surpasses current lightweight SR approaches and establishes state-of-the-art results. The codes for NGswin and SwinIR-NG are available at https://github.com/rami0205/NGramSwin.