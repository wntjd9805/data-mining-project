This paper addresses two challenges in multimodal learning for visual recognition. The first challenge is when a modality is missing during training or testing in real-world situations. The second challenge is when there is a lack of computational resources to fine-tune heavy transformer models. To overcome these challenges, we propose a solution that combines prompt learning and mitigates the aforementioned issues. By using modality-missing-aware prompts, we can integrate them into multimodal transformers to handle different missing-modality cases. This approach requires less than 1% of learnable parameters compared to training the entire model. We also investigate the impact of different prompt configurations and analyze the robustness to missing modality. Through extensive experiments, we demonstrate the effectiveness of our prompt learning framework in improving performance in various missing-modality scenarios, while reducing the need for heavy model re-training. The code for our framework is available.