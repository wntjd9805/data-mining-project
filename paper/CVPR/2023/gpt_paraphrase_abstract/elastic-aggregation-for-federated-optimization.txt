Federated learning is a technique used to train neural network models while preserving data privacy. The preferred optimizer for federated learning is FedAvg, which updates the server model by aggregating client models based on the number of instances used in their training. However, this naive aggregation method faces challenges when dealing with heterogeneous data, leading to unstable and slow convergence.To address these issues, we propose a new aggregation approach called elastic aggregation. Elastic aggregation adaptsively interpolates client models based on their parameter sensitivity, which is determined by measuring the change in the overall prediction function output when each parameter is altered. This measurement is done in an unsupervised and online manner.Elastic aggregation reduces the magnitude of updates to the more sensitive parameters to prevent the server model from being biased towards any particular client distribution. Conversely, it boosts updates to the less sensitive parameters to explore different client distributions more effectively. We conducted empirical and analytical experiments on real and synthetic data, which demonstrated that elastic aggregation enables efficient training in both convex and non-convex scenarios. It is also robust to client heterogeneity, large numbers of clients, partial participation, and imbalanced data.Furthermore, elastic aggregation can be seamlessly integrated with other federated optimizers and consistently achieves significant improvements across different scenarios.