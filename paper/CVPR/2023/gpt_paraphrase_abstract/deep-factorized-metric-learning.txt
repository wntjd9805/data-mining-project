Learning a universal and comprehensive measure of similarity to represent the semantic differences between images is crucial for various computer vision tasks. Existing methods achieve this by training a combination of embeddings with different objectives, but the backbone network still receives a mixture of all training signals. In contrast, our proposed approach, called deep factorized metric learning (DFML), aims to separate and utilize different samples to train specific components of the backbone network. We divide the network into sub-blocks and introduce a trainable router to dynamically assign training samples to each sub-block, maximizing information capture. The metric model trained using DFML captures distinct characteristics with each sub-block, resulting in a generalizable metric when combining all sub-blocks. DFML outperforms other methods on three prominent benchmarks for deep metric learning: CUB-200-2011, Cars196, and Stanford Online Products. Furthermore, we extend DFML to the image classification task on ImageNet-1K and consistently observe improved accuracy with reduced computational load. Specifically, we enhance the performance of ViT-B on ImageNet by 0.2% accuracy while reducing FLOPs by 24%.