This study introduces a novel approach to synthesizing high-quality impact sounds for video clips. Traditional methods rely on physics simulation, which requires detailed object geometries and impact locations. However, these details are often unavailable in real-world scenarios and cannot be applied to common videos. Existing deep learning-based approaches struggle to capture the relationship between visual content and impact sounds due to a lack of physics knowledge. To address these limitations, the researchers propose a physics-driven diffusion model that incorporates additional physics priors to guide the sound synthesis process. These priors consist of physics parameters estimated from real-world impact sound examples and learned residual parameters interpreted by neural networks. The researchers develop a unique diffusion model with specific training and inference strategies to combine physics priors with visual information, resulting in realistic impact sound generation. Experimental results demonstrate that their model outperforms existing systems. Additionally, the physics-based representations are highly interpretable and transparent, enabling flexible sound editing. The researchers encourage readers to visit their project page to watch demo videos and experience the results firsthand.