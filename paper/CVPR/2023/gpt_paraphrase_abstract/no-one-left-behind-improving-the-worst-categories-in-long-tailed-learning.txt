Neural networks trained with imbalanced datasets often show varying levels of accuracy for different categories. In long-tailed recognition, categories are divided into subsets and the average accuracy within each subset is reported. However, this evaluation setting sacrifices certain categories and does not necessarily reflect the performance of all categories accurately. Therefore, instead of focusing on average accuracy, we propose improving the lowest recall and the harmonic mean of all recall values. We suggest a simple plug-in method that can be applied to various techniques. By re-training the classifier of an existing pre-trained model with our proposed loss function and using an ensemble trick to combine predictions, we achieve a more balanced distribution of recall values across categories. This leads to a higher harmonic mean accuracy without compromising the average accuracy. We validate the effectiveness of our method on popular benchmark datasets.