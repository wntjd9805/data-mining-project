We present a new framework for Human-Object Interaction (HOI) detection that utilizes Contrastive Language-Image Pre-training (CLIP) to improve generalization. Our framework includes an interaction decoder that extracts informative regions from the visual feature map of CLIP, which is then combined with the detection backbone for more accurate human-object pair detection. We also leverage the text encoder of CLIP to generate a classifier for HOI descriptions and build a verb classifier using visual semantic arithmetic and a lightweight verb representation adapter. Additionally, we propose a training-free enhancement to utilize global HOI predictions from CLIP. Experimental results show that our method outperforms existing approaches by a significant margin on various settings. The source code is available at https://github.com/Artanic30/HOICLIP.