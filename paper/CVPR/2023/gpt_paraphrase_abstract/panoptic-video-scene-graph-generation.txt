To enhance real-world visual perception systems, we introduce a novel problem called panoptic scene graph generation (PVSG) and compare it to the existing video scene graph generation (VidSGG) problem. VidSGG focuses on temporal interactions between humans and objects in videos using bounding boxes, but this approach often overlooks important details. In contrast, PVSG utilizes pixel-level segmentation masks to ground nodes in scene graphs, enabling a comprehensive understanding of the scene. To support further research in this field, we provide a high-quality PVSG dataset comprising 400 videos (289 third-person and 111 egocentric) with 150K frames. These videos are labeled with panoptic segmentation masks and detailed temporal scene graphs.