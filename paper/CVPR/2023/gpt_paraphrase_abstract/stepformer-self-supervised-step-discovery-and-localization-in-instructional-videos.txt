Instructional videos are a valuable resource for learning procedural tasks through human demonstrations. However, these videos often contain irrelevant content, making it difficult to locate the important instruction steps. Existing methods for key-step localization require human annotations at the video level, which is not feasible for large datasets. In this study, we propose a self-supervised model called StepFormer that can discover and localize instruction steps in videos without human supervision. StepFormer is a transformer decoder that uses learnable queries to attend to the video and generate a sequence of slots representing the key-steps. We train the model on a large dataset of instructional videos, using automatically-generated subtitles as the sole source of supervision. To filter out irrelevant phrases, we supervise the model with a sequence of text narrations using an order-aware loss function. Our experiments demonstrate that StepFormer outperforms previous unsupervised and weakly-supervised approaches in step detection and localization on three challenging benchmarks. Additionally, our model exhibits the ability to solve zero-shot multi-step localization, surpassing all relevant baselines in this task.