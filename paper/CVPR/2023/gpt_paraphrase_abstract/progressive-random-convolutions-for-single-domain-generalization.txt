We propose a method called Progressive Random Convolution (Pro-RandConv) to address the limitations of the existing Random Convolutions (RandConv) technique in single domain generalization. RandConv uses randomly initialized convolution layers to distort local textures and improve generalizability of models. However, as the kernel size increases, RandConv loses semantics and lacks diversity. Our Pro-RandConv method overcomes this by recursively stacking random convolution layers with small kernel sizes instead of increasing the kernel size. This progressive approach reduces semantic distortions and increases style diversity. We also enhance the random convolution layer with deformable offsets and affine transformation to further diversify textures and contrast. Without complex generators or adversarial learning, our simple augmentation strategy outperforms state-of-the-art methods in single domain generalization benchmarks.