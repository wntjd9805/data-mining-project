Few-shot learning, which refers to the ability to quickly learn a new task with minimal instruction, is an important aspect of intelligent agents. However, existing few-shot benchmarks only utilize few-shot samples from a single modality, which may not fully capture the characteristics of an entire concept class. In contrast, humans leverage cross-modal information to efficiently learn new concepts. This study demonstrates that it is possible to enhance the performance of a visual dog classifier by incorporating cross-modal information. The approach leverages recent multimodal foundation models like CLIP, which map different modalities to the same representation space. A simple cross-modal adaptation approach is proposed, which learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, state-of-the-art (SOTA) results are achieved using a simple linear classifier for vision-language adaptation. Additionally, the approach proves to be beneficial for existing methods such as prefix tuning, adapters, and classifier ensembling. Furthermore, the study explores other modalities beyond vision and language by constructing the first audiovisual few-shot benchmark and utilizing cross-modal training to improve both image and audio classification performance. The project website can be found at the provided link.