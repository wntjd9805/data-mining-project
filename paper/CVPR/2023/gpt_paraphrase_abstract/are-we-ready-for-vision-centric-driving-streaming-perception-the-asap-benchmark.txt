The field of vision-centric perception in autonomous driving has seen significant advancements in recent years, particularly in tasks such as 3D detection, semantic map construction, motion forecasting, and depth estimation. However, the latency of these approaches is often too high for practical implementation, with camera-based 3D detectors taking more than 300ms for runtime. To bridge the gap between theoretical research and real-world applications, it is crucial to understand the trade-off between performance and efficiency. Existing benchmarks for autonomous driving perception primarily focus on offline evaluation, disregarding the inference time delay. To address this issue, we introduce the Autonomous-driving StreAming Perception (ASAP) benchmark, the first benchmark to assess the online performance of vision-centric perception in autonomous driving. Using the 2Hz annotated nuScenes dataset as a foundation, we develop a pipeline to generate high-frame-rate labels for the 12Hz raw images. Additionally, we propose the Streaming Perception Under constRained-computation (SPUR) evaluation protocol, which evaluates the streaming performance of the 12Hz inputs based on varying computational resource constraints. Through comprehensive experiments, we discover that different constraints result in alterations to the model rank, emphasizing the importance of considering model latency and computation budget as design choices to optimize practical deployment. To support further research, we establish baselines for camera-based streaming 3D detection, consistently improving streaming performance across different hardware. For more information on the ASAP benchmark, please visit our project page: https://github.com/JeffWang987/ASAP.