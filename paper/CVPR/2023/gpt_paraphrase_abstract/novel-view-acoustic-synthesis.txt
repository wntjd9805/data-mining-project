We present the novel-view acoustic synthesis (NVAS) task, which involves synthesizing the sound of a scene from an unseen viewpoint based on visual and auditory input. We propose a neural rendering approach called Visually-Guided Acoustic Synthesis (ViGAS) network, which analyzes audio-visual cues to generate sound for any point in space. To evaluate this task, we create two large-scale multi-view audio-visual datasets, one synthetic and one real. Our model successfully incorporates spatial cues and produces accurate audio in both datasets. This work is the first to introduce the concept, dataset, and approach for solving the NVAS task, which has potential applications in AR/VR, art, and design. We believe that future advancements in novel-view synthesis will involve multi-modal learning from videos.