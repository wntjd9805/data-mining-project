The main focus of out-of-distribution (OOD) detection is to learn a representation of in-distribution (ID) samples that can be distinguished from OOD samples. Previous approaches have used recognition-based methods to learn ID features, but these methods often learn shortcuts rather than comprehensive representations. Surprisingly, we find that using reconstruction-based methods can significantly improve OOD detection performance. We investigate the main factors contributing to OOD detection and discover that reconstruction-based pretext tasks have the potential to provide a widely applicable and effective prior, benefiting the model in learning the intrinsic data distributions of the ID dataset. Specifically, we propose a OOD detection framework called MOOD, which utilizes Masked Image Modeling as a pretext task. MOOD outperforms the previous state-of-the-art methods in one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even surpasses the outlier exposure OOD detection with 10-shot-per-class, despite not including any OOD samples in its training. The source code is available at https://github.com/lijingyao20010602/MOOD.