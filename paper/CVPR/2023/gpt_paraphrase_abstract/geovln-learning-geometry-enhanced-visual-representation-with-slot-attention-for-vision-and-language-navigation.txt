Current approaches to solving the Room-to-Room VLN problem lack consideration of local context and rely solely on RGB images, which do not provide enough visual cues about the surrounding environment. Additionally, the complex semantic information in natural language instructions makes it challenging to model its correlations with visual inputs. To address these limitations, we propose GeoVLN, a method that leverages cross attention and slot attention to learn a Geometry-enhanced visual representation for robust Visual-and-Language Navigation. Our approach compensates for the lack of visual cues by incorporating depth maps and normal maps predicted by Omnidata. We introduce a two-stage module that combines local slot attention and the CLIP model to generate geometry-enhanced representations from these inputs. We utilize V&L BERT to learn a cross-modal representation that incorporates both language and vision information. Furthermore, we design a novel multiway attention module that allows different phrases in the input instruction to exploit the most relevant features from the visual input. Through extensive experiments, we demonstrate the effectiveness of our newly designed modules and the compelling performance of our proposed method.