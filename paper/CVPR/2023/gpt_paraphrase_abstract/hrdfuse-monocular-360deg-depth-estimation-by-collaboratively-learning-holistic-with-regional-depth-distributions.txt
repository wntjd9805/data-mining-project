The problem of estimating depth from a monocular 360-degree image is gaining attention due to its comprehensive scene perception. Previous methods, such as OmniFusion, have used tangent projection (TP) to represent the image and predicted depth values through patch-wise regressions, which are then merged to create a depth map in equirectangular projection (ERP) format. However, these methods face challenges in merging numerous patches and fail to capture holistic-with-regional contextual information by directly regressing depth values for each pixel. In this paper, we propose a new framework called HRDFuse that combines convolutional neural networks (CNNs) and transformers to jointly learn holistic contextual information from ERP and regional structural information from TP. Firstly, we introduce a spatial feature alignment (SFA) module that learns feature similarities between TP and ERP to aggregate TP features into a complete ERP feature map on a pixel-wise basis. Secondly, we propose a collaborative depth distribution classification (CDDC) module that captures holistic-with-regional histograms of depth distributions from ERP and TP. Consequently, the final depth values can be predicted using a linear combination of histogram bin centers. Lastly, we adaptively combine the depth predictions from ERP and TP to generate the final depth map. Extensive experiments demonstrate that our method produces smoother and more accurate depth results, outperforming state-of-the-art methods. For additional multimedia material, including videos, code, demos, and more information, please visit https://VLIS2022.github.io/HRDFuse/.