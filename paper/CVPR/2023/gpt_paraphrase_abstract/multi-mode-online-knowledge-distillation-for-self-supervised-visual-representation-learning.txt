This study introduces a novel approach called Multi-mode Online Knowledge Distillation (MOKD) to enhance self-supervised visual representation learning. Unlike existing methods that transfer knowledge from a pre-trained teacher to a student, MOKD involves two models learning collaboratively in a self-supervised manner. It includes two modes of distillation: self-distillation and cross-distillation. Self-distillation allows each model to learn independently, while cross-distillation enables knowledge interaction between the models. Cross-attention feature search strategy is proposed in cross-distillation to improve the alignment of semantic features between the models. This allows the models to learn from each other and enhance their representation learning performance. Experimental results using different backbones and datasets demonstrate that MOKD benefits both heterogeneous models and outperforms independently trained baselines. Furthermore, MOKD surpasses existing SSL-KD methods for both the student and teacher models.