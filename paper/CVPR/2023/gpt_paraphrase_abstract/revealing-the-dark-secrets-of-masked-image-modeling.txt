Masked image modeling (MIM) has been proven effective for various vision downstream tasks, but its underlying mechanisms and impact are not fully understood. This study aims to compare MIM with the traditional supervised pre-trained models in terms of visualizations and experiments to uncover their key differences in representation. The visualizations reveal that MIM introduces a local bias to all layers of the trained models, while supervised models tend to focus locally at lower layers and globally at higher layers. This may explain why MIM is particularly beneficial for Vision Transformers with large receptive fields. MIM allows the model to maintain diversity in attention heads across all layers, whereas supervised models lose diversity in the last three layers, leading to worse fine-tuning performance. Experimental results demonstrate that MIM models outperform their supervised counterparts in geometric and motion tasks with weak semantics or fine-grained classification. A standard MIM pre-trained SwinV2-L achieves state-of-the-art performance in pose estimation, depth estimation, and video object tracking. For datasets with sufficient coverage by supervised pre-training, MIM models still achieve highly competitive transfer performance. The authors hope that this study will inspire further research in this direction. The code for this study is available at https://github.com/zdaxie/MIM-DarkSecrets.