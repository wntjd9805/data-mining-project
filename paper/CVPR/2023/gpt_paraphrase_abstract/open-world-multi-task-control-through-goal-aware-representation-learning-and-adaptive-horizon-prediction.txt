We investigate the problem of teaching goal-oriented policies in Minecraft, an open-ended environment that is popular and accessible. We highlight two main challenges in learning such policies: 1) the difficulty in distinguishing between tasks due to the wide range of scenes, and 2) the constantly changing dynamics caused by limited observability. To address the first challenge, we propose a Goal-Sensitive Backbone (GSB) that encourages the policy to focus on relevant visual state representations. To tackle the second challenge, we introduce an adaptive horizon prediction module that reduces learning uncertainty caused by the changing dynamics. Our experiments on 20 Minecraft tasks demonstrate that our method outperforms existing approaches, often doubling their performance. Additionally, our studies reveal how our approach overcomes the challenges and also showcases the unexpected benefit of generalizing to new scenes. We hope that our agent can contribute to the advancement of learning goal-oriented, multi-task agents in complex environments like Minecraft. The code for our method is available at https://github.com/CraftJarvis/MC-Controller.