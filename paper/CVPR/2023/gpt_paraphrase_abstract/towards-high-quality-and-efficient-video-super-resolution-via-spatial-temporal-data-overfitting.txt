Video resolution upscaling using deep convolutional neural networks (DNNs) has become a popular approach in computer vision. This involves dividing videos into chunks and overfitting each chunk with a super-resolution model to improve video quality and transmission efficiency. However, this method requires a large number of chunks, leading to increased storage and bandwidth usage. Alternatively, reducing the number of chunks slows down execution speed. To address these challenges, we propose a new method that accurately divides videos into chunks based on spatial-temporal information, minimizing the number of chunks and model size. We also introduce a data-aware joint training technique to create a single overfitting model, reducing storage requirements without compromising quality. Our method achieves real-time video super-resolution with high quality on a mobile phone. Experimental results demonstrate that our method outperforms existing approaches, achieving a streaming speed of 28 fps with a PSNR of 41.6, which is 14 times faster and 2.29 dB better in live video resolution upscaling tasks. The code for our method is available at https://github.com/coulsonlee/STDO-CVPR2023.git.