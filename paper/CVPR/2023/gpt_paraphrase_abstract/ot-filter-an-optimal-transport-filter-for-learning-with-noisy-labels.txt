Deep learning has achieved great success in large part due to training on clean data. However, in practice, data often comes with noisy labels. This presents a challenge because deep neural networks (DNN) perform poorly when trained on noisy labels, as they tend to memorize the noise. To address this issue, a recent approach focuses on sample selection, which involves identifying clean data samples from the noisy ones to improve the model's robustness and tolerance to noisy labels.In this paper, we propose a new method called the OT-Filter, which revamps the sample selection process using optimal transport theory. The OT-Filter incorporates geometrically meaningful distances and preserves distribution patterns to measure the discrepancy between data samples, thereby mitigating the confirmation bias caused by noisy labels. We conducted extensive experiments on benchmark datasets such as Clothing1M and ANIMAL-10N, and the results demonstrate that the OT-Filter outperforms other existing methods. Furthermore, we evaluated the OT-Filter on benchmarks with synthetic labels like CIFAR-10/100, and it exhibited superior performance in handling high levels of label noise.Overall, our approach, the OT-Filter, provides a novel perspective on sample selection by leveraging optimal transport theory. It effectively addresses the challenge of learning with noisy labels and improves the performance of deep neural networks.