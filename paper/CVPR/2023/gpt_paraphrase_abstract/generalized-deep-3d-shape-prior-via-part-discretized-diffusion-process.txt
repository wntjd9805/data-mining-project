We have developed a versatile 3D shape generation model that can be applied to various tasks such as unconditional shape generation, point cloud completion, and cross-modality shape generation. Our model utilizes a vector quantized variational autoencoder (VQ-VAE) to accurately capture local shape details by indexing local geometry from a learned codebook. Additionally, we introduce a discrete diffusion generator to model the structural dependencies between different components. To suppress high-frequency shape fluctuations, we have developed a multi-frequency fusion module (MFM) that uses contextual information. Our model incorporates these designs to generate high-quality, diverse shapes and align different modalities. Extensive experiments have shown that our model outperforms others in various 3D shape generation tasks.