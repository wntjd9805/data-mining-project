Increasing the coding efficiency of video codecs depends on the ability to find relevant contexts from previous reconstructed signals. Traditional codecs have shown that more contexts lead to better coding performance, but this process is time-consuming. However, neural video codecs (NVC) currently have limited contexts, resulting in low compression ratios. To address this, this study proposes enhancing the diversity of contexts in both temporal and spatial dimensions. Firstly, the model is trained to learn hierarchical quality patterns across frames, enabling the utilization of long-term and high-quality temporal contexts. Additionally, to leverage the optical flow-based coding framework, a group-based offset diversity is introduced, allowing for improved context mining through cross-group interaction. Moreover, a quadtree-based partition is employed to increase spatial context diversity during parallel encoding of the latent representation. Experimental results demonstrate that our codec achieves a 23.5% bitrate reduction compared to the state-of-the-art NVC. Furthermore, our codec outperforms the next generation traditional codec/ECM in both RGB and YUV420 colorspaces in terms of PSNR. The source code can be found at https://github.com/microsoft/DCVC.