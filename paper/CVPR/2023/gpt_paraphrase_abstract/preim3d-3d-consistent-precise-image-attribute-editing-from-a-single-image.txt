This paper addresses the problem of 3D-aware image attribute editing, which is widely applicable in various fields. Previous methods tackled this problem by training a shared encoder to map images into a 3D generator's latent space or optimizing latent codes for each image and performing editing in the latent space. However, these methods still face challenges in terms of 3D inconsistency at large camera poses and imprecise image attribute editing.   To address these issues, the authors propose two novel methods. Firstly, they train a shared encoder for all images to achieve more efficient image inversion. Secondly, they introduce an alternating training scheme and a multi-view identity loss to alleviate 3D inconsistency and maintain subject identity.   The authors also attribute the problem of imprecise image editing to the gap between the latent space of real images and that of generated images. They compare the latent space and inversion manifold of GAN models and demonstrate that editing in the inversion manifold yields better results in terms of both quantitative and qualitative evaluations.   Extensive experiments are conducted to validate the effectiveness of the proposed method. The results show that the method generates more 3D consistent images and achieves more precise image editing compared to previous approaches. The source code and pretrained models are provided on the authors' project page at https://mybabyyh.github.io/Preim3D/.