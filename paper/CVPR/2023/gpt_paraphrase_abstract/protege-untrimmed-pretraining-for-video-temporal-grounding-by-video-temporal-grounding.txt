Video temporal grounding (VTG) involves localizing a given text query in a long untrimmed video. Existing VTG methods rely on features from pretraining on trimmed videos because there is a lack of large-scale well-annotated VTG datasets for pretraining. However, these pretrained features lack temporal boundaries, making it difficult to distinguish between correct and incorrect alignments of video and text. To address this issue, we propose ProT´eG´e, the first method to perform untrimmed pretraining for VTG. We reconfigure the HowTo100M dataset with noisy video-text pairs to create a VTG dataset. We also introduce a novel Video-Text Similarity-based Grounding Module and a pretraining objective to make the pretraining process robust to noise in HowTo100M. Extensive experiments on various datasets and downstream tasks confirm that pretrained features from ProT´eG´e outperform features from trimmed pretrained backbones on VTG.