This study focuses on Composed Image Retrieval (CIR), where users combine a query image with text to search for specific targets. Current methods for CIR rely on supervised learning using labeled triplets, consisting of the query image, text description, and target image. However, labeling these triplets is costly and limits the practicality of CIR. In this research, we introduce Zero-Shot Composed Image Retrieval (ZS-CIR), which aims to develop a CIR model without the need for labeled triplets during training. To achieve this, we propose a new approach called Pic2Word, which only requires weakly labeled image-caption pairs and unlabeled image datasets for training. Unlike supervised CIR models, our model trained on weakly labeled or unlabeled datasets demonstrates strong adaptability across various ZS-CIR tasks, such as attribute editing, object composition, and domain conversion. Our approach surpasses several supervised CIR methods in performance on the widely used CIR benchmarks, CIRR and Fashion-IQ. The code for our approach will be publicly available at https://github.com/google-research/composed_image_retrieval.