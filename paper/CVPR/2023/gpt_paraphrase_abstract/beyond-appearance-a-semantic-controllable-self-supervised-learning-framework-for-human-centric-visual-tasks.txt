The increasing interest in human-centric visual tasks has led to the development of a new method called SOLIDER, which aims to learn a general human representation from a large number of unlabeled human images. Unlike existing self-supervised learning methods, SOLIDER utilizes prior knowledge from human images to create pseudo semantic labels and incorporate more semantic information into the learned representation. Additionally, SOLIDER recognizes that different downstream tasks require varying levels of semantic and appearance information. To address this, SOLIDER introduces a conditional network with a semantic controller that allows users to adjust the ratio of semantic information in the representations generated by the model. Six downstream human-centric visual tasks were used to evaluate SOLIDER, and it outperformed existing methods and established new benchmarks for these tasks. The code for SOLIDER is available at https://github.com/tinyvision/SOLIDER.