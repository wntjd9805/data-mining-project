The use of contrastive image-text pretrained models, such as CLIP, for video classification has become popular due to its cost-effectiveness and competitive performance. However, recent studies in this field face a dilemma. Fine-tuning the pretrained model to achieve strong supervised performance results in poor zero-shot generalization. On the other hand, freezing the backbone to maintain zero-shot capability leads to a significant drop in supervised accuracy. To address this issue, existing research often trains separate models for supervised and zero-shot action recognition. In this study, we propose a multimodal prompt learning approach that aims to balance supervised and zero-shot performance within a single unified training framework. Our vision-based prompting approach considers three aspects: 1) Global video-level prompts to capture the data distribution, 2) Local frame-level prompts to provide per-frame discriminative conditioning, and 3) a summary prompt to extract a condensed video representation. Additionally, we introduce a prompting scheme for the text side to enhance the textual context. By employing this prompting scheme, we achieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51, and UCF101 datasets, while remaining competitive in the supervised setting. By keeping the pretrained backbone frozen, we optimize a much smaller number of parameters and retain the existing general representation, which contributes to the strong zero-shot performance. Our codes and models will be made available at https://github.com/TalalWasim/Vita-CLIP.