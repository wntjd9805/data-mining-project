Understanding dense action in videos is a fundamental challenge in developing vision models that can generalize well. Previous studies have shown that compositionality, or the ability to combine known primitive elements, is crucial for achieving generalization, particularly when dealing with novel composited structures. Compositional temporal grounding involves localizing dense action in videos by using known words in novel combinations, expressed as query sentences, to guide the grounding process. Existing approaches either learn composition from pairs of whole videos and language embeddings or focus on learning fine-grained semantic correspondences between word-level primitive elements. However, these approaches overlook the granularity of compositions, which can vary based on the query and video segments. To address this, we propose a method that learns a coarse-to-fine compositional representation by decomposing the original query sentence into different granular levels and learning the correct correspondences between the video and recombined queries using a contrastive ranking constraint. We also employ a coarse-to-fine temporal boundary prediction for precise grounding boundary detection. Our experiments on Charades-CG and ActivityNet-CG datasets demonstrate the superior compositional generalizability of our approach.