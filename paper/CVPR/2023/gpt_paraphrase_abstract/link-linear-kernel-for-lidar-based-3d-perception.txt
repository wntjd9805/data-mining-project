Extending the success of 2D Large Kernel to 3D perception is a difficult task due to the increasing processing overhead and optimization challenges caused by data scarcity and sparsity. Previous attempts to scale up the kernel size have been limited by modest block sizes, preventing the use of larger kernels like 21 × 21 × 21. In this study, we propose a new method called LinK, which addresses this issue and allows for a wider-range perception receptive field. LinK achieves this through two core designs: replacing the static kernel matrix with a linear kernel generator that provides adaptive weights for non-empty voxels, and reusing pre-computed aggregation results in overlapped blocks to reduce computation complexity. Our method enables each voxel to perceive context within a range of 21 × 21 × 21. We conducted extensive experiments on 3D object detection and 3D semantic segmentation tasks, which demonstrated the effectiveness of our approach. Notably, we achieved the top rank on the public leaderboard of the nuScenes 3D detection benchmark (LiDAR track) by incorporating a LinK-based backbone into the basic detector, CenterPoint. Furthermore, we improved the mIoU of the strong segmentation baseline by 2.7% in the SemanticKITTI test set. The code for our method is available at https://github.com/MCG-NJU/LinK.