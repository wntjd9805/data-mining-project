The task of detecting and describing key-points is essential in visual applications. However, the labels generated from image correspondences can result in false samples, hindering accurate matching. To address this, we propose learning transformation-predictive representations using self-supervised contrastive learning. We maximize similarity between corresponding views of the same landmark, avoiding collapsing solutions. Additionally, we soften hard positive labels through self-supervised generation learning and curriculum learning. This helps overcome training bottlenecks and facilitates training under stronger transformations. Our self-supervised pipeline reduces computation load and memory usage, outperforming state-of-the-art methods on image matching benchmarks with excellent generalization capabilities on downstream tasks.