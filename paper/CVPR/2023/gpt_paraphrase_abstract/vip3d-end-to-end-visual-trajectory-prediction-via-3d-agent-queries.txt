Autonomous driving systems typically consist of separate modules for perception and prediction. These modules interact by using selected features like agent bounding boxes and trajectories. However, this separation limits the amount of information that the prediction module receives from perception. Additionally, errors from perception can accumulate and negatively impact the accuracy of predictions.To address these issues, we present ViP3D, a novel approach to visual trajectory prediction. ViP3D utilizes raw videos to directly forecast future trajectories of agents in a scene. It employs sparse agent queries throughout the pipeline for detection, tracking, and prediction, making it the first fully differentiable vision-based trajectory prediction method. Unlike traditional approaches that rely on historical feature maps and trajectories, ViP3D encodes useful information from previous timestamps in agent queries, resulting in a streamlined streaming prediction technique.Extensive experiments conducted on the nuScenes dataset demonstrate the superior performance of ViP3D compared to traditional pipelines and previous end-to-end models in terms of vision-based prediction.