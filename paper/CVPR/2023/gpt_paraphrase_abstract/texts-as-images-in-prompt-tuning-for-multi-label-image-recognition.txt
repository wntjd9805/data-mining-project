Prompt tuning has proven to be an effective method for adapting large vision-language pre-trained models to different tasks with limited data or labels. However, current methods require visual data for learning prompts. In this study, we propose the use of image-text contrastive learning to align text and images, allowing us to treat texts as images for prompt tuning. Text descriptions are easier to collect and their class labels can be directly derived. We specifically apply this approach to multi-label image recognition, using sentences as alternatives to images for prompt tuning. We also introduce double-grained prompt tuning (TaI-DPT) to extract both coarse-grained and fine-grained embeddings, improving the performance of multi-label recognition. Our experimental results demonstrate that TaI-DPT significantly outperforms zero-shot CLIP on multiple benchmarks, such as MS-COCO, VOC2007, and NUS-WIDE. Additionally, TaI-DPT can be combined with existing image prompting methods to further enhance recognition performance. The code for our approach is available at https://github.com/guozix/TaI-DPT.