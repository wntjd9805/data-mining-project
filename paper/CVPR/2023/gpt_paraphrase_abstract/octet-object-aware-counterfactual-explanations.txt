Nowadays, the use of deep vision models in safety-critical applications, such as autonomous driving, has raised concerns about the explainability of these models. Counterfactual explanations, which aim to identify minimal and understandable changes to input images that would alter the model's output, have been proposed as a solution. However, existing methods struggle to explain decision models trained on images with multiple objects, such as urban scenes, which are both challenging and crucial to explain.In this study, we propose an object-centric framework for generating counterfactual explanations to address this issue. Drawing inspiration from recent generative modeling approaches, our method encodes the query image into a latent space that allows for easy manipulation at the object level. By doing so, it enables end-users to control the search directions for generating counterfactuals, such as spatial displacement of objects or style modifications. We evaluate our method on counterfactual explanation benchmarks for driving scenes and demonstrate its adaptability beyond classification tasks, including explaining semantic segmentation models.To further analyze the effectiveness of counterfactual explanations in aiding understanding of decision models, we conduct a user study. This study measures the usefulness of our proposed method and its impact on comprehension. For those interested, the code for our method is available at https://github.com/valeoai/OCTET.