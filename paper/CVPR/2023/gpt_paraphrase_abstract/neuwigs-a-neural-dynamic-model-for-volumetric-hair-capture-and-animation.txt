Creating realistic avatars for virtual reality is a challenging task, particularly when it comes to capturing and animating human hair. Hair poses difficulties due to its complex geometry, appearance, and motion. This paper presents a two-stage approach to address these challenges in a data-driven manner. The first stage, known as state compression, utilizes an autoencoder-as-a-tracker strategy to learn a low-dimensional latent space of 3D hair states, encompassing both motion and appearance. To enhance the separation of hair and head during appearance learning, multi-view hair segmentation masks are employed alongside a differentiable volumetric renderer. In the second stage, a novel hair dynamics model is optimized, enabling temporal hair transfer based on the discovered latent codes. To ensure stability in driving the dynamics model, a 3D point-cloud autoencoder from the compression stage is utilized for denoising the hair state. The proposed model surpasses existing approaches in synthesizing novel views and is capable of generating unique hair animations without depending on observed hair movements as a driving signal.