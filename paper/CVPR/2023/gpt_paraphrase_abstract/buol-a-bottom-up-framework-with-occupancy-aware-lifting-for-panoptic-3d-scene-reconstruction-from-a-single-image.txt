This abstract discusses the challenge of understanding and modeling a 3D scene from a single image. A recent advancement in this field is the panoptic 3D scene reconstruction task, which combines 3D reconstruction and panoptic segmentation from a single image. However, current approaches only focus on top-down methods that fill 2D instances into 3D voxels based on estimated depth, leading to two ambiguities. The first ambiguity is the instance-channel ambiguity, where variable instance ids in each scene result in confusion during the filling of voxel channels with 2D information. This hinders the subsequent 3D refinement process. The second ambiguity is the voxel-reconstruction ambiguity, where lifting 2D information to 3D regions using single view depth only propagates information onto the surface of the regions, causing ambiguity in reconstructing regions behind the frontal view surface. To address these issues, this paper proposes a framework called BUOL (Bottom-Up framework with Occupancy-aware Lifting) for panoptic 3D scene reconstruction from a single image. To tackle the instance-channel ambiguity, the bottom-up framework lifts 2D information to 3D voxels based on deterministic semantic assignments rather than arbitrary instance id assignments. The 3D voxels are then refined and grouped into 3D instances using the predicted 2D instance centers. For the voxel-reconstruction ambiguity, the estimated multi-plane occupancy is utilized along with depth to fill the entire regions of objects and background. The proposed method outperforms state-of-the-art approaches on both synthetic dataset 3D-Front and real-world dataset Matterport3D. The code and models of the method will be made available.