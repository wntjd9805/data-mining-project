The researchers in this study focus on Source-free Unsupervised Domain Adaptation (SF-UDA), which involves adapting a model to a target domain without using any source data. They propose a new approach for SF-UDA that addresses the issue of noisy pseudo-labels by reweighting the classification loss based on the reliability of these labels. They also refine the pseudo-labels by incorporating knowledge from neighboring samples and use a self-supervised contrastive framework to improve knowledge aggregation. To deal with noise in the pseudo-labels, they introduce a negative pairs exclusion strategy. The proposed method outperforms previous approaches on three benchmark datasets, achieving new state-of-the-art results. It demonstrates robustness to noise and produces more accurate pseudo-labels compared to existing methods.