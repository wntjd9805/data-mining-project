Diffusion models have emerged as a promising alternative to Generative Adversarial Networks (GANs) for generating high-quality images. However, their slow training and inference speed have limited their use in real-time applications. A recent method called DiffusionGAN has improved the running time of diffusion models by reducing the number of sampling steps, but they still lag behind GANs in terms of speed. This paper introduces a new approach called wavelet-based diffusion scheme to narrow the speed gap. The proposed method utilizes wavelet decomposition to extract low and high-frequency components from both the image and feature levels. These components are then processed adaptively to achieve faster generation while maintaining good quality. Additionally, the paper suggests incorporating a reconstruction term to enhance model training convergence. Experimental results on various datasets demonstrate that our solution is a step towards achieving real-time and high-fidelity diffusion models. The code and pre-trained checkpoints for our method can be found at https://github.com/VinAIResearch/WaveDiff.git.