Pre-trained vision-language models (VLMs) are widely used to align visual and textual representations. However, existing object detectors only align individual regions with the VLMs, neglecting the compositional structure of semantic concepts in a scene. Our proposed method aligns the embedding of a bag of regions as a whole, treating the regions as words in a sentence. These embeddings are then sent to the text encoder of a VLM, which learns to align them with the corresponding features extracted from the VLM. We applied this approach to Faster R-CNN and achieved improved results on open-vocabulary COCO and LVIS benchmarks. Our method outperforms previous approaches by 4.6 box AP50 and 2.8 mask AP, respectively. The code and models for our approach are available at https://github.com/wusize/ovdet.