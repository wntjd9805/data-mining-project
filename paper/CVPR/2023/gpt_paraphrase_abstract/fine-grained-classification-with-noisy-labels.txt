Learning with noisy labels (LNL) is a technique used to ensure that machine learning models can generalize well even when the training data contains incorrect or noisy labels. This study focuses on LNL specifically in the context of fine-grained datasets (LNL-FG), where the presence of subtle differences between classes leads to a higher level of label noise. The researchers found that existing methods for LNL are not effective in addressing label noise in LNL-FG scenarios, highlighting the need for new solutions.To address this, the researchers propose a new framework called stochastic noise-tolerated supervised contrastive learning (SNSCL). This framework tackles label noise by encouraging the learning of distinguishable representations. They introduce a noise-tolerated supervised contrastive learning loss that incorporates a weight-aware mechanism for correcting noisy labels and selectively updating momentum queue lists. This mechanism helps mitigate the impact of noisy anchors and prevents the insertion of noisy labels into the momentum-updated queue.Additionally, the researchers propose an efficient stochastic module that samples feature embeddings from a generated distribution. This module eliminates the need for manually-defined augmentation strategies in contrastive learning and enhances the representation ability of deep models.SNSCL is a general framework that can be combined with existing robust LNL strategies to improve their performance in LNL-FG scenarios. The researchers conducted extensive experiments to demonstrate the effectiveness of SNSCL in reducing the impact of label noise in fine-grained datasets.