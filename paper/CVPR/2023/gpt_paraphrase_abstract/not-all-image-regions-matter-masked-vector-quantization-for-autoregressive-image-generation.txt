In current autoregressive models for image generation, a codebook is first created in the latent space for image reconstruction, and then the image generation is completed based on this codebook. However, the existing codebook learning approach lacks the ability to distinguish the importance of different local regions in images. This leads to redundancy in the learned codebook, which not only limits the autoregressive model's ability to capture important structures but also increases training cost and slows down generation speed. To address this issue, we propose a two-stage framework that incorporates the concept of importance perception from classical image coding theory. Our framework, called Masked Quantization VAE (MQ-VAE) and Stackformer, aims to eliminate redundancy in the model. MQ-VAE includes an adaptive mask module that masks redundant region features before quantization, and an adaptive de-mask module that recovers the original grid image feature map for accurate image reconstruction. Stackformer, on the other hand, learns to predict the combination of the next code and its position in the feature map. Through comprehensive experiments on various image generation tasks, we demonstrate the effectiveness and efficiency of our approach. The code for our framework can be found at the following GitHub repository: https://github.com/CrossmodalGroup/MaskedVectorQuantization.