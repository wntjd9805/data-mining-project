In the field of autonomous driving, constructing a semantic map from a bird's-eye view (BEV) is crucial. While LiDAR provides accurate 3D observations for projection onto the BEV space, the resulting LiDAR-based BEV feature often contains noisy and indistinct spatial features. To address this, we propose a LiDAR-based method for semantic map construction. Our method utilizes a BEV pyramid feature decoder that learns robust multi-scale BEV features, thus improving the accuracy of the LiDAR-based approach. Additionally, we introduce an online Camera-to-LiDAR distillation scheme to incorporate semantic information from images into the LiDAR data. This scheme involves distillation at both the feature-level and logit-level to enhance the semantic learning process. We evaluate our proposed method, LiDAR2Map, on the challenging nuScenes dataset, and it outperforms previous LiDAR-based methods by 27.9% in mean Intersection over Union (mIoU). Furthermore, LiDAR2Map even surpasses state-of-the-art camera-based approaches. The source code for our method is available at: https://github.com/songw-zju/LiDAR2Map.