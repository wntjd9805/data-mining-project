Two promising methods for training image encoders are self supervision and natural language supervision. Previous research has suggested that combining these approaches can be effective, but the results have been based on small pre-training datasets. In this study, we investigate whether this approach can still be effective with a much larger dataset. We find that combining masked auto-encoders and contrastive language image pre-training provides a benefit over contrastive language image pre-training alone when trained on 11.3 million image-text pairs. However, when trained on a larger dataset of 1.4 billion images, there is little to no benefit compared to training with contrastive language image pre-training alone. This research sheds light on the effectiveness of self supervision for large-scale image-text training.