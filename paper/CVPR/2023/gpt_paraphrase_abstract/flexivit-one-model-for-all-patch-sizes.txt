This paper explores the use of Vision Transformers, which convert images into sequences by dividing them into patches. The size of these patches determines the tradeoff between speed and accuracy, with smaller patches offering higher accuracy but requiring more computational resources. Traditionally, changing the patch size would require retraining the model. However, the authors demonstrate that by randomizing the patch size during training, a single set of weights can be obtained that performs well across a wide range of patch sizes. This approach allows for tailoring the model to different computational budgets. The authors acknowledge that all contributors made significant technical contributions, with Lucas leading the project. The research was conducted at Google Brain and NYU.