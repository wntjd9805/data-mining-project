We propose a unified structured light approach using an LED array and an LCD mask to capture both shape and reflectance from a single view. To encode spatial information, one LED projects learned mask patterns, and the decoded results from multiple LEDs are combined to generate a final depth map. For reflectance, learned light patterns pass through a transparent mask to efficiently capture angularly-varying reflectance. Per-point BRDF parameters are optimized based on corresponding measurements and stored as texture maps for the final reflectance. We introduce a differentiable pipeline to jointly optimize the mask and light patterns for optimal acquisition quality. Our approach is demonstrated on various physical objects and achieves comparable results to state-of-the-art techniques.