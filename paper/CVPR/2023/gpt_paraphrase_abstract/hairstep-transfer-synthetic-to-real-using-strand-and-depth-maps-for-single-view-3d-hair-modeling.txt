This study addresses the challenging problem of learning-based single-view 3D hair modeling. Collecting paired real image and 3D hair data is difficult, so using synthetic data as prior knowledge for the real domain is a common solution. However, this introduces the problem of domain gap. Existing methods use orientation maps instead of hair images to bridge this gap, but we argue that this approach is sensitive to noise and not a competent representation. Therefore, we propose a new intermediate representation called HairStep, which includes a strand map and a depth map. HairStep provides sufficient information for accurate 3D hair modeling and can be inferred from real images. We collected a dataset of 1,250 portrait images with two types of annotations and designed a learning framework to transfer real images to the strand map and depth map. Our dataset also includes the first quantitative metric for 3D hair modeling. Our experiments demonstrate that HairStep reduces the domain gap between synthetic and real data and achieves state-of-the-art performance in single-view 3D hair reconstruction.