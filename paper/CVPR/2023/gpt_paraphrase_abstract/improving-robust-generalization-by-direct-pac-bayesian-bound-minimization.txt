Recent research in robust optimization has highlighted a phenomenon similar to overfitting, where models trained to resist adversarial attacks demonstrate greater robustness on the training set compared to the test set. Although previous studies have provided theoretical explanations for this phenomenon using a robust PAC-Bayesian bound on the adversarial test error, the algorithmic derivations have had only loose connections to this bound. This suggests a gap between the empirical success of these algorithms and our understanding of adversarial robustness theory.To bridge this gap, this paper introduces a novel form of the robust PAC-Bayesian bound and directly minimizes it with respect to the model posterior. The optimal solution is derived by connecting PAC-Bayesian learning to the geometry of the robust loss surface through a Trace of Hessian (TrH) regularizer, which measures the flatness of the surface. In practical applications, the TrH regularizer is restricted to the top layer only, enabling an analytical solution to the bound that does not depend on the depth of the network.The proposed TrH regularization approach is evaluated on CIFAR-10/100 and ImageNet datasets using Vision Transformers (ViT), and is compared against baseline algorithms for adversarial robustness. Experimental results demonstrate that TrH regularization improves ViT robustness, either matching or surpassing previous state-of-the-art approaches, while also requiring less memory and computational cost.