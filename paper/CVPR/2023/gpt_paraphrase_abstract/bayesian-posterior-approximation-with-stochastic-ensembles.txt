We present a method of using ensembles of stochastic neural networks to approximate the Bayesian posterior. By combining stochastic techniques like dropout with deep ensembles, we create stochastic ensembles that are formulated as families of distributions. These ensembles are trained using variational inference to approximate the Bayesian posterior. We implement stochastic ensembles using Monte Carlo dropout, DropConnect, and a new non-parametric version of dropout. We evaluate the performance of these ensembles on a toy problem and CIFAR image classification. To assess the accuracy of the posteriors, we compare them to Hamiltonian Monte Carlo simulations. Our results demonstrate that stochastic ensembles provide more precise posterior estimates compared to other commonly used methods for Bayesian inference.