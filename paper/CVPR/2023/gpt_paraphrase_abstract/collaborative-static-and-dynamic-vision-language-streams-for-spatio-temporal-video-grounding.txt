Spatio-Temporal Video Grounding (STVG) is a challenging task that involves localizing a target object in both space and time based on a given language query. This requires the model to understand both dynamic visual cues (such as motion) and static visual cues (such as object appearances) in the language description. To address this, we propose a new framework that utilizes both static and dynamic vision-language streams to collectively reason about the target object. The static stream focuses on cross-modal understanding within a single frame and learns to spatially attend to the target object based on visual cues like object appearances. On the other hand, the dynamic stream models the visual-linguistic dependencies across multiple consecutive frames to capture dynamic cues like motions. To enable collaboration between the two streams, we introduce a novel cross-stream collaborative block. This block allows the static and dynamic streams to transfer useful and complementary information from each other, facilitating collaborative reasoning. Experimental results demonstrate the effectiveness of this collaboration, and our framework achieves new state-of-the-art performance on both HCSTVG and VidSTG datasets.