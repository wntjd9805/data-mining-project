This study addresses the challenge of estimating a detailed depth map from a single viewpoint, which is known to be difficult due to the lack of information. Existing methods rely on deep neural networks that learn the relationship between depth and visual appearance. Another approach, called Structure from Motion (SfM), uses multiple views to create accurate but sparse maps. However, SfM is limited by the availability of discriminative textures. To overcome these limitations, the researchers propose a new method called SfM-TTR, which combines the strengths of both approaches. SfM-TTR uses sparse SfM point clouds as a self-supervisory signal during test time, fine-tuning the network to improve its representation of the scene. The results show that incorporating SfM-TTR into existing self-supervised and supervised networks significantly improves their performance compared to previous methods based on multi-view consistency. The code for SfM-TTR is available at the provided GitHub link.