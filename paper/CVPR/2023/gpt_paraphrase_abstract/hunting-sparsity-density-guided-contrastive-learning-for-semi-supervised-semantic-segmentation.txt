Recent approaches to semi-supervised semantic segmentation involve combining pseudo labeling and consistency regularization to improve model generalization. However, this study suggests that supervision can be obtained directly from the geometry of the feature space. Drawing inspiration from density-based unsupervised clustering, the authors propose leveraging feature density to identify sparse regions within feature clusters defined by labels and pseudo labels. The hypothesis is that less dense features are not as well-trained as densely clustered ones. To address this, the authors propose a Density-Guided Contrastive Learning (DGCL) strategy that encourages anchor features in sparse regions to move towards cluster centers represented by high-density positive keys. The key component of their method is estimating feature density, which is defined as neighbor compactness. They design a multi-scale density estimation module that uses multiple nearest-neighbor graphs for more robust density modeling. Additionally, they propose a unified training framework that combines label-guided self-training and density-guided geometry regularization to provide complementary supervision on unlabeled data. Experimental results on PAS-CAL VOC and Cityscapes datasets demonstrate that their approach achieves state-of-the-art performance. The project code is available at the given GitHub link.