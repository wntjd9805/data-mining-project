This paper presents a novel method for estimating the scale of a single image using a new scale field representation. The scale field defines the local pixel-to-metric conversion ratio along the gravity direction for all ground pixels. This representation helps overcome the ambiguity in camera parameters and enables the collection of scale annotations from human annotators using a simple and effective approach. The proposed model is trained on calibrated panoramic image data and in-the-wild human annotated data, resulting in a robust scale field that can be used in different applications such as 3D understanding and scale-aware image editing.