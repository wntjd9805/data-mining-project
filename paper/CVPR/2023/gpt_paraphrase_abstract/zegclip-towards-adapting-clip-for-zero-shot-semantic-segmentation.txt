Recently, a two-stage scheme using CLIP has been utilized for pixel-level zero-shot learning tasks. This involves generating class-agnostic region proposals and then using CLIP for image-level zero-shot classification. However, this approach is complicated and computationally expensive as it requires two image encoders. In this study, we propose a simpler and more efficient one-stage solution that extends CLIP's zero-shot prediction capability to the pixel level. Our baseline approach involves generating semantic masks by comparing the similarity between text and patch embeddings from CLIP. However, this approach tends to overfit to seen classes and fails to generalize to unseen classes. To address this issue, we propose three simple yet effective modifications that significantly improve the zero-shot capacity and pixel-level generalization ability of CLIP. These modifications result in an efficient zero-shot semantic segmentation system called ZegCLIP. Through extensive experiments on three public benchmarks, ZegCLIP outperforms state-of-the-art methods by a large margin in both "inductive" and "transductive" zero-shot settings. Additionally, compared to the two-stage method, our one-stage ZegCLIP achieves a speedup of approximately five times during inference. The code for ZegCLIP is available at https://github.com/ZiqinZhou66/ZegCLIP.git.