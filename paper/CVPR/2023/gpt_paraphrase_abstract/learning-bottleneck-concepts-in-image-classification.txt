Interpreting and explaining the behavior of deep neural networks is crucial, but it often requires expert knowledge. Explainable AI offers a solution by providing per-pixel relevance to decision-making. However, understanding these explanations can still be challenging. Recently, some approaches have adopted a concept-based framework to establish a higher-level relationship between concepts and model decisions. This study introduces the Botleneck Concept Learner (BotCL), which represents an image solely based on the presence or absence of concepts learned during training without explicit supervision. BotCL utilizes self-supervision and tailored regularizers to ensure that the learned concepts are easily understandable to humans. Through experiments on image classification tasks, we demonstrate BotCL's potential to enhance neural networks for improved interpretability.