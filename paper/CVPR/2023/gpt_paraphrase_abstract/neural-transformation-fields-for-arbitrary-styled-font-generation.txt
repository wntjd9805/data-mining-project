Few-shot font generation (FFG) has gained significant attention in recent years due to its academic and commercial value. Traditionally, FFG approaches have focused on the style-content disentanglement paradigm, where font styles are transferred to characters by combining source character content representations with style codes from reference samples. However, existing methods primarily rely on exploring powerful style representations, which may not be optimal for the FFG task as they fail to model spatial transformation in style transfer. In this study, we propose a novel approach that models font generation as a continuous transformation process. We view the generation of the target font image as a transformation from the source character image, involving the creation and dissipation of font pixels. To achieve this, we embed the corresponding transformations into a neural transformation field. Our method utilizes the estimated transformation path to generate a series of intermediate transformation results through a sampling process. These intermediate results are then accumulated into the final target font image using a font rendering formula. Extensive experiments demonstrate that our approach outperforms existing methods, establishing state-of-the-art performance in the few-shot font generation task. The effectiveness of our proposed model is evident, and our implementation is publicly available at https://github.com/fubinfb/NTF.