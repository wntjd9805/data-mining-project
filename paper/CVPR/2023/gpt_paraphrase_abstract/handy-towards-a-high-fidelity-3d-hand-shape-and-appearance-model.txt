In recent years, there has been significant research focused on modeling, tracking, and reconstructing human hands with the emergence of virtual and augmented reality. Hands are crucial for expressing human behavior, but they pose challenges in terms of reconstruction. Most current methods rely on the low polygon MANO model, which has limitations in terms of expressiveness and shape reconstruction constraints due to its training on a small dataset of only 31 adult subjects. Additionally, hand appearance has been largely neglected in hand reconstruction methods.To address these limitations, we propose "Handy," a large-scale model of the human hand that includes both shape and appearance. This model is composed of over 1200 subjects and is publicly available for the research community. Unlike current models, our proposed hand model is trained on a dataset that encompasses a wide range of age, gender, and ethnicity, allowing for accurate reconstruction of out-of-distribution samples. We also developed a high-quality texture model by training a powerful GAN that preserves high-frequency details and generates high-resolution hand textures.To demonstrate the capabilities of our proposed model, we created a synthetic dataset of textured hands and trained a hand pose estimation network to reconstruct both the shape and appearance from single images. Through extensive quantitative and qualitative experiments, our model proves to be robust against state-of-the-art methods and realistically captures the 3D hand shape and pose, along with detailed textures, even in challenging "in-the-wild" conditions.