Recent advancements in 3D generative models have shown impressive capabilities in creating high-resolution photorealistic images with consistent views and detailed 3D shapes. However, training these models for different domains poses a challenge due to the need for a large number of training images and their corresponding camera distribution information. Text-guided domain adaptation methods have emerged as a solution to this problem by leveraging Contrastive Language-Image Pre-training (CLIP) to convert a 2D generative model from one domain to another with different styles, without the need for extensive datasets. Nonetheless, these methods have a drawback in that they do not preserve the sample diversity of the original generative model in the domain-adapted models, mainly because of the deterministic nature of the CLIP text encoder. Adapting text-guided domain adaptation to 3D generative models is even more challenging due to catastrophic diversity loss, inferior text-image correspondence, and poor image quality. To address these issues, we introduce DATID-3D, a domain adaptation method specifically designed for 3D generative models using text-to-image diffusion models. This approach allows for the synthesis of diverse images based on a given text prompt without requiring additional images or camera information from the target domain. Unlike previous methods, our novel pipeline fine-tunes state-of-the-art 3D generators from the source domain to generate high-resolution, multi-view consistent images in text-guided targeted domains, surpassing existing text-guided domain adaptation methods in terms of diversity and text-image correspondence. Additionally, we propose and demonstrate various 3D image manipulations, such as one-shot instance-selected adaptation and single-view manipulated 3D reconstruction, to fully exploit the diversity present in text prompts.