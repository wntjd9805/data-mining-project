Despite advancements in deep generative models, creating high-resolution and temporally coherent videos remains challenging due to their complexity and large size. Previous diffusion models have shown promise in addressing this challenge but are limited by computational and memory inefficiencies. To overcome these limitations, we propose a new generative model called the projected latent video diffusion model (PVDM). PVDM is a probabilistic diffusion model that learns a video distribution in a low-dimensional latent space, allowing for efficient training with limited resources. It consists of two components: an autoencoder that converts videos into 2D-shaped latent vectors, and a specialized diffusion model architecture designed for the factorized latent space. This model can generate videos of any length using a single model. Experimental results on popular video generation datasets demonstrate that PVDM outperforms previous methods. For instance, on the UCF-101 long video generation benchmark, PVDM achieves an FVD score of 639.7, surpassing the prior state-of-the-art score of 1773.4.