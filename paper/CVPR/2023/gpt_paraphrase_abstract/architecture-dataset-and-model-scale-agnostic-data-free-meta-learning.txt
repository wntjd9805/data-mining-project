Data-free meta-learning aims to acquire valuable prior knowledge from a set of pre-trained models without accessing their training data. However, existing approaches solely address this problem in parameter space, disregarding the valuable information contained within the pre-trained models and lacking scalability for large-scale models. Moreover, these approaches are limited to meta-learning models with the same network architecture. In order to tackle these limitations, we propose a unified framework called PURER. This framework consists of two components: ePisode cUrriculum inveRsion (ECI) during data-free meta training, and invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, our ECI approach enables pseudo episode training, allowing for fast adaptation to new unseen tasks. By synthesizing a sequence of pseudo episodes and progressively increasing their difficulty based on real-time feedback from the meta model, ECI effectively learns to adapt. The optimization process of meta training with ECI is formulated as an adversarial problem, enabling end-to-end learning. During meta testing, we introduce ICFIL as a supplementary method to narrow the gap between the task distribution in meta training and meta testing. ICFIL is a plug-and-play technique that is solely used during meta testing. Through extensive experiments in various real-world scenarios, we demonstrate the superior performance of our proposed framework.