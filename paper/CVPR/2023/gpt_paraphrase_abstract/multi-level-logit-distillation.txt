Knowledge Distillation (KD) is a technique used to transfer knowledge from a large teacher model to a smaller student model. Existing KD methods can be categorized into two types: logit distillation and feature distillation. Logit distillation is easy to implement but lacks performance, while feature distillation may not be suitable for certain practical scenarios due to concerns about privacy and safety. In this study, we propose a stronger logit distillation method by leveraging logit outputs more effectively. Our approach involves multi-level prediction alignment, which not only aligns predictions at the instance level but also at the batch and class levels. This allows the student model to learn instance prediction, input correlation, and category correlation simultaneously. Additionally, we introduce a prediction augmentation mechanism based on model calibration to further improve performance. Through extensive experiments, we demonstrate that our method consistently outperforms previous logit distillation methods and even achieves competitive performance compared to mainstream feature distillation methods. The code for our approach is available at [URL].