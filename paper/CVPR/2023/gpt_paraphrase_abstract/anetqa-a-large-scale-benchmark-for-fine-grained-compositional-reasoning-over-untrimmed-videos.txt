Creating benchmarks to systematically analyze the capabilities of video question answering (VideoQA) models is both challenging and crucial. Current benchmarks often consist of simple questions that are not compositional and suffer from language biases, making it difficult to effectively identify model weaknesses. However, a recent benchmark called AGQA has shown promise by automatically generating QA pairs from pre-annotated scene graphs, allowing for measurement of diverse reasoning abilities with detailed control. Despite this, AGQA has limitations in reasoning about the fine-grained semantics in videos, as this information is not present in its scene graphs.To address this issue, we introduce ANetQA, a large-scale benchmark that supports fine-grained compositional reasoning over challenging untrimmed videos from ActivityNet. Similar to AGQA, the QA pairs in ANetQA are automatically generated from annotated video scene graphs. However, ANetQA offers several improvements: (i) it includes untrimmed videos with fine-grained semantics; (ii) it incorporates spatio-temporal scene graphs with detailed taxonomies; and (iii) it generates diverse questions from fine-grained templates. ANetQA consists of 1.4 billion unbalanced and 13.4 million balanced QA pairs, making it significantly larger than AGQA, despite having a similar number of videos.Comprehensive experiments were conducted using state-of-the-art methods on ANetQA. The best model achieved an accuracy of 44.5%, while human performance reached 84.5%, indicating room for improvement. It is worth noting that the answer format in ANetQA provides only the abstraction, without additional details.