Human action recognition is a task that involves classifying the category of a human action based on a segment of a video. In recent years, researchers have focused on developing models based on Graph Convolutional Networks (GCNs) to extract features from skeletons, as skeleton representations are more efficient and robust compared to other modalities like RGB frames. However, when using skeleton data, important clues such as related items are often discarded, leading to ambiguous actions that are difficult to distinguish and prone to misclassification.To address this issue, we propose an auxiliary feature refinement head (FR Head) that consists of spatial-temporal decoupling and contrastive feature refinement. This approach aims to obtain discriminative representations of skeletons by dynamically discovering and calibrating ambiguous samples in the feature space. Additionally, FR Head can be applied at different stages of GCNs to provide stronger supervision through multi-level refinement.We conducted extensive experiments on three datasets: NTU RGB+D, NTU RGB+D 120, and NW-UCLA. Our proposed models achieved competitive results compared to state-of-the-art methods and were able to better discriminate ambiguous samples. The codes for our models are available at https://github.com/zhysora/FR-Head.Figure 1 illustrates the problem with recognizing certain actions using skeleton representations. Due to the lack of important interactive objects and contexts in the skeleton data, these actions are easily confused with each other.