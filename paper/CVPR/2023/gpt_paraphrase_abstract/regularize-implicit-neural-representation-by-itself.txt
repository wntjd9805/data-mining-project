This paper introduces a regularizer called Implicit Neural Representation Regularizer (INRR) to enhance the generalization ability of the Implicit Neural Representation (INR). The INR is a fully connected network that can accurately represent signals without being limited by grid resolution. However, its generalization ability can be improved, particularly with non-uniformly sampled data. The proposed INRR is based on the learned Dirichlet Energy (DE), which measures the similarities between rows/columns of the matrix. The smoothness of the Laplacian matrix is further incorporated by parameterizing DE with a small INR. By effectively integrating the signal's self-similarity with the smoothness of the Laplacian matrix, INRR enhances the generalization of INR in signal representation. The paper also presents various properties derived from INRR through carefully designed numerical experiments, including convergence trajectory and multi-scale similarity in momentum methods. Additionally, the proposed method demonstrates the potential to improve the performance of other signal representation techniques.