Automatically synthesizing co-speech gestures for virtual characters is increasingly in demand. However, this task remains challenging due to the complex connection between input speeches and desired gestures. Previous approaches have focused on predicting the best fitting gesture based on the data, but they lack the ability to plan for future gestures. To address this, we introduce a novel reinforcement learning framework called RACER. RACER utilizes a vector quantized variational autoencoder to learn concise representations of gestures and a policy architecture based on GPT to generate coherent sequences of gestures in an autoregressive manner. We propose a contrastive pre-training approach to calculate rewards, which incorporates contextual information into action evaluation and effectively captures the intricate relationships between speech and gesture data. Experimental results demonstrate that our method outperforms existing baselines in terms of objective metrics and subjective human judgments. For demonstrations, please visit https://github.com/RLracer/RACER.git.