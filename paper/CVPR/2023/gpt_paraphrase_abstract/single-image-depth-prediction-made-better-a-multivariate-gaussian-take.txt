Neural-network-based single image depth prediction (SIDP) is a challenging task that aims to predict per-pixel depth in a scene. This problem is ill-posed and requires an approach that can accurately model scene depth from training examples. Most existing techniques predict a single scalar depth value per pixel, but these models have limitations in terms of accuracy and precision. Therefore, an SIDP approach should consider the expected depth variations and uncertainties in its predictions. In this study, we propose a continuous modeling approach for per-pixel depth prediction. We use a multivariate Gaussian distribution to model the per-pixel scene depth and its distribution. Unlike previous methods that assume independence between per-pixel depths, we introduce per-pixel covariance modeling to capture depth dependencies among all scene points. However, this leads to a computationally expensive continuous loss function, which we efficiently solve using a learned low-rank approximation of the covariance matrix.Our approach, when tested on benchmark datasets like KITTI, NYU, and SUN-RGB-D, achieves state-of-the-art results in SIDP. Our method, named MG, performs among the top on the KITTI depth-prediction benchmark leaderboard.