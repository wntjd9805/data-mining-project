Current Siamese tracking methods rely on complex mechanisms to utilize temporal information, limiting their efficiency and practicality. This study introduces a sequence-level target matching approach that incorporates temporal context into spatial features using a feedforward video model. The standard video transformer architecture is adapted to enable spatiotemporal feature learning from frame-level patch sequences. To enhance tracking performance, spatiotemporal information is merged using sequential multi-branch triplet blocks within a video transformer backbone. Various model variants are compared, including tokenization strategies, hierarchical structures, and video attention schemes. Additionally, a disentangled dual-template mechanism is proposed to separate static and dynamic appearance clues and minimize temporal redundancy in video frames. Extensive experiments demonstrate that the proposed method, VideoTrack, achieves state-of-the-art results in real-time.