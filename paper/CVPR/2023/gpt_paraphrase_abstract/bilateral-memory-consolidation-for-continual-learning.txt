Humans excel at continuously acquiring and integrating new information, while deep models suffer from catastrophic forgetting, particularly when dealing with long task sequences. Drawing inspiration from the brain's ability to constantly rewrite and consolidate past memories, we propose a new framework called Bilateral Memory Consolidation (BiMeCo) that aims to enhance memory interaction capabilities. The key idea behind BiMeCo is to separate model parameters into short-term and long-term memory modules, responsible for representation ability and generalization across learned tasks, respectively. By promoting dynamic interactions between these two memory modules through knowledge distillation and momentum-based updating, BiMeCo prevents forgetting by forming generic knowledge. Furthermore, BiMeCo is parameter-efficient and can seamlessly integrate with existing methods. Extensive experiments on challenging benchmarks demonstrate that BiMeCo significantly improves the performance of existing continual learning methods. For instance, when combined with the state-of-the-art method CwD, BiMeCo achieves substantial gains of approximately 2% to 6% while utilizing only half the parameters on CIFAR-100 under ResNet-18.