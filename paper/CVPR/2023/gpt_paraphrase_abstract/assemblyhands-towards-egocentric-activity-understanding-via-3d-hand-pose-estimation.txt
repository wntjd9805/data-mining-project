We introduce AssemblyHands, a comprehensive dataset with precise 3D hand pose annotations. This dataset aims to facilitate research on egocentric activities that involve complex interactions between the hand and objects. It consists of synchronized ego-centric and exo-centric images taken from the Assembly101 dataset, which documents participants assembling and disassembling take-apart toys. To ensure accurate 3D hand pose annotations for the ego-centric images, we employ an efficient pipeline. This pipeline utilizes initial manual annotations to train a model that can automatically annotate a much larger dataset. Our annotation model combines multi-view feature fusion and an iterative refinement scheme, resulting in an average keypoint error of 4.20 mm. This error rate is 85% lower than the original annotations in Assembly101. AssemblyHands provides a substantial amount of annotated images, including 490,000 ego-centric images, making it the largest benchmark dataset for egocentric 3D hand pose estimation currently available. With this dataset, we establish a strong baseline for single-view 3D hand pose estimation from ego-centric images. Additionally, we introduce a novel action classification task to evaluate the predicted 3D hand poses. Our experiments demonstrate that higher-quality hand pose annotations significantly improve the accuracy of action recognition. Figure 1 illustrates the effectiveness of high-quality 3D hand poses in representing egocentric activity understanding. AssemblyHands offers these high-quality annotations, computed from multi-view exo-centric images in Assembly101, which originally had inaccurate annotations derived from ego-centric images (as shown by the incorrect left-hand pose prediction). Through our action classification task, we empirically demonstrate that models trained on high-quality annotations achieve substantially higher accuracy.