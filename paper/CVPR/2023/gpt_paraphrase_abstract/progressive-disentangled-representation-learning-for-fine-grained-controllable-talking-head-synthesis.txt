We present a new approach for synthesizing talking heads with precise control over various facial motions. Our method allows for separate control over lip motion, eye gaze and blink, head pose, and emotional expression. We achieve this by using disentangled latent representations and an image generator. To effectively separate each motion factor, we propose a progressive disentangled representation learning strategy. We first extract a unified motion feature from the driving signal and then isolate each fine-grained motion from this feature. For non-emotional motions, we use contrastive learning and regression techniques, while for emotional expression, we employ feature-level decorrelation and self-reconstruction. Through experiments, we demonstrate that our method achieves high-quality speech and lip-motion synchronization and enables precise control over multiple facial motions that was previously difficult to achieve.