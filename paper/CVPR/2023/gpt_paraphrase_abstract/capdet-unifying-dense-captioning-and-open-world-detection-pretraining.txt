By leveraging large-scale vision-language pre-training on image-text pairs, open-world detection methods have demonstrated strong generalization capabilities in zero-shot or few-shot detection scenarios. However, existing methods still require a predefined category space during the inference stage, limiting predictions to objects within that space. To address this limitation and create a truly open-world detector, this paper introduces a novel method called CapDet. CapDet is capable of making predictions based on a given category list or generating the category of predicted bounding boxes directly. This is achieved by integrating the open-world detection and dense caption tasks into a single framework, incorporating an additional dense captioning head to generate region-grounded captions. The inclusion of the captioning task also improves the generalization of detection performance, as the captioning dataset covers a broader range of concepts. Experimental results demonstrate that CapDet significantly outperforms baseline methods on the LVIS dataset, achieving a +2.1% mean Average Precision (mAP) on rare classes. Additionally, CapDet achieves state-of-the-art performance on dense captioning tasks, achieving a 15.44% mAP on VGV1.2 and 13.98% on the VG-COCO dataset.