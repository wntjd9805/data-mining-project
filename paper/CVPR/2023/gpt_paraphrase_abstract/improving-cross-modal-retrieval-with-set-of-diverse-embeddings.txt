Cross-modal retrieval between images and text poses challenges due to ambiguity. Images can depict multiple situations, and captions can be associated with different images. To address this problem, set-based embedding has been explored. This approach encodes a sample into a set of embedding vectors that capture various semantics. This paper introduces a new set-based embedding method with two distinct contributions. Firstly, a novel similarity function called smooth-Chamfer similarity is proposed to mitigate the limitations of existing similarity functions in set-based embedding. Secondly, a set prediction module is introduced, utilizing the slot attention mechanism to generate a set of embedding vectors that effectively capture diverse semantics. The proposed method is evaluated on the COCO and Flickr30K datasets using different visual backbones. It outperforms existing methods, including those with higher computational requirements during inference.