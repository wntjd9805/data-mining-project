Generating high-quality 3D head avatars from 2D videos is crucial for virtual human applications in the entertainment and telepresence industries. While linear 3D morphable models (3DMM) have been commonly used to represent these avatars, they are limited in capturing personalized facial features and expressions. Neural implicit fields offer a more powerful alternative by incorporating personalized shape, expressions, and facial parts such as hair and mouth interior. However, existing methods fail to accurately model fine-scale facial features or provide local control of facial parts, especially for asymmetric expressions in monocular videos. Additionally, these methods rely on 3DMM parameters with limited locality and struggle to capture local features effectively. To address these limitations, we introduce a novel approach that utilizes part-based implicit shape models to decompose the global deformation field into local ones. Our formulation incorporates multiple implicit deformation fields with local semantic rig-like control, leveraging 3DMM-based parameters and representative facial landmarks. We also introduce a local control loss and attention mask mechanism to encourage sparsity in each learned deformation field. Our approach significantly improves the quality of locally controllable nonlinear deformations, particularly for the mouth interior, asymmetric expressions, and facial details. More information about our project can be found at https://imaging.cs.cmu.edu/local-deformation-fields/.