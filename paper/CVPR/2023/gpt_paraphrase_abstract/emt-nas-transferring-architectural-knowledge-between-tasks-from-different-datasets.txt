The success of multi-task learning (MTL) is often attributed to the shared representation of related tasks, which allows models to generalize better. In deep learning, this is typically accomplished by using a common neural network architecture and jointly training the weights. However, training the weighting parameters jointly on multiple related tasks can sometimes lead to negative transfer, causing a decline in performance. To address this issue, this study introduces an evolutionary multi-tasking neural architecture search (EMT-NAS) algorithm to expedite the search process by transferring architectural knowledge across related tasks.Unlike traditional MTL methods, EMT-NAS assigns a personalized network for each task, enabling effective reduction of negative transfer. To mitigate performance fluctuations resulting from parameter sharing and mini-batch gradient descent training, a fitness re-evaluation method is proposed, ensuring that promising solutions are not lost during the search process. To rigorously evaluate EMT-NAS, classification tasks from various datasets, including CIFAR-10, CIFAR-100, and four MedMNIST datasets, are used in empirical assessments. Extensive comparative experiments across different numbers of tasks demonstrate that EMT-NAS requires 8% less time on CIFAR and up to 40% less time on MedMNIST to discover competitive neural architectures compared to its single-task counterparts.