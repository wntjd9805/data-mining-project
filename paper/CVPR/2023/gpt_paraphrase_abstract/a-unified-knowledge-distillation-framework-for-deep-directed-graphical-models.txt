The current methods of knowledge distillation (KD) have limitations when applied to deep directed graphical models (DGMs) with arbitrary layers of random variables. This study proposes a new knowledge distillation framework for deep DGMs in various applications. The framework utilizes the reparameterization trick to create a compact DGM by hiding intermediate latent variables. It also introduces a surrogate distillation loss to reduce error accumulation across multiple layers of random variables. The study evaluates the proposed framework on four applications: data-free hierarchical variational autoencoder (VAE) compression, data-free variational recurrent neural networks (VRNN) compression, data-free Helmholtz Machine (HM) compression, and VAE continual learning. The results demonstrate that the distillation method outperforms existing methods in data-free model compression tasks. Additionally, the study shows that the proposed method significantly improves the performance of KD-based continual learning for data generation. The source code for the framework is available at https://github.com/YizhuoChen99/KD4DGM-CVPR.