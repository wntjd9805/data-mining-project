The goal of Image-based Virtual Try-ON is to digitally place clothing on a person in a realistic manner. Current methods use a global warping module to account for the deformation of different parts of the garment. However, these methods struggle to preserve the semantic information of the garment when faced with challenging inputs, such as complex human poses or difficult garments. Additionally, these methods often warp the garment to align with the preserved region's boundary, leading to texture distortion due to squeezing. These limitations hinder the practical application of existing methods. To overcome these issues and make progress towards real-world virtual try-on, we introduce a General-Purpose Virtual Try-ON framework called GP-VTON. This framework incorporates a novel Local-Flow Global-Parsing (LFGP) warping module and a Dynamic Gradient Truncation (DGT) training strategy. Unlike the previous global warping mechanism, LFGP uses local flows to individually warp different parts of the garment and then combines them using global garment parsing. This approach produces realistic warped parts and maintains the semantic integrity of the garment even with challenging inputs. Our DGT training strategy dynamically truncates the gradient in the overlap area, eliminating the need for the warped garment to meet the boundary constraint and effectively avoiding texture squeezing problems. Furthermore, GP-VTON can be easily extended to handle multiple garment categories and can be jointly trained using data from different categories. Extensive experiments on two high-resolution benchmarks demonstrate the superiority of our method over existing state-of-the-art approaches.