Current techniques for single domain generalization (single-DG) in deep neural networks (DNNs) often rely on modality-specific data augmentation algorithms. These methods struggle to generalize well to out-of-distribution (OOD) data and are limited to a single modality. In contrast, our Modality-Agnostic Debiasing (MAD) framework addresses these limitations by introducing a versatile two-branch classifier. One branch focuses on identifying domain-specific features, while the other captures domain-generalized features. MAD can be easily integrated into existing single-DG models. We demonstrate the effectiveness of MAD in various single-DG scenarios involving different modalities, such as 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. Notably, MAD significantly improves accuracy and mIOU in recognition on 3D point clouds and semantic segmentation on 2D images, outperforming other methods by 2.82% and 1.5% respectively.