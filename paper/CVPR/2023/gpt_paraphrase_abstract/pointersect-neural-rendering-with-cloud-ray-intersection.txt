We present a new approach to rendering point clouds as surfaces, which offers several unique advantages. Our method is differentiable and does not require scene-specific optimization, allowing for surface normal estimation, rendering of room-scale point clouds, inverse rendering, and ray tracing with global illumination. Unlike previous methods that convert point clouds to other representations, our approach directly infers the intersection of a light ray with the underlying surface represented by the point cloud. This is achieved by training a set transformer that predicts the intersection point, surface normal, and material blending weights based on local neighborhood points along the light ray. By focusing on small neighborhoods, we can train a model with only 48 meshes and apply it to unseen point clouds. Our model outperforms state-of-the-art surface reconstruction and point-cloud rendering methods on three test sets, achieving higher estimation accuracy. When applied to room-scale point clouds without scene-specific optimization, our model produces results comparable to state-of-the-art novel-view rendering methods. Additionally, we demonstrate the ability to render and manipulate Lidar-scanned point clouds, including lighting control and object insertion.