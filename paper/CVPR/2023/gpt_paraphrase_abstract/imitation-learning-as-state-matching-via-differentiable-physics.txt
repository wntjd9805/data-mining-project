This study introduces a new method called Imitation Learning as State Matching via Differentiable Physics (ILD) to address the limitations of existing imitation learning (IL) methods. IL methods, such as inverse reinforcement learning (IRL), often involve a double-loop training process, which results in long training times and high variance. ILD eliminates the double-loop design by incorporating differentiable physics simulators into its computational graph. It unrolls the dynamics by sampling actions from a parameterized policy and minimizing the difference between the expert trajectory and the agent trajectory. By back-propagating the gradient into the policy through temporal physics operators, ILD improves transferability to unseen environments and achieves higher performance. ILD adopts a single-loop structure, which enhances stability and speeds up training. It dynamically selects learning objectives for each state during optimization to simplify the complex optimization landscape. Experimental results demonstrate that ILD outperforms state-of-the-art methods in continuous control tasks with Brax and can also be applied to deformable object manipulation tasks and generalized to unseen configurations.