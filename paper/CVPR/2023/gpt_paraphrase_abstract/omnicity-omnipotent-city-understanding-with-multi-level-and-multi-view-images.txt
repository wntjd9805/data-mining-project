This study introduces the OmniCity dataset, which aims to enhance the understanding of cities by utilizing multi-level and multi-view images. The dataset includes over 100,000 pixel-wise annotated images collected from 25,000 geo-locations in New York City. It consists of satellite images, street-level panoramas, and mono-view images. To reduce the effort required for pixel-wise annotation, a street-view image annotation pipeline is proposed. This pipeline utilizes existing label maps from satellite views and transformation relations between different views. The OmniCity dataset provides benchmarks for various tasks including building footprint extraction, height estimation, and building plane/instance/fine-grained segmentation. Compared to existing benchmarks, OmniCity contains a larger number of images with more annotation types and views. It also introduces a new task for fine-grained building instance segmentation on street-level panorama images. Additionally, OmniCity offers new problem settings for tasks like cross-view image matching, synthesis, segmentation, detection, etc. This dataset and the associated benchmarks will be made available at https://city-super.github.io/omnicity/.