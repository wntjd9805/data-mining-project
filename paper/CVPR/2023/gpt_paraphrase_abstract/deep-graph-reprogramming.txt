This paper introduces a new approach called "deep graph reprogramming" for reusing pre-trained graph neural networks (GNNs) to handle various downstream tasks. The goal is to reprogram the GNN without modifying the raw node features or model parameters. The authors propose two paradigms: Data Reprogramming and Model Reprogramming. Data Reprogramming addresses the challenge of different graph feature dimensions for different tasks, while Model Reprogramming tackles the issue of fixed per-task-per-model behavior. For Data Reprogramming, the authors propose a Meta-FeatPadding method to handle input dimensions, and also introduce Edge-Slimming and Meta-GraPadding approaches for different sample types. For Model Reprogramming, they propose a Reprogrammable-Aggregator to enhance the model's expressive capacities. The proposed methods are evaluated on various datasets and tasks, showing comparable results to re-training from scratch.