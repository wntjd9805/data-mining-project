Continual learning for segmentation has gained attention recently, but previous studies have mainly focused on narrow semantic segmentation and overlooked panoptic segmentation, which is a task with significant real-world implications. This paper presents the first continual learning model, called CoMFormer, capable of performing both semantic and panoptic segmentation. Inspired by transformer approaches that treat segmentation as a mask-classification problem, CoMFormer leverages the properties of transformer architectures to learn new classes over time. The proposed method introduces an innovative adaptive distillation loss and a mask-based pseudo-labeling technique to prevent forgetting effectively. To assess the performance of CoMFormer, a novel benchmark for continual panoptic segmentation is introduced using the challenging ADE20K dataset. CoMFormer surpasses existing baselines by retaining knowledge of old classes while effectively learning new classes. Additionally, an extensive evaluation is conducted in the large-scale continual semantic segmentation scenario, demonstrating that CoMFormer outperforms state-of-the-art methods.