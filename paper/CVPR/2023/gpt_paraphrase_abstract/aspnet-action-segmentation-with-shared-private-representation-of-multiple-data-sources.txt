Most current methods for action segmentation rely on a single input modality or a simple combination of multiple data sources. However, effectively combining complementary information has the potential to enhance segmentation models, making them more resilient to sensor noise and more accurate with limited training data. To enhance multimodal representation learning for action segmentation, we propose a method to separate hidden features in a multi-stream segmentation model into components that are shared across modalities and components that are specific to each modality. We then introduce an attention bottleneck to capture long-term temporal dependencies while maintaining the separation in consecutive processing layers. Our approach outperforms various fusion techniques on both multiview and multimodal data sources, achieving comparable or superior results compared to the current state-of-the-art methods. Additionally, our model demonstrates robustness to sensor noise and can achieve performance similar to strong video-based approaches even with limited training data.