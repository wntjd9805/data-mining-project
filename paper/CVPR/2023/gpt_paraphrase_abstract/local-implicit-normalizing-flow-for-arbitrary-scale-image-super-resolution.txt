Flow-based methods have shown promise in addressing the problem of super-resolution by using normalizing flow to learn the distribution of high-resolution images. However, these methods are limited to performing a fixed-scale super-resolution and cannot handle arbitrary-scale super-resolution. Previous methods that have attempted arbitrary-scale super-resolution have ignored the inherent challenges and have trained their models using per-pixel L1 loss, resulting in blurry outputs. In this study, we propose a solution called "Local Implicit Normalizing Flow" (LINF) that addresses these issues. LINF models the distribution of texture details at different scaling factors using normalizing flow, allowing it to generate high-quality, realistic images with rich texture details at arbitrary scales. Extensive experiments show that LINF achieves state-of-the-art perceptual quality compared to previous arbitrary-scale super-resolution methods.