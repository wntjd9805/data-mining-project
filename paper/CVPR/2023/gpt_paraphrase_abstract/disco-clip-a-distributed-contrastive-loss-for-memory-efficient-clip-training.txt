We propose DisCo-CLIP, a distributed approach to training CLIP models that reduces the memory usage of contrastive loss. Our method involves decomposing the contrastive loss and its gradient computation into two parts: intra-GPU gradients and inter-GPU gradients. The intra-GPU gradients are computed on the current GPU, while the inter-GPU gradients are collected from other GPUs through all reduce operations. By doing so, we can significantly decrease the GPU memory consumption required for contrastive loss computation, from N*B to N, where B is the batch size and N is the number of GPUs used for training. This distributed solution maintains the same level of computation accuracy as the original non-distributed approach. It is particularly beneficial for large-batch CLIP training, allowing for contrastive training of a ViT-B/32 model with batch sizes of 32K or 196K using only 8 or 64 A100 40GB GPUs, respectively. In comparison, the original CLIP method requires 128 A100 40GB GPUs to train a ViT-B/32 model with a batch size of 32K.