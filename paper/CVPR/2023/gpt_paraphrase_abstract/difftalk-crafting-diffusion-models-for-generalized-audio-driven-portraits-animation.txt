This abstract discusses the potential of talking head synthesis in the video production industry and highlights the need for improvements in generation quality and model generalization. The paper proposes a new approach called DiffTalk, which utilizes Latent Diffusion Models to generate talking head videos that are synchronized with audio input. Unlike previous works, DiffTalk incorporates reference face images and landmarks as conditions for personality-aware generalized synthesis, allowing it to produce high-quality videos across different identities without requiring additional fine-tuning. The proposed method also offers the flexibility to generate higher-resolution videos with minimal computational cost. Extensive experiments demonstrate the effectiveness of DiffTalk in synthesizing realistic audio-driven talking head videos for various identities.