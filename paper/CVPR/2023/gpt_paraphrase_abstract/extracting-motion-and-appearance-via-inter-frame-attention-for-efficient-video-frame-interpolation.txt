This study focuses on the importance of effectively extracting both inter-frame motion and appearance information for video frame interpolation (VFI). Existing methods either mix the extraction of both types of information or use separate modules, resulting in ambiguity and low efficiency. To address this, the authors propose a new module that explicitly extracts motion and appearance information through a unified operation. They reconsider the information process in inter-frame attention and utilize its attention map for enhancing appearance features and extracting motion information. Additionally, to improve the efficiency of VFI, the proposed module can be seamlessly integrated into a hybrid architecture combining convolutional neural networks (CNN) and Transformer. This hybrid pipeline reduces the computational complexity of inter-frame attention while preserving detailed low-level structure information. Experimental results demonstrate that their method achieves state-of-the-art performance on various datasets for both fixed- and arbitrary-timestep interpolation. Moreover, their approach has a lighter computation overhead compared to models with similar performance. The source code and models for this study are available at https://github.com/MCG-NJU/EMA-VFI.