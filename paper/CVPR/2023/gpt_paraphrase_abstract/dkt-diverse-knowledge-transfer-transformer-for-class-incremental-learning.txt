In the context of incremental class learning, deep neural networks often experience catastrophic forgetting, where the accuracy of previously learned classes significantly decreases as new knowledge is acquired. Previous attempts to address this issue have been hindered by either the stability-plasticity dilemma or excessive computational and parameter requirements. To overcome these challenges, we propose a new framework called the Diverse Knowledge Transfer Transformer (DKT). DKT incorporates two knowledge transfer mechanisms that utilize attention mechanisms to transfer both task-specific and task-general knowledge to the current task. It also includes a duplex classifier to handle the stability-plasticity dilemma. Moreover, we introduce a loss function that clusters similar categories and distinguishes between old and new tasks in the feature space. Our method requires only a small number of additional parameters, which are negligible compared to the increasing number of tasks. We conducted extensive experiments on CIFAR100, ImageNet100, and ImageNet1000 datasets, demonstrating that our approach outperforms other competitive methods and achieves state-of-the-art performance. The source code for our method is available at https://github.com/MIV-XJTU/DKT.