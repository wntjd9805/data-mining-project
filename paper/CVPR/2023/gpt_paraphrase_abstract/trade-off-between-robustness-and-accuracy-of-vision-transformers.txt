Deep neural networks (DNNs) have achieved remarkable success in computer vision tasks. However, they are susceptible to perturbations in input data, leading to a trade-off between natural accuracy and robustness. This trade-off arises from the presence of both robust non-predictive features and non-robust predictive features. Recent studies have found that Vision Transformers (ViTs) inherently possess robustness to various perturbations, but they still face this trade-off. To address this, we propose a method called Trade-off between Robustness and Accuracy of Vision Transformers (TORA-ViTs). TORA-ViTs consist of two key components: accuracy and robustness adapters, and a gated fusion module. The adapters are responsible for extracting predictive and robust features, while the gated fusion module adjusts the trade-off between these features. The fusion module compares tokens from different adapters at different spatial locations to generate attention scores, allowing for a balanced mixing of predictive and robust features. Experimental results on ImageNet with various robust benchmarks demonstrate that TORA-ViTs effectively enhance the robustness of pretrained ViTs while maintaining competitive natural accuracy. In our most balanced setting (TORA-ViTs with Î» = 0.5), we achieve 83.7% accuracy on clean ImageNet, and 54.7% and 38.0% accuracy under FGSM and PGD white-box attacks, respectively. Moreover, TORA-ViTs achieve 39.2% and 56.3% accuracy on ImageNet-A and ImageNet-R, and a 34.4% mCE on ImageNet-C, showcasing their performance on different ImageNet variants.