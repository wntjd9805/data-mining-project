We investigate the concept of visual prompting (VP) and its effectiveness in achieving downstream vision tasks. VP involves incorporating universal prompts into data points to reprogram a pre-trained source model for tasks in a different domain. However, the relationship between VP and the label mapping (LM) between the source and target classes is not well understood. We explore the influence of LM on VP and find that a higher quality of LM improves the effectiveness of VP. To optimize LM, we propose a new framework called ILM-VP, which automatically remaps the source labels to the target labels and progressively improves task accuracy. Additionally, we propose integrating an LM process into VP using a contrastive language-image pretrained (CLIP) model to enhance text prompt selection and task accuracy. Extensive experiments demonstrate that our approach outperforms existing VP methods, achieving significant accuracy improvements on various target datasets. Code for our approach is available at the provided GitHub link.