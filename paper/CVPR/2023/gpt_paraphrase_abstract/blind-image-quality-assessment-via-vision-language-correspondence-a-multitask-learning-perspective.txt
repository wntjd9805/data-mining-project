We seek to improve blind image quality assessment (BIQA), which predicts image quality without any reference information. Our approach involves a multitask learning scheme that utilizes auxiliary knowledge from other tasks. We automatically determine model parameter sharing and loss weighting. We describe label combinations using a textual template and calculate joint probabilities based on cosine similarities of visual-textual embeddings. Task predictions are inferred from the joint distribution and optimized using carefully designed loss functions. Through experiments on three tasks (BIQA, scene classification, and distortion type identification), we demonstrate that our method outperforms the state-of-the-art on multiple IQA datasets, is more robust in competition, and effectively realigns quality annotations from different datasets. The source code can be found at https://github.com/zwx8981/LIQE.