The irregularity of large-scale 3D point clouds makes it challenging to apply Masked Autoencoders (MAE) for tasks such as image and video development. Previous 3D MAE frameworks have complex decoders or sophisticated masking strategies, but we propose a simpler approach called Generative Decoder for MAE (GD-MAE). GD-MAE merges surrounding context to restore masked geometric knowledge in a hierarchical fusion manner, eliminating the need for heuristic decoder design and allowing for flexibility in masking strategies. Our approach has a lower latency compared to conventional methods and achieves better performance. We tested our method on the Waymo, KITTI, and ONCE datasets, where it consistently improved downstream detection tasks, demonstrating strong robustness and generalization capability. Surprisingly, even with only 20% of labeled data, our method achieved comparable accuracy on the Waymo dataset. We will release the code for our proposed method.