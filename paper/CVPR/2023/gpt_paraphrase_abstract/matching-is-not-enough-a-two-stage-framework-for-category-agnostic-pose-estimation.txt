Category-agnostic pose estimation (CAPE) is a task that involves predicting keypoints for various object categories using support images with annotated keypoints. Current approaches rely on matching keypoints across the image for localization, but this one-stage matching paradigm has limited accuracy due to the noisy nature of the open set in CAPE. For instance, mirror-symmetric keypoints in the query image can trigger high similarity on certain support keypoints, leading to duplicated or opposite predictions. To address this issue, we propose a two-stage framework. In the first stage, matched keypoints are considered as similarity-aware position proposals. Then, in the second stage, the model leverages relevant features to refine the initial proposals. We employ a transformer model specifically designed for CAPE, which enhances representation and similarity modeling in the first matching stage. In the second stage, the decoder utilizes cross-attention to refine the similarity-aware proposals. Our method outperforms previous approaches on the CAPE benchmark MP-100 in terms of both accuracy and efficiency. The code for our method is available at github.com/flyinglynx/CapeFormer.