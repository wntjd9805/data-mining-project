Recently, query-based end-to-end instance segmentation (QEIS) methods have surpassed CNN-based models on large-scale datasets, taking inspiration from DETR variants. However, these QEIS methods face challenges when there is limited training data available, as it becomes difficult for crucial queries/kernels to learn localization and shape priors. In order to address this issue, this research proposes a novel unsupervised pre-training approach for low-data scenarios.Drawing inspiration from the successful Prompting technique, a new pre-training method is introduced to enhance QEIS models by incorporating Saliency Prompts for queries/kernels. The proposed method consists of three components: 1) Saliency Masks Proposal: This component generates pseudo masks from unlabeled images using the saliency mechanism.2) Prompt-Kernel Matching: Pseudo masks are transformed into prompts and injected with corresponding localization and shape priors to the best-matched kernels.3) Kernel Supervision: This component provides supervision at the kernel level to ensure robust learning.From a practical standpoint, our pre-training method enables QEIS models to achieve similar convergence speed and comparable performance to CNN-based models in low-data scenarios. Experimental results demonstrate that our method significantly improves the performance of several QEIS models on three datasets.