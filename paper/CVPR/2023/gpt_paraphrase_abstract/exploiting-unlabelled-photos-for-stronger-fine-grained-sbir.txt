This paper introduces a strong baseline for fine-grained sketch-based image retrieval (FG-SBIR) that surpasses previous state-of-the-art methods by approximately 11%. The improvement is achieved by addressing two critical issues in the community: (i) the standard triplet loss does not enforce a holistic latent space geometry, and (ii) there is a scarcity of sketches available for training accurate models. To address the first issue, the authors propose a simple modification to the triplet loss that explicitly enforces separation between photo and sketch instances. They employ an intra-modal triplet loss among sketches to bring sketches of the same instance closer to each other, and another intra-modal triplet loss among photos to push away different photo instances while bringing a structurally augmented version of the same photo closer (resulting in a gain of approximately 4-6%).To tackle the second issue, the authors first pre-train a teacher model on a large set of unlabelled photos using the intra-modal photo triplet loss. Then, they distill the contextual similarity present in the teacher's embedding space to the student's embedding space by matching the distribution over inter-feature distances of samples in both spaces (resulting in a further gain of approximately 4-5%).These two modules are incorporated into a novel plug-n-playable training paradigm that allows for more stable training. The proposed model outperforms prior methods significantly and also demonstrates satisfactory results in generalizing to new classes. The project page for this research can be found at https://aneeshan95.github.io/SketchPVT/.