A new approach to multiscale video processing is introduced in this study. Previous methods have focused on either the encoder or decoder, but this research presents a unified encoder-decoder transformer. The proposed method is designed for dense prediction tasks in videos. By incorporating multiscale representation in both the encoder and decoder, the model can extract spatiotemporal features without relying on input optical flow. This approach also ensures temporal consistency during encoding and enables precise localization during decoding by using coarse-to-fine detection for high-level semantics. Additionally, a transductive learning scheme is introduced, which involves many-to-many label propagation to provide temporally consistent predictions. The Multiscale Encoder-Decoder VideoTransformer (MED-VT) is evaluated on Automatic Video Object Segmentation (AVOS) and actor/action segmentation tasks. Remarkably, the proposed approach achieves superior performance compared to state-of-the-art methods on multiple benchmarks, utilizing only raw images without the need for optical flow.