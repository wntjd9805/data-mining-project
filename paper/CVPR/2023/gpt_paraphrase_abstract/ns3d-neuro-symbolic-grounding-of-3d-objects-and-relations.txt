Grounding object properties and relations in 3D scenes is crucial for various artificial intelligence tasks, such as visually grounded dialogues and embodied manipulation. However, the challenges posed by the variability of the 3D domain include labeling expenses and the complexity of 3D grounded language. Therefore, it is important for models to be data-efficient, capable of generalizing to different data distributions and tasks with unseen semantic forms, and able to ground complex language semantics, such as viewpoint anchoring and multi-object reference. To tackle these challenges, we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D utilizes large language-to-code models to translate language into programs with hierarchical structures. Neural networks are employed to implement different functional modules within these programs. Notably, NS3D extends previous neuro-symbolic visual reasoning methods by introducing functional modules that effectively reason about high-arity relations, which are particularly useful in disambiguating objects in complex 3D scenes. The modular and compositional architecture of NS3D enables it to achieve state-of-the-art performance on the ReferIt3D view-dependence task, which serves as a benchmark for 3D referring expression comprehension. Importantly, NS3D exhibits significantly improved performance in terms of data-efficiency and generalization, and it also demonstrates the ability to transfer its skills to an unseen 3D question-answering task. The answer format, in this case, only provides the abstraction.