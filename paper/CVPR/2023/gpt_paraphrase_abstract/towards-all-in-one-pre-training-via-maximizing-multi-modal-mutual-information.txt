To fully utilize large-scale models, different pre-training strategies have been developed, such as supervised, weakly-supervised, and self-supervised pre-training, using extensive data from various sources. It has been proven that combining these strategies and data from different modalities/sources significantly enhances the training of large-scale models. However, current practices employ a multi-stage pre-training system, which introduces uncertainty and instability. Therefore, it is desirable to integrate these strategies into a single-stage approach. This paper introduces a general multi-modal mutual information formula as a unified optimization target, showing that existing approaches are special cases of this framework. Based on this unified perspective, the authors propose a single-stage pre-training approach called Maximizing Multi-modal Mutual Information Pre-training (M3I Pre-training). The proposed approach outperforms previous pre-training methods on several vision benchmarks, including ImageNet classification, COCO object detection, LVIS long-tailed object detection, and ADE20k semantic segmentation. Notably, the authors successfully pre-train a billion-level parameter image backbone and achieve state-of-the-art performance on various benchmarks using publicly available data. The code for their approach will be made available at https://github.com/OpenGVLab/M3I-Pretraining.