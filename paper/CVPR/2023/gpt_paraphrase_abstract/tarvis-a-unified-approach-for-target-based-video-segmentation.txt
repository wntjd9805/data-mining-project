Previously, the field of video segmentation consisted of various tasks that were specific to certain benchmarks. Despite advancements in the state-of-the-art, current methods are limited to their respective tasks and cannot be generalized to other tasks. In light of recent multi-task approaches, we introduce TarViS, a new network architecture capable of handling any task that involves segmenting a set of designated "targets" in a video. Our approach is adaptable to different task requirements, as we view these targets as abstract "queries" and predict precise target masks based on them. TarViS can be trained on multiple datasets covering different tasks and can switch between tasks during inference without requiring task-specific retraining. We demonstrate the effectiveness of TarViS by applying it to four tasks: Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS), Video Object Segmentation (VOS), and Point Exemplar-guided Tracking (PET). Our jointly trained model achieves state-of-the-art performance on 5 out of 7 benchmarks for these tasks and performs competitively on the remaining two. The code and model weights for TarViS are available at: https://github.com/Ali2500/TarViS.