The behavior of neural networks is heavily influenced by the loss and data used during training. However, there is often a need to adjust the model during inference based on external factors such as user preferences or changing data characteristics. This is particularly important for balancing the trade-off between perception and distortion in image-to-image translation tasks. To address this, we propose optimizing a parametric tunable convolutional layer with multiple kernels using a parametric multi-loss with an equal number of objectives. Our approach involves using a shared set of parameters to dynamically interpolate both the objectives and kernels. During training, these parameters are randomly sampled to optimize various combinations of objectives, allowing us to separate their effects into the corresponding kernels. During inference, these parameters serve as interactive inputs, enabling reliable and consistent control over the model's behavior. Extensive experiments demonstrate that our tunable convolutions can seamlessly replace traditional convolutions in existing neural networks without significant additional computational cost. Moreover, our approach outperforms state-of-the-art control strategies in various applications, including image denoising, deblurring, super-resolution, and style transfer.