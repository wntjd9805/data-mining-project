Federated learning (FL) is gaining attention for its ability to enable collaborative training while preserving privacy and communication constraints. However, existing FL algorithms that use iterative model averaging require a large number of communication rounds to obtain a well-performing model. This is due to the unbalanced and non-i.i.d data partitioning among different clients. To address this, we propose FedDM, which builds the global training objective from multiple local surrogate functions. This allows the server to have a more comprehensive view of the loss landscape. FedDM achieves this by creating synthetic data sets on each client that locally match the loss landscape of the original data through distribution matching. By transmitting more informative and smaller synthesized data instead of unwieldy model weights, FedDM reduces communication rounds and improves model quality. We conducted experiments on three image classification datasets and found that FedDM outperforms other FL counterparts in terms of efficiency and model performance, even with a limited number of communication rounds. Additionally, we demonstrate that FedDM can preserve differential privacy using the Gaussian mechanism and train a better model under the same privacy budget.