The Masked Autoencoder (MAE) is a self-supervised learning framework that achieves successful results in various vision tasks by reconstructing masked image regions. While there have been empirical observations on the effectiveness of MAE, a theoretical understanding is still lacking. This study aims to provide a formal characterization and justification of these empirical insights and offer theoretical guarantees for MAE. By formulating the data-generating process as a hierarchical latent variable model, it is shown that MAE can identify a set of latent variables in this model, explaining its ability to extract high-level information from pixels. The study also demonstrates how the choice of hyperparameters in MAE, such as the masking ratio and patch size, determines which latent variables are recovered and impacts the level of semantic information in the representation. In particular, extremely large or small masking ratios result in low-level representations. The theoretical framework presented in this study provides coherent explanations for existing empirical observations, offers insights for potential empirical improvements, and identifies fundamental limitations of the masked-reconstruction paradigm. Extensive experiments are conducted to validate the theoretical insights.