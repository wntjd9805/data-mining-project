The use of deep neural networks and large training datasets has become increasingly common in order to improve performance in practical applications. This has put a significant strain on GPU platforms, as training these complex models consumes a substantial amount of computing resources. Therefore, it is essential to accelerate the training process of deep neural networks.In this study, we propose a general method for reducing the time cost of model training. We achieve this by utilizing sparsity at both the width and depth levels of dense models. Initially, we select a sparse sub-network from the original dense model by reducing the number of parameters. This sparse architecture gradually expands during the training process until it becomes dense.We have designed different expanding strategies for convolutional neural networks (CNNs) and vision transformers (ViTs) due to the architectural differences between the two. Our method can be easily integrated into popular deep learning frameworks, saving significant training time and hardware resources.Extensive experiments demonstrate that our acceleration method can significantly speed up the training process of modern vision models on general GPU devices, with minimal impact on performance. For example, our method achieves a 1.42× speed improvement for ResNet-101 and a 1.34× speed improvement for DeiT-base on the ImageNet-1k dataset. The code for our method is available at the following links: [insert links].Overall, our proposed network expansion method effectively reduces the practical time cost of training deep neural networks, making it a valuable tool for accelerating model training.