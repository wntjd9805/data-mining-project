This study introduces a consistency-aware audio-visual saliency prediction network (CASP-Net) that incorporates the audio stream to mimic the selective attention mechanism of the human brain. Unlike most existing methods that focus on the benefits of joint auditory and visual information, CASP-Net also considers the negative effects caused by the temporal inconsistency of audio-visual elements. Inspired by the biological inconsistency-correction in multi-sensory information, CASP-Net takes into account both the audio-visual semantic interaction and consistent perception. It includes a two-stream encoder for linking video frames and corresponding sound sources, as well as a novel consistency-aware predictive coding to enhance consistency within audio and visual representations. The network also features a saliency decoder to aggregate multi-scale audio-visual information and generate the final saliency map. Extensive experiments demonstrate that CASP-Net outperforms other state-of-the-art methods on six challenging audio-visual eye-tracking datasets. A demo of the system can be found on the project webpage.