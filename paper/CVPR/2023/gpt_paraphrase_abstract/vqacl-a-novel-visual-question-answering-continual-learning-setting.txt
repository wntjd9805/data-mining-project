Research in the field of continual learning has mainly focused on unimodal tasks, neglecting the study of multimodal tasks like visual question answering (VQA). In this study, we introduce a new setting for VQA called VQACL, which consists of two main elements: a dual-level task sequence that incorporates both visual and linguistic data, and a novel composition testing that involves new combinations of skills and concepts. The dual-level task sequence aims to simulate the dynamic nature of multimodal data in the real world, while the composition testing evaluates the generalizability of models for cognitive reasoning.To evaluate the performance of five existing continual learning methods on our VQACL setting, we conducted thorough assessments and found that these methods suffer from catastrophic forgetting and lack strong generalizability. To address these issues, we propose a novel representation learning method that utilizes both sample-specific and sample-invariant features. This approach enables the learning of representations that are both discriminative and generalizable for VQA. Additionally, by extracting separate representations for visual and textual inputs, our method is able to explicitly disentangle skills and concepts. Extensive experimental results demonstrate that our method significantly outperforms existing models, highlighting the effectiveness and compositionality of our approach.For those interested, the code for our method is available at https://github.com/zhangxi1997/VQACL.