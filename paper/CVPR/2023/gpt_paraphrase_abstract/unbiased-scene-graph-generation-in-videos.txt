Dynamic scene graph generation from videos is a complex task due to the dynamic nature of scenes, fluctuation of model predictions, and the skewed distribution of visual relationships. Existing methods have focused on spatio-temporal context but fail to address the challenges, resulting in biased scene graphs. To overcome this, we propose TEMPURA, a framework that incorporates temporal consistency, memory-guided training, and uncertainty attenuation. TEMPURA utilizes transformer-based sequence modeling to capture object-level temporal consistencies, learns to synthesize unbiased relationship representations using memory guidance, and attenuates predictive uncertainty with a Gaussian Mixture Model. Our extensive experiments demonstrate that TEMPURA outperforms existing methods, achieving significant performance gains (up to 10%) in generating unbiased scene graphs. The code for TEMPURA is available at: https://github.com/sayaknag/unbiasedSGG.git.