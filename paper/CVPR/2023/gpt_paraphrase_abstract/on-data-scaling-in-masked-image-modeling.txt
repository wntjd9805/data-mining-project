The issue of scaling properties in self-supervised pre-training has been extensively studied in the context of language models, leading to the development of large-scale pre-trained models with impressive modeling capabilities. However, the importance of scaling properties in masked image modeling (MIM) has been overlooked in recent studies, with some even suggesting that MIM does not benefit from large-scale data. In this study, we aim to challenge these assumptions and conduct a comprehensive investigation into the scaling behaviors of MIM through a series of experiments. We vary the amount of data used, ranging from 10% of ImageNet-1K to the full ImageNet-22K dataset, as well as the model parameters, ranging from 49 million to one billion. We also explore different training lengths, ranging from 125K to 500K iterations. Our main findings can be summarized as follows: 1) MIM requires large-scale data to effectively scale up compute and model parameters; 2) unlike self-supervised language models or supervised vision models, additional data does not improve MIM performance in a non-overfitting scenario. Furthermore, we uncover interesting properties of MIM, including high sample efficiency in large MIM models and a strong correlation between pre-training validation loss and transfer performance. We believe that our findings contribute to a deeper understanding of masked image modeling and will facilitate the development of large-scale vision models in the future. The code and models used in this study will be made available at https://github.com/microsoft/SimMIM.