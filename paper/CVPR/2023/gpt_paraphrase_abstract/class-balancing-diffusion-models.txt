Diffusion-based models have been successful in generating high-quality visual data with better diversity. However, these models have been primarily tested on curated data distributions, where the data samples are uniformly distributed based on their labels. In real-world scenarios, class-imbalanced data distributions are more common, and it is unclear how diffusion models perform in such cases. This study aims to investigate this problem and finds that when diffusion models are trained on datasets with class-imbalanced distributions, there is a significant degradation in both diversity and fidelity. Particularly in the tail classes, the generated data lacks diversity and suffers from severe mode-collapse issues. To address this problem, the authors propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer. Experimental results demonstrate that images generated by CBDM exhibit higher diversity and quality, both quantitatively and qualitatively. The proposed method is evaluated on the CIFAR100/CIFAR100LT dataset and achieves outstanding performance in downstream recognition tasks.