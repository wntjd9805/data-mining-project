We propose a novel diffusion-based model for generating 3D neural fields, which outperforms existing methods in terms of quality and diversity. Our approach involves preprocessing training data, converting ShapeNet meshes into continuous occupancy fields and representing them as a set of axis-aligned triplane features. This allows us to train existing 2D diffusion models on these representations to generate high-quality 3D neural fields. We make essential modifications to the triplane factorization pipelines to facilitate learning for the diffusion model. Our method achieves state-of-the-art results in 3D generation for various object classes from ShapeNet.