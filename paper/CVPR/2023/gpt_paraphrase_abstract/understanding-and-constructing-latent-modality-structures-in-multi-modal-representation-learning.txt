Contrastive loss is commonly used in learning representations from multiple modalities, aiming to make the modalities match each other exactly in the latent space. However, it is still unclear how this modality alignment affects downstream task performance. In this study, we demonstrate through an information-theoretic argument that perfect modality alignment is generally suboptimal for downstream prediction tasks. Instead, we propose that meaningful latent modality structures are the key to better performance. To achieve this, we introduce three approaches: 1) a deep feature separation loss for intra-modality regularization, 2) a Brownian-bridge loss for inter-modality regularization, and 3) a geometric consistency loss for both intra- and inter-modality regularization. We conduct extensive experiments using two popular multi-modal representation learning frameworks and evaluate our model on various tasks such as image classification, image-text retrieval, visual question answering, visual reasoning, and visual entailment. Our proposed approach consistently outperforms existing methods, demonstrating the effectiveness and generalizability of our approach in regularizing latent modality structures.