Contrastive learning-based approaches that combine vision and language, like CLIP, have been successful in various tasks. These methods align image and text by creating similar feature embeddings from visual patches and language tokens. However, aligning different semantic levels and granularities is challenging. To address this, we propose FiniteDiscrete Tokens (FDT) as a multimodal representation. FDT consists of learnable tokens that represent visual-semantic concepts. Both images and texts are embedded using shared FDT by grounding inputs to FDT space and aggregating activated FDT representations. A sparse activation constraint ensures that matching visual and semantic concepts are represented by the same tokens, reducing granularity gap. Through analysis, we demonstrate that using FDT in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language tasks. Additionally, our method learns comprehensive representations where FDT captures meaningful cross-modal correspondence, including objects, actions, and attributes.