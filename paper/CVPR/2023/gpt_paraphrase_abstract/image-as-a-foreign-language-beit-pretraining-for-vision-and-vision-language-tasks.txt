A significant convergence is occurring in the fields of language, vision, and multimodal pretraining. In this study, we present a versatile multimodal foundation model called BEIT-3, which demonstrates impressive transfer performance in both vision and vision-language tasks. Our work contributes to this convergence in three main ways: through the backbone architecture, pretraining task, and model scaling. We utilize Multiway Transformers for general-purpose modeling, allowing for deep fusion and modality-specific encoding through a modular architecture. By employing a shared backbone, we conduct masked language modeling on images (Imglish), texts (English), and image-text pairs (parallel sentences) in a unified manner. Our experimental results demonstrate that BEIT-3 achieves remarkable performance in various tasks such as object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).