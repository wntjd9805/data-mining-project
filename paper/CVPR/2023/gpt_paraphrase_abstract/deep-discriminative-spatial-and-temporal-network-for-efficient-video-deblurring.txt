Video deblurring requires effective exploration of spatial and temporal information. Unlike existing methods that align adjacent frames without discrimination, we propose a deep discriminative spatial and temporal network to enhance the exploration of these features for improved video deblurring. Our approach involves two main components. Firstly, we introduce a channel-wise gated dynamic network to adaptively explore spatial information. Since adjacent frames often contain different content, directly combining their features without discrimination can hinder the restoration of the latent clear frame. By employing the channel-wise gated dynamic network, we are able to selectively incorporate relevant spatial information to enhance the deblurring process.Secondly, we address the challenge of effectively utilizing temporal information by developing a discriminative temporal feature fusion module. This module aims to capture useful temporal features that can aid in restoring the latent frame. It provides a simple yet effective approach to fuse the temporal features in a discriminative manner, taking into account the differences in content between adjacent frames.Additionally, we propose a wavelet-based feature propagation method to leverage information from long-range frames. By using the discriminative temporal feature fusion module as the basic unit, we are able to effectively propagate the main structures from long-range frames, which contributes to better video deblurring results.Importantly, our proposed method does not require additional alignment methods and achieves favorable performance compared to state-of-the-art methods on benchmark datasets. We demonstrate this through evaluation in terms of accuracy and model complexity. Overall, our approach enables effective exploration of both spatial and temporal information, leading to improved video deblurring results.