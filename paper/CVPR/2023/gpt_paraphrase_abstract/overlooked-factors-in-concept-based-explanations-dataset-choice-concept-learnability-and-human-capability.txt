Concept-based interpretability methods aim to explain the components and predictions of a deep neural network model by using a predefined set of semantic concepts. These methods evaluate a trained model on a new dataset called a "probe" dataset and correlate the model's outputs with the concepts labeled in that dataset. However, these methods have limitations that have not been well understood or explained in the literature. In this study, we have identified and examined three factors that are often overlooked in concept-based explanations. First, we have found that the choice of the probe dataset has a significant impact on the generated explanations. Our analysis has shown that different probe datasets result in very different explanations, indicating that these explanations may not be applicable beyond the probe dataset. Second, we have discovered that concepts in the probe dataset are often more difficult to learn than the target classes they are used to explain. This raises concerns about the accuracy and validity of the explanations. We argue that only easily learnable concepts should be utilized in concept-based explanations. Finally, existing methods employ a large number of concepts, ranging from hundreds to thousands. However, our human studies have revealed that a much smaller number, specifically 32 concepts or less, is more practically useful. Beyond this threshold, the explanations become less effective. We discuss the implications of our findings and provide suggestions for the future development of concept-based interpretability methods. For further information, our analysis code and user interface can be accessed at https://github.com/princetonvisualai/OverlookedFactors.