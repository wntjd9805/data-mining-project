This study introduces a new method called FAME-ViL (FAshion-focused Multi-task Efficient learning method for Vision-and-Language tasks) to address the challenges of parameter inefficiency and lack of inter-task relatedness in fashion-related vision-and-language tasks. Existing approaches typically use task-specific models and fine-tune them independently, resulting in inefficient parameter usage. FAME-ViL, on the other hand, utilizes a single model for multiple heterogeneous fashion tasks, making it more parameter-efficient. This is achieved through a task-versatile architecture that integrates cross-attention adapters and task-specific adapters into a unified vision-and-language model. Additionally, a stable and effective multi-task training strategy is employed to enable learning from diverse data and prevent negative transfer. Experimental results on four fashion tasks demonstrate that FAME-ViL outperforms independently trained single-task models while reducing parameter usage by 61.5%. The code for FAME-ViL is available at https://github.com/BrandonHanx/FAME-ViL.