Generating realistic talking head videos from a single face image and speech audio is a challenging task due to issues such as unnatural head movement, distorted expression, and identity modification. These problems are primarily caused by learning from 2D motion fields. However, using 3D information also leads to problems like stiff expression and incoherent videos. To address these issues, we propose SadTalker, a method that generates 3D motion coefficients (head pose and expression) of a 3D Morphable Model (3DMM) from audio and utilizes them to create a realistic talking head video. To learn the motion coefficients accurately, we establish explicit connections between audio and different types of motion coefficients. We introduce ExpNet to learn facial expression from audio by incorporating both coefficients and 3D-rendered faces. For head pose, we design PoseVAE, a conditional VAE that synthesizes head motion in various styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render to synthesize the final video. Extensive experiments demonstrate the superior motion and video quality achieved by our method. The code and demo videos can be found at https://sadtalker.github.io.