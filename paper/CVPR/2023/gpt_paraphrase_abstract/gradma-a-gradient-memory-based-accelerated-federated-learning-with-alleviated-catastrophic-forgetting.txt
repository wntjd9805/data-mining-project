Federated Learning (FL) has gained significant attention in the field of machine learning, but it faces challenges due to data heterogeneity and partial participation, resulting in catastrophic forgetting and reduced performance. To address these issues, we propose a novel FL approach called GradMA, which draws inspiration from continual learning. GradMA corrects the update directions on both the server-side and worker-side simultaneously, while leveraging the server's computing and memory resources effectively. Additionally, we introduce a memory reduction strategy to enable GradMA to handle large-scale FL with numerous workers. We provide a theoretical analysis of GradMA's convergence under the smooth non-convex setting, demonstrating its linear speedup with an increasing number of sampled active workers. Our extensive experiments on various image classification tasks reveal that GradMA outperforms state-of-the-art baselines in terms of accuracy and communication efficiency. The code for GradMA can be accessed at: https://github.com/lkyddd/GradMA.