We present MoFusion, a new framework for high-quality conditional human motion synthesis. Existing methods either lack diversity or struggle with the balance between motion diversity and quality. MoFusion overcomes these limitations by using denoising-diffusion techniques to generate long, realistic, and contextually accurate motions based on various conditioning contexts like music and text. We also incorporate well-known kinematic losses to ensure motion plausibility. The learned latent space enables interactive motion-editing applications such as in-betweening, seed-conditioning, and text-based editing, making it valuable for virtual-character animation and robotics. Through quantitative evaluations and a user study, we demonstrate MoFusion's effectiveness compared to state-of-the-art methods. We encourage readers to watch our supplementary video for more information.