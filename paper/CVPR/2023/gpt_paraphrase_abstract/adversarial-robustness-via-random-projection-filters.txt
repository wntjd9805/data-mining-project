Deep Neural Networks (DNNs) are highly effective in various tasks, but they are susceptible to adversarial attacks. Most defense techniques focus on adversarial training strategies, but relying solely on traditional adversarial training is challenging to achieve robust performance. This is primarily due to the ability to find aggressive perturbations that increase the loss through gradient ascent in a white-box setting. While some noise can be added to prevent attackers from obtaining precise gradients, there is a trade-off between defense capability and natural generalization.To address this issue, we propose a novel approach that leverages random projection. Specifically, we suggest replacing a portion of convolutional filters with random projection filters. We theoretically analyze the preservation of geometric representation of the synthesized filters using the Johnson-Lindenstrauss lemma. We extensively evaluate our approach on multiple networks and datasets, and the experimental results demonstrate the superiority of our random projection filters compared to state-of-the-art baselines. The code implementation of our approach is publicly available on GitHub.