The realism and vividness of speech-driven 3D facial animation is still a challenge due to limited audio-visual data. Existing methods use regression, resulting in overly smooth facial motions. To address this, we propose a code query approach using a learned codebook that reduces mapping uncertainty. Our method uses self-reconstruction to learn realistic facial motion priors. We employ a temporal autoregressive model to synthesize facial motions in a lip-sync and expressive manner. Our approach surpasses current methods in both qualitative and quantitative evaluations, as confirmed by a user study. Code and video demo are available at the provided URL.