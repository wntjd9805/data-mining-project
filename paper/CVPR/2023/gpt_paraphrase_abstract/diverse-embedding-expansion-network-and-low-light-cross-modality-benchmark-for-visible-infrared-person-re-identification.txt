The main challenge in visible-infrared re-identification (VIReID) is the large discrepancy between visible (VIS) and infrared (IR) images. Existing methods are limited by a lack of training samples and the inability to effectively exploit cross-modality clues. To address this, we propose a diverse embedding expansion network (DEEN) that generates diverse embeddings to reduce the modality gap and learn informative feature representations. Additionally, the existing VIReID datasets do not account for drastic illumination changes, so we introduce a low-light cross-modality (LLCM) dataset to address this issue. The LLCM dataset consists of 46,767 bounding boxes of 1,064 identities captured by 9 RGB/IR cameras. Experimental results on the SYSU-MM01, RegDB, and LLCM datasets demonstrate the superiority of DEEN compared to other state-of-the-art methods. The code and dataset are available at the following link: https://github.com/ZYK100/LLCM.