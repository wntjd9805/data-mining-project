This study addresses the problem of weakly-supervised temporal action localization, which involves detecting action boundaries in untrimmed videos using only video-level annotations. Existing approaches focus on identifying temporal regions that are most relevant for video-level classification, but they fail to consider the semantic consistency across frames. The authors propose a method that treats snippets with similar representations as the same action class, even without supervision signals for each individual snippet. They introduce a learnable dictionary consisting of class centroids for each action category. Snippets identified as the same action category are encouraged to have representations close to the corresponding class centroid, which helps the network understand the semantics of frames and avoid incorrect localization. Additionally, a two-stream framework is presented, combining an attention mechanism and a multiple-instance learning strategy to extract fine-grained clues and salient features, respectively. The complementary nature of these components enables the model to refine temporal boundaries. The proposed model is evaluated on the THUMOS-14 and ActivityNet-1.3 datasets, demonstrating significant improvements over existing methods through extensive experiments and analyses.