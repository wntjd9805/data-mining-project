The success of self-supervised pretraining in skeleton-based action recognition is limited by the equal treatment of motion and static parts, resulting in reduced accuracy. To address this, we introduce an adaptive approach called Actionlet-Dependent Contrastive Learning (ActCLR). We define actionlets as discriminative subsets of the skeleton, which effectively decompose motion regions for improved action modeling. By contrasting the static anchor with the motion region of the skeleton data, we extract actionlets in an unsupervised manner. We then apply motion-adaptive data transformations to actionlet and non-actionlet regions to introduce diversity while preserving their individual characteristics. Additionally, we propose a semantic-aware feature pooling method to create feature representations that distinguish between motion and static regions. Extensive experiments on NTU RGB+D and PKUMMD datasets demonstrate the exceptional performance of our method. Visualization and quantitative experiments further confirm its effectiveness. Our project website, available at https://langlandslin.github.io/projects/ActCLR/, provides more details.