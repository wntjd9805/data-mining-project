Abstract
High-resolution images enable neural networks to learn richer visual representations. However, this improved per-formance comes at the cost of growing computational com-plexity, hindering their usage in latency-sensitive applica-tions. As not all pixels are equal, skipping computations for less-important regions offers a simple and effective mea-sure to reduce the computation. This, however, is hard to be translated into actual speedup for CNNs since it breaks the regularity of the dense convolution workload. In this paper, we introduce SparseViT that revisits activation spar-sity for recent window-based vision transformers (ViTs). As window attentions are naturally batched over blocks, actual speedup with window activation pruning becomes possible: i.e., ∼50% latency reduction with 60% sparsity. Different layers should be assigned with different pruning ratios due to their diverse sensitivities and computational costs. We intro-duce sparsity-aware adaptation and apply the evolutionary search to efficiently find the optimal layerwise sparsity con-figuration within the vast search space. SparseViT achieves speedups of 1.5×, 1.4×, and 1.3× compared to its dense counterpart in monocular 3D object detection, 2D instance segmentation, and 2D semantic segmentation, respectively, with negligible to no loss of accuracy. 1.

Introduction
With the advancement of image sensors, high-resolution images become more and more accessible: e.g., recent mo-bile phones are able to capture 100-megapixel photos. The increased image resolution offers great details and enables neural network models to learn richer visual representations and achieve better recognition quality. This, however, comes at the cost of linearly-growing computational complexity, making them less deployable for resource-constrained appli-cations (e.g., mobile vision, autonomous driving).
∗ indicates equal contributions (listed in alphabetical order).
Figure 1. Sparse, high-resolution features are far more informative than dense, low-resolution ones. Compared with direct downsam-pling, activation pruning can retain important details at a higher resolution, which is essential for most image recognition tasks.
The simplest solution to address this challenge is to down-sample the image to a lower resolution. However, this will drop the fine details captured from the high-resolution sen-sor. What a waste! The missing information will bottleneck the model’s performance upper bound, especially for small object detection and dense prediction tasks. For instance, the detection accuracy of a monocular 3D object detector will degrade by more than 5% in mAP by reducing the height and width by 1.6×*. Such a large gap cannot be easily recovered by scaling the model capacity up.
Dropping details uniformly at all positions is clearly sub-optimal as not all pixels are equally informative (Figure 1a).
Within an image, the pixels that contain detailed object fea-tures are more important than the background pixels. Moti-*BEVDet [19] (with Swin Transformer [33] as backbone) achieves 31.2 mAP with 256×704 resolution and 25.90 mAP with 160×440 resolution.
vated by this, a very natural idea is to skip computations for less-important regions (i.e., activation pruning). However, activation sparsity cannot be easily translated into the actual speedup on general-purpose hardware (e.g., GPU) for CNNs.
This is because sparse activation will introduce randomly dis-tributed and unbalanced zeros during computing and cause computing unit under-utilization [54]. Even with dedicated system support [40], a high sparsity is typically required to realize speedup, which will hurt the model’s accuracy.
Recently, 2D vision transformers (ViTs) have achieved tremendous progress. Among them, Swin Transformer [33] is a representative work that generalizes well across different visual perception tasks (such as image classification, object detection, and semantic segmentation). Our paper revisits the activation sparsity in the context of window-based ViTs.
Different from convolutions, window attentions are naturally batched over windows, making real speedup possible with window-level activation pruning. We re-implement the other layers in the model (i.e., FFNs and LNs) to also execute at the window level. As a result, we are able to achieve around 50% latency reduction with 60% window activation sparsity.
Within a neural network, different layers have different impacts on efficiency and accuracy, which advocates for a non-uniform layerwise sparsity configuration: e.g., we may prune layers with larger computation and lower sensitivity more, while pruning layers with smaller computation and higher sensitivity less. To this end, we make use of the evo-lutionary search to explore the best per-layer pruning ratios under a resource constraint. We also propose sparsity-aware adaptation by randomly pruning a different subset of the ac-tivations at each iteration. This effectively adapts the model to activation sparsity and avoids the expensive re-training of every candidate within the large search space. Our Sparse-ViT achieves speedups of 1.5×, 1.4×, and 1.3× compared to its dense counterpart in monocular 3D object detection, 2D instance segmentation, and 2D semantic segmentation, respectively, with negligible to no loss of accuracy. 2.