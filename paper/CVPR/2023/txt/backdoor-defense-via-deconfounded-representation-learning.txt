Abstract
Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hid-den backdoors in the DNN model by injecting a few poi-soned examples into the training dataset. While exten-sive efforts have been made to detect and remove back-doors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations between the input im-ages and target labels, making the model predictions less reliable.
Inspired by the causal understanding, we pro-pose the Causality-inspired Backdoor Defense (CBD), to learn deconfounded representations for reliable classifi-cation. Specifically, a backdoored model is intentionally trained to capture the confounding effects. The other clean model dedicates to capturing the desired causal effects by minimizing the mutual information with the confounding representations from the backdoored model and employ-ing a sample-wise re-weighting scheme. Extensive exper-iments on multiple benchmark datasets against 6 state-of-the-art attacks verify that our proposed defense method is effective in reducing backdoor threats while maintaining high accuracy in predicting benign samples. Further anal-ysis shows that CBD can also resist potential adaptive at-tacks. The code is available at https://github.com/ zaixizhang/CBD. 1.

Introduction
Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks [18, 38, 53],
*Qi Liu is the corresponding author.
Figure 1. (a) A real example of the backdoor attack. The back-doored DNN classifies the “turtle” image with a trigger pattern as the target label “dog”. (b) The causal graph represents the causalities among variables: X as the input image, Y as the la-bel, and B as the backdoor attack. Besides the causal effect of X on Y (X → Y ), the backdoor attack can attach trigger patterns to images (B → X), and change the labels to the targeted label (B → Y ). Therefore, as a confounder, the backdoor attack B opens a spurious path between X and Y (X ← B → Y ). where attackers inject stealthy backdoors into DNNs by poi-soning a few training data. Specifically, backdoor attack-ers attach the backdoor trigger (i.e., a particular pattern) to some benign training data and change their labels to the attacker-designated target label. The correlations between the trigger pattern and target label will be learned by DNNs during training. In the inference process, the backdoored model behaves normally on benign data while its predic-tion will be maliciously altered when the backdoor is ac-tivated. The risk of backdoor attacks hinders the applica-tions of DNNs to some safety-critical areas such as auto-matic driving [38] and healthcare systems [14].
On the contrary, human cognitive systems are known to be immune to input perturbations such as stealthy trigger patterns induced by backdoor attacks [17]. This is because humans are more sensitive to causal relations than the sta-tistical associations of nuisance factors [29,60]. In contrast, deep learning models that are trained to fit the poisoned 1
datasets can hardly distinguish the causal relations and the statistical associations brought by backdoor attacks. Based on causal reasoning, we can identify causal relation [50,52] and build robust deep learning models [70, 71]. Therefore, it is essential to leverage causal reasoning to analyze and mitigate the threats of backdoor attacks.
In this paper, we focus on the image classification tasks and aim to train backdoor-free models on poisoned datasets without extra clean data. We first construct a causal graph to model the generation process of backdoor data where nuisance factors (i.e., backdoor trigger patterns) are consid-ered. With the assistance of the causal graph, we find that the backdoor attack acts as the confounder and opens a spu-rious path between the input image and the predicted label (Figure 1). If DNNs have learned the correlation of such a spurious path, their predictions will be changed to the target labels when the trigger is attached.
Motivated by our causal insight, we propose Causality-inspired Backdoor Defense (CBD) to learn deconfounded representations for classification. As the backdoor attack is stealthy and hardly measurable, we cannot directly block the backdoor path by the backdoor adjustment from causal inference [52].
Inspired by recent advances in disentan-gled representation learning [20, 36, 66], we instead aim to learn deconfounded representations that only preserve the causality-related information. Specifically in CBD, two
DNNs are trained, which focus on the spurious correlations and the causal effects respectively. The first DNN is de-signed to intentionally capture the backdoor correlations with an early stop strategy. The other clean model is then trained to be independent of the first model in the hidden space by minimizing mutual information. The information bottleneck strategy and sample-wise re-weighting scheme are also employed to help the clean model capture the causal effects while relinquishing the confounding factors. After training, only the clean model is used for downstream clas-sification tasks. In summary, our contributions are:
• From a causal perspective, we find the backdoor at-tack acts as the confounder that causes spurious corre-lations between the input images and the target label.
• With the causal insight, we propose a Causality-inspired Backdoor Defense (CBD), which learns de-confounded representations to mitigate the threat of poisoning-based backdoor attacks.
• Extensive experiments with 6 representative backdoor attacks are conducted. The models trained using CBD are of almost the same clean accuracy as they were directly trained on clean data and the average backdoor attack success rates are reduced to around 1%, which verifies the effectiveness of CBD.
• We explore one potential adaptive attack against CBD, which tries to make the backdoor attack stealthier by adversarial training. Experiments show that CBD is robust and resistant to such an adaptive attack. 2.