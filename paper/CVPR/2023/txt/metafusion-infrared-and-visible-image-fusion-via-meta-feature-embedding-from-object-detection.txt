Abstract
Fusing infrared and visible images can provide more tex-ture details for subsequent object detection task. Converse-ly, detection task furnishes object semantic information to improve the infrared and visible image fusion. Thus, a joint fusion and detection learning to use their mutual promotion is attracting more attention. However, the feature gap be-tween these two different-level tasks hinders the progress.
Addressing this issue, this paper proposes an infrared and visible image fusion via meta-feature embedding from ob-ject detection. The core idea is that meta-feature embedding model is designed to generate object semantic features ac-cording to fusion network ability, and thus the semantic fea-tures are naturally compatible with fusion features. It is op-timized by simulating a meta learning. Moreover, we further implement a mutual promotion learning between fusion and detection tasks to improve their performances. Comprehen-sive experiments on three public datasets demonstrate the effectiveness of our method. Code and model are available at: https://github.com/wdzhao123/MetaFusion. 1.

Introduction
Multi-modality sensor technology has promoted the ap-plication of multi-modality images in different areas. A-mong them, infrared images and visible images have been utilized commonly, as the information contained in these two modalities is complementary. Speciﬁcally, infrared im-ages can supply object thermal structures without being af-fected by illumination. But they are short of texture de-tails. On the contrary, visible images can catch the texture information for the scene. But they are severely affected by light. Thus, many methods [15,25,35,43–45,47] focus on s-∗Corresponding author
Figure 1. Different joint learning methods of infrared and visible image fusion (IVIF) and object detection (OD). (a) Separate opti-mization method: IVIF network Ψ is ﬁrstly optimized by fusion loss Lf . Then, fusion result z is generated by Ψ from the input infrared and visible image pair x, y. Finally, OD network Φ is op-timized by detection loss Ld using z. (b) Cascaded optimization method: OD network Φ is treated as a constraint to optimize IVIF network Ψ by the loss Lf and Ld. (c) Meta-feature embedding method: Meta-feature embedding Γ is optimized to learn how to guide Ψ to have a low Lf . Then, Γ generates meta feature from detection feature F d i . Finally, the meta fea-ture is used to guide Ψ by the loss Ld. i and fusion feature F d tudying pixel-level infrared and visible image fusion (IVIF), thereby helping high-level tasks improve performance, e.g., object detection (OD) [20, 34].
IVIF and OD can greatly beneﬁt form each other. IVIF
generates the fused image that contains more information than any single modality image to improve OD. Howev-er, IVIF mainly focuses on the pixel relationship between an image pair, and there is little consideration for object semantic. In contrast, OD can provide rich object seman-tic information to IVIF, as its aim is locating the objects.
Therefore, this paper studies a joint learning framework be-tween IVIF and OD to improve their performances.
Existing joint learning methods of IVIF and OD can be divided into two categories: separate optimization and cascaded optimization. Separate optimization ﬁrstly train-s IVIF network, and then trains OD network using IVIF results, as shown in Figure 1(a). Thus, most methods fo-cus on improving the fusion effect, e.g., designing network-s [13, 23, 35, 36] and introducing constraints [10, 26, 45].
Obviously, separate optimization neglects the help of OD.
Cascaded optimization adopts OD network as a constraint to train IVIF network, and thus forces the IVIF network to generate fusion images with easily detected objects [20], as shown in Figure 1(b). However, directly utilizing the high-level OD constraint to guide the pixel-level IVIF will result in limited effect. Therefore, we leverage OD feature maps that guide IVIF feature maps to obtain more semantic infor-mation. Unfortunately, OD features are mismatched with
IVIF features due to their task-level difference. Addressing this issue, we propose a meta-feature embedding network (M F E), as shown in Figure 1(c). The idea is that if M F E generates OD features according to the IVIF network abil-ity, the OD features are naturally compatible with the IVIF network, and the optimization can be achieved by simulat-ing a meta learning.
Speciﬁcally, an infrared and visible image fusion via meta-feature embedding from object detection is proposed, which is named as MetaFusion. MetaFusion includes IVIF network (F ), OD network (D), and M F E. In particular,
M F E is expected to generate meta features to bridge the gap between F and D, which is optimized by two alter-nate steps: inner update and outer update. In the inner up-date process, we ﬁrstly optimize F using meta training set
Smtr to obtain its updated network F (cid:48). Then, F (cid:48) calculates the fusion loss on meta testing set Smts to optimize M F E.
The motivation is that if M F E successfully generates meta features which are compatible with F , F (cid:48) will produce bet-ter fused images, i.e., the fusion loss should be lower. In the outer update process, F is optimized with the guide of the meta features generated by the ﬁxed M F E on Smtr and Smts. In this way, F can learn how to extract seman-tic information to improve fusion quality. In the above two alternate steps, D is ﬁxed to offer detection semantic infor-mation. Thus, we further implement a mutual promotion learning, where we use the improved F to generate fusion results to ﬁnetune D, and then the improved D offers better semantic information to optimize F .
In summary, our contributions are as follows. (1) We ex-plore the joint learning framework of IVIF and OD, and pro-pose MetaFuison to obtain superior performance on these two tasks. (2) Meta-feature embedding network is designed to generate meta features that bridge the gap between F and
D. (3) Sequentially, mutual promotion learning between F and D is introduced to improve their performances. (4) Ex-tensive experiments on image fusion and object detection validate the effectiveness of the proposed method. 2.