Abstract
Interpreting visual relationships is a core aspect of com-prehensive video understanding. Given a query visual re-lationship as <subject, predicate, object> and a test video, our objective is to localize the subject and object that are connected via the predicate. Given modern visio-lingual understanding capabilities, solving this problem is achiev-able, provided that there are large-scale annotated training examples available. However, annotating for every combi-nation of subject, object, and predicate is cumbersome, ex-pensive, and possibly infeasible. Therefore, there is a need for models that can learn to spatially and temporally lo-calize subjects and objects that are connected via an un-seen predicate using only a few support set videos shar-ing the common predicate. We address this challenging problem, referred to as few-shot referring relationships in videos for the first time. To this end, we pose the problem as a minimization of an objective function defined over a
T-partite random field. Here, the vertices of the random field correspond to candidate bounding boxes for the sub-ject and object, and T represents the number of frames in the test video. This objective function is composed of frame-level and visual relationship similarity potentials. To learn these potentials, we use a relation network that takes query-conditioned translational relationship embedding as inputs and is meta-trained using support set videos in an episodic manner. Further, the objective function is minimized using
a belief propagation-based message passing on the random field to obtain the spatiotemporal localization or subject and object trajectories. We perform extensive experiments using two public benchmarks, namely ImageNet-VidVRD and VidOR, and compare the proposed approach with com-petitive baselines to assess its efficacy. 1.

Introduction
Consider the following problem: given a video, a vi-sual relationship query represented as a <subject, predicate, object> tuple, and a support set of a few videos contain-ing the same predicate but not necessarily the same sub-jects and objects, our objective is to spatially and tempo-rally localize both subjects and objects that are related via the predicate within the video. We refer to this problem as
Few-shot Referring Relationship and illustrate it in Figure 1.
Solving this problem has the potential to benefit cross-task video understanding [41] and video retrieval [5, 7], among other applications.
Identifying its utility, referring rela-tionship task for images has been first introduced by [15].
However, referring relationships in videos poses additional video-specific challenges, such as understanding dynamic visual relationships. Some of these challenges have been addressed in recent research by Xiao et al. [35], but with a reliance on strong supervision. Referring relationships in videos within a few-shot setup is an under-explored area.
We aim to fill this research gap via our work.
Visual relationships inherently have long-tail distribu-tions in any video collection. For example, Image-Net Vid-VRD [27] dataset includes approximately 18.9% predicates with more than 100 instances but 20.5% predicates with less than 10 instances. This phenomenon is also shown in Fig-ure 1, where most predicates belong to the tail side of the distribution. The methods that work best for frequent visual relationships do not necessarily generalize well to unseen visual relationships. Moreover, in a real-world scenario an-notating visual relationships for each combination of sub-ject, object, and predicate are cumbersome, expensive, and possibly infeasible. Therefore, there is a need to study vi-sual relationship tasks in a few-shot setup. For instance: only with a few examples of the fly above predicate, such as videos containing <bird, fly above, person>, <helicopter, fly above, train> as shown in Figure 1 (a), a model should be able to generalize to the unseen visual relationship, such as <plane, fly above, person>. We propose a solution for
Few-shot Referring Relationship in videos in this work.
We pose the problem of a few-shot referring relation-ship in the video as a minimization of an objective func-tion defined over a T -partite random field where T is the number of frames in the test video. Furthermore, the ver-tices of the random field are treated as random variables and represent candidate bounding boxes for the subject and objects. The objective function consists of frame-level po-tentials and visual relationship similarity potentials, both of which are learned using a relation network that takes query-conditioned translational relationship embeddings as inputs.
We meta-train the relation network using support set videos in an episodic manner. Further, the objective function is minimized using a belief propagation-based message pass-ing on the random field to obtain subject and object tra-jectories. We perform extensive experiments on two pub-lic benchmarks, namely ImageNet-VidVRD [27] and Vi-dOR [31], and report the accuracy of localizing subject, object, and relation, denoted by Asub, Aobj, and Ar, re-spectively, along with other popular measures used in the literature. Our proposed approach clearly outperforms the related baselines.
The contributions of this work are three folds. (i) We propose a novel problem setup for referring relationship in videos, where the model must learn to localize the subject and object corresponding to a query visual relationship that was unseen during training using only a few support videos. (ii) We propose a new formulation to solve this task based on the minimization of an objective function on T -partite random field where T is the number of frames in the test video, and the vertices of the random field representing po-tential bounding boxes for subject and objects correspond to the random variables. (Section 3.1). (iii) Additionally, to enrich query-conditioned relational embeddings, we present two aggregation techniques, namely global semantic and local localization aggregations. The use of these aggrega-tion techniques results in enhanced relationship representa-tions, which helps to obtain better trajectories for objects and subjects related via the query visual relationship. This is evidenced by extensive experiments and ablations. (Sec-tions 3.2 and 4.5). 2.