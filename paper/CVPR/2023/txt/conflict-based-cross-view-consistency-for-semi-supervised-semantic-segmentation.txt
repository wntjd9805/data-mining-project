Abstract
Semi-supervised semantic segmentation (SSS) has re-cently gained increasing research interest as it can re-duce the requirement for large-scale fully-annotated train-ing data. The current methods often suffer from the confir-mation bias from the pseudo-labelling process, which can be alleviated by the co-training framework. The current co-training-based SSS methods rely on hand-crafted per-turbations to prevent the different sub-nets from collaps-ing into each other, but these artificial perturbations can-not lead to the optimal solution. In this work, we propose a new conflict-based cross-view consistency (CCVC) method based on a two-branch co-training framework which aims at enforcing the two sub-nets to learn informative features from irrelevant views. In particular, we first propose a new cross-view consistency (CVC) strategy that encourages the two sub-nets to learn distinct features from the same input by introducing a feature discrepancy loss, while these dis-tinct features are expected to generate consistent prediction scores of the input. The CVC strategy helps to prevent the two sub-nets from stepping into the collapse. In addition, we further propose a conflict-based pseudo-labelling (CPL) method to guarantee the model will learn more useful infor-mation from conflicting predictions, which will lead to a sta-ble training process. We validate our new CCVC approach on the SSS benchmark datasets where our method achieves new state-of-the-art performance. Our code is available at https://github.com/xiaoyao3302/CCVC. 1.

Introduction
Among different vision tasks, semantic segmentation is a fundamental vision task that enables the network to under-stand the world [3, 11, 12, 32, 33]. In recent years, deep neu-*This work was done during an internship at Samsung Research China-Beijing. This work is supported by Australian Research Council (ARC
DP200103223).
†Corresponding authors.
Figure 1. We compare the cosine similarity values between the features extracted by the two sub-nets of the traditional cross-consistency regularization (CCR) method and our CVC method.
We also compare the prediction accuracies of the two methods, measured by mIoU. We show that our CVC method can prevent the two sub-nets from collapsing into each other and inferring the input from irrelevant views, while CCR cannot guarantee the in-ferred views are different. We show our new method can increase the perception of the model, which produces more reliable pre-dictions. The experiments are implemented on the original Pascal
VOC dataset, under the 1/4 split partition with ResNet-101 as the backbone of the encoder. ral networks (DNNs) have shown great potential in seman-tic segmentation [18,31,57]. However, the success of DNNs is mainly due to the huge amount of annotated datasets. For the task of semantic segmentation, pixel-level annotations are often required, which means the annotators need to man-ually label up to hundreds of thousands of pixels per image.
Therefore, it takes great effort to collect precisely labelled data for training DNNs [1, 27, 30].
Various semi-supervised learning (SSL) methods are proposed to tackle the problem, which aim at learning a network by using only a small set of pixel-wise precisely annotated data and a large set of unlabelled data for seman-It is obvious that tic segmentation [2, 34, 37, 53, 54, 58]. the information from the labelled data is very limited as the number of labelled data is far less than the number of un-labelled data. Therefore, it becomes a key issue to fully exploit the unlabelled data to assist the labelled data for the model training.
One intuitive way to tackle this issue is pseudo-labelling [28,37,48]. However, SSL methods along this line may suffer from the so-called confirmation bias [48], which often leads to performance degradation due to the unsta-ble training process. Recently, consistency regularization-based SSL methods show promising performance [35, 38, 41, 46]. However, most of them rely on producing the pre-dictions of the weakly perturbed inputs to generate pseudo-labels, which are then used as the supervision to generate the predictions of the strongly perturbed inputs. Therefore, they still suffer from the confirmation bias issue.
On the other hand, co-training is a powerful framework for SSL as it enables different sub-nets to infer the same instance from different views and transfer the knowledge learnt from one view to another through pseudo-labelling.
Particularly, co-training relies on multi-view reference to increase the perception of the model, thus enhancing the reliability of the generated pseudo-labels [40]. Various semi-supervised semantic segmentation (SSS) approaches are based on co-training [10, 39]. The key point is how to prevent different sub-nets from collapsing into each other such that we can make correct predictions based on the in-put from different views. However, the hand-crafted pertur-bations used in most SSS methods cannot guarantee hetero-geneous features to be learned to effectively prevent sub-nets from stepping into a collapse.
Facing the above-mentioned issue, in this work, we come up with a new conflict-based cross-view consistency (CCVC) strategy for SSS, which makes sure the two sub-nets in our model can learn for different features separately so that reliable predictions could be learned from two irrel-evant views for co-training, thus further enabling each sub-net to make reliable and meaningful predictions. In particu-lar, we first raise a cross-view consistency (CVC) approach with a discrepancy loss to minimize the similarity between the feature extracted by the two sub-nets to encourage them to extract different features, which prevents the two sub-nets from collapsing into each other. Then we employ the cross pseudo-labelling to transfer the knowledge learnt from one sub-net to another to improve the perception of the network to correctly reason the same input from different views, thus producing more reliable predictions.
However, the discrepancy loss may introduce too strong a perturbation to the model that the feature extracted by the sub-nets may contain less meaningful information for the prediction, leading to inconsistent and unreliable predic-tions from the two sub-nets. This will incur the confirma-tion bias problem and thus harm the co-training of the sub-nets. To tackle this problem, we further propose a conflict-based pseudo-labelling (CPL) method, where we encourage the pseudo-labels generated by the conflicting predictions of each sub-net to have stronger supervision for the pre-diction of each other, to enforce the two sub-nets to make consistent predictions. Thereby, the useful features for the prediction could be preserved as well as the reliability of the predictions. In this way, hopefully, the influence of the confirmation bias can be reduced and the training process can be more stable.
As shown in Fig. 1, we can see the similarity scores be-tween the features extracted from the two sub-nets of the cross-consistency regularization (CCR) model remain at a high level, indicating the reasoning views of CCR are kind of relevant. In contrast, our CVC method ensures the rea-soning views are sufficiently different and thus produces more reliable predictions.
It should be mentioned that our CCVC method is com-patible with various existing data augmentation methods and it also benefits from an augmented training set with in-creased data diversity.
The contributions of our work are summarized as below:
• We introduce a cross-view consistency (CVC) strategy based on a co-training framework to make reliable pre-dictions, where we propose a feature discrepancy loss to enable the two-branch network to learn how to rea-son the input differently but make consistent predic-tions.
• We further propose a new conflict-based pseudo-labelling (CPL) method based on our cross-view con-sistency strategy to enable the two sub-nets to learn more useful semantic information from conflicting predictions to produce reliable and consistent predic-tions, which leads to a more stable training process.
• Our method achieves the state-of-the-art performance on the commonly used benchmark datasets, PASCAL
VOC 2012 [16] and Cityscapes [13]. 2.