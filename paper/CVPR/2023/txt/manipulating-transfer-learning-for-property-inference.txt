Abstract
Transfer learning is a popular method for tuning pre-trained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victim’s tuned downstream model. For example, to in-fer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to con-duct highly effective and specific property inference attacks (AUC score > 0.9), without incurring significant perfor-mance loss on the main task. The main idea of the ma-nipulation is to make the upstream model generate acti-vations (intermediate features) with different distributions for samples with and without a target property, thus en-abling the adversary to distinguish easily between down-stream models trained with and without training examples that have the target property. Our code is available at https:// github.com/ yulongt23/ Transfer-Inference. 1.

Introduction
Transfer learning is a popular method for efficiently training deep learning models [6, 21, 33, 39, 42]. In a typ-ical transfer learning scenario, an upstream trainer trains and releases a pretrained model. Then a downstream trainer will reuse the parameters of some layers of the released upstream models to tune a downstream model for a par-ticular task. This parameter reuse reduces the amount of data and computing resources required for training down-stream models significantly, making this technique increas-ingly popular. However, the centralized nature of transfer learning is open to exploitation by an adversary. Several previous works have considered security risks associated with transfer learning including backdoor attacks [39] and misclassification attacks [33].
*Indicates the corresponding author.
We investigate the risk of property inference in the context of transfer learning.
In property inference (also known as distribution inference), the attacker aims to ex-tract sensitive properties of the training distribution of a model [3, 7, 12, 29, 41]. We consider a transfer learning sce-nario where the upstream trainer is malicious and produces a carefully crafted pretrained model with the goal of infer-ring a particular property about the tuning data used by the victim to train a downstream model. For example, the at-tacker may be interested in knowing whether any images of a specific individual (or group, such as seniors or Asians) are contained in a downstream training set used to tune the pre-trained model. Such inferences can lead to severe pri-vacy leakage—for instance, if the adversary knows before-hand that the downstream training set consists of data of pa-tients that have a particular disease, confirming the presence of a specific individual in that training data is a privacy vi-olation. Property inference may also be used to audit mod-els for fairness issues [22]—for example, in a downstream dataset containing data of all the employees of an organi-zation, finding the absence of samples of a certain group of people (e.g., older people) may be evidence that those peo-ple are underrepresented in that organization.
Contributions. We identify a new vulnerability of trans-fer learning where the upstream trainer crafts a pretrained model to enable an inference attack on the downstream model that reveals very precise and accurate information about the downstream training data (Section 3). We develop methods to manipulate the upstream model training to pro-duce a model that, when used to train a downstream model, will induce a downstream model that reveals sensitive prop-erties of its training data in both white-box and black-box inference settings (Section 4). We demonstrate that this substantially increases property inference risk compared to baseline settings where the upstream model is trained nor-mally (Section 7). Table 1 summarizes our key results. The inference AUC scores are below 0.65 when the upstream models are trained normally; after manipulation, the infer-ences have AUC scores ≥ 0.89 even when only 0.1% (10 out of 10 000) of downstream samples have the target prop-Downstream Task
Upstream Task
Target Property
Normal Upstream Model Manipulated Upstream Model 0.1% (10) 0.1% (10) 1% (100) 1% (100)
Gender Recognition
Smile Detection
Age Prediction
Face Recognition
ImageNet Classification [9]
ImageNet Classification [9]
Specific Individuals
Smile Detection
ImageNet Classification [9]
Age Prediction
ImageNet Classification [9]
Senior
Asian 0.49 0.50 0.54 0.59 0.49 0.52 0.50 0.63 0.56 0.65 0.96 1.0 0.97 0.89 0.95 1.0 1.0 1.0 1.0 1.0
Table 1. Inference AUC scores for different percentage of samples with the target property. Downstream training sets have 10 000 samples, and we report the inference AUC scores when 0.1% (10) and 1% (100) samples in the downstream set have the target property. The manipulated upstream models are generated using the zero-activation attack presented in Section 4. erty and achieve perfect results (AUC score = 1.0) when the ratio increases to 1%. The manipulated models have negli-gible performance drops (< 0.9%) on their intended tasks.
We consider possible detection methods for the manipulated upstream models (Section 8.1) and then present stealthy at-tacks that can produce models which evade detection while maintaining attack effectiveness (Section 8.2). 2.