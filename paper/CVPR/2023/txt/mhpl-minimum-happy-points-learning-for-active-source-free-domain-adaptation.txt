Abstract
Source free domain adaptation (SFDA) aims to transfer a trained source model to the unlabeled target domain with-out accessing the source data. However, the SFDA setting faces a performance bottleneck due to the absence of source data and target supervised information, as evidenced by the limited performance gains of the newest SFDA meth-ods. Active source free domain adaptation (ASFDA) can break through the problem by exploring and exploiting a small set of informative samples via active learning.
In this paper, we first find that those satisfying the proper-ties of neighbor-chaotic, individual-different, and source-dissimilar are the best points to select. We define them as the minimum happy (MH) points challenging to explore with ex-isting methods. We propose minimum happy points learning (MHPL) to explore and exploit MH points actively. We de-sign three unique strategies: neighbor environment uncer-tainty, neighbor diversity relaxation, and one-shot query-ing, to explore the MH points. Further, to fully exploit MH points in the learning process, we design a neighbor focal loss that assigns the weighted neighbor purity to the cross entropy loss of MH points to make the model focus more on them. Extensive experiments verify that MHPL remarkably exceeds the various types of baselines and achieves signifi-cant performance gains at a small cost of labeling. 1.

Introduction
Transferring a trained source model instead of the source data to the unlabeled target domain, source-free domain adaptation (SFDA) has drawn much attention recently.
Since it prevents the external leakage of source data, SFDA meets privacy persevering [19, 47], data security [43], and data silos [39]. Moreover, it has important potential in many applications, e.g., object detection [23], object recog-nition [25], and semantic segmentation [17]. However, the
SFDA setting faces a performance bottleneck due to the ab-*denotes corresponding author. sence of source data and target supervised information. The state-of-the-art A2Net [50] is a very powerful method that seeks a classifier and exploits classifier design to achieve ad-versarial domain-level alignment and contrastive category-level matching, but it only improved the mean accuracy of the pioneering work (SHOT [25]) from 71.8% to 72.8% on the challenging Office-Home dataset [45]. Although some recent studies [20, 52] utilize the transformer or mix-up to improve the performance further, they have modified the structure of the source model or changed the source data, which is not universal in privacy-preserving scenarios.
Active source free domain adaptation (ASFDA) can pro-duce remarkable performance gains and breakthrough per-formance bottlenecks when a small set of informative tar-get samples labeled by experts. Two factors must be con-sidered to achieve significant performance gains: (1) Ex-ploring samples that, once labeled, will improve accuracy significantly; (2) Exploiting limited active labeled target data well in adaptation. However, these two factors have not been achieved. For example, ELPT [24] uses predic-tion uncertainty [27] to explore active samples and applies cross-entropy loss to exploit these selected samples. While the prediction uncertainty is error-prone due to the miscali-brated source model under distribution shift [32], and the pseudo-label noise of unlabeled samples easily influence the effect of standard cross-entropy loss on active samples.
In this paper, we first find the best informative sam-ples for ASFDA are Minimum Happy (MH) points that sat-isfy the properties of neighbor-chaotic, individual-different, and source-dissimilar. (1) The property of neighbor-chaotic refers to the sample’s neighbor labels being very inconsis-tent, which measures the sample uncertainty through its en-vironment. The Active Learning (AL) and Active DA meth-ods, which rely on the miscalibrated model output [32] or domain discrepancy, can’t identify these uncertain samples in ASFDA. As shown in Fig. 1, the samples selected by our method are more likely to fall into red blocks with label-(2) The property chaotic environments than BVSB [14]. of individual-different guarantees the diversity of selected uncertain samples to improve the effectiveness of querying.
(a) Initial pseudo-labels (b) BVSB [14] (c) CoreSet [36] (d) MHPL
Figure 1. Feature visualization for the source model with 5% actively labeled target data on the Cl→Pr task. Different colors in (a) represent different classes of pseudo-labels by clustering. Blue blocks include easily-adaptive source-similar samples with label-clean neighbors that can be learned well by SFDA methods. Red blocks include the hard-adaptive source-dissimilar samples with label-chaotic neighbors. In (b), (c), and (d), the dark green indicates that the pseudo-label is consistent with the true label, and light blue indicates the opposite. The red stars indicate the selected samples based on BVSB, CoreSet, and our MHPL.
Previous methods [32,36,38] ensure sample diversity across the global target domain. However, they would select al-ready well-aligned source-similar samples [32] that are less informative for target adaptation as they can be learned by
SFDA methods [16, 44]. Fig. 1 illustrates that compared with CoreSet [36], most samples selected by our method are diverse in the source-dissimilar regions (red blocks). (3)
The informative samples should be source-dissimilar, as the source-dissimilar samples are more representative of the tar-get domain and need to be explored. Most Active DA meth-ods [6, 40] ensure source-dissimilar samples based on mea-suring the distribution discrepancy across domains, which is unavailable in ASFDA due to unseen source data.
Concerning the exploitation of selected samples, most methods of AL [10, 34, 36], Active DA [6, 32, 40, 51], and
ELPT [24] deem them as ordinary labeled samples and use standard supervised losses to learn them. However, the number of selected samples is so tiny in ASFDA that they occupy a small region of the target domain. With standard supervised losses, the model cannot be well generalized to the entire target domain, leading to poor generalization.
We propose the Minimum Happy Points Learning (MHPL) to explore and exploit the informative MH points.
First, to measure the sample uncertainty, we propose a novel uncertainty metric, neighbor environment uncertainty, that is based on the purity and affinity characteristics of neigh-bor samples. Then, to guarantee the individual difference, we propose neighbor diversity relaxation based on perform-ing relaxed selection among neighbors. Furthermore, the source-dissimilar characteristic of samples is maintained by our proposed one-shot querying. We select target samples at once based on the source model, as the source model without fine-tuning can better describe the distribution dis-crepancy across domains and the source-dissimilar samples are more likely to be explored.
In addition, the selected samples are fully exploited by a new-designed neighbor fo-cal loss, which assigns the weighted neighbor purity to the cross-entropy loss of MH points to make the model focus
Figure 2. The comparison of ASFDA baselines (SHOT [25] + *, * denotes the active strategy), ELPT, and our MHPL with 5% active labeled target samples on Ar→Cl in the Office-Home. and learn more about them. As shown in Fig. 2, our MHPL significantly outperforms the ASFDA baselines, i.e., an ef-fective SFDA method (SHOT) + AL strategies, and the ex-isting state-of-the-art ASFDA approach, ELPT [24].
Our contributions can be summarized as follows: (1) We discover and define the most informative active samples,
Minimum Happy (MH) points for ASFDA; (2) We propose a novel MHPL framework to explore and exploit the MH points with the neighbor environment uncertainty, neighbor diversity relaxation, one-shot querying, and neighbor focal loss; (3) Extensive experiments verify that MHPL surpasses state-of-the-art methods. 2.