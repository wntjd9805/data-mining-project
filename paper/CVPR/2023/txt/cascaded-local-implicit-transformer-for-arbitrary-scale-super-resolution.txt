Abstract
Implicit neural representation has recently shown a promising ability in representing images with arbitrary res-olutions. In this paper, we present a Local Implicit Trans-former (LIT), which integrates the attention mechanism and frequency encoding technique into a local implicit image function. We design a cross-scale local attention block to ef-fectively aggregate local features and a local frequency en-coding block to combine positional encoding with Fourier domain information for constructing high-resolution im-ages. To further improve representative power, we pro-pose a Cascaded LIT (CLIT) that exploits multi-scale fea-tures, along with a cumulative training strategy that grad-ually increases the upsampling scales during training. We have conducted extensive experiments to validate the effec-tiveness of these components and analyze various training strategies. The qualitative and quantitative results demon-strate that LIT and CLIT achieve favorable results and out-perform the prior works in arbitrary super-resolution tasks. 1.

Introduction
Single Image Super-Resolution (SISR) is the process of reconstructing high-resolution (HR) images from their corresponding low-resolution (LR) counterparts. SISR has long been recognized as a challenging task in the low-level vision domain due to its ill-posed nature, and has attracted a number of researchers dedicated to this ﬁeld of study over the past decade [1–21]. A line of SISR research referred to as ‘ﬁxed-scale SR’ [1–15] focuses on extracting feature em-beddings from LR images and leveraging these embeddings to upsample images with a predeﬁned factor through learn-able deconvolutions [3] or sub-pixel convolutions [4]. De-spite their success, many of the proposed approaches neces-sitate a distinct deep neural network model for each upsam-pling scale, which is usually restricted to a limited selection of integers (e.g., 2×, 3×, 4×). Such a limitation constrains the potential applications and deployment options of SISR models. To overcome this limitation, approaches for up-sampling LR images in a continuous manner via a single model emerge and attracted considerable attention recently.
Over the past few years, arbitrary-scale SR has emerged and attracted considerable attention from researchers [16– 21]. Apart from the pioneering work Meta-SR [16], recent
*Equal contribution
Figure 1. An illustration and comparison of different approaches that take into account nearby pixels for continuous upsampling: (a) the local ensem-ble method used in [17], and (b) our proposed local attention mechanism. endeavors [17–21] have achieved arbitrary-scale SR by re-placing the upsampling layers commonly adopted by pre-vious approaches with local implicit image functions, and have demonstrated favorable performance. These local im-plicit functions employ multi-layer perceptrons (MLPs) to map 2D coordinates and corresponding latent representa-tions to RGB values. Fig. 1 illustrates how different ap-proaches sample latent representations based on the queried coordinates (depicted as the red dots). Fig. 1 (a) illus-trates the local ensemble technique adopted by contempo-rary mainstream methods [17–21]. It calculates the RGB value of the queried coordinate by taking the weighted av-erage of those of the surrounding four pixels based on their relative distances to the queried coordinate. This approach, however, does not consider contextual information and re-lies solely on distance. For instance, in Fig. 1, the queried coordinates are intentionally designed to lie on edges. How-ever, merely calculating the weighted average of pixels fails to reﬂect the contextual information about the image con-tent, thereby preventing the accurate capture of the neces-sary features for performing SR. As a result, although pixel distance plays a vital role in SR tasks, it is essential to con-centrate more on the contextual information in an image.
In light of the above observations, we propose a Lo-cal Implicit Transformer (LIT), which expands the num-bers of referenced latent vectors and accounts for the fea-ture correlation in the context by exploiting the attention mechanism [22]. LIT comprises a Cross-Scale Local Atten-tion Block (CSLAB), a Local Frequency Encoding Block
(LFEB), and a decoder. CSLAB generates attention maps based on the bilinearly interpolated latent vectors at the queried coordinates and the key latent vectors sampled from a grid of coordinates with relative positional bias [23, 24].
The ﬁrst and second columns of Fig. 1 (b) visualize the at-tention maps generated by LIT, where the attention areas align closely with the edges. By applying attention maps to feature embeddings, the RGB values of the queried co-ordinates can be contextually predicted. Moreover, inspired by [21, 25, 26], we introduce LFEB, which projects rela-tive coordinates into latent space to address the spectral bias problem [27] of an implicit neural function. Speciﬁcally, relative coordinates are encoded into relative positional en-coding, which are multiplied with the frequency encoding extracted from the feature embedding in the Fourier domain to generate the frequency embeddings. This design enables the frequency embedding to integrate the relative positional encoding with texture information, thereby augmenting the expressivity of relative coordinates. Finally, a decoder is adopted to produce RGB values by taking advantage of the attention feature embedding and the frequency embedding.
In order to address the issue of diverse scaling factors and achieve arbitrary-scale super-resolution, it is crucial to consider the role of upsampling factors in constructing high-resolution images within the local implicit image func-tion. However, simultaneously training a local implicit im-age function with a wide range of upsampling factors (e.g., 1× ∼ 30×) poses signiﬁcant challenges. As a result, we propose a cumulative training strategy to incrementally en-hance the fuction’s its representative power. The strategy initially trains the local implicit image function with small upsampling factors and then ﬁnetunes it with alternatively sampled small and large ones. Furthermore, we present
Cascaded LIT (CLIT) to harness the advantages of multi-scale feature embeddings, complementing missing details and information during one-step upsampling. The combi-nation of the cumulative training strategy and CLIT enables efﬁcient and effective handling of arbitrary-scale SR tasks.
The main contributions of our work are summarized as follows: (1) We introduce the LIT architecture, which incor-porates the local attention mechanism into arbitrary-scale
SR (2) We further develop a cumulative training strategy and the cascaded framework CLIT to effectively handle large-scale upsampling. (3) We carry out comprehensive analyses of the performance impacts for LIT and CLIT. The extensive experimental ﬁndings demonstrate that the pro-posed LIT and CLIT are able to yield remarkable or com-parable results across a wide range of benchmark datasets.
The paper is organized as follows. Section 2 reviews the related work. Section 3 walks through the proposed LIT and
CLIT frameworks and the implementation details. Section 4 presents the experimental results. Section 5 concludes. 2.