Abstract
Many visual recognition models are evaluated only on their classification accuracy, a metric for which they obtain strong performance. In this paper, we investigate whether computer vision models can also provide correct rationales for their predictions. We propose a “doubly right” ob-ject recognition benchmark, where the metric requires the model to simultaneously produce both the right labels as well as the right rationales. We find that state-of-the-art visual models, such as CLIP, often provide incorrect ratio-nales for their categorical predictions. However, by trans-ferring the rationales from language models into visual rep-resentations through a tailored dataset, we show that we can learn a “why prompt,” which adapts large visual repre-sentations to produce correct rationales. Visualizations and empirical experiments show that our prompts significantly improve performance on doubly right object recognition, in addition to zero-shot transfer to unseen tasks and datasets. 1.

Introduction
Computer vision models today are able to achieve high accuracy – sometimes super-human – at correctly recogniz-ing objects in images. However, most models today are not evaluated on whether they get the prediction right for the right reasons [14, 19, 48, 53]. Learning models that can ex-plain their own decision is important for building trustwor-thy systems, especially in applications that require human-machine interactions [2, 15, 37, 50]. Rationales that justify the prediction can largely improve user trust [54], which is a crucial metric that the visual recognition field should push forward in the future.
Existing methods in interpretability have investigated how to understand which features contribute to the mod-els’ prediction [33, 34, 44, 46, 47, 52, 60]. However, saliency explanations are often imprecise, require domain expertise to understand, and also cannot be evaluated. [20, 22] have instead explored verbal rationales to justify the decision-making. However, they require manual collections of the
Figure 1. Visual reasoning for doubly right object recognition task.
Motivated by prompting in NLP, we learn a why prompt from mul-timodal data, which allows us to instruct visual models to predict both the right category and the correct rationales that justify the prediction. plausible rationales in the first place, which subsequently are limited to small-scale datasets and tasks [24, 56].
Scalable methods for explainability have been developed in natural language processing (NLP) through prompting.
By adding additional instructions to the input, such as the sentence “think step-by-step,” language models then output descriptions of their reasoning through the chain of thought process [57]. Since the explanations are verbal, they are easily understandable by people, and since the mechanism emerges without explicit supervision, it is highly scalable.
In this paper, we investigate whether visual representations can also explain their reasoning through visual chain-of-thought prompts.
Our paper first introduces a benchmark for doubly right object recognition, where computer vision models must pre-dict both correct categorical labels as well as correct ra-tionales. Our benchmark is large, and covers many cate-gories and datasets. We found that the visual representa-tions do not have double right capability out-of-the-box on our benchmark. The recent large-scale image-language pre-trained models [41, 49] can retrieve open-world language descriptions that are closest to the image embedding in the feature space, serving as verbal explanations. However, the models often select the wrong rationales.
Instead, we propose a framework to explicitly trans-fer the chain-of-thought reasoning from NLP models into
vision models. We first query the large-scale language model [9] via the chain-of-thought reasoning for object cat-egory, where we obtain language rationales that explain dis-criminative features for an object. We then collect images containing both the category and the rationale features us-ing Google image search. We then train visual prompts to transfer the verbal chain of thought to visual chain of thought with contrastive learning, where features of im-ages and their rationales are pulled together. Our “why” prompts obtain up to 26 points gain at doubly right per-formance when evaluated on our benchmark. In addition, visualizations and quantitative results show that our why prompts zero-shot transfer to unseen tasks and datasets. We believe this “doubly right” object recognition task is a fu-ture direction which the visual recognition field should go forward for. Our data and code is available at https:
//github.com/cvlab-columbia/DoubleRight. 2.