Abstract
Language-guided image inpainting aims to fill the defec-tive regions of an image under the guidance of text while keeping the non-defective regions unchanged. However, di-rectly encoding the defective images is prone to have an adverse effect on the non-defective regions, giving rise to dis-torted structures on non-defective parts. To better adapt the text guidance to the inpainting task, this paper pro-poses N ¨UWA-LIP, which involves defect-free VQGAN (DF-VQGAN) and a multi-perspective sequence-to-sequence mod-ule (MP-S2S). To be specific, DF-VQGAN introduces relative estimation to carefully control the receptive spreading, as well as symmetrical connections to protect structure details unchanged. For harmoniously embedding text guidance into the locally defective regions, MP-S2S is employed by ag-gregating the complementary perspectives from low-level pixels, high-level tokens as well as the text description. Ex-periments show that our DF-VQGAN effectively aids the inpainting process while avoiding unexpected changes in non-defective regions. Results on three open-domain bench-marks demonstrate the superior performance of our method against state-of-the-arts. Our code, datasets, and model will be made publicly available1. 1.

Introduction
The task of image inpainting, which aims to fill missing pixels in the defective regions with photo-realistic struc-tures, is as ancient as art itself [2]. Despite its practical applications [18, 32, 34], such as image manipulation, image completion, and object removal, the task poses significant challenges, including the effective extraction of valid fea-tures from defective input and the generation of semantically consistent results.
With the remarkable success of vision-language learning, language-guided image inpainting has become a promis-ing topic [24, 28, 33], which enables the generation of con-trollable results with the guidance of text description (see the completed results in Fig. 1). Recently, multimodal pre-1https://github.com/kodenii/NUWA-LIP
Figure 1. Language-guided inpainting results via N ¨UWA-LIP.
Text descriptions provide effective guidance for inpainting the de-fective image with desired objects. More examples are in the suppl. training methods based on diffusion and autoregressive mod-els have exhibited impressive capabilities in synthesizing various and photo-realistic images, such as Stable Diffu-sion [21], Parti [29] and N ¨UWA [25]. In particular, N ¨UWA has demonstrated a promising capability for language-guided image generation, suggesting the potential for combining this pre-training schema with VQVAE and Transformer for language-guided image inpainting.
It is worth noting that this work focuses on addressing the challenge of processing defective images with some regions filled with zeros (see the first image in Fig. 1). This setting has been widely adopted in previous works [24, 28, 33] and is consistent with real-world corrupted image inpainting. To learn unified representations of the vision and language, it is crucial to ensure that the representation of non-defective regions is accurate and remains unaffected by defective parts.
This requirement exacerbates the challenges associated with our method, as it involves the effective extraction of valid features from defective input.
However, existing pre-trained image generative mod-els [21, 25, 29] are usually trained on non-defective images.
When used in the image inpainting task, these models en-code the whole image and fuse the features from defective regions into the representations of non-defective parts, re-Figure 2. Illustration of Receptive Spreading and Information Losing. As for receptive spreading, we can observe an obvious change and a blurry boundary on the reconstructed non-defective region. In terms of information losing, some unexpected changes are also apparent in the output of the non-defective regions. sulting in what we call receptive spreading of the defective region. These types of approaches can severely limit the ac-curacy of modeling non-defective regions, especially when the defective region is large, and make it difficult to match language descriptions. Furthermore, in the VQVAE-based model, the known non-defective regions are challenging to reconstruct exactly the same as the original image due to the compression of the image into discrete tokens. We refer to this as information losing in the non-defective region (see
Fig. 2 for an illustration).
To enable effective adaptation of text descriptions to these types of defective images, we propose N ¨UWA-LIP which leverages a novel defect-free VQGAN (DF-VQGAN) and a multi-perspective sequence-to-sequence module (MP-S2S).
In contrast to VQGAN [8], DF-VQGAN incorporates the relative estimation to decouple defective and non-defective regions. This helps to control receptive spreading and ob-tain accurate visual representations for vision-and-language (VL) learning in MP-S2S. To retain the information of non-defective regions, symmetrical connections replenish the lost information from the features in the encoding procedure.
Additionally, MP-S2S further enhances visual information from complementary perspectives, including low-level pix-els, high-level tokens, and the text description.
Moreover, we construct three datasets to evaluate the per-formance of language-guided image inpainting and conduct a comprehensive comparison with existing methods. Experi-ments demonstrate that N ¨UWA-LIP outperforms competing methods by a significant margin on all three benchmarks.
An ablation study is further conducted to evaluate the effec-tiveness of each component in our N ¨UWA-LIP.
The main contributions are summarized as follows:
• To effectively encode the defective input, we propose a
DF-VQGAN, which introduces relative estimation to control receptive spreading and symmetrical connec-tions to retain the information of non-defective regions.
• We propose a multi-perspective sequence-to-sequence module for enhancing visual information from comple-mentary perspectives of pixel, token, and text domains.
• We build three open-domain datasets for evaluating language-guided image inpainting. Experiments show that N ¨UWA-LIP achieves state-of-the-art performance in comparison with the competing methods. 2.