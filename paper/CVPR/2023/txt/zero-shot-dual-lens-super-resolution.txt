Abstract
The asymmetric dual-lens conﬁguration is commonly available on mobile devices nowadays, which naturally stores a pair of wide-angle and telephoto images of the same scene to support realistic super-resolution (SR). Even on the same device, however, the degradation for model-ing realistic SR is image-speciﬁc due to the unknown ac-quisition process (e.g., tiny camera motion). In this paper, we propose a zero-shot solution for dual-lens SR (ZeDuSR), where only the dual-lens pair at test time is used to learn an image-speciﬁc SR model. As such, ZeDuSR adapts itself to the current scene without using external training data, and thus gets rid of generalization difﬁculty. However, there are two major challenges to achieving this goal: 1) dual-lens alignment while keeping the realistic degradation, and 2) effective usage of highly limited training data. To overcome these two challenges, we propose a degradation-invariant alignment method and a degradation-aware training strat-egy to fully exploit the information within a single dual-lens pair. Extensive experiments validate the superiority of Ze-DuSR over existing solutions on both synthesized and real-world dual-lens datasets. The implementation code is avail-able at https://github.com/XrKang/ZeDuSR. 1.

Introduction
Mobile devices such as smartphones are generally equipped with an asymmetric camera system consisting of multiple lenses with different focal lengths. As a common conﬁguration, with a wide-angle lens and a telephoto lens, one can capture the same scene with different ﬁeld-of-views (FoVs). Within the overlapped FoV, the wide-angle and telephoto images naturally store low-resolution (LR) and high-resolution (HR) counterparts for learning a realistic super-resolution (SR) model. This provides a feasible way to obtain an HR image with a large FoV at the same time, which is beyond the capability of the original device.
There are a few pioneer works along this direction, which are referred to as dual-lens/camera/zoomed SR inter-∗Equal contribution. †Corresponding author. (cid:1)(cid:2) (cid:1)(cid:2) (cid:1)(cid:2) (cid:1) (cid:2)
Figure 1. Degradation kernel (estimated by KernelGAN [1]) clus-tering on wide-angle images from iPhone11 [43]. changeably. Beyond traditional methods that simply trans-fer the telephoto content to the wide-angle view in the overlapped FoV, recent deep-learning-based methods en-able resolution enhancement of the full wide-angle image.
Speciﬁcally, Wang et al. [43] utilize the overlapped FoV of dual-lens image pairs to learn a reference-based SR model, where the wide-angle view is super-resolved by using the telephoto view as a reference. However, the external train-ing data are synthesized with the predeﬁned bicubic degra-dation, which leads to generalization difﬁculty when the trained SR model is applied to real devices. To narrow the domain gap between the training and inference stages, they further adopt a self-supervised adaptation strategy to ﬁne-tune the pretrained model on real devices. On the other hand, Zhang et al. [53] adopt self-supervised learning to train an SR model directly on a dual-lens dataset to avoid the predeﬁned degradation, with the assumption of consis-tent degradation on the same device.
In practice, however, the degradation kernel for model-ing realistic SR is inﬂuenced by not only the camera optics but also tiny camera motion during the acquisition process on mobile devices [1, 6, 36], resulting in the image-speciﬁc degradation for each dual-lens pair, even on the same de-vice. Figure 1 gives an exemplar analysis, where we es-timate the degradation kernels of 146 wide-angle images captured by the dual-lens device iPhone11 [43] and cluster them using t-SNE [41]. Although these images are captured by the same device, they exhibit notably different degrada-tion kernels due to the unknown acquisition process. This limits the performances of previous dual-lens SR methods for practical applications, since they assume that the realis-e l g n a (cid:649) e d i
W o t o h p e l e
T
DCSR (cid:135) Predefined degradation (cid:135) Extra dual(cid:649)lens(cid:3)dataset (cid:135) Supervised(cid:3)learning
SelfDZSR (cid:135) Device(cid:649)consistent(cid:3)degradation (cid:135) Extra(cid:3)dual(cid:649)lens(cid:3)dataset (cid:135) Self(cid:649)supervised learning
ZeDuSR (cid:135) Image(cid:649)specific(cid:3)degradation (cid:135) No extra dual(cid:649)lens(cid:3)dataset (cid:135) Zero(cid:649)shot learning
DCSR
SelfDZSR
ZeDuSR
Input
Dual(cid:649)lens(cid:3)SR(cid:3)methods
Result
Figure 2. Overview of dual-lens SR methods and visual compari-son of 2× SR on the real-world data. tic degradation is at least device-consistent.
In this paper, we propose a zero-shot learning solution for realistic SR on dual-lens devices, termed as ZeDuSR, which learns an image-speciﬁc SR model solely from the dual-lens pair at test time. As such, ZeDuSR adapts itself to the current scene under the unknown acquisition process and gets rid of the generalization difﬁculty when using ex-ternal training data. Figure 2 summarizes the main differ-ences of ZeDuSR from existing dual-lens SR methods with a real-world reconstruction example. ZeDuSR gives a visu-ally improved result compared with its competitors, thanks to the image-speciﬁc degradation assumption.
There are two major challenges to achieving the success of ZeDuSR. First, as also noticed in previous works [43,53], it is non-trivial to exploit the information of the dual-lens image pair, due to the spatial misalignment caused by the physical offset between the two lenses. Moreover, we ﬁnd that the alignment process for generating the training data will introduce additional frequency information, which in-evitably changes the realistic degradation. To overcome this challenge, we propose to constrain the alignment pro-cess in spatial, frequency, and feature domains simulta-neously to keep the degradation. Speciﬁcally, we pro-pose a degradation-invariant alignment method by leverag-ing adversarial and contrastive learning. The other chal-lenge is how to effectively use the highly limited data for learning an image-speciﬁc SR model. To this end, we design a degradation-aware training strategy to fully ex-ploit the information within a single dual-lens pair. That is, we calculate the probability of each location for patch cropping according to the degradation similarity between the images before and after alignment. As such, samples that keep the degradation are assigned a higher probabil-ity to be selected during training, while the contribution of degradation-variant samples is reduced.
We evaluate the performance of ZeDuSR on both syn-thesized and real-world dual-lens datasets. For quantita-tive evaluation with the HR ground-truth, we adopt a stereo dataset (Middlebury2021 [35]) and a light ﬁeld dataset (HCI new [12], with two views selected) to simulate dual-lens image pairs with different baselines between the two lenses, by applying image-speciﬁc degradation on one of the two views. Besides, we also perform the qualita-tive evaluation on two real-world datasets, i.e., CameraFu-sion [43] captured by iPhone11 and RealMCVSR [18] cap-tured by iPhone12, where no additional degradation is in-troduced beyond the realistic one between the two views.
Extensive experiments on both synthesized and real-world datasets demonstrate the superiority of our ZeDuSR over existing solutions including single-image SR, reference-based SR, and dual-lens SR.
Contributions of this paper are summarized as follows:
• We propose a zero-shot learning solution for realis-tic SR on dual-lens devices, which assumes image-speciﬁc degradation and adapts itself to the current scene under the unknown acquisition process.
• We propose a degradation-invariant alignment method by leveraging adversarial and contrastive learning to con-strain the alignment process in spatial, frequency, and fea-ture domains simultaneously.
• We design a degradation-aware training strategy to ef-fectively exploit the information within the highly limited training data, i.e., a single dual-lens pair at test time.
• We conduct extensive experiments on both synthesized and real-world dual-lens datasets to validate the superiority of our zero-shot solution. 2.