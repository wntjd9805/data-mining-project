Abstract
Compared with traditional RGB-only visual tracking, few datasets have been constructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new RGB-D track-ing dataset for both static and dynamic scenes captured by consumer-grade LiDAR scanners equipped on Apple’s iPhone and iPad. ARKitTrack contains 300 RGB-D se-quences, 455 targets, and 229.7K video frames in total.
Along with the bounding box annotations and frame-level attributes, we also annotate this dataset with 123.9K pixel-level target masks. Besides, the camera intrinsic and cam-era pose of each frame are provided for future develop-ments. To demonstrate the potential usefulness of this dataset, we further present a unified baseline for both box-level and pixel-level tracking, which integrates RGB fea-tures with bird’s-eye-view representations to better explore
In-depth empirical analy-cross-modality 3D geometry.
†Equal contribution
*Corresponding author: Dr. Lijun Wang, ljwang@dlut.edu.cn sis has verified that the ARKitTrack dataset can signif-icantly facilitate RGB-D tracking and that the proposed baseline method compares favorably against the state of the arts. The code and dataset is available at https:
//arkittrack.github.io. 1.

Introduction
As a fundamental and longstanding problem in computer vision, visual tracking has been studied for decades and achieved significant progress in recent years with many ad-vanced RGB trackers [6, 7, 9, 20, 37, 39] and datasets [11, 14, 29, 45] being developed. Nonetheless, there still exist many challenging situations such as occlusion, distraction, extreme illumination, etc., which have not been well ad-dressed. With the wide application of commercially avail-able RGB-D sensors, many recent works [15, 33, 42, 54, 55] have focused on the RGB-D tracking problem, as depth can provide additional 3D geometry cues for tracking in com-plicated environments. The development of RGB-D track-ing is always boosted by the emergence of RGB-D tracking datasets. The early RGB-D datasets [36, 43] only have a limited number of video sequences and can hardly meet the requirement of sufficient training and evaluating sophisti-cated RGB-D trackers. To alleviate this issue, two larger datasets [26, 48] have been built recently and successfully adopted in the VOT-RGBD challenges [16–18].
Though existing RGB-D tracking datasets strongly bene-fit the development of RGB-D trackers, they are still limited in the following two aspects. First, these datasets are col-lected using Realsense or Kinect depth cameras, which re-quire edge devices for onsite computing or post-processing and are not easily portable. As a result, it severely re-stricts the scene diversity, and most videos are captured un-der static scenes, which causes a large domain gap between the collected dataset and real-world applications. Second, existing RGB-D tracking datasets only contain bounding box-level annotations and mostly fail to provide pixel-level mask labels. Therefore, they are not applicable for train-ing/evaluating pixel-level tracking tasks (i.e., VOS).
With the recent launch of built-in depth sensors of mo-bile phones (e.g., LiDAR, ToF, and stereo cameras) and the release of AR frameworks (e.g., Apple’s ARKit [3] and
Google’s ARCore [2]), it becomes more convenient than ever to capture depth data under diverse scenes using mobile phones. Compared to prior depth devices, mobile phones are highly portable and more widely used for daily video recording. Besides, the depth maps captured by consumer-grade sensors mounted on mobile phones are also different from previous datasets in terms of resolution, accuracy, etc.
In light of the above observations, we present ARKit-Track, a new RGB-D tracking dataset captured using iPhone built-in LiDAR with the ARKit framework. The dataset contains 300 RGB-D sequences, 229.7K video frames, and 455 targets. Precise box-level target loca-tions, pixel-level target masks, and frame-level attributes are also provided for comprehensive model training and evaluation. Compared to existing RGB-D tracking datasets,
ARKitTrack enjoys the following two distinct advantages.
First, ARKitTrack covers more diverse scenes captured un-der both static and dynamic viewpoints. Camera intrinsic and 6-DoF poses estimated using ARKit are also provided for more effective handling of dynamic scenes. Therefore,
ARKitTrack is more coincide with real application scenar-ios, particularly for mobile phones. Second, to our best knowledge, ARKitTrack is one of the first RGB-D track-ing datasets annotated with both box-level and pixel-level labels, which is able to benefit both VOT and VOS.
To demonstrate the strong potential of ARKitTrack, we design a general baseline RGB-D tracker, which effectively narrows the gap between visual object tracking (VOT) and segmentation (VOS). Most existing RGB-D trackers em-ploy the low-level appearance cues (e.g., contours and re-gions) of depth maps but fail to explore the 3D geometry information. To remedy this drawback, we propose to in-tegrate RGB features with bird’s-eye-view (BEV) represen-tations through a cross-view feature fusion scheme, where
RGB feature is mainly used for target appearance modeling and BEV representations built from depth maps can better capture 3D scene geometry. Experiments on our ARKit-Track datasets demonstrate the merit of the baseline tracker.
Our contribution can be summarized into three folds:
• A new RGB-D tracking dataset, ARKitTrack, contain-ing diverse static and dynamic scenes with both box-level and pixel-level precise annotations.
• A unified baseline method for RGB-D VOT and VOS, combining both RGB and 3D geometry for effective
RGB-D tracking.
• In-depth evaluation and analysis of the new dataset and the baseline method, providing new knowledge to pro-mote future study in RGB-D tracking. 2.