Abstract
In Composed Image Retrieval (CIR), a user combines a query image with text to describe their intended target.
Existing methods rely on supervised learning of CIR mod-els using labeled triplets consisting of the query image, text speciﬁcation, and the target image. Labeling such triplets is expensive and hinders broad applicability of CIR. In this work, we propose to study an important task, Zero-Shot
Composed Image Retrieval (ZS-CIR), whose goal is to build a CIR model without requiring labeled triplets for training.
To this end, we propose a novel method, called Pic2Word, that requires only weakly labeled image-caption pairs and unlabeled image datasets to train. Unlike existing super-vised CIR models, our model trained on weakly labeled or unlabeled datasets shows strong generalization across diverse ZS-CIR tasks, e.g., attribute editing, object com-position, and domain conversion. Our approach outper-forms several supervised CIR methods on the common CIR benchmark, CIRR and Fashion-IQ. Code will be made pub-licly available at https://github.com/google-research/composed_image_retrieval 1.

Introduction
Composed image retrieval (CIR) [2, 4, 16, 18, 35] aims to retrieve images using a query composed of an image and text. In contrast to the image-based retrieval systems [7],
CIR is advantageous as it allows retrieval of images with a higher precision thanks to the text query that incorporates user’s intent, such as a desired modiﬁcation to the query image. With a surge of image-text models [1, 20, 30], CIR has received attention recently for diverse real-world appli-cations in e-commerce and internet search.
Several approaches [2, 4, 16, 18, 35] have been proposed to solve CIR problems, including attribute manipulation for fashion image search, composing objects, and converting
∗Work done during internship at Google Cloud AI Research.
Figure 1. Composed Image Retrieval (CIR) takes a query com-posed of an image (denoted as (cid:63)) plus a text modiﬁer and retrieves matching images. CIR encompasses diverse tasks, such as domain conversion (origami of (cid:63)), scene or object composition ((cid:63) in the pool), or fashion-attribute manipulation ((cid:63) with blue ﬂoral print).
Top: Existing methods [2,35] train a separate model for each task, and require strong triplet supervision. Bottom: We propose a new task, Zero-shot CIR, and solve diverse CIR sub-tasks using a sin-gle model trained only on image-caption pairs and unlabeled im-age datasets. the style of images for content creation, as in Fig. 1. At the core of CIR is learning how to compose information from an image and text. We identify two fundamental issues with existing solutions. First, previous methods require a large amount of labeled data, which comes in the form of triplets consisting of a reference image, text, and a target image, to train their retrieval model. The dataset collection involves two processes [25] – collecting pairs of a related reference and the target images as a query-output pair to the CIR system, then giving a description that modiﬁes the reference to the target. Examples of labeled triplets are at 1
the top of Fig. 1. We note that both steps incur a signiﬁcant labeling cost. Second, the model trained on labeled data is specialized to speciﬁc use-cases and may not generalize to different CIR tasks.
To tackle these challenges, we propose a new task, zero-shot composed image retrieval (ZS-CIR). In ZS-CIR, our goal is to build a single CIR model that performs diverse
CIR tasks, such as object composition, attribute editing, or domain conversion, as in the bottom of Fig. 1, with-out requiring an expensive labeled triplet data collection ef-fort. Instead, we propose to train our retrieval model us-ing large-scale image-caption pairs and unlabeled images, which are considerably cheaper to collect than supervised
CIR datasets at scale.
To harness these weakly labeled and unlabeled datasets, we propose a two-stage framework for learning ZS-CIR models. The ﬁrst stage conducts contrastive language-image pretraining (CLIP) [30] on the image-caption dataset, training a two-tower model jointly to maximize the similar-ity between an image and a caption. We note that some previous works [2] build their models on CLIP followed by a second-stage supervised CIR training. On the contrary, instead of relying on the triplet-labeled training data, we leverage the linguistic capability of the language encoder in CLIP, which excels at composing diverse concepts or at-tributes to generate embeddings that are close to the corre-sponding visual representations. The idea is to map a pic-ture to a word token such that the language encoder can ﬂex-ibly and seamlessly compose the query image features and text descriptions. We learn a lightweight mapping network that converts an image embedding of the CLIP vision en-coder into a token embedding of its language encoder. The mapping network is trained with a contrastive loss to recon-struct the image embedding, which only requires unlabeled images. We call our method Pic2Word and illustrate it in
Fig. 2.
In experiments, we show the strength of our Pic2Word approach on various CIR tasks, including domain conver-sion, object composition, scene manipulation, and fashion attribute manipulation. We show that our zero-shot ap-proach performs on-par with or better than several recent supervised CIR methods [8, 11,25] relying on labeled train-ing data. Our contributions are threefold:
• We propose a new task, zero-shot composed image retrieval (ZS-CIR), which aims to solve diverse CIR tasks without requiring expensive triplet-labeled train-ing datasets.
• We propose Pic2Word, a novel method for ZS-CIR that is trainable using only image-caption and unla-beled image datasets. Pic2Word leverages pre-trained vision-language models and transforms an input image to a language token in order to ﬂexibly compose image and text queries.
• Pic2Word improves the ZS-CIR performance, e.g., rel-ative improvement of 10 - 100% on four CIR tasks, which is on-par with several recent CIR methods us-ing labeled training data. 2.