Abstract
While lightweight ViT framework has made tremendous progress in image super-resolution, its uni-dimensional self-attention modeling, as well as homogeneous aggre-gation scheme, limit its effective receptive field (ERF) to include more comprehensive interactions from both spa-tial and channel dimensions. To tackle these drawbacks, this work proposes two enhanced components under a new Omni-SR architecture. First, an Omni Self-Attention (OSA) block is proposed based on dense interaction princi-ple, which can simultaneously model pixel-interaction from both spatial and channel dimensions, mining the potential correlations across omni-axis (i.e., spatial and channel).
Coupling with mainstream window partitioning strategies,
OSA can achieve superior performance with compelling computational budgets. Second, a multi-scale interaction scheme is proposed to mitigate sub-optimal ERF (i.e., pre-mature saturation) in shallow models, which facilitates lo-cal propagation and meso-/global-scale interactions, ren-dering an omni-scale aggregation building block. Extensive experiments demonstrate that Omni-SR achieves record-high performance on lightweight super-resolution bench-marks (e.g., 26.95dB@Urban100 ×4 with only 792K pa-rameters). Our code is available at https://github. com/Francis0625/Omni-SR. 1.

Introduction
Image super-resolution (SR) is a long-standing low-level problem that aims to recover high-resolution (HR) images from degraded low-resolution (LR) inputs. Recently, vi-sion transformer [14, 51] based (i.e., ViT-based) SR frame-works [5, 31] have emerged, showing significant perfor-mance gains compared to previously dominant Convolu-tional Neural Networks (CNNs) [66]. However, most at-tempts [31] are devoted to improving the large-scale ViT-based models, while the development of lightweight ViTs (typically, less than 1M parameters) remains fraught with
*Equal Contribution.
†Corresponding author: Bingbing Ni.
Figure 1. Typical self-attention schemes [31, 59] can only perform uni-dimensional (e.g., spatial-only) interactions, and. difficulties. This paper focuses on boosting the restoration performance of lightweight ViT-based frameworks.
Two difficulties hinder the development of lightweight
ViT-based models: 1) Uni-dimensional aggregation oper-ators (i.e., spatial [31] only or channel [59] only) impris-ons the full potential of self-attention operators. Contem-porary self-attention generally realizes the interaction be-tween pixels by calculating the cross-covariance of the spa-tial direction (i.e., width and height) and exchanges context information in a channel-separated manner. This interac-tion scheme ignores the explicit use of channel informa-tion. However, recent evidences [59] and our practice show that self-attention in the channel dimension (i.e., compu-tationally more compact than spatial self-attention) is also crucial in low-level tasks. 2) Homogeneous aggregation schemes (i.e., Simple hierarchical stacking of single opera-tors, e.g., convolution, self-attention) neglect abundant tex-ture patterns of multi-scales, which is urgently needed in
SR task. Specifically, a single operator is only sensitive to information of one scale [6, 12], e.g., self-attention is sensi-tive to long-term information and pays little attention to lo-cal information. Additionally, stacking of homogeneous op-erators proves to be inefficient and suffers from premature saturation of the interaction range [8], which is reflected as a suboptimal effective receptive field. The above problem is exacerbated in lightweight models because lightweight models cannot stack enough layers.
In order to solve the above problems and pursue higher performance, this work proposes a novel omni-dimension feature aggregation scheme called Omni Self-Attention (OSA) exploiting both spatial and channel axis informa-tion in a simultaneous manner (i.e., extends the interaction into three-dimensional space), which offers higher-order receptive field information, as shown in Figure 1. Un-like scalar-based (a group of important coefficient) chan-nel interaction [19], OSA enables comprehensive informa-tion propagation and interaction by cascading computation of the cross-covariance matrices between spatial/channel dimensions. The proposed OSA module can be plugged into any mainstream self-attention variants (e.g., Swin [35],
Halo [50]), which provides a finer granularity of important encoding (compared to the vanilla channel attention [19]), achieving a perceptible improvement in contextual aggre-gation capabilities. Furthermore, a multi-scale hierarchical aggregation block, named Omni-Scale Aggregation Group (i.e., OSAG for short), is presented to achieve tailored en-coding of varying scales of texture patterns. Specifically,
OSAG builds three cascaded aggregators: local convolution (for local details), meso self-attention (focusing on mid-scale pattern processing), and global self-attention (pursu-ing global context understanding), rendering an omni-scale (i.e., local-/meso-/global-scale simultaneously) feature ex-traction capability. Compared to the homogenized feature extraction schemes [28, 31], our OSAG is able to mine richer information producing features with higher informa-tion entropy. Coupling with the above two designs, we es-tablish a new ViT-based framework for lightweight super-resolution, called Omni-SR, which exhibits superior restora-tion performance as well as covers a larger interaction range while maintaining an attractive model size, i.e., 792K.
We extensively experiment with the proposed frame-work with both qualitative and quantitative evaluations on mainstream open-source image super-resolution datasets.
It is demonstrated that our framework achieves state-of-the-art performance at the lightweight model scale (e.g.,
Urban100×4: 26.95dB, Manga109×4: 31.50dB). More importantly, compared to existing ViT-based super-solution frameworks, our framework shows superior optimization properties (e.g., convergence speed, smoother loss land-scape), which endow our model with better robustness. 2.