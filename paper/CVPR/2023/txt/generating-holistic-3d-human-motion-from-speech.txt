Abstract
This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech record-ing, we synthesize sequences of 3D body poses, hand ges-tures, and facial expressions that are realistic and diverse.
To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately.
The separated modeling stems from the fact that face artic-ulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a composi-tional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that gener-ates body poses and hand gestures, leading to coherent and
*Equal Contribution.
†Joint Corresponding Authors. realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively.
Our dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de/. 1.

Introduction
From linguistics and psychology we know that humans use body language to convey emotion and use gestures in communication [22, 28]. Motion cues such as facial expres-sion, body posture and hand movement all play a role. For instance, people may change their gestures when shifting to a new topic [52], or wave their hands when greeting an audience. Recent methods have shown rapid progress on modeling the translation from human speech to body mo-tion, and can be roughly divided into rule-based [38] and learning-based [20, 21, 23, 32, 33, 55] methods. Typically, the body motion in these methods is represented as the mo-tion of a 3D mesh of the face/upper-body [5, 15, 26, 43, 44], or 2D/3D landmarks of the face with 2D/3D joints of the
hands and body [21, 23, 55]. However, this is not sufficient to understand human behavior. Humans communicate with their bodies, hands and facial expressions together. Captur-ing such coordinated activities as well as the full 3D surface in tune with speech is critical for virtual agents to behave realistically and interact with listeners meaningfully.
In this work, we focus on generating the expressive 3D motion of person, including their body, hand gestures, and facial expressions, from speech alone; see Fig. 1. To do this, we must learn a cross-modal mapping between audio and 3D holistic body motion, which is very challenging in practice for several reasons. First, datasets of 3D holistic body meshes and synchronous speech recordings are scarce.
Acquiring them in the lab is expensive and doing so in the wild has not been possible. Second, real humans often vary in shape, and their faces and hands are highly deformable. It is not trivial to generate both realistic and stable results of 3D holistic body meshes efficiently. Lastly, as different body parts correlate differently with speech signals, it is difficult to model the cross-modal mapping and generate realistic and diverse holistic body motions.
We address the above challenges and learn to model the conversational dynamics in a data-driven way. Firstly, to overcome the issue of data scarcity, we present a new set of 3D holistic body mesh annotations with synchronous audio from in-the-wild videos. This dataset was previously used for learning 2D/3D gesture modeling with 2D body key-point annotations [21] and 3D keypoint annotations of the holistic body [23] by applying existing models separately.
Apart from facilitating speech and motion modeling, our dataset can also support broad research topics like realistic digital human rendering. Then, to support our data-driven approach to modeling speech-to-motion translation, an ac-curate holistic body mesh is needed. Existing methods have focused on capturing either the body shape and pose isolated from the hands and face [8, 17, 25, 34, 47, 57, 58, 61], or the different parts together, which often produces unreal-istic or unstable results, especially when applied to video sequences [18, 40, 62]. To solve this, we present SHOW, which stands for “Synchronous Holistic Optimization in the
Wild”. Specifically, SHOW adapts SMPLify-X [40] to the videos of talking persons, and further improves it in terms of stability, accuracy, and efficiency through careful design choices. Figure 2 shows example reconstruction results.
Lastly, we investigate the translation from audio to 3D holistic body motion represented as a 3D mesh (Fig. 1). We propose TalkSHOW, the first approach to autoregressively synthesize realistic and diverse 3D body motions, hand ges-tures and facial expression of a talking person from speech.
Motivated by the fact that the face (i.e. mouth region) is strongly correlated with the audio signal, while the body and hands are less correlated, or even uncorrelated, TalkSHOW designs separate motion generators for different parts and gives each part full play. For the face part, to model the highly correlated nature of phoneme-to-lip motion, we de-sign a simple encoder-decoder based face generator that encodes rich phoneme information by incorporating the pre-trained wav2vec 2.0 [6]. On the other hand, to predict the non-deterministic body and hand motions, we devise a novel
VQ-VAE [50] based framework to learn a compositional quantized space of motion, which efficiently captures a di-verse range of motions. With the learned discrete represen-tation, we further propose a novel autoregressive model to predict a multinomial distribution of future motion, cross-conditioned between existing motions. From this, a wide range of motion modes representing coherent poses can be sampled, leading to realistic looking motion generation.
We quantitatively evaluate the realism and diversity of our synthesized motion compared to ground truth and baseline methods and ablations. To further corroborate our qualitative results, we evaluate our approach through an extensive user study. Both quantitative and qualitative studies demonstrate the state-of-the-art quality of our speech-synthesized full expressive 3D character animations. 2.