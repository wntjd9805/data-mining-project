Abstract
We present the design of a productionized end-to-end stereo depth sensing system that does pre-processing, on-line stereo rectification, and stereo depth estimation with a fallback to monocular depth estimation when rectification is unreliable. The output of our depth sensing system is then used in a novel view generation pipeline to create 3D com-putational photography effects using point-of-view images captured by smart glasses. All these steps are executed on-device on the stringent compute budget of a mobile phone, and because we expect the users can use a wide range of smartphones, our design needs to be general and cannot
*Email: jialiangw@meta.com
†work was done at Meta be dependent on a particular hardware or ML accelerator such as a smartphone GPU. Although each of these steps is well studied, a description of a practical system is still lacking. For such a system, all these steps need to work in tandem with one another and fallback gracefully on failures within the system or less than ideal input data. We show how we handle unforeseen changes to calibration, e.g., due to heat, robustly support depth estimation in the wild, and still abide by the memory and latency constraints required for a smooth user experience. We show that our trained models are fast, and run in less than 1s on a six-year-old
Samsung Galaxy S8 phone’s CPU. Our models generalize well to unseen data and achieve good results on Middlebury and in-the-wild images captured from the smart glasses.
1.

Introduction
Stereo disparity estimation is one of the fundamental problems in computer vision, and it has a wide variety of applications in many different fields, such as AR/VR, com-putational photography, robotics, and autonomous driving.
Researchers have made significant progress in using neural networks to achieve high accuracy in benchmarks such as
KITTI [21], Middlebury [28] and ETH3D [30].
However, there are many practical challenges in using stereo in an end-to-end depth sensing system. We present a productionized system in this paper for smart glasses equipped with two front-facing stereo cameras. The smart glasses are paired with a mobile phone, so the main com-putation happens on the phone. The end-to-end system does pre-processing, online stereo rectification, and stereo depth estimation.
In the event that the rectification fails, we fallback on monocular depth estimation. The output of the depth sensing system is then fed to a rendering pipeline to create three-dimensional computational photographic ef-fects from a single user capture. To our knowledge there is limited existing literature discussing how to design such an end-to-end system running on a very limited computational budget.
The main goal of our system is to achieve the best user experience to create the 3D computational photogra-phy effects. We need to support any mainstream phone the user chooses to use and capture any type of surroundings.
Therefore, the system needs to be robust and operate on a very limited computational budget. Nevertheless, we show that our trained model achieves on-par performance with state-of-the-art (SotA) networks such as RAFT-stereo [18],
GA-Net [45], LEA-stereo [3] and MobileStereoNet on the task of zero-shot depth estimation on the Middlebury 2014 dataset [28] despite our network being orders of magnitude faster than these methods.
The main technical and system contributions are: 1. We describe an end-to-end stereo system with careful design choices and fallback plans. Our design strate-gies can be a baseline for other similar depth systems. 2. We introduce a novel online rectification algorithm that is fast and robust. 3. We introduce a novel strategy to co-design a stereo net-work and a monocular depth network to make both net-works’ output format similar. 4. We show that our quantized network achieves compet-itive accuracy on a tight compute budget. 2.