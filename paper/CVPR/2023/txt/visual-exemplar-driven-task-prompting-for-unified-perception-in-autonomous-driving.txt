Abstract
Multi-task learning has emerged as a powerful paradigm to solve a range of tasks simultaneously with good efficiency in both computation resources and inference time. However, these algorithms are designed for different tasks mostly not within the scope of autonomous driving, thus making it hard to compare multi-task methods in autonomous driving.
Aiming to enable the comprehensive evaluation of present multi-task learning methods in autonomous driving, we ex-tensively investigate the performance of popular multi-task methods on the large-scale driving dataset, which covers four common perception tasks, i.e., object detection, seman-tic segmentation, drivable area segmentation, and lane de-tection. We provide an in-depth analysis of current multi-task learning methods under different common settings and find out that the existing methods make progress but there is still a large performance gap compared with single-task baselines. To alleviate this dilemma in autonomous driving, we present an effective multi-task framework, VE-Prompt, which introduces visual exemplars via task-specific prompt-ing to guide the model toward learning high-quality task-specific representations. Specifically, we generate visual exemplars based on bounding boxes and color-based mark-ers, which provide accurate visual appearances of target categories and further mitigate the performance gap. Fur-thermore, we bridge transformer-based encoders and con-volutional layers for efficient and accurate unified percep-tion in autonomous driving. Comprehensive experimental results on the diverse self-driving dataset BDD100K show that the VE-Prompt improves the multi-task baseline and further surpasses single-task models. 1.

Introduction
Multi-task learning (MTL) has been the source of a num-ber of breakthroughs in autonomous driving over the last
†Corresponding author.
Figure 1. Comparison of different prompts in computer vi-sion. (a) Extracting textual prompts from a text encoder to per-form image-text alignment [62]. (b) Prepend learnable prompts to the embeddings of image patches [20]. (c) Visual exemplar driven prompts for multi-task learning (ours). The generated task prompts encode high-quality task-specific knowledge for down-stream tasks. few years [23, 50, 56] and general vision tasks recently
[2, 12, 26, 34, 55]. As the foundation of autonomous driv-ing, a robust vision perception system is required to pro-vide critical information, including the position of traffic participants, traffic signals like lights, signs, lanes, and ob-stacles that influence the drivable space, to ensure driving safety and comfort. These tasks gain knowledge from the same data source and present prominent relationships be-tween each other, like traffic participants, are more likely to appear within drivable spaces and traffic signs may appear near traffic lights, etc. Training these tasks independently is time costing and fails to mine the latent relationship among them. Therefore, it is crucial to solve these multiple tasks simultaneously, which can improve data efficiency and re-duce training and inference time.
Some recent works have attempted to apply unified train-ing on multiple tasks in autonomous training. Uncertainty
[21] trains per-pixel depth prediction, semantic segmenta-tion, and instance segmentation in a single model. CIL [19] introduces an extra traffic light classifier to learn different traffic patterns following traffic light changes. CP-MTL
[4] learns object detection and depth prediction together to identify dangerous traffic scenes. However, these works dif-fer in task types, evaluation matrix, and dataset, making it hard to compare their performances. For example, most of them are developed upon dense prediction [2, 55] and natu-ral language understanding [7,47], rather than being tailored for more common perception tasks for autonomous driving, thus these methods may produce poor results when applied to a self-driving system. As a result, there is an emerg-ing demand for a thorough evaluation of existing multi-task learning methods covering common tasks in autonomous driving.
In this paper, we focus on heterogeneous multi-task learning in common scenarios of autonomous driving and cover popular self-driving tasks, i.e., object detection, se-mantic segmentation, drivable area segmentation, and lane detection. We provide a systematic study of present MTL methods on large-scale driving dataset BDD100K [58].
Specifically, we find that task scheduling [26] is better than zeroing loss [51], but worse than pseudo labeling [15] on most tasks. Interestingly, in task-balancing methods, Un-certainty [21] produces satisfactory results on most tasks, while MGDA [41] only performs well on lane detection.
This indicates that negative transfer [8], which is a phe-nomenon that increasing the performance of a model on one task will hurt the performance on another task with different needs, is common among these approaches.
To mitigate the negative transfer problem, we introduce the visual exemplar-driven task-prompting (shorten as VE-Prompt) based on the following motivations: (1) Given the visual clues of each task, the model can extract task-related information from the pre-trained model. Different from cur-rent prompting methods which introduce textual prompts
[6, 38, 62, 63] or learnable context [20], we leverage exem-plars containing information of target objects to generate task-specific prompts by considering that the visual clues should represent the specific task to some extent, and give hints for learning task-specific information; (2) Transformer has achieved competitive performance on many vision tasks but usually requires long training time, thus tackling four tasks simultaneously on a pure transformer is resource-intensive. To overcome this challenge, we efficiently bridge transformer encoders and convolutional layers to build the hybrid multi-task architecture. Extensive experiments show that VE-Prompt surpasses multi-task baselines by a large margin.
We summarize the main contributions of our work be-low:
• We provide an in-depth analysis of current multi-task learning approaches under multiple settings that comply with real-world scenarios, consisting of three common multi-task data split settings, two partial-label learning approaches, three task scheduling techniques, and three task balancing strategies.
• We propose an effective framework VE-Prompt, which utilizes visual exemplars to provide task-specific visual clues and guide the model toward learning high-quality task-specific representations.
• The VE-Prompt framework is constructed in a computa-tionally efficient way and outperforms competitive multi-task methods on all tasks. 2.