Abstract
Reflections on glossy objects contain valuable and hidden information about the surrounding environment. By con-verting these objects into cameras, we can unlock exciting applications, including imaging beyond the camera’s field-of-view and from seemingly impossible vantage points, e.g. from reflections on the human eye. However, this task is challenging because reflections depend jointly on object ge-ometry, material properties, the 3D environment, and the observer’s viewing direction. Our approach converts glossy objects with unknown geometry into radiance-field cameras to image the world from the object’s perspective. Our key insight is to convert the object surface into a virtual sensor that captures cast reflections as a 2D projection of the 5D environment radiance field visible to and surrounding the object. We show that recovering the environment radiance fields enables depth and radiance estimation from the ob-ject to its surroundings in addition to beyond field-of-view
*Equal contribution novel-view synthesis, i.e. rendering of novel views that are only directly visible to the glossy object present in the scene, but not the observer. Moreover, using the radiance field we can image around occluders caused by close-by objects in the scene. Our method is trained end-to-end on multi-view images of the object and jointly estimates object geometry, diffuse radiance, and the 5D environment radiance field.
For more information, visit our website. 1.

Introduction
Imagine that you’re driving down a city street that is packed with lines of parked cars on both sides. Inspection of the cars’ glass windshields, glossy paint, and plastic re-veal sharp, but faint and distorted views of the surroundings that might be otherwise hidden from you. Humans can infer depth and semantic cues about the occluded areas in the en-vironment by processing reflections visible on reflective ob-jects, internally decomposing the object geometry and radi-ance from the specular radiance being reflected onto it. Our aim is to decompose the object from its reflections to “see” the world from the object’s perspective, effectively turning
Figure 2. ORCa Overview. We jointly estimate the object’s geometry and diffuse along with the environment radiance field estimation through a three-step approach. First, we model the object as a neural implicit surface (a). We model the reflections as probing the environment on virtual viewpoints (b) estimated analytically from surface properties. We model the environment as a radiance field queried on these viewpoints (c). Both neural implicit surface and environment radiance field are trained jointly on multi-view images of the object using a photometric loss. the object into a camera that images its environment. How-ever, reflections pose a long-standing challenge in computer vision as the reflections are a 2D projection of an unknown 3D environment that is distorted based on the shape of the reflector.
To capture the 3D world from the object’s perspective, we model the object’s surface as a virtual sensor that cap-tures the 2D projection of a 5D environment radiance field surrounding the object. This environment radiance field consists largely of areas only visible to the observer through the object’s reflections. Our use of environment radiance fields not only enables depth and radiance estimation from the object to its surroundings but also enables beyond field-of-view novel-view synthesis, i.e. rendering of novel views that are only directly visible to the glossy object present in the scene but not the observer. Unlike conventional ap-proaches that model the environment as a 2D map, our ap-proach models it as a 5D field without assuming the scene is infinitely far away. Moreover, by sampling the 5D radiance field, instead of a 2D map, we can capture depth and images around occluders, such as close-by objects in the scene, as shown in Fig. 3. These applications cannot be done from a 2D environment map.
We aim to decompose reflections on the object’s surface, from its surface and exploit those reflections to construct a radiance field surrounding the object, therefore captur-ing the 3D world in the process. This is a challenging task because the reflections are extremely sensitive to local ob-ject geometry, viewing direction and inter-reflections due to the object’s surface. To capture this radiance field, we convert glossy objects with unknown geometry and texture into radiance-field cameras. Specifically, we exploit neural rendering to estimate the local surface of the object viewed
Figure 3. Advantages of 5D environment radiance field. Mod-eling reflections on object surfaces (a) as a 5D env. radiance field enables beyond field-of-view novel-view synthesis, including ren-dering of the environment from translated virtual camera views (b). Depth (c) and environment radiance of translated and parallax views can further enable imaging behind occluders, for example revealing the tails behind the primary Pokemon occluders (d). from each pixel of the real camera. We then convert this local surface into a virtual pixel that captures radiance from the environment. This virtual pixel captures the environ-ment radiance as shown in Fig 5. We estimate the outgo-ing frustum from the virtual pixel as a cone that samples the scene. By sampling the scene from many virtual pix-els on the object surface, we construct an environment ra-diance field that can be queried independently of the object surface, enabling beyond field-of-view novel-view synthesis from previously unsampled viewpoints.
Our approach jointly estimates object geometry, diffuse radiance, and the environment radiance field from multi-view images of glossy objects with unknown geometry and diffuse texture in three steps. First, we use neural signed distance functions (SDF) and an MLP to model the glossy object’s geometry as a neural implicit surface and diffuse radiance, respectively, similar to PANDORA [10]. Then, for every pixel on the observer’s camera, we estimate the virtual pixels on the object’s surface based on the estimated local geometry from the neural SDF. We analytically com-pute the parameters of the virtual cone through the virtual pixel. Lastly, we use the cone formulation in MipNeRF [5] to cast virtual cones from the virtual camera to recover the environment radiance .
To summarize, we make the following contributions:
• We present a method to convert implicit surfaces into virtual sensors that can image their surroundings using virtual cones. (Sec. 3.3)
• We jointly estimate object geometry, diffuse radiance, and estimate the 5D environment radiance field sur-rounding the object. (Fig. 7 & 9)
• We show that the environment radiance field can be queried to perform beyond-field-of-view novel view-point synthesis, i.e render views only visible to the ob-ject in the scene (Section 3.4)
Scope. We only model glossy objects with low rough-ness as such specular reflections tend to have a high signal-to-noise ratio, therefore, are a sharper estimate of the en-vironment radiance field. However, we note that the vir-tual cone computation can be extended to model the cone radius as a function of surface roughness. Deblurring ap-proaches can further improve the resolution of estimated environment. In addition, we approximate the local curva-ture using mean curvature, which fails for objects with vary-ing radius of curvature along the tangent space. We explain how our virtual cone curvature estimation can be extended to handle general shape operators in the supplementary ma-terial. Lastly, similar to other multi-view approaches, our approach relies on a sufficient virtual baseline between vir-tual viewpoints to recover the environment radiance field. 2.