Abstract
Fusing LiDAR and camera information is essential for accurate and reliable 3D object detection in autonomous driving systems. This is challenging due to the difficulty of combining multi-granularity geometric and semantic fea-tures from two drastically different modalities. Recent ap-proaches aim at exploring the semantic densities of cam-era features through lifting points in 2D camera images (re-ferred to as “seeds”) into 3D space, and then incorporate 2D semantics via cross-modal interaction or fusion tech-niques. However, depth information is under-investigated in these approaches when lifting points into 3D space, thus 2D semantics can not be reliably fused with 3D points.
Moreover, their multi-modal fusion strategy, which is im-plemented as concatenation or attention, either can not effectively fuse 2D and 3D information or is unable to perform fine-grained interactions in the voxel space. To this end, we propose a novel framework with better uti-lization of the depth information and fine-grained cross-modal interaction between LiDAR and camera, which con-sists of two important components. First, a Multi-Depth
Unprojection (MDU) method is used to enhance the depth quality of the lifted points at each interaction level. Sec-ond, a Gated Modality-Aware Convolution (GMA-Conv) block is applied to modulate voxels involved with the cam-era modality in a fine-grained manner and then aggre-gate multi-modal features into a unified space. Together they provide the detection head with more comprehensive features from LiDAR and camera. On the nuScenes test benchmark, our proposed method, abbreviated as MSMD-Fusion, achieves state-of-the-art results on both 3D ob-ject detection and tracking tasks without using test-time-augmentation and ensemble techniques. The code is avail-able at https://github.com/SxJyJay/MSMDFusion.
*Equal contribution.
†Corresponding authors.
Figure 1. Illustration of our MSMDFusion pipeline. The yellow arrows indicate information passing or interaction between the Li-DAR and camera modalities. 1.

Introduction
Detecting 3D objects [1, 22, 29] is regarded as a fun-damental task for autonomous driving. Aiming at the ro-bust environmental perception, LiDARs and cameras are widely equipped on autonomous driving vehicles since they can provide complementary information. Characterized by point clouds, LiDARs can capture accurate spatial infor-mation, while cameras contain rich semantics and context with images. Therefore, developing multi-modal detec-tors that enjoy the benefits of the two worlds is promising.
Such an idea has catalyzed the emergence of a set of re-cent researches [1, 3, 12, 13, 15, 17, 19, 20, 25, 26]. Early works [1, 3, 11, 12, 18–20, 26] perform LiDAR-camera fu-sion by projecting 3D LiDAR points (or region proposals generated from them) onto 2D image planes to collect use-ful 2D semantics. However, such a paradigm suffers from the signal density mismatching of multi-modality sensors.
Since LiDAR points are much sparser than camera pixels, this projection manner will inevitably waste semantically rich 2D features [1, 13].
Recently, another paradigm [9,10,13,15,25] for LiDAR-Instead of collecting 2D se-camera fusion has emerged. mantic features with 3D queries, these methods first esti-mate depths of pixels, and then directly lift 2D pixels with
their semantics to the 3D world (we refer to these pixels and corresponding lifted points as “seeds” and “virtual points” in this paper) to fuse with the real 3D point cloud.
Two methods with the same name of BEVFusion [13, 15] treat every image feature pixel as a seed and generate vir-tual points in the BEV space. MVP [25] and VFF [10] sample pixels from foreground regions and lift them to the voxel space. Benefited from the dense virtual points, such a paradigm not only maintains the semantic consistency in the image [15], but also complements the geometric cues for the sparse LiDAR point cloud [25].
Despite significant improvements have been made, ex-isting methods in this line suffer from two major prob-lems, which hampers benefiting from virtual points. First, depth, as the key to the quality of virtual points, is under-investigated in generating virtual points. On the one hand, depth directly determines the spatial location in 3D space of a seed via perspective projection which can thereby sig-nificantly affect 3D object detection results. On the other hand, depth can also enhance the semantics carried by vir-tual points by providing color-insensitive cues in describ-ing objects [27], since combining RGB information with depth guidance correlates camera pixels of similar depths and enables them to jointly contribute to capturing instance-related semantics when lifted as virtual points. Existing multi-modal detectors [9, 13, 15] mainly pay attention on interacting LiDAR points with camera virtual points, while ignoring the importance of seed depths in generating the virtual points.
Second, the fine-grained cross-modal interaction be-tween virtual points and 3D points in the uncompressed space (e.g., the voxel space) is crucial but non-trivial. Gen-erated virtual points are geometrically and semantically in-consistent with real LiDAR points due to imperfect depths and inherent modality gap. Hence, in order to benefit from the semantically rich virtual points, it is necessary to adap-tively select helpful information from virtual points under the guidance of real LiDAR points in a fine-grained and controllable manner. However, such a cross-modal inter-action is constrained by the intensive memory and compu-tation cost brought by the massive amounts and unstruc-tured nature of point cloud data. Alternatively, existing ap-proaches combine the multi-modal information with simple concatenate [25] or add operations [9] in the voxel space, or perform cross-attention in a compressed BEV space [23].
Aiming at unlocking the potential of virtual points and addressing the drawbacks of existing methods, we pro-pose a multi-scale fusion framework, called MSMDFusion, and within each scale, there are two key novel designs, namely the Multi-Depth Unprojection (MDU) and Gated
Modality-Aware Convolution (GMA-Conv). As shown in
Fig.1, MDU is mainly for enhancing the quality of gen-erated virtual points in terms of geometric accuracy and semantic richness. To be specific, when lifting 2D seeds from image into the 3D space, multiple depths are ex-plored within a reference neighborhood to generate virtual points with more reliable depth. Next, the camera feature and depth are combined to produce depth-aware features as stronger 2D semantics to decorate these virtual points.
GMA-Conv takes real LiDAR points and generated virtual points as inputs, and performs fine-grained interaction in a select-then-aggregate manner. Concretely, we first adap-tively select useful information from camera voxel features under the guidance of reference LiDAR voxels, then aggre-gate them grouped sparse convolutions for sufficient multi-modal interaction. We also specifically adopt a voxel sub-sampling strategy to efficiently obtain reliable LiDAR ref-erences when implementing our GMA-Conv.
Finally, with the resulting multi-modal voxel features from multiple scales, we further associate them with cascade connections across scales to aggregate multi-granularity information. With the above designs, the cam-era semantics encapsulated in the virtual points are consis-tently combined with LiDAR points, and thereby providing a stronger multi-modal feature representation for boosting the 3D object detection. As shown in the Table 3, with 100 times fewer generated virtual points than the two BEVFu-sion methods [13, 15], our MSMDFusion can still achieve state-of-the-art performances.
In summary, our contributions lie in threefold: (1) We propose a novel MSMDFusion approach, which encour-ages sufficient LiDAR-Camera feature fusion in the multi-(2) Within each scale, we propose a scale voxel space.
Multi-Depth Unprojection strategy (MDU) to promote vir-tual points generation with better locations and semantics by fully leveraging depth of pixels, as well as a Gated
Modality-Aware Convolution (GMA-Conv) to achieve fine-grained controllable multi-modal interaction. (3) Extensive experimental results on the large-scale nuScenes dataset prove the effectiveness of our MSMDFusion and its compo-nents. We achieve state-of-the-art performances with 71.5% mAP and 74.0% NDS on the challenging nuScenes detec-tion track using single model1. When combining the sim-ple greedy tracking strategy [24], our method also achieves strong tracking results with 74.0% AMOTA. 2.