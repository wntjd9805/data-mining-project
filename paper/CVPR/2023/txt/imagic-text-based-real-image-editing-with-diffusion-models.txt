Abstract
Text-conditioned image editing has recently attracted considerable interest. However, most methods are cur-rently limited to one of the following: speciﬁc editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object.
In this paper we demonstrate, for the very ﬁrst time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For exam-ple, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a stand-ing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided nat-ural image. Contrary to previous work, our proposed method requires only a single input image and a target
It operates on real images, and text (the desired edit).
˚ Equal contribution.
The ﬁrst author performed this work as an intern at Google Research.
Project page: https://imagic-editing.github.io/. does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffu-sion model for this task.
It produces a text embedding that aligns with both the input image and the target text, while ﬁne-tuning the diffusion model to capture the image-speciﬁc appearance. We demonstrate the quality and versa-tility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single uniﬁed framework. To better assess performance, we introduce TEdBench, a highly chal-lenging image editing benchmark. We conduct a user study, whose ﬁndings show that human raters prefer Imagic to pre-vious leading editing methods on TEdBench. 1.

Introduction
Applying non-trivial semantic edits to real photos has long been an interesting task in image processing [41].
It has attracted considerable interest in recent years, en-abled by the considerable advancements of deep learning-based systems. Image editing becomes especially impres-Input Image
Edited Images
“A sitting dog”
“A jumping dog”
“A dog lying down”
“A dog playing  with a toy”
“A jumping dog  holding a frisbee”
“A person giving the thumbs up”
“A person with crossed arms”
“A person in a greeting  pose to Namaste hands”
“A person  holding a cup”
“A person making a heart sign”
“A cat wearing a  hat”
“A cat wearing an apron”
“A cat wearing a necklace”
“A cat wearing a jean jacket”
“A drawing of a cat”
“A zebra”
“A horse with  a saddle”
“A horse with its head down”
“A brown horse in (cid:73)(cid:3)(cid:79)(cid:90)(cid:73)(cid:91)(cid:91)(cid:3)(cid:197)(cid:77)(cid:84)(cid:76)”
“A cartoon of a horse”
“A pistachio cake”
“A chocolate cake”
“A strawberry cake”
“A wedding cake”
“A slice of cake”
Figure 2. Different target texts applied to the same image. Imagic edits the same image differently depending on the input text. sive when the desired edit is described by a simple natu-ral language text prompt, since this aligns well with human communication. Many methods were developed for text-based image editing, showing promising results and con-tinually improving [8, 10, 33]. However, the current lead-ing methods suffer from, to varying degrees, several draw-backs: (i) they are limited to a speciﬁc set of edits such as painting over the image, adding an object, or transferring style [6, 33]; (ii) they can operate only on images from a speciﬁc domain or synthetically generated images [20, 43]; or (iii) they require auxiliary inputs in addition to the in-put image, such as image masks indicating the desired edit location, multiple images of the same subject, or a text de-scribing the original image [6, 17, 39, 47, 51].
In this paper, we propose a semantic image editing method that mitigates all the above problems. Given only an input image to be edited and a single text prompt describing the target edit, our method can perform sophisticated non-rigid edits on real high-resolution images. The resulting im-age outputs align well with the target text, while preserving the overall background, structure, and composition of the original image. For example, we can make two parrots kiss
(A) Text Embedding Optimization
Reconstruction Loss (B) Model Fine-Tuning
Reconstruction Loss e s i o n
+
Pre-Trained
Diffusion Model
Input
Input e s i o n
+ eopt
Pre-Trained
Diffusion Model (C) Interpolation & Generation
Target Emb etgt eopt
Optimized Emb eopt
"A bird spreading wings." etgt
Fine-Tuned
Diffusion Process interpolate etgt eopt
Output
Figure 3. Schematic description of Imagic. Given a real image and a target text prompt: (A) We encode the target text and get the initial text embedding etgt, then optimize it to reconstruct the input image, obtaining eopt; (B) We then ﬁne-tune the generative model to improve
ﬁdelity to the input image while ﬁxing eopt; (C) Finally, we interpolate eopt with etgt to generate the ﬁnal editing result. or make a person give the thumbs up, as demonstrated in
Figure 1. Our method, which we call Imagic, provides the
ﬁrst demonstration of text-based semantic editing that ap-plies such sophisticated manipulations to a single real high-resolution image, including editing multiple objects. In ad-dition, Imagic can also perform a wide variety of edits, in-cluding style changes, color changes, and object additions.
To achieve this feat, we take advantage of the recent suc-cess of text-to-image diffusion models [47, 50, 53]. Diffu-sion models are powerful state-of-the-art generative models, capable of high quality image synthesis [16,22]. When con-ditioned on natural language text prompts, they are able to generate images that align well with the requested text. We adapt them in our work to edit real images instead of syn-thesizing new ones. We do so in a simple 3-step process, as depicted in Figure 3: We ﬁrst optimize a text embedding so that it results in images similar to the input image. Then, we
ﬁne-tune the pre-trained generative diffusion model (condi-tioned on the optimized embedding) to better reconstruct the input image. Finally, we linearly interpolate between the target text embedding and the optimized one, resulting in a representation that combines both the input image and the target text. This representation is then passed to the gen-erative diffusion process with the ﬁne-tuned model, which outputs our ﬁnal edited image.
We conduct several experiments and apply our method on numerous images from various domains. Our method outputs high quality images that both resemble the input image to a high degree, and align well with the target text. These results showcase the generality, versatility, and quality of Imagic. We additionally conduct an ablation study, highlighting the effect of each element of our method.
When compared to recent approaches suggested in the lit-erature, Imagic exhibits signiﬁcantly better editing qual-ity and faithfulness to the original image, especially when tasked with sophisticated non-rigid edits. This is further supported by a human perceptual evaluation study, where raters strongly prefer Imagic over other methods on a novel benchmark called TEdBench – Textual Editing Benchmark.
We summarize our main contributions as follows: 1. We present Imagic, the ﬁrst text-based semantic image editing technique that allows for complex non-rigid edits on a single real input image, while preserving its overall structure and composition. 2. We demonstrate a semantically meaningful linear inter-polation between two text embedding sequences, uncov-ering strong compositional capabilities of text-to-image diffusion models. 3. We introduce TEdBench – a novel and challenging com-plex image editing benchmark, which enables compar-isons of different text-based image editing methods. 2.