Abstract
Large pre-trained vision-language models (VLMs) re-duce the time for developing predictive models for various vision-grounded language downstream tasks by providing rich, adaptable image and text representations. However, these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data.
These biases manifest as the skewed similarity between the representations for speciﬁc text concepts and images of peo-ple of different identity groups and, therefore, limit the use-fulness of such models in real-world high-stakes applica-tions.
In this work, we present DEAR (Debiasing with
Additive Residuals), a novel debiasing method that learns additive residual image representations to offset the orig-inal representations, ensuring fair output representations.
In doing so, it reduces the ability of the representations to distinguish between the different identity groups. Further, we observe that the current fairness tests are performed
*Equal Contribution on limited face image datasets that fail to indicate why a speciﬁc text concept should/should not apply to them. To bridge this gap and better evaluate DEAR, we introduce the PROTECTED ATTRIBUTE TAG ASSOCIATION (PATA) dataset – a new context-based bias benchmarking dataset for evaluating the fairness of large pre-trained VLMs. Ad-ditionally, PATA provides visual context for a diverse human population in different scenarios with both positive and neg-ative connotations. Experimental results for fairness and zero-shot performance preservation using multiple datasets demonstrate the efﬁcacy of our framework. The dataset is released here. 1.

Introduction
Deep learning-based vision-language models (VLMs) [45] unify text and visual data into a common representation and reduce the computing cost of training for speciﬁc computer vision [19] and visually-grounded linguistic tasks [12, 47].
VLMs are trained with a large amount of data with the
aim of matching image and text representations for image-caption pairs to capture diverse visual and linguistic con-cepts. However, VLMs exhibit societal biases manifesting as the skew in the similarity between their representation of certain textual concepts and kinds of images [2, 4, 31].
These biases arise from the underlying imbalances in train-ing data [2, 3] and ﬂawed training practices [55].
In this work, we present a method to signiﬁcantly reduce the bias in VLMs by modifying the visual features of the models.
These societal biases in VLMs show up as the selective association (or dissociation) of their representations of hu-man images with speciﬁc physical characteristics and their text representations for describing social labels [52]. For in-stance, a higher degree of similarity between the representa-tion of the text “doctor” and images of men than that of the women can have trust consequences for models using the representations from these VLMs. To estimate the degree of such biases in VLMs, we compute the cosine similarity between the representations of a set of human images and speciﬁc key text phrases and compare their distribution over some associated protected attributes. These attributes rep-resent visually discernible characteristics like gender, race, and age common to certain collective identity groups. In this work, we introduce DEAR, an additive residual-based de-biasing technique that can be augmented with any pre-trained visual-language model with separate visual and lan-guage encoders [33, 45, 49] to improve their fairness.
Our empirical analysis indicates that the protected at-tribute (PA) information from an image-text pair can be disentangled using their representation by simply learning a linear transformation of the visual representations pro-duced by a VLM and subtracting (adding a negative resid-ual) them from the original representation. We propose to train our framework using two objectives: i) learn a resid-ual representation that when added to the original repre-sentation, renders it incapable of predicting different pro-tected attributes, and ii) ensure that this modiﬁed represen-tation is as close to the original as possible. We demon-strate that learning additive residual enables the de-biasing of pre-trained VLMs using quantitative skew computations on multiple datasets and qualitative evaluations. We also show that the resulting representation retains much of its predictive properties by means of zero-shot evaluation on different downstream tasks.
Recent efforts to mitigate these biases from VLMs, such as by Berg et al. [2], use unimodal datasets like the
FairFace [28] and UTK [63] datasets. These face-image datasets lack the context necessary to infer the situations in which benign text associations can turn offensive when applied selectively. For instance, if the image of a person drinking water from a bottle is misconstrued as partaking of alcohol and if this association (in terms of image-text similarity) is selective to a speciﬁc group of people with some speciﬁc visual characteristics, the association may be deemed offensive. To this end, we introduce the PRO-TECTED ATTRIBUTE TAG ASSOCIATION (PATA) dataset to test different associations for VLMs. PATA comprises im-ages of persons in different contexts and their respective text captions with positive and negative connotations. We present our de-biasing results on both the FairFace and the
PATA datasets. In summary, the paper makes the following contributions:
• We present the DEAR framework – a simple, computationally-efﬁcient, and effective de-biasing method for VLMs that only adapts the image encoder of the VLM by adding a learned residual representa-tion to it.
• We introduce the PROTECTED ATTRIBUTE TAG AS-SOCIATION dataset – a novel context-based bias eval-uation dataset that enables nuanced reporting of biases in VLMs for race, age, and gender protected attributes. 2.