Abstract
We present Deformable mesh transFormer (DeFormer), a novel vertex-based approach to monocular 3D human mesh recovery. DeFormer iteratively fits a body mesh model to an input image via a mesh alignment feedback loop formed within a transformer decoder that is equipped with efficient body mesh driven attention modules: 1) body sparse self-attention and 2) deformable mesh cross atten-tion. As a result, DeFormer can effectively exploit high-resolution image feature maps and a dense mesh model which were computationally expensive to deal with in pre-vious approaches using the standard transformer attention.
Experimental results show that DeFormer achieves state-of-the-art performances on the Human3.6M and 3DPW bench-marks. Ablation study is also conducted to show the ef-fectiveness of the DeFormer model designs for leveraging multi-scale feature maps. Code is available at https:
//github.com/yusukey03012/DeFormer. 1.

Introduction
Human mesh recovery from a single image is an im-portant problem in computer vision, with a wide range of applications like virtual reality, sports motion analysis and human-computer interactions. It is a challenging prob-lem as it requires modeling of complex nonlinear mappings from an image to 3D body shape and pose.
In the past decade, remarkable progress has been accom-plished in the field of 3D human mesh recovery based on convolutional neural networks (CNNs) [41]. A common way used in this field is a parametric approach that employs statistical human models parameterized by shape and pose parameters [33]. Here, CNNs features are regressed with body shape and pose parameters with approximately 100 di-mensions. Then, a body mesh surface is obtained from these body parameter predictions by inputting them to the human body kinematics and statistical shape models. Leveraging multi-scale image feature maps and iteratively refining the body parameters based on global and local spatial contexts in an image, the pyramidal mesh alignment feedbacks (Py-Figure 1. Summary of this work. We propose DeFormer, a mem-ory efficient decoder-only transformer architecture for 3D human mesh recovery based on block sparse self-attention [16, 40, 51] and multi-scale (MS) deformable cross attention [56]. Leverag-ing MS feature maps efficiently in transformer, our method out-performs previous parametric approaches using MS feature maps
[52, 53] and vertex transformer approaches using a single-scale feature [28, 29]. Further, by learning sparse self-attention patterns based on body mesh and skeleton connectivity, DeFormer can re-cover a dense mesh with 6.9K joint/vertex queries at interactive rates. The units of the MPJPE and PA-MPJPE scores are in mil-limeters. The lower is better.
MAF) model [53] produces a 3D mesh recovery result well-aligned to an input image.
Another paradigm for human mesh recovery is a vertex-based approach that directly regresses 3D vertex coordi-nates [14, 15, 26, 28, 29, 35]. Recently, transformer archi-tectures have been applied to vertex-based human mesh re-covery and show good reconstruction performances by cap-turing long-range interactions between body parts via self-attentions. Furthermore, as a vertex-based approach, they naturally possess potential in learning fine-grained effects like facial expressions, finger poses and clothing non-rigid deformations, if such supervisions are given. Despite their
promising performances, the main challenge of the current transformer-based approaches based on the standard trans-former attention is its memory cost, which limits the use of high-resolution image feature maps and a dense body mesh model within it for better 3D reconstruction. In fact, the cur-rent methods are limited to managing a single-scale coarse feature map with 7 × 7 grids and a coarse mesh with around 400 vertices [28, 29].
In this paper, we propose a novel vertex-based trans-former approach to 3D human mesh recovery, named
Deformable mesh transFormer (DeFormer). DeFormer it-eratively fits a human mesh model to an image using the mesh-alignment feedback loop formed in a transformer de-In order to leverage multi-scale feature maps and coder. a dense mesh model in the human mesh recovery trans-former model, we design a decoder network architecture us-ing block sparse attention and multi-scale (MS) deformable attention [56] as illustrated in Fig. 1. The block sparse self-attention module exploits the sparse connectivity patterns established from a human body mesh and its skeleton. This sparsifies self-attention access patterns and reduces mem-ory consumption. The MS deformable cross attention mod-ule aggregates multi-scale image features by attending only to a small set of key sampling points around the mesh ver-tex of the current reconstruction. DeFormer therefore can attend to visual feature maps in both coarse and fine levels, which contain not only global contexts but also local spa-tial information. It can then extract useful contextualized features for human mesh recovery and regress corrective displacements to refine the estimated mesh shape. Conse-quently, DeFormer can exploit high-resolution image fea-ture maps and a dense mesh model that are not accessible to previous approaches based on the standard transformer attention [14,28,29] due to time/memory computational de-mands. As a result, DeFormer achieves better performance than previous approaches in 3D human mesh recovery.
The main contributions of this paper include:
• DeFormer: A novel decoder-only transformer archi-tecture for monocular 3D human mesh recovery based on the new body-mesh-driven attention modules that leverage multi-scale visual features and a dense mesh reasonably efficiently. DeFormer achieves new SOTA results on the Human3.6M and 3DPW benchmarks.
• Body Sparse Self-Attention that exploits the sparse connectivity patterns extracted from a human body mesh model and its skeleton to restrict self-attention access patterns, improving memory efficiency.
• Deformable Mesh cross Attention (DMA) that effi-ciently aggregates and exploits multi-scale image fea-ture maps by attending only to a small set of key sam-pling points around the reference points obtained from the current body joint and mesh vertex reconstructions. 2.