Abstract
This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework.
It emphasizes spatio-temporal continuity and integrates both past and fu-ture reasoning for tracked objects. Thus, we name it “Past-and-Future reasoning for Tracking” (PF-Track). Specifi-cally, our method adopts the “tracking by attention” frame-work and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our
“Past Reasoning” module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The “Future Rea-soning” module digests historical information and predicts robust future trajectories. In the case of long-term occlu-sions, our method maintains the object positions and en-ables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an order of mag-nitude less. The code and models are made available at https://github.com/TRI-ML/PF-Track. 1.

Introduction
Reasoning about object trajectories in 3D is the cor-nerstone of autonomous navigation. While many LiDAR-based approaches exist [36, 58, 63], their applicability is limited by the cost and reliability of the sensor. Detecting, tracking, and forecasting object trajectories only with cam-eras is hence a critical problem. Significant progress has been achieved on these tasks separately, but they have been historically primarily studied in isolation and combined into a full-stack pipeline in an ad-hoc fashion.
In particular, 3D detection has attracted a lot of atten-tion [20,24,25,28,53], but associating these detections over time has been mostly done independently from localiza-*Work done while interning at Toyota Research Institute.
†Corresponding to Ziqi Pang at ziqip2@illinois.edu and Yu-Xiong Wang at yxw@illinois.edu.
Figure 1. We visualize the output of our model by projecting pre-dicted 3D bounding boxes onto images. In the beginning, image-based detection can be inaccurate (t = 0) due to depth ambiguity.
With “Past Reasoning,” the bounding box quality (t = t1) gradu-ally improves by leveraging historical information. With “Future
Reasoning,” our PF-Track predicts the long-term motions of ob-jects and maintains their states even under occlusions (t = t2) and camera switches. This enables re-association without explicit re-identification (t = T ), as the object ID does not switch. Our PF-Track further combines past and future reasoning in a joint frame-work to improve spatio-temporal coherence. tion [19, 31, 43]. Recently, a few approaches to end-to-end detection and tracking have been proposed, but they operate on neighboring frames and fail to integrate longer-term spatio-temporal cues [7, 12, 33, 65].
In the predic-tion literature, on the other hand, it is common to assume the availability of ground truth object trajectories and HD-Maps [4,8,11,59]. A few attempts for a more realistic eval-uation have been made [16, 21], focusing only on the pre-diction performance.
In this paper, we argue that multi-object tracking can be dramatically improved by jointly optimizing the detection-tracking-prediction pipeline, especially in a camera-based system. We provide an intuitive example from our real-world experiment in Fig. 1. At first, the pedestrian is fully visible, but a model with only single-frame information makes a prediction with large deviation (frame t = 0 in
Fig. 1). After this, integrating the temporal information from the past gradually corrects the error over time (frame t = t1 in Fig. 1), by capitalizing on the notion of spatio-temporal continuity. Moreover, as the pedestrian becomes fully occluded (frame t = t2 in Fig. 1), we can still pre-dict their location by using the aggregated past informa-tion to estimate a future trajectory. Finally, we can suc-cessfully track the pedestrian on re-appearance even on a different camera via long-term prediction, resulting in cor-rect re-association (frame t = T in Fig. 1). The above ro-bust spatio-temporal reasoning is enabled by seamless, bi-directional integration of past and future information, which starkly contrasts with the mainstream pipelines for vision-based, multi-camera, 3D multi-object tracking (3D MOT).
To this end, we propose an end-to-end framework for joint 3D object detection, tracking, and trajectory predic-tion for the task of 3D MOT, as shown in Fig. 2, adopting the “tracking by attention” [34,64,65] paradigm. Compared to our closest baseline under the same paradigm [65], we are different in explicit past and future reasoning: a 3D object query consistently represents the object over time, propa-gates the spatio-temporal information of the object across frames, and generates the corresponding bounding boxes and future trajectories. To exploit spatio-temporal cues, our algorithm leverages simple attention operations to capture object dynamics and interactions, which are then used for track refinement and robust, long-term trajectory prediction.
Finally, we close the loop by integrating predicted trajecto-ries back into the tracking module to replace missing detec-tions (e.g., due to an occlusion). To highlight the capabil-ity of joint past and future reasoning, our method is named
“Past-and-Future reasoning for Tracking” (PF-Track).
We provide a comprehensive evaluation of PF-Track on nuScenes [4] and demonstrate that joint modeling of past and future information provides clear benefits for object
In particular, PF-Track decreases ID-Switches tracking. by over 90% compared to previous multi-camera 3D MOT methods.
To summarize, our contributions are as follows. 1. We propose an end-to-end vision-only 3D MOT frame-work that utilizes object-level spatio-temporal reasoning for both past and future information. 2. Our framework improves the quality of tracks by cross-attending to features from the “past.” 3. We propose a joint tracking and prediction pipeline, whose constituent part is “Future Reasoning”, and demonstrate that tracking can explicitly benefit from long-term prediction into the “future.” 4. Our method establishes new state-of-the-art on large-scale nuScenes dataset [4] with significant improvement for both AMOTA and ID-Switch. 2.