Abstract
In this work, we tackle the challenging problem of learning-based single-view 3D hair modeling. Due to the great difficulty of collecting paired real image and 3D hair data, using synthetic data to provide prior knowledge for real domain becomes a leading solution. This unfortunately introduces the challenge of domain gap. Due to the inherent difficulty of realistic hair rendering, existing methods typi-cally use orientation maps instead of hair images as input to bridge the gap. We firmly think an intermediate represen-tation is essential, but we argue that orientation map using the dominant filtering-based methods is sensitive to uncer-tain noise and far from a competent representation. Thus, we first raise this issue up and propose a novel intermedi-ate representation, termed as HairStep, which consists of a strand map and a depth map. It is found that HairStep not only provides sufficient information for accurate 3D hair modeling, but also is feasible to be inferred from real im-ages. Specifically, we collect a dataset of 1,250 portrait im-ages with two types of annotations. A learning framework is further designed to transfer real images to the strand map and depth map. It is noted that, an extra bonus of our new dataset is the first quantitative metric for 3D hair modeling.
*Corresponding author: hanxiaoguang@cuhk.edu.cn
Our experiments show that HairStep narrows the domain gap between synthetic and real and achieves state-of-the-art performance on single-view 3D hair reconstruction. 1.

Introduction
High-fidelity 3D hair modeling is a critical part in the creation of digital human. A hairstyle of a person typically consists of about 100,000 strands [1]. Due to the complex-ity, high-quality 3D hair model is expensive to obtain. Al-though high-end capture systems [9, 18] are relatively ma-ture, it is still difficult to reconstruct satisfactory 3D hair with complex geometries.
Chai et al. [3,4] first present simple hair modeling meth-ods from single-view images, which enable the acquisition of 3D hair more user-friendly. But these early systems re-quire extra input such as user strokes. Moreover, they only work for visible parts of the hair and fail to recover in-visible geometries faithfully. Recently, retrieval-based ap-proaches [2, 10] reduce the dependency of user input and improve the quality of reconstructed 3D hair model. How-ever, the accuracy and efficiency of these approaches are directly influenced by the size and diversity of the 3D hair database.
Inspired by the advances of learning-based shape recon-struction, 3D strand models are generated by neural net-works as explicit point sequences [45], volumetric orien-tation field [25, 29, 40], and implicit orientation field [36] from single-view input. With the above evolution of 3D hair representations, the quality of recovered shape has been im-proved significantly. As populating pairs of 3D hair and real images is challenging [45], existing learning-based meth-ods [25, 29, 36, 39, 45] are just trained on synthetic data be-fore applying on real portraits. However, the domain gap between rendered images (from synthetic hair models) and real images has a great and negative impact on the quality of reconstructed results. 3D hairstyles recovered by these approaches often mismatch the given images in some im-portant details (e.g., orientation, curliness, and occlusion).
To narrow the domain gap between the synthetic data and real images, most existing methods [36, 37, 40, 45] take 2D orientation map [22] as an intermediate representation between the input image and 3D hair model. However, this undirected 2D orientation map is ambiguous in growing di-rection and loses 3D hints given in the image. More impor-tantly, it relies on image filters, which leads to noisy orien-tation maps. In this work, we re-consider the current issues in single-view 3D hair modeling and believe that it is neces-sary to find a more appropriate intermediate representation to bridge the domain gap between real and synthetic data.
This representation should provide enough information for 3D hair reconstruction. Also, it should be domain invariant and can be easily obtained from real image.
To address the above issues, we propose HairStep, a strand-aware and depth-enhanced hybrid representation for single-view 3D hair modeling. Motivated by how to gener-ate clean orientation maps from real images, we annotate strand maps (i.e., directed 2D orientation maps) for real images via drawing well-aligned dense 2D vector curves along the hair. With this help, we can predict directed and clean 2D orientation maps from input single-view images directly. We also need an extra component of the inter-mediate representation to provide 3D information for hair reconstruction.
Inspired by depth-in-the-wild [5], we an-notate relative depth information for the hair region of real portraits. But depth learned from sparse and ordinal an-notations has a non-negligible domain gap against the syn-thetic depth. To solve this, we propose a weakly-supervised domain adaptive solution based on the borrowed synthetic domain knowledge. Once we obtain the strand map and depth map, we combine them together to form HairStep.
Then this hybrid representation will be fed into a network to learn 3D orientation field and 3D occupancy field of 3D hair models in implicit way. Finally, the 3D strand models can be synthesized from these two fields. The high-fidelity results are shown in Fig. 1. We name our dataset of hair im-ages with strand annotation as HiSa and the one with depth annotation as HiDa for convenience.
Previous methods are mainly evaluated on real inputs through the comparison of the visual quality of recon-structed 3D hair and well-prepared user study. This subjec-tive measurement may lead to unfair evaluation and biased conclusion. NeuralHDHair [36] projects the growth direc-tion of reconstructed 3D strands, and compares with the 2D orientation map filtered from real image. This is a notewor-thy progress, but the extracted orientation map is noisy and inaccurate. Moreover, only 2D growing direction is evalu-ated and 3D information is ignored. Based on our annota-tions, we propose novel and objective metrics for the eval-uation of single-view 3D hair modeling on realistic images.
We render the recovered 3D hair model to obtain strand and depth map, then compare them with our ground-truth anno-tations. Extensive experiments on our real dataset and the synthetic 3D hair dataset USC-HairSalon [10] demonstrate the superiority of our novel representation.
The main contributions of our work are as follows:
• We first re-think the issue of the significant domain gap between synthetic and real data in single-view 3D hair modeling, and propose a novel representation HairStep.
Based on it, we provide a fully-automatic system for single-view hair strands reconstruction which achieves state-of-the-art performance.
• We contribute two datasets, namely HiSa and HiDa, to annotate strand maps and depth for 1,250 hairstyles of real portrait images. This opens a door for future research about hair understanding, reconstruction and editing.
• We carefully design a framework to generate HairStep from real images. More importantly, we propose a weakly-supervised domain adaptive solution for hair depth estimation.
• Based on our annotations, we introduce novel and fair metrics to evaluate the performance of single-view 3D hair modeling methods on real images. 2.