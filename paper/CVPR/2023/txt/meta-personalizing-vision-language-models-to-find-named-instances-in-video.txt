Abstract
Large-scale vision-language models (VLM) have shown impressive results for language-guided search applications.
While these models allow category-level queries, they cur-rently struggle with personalized searches for moments in a video where a specific object instance such as “My dog
Biscuit” appears. We present the following three contribu-tions to address this problem. First, we describe a method to meta-personalize a pre-trained VLM, i.e., learning how to learn to personalize a VLM at test time to search in video.
Our method extends the VLM’s token vocabulary by learn-ing novel word embeddings specific to each instance. To capture only instance-specific features, we represent each instance embedding as a combination of shared and learned global category features. Second, we propose to learn such personalization without explicit human supervision. Our approach automatically identifies moments of named visual instances in video using transcripts and vision-language similarity in the VLM’s embedding space. Finally, we intro-duce This-Is-My, a personal video instance retrieval bench-mark. We evaluate our approach on This-Is-My and Deep-Fashion2 and show that we obtain a 15% relative improve-ment over the state of the art on the latter dataset. 1.

Introduction
The recent introduction of large-scale pre-trained vision-language models (VLMs) has enabled many new vision tasks, including zero-shot classification and retrieval [14, 18, 22], image/video generation [12, 24, 25, 27, 28, 32], or language-guided question answering [1, 19, 42]. It is now possible to search not only for specific object categories (e.g., dogs) but also for more specific descriptions of both
†Equal advising.
∗Work done during CHY’s summer internship at Adobe Research. 2Czech Institute of Informatics, Robotics and Cybernetics at the
Czech Technical University in Prague.
Figure 1. Meta-Personalized Vision-Language Model (VLM) to Retrieve Named Instances in Video. Given a video where a user-specific instance, e.g., “My dog Biscuit” is mentioned, our method automatically learns a representation for the user-specific instance in the VLM’s text input space. The personalized VLM can then be used to retrieve the learned instance in other contexts through natural language queries, e.g., <my dog Biscuit> grabbing a pink frisbee. This result is enabled by meta-personalizing the VLM on a large-scale dataset of narrated videos by pre-learning shared global category tokens (in this example for the category of ’dogs’), which are then easily personalized to user-specific instances from only a few user-given training examples. the object and scene attributes (e.g., “A small white dog playing at the dog park”). However, we often do not want to search for just any example of a generic category but in-stead to find a specific instance. For example, a user might want to search their personal video library for all the scenes that show their dog “Biscuit grabbing a pink frisbee”, as illustrated in Figure 1. Since VLMs do not have a represen-tation of “Biscuit,” such queries are beyond the capabilities
of off-the-shelf VLMs.
Recent work [5] proposed a method to extend the lan-guage encoder’s vocabulary with a newly learned token that represents a specific personal instance to address this is-sue. While this approach enables language-guided search for personal instances by placing the learned tokens in the query prompt, their solution assumes a collection of man-ually annotated images showing the individual instance in various contexts for successful token learning. For this ap-proach to work in practice, a user must manually annotate all their important personal instances in various contexts, such that the instance representation does not capture nui-sance features, e.g., the background. We thus identify two key challenges: 1) collecting personal instance examples without explicit human labeling and 2) learning a gener-alizable object-centric representation of personal instances from very few examples.
The contributions of this work are three-fold. As our first contribution, we propose a method to automatically identify important personal instances in videos for personalizing a vision-language model without explicit human annotations.
Indeed, people often record and refer to personal items or relationships in videos found online. Our approach is thus to identify mentions of personal instances in a video au-tomatically and leverage these moments to build a set of personal instances for training. To this end, we extract the transcripts of videos using speech-to-text models and find candidate moments by looking for occurrences of “this is my *” or similar possessive adjective patterns. The symbol
* in this example could represent a single word or sequence of words describing the instance (e.g., *= “dog Biscuit”).
We then use vision-language similarity to filter non-visual examples and to find additional occurrences in the video for training For example, we found more than six thousand named instances in 50K videos randomly sampled from the
Merlot Reserve dataset [44]. We call the resulting collection of named instances in videos the This-Is-My dataset.
As our second contribution, we propose a novel model and training procedure to learn text tokens representing the named instances in video from possibly very few and noisy training examples. Our method represents each in-stance with learned tokens and models each token as a linear combination of a set of pre-learned category-specific fea-tures shared across different instances. This set of shared category-specific features (similar to object attributes) im-proves the generalization of our method by preventing the instance representations from capturing nuisance features (e.g., the scene background). Furthermore, we show how to pre-train and adapt the shared category features using a large set of automatically collected This-Is-My examples, further improving our model’s few-shot personalization per-formance at test-time. We call this pre-training of shared category features meta-personalization. In contrast to prior work [5], our method does not require training additional neural network models and requires only the optimization of a contrastive learning objective.
As our final contribution, we demonstrate and evalu-ate our model on an existing fashion item retrieval bench-mark, DeepFashion2, and our new challenging This-Is-My video instance retrieval dataset4 depicting specific object instances across different videos and contexts. Our ex-periments demonstrate that our method outperforms sev-eral baselines and prior approaches on these challenging language-guided instance retrieval tasks. 2.