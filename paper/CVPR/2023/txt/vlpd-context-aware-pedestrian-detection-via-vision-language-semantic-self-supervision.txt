Abstract
Detecting pedestrians accurately in urban scenes is sig-nificant for realistic applications like autonomous driving or video surveillance. However, confusing human-like ob-jects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem.
Meanwhile, previous context-aware pedestrian detectors ei-ther only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic con-texts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly se-mantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection and contextual segmentation via self-generated explicit labels of semantic classes by vision-language models. Furthermore, a self-supervised Prototyp-ical Semantic Contrastive (PSC) learning method is pro-posed to better discriminate pedestrians and other classes, based on more explicit and semantic contexts obtained from
VLS. Extensive experiments on popular benchmarks show that our proposed VLPD achieves superior performances over the previous state-of-the-arts, particularly under chal-lenging circumstances like small scale and heavy occlusion.
Code is available at https://github.com/lmy98129/VLPD. 1.

Introduction
With the recent advances of pedestrian detection, enor-mous applications benefit from such a fundamental per-∗ Equal contribution. † Corresponding author.
Figure 1. Illustration of the problems by previous works (top) and our proposed method to tackle them (bottom). (a) and (b) are pre-dicted by [27]. Green boxes are correct, red ones are human-like traffic signs, and dashed blue ones are missing heavily occluded or small scale pedestrians. (c) and (d): We propose self-supervisions to recognize the contexts and discriminate them from pedestrians. ception technique, including person re-identification, video surveillance and autonomous driving.
In the meantime, various challenges from the urban contexts, i.e., pedestri-ans and non-human objects, still hinder the better perfor-mances of detection. For example, confusing appearances of human-like objects often mislead the detector, as shown in Figure 1(a). Moreover, heavily occluded or small scale pedestrians have unusual appearances and cause missing detections as Figure 1(a) and (b). Apart from the object re-gions, the contexts are crucial to address these challenges.
Nevertheless, previous methods still make inadequate in-vestigations on the contexts in urban scenarios. For in-Figure 2. The overall architecture of our proposed VLPD approach. (a) Vision-Language Semantic (VLS) segmentation obtains pseudo labels via Cross-Modal Mapping, then the Pretrained Visual Encoder learns fully-supervised detection (LDet) and self-supervised segmen-tation to recognize semantic classes for explicit contexts without any annotations. (b) Prototypical Semantic Contrastive (PSC) learning lets the pixel-wise pedestrian features as queries closer to positive prototypes and further to negative ones based on Pixel-wise Aggregation. stance, manual contextual annotations from CityScapes [6] boost SMPD [15] on the pedestrian benchmark CityPersons
[43], because they share homologous image data. Besides, a semi-supervised model yields pseudo labels for the Cal-tech dataset [9]. However, both these two solutions require expensive fine-grained annotations, especially for training the semi-supervised model. Moreover, other methods learn regional latent contexts merely from limited visual neigh-borhood [47], or non-human local proposals as negative samples for contrastive learning [23]. Without an explicit awareness of semantic classes in the contexts, these meth-ods thus still suffer from unsatisfactory performance.
Besides, some pedestrian detection methods also indi-rectly handle the contexts. For the occlusion problems, many part-aware methods [4, 13, 19, 20, 28, 33, 41, 44, 45] adopt visible annotations for the occluded pedestrians, which indicate the occlusion by other pedestrians or non-human objects in the contexts. Whereas, these labels still need heavy labors of human annotators. For scale varia-tion [2, 8, 21, 39, 46], crowd occlusion [14, 25, 38, 40, 48, 50] or generic hard pedestrians [1, 24, 26, 27, 34], most previ-ous works are intra-class, e.g., small pedestrians or crowded scenes, and thus irrelevant to context modeling problems.
Inspired by the vision-language models, we notice a more explicit context modeling without any annotations via cross-modal mapping. For instance, DenseCLIP [32] is ini-tialized with vision-language pretrained CLIP model [31] to learn cross-modal mapping from pixel-wise features to linguistic vectors of human-annotated classes. Meanwhile,
MaskCLIP [49] generates pseudo labels via cross-modal mapping and train another visual model. Hence, comple-menting the initialized mapping and pseudo labeling, we propose to recognize the semantic classes for explicit con-texts via self-supervised Vision-Language Semantic (VLS) segmentation, as shown in Figure 1(c) and 2(a).
Furthermore, we consider that only pixel-wise scores are ambiguous to discriminate pedestrians and contexts. Due to the coarse-grained pseudo labels, some parts of pedestrians might have higher scores of other classes. Different from the regional contrastive learning [23], we introduce the con-cept of prototype [35, 51] for a global discrimination. Each pixel of pedestrian features is pulled closer to pixel-wise aggregated positive prototypes and pushed away from the negative ones of other classes based on the explicit contexts obtained from VLS. As illustrated in Figure 1(d) and 2(b), a novel contrastive self-supervision for pedestrian detection is proposed to better discriminate pedestrians and contexts.
In conclusion, we have observed a dilemma between the heavy burden of manual annotation for explicit contexts and local implicit context modeling. Hence, we propose a novel approach to tackle these problems via Vision-Language se-mantic self-supervision for Pedestrian Detection (VLPD).
The main contributions of this paper are as follows:
• Firstly, the Vision-Language Semantic (VLS) segmen-tation method is proposed to model explicit seman-tic contexts by vision-language models. With pseudo labels via cross-modal mapping, the visual encoder learns fully-supervised detection and self-supervised segmentation to recognize the semantic classes for ex-plicit contexts. To our best knowledge, this is the first work to propose such a vision-language extra-annotation-free method for pedestrian detection.
• Secondly, we further propose the Prototypical Seman-tic Contrastive (PSC) learning method to better dis-criminate pedestrians and contexts. The negative and
positive prototypes are aggregated via the score maps of contextual semantic classes obtained from VLS and pedestrian bounding boxes, respectively. Each pixel of pedestrian features is pulled close to positive proto-types and pushed away from the negative ones, in order to strengthen the discrimination power of the detector.
• Finally, by the integration of VLS and PSC, our pro-posed approach VLPD achieves superior performances over the previous state-of-the-art methods on popu-lar Caltech and CityPersons benchmarks, especially on the challenging small scale and occlusion subsets. 2.