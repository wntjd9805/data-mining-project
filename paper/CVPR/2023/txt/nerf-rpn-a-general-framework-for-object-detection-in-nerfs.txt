Abstract
This paper presents the first significant object detection framework, NeRF-RPN, which directly operates on NeRF.
Given a pre-trained NeRF model, NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting a novel voxel representation that incorporates multi-scale 3D neural volumetric features, we demonstrate it is possible to regress the 3D bounding boxes of objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN is a general framework and can be applied to detect objects without class labels. We experimented NeRF-RPN with various backbone architectures, RPN head designs and loss functions. All of them can be trained in an end-to-end manner to estimate high quality 3D bounding boxes.
To facilitate future research in object detection for NeRF, we built a new benchmark dataset which consists of both synthetic and real-world data with careful labeling and clean up. Code and dataset are available at https:
//github.com/lyclyc52/NeRF_RPN . 1.

Introduction 3D object detection is fundamental to important appli-cations such as robotics and autonomous driving, which require scene understanding in 3D. Most existing relevant methods require 3D point clouds input or at least RGB-D images acquired from 3D sensors. Nevertheless, recent advances in Neural Radiance Fields (NeRF) [34] provide an effective alternative approach to extract highly semantic features of the underlying 3D scenes from 2D multi-view
Inspired by Region Proposal Network (RPN) images. for 2D object detection, in this paper, we present the first 3D NeRF-RPN, which directly operates on the NeRF representation of a given 3D scene learned entirely from
RGB images and camera poses. Specifically, given the radiance field and the density extracted from a NeRF model, our method produces bounding box proposals, which can be deployed in downstream tasks.
Recently, NeRF has provided very impressive results
*Equal contribution.
The order of authorship was determined alphabetically.
†This research is supported in part by the Research Grant Council of the Hong Kong SAR under grant no. 16201420.
Figure 1. Region proposal results on a NeRF. Top 12 proposals in eight orientations with highest confidence are visualized. The
NeRF is trained from the Living Room scene from INRIA [38]. in novel view synthesis, while 3D object detection in many real-world has become increasingly important applications such as autonomous driving and augmented reality. Compared to 2D object detection, detection in 3D is more challenging due to the increased difficulty in data collection where various noises in 3D can be captured there is a lot of as well. Despite some good works, room for exploration in the field of 3D object detection.
Image-based 3D object detectors either use a single image (e.g., [1, 4, 62]) or utilize multi-view consensus of multiple images (e.g., [29, 51, 63]). Although the latter use multi-view projective geometry to combine information in the 3D space, they still use 2D features to guide the pertinent 3D prediction. Some other 3D detectors based on point cloud representation (e.g., [31,33,41,73]) heavily rely on accurate data captured by sensors. To our knowledge, there is still no representative work on direct 3D object detection in NeRF.
Thus, we propose NeRF-RPN to propose 3D ROIs in a given NeRF representation. Specifically, the network takes as input the 3D volumetric information extracted from NeRF, and directly outputs 3D bounding boxes of ROIs. NeRF-RPN will thus be a powerful tool for 3D object detection in NeRF by adopting the “3D-to-3D learning” paradigm, taking full advantages of 3D information inherent in NeRF and predicting 3D region proposals directly in 3D space.
As the first significant attempt to perform 3D object detection directly in NeRFs trained from multi-view images, this paper’s focus contributions consist of:
• First significant attempt on introducing RPN to NeRF
for 3D objection detection and related tasks.
• A large-scale public indoor NeRF dataset for 3D object detection, based on the existing synthetic indoor dataset Hypersim [46] and 3D-FRONT [11], and real indoor dataset ScanNet [6] and SceneNN [19], carefully curated for NeRF training.
• Implementation and comparisons of NeRF-RPNs on various backbone networks, detection heads and loss functions. Our model can be trained in 4 hrs using it can 2 NVIDIA RTX3090 GPUs. At runtime, process a given NeRF scene in 115 ms (excluding postprocessing) while achieving a 99% recall on our 3D-FRONT NeRF dataset.
• Demonstration of 3D object detection over NeRF and related applications based on our NeRF-RPN. 2.