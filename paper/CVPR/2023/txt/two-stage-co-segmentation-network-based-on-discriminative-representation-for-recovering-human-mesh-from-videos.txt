Abstract
Recovering 3D human mesh from videos has recently made signiﬁcant progress. However, most of the existing methods focus on the temporal consistency of videos, while ignoring the spatial representation in complex scenes, thus failing to recover a reasonable and smooth human mesh sequence under extreme illumination and chaotic back-grounds. To alleviate this problem, we propose a two-stage co-segmentation network based on discriminative rep-resentation for recovering human body meshes from videos.
Speciﬁcally, the ﬁrst stage of the network segments the video spatial domain to spotlight spatially ﬁne-grained informa-tion, and then learns and enhances the intra-frame discrimi-native representation through a dual-excitation mechanism and a frequency domain enhancement module, while sup-*represents corresponding author, † represents the equal contribution. pressing irrelevant information (e.g., background). The sec-ond stage focuses on temporal context by segmenting the video temporal domain, and models inter-frame discrimina-tive representation via a dynamic integration strategy. Fur-ther, to efﬁciently generate reasonable human discrimina-tive actions, we carefully elaborate a landmark anchor area loss to constrain the variation of the human motion area.
Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art. The Code will be made public. 1.

Introduction 3D human mesh recovery from images and videos has been widely concerned in recent years. Existing methods for estimating human pose and shape from a single im-age are based on parametric human models such as SMPL
[17] etc, which takes a set of model parameters as input  
and ﬁnally outputs a human body mesh. These methods capture the statistical information on human body shape and provide human body mesh for various applications.
While these methods recover body mesh from a single im-age [3, 12, 15, 16] can accurately predict human pose, they may be jittery and intermittent when applied to videos.
The reason for this problem is that the body pose is in-consistent over successive frames and does not reﬂect the body’s motion in the rapidly changing complex scenes of the video. This thus leads to temporal non-smoothness and spatial non-accuracy. Several approaches [5, 7, 12, 14, 22] have been proposed to efﬁciently extend single image-based methods to video. They utilize different temporal encoders to learn the temporal representation directly from videos to better capture temporal information. However, these meth-ods only encode spatial features, ignoring the effective uti-lization of spatial ﬁne-grained features and human motion discriminative features. Therefore, it fails to recover a rea-sonable and smooth human sequence in chaotic and extreme illumination scenes. For example, TCMR [5] recovers the unsatisfactory motion on the left arm of the actor in Figure 1 in complex scenes.
The background and the human in spatial features have a complex relationship. When spatial features are input to the network, it is difﬁcult for the network to distinguish be-tween the human body and the background. At the same time, this relationship is not conducive to our discovery of
ﬁne-grained and discriminable features. Speciﬁcally, in ex-treme illumination and chaotic scenes, messy background severely interferes with human details and movement infor-mation, thus the network cannot reason about accurate hu-man detail features in complex scenes and lacks the ability to discriminate reasonable human movements. We consider both intra-frame and inter-frame multi-level spatial repre-sentations are ideal cues to efﬁciently reason about spa-tial ﬁne-grained information and temporal contextual dis-In addition, learning to repre-criminative information. sent features at different stages is expected to strengthen the model to strip away the complex background and ﬁnd human-separable motion features, thereby further improv-ing human-speciﬁc discriminative capabilities.
Based on the above perspectives, we propose a two-stage co-segmentation network based on discriminative represen-tation for recovering human mesh from videos. In contrast to previous approaches using common spatial features for encoding temporal features, we attempt to segment spatial features into distinct hierarchical of spatial representations and process them separately in different stages. Speciﬁ-cally, the network learns and models intra-frame and inter-frame multi-level discriminative representations by seg-menting spatial features along feature channels and tempo-ral dimensions in two stages. In the ﬁrst stage of the intra-frame discriminative representation, we design a dual exci-tation mechanism that combines self-excitation and channel excitation mechanism to simulate and activate human mo-tion while attenuating the interferences of complex back-grounds.
In addition, we design a frequency domain en-hancement module to capture motion information that can highlight motion features in the frequency domain. In the second stage of inter-frame discriminative representation, we offer a new discriminative representation: the superposi-tion of fragments, which enhances the spatio-temporal rep-resentation of past and future frames by a dynamic integra-tion strategy, while modeling the discriminative represen-tation of the temporal context. Furthermore, to ensure the integrity and plausibility of discriminative motion represen-tation in consecutive frames, we also carefully design a new landmark anchor area loss to optimize the network, thereby further helping the model to reconstruct accurate 3D human actions and poses.
The core contributions of our work are as follows:
• We present a co-segmentation network based on dis-criminative representation for recovering human mesh from videos. Our method motivates and learns spatio-temporal discriminative features at different stages.
• In Stage 1, our proposed dual excitation mechanism and frequency domain enhancement effectively en-hance human motion features and mitigate background interference. In Stage 2, we develop a dynamic inte-gration strategy to integrate the discriminative repre-sentations of distinct stages. We also carefully design a landmark anchor area loss to constrain the generation of the reasonable pose.
• Both the quantitative and qualitative results of our method show the effectiveness of the proposed method on widely evaluated benchmark datasets in comparison with state-of-the-arts. 2.