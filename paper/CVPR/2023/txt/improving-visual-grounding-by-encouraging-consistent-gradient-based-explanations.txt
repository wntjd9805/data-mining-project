Abstract
We propose a margin-based loss for tuning joint vision-language models so that their gradient-based explanations are consistent with region-level annotations provided by humans for relatively smaller grounding datasets. We re-fer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual ground-ing results than previous methods that rely on using vision-language models to score the outputs of object detectors.
Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.49% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when com-pared to the best previous model trained under the same level of supervision. Our approach also performs exceed-ingly well on established benchmarks for referring expres-sion comprehension where it obtains 80.34% accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split.
AMC is effective, easy to implement, and is general as it can be adopted by any vision-language model, and can use any type of region annotations. 1.

Introduction
Vision-language pretraining using images paired with captions has led to models that can transfer well to an ar-ray of tasks such as visual question answering, image-text retrieval and visual commonsense reasoning [6, 18, 22]. Re-markably, some of these models are also able to perform vi-sual grounding by relying on gradient-based explanations.
While Vision-Language Models (VLMs) take advantage of the vast amounts of images and text that can be found on the web, carefully curated data with grounding annotations in the form of boxes, regions, or segments is consider-ably more limited. Our work aims to improve the ground-ing or localization capabilities of vision-language models further by tuning them under a training objective that en-courages their gradient-based explanations to be consistent with human-provided region-based annotations from visu-ally grounded data when those are available.
Figure 1. Gradient-based methods can generate heatmaps that ex-plain the match between images and text for a Vision-language model (VLM). Our work aims to improve their ability to produce visual groundings by directly optimizing their gradient-based ex-planations so that they are consistent with human annotations pro-vided for a reduced set of images.
Vision-language transformers extend the success of masked language modeling (MLM) to multi-modal prob-In vision-language transformers, objectives such lems. as image-text matching (ITM), and image-text contrastive losses (ITC) are used in addition to MLM to exploit com-monalities between images and text [6, 17, 18, 22]. We fur-ther extend these objectives to include our proposed Atten-tion Mask Consistency (AMC) objective. Our formulation is based on the observation that gradient-based explanation maps obtained using methods such as GradCAM [30], can be used to explain the image-text matching of a VLM. Our
AMC objective explicitly optimizes these explanations dur-ing training so that they are consistent with region annota-tions. Figure 1 illustrates an example input image and text
Figure 2. Overview of our method. Among other objectives, standard vision-language models are trained to produce a matching score y given an input image-text pair (V, T ). For inputs containing an extra level of supervision in the form of region annotations (e.g. a triplet (V, T, M )), where M is a binary mask indicating the regions annotated by a human, we optimize the GradCAM [30] gradient-based explanations of the model so that the produced explanations are consistent with region annotations using Lamc by maximizing the energy in the heatmap that falls inside the region annotation and minimizing what falls outside. We accomplish this through soft margin losses as described in Sec. 3.2. pair along with a gradient-based explanation obtained from a VLM model, a region annotation provided by a human, and an improved gradient-based explanation after the VLM model was tuned under our proposed objective.
Our work builds particularly upon the ALBEF model [17] which incorporates a vision-language model architecture based on transformers [36] and has already demonstrated off-the-shelf grounding capabilities using
GradCAM. Gradient-based explanations in the form of heatmaps have been used extensively to explain the areas of the input images that most impact an output value of a model.
In our formulation we actively leverage these heatmaps by designing a loss function that encourages most of the energy in the heatmaps to fall within the areas of the input image that most align with human provided region annotations. Figure 2 shows a detailed overview of our method and objective function. Given an input image and text pair, our goal is to maximize a soft margin between the energy of the heatmap inside the region annotation and the energy of the heatmap outside the region annotation.
A soft-margin is important since typical human region annotations in the form of boxes do not exactly outline objects of different shapes, and in many cases models should still be able to ground an input text with multiple regions across the image.
We compare AMC extensively against other methods that use the same level of supervision but instead use an object detector such as Faster-RCNN [10, 11, 17, 23]. Our method obtains state-of-the-art pointing game accuracy on both Flickr30k and RefCOCO+. Our contributions can be summarized as follows: (1) We introduce a new training objective, AMC, which is effective, simple to implement and can handle multiple types of region annotations, (2) We show that AMC can improve the grounding capabilities of an existing vision-language model â€“ ALBEF, and (3) the resulting model is state-of-the-art in two benchmarks for phrase grounding and referring expression comprehension. 2.