Abstract
Recent advances in 3D scene representation and novel view synthesis have witnessed the rise of Neural Radiance
Fields (NeRFs). Nevertheless, it is not trivial to exploit
NeRF for the photorealistic 3D scene stylization task, which aims to generate visually consistent and photorealistic styl-ized scenes from novel views. Simply coupling NeRF with photorealistic style transfer (PST) will result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be simplified in a new light: When trans-forming the appearance representation of a pre-trained
NeRF with Lipschitz mapping, the consistency and pho-torealism across source views will be seamlessly encoded into the syntheses. That motivates us to build a concise and flexible learning framework namely LipRF, which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the 3D scene. Technically, LipRF first pre-trains a ra-diance field to reconstruct the 3D scene, and then emulates the style on each view by 2D PST as the prior to learn a Lip-schitz network to stylize the pre-trained appearance. In view of that Lipschitz condition highly impacts the expressivity of the neural network, we devise an adaptive regularization to balance the reconstruction and stylization. A gradual gra-dient aggregation strategy is further introduced to optimize
LipRF in a cost-efficient manner. We conduct extensive ex-periments to show the high quality and robust performance of LipRF on both photorealistic 3D stylization and object appearance editing. 1.

Introduction
Photorealistic style transfer (PST) [52] is one of the im-portant tasks for visual content creation, which aims to au-tomatically apply the color style of a reference image to an-*Corresponding author other input (e.g., image [38] or video [65]).
In this task, the stylized result is required to look like a camera shot and preserve the input structure (e.g., edges and regions).
Benefiting from the launch of deep learning, a series of so-phisticated deep PST methods [19, 30, 38, 59, 65] have been developed for practical usage. Recent progress in 3D scene representation has featured Neural Radiance Field [43, 67] (NeRF) with efficient training and high-quality view syn-thesis. This inspires us to go one step further to explore a more challenging task of photorealistic 3D scene styl-ization, which is to generate visually consistent and pho-torealistic stylized syntheses from arbitrary views. Such a task enables an automatic modification of 3D scene appear-ance with different lighting, time of day, weather, or other effects, thereby enhancing user experience and stimulating emotions for virtual reality [57].
Nevertheless, it is not trivial to build an effective frame-work for photorealistic 3D scene stylization. The diffi-culty mainly originates from the fact that there is no valid photorealistic style loss tailored for training NeRF. In gen-eral, the image-based PST is commonly tackled via either the neural style transfer [11] combined with complicated post-processing [30, 38, 41], or particular network struc-tures [29, 61, 65]. However, none of them can be directly applied to the learning of NeRF. As shown in Figure 1, sim-ply employing the state-of-the-art 2D PST on each view might result in noise, disharmony and even inconsistency across views, since the PST methods rely on the size or ob-ject masks of the inputs. Such downsides will be further amplified after reconstructing the 3D scene with NeRF.
To alleviate these limitations, we start with a basic un-derstanding of this task: Though preserving the photoreal-ism and consistency seems to be different in the context of 2D images, they do have the same essence when moving to 3D volume rendering [40]. From this standpoint, the task is simplified as a problem to regulate the volume rendering variance of the radiance field before and after stylization.
According to the studies of color mapping [48,51,52], some
Figure 1. Illustrations of different strategies for photorealistic 3D scene stylization. The 2D PST method [9] generates disharmonious orange color on the stem and petaline edge. The regions bounded by the dotted ellipses are inconsistent due to the white spot in the first view. When employing a radiance field to reconstruct the results of 2D PST method, the disharmony and inconsistency are still retained.
Our LipRF successfully eliminates these downsides, and renders high-quality stylized view syntheses. specific linear mappings of image pixels can nicely preserve the image structures with photorealistic effect. Motivated by this, we theoretically demonstrate that a simple yet ef-fective design of Lipschitz-constrained linear mapping over appearance representation can elegantly control the volume rendering variance. Furthermore, we prove that replac-ing linear mapping with a Lipschitz multilayer perceptron (MLP) also holds these properties under extra assumptions which can be relaxed in practice. Such a way completely eliminates the drawbacks of 2D PST when transforming the radiance field with a Lipschitz MLP (see Figure 1). In a nutshell, our analysis verifies that Lipschitz MLP can be in-terpreted as an implicit regularization to safeguard the 3D photography of stylized scenes.
By consolidating the idea of transforming the radiance field with the Lipschitz network, we propose a novel NeRF-based architecture (namely LipRF) for photorealistic 3D scene stylization. Technically, LipRF contains two stages: 1) training a radiance field to reconstruct the source 3D scene; 2) learning a Lipschitz network to transform the pre-trained appearance representation to the stylized 3D scene with the guidance of style emulation on each view by ar-bitrary 2D PST. We adopt the Plenoxels [67] as the base radiance field due to its advanced reconstruction quality and compressed appearance representation by spheric har-monics. Considering that the Lipschitz condition greatly impacts the expressivity of neural networks, we design an adaptive regularization based on spectral normalization [44] to allow a mild relaxation of the Lipschitz constant in each linear layer. Finally, we capitalize on gradual gradient ag-gregation to optimize LipRF in a cost-efficient fashion.
In summary, we have made the following contributions: (I) We present a thorough and insightful analysis of photo-realistic 3D scene stylization on the basis of volume render-ing and Lipschitz transformation. (II) We build a concise and flexible framework (LipRF) for photorealistic 3D scene stylization by novelly transforming the appearance repre-sentation of a pre-trained NeRF with the Lipschitz Network. (III) Under the Lipschitz condition, we design adaptive reg-ularization and gradual gradient aggregation to seek a better trade-off among the reconstruction, stylization quality, and computational cost. We evaluate LipRF on both photoreal-istic 3D stylization and object appearance editing tasks to validate the effectiveness of our proposal. 2.