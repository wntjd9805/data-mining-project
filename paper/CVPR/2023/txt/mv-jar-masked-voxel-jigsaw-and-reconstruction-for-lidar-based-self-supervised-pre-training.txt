Abstract
This paper introduces the Masked Voxel Jigsaw and
Reconstruction (MV-JAR) method for LiDAR-based self-supervised pre-training and a carefully designed data-efﬁcient 3D object detection benchmark on the Waymo dataset.
Inspired by the scene-voxel-point hierarchy in downstream 3D object detectors, we design masking and re-construction strategies accounting for voxel distributions in the scene and local point distributions within the voxel. We employ a Reversed-Furthest-Voxel-Sampling strategy to ad-dress the uneven distribution of LiDAR points and propose
MV-JAR, which combines two techniques for modeling the aforementioned distributions, resulting in superior perfor-mance. Our experiments reveal limitations in previous data-efﬁcient experiments, which uniformly sample ﬁne-tuning splits with varying data proportions from each LiDAR se-quence, leading to similar data diversity across splits. To address this, we propose a new benchmark that samples scene sequences for diverse ﬁne-tuning splits, ensuring ad-equate model convergence and providing a more accu-rate evaluation of pre-training methods. Experiments on our Waymo benchmark and the KITTI dataset demonstrate that MV-JAR consistently and signiﬁcantly improves 3D detection performance across various data scales, achiev-ing up to a 6.3% increase in mAPH compared to training from scratch. Codes and the benchmark are available at https://github.com/SmartBot-PJLab/MV-JAR. 1.

Introduction
Self-supervised pre-training has gained considerable at-tention, owing to its exceptional performance in visual rep-resentation learning. Recent advancements in contrastive (cid:0) Corresponding author. 67 66 65 64 63 62 61 60
H
P
A m 2
L
Accelerating Convergence
MV-JAR
Random 64 60 56 52 48 44
H
P
A m 2
L
Data-Efficient Results
MV-JAR
Random 59 0 5 10 15 20 25 40 0 10
Epoch 20 30
Data (%) 40 50
Figure 1. 3D object detection results on the Waymo dataset. Our
MV-JAR pre-training accelerates model convergence and greatly improves the performance with limited ﬁne-tuning data. learning [4, 6, 8, 16, 45] and masked autoencoders [1, 7, 15, 36, 47] for images have sparked interest among researchers and facilitated progress in modalities such as point clouds.
However, LiDAR point clouds differ from images and dense point clouds obtained by reconstruction as they are naturally sparse, unorganized, and irregularly distributed.
Developing effective self-supervised proxy tasks for these unique properties remains an open challenge. Construct-ing matching pairs for contrastive learning in geometry-dominant scenes is more difﬁcult [20, 40], as points or re-gions with similar geometry may be assigned as negative samples, leading to ambiguity during training. To address this, our study explores masked voxel modeling paradigms for effective LiDAR-based self-supervised pre-training.
Downstream LiDAR-based 3D object detectors [12, 19, 33, 38, 41, 50] typically quantize the 3D space into vox-els and encode point features within them. Unlike pix-els, which are represented by RGB values, the 3D space introducing new presents a scene-voxel-point hierarchy,    
challenges for masked modeling. Inspired by this, we de-sign masking and reconstruction strategies that consider voxel distributions in the scene and local point distributions in the voxel. Our proposed method, Masked Voxel Jigsaw
And Reconstruction (MV-JAR), harnesses the strengths of both voxel and point distributions to improve performance.
To account for the uneven distribution of LiDAR points, we ﬁrst employ a Reversed-Furthest-Voxel-Sampling (R-FVS) strategy that samples voxels to mask based on their sparseness. This approach prevents masking the furthest distributed voxels, thereby avoiding information loss in re-gions with sparse points. To model voxel distributions, we propose Masked Voxel Jigsaw (MVJ), which masks the voxel coordinates while preserving the local shape of each voxel, enabling scene reconstruction akin to solving a jigsaw puzzle. For modeling local point distributions, we introduce Masked Voxel Reconstruction (MVR), which masks all coordinates of points within the voxel but retains one point as a hint for reconstruction. Combining these two methods enhances masked voxel modeling.
Our experiments indicate that existing data-efﬁcient ex-periments [20, 40] inadequately evaluate the effectiveness of various pre-training methods. The current benchmarks, which uniformly sample frames from each data sequence to create diverse ﬁne-tuning splits, exhibit similar data di-versity due to the proximity of neighboring frames in a sequence [3, 14, 30]. Moreover, these experiments train models for the same number of epochs across different
ﬁne-tuning splits, potentially leading to incomplete conver-gence. As a result, the beneﬁts of pre-trained representa-tions become indistinguishable across splits once the ob-ject detector is sufﬁciently trained on the ﬁne-tuning data.
To address these shortcomings, we propose sampling scene sequences to form diverse ﬁne-tuning splits and establish a new data-efﬁcient 3D object detection benchmark on the
Waymo [30] dataset, ensuring sufﬁcient model convergence for a more accurate evaluation.
We employ the Transformer-based SST [12] as our de-tector and pre-train its backbone for downstream detec-tion tasks. Comprehensive experiments on the Waymo and KITTI [14] datasets demonstrate that our pre-training method signiﬁcantly enhances the model’s performance and convergence speed in downstream tasks. Notably, it im-proves detection performance by 6.3% mAPH when using only 5% of scenes for ﬁne-tuning and reduces training time by half when utilizing the entire dataset (Fig. 1). With the representation pre-trained by MV-JAR, the 3D object de-tectors pre-trained on Waymo also exhibit generalizability when transferred to KITTI. 2.