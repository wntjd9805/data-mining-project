Abstract
Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality im-age generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which di-rectly leads to degraded generation especially in genera-tion diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative
KD that mitigates the discriminator overfitting by challeng-ing the discriminator with harder learning tasks and dis-tilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that im-proves the generation diversity by distilling and preserv-ing the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN com-plements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released. 1.

Introduction
Generative Adversarial Networks (GANs) [12] have be-come the cornerstone technique in various image generation tasks. On the other hand, effective training of GANs relies heavily on large-scale training images that are usually labo-rious and expensive to collect. With limited training data, the discriminator in GANs often suffers from severe over-fitting [40, 53], leading to degraded generation as shown in
Fig. 1. Recent works attempt to address this issue from two major perspectives: i) massive data augmentation that aims
*corresponding author.
Figure 1. With limited training samples, state-of-the-art GANs such as BigGAN suffer from clear discriminator overfitting which directly leads to degraded generation. The recent work attempts to mitigate the overfitting via mass data augmentation in DA [53] or regularization in LeCam-GAN [40]. The proposed KD-DLGAN distills the rich and diverse text-image knowledge from the power-ful visual-language model to the discriminator which greatly mit-igates the discriminator overfitting. Additionally, KD-DLGAN is designed specifically for image generation tasks, which also out-performs the vanilla knowledge distillation (DA+KD) greatly. to expand the distribution of the limited training data [53]; ii) model regularization that introduces regularizers to mod-ulate the discriminator learning [40]. We intend to mitigate the discriminator overfitting from a new perspective.
Recent studies show that knowledge distillation (KD) from powerful vision-language models such as CLIP [36] can effectively relieve network overfitting in visual recog-nition tasks [2, 9, 29, 42]. Inspired by these prior studies, we explore KD for data-limited image generation, aiming to mitigate the discriminator overfitting by distilling the rich
image-text knowledge from vision-language models. One intuitive approach is to adopt existing KD methods [17, 37] for training data-limited GANs, e.g., by forcing the discrim-inator to mimic the representation space of vision-language models. However, such approach does not work well as most existing KD methods are designed for visual recogni-tion instead of GANs as illustrated in DA+KD in Fig. 1.
We propose KD-DLGAN, a knowledge-distillation based image generation framework that introduces the idea of generative KD for training effective data-limited GANs.
KD-DLGAN is designed based on two observations in data-limited image generation: 1) the overfitting is largely at-tributed to the simplicity of the discriminator task, i.e., the discriminator can easily memorize the limited training sam-ples and distinguish them with little efforts; 2) the degra-dation in data-limited generation is largely attributed to poor generation diversity, i.e., the trained data-limited GAN models tend to generate similar images.
Inspired by the two observations, we design two gen-erative KD techniques that jointly distill knowledge from
CLIP [36] to the GAN discriminator for effective training of data-limited GANs. The first is aggregated generative
KD (AGKD) that challenges the discriminator by forcing fake samples to be similar to real samples while mimicking
CLIPâ€™s visual feature space. It mitigates the discriminator overfitting by aggregating features of real and fake samples and distilling generalizable CLIP knowledge concurrently.
The second is correlated generative KD (CGKD) that strives to distill CLIP image-text correlations to the GAN discrim-inator. It improves the generation diversity by enforcing the diverse correlations between images and texts, ultimately improving the generation performance. The two designs distill the rich yet diverse CLIP knowledge which effec-tively mitigates the discriminator overfitting and improve the generation as illustrated in KD-DLGAN in Fig. 1.
The main contributions of this work can be summarized in three aspects. First, we propose KD-DLGAN, a novel image generation framework that introduces knowledge dis-tillation for effective GAN training with limited training data. To the best of our knowledge, this is the first work that exploits the idea of knowledge distillation in data-limited image generation. Second, we design two generative KD techniques including aggregated generative KD and corre-lated generative KD that mitigate the discriminator overfit-ting and improves the generation performance effectively.
Third, extensive experiments over multiple widely adopted benchmarks show that KD-DLGAN achieves superior im-age generation and it also complements the state-of-the-art with consistent and substantial performance gains. 2.