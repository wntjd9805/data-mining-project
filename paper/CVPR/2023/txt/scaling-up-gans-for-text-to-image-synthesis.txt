Abstract
The recent success of text-to-image synthesis has taken the world by storm and captured the general public’s imag-ination. From a technical standpoint, it also marked a dras-tic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL·E 2, autoregressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na¨ıvely in-creasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN ar-chitecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of mag-nitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations. 1.

Introduction
Recently released models, such as DALL·E 2 [53], Ima-gen [59], Parti [73], and Stable Diffusion [58], have ushered in a new era of image generation, achieving unprecedented levels of image quality and model flexibility. The now-dominant paradigms, diffusion models and autoregressive models, both rely on iterative inference. This is a double-edged sword, as iterative methods enable stable training with simple objectives but incur a high computational cost during inference.
Contrast this with Generative Adversarial Networks (GANs) [5,17,33,51], which generate images through a sin-gle forward pass and thus inherently efficient. While such models dominated the previous “era” of generative mod-eling, scaling them requires careful tuning of the network architectures and training considerations due to instabilities in the training procedure. As such, GANs have excelled at modeling single or multiple object classes, but scaling to complex datasets, much less an open world, has remained challenging. As a result, ultra-large models, data, and com-pute resources are now dedicated to diffusion and autore-gressive models. In this work, we ask – can GANs continue to be scaled up and potentially benefit from such resources, or have they plateaued? What prevents them from further scaling, and can we overcome these barriers?
We first experiment with StyleGAN2 [34] and observe that simply scaling the backbone causes unstable training.
We identify several key issues and propose techniques to stabilize the training while increasing the model capacity.
First, we effectively scale the generator’s capacity by re-taining a bank of filters and taking a sample-specific linear combination. We also adapt several techniques commonly used in the diffusion context and confirm that they bring similar benefits to GANs. For instance, interleaving both self-attention (image-only) and cross-attention (image-text) with the convolutional layers improves performance.
Furthermore, we reintroduce multi-scale training, find-ing a new scheme that improves image-text alignment and low-frequency details of generated outputs. Multi-scale training allows the GAN-based generator to use parameters in low-resolution blocks more effectively, leading to better image-text alignment and image quality. After careful tun-ing, we achieve stable and scalable training of a one-billion-parameter GAN (GigaGAN) on large-scale datasets, such as
LAION2B-en [63]. Our results are shown in Figure 1.
In addition, our method uses a multi-stage approach [12, 76]. We first generate at 64 × 64 and then upsample to 512 × 512. These two networks are modular and robust enough to be used in a plug-and-play fashion. We show that our text-conditioned GAN-based upsampling network can be used as an efficient, higher-quality upsampler for a base diffusion model such as DALL·E 2, despite never having seen diffusion images at training time (Figures 1).
Together, these advances enable our GigaGAN to go far beyond previous GANs: than Style-GAN2 [34] and 6× larger than StyleGAN-XL [62] and 36× larger
Figure 1. Our model, GigaGAN, shows GAN frameworks can also be scaled up for general text-to-image synthesis and super-resolution tasks, generating a 512px output at an interactive speed of 0.13s, and 4096px within 3.7s. Selected examples at 2K resolution (text-to-image synthesis) and 1k or 4k resolutions (super-resolution) are shown. For the super-resolution task, we use the caption of “Portrait of a colored iguana dressed in a hoodie.” and compare our model with the text-conditioned upscaler of
Stable Diffusion [57] and unconditional Real-ESRGAN [26]. Please zoom in for more details. See our arXiv paper and website for more uncurated comparisons.
XMC-GAN [75]. While our 1B parameter count is still lower than the largest synthesis models, such as DALL·E 2 (5.5B), and Parti (20B), we have not yet observed a qual-ity saturation regarding the model size. GigaGAN achieves a zero-shot FID of 9.09 on COCO2014, lower than the FID of DALL·E 2, Parti-750M, and Stable Diffusion.
Furthermore, GigaGAN has three major practical ad-vantages compared to diffusion and autoregressive models.
First, it is orders of magnitude faster, generating a 512px image in 0.13 seconds. Second, it can synthesize ultra high-res images at 4k resolution in 3.66 seconds. Third, it is endowed with a controllable, latent vector space that lends itself to well-studied controllable image synthesis applica-tions, such as prompt mixing (Figure 4), style mixing (Fig-ure 5), and prompt interpolation (Figures A7 and A8).
In summary, our model is the first GAN-based method that successfully trains a billion-scale model on billions of real-world complex Internet images. This suggests that
GANs are still a viable option for text-to-image synthe-sis and should be considered for future aggressive scaling.
Please visit our website for additional results. 2.