Abstract
Scaling up neural networks has led to remarkable perfor-mance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guid-ance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has pri-marily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training dis-tribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to en-sure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study is available at https://github.com/LAION-AI/scaling-laws-openclip. 1.

Introduction
Large pre-trained models now achieve state-of-the-art performance on a wide range of tasks. In particular, large models have led to substantial advances in speech [56], language [8, 17, 28, 57], vision [38, 84], and multi-modal language-vision settings [33,54,55,59,62]. A key ingredient in these breakthroughs has been self- or weakly-supervised learning, which enabled the use of Internet-harvested train-ing sets and reduced the need for human annotated data. In
Data
Arch. ImageNet VTAB+ COCO
CLIP [55] WIT-400M L/14
LAION-2B L/14
Ours
LAION-2B H/14
Ours 75.5 75.2 78.0 55.8 54.6 56.4 61.1 71.1 73.4
Table 1. We study the scaling behavior of large CLIP models using fully open-source training code and data. All models in our inves-tigation are available, including the largest public CLIP models.
This table shows zero-shot performance at 224 pixel resolution, displaying accuracy on ImageNet [15], average accuracy on 35
VTAB+ datasets [65, 85], and image retrieval recall at 5 on MS-COCO image retrieval [46]. addition, recent pre-trained models relied on increasing the compute, model, and data scale by orders of magnitude.
When varying model size, compute amount, and data quantity, several papers have empirically observed that both pre-training loss and downstream task performance reliably improve with scale. Specifically, researchers have postulated scaling laws in the form of power law relationships between model performance and model compute, or data scale [35, 61, 73, 84]. Such scaling laws allow practitioners to predict model performance for a given model and compute scale, extrapolate to larger scales, and can be used to determine pre-training regimes that obtain optimal model performance for a fixed amount of compute [28, 35].
So far, the literature on empirical scaling laws has fo-cused on language-only [28, 35, 73] or vision-only mod-els [25, 61, 83]. In the multimodal domain of language and vision, contrastive language-image models such as CLIP [55] have recently achieved large performance gains in zero-image classification, for instance improving zero-shot Im-ageNet accuracy from the prior state-of-the-art of 12% to 76%. Moreover, these models demonstrate unprecedented robustness to distribution shifts compared to prior supervised models [55, 71, 78]. However, there is currently no system-atic investigation for scaling trends in contrastive language-(a) Relationship between total training compute and zero-shot classification performance on downstream tasks. Left: ImageNet performance.
Right: average performance on five ImageNet robustness datasets (ImageNet-V2 [60], ImageNet-R [22], ImageNet-Sketch [75], ObjectNet [5], and ImageNet-A [24]). Scaling model size, data size, and samples seen leads to better performance on zero-shot classification. Models trained on
OpenAI’s WebImageText (WIT) show a stronger scaling than models trained on LAION. (b) Relationship between total training compute and zero-shot image retrieval performance on MS-COCO (Left) and Flickr30K (Right).
Scaling model size, data size, and samples seen leads to better performance on zero-shot image retrieval. Interestingly, in contrast to zero-shot classification (Figure 1a), models trained on LAION show a stronger scaling trend than OpenAI CLIP models trained on OpenAI’s WebImageText (WIT) dataset.
Figure 1. Relationship between total training compute and performance in zero-shot classification (1a) and retrieval (1b). We fit a power-law on the Pareto frontier of the available models. Since total compute budgets (measured in GMAC) of different trained models are not exactly aligned, we divide the total compute scale into bins and select the best model performance from each bin. image learning. One substantial challenge in this direction is that until recently, there were no datasets of sufficiently large scale openly available for the research community to undertake such experiments.
In this work, we conduct a scaling laws study for con-trastive language-vision learning by utilizing the recently re-leased LAION-5B [65] dataset of 5 billion image-text pairs.
To ensure that our experiments are fully reproducible, we use the open source OpenCLIP [32] code to train CLIP models while varying model, data, and samples seen. We evaluate our CLIP models on several downstream tasks, including zero-shot classification, image retrieval, and fine-tuning via linear probing and end-to-end optimization. We observe a consistent increase in performance when scaling model, data, and compute, and derive scaling laws of power law form across different downstream tasks (Figure 1a, 1b). In-terestingly, when comparing our OpenCLIP and OpenAI’s original CLIP models, we find larger scaling coefficients for OpenCLIP models on zero-shot retrieval, while OpenAI
CLIP models show stronger scaling for zero-shot classifica-tion. Table 1 shows two of our models and their results on image classification and retrieval benchmarks.
We hypothesize that the training dataset is responsible for the task-dependent differences in scaling behavior between the OpenCLIP and OpenAI models. Our experiments have used the same ViT architectures as the OpenAI models, and the training recipes are largely matched. The main difference in training recipes is the batch size due to different compute environments, and our experiments with varying batch sizes suggest that the batch size changes do not explain the change in scaling trends.
Overall our findings highlight the design of pre-training
datasets as an important direction to further improve image-text models. Dataset designers should measure scaling be-havior so that the generalization capabilities of image-text models can continue to improve as we increase model size and the amount of compute. Moreover, pre-training datasets should be evaluated on a broad range of downstream tasks because model scaling can differ substantially by task with different pre-training sources leading to different scaling behavior by task. We hope that our open-source and re-producible scaling trends offer concrete starting points for improving current image-text datasets and models. 2.