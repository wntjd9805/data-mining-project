Abstract
Training a 3D human keypoint detector from point clouds in a supervised manner requires large volumes of high quality labels. While it is relatively easy to capture large amounts of human point clouds, annotating 3D key-points is expensive, subjective, error prone and especially difficult for long-tail cases (pedestrians with rare poses, scooterists, etc.).
In this work, we propose GC-KPL -Geometry Consistency inspired Key Point Leaning, an ap-proach for learning 3D human joint locations from point clouds without human labels. We achieve this by our novel unsupervised loss formulations that account for the struc-ture and movement of the human body. We show that by training on a large training set from Waymo Open Dataset
[21] without any human annotated keypoints, we are able to achieve reasonable performance as compared to the fully supervised approach. Further, the backbone benefits from the unsupervised training and is useful in downstream few-shot learning of keypoints, where fine-tuning on only 10 per-cent of the labeled training data gives comparable perfor-mance to fine-tuning on the entire set. We demonstrated that
GC-KPL outperforms by a large margin over SoTA when trained on entire dataset and efficiently leverages large vol-umes of unlabeled data. 1.

Introduction
Estimation of human pose in 3D is an important prob-lem in computer vision and it has a wide range of appli-cations including AR/VR, AI-assisted healthcare, and au-tonomous driving [4, 29, 32]. For autonomous systems, be-ing able to perceive human poses from sensor data (e.g. Li-DAR point clouds) is particularly essential to reason about the surrounding environment and make safe maneuvers.
Despite the high level of interest in human pose estima-tion in the wild, only few papers approached outdoor 3D keypoint detection using point cloud. A main reason is that
*Work done as an intern at Waymo.
Figure 1. We present GC-KPL, a novel method for learning 3D human keypoints from in-the-wild point clouds without any human labels. We propose to learn keypoint locations using unsupervised losses that account for the structure and movement of the human body. The backbone learns useful semantics from unsupervised learning and can be used in down-stream fine-tuning tasks to boost the performance of 3D keypoint estima-tion. training a pedestrian pose estimation model requires large amount of high quality in-the-wild data with ground truth labels. Annotating 3D human keypoints on point cloud data is expensive, time consuming and error prone. Although there are a few existing point cloud datasets with ground truth human poses [11, 13, 21], they are limited in terms of the quantity of the 3D annotations and diversity of the data. Therefore, fully-supervised human keypoint detectors trained on such datasets do not generalize well for long tail cases. For this reason, previous approaches on pedestrian 3D keypoint estimation have mainly focused on utilizing 2D weak supervision [4, 32] which is easier to obtain, or lever-aging signals from others modalities (e.g. RGB, depth) [29].
Nonetheless, there is a lot of useful information in the large amount of unlabeled LiDAR data that previous works on human pose estimation have not made an effort to utilize.
In this work, we propose a novel and effective method for learning 3D human keypoints from in-the-wild point clouds without using any manual labeled 3D keypoints. Our ap-proach is built on top of the key observation that human skeletons are roughly centered within approximately rigid body parts and that the location and movement of the sur-face points should explain the movement of the skeleton and vice versa. To that end, we design novel unsupervised loss terms for learning locations of the 3D keypoints/skeleton within human point clouds which correspond to 3D loca-tions of major joints of human body.
In the proposed method, we first train a transformer-based regression model for predicting keypoints and a se-mantic segmentation model for localizing body parts on a synthetic data constructed from randomly posed SMPL hu-man body model [15]. Then, we train on the entire Waymo
Open Dataset [21] without using any 3D ground-truth anno-tation of human keypoints. Through unsupervised training, keypoint predictions are refined and the backbone learns useful information from large amount of unannotated data.
In summary, we make the following contributions:
• We present GC-KPL, a method for learning human 3D keypoints for in-the-wild point clouds without any manual keypoint annotations.
• Drawing insight from the structure and movement of the human body, we propose three effective and novel unsupervised losses for refining keypoints. We show that the proposed losses are effective for unsupervised keypoint learning on Waymo Open Dataset.
• Through downstream fine-tuning/few-shot experi-ments, we demonstrate that GC-KPL can be used as unsupervised representation learning for human point clouds, which opens up the possibility to utilize a prac-tically infinite amounts of sensor data to improve hu-man pose understanding in autonomous driving. 2.