Abstract
Decentralized learning with private data is a cen-tral problem in machine learning. We propose a novel distillation-based decentralized learning technique that al-lows multiple agents with private non-iid data to learn from each other, without having to share their data, weights or weight updates. Our approach is communication efﬁcient, utilizes an unlabeled public dataset and uses multiple aux-iliary heads for each client, greatly improving training ef-ﬁciency in the case of heterogeneous data. This approach allows individual models to preserve and enhance perfor-mance on their private tasks while also dramatically im-proving their performance on the global aggregated data distribution. We study the effects of data and model archi-tecture heterogeneity and the impact of the underlying com-munication graph topology on learning efﬁciency and show that our agents can signiﬁcantly improve their performance compared to learning in isolation. 1.

Introduction
Supervised training of large models historically relied on access to massive amounts of labeled data. Unfortunately, since data collection and labeling are very time-consuming, curating new high-quality datasets remains expensive and practitioners are frequently forced to get by with a limited set of available labeled datasets. Recently it has been pro-posed to circumvent this issue by utilizing the existence of large amounts of siloed private information. Algorithms ca-pable of training models on the entire available data with-out having a direct access to private information have been developed with Federated Learning approaches [24] taking the leading role.
While very effective in large-scale distributed environ-ments, more canonical techniques based on federated av-eraging, have several noticeable drawbacks. First, gradi-ent aggregation requires individual models to have fully compatible weight spaces and thus identical architectures.
While this condition may not be difﬁcult to satisfy for suf-Figure 1. Conceptual diagram of a distillation in a distributed system. Clients use a public dataset to distill knowledge from other clients, each having their primary private dataset. Individ-ual clients may have different architectures and different objective functions.
ﬁciently small models trained across devices with compat-ible hardware limitations, this restriction may be disadvan-tageous in a more general setting, where some participant hardware can be signiﬁcantly more powerful than the oth-ers. Secondly, federated averaging methods are generally trained in a centralized fashion. Among other things, this prohibits the use of complex distributed communication patterns and implies that different groups of clients cannot generally be trained in isolation from each other for pro-longed periods of time.
Another branch of learning methods suitable for dis-tributed model training on private data are those based on distillation [3, 6, 15].
Instead of synchronizing the inner states of the models, such methods use outputs or intermedi-ate representations of the models to exchange the informa-tion. The source of data for computing exchanged model predictions is generally assumed to be provided in the form of publicly available datasets [12] that do not have to be an-notated since the source of annotation can come from other models in the ensemble (see Figure 1). One interesting in-terpretation of model distillation is to view it as a way of us-ing queries from the public dataset to indirectly gather infor-mation about the weights of the network (see Appendix A).
Unlike canonical federated-based techniques, where the en-tire model state update is communicated, distillation only reveals activations on speciﬁc samples, thus potentially re-ducing the amount of communicated bits of information. By the data processing inequality, such reduction, also trans-lates into additional insulation of the private data used to train the model from adversaries. However, it is worth not-ing that there exists multiple secure aggregation protocols including SecAgg [5] that provide data privacy guarantees for different Federated Learning techniques.
The family of approaches based on distillation is less restrictive than canonical federated-based approaches with respect to the communication pattern, supporting fully dis-tributed knowledge exchange. It also permits different mod-els to have entirely different architectures as long as their outputs or representations are compatible with each other. It even allows different models to use various data modalities and be optimizing different objectives, for example mixing supervised and self-supervised tasks within the same do-main. Finally, notice that the distillation approaches can and frequently are used in conjunction with weight aggrega-tion [21, 30, 31, 37], where some of the participating clients may in fact be entire ensemble of models with identical ar-chitectures continuously synchronized using federated ag-gregation (see Figure 8 in Supplementary).
Our contributions.
In this paper, we propose and em-pirically study a novel distillation-based technique that we call Multi-Headed Distillation (MHD) for distributed learn-ing on a large-scale ImageNet [9] dataset. Our approach (a) inspired by self-distillation is based on two ideas:
[2, 10, 38] we utilize multiple model heads distilling to each other (see Figure 2) and (b) during training we simultane-ously distill client model predictions and intermediate net-work embeddings to those of a target model. These tech-niques allow individual clients to effectively absorb more knowledge from other participants, achieving a much higher accuracy on a set of all available client tasks compared with the naive distillation method.
In our experiments, we explore several key properties of the proposed model including those that are speciﬁc to decentralized distillation-based techniques. First, we anal-yse the effects of data heterogeneity, studying two scenar-ios in which individual client tasks are either identical or very dissimilar. We then investigate the effects of work-ing with nontrivial communication graphs and using het-erogeneous model architectures. Studying complex com-munication patterns, we discover that even if two clients in the ensemble cannot communicate directly, they can still learn from each other via a chain of interconnected clients.
This “transitive” property relies in large part on utilization of multiple auxiliary heads in our method. We also con-duct experiments with multi-client systems consisting of both ResNet-18 and ResNet-34 models [14] and demon-strate that: (a) smaller models beneﬁt from having large models in the ensemble, (b) large models learning from a collection of small models can reach higher accuracies than those achievable with small models only. 2.