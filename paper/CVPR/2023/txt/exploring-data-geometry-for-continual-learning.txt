Abstract
Continual learning aims to efficiently learn from a non-stationary stream of data while avoiding forgetting the knowledge of old data.
In many practical applications, data complies with non-Euclidean geometry. As such, the commonly used Euclidean space cannot gracefully capture non-Euclidean geometric structures of data, leading to in-ferior results.
In this paper, we study continual learning from a novel perspective by exploring data geometry for the non-stationary stream of data. Our method dynami-cally expands the geometry of the underlying space to match growing geometric structures induced by new data, and pre-vents forgetting by keeping geometric structures of old data into account.
In doing so, making use of the mixed cur-vature space, we propose an incremental search scheme, through which the growing geometric structures are en-coded. Then, we introduce an angular-regularization loss and a neighbor-robustness loss to train the model, capa-ble of penalizing the change of global geometric structures and local geometric structures. Experiments show that our method achieves better performance than baseline methods designed in Euclidean space. 1.

Introduction
Unlike humans, artificial neural networks perform poorly to learn new knowledge in a continual manner. The tendency to lose the knowledge previously learned, known as catastrophic forgetting, is due to the fact that important parameters of a neural network for old data are changed to meet the objectives of new data. There have been many continual learning methods [12, 14, 23, 32, 49, 53], and their
âˆ— Corresponding authors: Chen Xu and Yuwei Wu. goal is to remember the knowledge from old data while ef-fectively learning from new data. They have achieved im-pressive performance in alleviating catastrophic forgetting.
However, a long-lasting issue with existing methods is that data geometry is rarely studied in continual learning.
Existing methods usually assume that data is Euclidean and they use Euclidean geometry to process the data stream.
In fact, data in countless applications intrinsically has non-Euclidean geometric structures [2, 6]. Several studies show that non-Euclidean geometric structures can be better cap-tured by particular forms of Riemannian geometry [15, 35].
For example, the hyperbolic geometry has a natural expres-sive ability for the hierarchical structure and is hence used successfully for fine-grained images [22, 29]. The spheri-cal geometry is shown as a suitable choice for face images that have the cyclical structure [27, 46]. In addition to the geometric structures discussed above, natural data may be diverse and irregular in structure, e.g., data exhibits hier-archical forms in some regions and cyclical forms in oth-ers [31, 40]. Overall, distortions produced when using Eu-clidean geometry for non-Euclidean geometric structures are overwhelming, causing the loss of semantic informa-tion, and hence resulting in inferior performance [3]. In this paper, we study how to attain suitable non-Euclidean ge-ometry to capture the intrinsic geometric structures of data during continual learning.
To achieve our goal, we have to face two challenges (see
Fig. 1). (1) Non-stationary stream of data will inevitably increase the complexity of intrinsic geometric structures. In other words, fixing the geometry of the underlying space cannot always match new and unseen data in continual learning. For example, more and more complex hierarchies in a data stream bring more leaf nodes, requiring a faster growing space volume with the radius, which conflicts with a fixed geometry [17]. (2) Old data is not accessible in con-spaces and features are projected to them using initial curva-tures. Given new data, we select constant curvature spaces that contribute significantly to the current task to expand ge-ometry of the mixed-curvature space. In this case, the grow-ing geometric structures are well encoded. We introduce two loss functions, i.e., an angular-regularization loss and a neighbor-robustness loss, to solve the second challenge.
The angular-regularization loss penalizes the change of an-gles between any pair of instances to preserve global struc-tures. The neighbor-robustness loss realizes within-class compactness and between-class separability in a neighbor to preserve the discriminative power of local structures. As a result, our method is capable of efficiently learning from new data and preventing forgetting of old data. Our method is evaluated on multiple continual learning settings, and ex-perimental results show the effectiveness of our method.
In summary, our contributions are three-fold. (1) To the best of our knowledge, we are the first to explore data ge-ometry for continual learning. Our method is efficient for learning from a non-stationary stream of data. (2) We intro-duce an incremental search scheme that identifies the suit-able geometry for growing geometric structures of data. (3)
We introduce an angle-regularization loss and a neighbor-robustness loss, capable of preserving geometric structures of old data in the mixed-curvature space. 2.