Abstract
This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is cru-cial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders apply-ing TTA in real-world deployments. Our approach con-sists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel archi-tecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropaga-tion. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain.
Without additional memory, this regularization prevents er-ror accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation.
We demonstrate that our simple yet effective strategy out-performs other state-of-the-art methods on various bench-marks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and
WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA. 1.

Introduction
Despite recent advances in deep learning [14, 21, 20, 19], deep neural networks often suffer from performance degra-dation when the source and target domains differ signifi-cantly [8, 36, 31]. Among several tasks addressing such domain shifts, test-time adaptation (TTA) has recently re-ceived a significant amount of attention due to its practi-cality and wide applicability especially in on-device set-*Work done during an internship at Qualcomm AI Research.
†Corresponding author.
Qualcomm Technologies, Inc.
‡ Qualcomm AI Research is an initiative of
Figure 1. (a) Memory cost comparison between TTA methods.
The size of activations, not the parameters, is the primary mem-ory bottleneck during training. (b) CIFAR-C adaptation perfor-mance. We perform the continual online adaptation on CIFAR-C dataset. The x- and y-axis are the average error of all corruptions and the total memory consumption including the parameters and activations, respectively. Our approach, EcoTTA, achieves the best results while consuming the least amount of memory, where K is the model partition factor used in our method. tings [53, 35, 26, 15]. This task focuses on adapting the model to unlabeled online data from the target domain with-out access to the source data.
While existing TTA methods show improved TTA per-formances, minimizing the sizes of memory resources have been relatively under-explored, which is crucial considering the applicability of TTA in on-device settings. For example, several studies [54, 35, 9] update entire model parameters
Figure 2. Architecture for test-time adaptation. We illustrate TTA methods: TENT [53], EATA [41], CoTTA [54], and Ours (EcoTTA).
TENT and EATA update multiple batch norm layers, in which large activations have to be stored for gradient calculation. In CoTTA, an entire network is trained with additional strategies for continual adaptation that requires a significant amount of both memory and time. In contrast, our approach requires a minimum size of activations by updating only a few layers. Also, stable long-term adaptation is performed by our proposed regularization, named self-distilled regularization. to achieve large performance improvements, which may be impractical when the available memory sizes are limited.
Meanwhile, several TTA approaches update only the batch normalization (BN) parameters [53, 41, 16] to make the optimization efficient and stable However, even updating only BN parameters is not memory efficient enough since the amount of memory required for training models signifi-cantly depends on the size of intermediate activations rather than the learnable parameters [4, 13, 57]. Throughout the paper, activations refer to the intermediate features stored during the forward propagation, which are used for gradi-ent calculations during backpropagation. Fig. 1 (a) demon-strates such an issue.
Moreover, a non-trivial number of TTA studies assume a stationary target domain [53, 35, 9, 48], but the target do-main may continuously change in the real world (e.g., con-tinuous changes in weather conditions, illuminations, and location [8] in autonomous driving). Therefore, it is nec-essary to consider long-term TTA in an environment where the target domain constantly varies. However, there exist two challenging issues: 1) catastrophic forgetting [54, 41] and 2) error accumulation. Catastrophic forgetting refers to degraded performance on the source domain due to long-term adaptation to target domains [54, 41]. Such an issue is important since the test samples in the real world may come from diverse domains, including the source and tar-get domains [41]. Also, since target labels are unavailable,
TTA relies on noisy unsupervised losses, such as entropy minimization [17], so long-term continual TTA may lead to error accumulation [63, 2].
To address these challenges, we propose memory-Efficient continual Test-Time Adaptation (EcoTTA), a sim-ple yet effective approach for 1) enhancing memory effi-ciency and 2) preventing catastrophic forgetting and error accumulation. First, we present a memory-efficient archi-tecture consisting of frozen original networks and our pro-posed meta networks attached to the original ones. During the test time, we freeze the original networks to discard the intermediate activations that occupy a significant amount of memory. Instead, we only adapt lightweight meta networks to the target domain, composed of only one batch normal-ization and one convolution block. Surprisingly, updating only the meta networks, not the original ones, can result in significant performance improvement as well as consider-able memory savings. Moreover, we propose a self-distilled regularization method to prevent catastrophic forgetting and error accumulation. Our regularization leverages the pre-served source knowledge distilled from the frozen original networks to regularize the meta networks. Specifically, we control the output of the meta networks not to deviate from the one extracted by the original networks significantly. No-tably, our regularization leads to negligible overhead be-cause it requires no extra memory and is performed in par-allel with adaptation loss, such as entropy minimization.
Recent TTA studies require access to the source data be-fore model deployments [35, 9, 28, 1, 33, 41]. Similarly, our method uses the source data to warm up the newly attached meta networks for a small number of epochs before model deployment. If the source dataset is publicly available or the owner of the pre-trained model tries to adapt the model to a target domain, access to the source data is feasible [9].
Here, we emphasize that pre-trained original networks are frozen throughout our process, and our method is applicable to any pre-trained model because it is agnostic to the archi-tecture and pre-training method of the original networks.
Our paper presents the following contributions:
• We present novel meta networks that help the frozen original networks adapt to the target domain. This architecture significantly minimize memory consump-tion up to 86% by reducing the activation sizes of the original networks.
• We propose a self-distilled regularization that controls the output of meta networks by leveraging the output of frozen original networks to preserve the source knowl-edge and prevent error accumulation.
• We improve both memory efficiency and TTA perfor-mance compared to existing state-of-the-art methods on 1) image classification task (e.g., CIFAR10/100-C and ImageNet-C) and 2) semantic segmentation task (e.g., Cityscapes with weather corruption)
2.