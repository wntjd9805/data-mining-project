Abstract
Convolutional neural networks (CNNs) have shown re-markable performance on various tasks. Despite its widespread adoption, the decision procedure of the network still lacks transparency and interpretability, making it diffi-cult to enhance the performance further. Hence, there has been considerable interest in providing explanation and in-terpretability for CNNs over the last few years. Explain-able artificial intelligence (XAI) investigates the relation-ship between input images or videos and output predic-tions. Recent studies have achieved outstanding success in explaining 2D image classification ConvNets. On the other hand, due to the high computation cost and complex-ity of video data, the explanation of 3D video recognition
ConvNets is relatively less studied. And none of them are able to produce a high-level explanation. In this paper, we propose a STCE (Spatial-temporal Concept-based Expla-nation) framework for interpreting 3D ConvNets.
In our approach: (1) videos are represented with high-level su-pervoxels, similar supervoxels are clustered as a concept, which is straightforward for human to understand; and (2) the interpreting framework calculates a score for each con-cept, which reflects its significance in the ConvNet decision procedure. Experiments on diverse 3D ConvNets demon-strate that our method can identify global concepts with dif-ferent importance levels, allowing us to investigate the im-pact of the concepts on a target task, such as action recog-nition, in-depth. The source codes are publicly available at https://github.com/yingji425/STCE. 1.

Introduction
With the rapid development of large-scale datasets and powerful computational devices, convolutional neural net-works (CNNs) have been widely used in various computer vision tasks, such as image classification [16, 17, 37], se-mantic segmentation [24, 42], object detection [22, 27] and so on. Despite the fact that CNN models show competi-tive performance in these tasks, current neural networks are
*These authors contributed equally to this work.
†Jien Kato (jien@fc.ritsumei.ac.jp) is the corresponding author. still regarded as black boxes. Due to the large number of parameters and high nonlinearity [25], the underlying pre-diction mechanism is opaque. This reduces the reliability of neural networks in high-stakes real-world applications such as autonomous driving and medical image analysis [18,29].
In recent years, explainable artificial intelligence (XAI) has become a popular topic to help comprehend model predic-tions and increase the credibility of CNNs.
In general, the explanation methods can be divided into local methods and global methods. Local methods con-centrate on understanding predictions on individual data in-stances, while global methods attempt to explain the overall logic of the target ConvNets at the class or dataset level. In this paper, we focus on the global explanation, which is cru-cial to comprehend the overall behavior of the black boxes.
There are already some methods that provide explana-tions for 2D image classification ConvNets [6,14,26,28,36], and most of them are local techniques. Zhou et al.
[43] generated a Class Activation Map (CAM) using global av-erage pooling for each image to highlight the discriminate regions that are used for the 2D ConvNet to predict class.
Ribeiro et al. proposed Local Interpretable Model-agnostic
Explanations (LIME) [28] to interpret the model by approx-imating the predictions in a local similarity neighborhood of a target image. However, these methods are not only limited to a single prediction, but they are also difficult for humans to comprehend. The highlighted regions are pixel-level, devoid of human-understandable semantic interpreta-tion. More recently, interpretation with high-level concepts has attracted considerable attention. Kim et al. [19] intro-duced concept activation vectors (CAVs) which use the di-rectional derivatives to quantify the importance of the net-work prediction to user-defined concepts. Based on [19],
Ghorbani et al.
[12] proposed ACE (Automatic Concept-based) to discover the relationship between image segments and image classification prediction.
Despite solid achievements in 2D image classification interpretation, only a few studies have attempted to inter-pret 3D action recognition ConvNets, primarily due to the huge computational cost and rich spatial-temporal content of video data. Existing 3D explanation methods are mainly extended from 2D local explanation methods. Stergiou et
al.
[33] proposed Saliency Tubes, which applied Grad-CAM [31] to 3D ConvNets. The activation maps of the 3D
ConvNet’s final convolutional layer are combined to pro-duce heatmaps of input videos. Li et al.
[21] adopt ex-tremal perturbations (EP) [8] to the video case by adding a spatial-temporal smoothness constraint. However, these methods have two major drawbacks: (1) the discriminative 3D regions are based on a single frame and lack spatial-temporal consistency; and (2) the regions are pixel-level and lack high-level semantic information.
To address these issues, we extend 2D ACE [12] to 3D and propose a high-level global interpretation. For each class, videos are segmented into multiple spatial-temporal supervoxels. Similar supervoxels are grouped to form a meaningful concept. Our method can assign a score for each concept according to its contribution when network predicting. When interpreting the decision procedure of 3D action recognition ConvNets, instead of highlighting essen-tial pixels for a single video, our method can answer two fundamental questions at the class level: which objects or motions in the video are significant for a particular action recognition class and which object or motion is the most crucial clue in this class.
Our main contributions can be summarized as follows: 1. We propose a novel Spatial-temporal Concept-based Ex-planation (STCE) for 3D ConvNets. The discrimina-tive regions are spatial-temporal continuous and human-understandable. To the best of our knowledge, STCE is among the first to achieve action recognition interpreta-tion based on high-level video supervoxels. 2. We validate our method using various 3D ConvNets on the Kinetics and KTH datasets. Both qualitative and quantitative results demonstrate that our method can explain the 3D action recognition ConvNets consistent with human cognition. 2.