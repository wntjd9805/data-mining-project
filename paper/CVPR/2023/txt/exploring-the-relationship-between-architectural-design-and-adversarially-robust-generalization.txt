Abstract
Adversarial training has been demonstrated to be one of the most effective remedies for defending adversarial exam-ples, yet it often suffers from the huge robustness general-ization gap on unseen testing adversaries, deemed as the adversarially robust generalization problem. Despite the preliminary understandings devoted to adversarially robust generalization, little is known from the architectural per-spective. To bridge the gap, this paper for the first time systematically investigated the relationship between adver-sarially robust generalization and architectural design. In particular, we comprehensively evaluated 20 most repre-sentative adversarially trained architectures on ImageNette and CIFAR-10 datasets towards multiple ℓp-norm adversar-ial attacks. Based on the extensive experiments, we found that, under aligned settings, Vision Transformers (e.g., PVT,
CoAtNet) often yield better adversarially robust generaliza-tion while CNNs tend to overfit on specific attacks and fail to generalize on multiple adversaries. To better understand the nature behind it, we conduct theoretical analysis via the lens of Rademacher complexity. We revealed the fact that the higher weight sparsity contributes significantly towards the better adversarially robust generalization of Transform-ers, which can be often achieved by the specially-designed attention blocks. We hope our paper could help to better understand the mechanism for designing robust DNNs. Our model weights can be found at http://robust.art. 1.

Introduction
DNNs have achieved remarkable performance across a wide variety of applications [15,28,41,79], yet they are vul-nerable to adversarial examples (malicious inputs that could fool DNNs by adding human-imperceptible perturbations
The first three authors contribute equally. Corresponding author: Xi-anglong Liu, xlliu@buaa.edu.cn.
Figure 1. This paper aims to better understand adversarially robust generalizations from the architectural design perspective.
We comprehensively evaluated 20 adversarially trained architec-tures on several datasets toward multiple ℓp adversaries and found that Transformers often yield better adversarially robust gener-alization. We also undertake theoretical analysis through the
Rademacher complexity lens to further comprehend this.
[12, 33, 37, 54]). Numerous defensive strategies have been offered to combat this issue. The most effective and promis-ing of these approaches is adversarial training [42, 58], which improves adversarial robustness by adding adversar-ial examples into training data.
However, adversarial training tends to exhibit a huge generalization gap between training and testing data [53,59, 76]. Unlike models trained on clean data that usually gener-alize well on clean testing data, differences between train-ing and testing robustness for adversarially-trained models tend to be large. Moreover, the robustness of adversarial training fails to generalize to unseen adversaries during test-ing [29, 31]. The aforementioned adversarially robust gen-eralization problem puts a barrier in the way of the poten-tial applications of adversarial training in practice, where at-tacks are often drawn from multiple unknown attacks, there-fore attracting intensive research interests. Prior works have studied the characteristic of adversarially robust general-ization and proposed many approaches for mitigation (e.g., unlabeled data [4, 47], robust local features [50], genera-tive adversarial training [44]), while little is known about adversarially robust generalization in terms of architectural design. Model architecture reflects the inherent nature of model robustness [55]. To better comprehend and create robust DNNs, it is important to carefully explore the links between architectural design and robust generalization.
In this paper, we present the first comprehensive investi-gation on adversarially robust generalization w.r.t. archi-tectural design.
In particular, we first provide an exten-sive evaluation on 20 adversarially trained architectures, in-cluding the prevailing Vision Transformers (e.g., ViT, PVT,
Swin Transformer) as well as some representative CNNs (e.g., ResNet, VGG) for ImageNette and CIFAR-10 datasets towards multiple adversaries (ℓ1, ℓ2, ℓ∞ adversarial at-tacks). Based on our experiments, we found that, under aligned setting, Vision Transformers (e.g., PVT, CoAtNet) often yield better adversarially robust generalization com-pared to CNNs; yet CNNs often tend to overfit on specific attacks and fail to generalize on multiple adversaries. To better understand the mechanism, we then conduct theoreti-cal analysis via the lens of Rademacher complexity. We the-oretically revealed that higher weight sparsity contributes significantly to the better adversarially robust generaliza-tion of Transformers, which can often be achieved by the specially-designed architectural ingredient attention blocks.
At last, we also provided more detailed analyses to better understand generalization (e.g., generalization on common corruptions, larger dataset ImageNet) and discussed the po-tential directions for designing robust architectures. Our main contributions can be summarized as:
• We, for the first time, systematically studied 20 adversarially-trained architectures against multiple at-tacks and revealed the close relationship between ar-chitectural design and robust generalization.
• We theoretically revealed that higher weight sparsity contributes to the better adversarially robust general-ization of Transformers, which can often be achieved by attention blocks.
• We provide more detailed analyses of the generaliz-ability from several viewpoints and discuss potential pathways that may improve architecture robustness.
We open-sourced the weights of our adversarially-trained model zoo. Together with our analysis, we hope this paper could help researchers build more robust DNN architectures in the future. 2.