Abstract
To effectively exploit the potential of large-scale mod-els, various pre-training strategies supported by massive data from different sources are proposed, including su-pervised pre-training, weakly-supervised pre-training, and self-supervised pre-training. It has been proved that com-bining multiple pre-training strategies and data from var-ious modalities/sources can greatly boost the training of large-scale models. However, current works adopt a multi-stage pre-training system, where the complex pipeline may increase the uncertainty and instability of the pre-training.
It is thus desirable that these strategies can be integrated in a single-stage manner.
In this paper, we first pro-pose a general multi-modal mutual information formula as a unified optimization target and demonstrate that all mainstream approaches are special cases of our frame-work. Under this unified perspective, we propose an all-in-one single-stage pre-training approach, named Maximizing
Multi-modal Mutual Information Pre-training (M3I Pre-training). Our approach achieves better performance than previous pre-training methods on various vision bench-marks, including ImageNet classification, COCO object de-tection, LVIS long-tailed object detection, and ADE20k se-mantic segmentation. Notably, we successfully pre-train a billion-level parameter image backbone and achieve state-of-the-art performance on various benchmarks under pub-lic data setting. Code shall be released at https:// github.com/OpenGVLab/M3I-Pretraining. 1.

Introduction
In recent years, large-scale pre-trained models [5, 13, 27, 30, 37, 55, 65, 91] have swept a variety of computer vision
∗ Equal contribution. †This work is done when Weijie Su and Chenxin
Tao are interns at Shanghai Artificial Intelligence Laboratory. (cid:0) Corre-sponding author.
Figure 1. Comparison between different pre-training paradigms and M3I Pre-training. Existing pre-training methods are all opti-mizing the mutual information between the input and target repre-sentations, which can be integrated by M3I Pre-training. “GAP” refers to global average pooling. tasks with their strong performance. To adequately train large models with billions of parameters, researchers design various annotation-free self-training tasks and obtain suffi-ciently large amounts of data from various modalities and sources. In general, existing large-scale pre-training strate-gies are mainly divided into three types: supervised learn-ing [20, 65] on pseudo-labeled data (e.g., JFT-300M [85]), weakly supervised learning [37,55] on web crawling images text pairs (e.g., LAION-400M [56]), and self-supervised learning [5, 13, 27, 30, 91] on unlabeled images. Supported by massive data, all these strategies have their own advan-tages and have been proven to be effective for large models of different tasks. In pursuit of stronger representations of large models, some recent approaches [47, 78, 81] combine the advantages of these strategies by directly using differ-ent proxy tasks at different stages, significantly pushing the performance boundaries of various vision tasks.
Nevertheless, the pipeline of these multi-stage pre-training approaches is complex and fragile, which may lead to uncertainty and catastrophic forgetting issues. Specif-ically, the final performance is only available after com-pleting the entire multi-stage pre-training pipeline. Due to the lack of effective training monitors in the intermedi-ate stages, it is difficult to locate the problematic training stage when the final performance is poor. To eliminate this dilemma, it is urgent to develop a single-stage pre-training framework that can take advantage of various supervision signals. It is natural to raise the following question: Is it possible to design an all-in-one pre-training method to have all the desired representational properties?
To this end, we first point out that different single-stage pre-training methods share a unified design princi-ple through a generic pre-training theoretical framework.
We further extend this framework to a multi-input multi-target setting so that different pre-training methods can be integrated systematically. In this way, we propose a novel single-stage pre-training method, termed M3I Pre-training, that all desired representational properties are combined in a unified framework and trained together in a single stage.
Specifically, we first introduce a generic pre-training the-oretical framework that can be instantiated to cover existing mainstream pre-training methods. This framework aims to maximize the mutual information between input represen-tation and target representation, which can be further de-rived into a prediction term with a regularization term. (1)
The prediction term reconstructs training targets from the network inputs, which is equivalent to existing well-known pre-training losses by choosing proper forms for the pre-dicted distribution. (2) The regularization term requires the distribution of the target to maintain high entropy to prevent collapse, which is usually implemented implicitly through negative samples or stop-gradient operation. As shown in
Fig. 1, by adopting different forms of input-target paired data and their representations, our framework can include existing pre-training approaches and provide possible direc-tions to design an all-in-one pre-training method.
To meet the requirement of large-scale pre-training with various data sources, we further extend our framework to the multi-input multi-target setting, with which we show that multi-task pre-training methods are optimizing a lower bound of the mutual information.
In addition, we mix two masked views from two different images as the in-put. The representation of one image is used to recon-struct the same view, while the other image is used to re-construct a different augmented view. Both representa-tions will predict their corresponding annotated category or paired texts. In this way, we propose a novel pre-training approach, called M3I Pre-training, which can effectively combine the merits of supervised/weakly-supervised/self-supervised pre-training and enables large-scale vision foun-dation models to benefit from multi-modal/source large-scale data. Our contributions can be summarized as follows:
• We theoretically demonstrate all existing mainstream pre-training methods share a common optimization ob-jective, i.e., maximizing the mutual information between input and target representation. We also show how to in-stantiate our framework as distinct pre-training methods.
• We propose a novel single-stage pre-training approach called M3I Pre-training to gather the benefit of various pre-training supervision signals, via extending our mu-tual information pre-training framework to a multi-input multi-target setting.
• Comprehensive experiments demonstrate the effective-ness of our approach. We successfully pre-train
InternImage-H [79], a model with billion-level parame-ters, and set a new record on basic detection and segmen-tation tasks, i.e., 65.4 box AP on COCO test-dev [46], 62.9 mIoU on ADE20K [98]. 2.