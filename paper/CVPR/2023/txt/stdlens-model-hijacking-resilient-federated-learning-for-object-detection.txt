Abstract
Scenario
Object Detection Results APperson APcar
Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed popula-tion of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the ob-ject detection system should misbehave by implanting Tro-janed gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguard-ing FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradi-ents. Based on the insights, we introduce a three-tier foren-sic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We con-sider three types of adaptive attacks and demonstrate the ro-bustness of STDLens against advanced adversaries. Exten-sive experiments show that STDLens can protect FL against different model hijacking attacks and outperform existing methods in identifying and removing Trojaned gradients with significantly higher precision and much lower false-positive rates. The source code is available at https:
//github.com/git-disl/STDLens. 1.

Introduction
Federated Learning (FL) for object detection has at-tracted numerous applications [12], especially in healthcare, with strict privacy regulations protecting patient medical records [25, 27]. Instead of using a centralized data cura-tor, FL can train a global object detection model with a dis-tributed population of clients. Each client only shares its fixed-size gradient (updated model) parameters (e.g., 246.9
MB for YOLOv3 [15]) with the FL server for aggregation while keeping its private raw data local (e.g., terabytes of videos) [13]. Such a paradigm lowers the bar for knowl-edge sharing and eases the recruitment of contributors, but it becomes vulnerable to model hijacking [9, 21].
Model hijacking aims at interfering with the training pro-Benign 52.43 74.12
Class-Poison
Victim: Person
BBox-Poison
Victim: Person
Objn-Poison
Victim: Person 38.22 (↓27%) 71.79 (↓3%) 18.06 (↓66%) 72.82 (↓2%) 37.61 (↓28%) 71.59 (↓3%)
Table 1. FL-trained detectors can be hijacked by perception poi-soning to misdetect objects of designated classes (e.g., person) in three different ways (2nd to 4th rows) [3], while objects of non-victim classes (e.g., car) have negligible performance degradation. cess of a machine learning model and causes it to misbe-have at the deployment stage [17]. The FL-trained global model can be indirectly hijacked by a small number of com-promised clients who share Trojaned gradients to gradually confuse the learning trajectory of FL [10]. Such gradients can be generated in black-box through data poisoning. The attacker only has to poison the local training data owned by a compromised client, such as changing the ground-truth label of certain objects by dirty-label poisoning [21] or im-planting a backdoor to trigger the malfunctioning of a hi-jacked model [1, 2, 8, 20]. The FL software will take the poisoned local training data and unintentionally generate malicious gradients for sharing. With the growing num-ber of real-world applications, recent work has begun to understand the hijacking of FL-based object detection sys-tems [3]. As shown in Table 1, the adversary can designate how the hijacked model should misdetect objects of cer-tain classes (e.g., person) while ensuring other objects can still be precisely detected (e.g., car). This fine-grained con-trol opens the door for an adversary to design a stealthy at-tack configuration to hurt the FL system yet remains hardly noticeable by the owner [5], leading to catastrophic conse-quences such as car crashes.
This paper introduces STDLens, a three-tier defense methodology against model hijacking attacks, with the fol-lowing original contributions. First, we analyze existing de-fenses based on spatial signature analysis and show their ineffectiveness in protecting FL. Second, we introduce a pipeline with per-client spatio-temporal signature analysis to identify Trojaned gradients, track their contributors, re-voke their subscriptions, and reclaim the detection perfor-mance. Third, we present a density-based confidence in-spection mechanism to manage the spatio-temporal uncer-tainty. It avoids purging benign clients contributing useful learning signals for the FL system. Finally, we extend per-ception poisoning with three types of adaptiveness for an-alyzing STDLens in countering advanced adversaries. Ex-tensive experiments show that STDLens outperforms com-petitive defense methods by a large margin in defense pre-cision with a low false-positive rate, reducing the accuracy drop under attack from 34.47% to 0.24%, maintaining the object detection accuracy on par with the performance un-der the benign scenario. 2.