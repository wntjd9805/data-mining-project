Abstract
While current multi-frame restoration methods combine information from multiple input images using 2D align-ment techniques, recent advances in novel view synthesis are paving the way for a new paradigm relying on volu-metric scene representations.
In this work, we introduce the first 3D-based multi-frame denoising method that sig-nificantly outperforms its 2D-based counterparts with lower computational requirements. Our method extends the mul-tiplane image (MPI) framework for novel view synthesis by introducing a learnable encoder-renderer pair manip-ulating multiplane representations in feature space. The encoder fuses information across views and operates in a depth-wise manner while the renderer fuses information across depths and operates in a view-wise manner. The two modules are trained end-to-end and learn to separate depths in an unsupervised way, giving rise to Multiplane
Feature (MPF) representations. Experiments on the Spaces and Real Forward-Facing datasets as well as on raw burst data validate our approach for view synthesis, multi-frame denoising, and view synthesis under noisy conditions. 1.

Introduction
Multi-frame denoising is a classical problem of com-puter vision where a noise process affecting a set of images must be inverted. The main challenge is to extract con-sistent information across images effectively and the cur-rent state of the art relies on optical flow-based 2D align-ment [3, 7, 45]. Novel view synthesis, on the other hand, is a classical problem of computer graphics where a scene is viewed from one or more camera positions and the task is to predict novel views from target camera positions. This problem requires to reason about the 3D structure of the scene and is typically solved using some form of volumetric representation [28, 32, 55]. Although the two problems are traditionally considered distinct, some novel view synthe-sis approaches have recently been observed to handle noisy
Noisy inputs
IBRNet [48]
NAN [31]
MPFER (ours)
Top: Our Multiplane Features Encoder-Renderer
Figure 1. (MPFER) reimagines the MPI pipeline by moving the multiplane representation to feature space. Bottom: MPFER significantly out-performs existing methods in multiple challenging scenarios, in-cluding here, novel view synthesis from 8 highly degraded inputs. inputs well, and to have a denoising effect in synthesized views by discarding inconsistent information across input views [18,26]. This observation opens the door to 3D-based multi-frame denoising, by recasting the problem as a special case of novel view synthesis where the input views are noisy and the target views are the clean input views [26, 31].
Recently, novel view synthesis has been approached as an encoding-rendering process where a scene representa-tion is first encoded from a set of input images and an ar-bitrary number of novel views are then rendered from this scene representation. In the Neural Radiance Field (NeRF) framework for instance, the scene representation is a radi-ance field function encoded by training a neural network on the input views. Novel views are then rendered by querying and integrating this radiance field function over light rays originating from a target camera position [2, 24, 28]. In the
Multiplane Image (MPI) framework on the other hand, the scene representation is a stack of semi-transparent colored layers arranged at various depths, encoded by feeding the input views to a neural network trained on a large number of scenes. Novel views are then rendered by warping and overcompositing the semi-transparent layers [8, 41, 55].
In the present work, we adopt the MPI framework be-cause it is much lighter than the NeRF framework compu-tationally. The encoding stage only requires one inference pass on a network that generalizes to new scenes instead of training one neural network per-scene, and the rendering stage is essentially free instead of requiring a large number of inference passes. However, the standard MPI pipeline struggles to predict multiplane representations that are self-consistent across depths from multiple viewpoints. This problem can lead to depth-discretization artifacts in syn-thesized views [40] and has previously been addressed at the encoding stage using computationally expensive mech-anisms and a large number of depth planes [8, 11, 27, 40].
Here, we propose to enforce cross-depth consistency at the rendering stage by replacing the fixed overcompositing op-erator with a learnable renderer. This change of approach has three important implications. First, the encoder mod-ule can now process depths independently from each other and focus on fusing information across views. This sig-nificantly reduces the computational load of the encoding stage. Second, the scene representation changes from a static MPI to Multiplane Features (MPF) rendered dynam-ically. This significantly increases the expressive power of the scene encoding. Finally, the framework’s overall per-formance is greatly improved, making it suitable for novel scenarios including multi-frame denoising where it outper-forms standard 2D-based approaches at a fraction of their computational cost. Our main contributions are as follow:
• We solve the cross-depth consistency problem for multi-plane representations at the rendering stage, by introduc-ing a learnable renderer.
• We introduce the Multiplane Feature (MPF) representa-tion, a generalization of the multiplane image with higher representational power.
• We re-purpose the multiplane image framework origi-nally developed for novel view synthesis to perform 3D-based multi-frame denoising.
• We validate the approach with experiments on 3 tasks and 3 datasets and significantly outperform existing 2D-based and 3D-based methods for multi-frame denoising. 2.