Abstract
Extending from unimodal to multimodal is a critical challenge for unsupervised domain adaptation (UDA). Two major problems emerge in unsupervised multimodal domain adaptation: domain adaptation and modality alignment. An intuitive way to handle these two problems is to fulfill these tasks in two separate stages: aligning modalities followed by domain adaptation, or vice versa. However, domains and modalities are not associated in most existing two-stage studies, and the relationship between them is not lever-aged which can provide complementary information to each other. In this paper, we unify these two stages into one to align domains and modalities simultaneously. In our model, a tensor-based alignment module (TAL) is presented to ex-plore the relationship between domains and modalities. By this means, domains and modalities can interact sufficiently and guide them to utilize complementary information for better results. Furthermore, to establish a bridge between domains, a dynamic domain generator (DDG) module is proposed to build transitional samples by mixing the shared information of two domains in a self-supervised manner, which helps our model learn a domain-invariant common representation space. Extensive experiments prove that our method can achieve superior performance in two real-world applications. The code will be publicly available. 1.

Introduction
With explosively emerging multimedia data on the In-ternet, the field of multimodal analysis achieves more and more attention [10, 13, 18, 19, 43]. Compared to extensive unimodal models in NLP and CV, learning adequate knowl-edge from multimodal signals is still preliminary but very important. Abundant data plays a key role in different sce-narios of multimodal analysis, such as pre-training or down-stream multimedia tasks. However, it is prohibitively ex-pensive and time-consuming to obtain large amounts of la-beled data. To eliminate this issue, domain adaptation (DA)
Figure 1. Conception of our one-stage model. is raised to learn a model from a labeled dataset (source domain) that can be generalized to other related tasks with-out sufficient labeled data (target domain) [3]. Classical do-main adaptation can be classified into different categories: unsupervised domain adaptation (UDA), fully supervised domain adaptation, and semi-supervised domain adaptation
[31]. In this paper, we focus on UDA where no samples in target domain are annotated. With this technique, it is not necessary to prepare a customized training dataset for a specific task, but it can perform the task effectively and efficiently.
There are two challenges when applying domain adap-tation to multimodal scenarios [17]: (1) how to align the source and target domains and remit domain discrepancy, and (2) how to align multiple modalities and leverage mul-timodal information. Most existing works address these two problems in two consecutive stages: multimodal align-ment followed by domain adaptation [34, 41], or vice versa
[14, 44]. However, they solve these two issues separately without considering their relationship: domain and modal-ity can be treated as two views to portray the intrinsic char-acteristic of multimodal data [8], and the hidden underlying relationship in these two views can provide complemen-tary information to each other. Unimodal domain adapta-tion methods can not work well in multimodal tasks due to the inability to preserve the relations between modalities at the same time. Through our experiments and analysis, we observe that the two-stage model could not achieve ideal performance. Fig.2 shows the learning curve of two-stage
model during training phase by 800 iterations for the task of multimodal sentiment analysis. It can be found that the learning curve of two-stage model is oscillating and con-verges slowly, which indicates that two-stage model is prob-ably not a superior solution. To handle these challenges, the objective of multimodal domain adaptation can be defined as: (1) Exploring the relationship between domains and modalities; (2) Finding a common domain-invariant cross-modal representation space to align domains and modalities simultaneously.
Therefore, in this paper, we design a One-Stage
Alignment Network (OSAN) to unify multimodal align-ment and domain adaptation in one stage. Fig.1 shows the conception of our one-stage model. Our method benefits from: (1) The modality and domain are associated and in-teracted to capture the relationship between domains and modalities, which can provide rich complementary infor-mation to each other. (2) Multimodal alignment and domain adaptation are unified in one stage, which allows our model to perform domain adaptation and leverage multimodal in-formation at the same time. In Fig.2, we observe that the learning curve of our method is relatively stable and con-verges better, which indicates that exploring the relation be-tween modality and domain contributes to our task.
In summary, our contributions are as follows: (1) To capture the relationship between domain and modality, we propose a one-stage alignment network, called
OSAN, to associate domain and modality.
In this way, a joint domain-invariant and cross-modal representation space is learned in one stage. (2) We design a TAL module to bring sufficient interac-tions between domains and modalities and guide them to utilize complementary information for each other. (3) To effectively bridge distinct domains, a DDG mod-ule is developed to dynamically construct multiple new do-mains by combining knowledge of source and target do-mains and exploring intrinsic structure of data distribution. (4) Extensive experiments on two totally different tasks demonstrate the effectiveness of our method compared to the supervised and strongly UDA methods. 2.