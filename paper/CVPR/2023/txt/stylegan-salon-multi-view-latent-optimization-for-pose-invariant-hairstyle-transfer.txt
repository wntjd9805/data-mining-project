Abstract 1.

Introduction
Our paper seeks to transfer the hairstyle of a reference image to an input photo for virtual hair try-on. We target a variety of challenges scenarios, such as transforming a long hairstyle with bangs to a pixie cut, which requires re-moving the existing hair and inferring how the forehead would look, or transferring partially visible hair from a hat-wearing person in a different pose. Past solutions lever-age StyleGAN for hallucinating any missing parts and pro-ducing a seamless face-hair composite through so-called
GAN inversion or projection. However, there remains a chal-lenge in controlling the hallucinations to accurately transfer hairstyle and preserve the face shape and identity of the input. To overcome this, we propose a multi-view optimiza-tion framework that uses two different views of reference composites to semantically guide occluded or ambiguous regions. Our optimization shares information between two poses, which allows us to produce high fidelity and realistic results from incomplete references. Our framework produces high-quality results and outperforms prior work in a user study that consists of significantly more challenging hair transfer scenarios than previously studied. Project page: https://stylegan-salon.github.io/.
What makes Jennifer Aniston keep her same hairstyle for over three decades? Perhaps she likes the classic, or perhaps changing her hairstyle is a decision too high-stakes that she could later regret. Unlike garments or makeup, trying on a new hairstyle is not easy, and being able to imagine yourself in different hairstyles could be an indispensable tool.
Recent approaches for hairstyle transfer, StyleY-ourHair [20], Barbershop [46], LOHO [30], and Michi-GAN [35], allow users to manipulate multiple hair attributes of an input image, such as appearance, shape, or color by pro-viding a reference image for each different attribute. These methods [20, 30, 46] rely on a generative adversarial net-work [11], specifically StyleGAN2 [19], which can synthe-size highly realistic face images. Their key idea, which also forms the basis of our method, is to leverage the realistic face distribution learned by StyleGAN and search for a hairstyle-transfer output whose latent code lies within the learned distribution using optimization (commonly known as GAN projection or inversion).
Our extensive study on these state-of-the-art techniques still reveal several unsolved challenges for in-the-wild hairstyle transfer. One of the main challenges is when the reference hair comes from a person with a very different head
pose or facial shape. In this case, the transfer result often degrades significantly [30, 46]. HairFIT [8] and the recently proposed StyleYourHair [20] both attempt to solve this using an additional alignment step to align the pose of the target hair to the input face. In HairFIT [8], this is done explicitly via a flow-based warping module for hair segmentation, but this requires training on multi-view datasets [21, 26]. Sty-leYourHair [20] avoids this issue and also improves upon
HairFIT’s results by optimizing for an aligned pose within
StyleGAN2’s latent space using distances between the de-tected facial keypoints. While StyleYourHair can handle a certain degree of misalignment, it often struggles to preserve details of the reference hair texture, especially for intricate and non-straight hairstyles, e.g., in Figure 3.
In general, we have observed that there is a trade-off be-tween hallucinating new details, which is crucial for handling highly different poses, and preserving the original texture from the reference images. These two goals are often at odds with each others. This trade-off is also evident in the results from StyleYourHair, as reported in [20] and as shown in ta-ble 3, where BarberShop [46] can still produce better results, for example, when the pose difference is small.
We tackle this dilemma by performing a multi-stage op-timization. This serves two purposes: first, to hallucinate new details necessary for aligning the poses, and second, to recover face-hair details from the original input images. We preserve these details in a form of two guide images in both viewpoints, which will be jointly optimized to allow new details to be filled while retaining face-hair texture in the original pose. Our pose alignment is done both explicitly via 3D projection [6] on RGB images, and implicitly via latent code(s) sharing during our multi-view optimization.
In summary, our contributions are as follows: 1. We propose StyleGAN Salon: a pose-invariant hairstyle transfer pipeline that is flexible enough to handle a vari-ety of challenging scenarios including, but not limited to, bangs/hat removal and background inpainting. 2. Unlike previous works, our method operates entirely on
RGB images, which are more flexible than segmenta-tion masks. This allows us to first draft the output and then refine them via multi-stage optimization. 3. We introduce multi-view optimization for hairstyle transfer which incorporates 3D information to align the poses for both face and hair images. Our method leverages both views to help preserve details from the original images. 4. We thoroughly analyze the results in several experi-ments, including a user study with detailed breakdown into various challenging scenarios. Our method shows superior results over existing works in all scenarios. 2.