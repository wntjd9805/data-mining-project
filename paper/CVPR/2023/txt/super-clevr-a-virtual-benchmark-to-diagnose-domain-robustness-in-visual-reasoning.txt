Abstract
Visual Question Answering (VQA) models often perform poorly on out-of-distribution data and struggle on domain generalization. Due to the multi-modal nature of this task, multiple factors of variation are intertwined, making gen-eralization difficult to analyze. This motivates us to in-troduce a virtual benchmark, Super-CLEVR, where differ-ent factors in VQA domain shifts can be isolated in or-der that their effects can be studied independently. Four factors are considered: visual complexity, question redun-dancy, concept distribution and concept compositionality.
With controllably generated data, Super-CLEVR enables us to test VQA methods in situations where the test data differs from the training data along each of these axes. We study four existing methods, including two neural symbolic meth-ods NSCL [45] and NSVQA [59], and two non-symbolic methods FiLM [50] and mDETR [29]; and our proposed method, probabilistic NSVQA (P-NSVQA), which extends
NSVQA with uncertainty reasoning. P-NSVQA outperforms other methods on three of the four domain shift factors. Our results suggest that disentangling reasoning and perception, combined with probabilistic uncertainty, form a strong VQA model that is more robust to domain shifts. The dataset and code are released at https://github.com/Lizw14/
Super-CLEVR. 1.

Introduction
Visual question answering (VQA) is a challenging task that assesses the reasoning ability of models to answer ques-tions based on both visual and linguistic inputs. Current
VQA methods are typically developed on standard bench-marks like VQAv2 [16] or GQA [25], with the implicit as-sumption that testing data comes from the same underlying distribution as training data. However, as has been widely studied in computer vision [15, 36, 51], algorithms trained on one domain often fail to generalize to other domains.
Moreover, having learned the distributional prior of training data, models often struggle on out-of-distribution tests. This has been studied in VQA from the perspective of domain transfer [8,57,62], dataset bias [2,11,48], counter-factual di-agnosis [9, 47], and out-of-distribution benchmarking [30].
The multi-modal nature of VQA gives rise to multiple in-tertwined factors of variation, making domain shift an espe-cially difficult problem to study. For example, [8] suggests that VQA domain shifts are a combination of differences in images, questions or answers; and [39] reveals a gap be-tween synthetic and real VQA datasets by differences in the over-specification of questions and the underlying distribu-tion of concepts. However, despite a wealth of research on domain generalization in VQA [3,26,57,62], there is no sys-tematic analysis of the contributing factors in domain shifts.
To this end, we introduce a virtual benchmark, Super-CLEVR, which enables us to test VQA algorithms in sit-uations where the test data differs from the training data.
We decompose the domain shift into a set of isolated con-tributing factors, so that their effects can be diagnosed inde-pendently. We study four factors: visual complexity, ques-tion redundancy, concept distribution, and concept compo-sitionality. These are illustrated in Fig. 1 and described in
Sec. 3.1. With controllable data generation using our Super-CLEVR virtual benchmark, we are able to isolate the differ-ent factors in VQA domain shifts so that their effects can be studied independently. Compared with the original CLEVR dataset [27], Super-CLEVR contains more complicated vi-sual components and has better controllability over the do-main shift factors. As shown in Fig. 1, the Super-CLEVR dataset contains images rendered from 3D graphical vehicle models in the UDA-Part dataset [40], paired with questions and answers automatically generated from templates. The objects and questions are sampled based on the specified un-derlying probability distribution, which can be controlled to produce distribution shifts in different factors.
With Super-CLEVR, we diagnose the domain robust-ness of current VQA models. Four representative mod-els are studied: for the classic two-stream feature fusing
Figure 1. We decompose VQA domain shifts into four contributing factors: visual complexity, question redundancy, concept distribution and concept compositionality. The domain shifts along each factor can be independently studied with the proposed Super-CLEVR dataset. architecture, we choose FiLM [50]; for a large-scale pre-trained model we take mDETR [29]; we use NSCL [45] and NSVQA [59] as representative neuro-symbolic meth-ods. We observe that all these models suffer from do-main shifts to varying degrees of sensitivity. We analyze each factor separately to examine the influence of differ-ent model designs. Specifically, we find that the step-by-step design of neural modular methods enhances their ro-bustness to changes in question redundancy compared with non-modular ones; however, the non-modular models are more robust to visual complexity. Furthermore, thanks to its decomposed reasoning and perception, NSVQA is more robust to concept distribution shifts.
While existing models suffer from domain shifts with different characteristics, we make a technical improvement over NSVQA which enables it to significantly outperform existing models on three of the four factors.
In partic-ular, we inject probabilities into the deterministic sym-bolic executor of NSVQA, empowering it to take into ac-count the uncertainty of scene understanding. We name our model probabilistic NSVQA (P-NSVQA), and show that its performance improvement in both the in-domain and out-of-domain settings. With superior results of P-NSVQA, we suggest that disentangling reasoning from vision and language understanding, together with probabilistic uncer-tainty, gives a strong model that is robust to domain shifts. (1) We introduce the Super-CLEVR benchmark to diagnose VQA robustness along four different factors independently. This benchmark can also be used for part-based reasoning. (2) We enhance a neural-symbolic method by taking the uncertainty of visual understanding into account in reasoning. (3) We conduct detailed analysis of four existing methods, as well as our novel approach to study the influence of model designs on
Our contributions are as follows. distinct robustness factors. We conclude that disentangled reasoning and perception plus explicit modeling of uncer-tainty leads to a more robust VQA model. 2.