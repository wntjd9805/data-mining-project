Abstract
Video frame interpolation (VFI) is the task that synthe-sizes the intermediate frame given two consecutive frames.
Most of the previous studies have focused on appropriate frame warping operations and refinement modules for the warped frames. These studies have been conducted on natural videos containing only continuous motions. How-ever, many practical videos contain various unnatural ob-jects with discontinuous motions such as logos, user in-terfaces and subtitles. We propose three techniques that can make the existing deep learning-based VFI architec-tures robust to these elements. First is a novel data aug-mentation strategy called figure-text mixing (FTM) which can make the models learn discontinuous motions during training stage without any extra dataset. Second, we pro-pose a simple but effective module that predicts a map called discontinuity map (D-map), which densely distin-guishes between areas of continuous and discontinuous mo-tions. Lastly, we propose loss functions to give supervi-sions of the discontinuous motion areas which can be ap-plied along with FTM and D-map. We additionally collect a special test benchmark called Graphical Discontinuous
Motion (GDM) dataset consisting of some mobile games and chatting videos. Applied to the various state-of-the-art
VFI networks, our method significantly improves the inter-polation qualities on the videos from not only GDM dataset, but also the existing benchmarks containing only continu-ous motions such as Vimeo90K, UCF101, and DAVIS. 1.

Introduction
Video frame interpolation (VFI) task is to generate the intermediate frame given some consecutive frames from a video. When the time interval of each input frames is fixed, we can get smoother video, and when the frame rate is fixed, we can get slow-motion video. This can also be applied to other vision tasks such as video compression [2, 34], view
*Both authors contributed equally to this work.
Figure 1. The examples of discontinuous motion. synthesis [7, 13, 39], and other real-world applications [23, 29, 37].
Most of the previous works focus on the motion of the objects in videos. They utilize the estimated flow maps [12, 16], kernels [6, 15, 17, 21, 22], or externally es-timated optical flow maps [19, 20, 24, 25, 29] to place each object in the middle of its position on the adjacent frames.
However, as personal broadcast and cloud gaming con-tents increase, many of the practical videos contain spe-cial objects which do not move continuously such as user interfaces, watermarks, logos, chatting windows and subti-tles (as the examples on Figure 1). Besides, these elements are received at the display devices as part of each frame, not as additional information. Therefore, many of the video en-hancement frameworks, including VFI, should be improved to be robust to the discontinuous motions.
In this paper, our purpose is to expand the spectrum of motions to address efficiently both continuous and discon-tinuous ones, not focusing only on special videos with dis-continuous motions. To achieve this goal, we propose three techniques to process the videos containing both types of motions. First, we propose novel data augmentation method called Figure-Text Mixing (FTM) which consists of Figure
Mixing (FM) and Text Mixing (TM). FM is an augmen-tation of fixed random figures and TM is an augmentation of discontinuity moving random texts. The networks can learn to be robust on both continuous and discontinuous motions using FTM without any additional train datasets.
Second, we propose a lightweight module which estimates a map called discontinuity map (D-map), which determines whether the motion of each output pixel is continuous or discontinuous. When estimating the pixels in the discontin-uous area, a pixel on the same location of one of the input frames is copied instead of predicting the interpolated value.
To prove the versatility and adaptivity of our D-map esti-mation module, we apply it to various state-of-the-art VFI models instead of proposing our original VFI architecture.
Lastly, if we utilize both FTM and D-map, it is possible to supervise the model by giving the ground-truth of D-map.
Therefore, we propose an additional loss function to help our model estimate D-map easily.
We construct a special test set called Graphic Discontin-uous Motion (GDM) dataset to evaluate how our method and the competitive works deal with the discontinuous mo-tions. Our approach shows significantly improved results compared to the other methods on GDM dataset. More-over, our method outperforms those methods on the regu-lar benchmarks that only contain continuous motions such as Vimeo90K test [36], DAVIS [26] and UCF101 [30] datasets. Our main contributions can be summarized as fol-lows:
• New Data Augmentation Strategy. We propose a new data augmentation strategy called FTM that, when ap-plied to existing video datasets, makes models learn both continuous and discontinuous motions without any additional train datasets.
• New Module & Loss Function. We propose a new module which can separate continuous and discontin-uous motions. This module can be applied to many recent deep learning-based VFI architectures. We also propose a loss function to supervise the module.
• Performance. Applied to various state-of-the-art VFI models, our method achieves performance improve-ment on not only the dataset containing discontinuous motions, but also many of the other benchmarks with only continuous motions. 2.