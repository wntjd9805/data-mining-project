Abstract
In this paper, we propose Mixed and Masked AutoEn-coder (MixMAE), a simple but efficient pretraining method that is applicable to various hierarchical Vision Transform-ers. Existing masked image modeling (MIM) methods for hierarchical Vision Transformers replace a random subset of input tokens with a special [MASK] symbol and aim at reconstructing original image tokens from the corrupted im-age. However, we find that using the [MASK] symbol greatly slows down the training and causes pretraining-finetuning inconsistency, due to the large masking ratio (e.g., 60% in SimMIM). On the other hand, MAE does not introduce
[MASK] tokens at its encoder at all but is not applicable for hierarchical Vision Transformers. To solve the issue and accelerate the pretraining of hierarchical models, we replace the masked tokens of one image with visible tokens of an-other image, i.e., creating a mixed image. We then conduct dual reconstruction to reconstruct the two original images from the mixed input, which significantly improves efficiency.
While MixMAE can be applied to various hierarchical Trans-formers, this paper explores using Swin Transformer with a large window size and scales up to huge model size (to reach 600M parameters). Empirical results demonstrate that Mix-MAE can learn high-quality visual representations efficiently.
Notably, MixMAE with Swin-B/W14 achieves 85.1% top-1 accuracy on ImageNet-1K by pretraining for 600 epochs.
Besides, its transfer performances on the other 6 datasets show that MixMAE has better FLOPs / performance tradeoff than previous popular MIM methods. 1.

Introduction
Utilizing unlabeled visual data in self-supervised manners to learn representations is intriguing but challenging. Follow-ing BERT [12] in natural language processing, pretraining with masked image modeling (MIM) shows great success (cid:0) Corresponding author. in learning visual representations for various downstream vision tasks [4, 16, 33, 39, 40], including image classifica-tion [11], object detection [25], semantic segmentation [42], video classification [15], and motor control [39]. While those state-of-the-art methods [4, 16] achieved superior per-formance on vanilla Vision Transformer (ViT) [14, 34], it is still an open question that how to effectively pretrain hierar-chical ViT to purchase further efficiencies [8, 10, 26, 28, 35] on broad vision tasks.
In general, existing MIM approaches replace a portion of input tokens with a special [MASK] symbol and aim at re-covering the original image patches [4, 40]. However, using
[MASK] symbol leads to two problems. On the one hand, the [MASK] symbol used in pretraining never appears in the finetuning stage, resulting in pretraining-finetuning incon-sistency [12]. On the other hand, the pretrained networks waste much computation on processing the less informative
[MASK] symbols, making the pretraining process inefficient.
Those problems become severer when a large masking ratio is used [4,16,33,40]. For example, in SimMIM [40], a mask-ing ratio of 60% is used during the pretraining, i.e., 60% of the input tokens are replaced with the [MASK] symbols. As a result, SimMIM needs relatively more epochs (i.e., 800) for pretraining. In addition, as the high masking ratio causes much pretraining-finetuning inconsistency, the performances of SimMIM on downstream tasks are limited.
In contrast, MAE [16] does not suffer from the above problems by discarding the masked tokens in the encoder and uses the [MASK] symbols only in the lightweight decoder.
MAE utilizes the vanilla ViT [14] as the encoder, which can process the partial input efficiently with the self-attention operation. However, the design also limits the application of
MAE on hierarchical ViTs as the hierarchical ViTs cannot process 1D token sequences with arbitrary lengths [28, 35].
In this work, we propose MixMAE, a generalized pre-training method that takes advantage of both SimMIM [40] and MAE [40] while avoiding their limitations. In particular, given two random images from the training set, MixMAE creates a mixed image with random mixing masks as input
Approach
BEiT [4]
SimMIM [40]
MAE [16]
MixMAE
Compatible with hierarchical ViT
✓
✓
✗
✓
Pretraining efficient
✗
✗
✓
✓
Pretrain-finetune consistent
✗
✗
✓
✓
Table 1. Key differences between MixMAE and related works. and trains a hierarchical ViT to reconstruct the two original images to learn visual representations. From one image’s perspective, instead of replacing the masked tokens of the image with the special [MASK] symbols, the masked tokens are replaced by visible tokens of the other image. MixMAE adopts an encoder-decoder design. The encoder is a hierar-chical ViT and processes the mixed image to obtain hidden representations of the two partially masked images. Before the decoding, the hidden representations are unmixed and filled with the [MASK] tokens. Following MAE [16], the decoder is a small ViT to reconstruct the two original images.
We illustrate the proposed MixMAE in Figure 1.
MixMAE can be widely applied to pretrain different hi-erarchical ViTs, such as Swin Transformer [28], Twins [8],
PVT [35], etc. Thanks to the utilization of the hierarchical architecture, we can naturally apply the pretrained encoder to object detection and semantic segmentation tasks. Em-pirically, with similar model sizes and FLOPs, MixMAE consistently outperforms BEiT [4] and MAE [16] on a wide spectrum of downstream tasks, including image classifica-tion on iNaturalist [18] and Places [41], object detection and instance segmentation on COCO [25], and semantic seg-mentation on ADE20K [42]. By abandoning using [MASK] tokens in the encoder, MixMAE shows much better pretrain-ing efficiency than SimMIM [40] on various hierarchical
ViTs [8, 28, 35]. 2.