Abstract
The goal of few-shot learning is to learn the general-ization from seen to unseen data with only a few samples.
Most previous few-shot learning methods focus on learn-ing the generalization within particular domains. However, the more practical scenarios may also require the gener-In this paper, we study alization ability across domains. the problem of few-shot domain generalization (FSDG), which is a more challenging variant of few-shot classi-fication. FSDG requires additional generalization with larger gap from seen domains to unseen domains. We address FSDG problem by meta-learning two levels of meta-knowledge, where the lower-level meta-knowledge is domain-specific embedding spaces as subspaces of a base space for intra-domain generalization, and the upper-level meta-knowledge is the base space and a prior subspace over domain-specific spaces for inter-domain generaliza-tion. We formulate the two levels of meta-knowledge learn-ing problem with bi-level optimization, and further develop an optimization algorithm without higher-order derivative information to solve it. We demonstrate our method is sig-nificantly superior to the previous works by evaluating it on the widely used benchmark Meta-Dataset. 1.

Introduction
Traditional few-shot classification addresses the problem of learning to classify unseen classes through knowledge of seen classes, and it is based on the i.i.d. assumption, corre-sponding to the practice that test (unseen) classes are con-sistently sampled from the same dataset (domain) as train-ing (seen) classes. We refer to this generalization across classes of the same domain as intra-domain generalization.
In real-life, how to achieve further generalization when fac-ing unseen data from unknown domains may be a problem with more practical interest.
The recently attention-grabbing problem few-shot do-main generalization (FSDG) [5, 6, 28, 50] is to learn a uni-versal model via multiple training domains (e.g., cat breeds, traffic signs and textures) for good generalization to novel classes from a wide range of domains, both in- and out-of-domain. FSDG learns novel classes across domains, and solves few-shot classification problem at an upper level involving more general domain distribution.
It is inter-domain generalization, which is downward compatible with intra-domain generalization.
Typically, the generalization ability of previous few-shot works is obtained by virtue of meta-knowledge [15, 53] learned through meta-learning. ProtoNet [48], a meta-learning metric-based method, learns an embedding space as meta-knowledge, in which instances gather around a pro-totype representation for each class. Such meta-knowledge is learned with the assumption that all data comes from one domain, and the generalization ability is required within the domain. [19] shows that ProtoNet is only better adapted to intra-domain generalization of a single domain, but leads to a negative gain in inter-domain generalization based on multiple known domains. It is manifested in that the model co-learned by multi-domain data is worse than respective models trained by each single domain on in-domain test data, and that the generalization to out-of-domain data is also poor. Most previous works [6, 11, 44] on FSDG tackle the challenges by learning a flexible embedding space with task-adaptive modules based on ProtoNet, which can be seen as learning meta-knowledge that is more generalized to domain-agnostic data than that of ProtoNet. But we first argue that such a stronger meta-knowledge learned in previous works is not effective enough for solving si-multaneously in- and out-of-domain tasks with larger and smaller generalization gaps respectively , and second that the challenges about the negative gain and poor generaliza-tion arise because the meta-knowledge learning is not con-sidered at two levels, inter- and intra-domain levels, where inter-domain meta-knowledge applicable to all domains is more meta-level than intra-domain meta-knowledge.
To this end, to solve inter-domain generalization com-patible with intra-domain generalization, we propose to
MAML [12], respectively. ProtoNet tackles the problem by learning a universal embedding space for a specific do-main such that the distance between images on the embed-ding space is consistent with their semantic distance, but it cannot adapt to the problem of multi-domain learning and generalization. Our work proposes an upper-level meta-knowledge over embedding spaces for multiple domains.
MAML learns a group of initialization parameters suitable for all tasks through bi-level optimization mechanism. In our method, we also use the mechanism, but we learn two levels of meta-knowledge, both inter- and intra-domain.
Multi-domain learning The goal of multi-domain learn-ing in the visual field is to train a single model to handle multiple visual domains.
If data per domain is relatively small and the domains are similar, this common learning of a single model will improve performance across domains than training a separate model for each domain [30]. How-ever, when there is more data and differences between do-mains are significant, the shared model trained by multiple domains does not perform as well as the individual model of each domain [42, 55]. To solve this problem, most of the traditional methods in multi-domain learning develop a tun-able deep network architecture with shared parameters and domain-specific parameters, where shared parameters are generally universal representations [42](i.e. feature extrac-tor) learned by all domains together, and domain-specific parameters are generally small-volume modules learned by each domain data separately, such as adapter residual mod-ules in [42], serial and parallel residual adapters in [43] and feature critic network in [30].
Few-shot domain generalization Recently, some com-munities have made different attempts based on the chal-lenging FSDG. The first type is to learn multiple single-domain models and adaptively select features from mul-tiple model outputs for classification by a feature selec-tion strategy during meta-testing. SUR [11] automatically learns the weights of multiple outputs for feature selection based on the support set and URT [33] proposed a meta-learning layer that can dynamically re-weights and com-poses the most appropriate domain-specific representations in the meta-test phase. The methods have good performance and are simple to implement, but in them, the model storage is large and multiple forward passes lead to high computa-tional efficiency. Another common class of approaches are to learn a shared network with small-capacity task-specific adaptors, whose parameters are usually finetuned or pre-dicted through a meta auxiliary network conditioned on the support set. CNAPS [44] uses a pre-trained feature extractor augmented with FiLM layers [40] and a few-shot adaptive classifier based on conditional neural processes (CNP) [13].
The FiLM layers are adapted for each task using support
Figure 1. (a) Images from different domains in Meta-Dataset [51]. (b) A shared embedding space for all domains. (c) Subspaces of a base space as domain-specific embedding spaces. learn two levels of meta-knowledge, where lower-level intra-domain meta-knowledge is built on upper-level inter-domain meta-knowledge. Specifically, we treat a base fea-ture space as inter-domain meta-knowledge, and then, we model the lower-level intra-domain meta-knowledge with low-rank subspaces of the base feature space, i.e., intra-domain embedding spaces, due to the variability of discrim-inative features across domains, as shown in Figure 1(c).
For a test task from one domain, we need to find the op-timal domain-specific embedding space based on the base space for classification. Naturally, we use a bi-level opti-mization framework [9] for it, where the upper layer learns the base feature space and the lower layer learns a sub-space for each specific domain. In addition, for fast adap-tation or over-fitting prevention on specific domains in the lower level, we not only learn small-capacity subspace or-thogonal projections as subspace representation to project base features, but also propose another inter-domain meta-knowledge, namely a prior subspace. It is used for regular-ization shared across domains such that the lower optimiza-tion can converge rapidly to the optimal solution on each domain. We use projection metric on the Grassmann mani-fold in the regularization term. How to solve this optimiza-tion problem with multi-domain data? We develop an algo-rithm without higher-order derivative information to learn the upper-layer parameters. Through such a mechanism, a more general base feature extractor and a prior subspace are learned, as well as specific subspaces for all seen domains. 2.