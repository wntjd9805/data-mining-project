Abstract
Solving the camera-to-robot pose is a fundamental re-quirement for vision-based robot control, and is a process that takes considerable effort and cares to make accurate.
Traditional approaches require modification of the robot via markers, and subsequent deep learning approaches enabled markerless feature extraction. Mainstream deep learning methods only use synthetic data and rely on Domain Ran-domization to fill the sim-to-real gap, because acquiring the 3D annotation is labor-intensive.
In this work, we go beyond the limitation of 3D annotations for real-world data. We propose an end-to-end pose estimation framework that is capable of online camera-to-robot calibration and a self-supervised training method to scale the training to unlabeled real-world data. Our framework combines deep learning and geometric vision for solving the robot pose, and the pipeline is fully differentiable. To train the Camera-to-Robot Pose Estimation Network (CtRNet), we leverage foreground segmentation and differentiable rendering for image-level self-supervision. The pose prediction is visu-alized through a renderer and the image loss with the input image is back-propagated to train the neural network. Our experimental results on two public real datasets confirm the effectiveness of our approach over existing works. We also integrate our framework into a visual servoing system to demonstrate the promise of real-time precise robot pose es-timation for automation tasks. 1.

Introduction
The majority of modern robotic automation utilizes cam-eras for rich sensory information about the environment to infer tasks to be completed and provide feedback for closed-loop control. The leading paradigm for converting the valuable environment information to the robot’s frame of reference for manipulation is position-based visual ser-voing (PBVS) [4]. At a high level, PBVS converts 3D en-vironmental information inferred from the visual data (e.g. 0 9 0 8
C
U
A 0 7 0 6 0 5
Rendering-based
Keypoint-based
≤ 1 FPS 15 FPS
Speed 30 FPS
Aruco Marker
DREAM-Q
RoboPose(online) Diff. Rendering Ours
DREAM-F
Opt. Keypoints
DREAM-H
RoboPose
Figure 1. Comparison of speed and accuracy (based on AUC met-ric) for existing image-based robot pose estimation methods. the pose of an object to be grasped) and transforms it to the robot coordinate frame where all the robot geometry is known (e.g. kinematics) using the camera-to-robot pose.
Examples of robotic automation using the PBVS range from bin sorting [35] to tissue manipulation in surgery [31].
Calibrating camera-to-robot pose typically requires a significant amount of care and effort. Traditionally, the camera-to-robot pose is calibrated with externally attached fiducial markers (e.g. Aruco Marker [14], AprilTag [38]).
The 2D location of the marker can be extracted from the im-age and the corresponding 3D location on the robot can be calculated with forward kinematics. Given a set 2D-3D cor-respondence, the camera-to-robot pose can be solved using
Perspective-n-Point (PnP) methods [13, 30]. The procedure usually requires multiple runs with different robot configu-rations and once calibrated, the robot base and the camera are assumed static. The incapability of online calibration limits the potential applications for vision-based robot con-trol in the real world, where minor bumps or simply shifting due to repetitive use will cause calibrations to be thrown off,
not to mention real-world environmental factors like vibra-tion, humidity, and temperature, are non-constant. Having flexibility on the camera and robot is more desirable so that the robot can interact with an unstructured environment.
Deep learning, known as the current state-of-the-art ap-proach for image feature extraction, brings promising ways for markerless camera-to-robot calibration. Current ap-proaches to robot pose estimation are mainly classified into two categories: keypoint-based methods [27–29,34,43] and rendering-based methods [16,26]. Keypoint-based methods are the most popular approach for pose estimation because of the fast inference speed. However, the performance is limited to the accuracy of the keypoint detector which is often trained in simulation such that the proposed methods can generalize across different robotic designs. Therefore, the performance is ultimately hampered by the sim-to-real gap, which is a long-standing challenge in computer vision and robotics [55].
Rendering-based methods can achieve better perfor-mance by using the shape of the entire robot as observation, which provides dense correspondence for pose estimation.
The approaches in this category usually employ an iterative refinement process and require a reasonable initialization for the optimization loop to converge [32]. Due to the na-ture that iteratively render and compare is time- and energy-consuming, rendering-based methods are more suitable for offline estimation where the robot and camera are held sta-tionary. In more dynamic scenarios, such as a mobile robot, the slow computation time make the rendering-based meth-ods impracticable to use.
In this work, we propose CtRNet, an end-to-end frame-work for robot pose estimation which, at inference, uses keypoints for the fast inference speed and leverages the high performance of rendering-based methods for training to overcome the sim-to-real gap previous keypoint-based methods faced. Our framework contains a segmentation module to generate a binary mask of the robot and keypoint detection module which extracts point features for pose es-timation. Since segmenting the robot from the background is a simpler task than estimating the robot pose and localiz-ing point features on robot body parts, we leverage fore-ground segmentation to provide supervision for the pose estimation. Toward this direction, we first pretrained the network on synthetic data, which should have acquired es-sential knowledge about segmenting the robot. Then, a self-supervised training pipeline is proposed to transfer our model to the real world without manual labels. We connect the pose estimation to foreground segmentation with a dif-ferentiable renderer [24,33]. The renderer generates a robot silhouette image of the estimated pose and directly com-pares it to the segmentation result. Since the entire frame-work is differentiable, the parameters of the neural network can be optimized by back-propagating the image loss.
Contributions. Our main contribution is the novel framework for image-based robot pose estimation together with a scalable self-training pipeline that utilizes unlim-ited real-world data to further improve the performance without any manual annotations. Since the keypoint de-tector is trained with image-level supervision, we effec-tively encompass the benefits from both keypoint-based and rendering-based methods, where previous methods were di-vided. As illustrated in the Fig. 1, our method maintains high inference speed while matching the performance of the rendering-based methods. Moreover, we integrate the CtR-Net into a robotic system for PBVS and demonstrate the effectiveness on real-time robot pose estimation. 2.