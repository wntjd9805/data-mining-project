Abstract
Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces the
Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set cov-ering 117 attribute classes on the 80 object classes of MS
COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the bench-mark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary at-tribute detection. Moreover, we demonstrate the bench-mark’s value by studying the attribute detection perfor-mance of several foundation models. 1.

Introduction
One of the main goals of computer vision is to develop models capable of localizing and recognizing an open set of visual concepts in an image. This has been the main direc-tion for the recently proposed Open-Vocabulary Detection (OVD) task [50] for object detection, where the goal is to detect a flexible set of object classes that are only defined at test time via a text query. Classical supervised object de-tection methods are bound to predict objects from a fixed set of pre-defined classes, and extending them to a very large number of classes is limited by the annotation effort.
Acknowledgements This work was supported by the German Aca-demic Exchange Service (DAAD) - 57440921 Research Grants - Doctoral
Programmes in Germany, 2019/20. This work was partially funded by the
German Research Foundation (DFG) - 401269959 and 417962828. We thank Philipp Schr¨oppel, Silvio Galesso and Jan Bechtold for their critical feedback on the paper. We thank all OVAD annotators especially Mariana
Sarmiento and Jorge Bravo for their help, time, and effort.
Figure 1. Example from the presented open vocabulary attribute detection benchmark. The objective is to detect all objects and visual attributes of each object in the image. Objects and attributes are only specified at test time via text prompts.
OVD methods overcome this constraint by utilizing vision-language modeling to learn about novel objects using the weak supervision of image-text pairs.
OVD methods for object detection have made fast progress and have even surpassed supervised baselines for rare (tail) classes [16]. Best OVD methods [16, 35, 53, 54] train with extra weak supervision using image classification datasets, which are focused on retrieving object informa-tion. However, it is unclear on how well OVD methods generalize information beyond the object class. This pa-per focuses on object-level attribute information, such as the object’s state, size, and color.
Attributes play a significant role in an object’s identity.
A small change of an attribute in a description can mod-ify our understanding of an object’s appearance and percep-tion. Imagine driving in a forest where you encounter a bear like the one in Figure 1. Even if you do not distinguish or know the type of bear, recognizing that it is made of wood is enough to realize that it is fake and harmless. A model capa-ble of detecting object attributes enables a richer reasoning ability via combining objects and attributes. It allows the model to potentially extrapolate to novel object classes.
In this paper, we introduce the Open-Vocabulary At-tribute Detection (OVAD) task. Its objective is to detect and recognize an open set of objects in an image together with an open set of attributes for every object. Both sets are de-fined by text queries during inference without knowledge of the tested classes during training. The OVAD task is a two-stage task. The first stage, referred to as open-vocabulary object detection [50], seeks to detect all objects in the im-age, including novel objects for which no bounding box or class annotation is available during training. The second stage seeks to determine all attributes present for each de-tected object. None of the attributes is annotated; therefore, all attributes are novel.
Testing the OVAD task requires an evaluation bench-mark with unambiguous and dense attribute annotations to identify misses as well as false positive predictions. Current datasets [32, 33] for predicting attributes in-the-wild come with many missing or erroneous annotations, as discussed in more detail in Section 3.2. Thus, in this paper, we introduce the OVAD benchmark, an evaluation benchmark for open-vocabulary attribute detection. It is based on images of the
MS COCO [29] dataset and only contains visually identifi-able attributes. On average, the proposed benchmark has 98 attribute annotations per object instance, with 7.2 objects per image, for a total of 1.4 million attribute annotations, making it the most densely annotated object-level attribute dataset. It has a large coverage with 80 object categories and 117 attribute categories. It also provides negative at-tribute annotations, which enables quantifying false posi-tive predictions. The benchmark is devoid of various label-ing errors since it is manually annotated and quality-tested for annotation consistency. Our OVAD benchmark also ex-tends the OVD benchmark [50] by including all 80 COCO object classes. This extension increases the novel set of ob-jects from 17 to 32 classes. Together with the benchmark, we provide a first baseline method that learns the OVAD task to a reasonable degree. It learns the task from image-caption pairs by using all components of the caption, not only nouns. We also compare the performance of several off-the-shelf OVD models to get an insight of how much attribute information is implicitly comprised in nouns (e.g., puppy implies a young dog).
Moreover, we demonstrate the value of the benchmark by evaluating object-level attribute information learned by several open-source vision-language models, some-times also referred to as foundation models, including
CLIP [34], Open CLIP [20], BLIP [26], ALBEF [27], and
X-VLM [51]. Such models learn from the weak supervi-sion of image-text pairs, which is assumed to be available particularly via web content. The results show the extent to which the present success of foundation models on object classes generalizes to attributes.
Contributions (1) We introduce the Open-Vocabulary
Attribute Detection (OVAD) task, where the objective is to detect all objects and predict their associated attributes.
These objects and attributes belong to an open set of classes and can be queried using textual input. (2) We propose the
OVAD benchmark: a clean and densely annotated evalua-tion dataset for open-vocabulary attribute detection, which can be used to evaluate open-vocabulary methods as well as foundation models. (3) We provide an attribute-focused baseline method for the OVAD task, which outperforms the existing open-vocabulary models that only aim for the ob-ject classes. (4) We test the performance of several open-source foundation models on visual attribute detection. 2.