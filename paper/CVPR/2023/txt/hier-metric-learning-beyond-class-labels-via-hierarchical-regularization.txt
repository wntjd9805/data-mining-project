Abstract
Supervision for metric learning has long been given in the form of equivalence between human-labeled classes. Al-though this type of supervision has been a basis of metric learning for decades, we argue that it hinders further ad-vances in the field. In this regard, we propose a new regular-ization method, dubbed HIER, to discover the latent seman-tic hierarchy of training data, and to deploy the hierarchy to provide richer and more fine-grained supervision than inter-class separability induced by common metric learn-ing losses. HIER achieves this goal with no annotation for the semantic hierarchy but by learning hierarchical proxies in hyperbolic spaces. The hierarchical proxies are learn-able parameters, and each of them is trained to serve as an ancestor of a group of data or other proxies to approxi-mate the semantic hierarchy among them. HIER deals with the proxies along with data in hyperbolic space since the geometric properties of the space are well-suited to repre-sent their hierarchical structure. The efficacy of HIER is evaluated on four standard benchmarks, where it consis-tently improved the performance of conventional methods when integrated with them, and consequently achieved the best records, surpassing even the existing hyperbolic metric learning technique, in almost all settings. 1.

Introduction
Learning a discriminative and generalizable embedding space has been a vital step within many machine learning tasks including content-based image retrieval [18, 26, 36, 37], face verification [20, 33], person re-identification [6, 50], few-shot learning [29, 35, 39], and representation learning [18, 45, 53]. Deep metric learning has aroused lots of attention as an effective tool for this purpose. Its goal is to learn an embedding space where semantically simi-lar samples are close together and dissimilar ones are far apart. Hence, the semantic affinity between samples serves as the main supervision for deep metric learning, and has long been given in the form of equivalence between their human-labeled classes.
Figure 1. Motivation of HIER. HIER aims to discover an in-herent but latent semantic hierarchy of data (colored dots on the boundary) by learning hierarchical proxies (larger black dots) in hyperbolic space. The semantic hierarchy is deployed to provide rich and fine-grained supervision that cannot be derived only by human-labeled classes.
Although this type of supervision has been a basis of metric learning for decades, we argue that it now hinders further advances of the field. The equivalence of human-labeled classes deals with only a tiny subset of possible re-lations between samples due to the following two reasons.
First, the class equivalence is examined at only a single fixed level of semantic hierarchy, although different classes can be semantically similar if they share the same super-class. Second, the equivalence is a binary relation that ig-nores the degree of semantic affinity between two classes.
It is difficult to overcome these two limitations since the se-mantic hierarchy of data is latent and only human-labeled classes are available from existing datasets in the standard metric learning setting. However, once they are resolved, one can open up the possibility of providing rich supervi-sion beyond human-labeled classes.
In this regard, we propose a new regularization method, called HIErarchical Regularization (HIER), to discover and deploy the latent semantic hierarchy of training data for metric learning. Since the semantic hierarchy can capture not only the predefined set of classes but also their sub-classes, super-classes, and affinities between them, our reg-ularizer is expected to provide richer and more fine-grained supervision beyond inter-class separability induced by com-mon metric learning losses. However, it is challenging to establish such a semantic hierarchy of data given their 1
human-labeled classes only due to the absence of annota-tion for the semantic hierarchy.
HIER tackles this challenge by learning hierarchical proxies in hyperbolic space. The hierarchical proxies are learnable parameters, and each of them is trained to serve as an ancestor of a group of data or other proxies to approx-imate the semantic hierarchy. Also, HIER deals with the proxies in hyperbolic space since the space is well-suited to represent hierarchical structures of the proxies and data.
It has been reported in literature that Euclidean space with zero curvature is not optimal for representing data exhibit-ing such a semantic hierarchy [16]. In contrast, hyperbolic space with constant negative curvature can represent the se-mantic hierarchy of data effectively using relatively small dimensions since its volume increases exponentially as its
Poincar´e radius increases [31, 32].
To be specific, HIER is designed as a soft approxima-tion of hierarchical clustering [7, 25, 43], where similar data should be near by one another in a tree-structured hierarchy while dissimilar ones should be placed in separate branches of the hierarchy (See Figure 1). During training, HIER con-structs a triplet of samples or proxies, in which two of them are similar and the other is dissimilar based on their hyper-bolic distances. Then, the proxy closest to the entire triplet is considered as the lowest common ancestor (LCA) of the triplet in a semantic hierarchy; likewise, we identify another proxy as the LCA of only the similar pair in the same man-ner. Given the triplet and two LCAs (proxies), HIER en-courages that each of the LCAs and its associated members of the triplet are close together and that the dissimilar one of the triplet is far apart from the LCA of the similar pair. This allows the hierarchical proxies to approximate a semantic hierarchy of data in the embedding space, without any off-the-shelf module for pseudo hierarchical labeling [51].
The major contribution of our work from the perspective of conventional metric learning is three-fold:
• We study a new and effective way of providing seman-tic supervision beyond human-labeled classes, which has not been actively studied in metric learning [13, 24, 30, 51, 52].
• HIER consistently improved performance over the state-of-the-art metric learning losses when integrated with them, and consequently achieved the best records in almost all settings on four public benchmarks.
• By taking advantage of hyperbolic space, HIER sub-stantially improved performance particularly on low-dimensional embedding spaces.
Also, when regarding our work as a hyperbolic metric learn-ing method, a remarkable contribution of HIER is that it al-lows taking full advantage of both hyperbolic embedding space and the great legacy of conventional metric learn-ing since it can be seamlessly incorporated with any metric learning losses based on spherical embedding spaces. The only prior studies on hyperbolic metric learning [11, 51] are not compatible with conventional losses for metric learn-ing since they learn a distance metric directly on hyperbolic space, in which, unlike the spherical embedding spaces, norms of embedding vectors vary significantly. 2.