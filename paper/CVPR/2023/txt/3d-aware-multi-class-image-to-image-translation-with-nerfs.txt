Abstract
Recent advances in 3D-aware generative models (3D-aware GANs) combined with Neural Radiance Fields (NeRF) have achieved impressive results. However no prior works investigate 3D-aware GANs for 3D consistent multi-class image-to-image (3D-aware I2I) translation. Naively using 2D-I2I translation methods suffers from unrealistic shape/identity change. To perform 3D-aware multi-class
I2I translation, we decouple this learning process into a multi-class 3D-aware GAN step and a 3D-aware I2I trans-*The corresponding author. lation step.
In the first step, we propose two novel tech-niques: a new conditional architecture and an effective training strategy.
In the second step, based on the well-trained multi-class 3D-aware GAN architecture, that pre-serves view-consistency, we construct a 3D-aware I2I trans-lation system. To further reduce the view-consistency prob-lems, we propose several new techniques, including a U-net-like adaptor network design, a hierarchical representa-tion constrain and a relative regularization loss. In exten-sive experiments on two datasets, quantitative and qualita-tive results demonstrate that we successfully perform 3D-aware I2I translation with multi-view consistency. Code is
available in 3DI2I. 1.

Introduction
Neural Radiance Fields (NeRF) have increasingly gained attention with their outstanding capacity to synthesize high-quality view-consistent images [31,39,66]. Benefiting from the adversarial mechanism [11], StyleNeRF [12] and con-current works [4, 8, 44, 69] have successfully synthesized high-quality view-consistent, detailed 3D scenes by com-bining NeRF with StyleGAN-like generator design [22].
This recent progress in 3D-aware image synthesis has not yet been extended to 3D-aware I2I translation, where the aim is to translate in a 3D-consistent manner from a source scene to a target scene of another class (see Figure 1).
A naive strategy is to use well-designed 2D-I2I trans-lation methods [15, 16, 26, 28, 46, 63, 65, 70]. These meth-ods, however, suffer from unrealistic shape/identity changes when changing the viewpoint, which are especially notable when looking at a video. Main target class characteristics, such as hairs, ears, and noses, are not geometrically realis-tic, leading to unrealistic results which are especially dis-turbing when applying I2I to translate videos. Also, these methods typically underestimate the viewpoint change and result in target videos with less viewpoint change than the source video. Another direction is to apply video-to-video synthesis methods [2, 3, 6, 30, 53]. These approaches, however, either rely heavily on labeled data or multi-view frames for each object. In this work, we assume that we only have access to single-view RGB data.
To perform 3D-aware I2I translation, we extend the the-ory developed for 2D-I2I with recent developments in 3D-aware image synthesis. We decouple the learning process into a multi-class 3D-aware generative model step and a 3D-aware I2I translation step. The former can synthesize view-consistent 3D scenes given a scene label, thereby ad-dressing the 3D inconsistency problems we discussed for 2D-I2I. We will use this 3D-aware generative model to ini-tialize our 3D-aware I2I model. It therefore inherits the ca-pacity of synthesizing 3D consistent images. To train ef-fectively a multi-class 3D-aware generative model (see Fig-ure 2(b)), we provide a new training strategy consisting of: (1) training an unconditional 3D-aware generative model (i.e., StyleNeRF) and (2) partially initializing the multi-class 3D-aware generative model (i.e., multi-class StyleN-eRF) with the weights learned from StyleNeRF. In the 3D-aware I2I translation step, we design a 3D-aware I2I trans-lation architecture (Figure 2(f)) adapted from the trained multi-class StyleNeRF network. To be specific, we use the main network of the pretrained discriminator (Figure 2(b)) to initialize the encoder E of the 3D-aware I2I translation model (Figure 2(f)), and correspondingly, the pretrained generator (Figure 2(b)) to initialize the 3D-aware I2I gen-erator (Figure 2(f)). This initialization inherits the capacity of being sensitive to the view information.
Directly using the constructed 3D-aware I2I translation model (Figure 2(f)), there still exists some view-consistency problem. This is because of the lack of multi-view consis-tency regularization, and the usage of the single-view im-age. Therefore, to address these problems we introduce several techniques, including a U-net-like adaptor network design, a hierarchical representation constrain and a relative regularization loss.
In sum, our work makes the following contributions:
• We are the first to explore 3D-aware multi-class I2I trans-lation, which allows generating 3D consistent videos.
• We decouple 3D-aware I2I translation into two steps.
First, we propose a multi-class StyleNeRF. To train this multi-class StyleNeRF effectively, we provide a new training strategy. The second step is the proposal of a 3D-aware I2I translation architecture.
• To further address the view-inconsistency problem of 3D-aware I2I translation, we propose several techniques: a U-net-like adaptor, a hierarchical representation constraint and a relative regularization loss.
• On extensive experiments, we considerably outperform existing 2D-I2I systems with our 3D-aware I2I method when evaluating temporal consistency. 2.