Abstract
Large vision Transformers (ViTs) driven by self-supervised pre-training mechanisms achieved unprece-dented progress. Lightweight ViT models limited by the little from those pre-model capacity, however, benefit training mechanisms. Knowledge distillation defines a paradigm to transfer representations from large (teacher) models to small (student) ones. However, the conventional single-stage distillation easily gets stuck on task-specific transfer, failing to retain the task-agnostic knowledge cru-cial for model generalization.
In this study, we propose generic-to-specific distillation (G2SD), to tap the potential of small ViT models under the supervision of large mod-els pre-trained by masked autoencoders. In generic distil-lation, decoder of the small model is encouraged to align feature predictions with hidden representations of the large model, so that task-agnostic knowledge can be transferred.
In specific distillation, predictions of the small model are constrained to be consistent with those of the large model, to transfer task-specific features which guarantee task per-formance. With G2SD, the vanilla ViT-Small model respec-tively achieves 98.7%, 98.1% and 99.3% the performance of its teacher (ViT-Base) for image classification, object de-tection, and semantic segmentation, setting a solid baseline for two-stage vision distillation. Code will be available at https://github.com/pengzhiliang/G2SD. 1.

Introduction
Vision transformers (ViTs) [11, 55] have been promis-ing representation models, particularly when trained upon large-scale datasets using self-supervised learning meth-ods [5]. The masked image modeling (MIM) methods [4, 13], which train representation models by reconstructing pixels [13, 52, 57], tokens [4, 8, 35] or features [2, 47], pro-moted the performance of large ViT models to a new height.
However, when acclaiming the promising performance of large ViT models, we notice that small ViT models,
* Equal contribution. § Contribution during internship at Microsoft
Research. † Corresponding authors.
Figure 1. Comparison of single-stage distillation models (from scratch [41] and pre-trained by the self-supervised method
MAE [13]) with the two-stage distillation counterparts (G2SD) using the same teacher model. G.D and S.D respectively denote generic and specific distillation. is the symbol of distillation. e.g., ViT-Tiny and ViT-Small, unfortunately, benefit little from either the big training data or self-supervised learn-ing methods. For example, the ViT-Large model trained by
MAE [13] outperforms the CNN model [29] by 1.6 points on ImageNet-1k, while the ViT-Small model is inferior to its
CNN counterpart [29]. In most scenarios with limited com-putational resources, e.g., front-end recognition systems,
CNNs [15, 19] remain the preferred models.
Do vanilla small ViT models really have no future? We attempt to answer this question from the perspective of knowledge distillation in this study. To fulfill this pur-pose, the first step is to revisit the conventional knowl-edge distillation methods [18, 38, 41] in the age of super-vised learning.
It is observed that task-oriented distilla-tion [41] reports unsatisfactory performance, Fig. 1. One reason could be that this kind of task-oriented distillation only focus on task-specific knowledge while missing some kind of task-agnostic knowledge which is beneficial to gen-eralization ability improvement and can be effectively en-In natural lan-dowed by self-supervised teacher model. guage processing, two-stage distillation method, e.g., Tiny-BERT [22], was exploited to overcome the limitation and transfer generic knowledge embedded from teacher to stu-dent models. Nevertheless, whether or not this paradigm applicable to vision tasks remains unexplored.
In this study, we aim to establish a general-to-specific
distillation baseline for vision tasks based on sophisticated self-supervised learning (e.g., MAE [13]), to guarantee that lightweight ViTs can simultaneously soak up task-agnostic and task-specific representations from teacher models for greater generalization and higher task performance, Fig. 1.
Specifically, at the generic distillation stage, a student model is encouraged to obtain the task-agnostic knowl-edge from the teacher models. The encoder and decoder of pre-trained MAE constitute the teacher model while a light-weight decoder is attached to the lightweight vision
Transformer as the student model, Fig. 2. The input im-age is randomly partitioned to visible and masked patches.
The visible patches are fed to encoders. The hidden fea-ture outputs of teacher decoder’s intermediate layer is used to guide training of the student model. For task-specific distillation, the fine-tuned MAE model equipped with task layers [14, 41, 51] teaches student model the task-specific knowledge (e.g., classification score). The student back-bone is initialized from the previous distillation stage while the task layers are randomly initialized. Predictions of the student are constrained to be consistent with those of the teacher as well as ground truth labels. Such a task-specific distillation phase guarantees the performance of downstream tasks, e.g., image classification, object detec-tion and semantic segmentation.
With G2SD, the vanilla ViT-Small model with 26% parameters and 2.6× throughput of the ViT-Base teacher, obtains 1) 98.6% (82.5% vs. 83.6%) top-1 accuracy of its teacher on ImageNet-1k [39] for image classification task, 2) 98.1% (50.6 vs. 51.6) mAP of its teacher on MS
COCO [26] for object detection and 3) 99.3% (48.0 vs. 48.3) mIoU of its teacher on ADE20k [58] for semantic segmentation. Furthermore, G2SD demonstrates better gen-eralization ability than its single-stage distillation counter-parts in terms of occlusion invariance and robustness.
The contributions are summarized as follows:
• We propose general-to-specific distillation (G2SD) to transfer task-agnostic and task-specific knowledge from masked autoencoders to lightweight ViTs, setting a solid baseline for two-stage vision model distillation.
• We design a simple-yet-effective generic distillation strategy by aligning the student’s predictions with hid-den features of the pre-trained masked autoencoder at visible and masked patches.
• Experiments show that the lightweight student model with G2SD achieves competitive results across vision tasks, improving the performance of lightweight ViT models to a new height. 2.