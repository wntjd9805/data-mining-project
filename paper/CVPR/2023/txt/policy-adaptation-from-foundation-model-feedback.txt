Abstract
Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across differ-ent objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment.
In this work, we propose Policy Adaptation from Founda-tion model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feed-back to relabel the demonstrations. This automatically pro-vides new pairs of demonstration-instruction data for pol-icy fine-tuning. We evaluate our method on a broad range
*Work done during internship at UCSD.
†Work done outside of Amazon. of experiments with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer. We show PAFF improves baselines by a large mar-gin in all cases. 1.

Introduction
Learning generalizable manipulation policies have been a long standing problem in robotics. The goal is to train a general-purpose robot which can tackle multiple tasks with different object compositions in diverse environments.
However, most current policy learning approaches with im-itation learning or reinforcement learning can only learn to solve one task at a time, and usually operate on a fixed set of objects. To achieve human-level generalization, many efforts have been made on performing robotic manipulation tasks specified by natural language [34, 35, 56]. Language can not only bring its compositionality to low-level robot skills, but also operate as a high-level planner for long-horizon tasks.
Recently, there is a trend on leveraging the pre-trained vision-language foundation models [38,47] as the backbone encoders for generalizing robot skills. For example, CLI-PORT [56] uses the pre-trained CLIP model [47] as the image observation and language instruction encoder, and learn a manipulation policy on top for tasks across different object arrangements. While encouraging results have been shown in this line of research on generalization across train-ing tasks, it is still very challenging for the learned policy to generalize to unseen tasks and environments. For example, as shown in Figure 1 (a), our experiments show that if we train such a policy on two tasks including (i) pack objects of different shapes in the brown box and (ii) put blocks of different colors in the bowls of different colors, it is very challenging for the policy to generalize to a task that put objects of different shapes in different colored bowls. Fur-thermore, the difficulty drastically increases when we need to perform this task in the real world with a different robot.
Our key insight is that, while the action generator (i.e., the policy) cannot generalize well, the action classifier with foundation models can still achieve a high accuracy even when “zero-shot” transferred to unseen environments [35, 56].
In this paper, we leverage vision-language founda-tion models to provide feedback during deploying the pol-icy in unseen tasks and environments. We utilize the feed-back from foundation models to fine-tune the policy fol-lowing test-time training [28, 57, 61], which updates model parameters during test-time. Specifically, we propose Pol-icy Adaptation from Foundation model Feedback (PAFF) with a play and relabel pipeline. When adapting a trained policy to a new task or new environment, we first let the policy play, that is, the model continuously generates and performs actions given a series of language instructions in the new task and we record the demonstrations including the visual observations and model’s actions. Of course, the instructions and the outcome demonstrations will often not match under the out-of-distribution environment. We then let the model relabel to make the correction, that is, the recorded demonstrations can be automatically relabeled by the vision-language pre-trained model. By taking the vi-sual observations of recorded demonstrations as inputs, the pre-trained model can retrieve accurate language instruc-tions correspondingly. Given the accurate paired demon-strations and instructions in the new environment, we can fine-tune and adapt the policy with them. We emphasize that the whole process of PAFF performs in an automatic way using trained models without human interventions.
We carefully design a broad range of language condi-tioned robotic adaptation experiments to evaluate the pol-icy adaptation across object composition, tasks and envi-ronments including from simulation to the real world. Our evaluations consist of (i) Compositional Generalization in
Fig. 1 (a), where we train a policy to pack objects of dif-ferent shapes in the brown box, and put blocks of differ-ent colors in the bowls of different colors, and adapt it to put objects of different shapes in the bowls of different col-ors. (ii) Out-of-distribution Generalization in Fig. 1 (c), where we train a policy to pack certain objects in the brown box, and adapt it to unseen objects; and in Fig. 1 (d), where we adapt a policy trained on seen environments to an un-seen environment with different textures and placements of static elements such as the sliding door, the drawer and the switch. (iii) Sim-to-real Transfer in Fig. 1 (b), where we adapt a policy trained on simulation data to the real-world.
We show PAFF improves baselines by a large-margin in all evaluations. In sim-to-real transfer, our method signifi-cantly improves the success rate by an average of 49.6% on four tasks than the baseline. Our pipeline fills the domain gap between simulation and real world through utilizing the generalization capability of the foundation model. Our method also increases the success rate from 17.8% to 35.0% in the compositional generalization evaluation, and from 48.4% to 63.8% for packing unseen objects. When adapting the policy to an unseen environment, our method increases the success rate of completing 5 chains of language instruc-tions from 5% to 11% over the baseline method. The exten-sive evaluation results show that PAFF can effectively adapt a language conditioned policy to unseen objects, tasks, en-vironments, and realize sim-to-real transfer. 2.