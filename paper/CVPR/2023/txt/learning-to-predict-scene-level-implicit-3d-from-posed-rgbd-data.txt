Abstract
We introduce a method that can learn to predict scene-level implicit functions for 3D reconstruction from posed
RGBD data. At test time, our system maps a previously unseen RGB image to a 3D reconstruction of a scene via implicit functions. While implicit functions for 3D recon-struction have often been tied to meshes, we show that we can train one using only a set of posed RGBD images. This setting may help 3D reconstruction unlock the sea of ac-celerometer+RGBD data that is coming with new phones.
Our system, D2-DRDF, can match and sometimes outper-form current methods that use mesh supervision and shows better robustness to sparse data. 1.

Introduction
Consider the image in Figure 1. From this ordinary RGB image, we can understand the complete 3D geometry of the scene, including the floor and walls behind the chairs. Our goal is to enable computers to recover this geometry from a single RGB image. To this end, we present a method that does so while learning only on posed RGBD images.
The task of reconstructing the full 3D of a scene from a single previously unseen RGB image has long been known to be challenging. Early work on full 3D relied on vox-els [6, 17] or meshes [19], but these representations fail on scenes due to topology and memory challenges.
Implicit functions (or learning to map each point in R3 to a value like the distance to the nearest surface) have shown substantial promise at overcoming these challenges. When conditioned on an image, these have led to several successful methods.
Unfortunately, the implicit function status quo mainly ties implicit function reconstruction methods to mesh su-pervision. This symbiosis has emerged since meshes give excellent direct supervision. However, methods are limited to training with an image-aligned mesh that is usually wa-tertight (and often artist-created) [35, 38, 44] and occasion-ally non-watertight but professionally-scanned [27, 61].
We present a method, Depth to DRDF (D2-DRDF), that breaks the implicit-function/mesh connection and can train an effective single RGB image implicit 3D reconstruction
Figure 1. We propose a method, D2-DRDF, that reconstructs full 3D from a single RGB image. During inference (left), our method uses implicit functions to reconstruct the complete 3D in-cluding visible and occluded surfaces (visualized as surface nor-mals
) such as the occluded wall and empty floor. For training, our method uses only RGBD images and poses, unlike most prior works that need an explicit and often watertight 3D mesh. system using a set of RGBD images with known pose. We envision that being able to entirely skip meshing will enable the use of vast quantities of lower-quality data from con-sumers (e.g., from increasingly common consumer phones with LiDAR scanners and accelerometers) as well as robots.
In addition to permitting training, the bypassing of meshing may enable adaptation in a new environment on raw data without needing an expert to ensure mesh acquisition.
Our key insight is that we can use segments of observed free space in depth maps in other views to constrain dis-tance functions. We show this using the Directed Ray Dis-tance Function (DRDF) [27] that has recently shown good performance in 3D reconstruction using implicit functions and has the benefit of not needing watertight meshes for training. Given an input reference view, the DRDF breaks the problem of predicting the 3D surface into a set of in-dependent 1D distance functions, each along a ray through a pixel in the reference view and accounting for only sur-faces on the ray. Rather than use an ordinary unsigned dis-tance function, the DRDF signs the distance using the lo-cation of the cameraâ€™s pinhole and the intersections along the ray. While [27] showed their method could be trained on non-watertight meshes, their method is still dependent
on meshes. In our paper, we show that the DRDF can be cleanly supervised using auxiliary views of RGBD obser-vations and their poses. We derive constraints on the DRDF that power loss functions for training our system. While the losses on any one image are insufficient to produce a recon-struction, they provide a powerful signal when accumulated across thousands of training views.
We evaluate our method on realistic indoor scene datasets and compare it with methods that train with full mesh supervision. Our method is competitive and some-times even better compared to the best-performing mesh-supervised method [28] with full, professional captures. As we degrade scan completeness, our method largely main-tains its performance while mesh-based methods perform substantially worse. We conclude by showing that fine-tuning of our method on a handful of images enables a sim-ple, effective method for fusing and completing scenes from a handful of RGBD images. 2.