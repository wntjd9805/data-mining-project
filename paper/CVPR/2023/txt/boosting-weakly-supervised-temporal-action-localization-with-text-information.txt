Abstract
Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete local-In this paper, we aim to leverage the text infor-ization. i.e., (a) the dis-mation to boost WTAL from two aspects, criminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objec-tive to enhance the intra-class integrity, thus ﬁnding more complete temporal boundaries. For the discriminative ob-jective, we propose a Text-Segment Mining (TSM) mecha-nism, which constructs a text description based on the ac-tion class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Comple-tion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and Activi-tyNet1.3. Surprisingly, we also ﬁnd our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is avail-able at https://github.com/lgzlIlIlI/Boosting-WTAL. 1.

Introduction
Temporal action localization attempts to temporally lo-calize the action instances of interest in untrimmed videos.
Although current fully-supervised temporal action local-ization methods [5, 26, 42, 51] have achieved remarkable
*Corresponding author
GT
Prediction
Incomplete localization Over-complete localization
Video 
Encoder (a)
Classifier (b)
T
T-CAM
Action 
Localization
Class Text 
Query diving high jump
… pole vault
T
T
Attention
Language completion (c)
A video of  
[MASK]
T pull close push away consistency
T
T
A video of 
Diving
Figure 1. Comparison of our proposed framework with current
WTAL methods. (a) Common failures in existing WTAL methods. (b) Existing WTAL model’s pipeline. (c) The proposed frame-work with text-segment mining and video-text language comple-tion, where the depth of color represents the degree of correlation between segments and texts. progress, time-consuming and labor-intensive frame-level annotations are required. To alleviate the annotation cost, weakly-supervised temporal action localization (WTAL) methods [15,22,32,35] have gained more attention recently, which only requires efﬁcient video-level annotations.
With only video-level supervision, existing WTAL meth-ods [15, 22, 35, 45] generally utilize video information to train a classiﬁer, which is used to generate a sequence of class logits or predictions named temporal class acti-vation map (T-CAM). While signiﬁcant improvement has been achieved, current methods still suffer from two prob-lems, i.e., incomplete and over-complete localization. As shown in Fig. 1 (a), some sub-action with low discriminabil-ity may be ignored, while some background segments that contribute to classiﬁcation can be misclassiﬁed as action, causing incomplete and over-complete localization.
Differently from current methods that only utilize the video information, in this paper, we aim to explore the text information to improve WTAL from two aspects: (a) the discriminative objective to enlarge the inter-class differ-ence, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus ﬁnding more complete temporal boundaries. For the discrimina-tive objective, we propose a Text-Segment Mining (TSM) mechanism, where the action label texts can be used as queries to mine all related segments in videos. Speciﬁcally, we ﬁrst use the prompt templates to incorporate the class label information into the text query. Without temporal an-notations, TSM requires to compare the text query with all segments of the different videos across the dataset, as shown in Fig. 1 (c). During the comparison, the segments that is best matching to the text query would be mined, while other irrelevant segments would be ignored, which is similar to
‘matched ﬁlter’ [43, 50]. In this way, the segments and text queries with the same class from all videos would be pulled close while pushing away others, hence enhancing the inter-class difference.
For different categories of videos, there are some shared sub-actions, e.g., sub-action “Approach” exists in both
“High Jump” and “Long Jump” videos. Merely using
TSM is too strict to neglect the semantic-related segments, which results in incomplete localization, e.g., neglecting
“Approach” segments. To overcome this problem, we fur-ther introduce a generative objective named Video-text Lan-guage Completion (VLC) which focuses on all semantic-related segments to complete the text sentence. First, we construct a description sentence for the action label of the video and mask the key action words in the sentence, as shown in Fig. 2. Then an attention mechanism is design to collect semantic related segments as completely as possi-ble to predict masked action text via the language recon-structor, which enhances the intra-class integrity. Com-bining TSM and VLC by a self-supervised constraint, our method achieves the new state-of-the-art on two popular benchmarks, i.e., THUMOS14 [17] and ActivityNet1.3 [1].
Furthermore, we also ﬁnd our proposed method can be ap-plied into existing methods, and improve their performances with a clear margin.
Our contributions are summarized as three-folds: (a) To best of our knowledge, we are the ﬁrst to leverage text infor-mation to boost WTAL. We also prove that our method can be easy to extend to existing state-of-the-art approaches and improve their performance. (b) To leverage the text infor-mation, we devise two objective: the discriminative objec-tive to enlarge the inter-class difference, thus reducing the over-complete; and the generative objective to enhance the intra-class integrity, thus ﬁnding more complete temporal boundaries. (c) Extensive experiments illustrate our method outperforms current methods on two public datasets, and comprehensive ablation studies reveal the effectiveness of the proposed objectives. 2.