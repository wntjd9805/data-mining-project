Abstract
We present an effective and efﬁcient method that explores the properties of Transformers in the frequency domain for high-quality image deblurring. Our method is motivated by the convolution theorem that the correlation or convo-lution of two signals in the spatial domain is equivalent to an element-wise product of them in the frequency domain.
This inspires us to develop an efﬁcient frequency domain-based self-attention solver (FSAS) to estimate the scaled dot-product attention by an element-wise product operation instead of the matrix multiplication in the spatial domain. In addition, we note that simply using the naive feed-forward network (FFN) in Transformers does not generate good de-blurred results. To overcome this problem, we propose a simple yet effective discriminative frequency domain-based
FFN (DFFN), where we introduce a gated mechanism in the FFN based on the Joint Photographic Experts Group (JPEG) compression algorithm to discriminatively determine which low- and high-frequency information of the features should be preserved for latent clear image restoration. We formulate the proposed FSAS and DFFN into an asymmetri-cal network based on an encoder and decoder architecture, where the FSAS is only used in the decoder module for better image deblurring. Experimental results show that the pro-posed method performs favorably against the state-of-the-art approaches. 1.

Introduction
Image deblurring aims to restore high-quality images from blurred ones. This problem has achieved signiﬁcant progress due to the development of various effective deep models with large-scale training datasets.
Most state-of-the-art methods for image deblurring are mainly based on deep convolutional neural networks (CNNs).
The main success of these methods is due to developing kind-s of network architectural designs, for example, the multi-∗Corresponding author: Jiangxin Dong and Jinshan Pan.
Figure 1. Comparisons of the proposed method and state-of-the-art ones on the GoPro dataset [16] in terms of accuracy, ﬂoating point operations (FLOPs), and network parameters. The circle size indicates the number of the network parameter. scale [4, 16, 22] or multi-stage [31, 32] network architectures, generative adversarial learning [11, 12], physics model in-spired network structures [6–8, 17, 33], and so on. As the basic operation in these networks, the convolution opera-tion is a spatially-invariant local operation, which does not model the spatially variant properties of the image contents.
Most of them use larger and deeper models to remedy the limitation of the convolution. However, simply increasing the capacity of deep models does not always lead to better performance as shown in [17, 33].
Different from the convolution operation that models the local connectivity, Transformers are able to model the global contexts by computing the correlations of one token to all other tokens. They have been shown to be an effective ap-proach in lots of high-level vision tasks and also have great potential to be the alternatives of deep CNN models. In im-age deblurring, the methods based on Transformers [27, 30] also achieve better performance than the CNN-based meth-ods. However, the computation of the scaled dot-product attention in Transformers leads to quadratic space and time complexity in terms of the number of tokens. Although using smaller and fewer tokens can reduce the space and
time complexity, such strategy cannot model the long-range information of features well and usually leads to signiﬁcant artifacts when handling high-resolution images, which thus limits the performance improvement.
To alleviate this problem, most approaches use the down-sampling strategy to reduce the spatial resolution of fea-tures [26]. However, reducing the spatial resolution of fea-tures will cause information loss and thus affect the image deblurring. Several methods reduce the computational cost by computing the scaled dot-product attention in terms of the number of features [29, 30]. Although the computational cost is reduced, the spatial information is well not explored, which may affect the deblurring performance.
In this paper, we develop an effective and efﬁcient method that explores the properties of Transformers for high-quality image deblurring. We note that the scaled dot-product at-tention computation is actually to estimate the correlation of one token from the query and all the tokens from the key.
This process can be achieved by a convolution operation when rearranging the permutations of tokens. Based on this observation and the convolution theorem that the convolu-tion in the spatial domain equals a point-wise multiplication in the frequency domain, we develop an efﬁcient frequen-cy domain-based self-attention solver (FSAS) to estimate the scaled dot-product attention by an element-wise product operation instead of the matrix multiplication. Therefore, the space and time complexity can be reduced to O(N )
O(N log N ) for each feature channel, where N is the num-ber of the pixels.
In addition, we note that simply using the feed-forward network (FFN) by [30] does not generate good deblurred results. To generate better features for latent clear image restoration, we develop a simple yet effective discrimina-tive frequency domain-based FFN (DFFN). Our DFFN is motivated by the Joint Photographic Experts Group (JPEG) compression algorithm. It introduces a gated mechanism in the FFN to discriminatively determine which low- and high-frequency information should be preserved for latent clear image restoration.
We formulate the proposed FSAS and DFFN into an end-to-end trainable network based on an encoder and decoder architecture to solve image deblurring. However, we ﬁnd that as features of shallow layers usually contain blur effects, applying the scaled dot-product attention to shallow features does not effectively explore global clear contents. As the features from deep layers are usually clearer than those from shallow layers, we develop an asymmetric network architec-ture, where the FSAS is only used in the decoder module for better image deblurring. We analyze that the exploring properties of Transformers in the frequency domain is able to facilitate blur removal. Experimental results demonstrate that the proposed method generates favorable results against state-of-the-art methods in terms of accuracy and efﬁciency (Figure 1).
The main contributions are summarized as follows:
• We develop an efﬁcient frequency domain-based self-attention solver to estimate the scaled dot-product at-tention. Our analysis demonstrates that using the fre-quency domain-based solver reduces the space and time complexity and is much more effective and efﬁcient.
• We propose a simple yet effective discriminative fre-quency domain-based FFN based on the JPEG compres-sion algorithm to discriminatively determine which low and high-frequency information should be preserved for latent clear image restoration.
• We develop an asymmetric network architecture based on an encoder and decoder network, where the frequen-cy domain-based self-attention solver is only used in the decoder module for better image deblurring.
• We analyze that the exploring properties of Transform-ers in the frequency domain is able to facilitate blur removal and show that our approach performs favor-ably against state-of-the-art methods. 2.