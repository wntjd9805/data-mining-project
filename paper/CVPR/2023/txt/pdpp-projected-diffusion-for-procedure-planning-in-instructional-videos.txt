Abstract
In this paper, we study the problem of procedure plan-ning in instructional videos, which aims to make goal-directed plans given the current visual observations in un-structured real-life videos. Previous works cast this prob-lem as a sequence planning problem and leverage either heavy intermediate visual observations or natural language instructions as supervision, resulting in complex learning
In contrast, we schemes and expensive annotation costs. treat this problem as a distribution fitting problem. In this sense, we model the whole intermediate action sequence distribution with a diffusion model (PDPP), and thus trans-form the planning problem to a sampling process from this distribution.
In addition, we remove the expensive inter-mediate supervision, and simply use task labels from in-structional videos as supervision instead. Our model is a
U-Net based diffusion model, which directly samples ac-tion sequences from the learned distribution with the given start and end observations. Furthermore, we apply an ef-ficient projection method to provide accurate conditional guides for our model during the learning and sampling pro-cess. Experiments on three datasets with different scales show that our PDPP model can achieve the state-of-the-art performance on multiple metrics, even without the task supervision. Code and trained models are available at https://github.com/MCG-NJU/PDPP. 1.

Introduction
Instructional videos [1,31,38] are strong knowledge car-riers, which contain rich scene changes and various actions.
People watching these videos can learn new skills by fig-uring out what actions should be performed to achieve the desired goals. Although this seems to be natural for hu-mans, it is quite challenging for AI agents. Training a model that can learn how to make action plans to transform from the start state to goal is crucial for the next-generation AI system as such a model can analyze complex human be-(cid:66): Corresponding author (lmwang@nju.edu.cn).
Figure 1. Procedure planning example. Given a start observation ostart and a goal state ogoal, the model is required to generate a sequence of actions that can transform ostart to ogoal. Previous approaches rely on heavy intermediate supervision during training, while our model only needs the task class labels (bottom row). haviours and help people with goal-directed problems like cooking or repairing items. Nowadays the computer vision community is paying growing attention to the instructional video understanding [4, 8, 9, 24, 37]. Among them, Chang et al. [4] proposed a problem named as procedure planning in instructional videos, which requires a model to produce goal-directed action plans given the current visual observa-tion of the world. Different with traditional procedure plan-ning problem in structured environments [12, 29], this task deals with unstructured environments and thus forces the model to learn structured and plannable representations in real-life videos. We follow this work and tackle the proce-dure planning problem in instructional videos. Specifically, given the visual observations at start and end time, we need to produce a sequence of actions which transform the envi-ronment from start state to the goal state, as shown in Fig. 1.
Previous approaches for procedure planning in instruc-tional videos often treat it as a sequence planning prob-lem and focus on predicting each action accurately. Most works rely on a two-branch autoregressive method to pre-dict the intermediate states and actions step by step [2,4,30].
Such models are complex and easy to accumulate errors during the planning process, especially for long sequences.
Recently, Zhao et al. [36] proposed a single branch non-autoregressive model based on transformer [33] to predict all intermediate steps in parallel. To obtain a good per-formance, they used a learnable memory bank in the trans-former decoder, augmented their model with an extra gen-erative adversarial framework [13] and applied a Viterbi post-processing method [34]. This method brought multiple learning objectives, complex training schemes and tedious inference process. Instead, we assume procedure planning as a distribution fitting problem and planning is solved with a sampling process. We aim to directly model the joint dis-tribution of the whole action sequence in instructional video rather than every discrete action. In this perspective, we can use a simple MSE loss to optimize our generative model and generate action sequence plans in one shot with a sam-pling process, which results in less learning objectives and simpler training schemes.
For supervision in training, in addition to the action se-quence, previous methods often require heavy intermediate visual [2, 4, 30] or language [36] annotations for their learn-ing process. In contrast, we only use task labels from in-structional videos as a condition for our learning (as shown in Fig. 1), which could be easily obtained from the key-words or captions of videos and requires much less labeling cost. Another reason is that task information is closely re-lated to the action sequences in a video. For example, in a video of jacking up a car, the possibility for action add sugar appears in this process is nearly zero.
Modeling the uncertainty in procedure planning is also an important factor that we need to consider. That is, there might be more than one reasonable plan sequences to trans-form from the given start state to goal state. For example, change the order of add sugar and add butter in making cake process will not affect the final result. So action se-quences can vary even with the same start and goal states.
To address this problem, we consider adding randomness to our distribution-fitting process and perform training with a diffusion model [18, 26]. Solving procedure planning prob-lem with a diffusion model has two main benefits. First, a diffusion model changes the goal distribution to a random
Gaussian noise by adding noise slowly to the initial data and learns the sampling process at inference time as an iter-ative denoising procedure starting from a random Gaussian noise. So randomness is involved both for training and sam-pling in a diffusion model, which is helpful to model the un-certain action sequences for procedure planning. Second, it is convenient to apply conditional diffusion process with the given start and goal observations based on diffusion models, so we can model the procedure planning problem as a con-ditional sampling process with a simple training scheme. In this work, we concatenate conditions and action sequences together and propose a projected diffusion model to perform conditional diffusion process.
Contributions. To sum up, the main contributions of this work are as follows: a) We cast the procedure plan-ning as a conditional distribution-fitting problem and model the joint distribution of the whole intermediate action se-quence as our learning objective, which can be learned with a simple training scheme. b) We introduce an efficient ap-proach for training the procedure planner, which removes the supervision of visual or language features and relies on task supervision instead. c) We propose a novel projected diffusion model (PDPP) to learn the distribution of action sequences and produce all intermediate steps at one shot.
We evaluate our PDPP on three instructional videos datasets and achieve the state-of-the-art performance across differ-ent prediction time horizons. Note that our model can still achieve excellent results even if we remove the task super-vision and use the action labels only. 2.