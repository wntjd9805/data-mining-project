Abstract
Contrastive learning-based video-language representa-tion learning approaches, e.g., CLIP, have achieved out-standing performance, which pursue semantic interaction upon pre-defined video-text pairs. To clarify this coarse-grained global interaction and move a step further, we have to encounter challenging shell-breaking interactions for fine-grained cross-modal learning. In this paper, we creatively model video-text as game players with multivariate coopera-tive game theory to wisely handle the uncertainty during fine-grained semantic interaction with diverse granularity, flex-ible combination, and vague intensity. Concretely, we pro-pose Hierarchical Banzhaf Interaction (HBI) to value pos-sible correspondence between video frames and text words for sensitive and explainable cross-modal contrast. To effi-ciently realize the cooperative game of multiple video frames and multiple text words, the proposed method clusters the original video frames (text words) and computes the Banzhaf
Interaction between the merged tokens. By stacking token merge modules, we achieve cooperative games at different semantic levels. Extensive experiments on commonly used text-video retrieval and video-question answering bench-marks with superior performances justify the efficacy of our
HBI. More encouragingly, it can also serve as a visualization tool to promote the understanding of cross-modal interac-tion, which have a far-reaching impact on the community.
Project page is available at https://jpthu17.github.io/HBI/. 1.

Introduction
Representation learning based on both vision and lan-guage has many potential benefits and direct applicability to
*Corresponding author: Li Yuan, Jie Chen.
Figure 1. (a) Cross-modal contrastive methods only learn a global semantic interaction from the coarse-grained labels of video-text pairs. (b) We model cross-modal alignment as a multivariate co-operative game process. Specifically, we use Banzhaf Interaction to value possible correspondence between video frames and text words and consider it as an additional learning signal. cross-modal tasks, such as text-video retrieval [20, 32] and video-question answering [28, 49]. Visual-language learning has recently boomed due to the success of contrastive learn-ing [9–11,19,48,61–64], e.g., CLIP [40], to project the video and text features into a common latent space according to the semantic similarities of video-text pairs. In this manner, cross-modal contrastive learning enables networks to learn discriminative video-language representations.
The cross-modal contrastive approach [14, 20, 32] typi-cally models the cross-modal interaction via solely the global similarity of each modality. Specifically, as shown in Fig. 1a,
it only exploits the coarse-grained labels of video-text pairs to learn a global semantic interaction. However, in most cases, we expect to capture fine-grained interpretable infor-mation, such as how much cross-modal alignment is helped or hindered by the interaction of a visual entity and a textual phrase. Representation that relies on cross-modal contrastive learning cannot do this in a supervised manner, as manu-ally labeling these interpretable relationships is unavailable, especially on large-scale datasets. This suggests that there might be other learning signals that could complement and improve pure contrastive formulations.
In contrast to prior works [20, 32, 45], we model cross-modal representation learning as a multivariate cooperative game by formulating video and text as players in a coop-erative game, as illustrated in Fig. 1b. Intuitively, if visual representations and textual representations have strong se-mantic correspondence, they tend to cooperate together and contribute to the cross-modal similarity score. Motivated by this spirit, we consider the set containing multiple represen-tations as a coalition, and propose to quantify the trend of cooperation within a coalition via the game-theoretic inter-action index, i.e., Banzhaf Interaction [18] for its simplicity and efficiency. Banzhaf Interaction is one of the most popu-lar concepts in cooperative games [33]. As shown in Fig. 2, it measures the additional benefits brought by the coalition compared with the costs of the lost coalitions of these players with others. When a coalition has high Banzhaf Interaction, it will also have a high contribution to the semantic similar-ity. Thus, we can use Banzhaf Interaction to value possible correspondence between video frames and text words for sensitive and explainable cross-modal contrast.
To this end, we propose Hierarchical Banzhaf Interac-tion (HBI). Concretely, we take video frames and text words as players and the cross-modality similarity measurement as the characteristic function in the cooperative game. Then, we use the Banzhaf Interaction to represent the trend of cooperation between any set of features. Besides, to effi-ciently generate coalitions among game players, we propose an adaptive token merge module to cluster the original video frames (text words). By stacking token merge modules, we achieve hierarchical interaction, i.e., entity-level inter-actions on the frames and words, action-level interactions on the clips and phrases, and event-level interactions on the segments and paragraphs. In particular, we show that the
Banzhaf Interaction index satisfies Symmetry, Dummy, Addi-tivity, and Recursivity axiom in Sec. 3.4. This result implies that the representation learned via Banzhaf Interaction has four properties that the features of the contrastive method do not. We find that explicitly establishing the fine-grained interpretable relationships between video and text brings a sensible improvement to already very strong video-language representation learning results. Experiment results on three text-video retrieval benchmark datasets (MSRVTT [50], Ac-Figure 2. The intuition of Banzhaf Interaction in video-text representation learning. We refer the reader to Eq. 3 for the detailed formula. When some players (frames and words) form a coalition, we lose the coalitions of these players with others. In other words, the lost coalition is mutually exclusive from the target coalition. Banzhaf Interaction measures the difference between the benefits of the coalition and the costs of the lost coalitions. tivityNet Captions [21], and DiDeMo [1]) and the video question answering benchmark dataset (MSRVTT-QA [49]) show the advantages of the proposed method. The main contributions are as follows:
• To the best of our knowledge, we are the first to model video-language learning as a multivariate cooperative game process and propose a novel proxy training objec-tive, which uses Banzhaf interaction to value possible correspondence between video frames and text words for sensitive and explainable cross-modal contrast.
• Our method achieves new state-of-the-art performance on text-video retrieval benchmarks of MSRVTT, Activi-tyNet Captions and DiDeMo, as well as on the video-question answering task on MSRVTT-QA.
• More encouragingly, our method can also serve as a visualization tool to promote the understanding of cross-modal interaction, which may have a far-reaching im-pact on the community. 2.