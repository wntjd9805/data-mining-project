Abstract
Current audio-visual separation methods share a stan-dard architecture design where an audio encoder-decoder network is fused with visual encoding features at the en-coder bottleneck. This design confounds the learning of multi-modal feature encoding with robust sound decod-ing for audio separation. To generalize to a new instru-ment, one must fine-tune the entire visual and audio net-work for all musical instruments. We re-formulate the visual-sound separation task and propose Instruments as
Queries (iQuery) with a flexible query expansion mech-anism. Our approach ensures cross-modal consistency and cross-instrument disentanglement. We utilize “visually named” queries to initiate the learning of audio queries and use cross-modal attention to remove potential sound source interference at the estimated waveforms. To gen-eralize to a new instrument or event class, drawing inspi-ration from the text-prompt design, we insert additional queries as audio prompts while freezing the attention mech-anism. Experimental results on three benchmarks demon-strate that our iQuery improves audio-visual sound source separation performance. Code is available at https:
//github.com/JiabenChen/iQuery. 1.

Introduction
Humans use multi-modal perception to understand com-plex activities. To mimic this skill, researchers have studied audio-visual learning [3, 17, 33] by exploiting the synchro-nization and correlation between auditory and visual infor-mation. In this paper, we focus on the sound source sepa-ration task, where we aim to identify and separate different sound components within a given sound mixture [60, 74].
Following the “Mix-and-Separate” framework [32, 34, 81], we learn to separate sounds by mixing multiple audio sig-nals to generate an artificially complex auditory represen-tation and then use it as a self-supervised task to separate individual sounds from the mixture. The works [26, 53, 89] showed that visually-guided sound separation is achievable by leveraging visual information of the sound source.
Prevalent architectures take a paradigm of a visual-conditioned encoder-decoder architecture [23, 26, 58, 88], where encoded features from audio and visual modalities are fused at the bottleneck for decoding to yield separated spectrogram masks. However, it is noticed that this design often creates a “muddy” sound and “cross-talk” that leaks from one instrument to another. To create a clean sound separation, one would like the audio-visual encoders to be (1) self-consistent within the music instrument and (2) con-trasting across. One approach [27] added critic functions explicitly to enforce these properties. Another method [99] used a two-step process with the second motion-conditioned generation process to filter out unwanted cross-talks. We call these approaches decoder-centric.
Most recent works focus on addressing the “muddy” and
“cross-talk” issue by improving fine details of audio-visual feature extraction: for example, adding human motion en-coding as in [23, 88, 99], or cross-modality representations
[58] via self-supervised learning. Once the feature repre-sentations are learned, the standard encoder-decoder FCN style segmentation is used as an afterthought. We consider these methods feature-centric. The standard designs have two limitations. First, it is hard to balance decoder-centric and feature-centric approaches that enforce a common goal of cross-modality consistency and cross-instrument con-trast. Second, to learn a new musical instrument, one has to retrain the entire network via self-supervision.
To tackle these limitations, we propose a query-based sound separation framework, iQuery. We recast this prob-lem from a query-based transformer segmentation view, where each query learns to segment one instrument, similar to visual segmentation [15, 16, 65, 78]. We treat each au-dio query as a learnable prototype that parametrically mod-els one sound class. We fuse visual modality with audio by “visually naming” the audio query: using object detec-tion to assign visual features to the corresponding audio query. Within the transformer decoder, the visually initial-ized queries interact with the audio features through cross-attention, thus ensuring cross-modality consistency. Self-Figure 1. Pipeline of iQuery. Our system takes as input an audio mixture and its corresponding video frames, and disentangles separated sound sources for each video. Our pipeline consists of two main modules: an Audio-Visual Feature Extraction module which extracts audio, object, and motion features through three corresponding encoders, and an Audio-Visual Transformer module for sound separation.
The query-based sound separation transformer has three key components: 1) “visually-named” audio queries are initialized by extracted object features, 2) cross-attention between the audio queries with static image features, dynamic motion features and audio features, 3) self-attention between the learned audio queries to ensure cross-instrument contrast. attention across the audio queries for different instruments implements a soft version of the cross-instrument contrast objective. With this design, we unify the feature-centric with the decoder-centric approach.
How do we achieve generalizability? Motivated by re-cent success in fine-tuning domain transfer with the text-prompt [28] and visual-prompt designs [7, 35, 41, 86], we adaptively insert the additional queries as audio prompts to accommodate new instruments. With the audio-prompt de-sign, we freeze most of the transformer network parame-ters and only fine-tune the newly added query embedding layer. We conjecture that the learned prototype queries are instrument-dependent, while the cross/self-attention mech-anism in the transformer is instrument-independent.
Our main contributions are:
• To the best of our knowledge, we are the first to study the audio-visual sound separation problem from a tun-able query view to disentangle different sound sources explicitly through learnable audio prototypes in a mask transformer architecture.
• To generalize to a new sound class, we design an audio prompt for fine-tuning with most of the transformer ar-chitecture frozen.
• Extensive experiments and ablations verify the ef-fectiveness of our core designs for disentangle-ment, demonstrating performance gain for audio-visual sound source separation on three benchmarks. 2.