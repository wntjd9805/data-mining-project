Abstract
Deep learning has achieved great success in recent years with the aid of advanced neural network structures and large-scale human-annotated datasets. However, it is of-ten costly and difficult to accurately and efficiently anno-tate large-scale datasets, especially for some specialized domains where fine-grained labels are required. In this set-ting, coarse labels are much easier to acquire as they do not require expert knowledge. In this work, we propose a con-trastive learning method, called masked contrastive learn-ing (MaskCon) to address the under-explored problem set-ting, where we learn with a coarse-labelled dataset in or-der to address a finer labelling problem. More specifically, within the contrastive learning framework, for each sam-ple our method generates soft-labels with the aid of coarse labels against other samples and another augmented view of the sample in question. By contrast to self-supervised contrastive learning where only the sample’s augmentations are considered hard positives, and in supervised contrastive learning where only samples with the same coarse labels are considered hard positives, we propose soft labels based on sample distances, that are masked by the coarse labels.
This allows us to utilize both inter-sample relations and coarse labels. We demonstrate that our method can obtain as special cases many existing state-of-the-art works and that it provides tighter bounds on the generalization error.
Experimentally, our method achieves significant improve-ment over the current state-of-the-art in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford
Online Products and Stanford Cars196 datasets. Code and annotations are available at https://github.com/
MrChenFeng/MaskCon_CVPR2023. 1.

Introduction
Supervised learning with deep neural networks has achieved great success in various computer vision tasks such as image classification, action detection and ob-ject the success of supervised learning relies on large-scale and high-quality human-localization. However,
Figure 1. Contrastive learning sample relations using MaskCon (ours) and other learning paradigms when only coarse labels are available. MaskCon are closer to the fine ones. annotated datasets, whose annotations are time-consuming and labour-intensive to produce. To avoid such reliance, various learning frameworks have been proposed and in-vestigated: Self-supervised learning aims to learn mean-ingful representations with heuristic proxy visual tasks, such as rotation prediction [16] and the more prevalent in-stance discrimination task, the latter, being widely applied in self-supervised contrastive learning framework; semi-supervised learning usually considers a dataset for which only a small part is annotated – within this setting, pseudo labelling methods [24] and consistency regularization tech-niques [1, 29] are typically used; Moreover, learning using more accessible but noisy data, such as web-crawled data, has also received increasing attention [13, 25].
In this work, we consider an under-explored problem setting aiming at reducing the annotation effort – learn-ing fine-grained representations with a coarsely-labelled dataset. Specifically, we learn with a dataset that is fully la-beled, albeit at a coarser granularity than we are interested in (i.e., that of the test set). Compared to fine-grained la-bels, coarse labels are often significantly easier to obtain, especially in some of the more specialized domains, such as the recognition and classification of medical pathology images. As a simple example, for the task of differentia-tion between different pets, we need a knowledgeable cat lover to distinguish between ‘British short’ and ‘Siamese’,
but even a child annotator may help to discriminate between
‘cat’ and ‘non-cat’ (Fig. 1). Unfortunately, learning with a coarse labelled dataset has been less investigated compared to other weakly supervised learning paradigms. Recently,
Bukchi et al. [2] investigate on learning with coarse labels in the few-shot setting. More closely related to us, Grafit [31] proposes a multi-task framework by a weighted combina-tion of self-supervised contrastive learning and supervised contrastive learning cost; Similarly, CoIns [37] uses both a self-supervised contrastive learning cost and a supervised learning cross-entropy loss. Both works combine a fully su-pervised learning cost (cross entropy or contrastive) with a self-supervised contrastive loss – these works are the main ones with which we compare.
Differently than them, instead of using self-supervised contrastive learning as an auxiliary task, we propose a novel learning scheme, namely Masked Contrastive Learn-ing (MaskCon). Our method aims to learn by consider-ing inter-sample relations of each sample with other sam-ples in the dataset. Specifically, we always consider the relation to oneself as confidently positive. To estimate the relations to other samples, we derive soft labels by contrasting an augmented view of the sample in question with other samples, and further improve it by utilizing the mask generated based on the coarse labels. Our approach generates soft inter-sample relations that can more accu-rately estimate fine inter-sample relations compared to the baseline methods (Fig. 1). Efficiently and effectively, our method achieves significant improvements over the state-of-the-art in various datasets, including CIFARtoy, CIFAR100,
ImageNet-1K and more challenging fine-grained datasets
Stanford Online Products and Stanford Cars196. 2.