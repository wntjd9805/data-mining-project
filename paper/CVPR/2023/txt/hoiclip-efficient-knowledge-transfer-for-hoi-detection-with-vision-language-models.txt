Abstract
Human-Object Interaction (HOI) detection aims to lo-calize human-object pairs and recognize their interactions.
Recently, Contrastive Language-Image Pre-training (CLIP) has shown great potential in providing interaction prior for
HOI detectors via knowledge distillation. However, such approaches often rely on large-scale training data and suf-fer from inferior performance under few/zero-shot scenar-ios. In this paper, we propose a novel HOI detection frame-work that efficiently extracts prior knowledge from CLIP and achieves better generalization. In detail, we first intro-duce a novel interaction decoder to extract informative re-gions in the visual feature map of CLIP via a cross-attention mechanism, which is then fused with the detection backbone by a knowledge integration block for more accurate human-object pair detection. In addition, prior knowledge in CLIP text encoder is leveraged to generate a classifier by embed-ding HOI descriptions. To distinguish fine-grained interac-tions, we build a verb classifier from training data via visual semantic arithmetic and a lightweight verb representation adapter. Furthermore, we propose a training-free enhance-ment to exploit global HOI predictions from CLIP. Exten-sive experiments demonstrate that our method outperforms the state of the art by a large margin on various settings, e.g.
+4.04 mAP on HICO-Det. The source code is available in https://github.com/Artanic30/HOICLIP. 1.

Introduction
Human-Object Interaction (HOI) detection, which aims to localize human-object pairs and identify their interac-tions, is a core task towards a comprehensive understanding of visual scenes. It has attracted increasing interest in recent years for its key role in a wide range of applications, such
*These authors contributed equally to this work, which was supported by Shanghai Science and Technology Program 21010502700 and Shanghai
Frontiers Science Center of Human-centered Artificial Intelligence. (a) Data Efficiency Comparison (b) Verb Long-tail Problem
Figure 1. Data efficiency comparison and verb distribution analysis. In panel (a), we increase training data from 5% to 100% and show the result of HOICLIP and GEN-VLKT. In panel (b), the dots indicate the mean mAP and length of vertical line indicate the variance of mAP for verbs grouped by sample number. as assistive robots, visual surveillance and video analysis
[3,4,9,11]. Thanks to the development of end-to-end object detectors [5], recent research [23,28,29,37,49] has made re-markable progress in localizing human-object instances in interaction. Nonetheless, the problem of identifying inter-action classes between human-object pairs remains partic-ularly challenging. Conventional strategies [7, 23, 37, 49] simply learn a multi-label classifier and typically require large-scale annotated data for training. As such, they of-ten suffer from long-tailed class distributions and a lack of generalization ability to unseen interactions.
Recently, Contrastive Vision-Language Pre-training [33] has been explored to address such open-vocabulary and zero-shot learning problems as its learned visual and lin-guistic representations demonstrate strong transfer ability in various downstream tasks.
In particular, recent work on open-vocabulary detection utilizes knowledge distilla-tion to transfer CLIP’s object representation to object detec-tors [10,12,14,31,45,52]. Such a strategy has been adopted in the work of HOI detection, including GEN-VLKT [28] and EoID [43], which leverage CLIP’s knowledge to tackle the long-tail and zero-shot learning in the HOI tasks.
Despite their promising results, it remains an open ques-tion how we effectively transfer CLIP knowledge to the HOI recognition task it involves compositional concepts com-posed of visual objects and interactions. First, as pointed out in [8, 35], the commonly-adopted teacher-student distil-lation objective is not aligned with improving the general-ization of student models. In addition, as shown in Figure 1, we empirically observe that the knowledge distillation in learning HOIs (e.g., GEN-VLKT) typically requires a sub-stantial amount of training data, which indicates its low data efficiency. Furthermore, knowledge distillation often suffers from performance degradation in zero-shot generalization as it lacks training signal for unseen classes which is critical to inherit knowledge from the teacher model.
To address those challenges, we propose a novel strat-egy, dubbed HOICLIP, for transferring CLIP knowledge to the HOI detection task in this work. Our design ethos is to directly retrieve learned knowledge from CLIP instead of relying on distillation and to mine the prior knowledge from multiple aspects by exploiting the compositional na-ture of the HOI recognition. Moreover, to cope with the long-tail and zero-shot learning in verb recognition under low data regime, we develop a verb representation based on visual semantic arithmetic, which does not require large amount of training data as in knowledge distillation based methods. Our methods enable us to improve the data ef-ficiency in HOI representation learning and achieve better generalization as well as robustness.
Specifically, our HOICLIP framework learns to retrieve the prior knowledge from the CLIP model from three as-pects: 1) Spatial feature. As the feature location is key to the detection task, we fully exploit the visual represen-tation in CLIP and extract features only from informative image regions. To this end, we utilize CLIP’s feature map with spatial dimensions, and develop a transformer-based interaction decoder that learns a localized interaction fea-ture with cross-modal attention. 2) Verb feature. To address the long-tailed verb-class problem as shown in Figure 1, we develop a verb classifier focusing on learning a better rep-resentation for the verbs. Our verb classifier consists of a verb feature adapter [13,20,36,51] and a set of class weights computed via visual semantic arithmetic [38]. We enhance the HOI prediction by fusing the outputs of the verb clas-sifier and the common interaction classifier. 3) Linguistic feature. To cope with the very rare and unseen class for
HOI prediction, we adopt a prompt-based linguistic repre-sentation for HOIs and build a zero-shot classifier for the
HOI classification [42]. This classifier branch requires no training and we integrate its output with the HOI classifier during model inference.
We evaluate our HOICLIP on two representative HOI detection datasets, HICO-DET [6] and V-COCO [15]. To validate HOICLIP, we perform extensive experiments under fully-supervised setting, zero-shot setting and data-efficient setting. The experiment results demonstrate the superior-ity of our methods: HOICLIP achieves competitive per-formance across all three settings, outperforming previous state-of-the-art methods on the zero-shot setting by 4.04 mAP and improving the data efficiency significantly.
The main contributions of our paper can be summarized as follows:
• To our best knowledge, HOICLIP is the first work to utilize query-based knowledge retrieval for efficient knowledge transfer from the pre-trained CLIP model to HOI detection tasks.
• We develop a fine-grained transfer strategy, leveraging regional visual feature of HOIs via cross attention and a verb representation via visual semantic arithmetic for more expressive HOI representation.
• We further improve the performance of HOICLIP by exploiting zero-shot CLIP knowledge without addi-tional training. 2.