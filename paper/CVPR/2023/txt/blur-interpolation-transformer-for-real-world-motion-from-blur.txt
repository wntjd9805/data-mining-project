Abstract
This paper studies the challenging problem of recovering motion from blur, also known as joint deblurring and inter-polation or blur temporal super-resolution. The challenges are twofold: 1) the current methods still leave considerable room for improvement in terms of visual quality even on the synthetic dataset, and 2) poor generalization to real-world data. To this end, we propose a blur interpolation trans-former (BiT) to effectively unravel the underlying temporal correlation encoded in blur. Based on multi-scale residual
Swin transformer blocks, we introduce dual-end temporal supervision and temporally symmetric ensembling strate-gies to generate effective features for time-varying motion rendering.
In addition, we design a hybrid camera sys-tem to collect the first real-world dataset of one-to-many blur-sharp video pairs. Experimental results show that BiT has a significant gain over the state-of-the-art methods on the public dataset Adobe240. Besides, the proposed real-world dataset effectively helps the model generalize well to real blurry scenarios. Code and data are available at https://github.com/zzh-tech/BiT. 1.

Introduction
Aside from time-lapse photography, motion blur is usu-ally one of the most undesirable artifacts during photo shooting. Many works have been devoted to studying how to recover sharp details from the blur, and great progress has been made. Recently, starting from Jin et al. [9], the community has focused on the more challenging task of re-covering high-frame-rate sharp videos from blurred images, which can be collectively termed joint deblurring and inter-polation [37, 38] or blur temporal super-resolution [26, 33– 35]. This joint task can serve various applications, such as video visual perception enhancement, slow motion gener-ation [26], and fast moving object analysis [33–35]. For brevity, we will refer to this task as blur interpolation.
Recent works [7, 8, 37] demonstrate that the joint ap-proach outperforms schemes that cascade separate deblur-ring and video frame interpolation methods. Most joint approaches follow the center-frame interpolation pipeline, which means that they can only generate latent frames for middle moments in a recursive manner. DeMFI [26] breaks this constraint by combining self-induced feature-flow-based warping and pixel-flow-based warping to syn-thesize latent sharp frame at arbitrary time t. However, even on synthetic data, the performance of current methods is still far from satisfactory for human perception. We find that the potential temporal correlation in blur has been underuti-lized, which allows huge space for performance improve-ment of the blur interpolation algorithm. In addition, blur interpolation suffers from the generalization issue because there is no real-world dataset to support model training.
The goal of this work is to resolve the above two issues.
In light of the complex distribution of time-dependent re-construction and temporal symmetry property, we propose dual-end temporal supervision (DTS) and temporally sym-metric ensembling (TSE) strategies to enhance the shared temporal features of blur interpolation transformer (BiT) for time-varying motion reconstruction. In addition, a multi-scale residual Swin transformer block (MS-RSTB) is intro-duced to empower the model with the ability to effectively handle the blur in different scales and to fuse information from adjacent frames. Due to our design, BiT achieves state-of-the-art on the public benchmark performance even without optical flow-based warping operations. Meanwhile, to provide a real-world benchmark to the community, we further design an accurate hybrid camera system follow-ing [32, 51] to capture a dataset (RBI) containing time-aligned low-frame-rate blurred and high-frame-rate sharp video pairs. Thanks to RBI, the real data generalization problem of blur interpolation can be greatly alleviated, and a more reasonable evaluation platform becomes available.
With these improvements, our model presents impressive arbitrary blur interpolation performance, and we show an example of extracting 30 frames of sharp motion from the blurred image in Fig. 1 for reference.
Our contributions can be summarized as follows: 1) We propose a novel transformer-based model, BiT, for arbitrary
Figure 1. Arbitrary blur interpolation by BiT. This is an example of generating 30 sharp frames from blurred image using BiT. time motion from blur reconstruction. BiT outperforms prior art quantitatively and qualitatively with faster speed. 2) We present and verify two successful strategies includ-ing dual-end temporal supervision and temporally symmet-ric ensembling to enhance the shared temporal features for arbitrary time motion reconstruction. 3) To the best of our knowledge, we provide the first real-world dataset for gen-eral blur interpolation tasks. We verify the validity of this real dataset and its meaningfulness to the community by ex-tensive experiments. 2.