Abstract
Since training a deep neural network (DNN) is costly, the well-trained deep models can be regarded as valuable intellectual property (IP) assets. The IP protection asso-ciated with deep models has been receiving increasing at-tentions in recent years. Passport-based method, which re-places normalization layers with passport layers, has been one of the few protection solutions that are claimed to be secure against advanced attacks. In this work, we tackle the issue of evaluating the security of passport-based IP protec-tion methods. We propose a novel and effective ambiguity attack against passport-based method, capable of success-fully forging multiple valid passports with a small train-ing dataset. This is accomplished by inserting a specially designed accessory block ahead of the passport parame-ters. Using less than 10% of training data, with the forged passport, the model exhibits almost indistinguishable per-formance difference (less than 2%) compared with that of the authorized passport. In addition, it is shown that our attack strategy can be readily generalized to attack other IP protection methods based on watermark embedding. Direc-tions for potential remedy solutions are also given. 1.

Introduction
With the geometric growth of computing power of com-putational devices in recent decades, there have emerged many deep learning applications that have contributed to the human world such as super-resolution reconstruction
[7, 9, 30], image inpainting [31, 34, 35] and forgery detec-tion [32]. It usually costs many resources to develop new
DNN models and developers will not tolerate the act of theft of their IP. The IP protection problem of deep models be-†Corresponding author. comes more severe with the birth of Machine Learning as a Service (MLaaS) [26]. Preventing the infringement be-havior of deep models now emerges as a necessary concern when developing new algorithms and systems.
Model watermark [20, 25, 27, 28, 37] has been a popu-lar method to protect the IP of DNN models. In the em-bedding process, the owners embed the secret signatures (watermarks), and then in the verification process, they can claim their ownership to the model by matching the ex-tracted signatures with the original versions. The existing model watermark methods can be roughly divided into two categories [10, 11]: feature-based and trigger-based meth-ods. Specifically, feature-based methods [4, 8, 24, 29] ap-plied a regularizer to embed the secret watermark into the activation functions or model weights. Uchida et al. [29] proposed to use a regularizer to embed a watermark into the model weights. Darvish et al. [8] embedded the fingerprints in the Probability Density Function of trainable weights in-stead. Aramoon et al. [3] inserted the signature into the gradient of the cross-entropy loss function with respect to the inputs. In contrast, trigger-based methods make the out-put target respond to specific inputs. Along this line, Adi et al. [1] used the backdoor attack as a means to watermark the model. Merrer et al. [18] designed a zero-bit watermarking algorithm that uses adversarial samples as watermarks to claim the ownership. Zhang et al. [39] applied watermarks to images and then trained the network to output target la-bels when input images carry these watermarks.
Despite the strength in retaining ownership of DNN models, most existing model watermark methods are shown to be vulnerable to the so-called ambiguity attack, in which the attacker manages to cast doubts on the ownership ver-ification by crafting counterfeit (forged) watermarks [11].
Recently, Fan et al. [10] first designed a series of ambiguity attacks, which are effective in attacking DNN watermark
It was stated that for conventional watermark methods.
methods, a counterfeit watermark can be forged as along as the model performance is independent of the signature [11].
Following this proposition, Fan et al. designed a passport layer through which the functionality of the model is con-trolled by the signature called passport. However, Fan et al. encountered a heavy performance drop when batch nor-malization layers exist. To solve this problem, Zhang et al. [38] added learnable affine transformations to the scale and bias factors. It was claimed that an attacker cannot find a substitute passport that maintains the model performance, which ensures the security of these passport-based methods against existing ambiguity attacks.
In this work, we aim to design an advanced ambiguity attack to the passport-based method, capable of generat-ing valid substitute passports with only a small number of data. Here, valid substitute passports are defined as those leading to an indistinguishable model performance, but suf-ficiently different from the original authorized passports.
Clearly, with such valid substitute passports, an attacker can claim the ownership of the model. To this end, we first experimentally justify the existence of multiple valid substitute passports. Noticing the fact that it is easy to lo-calize the passport layers, we then propose our ambiguity attack by replacing passport layers with our designed two types of structures, namely Individual Expanded Residual
Block (IERB) and Collective Expanded Residual Block (CERB). Both structures are built in a way to encourage the significant changes of the parameters in the passport layers during the training, which could help us search for valid substitute passports. Benefiting from these two struc-tures and assisting with a small amount training data, we can obtain valid substitute passports, and hence, defeat the passport-based methods which are the only type of method claimed to be immune to existing ambiguity attacks.
Our major contributions can be summarized as follows:
• We propose a novel and effective ambiguity attack against the passport-based IP protection schemes.
With less than 10% of training data, our ambiguity at-tack on passport-layer protected model can restore the functionality of the model with a less than 2% perfor-mance gap from the original accuracy.
• We design two novel structures for replacing the pass-port layers, based on the multi-layer perceptron (MLP) and skip connection to assist with our ambiguity attack for searching valid substitute passports with a small amount of training data.
• Experiments on both overlapping (attacker’s training dataset is part of the original training dataset) and non-overlapping datasets (attacker’s dataset and the origi-nal one come from the same source but no overlap ex-ists), and on different network structures have proved the effectiveness of our ambiguity attack.
• Our attack method can be readily generalized to attack other DNN watermark methods [8, 21, 29]. 2.