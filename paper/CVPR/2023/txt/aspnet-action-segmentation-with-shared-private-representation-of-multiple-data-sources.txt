Abstract
Most state-of-the-art methods for action segmentation are based on single input modalities or na¨ıve fusion of mul-tiple data sources. However, effective fusion of comple-mentary information can potentially strengthen segmenta-tion models and make them more robust to sensor noise and more accurate with smaller training datasets. In order to improve multimodal representation learning for action segmentation, we propose to disentangle hidden features of a multi-stream segmentation model into modality-shared components, containing common information across data sources, and private components; we then use an attention bottleneck to capture long-range temporal dependencies in the data while preserving disentanglement in consecutive processing layers. Evaluation on 50salads, Breakfast and
RARP45 datasets shows that our multimodal approach out-performs different data fusion baselines on both multiview and multimodal data sources, obtaining competitive or bet-ter results compared with the state-of-the-art. Our model is also more robust to additive sensor noise and can achieve performance on par with strong video baselines even with less training data. 1.

Introduction
Action segmentation is the task of predicting which ac-tion is occurring at each frame in untrimmed videos of com-plex and semantically structured human activities [18, 32].
While conventional methods for human action understand-ing focus on classification of short video clips [6, 27, 34], action segmentation models have to learn the semantics of
This research was funded in part by the Wellcome/EPSRC Cen-tre for Interventional and Surgical Sciences (WEISS) [203145/Z/16/Z]; the Engineering and Physical Sciences Research Council (EPSRC)
[EP/P012841/1]; and the Royal Academy of Engineering Chair in Emerg-ing Technologies Scheme. For the purpose of open access, the author has applied a CC BY public copyright licence to any author accepted manuscript version arising from this submission.
Figure 1. Different paradigms for multi-source data fusion via (a) early fusion, (b) disentanglement of modality-shared and modality-specific representations (our model) and (c) late fusion; (d) Example from 50salads highlighting shared and private infor-mation that can be extracted from video and accelerometer data.
While both modalities can detect the activation of relevant tools and common motion cues, RGB videos additionally capture funda-mental details about objects without acceleration sensors and their state (e.g. chopped tomatoes), the overall spatial configuration and the localization of motion in the scene. Accelerometer signals, on the other hand, contain explicit and complementary informa-tion about 3D fine motion patterns of activated objects and their co-occurrence. In the presence of noise (e.g. video occlusions) or other variability factors, some shared attributes could become part of the private space of the uncorrupted modality. all action classes as well as their temporal boundaries and contextual relations, which is challenging and requires the design of efficient strategies to capture long range temporal information and inter-action correlations.
Recent methods for action segmentation input pre-computed low-dimensional visual features [6, 11] into dif-ferent long-range temporal processing units, such as tem-poral convolutions [11, 19], temporal self-attention [38, 45] or graph neural networks [46]. While these methods utilize
only video data, recent computer vision datasets have in-creasing availability of multiple synchronized data sources
[9,24,32], some of which could be collected readily in real-case scenarios (e.g. audio recordings [9], teleoperated robot kinematics [35]). Effective fusion of different data modali-ties or different ‘ views’ of the same modality (here we use the term ‘view’ to denote any different representation of the same data source) is not trivial and still a very active area of research, as potential advantages include higher recogni-tion performance, improved robustness to sensor noise and mitigating the need for large training datasets [3].
Action segmentation with multiple data sources has not been investigated as extensively as similar tasks like action
It has generally been addressed via na¨ıve classification. fusion strategies such as multimodal feature concatenation
[5, 45] and prediction fusion [37], or limited to the fea-ture encoding stage [22]. However, sensor fusion can also benefit from long-range temporal modelling performed in later stages. Inspired by work on multimodal representa-tion learning [5, 20], we approach the problem implement-ing a multi-stream action segmentation model, one stream for each available data source, and disentangling their latent space into modality-shared versus modality-specific repre-sentations (Fig. 1b and 1d), aiming at learning more dis-criminative features and more robust action recognition.
We assume that creating a shared feature space across data sources produces more abstract action representations and reduces over-fitting to modality-specific nuances and noise, while private features could retain useful complementary
Instead of relying information for the downstream task. on adversarial mechanisms [41], autoencoders [5, 20] or generative approaches [20], we learn shared feature spaces with minimal model modification by minimizing Maxi-mum Mean Discrepancy (MMD) on partitions of the la-tent spaces to reduce the distance between their distribu-tions. In order to capture long-range temporal dependen-cies in the data while preserving feature disentanglement in consecutive processing layers, an attention bottleneck [26] is then integrated into the segmentation model and initial-ized with learned modality-shared features, allowing inde-pendent processing of all private features. We called the model ASPnet (Action Shared-Private network).
Evaluation results of our model on three challeng-ing benchmark datasets show improvement over unimodal baselines and different fusion strategies using both multi-modal (e.g. video and accelerometer) and multiview (e.g.
RGB and optical flow) inputs, leading to competitive or better results than the state-of-the-art. In addition, results suggest that ASPnet could generalize well to multiple data sources, improving its performance with growing number of inputs. Despite requiring synchronized recordings of multiple sensors, we demonstrated that our model is also more robust to additive input noise and can match the per-formance of strong video baselines with less data. In sum-mary, our contributions are the following:
• We present ASPnet, a new multi-source activity recog-nition model to effectively exploit shared and com-plementary information contained in multiple data sources for robust action segmentation. ASPnet par-titions the latent representation of each modality and exploits a bottleneck mechanism to allow feature inter-action at multiple levels of abstraction while preserv-ing disentanglement. Additionally, modality fusion is influenced by long-range temporal dynamics captured at different scales.
• We show the advantage of feature disentanglement to fuse not only multimodal data, but also multiple repre-sentations of the same modality.
• We perform extensive ablation studies to evaluate
ASPnet against strong baselines, different levels of noise and less training data.
• We evaluate ASPnet on three challenging benchmark datasets and achieved competitive or better results than state-of-the-art models. 2.