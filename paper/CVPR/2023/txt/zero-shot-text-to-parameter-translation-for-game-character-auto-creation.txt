Abstract
Recent popular Role-Playing Games (RPGs) saw the great success of character auto-creation systems. The bone-driven face model controlled by continuous parameters (like the position of bones) and discrete parameters (like the hairstyles) makes it possible for users to personalize and customize in-game characters. Previous in-game character auto-creation systems are mostly image-driven, where fa-cial parameters are optimized so that the rendered charac-ter looks similar to the reference face photo. This paper pro-poses a novel text-to-parameter translation method (T2P) to achieve zero-shot text-driven game character auto-creation.
With our method, users can create a vivid in-game char-acter with arbitrary text description without using any ref-erence photo or editing hundreds of parameters manually.
In our method, taking the power of large-scale pre-trained multi-modal CLIP and neural rendering, T2P searches both continuous facial parameters and discrete facial parame-*Corresponding Authors. ters in a unified framework. Due to the discontinuous pa-rameter representation, previous methods have difficulty in effectively learning discrete facial parameters. T2P, to our best knowledge, is the first method that can handle the op-timization of both discrete and continuous parameters. Ex-perimental results show that T2P can generate high-quality and vivid game characters with given text prompts. T2P outperforms other SOTA text-to-3D generation methods on both objective evaluations and subjective evaluations. 1.

Introduction
Role-Playing Games (RPGs) are praised by gamers for providing immersive experiences. Some of the recent pop-ular RPGs, like Grand Theft Auto Online1 and Naraka2, have opened up character customization systems to play-ers. In such systems, in-game characters are bone-driven and controlled by continuous parameters, like the position, 1https://www.rockstargames.com/GTAOnline 2http://www.narakathegame.com
rotation, scale of each bone, and discrete parameters, like the hairstyle, beard styles, make-ups, and other facial el-ements. By manually adjusting these parameters, players can control the appearance of the characters in the game according to their personal preferences, rather than using predefined character templates. However, it is cumbersome and time-consuming for users to manually adjust hundreds of parameters - usually taking up to hours to create a char-acter that matches their expectations.
To automatically create in-game characters, the method named Face-to-parameter translation (F2P) was recently proposed to automatically create game characters based on a single input face image [38]. F2P and its variants [39, 41] have been successfully used in recent RPGs like Narake and Justice, and virtual meeting platform Yaotai. Recent 3D face reconstruction methods [2, 7, 26, 33, 42â€“44] can also be adapted to create game characters. However, all the above-mentioned methods require reference face pho-tos for auto-creation. Users may take time to search, down-load and upload suitable photos for their expected game characters. Compared with images, text prompts are more flexible and time-saving for game character auto-creation.
A very recent work AvatarCLIP [10] achieved text-driven avatar auto-creation and animation.
It optimizes implicit neural networks to generate characters. However, the cre-ated characters are controlled by implicit parameters, which lack explicit physical meanings, thus manually adjusting them needs extra designs. This will be inconvenient for players or game developers to further fine-tune the created game characters as they want.
To address the above problems, we propose text-to-parameter translation (T2P) to tackle the in-game charac-ter auto-creation task based on arbitrary text prompts. T2P takes the power of large-scale pre-trained CLIP to achieve zero-shot text-driven character creation and utilizes neural rendering to make the rendering of in-game characters dif-ferentiable to accelerate the parameters optimization. Pre-vious works like F2Ps give up controlling discrete facial parameters due to the problem of discontinuous parameter gradients. To our best knowledge, the proposed T2P is the first method that can handle both continuous and discrete facial parameters optimization in a unified framework to create vivid in-game characters. F2P is also the first text-driven automatic character creation suitable for game envi-ronments.
Our method consists of a pre-training stage and a text-to-parameter translation stage. In the pre-training stage, we first train an imitator to imitate the rendering behavior of the game engine to make the parameter searching pipeline end-to-end differentiable. We also pre-train a translator to trans-late the CLIP image embeddings of random game charac-ters to their facial parameters. Then at the text-to-parameter translation stage, on one hand, we fine-tune the translator on un-seen CLIP text embeddings to predict continuous pa-rameters given text prompt rather than images, on the other hand, discrete parameters are evolutionally searched. Fi-nally, the game engine takes in the facial parameters and creates the in-game characters which correspond to the text prompt described, as shown in Fig 1. Objective evaluations and subjective evaluations both indicate our method outper-forms other SOTA zero-shot text-to-3D methods.
Our contributions are summarized as follows: 1) We propose a novel text-to-parameter translation method for zero-shot in-game character auto-creation. To the best of our knowledge, we are the first to study text-driven character creation ready for game environments. 2) The proposed T2P can optimize both continuous and discrete parameters in a unified framework, unlike earlier methods giving up controlling difficult-to-learn discrete pa-rameters. 3) The proposed text-driven auto-creation paradigm is flexible and friendly for users, and the predicted physically meaningful facial parameters enable players or game devel-opers to further finetune the game character as they want. 2.