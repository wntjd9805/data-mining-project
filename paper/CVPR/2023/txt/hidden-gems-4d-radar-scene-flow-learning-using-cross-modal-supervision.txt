Abstract
This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our ap-proach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implic-itly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learn-ing problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experi-ments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal super-vised learning to infer more accurate 4D radar scene flow.
We also show its usefulness to two subtasks - motion seg-mentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow. 1.

Introduction
Scene flow estimation is to obtain a 3D motion vector field of the static and dynamic environment relative to an ego-agent. In the context of self-driving, scene flow is a key enabler to navigational safety in dynamic environments by providing holistic motion cues to multiple subtasks, such as ego-motion estimation, motion segmentation, point cloud accumulation, multi-object tracking, etc.
Driven by the recent successes of deep neural networks in point cloud processing [22, 35, 36, 40, 51, 54], predomi-nant approaches to scene flow estimation from point clouds adopt either fully- [12, 27, 34, 49, 52] or weakly- [10, 11] supervised learning, or only rely on self-supervised sig-nals [2, 9, 21, 24, 31]. For supervised ones, the acquisi-tion of scene flow annotations is costly and requires tedious and intensive human labour.
In contrast, self-supervised learning methods require no annotations and can exploit the inherent spatio-temporal relationship and constraints in the input data to bootstrap scene flow learning. Neverthe-*Corresponding author: Chris Xiaoxuan Lu (xiaoxuan.lu@ed.ac.uk)
Figure 1. Cross-modal supervision cues are retrieved from co-located odometer, LiDAR and camera sensors to benefit 4D radar scene flow learning. The source point cloud (red) is warped with our estimated scene flow and gets closer to the target one (blue). less, due to the implicit supervision signals, self-supervised learning performance is often secondary to the supervised ones [10,23,49], and they fail to provide sufficiently reliable results for safety-critical autonomous driving scenarios.
These challenges become more prominent when it comes to 4D radar scene flow learning. 4D automotive radars receive increasing attention recently due to their robust-ness against adverse weather and poor lighting conditions (vs. camera), availability of object velocity measurements (vs. camera and LiDAR) and relatively low cost (vs. Li-DAR) [5, 30, 32, 41]. However, 4D radar point clouds are significantly sparser than LiDARs’ and suffer from non-negligible multi-path noise. Such low-fidelity data signif-icantly complicates the point-level scene flow annotation for supervised learning and makes it difficult to rely ex-clusively on self-supervised training-based methods [9] for performance and safety reasons.
To find a more effective framework for 4D radar scene flow learning, this work aims to exploit cross-modal su-pervision signals in autonomous vehicles. Our motiva-tion is based on the fact that autonomous vehicles today are equipped with multiple heterogeneous sensors, e.g., Li-DARs, cameras and GPS/INS, which can provide comple-mentary sensing and redundant perception results for each other, jointly safeguarding the vehicle operation in com-plex urban traffic. This co-located perception redundancy
can be leveraged to provision multiple supervision cues that bootstrap radar scene flow learning. For example, the static points identified by radar scene flow can be used to estimate the vehicle odometry. The consistency between this esti-mated odometry and the observed odometry from the co-located GPS/INS on the vehicle forms a natural constraint and can be used to supervise scene flow estimation. Simi-lar consistency constraints can be also found for the optical flow estimated from co-located cameras and the projected radar scene flow onto the image plane.
While the aforementioned examples are intuitive, re-trieving accurate supervision signals from co-located sen-sors is non-trivial. With the same example of optical and scene flow consistency, minimizing flow errors on the im-age plane suffers from the depth-unaware perspective pro-jection, potentially incurring weaker constraints to the scene flow of far points. This motivates the following research question: How to retrieve the cross-modal supervision sig-nals from co-located sensors on a vehicle and apply them collectively to bootstrap radar scene flow learning.
Towards answering it, in this work we consider op-portunistically exploiting useful supervision signals from three commonly co-existent sensors with the 4D radar on a vehicle: odometer (GPS/INS), LiDAR, and RGB camera (See Fig. 1). This cross-modal supervision is expected to help us realize radar scene flow learning without human an-notation. In our setting, the multi-modal data are only avail-able in the training phase, and only 4D radar is used during the inference stage. Our contributions can be summarized as follows:
• Our work is the first 4D radar scene flow learning us-ing cross-modal supervision from co-located heteroge-neous sensors on an autonomous vehicle.
• We introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to effectively engage scene flow estima-tion using multiple cross-modal constraints for model training.
• We demonstrate the state-of-the-art performance of the proposed CMFlow method on a public dataset and show its effectiveness in downstream tasks as well. 2.