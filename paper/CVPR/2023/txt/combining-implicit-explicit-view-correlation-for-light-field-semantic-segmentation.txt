Abstract
Since light field simultaneously records spatial informa-tion and angular information of light rays, it is considered to be beneficial for many potential applications, and seman-tic segmentation is one of them. The regular variation of image information across views facilitates a comprehensive scene understanding. However, in the case of limited mem-ory, the high-dimensional property of light field makes the problem more intractable than generic semantic segmenta-tion, manifested in the difficulty of fully exploiting the re-lationships among views while maintaining contextual in-formation in single view. In this paper, we propose a novel network called LF-IENet for light field semantic segmenta-tion. It contains two different manners to mine complemen-tary information from surrounding views to segment cen-tral view. One is implicit feature integration that leverages attention mechanism to compute inter-view and intra-view similarity to modulate features of central view. The other is explicit feature propagation that directly warps features of other views to central view under the guidance of disparity.
They complement each other and jointly realize complemen-tary information fusion across views in light field. The pro-posed method achieves outperforming performance on both real-world and synthetic light field datasets, demonstrating the effectiveness of this new architecture. 1.

Introduction
Semantic segmentation is a pixel-level task that assigns a class label to each pixel of the given image, serving as a key fundamental of visual understanding. Due to the partial visibility incurred by occlusion as well as high intra-class variation with diverse appearances, viewpoints and scales,
*Corresponding author
Figure 1. Illustration of light field imaging. Red rectangle shows an occlusion scene, in which the front wheel of bicycle is di-vided into two areas (enclosed in blue and yellow boxes) and the back wheel is complete (enclosed in green box). Since viewpoints are arranged on a regular grid in angular plane, the location and scale of these areas are regularly changed across views, which is a unique advantage of light field. Influenced by the pedestrian with big disparity, the changes near the front wheel are significant. accurate segmentation is a fairly challenging problem. A series of image segmentation methods [4, 11, 41, 43] have been proposed to address these challenges. Furthermore,
[1, 7, 23, 37] take depth information into consideration to overcome the deficiency of single image. Recently, [16, 24] employ light field to achieve impressive performance, pro-viding a new perspective for semantic segmentation.
Compared to traditional imaging system, 4D light field records intensity for rays in terms of position and direction, yielding a regularly distributed multi-view image array. The information embedded in additional angular dimensions is beneficial for detail analysis to thoroughly parse scenes. As shown in Fig. 1, the front wheel of bicycle is occluded, forming two small areas that are hard to assign labels. With the transformation of viewpoint, the scale of areas changes accordingly. Capturing such regular change with the help
modeling mechanism is commonly used in research of light field like super-resolution [33, 38] and disparity estimation
[27, 32]. Considering prohibitive inference cost and lim-ited memory usage, each SAI is cropped into multiple small patches for calculation. However, as illustrated in Fig. 2, it is catastrophic and unsuitable for semantic segmentation because independent small patches discard valuable contex-tual information [35]. Resizing all SAIs to an extremely low scale along the spatial dimension is an alternative manner, but it gives up resolution and granularity which are critical for dense prediction tasks.
In the light of above issues, we present a well-engineered framework, which includes an implicit branch and an ex-plicit branch to fully explore structural information in light field for robust semantic segmentation of central view. The implicit branch only processes a few SAIs and utilizes self-attention and cross-attention mechanisms to calculate simi-larity for feature integration. The explicit branch processes all SAIs to estimate disparity for subsequent feature prop-agation. In brief, our framework realizes feature enhance-ment for central view through implicit feature integration and explicit feature propagation. It is worth noting that two branches transmit supplemental information to each other.
Specifically, implicit branch leverages estimated disparity from explicit branch to adjust the weight of cross-attention, enhancing the perception of variation among views. On the other hand, the features to be warped in explicit branch derive from implicit branch. The output features of two branches are fused for final prediction.
Our contributions can be summarized as follows. (1) We present a network called LF-IENet which incorporates im-plicit and explicit view correlation to exploit light field. The former learns a unified representation within target view and across views. The latter uses disparity to propagate features to target view. (2) The proposed network exists information interaction between two manners, acting as a supplemen-tary item for one another rather than standalone and jointly improving the utilization efficiency of light field. (3) Exten-sive experiments on the light field semantic segmentation dataset confirm the effectiveness of our method. 2.