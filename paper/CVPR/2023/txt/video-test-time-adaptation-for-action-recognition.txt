Abstract
Although action recognition systems can achieve top per-formance when evaluated on in-distribution test points, they are vulnerable to unanticipated distribution shifts in test data. However, test-time adaptation of video action recog-nition models against common distribution shifts has so far not been demonstrated. We propose to address this prob-lem with an approach tailored to spatio-temporal models that is capable of adaptation on a single video sample at a step. It consists in a feature distribution alignment tech-nique that aligns online estimates of test set statistics to-wards the training statistics. We further enforce prediction consistency over temporally augmented views of the same test video sample. Evaluations on three benchmark ac-tion recognition datasets show that our proposed technique is architecture-agnostic and able to significantly boost the performance on both, the state of the art convolutional ar-chitecture TANet and the Video Swin Transformer. Our pro-posed method demonstrates a substantial performance gain over existing test-time adaptation approaches in both eval-uations of a single distribution shift and the challenging case of random distribution shifts. Code will be available at https://github.com/wlin-at/ViTTA. 1.

Introduction
State-of-the-art neural architectures [8,40,46,48–50] are very effective in action recognition, but recent work shows they are not robust to shifts in the distribution of the test data [33,51]. Unfortunately, in practical scenarios, such dis-tribution shifts are very difficult to avoid, or account for. For example, cameras used for recognizing motorized or pedes-∗ Equally contributing authors.
† Correspondence: wei.lin@icg.tugraz.at trian traffic events may register rare weather conditions, like a hailstorm, and sports action recognition systems can be affected by perturbations generated by spectators at sports arenas, such as the smoke of flares. Shifts in the data dis-tribution can also result from inconspicuous changes in the video processing setup, for instance, a change of the algo-rithm used to compress the video feed.
In image classification, distribution shift can be miti-gated by Test-Time-Adaptation (TTA) [16, 19, 21, 23, 28, 34, 38, 42], which uses the unlabeled test data to adapt the model to the change in data distribution. However, methods developed for image classification are not well suited for action recognition. Most action recognition applications re-quire running memory- and computation-hungry temporal models online, with minimal delay, and under tight hard-ware constraints. Moreover, videos are more vulnerable to distribution shifts than images [2,33,51]. Some examples of such distribution shifts are given in Fig. 1. Due to limited exposure times, video frames are likely to feature higher variations of noise level, provoked by illumination changes.
They are more affected by motion blur, which varies with the speed of motion observed in the scene. They also fea-ture stronger compression artifacts, which change with the compression ratio, often dynamically adjusted to the avail-able bandwidth. Our experiments show that existing TTA algorithms, developed for image classification, do not cope with these challenges well, yielding marginal improvement over networks used on corrupted data without any adapta-tion.
Our goal is to propose an effective method for online test-time-adaptation of action recognition models. Oper-ating online and with small latency can require drastically constraining the batch size, especially when the hardware resources are limited and the employed model is large. We therefore focus on the scenario in which test samples are
ple augmented views lead to more accurate statistics of the overall video content. Second, it allows us to enforce pre-diction consistency across the views, making the adaptation more effective.
Our extensive evaluations on three most popular action recognition benchmarks demonstrate that ViTTA boosts the performance of both TANet [27], the state-of-the-art convo-lutional architecture, and the Video Swin Transformer [26], and outperforms the existing TTA methods proposed for im-age data by a significant margin. ViTTA performs favorably in both, evaluations of single distribution shift and the chal-lenging case of random distribution shift.
ViTTA also has a high practical valor. It is fully online
It and applicable in use cases requiring minimal delays. does not require collection or storage of test video dataset, which is significant in terms of data privacy protection, es-pecially in processing confidential user videos. ViTTA can be seamlessly incorporated in systems already in operation, as it has no requirement of re-training existing networks.
Therefore it can harness state-of-the-art video architectures.
Our contributions can be summarized as follows:
• We benchmark existing TTA methods in online adaptation of action recognition models to distribution shifts in test data, on three most popular action recog-nition datasets, UCF101 [35], Something-something v2 [13] and Kinetics 400 [18]
• We adapt the feature alignment approach to online ac-tion recognition, generating a substantial performance gain over existing techniques.
• We propose a novel, video-specific adaptation tech-nique (ViTTA) that enforces consistency of predictions for temporally re-sampled frame sequences and show that it contributes to adaptation efficacy. 2.