Abstract
Video semantic segmentation (VSS) is a computationally expensive task due to the per-frame prediction for videos of high frame rates.
In recent work, compact models or adaptive network strategies have been proposed for efficient
VSS. However, they did not consider a crucial factor that affects the computational cost from the input side: the in-put resolution. In this paper, we propose an altering res-olution framework called AR-Seg for compressed videos to achieve efficient VSS. AR-Seg aims to reduce the computa-tional cost by using low resolution for non-keyframes. To prevent the performance degradation caused by downsam-pling, we design a Cross Resolution Feature Fusion (CR-eFF) module, and supervise it with a novel Feature Similar-ity Training (FST) strategy. Specifically, CReFF first makes use of motion vectors stored in a compressed video to warp features from high-resolution keyframes to low-resolution non-keyframes for better spatial alignment, and then selec-tively aggregates the warped features with local attention mechanism. Furthermore, the proposed FST supervises the aggregated features with high-resolution features through an explicit similarity loss and an implicit constraint from the shared decoding layer. Extensive experiments on CamVid and Cityscapes show that AR-Seg achieves state-of-the-art performance and is compatible with different segmenta-tion backbones. On CamVid, AR-Seg saves 67% computa-tional cost (measured in GFLOPs) with the PSPNet18 back-bone while maintaining high segmentation accuracy. Code: https://github.com/THU-LYJ-Lab/AR-Seg. 1.

Introduction
Video semantic segmentation (VSS) aims to predict pixel-wise semantic labels for each frame in a video se-In contrast to a single image, a video sequence quence.
*Corresponding author. (a) (b) (c)
Figure 1. Comparison of different VSS methods: (a) per-frame framework, (b) Accel [19] that alters the depth of models, and (c) our AR-Seg. AR-Seg reduces the computational cost for non-keyframes by lowering the input resolution (depicted by narrow blocks), which is a dimension orthogonal to the depth of networks. is a series of consecutive image frames recorded at a cer-tain frame rate (usually 25fps or higher). Applying image-based segmentation methods [6, 25, 48, 52, 55] to a video frame by frame consumes considerable computational re-sources. To improve the efficiency of VSS, existing meth-ods mainly focus on the design of network architectures.
A class of methods proposes compact and efficient image-based architectures to reduce the computational overhead per-frame [22, 23, 28, 49, 51, 52, 54]. Another class of meth-ods applies a deep model to keyframes and a shallow net-work for non-keyframes to avoid the repetitive computa-tion [19, 24, 27, 34] for videos.
The work presented in this paper is based on an impor-tant observation: the above existing works ignored a cru-cial factor that affects the computational cost from the input side: the input resolution. For image-related tasks, the in-put resolution directly determines the amount of computa-tion, e.g., the computational cost of 2D convolution is pro-portional to the product of image width and height. Once we
downsample the input frame by 0.5Ã—0.5, the computational overhead can be reduced by 75%. On the other hand, de-creasing resolution often leads to worse segmentation accu-racy due to the loss of information [43,54]. In this paper, we propose to prevent the accuracy degradation by using tem-poral correlation in the video. Since the contents of video frames are temporally correlated, the local features lacking in low-resolution (LR) frames can be inferred and enriched by finding correspondences in sparse high-resolution (HR) reference frames based on motion cues. In a compressed video, the motion vectors contain such motion cues and can be obtained along with the video frames from video decod-ing at almost no additional cost.
Motivated by the above observation, we propose an al-tering resolution framework for compressed videos, named
AR-Seg, to achieve efficient VSS. As shown in Figure 1(c),
AR-Seg uses an HR branch to process keyframes at high resolution and an LR branch to process non-keyframes at low resolution. In particular, to prevent performance drop caused by downsampling, we insert a novel Cross Resolu-tion Feature Fusion (CReFF) module into the LR branch and supervise the training with a Feature Similarity Train-ing (FST) strategy to enrich local details in the LR fea-tures. CReFF fuses the HR keyframe features into LR non-keyframe features in two steps: 1) Align the spatial struc-tures of features from different frames by feature warping with motion vectors, which can be readily obtained from compressed videos at almost no additional cost; 2) Selec-tively aggregate the warped features (which may be noisy after warping) into LR features with the aid of local atten-tion mechanism. Since local attention assigns different im-portance to each location in the neighborhood, it is an ef-fective way to avoid misleading by noisy warped features.
Furthermore, our proposed FST strategy guides the learning of the CReFF aggregated features. FST consists of an explicit similarity loss (between the aggregated features and HR features inferred from non-keyframes) and an im-plicit constraint from the shared decoding layer across the
HR and LR branches. Such a training strategy helps the LR branch to learn from features extracted from the HR branch, which is reliable and effective. Integrated with CReFF and
FST, AR-Seg efficiently compensates for the accuracy loss of LR frames. Overall, AR-Seg significantly reduces the computational cost of VSS by altering input resolutions, while maintaining high segmentation accuracy.
To sum up, we make three contributions in this paper: 1)
We propose an efficient framework for compressed videos, named AR-Seg, that uses altering input resolution for VSS and significantly reduces the computational cost without losing segmentation accuracy. 2) We design an efficient
CReFF module to prevent the accuracy loss by aggregat-ing HR keyframe features into LR non-keyframe features. 3) We propose a novel FST strategy that supervises the
LR branch to learn from the HR branch through both ex-plicit and implicit constraints. Experiment results demon-strate the effectiveness of AR-Seg with different resolu-tions, backbones, and keyframe intervals. On both CamVid
[3] and Cityscapes [9] datasets, compared to the constant-resolution baselines, AR-Seg reduces the computational cost by nearly 70% without compromising accuracy. 2.