Abstract
We introduce Correlational Image Modeling (CIM), a novel and surprisingly effective approach to self-supervised visual pre-training. Our CIM performs a simple pretext task: we randomly crop image regions (exemplars) from an input image (context) and predict correlation maps be-tween the exemplars and the context. Three key designs enable correlational image modeling as a nontrivial and meaningful self-supervisory task. First, to generate useful exemplar-context pairs, we consider cropping image regions with various scales, shapes, rotations, and transformations.
Second, we employ a bootstrap learning framework that in-volves online and target encoders. During pre-training, the former takes exemplars as inputs while the latter converts the context. Third, we model the output correlation maps via a simple cross-attention block, within which the context serves as queries and the exemplars offer values and keys. We show that CIM performs on par or better than the current state of the art on self-supervised and transfer benchmarks. Code is available at https://github.com/weivision/
Correlational-Image-Modeling.git. 1.

Introduction
Recent advances in self-supervised visual pre-training have shown great capability in harvesting meaningful representations from hundreds of millions of—often eas-ily accessible—unlabeled images. Among existing pre-training paradigms, Multi-View Self-Supervised Learning (MV-SSL) [8–12, 21, 23] and Masked Image Modeling (MIM) [2, 22, 54, 68] are two leading methods in the self-supervised learning racetrack, thanks to their nontrivial and meaningful self-supervisory pretext tasks.
MV-SSL follows an augment-and-compare paradigm (Figure 1(a)) – randomly transforming an input image into two augmented views and then comparing two different views in the representation space. Such an instance-wise discriminative task is rooted in view-invariant learning [43], i.e., changing views of data does not affect the conveyed information. On the contrary, following the success of
Masked Language Modeling (MLM) [16], MIM conducts a mask-and-predict pretext task within a single view (Fig-ure 1(b)) – removing a proportion of random image patches and then learning to predict the missing information. This simple patch-wise generative recipe enables Transformer-based deep architectures [17] to learn generalizable repre-sentations from unlabeled images.
Beyond augment-and-compare or mask-and-predict pre-text tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for self-supervised visual representation learning. We take inspira-tion from visual tracking [70] in computer vision that defines the task of estimating the motion or trajectory of a target object (exemplar) in a sequence of scene images (contexts).
To cope with challenging factors such as scale variations, deformations, and occlusions, one typical tracking pipeline 1
is formulated as maximizing the correlation between the spe-cific exemplar and holistic contexts [3,5,46,52]. Such simple correlational modeling can learn meaningful representations in the capability of both localization and discrimination, thus making it appealing to serve as a promising pretext task for self-supervised learning.
Training a standard correlational tracking model, however, requires access to numerous labeled data, which is unavail-able in unsupervised learning. Also, the task goal of vi-sual tracking is intrinsically learning toward one-shot object detection—demanding rich prior knowledge of objectness— while less generic for representation learning. Therefore, it is nontrivial to retrofit supervised correlational modeling for visual tracking into a useful self-supervised pretext task.
Driven by this revelation, we present a novel crop-and-correlate paradigm for self-supervised visual representa-tion learning, dubbed as Correlational Image Modeling (CIM). To enable correlational modeling for effectively self-supervised visual pre-training, we introduce three key de-signs. First, as shown in Figure 1(c), we randomly crop image regions (treated as exemplars) with various scales, shapes, rotations, and transformations from an input image (context). The corresponding correlation maps can be de-rived from the exact crop regions directly. This simple crop-ping recipe allows us to easily construct the exemplar-context pairs together with ground-truth correlation maps without human labeling cost. Second, we employ a bootstrap learn-ing framework that is comprised of two networks: an online encoder and a target encoder, which, respectively, encode exemplars and context into latent space. This bootstrapping effect works in a way that the model learns to predict the spatial correlation between the updated representation of exemplars and the slow-moving averaged representation of context. Third, to realize correlational learning, we introduce a correlation decoder built with a cross-attention layer and a linear predictor, which computes queries from context, with keys and values from exemplars, to predict the corresponding correlation maps.
Our contributions are summarized as follows: 1) We present a simple yet effective pretext task for self-supervised visual pre-training, characterized by a novel unsupervised correlational image modeling framework (CIM). 2) We demonstrate the advantages of our CIM in learning trans-ferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art
MIM and MV-SSL learners while improving model robust-ness and training efficiency. We hope our work can motivate future research in exploring new useful pretext tasks for self-supervised visual pre-training. 2.