Abstract
Movie highlights stand out of the screenplay for efficient browsing and play a crucial role on social media platforms.
Based on existing efforts, this work has two observations: (1) For different annotators, labeling highlight has un-certainty, which leads to inaccurate and time-consuming annotations. (2) Besides previous supervised or unsuper-vised settings, some existing video corpora can be useful, e.g., trailers, but they are often noisy and incomplete to cover the full highlights.
In this work, we study a more practical and promising setting, i.e., reformulating high-light detection as “learning with noisy labels”. This setting does not require time-consuming manual annotations and can fully utilize existing abundant video corpora. First, based on movie trailers, we leverage scene segmentation to obtain complete shots, which are regarded as noisy labels. Then, we propose a Collaborative noisy Label
*Corresponding author.
Cleaner (CLC) framework to learn from noisy highlight moments. CLC consists of two modules: augmented cross-propagation (ACP) and multi-modality cleaning (MMC).
The former aims to exploit the closely related audio-visual signals and fuse them to learn unified multi-modal repre-sentations. The latter aims to achieve cleaner highlight labels by observing the changes in losses among different modalities. To verify the effectiveness of CLC, we further collect a large-scale highlight dataset named MovieLights.
Comprehensive experiments on MovieLights and YouTube
Highlights datasets demonstrate the effectiveness of our approach. Code has been made available at:https:
/ / github . com / TencentYoutuResearch /
HighlightDetection-CLC. 1.

Introduction
With the growing number of new publications of movies in theaters and streaming media, audiences become even
harder to choose their favorite one to enjoy for the next two hours. An effective solution is to watch the movie trailers before choosing the right movie. This is because trailers are generally carefully edited by filmmakers and contain the most prominent clips from the original movies. As a condensed version of full-length movies, trailers are elab-orately made with highlight moments to impress the audi-ences. Consequently, they are high potential in serving as supervision sources to train automatic video highlight de-tection algorithms and facilitating the mass production of derivative works for video creators in online video plat-forms, e.g., YouTube and TikTok.
Existing video highlight detection (VHD) approaches are generally trained with annotated key moments of long-form videos. However, they are not suitable to tackle the movie highlight detection task by directly learning from trailers. The edited shots in trailers are not equivalent to ground-truth highlight annotations in movies. Although a previous work [43] leverages the officially-released trail-ers as the weak supervision to train a highlight detector, the highlighted ness of trailer shots is extremely noisy and varies with the preference of audiences, as shown in Fig. 1.
On one hand, trailers tend to be purposefully edited to avoid spoilers, thus missing key moments of the storylines. On the other hand, some less important moments in the orig-inal movies are over-emphasized in the trailers because of some artistic or commercial factors. The subjective nature of trailer shots makes them noisy for the VHD task, which is ignored by existing VHD approaches.
To alleviate the issue, we reformulate the highlight de-tection task as “learning with noisy labels”. Specifically, we first leverage a scene-segmentation model to obtain the movie scene boundaries. The clips containing trailers and clips from the same scenes as the trailers provide more com-plete storylines. They have a higher probability of being highlight moments but still contain some noisy moments.
Subsequently, we introduce a framework named Collab-orative noisy Label Cleaner (CLC) to learn from these pseudo-noisy labels. The framework firstly enhances the modality perceptual consistency via the augmented cross-propagation (ACP) module, which exploits closely related audio-visual signals during training. In addition, a multi-modality cleaning (MMC) mechanism is designed to filter out noisy and incomplete labels.
To support this study and facilitate benchmarking exist-ing methods in this direction, we construct MovieLights, a Movie Highlight Detection Dataset. MovieLights con-tains 174 movies and the highlight moments are all from officially released trailers. The total length of these videos is over 370 hours. We conduct extensive experiments on
MovieLights, in which our CLC exhibits promising results.
We also demonstrate that our proposed CLC achieves sig-nificant performance-boosting over the state-of-the-art on the public VHD benchmarks.
In summary, our major contributions are as follows:
• We introduce a scene-aware paradigm to learn high-light moments in movies without any manual annota-tion. To the best of our knowledge, this is the first time that highlights detection is regarded as learning with noisy labels.
• We present an augmented cross-propagation to capture the interactions across modalities and a consistency loss to maximize the agreement between the different modalities.
• We incorporate a multi-modality noisy label cleaner to tackle label noise, which further improves the robust-ness of networks to annotation noise.
• Experiments on movie datasets and benchmark datasets validate the effectiveness of our framework. 2.