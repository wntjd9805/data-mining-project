Abstract
Unsupervised domain adaptation (UDA) approaches fo-cus on adapting models trained on a labeled source do-main to an unlabeled target domain. In contrast to UDA, source-free domain adaptation (SFDA) is a more practi-cal setup as access to source data is no longer required during adaptation. Recent state-of-the-art (SOTA) methods on SFDA mostly focus on pseudo-label refinement based self-training which generally suffers from two issues: i) in-evitable occurrence of noisy pseudo-labels that could lead to early training time memorization, ii) refinement process requires maintaining a memory bank which creates a signif-icant burden in resource constraint scenarios. To address these concerns, we propose C-SFDA, a curriculum learn-ing aided self-training framework for SFDA that adapts effi-ciently and reliably to changes across domains based on se-lective pseudo-labeling. Specifically, we employ a curricu-lum learning scheme to promote learning from a restricted amount of pseudo labels selected based on their reliabili-ties. This simple yet effective step successfully prevents la-bel noise propagation during different stages of adaptation and eliminates the need for costly memory-bank based label refinement. Our extensive experimental evaluations on both image recognition and semantic segmentation tasks confirm the effectiveness of our method. C-SFDA is also applica-ble to online test-time domain adaptation and outperforms previous SOTA methods in this task. 1.

Introduction
Deep neural network (DNN) models have achieved re-markable success in various visual recognition tasks [16, 22, 41, 44]. However, even very large DNN models of-ten suffer significant performance degradation when there
*Most of this work was done during Nazmul Karim’s internship with
SRI International. Project Page: https://sites.google.com/ view/csfdacvpr2023/home is a distribution or domain shift [54, 78] between training (source) and test (target) domains. To address the prob-lem of domain shifts, various Unsupervised Domain Adap-tation (UDA) [19, 31] algorithms have been developed over recent years. Most UDA techniques require access to la-beled source domain data during adaptation, which limits their application in many real-world scenarios, e.g. source data is private, or adaptation in edge devices with lim-ited computational capacity. In this regard, source-free do-main adaptation setting has recently gained significant in-terest [34, 35, 84], which considers the availability of only source pre-trained model and unlabeled target domain data.
Recent state-of-the-art SFDA methods (e.g., SHOT [42],
NRC [83], G-SFDA [85], AdaContrast [4]) mostly rely on the self-training mechanism that is guided by the source pre-trained model generated pseudo-labels (PLs). Label refine-ment using the knowledge of per-class cluster structure in feature space is recurrently used in these methods. At early stages of adaptation, the label information formulated based on cluster structure can be severely misleading or noisy; shown in Fig. 1. As the adaptation progresses, this label noise can negatively impact the subsequent cluster struc-ture as the key to learning meaningful clusters hinges on the quality of pseudo-labels itself. Therefore, the inevitable presence of label noise at early training time is a critical is-sue in SFDA and requires proper attention. Furthermore, distributing cluster knowledge among neighbor samples re-quires a memory bank [4, 42] which creates a significant burden in resource-constraint scenarios. In addition, most memory bank dependent SFDA techniques are not suitable for online test-time domain adaptation [73,76]; an emerging area of UDA that has gained traction in recent times. De-signing a memory-bank-free SFDA approach that can guide the self-training with highly precise pseudo-labels is a very challenging task and a major focus of this work.
In our work, we focus on increasing the reliability of generated pseudo-labels without using a memory-bank and clustering-based pseudo-label refinement. Our analy-Figure 1. Left: In SFDA, we only have a source model that needs to be adapted to the target data. Among the source-generated pseudo-labels, a large portion is noisy which is important to avoid during supervised self-training (SST) with regular cross-entropy loss. Instead of using all pseudo-labels, we choose the most reliable ones and effectively propagate high-quality label information to unreliable samples. As the training progresses, the proposed selection strategy tends to choose more samples for SST due to the improved average reliability of pseudo-labels. Such a restricted self-training strategy creates a model with better discrimination ability and eventually corrects the noisy predictions. Here, T is the total number of iterations. Right: While existing SFDA techniques leverages cluster structure knowledge in the feature space, there may exist many misleading neighbors— pseudo-labels of neighbors’ that are different from the anchors’ true label. Therefore, clustering-based label propagation inevitably suffers from label noise in subsequent training. sis shows that avoiding early training-time memorization (ETM) of noisy labels encourages noise-free learning in subsequent stages of adaptation. We further analyze that even with an expensive label refinement technique in place, learning equally from all labels eventually leads to label-noise memorization. Therefore, we employ a curriculum learning-aided self-training framework, C-SFDA, that pri-oritizes learning from easy-to-learn samples first and hard samples later on. We show that one can effectively iden-tify the group of easy samples by utilizing the reliabil-ity of pseudo-labels, i.e. prediction confidence and uncer-tainty. We then follow a carefully designed curriculum learning pipeline to learn from highly reliable (easy) sam-ples first and gradually propagate more refined label infor-mation among less reliable (hard) samples later on. In ad-dition to the self-training, we facilitate unsupervised con-trastive representation learning that helps us prevent the early training-time memorization phenomenon. Our main contributions are summarized as follows:
• We introduce a novel SFDA technique that focuses on noise-free self-training exploiting the reliability of generated pseudo-labels. With the help of curriculum learning, we aim to prevent early training time memo-rization of noisy pseudo-labels and improve the quality of subsequent self-training as shown in Fig. 1.
• By prioritizing the learning from highly reliable pseudo-labels first, we aim to propagate refined and accurate label information among less reliable sam-ples. Such a selective self-training strategy elimi-nates the requirement of a computationally costly and memory-bank dependent label refinement framework.
• C-SFDA achieves state-of-the-art performance on ma-jor benchmarks for image recognition and semantic segmentation. Being highly memory-efficient, the pro-posed method is also applicable to online test-time adaptation settings and obtains SOTA performance. 2.