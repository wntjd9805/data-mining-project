Abstract
Semantic map construction under bird’s-eye view (BEV) plays an essential role in autonomous driving. In contrast to camera image, LiDAR provides the accurate 3D obser-vations to project the captured 3D features onto BEV space inherently. However, the vanilla LiDAR-based BEV feature often contains many indefinite noises, where the spatial fea-tures have little texture and semantic cues. In this paper, we propose an effective LiDAR-based method to build se-mantic map. Specifically, we introduce a BEV pyramid fea-ture decoder that learns the robust multi-scale BEV fea-tures for semantic map construction, which greatly boosts the accuracy of the LiDAR-based method. To mitigate the defects caused by lacking semantic cues in LiDAR data, we present an online Camera-to-LiDAR distillation scheme to facilitate the semantic learning from image to point cloud.
Our distillation scheme consists of feature-level and logit-level distillation to absorb the semantic information from camera in BEV. The experimental results on challenging nuScenes dataset demonstrate the efficacy of our proposed
LiDAR2Map on semantic map construction, which signifi-cantly outperforms the previous LiDAR-based methods over 27.9% mIoU and even performs better than the state-of-the-art camera-based approaches. Source code is available at: https://github.com/songw-zju/LiDAR2Map. 1.

Introduction
High-definition (HD) map contains the enriched seman-tic understanding of elements on road, which is a fundamen-tal module for navigation and path planning in autonomous driving. Recently, online semantic map construction has at-tracted increasing attention, which enables to construct HD map at runtime with onboard LiDAR and cameras. It pro-vides a compact way to model the environment around the ego vehicle, which is convenient to obtain the essential in-formation for the downstream tasks.
Most of recent online approaches treat semantic map
*Corresponding author is Jianke Zhu.
Figure 1. Comparisons on semantic map construction frameworks (camera-based, LiDAR-based, Camera-LiDAR fusion methods) and our proposed LiDAR2Map that presents an effective online
Camera-to-LiDAR distillation scheme with a BEV feature pyra-mid decoder in training. learning as a segmentation problem in bird’s-eye view (BEV), which assign each map pixel with a category label.
As shown in Fig. 1, the existing methods can be roughly divided into three groups, including camera-based meth-ods [19, 20, 30, 32, 52], LiDAR-based methods [10, 19] and
Camera-LiDAR fusion methods [19, 27, 37]. Among them, camera-based methods are able to make full use of multi-view images with the enriched semantic information, which dominate this task with the promising performance. In con-trast to camera image, LiDAR outputs the accurate 3D spa-tial information that can be used to project the captured features onto the BEV space. By taking advantage of the geometric and spatial information, LiDAR-based methods are widely explored in 3D object detection [18, 38, 47, 57] while it is rarely investigated in semantic map construction.
HDMapNet-LiDAR [19] intends to directly utilize the Li-DAR data for map segmentation, however, it performs in-ferior to the camera-based models due to the vanilla BEV feature with the indefinite noises. Besides, map segmenta-tion is a semantic-oriented task [27] while the semantic cues in LiDAR are not as rich as those in image. In this work, we aim to exploit the LiDAR-based semantic map construction by taking advantage of the global spatial information and auxiliary semantic density from the image features.
In this paper, we introduce an efficient framework for se-mantic map construction, named LiDAR2Map, which fully exhibits the potentials of LiDAR-based model. Firstly, we present an effective decoder to learn the robust multi-scale BEV feature representations from the accurate spa-tial point cloud information for semantic map. It provides distinct responses and boosts the accuracy of our baseline model. To make full use of the abundant semantic cues from camera, we then suggest a novel online Camera-to-LiDAR distillation scheme to further promote the LiDAR-based model. It fully utilizes the semantic features from the image-based network with a position-guided feature fusion module (PGF2M). Both the feature-level and logit-level dis-tillation are performed in the unified BEV space to facili-tate the LiDAR-based network to absorb the semantic rep-resentation during the training. Specially, we suggest to generate the global affinity map with the input low-level and high-level feature guidance for the satisfactory feature-level distillation. The inference process of LiDAR2Map is efficient and direct without the computational cost of dis-tillation scheme and auxiliary camera-based branch. Ex-tensive experiments on the challenging nuScenes bench-mark [4] show that our proposed model significantly outper-forms the conventional LiDAR-based method (29.5% mIoU vs. 57.4% mIoU). It even performs better than the state-of-the-art camera-based methods by a large margin.
Our main contributions are summarized as: 1) an effi-cient framework LiDAR2Map for semantic map construc-tion, where the presented BEV pyramid feature decoder can learn the robust BEV feature representations to boost the baseline of our LiDAR-based model; 2) an effective online
Camera-to-LiDAR distillation scheme that performs both feature-level and logit-level distillation during the training to fully absorb the semantic representations from the im-ages; 3) extensive experiments on nuScenes for semantic map construction including map and vehicle segmentation under different settings, shows the promising performance of our proposed LiDAR2Map. 2.