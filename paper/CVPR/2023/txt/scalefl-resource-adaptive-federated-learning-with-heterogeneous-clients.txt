Abstract
Federated learning (FL) is an attractive distributed learning paradigm supporting real-time continuous learn-ing and client privacy by default. In most FL approaches, all edge clients are assumed to have sufﬁcient computation capabilities to participate in the learning of a deep neural network (DNN) model. However, in real-life applications, some clients may have severely limited resources and can only train a much smaller local model. This paper presents
ScaleFL, a novel FL approach with two distinctive mecha-nisms to handle resource heterogeneity and provide an equi-table FL framework for all clients. First, ScaleFL adaptively scales down the DNN model along width and depth dimen-sions by leveraging early exits to ﬁnd the best-ﬁt models for resource-aware local training on distributed clients. In this way, ScaleFL provides an efﬁcient balance of preserving basic and complex features in local model splits with vari-ous sizes for joint training while enabling fast inference for model deployment. Second, ScaleFL utilizes self-distillation among exit predictions during training to improve aggre-gation through knowledge transfer among subnetworks. We conduct extensive experiments on benchmark CV (CIFAR-10/100, ImageNet) and NLP datasets (SST-2, AgNews). We demonstrate that ScaleFL outperforms existing representa-tive heterogeneous FL approaches in terms of global/local model performance and provides inference efﬁciency, with up to 2x latency and 4x model size reduction with negligible performance drop below 2%. 1.

Introduction
Mobile and Internet-of-Things (IoT) devices are the pri-mary computing sources for most daily life tasks and they are becoming increasingly essential for billions of users worldwide (12; 18). These devices generate an unprece-dented amount of data, which can be used to optimize ser-vices and improve user experience. Since the data is huge and mostly private, communicating, storing and organizing it in a central server poses serious privacy risks and brings logistic concerns (12). Federated learning (FL) emerged as a machine learning paradigm for this scenario, where stor-ing the data and training the model in a central server is not feasible. In FL, instead of centralizing the data, the model is distributed to clients for local training and the central server aggregates the local updates received from clients (22).
Existing FL algorithms such as FedAVG (22), SCAF-FOLD (13) and FedOpt (26) rely on the assumption that every participating client has similar resources and can lo-cally execute the same model. However, in most real-life applications, the computation resources tend to differ sig-niﬁcantly across clients (5; 19). This heterogeneity prevents clients with insufﬁcient resources to participate in certain
FL tasks that require large models. Although existing gra-dient compression (8) or model pruning techniques (7) may be applied to reduce the cost at the expense of small accu-racy loss, these methods are not ﬂexible enough to meet di-verse constraint scenarios (computational, storage, network etc.) based on the heterogeneous resources of edge clients.
We argue that FL should promote equitable AI practice by supporting a resource-adaptive learning framework that can scale to heterogeneous clients with limited capacity.
To this end, we present ScaleFL, a scalable and equitable
FL framework. By design, ScaleFL has two novel features.
First, ScaleFL can adaptively scale down the global model along the width and depth dimensions based on the com-putational resources of participating clients. The downscal-ing procedure in ScaleFL is inspired by EfﬁcientNet (28), which demonstrates the importance of balancing the size of different dimensions while scaling a neural network.
Since a deeper model is more capable of extracting higher-order, complex features while a wider model has access to a larger variety of lower-order, basic features, performing model size reduction across one dimension causes unbal-ance in terms of the learning capabilities of the resulting model. This motivates the design of ScaleFL that uniformly scales down the global model on both dimensions to pro-vide the balance of preserving access to both complex and basic features as efﬁciently as possible. To perform split-ting along the depth dimension, ScaleFL injects early exit
classiﬁers (29) to the global model at certain layers based on the model architecture and computational constraints at each complexity level. As a result, with ScaleFL, not only the global model achieves better performance compared to baseline FL approaches (22) and existing algorithms (5; 19) but also the local models at different complexity levels per-form signiﬁcantly better in case the clients are resource-constrained at inference time.
The second novelty of ScaleFL is providing effective aggregation mechanisms for combining local model up-dates from heterogeneous participating clients, and aug-menting self-distillation for effective model update aggre-gation. During the training of local models, we perform self-distillation among the exit predictions to improve the knowledge transfer among subnetworks. Knowledge distil-lation enables transferring knowledge from a large (teacher) network to a smaller (student) network by training the stu-dent network on teacher predictions as soft labels (10). Self-distillation is a form of knowledge distillation, where the same network is used as both teacher and the student to im-prove performance during training, especially for multi-exit models (16; 24; 25; 33). In particular, we optimize these models with the additional objective of minimizing the KL divergence among the early exit (student) and ﬁnal (teacher) predictions, which provides effective aggregation through increasing knowledge ﬂow among local models. This pro-cedure is an integrated component of the local optimization procedure and does not introduce any additional computa-tional overhead as in standard knowledge distillation.
In summary, our main contributions are as follows: (1)
We introduce a novel FL approach ScaleFL, which performs resource-adaptive 2-D model downscaling using early exits to handle system heterogeneity. Our method preserves the balance of basic and complex features at different complex-ity levels and enables efﬁcient local models for resource-constrained clients. (2) We further enhance ScaleFL for ef-fective integration of local training model updates by utiliz-ing self-distillation among the exit predictions during local training to increase knowledge ﬂow among subnetworks by minimizing the KL divergence among the early exit (stu-dent) and ﬁnal (teacher) predictions. (3) We validate the advantages of ScaleFL in both model production quality for FL and model inference speedup for model deployment at edge clients. With extensive experiments on three vision benchmarks and two NLP benchmarks, we ﬁrst demonstrate the signiﬁcant improvements of ScaleFL in terms of global model performance on image/text classiﬁcation tasks and various data heterogeneity settings, compared to recent ap-proaches for FL with system heterogeneity. We then analyze the inference performance of local models and show that lo-cal models can provide up to 2x inference latency reduction and 4x model size reduction, with negligible performance drop under 2%. 2.