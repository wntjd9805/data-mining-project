Abstract
Facial micro-expressions (MEs) refer to brief sponta-neous facial movements that can reveal a personâ€™s gen-uine emotion. They are valuable in lie detection, crimi-nal analysis, and other areas. While deep learning-based
ME recognition (MER) methods achieved impressive suc-cess, these methods typically require pre-processing using conventional optical flow-based methods to extract facial motions as inputs. To overcome this limitation, we pro-posed a novel MER framework using self-supervised learn-ing to extract facial motion for ME (SelfME). To the best of our knowledge, this is the first work using an automatically self-learned motion technique for MER. However, the self-supervised motion learning method might suffer from ignor-ing symmetrical facial actions on the left and right sides of faces when extracting fine features. To address this issue, we developed a symmetric contrastive vision transformer (SCViT) to constrain the learning of similar facial action features for the left and right parts of faces. Experiments were conducted on two benchmark datasets showing that our method achieved state-of-the-art performance, and ab-lation studies demonstrated the effectiveness of our method. 1.

Introduction
Personality and emotions are crucial aspects of human cognition and play a vital role in human understanding and human-computer interaction [20]. Facial expressions provide an important cue for understanding human emo-tions [3]. According to neuropsychological research, micro-expressions (MEs) are revealed when voluntary and invol-untary expressions collide [8]. As a slight leakage of ex-pression, MEs are subtle in terms of intensities, brief in du-ration (occur less than 0.5 seconds), and affect small facial areas [3]. Because MEs are hard to be controlled, they are more likely to reflect genuine human emotions, and thus have been implemented in various fields, such as national security, political psychology, and medical care [41]. De-spite the fact that MEs are valuable, their unique character-istics bring multifarious challenges for ME analysis.
Figure 1. MEs may be imperceptible to the naked eye, but the mo-tion between the onset (the moment when the facial action begins to grow stronger) and the apex (the moment when the facial action reaches its maximum intensity) makes them readily observable.
SelfME learns this motion automatically.
The task of ME recognition (MER) is to classify MEs by the type of emotion [3]. Due to the small sample size of datasets, almost all methods for MER have used hand-crafted features, which can be non-optical flow-based [2, 44, 50] or optical flow-based [25, 32, 42]. The extracted features were then fed into either traditional classification models [40, 43, 47], or deep learning-based classification models [12, 24, 51, 52]. Although recently proposed meth-ods claimed they are deep learning-based methods, the ones with the highest performance often rely on traditional op-tical flow [10, 49] between the onset and apex frames as inputs. These optical flow methods are computed in a com-plex manner, and a number of researchers even proposed approaches for further processing the optical flow features prior to inputting them into the networks to improve their performance. This type of pipeline may hinder the develop-ment of MER in the deep learning era.
To overcome this limitation, we proposed a novel MER framework using self-supervised learning to extract facial motion for ME (SelfME) in this work. To the best of our knowledge, this is the first work with an automatically self-learned motion technique for MER. We visualized the learned motion by SelfME in Fig. 1. The symmetry of fa-cial actions is important in MER, as spontaneous expres-sions are more symmetric than posed ones, or have inten-sity differences between left and right faces that are negligi-ble [9,13]. However, the learned motion may suffer from ig-noring symmetrical facial actions on the left and right sides of the face when extracting fine features. To address this issue, we developed a symmetric contrastive vision trans-former (SCViT) to constrain the learning of similar facial action features for the left and right parts of the faces, mit-igating asymmetry information irrelevant to MER. Experi-ments were conducted on two benchmark datasets, showing that our method achieved state-of-the-art performance. In addition, the ablation studies demonstrated the effectiveness of our method.
The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes our method-ology. Section 4 presents the experiments with analysis and discussions. Limitations and ethical concerns are discussed in Section 5. Section 6 concludes the paper. 2.