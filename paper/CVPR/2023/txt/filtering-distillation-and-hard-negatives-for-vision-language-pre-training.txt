Abstract
Vision-language models trained with contrastive learn-ing on large-scale noisy data are becoming increasingly popular for zero-shot recognition problems. In this paper we improve the following three aspects of the contrastive pre-training pipeline: dataset noise, model initialization and the training objective. First, we propose a straightfor-ward ﬁltering strategy titled Complexity, Action, and Text-spotting (CAT) that signiﬁcantly reduces dataset size, while achieving improved performance across zero-shot vision-language tasks. Next, we propose an approach titled Con-cept Distillation to leverage strong unimodal representa-tions for contrastive training that does not increase train-ing complexity while outperforming prior work. Finally, we modify the traditional contrastive alignment objective, and propose an importance-sampling approach to up-sample the importance of hard-negatives without adding additional complexity. On an extensive zero-shot benchmark of 29 tasks, our Distilled and Hard-negative Training (DiHT) ap-proach improves on 20 tasks compared to the baseline. Fur-thermore, for few-shot linear probing, we propose a novel approach that bridges the gap between zero-shot and few-shot performance, substantially improving over prior work.
Models are available at github.com/facebookresearch/diht. 1.

Introduction
An increasingly popular paradigm in multimodal learn-ing is contrastive pre-training [11, 28, 41, 43, 62, 76, 85, 87], which involves training multimodal models on very large-scale noisy datasets of image-text pairs sourced from the web. It has been shown to be incredibly effective for a vari-ety of vision-language tasks without any task-speciﬁc ﬁne-tuning (i.e., zero-shot), such as image classiﬁcation [65], text and image retrieval [45, 59], visual question answer-ing [21], among several others.
In this paper, we study the problem of contrastive pre-training for dual-encoder ar-chitectures [62] with the objective of improving image-text alignment for zero-shot tasks. We revisit three important aspects of the contrastive pre-training pipeline – noise in
*Equal contribution. †Work done at Meta AI. ‡Research Lead.
ImageNet1K
COCO (T2I)
Flickr (T2I) 1
@ y c a r u c c
A 80 75 70 65 60 50 45 1
@ 40 l l a c e
R 35 30
DiHT
CLIP
B/32
B/16 L/14
Model Complexity
@336
B/32
B/16 L/14
Model Complexity
@336 79 74 1
@ 69 l l a c e
R 64 59
B/32
B/16 L/14
Model Complexity
@336
Figure 1. DiHT trained on 438M LAION-CAT samples vs.
CLIP [62] trained on 400M OpenAI samples. datasets, model initialization, and contrastive training, and present strategies that signiﬁcantly improve model perfor-mance on a variety of zero-shot benchmarks, see Figure 1.
Most image-text datasets are noisy and poorly-aligned.
Few recent efforts [27] have tried to clean the noise by ﬁl-tering samples based on alignment scores from an existing model like CLIP [62]. However, this approach is limited by the biases and ﬂaws of the model itself. On the other hand, momentum-based approaches [41] to reduce noise are in-feasible for large-scale training due to their increased com-pute and memory requirements. To this end, we provide a scalable and effective approach titled Complexity, Action and Text-spotting (CAT) ﬁltering. CAT is a ﬁltering strat-egy to select only informative text-image pairs from noisy web-scale datasets. We show that training on a CAT-ﬁltered version of large-scale noisy datasets such as LAION [66] can provide up to 12% relative improvements across vision-language tasks despite removing almost 80% of the training data, see Section 4.2 and Table 1 for more details.
A common strategy [58, 89] to further improve multi-modal training is to warm-start it with image and text mod-els pre-trained at large scale on their respective modali-ties. However, due to the increased noise in image-text data, ﬁne-tuning the entire model undermines the beneﬁts of the warm-start. One can alternatively use model freezing strategies like locked-image tuning [89], but they are un-able to adapt to the complex queries present in multimodal problems (e.g., cross-modal retrieval) and the models per-form poorly on retrieval benchmarks (see Section 4.2). We
propose an entirely different approach, concept distillation (CD), to leverage strong pre-trained vision models. The key idea behind concept distillation is to train a linear classiﬁer on the image encoder to predict the distilled concepts from a pre-trained teacher model, inspired by results in weakly-supervised large-scale classiﬁcation [49, 71].
Finally, we revisit the training objective: almost all prior work has utilized noise-contrastive estimation via the In-foNCE loss [55], shortcomings have been identiﬁed in the standard InfoNCE formulation [12, 30]. We demonstrate that by using a model-based importance sampling technique to emphasize harder negatives, one can obtain substantial improvements in performance.
A summary of our pipeline is available in Figure 2.
Our combined approach obtains signiﬁcant improvements over the baseline for dual-encoder architectures on an elab-orate benchmark of 29 tasks. Speciﬁcally, with the ViT-B/16 [17] architecture, we improve zero-shot performance on 20 out of 29 tasks, over CLIP training on the LAION-2B dataset [27, 66], despite training on a subset that is 80% smaller, see Figure 4. Furthermore, we demonstrate that even when trained with the smaller (but relatively less noisy) pretraining dataset PMD, our performance is better on 28 out of 29 tasks than CLIP trained on the same data, often with a large margin, see Figure 5.
Additionally, we present a simple yet effective approach to maintain the performance continuum as one moves from zero-shot to few-shot learning in the low data regime. Prior work [62] has shown a substantial drop in performance as one moves from zero-shot to k-shot learning, which is undesirable for practical scenarios. We propose an alter-nate linear probing approach that initializes the linear clas-siﬁer with zero-shot text prompts and ensures that ﬁnal weights do not drift away too much via projected gradient descent [5]. On ImageNet1K, we show huge improvements over prior work for small k values. For example, our ap-proach improves 5-shot top-1 accuracy by an absolute mar-gin of 7% (see Figure 6) compared to the baseline strategy of linear probing with a random initialization. 2.