Abstract
Autoregressive transformers have shown remarkable success in video generation. However, the transformers are prohibited from directly learning the long-term dependency in videos due to the quadratic complexity of self-attention, and inherently suffering from slow inference time and er-ror propagation due to the autoregressive process. In this paper, we propose Memory-efficient Bidirectional Trans-former (MeBT) for end-to-end learning of long-term depen-dency in videos and fast inference. Based on recent ad-vances in bidirectional transformers, our method learns to decode the entire spatio-temporal volume of a video in par-allel from partially observed patches. The proposed trans-former achieves a linear time complexity in both encoding and decoding, by projecting observable context tokens into a fixed number of latent tokens and conditioning them to decode the masked tokens through the cross-attention. Em-powered by linear complexity and bidirectional modeling, our method demonstrates significant improvement over the autoregressive transformers for generating moderately long videos in both quality and speed. Videos and code are avail-able at https://sites.google.com/view/mebt-cvpr2023. 1.

Introduction
Modeling the generative process of videos is an impor-tant yet challenging problem. Compared to images, gener-ating convincing videos requires not only producing high-quality frames but also maintaining their semantic struc-tures and dynamics coherently over long timescale [11, 16, 27, 36, 38, 54].
Recently, autoregressive transformers on discrete repre-sentation of videos have shown promising generative mod-eling performances [11,32,51,53]. Such methods generally involve two stages, where the video frames are first turned into discrete tokens through vector quantization, and then their sequential dynamics are modeled by an autoregressive transformer. Powered by the flexibility of discrete distri-butions and the expressiveness of transformer architecture, these methods demonstrate impressive results in learning and synthesizing high-fidelity videos.
However, autoregressive transformers for videos suffer from critical scaling issues in both training and inference.
During training, due to the quadratic cost of self-attention, the transformers are forced to learn the joint distribution of frames entirely from short videos (e.g., 16 frames [11, 53]) and cannot directly learn the statistical dependencies of frames over long timescales. During inference, the mod-els are challenged by two issues of autoregressive gener-ative process – its serial process significantly slows down the inference speed, and perhaps more importantly, autore-gressive prediction is prone to error propagation where the prediction error of the frames accumulates over time.
To (partially) address the issues, prior works proposed improved transformers for generative modeling of videos, (a) Employing which are categorized as the following: sparse attention to improve scaling during training [12, 16, 51], (b) Hierarchical approaches that employ separate mod-els in different frame rates to generate long videos with a smaller computation budget [11, 16], and (c) Remov-ing autoregression by formulating the generative process as masked token prediction and training a bidirectional trans-former [12, 13]. While each approach is effective in ad-dressing specific limitations in autoregressive transformers, none of them provides comprehensive solutions to afore-mentioned problems – (a, b) still inherits the problems in autoregressive inference and cannot leverage the long-term dependency by design due to the local attention window, and (c) is not appropriate to learn long-range dependency due to the quadratic computation cost. We believe that an alternative that jointly resolves all the issues would provide a promising approach towards powerful and efficient video generative modeling with transformers.
In this work, we propose an efficient transformer for video synthesis that can fully leverage the long-range de-pendency of video frames during training, while being able to achieve fast generation and robustness to error propaga-tion. We achieve the former with a linear complexity ar-chitecture that still imposes dense dependencies across all timesteps, and the latter by removing autoregressive seri-alization through masked generation with a bidirectional transformer. While conceptually simple, we show that ef-ficient dense architecture and masked generation are highly complementary, and when combined together, lead to sub-stantial improvements in modeling longer videos compared to previous works both in training and inference. The con-tributions of this paper are as follows:
• We propose Memory-efficient Bidirectional Trans-former (MeBT) for generative modeling of video. Un-like prior methods, MeBT can directly learn long-range dependency from training videos while enjoying fast inference and robustness in error propagation.
• To train MeBT for moderately long videos, we propose a simple yet effective curriculum learning that guides the model to learn short- to long-term dependencies gradually over training.
• We evaluate MeBT on three challenging real-world video datasets. MeBT achieves a performance compet-itive to state-of-the-arts for short videos of 16 frames, and outperforms all for long videos of 128 frames while being considerably more efficient in memory and computation during training and inference. 2.