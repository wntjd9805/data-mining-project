Abstract
Talking head generation aims to generate faces that maintain the identity information of the source image and imitate the motion of the driving image. Most pioneering methods rely primarily on 2D representations and thus will inevitably suffer from face distortion when large head ro-tations are encountered. Recent works instead employ ex-plicit 3D structural representations or implicit neural ren-dering to improve performance under large pose changes.
Nevertheless, the fidelity of identity and expression is not so desirable, especially for novel-view synthesis. In this paper, we propose HiDe-NeRF, which achieves high-fidelity and free-view talking-head synthesis. Drawing on the recently proposed Deformable Neural Radiance Fields, HiDe-NeRF represents the 3D dynamic scene into a canonical appear-ance field and an implicit deformation field, where the for-mer comprises the canonical source face and the latter models the driving pose and expression. In particular, we improve fidelity from two aspects: (i) to enhance identity ex-pressiveness, we design a generalized appearance module that leverages multi-scale volume features to preserve face shape and details; (ii) to improve expression preciseness, we propose a lightweight deformation module that explic-itly decouples the pose and expression to enable precise ex-pression modeling. Extensive experiments demonstrate that our proposed approach can generate better results than pre-vious works. Project page: https://www.waytron. net/hidenerf/ 1.

Introduction
Talking-head synthesis aims to preserve the identity in-formation of the source image and imitate the motion of the driving image. Synthesizing talking faces of a given person driven by other speaker is of great importance to various ap-plications, such as film production, virtual reality, and digi-tal human. Existing talking head methods are not capable of
* denotes equal contribution
† denotes the corresponding authors
generating high-fidelity results, they cannot precisely pre-serve the source identity or mimic the driving expression.
Most pioneering approaches [11, 17, 24, 35, 37, 44, 44] learn source-to-driving motion to warp the source face to the desired pose and expression. According to the warping types, previous works can be roughly divided into: 2D warping-based methods, mesh-based methods, and neural rendering-based methods. 2D warping-based methods [17, 35, 37] warps source feature based on the motion field estimated from the sparse keypoints. However, these methods encounter the collapse of facial structure and expression under large head rotations. Moreover, they cannot fully disentangle the motion with identity informa-tion of the driving image, resulting in a misguided face shape. Mesh-based methods [13] are proposed to tackle the problem of facial collapse by using 3D Morphable Models (3DMM) [4, 30] to explicitly model the geometry. Limited by non-rigid deformation modeling ability of 3DMM, such implementation leads to rough and unnatural facial expressions. Besides, it ignores the influence of vertex offset on face shape, resulting in low identity fidelity. With the superior capability in multi-view image synthesis of
Neural Radiance Fields (NeRF) [25], a concurrent work named FNeVR [47] takes the merits of 2D warping and 3D neural rendering. It learns a 2D motion field to warp the source face and utilizes volume rendering to refine the warped features to obtain final results. Therefore, it inherits the same problem as other warping-based methods.
To address above issues and improve the fidelity of talking head synthesis, we propose High-fideltiy and
Deformable NeRF, dubbed HiDe-NeRF. Drawing on the idea of recently emerged Deformable NeRF [1, 27, 28].
HiDe-NeRF represents the 3D dynamic scene into a canon-ical appearance field and an implicit deformation field. The former is a radiance field of the source face in canonical pose, and the latter learns the backward deformation for each observed point to shift them into the canonical field and retrieve their volume features for neural rendering. On this basis, we devise a Multi-scale Generalized Appear-ance module (MGA) to ensure identity expressiveness and a
Lightweight Expression-aware Deformation module (LED) to improve expression preciseness.
To elaborate, MGA encodes the source image into multi-scale canonical volume features, which integrate high-level face shape information and low-level facial details, for bet-ter identity preservation. We employ the tri-plane [7, 31] as volume feature representation in this work for two reasons: (i) it enables generalization across 3D representations; (ii) it is fast and scales efficiently with resolution, facilitating us to build hierarchical feature structures. Moreover, we modify the ill-posed tri-plane representation by integrating a camera-to-world feature transformation, so that we can extract the planes from the source image with full control of identity. This distinguishes our model from those identity-uncontrollable approaches [3, 7] that generate the planes from noise with StyleGAN2-based generators [20]. No-tably,the MGA enables our proposed HiDe-NeRF to be im-plemented in a subject-agnostic manner, breaking the limi-tation that existing Deformable NeRFs can only be trained for a specific subject.
The deformations in talking-head scenes could be de-composed into the global pose and local expression de-formation. The former is rigid and easy to handle, while the latter is non-rigid and difficult to model. Existing De-formable NeRFs predict them as a whole, hence failing to capture precise expression. Instead, our proposed LED could explicitly decouple the expression and pose in defor-mation prediction, thus significantly improving the expres-sion fidelity. Specifically, it uses a pose-agnostic expression encoder and a position encoder to obtain the latent expres-sion embeddings and latent position embeddings, where the former models the expression independently and the latter encodes positions of points sampled from rays under arbi-trary observation views. Then, a deformation decoder takes the combination of two latent embeddings as input and out-puts point-wise deformation. In this way, our work achieves precise expression manipulation and maintains expression consistency for free-view rendering (as shown in Fig. 1).
To summarize, the contributions of our approach are:
• Firstly, we introduce the HiDe-NeRF for high-fidelity and free-view talking head synthesis. To the best of our knowledge, HiDe-NeRF is the first one-shot and subject-agnostic Deformable Neural Radiance Fields.
• Secondly, we propose the Multi-scale Generalized
Appearance module (MGA) and the Lightweight
Expression-aware Deformation module (LED) to sig-nificantly improve the fidelity of identity and expres-sion in talking-head synthesis.
• Lastly, extensive experiments demonstrate that our proposed approach can generate more realistic results than state-of-the-art in terms of capturing the driving motion and preserving the source identity information. 2.