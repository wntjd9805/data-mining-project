Abstract
ViT-Tiny ViT-Small
ViT-Base
ViT-Large
The real-world data tends to be heavily imbalanced and severely skew the data-driven deep neural networks, which makes Long-Tailed Recognition (LTR) a massive challeng-ing task. Existing LTR methods seldom train Vision Trans-formers (ViTs) with Long-Tailed (LT) data, while the off-the-shelf pretrain weight of ViTs always leads to unfair comparisons. In this paper, we systematically investigate the ViTs’ performance in LTR and propose LiVT to train
ViTs from scratch only with LT data. With the observa-tion that ViTs suffer more severe LTR problems, we con-duct Masked Generative Pretraining (MGP) to learn gener-alized features. With ample and solid evidence, we show that MGP is more robust than supervised manners. Al-though Binary Cross Entropy (BCE) loss performs well with
ViTs, it struggles on the LTR tasks. We further propose the balanced BCE to ameliorate it with strong theoreti-cal groundings. Specially, we derive the unbiased exten-sion of Sigmoid and compensate extra logit margins for de-ploying it. Our Bal-BCE contributes to the quick conver-gence of ViTs in just a few epochs. Extensive experiments demonstrate that with MGP and Bal-BCE, LiVT success-fully trains ViTs well without any additional data and out-performs comparable state-of-the-art methods significantly, e.g., our ViT-B achieves 81.0% Top-1 accuracy in iNatural-ist 2018 without bells and whistles. Code is available at https://github.com/XuZhengzhuo/LiVT. 1.

Introduction
With the vast success in the computer vision field, Vision
Transformers (ViTs) [15, 43] get increasingly popular and have been widely used in visual recognition [15], detec-tion [5], and video analysis [16]. These models are heavily dependent on large-scale and balanced data to avoid overfit-ting [39,52,82]. However, real-world data usually confronts severe class-imbalance problems, i.e., most labels (tail) are associated with limited instances while a few categories (head) occupy dominant samples. The models simply clas-sify images into head classes for lower error because the
NCL
R50×3
R50
RIDE-4E
R50
[13]
[34]
[61]
[55]
[18]
Figure 1. Top-1 Acc v.s. Model Size on ImageNet-LT dataset.
We choose the Tiny / Small / Base / Large ViT and multi-expert approaches. R50 represents the ResNet50 model. ViT-Base gets lower Acc than ResNet50 when trained in a supervised manner. head always overwhelms tail ones in LTR. The data paucity also results in the model overfitting on the tail with unac-cepted generalization. The aforementioned problems make
Long Tail Recognization (LTR) a challenging task.
Numerous papers [4,13,22,34,35,44,70] handle the LTR problem with traditional supervised cross-entropy learning based on ResNet [20] or its derivatives [68]. Some meth-ods use ViTs with pretrained weights on ImageNet [52] (or larger datasets), which leads to unfair comparisons with ad-ditional data, e.g. on ImageNet-LT (a subset of ImageNet-1K) benchmark. Moreover, there are still limited explo-rations on the utilization of Long-Tailed (LT) data to train
ViTs effectively. Therefore, in this paper, we try to train
ViTs from scratch with LT data. We observe that it is par-ticularly difficult to train ViT with LT labels’ supervision.
As Tab. 1 shows, ViTs degrade heavily when training data become skewed. ViT-B is much worse than ResNet50 with the same CE training manner (c.f. Fig. 1). One reason-able explanation is that ViTs require longer training to learn the inductive bias, while CNNs offer the built-in translation invariance implicitly. Yet another one lies in the label sta-tistical bias in the LTR datasets, which confuses models to make predictions with an inherent bias to the head [12, 47].
The well-trained ViTs have to overcome the above plights simultaneously to avoid falling into dilemmas.
Inspired by decoupling [29], many methods [9, 12, 60, 80, 83] attempt to enhance feature extraction in supervised manners like mixup [74] / remix [9], or Self-Supervised
Learning (SSL) like Contrastive Learning (CL) [7, 19]. Liu et al. [41] claim that SSL representations are more robust to class imbalance than supervised ones, which inspires us to train ViTs with SSL. However, CL is quite challenging for extensive memory requisition and converge difficulties [8], where more explorations are required to work well with
ViTs in LTR. In contrast, we propose to Learn imbalanced data with ViTs (LiVT) by Masked Generative Pretraining (MGP) and Balanced Fine Tuning (BFT).
Firstly, LiVT adopts MGP to enhance ViTs’ feature ex-traction, which has been proven effective on BeiT [2] and
MAE [18].
It reconstructs the masked region of images with an extra lightweight decoder. We observe that MGP is stable with ViTs and robust enough to LT data with em-pirical evidence. Despite the label distribution, the compa-rable number of training images will bring similar feature extraction ability, which greatly alleviates the toxic effect of LT labels [26]. Meanwhile, the training is accelerated by masked tokens with acceptable memory requisition.
Secondly, LiVT trains the downstream head with rebal-ancing strategies to utilize annotation information, which is consistent with [29, 35, 80]. Generally, Binary Cross-Entropy (BCE) loss performs better than Cross-Entropy loss when collaborating with ViTs [55]. However, it fails to catch up with widely adopted Balanced Cross-Entropy (Bal-CE) loss and shows severe training instability in LTR.
We propose the Balanced BCE (Bal-BCE) loss to revise the mismatch margins given by Bal-CE. Detailed and solid the-oretical derivations are provided from Bayesian theory. Our
Bal-BCE ameliorates BCE by a large margin and achieves state-of-the-art (SOTA) performance with ViTs.
Extensive experiments show that LiVT learns LT data more efficiently and outperforms vanilla ViT [15], DeiT
III [55], and MAE [18] remarkably. As detailed compar-isons in Fig. 1, LiVT achieves SOTA on ImageNet-LT with affordable parameters, despite that ImageNet-LT is a rel-atively small dataset for ViTs. The ViT-Small [55] also achieves outstanding performance compared to ResNet50.
Our key contributions are summarized as follows.
• To our best knowledge, we are the first to investigate training ViTs from scratch with LT data systematically.
• We pinpoint that the masked generative pretraining is robust to LT data, which avoids the toxic influence of imbalanced labels on feature learning.
• With a solid theoretical grounding, we propose the bal-anced version of BCE loss (Bal-BCE), which improves the vanilla BCE by a large margin in LTR.
• We propose LiVT recipe to train ViTs from scratch, and the performance of LiVT achieves state-of-the-art across various benchmarks for long-tailed recognition.
Table 1. Top-1 accuracy (%) of different recipes to train ViT-B-16 from scratch on ImageNet-LT/BAL. All perform much worse on
LT than BAL. See descriptions of LT & BAL in section 5.1.
Dataset
ImageNet-BAL
ImageNet-LT
ViT 38.7 31.6 2.