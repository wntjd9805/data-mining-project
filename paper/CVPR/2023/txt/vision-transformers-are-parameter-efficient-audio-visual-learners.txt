Abstract
Vision transformers (ViTs) have achieved impressive re-sults on various computer vision tasks in the last several years. In this work, we study the capability of frozen ViTs, pretrained only on visual data, to generalize to audio-visual data without ﬁnetuning any of its original parameters. To do so, we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT. To efﬁciently fuse visual and audio cues, our LAVISH adapter uses a small set of latent to-kens, which form an attention bottleneck, thus, eliminating the quadratic cost of standard cross-attention. Compared to the existing modality-speciﬁc audio-visual methods, our approach achieves competitive or even better performance on various audio-visual tasks while using fewer tunable pa-rameters and without relying on costly audio pretraining or external audio encoders. Our code is available at https:
//genjib.github.io/project_page/LAVISH/ 1.

Introduction
Humans can seamlessly process audio-visual cues and use them in unison to learn associations between auditory and visual signals (e.g., the sound of barking and the visual concept of dog). In contrast, most modern computational audio-visual models [34, 38, 79, 80, 82, 84, 92] study each of these modalities in isolation, which leads to individually-tailored modality-speciﬁc models. While such modality-speciﬁc approaches often achieve state-of-the-art results on various audio-visual benchmarks, they also have several major shortcomings. First, optimizing and training mod-els for a speciﬁc modality (e.g., audio or video) requires signiﬁcant research effort and computing power. For exam-ple, training large-scale models for audio and video requires more than 2,000 and 5,000 V100 hours respectively [10,86], which is not feasible for many smaller research labs. Ad-ditionally, since modern visual and audio models are be-coming larger, it can be quite costly to use separate back-bone networks for processing each modality. For instance, the audio-visual MBT-Large model [60], built using sepa-Figure 1. We investigate whether frozen vision transformers (ViTs) pretrained only on visual data can generalize to audio data for complex audio-visual understanding tasks. For this pur-pose, we introduce a latent audio-visual hybrid adapter (LAVISH), which is inserted into every layer of a frozen ViT model. By tun-ing only a small number of additional parameters we can enable a pretrained ViT to efﬁciently (i) adapt to the audio data, and (ii) fuse relevant cues across audio and visual modalities. rate audio and visual encoders, requires more than 48 GB of GPU memory, which is only available on the costly, high-end GPU servers such as A100. Lastly, the modality-speciﬁc approaches are only trained on individual modali-ties and then typically combined via late fusion. As a result, such models cannot beneﬁt from cross-modal cues in the early layers, which often leads to suboptimal performance on audio-visual tasks requiring joint audio-visual reasoning.
The recent emergence of transformer models [2, 21, 24, 40, 60] has propelled research in modality-agnostic archi-tectures for multi-modal understanding. In particular, the generality of the transformer architecture [16] makes it easy to apply these models to different modalities without any modality-speciﬁc adaptations. This property is well illus-trated by the fact that transformers [16] currently deﬁne state-of-the-art across many domains, including natural lan-guage processing (NLP) [8, 15, 41, 42, 51, 59, 64, 91], com-puter vision (CV) [6, 10, 20], audio analysis [22, 23, 86], speech processing [7, 73, 77]. Such an architecture con-vergence across different domains/modalities inspired sev-eral recent works to investigate the cross-modal general-ization of pretrained transformers [40, 50, 62, 75]. How-ever, most of them are either focused on language mod-els [47, 50, 75], or study close-domain transfer (e.g., image
→ video) [20, 21, 62].
In this work, we focus on the cross-modal generalization of pretrained vision transformers (ViT) [16] to the audio-visual data. Our main inspiration for this study stems from the fact that audio can be represented as a 2D spectrogram, which summarizes 1D raw audio signal into a 2D structure akin to audio images. Prior work has shown that vision ar-chitectures (e.g., CNNs [12, 26] or ViTs [23, 77]) can be used to process such audio images. However, most prior methods use these architectures for large-scale audio rep-resentation learning. Instead of pretraining ViTs on large-scale audio data, we hypothesize that the ViTs pretrained on images can simultaneously encode representations that are useful for both images and audio, making them useful for audio-visual tasks without large-scale audio pretraining.
To investigate this hypothesis, we propose a latent audio-visual hybrid (LAVISH) adapter that directly adapts frozen
ViTs, pretrained only on images, to audio-visual tasks by adding a small number of trainable parameters for audio specialization and audio-visual fusion. Such a scheme al-lows us to apply frozen ViTs to audio-visual data without updating the original ViT parameters but only the param-eters of our proposed LAVISH modules, which we insert into every layer of a frozen ViT. For an efﬁcient cross-modal fusion within the LAVISH module, we use a small set of latent tokens to ﬁrst compress the information from all modality-speciﬁc tokens (e.g., either audio or video) and then apply cross-attention between the latent tokens and all the tokens of another modality (e.g., either video or audio).
Such a scheme allows us to eliminate the quadratic cost of standard cross-attention. Furthermore, to allow information transfer between audio-to-video and, conversely, video-to-audio, we adopt a bi-directional LAVISH scheme, which enables learning a better audio-visual representation.
In our experimental section, we demonstrate that by keeping all the original ViT parameters frozen and updat-ing only a small set of newly added parameters, the frozen
ViTs, pretrained only on image data, learn to solve com-plex audio-visual understanding tasks requiring a joint un-derstanding of audio and visual contents. In particular, com-pared to the state-of-the-art modality-speciﬁc audio-visual approaches, our method achieves competitive or even bet-ter results on the tasks of audio-visual event localization, audio-visual segmentation, and audio-visual question an-swering while using a smaller number of tunable parame-ters, and without relying on a separate pre-trained audio en-coder (e.g., VGGish [26], AST [23], etc.), or costly large-scale audio pretraining. We also show that our proposed latent audio-visual hybrid adapter (LAVISH) is more effec-tive and efﬁcient than the standard adapter schemes [27]. 2.