Abstract
Fashion representation learning involves the analysis and understanding of various visual elements at different granularities and the interactions among them. Existing works often learn fine-grained fashion representations at the attribute level without considering their relationships and inter-dependencies across different classes.
In this work, we propose to learn an attribute and class- specific fashion representation duet to better model such attribute relationships and inter-dependencies by leveraging prior knowledge about the taxonomy of fashion attributes and classes. Through two sub-networks for the attributes and classes, respectively, our proposed an embedding network progressively learns and refines the visual representation of a fashion image to improve its robustness for fashion re-trieval. A multi-granularity loss consisting of attribute-level and class-level losses is proposed to introduce appropri-ate inductive bias to learn across different granularities of the fashion representations. Experimental results on three benchmark datasets demonstrate the effectiveness of our method, which outperforms the state-of-the-art methods by a large margin. 1.

Introduction
Fashion products have become one of the most con-sumed products in online shopping. Unlike other types of products, fashion products are usually rich in visual ele-ments at different levels of granularity. For instance, besides the overall visual appearance, a fashion product can be de-scribed by a set of attributes, such as “shape”, “color” and
“style”, which focus on different aspects of the visual rep-resentation. Each attribute can be further categorized into various classes. For example, “fit”, “flare” and “pencil” are different classes under attribute “shape” (Fig. 1). There-fore, modeling fashion representation in different granu-larities is essential for online shopping and other down-stream applications, especially those that require analysis
Figure 1. Left: existing fine-grained representation learning meth-ods often learn attribute-specific representations for fashion prod-ucts, thus may not be able to discern the two dresses that have dif-ferent compositions of visual elements at the class level.Right: our proposed method (right) jointly learns attribute and class-specific representations. Therefore, it can discriminate between the two dresses by their class-specific representations. of subtle or fine-grained details such as attribute-based fash-ion manipulation [1, 2, 27] and retrieval [6, 14, 19, 23, 24], fashion copyright [6, 19], and fashion compatibility analy-sis [11, 15, 21, 23].
Fine-grained fashion modeling and analysis in recent years explore the attribute-specific representation learning.
The focus has recently shifted from earlier works that learn separate representations for each attribute indepen-dently [1, 2] to multi-task learning, which uses a common backbone for different attributes while tailoring the learning for each specific attribute via mechanisms such as attention masks [6,14,19,24]. Success of these attribute-specific rep-resentation learning methods for fine-grained fashion anal-ysis can be attributed to their capabilities to discriminate visual features associated with different aspects of fashion products, which learning an image-level global representa-tion finds challenging.
However, when it comes to classes, such attribute-specific representation methods face a similar challenge to the above. The reason is that due to the dynamic and aes-thetic nature of fashion products, different visual elements are often composited together to achieve certain visual ef-fects, making an attribute-level description insufficient to capture such interactions and granularity. For instance, un-der the same “shape” attribute, one may go for a dress de-sign that combines classes “fit” and “flare” for a more ca-sual look (top image, Fig. 1), but go for a different dress that combines “fit” and “pencil” for a more formal look while flattering one’s natural curves (bottom image, Fig. 1).
Therefore, an attribute-level representation is hard to dif-ferentiate the two dresses. Alternatively, one may directly learn a class-specific representation for each class under the
“shape” attribute, which, however, faces the scalability is-sue. For instance, if a fashion image is associated with N attributes and M classes per attribute, one would need to learn N × M class-specific representations.
To better discriminate fashion products with distinct de-sign considerations and model the interplay among vari-ous visual elements, we propose to leverage prior knowl-edge about fashion taxonomy to model fashion products.
We jointly learn both attribute-specific and class-specific fashion representations through a multi-attribute multi-granularity multi-label embedding network (M3-Net). M3-Net consists of two sub-networks, for attributes and classes, respectively. Different attributes share the same backbone sub-network as well as two attribute-conditional attention modules, while different classes under a given attribute share two class-conditional attention modules.
The shared backbone and conditional attention modules allow the network to better capture the inter-dependencies and shared visual statistics among the attributes and classes.
Through multi-label learning on attribute-specific represen-tations, we also improve the scalability of the proposed net-work by focusing class-specific representation learning on high likelihood classes only. Finally, a multi-granularity loss consisting of attribute-level and class-level losses is de-signed to introduce appropriate inductive bias for learning across different granularities.
In summary, our contributions are:
• We propose to model fashion products at both attribute and class levels based on fashion taxonomy to better capture the inter-dependencies of various visual ele-ments and improve the discriminative power of learned fashion representations.
• We design a multi-attribute multi-granularity multi-label network (M3-Net) to jointly learn attribute-specific and class-specific representation duet for fine-grained fashion analysis. Through two sub-networks and conditional attention modules, M3-Net is able to progressively learn discriminative representations at different granularities, with appropriate inductive bias introduced by the attribute-level and class-level losses.
• Our model outperforms state-of-the-art methods in fine-grained fashion retrieval on three benchmark datasets. The experimental results demonstrate the ef-ficacy of our proposed method. 2.