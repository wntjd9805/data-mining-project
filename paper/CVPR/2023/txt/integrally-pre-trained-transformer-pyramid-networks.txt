Abstract
In this paper, we present an integral pre-training frame-work based on masked image modeling (MIM). We advo-cate for pre-training the backbone and neck jointly so that the transfer gap between MIM and downstream recogni-tion tasks is minimal. We make two technical contributions.
First, we unify the reconstruction and recognition necks by inserting a feature pyramid into the pre-training stage.
Second, we complement mask image modeling (MIM) with masked feature modeling (MFM) that offers multi-stage su-pervision to the feature pyramid. The pre-trained mod-els, termed integrally pre-trained transformer pyramid net-works (iTPNs), serve as powerful foundation models for vi-sual recognition. In particular, the base/large-level iTPN achieves an 86.2%/87.8% top-1 accuracy on ImageNet-1K, a 53.2%/55.6% box AP on COCO object detection with 1× training schedule using Mask-RCNN, and a 54.7%/57.7% mIoU on ADE20K semantic segmentation using UPerHead – all these results set new records. Our work inspires the community to work on unifying upstream pre-training and downstream fine-tuning tasks. Code is available at github.com/sunsmarterjie/iTPN. 1.

Introduction
Recent years have witnessed two major progresses in vi-sual recognition, namely, the vision transformer architec-ture [22] as network backbone and masked image mod-eling (MIM) [3, 28, 68] for visual pre-training. Combin-ing these two techniques yields a generalized pipeline that achieves state-of-the-arts in a wide range of visual recogni-tion tasks, including image classification, object detection, and instance/semantic segmentation.
One of the key issues of the above pipeline is the transfer gap between upstream pre-training and downstream fine-*Corresponding author. base-level models iTPN-B prev. best large-level models iTPN-L prev. best
IN-1K COCO (MR, 1×) ADE20K (UP) cls. acc. det. AP seg. AP seg. mIoU 86.2 53.2 85.5 [50] 50.0 [13] 44.0 [13] 46.6 54.7 53.0 [50]
IN-1K COCO (MR, 1×) ADE20K (UP) cls. acc. det. AP seg. AP seg. mIoU 87.8 55.6 87.3 [50] 54.5 [13] 47.6 [13] 48.6 57.7 56.7 [50]
Figure 1. Top: on ImageNet-1K classification, iTPN shows sig-nificant advantages over prior methods, either only using pixel su-pervision (top) or leveraging knowledge from a pre-trained teacher (bottom, in the parentheses lies the name of teacher model). Bot-tom: iTPN surpasses previous best results in terms of recognition accuracy (%) on several important benchmarks. Legends – IN-1K:
ImageNet-1K, MR: Mask R-CNN [30], UP: UPerHead [66]. tuning. From this point of view, we argue that downstream visual recognition, especially fine-scaled recognition (e.g., detection and segmentation), requires hierarchical visual
features. However, most existing pre-training tasks (e.g.,
BEiT [3] and MAE [28]) were built upon plain vision trans-formers. Even if hierarchical vision transformers have been used (e.g., in SimMIM [68], ConvMAE [25], and Green-MIM [33]), the pre-training task only affects the backbone but leaves the neck (e.g., a feature pyramid) un-trained. This brings extra risks to downstream fine-tuning as the opti-mization starts with a randomly initialized neck which is not guaranteed to cooperate with the pre-trained backbone.
In this paper, we present an integral pre-training frame-work to alleviate the risk. We establish the baseline with
HiViT [74], an MIM-friendly hierarchical vision trans-former, and equip it with a feature pyramid. To jointly op-timize the backbone (HiViT) and neck (feature pyramid), we make two-fold technical contributions. First, we unify the upstream and downstream necks by inserting a feature pyramid into the pre-training stage (for reconstruction) and reusing the weights in the fine-tuning stage (for recogni-tion). Second, to better pre-train the feature pyramid, we propose a new masked feature modeling (MFM) task that (i) computes intermediate targets by feeding the original image into a moving-averaged backbone, and (ii) uses the output of each pyramid stage to reconstruct the intermedi-ate targets. MFM is complementary to MIM and improves the accuracy of both reconstruction and recognition. MFM can also be adapted to absorb knowledge from a pre-trained teacher (e.g., CLIP [52]) towards better performance.
The obtained models are named integrally pre-trained pyramid transformer networks (iTPNs). We evaluate them on standard visual recognition benchmarks. As highlighted in Figure 1, the iTPN series report the best known down-stream recognition accuracy. On COCO and ADE20K, iTPN largely benefits from the pre-trained feature pyra-mid. For example, the base/large-level iTPN reports a 53.2%/55.6% box AP on COCO (1× schedule, Mask
R-CNN) and a 54.7%/57.7% mIoU on ADE20K (UPer-Net), surpassing all existing methods by large margins. On
ImageNet-1K, iTPN also shows significant advantages, im-plying that the backbone itself becomes stronger during the joint optimization with neck. For example, the base/large-level iTPN reports an 86.2%/87.8% top-1 classification accuracy, beating the previous best record by 0.7%/0.5%, which is not small as it seems in such a fierce competition.
In diagnostic experiments, we show that iTPN enjoys both (i) a lower reconstruction error in MIM pre-training and (ii) a faster convergence speed in downstream fine-tuning – this validates that shrinking the transfer gap benefits both up-stream and downstream parts.
Overall, the key contribution of this paper lies in the inte-gral pre-training framework that, beyond setting new state-of-the-arts, enlightens an important future research direc-tion – unifying upstream pre-training and downstream fine-tuning to shrink the transfer gap between them. 2.