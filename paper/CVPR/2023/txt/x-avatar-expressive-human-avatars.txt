Abstract
We present X-Avatar, a novel avatar model that captures the full expressiveness of digital humans to bring about life-like experiences in telepresence, AR/VR and beyond. Our method models bodies, hands, facial expressions and ap-pearance in a holistic fashion and can be learned from ei-ther full 3D scans or RGB-D data. To achieve this, we pro-pose a part-aware learned forward skinning module that can be driven by the parameter space of SMPL-X, allow-ing for expressive animation of X-Avatars. To efficiently learn the neural shape and deformation fields, we pro-pose novel part-aware sampling and initialization strate-gies. This leads to higher fidelity results, especially for smaller body parts while maintaining efficient training de-spite increased number of articulated bones. To capture the appearance of the avatar with high-frequency details, we extend the geometry and deformation fields with a texture network that is conditioned on pose, facial expression, ge-*These authors contributed equally to this work
†Corresponding author ometry and the normals of the deformed surface. We show experimentally that our method outperforms strong base-lines both quantitatively and qualitatively on the animation task. To facilitate future research on expressive avatars we contribute a new dataset, called X-Humans, contain-ing 233 sequences of high-quality textured scans from 20 participants, totalling 35,500 data frames. Project page: https://ait.ethz.ch/X-Avatar. 1.

Introduction
A significant part of human communication is non-verbal in which body pose, appearance, facial expressions, and hand gestures play an important role. Hence, it is clear that the quest towards immersive, life-like remote telepres-ence and other experiences in AR/VR, will require methods to capture the richness of human expressiveness in its en-tirety. Yet, it is not clear how to achieve this. Non-verbal communication involves an intricate interplay of several ar-ticulated body parts at different scales, which makes it dif-ficult to capture and model algorithmically.
Parametric body models such as the SMPL family [35, 47, 49] have been instrumental in advancing the state-of-the-art in modelling of digital humans in computer vision and graphics. However, they rely on mesh-based represen-tations and are limited to fixed topologies and in resolution of the 3D mesh. These models are focused on minimally clothed bodies and do not model garments or hair. Hence, it is difficult to capture the full appearance of humans.
Neural implicit representations hold the potential to overcome these limitations. Chen et al. [13] introduced a method to articulate human avatars that are represented by continuous implicit functions combined with learned for-ward skinning. This approach has been shown to generalize to arbitrary poses. While SNARF [13] only models the ma-jor body bones, other works have focused on creating im-plicit models of the face [22, 67], the hands [17], or how to model humans that appear in garments [20] and how to additionally capture appearance [50, 55]. Although neural implicit avatars hold great promise, to date no model exists that holistically captures the body and all the parts that are important for human expressiveness jointly.
In this work, we introduce X-Avatar, an animatable, im-plicit human avatar model that captures the shape, appear-ance and deformations of complete humans and their hand poses, facial expressions, and clothing. To this end we adopt the full-body pose space of SMPL-X [47]. This causes two key challenges for learning X-Avatars from data: (i) the significantly increased number of involved articulated body parts (9 used by [13] vs. 45 when including hands and face) and (ii) the different scales at which they appear in obser-vations. The hands and the face are much smaller in size compared to the torso, arms and legs, yet they exhibit simi-larly or even more complex articulations.
X-Avatar consists of a shape network that models the ge-ometry in canonical space and a deformation network to es-tablish correspondences between canonical and deformed space via learned linear blend skinning (LBS). The param-eters of the shape and deformation fields must be learned only from posed observations. SNARF [13] solves this via iterative correspondence search. This optimization prob-lem is initialized by transforming a large number of can-didate points via the bone transformations. Directly adopt-ing SNARF and initializing root-finding with only the body bones leads to poor results for the hands and face. Hence, to account for the articulation of these smaller body parts, their bone transformations must also be considered. How-ever, correspondence search scales poorly with the num-ber of bones, so na¨ıvely adding them makes training slow.
Therefore, we introduce a part-aware initialization strategy which is almost 3 times faster than the na¨ıve version while outperforming it quantitatively. Furthermore, to counteract the imbalance in scale between the body, hands, and face, we propose a part-aware sampling strategy, which increases the sampling rate for smaller body parts. This significantly improves the fidelity of the final result. To model the ap-pearance of X-Avatars, we extend the shape and deforma-tion fields with an additional appearance network, condi-tioned on pose, facial expression, geometry and the normals in deformed space. All three neural fields are trained jointly.
X-Avatar can learn personalized avatars for multiple peo-ple and from multiple input modalities. To demonstrate this, we perform several experiments. First, we compare our method to its most related work (SCANimate [50], SNARF
[13]) on the GRAB dataset [9, 54] on the animation task of minimally clothed humans. Second, we contribute a novel dataset consisting of 233 sequences of 20 clothed partici-pants recorded in a high-quality volumetric capture stage
[16]. The dataset consists of subjects that perform diverse body and hand poses (e.g., counting, pointing, dancing) and facial expressions (e.g., laughing, screaming, frowning). On this dataset we show that X-Avatar can learn from 3D scans and (synthesized) RGB-D data. Our experiments show that
X-Avatar outperforms strong baselines both in quantitative and qualitative measures in terms of animation quality. In summary, we contribute:
• X-Avatar, the first expressive implicit human avatar model that captures body pose, hand pose, facial ex-pressions and appearance in a holistic fashion.
• Part-aware initialization and sampling strategies, which together improve the quality of the results and keep training efficient.
• X-Humans, a new dataset consisting of 233 sequences, of high-quality textured scans showing 20 participants with varied body and hand movements, and facial ex-pressions, totalling 35,500 frames. 2.