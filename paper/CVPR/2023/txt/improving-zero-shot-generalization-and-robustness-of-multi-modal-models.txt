Abstract
Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image clas-sification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accu-racies are much lower (over 25% gap in some cases). We investigate the reasons for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First, we develop a simple and efficient zero-shot post-hoc method to identify images whose top-1 prediction is likely to be incorrect, by measuring consis-tency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better pre-dicts mistakes, outperforming the popular max logit base-line on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain im-ages by making use of the WordNet hierarchy; specifically we augment the original class by incorporating its parent and children from the semantic label hierarchy, and plug the augmentation into text prompts. We conduct experiments on both CLIP and LiT models with five different ImageNet-based datasets. For CLIP, our method improves the top-1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method improves across ImageNet shifted datasets, four other datasets, and other model architectures such as LiT.
The proposed method1 is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code is available at https://github.com/gyhandy/Hierarchy-CLIP. 1.

Introduction
Vision-language multi-modal models trained on large-scale data have achieved significant success in numerous domains and have demonstrated excellent zero-shot gener-alization ability [7, 12, 18, 19, 20, 28]. Given a test image and a set of candidate class labels, one can compute the similarity between the embedding of the image and the em-bedding of each candidate class labels, and predict the class 1Work carried out mainly at Google as the one with the highest similarity. The zero-shot top-1 accuracy for ImageNet [4] using CLIP variants (CLIP ViT-L) matches the performance of the original ResNet model trained from scratch. Recently, CLIP has been found to be more robust to distribution shift than ResNet, achieving good performance on ImageNet-V2 [21], ImageNet-R [9],
ImageNet-A [11], and ImageNet-Sketch [25].
We noticed a large gap between the top-1 accuracy and top-5 accuracy, 64.2% vs. 89.4% respectively, revealing potential headroom for improvement. We investigated the cases where the top-1 prediction was incorrect but the top-5 prediction was correct, and identified several typical failure modes. Despite the well-known multi-label issues in Ima-geNet [1], we found many of the remaining failure cases are caused by noise and ambiguous text prompts related to the
WordNet hierarchical structure of ImageNet. Some class names are quite general so that the model cannot correctly match images from their specific subclasses. For example, the hot-air balloon images belonging to the “balloon” class were misclassified as “airship”, see Figure 1 middle. On the other hand, some class names are too specific such that the model fails to correlate them with their more generic super-classes. For example, 96% of images with ground truth label “tusker” are wrongly classified as other elephant classes such as “Asian elephant”, see Figure 1 left. The fail-ure modes analysis suggests that the text encoder is very sensitive to inputs and as a result, the overall classification lacks robustness.
Inspired by these observations, we propose to first iden-tify the subset of images whose top-1 prediction is likely to be incorrect, and then improve the accuracy for those images by a principled framework to augment their class labels by WordNet hierarchy. To estimate whether an im-age has an incorrect prediction, i.e., to estimate the predic-tion confidence, we use the consistency of predictions under different text prompt templates and image augmentations as a signal for prediction confidence estimation. Although prediction confidence estimation has been well studied in single-modal classification models, we found those com-monly used confidence scores, maximum softmax proba-bility [10] and maximum logit score [8], are not always re-liable for the multi-modal CLIP and LiT models due to the poor calibration of the logits scores. For example, among the 1K classes in ImageNet, the class with the greatest mean logit value (computed as the cosine similarity between im-age and text embeddings) is “fig” (the fruit). Though we don’t have access to CLIP private training data, we hypoth-esize that this might be due to “fig” being a common abbre-viation for “figure”, which frequently occurs in the training data and thus includes many non-fruit illustrations.
In this work, we first propose a simple yet efficient zero-shot confidence estimation method better suited for CLIP, based on predictions’ self-consistency over different text prompts and image perturbations. [26] proposed using self-consistency among multiple model outputs to improve the reasoning accuracy of large language models. Here we extend the idea for confidence estimation in multi-modal models by measuring consistency of predictions under mul-tiple input text prompts and image transformations. Our method is effective at predicting mistakes; the identified low confidence subset has significantly lower top-1 accu-racy (21.58%) than the average accuracy (64.18%). Next, to improve the accuracy for the low confidence subset, we develop a label augmentation technique using Word-Net label hierarchy. Our method leverages semantic in-formation from ancestors (top-down) as well as children (bottom-up) and improves the top-1 accuracy of the subset to 38.71% (17.13% improvement). Our method not only improves model accuracy, but also model robustness, im-proving on ImageNet variants with distribution shift such as ImageNet-v2, ImageNet-R, ImageNet-Adversarial and
Imagenet-Sketch.
The main contributions of this work are:
• We identified several failure modes for zero-shot Im-ageNet classification using multi-modal models, and our findings suggest that the text encoder is very sen-sitive to prompts. To improve the prediction accuracy, prompts need to be better designed.
• We propose a simple yet efficient zero-shot confidence score that is better suited for multi-modal models, based on predictions’ self-consistency under different text prompts and image perturbations.
• We develop a label augmentation technique that uses both ancestor and children labels from WordNet. By applying the label augmentation to the previously iden-tified low confidence subset of images, we signifi-cantly improve their prediction accuracy. 2.