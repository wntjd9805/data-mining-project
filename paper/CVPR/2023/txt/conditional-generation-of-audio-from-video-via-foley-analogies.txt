Abstract
The sound effects that designers add to videos are de-signed to convey a particular artistic effect and, thus, may be quite different from a scene’s true sound. Inspired by the challenges of creating a soundtrack for a video that dif-fers from its true sound, but that nonetheless matches the actions occurring on screen, we propose the problem of con-ditional Foley. We present the following contributions to address this problem. First, we propose a pretext task for training our model to predict sound for an input video clip using a conditional audio-visual clip sampled from another time within the same source video. Second, we propose a model for generating a soundtrack for a silent input video, given a user-supplied example that specifies what the video should “sound like”. We show through human studies and automated evaluation metrics that our model successfully generates sound from videos, while varying its output ac-cording to the content of a supplied example. Project site: https://xypb.github.io/CondFoleyGen. 1.

Introduction
When artists create sound effects for videos, they often
“borrow” sounds from other sources, then manipulate them to match the on-screen actions. These artists’ aim is not necessarily to convey the scene’s true sound, but rather to achieve a desired artistic effect. Thus, the clunk of a coconut shell becomes a trotting horse, or the sizzle of cooking bacon becomes rain1.
The problem of creating sound effects for video, known as Foley [1], has often been posed as predicting a video’s co-occurring sound [29,42,68]. Yet the task that artists solve is subtly different. They create a soundtrack for a video that differs from its true sound, but that still plausibly matches the on-screen events. Also, these prior systems largely do not give artists control over the output sound.
To aid Foley artists while giving them artistic control, we propose a conditional Foley problem inspired by classic work on image analogies [27]. Our task is to generate a soundtrack for an input silent video from a user-provided conditional audio-visual example that specifies what the input video should “sound like.” The generated soundtrack should relate to the input video in an analogous way as the provided example (Fig. 1). This formulation naturally separates the problem of selecting an exemplar sound, which arguably requires the artist’s judgment, from the problem of manipulating that sound to match a video, such as by precisely adjusting its timing and timbre.
This proposed task is challenging, since a system must 1We encourage you to watch and listen to how sound artists work: https://www.youtube.com/watch?v=UO3N_PRIgX0
learn to adapt the exemplar (conditional) sound to match the timing of the visual content of a silent video while preserv-ing the exemplar sound’s timbre. While prior methods can predict a video’s sound [29, 42, 68], they cannot incorporate an artist’s exemplary conditional sound. Furthermore, while vision-to-sound methods can pose the problem as predicting a video’s soundtrack from its images, it is less clear how supervision for conditional examples can be obtained.
To address these challenges, we contribute a self-supervised pretext task for learning conditional Foley, as well as a model for solving it. Our pretext task exploits the fact that natural videos tend to contain repeated events that produce closely related sounds. To train the model, we randomly sample two pairs of audio-visual clips from a video, and use one as the conditional example for the other. Our model learns to infer the types of actions within the scene from the conditional example, and to generate analogous sounds to match the input example. At test time, our model generalizes to conditional sounds obtained from other videos. To solve the task, we train a Transformer [58] to autoregressively predict a sequence of audio codes for a spectrogram VQGAN [13], while conditioning on the provided audio-visual example. We improve the model’s performance at test time by generating a large number of soundtracks, then using an audio-visual synchronization model [8, 30, 41] to select the sound with the highest degree of temporal alignment with the video.
We evaluate our model on the Greatest Hits dataset [42], which contains videos that require an understanding of mate-rial properties and physical interactions, and via qualitative examples from the highly diverse CountixAV dataset [66].
Through perceptual studies and quantitative evaluations, we show that our model generates soundtracks that convey the physical properties of conditional examples while reflecting the timing and motions of the on-screen actions. 2.