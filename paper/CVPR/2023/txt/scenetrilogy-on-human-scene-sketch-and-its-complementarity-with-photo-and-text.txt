Abstract
In this paper, we extend scene understanding to include that of human sketch. The result is a complete trilogy of scene representation from three diverse and complementary modalities – sketch, photo, and text. Instead of learning a rigid three-way embedding and be done with it, we focus on learning a flexible joint embedding that fully supports the
“optionality” that this complementarity brings. Our em-bedding supports optionality on two axes: (i) optionality across modalities – use any combination of modalities as query for downstream tasks like retrieval, (ii) optionality across tasks – simultaneously utilising the embedding for ei-ther discriminative (e.g., retrieval) or generative tasks (e.g., captioning). This provides flexibility to end-users by ex-ploiting the best of each modality, therefore serving the very purpose behind our proposal of a trilogy in the first place.
First, a combination of information-bottleneck and condi-tional invertible neural networks disentangle the modality-specific component from modality-agnostic in sketch, photo, and text. Second, the modality-agnostic instances from sketch, photo, and text are synergised using a modified cross-attention. Once learned, we show our embedding can accommodate a multi-facet of scene-related tasks, including those enabled for the first time by the inclusion of sketch, all without any task-specific modifications. Project Page: https:// pinakinathc.github.io/ scenetrilogy 1.

Introduction
Scene understanding sits at the very core of computer vi-sion. As object-level research matures [24, 32], an encour-aging shift can be observed in recent years on scene-level tasks, e.g., scene recognition [113], scene captioning [55], scene synthesis [34], and scene retrieval [13, 57].
Scene research has generally progressed from that of sin-gle modality [113, 114] to the very recent focus on multi-modality [3, 13, 19]. The latter setting not only triggered a series of practical applications [34, 57, 101, 115] but im-Figure 1. Some scenes are easy to describe via sketch; for oth-ers, text is better. We provide the option to sketch, write, or both (sketch+text). For “optionality” across tasks, we disentangle sketch, text, and photo into a discriminative (e.g., retrieval) part f ag shared across modalities, and a generative (e.g., captioning) s , f sp part specific to one modality (f sp p ). This supports a multi-facet of scene-related tasks without task-specific modifications. t f sp portantly helped to cast insights into scene understanding on a conceptual level (i.e., what is really being perceived by humans). To date, research on multi-modal scene under-standing has mainly focused on two modalities – text and photo [59, 61, 62], via applications such as text-based scene retrieval (TBIR) [35], and scene captioning [23, 61, 62].
This paper follows the said trend of multi-modal scene understanding and extend it to also include human scene-sketch. Sketch is identified because of its unique charac-teristics of being both expressive and subjective, evident in an abundance of object-level sketch research [11], and very recently on scene-level [19]. To verify there is indeed use-ful complementarity that sketch can bring to multi-modal scene understanding, we first conducted two pilot studies (i) on expressivity, we compare text and sketch in terms of scene image retrieval, and (ii) on subjectivity, we test a novel task of subjective captioning where sketch or parts-of-speech [26] are used as guidance for image captioning. On (i), results show there is significant disagreement in terms of retrieval accuracy when one is used as query over the other,
indicating there is complementary information between the two modalities. On (ii), sketch is shown to offer more sub-jectivity as a guiding signal than text, when quantified using common metrics such as BELU-4 [67] and CIDEr [95]. s , f sp p , f sp
To fully explore the complementarity of all three modal-ities, we desire a flexible joint embedding that best sustains
“optionality” across modalities, and also across tasks. The former enables end-users to use any combination of modal-ities (e.g., only sketch, only text, or both sketch+text) as a query for downstream tasks; and the latter provides option of utilising the learned embedding for both discriminative (e.g., retrieval) and generative problems (e.g., captioning).
This desired level of “optionality” is however not achiev-able via naive three-way joint embeddings common in the literature [3, 13, 19]. Instead, we advocate a three-way dis-entanglement (Fig. 1(b)), where each of the three modal-ities is disentangled into their modality-specific compo-nent (f sp t , for sketch, photo and text), and a shared modality-agnostic component (f ag). The idea is that modality-specific will hold information specific to each modality (e.g., drawing style for sketch, texture for photo,
It follows that fil-and grammatical knowledge for text). tering away modality-specific parts from each of the three modalities gives a shared modality-agnostic part that car-ries shared abstract semantic across all three modalities, (as shown in Fig. 1(b)). How optionality is supported in such a disentangled space then becomes trivial (Fig. 1(c),(d)). To achieve optionality across tasks, we simply use modality-agnostic information as the joint embedding to perform discriminative tasks (e.g., cross-modal retrieval), and for cross-modal generative tasks (e.g., captioning), we just combine modality-agnostic information (from source) with modality-specific (from target) to generate the target modal-ity. Optionality across modality is a little harder, where we make use of a cross-attention [50] mechanism to capture the synergy across the modality-agnostic components. s ↔ f ag t ↔ f ag s + f ag
Benefiting from our optionality-enabled embedding, we can perform a multi-facet of tasks without any task-specific modifications: (i) Fig. 1 (c) show cross-modal discrimina-tive tasks such as sketch-based image retrieval (SBIR) using p ), text-based image retrieval (TBIR) using (f ag (f ag
↔ f ag p ), or sketch+text based image retrieval (STBIR) using (f ag p ). (ii) Fig. 1 (d) show cross-modal gen-erative tasks such as image captioning (photo branch) using p + f sp f ag t → ft to generate textual descriptions ft. Sim-ilarly, for sketch captioning (sketch branch) we use f ag s + f sp t → ft. (iii) Last but not least, to demonstrate what the expressiveness of human sketch can bring to scene under-standing, we introduce a novel task of subjective captioning where we guide image captioning using sketch as a signal p + f ag (subjective branch) as f ag s → ft. t
In summary, our contributions are: (i) We extend multi-modal scene understanding to include human scene-sketches, thereby completing a trilogy of scene representa-tion from three diverse and complementary modalities. (ii)
We provide optionality to end-users by learning a flexible joint embedding that supports: optionality across modali-ties and optionality across tasks. (iii) Using computation-ally efficient techniques like information bottleneck, con-ditionally invertible neural networks, and modified cross-attention mechanism, we model this flexible joint embed-ding. (iv) Once learned, our embedding accommodates a multi-facet of scene-related tasks like retrieval, captioning. 2.