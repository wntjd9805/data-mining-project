Abstract
Vision-Language Pre-Training (VLP) has shown promis-ing capabilities to align image and text pairs, facilitating a broad variety of cross-modal learning tasks. However, we observe that VLP models often lack the visual ground-ing/localization capability which is critical for many down-stream tasks such as visual reasoning. In this work, we pro-pose a novel Position-guided Text Prompt (PTP) paradigm to enhance the visual grounding ability of cross-modal mod-els trained with VLP. Specifically, in the VLP phase, PTP divides the image into N × N blocks, and identifies the ob-jects in each block through the widely used object detector in VLP. It then reformulates the visual grounding task into a fill-in-the-blank problem given a PTP by encouraging the model to predict the objects in the given blocks or regress the blocks of a given object, e.g. filling “[P]” or “[O]” in a PTP “The block [P] has a [O]”. This mechanism im-proves the visual grounding capability of VLP models and thus helps them better handle various downstream tasks. By introducing PTP into several state-of-the-art VLP frame-works, we observe consistently significant improvements across representative cross-modal learning model architec-tures and several benchmarks, e.g. zero-shot Flickr30K
Retrieval (+4.8 in average recall@1) for ViLT [16] base-line, and COCO Captioning (+5.3 in CIDEr) for SOTA
BLIP [19] baseline. Moreover, PTP achieves comparable results with object-detector based methods [8, 23, 45], and much faster inference speed since PTP discards its object detector for inference while the later cannot. 1.

Introduction
The vision-and-language pre-training (VLP) models like
CLIP [31], ALIGN [14] and CoCa [42] have greatly ad-vanced the state-of-the-art performance of many cross-modal learning tasks, e.g., visual question answering [4], reasoning [35], and image captioning [1, 7]. Typically, a generic cross-modal model is first pre-trained on large-scale
*Corresponding authors.
Figure 1. Comparison of three VLP learning frameworks and their performance. (a) compares region feature based VLP (RF-VLP), end-to-end VLP (E2E-VLP), and our position-guided text prompt based VLP (PTP-VLP). Our PTP-VLP only needs about 15ms for inference which is the same as E2E-VLP but is much faster than RF-VLP. (b) On position-aware questions widely occurred in many downstream tasks, with masked text and image input, RF-VLP and PTP-VLP can well predict objects, while E2E-VLP can-not pinpoint the position information of the object in the image. image-caption data in a self-supervised fashion to see suf-ficient data for better generalization ability, and then fine-tuned on downstream tasks for adaptation. With remarkable effectiveness, this pre-training-then-fine-tuning paradigm of VLP models has dominated the multi-modality field.
In VLP, visual grounding is critical for many tasks as observed in previous research [3, 40]. To model the posi-tion information, traditional VLP models [3,23,45] (the top of Fig. 1 (a)) employ a faster-rcnn [33] pre-trained on the 1600 classes Visual Genome [17] to extract salient region features and bounding boxes. Then these models use both the bounding box and object feature as input. In this way, these models not only learn what objects are contained in the salient region and where are these objects. However, when using region features as input, the model pays atten-tion to the items inside the bounding boxes and ignores the contextual data outside of them [13]. More seriously, on downstream task, these methods still need to use detectors to extract objects, giving very slow inference speed.
To get rid of region feature for higher efficiency, recent
works [13,16] (the middle of Fig. 1 (a)) adopt raw-pixel im-age as input instead of region features, and train the model with Image Text Matching [8] and Masked Language Mod-eling [10] loss end-to-end. Despite their faster speed, these models cannot well learn the object positions and also their relations. As shown in Fig. 1 (b), we observe that a well-trained ViLT model [16] well know what objects are in an image. But this model does not learn the object positions accurately. For example, it wrongly predicts “the dog is on the right of this image”. However, during fine-tuning, downstream tasks actually require the object position infor-mation to comprehensively understand the image. Such a gap largely impairs the performance on downstream tasks.
In this work, we aims to ease the position missing prob-lem for these end-to-end models, and keep fast inference time for downstream tasks at the same time.
Inspired by the recently prompt learning methods [15, 25, 32, 41], we propose a novel and effective Position-guided Text
Prompt (PTP) paradigm (the bottom of Fig. 1 (a)) for cross-modality model pre-training. The key insight is that by adding position-based co-referential markers in both im-age and text, visual grounding can be reformulated into a fill-in-the-blank problem, maximally simplify the learning of object information. PTP grounds language expressions in images through two components: 1) block tag genera-tion, dividing images into N × N blocks and identifying objects, and 2) text prompt generation, placing query text into a position-based template.
By bringing the position information into pre-training, our PTP enables strong visual grounding capabilities of
VLP models. At the same time, as we do not used ob-ject detector for downstream tasks, we keep fast inference time. Experimental results show that our method outper-forms their counterparts by a large margin especially for zero-shot setting. For example, our PTP-BLIP achieves 3.4% absolute accuracy gain over CoCa [42] in zero-shot retrieval Recall@1 on coco dataset with much less training data (4M vs. 3B) and a much smaller model (220M vs. 2.1B). In addition to the zero-shot task, we show that PTP can achieve strong performance for object position guided visual reasoning and the other common VLP tasks such as visual question answering, and image captioning. 2.