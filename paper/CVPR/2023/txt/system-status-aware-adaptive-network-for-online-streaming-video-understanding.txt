Abstract
Recent years have witnessed great progress in deep neu-ral networks for real-time applications. However, most existing works do not explicitly consider the general case where the device’s state and the available resources fluc-tuate over time, and none of them investigate or address the impact of varying computational resources for online video understanding tasks. This paper proposes a System-status-aware Adaptive Network (SAN) that considers the device’s real-time state to provide high-quality predictions with low delay. Usage of our agent’s policy improves ef-ficiency and robustness to fluctuations of the system status.
On two widely used video understanding tasks, SAN obtains state-of-the-art performance while constantly keeping pro-cessing delays low. Moreover, training such an agent on various types of hardware configurations is not easy as the labeled training data might not be available, or can be com-putationally prohibitive. To address this challenging prob-lem, we propose a Meta Self-supervised Adaptation (MSA) method that adapts the agent’s policy to new hardware con-figurations at test-time, allowing for easy deployment of the model onto other unseen hardware platforms. 1.

Introduction
Online video understanding, where certain predictions are immediately made for each video frame by using in-formation in the current frame and potentially past frames, is an important task right at the intersection of video-based research and practical vision applications (e.g., self-driving vehicles [11], security surveillance [4], streaming services
[32], and human-computer interactions [20]). In particular, in many of these real-world video-based applications, a fast and timely response is often crucial to ensure high usability and reduce potential security risk. Therefore, in many prac-tical online applications, it is essential to ensure that the model is working with low delay while maintaining a good
† Equal contribution; § Currently at Meta; ‡ Corresponding author performance, which can be challenging for many existing deep neural networks.
Recently, much effort has been made to reduce the de-lay of deep neural networks, including research into effi-cient network design [16, 36, 44], input-aware dynamic net-works [5,6,12,24], and latency-constrained neural architec-tures [1, 2, 21]. However, all these works do not explicitly consider the dynamic conditions of the hardware platform, and assume stable computation resources are readily avail-able. In practical scenarios, the accessible computing re-sources of the host devices can be fluctuating and dynamic due to the fact that multiple computationally expensive yet important threads are running concurrently. For example, in addition to performing vision-related tasks such as object detection, human activity recognition, and pose estimation, state-of-the-art robotic systems usually need to simultane-ously perform additional tasks like simultaneous localiza-tion and mapping (SLAM) to successfully interact with hu-mans and the environment. Those tasks are also often com-putationally heavy and could compete with vision tasks for computing resources. As a result, at times when the host device is busy with other processes, conducting inference for each model might require significantly more time than usual, leading to extremely long delays, which could cause safety issues and lagging responses in many real-world ap-plications. Therefore, the study and development of models providing reliable yet timely responses under various hard-ware devices and fluctuating computing resources is cru-cially important. Unfortunately, such studies are lacking in the field.
To achieve and maintain low delay for online video understanding tasks under a dynamic computing resource budget, we propose a novel System-status-aware Adaptive
Network (SAN). Different from previous works, SAN ex-plicitly considers the system status of its host device to make on-the-fly adjustments to its computational complexity, and is thus capable of processing video streams effectively and efficiently in a dynamic system environment. SAN com-prises of two components: a) a simple yet effective dynamic main module that offers reliable predictions under various
network depths and input resolutions; b) a lightweight agent that learns a dynamic system-status-aware policy used to control the execution of the main module, which facilitates adaptation to the fluctuating system load. With the adaptiv-ity of the main module and the control policy generated by the agent, our SAN can achieve good performance on the online video understanding task while maintaining a low delay under fluctuating system loads.
In various applications, we may need to deploy SAN onto different hardware platforms for online video under-standing. However, it is inconvenient to train SAN for each hardware platform, and it might also be difficult to find adequate storage to load the large labeled dataset on all platforms (e.g., mobile devices). In light of these dif-ficulties, we further propose a method for deployment-time self-supervised agent adaptation, which we call Meta Self-supervised Adaptation (MSA). With MSA, we can conve-niently train a SAN model on a set of local platforms, and perform a quick deployment-time agent adaptation on a tar-get device, without the need for the original labeled training data. Specifically, our proposed MSA introduces an auxil-iary task of delay prediction together with a meta-learning procedure, that facilitates the adaptation to the target de-ployment device.
In summary, the main contributions of this paper are:
• We are the first to explicitly consider the fluctuating system status of the hardware device at inference time for online video understanding. To address this, we propose SAN, a novel system-status-aware network that adapts its behavior according to the video stream and the real-time status of the host system.
• We further propose a novel Meta Self-supervised
Adaptation method MSA that alleviates the training burden and allows our model to effectively adapt to new host devices with potentially unclear computation profiles at deployment time.
• We empirically demonstrate that our proposed method achieves promising performance on the challenging online action recognition and pose estimation tasks, where we achieve low delays under a rapidly fluctu-ating system load without jeopardizing the quality of the predictions. 2.