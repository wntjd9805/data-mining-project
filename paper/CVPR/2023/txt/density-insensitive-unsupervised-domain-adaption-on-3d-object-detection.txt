Abstract 3D object detection from point clouds is crucial in safety-critical autonomous driving. Although many works have made great efforts and achieved significant progress on this task, most of them suffer from expensive annotation cost and poor transferability to unknown data due to the do-main gap. Recently, few works attempt to tackle the do-main gap in objects, but still fail to adapt to the gap of varying beam-densities between two domains, which is crit-ical to mitigate the characteristic differences of the LiDAR collectors. To this end, we make the attempt to propose a density-insensitive domain adaption framework to address the density-induced domain gap. In particular, we first in-troduce Random Beam Re-Sampling (RBRS) to enhance the robustness of 3D detectors trained on the source domain to the varying beam-density. Then, we take this pre-trained de-tector as the backbone model, and feed the unlabeled target domain data into our newly designed task-specific teacher-student framework for predicting its high-quality pseudo la-bels. To further adapt the property of density-insensitivity into the target domain, we feed the teacher and student branches with the same sample of different densities, and propose an Object Graph Alignment (OGA) module to con-struct two object-graphs between the two branches for en-forcing the consistency in both the attribute and relation of cross-density objects. Experimental results on three widely adopted 3D object detection datasets demonstrate that our proposed domain adaption method outperforms the state-of-the-art methods, especially over varying-density data.
Code is available at https://github.com/WoodwindHu/DTS. 1.

Introduction 3D object detection is a fundamental task in various real-world scenarios, such as autonomous driving [24, 35] and robot navigation [29], aiming to detect and localize traffic-related objects such as cars, pedestrians, and cy-(cid:66) Corresponding author: W. Hu. This work was supported by Na-tional Natural Science Foundation of China (61972009).
Figure 1. (a) The significant difference of beam densities among
Waymo, KITTI, and nuScenes datasets. The beam density Dζ represents the number of beams per unit zenith angle. The beams are evenly distributed in nuScenes, while the density of beams in
Waymo and KITTI increases as the zenith angle ζ increases, with the highest density near the horizontal direction. (b) Compared to previous works (SN [43] and ST3D [51]), our method is more effective in transferring the knowledge from low density to high density or high density to low density (N: nuScenes, K: KITTI,
W: Waymo.). clists in 3D point clouds [16, 25, 26]. With the advent of deep learning, this task has obtained remarkable advances
[24, 35–37, 49, 56, 60] in recent years, which however re-quires costly dense annotations of point clouds. Further, in real-world scenarios, upgrading LiDARs to other prod-uct models can be time-consuming and labor-intensive to collect and annotate massive data for each kind of prod-uct, while it is reasonable to use labeled data from previous sensors. Also, the number of LiDAR points used in mass-produced robots and vehicles is usually fewer than that in large-scale public datasets [44]. To bridge the domain gap caused by different LiDAR beams, it is essential to develop methods that address these differences. However, the gen-eralization ability of existing methods is proved to be lim-ited [43] when the 3D models trained on a specific dataset are directly applied to an unknown dataset collected with a different LiDAR, which prevents the wide applicability of 3D object detection in autonomous driving.
To reduce the domain gap between different datasets, some works [13, 14, 27, 34, 43, 44, 47, 51, 55, 57] proposed unsupervised domain adaptation (UDA) methods to transfer knowledge from a labeled source domain to an unlabeled target domain. However, most of them focus on reducing
the domain gap introduced by the bias in object sizes on the labeled source domain, which neglect another important do-main gap induced by varying densities of point clouds ac-quired from different types of LiDAR. We argue that this domain gap is crucial for 3D object detection in two aspects: 1) As demonstrated in Figure 1(a), different LiDAR col-lectors generally produce point cloud data with distinctive densities and distributions, leading to huge density-induced domain gap. 2) Most 3D detectors are directly trained on a single environment and thus sensitive to the cross-domain density variation. As shown in Figure 1(b), existing domain adaption methods suffer from performance bottlenecks in cross-density scenarios. Although few works [44] attempt to downsample point clouds of high density and transfer its knowledge to the low-density domain, they are limited to the model design that cannot realize the knowledge trans-fer from a low-density domain to a high-density domain.
Hence, it is demanded to train robust 3D feature representa-tions that can adapt to point cloud data of varying densities.
To this end, we make the attempt to propose a novel
Density-insensitive Teacher-Student (DTS) framework to address the domain gap induced by varying point densities and distributions. The key idea of DTS is to first pre-train a density-insensitive object detector on the source domain, and then employ a self-training strategy [20, 51, 58] to fine-tune this detector on the unlabeled target domain by itera-tively predicting and updating its pseudo results. However, there still remain two concerns: 1) Previous self-training methods may be prone to its mistake by using single-branch prediction. 2) How to adapt and improve the property of density-insensitivity of the pre-trained 3D detector on the target domain is important. Therefore, we introduce a task-specific teacher-student framework in order to provide more reliable and robust supervision, in which the teacher and student branches are fed with variants of the same sample in different densities. Further, considering the object pre-diction should be invariant in the two branches, we propose to capture their cross-density object-aware consistency for enhancing the density-insensitivity on the target domain.
To be specific, we first introduce Random Beam Re-Sampling (RBRS) to train the density-invariant 3D object detector on the labeled source domain, by randomly mask-ing or interpolating the beams of the point clouds. Then, we take this pre-trained 3D detector as the backbone model to build a teacher-student framework to iteratively predict and update the pseudo labels on the unlabeled target do-main. To achieve the goal of density-insensitivity, we feed the student and teacher models with the RBRS-augmented sample and the original sample, respectively. Moreover, in order to enforce the consistency in attributes and relations of detected objects in the teacher and student branches for more reliable supervision, we construct two graphs based on the objects predicted from the teacher and student mod-els, and propose a novel Object Graph Alignment (OGA) to keep consistent cross-density object-attributes (node-level) and object-relations (edge-level) between the two graphs.
During the training, the student model is optimized based on the predictions of the teacher while the weights of the teacher model are updated by taking the exponential mov-ing average of the weights of the student model.
In this way, our DTS is effective in reducing the density-induced domain gap and achieving state-of-the-art performance on the unknown target data.
In summary, our main contributions include
• We propose a density-insensitive unsupervised domain adaption framework to alleviate the influence of the domain gap caused by varying density distributions.
We develop beam re-sampling to randomize the den-sity of point clouds, which effectively enhances the ro-bustness of 3D object detection to varying densities.
• We exploit a task-specific teacher-student framework to fine-tune the pre-trained 3D detector on the tar-get domain.
To adapt and improve the density-insensitivity on the target domain, we introduce an ob-ject graph alignment module to keep the cross-density object-aware consistency.
• Experimental results demonstrate our model signif-icantly outperforms the state-of-the-art methods on three widely adopted 3D object detection datasets in-cluding NuScenes [4], KITTI [11], and Waymo [39]. 2.