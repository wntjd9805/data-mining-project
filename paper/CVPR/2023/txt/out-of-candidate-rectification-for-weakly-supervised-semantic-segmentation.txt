Abstract
Weakly supervised semantic segmentation is typically in-spired by class activation maps, which serve as pseudo masks with class-discriminative regions highlighted. Al-though tremendous efforts have been made to recall precise and complete locations for each class, existing methods still commonly suffer from the unsolicited Out-of-Candidate (OC) error predictions that do not belong to the label can-didates, which could be avoidable since the contradiction with image-level class tags is easy to be detected. In this paper, we develop a group ranking-based Out-of-Candidate
Rectification (OCR) mechanism in a plug-and-play fashion.
Firstly, we adaptively split the semantic categories into In-Candidate (IC) and OC groups for each OC pixel according to their prior annotation correlation and posterior predic-tion correlation. Then, we derive a differentiable rectifica-In-tion loss to force OC pixels to shift to the IC group. corporating OCR with seminal baselines (e.g., AffinityNet,
SEAM, MCTformer), we can achieve remarkable perfor-mance gains on both Pascal VOC (+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with negligible extra training overhead, which jus-tifies the effectiveness and generality of OCR. † 1.

Introduction
Due to the development of deep learning, significant progress has been made in deep learning-based semantic segmentation [42, 47]. However, its effectiveness requires huge amounts of data with precise pixel-level labels. Col-lecting precise pixel-level labels is very time-consuming and labor-intensive, thus much research shifts attention to
* Equal contribution (cid:0) Corresponding Author
† (cid:135) github.com/sennnnn/Out-of-Candidate-Rectification
Figure 1. Motivation of our OCR. We visualize the segmentation results from the baseline method (e.g. SEAM) and the baseline with our proposed OCR. The predictions from baseline methods are easily disturbed by OC pixels, that is, pixels whose semantic categories are in contradiction with label candidate set (inner of the
Yellow contour). Our proposed OCR can rectify these OC pixels and suppress this unreasonable phenomenon. training effective semantic segmentation models with rel-atively low manual annotation cost, i.e., Weakly Super-vised Semantic Segmentation (WSSS). There exist vari-ous types of weak supervision for semantic segmentation such as image-level tag labels [1, 2, 26, 61, 77], bounding boxes [14, 30, 34], scribbles [40, 57] and points [5]. In this work, we focus on WSSS based on image-level tag labels since image-level tags demand the least annotation cost, which just needs the information on the existence of the tar-get object categories.
Most of the previous WSSS methods follow such a stan-dard workflow [2]: 1). generating high-quality Class Ac-tivation Maps (CAM) [61, 70]; 2). generating pseudo la-bels from CAMs [2, 78]; 3). training segmentation net-works from pseudo labels. Previous works mainly fo-Figure 2. Conceptual workflow of our OCR. The OC pixels are selected out by checking if the semantic categories are in contradiction with image-level candidate tags. Then we adaptively split the categories into IC group and OC group. Finally, we utilize rectification loss for group ranking and let OC pixels escape from OC group to IC group. cus on the first and second procedures. However, train-ing segmentation network from pseudo labels is also vi-tal because neural network can exploit shared patterns be-tween pseudo labels [4] and largely improve final segmen-tation results [39]. But the pseudo label generation relies on high-quality CAM, while the pseudo labels usually are incomplete and imprecise because CAM only focuses on discriminative object parts and can not fully exploit object regions [7]. The existence of noise in pseudo labels pro-vides confused knowledge to segmentation networks and results in error predictions. According to the observations in
Fig. 1, the segmentation networks trained by noisy pseudo labels usually output pixels with semantic categories that do not belong to the candidate label set, i.e., image-level tag labels. This special type of prediction errors are defined as Out-of-Candidate (OC). These errors can be easily de-tected by checking if the semantic category of pixel is in contradiction with image-level tag labels, which is seldom considered before. For better identifying this phenomenon, we extra name these error pixels as OC pixels and name the illegal categories as OC categories. In contrast, the po-tentially correct categories for OC pixels are defined as In-Candidate (IC) categories. group
To suppress propose the occurrence of OC phenomenon, ranking-based Out-of-Candidate we
Rectification (OCR) to rectify OC pixels from OC cate-gories to IC categories by solving a group ranking problem (i.e., the prediction score of IC group needs to be larger
In Fig. 2, OCR than the prediction score of OC group). is illustrated as three procedures: OC pixels selection,
IC/OC categories group split and rectification. Firstly, we find out OC pixels whose classification result is in contradiction with image-level tag labels. Secondly, we adaptively split the classes into IC classes group and OC classes group for each OC pixel by considering prior label correlation information from the image-level tag labels and posterior label correlation information from the network prediction. Finally, rectification loss is used to modulate the distance between OC pixels and class centers of IC group and OC group. It constraints that the OC pixels and
OC class centers are pushed away and the OC pixels and
IC class centers are pulled closer so that those OC pixels are rectified to correct classes.
Out-of-Candidate Rectification (OCR) is designed in a plug-and-play style to provide reasonable supervision sig-nals with trivial training costs and to improve evaluation results with no extra cost for inference. To fairly show the effectiveness and generality, we adopt the same settings of several previous methods (AffinityNet [2], SEAM [61],
MCTformer [70]) and evaluate our proposed OCR on the
PASCAL VOC 2012 and MS COCO 2014 datasets. Experi-ments demonstrate that our OCR improves the performance of final segmentation results. Specifically, our module im-proves AffinityNet, SEAM and MCTformer by 3.2%, 3.3% and 0.8% mIoU on PASCAL VOC 2012 dataset and 1.0%, 1.3% and 0.5% mIoU on MS COCO 2014 dataset. 2.