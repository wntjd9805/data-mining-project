Abstract
Transformer-based models achieve favorable perfor-mance in artistic style transfer recently thanks to its global receptive field and powerful multi-head/layer atten-tion operations. Nevertheless, the over-paramerized multi-layer structure increases parameters significantly and thus presents a heavy burden for training. Moreover, for the task of style transfer, vanilla Transformer that fuses con-tent and style features by residual connections is prone to content-wise distortion.
In this paper, we devise a novel
Transformer model termed as Master specifically for style transfer. On the one hand, in the proposed model, differ-ent Transformer layers share a common group of parame-ters, which (1) reduces the total number of parameters, (2) leads to more robust training convergence, and (3) is read-ily to control the degree of stylization via tuning the num-ber of stacked layers freely during inference. On the other hand, different from the vanilla version, we adopt a learn-able scaling operation on content features before content-style feature interaction, which better preserves the original similarity between a pair of content features while ensuring the stylization quality. We also propose a novel meta learn-ing scheme for the proposed model so that it can not only work in the typical setting of arbitrary style transfer, but also adaptable to the few-shot setting, by only fine-tuning the Transformer encoder layer in the few-shot stage for one
*Equal contribution.
†Corresponding author. specific style. Text-guided few-shot style transfer is firstly achieved with the proposed framework. Extensive exper-iments demonstrate the superiority of Master under both zero-shot and few-shot style transfer settings. 1.

Introduction
Artistic style transfer aims at applying style patterns like colors and textures of a reference image to a given content image while preserving the semantic structure of the con-tent. In contrast to the pioneering optimization method [9] and early per-style-per-model methods like [17, 26], arbi-trary style transfer methods [3, 13, 16, 21, 22, 24, 31] enable real time style transfer for any style image in the test time in a zero-shot manner. The flexibility has led to this arbitrary-style-per-model fashion to dominate style transfer research.
Recently, to enhance the representation of global infor-mation in arbitrary style transfer, Transformer [40] is intro-duced to this area [4], leveraging the global receptive field and powerful multi-head/layer structure, and achieves su-perior performance. Nevertheless, the over-parameterized multi-layer structure increases model parameters signifi-cantly. As shown in Fig. 1(a), there are 25.94M learn-able parameters for a 3-layer Transformer structure in
StyTr2 [4], v.s. 3.50M in AdaIN [13], a simple but effective baseline in arbitrary style transfer. Such a large model for standard Transformer inevitably presents a heavy burden for training. As shown in Fig. 1(b), when there are more than
adaptation to the model for a specific style within a lim-ited number of updates is possible, where the stylization with only 1 layer can be further improved, as shown in
Fig. 1. Beyond that, we first achieve text-guided few-shot style transfer with this framework, which largely al-leviates the training burden of previous per-text-per-model solution. In this sense, we term the overall pipeline Meta
Style Transformer (Master). Our contributions are summa-rized as follows:
• We propose a novel Transformer architecture specif-ically for artistic style transfer.
It shares parameters between different layers, which not only helps training convergence, but also allows convenient control over the stylization effect.
• We identity the content distortion problem of resid-ual connections in Transformer and propose learnable scale parameters as an option to alleviate the problem.
• We introduce a meta learning framework for adapting original training setting of zero-shot style transfer to the few-shot scenario, with which our Master achieves very good trade-off between flexibility and quality.
• Experiments show that our model achieves results bet-ter than those of arbitrary-style-per-model methods.
Furthermore, under the few-shot setting, either condi-tioned on image or text, Master can even yield perfor-mance on par with that of per-style-per-model methods with significantly less training cost. 2.