Abstract
Unsupervised stereo matching has received a lot of atten-tion since it enables the learning of disparity estimation without ground-truth data. However, most of the un-supervised stereo matching algorithms assume that the left and right images have consistent visual properties, i.e., symmetric, and easily fail when the stereo images are asymmetric.
In this paper, we present a novel spatially-adaptive self-similarity (SASS) for unsupervised asymmetric stereo matching.
It extends the concept of self-similarity and generates deep features that are robust to the asymmetries. The sampling patterns to calculate self-similarities are adaptively generated throughout the image regions to effectively encode diverse patterns.
In order to learn the effective sampling patterns, we design a contrastive similarity loss with positive and negative weights. Consequently, SASS is further encouraged to encode asymmetry-agnostic features, while maintaining the distinctiveness for stereo correspondence. We present extensive experimental results including ablation studies and comparisons with different methods, demonstrating effectiveness of the proposed method under resolution and noise asymmetries. 1.

Introduction
Scene depth is an indispensable information in computer vision, as it can beneﬁt numerous subsequent applications including scene recognition [5, 18], 3D scene reconstruc-tion [22], and autonomous driving [17]. Stereo matching, which aims to ﬁnd disparities of corresponding points in rectiﬁed left and right (stereo) images, has been widely ex-plored since the disparity can directly converted to depth with camera calibration parameters. Recent advent of large-This research was supported by the National Research Founda-tion of Korea (NRF) grant funded by the Korea government (MSIP) (NRF2021R1A2C2006703). The work of S. Kim was supported by the
National Research Foundation of Korea(NRF) grant funded by the Korea government (MSIP) (NRF-2021R1C1C2005202). (Corresponding author:
Kwanghoon Sohn.) scale datasets and advanced hardware led the researchers to solve stereo matching with Convolutional Neural Net-It resulted in a number of CNN-based works (CNNs). stereo matching algorithms that are learned in both super-vised [1, 16, 19] and unsupervised manner [3, 25]. Even though the recent methods have achieved signiﬁcant gain in both accuracy and speed, the existing algorithms assume that the stereo images are symmetric, where the stereo im-ages have consistent visual properties in terms of bright-ness, resolution, noise level, modality, etc.
Recently, multi-camera systems have become more com-mon, such as RGB-NIR cameras in Kinect, and tele-wide cameras in smartphones. Such systems usually consist of different sensors, resulting in asymmetric stereo images, i.e., the stereo images with different visual properties. The asymmetric images are embedded into inconsistent features and make it difﬁcult to accurately calculate the cost volume.
Furthermore, the most widely adopted assumption for un-supervised stereo matching, photometric consistency, is in-valid for the corresponding points in the asymmetric stereo images [2]. Consequently, the widely-used stereo matching methods assuming symmetric images [1, 3, 19] easily fail in the asymmetric scenario [15].
There have been relatively less efforts to handle stereo matching under asymmetries such as visual quality [2, 15] and spectrum [23, 31]. Several methods adopt supervised
[15], or proxy-supervised [23] paradigm to solve the deep asymmetric stereo matching. However, such methods re-quire additional active depth [15] or image [23] sensor to ac-quire the training label, which makes it difﬁcult to construct the training data. In order to tackle the problem and learn the asymmetric stereo matching in an unsupervised manner, a few methods adopt feature consistency loss [2, 24]. On the other hand, several spectral-asymmetric stereo match-ing methods use unpaired image-to-image translation [33] algorithm to project the images into a same spectrum, fol-lowed by photometric consistency loss [14, 31]. A common approach in the unsupervised asymmetric stereo matching methods is to transfer the images into a shared space to ex-(a) (b)
Figure 1. Self-similarity sampling patterns of (a) FCSS [10] and (b) the proposed SASS. For different pixels indicated with red cir-cles, the sampling patterns are represented with squares, connected with dashed lines. FCSS has equivalent patterns for all pixels, while the proposed SASS generates adaptive patterns. ploit the consistency constraint as training loss. The im-portance of consistent space in loss calculation for unsuper-vised stereo matching is further emphasized in [2].
There have been a number of researches to extract im-age features that are robust to different types of varia-tions. In [21], Local Self-Similarity (LSS) descriptor has been presented based on an observation that local inter-nal layout of self-similarity is less sensitive to photomet-It has demonstrated impressive robust-ric differences. ness against large modality differences, and various deriva-tions based on self-similarity have been formulated in hand-crafted [11,12] and deep-learning [10] frameworks, demon-strating effectiveness in cross-modal visual [11, 12] and se-mantic [10] correspondences. In [10, 11], in order to design self-similarity based descriptors with improved robustness and efﬁciency, sampling patterns are learned throughout the data. However, the learned sampling patterns are ﬁxed for all regions as in Fig. 1(a), limiting the capability to encode robust features of varying geometries across the images.
In this paper, we present a novel Spatially-Adaptive
Self-Similarity (SASS) for unsupervised stereo matching in asymmetric scenario. Motivated by the importance of the symmetry in loss calculation [2], we design a novel frame-work to extract asymmetry-agnostic features. We take ad-vantage of self-similarity [21] which is robustness to do-main discrepancy, and further extend it by adaptively gen-erating the sampling patterns across the spatial locations, as illustrated in Fig. 1(b).
It enables to extract asymmetry-agnostic features from the asymmetric stereo images to cal-In culate the stereo matching loss in a symmetric space. addition, we design a contrastive similarity loss with ad-ditional positive and negative weights to further encourage the asymmetry-agnostic property of the SASS, while pre-serving the discriminative capability.
The main contributions of this paper are summarized as:
• We propose a novel Spatially-Adaptive Self-Similarity (SASS) to adaptively encode asymmetry-agnostic fea-tures for unsupervised asymmetric stereo matching.
The features are used to calculate the unsupervised stereo matching loss based on view consistency.
• We design a contrastive similarity loss with a novel positive and negative weighting strategy to further en-hance the asymmetry-agnostic property while main-taining the discriminative capability of SASS.
• Extensive experimental results including ablation stud-ies and comparisons with different methods demon-strate the effectiveness of the proposed method on res-olution and noise asymmetries.
The rest of this paper is organized as follows: In Sec. 2, we present previous works that are related to ours. Sec. 3 ex-plains the background and details of the proposed method.
Experimental results are given in Sec. 4, followed by con-clusion and future works in Sec. 5. 2.