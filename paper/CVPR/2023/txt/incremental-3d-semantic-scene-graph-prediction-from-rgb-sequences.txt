Abstract 3D semantic scene graphs are a powerful holistic rep-resentation as they describe the individual objects and de-pict the relation between them. They are compact high-level graphs that enable many tasks requiring scene reasoning.
In real-world settings, existing 3D estimation methods pro-duce robust predictions that mostly rely on dense inputs.
In this work, we propose a real-time framework that incre-mentally builds a consistent 3D semantic scene graph of a scene given an RGB image sequence. Our method con-sists of a novel incremental entity estimation pipeline and a scene graph prediction network. The proposed pipeline simultaneously reconstructs a sparse point map and fuses entity estimation from the input images. The proposed net-work estimates 3D semantic scene graphs with iterative message passing using multi-view and geometric features extracted from the scene entities. Extensive experiments on the 3RScan dataset show the effectiveness of the pro-posed method in this challenging task, outperforming state-of-the-art approaches. Our implementation is available at https://shunchengwu.github.io/MonoSSG. 1.

Introduction
Scene understanding is a cornerstone in many computer vision applications requiring perception, interaction, and manipulation, such as robotics, AR/VR and autonomous systems [17, 54â€“56]. Semantic Scene Graphs (SSGs) go beyond recognizing individual entities (objects and stuff) by reasoning about the relationships among them [61, 66].
They also proved to be a valuable representation for com-plex scene understanding tasks, such as image caption-ing [26, 67], generation [13, 24], scene manipulation [10, 11], task planning [27], and surgical procedure estima-tion [42, 43]. Given the benefits of such representations, scene graph estimation received increasing attention in the computer vision community.
While earlier methods mainly estimate SSGs from im-ages [18, 19, 33, 66, 72], recent approaches have also in-vestigated estimating them from 3D data. Compared to 2D scene graphs, which describe a single image, 3D scene
Figure 1. We propose a real-time 3D semantic scene graph estima-tion method that relies on an abstract understanding of a scene ge-ometry built with RGB input. Our method estimates scene graphs incrementally by continuously estimating scene graphs and fusing local predictions into a global 3D scene graph. graphs depict the entire 3D scenes, enabling applications requiring a holistic understanding of the whole scene, such as path planning [47], camera localization, and loop clo-sure detection [23]. However, existing 3D methods either require dense 3D geometry of the scenes to estimate 3D scene graphs [1, 23, 61, 64], which limits the use case since dense geometry is not always available, or constraints the scene graph estimation at the image-level [15,27,66], which tend to fail inferring relationships among objects beyond the individual viewpoints. A method that estimates 3D scene graphs relies on sparse scene geometry and reasoning about relationships globally has not been explored yet.
In this work, we propose a real-time framework that in-crementally estimates a global 3D SSG of a scene simply requiring an RGB sequence as input. The process is illus-trated in Fig. 1. Our method simultaneously reconstructs a segmented point cloud while estimating the SSGs of the current map. The estimations are bound to the point map, which allows us to fuse them into a consistent global scene
graph. The segmented map is constructed by fusing en-tity estimation from images to the points estimated from a sparse Simultaneous Localization and Mapping (SLAM) method [3]. Our network takes the entities and other proper-ties extracted from the segmented map to estimate 3D scene graphs. Fusing entities across frames is non-trivial. Exist-ing methods often rely on dense inputs [38,58] and struggle with sparse inputs since the points are not uniformly dis-tributed. Estimating scene graphs with sparse input points is also challenging. Sparse and ambiguous geometry ren-ders the node representations unreliable. On the other hand, directly estimating scene graphs from 2D images ignores the relationship beyond visible viewpoints. We aim to over-come the aforementioned issues by proposing two novel approaches. First, we propose a confidence-based fusion scheme which is robust to variations in the point distribu-tion. Second, we present a scene graph prediction network that mainly relies on multi-view images as the node feature representation. Our approach overcomes the need for exact 3D geometry and is able to estimate relationships without view constraints. In addition, our network is flexible and generalizable as it works not only with sparse inputs but also with dense geometry.
We comprehensively evaluate our method on the 3D SSG estimation task from the public 3RScan dataset [60]. We ex-periment and compare with three input types, as well as 2D and 3D approaches. Moreover, we provide a detailed abla-tion study on the proposed network. The results show that our method outperforms all existing approaches by a signif-icant margin. The main contributions of this work can be summarized as follows: (1) We propose the first incremen-tal 3D scene graph prediction method using only RGB im-ages. (2) We introduce an entity label association method that works on sparse point maps. (3) We propose a novel network architecture that generalizes with different input types and outperforms all existing methods. 2.