Abstract
We present Fast Language-Image Pre-training (FLIP), a simple and more efﬁcient method for training CLIP [52].
Our method randomly masks out and removes a large por-tion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 mil-lion image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encour-aging results and comparisons. We hope that our work will foster future research on scaling vision-language learning. 1.

Introduction
Language-supervised visual pre-training, e.g., CLIP [52], has been established as a simple yet powerful methodology for learning representations. Pre-trained CLIP models stand out for their remarkable versatility: they have strong zero-shot transferability [52]; they demonstrate unprecedented quality in text-to-image generation (e.g., [53, 55]); the pre-trained encoder can improve multimodal and even unimodal visual tasks. Like the role played by supervised pre-training a decade ago [40], language-supervised visual pre-training is new fuel empowering various tasks today.
Unlike classical supervised learning with a pre-deﬁned label set, natural language provides richer forms of supervi-sion, e.g., on objects, scenes, actions, context, and their re-lations, at multiple levels of granularity. Due to the complex nature of vision plus language, large-scale training is essen-tial for the capability of language-supervised models. For example, the original CLIP models [52] were trained on 400 million data for 32 epochs—which amount to 10,000 Ima-geNet [16] epochs, taking thousands of GPU-days [52, 36]. 73 72 71 70 69
)
% ( y c a r u c c a t o h s
-o r e z 68 0 3.7  speedup mask 0% (our CLIP repro.) mask 50% mask 75% 50 100 150 training time (hours) 200 250
Figure 1. Accuracy vs. training time trade-off. With a high masking ratio of 50% or 75%, our FLIP method trains faster and is more accurate than its CLIP counterpart. All entries are bench-marked in 256 TPU-v3 cores. Training is done on LAION-400M for 6.4, 12.8, or 32 epochs, for each masking ratio. Accuracy is evaluated by zero-shot transfer on the ImageNet-1K validation set.
The model is ViT-L/16 [20]. More details are in Fig. 3. As the
CLIP baseline takes ⇠2,500 TPU-days training, a speedup of 3.7 can save ⇠1,800 TPU-days.
⇥
Even using high-end infrastructures, the wall-clock training time is still a major bottleneck hindering explorations on scaling vision-language learning.
We present Fast Language-Image Pre-training (FLIP), a simple method for efﬁcient CLIP training. Inspired by the sparse computation of Masked Autoencoders (MAE) [29], we randomly remove a large portion of image patches dur-ing training. This design introduces a trade-off between
“how carefully we look at a sample pair” vs. “how many sample pairs we can process”. Using masking, we can: (i) see more sample pairs (i.e., more epochs) under the same wall-clock training time, and (ii) compare/contrast more sample pairs at each step (i.e., larger batches) under simi-lar memory footprint. Empirically, the beneﬁts of process-ing more sample pairs greatly outweigh the degradation of per-sample encoding, resulting in a favorable trade-off.    
⇥
⇥
⇥
By removing 50%-75% patches of a training image, our
; it also allows using method reduces computation by 2-4 2-4 larger batches with little extra memory cost, which boost accuracy thanks to the behavior of contrastive learn-ing [30, 11]. As summarized in Fig. 1, FLIP trains >3
⇥ faster in wall-clock time for reaching similar accuracy as its
CLIP counterpart; with the same number of epochs, FLIP reaches higher accuracy than its CLIP counterpart while still being 2-3 faster.
We show that FLIP is a competitive alternative to CLIP on various downstream tasks. Pre-trained on the same
LAION-400M dataset [56], FLIP dominantly outperforms its CLIP counterparts (OpenCLIP [36] and our own repro-duction), as evaluated on a large variety of downstream datasets and transfer scenarios. These comparisons suggest that FLIP can readily enjoy the faster training speed while still providing accuracy gains.
Facilitated by faster training, we explore scaling FLIP pre-training. We study these three axes: (i) scaling model size, (ii) scaling dataset size, or (iii) scaling training sched-ule length. We analyze the scaling behavior through care-fully controlled experiments. We observe that model scal-ing and data scaling can both improve accuracy, and data scaling can show gains at no extra training cost. We hope our method, results, and analysis will encourage future re-search on scaling vision-language learning. 2.