Abstract
We propose Gated Stereo, a high-resolution and long-range depth estimation technique that operates on active gated stereo images. Using active and high dynamic range passive captures, Gated Stereo exploits multi-view cues alongside time-of-flight intensity cues from active gating.
To this end, we propose a depth estimation method with a monocular and stereo depth prediction branch which are combined in a final fusion stage. Each block is super-vised through a combination of supervised and gated self-supervision losses. To facilitate training and validation, we acquire a long-range synchronized gated stereo dataset for automotive scenarios. We find that the method achieves an improvement of more than 50 % MAE compared to the next best RGB stereo method, and 74 % MAE to existing monoc-ular gated methods for distances up to 160 m. Our code, models and datasets are available here1. 1.

Introduction
Long-range high-resolution depth estimation is critical for autonomous drones, robotics, and driver assistance sys-tems. Most existing fully autonomous vehicles strongly rely on scanning LiDAR for depth estimation [51, 52]. While these sensors are effective for obstacle avoidance the mea-surements are often not as semantically rich as RGB im-ages. LiDAR sensing also has to make trade-offs due to physical limitations, especially beyond 100 meters range, including range range versus eye-safety and spatial reso-lution. Although recent advances in LiDAR sensors such as, MEMS scanning [60] and photodiode technology [58] have drastically reduced the cost and led to a number of sen-sor designs with ≈ 100 - 200 scanlines, these are still sig-nificantly lower resolutions than modern HDR megapixel camera sensors with a vertical resolution more than ≈ 5000 pixels. However, extracting depth from RGB images with monocular methods is challenging as existing estimation methods suffer from a fundamental scale ambiguity [16].
Stereo-based depth estimation methods resolve this issue but need to be well calibrated and often fail on texture-less 1https://light.princeton.edu/gatedstereo/ regions and in low-light scenarios when no reliable features, and hence triangulation candidate, can be found.
To overcome the limitations of existing scanning LiDAR and RGB stereo depth estimation methods, a body of work has explored gated imaging [2, 7–9, 22, 27]. Gated im-agers integrate the transient response from flash-illuminated scenes in broad temporal bins, see Section 3 for more de-tails. This imaging technique is robust to low-light, and adverse weather conditions [7] and the embedded time-of-flight information can be decoded as depth. Specifically,
Gated2Depth [23] estimates depth from three gated slices and learns the prediction through a combination of simula-tion and LiDAR supervision. Building on these findings, re-cently, Walia et al. [59] proposed a self-supervised training approach predicting higher-quality depth maps. However, both methods have in common that they often fail in condi-tions where the signal-to-noise ratio is low, e.g., in the case of strong ambient light.
We propose a depth estimation method from gated stereo observations that exploits both multi-view and time-of-flight cues to estimate high-resolution depth maps. We propose a depth reconstruction network that consists of a monocular depth network per gated camera and a stereo network that utilizes both active and passive slices from the gated stereo pair. The monocular network exploits depth-dependent gated intensity cues to estimate depth in monoc-ular and low-light regions while the stereo network relies on active stereo cues. Both network branches are fused in a learned fusion block. Using passive slices allows us to per-form robustly under bright daylight where active cues have a low signal-to-noise ratio due to ambient illumination. To train our network, we rely on supervised and self-supervised losses tailored to the stereo-gated setup, including ambient-aware and illuminator-aware consistency along with multi-camera consistency. To capture training data and assess the method, we built a custom prototype vehicle and captured a stereo-gated dataset under different lighting conditions and automotive driving scenarios in urban, suburban and high-way environments across 1000 km of driving.
Specifically, we make the following contributions:
• We propose a novel depth estimation approach us-ing gated stereo images that generates high-resolution
dense depth maps from multi-view and time-of-flight depth cues.
• We introduce a depth estimation network with two different branches for depth estimation, a monocular branch and a stereo branch, that use active and passive measurement, and a semi-supervised training scheme to train the estimator.
• We built a prototype vehicle to capture test and training data, allowing us to assess the method in long-range automotive scenes, where we reduce the MAE error by 50 % to the next best RGB stereo method and by 74 % on existing monocular gated methods for distances up to 160 m. 2.