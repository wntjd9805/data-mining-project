Abstract
In this paper, we show that a binary latent space can be explored for compact yet expressive image representa-tions. We model the bi-directional mappings between an image and the corresponding latent binary representation by training an auto-encoder with a Bernoulli encoding dis-tribution. On the one hand, the binary latent space provides a compact discrete image representation of which the distri-bution can be modeled more efﬁciently than pixels or con-tinuous latent representations. On the other hand, we now represent each image patch as a binary vector instead of an index of a learned cookbook as in discrete image repre-sentations with vector quantization. In this way, we obtain binary latent representations that allow for better image quality and high-resolution image representations without any multi-stage hierarchy in the latent space. In this binary latent space, images can now be generated effectively us-ing a binary latent diffusion model tailored speciﬁcally for modeling the prior over the binary image representations.
We present both conditional and unconditional image gen-eration experiments with multiple datasets, and show that the proposed method performs comparably to state-of-the-art methods while dramatically improving the sampling ef-ﬁciency to as few as 16 steps without using any test-time acceleration. The proposed framework can also be seam-1024 high-resolution image gener-lessly scaled to 1024 ation without resorting to latent hierarchy or multi-stage reﬁnements.
⇥ 1.

Introduction
The goal of modeling the image distribution that allows the efﬁcient generation of high-quality novel samples drives the research of representation learning and generative mod-els. Directly representing and generating images in the pixel space stimulates various research such as generative adver-sarial networks [2, 7, 18, 29], ﬂow models [11, 34, 43, 47], energy-based models [12, 13, 66, 71], and diffusion mod-els [24, 41, 54, 55]. As the resolution grows, it becomes increasingly difﬁcult to accurately regress the pixel values.
And this challenge usually has to be addressed through hi-Q
H
F
F
-Q
H
A b e l e
C s e h c r u h
C
N
U
S
L s m o o r d e
B
N
U
S
L
A b e l e
C
Q
H
F
F t e
N e g a m
I 1024 x 1024 256 x 256
Figure 1. Examples of generated images with different resolutions using the proposed method. erarchical model architectures [29, 72] or at a notably high cost [24]. Moreover, while demonstrating outstanding gen-erated image quality, GAN models suffer from issues in-cluding insufﬁcient mode coverage [38] and training insta-bility [21].
Representing and generating images in a learned latent space [33, 42, 49] provides a promising alternative. Latent diffusion [49] performs denoising in latent feature space with a lower dimension than the pixel space, therefore re-ducing the cost of each denoising step. However, regress-ing the real-value latent representations remains complex and demands hundreds of diffusion steps. Variational auto-encoders (VAEs) [23,33,48] generate images without any it-erative steps. However, the static prior of the latent space re-stricts the expressiveness, and can lead to posterior collapse.
To achieve higher ﬂexibility of the latent distribution with-out signiﬁcantly increasing the modeling complexity, VQ-VAE [61] introduces a vector-quantized latent space, where
each image is represented as a sequence of indexes, each of which points to a vector in a learned codebook. The prior over the vector-quantized representations is then modeled by a trained sampler, which is usually parametrized as an autoregressive model. The success of VQ-VAE stimulates a series of works that model the discrete latent space of code-book indexes with different models such as accelerated par-allel autoregressive models [8] and multinomial diffusion models [6, 20]. VQ-based generative models demonstrate surprising image synthesis performance and model cover-age that is better than the more sophisticated methods like
GANs without suffering from issues like training instability.
However, the hard restriction of using one codebook index to represent each image patch introduces a trade-off on the codebook size, as a large enough codebook to cover more image patterns will introduce an over-complex multinomial latent distribution for the sampler to model.
In this research, we explore a compact yet expressive representation of images in a binary latent space, where each image patch is now represented as a binary vector, and the prior over the discrete binary latent codes is effectively modeled by our improved binary diffusion model tailored for Bernoulli distribution. Speciﬁcally, the bi-directional mappings between images and the binary representations are modeled by a feed-forward autoencoder with a binary latent space. Given an image, the encoder now outputs the normalized parameters of a sequence of independently dis-tributed Bernoulli variables, from which a binary represen-tation of this image is sampled, and fed into the decoder to reconstruct the image. The discrete sampling in Bernoulli distribution does not naturally permit gradient propagation.
We ﬁnd that a simple straight-through gradient copy [4, 17] is sufﬁcient for high-quality image reconstruction while maintaining high training efﬁciency.
With images compactly represented in the binary latent space, we then introduce how to generate novel samples by modeling the prior over binary latent codes of images.
To overcome the shortcomings of many existing generative models such as being uni-directional [46, 61] and the non-regrettable greedy sampling [6, 8], we introduce binary la-tent diffusion that generates the binary representations of novel samples by a sequence of denoising starting from a random Bernoulli distribution. Performing diffusion in a binary latent space, modeled as Bernoulli distribution, re-duces the need for precisely regressing the target values as in Gaussian-based diffusion processes [24, 49, 55], and per-mits sampling at a higher efﬁciency. We then introduce how to progressively reparametrize the prediction targets at each denoising step as the residual between the inputs and the de-sired samples, and train the proposed binary latent diffusion models to predict such ‘ﬂipping probability’ for improved training and sampling stability.
We support our ﬁndings with both conditional and unconditional image generation experiments on multiple datasets. We show that our method can deliver remarkable image generation quality and diversity with more compact latent codes, larger image-to-latent resolution ratios, as well as fewer sampling steps, and faster sampling speed. We present some examples with different resolutions generated by the proposed method in Figure 1.
We organize this paper as follows: