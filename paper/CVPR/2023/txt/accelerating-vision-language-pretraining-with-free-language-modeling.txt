Abstract
The state of the arts in vision-language pretraining (VLP) achieves exemplary performance but suffers from high training costs resulting from slow convergence and long training time, especially on large-scale web datasets.
An essential obstacle to training efficiency lies in the entan-gled prediction rate (percentage of tokens for reconstruc-tion) and corruption rate (percentage of corrupted tokens) in masked language modeling (MLM), that is, a proper cor-ruption rate is achieved at the cost of a large portion of output tokens being excluded from prediction loss. To ac-celerate the convergence of VLP, we propose a new pre-training task, namely, free language modeling (FLM), that enables a 100% prediction rate with arbitrary corruption rates. FLM successfully frees the prediction rate from the tie-up with the corruption rate while allowing the corrup-tion spans to be customized for each token to be predicted.
FLM-trained models are encouraged to learn better and faster given the same GPU time by exploiting bidirectional contexts more flexibly. Extensive experiments show FLM could achieve an impressive 2.5× pretraining time reduc-tion in comparison to the MLM-based methods, while keep-ing competitive performance on both vision-language un-derstanding and generation tasks. Code will be public at https://github.com/TencentARC/FLM . 1.

Introduction
Vision-language pretraining (VLP) has recently demon-strated impressive performance on a handful of vision-language tasks [7,10,14,18,19,22], e.g., visual question an-swering, cross-modal retrieval, and image captioning. Sev-eral factors are responsible for the success: the availabil-ity of large-scale image-text datasets collected from the web [30], high-capacity model architectures like Trans-† Work done during internship in ARC Lab, Tencent PCG.
∗ Corresponding author
Figure 1. (a) Large prediction rate accelerates training. Given a fixed corruption rate, we vary the prediction rate by randomly selecting a subset of output tokens for prediction loss. The learn-ing rate schedule follows METER [10]. (b) The proposed FLM achieves competitive performance compared with MLM mean-while significantly accelerating the pretraining stage. The down-stream performance on NLVR2 [32] is reported. We show accu-racy curves before convergence for better visualization. former [34], and effective pretraining objectives for cross-modal learning.
One of the dominant pretraining objectives is masked language modeling (MLM), which was first introduced in natural language processing [9] and has been applied to vision-language areas in recent years [19]. MLM is a generative pretraining task designed to reconstruct a few (usually 40% for VLP) masked text tokens via reasoning
among the context of the remaining texts and the paired image. While effective in capturing cross-modal interac-tions, MLM-based methods [7,15,21] suffer from slow con-vergence and long training time, especially for large-scale models and noisy web data.
We argue that the limited prediction rate in MLM im-pedes the convergence speed of pretraining, since a large portion of tokens accompanied by corruption are excluded from prediction loss. As shown in Fig. 1 (top), under the same corruption rate , a larger prediction rate for MLM results in faster convergence of validation loss and down-stream performance. It is intuitive to set a prediction rate of 100% to fully exploit text tokens. However, a paradox emerges where a large prediction rate can only be achieved with a greater corruption rate in MLM, but an extremely large corruption rate leads to an extremely tough pretrain-ing task that may cause training collapse.
Autoregressive language modeling (AR) provides a workable solution to enable a 100% prediction rate. It pre-dicts the next token given the observation of previous to-kens. As shown in Fig. 1 (bottom), AR performs favor-ably in training efficiency against MLM, i.e., 6.1×speed-up for convergence. However, the converged performance by
AR is, unfortunately, much inferior to MLM. It is probably caused by the sub-optimal unidirectional corruption pattern, which is insufficient for downstream understanding tasks that usually rely on bidirectional contexts.
A natural question arises, can we accelerate the conver-gence of VLP by predicting 100% tokens like AR mean-while achieving competitive performance with MLM? To-wards this end, we introduce a new pretraining task, dubbed free language modeling (FLM), for VLP, that enjoys an ex-treme 100% prediction rate and flexible bidirectional con-textualized representations. We for the first time break up the entanglement between corruption and prediction rates, making the two factors freely determined. Furthermore, for each output token to be predicted, we allow independent and arbitrary-length spans (from one to 100% tokens) as cor-rupted connections. Rather than the suffix-like corruption pattern as in AR (as well as PrefixLM [37]), the corruption span of FLM is primarily distributed in the middle of the sequence, establishing a flexible perception of bidirectional contexts for better adaptation to VL understanding tasks.
The comparison between different pretraining objectives is illustrated in Fig. 2.
To perform VLP with FLM, we propose an encode-corrupt-predict framework, which performs feature encod-ing once and reconstructs several corrupted versions of the text sequence in parallel.
In the encoding step, bidirec-tional representations are achieved by learning forward and reverse unidirectional representations respectively, the or-der of which is manipulated by (reverse) casual masks in the same text Transformer. Subsequently, we ensure a 100% prediction rate by customizing corruption-prediction tasks for predicting each input token. In each corruption-prediction task, a span of corruption is randomly sampled and attached to the encoded sequence, followed by a re-constructor to solve the prediction task by reasoning among the remaining contexts. Unlike previous works (e.g., MLM,
AR) that adopt pre-encoding corruption, we inject corrup-tions after one-time feature encoding, encouraging flexible corruption patterns and efficient parallel prediction.
Our contributions are three-fold. (1) A novel pretraining objective for VLP, namely, free language modeling (FLM), is proposed to free the prediction rate from the constraints of corruption rate, enabling an appealing 100% prediction rate for accelerating convergence speed during pretraining. (2)
An encode-corrupt-predict framework built upon FLM ob-jective is proposed, allowing efficient and effective learning of a set of prediction tasks by merely conducting feature en-coding once. (3) Extensive experiments on VQA, NLVR2, image captioning, and image-text retrieval demonstrate the effectiveness of our FLM, where comparable performances to MLM are achieved with less than 50% pretraining time. 2.