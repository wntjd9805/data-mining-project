Abstract
Existing studies indicate that deep neural networks (DNNs) can eventually memorize the label noise. We ob-serve that the memorization strength of DNNs towards each instance is different and can be represented by the con-fidence value, which becomes larger and larger during the training process. Based on this, we propose a Dy-namic Instance-specific Selection and Correction method (DISC) for learning from noisy labels (LNL). We first use a two-view-based backbone for image classification, ob-taining confidence for each image from two views. Then we propose a dynamic threshold strategy for each instance, based on the momentum of each instance’s memorization strength in previous epochs to select and correct noisy la-beled data. Benefiting from the dynamic threshold strategy and two-view learning, we can effectively group each in-stance into one of the three subsets (i.e., clean, hard, and purified) based on the prediction consistency and discrep-ancy by two views at each epoch. Finally, we employ differ-ent regularization strategies to conquer subsets with differ-ent degrees of label noise, improving the whole network’s robustness. Comprehensive evaluations on three control-lable and four real-world LNL benchmarks show that our method outperforms the state-of-the-art (SOTA) methods to leverage useful information in noisy data while allevi-ating the pollution of label noise. Code is available at https://github.com/JackYFL/DISC. 1.

Introduction
Label noise is inevitable in image classification model learning, especially for large-scale database annotations through web-crawling [31,40], crowd-sourcing [52], or pre-This research was supported in part by the National Key R&D Pro-gram of China (grant 2021ZD0111901), and the National Natural Science
Foundation of China (grant 62176249).
Figure 1. An illustration of DNN’s increasing memorization strength during network training. The class prototypes are weights of the DNN classifier, and for simplicity, we only take a two-class case as an example. (a) In the beginning, DNN first fits clean data whose features are closer to class prototypes than noisy data, which is more sparsely distributed in feature space. (b) As training progresses, DNN begins to fit slightly noisy data, some of which can also be classified to its labeled class with relatively high con-fidence. (c) By the end of the training, the DNN has greatly in-creased its memorization strength, and even extremely noisy data can also be grouped into its labeled class with high confidence. trained models [12], etc. Recent studies show that DNNs are susceptible to label noise and could fit to the entire data set [2, 55] including the noisy set. Meanwhile, researchers found that DNNs have a memorization effect [2], i.e., the learning process of DNNs follows a curriculum, in which simple patterns are memorized first, followed by more diffi-cult ones like data with noisy labels. Recent studies have explored the use of memorization effect for LNL tasks, with many of these approaches being "early-learning"-based methods [1, 7, 11, 17, 23, 24, 29, 30, 32, 44, 53, 57, 58].
These methods leverage an early-stage DNN to improve the model robustness and generalization ability.
Early-learning-based LNL methods can be divided into three main directions: sample selection [7,17,23,24,29,53], label correction [1, 30, 44, 57] and regularization [11, 32, 37, 56, 58, 60]. Sample selection-based methods usually utilize the early-stage DNN’s losses or confidence to select reliable
(a) (b)
Figure 2. An illustration of different threshold strategies, including (a) the global threshold, (b) the class-wise threshold, and (c) the proposed dynamic instance-specific threshold. (c) instances, which are utilized to update the network. Some of these methods require a predefined threshold [24, 30] or prior knowledge about the noise label rate [17, 53] to select instances. Such a global predefined threshold, as shown in
Fig. 2 (a), is usually difficult to determine, and may require prior knowledge about the noise label rate to avoid exces-sive or insufficient noisy data selection, which will further lead to over-fitting and confirmation bias issues [17]. La-bel correction-based methods try to learn [44] or generate pseudo-labels [30, 57] to replace the original noisy ones.
Many of these methods employ the semi-supervised learn-ing (SSL) techniques to pseudo-label the noisy data, and most of them use global (say MixMatch [3], FixMatch [39]) or class-wise threshold (say FlexMatch [54]) to recalibrate labels. As shown in Fig. 2 (b), while a class-wise thresh-old considers the fitting difficulty of different classes, it still applies a uniform threshold for individual instances in each class. This remains sub-optimum if we consider an example of face images, in which profile face images are more dif-ficult to fit (relatively lower classification confidence) than frontal ones (relatively higher classification confidence) of the same subject. Regularization-based methods aim to de-sign robust loss functions [11, 32, 37, 58, 60, 61] or regular-ization techniques such as augmentation [56] that can uti-lize all instances to improve the model robustness against label noise. While these methods work well on moderately noisy data, they may have poor generalization ability under extremely noisy data (see Table 1), since all instances are utilized during the training process.
Based on the observation that the memorization strength for individual instances increases during network training, we argue that neither a global threshold nor a class-wise threshold is optimum for LNL. Therefore, we propose a Dy-namic Instance-specific Selection and Correction (DISC) approach (see Fig. 3 (a)) for LNL. DISC leverages a dy-namic instance-specific threshold strategy (Fig. 2 (c)) fol-lowing a memorization curriculum to select reliable in-stances and correct noisy labels. Each threshold is ob-tained through the momentum of each instance’s memoriza-tion strength in previous epochs. Such a dynamic threshold strategy can determine a reasonable threshold for each in-stance according to its memorization strength by the net-work. Inspired by previous methods of RRL [30], AugDisc
[35] and FixMatch [39], DISC also adopts weak and strong augmentations to produce two different views for image classification via a shared-weight model. Unlike previous methods, which use predictions from one view to select reliable instances or generate pseudo-labels for unlabeled data, DISC considers the consistency and discrepancy of two views and divides the noisy data into reliable instances (clean set), hard instances (hard set), and recalibrate noisy labeled instances (purified set), reflecting different degrees of label noise. By dividing the noisy data into three differ-ent subsets, DISC can alleviate the contamination of noisy labels to LNL model learning by conquering them via dif-ferent regularization strategies. As a result, the method can better make full use of the whole noisy dataset. The contri-butions of this paper include:
• We observe the memorization strength of DNNs towards individual instances can be represented by confidence value, which increases along with training. We provide evidence and experimental analyses to validate this claim.
• Based on the insight of memorization strength, we pro-pose a simple yet effective dynamic instance-specific threshold strategy of LNL that selects reliable instances and recalibrates noisy labels following an easy to hard curriculum.
• Additionally, we leverage the dynamic threshold strategy to group noisy data into three subsets based on predictions from two views generated from weak and strong augmen-tations. We then adopt different regularization strategies to handle individual subsets. 2.