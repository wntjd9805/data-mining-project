Abstract
In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REV EAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries.
REV EAL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowl-edge (e.g. image-text pairs, question answering pairs, knowl-edge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Fur-thermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REV EAL achieves state-of-the-art re-sults on visual question answering and image captioning.
The project page of this work is reveal.github.io. 1.

Introduction
Recent large-scale models such as T5 [33], GPT-3 [4],
PaLM [9], CoCa [49], Flamingo [2], BEIT-3 [43] and
*This work was done when Ziniu was an intern at Google.
PaLI [7] have demonstrated the ability to store substantial amounts of world knowledge, when scaled to tens of billions of parameters and trained on vast text and image corpora.
These models achieve state-of-the-art results in downstream tasks such as image captioning, visual question answering and open vocabulary recognition. Yet, these models have a number of drawbacks: (i) they require massive scale, of parameters, data and computation, and (ii) they need to be re-trained every time the world knowledge is updated.
To address these issues, we adopt a different approach.
Instead of statically compiling world knowledge into model weights, we transform the knowledge into a key-value mem-ory through neural representation learning. Our model learns to utilize the memory for answering knowledge-intensive queries. By decoupling the knowledge memorization from reasoning, we enable our model to leverage various external sources of knowledge (e.g., Wikipedia passages and im-ages [37], the WikiData knowledge graph [40], Web image-text pairs [5] and visual question answering data [12]). This enables the model parameters to focus on understanding the query and conducting reasoning, rather than being dedicated to memorization.
Retrieval-augmented models have attracted a fair amount of attention in the fields of NLP [14, 18] and computer vi-sion [13,25]. Typically, these models often use a pre-existing single-modality backbone to encode and retrieve informa-tion from the knowledge corpus. Such approaches do not leverage all available modalities in the query and knowl-edge corpora, and hence they might not find the information that is most helpful for generating the model output. A key novelty in our approach is that we encode and store various sources of multimodal world knowledge into a unified mem-ory, which the retriever can access via multimodal query encodings, to find the most relevant information from across complementary sources. Our multimodal memory and re-triever are pre-trained end-to-end together together with the rest of the model, on a massive amount of data and using diverse knowledge sources.
A key challenge of pre-training the multimodal retriever end-to-end is the lack of direct supervision. There is no ground-truth indicating which knowledge entries are most helpful for answering knowledge-intensive queries. Some of the existing works in NLP [14, 23, 34] propose to acquire training signal by assessing the usefulness of each retrieved knowledge entry independently for helping language mod-elling. This approach is inefficient, as it involves estimating hundreds of retrieved knowledge entries independently, and also inaccurate as it discards the dependency between dif-ferent knowledge entries in the retrieval set. In contrast, we propose to get this training signal while simultaneously considering multiple retrieved knowledge entries, by intro-ducing an attentive fusion layer that injects retrieval score into the attention calculation procedure. This enables the retrieval module to be differentiable and jointly pre-trained with the rest of the model.
In summary, our key contributions are as follows:
• We are the first to propose an end-to-end pre-training paradigm that learns to index into a large-scale memory to solve knowledge-intensive visual-language tasks.
• Our method can construct a large-scale memory by en-coding various sources of multimodal world knowledge, including Wikipedia passage, web images with alt-text captions, and knowledge graph triplets.
• REVEAL achieves state-of-the-art performance on sev-eral knowledge-intensive visual question answering and image captioning datasets. Notably on the OKVQA benchmark, REVEAL achieves a new state-of-the-art, 59.1% accuracy, while using order of magnitude fewer parameters than previous works. 2.