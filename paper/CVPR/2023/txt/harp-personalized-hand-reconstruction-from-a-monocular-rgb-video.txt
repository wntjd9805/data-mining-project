Abstract
We present HARP (HAnd Reconstruction and Personal-ization), a personalized hand avatar creation approach that takes a short monocular RGB video of a human hand as input and reconstructs a faithful hand avatar exhibiting a high-ﬁdelity appearance and geometry. In contrast to the major trend of neural implicit representations, HARP mod-els a hand with a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without any neural components. The explicit nature of our representation enables a truly scalable, robust, and efﬁcient approach to hand avatar creation as validated by our ex-periments. HARP is optimized via gradient descent from a short sequence captured by a hand-held mobile phone and can be directly used in AR/VR applications with real-time rendering capability. To enable this, we carefully de-sign and implement a shadow-aware differentiable render-ing scheme that is robust to high degree articulations and self-shadowing regularly present in hand motions, as well as challenging lighting conditions. It also generalizes to un-seen poses and novel viewpoints, producing photo-realistic renderings of hand animations. Furthermore, the learned
HARP representation can be used for improving 3D hand pose estimation quality in challenging viewpoints. The key advantages of HARP are validated by the in-depth analyses on appearance reconstruction, novel view and novel pose synthesis, and 3D hand pose reﬁnement. It is an AR/VR-ready personalized hand representation that shows superior
ﬁdelity and scalability. 1.

Introduction
Advancements in AR/VR devices are introducing a new reality in which the physical and digital worlds merge. The human hand is a crucial element for an intimate and in-teractive experience in these environments, serving as the primary interface between humans and the digital world.
Therefore, it is essential to capture, reconstruct, and animate life-like digital hands for AR and VR applications. Without
this capability, the authenticity and practicality of AR/VR consumer products will always be limited.
Despite its importance, the research into hand avatar cre-ation has so far been limited. Most works [8, 35, 59] focus on creating an appearance space on top of a parametric hand model such as MANO [62]. Such an appearance space pro-vides a compact way to represent hand texture but is rather limited in expressivity to handle non-standard textures. The recent LISA [12] model has emerged as an alternative, us-ing an implicit function to represent hand geometry and tex-ture color ﬁelds. Training a new identity in LISA, how-ever, requires a multi-view capturing setup as well as a large amount of data and computing power. In the nearby ﬁelds of face and body avatar creation, many works that leverage an implicit function [19,40,41,74] or NeRF-based [42] volume rendering [39,53,77] have also been recently explored. The
NeRF-based method such as HumanNeRF [77] produces a convincing novel view synthesis but still shows blurry ar-tifacts around highly articulated parts and cannot be easily exported to other applications.
We argue that democratizing hand avatar creation for
AR/VR users requires a method that is (1) accurate: so that personalized hand appearance and geometry can be faith-fully reconstructed; (2) scalable: allowing hand avatars to be obtained using a commodity camera; (3) robust: ca-pable of handling out-of-distribution appearance and self-shadows between ﬁngers and palm; and (4) efﬁcient: with real-time rendering capability.
To this end, we propose HARP, a personalized hand re-construction method that can create a faithful hand avatar from a short RGB video captured by a hand-held mobile phone. HARP leverages a parametric hand model, an ex-plicit appearance, and a differentiable rasterizer and shader to reconstruct a hand avatar and environment lighting in an analysis-by-synthesis manner, without any neural net-work component. Our observation is that human hands are highly articulated. The appearance changes of observed hands in a captured sequence can be dramatic and largely attributed to articulations and light interaction. Learning neural representations, such as implicit texture ﬁelds [12] or volume-based representations like NeRF [70], is vulnerable to the over-ﬁtting to a short monocular training sequence and can hardly generalize well to sophisticated and dexter-ous hand movements. By properly disentangling geometry, appearance, and self-shadow with explicit representations,
HARP can signiﬁcantly improve the reconstruction quality and generate life-like renderings on novel views and novel animations performing highly articulated motions. Further-more, the nature of the explicit representation allows the results from HARP to be conveniently exported to standard graphics applications.
In summary, (1)
HARP is a simple personalized hand avatar creation method the key advantages of HARP are: that reconstructs high-ﬁdelity appearance and geometry us-ing only a short monocular video. HARP demonstrates that an explicit representation with a differentiable raster-izer and shader is enough to obtain life-like hand avatars. (2) The hand avatar from HARP is controllable and com-patible with standard rasterization graphics pipelines allow-ing for photo-realistic rendering in AR/VR applications. (3)
Moreover, HARP can be used to improve 3D hand pose estimation in challenging viewpoints. We perform exten-sive experiments on the tasks of appearance reconstruction, novel-view-and-pose synthesis, and 3D hand poses reﬁne-ment. Compared to existing approaches, HARP is more ac-curate, robust, and generalizable with superior scalability. 2.