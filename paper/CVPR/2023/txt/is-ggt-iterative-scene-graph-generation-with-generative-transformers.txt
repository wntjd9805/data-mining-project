Abstract
Scene graphs provide a rich, structured representation of a scene by encoding the entities (objects) and their spa-tial relationships in a graphical format. This representation has proven useful in several tasks, such as question answer-ing, captioning, and even object detection, to name a few.
Current approaches take a generation-by-classification ap-proach where the scene graph is generated through labeling of all possible edges between objects in a scene, which adds computational overhead to the approach. This work in-troduces a generative transformer-based approach to gen-erating scene graphs beyond link prediction. Using two transformer-based components, we first sample a possible scene graph structure from detected objects and their visual features. We then perform predicate classification on the sampled edges to generate the final scene graph. This ap-proach allows us to efficiently generate scene graphs from images with minimal inference overhead. Extensive exper-iments on the Visual Genome dataset demonstrate the effi-ciency of the proposed approach. Without bells and whis-tles, we obtain, on average, 20.7% mean recall (mR@100) across different settings for scene graph generation (SGG), outperforming state-of-the-art SGG approaches while offer-ing competitive performance to unbiased SGG approaches. 1.

Introduction
Graph-based visual representations are becoming in-creasingly popular due to their ability to encode visual, se-mantic, and even temporal relationships in a compact rep-resentation that has several downstream tasks such as ob-ject tracking [4], scene understanding [17] and event com-plex visual commonsense reasoning [2, 3, 22]. Graphs can help navigate clutter and express complex semantic struc-tures from visual inputs to mitigate the impact of noise, clutter, and (appearance/scene) variability, which is essen-tial in scene understanding. Scene graphs, defined as di-rected graphs that model the visual-semantic relationships among entities (objects) in a given scene, have proven to be very useful in downstream tasks such as visual question-answering [14, 34], captioning [17], and even embodied tasks such as navigation [27], to name a few.
There has been a growing body of work [7, 10, 29, 33, 36, 38, 41] that has focused on the problem of scene graph generation (SGG), that aims to generate scene graph from a given input observation. However, such approaches have tackled the problem by beginning with a fully connected graph, where all entities interact with each other before pruning it down to a more compact graph by predicting edge relationships, or the lack of one, between each pair of local-ized entities. This approach, while effective, has several limitations. First, by modeling the interactions between en-tities with a dense topology, the underlying semantic struc-ture is ignored during relational reasoning, which can lead to poor predicate (relationship) classification. Second, by constructing pairwise relationships between all entities in a scene, there is tremendous overhead on the predicate classi-fication modules since the number of pairwise comparisons can grow non-linearly with the increase in the number of detected concepts. Combined, these two issues aggravate the existing long-tail distribution problem in scene graph generation. Recent progress in unbiasing [21, 31â€“33] has attempted to address this issue by tackling the long-tail dis-tribution problem. However, they depend on the quality of the underlying graph generation approaches, which suffer from the above limitations.
In this work, we aim to overcome these limitations using a two-stage, generative approach called IS-GGT, a transformer-based iterative scene graph generation ap-proach. An overview of the approach is illustrated in Fig-ure 1. Contrary to current approaches to SGG, we leverage advances in generative graph models [5, 23] to first sam-ple the underlying interaction graph between the detected entities before reasoning over this sampled semantic struc-ture for scene graph generation. By decoupling the ideas of graph generation and relationship modeling, we can con-strain the relationship classification process to consider only those edges (pairs of entities) that have a higher probability 1
Figure 1. Our goal is to move towards a generative model for scene graph generation using a two-stage approach where we first sample the underlying semantic structure between entities before predicate classification. This is different from the conventional approach of modeling pairwise relationships among all detected entities and helps constrain the reasoning to the underlying semantic structure. of interaction (both semantic and visual) and hence reduce the computational overhead during inference. Additionally, the first step of generative graph sampling (Section 3.2) al-lows us to navigate clutter by rejecting detected entities that do not add to the semantic structure of the scene by iter-atively constructing the underlying entity interaction graph conditioned on the input image. A relation prediction model (Section 3.3) reasons over this constrained edge list to clas-sify the relationships among interacting entities. Hence, the relational reasoning mechanism only considers the (pre-dicted) global semantic structure of the scene and makes more coherent relationship predictions that help tackle the long-tail distribution problem without additional unbiasing steps and computational overhead.
Contributions. The contributions of this paper are three-fold: (i) we are among the first to tackle the prob-lem of scene graph generation using a graph generative approach without constructing expensive, pairwise compar-isons between all detected entities, (ii) we propose the idea of iterative interaction graph generation and global, contex-tualized relational reasoning using a two-stage transformer-based architecture for effective reasoning over cluttered, complex semantic structures, and (iii) through extensive evaluation on Visual Genome [19] we show that the proposed approach achieves state-of-the-art performance (without unbiasing) across all three scene graph generation tasks while considering only 20% of all possible pairwise edges using an effective graph sampling approach. 2.