Abstract
Asynchronously operating event cameras find many ap-plications due to their high dynamic range, vanishingly low motion blur, low latency and low data bandwidth. The field saw remarkable progress during the last few years, and ex-isting event-based 3D reconstruction approaches recover sparse point clouds of the scene. However, such sparsity is a limiting factor in many cases, especially in computer vision and graphics, that has not been addressed satisfacto-rily so far. Accordingly, this paper proposes the first ap-proach for 3D-consistent, dense and photorealistic novel view synthesis using just a single colour event stream as in-put. At its core is a neural radiance field trained entirely in a self-supervised manner from events while preserving the original resolution of the colour event channels. Next, our ray sampling strategy is tailored to events and allows for data-efficient training. At test, our method produces results in the RGB space at unprecedented quality. We evaluate our method qualitatively and numerically on several chal-lenging synthetic and real scenes and show that it produces significantly denser and more visually appealing renderings than the existing methods. We also demonstrate robustness in challenging scenarios with fast motion and under low lighting conditions. We release the newly recorded dataset and our source code to facilitate the research field, see https://4dqv.mpi-inf.mpg.de/EventNeRF. 1.

Introduction
Event cameras record an asynchronous stream of per-pixel brightness changes. This is in contrast to conventional
RGB cameras that record the absolute intensity values at a pre-defined framerate. Event cameras have several advan-tages over conventional cameras such as virtually no motion blur, higher dynamic range, ultra low power consumption and lower latency. Because of these advantages, event cam-eras have been applied to many computer vision problems, including optical-flow estimation [11, 12], video interpola-tion [26, 38, 57, 62], deblurring [26, 68], 3D pose estima-tion [36, 46, 64, 75], geometry reconstruction [1, 6, 58, 61]
Figure 1. The original NeRF approach [35] (first row) produces strong artefacts when processing RGB images undergoing fast motion. Our approach (second row) operates purely on an event stream and thus produces significantly better and sharper results, capturing important details such as specularities (see yellow box). and many more [4, 9, 51, 60, 62, 63, 71, 74].
Generating dense photorealistic rendering of a scene in a 3D-consistent manner is a long-standing problem in com-puter vision and computer graphics [7,13,23,35,49]. There is currently, however, no event-based method for solving this problem. The closest work in literature usually ad-dresses the problem either from a SLAM [21, 41, 59, 72] or an odometry perspective [14,22,42,73,76]. Here, the aim is to estimate the position of a moving camera, and to recon-struct the 3D scene to some extend. These methods usually rely on explicit feature matching in the event space and re-construct sparse 3D models of environments from a single event stream. As a result, novel views generated by ex-isting methods are usually sparse, mostly containing edges and details causing events. The same observation applies to methods using stereo event cameras, even though they aim to densify the 3D reconstructions [72, 73].
The research question we are addressing in this work
is, whether an event stream from an event camera moving around the scene is sufficient to reconstruct a dense volu-metric 3D representation of a static scene. Here, we adopt the state-of-the-art 3D representation of Neural Radiance
Fields (NeRF). To this end, we present the first approach for inferring a NeRF volume from only a monocular colour event stream that enables 3D-consistent, dense and photo-realistic novel view synthesis in the RGB space at test time.
Our method, which we name EventNeRF, is designed for purely event-based supervision, during which we preserve the resolution of the individual RGB event channels. We evaluate our technique for the task of novel view synthesis on a new dataset of synthetic and real event sequences. This allows subjective and numerical evaluations against ground truth. We evaluate our NeRF estimation in scenarios that would not be conceivable with a traditional RGB camera (e.g., high speed movements, motion blur or insufficient lighting), and show our method produce significantly better results (see Fig. 1). We also show an application of extract-ing the depth map of an arbitrary viewpoint. To summarise, the primary technical contributions are as follows: 1) EventNeRF, the first approach for inferring NeRF from a monocular colour event stream that enable novel view synthesis in the RGB space at test time. Our method is designed for event-based supervision, during which we preserve the resolution of the individual RGB event channels (while avoiding demosaicing; see Sec. 3.4). 2) A ray sampling strategy tailored to events that allows for data-efficient training. As a part of it, our negative sampling avoids artefacts in the background (Sec. 3.7). 3) An experimental evaluation protocol for the task of novel view synthesis from event streams in the RGB space. We will release our code and dataset to estab-lish a benchmark for future work (Sec. 4). 2.