Abstract
Input segmentation
Class affinity transfer, Standard training, 20k training images 100 training images
Semantic image synthesis aims to generate photo re-alistic images given a semantic segmentation map. De-spite much recent progress, training them still requires large datasets of images annotated with per-pixel label maps that are extremely tedious to obtain. To alleviate the high an-notation cost, we propose a transfer method that leverages a model trained on a large source dataset to improve the learning ability on small target datasets via estimated pair-wise relations between source and target classes. The class affinity matrix is introduced as a first layer to the source model to make it compatible with the target label maps, and the source model is then further finetuned for the target do-main. To estimate the class affinities we consider different approaches to leverage prior knowledge: semantic segmen-tation on the source domain, textual label embeddings, and self-supervised vision features. We apply our approach to
GAN-based and diffusion-based architectures for semantic synthesis. Our experiments show that the different ways to estimate class affinity can be effectively combined, and that our approach significantly improves over existing state-of-the-art transfer approaches for generative image models. 1.

Introduction
Image synthesis with deep generative models has made remarkable progress in the last decade with the introduc-tion of GANs [11], VAEs [17], and diffusion models [14].
Generated images can be conditioned on diverse types of in-puts, such as class labels [3, 18], text [10, 25, 27], bounding boxes [35], or seed images [5]. In semantic image synthesis, the generation is conditioned on a semantic map that indi-cates the desired class label for every pixel. This task has been thoroughly explored with models such as SPADE [23] and OASIS [33], capable of generating high-quality and di-verse images on complex datasets such as ADE20K [43] and COCO-Stuff [4]. However, these approaches heavily rely on the availability of large datasets with tens to hun-dreds of thousands of images annotated with pixel-precise label maps that are extremely costly to acquire. For the
Figure 1. Can we train a semantic image synthesis model from only 100 images? Our diffusion-based transfer results using train-ing set of 100 ADE20K images (2nd col.) compared to the same model trained from scratch on full dataset (20k images, 3rd col.).
Cityscapes dataset [7], e.g., on average more than 1.5h per image was required for annotation and quality control.
High annotation costs can be a barrier to deployment of machine learning models in practice, and motivates the development of transfer learning strategies to alleviate the annotation requirements. These techniques allow training models on small target datasets via the use of models pre-trained on a source dataset with many available annotations.
Transfer learning has been widely studied for classification tasks such as object recognition [2, 16, 26], but received much less attention in the case of generation tasks. This task has been considered for unconditional and class-conditional generative models [19, 21, 22, 38, 39, 41], but to the best of our knowledge few-shot transfer learning has not yet been explored in the setting of semantic image synthesis.
We introduce CAT, a finetuning procedure that models
Class Affinity to Transfer knowledge from pre-trained se-mantic image synthesis models. Our method takes advan-tage of prior knowledge to establish pairwise relations be-tween source and target classes, and encodes them in a class affinity matrix. This solution considerably eases learning
when few instances of the target classes are available at training time. The affinity matrix is prepended to the source model to make it compatible with the label space of the tar-get domain. The model can then be further finetuned us-ing the available data for the target domain. To illustrate the generality of the proposed approach, we integrate our transfer learning strategy in state-of-the-art adversarial and diffusion models. We explore different ways to extract sim-ilarities between source and target classes, using semantic segmentation models for the source data, self-supervised vi-sion features, and text-based class embeddings.
We conduct extensive experiments on the ADE20K,
COCO-Stuff, and Cityscapes datasets, using target datasets with sizes ranging from as little as 25 up to 400 images.
Our experiments show that our approach significantly im-proves over state-of-the-art transfer methods. As illustrated in Figure 1, our approach allows realistic synthesis from no more than 100 target images, and achieves image quality close to standard training on the full target datasets. More-over, unlike previous transfer methods, our approach also enables non-trivial training-free transfer results, where we only prepend the class affinity matrix to the source model, without further finetuning it.
In summary, our contributions are the following:
• We introduce Class Affinity Transfer (CAT), the first transfer method for semantic image synthesis for small target datasets, and explore different methods to define class affinity, based on semantic segmentation, self-supervised features, and text-based similarity.
• We integrate our approach in state-of-the-art adversar-ial and diffusion based semantic synthesis models.
• We obtain excellent experimental transfer results, im-proving over existing state-of-the-art approaches. 2.