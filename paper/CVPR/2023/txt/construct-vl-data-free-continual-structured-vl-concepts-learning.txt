Abstract
Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL
Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting.
In this work, we introduce the first Continual Data-Free Structured VL
Concepts Learning (ConStruct-VL) benchmark1 and show it is challenging for many existing data-free CL strategies.
We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Replay (APR) which generates adversarial reminders of past tasks from past task models. To use this method efficiently, we also propose a continual parameter-efficient Layered-LoRA (LaLo) neural architecture allowing no-memory-cost access to all past models at train time. We show this approach outperforms all data-free methods by as much as ∼ 7% while even matching some levels of experience-replay (prohibitive for applications where data-privacy must be preserved). 1.

Introduction
Recently, large Vision-and-Language (VL) models achieved great advances in zero-shot learning [4, 16, 16, 19, 23, 25, 40, 51, 58]. Pre-trained on hundreds of millions [40] or billions [45] of image-and-text pairs collected from the
*This work is supported by the Defense Advanced Research Projects
Agency (DARPA) Contract No. FA8750-19-C-1001. Any opinions, find-ings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA.
†Equal contribution 1Our code is publicly available at https : / / github . com / jamessealesmith/ConStruct-VL
Illustration for the Continual Data-Free Structured
Figure 1.
VL Concepts Learning (ConStruct-VL). Structured VL Concept (SVLC) understanding skills are added / refined over time, with
Adversarial Pseudo-Replay (APR) effectively countering catas-trophic forgetting using our Layered-LoRA (LaLo) architecture’s ability of efficient no-memory-cost access to past task models. web, these VL models have demonstrated remarkable ca-pabilities in understanding (recognizing [16, 40], detecting
[63], segmenting [57], etc.) objects appearing in images or videos [39], thus moving beyond the previous paradigm of fixed object classes to open-vocabulary models.
Despite these great advances, several recent works [52, 67] have found VL models to be brittle with respect to un-derstanding Structured VL Concepts (SVLCs) - non-object textual content such as object attributes, states, inter-object relations (e.g. spatial, interaction, etc.), and more. Nat-urally, it is important to alleviate these shortcomings, as this lack of understanding of SVLCs can lead to embarrass-ing errors on the VL model’s part for many applications, such as analysis of multi-modal social networks data, multi-modal chat, multi-modal document analysis, and more. Im-portantly, in most of these applications: (i) different errors made by the model surface sequentially in the course of time, each time identifying another SVLC ‘skill’ missing in the model; (ii) typically, to fix the errors, one would at-tempt additional fine-tuning of the model on a data collected from the source on which the errors were observed (hoping to avoid catastrophic forgetting of the previous model im-Figure 2. Layered-LoRA (LaLo) architecture and Adversarial Pseudo-Replay (APR), illustrated for a two-task ConStruct-VL sequence teaching ‘color’→‘spatial relation’ understanding skills. (1) A text+image pair is entering the model M2 currently training to understand spatial relations; M2 is an extension of the model M1 (previously trained to understand color) via adding a layer of low-rank (LoRA) adapters to every parametric function of M1 while keeping M1 frozen. (2) M2 produces positive prediction probability PM2 (after softmax) and applies CE loss. (3) Text+image pair are passed through M1 (done without reloading in LaLo). (4) M1 produces a positive prediction probability PM1 ; using −PM1 as adversarial loss, we perform a sign-gradient attack on the text input (after tokenizer+first embedding layer). The produced text is expected to be adversarial in the direction of the SVLC used to train M1, color in this case. We, therefore, expect some embedded text tokens to become ‘colored’ inconsistently with the image. (5) We use the produced adversarial sample to distill from M1 into M2 via the proposed pseudo-replay positive prediction probability drop preserving loss. provement rounds); (iii) data sources where errors are ob-served are typically private and cannot be passed between these fine-tuning tasks. We, therefore, ask the question:
Can we sequentially add SVLC skills to multi-modal VL models, in a privacy-preserving manner? This leads to the data-free continual learning problem (e.g., [55, 56]) cast in the multi-modal VL domain.
To this end, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) multi-modal benchmark, built on top of the popular Visual
Genome [21] and Visual Attributes in the Wild [37] datasets using the protocol proposed in VL-Checklist [67], and show it is challenging for many existing data-free CL strate-gies, including recent SOTA prompting-based CL meth-ods [55, 56]. We then offer a novel data-free CL method, leveraging the multi-modal nature of the problem, to effec-tively avoid forgetting.We propose the concept of Adversar-ial Pseudo-Replay (APR), that (as opposed to the previous pseudo-replay works [5, 46, 60]) generates negative exam-ples to past task models conditioned on the current batch data. For continual VL training we generate negatives in one of the modalities by making it inconsistent with the other modality via a series of adversarial attack steps utiliz-ing past models (see Fig. 2 for an example). Intuitively, gen-erating (inconsistent) negatives (APR) is easier than gener-ating (consistent) positives (pseudo-replay). Also, past task model attacks are likely to generate inconsistencies corre-sponding to their tasks, thus leading to reduced forgetting when we use the generated negative samples to distill from the past task models the drop in prediction probabilities af-ter adversarial examples are applied (Fig. 1). To use the proposed APR technique we need the ability to efficiently invoke past models at training time. We, therefore, pro-pose a Layered-LoRA (LaLo) continual learning neural ar-chitecture utilizing layered parameter-efficient (low-rank) residual adapters, supporting invocation of any of the past task models on any given training batch at no additional memory cost (without the need to reload these past mod-els into memory). Moreover, our proposed architecture can be collapsed to the original model size by collapsing all the adapters into their adapted parametric functions, thus sup-porting inference on the final model at no additional cost.
Contributions: (i) we propose the challenging ConStruct-VL benchmark, and show that existing data-free CL meth-ods struggle in this setting; (ii) we propose a new concept of Adversarial Pseudo-Replay (APR) specifically designed for multi-modal continual learning, alongside a Layered-LoRA (LaLo) architecture allowing invoking any of the past task models efficiently without reloading and having no additional inference time or parameters cost; and (iii) we demonstrate significant improvements (over 6.8% increase in final accuracy and ×5 smaller average forgetting) of the proposed approach compared to all the popular data-free CL baselines, as well as some amounts of experience replay. 2.