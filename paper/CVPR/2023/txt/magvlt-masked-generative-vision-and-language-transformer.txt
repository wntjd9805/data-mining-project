Abstract
While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a gener-ation of one fixed modality conditioned on the other modal-ity. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask predic-tion, named MAGVLT, and compare it with an autoregres-sive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predic-tions in an iterative refinement, and extended editing capa-bilities such as image and text infilling. For rigorous train-ing of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreover, we devise two additional tasks based on the step-unrolled mask prediction and the selective prediction on the mixture of two image-text pairs. Experimental results on various downstream gener-ation tasks of VL benchmarks show that our MAGVLT out-performs ARGVLT by a large margin even with significant inference speedup. Particularly, MAGVLT achieves com-petitive results on both zero-shot image-to-text and text-to-image generation tasks from MS-COCO by one moderate-sized model (fewer than 500M parameters) even without the use of monomodal data and networks. 1.

Introduction
Generalizable multimodal modeling has recently made a lot of progress, especially in the field of vision-and-language (VL) modeling [3, 17, 30, 43, 45, 46, 48, 62, 63, 66, 67]. In particular, a massive amount of image-text data
[9, 11, 51, 52, 54, 55] allows robust pretraining of large-scale multimodal VL models that can be easily transferred to var-ious downstream tasks including image captioning [2, 38],
∗Contributed equally.
†Work done at Kakao Brain. Corresponding author. text-guided image generation [16, 17, 19, 40, 45, 46, 48, 67], visual question answering [23], and image-text retrieval
[30, 38, 42, 43]. In this respect, many multimodal VL pre-training algorithms have been proposed in the literature, and these can be broadly categorized into either discrimina-tive or generative learning algorithms. Discriminative pre-training such as contrastive learning [30, 43] aims to obtain semantic representations effective for discriminative tasks while generative pretraining learns them to reconstruct an input [4, 8, 13, 25, 32, 35, 61, 63, 64]. The recent growth of model capacity and data size has led to more interest in gen-erative pretraining since it can provide more diverse and im-proved generalization ability both for VL understanding and
VL generation tasks.
While generative VL pretraining has been widely ex-ploited, most existing algorithms focus on representation learning for VL understanding tasks [7, 8, 12, 13, 33, 35, 63] or conditional generation tasks where a generation is per-formed on one fixed modality conditioned on the other modality [3, 13, 14, 16, 17, 19, 25, 31, 40, 45, 46, 48, 61, 64, 66, 67]. A few algorithms have tried to produce data in both modalities from a single VL model [4, 32]. If one universal model can generate both modalities, it would be beneficial in concentrating training efforts on a single model as well as resource-saving under a resource-constrained deployment.
Moreover, we can expect task extension as well as syn-ergetic performance improvement between the modalities from this multimodal generation. Therefore, in this work, we develop a unified generative VL model.
There are two prevalent paradigms of the generative modeling for image and text processing: autoregressive (AR) generative modeling [16, 32, 46, 61, 67] and non-AR generative modeling [4, 5, 45, 48]. Many previous algo-rithms adopt AR modeling due to its excellent generation results and high training scalability, especially with trans-former networks. However, AR modeling has restrictions in unidirectional conditioning in that an image needs to be flattened into a 1D sequence by an unnatural ordering. In addition, AR sampling is performed by one-by-one predic-tions of elements, which incurs very slow generation for a long sequence. Recently, in order to overcome these limita-tions of AR modeling, non-AR generative modeling based
on the mask prediction has been proposed for language [22], image [10, 36], and video processing [24, 59]. Masked modeling is usually employed for representation learning to solve understanding tasks in language, vision, and VL do-mains. However, with an iterative refinement-based gener-ation and a variable mask ratio during training, it has been shown to be used as a promising generative modeling. In this regard, for our generative VL modeling, we propose
Masked Generative VL Transformer (MAGVLT). In con-trast to AR-based generative VL transformer (ARGVLT), the proposed MAGVLT is able to exploit bidirectional con-ditioning and fast generation through a small number of re-finement steps and parallel token predictions.
In specific, MAGVLT can generate any or both of an im-age and a text sequence conditioned also on any or both of them. Namely, it can perform any kind of task in a form of image-and-text-to-image-and-text (IT2IT), includ-ing image-to-text (I2T) and text-to-image (T2I) tasks. Fol-lowing the previous masked generative modeling [10, 22], we conduct sampling by iterative denoising based on the masked token prediction and train MAGVLT by the masked token prediction objective with a randomly sampled mask ratio to take into account various denoising steps. Here, to perform robust training of MAGVLT especially with only image-text pairs from scratch, MAGVLT is learned basi-cally by the composition of image-to-text, text-to-image, and joint image-and-text mask prediction objectives. We observe that our cross-modal masking (joint image-and-text mask prediction) during training helps in improving both performances of I2T and T2I tasks over single-modal mask-ing (image-to-text + text-to-image mask predictions). Note that only masked generative modeling used in MAGVLT enables this cross-modal mask prediction during training.
In addition, we propose to use two additional tasks based on the step-unrolled mask prediction and the selective pre-diction on the mixture of two image-text pairs. The former one is motivated by SUNDAE [50] and is modified to per-form the mask prediction on the unrolled prediction, which simulates the masked input samples encountered at the in-termediate refinement steps. On the other hand, the latter one learns to reconstruct the masked tokens in accordance with a selected context between two VL contexts that are mixed as a noisy input. This selective prediction improves cross-modal attention for an accurate generation.
Through experiments on various downstream VL gener-ation tasks, we empirically demonstrate that our MAGVLT significantly outperforms ARGVLT even with greatly re-duced inference time. Especially, to the best of our knowl-edge, MAGVLT is the first model that obtains strong perfor-mances on both zero-shot I2T and zero-shot T2I generation tasks of MS-COCO benchmark [38] by a single moderate-sized model (fewer than 500M parameters) without relying on monomodal data and networks. Previously, as unified generative VL models, L-Verse [32] and UPGen [4] have not showed zero-shot I2T results while OFA [62] has used monomodal data and also has not showed zero-shot I2T and
T2I results. Extensive ablations also validate the contribu-tion of each component for MAGVLT.
To summarize, our main contributions are: (1) a masked generative VL transformer as a unified generative VL model that can produce both images and texts; (2) a robust train-ing on image-text pairs by multiple training tasks that in-clude the cross-modal mask prediction in tandem with the step-unrolled mask prediction and the selective prediction on the mixed context; and (3) an empirical validation of
MAGVLT that outperforms the autoregressive model and moreover shows competitive performances on both of zero-shot I2T and T2I generation tasks for the first time without employing extra monomodal data and networks. 2.