Abstract
Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently pop-ularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation
Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation into the learned latent pose structure and show that it allows meaningful test-time camera transformations and accurate explicit pose readouts. Perhaps surprisingly, RUST achieves similar quality as methods which have access to perfect camera pose, thereby unlocking the potential for large-scale training of amortized neural scene representations. 1.

Introduction
Implicit neural representations have shown remarkable ability in capturing the 3D structure of complex real-world scenes while circumventing many of the downsides of mesh based, point cloud based, and voxel grid based representa-tions [24]. Apart from visually pleasing novel view syn-*Correspondence: rust@msajjadi.com. Project page: rust-paper.github.io.
Contributions: MS: Conception, model design, implementation lead, anal-ysis, infrastructure, experiments, writing, project lead. AM: Conception, model design, implementation, code reviews, experiments-MSN, analysis-MSN, writing. TK: RUST explicit pose estimation, model ﬁgures, scoping, advising, writing. EP: GNeRF experiments, Street View dataset, interac-tive visualization for embedding inspection. DD: COLMAP experiments, evaluation. ML: Team buy-in, scoping, writing. KG: Conception, early
MSN analysis, analysis-SV, data-collection and analysis, dataset genera-tion, visualizations, scoping, advising, writing.
Figure 1. Model overview – RUST produces 3D-centric scene representations through novel view synthesis purely from RGB images without requiring any camera poses. For training, a novel
Pose Estimator module glimpses at the target view and passes a low-dimensional latent pose feature to the decoder. thesis [14, 16], such representations have shown potential for semantics [25], object decomposition [21], and physics simulation [2] which makes them promising candidates for applications in augmented reality and robotics. However, to be useful in such applications, they need to (1) provide meaningful representations when conditioned on a very lim-ited number of views, (2) have low latency for real-time rendering, and (3) produce scene representations that facili-tate generalization of knowledge to novel views and scenes.
The recently proposed Scene Representation Transformer (SRT) [22] exhibits most of these properties. It achieves state-of-the-art novel view synthesis in the regime when only a handful of posed input views are available, and produces representations that are well-suited for later segmentation at both semantic [22] and instance level [21]. A major chal-lenge in scaling methods such as SRT, however, is the dif-ﬁculty in obtaining accurately posed real world data which precludes the training of the models. We posit that it should be possible to train truly pose-free models from RGB images alone without requiring any ground truth pose information.
We propose RUST (Really Unposed Scene representation
Transformer), a novel method for neural scene representation learning through novel view synthesis that does not require pose information: neither for training, nor for inference; neither for input views, nor for target views. While at ﬁrst
glance it may seem unlikely that such a model could be trained, our key insight is that a sneak-peek at the target view at training can be used to infer an implicit latent pose, thereby allowing the rendering of the correct view. We ﬁnd that our model not only learns meaningful, controllable latent pose spaces, but the quality of its novel views are even comparable to the quality of posed methods.
Our key contributions are as follows:
• We propose RUST, a novel method that learns latent 3D scene representations through novel view synthesis on very complex synthetic and real datasets without any pose information.
• Our model strongly outperforms prior methods in settings with noisy camera poses while matching their performance when accurate pose information is available to the base-lines.
• We provide an investigation into the structure of the learned latent pose spaces and demonstrate that mean-ingful camera transformations naturally emerge in a fully unsupervised fashion.
• Finally, we demonstrate that the representations learned by the model allow explicit pose readout and dense semantic segmentation. 2. Method 2 3
⇥
W
The model pipeline is shown in Fig. 1. A data point consists of an unordered set of N input views x = xi 2
RH of a scene. Unlike SRT [22], they only consist of
RGB images since RUST does not use explicit poses. Given these input views, the training objective is to predict a novel target view y 3 of the same scene.
W
}
{
⇥
⇥
⇥
RH
To this end, the input views x are ﬁrst encoded using a combination of a CNN and a transformer, resulting in the
Set-Latent Scene Representation (SLSR) S which captures the contents of the scene. The target view y is then rendered by a transformer-based decoder that attends into the SLSR
S to retrieve relevant information about the scene for novel view synthesis. In addition, the decoder must be conditioned on some form of a query that identiﬁes the desired view.
Existing methods, including SRT [22], often use the explicit relative camera pose p between one of the input views and the target view for this purpose. This imbues an explicit notion of 3D space into the model, which introduces a bur-densome requirement for accurate camera poses, especially for training such models. RUST resolves this fundamental limitation by learning its own notion of implicit poses.
Instead of querying the decoder with ex-Implicit poses. plicit poses, we allow the model to learn its own implicit space of camera poses through a learned Pose Estimator module. For training, the Pose Estimator sees parts of the tar-get view y and the SLSR S and extracts a low-dimensional
Latent Pose
Avg. Pool + Project to 8 dims
MLP
Multi-Head Attention 3x
Key
Value
Query
Multi-Head Attention
Key
Value
Query
Ref. View SLSR
CNN
Half Target View
Figure 2. Pose estimator model – A randomly chosen half of the target image is encoded into a latent pose. The model mainly consists of a CNN encoder followed by alternating cross- and self-attention layers. The ﬁnal output is projected to 8 dimensions to encourage an easily controllable latent representation. latent pose feature ˜p. The decoder transformer then uses ˜p as a query to cross-attend into the SLSR to ultimately render the full novel view ˜y. This form of self supervision allows the model to be trained with standard reconstruction losses without requiring any pose information. At test time, latent poses can be computed on the input views and subsequently modiﬁed for novel view synthesis, see Sec. 4.2. 2.1. Model components
Input view encoder. The encoder consists of a convo-lutional neural network (CNN) followed by a transformer.
Each input image xi is encoded independently by the shared
CNN which consists of 3 downsampling blocks, each of which halves the image height and width. As a result, each spatial output feature corresponds to an 8 8 patch in the original input image.
⇥
We add the same learned position embeddings to all spa-tial feature maps to mark their spatial 2D position in the images. We further add another learned embedding only to the features of the ﬁrst input image x1 to allow the model to distinguish them from the others. This is relevant for the
Pose Estimator module, as explained below. Finally, we ﬂat-ten all spatial feature maps and combine them across input views into a single set of tokens. The encoder transformer then performs self-attention on this set of tokens, thereby exchanging information between the patch features. This results in the SLSR S which captures the scene content as a bag of tokens.
Pose estimator. The model architecture of the Pose Estima-tor is shown in Fig. 2. A randomly chosen half of the target
⇢ view y (i.e., either the left or the right half of the image) is
ﬁrst embedded into a set of tokens using a CNN similar to the input view encoder. A transformer then alternates between cross-attending from the target view tokens into a speciﬁc subset ˜S
S of the SLSR and self-attending between the target view tokens. The intuition behind cross-attending into
˜S is that the latent pose ˜p should be relative to the scene. We allow the Pose Estimator to only attend into SLSR tokens belonging to the (arbitrarily chosen) ﬁrst input view after empirical ﬁndings that this leads to better-structured latent pose spaces. It is important to note that ˜S contains infor-mation about all input views due to the self-attention in the preceding encoder transformer.
Finally, we apply global mean pooling on the trans-former’s output and linearly project it down to an 8-dimensional latent pose ˜p. It is important to note that we call
˜p the estimated “pose” since it primarily serves the purpose of informing the decoder of the target camera pose. How-ever, we do not enforce any explicit constraints on the latent, instead allowing the model to freely choose the structure of the latent poses. Nevertheless, we ﬁnd that the model learns to model meaningful camera poses, see Sec. 4.2.
Decoder transformer. Similar to SRT, each pixel is de-coded independently by a decoder transformer that cross-attends from a query into the SLSR S, thereby aggregating relevant information from the scene representation to estab-lish the appearance of the novel view point. We initialize the query by concatenating the latent pose ˜p with the spatial 2D position of the pixel in the target image and passing it through a small query MLP. The output of the decoder is the single RGB value of the target pixel. 2.2. Training procedure
For each data point during training, we use 5 input views and 3 novel target views. The training objective is to render the target views by minimizing the mean-squared error be-2 2. tween the predicted output and the target view:
The entire model is trained end-to-end using the Adam opti-mizer [9]. In practice, we found the model to perform better when gradients ﬂowing to and through the Pose Estimator module are scaled down by 0.2 which is inspired by spatial transformer networks [7].
 
˜y
||
|| y 3.