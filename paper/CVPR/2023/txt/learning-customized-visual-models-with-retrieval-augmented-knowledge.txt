Abstract
Image-text contrastive learning models such as CLIP have demonstrated strong task transfer ability. The high generality and usability of these visual models is achieved via a web-scale data collection process to ensure broad con-cept coverage, followed by expensive pre-training to feed all the knowledge into model weights. Alternatively, we propose REACT, REtrieval-Augmented CusTomization, a framework to acquire the relevant web knowledge to build customized visual models for target domains. We retrieve the most relevant image-text pairs ( 3% of CLIP pre-training data) from the web-scale database as external knowledge and propose to customize the model by only training new modularized blocks while freezing all the original weights.
The effectiveness of REACT is demonstrated via extensive experiments on classification, retrieval, detection and seg-mentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark (20 datasets).
∼ 1.

Introduction
It has been a fundamental research problem in computer vision (CV) to build a transferable visual system that can easily adapt to a wide range of downstream tasks. With remarkable advances in deep learning, a de facto solution to achieve this is to train deep neural networks on a large amount of data to pursue the so-called generic visual repre-sentations. This dates back to the standard supervised train-ing on ImageNet [10], whose superb representation power is further demonstrated in BiT [23]/ViT [12] by scaling up the training to JFT300M [50]. Along the way, recent efforts have been applied to the popular image self-supervised learn-ing [6, 16, 17] to reduce the demand for labeled data. The
♠ core contribution; ¶ equal advising; § work initiated during an internship at Microsoft. third approach is image-text contrastive learning trained on billion-scale web-crawled image-text pairs. Such models, like CLIP [43] and ALIGN [20], are able to achieve great performance on different downstream domains, without the need of any human labels.
Excellent empirical performance has been achieved with the above three pre-training methods, by following the well established two-stage pre-training then adaptation pipeline: model pre-training from scratch on large data, then model adaptation directly on downstream tasks. Specifically, the pre-trained models are adapted to downstream tasks by con-sidering the available task-specific samples only: either eval-uated in a zero-shot task transfer manner, or updated using linear probing (LP) [43], finetuning (FT) [27], or prompt tun-ing [44,71]. Following this two-stage pipeline, most research has reverted to the faith that building transferable visual sys-tems is equivalent to developing more generic visual models by feeding all knowledge in the model pre-training stage.
Therefore, the community has been witnessing a trend in exploring scaling success of pre-training model and data size with less care on the target domain, hoping that the model can adapt to any downstream scenario.
In this paper, we argue that the conventional two-stage pipeline above is over-simplified and less efficient, in achiev-ing the goal of building a transferable visual system in real-world settings. Instead, we propose a customization stage in between the pre-training and adaptation, where customiza-tion is implemented by systematically leveraging retrieved external knowledge. The inspiration comes from how hu-mans are specialized in society for better generalization: instead of trying to memorize all concepts, humans are trained/prepared in a relevant subject to master a certain skill, while maintaining the basic skills in pre-training.
To this end, we explore a systematic approach to acquire and learn with external knowledge sources from a large image-text corpus for model customization. The process of collecting external image-text knowledge is fully automatic without extra human annotation. The acquired knowledge typically contains richer information about the concept: rel-evant images that never appear in the downstream training
Figure 1. REACT achieves the best zero-shot ImageNet performance among public checkpoints (Left), achieves new SoTA on semi-supervised
ImageNet classification in the 1% labeled data setting (Middle), and consistently transfer better than CLIP on across a variety of tasks, including ImageNet classification, zero/few/full-shot classification on 20 datasets in ELEVATER benchmark, image-text retrieval, object detection and segmentation (Right). Please see the detailed numbers and settings in the experimental section. For the left figure, circle size indicates model size. and evaluation set, and richer text descriptions about concept semantics. Such multi-modal knowledge sources are gen-erally available on the web, and further open-sourced like
LAION [45, 46]. They cover a variety of domains, making it possible to develop customized visual models for task-level transfer. Similar retrieval-augmented intuitions have been exploited in computer vision for class-level transfer [32], but not yet for task-level transfer (similar to that of CLIP). Our main findings/contributions can be summarized as follows.
We propose to explore the potential of the web-scale image-text corpus as external knowledge to significantly improve task-level transfer performance on the target do-main at an affordable cost. A simple and effective strategy is proposed. To begin with, we build a large-scale multi-modal indexing system to retrieve the relevant image-text pairs using CLIP features and approximate nearest neigh-bor search. For a CV problem, the task instruction is often sufficiently specified with text such as class names, which allows us to utilize them as queries to retrieve the relevant image-text pair knowledge from the indexing system. No images from the CV problem are needed. To efficiently build the customized visual model, we propose a novel modular-ized learning strategy: only updating the additional trainable weights on the retrieved knowledge, and freezing the origi-nal model weights. Hence, the model masters the new skill without forgetting basic skills.
The generality and effectiveness of the proposed cus-tomization strategy is demonstrated on four CV problems.
We instantiate it with CLIP, and develop the customized visual models for image classification on ImageNet and 20 datasets in ELEVATER [27], image-text retrieval on
COCO [30]/Flickr [41], as well as object detection and se-mantic segmentation on COCO [30]. The knowledge bases are considered as LAION [46] and larger web-crawled multi-3% modal data. The retrieval-augmented knowledge ( image-text pairs compared with the original training data) significantly improves the model’s zero-shot performance
∼ without the need of accessing any images on downstream tasks. See Figure 1 for highlighted results. For example, our ViT-L/14 checkpoint achieves 78.5% zero-shot accuracy on ImageNet [10], surpassing all public checkpoints from
CLIP [43] and OpenCLIP [18], including those with larger model size and trained on a much larger LAION-2B [45].
The new customized models demonstrate higher few/full-shot performance than the generic model counterparts.
Our retrieval system, codebase, and pre-trained models are publicly available. To make this line of research more accessible, our retrieved subsets for both ELEVATER and
ImageNet will also be made available, with an easy-to-use toolkit to download the subsets without storing the whole dataset locally. It poses a feasible direction for leveraging the ever-increasing data from the Internet for customized visual recognition, especially for the low-resource regimes. 2.