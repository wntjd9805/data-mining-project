Abstract
Human body trajectories are a salient cue to identify actions in the video. Such body trajectories are mainly conveyed by hands and face across consecutive frames in sign language. However, current methods in continuous sign language recognition (CSLR) usually process frames independently, thus failing to capture cross-frame trajec-tories to effectively identify a sign. To handle this limita-tion, we propose correlation network (CorrNet) to explic-itly capture and leverage body trajectories across frames to identify signs. In specific, a correlation module is first pro-posed to dynamically compute correlation maps between the current frame and adjacent frames to identify trajec-tories of all spatial patches. An identification module is then presented to dynamically emphasize the body trajec-tories within these correlation maps. As a result, the gen-erated features are able to gain an overview of local tem-poral movements to identify a sign. Thanks to its spe-cial attention on body trajectories, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, i.e.,
PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. A com-prehensive comparison with previous spatial-temporal rea-soning methods verifies the effectiveness of CorrNet. Visu-alizations demonstrate the effects of CorrNet on emphasiz-ing human body trajectories across adjacent frames. 1.

Introduction
Sign language is one of the most widely-used commu-nication tools for the deaf community in their daily life.
However, mastering this language is rather difficult and time-consuming for the hearing people, thus hindering di-rect communications between two groups. To relieve this problem, isolated sign language recognition tries to classify a video segment into an independent gloss1. Continuous sign language recognition (CSLR) progresses by sequen-tially translating images into a series of glosses to express a sentence, more prospective toward real-life deployment. 1Gloss is the atomic lexical unit to annotate sign languages.
Figure 1. Visualization of correlation maps with Grad-CAM [39].
It’s observed that without extra supervision, our method could well attend to informative regions in adjacent left/right frames to iden-tify human body trajectories.
Human body trajectories are a salient cue to identify ac-tions in human-centric video understanding [44].
In sign language, such trajectories are mainly conveyed by both manual components (hand/arm gestures), and non-manual components (facial expressions, head movements, and body postures) [10,35]. Especially, both hands move horizontally and vertically across consecutive frames quickly, with fin-ger twisting and facial expressions to express a sign. To track and leverage such body trajectories is of great impor-tance to understanding sign language.
However, current CSLR methods [5, 6, 16, 33, 34, 36, 54] usually process each frame separately, thus failing to exploit such critical cues in the early stage. Especially, they usually adopt a shared 2D CNN to capture spatial features for each frame independently.
In this sense, frames are processed individually without interactions with adjacent neighbors, thus inhibited to identify and leverage cross-frame trajec-tories to express a sign. The generated features are thus not aware of local temporal patterns and fail to perceive the hand/face movements in expressing a sign. To han-dle this limitation, well-known 3D convolution [4] or its (2+1)D variants [42, 49] are potential candidates to cap-ture short-term temporal information to identify body tra-jectories. Other temporal methods like temporal shift [30] or temporal convolutions [31] can also attend to short-term temporal movements. However, it’s hard for them to aggre-gate beneficial information from distant informative spatial regions due to their limited spatial-temporal receptive field.
Besides, as their structures are fixed for each sample dur-ing inference, they may fail to dynamically deal with dif-ferent samples to identify informative regions. To tackle these problems, we propose to explicitly compute correla-tion maps between adjacent frames to capture body trajec-tories, referred to as CorrNet. As shown in fig. 1, our ap-proach dynamically attends to informative regions in adja-cent left/right frames to capture body trajectories, without relying on extra supervision.
In specific, our CorrNet first employs a correlation mod-ule to compute correlation maps between the current frame and its adjacent frames to identify trajectories of all spa-tial patches. An identification module is then presented to dynamically identify and emphasize the body trajecto-ries embodied within these correlation maps. This proce-dure doesn’t rely on extra expensive supervision like body keypoints [53] or heatmaps [54], which could be end-to-end trained in a lightweight way. The resulting features are thus able to gain an overview of local temporal move-ments to identify a sign. Remarkably, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, i.e.,
PHOENIX14 [26], PHOENIX14-T [2], CSL-Daily [52], and CSL [23], thanks to its special attention on body tra-jectories. A comprehensive comparison with other spatial-temporal reasoning methods demonstrates the superiority of our method. Visualizations hopefully verify the effects of CorrNet on emphasizing human body trajectories across adjacent frames. 2.