Abstract
Single Domain Generalization (SDG) tackles the prob-lem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on
SDG object detection remains almost non-existent. To ad-dress the challenges of simultaneously learning robust ob-ject localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features ex-tracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD [52], on their own diverse weather-driving benchmark. 1.

Introduction
As for most machine learning models, the performance of object detectors degrades when the test data distribu-tion deviates from the training data one. Domain adap-tation techniques [3, 5, 8, 33, 44, 46] try to alleviate this problem by learning domain invariant features between a source and a known target domain. In practice, however, it is not always possible to obtain target data, even un-labeled, precluding the use of such techniques. Domain generalization tackles this by seeking to learn representa-tions that generalize to any target domain. While early approaches [1, 10, 28, 29, 31, 50, 60] focused on the sce-nario where multiple source domains are available during training, many recent methods tackle the more challenging, yet more realistic, case of Single Domain Generalization (SDG), aiming to learn to generalize from a single source dataset. While this has been well studied for image clas-sification [14, 38, 48, 51, 59], it remains a nascent topic in object detection. To the best of our knowledge, a single ex-isting approach, Single-DGOD [52], uses disentanglement and self-distillation [25] to learn domain-invariant features.
In this paper, we introduce a fundamentally different ap-Figure 1. Semantic Augmentation: We compare the PCA pro-jections of CLIP [39] image embeddings obtained in two different manners: (Top) The embeddings were directly obtained from the real images from 5 domains corresponding to different weather conditions. (Bottom) The embeddings were obtained from the day images only and modified with our semantic augmentation strat-egy based on text prompts to reflect the other 4 domains. Note that the relative positions of the clusters in the bottom plot resembles that of the top one, showing that our augmentations let us gener-alize to different target domains. The principal components used are the same for both the figures. proach to SDG for object detection. To this end, we build on two observations: (i) Unsupervised/self-supervised pre-training facilitates the transfer of a model to new tasks [2,
4, 20]; (ii) Exploiting language supervision to train vision models allows them to generalize more easily to new cat-egories and concepts [9, 39].
Inspired by this, we there-fore propose to leverage a self-supervised vision-language model, CLIP [39], to guide the training of an object detec-tor so that it generalizes to unseen target domains. Since the visual CLIP representation has been jointly learned with the textual one, we transfer text-based domain variations to the image representation during training, thus increasing the di-versity of the source data.
Specifically, we define textual prompts describing po-tential target domain concepts, such as weather and day-time variations for road scene understanding, and use these prompts to perform semantic augmentations of the images.
These augmentations, however, are done in feature space, not in image space, which is facilitated by the joint image-text CLIP latent space. This is illustrated in Fig. 1, which shows that, even though we did not use any target data for semantic augmentation, the resulting augmented embed-dings reflect the distributions of the true image embeddings from different target domains.
We show the effectiveness of our method on the SDG driving dataset of [52], which reflects a practical scenario where the training (source) images were captured on a clear day whereas the test (target) ones were acquired in rainy, foggy, night, and dusk conditions. Our experiments demonstrate the benefits of our approach over the Single-DGOD [52] one.
To summarize our contributions, we employ a vision-language model to improve the generalizability of an object detector; during training, we introduce domain concepts via text-prompts to augment the diversity of the learned image features and make them more robust to an unseen target do-main. This enables us to achieve state-of-the-art results on the diverse weather SDG driving benchmark of [52]. Our implementation can be accessed through the following url: https://github.com/vidit09/domaingen. 2.