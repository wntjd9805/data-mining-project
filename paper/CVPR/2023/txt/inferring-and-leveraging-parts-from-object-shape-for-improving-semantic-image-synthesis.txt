Abstract 1.

Introduction
Despite the progress in semantic image synthesis, it re-mains a challenging problem to generate photo-realistic parts from input semantic map. Integrating part segmen-tation map can undoubtedly beneﬁt image synthesis, but is bothersome and inconvenient to be provided by users. To improve part synthesis, this paper presents to infer Parts from Object ShapE (iPOSE) and leverage it for improving semantic image synthesis. However, albeit several part seg-mentation datasets are available, part annotations are still not provided for many object categories in semantic image synthesis. To circumvent it, we resort to few-shot regime to learn a PartNet for predicting the object part map with the guidance of pre-deﬁned support part maps. PartNet can be readily generalized to handle a new object cate-gory when a small number (e.g., 3) of support part maps for this category are provided. Furthermore, part semantic modulation is presented to incorporate both inferred part map and semantic map for image synthesis. Experiments show that our iPOSE not only generates objects with rich part details, but also enables to control the image synthe-sis ﬂexibly. And our iPOSE performs favorably against the state-of-the-art methods in terms of quantitative and qual-itative evaluation. Our code will be publicly available at https://github.com/csyxwei/iPOSE.
Semantic image synthesis allows to generate an image with input semantic map, which provides signiﬁcant ﬂexi-bility for controllable image synthesis. Recently, it has at-tracted intensive attention due to its wide applications, e.g., content generation and image editing [28,36], and extensive beneﬁts for many vision tasks [33].
Albeit rapid progress has been made in semantic im-age synthesis [10, 25, 28, 30, 33, 36, 43–45], it is still chal-lenging to generate photo-realistic parts from the semantic map. Most methods [28, 33] tackle semantic image syn-thesis with a spatially adaptive normalization architecture, while other frameworks are also explored, such as Style-GAN [19] and diffusion model [43, 45], etc. However, due to the lack of ﬁne-grained guidance (e.g., object part infor-mation), these methods only exploited the object-level se-mantic information for image synthesis, and usually failed to generate photo-realistic object parts (see the top row of
Fig. 1(c)). SAFM [25] adopted the shape-aware position descriptor to exploit the pixel’s position feature inside the object. However, as illustrated in Fig. 1(a), the obtained de-scriptor tends to be only region-aware instead of part-aware, leading to limited improvement in synthesized results (see the middle row of Fig. 1(c)).
To improve image parts synthesis, one straightforward solution is to ask the user to provide part segmentation map and integrate it into semantic image synthesis dur-ing training and inference. However, it is bothersome and inconvenient for users to provide it, especially during in-ference. Fortunately, with the existing part segmentation datasets [7], we propose a method to infer Parts from Object
ShapE (iPOSE), and leverage it for improving semantic im-age synthesis. Speciﬁcally, based on these datasets, we ﬁrst construct an object part dataset that consists of paired (ob-ject shape, object part map) to train a part prediction net-work (PartNet). Besides, although the part dataset con-tains part annotations for several common object categories, many object categories in semantic image synthesis are still not covered. To address this issue, we introduce the few shot mechanism to our proposed PartNet. As shown in Fig. 1(b), for each category, a few annotated object part maps are se-lected as pre-deﬁned supports to guide the part prediction.
Cross attention block is adopted to aggregate the part infor-mation from support part maps. Beneﬁted from the few shot setting, our PartNet can be readily generalized to handle a new object category not in the object part dataset. Partic-ularly, we can manually label k object part maps for this category (e.g., 3) as supports, and use them to infer the part map without ﬁne-tuning the part prediction model.
By processing each object in the semantic map, we ob-tain the part map. With that, we further present a part se-mantic modulation (PSM) residual block to incorporate the part map with semantic map to improve the image syn-thesis. Speciﬁcally, the part map is ﬁrst used to modulate the normalized activations spatially to inject the part struc-ture information. Then, to inject the semantic texture in-formation, the semantic map and a randomly sampled 3D noise are further used to modulate the features with the
SPADE module [28]. We ﬁnd that performing part modula-tion and semantic modulation sequentially disentangles the structure and texture for image synthesis, and is also beneﬁ-cial for generating images with realistic parts. Additionally, to facilitate model training, a global adversarial loss and an object-level CLIP style loss [54] are further introduced to encourage model to generate photo-realistic images.
Experiments show that our iPOSE can generate more photo-realistic parts from the given semantic map, while having the ﬂexibility to control the generated objects (as illustrated in Fig. 1 (c)(d)). Both quantitative and quali-tative evaluations on different datasets further demonstrate that our method performs favorably against the state-of-the-art methods.
The contributions of this work can be summarized as:
• We propose a method iPOSE to infer parts from object shape and leverage them to improve semantic image synthesis. Particularly, a PartNet is proposed to predict the part map based on a few support part maps, which can be easily generalized to new object categories.
• A part semantic modulation Resblock is presented to incorporate the predicted part map and semantic map for image synthesis. And global adversarial and object-level CLIP style losses are further introduced to generate photo-realistic images.
• Experimental results show that our iPOSE performs fa-vorably against state-of-the-art methods and can gen-erate more photo-realistic results with rich part details. 2.