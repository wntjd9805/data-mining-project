Abstract
The ability to quickly learn a new task with minimal in-struction – known as few-shot learning – is a central as-pect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufﬁcient to characterize an entire concept class. In contrast, humans use cross-modal infor-mation to learn new concepts efﬁciently. In this work, we demonstrate that one can indeed build a better visual dog classiﬁer by reading about dogs and listening to them bark.
To do so, we exploit the fact that recent multimodal founda-tion models such as CLIP are inherently cross-modal, map-ping different modalities to the same representation space.
Speciﬁcally, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning dif-ferent modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classiﬁer for vision-language adaptation. Furthermore, we show that our approach can beneﬁt existing methods such as preﬁx tuning, adapters, and classiﬁer ensembling. Finally, to explore other modalities beyond vision and language, we construct the ﬁrst (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classiﬁcation. Project site at link. 1.

Introduction
Learning with minimal instruction is a hallmark of hu-man intelligence [86, 91, 98], and is often studied under the guise of few-shot learning. In the context of few-shot visual classiﬁcation [18, 20, 29, 46, 79, 82], a classiﬁer is ﬁrst pre-trained on a set of base classes to learn a good feature repre-sentation and then adapted or ﬁnetuned on a small amount of novel class data. However, such few-shot setups often face an inherent ambiguity – if the training image contains a golden retriever wearing a hat, how does the learner know if
*Equal contribution.
Figure 1. Human perception is internally cross-modal. When we perceive from one modality (such as vision), the same neu-rons will be triggered in our cerebral cortex as if we are perceiv-ing the object from other modalities (such as language and au-dio) [24, 67, 70]. This phenomenon grants us a strong ability to learn from a few examples with cross-modal information [52, 67].
In this work, we propose to leverage cross-modality to adapt mul-timodal models (such as CLIP [81] and AudioCLIP [27]), that en-code different modalities to the same representation space. the task is to ﬁnd dogs, golden retrievers, or even hats? On the other hand, humans have little trouble under-standing and even generalizing from as few as one example.
How so?
We argue that humans make use of multimodal sig-nals and representations (Figure 1) when learning concepts.
For example, verbal language has been shown to help tod-dlers better recognize visual objects given just a few ex-amples [42, 90]. Indeed, there exists ample evidence from neuroscience suggesting that cognitive representations are inherently multimodal. For instance, visual images of a person evoke the same neurons as the textual strings of the person’s name [80] and even audio clips of that person talk-ing [70]. Even for infants as young as 1-5 months old, there is a strong correspondence between auditory-visual [52] as well as visual-tactile signals [67]. Such cross-modal or inter-modal representations are fundamental to the human perceptual-cognitive system, allowing us to understand new concepts even with few examples [24].
Cross-modal adaptation (our approach). In this paper, we demonstrate that cross-modal understanding of different modalities (such as image-text or image-audio) can improve the performance of individual modalities. That is, reading about dogs and listening to them bark can help build a better visual classiﬁer for them! To do so, we present a remark-ably simple strategy for cross-modal few-shot adaptation: we treat examples from different modalities as additional few-shot examples. For example, given the “1-shot” task of learning a dog classiﬁer, we treat both the textual dog label and the single visual image as training examples for learning a (visual) dog classiﬁer. Learning is straightfor-ward when using frozen textual and visual encoders, such as CLIP [81], that map different modalities to the same rep-resentational space. In essence, we have converted the “n-shot” problem to a “(n+1)-shot” problem (Figure 2)! We demonstrate that this basic strategy produces SOTA results across the board with a simple linear classiﬁer, and can be applied to existing ﬁnetuning methods [100,111,113] or ad-ditional modalities (e.g. audio).
Why does it work? From one perspective, it may not be surprising that cross-modal adaptation improves accu-racy, since it takes advantage of additional training exam-ples that are “hidden” in the problem deﬁnition, e.g. a label name [104] or an annotation policy [68] for each class. However, our experiments demonstrate that multi-modal cues are often complementary since they capture dif-ferent aspects of the underlying concept; a dog label paired with a single visual example is often more performant than two images! For example, Figure 3 demonstrates a one-shot example where the target concept is ambiguous, but becomes clear once we add information from other modali-ties like language and sound.
{ cls
}
Multimodal adaptation (prior art). In contrast to our cross-modal approach, most prior works simply follow the popular practice of ﬁnetuning uni-modal foundation mod-els, such as large vision [12, 31, 32] or language mod-els [8, 17, 62]. For example, CoOp [113] and other prompt-ing methods [63,112,114] ﬁnetune CLIP via preﬁx tuning to replace hand-engineered prompts such as "a photo of
" with learned word tokens. Similarly, inspired a by parameter-efﬁcient tuning of language models [39], adapter-based methods [21,111] ﬁnetune CLIP by inserting lightweight multi-layer-perceptrons (MLPs). However, we aim to study the fundamental question of how to ﬁnetune multi-modal (as opposed to uni-modal) models. A crucial difference between prior art and ours is the use of textual in-formation, as all existing methods [41, 100, 111, 113] repur-pose additional text features as classiﬁer weights instead of training samples. We demonstrate in this paper that cross-modal adaptation is not only more performant but can also beneﬁt prior uni-modal approaches.
Problem setup. We begin by replicating the existing
Figure 2. Adding additional modalities helps few-shot learn-ing. Adding textual labels to a 2-shot cat-vs-dog classiﬁcation task leads to better test performance (by turning the problem into a 3-shot cross-modal task!). We visualize cross-modal CLIP [21] features (projection to 2D with principal component analysis) and the resulting classiﬁer learned from them, and observe a large shift in the decision boundary. See Figure 5 for more examples. evaluation protocol of other works [81, 111, 113] on few-shot adaptation of vision-language models, and report per-formance on 11 diverse downstream datasets. We produce state-of-the-art accuracy with an embarrassingly simple lin-ear classiﬁer that has access to additional “hidden” train-ing examples in the form of textual labels, resulting in a system that is far more lightweight than prior art. Interest-ingly, we show that existing approaches [100, 111, 113], de-spite already repurposing text features as classiﬁer weights, can still beneﬁt from cross-modal learning. Finally, we ex-tend our work to the audio domain by taking advantage of
AudioCLIP [27] that maps audio to the same frozen CLIP representation space. We construct the ﬁrst (to our knowl-edge) cross-modal few-shot learning benchmark with audio by intersecting ImageNet [15] and the ESC-50 audio clas-siﬁcation dataset [77]. We show that cross-modal audiovi-sual learning helps for both downstream image and audio classiﬁcation; in summary, one can train better dog image classiﬁers by listening to them bark! 2.