Abstract 1.

Introduction
We introduce Structured 3D Features, a model based on a novel implicit 3D representation that pools pixel-aligned image features onto dense 3D points sampled from a para-metric, statistical human mesh surface. The 3D points have associated semantics and can move freely in 3D space. This allows for optimal coverage of the person of interest, be-yond just the body shape, which in turn, additionally helps modeling accessories, hair, and loose clothing. Owing to this, we present a complete 3D transformer-based attention framework which, given a single image of a person in an un-constrained pose, generates an animatable 3D reconstruc-tion with albedo and illumination decomposition, as a re-sult of a single end-to-end model, trained semi-supervised, and with no additional postprocessing. We show that our
S3F model surpasses the previous state-of-the-art on vari-ous tasks, including monocular 3D reconstruction, as well as albedo & shading estimation. Moreover, we show that the proposed methodology allows novel view synthesis, re-lighting, and re-posing the reconstruction, and can natu-rally be extended to handle multiple input images (e.g. dif-ferent views of a person, or the same view, in different poses, in video). Finally, we demonstrate the editing capabilities of our model for 3D virtual try-on applications.
† Work was done while Enric and Mihai were with Google Research.
Human digitization is playing a major role in several im-portant applications, including AR/VR, video games, social telepresence, virtual try-on, or the movie industry. Tradi-tionally, 3D virtual avatars have been created using multi-view stereo [33] or expensive equipment [48]. More flex-ible or low-cost solutions are often template-based [1, 4], but these lack expressiveness for representing details such as hair. Recently, research on implicit representations [6, 16, 22, 52, 53] and neural fields [24, 46, 63, 70] has made significant progress in improving the realism of avatars.The proposed models produce detailed results and are also ca-pable, within limits, to represent loose hair or clothing.
Even though model-free methods yield high-fidelity avatars, they are not suited for downstream tasks such as animation. Furthermore, proficiency is often limited to a certain range of imaged body poses. Aiming at these problems, different efforts have been made to combine parametric models with more flexible implicit represen-tations [21, 22, 62, 71]. These methods support anima-tion [21,22] or tackle challenging body poses [62,71]. How-ever, most work relies on 2D pixel-aligned features. This leads to two important problems: (1) First, errors in body pose or camera parameter estimation will result in misalign-ment between the projections of 3D points on the body sur-face and image features, which ultimately results in low
quality reconstructions. (2) image features are coupled with the input view and cannot be easily manipulated e.g. to edit the reconstructed body pose.
In this paper we introduce Structure 3D Features (S3F), a flexible extension to image features, specifically designed to tackle the previously discussed challenges and to provide more flexibility during and after digitization. S3F store lo-cal features on ordered sets of points around the body sur-face, taking advantage of the geometric body prior. As body models do not usually represent hair or loose clothing, it is difficult to recover accurate body parameters for images in-the-wild. To this end, instead of relying too much on the ge-ometric body prior, our model freely moves 3D body points independently to cover areas that are not well represented by the prior. This process results in our novel S3Fs, and is trained without explicit supervision only using reconstruc-tion losses as signals. Another limitation of prior work is its dependence on 3D scans. We alleviate this dependence by following a mixed strategy: we combine a small col-lection of 3D scans, typically the only training data con-sidered in previous work, with large-scale monocular in-the-wild image collections. We show that by guiding the training process with a small set of 3D synthetic scans, the method can efficiently learn features that are only available for the scans (e.g. albedo), while self-supervision on real images allow the method to generalize better to diverse ap-pearances, clothing types and challenging body poses.
In this paper, we show how the proposed S3Fs are sub-stantially more flexible than current state-of-the-art repre-sentations. S3Fs enable us to train a single end-to-end model that, based on an input image and matching body pose parameters, can generate a 3D human reconstruction that is relightable and animatable. Furthermore, our model supports the 3D editing of e.g. clothing, without additional post-processing. See Fig.1 for an illustration and Table 1 for a summary of our model’s properties, in relation to prior work. We compare our method with prior work and demon-strate state-of-the-art performance for monocular 3D hu-man reconstruction from challenging, real-world and un-constrained images, and for albedo & shading estimation.
We also provide an extensive ablation study to validate our different architectural choices and training setup. Finally, we show how the proposed approach can also be naturally extended to integrate observations across different views or body poses, further increasing reconstruction quality. 2.