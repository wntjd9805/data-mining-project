Abstract
Deep learning in general domains has constantly been extended to domain-speciﬁc tasks requiring the recognition of ﬁne-grained characteristics. However, real-world appli-cations for ﬁne-grained tasks suffer from two challenges: a high reliance on expert knowledge for annotation and ne-cessity of a versatile model for various downstream tasks in a speciﬁc domain (e.g., prediction of categories, bounding boxes, or pixel-wise annotations). Fortunately, the recent self-supervised learning (SSL) is a promising approach to pretrain a model without annotations, serving as an effective initialization for any downstream tasks. Since SSL does not rely on the presence of annotation, in general, it utilizes the large-scale unlabeled dataset, referred to as an open-set. In this sense, we introduce a novel Open-Set Self-Supervised
Learning problem under the assumption that a large-scale unlabeled open-set is available, as well as the ﬁne-grained target dataset, during a pretraining phase. In our problem setup, it is crucial to consider the distribution mismatch be-tween the open-set and target dataset. Hence, we propose
SimCore algorithm to sample a coreset, the subset of an open-set that has a minimum distance to the target dataset in the latent space. We demonstrate that SimCore signiﬁcantly improves representation learning performance through ex-tensive experimental settings, including eleven ﬁne-grained datasets and seven open-sets in various downstream tasks. 1.

Introduction
The success of deep learning in general computer vision tasks has encouraged its widespread applications to speciﬁc domains of industry and research [21, 46, 73], such as facial recognition or vehicle identiﬁcation. We particularly focus on the visual recognition of ﬁne-grained datasets, where the goal is to differentiate between hard-to-distinguish images.
However, real-world application for ﬁne-grained tasks poses two challenges for practitioners and researchers developing algorithms. First, it requires a number of experts for anno-*equal contribution tation, which incurs a large cost [3, 14, 60]. For example, ordinary people do not have professional knowledge about aircraft types or ﬁne-grained categories of birds. Therefore, a realistic presumption for a domain-speciﬁc ﬁne-grained dataset is that there may be no or very few labeled samples.
Second, ﬁne-grained datasets are often re-purposed or used for various tasks according to the user’s demand, which mo-tivates development of a versatile pretrained model. One might ask, as a target task, that bird images be classiﬁed by species or even segmented into foreground and back-ground. A good initialization model can handle a variety of annotations for ﬁne-grained datasets, such as multiple attributes [43, 47, 74], pixel-level annotations [53, 74], or bounding boxes [35, 47, 53].
Recently, self-supervised learning (SSL) [11, 14, 24, 28] has enabled learning how to represent the data even without annotations, such that the representations serve as an effec-tive initialization for any future downstream tasks. Since labeling is not necessary, SSL generally utilizes an open-set, or large-scale unlabeled dataset, which can be easily obtained by web crawling [23, 64], for the pretraining. In this paper, we introduce a novel Open-Set Self-Supervised
Learning (OpenSSL) problem, where we can leverage the open-set as well as the training set of ﬁne-grained target dataset. Refer to Figure 1 for the overview of OpenSSL.
In the OpenSSL setup, since the open-set may contain instances irrelevant to the ﬁne-grained dataset, we should consider the distribution mismatch. A large distribution mismatch might inhibit representation learning for the target task. For instance, in Figure 2, SSL on the open-set (OS) does not always outperform SSL on the ﬁne-grained dataset (X) because it depends on the semantic similarity between X and
OS. This is in line with the previous observations [21, 22, 64] that the performance of self-supervised representation on downstream tasks is correlated with similarity of pretraining and ﬁne-tuning datasets.
To alleviate this mismatch issue, we could exploit a core-set, a subset of an open-set, which shares similar semantics with the target dataset. As a motivating experiment, we man-ually selected the relevant classes from ImageNet (OSoracle) that are supposed to be helpful according to each target
Self-Supervised
Pretraining
Downstream 
Tasks
?
+Coreset
?
Image
Classification
Semantic
Segmentation
Object
Detection manufacturer variant airplane: 93%
Fine-Grained Dataset
Open-Set
Figure 1. Overview of an OpenSSL problem. For any downstream tasks, we pretrain an effective model with the ﬁne-grained dataset via self-supervised learning (SSL). Here, the assumption for a large-scale unlabeled open-set in the pretraining phase is well-suited for a real-world scenario. The main goal is to ﬁnd a coreset, highlighted by the blue box, among the open-set to enhance ﬁne-grained SSL.
X
OS
X+OS
X+OSrand
X+OSoracle 50% 45% 40% 35% 30% 60% 50% 40% 30% 80% 70% 60% 50% 40% 40% 35% 30% 25% 20%
Target (X) Classes for OSoracle (#)
Aircraft
Cars
Pet
Birds airliner, warplane, ... (8) convertible, jeep, ... (10)
Persian cat, beagle, ... (24) goldﬁnch, junco, ... (20)
Aircraft
Cars
Pet
Birds
Figure 2. Linear evaluation performance on the ﬁne-grained target dataset. Each color corresponds to a pretraining dataset, while “+” means merging two datasets. The table on the right side shows the manually selected categories from an open-set (OS), ImageNet-1k [20] in this case, according to each target dataset (X). Selected categories and exact numbers are detailed in Appendix A. We followed the typical linear evaluation protocol [14, 24] and used the SimCLR method [14] on ResNet50 encoder [29]. dataset. Interestingly, in Figure 2, merging OSoracle to X shows a signiﬁcant performance gain, and its superiority over merging the entire open-set (X+OS) or the randomly sampled subset (X+OSrand) implies the necessity of a sam-pling algorithm for the coreset in the OpenSSL problem.
Therefore, we propose SimCore, a simple yet effective coreset sampling algorithm from an unlabeled open-set. Our main goal is to ﬁnd a subset semantically similar to the target dataset. We formulate the data subset selection problem to obtain a coreset that has a minimum distance to the target dataset in the latent space. SimCore signiﬁcantly improves performance in extensive experimental settings (eleven ﬁne-grained datasets and seven open-sets), and shows consistent gains with different architectures, SSL losses, and down-stream tasks. Our contributions are outlined as follows:
• We ﬁrst propose a realistic OpenSSL task, assuming an unlabeled open-set available during the pretraining phase on the ﬁne-grained dataset.
• We propose a coreset selection algorithm, SimCore, to leverage a subset semantically similar to the target dataset.
• Our extensive experiments with eleven ﬁne-grained datasets and seven open-sets substantiate the signiﬁcance of data selection in our OpenSSL problem. 2.