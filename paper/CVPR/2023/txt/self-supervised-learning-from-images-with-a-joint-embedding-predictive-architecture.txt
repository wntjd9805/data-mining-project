Abstract
ImageNet-1K Linear Evaluation vs GPU Hours
This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide
I-JEPA towards producing semantic representations is the masking strategy; speciﬁcally, it is crucial to (a) sample tar-get blocks with sufﬁciently large scale (semantic), and to (b) use a sufﬁciently informative (spatially distributed) context block. Empirically, when combined with Vision Transform-ers, we ﬁnd I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classiﬁcation to object counting and depth prediction. 1.

Introduction
In computer vision, there are two common families of approaches for self-supervised learning from images: invariance-based methods [9, 16, 17, 23, 34, 36, 71] and gen-erative methods [7, 27, 35, 56].
Invariance-based pretraining methods optimize an en-coder to produce similar embeddings for two or more views of the same image [14, 19], with image views typically constructed using a set of hand-crafted data augmentations, such as random scaling, cropping, and color jittering [19], amongst others [34]. These pretraining methods can pro-duce representations of a high semantic level [3, 17], but they also introduce strong biases that may be detrimental for certain downstream tasks or even for pretraining tasks with different data distributions [1]. Often, it is unclear
*massran@meta.com
)
% ( 1 p o
T
I-JEPA
ViT-H/16448 (300ep) 82 81
I-JEPA
ViT-H/14 (300ep) 80 79 78 77 76
CAE
ViT-L/16 (1600ep)
MAE
ViT-H/14 (1600ep) data2vec
ViT-L/16 (1600ep) 103 104 105 106
Pretraining GPU Hours
Figure 1.
ImageNet Linear Evaluation. The I-JEPA method learns semantic image representations without using any view data augmentations during pretraining. By predicting in representation space, I-JEPA produces semantic representations while using less compute than previous methods. how to generalize these biases for tasks requiring differ-ent levels of abstraction. For example, image classiﬁcation and instance segmentation do not require the same invari-ances [10]. Additionally, it is not straightforward to gen-eralize these image-speciﬁc augmentations to other modal-ities such as audio.
Cognitive learning theories have suggested that a driv-ing mechanism behind representation learning in biologi-cal systems is the adaptation of an internal model to pre-dict sensory input responses [30, 58]. This idea is at the core of self-supervised generative methods, which remove or corrupt portions of the input and learn to predict the cor-rupted content [8, 35, 56, 65, 66, 69].
In particular, mask-denoising approaches learn representations by reconstruct-ing randomly masked patches from an input, either at the pixel or token level. Masked pretraining tasks require less
sx x-encoder
D(sx, sy) sy y-encoder z decoder
ˆy
D( ˆy, y) z predictor x-encoder x-encoder
ˆsy
D( ˆsy, sy) sy y-encoder x y x y x y (a) Joint-Embedding Architecture (b) Generative Architecture (c) Joint-Embedding Predictive Architecture
Figure 2. Common architectures for self-supervised learning, in which the system learns to capture the relationships between its inputs.
The objective is to assign a high energy (large scaler value) to incompatible inputs, and to assign a low energy (low scaler value) to compat-ible inputs. (a) Joint-Embedding Architectures learn to output similar embeddings for compatible inputs x, y and dissimilar embeddings for incompatible inputs. (b) Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a decoder network that is conditioned on additional (possibly latent) variables z to facilitate reconstruction. (c) Joint-Embedding Predictive Architec-tures learn to predict the embeddings of a signal y from a compatible signal x, using a predictor network that is conditioned on additional (possibly latent) variables z to facilitate prediction. prior knowledge than view-invariance approaches and eas-ily generalize beyond the image modality [7]. However, the resulting representations are typically of a lower semantic level and underperform invariance-based pretraining in off-the-shelf evaluations (e.g., linear-probing) and in transfer settings with limited supervision for semantic classiﬁcation tasks [3]. Consequently, a more involved adaptation mech-anism (e.g., end-to-end ﬁne-tuning) is required to reap the full advantage of these methods.
In this work, we explore how to improve the semantic level of self-supervised representations without using extra prior knowledge encoded through image transformations.
To that end, we introduce a joint-embedding predictive ar-chitecture [47] for images (I-JEPA). An illustration of the method is provided in Figure 3. The idea behind I-JEPA is to predict missing information in an abstract representa-tion space; e.g., given a single context block, predict the representations of various target blocks in the same im-age, where target representations are computed by a learned target-encoder network. that predict
Compared to generative methods in pixel/token space, I-JEPA makes use of abstract prediction targets for which unnecessary pixel-level details are poten-tially eliminated, thereby leading the model to learn more semantic features. Another core design choice to guide
I-JEPA towards producing semantic representations is the proposed multi-block masking strategy. Speciﬁcally, we demonstrate the importance of predicting sufﬁciently large target blocks in the image, using an informative (spatially distributed) context block.
Through an extensive empirical evaluation, we demon-strate that:
• I-JEPA learns strong off-the-shelf representations without the use of hand-crafted view augmentations
I-JEPA outperforms pixel-reconstruction (cf. Fig.1). methods such as MAE [35] on ImageNet-1K linear probing, semi-supervised 1% ImageNet-1K, and se-mantic transfer tasks.
• I-JEPA is competitive with view-invariant pretraining approaches on semantic tasks and achieves better per-formance on low-level visions tasks such as object counting and depth prediction (Sections 5 and 6). By using a simpler model with less rigid inductive bias,
I-JEPA is applicable to a wider set of tasks.
• I-JEPA is also scalable and efﬁcient (Section 7). Pre-training a ViT-H/14 on ImageNet requires less than 1200 GPU hours, which is over 2.5 faster than a ViT-S/16 pretrained with iBOT [75] and over 10 more ef-ﬁcient than a ViT-H/14 pretrained with MAE. Predict-ing in representation space signiﬁcantly reduces the to-tal computation needed for self-supervised pretraining.
⇥
⇥ 2.