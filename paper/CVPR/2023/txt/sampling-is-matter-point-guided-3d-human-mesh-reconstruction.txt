Abstract
This paper presents a simple yet powerful method for 3D human mesh reconstruction from a single RGB image.
Most recently, the non-local interactions of the whole mesh vertices have been effectively estimated in the transformer while the relationship between body parts also has begun to be handled via the graph model. Even though those ap-proaches have shown the remarkable progress in 3D hu-man mesh reconstruction, it is still difficult to directly infer the relationship between features, which are encoded from the 2D input image, and 3D coordinates of each vertex. To resolve this problem, we propose to design a simple fea-ture sampling scheme. The key idea is to sample features in the embedded space by following the guide of points, which are estimated as projection results of 3D mesh ver-tices (i.e., ground truth). This helps the model to concen-trate more on vertex-relevant features in the 2D space, thus leading to the reconstruction of the natural human pose.
Furthermore, we apply progressive attention masking to precisely estimate local interactions between vertices even under severe occlusions. Experimental results on bench-mark datasets show that the proposed method efficiently im-proves the performance of 3D human mesh reconstruction.
The code and model are publicly available at: https:
//github.com/DCVL-3D/PointHMR_release. 1.

Introduction
The goal of 3D human mesh reconstruction is to esti-mate 3D coordinates of points, which make up the human body surface. Since the high-quality 3D human model has been consistently required for various immersive applica-tions, many studies have devoted considerable efforts to accurately reconstruct the 3D human mesh.
In the early stage of this field, complex optimization techniques were
*equal contribution
†corresponding author
Figure 1. (a) Traditional process of feature extraction for estimat-ing 3D coordinates. (b) Vertex-relevant feature extraction process based on the proposed point-guided sampling method for estimat-ing 3D coordinates. adopted to generate the 3D human model based on the re-lationship between multiple scenes, which are acquired by using stereo or multiple-view camera systems. Recently, owing to the great success of deep learning, the problem of 3D human mesh reconstruction now can be resolved only with a single RGB image, thus the majority has begun to develop compact network architectures and efficient train-ing strategies. Even though such deep learning-based ap-proaches have shown the significant progress in 3D human mesh reconstruction, this task is still challenging due to se-vere occlusions by diverse human poses and depth ambigu-ities by the monocular setting.
Deep learning-based approaches can be divided into two main groups: model-based and model-free methods.
In the former, most methods aim to estimate shape and pose parameters of the skinned multi-person linear (SMPL) model [24], which is capable of yielding the whole vertices via these two simple factors, thus most widely employed in this field. Traditional encoder-decoder architectures, which are mostly composed of stacked convolutional layers, are sufficient to conduct the regression for estimating those pa-rameters. Despite their great performance, model-based
methods have the obvious shortcoming, i.e., reconstruction results are limited to the pre-defined types of human body models. On the other hand, model-free methods have at-tempted to directly infer 3D coordinates of mesh vertices from input features without using any specific human body model. Compared to the model-based approach, which ob-tains the well-defined full mesh by adjusting shape and pose parameters, the model-free approach needs to estimate 3D coordinates of whole vertices directly from the network.
Most methods in this category are based on the transformer to grasp non-local interactions between mesh vertices. The graph model (e.g., graph convolution) also has been utilized together to allow for body part relations in a local manner.
One important advantage of the model-free approach is the flexibility to adapt to other applications, e.g., hand pose es-timation, without significant changes of the data format and the training strategy. However, inferring the 3D coordinate from a single monocular image is still challenging due to lack of learning the correspondence between encoded fea-tures and spatial positions.
In this paper, we propose a simple yet powerful method for 3D human mesh reconstruction. To this end, we con-duct feature sampling at vertex-relevant points of the input image as shown in Fig. 1, which are estimated through the heatmap decoder trained by projection results of 3D mesh vertices (i.e., ground truth). These sampled features are sub-sequently fed into the transformer encoder as the form of the vertex token (see Fig. 2). In a similar way of [6], we apply attention masking to the transformer encoder, how-ever, the difference is that the local connection is defined with the range of multiple levels through the sequence of transformer encoders. This progressive attention masking helps the model understand local relations between vertices precisely even in occlusions. The main contribution of the proposed method can be summarized as follows:
• We propose to utilize the correspondence between en-coded features and vertex positions, which are pro-jected into the 2D space, via our point-guided fea-ture sampling scheme. By explicitly indicating such vertex-relevant features to the transformer encoder, co-ordinates of the 3D human mesh are accurately esti-mated.
• Our progressive attention masking scheme helps the model efficiently deal with local vertex-to-vertex rela-tions even under complicated poses and occlusions. 2.