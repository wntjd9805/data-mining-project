Abstract
We introduce MarginMatch, a new SSL approach combin-ing consistency regularization and pseudo-labeling, with its main novelty arising from the use of unlabeled data train-ing dynamics to measure pseudo-label quality. Instead of using only the model’s confidence on an unlabeled example at an arbitrary iteration to decide if the example should be masked or not, MarginMatch also analyzes the behavior of the model on the pseudo-labeled examples as the training progresses, to ensure low quality predictions are masked out.
MarginMatch brings substantial improvements on four vi-sion benchmarks in low data regimes and on two large-scale datasets, emphasizing the importance of enforcing high-quality pseudo-labels. Notably, we obtain an improvement in error rate over the state-of-the-art of 3.25% on CIFAR-100 with only 25 labels per class and of 3.78% on STL-10 using as few as 4 labels per class. We make our code available at https://github.com/tsosea2/MarginMatch. 1.

Introduction
Deep learning models have seen tremendous success in many vision tasks [14, 22, 27, 42, 43]. This success can be attributed to their scalability, being able to produce better re-sults when they are trained on large datasets in a supervised fashion [15, 27, 34, 35, 43, 47]. Unfortunately, large labeled datasets annotated for various tasks and domains are diffi-cult to acquire and demand considerable annotation effort or domain expertise. Semi-supervised learning (SSL) is a powerful approach that mitigates the requirement for large labeled datasets by effectively making use of information from unlabeled data, and thus, has been studied extensively in vision [4, 5, 23, 25, 30, 36, 38, 39, 44–46].
Recent SSL approaches integrate two important com-ponents: consistency regularization [46, 49] and pseudo-labeling [25]. Consistency regularization works on the as-sumption that a model should output similar predictions when fed perturbed versions of the same image, whereas pseudo-labeling uses the model’s predictions of unlabeled examples as labels to train against. For example, Sohn et al. [41] introduced FixMatch that combines consistency reg-ularization on weak and strong augmentations with pseudo-labeling. FixMatch relies heavily on a high-confidence threshold to compute the unsupervised loss, disregarding any pseudo-labels whose confidence falls below this threshold.
While training using only high-confidence pseudo-labels has shown to consistently reduce the confirmation bias [1], this rigid threshold allows access only to a small amount of un-labeled data for training, and thus, ignores a considerable amount of unlabeled examples for which the model’s predic-tions do not exceed the confidence threshold. More recently,
Zhang et al. [49] introduced FlexMatch that relaxes the rigid confidence threshold in FixMatch to account for the model’s learning status of each class in that it adaptively scales down the threshold for a class to encourage the model to learn from more examples from that class. The flexible thresh-olds in FlexMatch allow the model to have access to a much larger and diverse set of unlabeled data to learn from, but lowering the thresholds can lead to the introduction of wrong pseudo-labels, which are extremely harmful for generaliza-tion. Interestingly, even when the high-confidence threshold is used in FixMatch can result in wrong pseudo-labels. See
Figure 1 for incorrect pseudo-labels detected in the training set after we apply FixMatch and FlexMatch on ImageNet.
We posit that a drawback of FixMatch and FlexMatch and in general of any pseudo-labeling approach is that they use the confidence of the model only at the current iteration to enforce quality of pseudo-labels and completely ignore model’s predictions at prior iterations.
In this paper, we propose MarginMatch, a new SSL ap-proach that monitors the behavior of the model on the unla-beled examples as the training progresses, from the begin-ning of training until the current iteration, instead of using only the model’s current belief about an unlabeled example (i.e., its confidence at the current iteration) to decide if the ex-ample should be masked or not. We estimate a pseudo-label’s contribution to learning and generalization by introducing pseudo-margins of unlabeled examples averaged across train-ing iterations. Pseudo-margins of unlabeled examples extend the margins from machine learning [3, 11, 18, 33] which pro-vide a measure of confidence of the outputs of the model and capture the difference between the output for the correct
FixMatch
Predicted:
Actual:
FlexMatch
Predicted:
Actual: onion bell pepper elephant camel fossa cougar green pepper handrail pop art poncho crowd uniform firefighter voleyball horse bison crowd meat market screen stopwatch pyramid obelisk decoration socks scale parking meter computer heater carpet teddy bear cabbage cauliflower tower torch screen ipod
Figure 1. Incorrect pseudo-labels propagated until the end of the training process for FixMatch and FlexMatch on ImageNet. (gold) label and the other labels. In our case, the pseudo-margins capture how much larger the assigned logit (the logit corresponding to the argmax of the model’s predic-tion) is compared with all other logits at iteration t. Similar to FlexMatch, in MarginMatch we take advantage of the flexible confidence thresholds to allow the model to learn from larger and more diverse sets of unlabeled examples, but unlike FlexMatch, we train the model itself to identify the characteristics of mislabeled pseudo-labels simply by monitoring the model’s training dynamics on unlabeled data over the iterations.
We carry out comprehensive experiments using established SSL experimental setups on CIFAR-10,
CIFAR-100 [21], SVHN [31], STL-10 [8], ImageNet [10], and WebVision [26]. Despite its simplicity, our findings indicate that MarginMatch produces improvements in performance over strong baselines and prior works on all datasets at no additional computational cost. Notably, compared to current state-of-the-art, on CIFAR-100 we see 3.02% improvement in error rate using only 4 labels per class and 3.78% improvement on STL-10 using the same extremely label-scarce setting of 4 labels per class.
In addition, on ImageNet [10] and WebVision [26] we find that MarginMatch pushes the state-of-the-art error rates by 0.97% on ImageNet and by 0.79% on WebVision.
Our contributions are as follows: 1. We introduce a new SSL approach which we call
MarginMatch that enforces high pseudo-label quality during training. Our approach allows access to a large set of unlabeled data to learn from (thus, incorporating more information from unlabeled data) and, at the same time, monitors the training dynamics of unlabeled data as training progresses to detect and filter out potentially incorrect pseudo-labels. 2. We show that MarginMatch outperforms existing works on six well-established computer vision benchmarks showing larger improvements in error rates especially on challenging datasets, while achieving similar con-vergence performance (or better) than prior works. 3. We perform a comprehensive analysis of our approach and indicate potential insights into why our Margin-Match substantially outperforms other SSL techniques. 2. MarginMatch
Notation Let L = {(x1, y1), ..., (xB, yB)} be a batch of size B of labeled examples and U = {ˆx1, ..., ˆxνB} be a batch of size νB of unlabeled examples, where ν is the batch-wise ratio of unlabeled to labeled examples. Let pθ(y|x) denote the class distribution produced by model
θ on input image x and ˆpθ(y|x) denote the argmax of this distribution as a one-hot label. Let also H(p, q) denote the cross-entropy between two probability distributions p and q. 2.1.