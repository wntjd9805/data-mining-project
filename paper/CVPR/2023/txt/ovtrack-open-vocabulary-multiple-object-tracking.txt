Abstract
The ability to recognize, localize and track dynamic ob-jects in a scene is fundamental to many real-world appli-cations, such as self-driving and robotic systems. Yet, tra-ditional multiple object tracking (MOT) benchmarks rely only on a few object categories that hardly represent the multitude of possible objects that are encountered in the real world. This leaves contemporary MOT methods limited to a small set of pre-defined object categories. In this paper, we address this limitation by tackling a novel task, open-vocabulary MOT, that aims to evaluate tracking beyond pre-defined training categories. We further develop OVTrack, an open-vocabulary tracker that is capable of tracking arbitrary object classes. Its design is based on two key ingredients:
First, leveraging vision-language models for both classifi-cation and association via knowledge distillation; second, a data hallucination strategy for robust appearance feature learning from denoising diffusion probabilistic models. The result is an extremely data-efficient open-vocabulary tracker that sets a new state-of-the-art on the large-scale, large-vocabulary TAO benchmark, while being trained solely on static images. 1.

Introduction
Multiple Object Tracking (MOT) aims to recognize, lo-calize and track objects in a given video sequence. It is a cornerstone of dynamic scene analysis and vital for many real-world applications such as autonomous driving, aug-mented reality, and video surveillance. Traditionally, MOT benchmarks [9, 11, 19, 64, 71] define a set of semantic cate-gories that constitute the objects to be tracked in the training and testing data distributions. The potential of traditional
MOT methods [3, 4, 33, 43] is therefore limited by the tax-onomies of those benchmarks. As consequence, contem-porary MOT methods struggle with unseen events, leading to a gap between evaluation performance and real-world
*Equal contribution.
Figure 1. OVTrack. We approach the task of open-vocabulary mul-tiple object tracking. During training, we leverage vision-language (VL) models both for generating samples and knowledge distilla-tion. During testing, we track both base and novel classes unseen during training by querying a vision-language model. deployment.
To bridge this gap, previous works have tackled MOT in an open-world context. In particular, Oˇsep et al. [46, 48] approach generic object tracking by first segmenting the scene and performing tracking before classification. Other works have used class agnostic localizers [10, 49] to per-form MOT on arbitrary objects. Recently, Liu et al. [37] defined open-world tracking, a task that focuses on the eval-uation of previously unseen objects. In particular, it requires any-object tracking as a stage that precedes object classifica-tion. This setup comes with two inherent difficulties. First, in an open-world context, densely annotating all objects is prohibitively expensive. Second, without a pre-defined taxonomy of categories, the notion of what is an object is am-biguous. As a consequence, Liu et al. resort to recall-based evaluation, which is limited in two ways. Penalizing false positives (FP) becomes impossible, i.e. we cannot measure the tracker precision. Moreover, by evaluating tracking in a class-agnostic manner, we lose the ability to evaluate how
well a tracker can infer the semantic category of an object.
In this paper, we propose open-vocabulary MOT as an effective solution to these problems. Similar to open-world
MOT, open-vocabulary MOT aims to track multiple objects beyond the pre-defined training categories. However, instead of dismissing the classification problem and resorting to recall-based evaluation, we assume that at test time we are given the classes of objects we are interested in. This allows us to apply existing closed-set tracking metrics [34, 70] that capture both precision and recall, while still evaluating the tracker’s ability to track arbitrary objects during inference.
We further present the first Open-Vocabulary Tracker,
OVTrack (see Fig. 1). To this end, we identify and ad-dress two fundamental challenges to the design of an open-vocabulary multi-object tracker. The first is that closed-set
MOT methods are simply not capable of extending their pre-defined taxonomies. The second is data availability, i.e. scaling video data annotation to a large vocabulary of classes
Inspired by existing works in open-is extremely costly. vocabulary detection [1, 13, 20, 76], we replace our classi-fier with an embedding head, which allows us to measure similarities of localized objects to an open vocabulary of semantic categories. In particular, we distill knowledge from
CLIP [52] into our model by aligning the image feature representations of object proposals with the corresponding
CLIP image and text embeddings.
Beyond detection, association is the core of modern MOT methods. It is driven by two affinity cues: motion and ap-pearance. In an open-vocabulary context, motion cues are brittle since arbitrary scenery contains complex and diverse camera and object motion patterns. In contrast, diverse ob-jects usually exhibit heterogeneous appearance. However, relying on appearance cues requires robust representations that generalize to novel object categories. We find that CLIP feature distillation helps in learning better appearance rep-resentations for improved association. This is especially intriguing since object classification and appearance model-ing are usually distinct in the MOT pipeline [3, 16, 65].
Learning robust appearance features also requires strong supervision that captures object appearance changes in dif-ferent viewpoints, background, and lighting. To approach the data availability problem, we utilize the recent success of denoising diffusion probabilistic models (DDPMs) in image synthesis [54,59] and propose an effective data hallucination strategy tailored to appearance modeling. In particular, from a static image, we generate both simulated positive and nega-tive instances along with random background perturbations.
The main contributions are summarized as follows: 1. We define the task of open-vocabulary MOT and pro-vide a suitable benchmark setting on the large-scale, large-vocabulary MOT benchmark TAO [9]. 2. We develop OVTrack, the first open-vocabulary multi-object tracker. It leverages vision-language models to t t + 2 t + 4
Figure 2. OVTrack qualitative results. We condition our tracker on text prompts unseen during training, namely ‘heron’, ‘hippo’ and ‘drone’, and successfully track the corresponding objects in the videos. The box color depicts object identity. improve both classification and association compared to closed-set trackers. 3. We propose an effective data hallucination strategy that allows us to address the data availability problem in open-vocabulary settings by leveraging DDPMs.
Owing to its thoughtful design, OVTrack sets a new state-of-the-art on the challenging TAO benchmark [9], outper-forming existing trackers by a significant margin while be-ing trained on static images only. In addition, OVTrack is capable of tracking arbitrary object classes (see Fig. 2), overcoming the limitation of closed-set trackers. 2.