Abstract
This paper presents a method that effectively com-bines two prevalent visual recognition methods, i.e., image classification and contrastive language-image pre-training, dubbed iCLIP. Instead of na¨ıve multi-task learning that use two separate heads for each task, we fuse the two tasks in a deep fashion that adapts the image classification to share the same formula and the same model weights with the language-image pre-training. To further bridge these two tasks, we propose to enhance the category names in image classification tasks using external knowledge, such as their descriptions in dictionaries. Extensive experiments show that the proposed method combines the advantages of two tasks well: the strong discrimination ability in im-age classification tasks due to the clean category labels, and the good zero-shot ability in CLIP tasks ascribed to the richer semantics in the text descriptions.
In particu-lar, it reaches 82.9% top-1 accuracy on IN-1K, and mean-while surpasses CLIP by 1.8%, with similar model size, on zero-shot recognition of Kornblith 12-dataset benchmark.
The code and models are publicly available at https:
//github.com/weiyx16/iCLIP. 1.

Introduction
Image classification is a classic visual problem whose goal is to classify images into a fixed set of pre-defined cat-egories. For example, the widely used ImageNet dataset [8] carefully annotated 14 million images and categorize them into 21,841 categories chosen from the WordNet [36]. For image classification, each category provides a clear taxon-omy that groups images of the same category together and separates images from different categories, and thus endows the learnt representation with strong discriminant ability.
However, this classification ability is limited to a fixed set of categories [8, 29, 51].
*Corresponding Author. The work is done when Yixuan Wei, Zhuliang
Yao, and Zhenda Xie are interns at Microsoft Research Asia.
Figure 1. An illustration of the proposed iCLIP framework. The iCLIP framework can take two types of annotations for training: classes and alt-texts. It converts the conventional image classifi-cation formula to share the same text encoder and the same co-sine classifier as that used in the contrastive language-image pre-training (CLIP). It also uses a dictionary-enhanced approach to enrich the original class names in the image classification prob-lem with external information involved in dictionaries. The deep fusion and knowledge-enriched classes both greatly improve the performance compared to na¨ıve multi-task learning or performing one of the two tasks alone.
Recently, the method that learns to contrast image-text pairs, known as contrastive language-image pre-training (abbr. CLIP), has well made up such shortage of the con-ventional image classification methods to achieve strong zero-shot recognition ability [24, 44]. These methods em-ploy a contrastive learning framework, where images and their corresponding alt-texts are treated as positive pairs, while images with all other alt-texts are treated as negative pairs. Thanks to the rich semantics involved in the alt-texts, the images can be weakly connected to almost arbitrary cat-egories that already appear in the alt-texts, resulting in its zero-shot ability. A drawback is that the image-text pairs are usually crawled from the internet without human label-ing, leading to their noisy and ambiguous nature. Thus the learnt representations are often not conceptual compact, and may lack certain discriminative ability.
This paper explores how to effectively combine these two powerful visual recognition and representation learn-ing methods, to take advantages of both methods and data sources while relieving their shortages. We first try a na¨ıve multi-task learning framework that applies the original head networks of the two tasks on top of a shared visual encoder, and jointly learn the network with separate losses of the two tasks. This na¨ıve multi-task learning approach has been able to benefit each individual tasks, but the effect is marginal.
We thus seek to fuse the two tasks more deeply, so that the advantages of the two tasks can be more effectively joined for better visual recognition, as well as for better transfer-able representations.
To this end, our first technique is to deeply unify the formulations of image classification and CLIP learning.
By examining their formulations, we found there are two main differences: 1) Different classification losses. Image classification tasks typically use a linear classification loss which has better fitting ability due to the non-normalized nature, while the CLIP-based methods adopt a cosine clas-sifier which has better transferability for new domains and categories [2, 6, 9, 18, 38, 57]. 2) Different parameteriza-tion methods for classifier weights.
Image classification tasks usually directly optimize the parametric classification weights without a need to process text semantics in class names. The CLIP method can be regarded as generating classifier weights through a text encoder and learns the text encoder instead. The text-encoder-based classifier allows sharing between alt-texts as well as modeling their relation-ships, which enables the ability to tackle any classes.
Although the linear classifier and direct classifier weight parameterization have been common practice in image clas-sification for many years, it is interesting to find that chang-ing the old formulation as that in the CLIP approach has almost no performance degradation for pure image classifi-cation problems. This indicates that we can directly adapt the image classification formulation to the cosine classifier and the text encoder parameterization used by CLIP, with almost no loss. This also allows us to further share the text encoder for both class names and alt-texts. Our experiments show that this deep fusion approach performs much better than the na¨ıve multi-task method for both in-domain/zero-shot classification and multi-modal retrieval tasks learning (see 3).
Another gap between the image classification and CLIP lies in the different text richness. Class names are usually in short, i.e., one or a few words, and sometimes are even ambiguous and polysemous in referring to specific seman-tics, for example, “night bird” can represents either “owl” or “nightingale”. On the contrary, alt-texts in CLIP are usu-ally full sentences containing rich information. To further bridge the gap between the image classification and CLIP, we propose a second technique that leverages the knowledge base to enhance the original class names, such as the expla-nations in dictionaries. In our implementation, knowledge is simply encoded as a prefix/suffix prompt, as illustrated in
Fig 1. Although simple, dictionary enhanced method shows to maintain the accuracy for pure image classification prob-lem (see Table 1), while greatly improve the zero-shot and multi-modal retrieval performance as shown in Table 2 and 3. Note the process is just like human beings who learn new words or concepts through both real examples and explana-tions in dictionaries.
By these techniques, we present a framework that deeply fuses the two important tasks of image classification and contrastive language-image pre-training, dubbed iCLIP. Ex-tensive experiments using different combinations of image classification and image-text pair datasets show that the iCLIP method can take advantages of both the discrimina-tive power of image classification tasks and the zero-shot ability in CLIP-like tasks, and perform significantly bet-ter than conducting each task alone or the na¨ıve multi-task learning in both the in-domain/zero-shot classification and multi-modal retrieval problems. The iCLIP method also shows that learning a stronger transferable representation than using each of the two tasks alone, verified on a vari-ety of downstream tasks, including ADE20K semantic seg-mentation [68], LVIS long-tail detection [17], and video ac-tion recognition [26], as well as different evaluation settings of few-shot and fine-tuning. Our contributions are summa-rized as follows:
• We combined two important vision tasks of im-age classification and contrastive language-image pre-training into a single framework.
• We found that the original image classification for-mulation can be adapted to CLIP approach with al-most no performance degradation. With this finding, we present a deep fusion approach in which the two tasks share the same text encoder and the same clas-sifier type, whose effectiveness is extensively verified on benchmarks.
• We proposed a simple yet effective method to intro-duce knowledge bases into image classification, ad-dressing the ambiguous and polysemous issue of the originally short image names as well as further bridges the gap between classes and alt-texts. It also provides the first showcase of applying knowledge bases into computer vision problems. 2.