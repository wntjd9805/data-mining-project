Abstract
In this paper, we address the problem of detecting 3D ob-jects from multi-view images. Current query-based methods rely on global 3D position embeddings (PE) to learn the ge-ometric correspondence between images and 3D space. We claim that directly interacting 2D image features with global 3D PE could increase the difficulty of learning view trans-formation due to the variation of camera extrinsics. Thus we propose a novel method based on CAmera view Position
Embedding, called CAPE. We form the 3D position embed-dings under the local camera-view coordinate system instead of the global coordinate system, such that 3D position em-bedding is free of encoding camera extrinsic parameters.
Furthermore, we extend our CAPE to temporal modeling by exploiting the object queries of previous frames and encod-ing the ego motion for boosting 3D object detection. CAPE achieves the state-of-the-art performance (61.0% NDS and 52.5% mAP) among all LiDAR-free methods on nuScenes dataset. Codes and models are available.1 1.

Introduction 3D perception from multi-view cameras is a promising solution for autonomous driving due to its low cost and rich semantic knowledge. Given multiple sensors equipped on autonomous vehicles, how to perform end-to-end 3D percep-tion integrating all features into a unified space is of critical importance. In contrast to traditional perspective-view per-ception that relies on post-processing to fuse the predictions from each monocular view [44, 45] into the global 3D space, perception in the bird’s-eye-view (BEV) is straightforward and thus arises increasing attention due to its unified repre-sentation for 3D location and scale, and easy adaptation for downstream tasks such as motion planning.
The camera-based BEV perception is to predict 3D ge-ometric outputs given the 2D features and thus the vital
*Equal contribution.
†Corresponding author. This work is done when
Kaixin Xiong is an intern at Baidu Inc. 1Codes of Paddle3D and PyTorch Implementation.
Figure 1. Comparison of the network structure between PETRv2 and our proposed CAPE. (a) In PETRv2, position embedding of queries and keys are in the global system. (b) In CAPE, position embeddings of queries and keys are within the local system of each view. Bilateral cross-attention is adopted to compute attention weights in the local and global systems independently. challenge is to learn the view transformation relationship between 2D and 3D space. According to whether the explicit dense BEV representation is constructed, existing BEV ap-proaches could be divided into two categories: the explicit
BEV representation methods and the implicit BEV repre-sentation methods. The former constructs an explicit BEV feature map by lifting the 2D perspective-view features to 3D space [12, 19, 34]. The latter mainly follow DETR-based [3] approaches in an end-to-end manner. Without projection or lift operation, those methods [24, 25, 52] implicitly encode the 3D global information into 3D position embedding (3D
PE) to obtain 3D position-aware multi-view features, which is shown in Figure 1(a).
Though learning the transformation from 2D images to 3D global space is straightforward, we reveal that the in-teraction in the global space for the query embeddings and 3D position-aware multi-view features hinders performance.
The reasons are two-fold. For one thing, defining each cam-named CAPE-T. Different from previous methods that either warp the explicit BEV features using ego-motion [11, 19] or encode the ego-motion into the position embedding [25], we adopt separated sets of object queries for each frame and encode the ego-motion to fuse the queries.
We summarize our key contributions as follows:
• We propose a novel multi-view 3D detection method, called CAPE, based on camera-view position embed-ding, which eliminates the variances of view transfor-mation caused by different camera extrinsics.
• We further generalize our CAPE to temporal modeling, by exploiting the object queries of previous frames and leveraging the ego-motion explicitly for boosting 3D object detection and velocity estimation.
• Extensive experiments on the nuScenes dataset show the effectiveness of our proposed approach and we achieve the state-of-the-art among all LiDAR-free meth-ods on the challenging nuScenes benchmark. 2.