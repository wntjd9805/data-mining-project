Abstract
We propose a novel framework and a solution to tackle the continual learning (CL) problem with changing network architectures. Most CL methods focus on adapting a single architecture to a new task/class by modifying its weights.
However, with rapid progress in architecture design, the problem of adapting existing solutions to novel architectures becomes relevant. To address this limitation, we propose
Heterogeneous Continual Learning (HCL), where a wide range of evolving network architectures emerge continually together with novel data/tasks. As a solution, we build on top of the distillation family of techniques and modify it to a new setting where a weaker model takes the role of a teacher; meanwhile, a new stronger architecture acts as a student. Furthermore, we consider a setup of limited access to previous data and propose Quick Deep Inversion (QDI) to recover prior task visual features to support knowledge trans-fer. QDI significantly reduces computational costs compared to previous solutions and improves overall performance. In summary, we propose a new setup for CL with a modified knowledge distillation paradigm and design a quick data inversion method to enhance distillation. Our evaluation of various benchmarks shows a significant improvement on accuracy in comparison to state-of-the-art methods over various networks architectures. 1.

Introduction
Over the past decade, we have seen a plethora of inno-vations in deep neural networks (DNNs) that have led to remarkable improvement in performance on several applica-tions [34, 35, 40]. AlexNet [24] was the first breakthrough showing the potential of deep learning on the ImageNet benchmark [12], it was followed by various architectural advances such as VGG [46], Inception [48], ResNet and its variants [15, 53, 58], efficient architectures [18, 49], and ap-plications of Transformer [52] in computer vision [14, 32]; however, all these architectures are trained from scratch and
*Work done during an internship at NVIDIA. compared on ImageNet classification [12]. In real-world sce-narios, the dataset size is not constant, yet storage of billions of data instances and retraining the model representations from scratch is computationally expensive and infeasible.
Further, access to old datasets is often not available due to data privacy. Thus, it is crucial to transfer the knowledge learned by the previous model representations to the recent state-of-the-advances without forgetting knowledge and ac-cessing prior datasets.
Continual learning (CL) [50] is a common way to train representations on a data stream without forgetting the learned knowledge. However, all prior CL techniques [1, 4, 33, 41, 42, 43, 59] continually adapt a fixed representation structure – a fully-connected network with two hidden lay-ers for MNIST [26] and standard ResNet-18 [15] for all the other datasets (see Figure 1). While recent works investi-gate the effect of depth and width [37] on continual learning, to our knowledge, no work has focused on the real prob-lem of learning on sequentially arriving tasks with changing network architectures. Moreover, most works assume the weight transfer with the same architecture [43, 56], in case of the previous model being available only as black box, these methods are not applicable.
In this work, we argue that maintaining the same net-work structure during training is not a realistic assumption for practical applications such as autonomous driving, clin-ical applications, and recommendation systems. Instead, constant upgrades to stronger models with the best-known architecture are vital for the best customer experience and competitive advantage, and all streams of previous data are not only large but difficult to store, transfer, manage, and protect against security breach. As a motivating example, consider the tasks of clinical diagnosis and segmentation using medical images, where it is essential to continually update the model on arriving data to adapt to the changing environment. However, continually adapting the old model architectures can potentially hurt the current and future task performance. Also, it is often not allowed to retain the previ-ous patients’ data to update to the new model due to privacy concerns. Based on this motivation, our goal is to continually update the state-of-the-art deep learning architectures with-Figure 1. Illustration of standard and heterogenous continual learning. Standard CL updates the same representational structure while preserving previous information with incoming tasks. On the contrary, the goal of HCL is to continuously evolve the network architecture while maintaining the accuracy of new data instances. out storing the previous data while preserving the knowledge from previously learned representations.
We propose a novel CL framework called Heterogeneous
Continual Learning (HCL) to learn continual representations on a data stream without any restrictions on the training con-figuration or network architecture of the representations (see
Figure 1). In particular, we consider the continual learner as a stream of learners, where we train each learner with the same or different backbone architecture. To this end, we consider a diverse range of heterogeneous architectures including LeNet [27], ResNet and its variants [15, 53, 58], and recent architectures such as RegNet [54] and vision transformers [32]. However, due to the assumption of the same architectural structure across different tasks, conven-tional regularization [1, 30, 44, 59] and architectural meth-ods [29, 43, 56] are not directly applicable for HCL.
To continually learn heterogeneous representations, we revisit knowledge distillation [16] – a method to transfer the knowledge learned across different networks. While knowledge distillation has been formulated for continual learning [4, 30, 41], prior methods do not outperform the experience replay based methods [2, 4, 41, 42]. Moreover, the distillation pipeline has not been considered for network evolution. This work shows that knowledge distillation can outperform state-of-the-art methods with and without replay examples in standard and heterogeneous continual learning settings with proper modifications. Specifically, we incorpo-rate consistency and augmentation [3] into the knowledge distillation paradigm for CL and low-temperature transfer with label-smoothing [6] for learning the network represen-tations. Our solution comes at the cost of storing and reusing only the most recent model.
The continual learner might not have access to the prior data. Therefore, motivated by DeepInversion (DI) [55], we consider data-free continual learning. In particular, DI opti-mizes random noise to generate class-conditioned samples; however, it requires many steps to optimize a single batch of instances. This increases its computational cost and slows
CL. In response to this, we propose Quick Deep Inversion (QDI), which utilizes current task examples as the starting point for synthetic examples and optimizes them to mini-mize the prediction error on the previous tasks. Specifically, this leads to an interpolation between the current task and previous task instances, which promotes current task adapta-tion while minimizing catastrophic forgetting. We compare our proposed method on various CL benchmarks against
state-of-the-art methods, where it outperforms them across all settings with a wide variety of architectures. In summary, the contributions of our work are:
• We propose a novel CL framework called Heteroge-neous Continual Learning (HCL) to learn a stream of different architectures on a sequence of tasks while transferring the knowledge from past representations.
• We revisit knowledge distillation and propose Quick
Deep Inversion (QDI), which inverts the previous task parameters while interpolating the current task exam-ples with minimal additional cost.
• We benchmark existing state-of-the-art solutions in the new setting and outperform them with our proposed method across a diverse stream of architectures for both task-incremental and class-incremental CL. 2.