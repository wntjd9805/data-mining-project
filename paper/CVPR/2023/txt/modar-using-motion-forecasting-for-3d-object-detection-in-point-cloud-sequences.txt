Abstract
Occluded and long-range objects are ubiquitous and challenging for 3D object detection. Point cloud sequence data provide unique opportunities to improve such cases, as an occluded or distant object can be observed from differ-ent viewpoints or gets better visibility over time. However, the efﬁciency and effectiveness in encoding long-term se-quence data can still be improved. In this work, we propose
MoDAR, using motion forecasting outputs as a type of vir-tual modality, to augment LiDAR point clouds. The MoDAR modality propagates object information from temporal con-texts to a target frame, represented as a set of virtual points, one for each object from a waypoint on a forecasted tra-jectory. A fused point cloud of both raw sensor points and the virtual points can then be fed to any off-the-shelf point-cloud based 3D object detector. Evaluated on the Waymo
Open Dataset, our method signiﬁcantly improves prior art detectors by using motion forecasting from extra-long se-quences (e.g. 18 seconds), achieving new state of the arts, while not adding much computation overhead. 1.

Introduction 3D object detection is a fundamental task for many appli-cations such as autonomous driving. While there has been tremendous progress in architecture design and LiDAR-camera sensor fusion, occluded and long-range object de-tection remains a challenge. Point cloud sequence data pro-vide unique opportunities to improve such cases. In a dy-namic scene, as the ego-agent and other objects move, the sequence data can capture different viewpoints of objects or improve their visibility over time. The key challenge though, is how to efﬁciently and effectively leverage se-quence data for 3D object detection.
Existing multi-frame 3D object detection methods often fuse sequence data at two different levels. At scene level, the most straightforward approach is to transform point clouds of different frames to a target frame using known
∗equal contributions
Figure 1. 3D detection model performance vs. number of input frames. Naively adding more frames to existing methods, such as
CenterPoint [59] and SWFormer [40], quickly plateaus the gains while our method, MoDAR, scales up to many more frames and gets much larger gains. L2 3D mAPH is computed by averaging vehicle and pedestrian L2 3D APH. ego motion poses [3, 40, 55, 59]. Each point can be dec-orated with an extra time channel to indicate which frame it is from. However, according to previous studies [7, 33] and our experiments shown in Fig. 1, it is difﬁcult to fur-ther improve the detection model by including more input frames due to its large computation overhead as well as in-effective temporal data fusion at scene level (especially for moving objects). On the other side, 3D Auto Labeling [33] and MPPNet [7] propose to aggregate longer temporal con-texts at object level, which is more tractable as there are much less points from objects than those from the entire scenes. However, they also fail to scale up temporal con-text aggregation to long sequences due to efﬁciency issues or alignment challenges.
In our paper, we propose to use motion forecasting to propagate object information from the past (and the future) to a target frame. The output of the forecasting model can be considered another (virtual) sensor modality to the detector model. Inspired by the naming of the LiDAR sensor, we name this new modality MoDAR, Motion forecasting based
Detection And Ranging (see Fig. 2 for an example).
Traditionally 3D object detection is a pre-processing step for a motion forecasting model, where the detector boxes are either used as input (for past frames) or learning targets (for future frames). In contrast, we use motion forecasting outputs as input to LiDAR-MoDAR multi-modal 3D object
Open Dataset [39], we show that adding MoDAR signiﬁ-cantly improves detection quality, improving CenterPoints and SWFormer by 11.1 and 8.5 mAPH respectively; and it especially helps detection of long-range and occluded objects. Using MoDAR with a 3-frame SWFormer detec-tor, we have achieved state-of-the-art mAPH on the Waymo
Open Dataset. We further provide extensive ablations and analysis experiments to validate our designs and show im-pacts of different MoDAR choices. 2.