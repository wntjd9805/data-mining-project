Abstract also provide a variety of baseline methods and share useful design practices for future work.
Towards building comprehensive real-world visual per-ception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG is related to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects localized with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes
VidSGG systems to miss key details that are crucial for comprehensive video understanding. In contrast, PVSG re-quires nodes in scene graphs to be grounded by more pre-cise, pixel-level segmentation masks, which facilitate holis-tic scene understanding. To advance research in this new area, we contribute a high-quality PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with totally 150K frames labeled with panoptic seg-mentation masks as well as fine, temporal scene graphs. We 1.

Introduction
In the last several years, scene graph generation has re-ceived increasing attention from the computer vision com-munity [15, 16, 24, 48–51]. Compared with object-centric labels like “person” or “bike,” or precise bounding boxes commonly seen in object detection, scene graphs provide far richer information in images, such as “a person riding a bike,” which capture both objects and the pairwise rela-tionships and/or interactions. A recent trend in the scene graph community is the shift from static, image-based scene graphs to temporal, video-level scene graphs [1, 41, 49].
This has marked an important step towards building more comprehensive visual perception systems.
Compared with individual images, videos clearly contain more information due to the additional temporal dimension, 1
Table 1. Comparison between the PVSG dataset and some related datasets. Specifically, we choose three video scene graph generation (VidSGG) datasets, three video panoptic segmentation (VPS) datasets, and two egocentric video datasets—one for short-term action antic-ipation (STA) while the other for video object segmentation (VOS). Our PVSG dataset is the first long-video dataset with rich annotations of panoptic segmentation masks and temporal scene graphs.
Dataset
Task
#Video Video Hours Avg. Len.
View
#ObjCls #RelCls
Annotation
# Seg Frame Year
Source
ImageNet-VidVRD [35]
Action Genome [15]
VidOR [34]
VidSGG 1,000
VidSGG 10,000
VidSGG 10,000
Cityscapes-VPS [17]
KITTI-STEP [45]
VIP-Seg [28]
Ego4D-STA [12]
VISOR [8]
PVSG
VPS
VPS
VPS
STA
VOS 500 50 3,536 1,498 179
PVSG 400
-99 82
--5 111 36 9
-35s 30s
--5s 264s 720s 77s 3rd 3rd 3rd vehicle vehicle 3rd ego ego 3rd + ego 35 80 35 19 19 124
-257 126 132 50 25
----2
Bounding Box
Bounding Box
Bounding Box
Panoptic Seg.
Panoptic Seg.
Panoptic Seg.
Bounding Box
Semantic Seg.
---3K 18K 85K
-51K 2017 2019 2020 2020 2021 2022 2022 2022
ILVSRC2016-VID [33]
YFCC100M [42]
Charades [36]
----EPIC-KITCHENS [7] 57
Panoptic Seg. 150K 2023 VidOR + Ego4D + EPIC-KITCHENS which largely facilitates high-level understanding of tempo-ral events (e.g., actions [14]) and is useful for reasoning [59] and identifying causality [10] as well. However, we ar-gue that current video scene graph representations based on bounding boxes still fall short of human visual perception due to the lack of granularity—which can be addressed with panoptic segmentation masks. This is echoed by the evo-lutionary path in visual perception research: from image-level labels (i.e., classification) to spatial locations (i.e., ob-ject detection) to more fine-grained, pixel-wise masks (i.e., panoptic segmentation [20]).
In this paper, we take scene graphs to the next level by proposing panoptic video scene graph generation (PVSG), a new problem that requires each node in video scene graphs to be grounded by a pixel-level segmentation mask. Panop-tic video scene graphs can solve a critical issue exposed in bounding box-based video scene graphs: both things and stuff classes (i.e., amorphous regions containing water, grass, etc.) can be well covered—the latter are crucial for understanding contexts but cannot be localized with bound-ing boxes. For instance, if we switch from panoptic video scene graphs to bounding box-based scene graphs for the video in Figure 1, some nontrivial relations useful for con-text understanding like “adult-1 standing on/in ground” and
“adult-2 standing on/in water” will be missing. It is also worth noting that bounding box-based video scene graph annotations, at least in current research [15], often miss small but important details, such as the “candles” on cakes.
To help the community progress in this new area, we contribute a high-quality PVSG dataset, which consists of 400 videos among which 289 are third-person videos and 111 are egocentric videos. Each video contains an average length of 76.5 seconds. In total, 152,958 frames are labeled with fine panoptic segmentation and temporal scene graphs.
There are 126 object classes and 57 relation classes. A more detailed comparison between our PVSG dataset and some related datasets is shown in Table 1.
To solve the PVSG problem, we propose a two-stage framework: the first stage produces a set of features for each mask-based instance tracklet while the second stage gener-ates video-level scene graphs based on tracklets’ features.
We study two design choices for the first stage: 1) a panop-tic segmentation model + a tracking module; 2) an end-to-end video panoptic segmentation model. For the second scene graph generation stage, we provide four different im-plementations covering both convolution and Transformer-based methods.
In summary, we make the following contributions to the scene graph community: 1. A new problem: We identify several issues associated with current research in scene graph generation and propose a new problem, which combines video scene graph generation with panoptic segmentation for holis-tic video understanding. 2. A new dataset: A high-quality dataset with fine, tem-poral scene graph annotations and panoptic segmenta-tion masks is proposed to advance the area of PVSG. 3. New methods and a benchmark: We propose a two-stage framework to address the PVSG problem and benchmark a variety of design ideas, from which valu-able insights on good design practices are drawn for future work. 2.