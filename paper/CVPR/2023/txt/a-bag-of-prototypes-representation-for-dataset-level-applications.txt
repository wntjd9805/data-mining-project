Abstract
This work investigates dataset vectorization for two dataset-level tasks: assessing training set suitability and test set difficulty. The former measures how suitable a train-ing set is for a target domain, while the latter studies how challenging a test set is for a learned model. Central to the two tasks is measuring the underlying relationship be-tween datasets. This needs a desirable dataset vectoriza-tion scheme, which should preserve as much discriminative dataset information as possible so that the distance between the resulting dataset vectors can reflect dataset-to-dataset similarity. To this end, we propose a bag-of-prototypes (BoP) dataset representation that extends the image-level bag consisting of patch descriptors to dataset-level bag con-sisting of semantic prototypes. Specifically, we develop a codebook consisting of K prototypes clustered from a ref-erence dataset. Given a dataset to be encoded, we quan-tize each of its image features to a certain prototype in the codebook and obtain a K-dimensional histogram. Without assuming access to dataset labels, the BoP representation provides rich characterization of the dataset semantic dis-tribution. Furthermore, BoP representations cooperate well with Jensen-Shannon divergence for measuring dataset-to-dataset similarity. Although very simple, BoP consistently shows its advantage over existing representations on a se-ries of benchmarks for two dataset-level tasks. 1.

Introduction
Datasets are fundamental in machine learning research, forming the basis of model training and testing [18, 51, 52, 61]. While large-scale datasets bring opportunities in al-gorithm design, there lack proper tools to analyze and make the best use of them [6,51,56]. Therefore, as opposed to tra-ditional algorithm-centric research where improving mod-els is of primary interest, the community has seen a grow-ing interest in understanding and analyzing the data used for developing models [51, 56]. Recent examples of such goal include data synthesis [29], data sculpting [25,51], and data valuation [6, 32, 56]. These tasks typically focus on individ-ual sample of a dataset. In this work, we aim to understand nature of datasets from a dataset-level perspective.
This work considers two dataset-level tasks: suitability in training and difficulty in testing. First, training set suit-ability denotes whether a training set is suitable for training models for a target dataset. In real-world applications, we are often provided with multiple training sets from various data distributions (e.g., universities and hospitals). Due to distribution shift, their trained models have different perfor-mance on the target dataset. Then, it is of high practical value to select the most suitable training set for the target dataset. Second, test set difficulty means how challenging a test set is for a learned model. In practice, test sets are usu-ally unlabeled and often come from different distributions than that of the training set. Measuring the test set difficulty for a learned model helps us understand the model reliabil-ity, thereby ensuring safe model deployment.
The core of the two dataset-level tasks is to measure the relationship between datasets. For example, a training set is more suitable for learning a model if it is more similar to the target dataset. To this end, we propose a vectoriza-tion scheme to represent a dataset. Then, the relationship between a pair of datasets can be simply reflected by the distance between their representations. Yet, it is challeng-ing to encode a dataset as a representative vector, because (i) a dataset has a different cardinality (number of images) and (ii) each image has its own semantic content (e.g., cate-gory). It is thus critical to find an effective way to aggregate all image features to uncover dataset semantic distributions.
In the literature, some researchers use the first few mo-ments of distributions such as feature mean and co-variance to represent datasets [20, 62, 74, 75, 82]. While being com-putational friendly, these methods do not offer sufficiently strong descriptive ability of a dataset, such as class distri-butions, and thus have limited effectiveness in assessing at-tributes related to semantics. There are also some methods learn task-specific dataset representations [1, 63]. For ex-ample, given a dataset with labels and a task loss function,
Task2Vec [1] computes an embedding based on estimates of the Fisher information matrix associated with a probe network’s parameters. While these task-specific represen-tations are able to predict task similarities, they are not suit-able for characterizing dataset properties of interest. They
require training a network on the specific task [1] or on mul-tiple datasets [63], so they are not effective in assessing the training set suitability. Additionally, they require image la-bels for the specific task, so they cannot be used to measure the difficulty of unlabeled test sets.
In this work, we propose a simple and effective bag-of-prototypes (BoP) dataset representation.
Its computation starts with partitioning the image feature space into seman-tic regions through clustering, where the region centers, or prototypes, form a codebook. Given a new dataset, we quantize its features to their corresponding prototypes and compute an assignment histogram, which, after normaliza-tion, gives the BoP representation. The dimensionality of
BoP equals the codebook size, which is usually a few hun-dred and is considered memory-efficient. Meanwhile, the histogram computed on the prototypes is descriptive of the dataset semantic distribution.
Apart from being low dimensional and semantically rich,
BoP has a few other advantages. First, while recent works in task-specific dataset representation usually require full image annotations and additional learning procedure [1,63], the computation of BoP does not rely on any. It is relatively efficient and allows for unsupervised assessment of dataset attributes. Second, BoP supports dataset-to-dataset similar-ity measurement through Jensen-Shannon divergence. We show in our experiment that this similarity is superior to commonly used metrics such as Fr´echet distance [27] and maximum mean discrepancy [33] in two dataset-level tasks. 2.