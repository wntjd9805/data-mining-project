Abstract
The capture and animation of human hair are two of the major challenges in the creation of realistic avatars for the virtual reality. Both problems are highly challenging, be-cause hair has complex geometry and appearance and ex-hibits challenging motion. In this paper, we present a two-stage approach that models hair independently of the head to address these challenges in a data-driven manner. The first stage, state compression, learns a low-dimensional la-tent space of 3D hair states including motion and appear-ance via a novel autoencoder-as-a-tracker strategy. To bet-ter disentangle the hair and head in appearance learning, we employ multi-view hair segmentation masks in combi-nation with a differentiable volumetric renderer. The sec-ond stage optimizes a novel hair dynamics model that per-forms temporal hair transfer based on the discovered latent codes. To enforce higher stability while driving our dynam-ics model, we employ the 3D point-cloud autoencoder from the compression stage for de-noising of the hair state. Our model outperforms the state of the art in novel view synthe-sis and is capable of creating novel hair animations without relying on hair observations as a driving signal.
† 1.

Introduction
The ability to model the details of human hair with high fidelity is key to achieving realism in human avatar creation: hair can be very important to one’s appearance! The real-ism of human hair involves many aspects like geometry, ap-pearance and interaction with light. The sheer number of hair strands leads to a very complex geometry, while the in-teractions between light and hair strands leads to non-trivial view-dependent appearance changes. The dynamics of hair creates another axis for evaluating the realism of a human avatar while being similarly hard to capture and model due to the complexity of the hair motion space as well as severe self-occlusions.
These problems lead to two major challenges for cre-†
Project page at https://ziyanw1.github.io/neuwigs/.
Figure 1. Animation from Single View Captures. Our model can generate realistic hair animation from single view video based on head motion and gravity direction. Original captures of subjects wearing a wig cap are shown in red boxes. ating realistic avatars with hair: appearance modeling and dynamics modeling. While modern capture systems can re-construct high fidelity hair appearance from a sparse and discrete set of real world observations frame by frame, no dynamics information is discovered through this process.
To capture dynamics, we need to perform tracking to align those reconstructions in both space and time. However, do-ing tracking and reconstruction do not directly solve the problem of novel motion generation. To achieve that, we
need to go beyond the captured data in space and time by creating a controllable dynamic hair model.
In conventional animation techniques, hair geometry is created by an artist manually preparing 3D hair grooms.
Motion of the 3D hair groom is created by a physics simula-tor where an artist selects the parameters for the simulation.
This process requires expert knowledge. In contrast, data-driven methods aim to achieve hair capture and animation in an automatic way while preserving metric photo-realism.
Most of the current data-driven hair capture and animation approaches learn to regress a dense 3D hair representation that is renderable directly from per-frame driving signals, without modeling dynamics.
However, there are several factors that limit the practical use of these data-driven methods for hair animation. First of all, these methods mostly rely on sophisticated driving signals, like multi-view images [22, 33], a tracked mesh of the hair [24], or tracked guide hair strands [42], which are hard to acquire. Furthermore, from an animation perspec-tive, these models are limited to rendering hair based on hair observations and cannot be used to generate novel motion of hair. Sometimes it is not possible to record the hair driving signals at all. We might want to animate hair for a person wearing accessories or equipment that (partially) obstructs the view of their hair, for example VR glasses; or animate a novel hair style for a subject.
To address these limitations of existing data-driven hair capture and animation approaches, we present a neural dy-namic model that is able to animate hair with high fidelity conditioned on head motion and relative gravity direction.
By building such a dynamic model, we are able to gener-ate hair motions by evolving an initial hair state into a fu-ture one, without relying on per-frame hair observation as a driving signal. We utilize a two-stage approach for cre-ating this dynamic model: in the first stage, state compres-sion, we perform dynamic hair capture by learning a hair autoencoder from multi-view video captures with an evolv-ing tracking algorithm. Our method is capable of capturing a temporally consistent, fully renderable volumetric repre-sentation of hair from videos with both head and hair. Hair states with different time-stamps are parameterized into a semantic embedding space via the autoencoder. In the sec-ond stage, we sample temporally adjacent pairs from the semantic embedding space and learn a dynamic model that can perform the hair state transition between each state in the embedding space given the previous head motion and gravity direction. With such a dynamic model, we can per-form hair state evolution and hair animation in a recurrent manner which is not driven by existing hair observations.
As shown in Fig. 1, our method is capable of generating realistic hair animation with different hair styles of single view captures of a moving head with a wig cap. In sum-mary:
• We present NeuWigs, a novel end-to-end data-driven pipeline with a volumetric autoencoder as the backbone for real human hair capture and animation, learnt from multi-view RGB images.
• We learn the hair geometry, tracking and appearance end-to-end with a novel autoencoder-as-a-tracker strategy for hair state compression, where the hair is modeled sepa-rately from the head using multi-view hair segmentation.
• We train an animatable hair dynamic model that is robust to drift using a hair state denoiser realized by the 3D au-toencoder from the compression stage. 2.