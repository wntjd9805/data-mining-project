Abstract
Despite significant efforts, cutting-edge video segmenta-tion methods still remain sensitive to occlusion and rapid movement, due to their reliance on the appearance of ob-jects in the form of object embeddings, which are vul-nerable to these disturbances. A common solution is to use optical flow to provide motion information, but essen-tially it only considers pixel-level motion, which still re-lies on appearance similarity and hence is often inaccu-In this work, rate under occlusion and fast movement. we study the instance-level motion and present InstMove, which stands for Instance Motion for Object-centric Video
Segmentation.
In comparison to pixel-wise motion, Inst-Move mainly relies on instance-level motion information that is free from image feature embeddings, and features physical interpretations, making it more accurate and ro-bust toward occlusion and fast-moving objects. To better fit in with the video segmentation tasks, InstMove uses in-stance masks to model the physical presence of an object and learns the dynamic model through a memory network to predict its position and shape in the next frame. With only a few lines of code, InstMove can be integrated into current
SOTA methods for three different video segmentation tasks and boost their performance. Specifically, we improve the previous arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and 4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast moving objects. These results suggest that instance-level motion is robust and accurate, and hence serving as a powerful solution in complex sce-narios for object-centric video segmentation. 1.

Introduction
Segmenting and tracking object instances in a given video is a critical topic in computer vision, with vari-ous applications in video understanding, video editing, au-tonomous driving, augmented reality, etc. Three represen-tative tasks include video object segmentation (VOS), video
*First two authors contributed equally. Work done during an internship at ByteDance. The code and models are available for research purposes at https://github.com/wjf5203/VNext.
Figure 1. Different from optical flow that estimates pixel-level motion, InstMove learns instance-level motion and deformation directly from previous instance masks and predicts more accurate and robust position and shape estimates for the current frame, even in scenarios with occlusions and rapid motion. instance segmentation (VIS), and multi-object tracking and segmentation (MOTS). These tasks differ significantly from video semantic segmentation [16, 19, 31, 52], which aims to classify every pixel in a video frame, hence we refer to them as object-centric video segmentation in this paper. De-spite significant progress, state-of-the-art (SOTA) methods are still struggle with occlusion, rapid motion, and signifi-cant changes in objects, resulting in a marked drop in han-dling longer or more complex videos.
One reason we observe is that most methods rely solely on appearance to localize objects and track them across frames. Specifically, a majority of VOS methods [10, 30, 35, 40, 51, 67] use the previous frames as target templates and construct a feature memory bank of embeddings for all target objects. This is then used to match the pixel-level fea-ture in the new frame. Online VIS [6, 15, 21, 28, 65, 72, 73] and MOTS [25, 71] methods directly perform per-frame in-stance segmentation based on image features and use the object embeddings to track them through the video. While these paradigms work well on simple videos, they are sen-sitive to intense appearance changes and struggle with han-dling multiple object instances with similar appearances, re-sulting in large errors when dealing with complex scenarios
with complex motion patterns, occlusion, or deformation.
Apart from appearance cues, object motion, which is an-other crucial piece of information provided by videos, has also been extensively studied for video segmentation. The majority of motion models in related fields fall into two cat-egories: One line of work uses optical flow to learn pixel-level motion. However, this approach does not help solve the problem of occlusion or fast motion since flow itself is often inaccurate in these scenarios [39,57]. The main reason causing the failure we argue is that optical flow still heavily relies on appearance cues to compute the pixel-level mo-tion across the frame. The other line of work uses a linear speed model, which helps alleviate these tracking problems caused by occlusion and fast motion in MOT [5, 44, 63, 77].
However, it oversimplifies the problem and thus provides limited benefits in other tasks such as VOS and VIS.
In this work, we aim at narrowing the gap between the two aforementioned lines of work by reformulating the mo-tion module and providing InstMove, a simple yet efficient motion prediction plugin that enjoys the advantages of both solutions. First, it is portable and is compatible with and beneficial to approaches of video segmentation tasks. More importantly, similar to optical flow, it also provides high-dimensional information of position and shape, which can be beneficial for a range of downstream tasks in a variety of ways, and, similar to the dynamic motion model, it learns physical interpretation to model motion information, im-proving robustness toward occlusion and fast motion.
To achieve our objective, we utilize an instance mask to indicate the position and shape of a target object, and provide an RNN-based module with a memory network to extract motion features from previous masks, store and re-trieve dynamic information, and predict the position and shape information of the next frame based on motion cues.
However, while being robust towards appearance changes, predicting shape without the object appearance or image features results in an even less accurate boundary in sim-ple cases. To solve this, we incorporate the low-level image features at the end of InstMove. Finally, to prove the effec-tiveness of InstMove on object-centric video segmentation tasks, we present two simple ways to integrate InstMove into recent SOTA methods in VIS, VOS, and MOTS, which improve their robustness with minimal modifications.
In the experiments section, we first validate that our mo-tion module is more accurate and compatible with existing methods compared with learning motion and deformation with optical flow methods such as RAFT [55]. We also show that it is more robust to occlusion and rapid move-ments. Then we demonstrate the improvement of integrat-ing our motion plugin into all recent SOTA methods in
VOS, VIS, and MOTS tasks, particularly in complex sce-narios with heavy occlusion and rapid motion. Remarkably, with only a few lines of code, we significantly boost the cur-rent art by 1.5 AP on OVIS [47], 4.9 AP on YouTubeVIS-Long [69], and reduce IDSw on BDD100K [75] by 28.6%.
In summary, we have revisited the motion models used in video segmentation tasks and propose InstMove, which contains both pixel-level information and instance-level dy-namic information to predict shape and position.
It pro-vides additional information that is robust to occlusion and rapid motion. The improvements in SOTA methods of all three tasks demonstrate the effectiveness of incorporating instance-level motion in tackling complex scenarios. 2.