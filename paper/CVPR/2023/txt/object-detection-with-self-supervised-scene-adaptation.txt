Abstract
This paper proposes a novel method to improve the per-formance of a trained object detector on scenes with fixed camera perspectives based on self-supervised adaptation.
Given a specific scene, the trained detector is adapted using pseudo-ground truth labels generated by the detector itself and an object tracker in a cross-teaching manner. When the camera perspective is fixed, our method can utilize the background equivariance by proposing artifact-free object mixup as a means of data augmentation, and utilize accu-rate background extraction as an additional input modal-ity. We also introduce a large-scale and diverse dataset for the development and evaluation of scene-adaptive ob-ject detection. Experiments on this dataset show that our method can improve the average precision of the original detector, outperforming the previous state-of-the-art self-supervised domain adaptive object detection methods by a large margin. Our dataset and code are published at https://github.com/cvlab-stonybrook/scenes100. 1.

Introduction
The need to detect objects in video streams from station-ary cameras arises in many computer vision applications, including video surveillance and autonomous retail. In gen-eral, different applications require the detection of different object categories, and each computer-vision-based product will have its own detector. However, for a specific product, there is typically a single detector that will be used for many cameras/scenes. For example, a typical video surveillance product would use the same detector to detect pedestrians and vehicles for network cameras installed at different loca-tions. Unfortunately, a single detector might not work well for all scenes, leading to trivial and unforgiving mistakes.
This fundamental problem of many computer vision products stems from the limited generalization power of a single model, due to limited training data, limited model ca-pacity, or both. One can attempt to address this problem by using more training data, but it will incur additional cost for data collection and annotation. Furthermore, in many cases, due to the low latency requirement or the limited comput-ing resources for inference, a product is forced to use a very lightweight network, and this network will have limited rep-resentation capacity to generalize across many scenes.
In this paper, instead of having a single scene-generic detector, we propose using scene-specific detectors. This yields higher detection performance as each detector is customized for a specific scene, and allows us to use a lightweight model without sacrificing accuracy as each de-tector is only responsible for one scene.
Obtaining scene-specific detectors, however, is very challenging. A trivial approach is to train a detector for each scene separately, but this requires an enormous amount of annotated training data.
Instead, we propose a self-supervised method to adapt a pre-trained detector to each scene. Our method records the unlabeled video frames in the past, uses the trained detector to detect objects in those frames, and generates augmented training data based on those detections. Although the detections made by the pre-trained model can be noisy, they can still be useful for generating pseudo annotated data. We further extend those pseudo bounding boxes by applying object tracking [2, 57] along the video timeline, aiming to propagate the detections to adjacent frames to recover some of the false negatives not returned by the detector. We also use multiple detectors to obtain the pseudo labels and train the detector in a cross-teaching manner, taking the advantage of the ensemble of models [13, 24].
Exploiting the stationary nature of the camera, we pro-pose two additional techniques to boost the detection perfor-mance: location-aware mixup and background-augmented input. The former is to generate more samples during train-ing through object mixup [76] that contains less artifacts, based on the aforementioned pseudo boxes generated from detection and tracking. The latter involves estimating the background image and fusing it with the detectorâ€™s input.
In short, the main contribution of our paper is a novel framework that utilizes self-supervision, location-aware ob-ject mixup, and background modeling to improve the detec-tion performance of a pre-trained object detector on scenes with stationary cameras. We also contribute a large scale and diverse dataset for the development of scene adap-tive object detection, which contains sufficient quantity and quality annotations for evaluation. 2.