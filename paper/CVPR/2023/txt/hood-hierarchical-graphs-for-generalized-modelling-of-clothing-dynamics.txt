Abstract 1.

Introduction
We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable efficient prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method, called HOOD, is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing cloth-ing. Furthermore, HOOD handles changes in topology (e.g., garments with buttons or zippers) and material prop-erties at inference time. As one key contribution, we pro-pose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local de-tail. We empirically show that HOOD outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods.
The ability to model realistic and compelling clothing behavior is crucial for telepresence, virtual try-on, video games and many other applications that rely on high-fidelity digital humans. A common approach to generate plausible dynamic motions is physics-based simulation [2]. While impressive results can be obtained, physical simulation is sensitive to initial conditions, requires animator exper-tise, and is computationally expensive; state-of-the-art ap-proaches [14, 22, 36] are not designed for the strict compu-tation budgets imposed by real-time applications.
Deep learning-based methods have started to show promising results both in terms of efficiency and quality.
However, there are several limitations that have so far pre-vented such approaches from unlocking their full poten-tial: First, existing methods rely on linear-blend skinning and compute clothing deformations primarily as a func-tion of body pose [24, 43]. While compelling results can be obtained for tight-fitting garments such as shirts and sportswear, skinning-based methods struggle with dresses, skirts, and other types of loose-fitting clothing that do not closely follow body motion. Crucially, many state-of-the-art learning-based methods are garment-specific [20, 38, 43, 45, 53] and can only predict deformations for the particular outfit they were trained on. The need to retrain these meth-ods for each garment limits applicability.
In this paper, we propose a novel approach for predict-ing dynamic garment deformations using graph neural net-works (GNNs). Our method learns to predict physically-realistic fabric behavior by reasoning about the map be-tween local deformations, forces, and accelerations. Thanks to its locality, our method is agnostic to both the global structure and shape of the garment and directly generalizes to arbitrary body shapes and motions.
GNNs have shown promise in replacing physics-based simulation [40, 41], but a straightforward application of this concept to clothing simulation yields unsatisfying results.
GNNs apply local transformations (implemented as MLPs) to feature vectors of vertices and their one-ring neighbor-hood in a given mesh. Each transformation results in a set of messages that are then used to update feature vec-tors. This process is repeated, allowing signals to propagate through the mesh. However, a fixed number of message-passing steps limits signal propagation to a finite radius.
This is problematic for garment simulation, where elastic waves due to stretching travel rapidly through the material, leading to quasi-global and immediate long-range coupling between vertices. Using too few steps delays signal propa-gation and leads to disturbing over-stretching artifacts, mak-ing garments look unnatural and rubbery. Na¨ıvely increas-ing the number of iterations comes at the expense of rapidly growing computation times. This problem is amplified by the fact that the maximum size and resolution of simulation meshes is not known a priori, which would allow setting a conservative, sufficiently large number of iterations.
To address this problem, we propose a message-passing scheme over a hierarchical graph that interleaves propaga-tion steps at different levels of resolution. In this way, fast-travelling waves due to stiff stretching modes can be effi-ciently treated on coarse scales, while finer levels provide the resolution needed to model local detail such as folds and wrinkles. We show through experiments that our graph representation improves predictions both qualitatively and quantitatively for equal computation budgets.
To extend the generalization capabilities of our ap-proach, we combine the concepts of graph-based neural networks and differentiable simulation by using an incre-mental potential for implicit time stepping as a loss func-tion [33, 43]. This formulation allows our network to be trained in a fully unsupervised way and to simultaneously learn multi-scale clothing dynamics, the influence of mate-rial parameters, as well as collision reaction and frictional contact with the underlying body, without the need for any ground-truth (GT) annotations. Additionally, the graph for-mulation enables us to model garments of varied and chang-ing topology; e.g. the unbuttoning of a shirt in motion.
In summary, we propose a method, called HOOD, that leverages graph neural networks, multi-level message pass-ing, and unsupervised training to enable real-time predic-tion of realistic clothing dynamics for arbitrary types of garments and body shapes. We empirically show that our method offers strategic advantages in terms of flexibil-ity and generality compared to state-of-the-art approaches.
Specifically, we show that a single trained network: (i) ef-ficiently predicts physically-realistic dynamic motion for a large variety of garments; (ii) generalizes to new gar-ment types and shapes not seen during training; (iii) al-lows for run-time changes in material properties and gar-ment sizes, and (iv) supports dynamic topology changes such as opening zippers or unbuttoning shirts. Code and models are available for research purposes: https:// dolorousrtur.github.io/hood/. 2.