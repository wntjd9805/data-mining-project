Abstract
Diffusion models have emerged as the new state-of-the-art generative model with high quality samples, with intriguing properties such as mode coverage and high flexibility.
They have also been shown to be effective inverse problem solvers, acting as the prior of the distribution, while the information of the forward model can be granted at the
This work was supported by the Korea Medical Device Develop-ment Fund grant funded by the Korea government (the Ministry of Sci-ence and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project
Number: 1711137899, KMDF PR 20200901 0015), by the National Re-search Foundation of Korea under Grant NRF-2020R1A2B5B03001980, by the KAIST Key Research Institute (Interdisciplinary Research Group)
Project, and by the MSIT (Ministry of Science and ICT), Korea, under the
ITRC(Information Technology Research Center) support program(IITP-2022-2020-0-01461) supervised by the IITP(Institute for Information & communications Technology Planning & Evaluation). sampling stage. Nonetheless, as the generative process remains in the same high dimensional (i.e. identical to data dimension) space, the models have not been extended to 3D inverse problems due to the extremely high memory and computational cost.
In this paper, we combine the ideas from the conventional model-based iterative recon-struction with the modern diffusion models, which leads to a highly effective method for solving 3D medical image reconstruction tasks such as sparse-view tomography, limited angle tomography, compressed sensing MRI from
In essence, we propose pre-trained 2D diffusion models. to augment the 2D diffusion prior with a model-based prior in the remaining direction at test time, such that one can achieve coherent reconstructions across all di-mensions. Our method can be run in a single commodity
GPU, and establishes the new state-of-the-art, showing the proposed method can perform reconstructions that
of high fidelity and accuracy even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal that the generalization capacity of the proposed method is surprisingly high, and can be used to reconstruct volumes that are entirely different from the training dataset. Code available: https://github.com/HJ-harry/DiffusionMBIR 1.

Introduction
Diffusion models learn the data distribution implic-itly by learning the gradient of the log density (i.e.
∇x log pdata(x); score function) [9, 32], which is used at inference to create generative samples. These models are known to generate high-quality samples, cover the modes well, and be highly robust to train, as it amounts to merely minimizing a mean squared error loss on a denoising prob-lem. Particularly, diffusion models are known to be much more robust than other popular generative models [8], for example, generative adversarial networks (GANs). Further-more, one can use pre-trained diffusion models to solve in-verse problems in an unsupervised fashion [5–7, 15, 32].
Such strategies has shown to be highly effective in many cases, often establishing the new state-of-the-art on each task. Specifically, applications to sparse view computed to-mography (SV-CT) [5, 31], compressed sensing MRI (CS-MRI) [6, 7, 31], super-resolution [4, 6, 15], inpainting [5, 15] among many others, have been proposed.
Nevertheless, to the best of our knowledge, all the meth-ods considered so far focused on 2D imaging situations.
This is mostly due to the high-dimensional nature of the generative constraint. Specifically, diffusion models gen-erate samples by starting from pure noise, and iteratively denoising the data until reaching the clean image. Conse-quently, the generative process involves staying in the same dimension as the data, which is prohibitive when one tries to scale the data dimension to 3D. One should also note that training a 3D diffusion model amounts to learning the 3D prior of the data density. This is undesirable in two as-pects. First, the model is data hungry, and hence training a 3D model would typically require thousands of volumes, compared to 2D models that could be trained with less than 10 volumes. Second, the prior would be needlessly compli-cated: when it comes to dynamic imaging or 3D imaging, exploiting the spatial/temporal correlation [12, 33] is stan-dard practice. Naively modeling the problem as 3D would miss the chance of leveraging such information.
Another much more well-established method for solv-ing 3D inverse problems is model-based iterative recon-struction (MBIR) [14, 17], where the problem is formu-lated as an optimization problem of weighted least squares (WLS), constructed with the data consistency term, and the regularization term. One of the most widely acknowl-edged regularization in the field is the total variation (TV) penalty [18, 28], known for its intriguing properties: edge-preserving, while imposing smoothness. While the TV prior has been widely explored, it is known to fall behind the data-driven prior of the modern machine learning prac-tice, as the function is too simplistic to fully model how the image “looks like”.
In this work, we propose DiffusionMBIR, a method to combine the best of both worlds: we incorporate the MBIR optimization strategy into the diffusion sampling steps in or-der to augment the data-driven prior with the conventional
TV prior, imposed to the z-direction only. Particularly, the standard reverse diffusion (i.e. denoising) steps are run independently with respect to the z-axis, and hence stan-dard 2D diffusion models can be used. Subsequently, the data consistency step is imposed by aggregating the slices, then taking a single update step of the alternating direction method of multipliers (ADMM) [3]. This step effectively coerces the cross-talk between the slices with the measure-ment information, and the TV prior. For efficient optimiza-tion, we further propose a strategy in which we call vari-able sharing, which enables us to only use a single sweep of ADMM and conjugate gradient (CG) per denoising itera-tion. Note that our method is fully general in that we are not restricted to the given forward operator at test time. Hence, we verify the efficacy of the method by performing exten-sive experiments on sparse-view CT (SV-CT), limited an-gle CT (LA-CT), and compressed sensing MRI (CS-MRI): out method shows consistent improvements over the current diffusion model-based inverse problem solvers, and shows strong performance on all tasks (For representative results, see Fig. 1. For conceptual illustration of the inverse prob-lems, see Fig. 2).
In short, the main contributions of this paper is to devise a diffusion model-based reconstruction method that 1) op-erate with the voxel representation, 2) is memory-efficient such that we can scale our solver to much higher dimen-sions (i.e. > 2563), and 3) is not data hungry, such that it can be trained with less than ten 3D volumes. 2.