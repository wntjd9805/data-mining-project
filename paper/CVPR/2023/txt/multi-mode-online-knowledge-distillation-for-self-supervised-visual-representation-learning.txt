Abstract
Self-supervised learning (SSL) has made remarkable progress in visual representation learning. Some studies combine SSL with knowledge distillation (SSL-KD) to boost the representation learning performance of small models. In this study, we propose a Multi-mode Online Knowledge Dis-tillation method (MOKD) to boost self-supervised visual rep-resentation learning. Different from existing SSL-KD meth-ods that transfer knowledge from a static pre-trained teacher to a student, in MOKD, two different models learn collab-oratively in a self-supervised manner. Specifically, MOKD consists of two distillation modes: self-distillation and cross-distillation modes. Among them, self-distillation performs self-supervised learning for each model independently, while cross-distillation realizes knowledge interaction between dif-ferent models. In cross-distillation, a cross-attention feature search strategy is proposed to enhance the semantic feature alignment between different models. As a result, the two models can absorb knowledge from each other to boost their representation learning performance. Extensive experimen-tal results on different backbones and datasets demonstrate that two heterogeneous models can benefit from MOKD and outperform their independently trained baseline. In addition,
MOKD also outperforms existing SSL-KD methods for both the student and teacher models. 1.

Introduction
Due to the promising performance of unsupervised visual representation learning in many computer vision tasks, self-supervised learning (SSL) has attracted widespread attention from the computer vision community. SSL aims to learn general representations that can be transferred to downstream tasks by utilizing massive unlabeled data.
Among various SSL methods, contrastive learning [8, 21] has shown significant progress in closing the performance
*Corresponding author.
Figure 1. Overview of the proposed Multi-mode Online Knowledge
Distillation (MOKD). In MOKD, two different models are trained collaboratively through two types of knowledge distillation modes, i.e., a self-distillation mode and a cross-distillation mode. EMA denotes exponential-moving-average. gap with supervised methods in recent years. It aims at max-imizing the similarity between views from the same instance (positive pairs) while minimizing the similarity among views from different instances (negative pairs). MoCo [10, 21] and
SimCLR [8, 9] use both positive and negative pairs for con-trast. They significantly improve the performance compared to previous methods [45, 51]. After that, many methods are proposed to solve the limitations in contrastive learning, such as the false negative problem [14, 20, 29, 32, 43], the limi-tation of large batch size [31, 54], and the problem of hard augmented samples [27, 49]. At the same time, other stud-ies [2,4,5,11,17,18,55] abandon the negative samples during contrastive learning. With relatively large models, such as
ResNet50 [23] or larger, these methods achieve comparable performance on different tasks than their supervised coun-terparts. However, as revealed in previous studies [15, 16], they do not perform well on small models [26, 42] and have a large gap from their supervised counterparts.
To address this challenge in contrastive learning, some
studies [1, 9, 15, 16, 37, 44, 58] propose to combine knowl-edge distillation [24] with contrastive learning (SSL-KD) to improve the performance of small models. These methods first train a larger model in a self-supervised manner and then distill the knowledge of the trained teacher model to a smaller student model. There is a limitation in these SSL-KD methods, i.e., knowledge is distilled to the student model from the static teacher model in a unidirectional way. The teacher model cannot absorb knowledge from the student model to boost its performance.
In this study, we propose a Multi-mode Online Knowl-edge Distillation method (MOKD), as illustrated in Fig. 1, to boost the representation learning performance of two models simultaneously. Different from existing SSL-KD methods that transfer knowledge from a static pre-trained teacher to a student, in MOKD, two different models learn collabora-tively in a self-supervised manner. Specifically, MOKD con-sists of a self-distillation mode and a cross-distillation mode.
Among them, self-distillation performs self-supervised learn-ing for each model independently, while cross-distillation realizes knowledge interaction between different models.
In addition, a cross-attention feature search strategy is pro-posed in cross-distillation to enhance the semantic feature alignment between different models. Extensive experimental results on different backbones and datasets demonstrate that model pairs can both benefit from MOKD and outperform their independently trained baseline. For example, when trained with ResNet [23] and ViT [13], two models can ab-sorb knowledge from each other, and representations of the two models show the characteristics of each other. In addi-tion, MOKD also outperforms existing SSL-KD methods for both the student and teacher models.
The contributions of this study are threefold:
• We propose a novel self-supervised online knowledge distillation method, i.e., MOKD.
• MOKD can boost the performance of two models simul-taneously, achieving state-of-the-art contrastive learn-ing performance on different models.
• MOKD achieves state-of-the-art SSL-KD performance. 2.