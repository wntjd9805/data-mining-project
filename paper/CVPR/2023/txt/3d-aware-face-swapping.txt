Abstract 1.

Introduction
Face swapping is an important research topic in com-puter vision with wide applications in entertainment and privacy protection. Existing methods directly learn to swap 2D facial images, taking no account of the geometric in-formation of human faces.
In the presence of large pose variance between the source and the target faces, there always exist undesirable artifacts on the swapped face.
In this paper, we present a novel 3D-aware face swap-ping method that generates high-fidelity and multi-view-consistent swapped faces from single-view source and tar-get images. To achieve this, we take advantage of the strong geometry and texture prior of 3D human faces, where the 2D faces are projected into the latent space of a 3D genera-tive model. By disentangling the identity and attribute fea-tures in the latent space, we succeed in swapping faces in a 3D-aware manner, being robust to pose variations while transferring fine-grained facial details. Extensive experi-ments demonstrate the superiority of our 3D-aware face swapping framework in terms of visual quality, identity sim-ilarity, and multi-view consistency. Code is available at https://lyx0208.github.io/3dSwap.
∗ Corresponding authors.
Face swapping aims to transfer the identity of a person in the source image to another person in the target image while preserving other attributes like head pose, expression, illumination, background, etc. It has attracted extensive at-tention recently in the academic and industrial world for its potential wide applications in entertainment [14,30,38] and privacy protection [7, 37, 48].
The key of face swapping is to transfer the geometric shape of the facial region (i.e., eyes, nose, mouth) and detailed texture information (such as the color of eyes) from the source image to the target image while pre-serving both geometry and texture of non-facial regions (i.e., hair, background, etc). Currently, some 3D-based methods consider geometry prior of human faces by fit-ting the input image to 3D face models such as 3D Mor-phable Model (3DMM) [8] to overcome the differences of face orientation and expression between sources and tar-gets [7, 15, 34, 43]. However, these parametric face mod-els only produce coarse frontal faces without fine-grained details, leading to low-resolution and fuzzy swapping re-sults. On the other hand, following Generative Adversarial
Network [24], GAN-based [6, 23, 32, 39, 40, 42] or GAN-inversion-based [44, 55, 57, 60] approaches adopt the ad-versarial training strategy to learn texture information from inputs. Despite the demonstrated photorealistic and high-resolution images, the swapped faces via 2D GANs sustain undesirable artifacts when two input faces undergo large pose variation since the strong 3D geometry prior of human faces is ignored. Moreover, learning to swap faces in 2D images makes little use of the shaped details from sources, leading to poorer performance on identity transferring.
Motivated by the recent advances of 3D generative mod-els [12, 13, 20, 25, 45] in synthesizing multi-view consis-tent images and high-quality 3D shapes, it naturally raises a question: can we perform face swapping in a 3D-aware manner to exploit the strong geometry and texture priors?
To answer this question, two challenges arise. First, how to infer 3D prior directly from 3D-GAN models still remains open. Current 3D-aware generative models synthesize their results from a random Gaussian noise z, so that their output images are not controllable. This increases the complexity of inferring the required prior from arbitrary input. Second, the inferred prior corresponding to input images is in the form of a high-dimension feature vector in the latent space of 3D GANs. Simply synthesizing multi-view target im-ages referring to the prior and applying 2D face swapping to them produces not only inconsistent artifacts but also a heavy computational load.
To address these challenges, we systematically inves-tigate the geometry and texture prior of these 3D gener-ative models and propose a novel 3D-aware face swap-ping framework 3dSwap. We introduce a 3D GAN inver-sion framework to project the 2D inputs into the 3D latent space, motivated by recent GAN inversion approaches [46, 47, 51]. Specifically, we design a learning-based inver-sion algorithm that trains an encoding network to efficiently and robustly project input images into the latent space of
EG3D [12]. However, directly borrowing the architecture from 2D approaches is not yet enough since a single-view input provides limited information about the whole human face. To further improve the multi-view consistency of la-tent code projection, we design a pseudo-multi-view train-ing strategy. This design effectively bridges the domain gap between 2D and 3D. To tackle the second problem, we design a face swapping algorithm based on the 3D la-tent codes and directly synthesize the swapped faces with the 3D-aware generator. In this way, we achieve 3D GAN-inversion-based face swapping by a latent code manipulat-ing algorithm consisting of style-mixing and interpolation, where latent code interpolation is responsible for identity transferring while style-mixing helps to preserve attributes.
In summary, our contributions are threefold:
• To the best of our knowledge, we first address the 3D-aware face swapping task. The proposed 3dSwap method sets a strong baseline and we hope this work will foster future research into this task.
• We design a learning-based 3D GAN inversion with the pseudo-multi-view training strategy to extract ge-ometry and texture prior from arbitrary input images.
We further utilize these strong prior by designing a la-tent code manipulating algorithm, with which we di-rectly synthesize the final results with the pretrained generator.
• Extensive experiments on benchmark datasets demon-strate the superiority of the proposed 3dSwap over state-of-the-art 2D face swapping approaches in iden-tity transferring. Our reconstruction module for 3D-GAN inversion performs favorably over the state-of-the-art methods as well. 2.