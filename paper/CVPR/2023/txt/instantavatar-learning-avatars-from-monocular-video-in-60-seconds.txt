Abstract 1.

Introduction
In this paper, we take one step further towards real-world applicability of monocular neural avatar reconstruction by contributing InstantAvatar, a system that can reconstruct human avatars from a monocular video within seconds, and these avatars can be animated and rendered at an inter-active rate. To achieve this efficiency we propose a care-fully designed and engineered system, that leverages emerg-ing acceleration structures for neural fields, in combination with an efficient empty-space skipping strategy for dynamic scenes. We also contribute an efficient implementation that we will make available for research purposes. Compared to existing methods, InstantAvatar converges 130× faster and can be trained in minutes instead of hours. It achieves comparable or even better reconstruction quality and novel pose synthesis results. When given the same time budget,
In-our method significantly outperforms SoTA methods. stantAvatar can yield acceptable visual quality in as little as 10 seconds training time. For code and more demo re-sults, please refer to https://ait.ethz.ch/InstantAvatar.
*Equal contribution.
†Corresponding author is
Creating high-fidelity digital humans important for many applications including immersive telepresence,
AR/VR, 3D graphics, and the emerging metaverse. Cur-rently acquiring personalized avatars is an involved process that typically requires the use of calibrated multi-camera systems and incurs significant computational cost. In this paper, we embark on the quest to build a system for the learning of 3D virtual humans from monocular video alone that is lightweight enough to be widely deployable and fast enough to allow for walk-up and use scenarios.
The emergence of powerful neural fields has enabled a number of methods for the reconstruction of animatable avatars from monocular videos of moving humans [1, 2, 6, 49, 62]. These methods typically model human shape and appearance in a pose-independent canonical space.
To reconstruct the model from images that depict hu-mans in different poses, such methods must use anima-tion (e.g. skinning) and rendering algorithms, to deform and render the model into posed space in a differentiable way. This mapping between posed and canonical space al-lows optimization of network weights by minimizing the difference between the generated pixel values and real im-ages. Especially methods that leverage neural radiance
fields (NeRFs) [40] as the canonical model has demon-strated high-fidelity avatar reconstruction results. However, due to the dual need for differentiable deformation mod-ules and for volume rendering, these models require hours of training time and cannot be rendered at interactive rates, prohibiting their broader application.
In this paper, we aim to take a further step toward real-world applicability of monocular neural avatar reconstruc-tion by contributing a method that takes no longer for recon-struction, than it takes to capture the input video. To this end, we propose InstantAvatar, a system that reconstructs high-fidelity avatars within 60 seconds, instead of hours, given a monocular video, pose parameters and masks. Once learned the avatar can be animated and rendered at interac-tive rates. Achieving such a speed-up is clearly a challeng-ing task that requires careful method design, requires fast differentiable algorithms for rendering and articulation, and requires efficient implementation.
Our simple yet highly efficient pipeline combines sev-eral key components. First, to learn the canonical shape and appearance we leverage a recently proposed neural radiance field variant [42]. Instant-NGP [42] accelerates neural vol-ume rendering by replacing multi-layer perceptrons (MLPs) with a more efficient hash table as data structure. How-ever, because the spatial features are represented explicitly,
Instant-NGP is limited to rigid objects. Second, to enable learning from posed observations and to be able to animate the avatar, we interface the canonical NeRF with an efficient articulation module, Fast-SNARF [7], which efficiently de-rives a continuous deformation field to warp the canonical radiance field into the posed space. Fast-SNARF is orders of magnitude faster compared to its slower predecessor [9].
Finally, simply integrating existing acceleration tech-niques is not sufficient to yield the desired efficiency. With acceleration structures for the canonical space and a fast ar-ticulation module in place, rendering the actual volume be-comes the computational bottleneck. To compute the color of a pixel, standard volume rendering needs to query and accumulate densities of hundreds of points along the ray.
A common approach to accelerating this is to maintain an occupancy grid to skip samples in the empty space. How-ever, such an approach assumes rigid scenes and can not be applied to dynamic scenes such as humans in motion.
We propose an empty space skipping scheme that is de-signed for dynamic scenes with known articulation patterns.
At inference time, for each input body pose, we sample points on a regular grid in posed space and map them back to the canonical model to query densities. Thresholding these densities yields an occupancy grid in canonical space, which can then be used to skip empty space during volume rendering. For training, we maintain a shared occupancy grid over all training frames, recording the union of occu-pied regions over individual frames. This occupancy grid is updated every few training iterations with the densities of randomly sampled points, in the posed space of randomly sampled frames. This scheme balances computational effi-ciency and rendering quality.
We evaluate our method on both synthetic and real monocular videos of moving humans and compare it with state-of-the-art methods on monocular avatar reconstruc-tion. Our method achieves on-par reconstruction quality and better animation quality in comparison to SoTA meth-ods, while only requiring minutes of training time instead of more than 10 hours. When given the same time budget, our method significantly outperforms SoTA methods. We also provide an ablation study to demonstrate the effect of our system’s components on speed and accuracy. 2.