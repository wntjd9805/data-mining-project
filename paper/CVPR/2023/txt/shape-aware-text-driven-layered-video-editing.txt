Abstract 1.

Introduction
Temporal consistency is essential for video editing appli-cations. Existing work on layered representation of videos allows propagating edits consistently to each frame. These methods, however, can only edit object appearance rather than object shape changes due to the limitation of using a fixed UV mapping field for texture atlas. We present a shape-aware, text-driven video editing method to tackle this challenge. To handle shape changes in video editing, we first propagate the deformation field between the in-put and edited keyframe to all frames. We then leverage a pre-trained text-conditioned diffusion model as guidance for refining shape distortion and completing unseen regions.
The experimental results demonstrate that our method can achieve shape-aware consistent video editing and compare favorably with the state-of-the-art.
Image editing.
Recently, image editing [19, 20, 24, 34, 40, 44] has made tremendous progress, especially those using diffusion models [19, 20, 40, 44]. With free-form text prompts, users can obtain photo-realistic edited images without artistic skills or labor-intensive editing. However, unlike image editing, video editing is more challenging due to the requirement of temporal consistency. Independently editing individual frames leads to undesired inconsistent frames, as shown in Fig. 2a. A na¨ıve way to deal with tem-poral consistency in video editing is to edit a single frame and then propagate the change to all the other frames. Nev-ertheless, artifacts are presented when there are unseen pix-els from the edited frame in the other frames, as shown in
Fig. 2b.
Video editing and their limitations. For consistent video editing, Neural Layered Atlas (NLA) [18] decomposes a video into unified appearance layers atlas. The layered de-composition helps consistently propagate the user edit to
(a) Multi-frame editing with frame interpolation [42] (b) Single-frame editing with frame propagation [17] (c) Text2LIVE [2] with prompt “sports car”
Figure 2. Limitation of existing work. Compare these results from baseline methods with our “sports car” result in Fig. 1. (a)
Multiple frames are edited independently and interpolated by frame interpolation method [42]. Such an approach shows realistic per-frame results but suffers from temporal flickering. (b) Extracting a single keyframe for image editing, the edits are propagated to each frame via [17]. The propagated edits are temporally stable. However, it yields visible distortions due to the unseen pixels from the keyframe. (c) The SOTA Text2LIVE [2] results demonstrate temporally-consistent appearance editing but remain the source shape
“Jeep” instead of the target prompt “sports car” by using the fixed UV mapping of NLA. individual frames with per-frame UV sampling association.
Based on NLA, Text2LIVE [2] performs text-driven editing on atlases with the guidance of the Vision-Language model,
CLIP [39]. Although Text2LIVE [2] makes video editing easier with a text prompt, it can only achieve appearance manipulation due to the use of fixed-shape associated UV sampling. Since per-frame UV sampling gathers informa-tion on motion and shape transformation in each frame to learn the pixel mapping from the atlas, shape editing is not feasible, as shown in Fig. 2c.
In this paper, we propose a shape-aware text-Our work. guided video editing approach. The core idea in our work lies in a novel UV map deformation formulation. With a se-lected keyframe and target text prompt, we first generate an edited frame by image-based editing tool (e.g., Stable Diffu-sion [44]). We then perform pixel-wise alignment between the input and edited keyframe pair through a semantic cor-respondence method [51]. The correspondence specifies the deformation between the input-edited pair at the keyframe.
According to the correspondence, the shape and appearance change can then be mapped back to the atlas space. We can thus obtain per-frame deformation by sampling the defor-mation from the atlas to the original UV maps. While this method helps with shape-aware editing, it is insufficient due to unseen pixels in the edited keyframe. We tackle this by further optimizing the atlas texture and the deformation us-ing a pretrained diffusion model by adopting the gradient update procedure described in DreamFusion [38]. Through the atlas optimization, we achieve consistent shape and ap-pearance editing, even in challenging cases where the mov-ing object undergoes 3D transformation (Fig. 1).
Our contributions.
• We extend the capability of existing video editing methods to enable shape-aware editing.
• We present a deformation formulation for frame-dependent shape deformation to handle target shape edits.
• We demonstrate the use of a pre-trained diffusion model for guiding atlas completion in layered video representation. 2.