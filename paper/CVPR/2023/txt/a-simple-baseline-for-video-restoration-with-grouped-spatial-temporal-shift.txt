Abstract
Video restoration, which aims to restore clear frames from degraded videos, has numerous important applica-tions. The key to video restoration depends on utilizing inter-frame information. However, existing deep learn-ing methods often rely on complicated network architec-tures, such as optical flow estimation, deformable convo-lution, and cross-frame self-attention layers, resulting in high computational costs. In this study, we propose a sim-ple yet effective framework for video restoration. Our ap-proach is based on grouped spatial-temporal shift, which is a lightweight and straightforward technique that can implicitly capture inter-frame correspondences for multi-frame aggregation. By introducing grouped spatial shift, we attain expansive effective receptive fields. Combined with basic 2D convolution, this simple framework can effectively aggregate inter-frame information. Extensive experiments demonstrate that our framework outperforms the previous state-of-the-art method, while using less than a quarter of its computational cost, on both video deblurring and video denoising tasks. These results indicate the potential for our approach to significantly reduce computational overhead while maintaining high-quality results. Code is avaliable at https://github.com/dasongli1/Shift-Net. 1.

Introduction
The popularity of capturing videos using handheld de-vices continues to surge. However, these videos often suffer from various types of degradation, including image noise due to low-cost sensors and severe blurs resulting from cam-era shake or object movement. Consequently, video restora-tion has garnered significant attention in recent years.
The keys of video restoration methods lie in designing components to realize alignment across frames. While sev-eral methods [7, 38, 39, 53, 60] employ convolutional net-works for multi-frame fusion without explicit alignment,
Figure 1. Video deblurring on GoPro dataset [40]. Our models have fewer parameters (disk sizes) and occupy the top-left corner, indicating superior performances (PSNR on y-axis) with less com-putational cost (FLOPS on x-axis). their performance tends to be suboptimal. Most methods rely on explicit alignment to establish temporal correspon-dences, using techniques such as optical flow [46, 61] or deformable convolution [11, 69]. However, these ap-proaches often necessitate either complex or computation-ally expensive network architectures to achieve large recep-tive fields, and they may fail in scenarios involving large displacements [27], frame noise [8, 63], and blurry regions
[7, 48]. Recently, transformer [12, 15, 34] becomes promis-ing alternatives for attaining long-range receptive fields. A video restoration transformer (VRT) [32] is developed to model long-range dependency, but its large number of self-attention layers make it computationally demanding.
In-spired by the success of the Swin transformer [34], large kernel convolutions [14, 35] emerge as a direct solution to obtain large effective receptive fields. However, extremely large kernels (e.g. kernel size > 13×13) does not necessar-ily guarantee improved performance. (shown in 5).
In this study, we propose a simple, yet effective spatial-temporal shift block to achieve large effective receptive field for temporal correspondence. We introduce a Group Shift-Net, which incorporates the proposed spatial-temporal shift
Figure 2. Different modules for multi-frame aggregation: a) convolution [53], b) optical flow [32,42], c) deformable convolution [11,54,57], d) self-attention [32, 34] and e) our grouped spatial shift. Point-wise convolution, shortcut and normalization are omitted for simplicity. blocks for alignment along with basic 2D U-Nets for frame-wise feature encoding and restoration. The grouped spatial-temporal shift process involves the separate shifting of input clip features in both temporal and spatial dimensions, fol-lowed by fusion using 2D convolution blocks. Despite its minimal computational demands, the shift block offers large receptive fields for efficient multi-frame fusion. By stacking multiple spatial-temporal shift blocks, the aggregation of long-term information is achieved. This streamlined frame-work models long-term dependencies without depending on resource-demanding optical flow estimation [19,47,61], de-formable convolution [11, 54, 57], or self-attention [32].
Notably, while temporal shift module (TSM) [33] was originally proposed for video understanding, it is not ef-fective for video restoration. Our method distinguishes itself from TSM in three fundamental ways: a) Alterna-tive bi-directional temporal shift. TSM [33] employs bi-directional channel shift during training, causing misalign-ment of channels across three frames, which in turn in-creases the difficulty of multi-frame aggregation. Con-versely, our method utilizes alternative temporal shifts, ef-fectively circumventing this issue. b) Spatial shift. In addi-tion, our approach also incorporates a spatial shift for multi-frame features. We divide the features into several groups, each with distinct shift lengths and directions in the 2D dimension. This grouped spatial shift offers multiple can-didate displacements for matching misaligned features. c)
Feature fusion. To seamlessly merge various shifted groups, the kernel size of the convolution is set equal to the base shift length. By combining elements b) and c), the spatial-temporal shift achieves large receptive fields (e.g. 23 × 23).
The contributions of this study are two-fold: 1) We pro-pose a simple, yet effective framework for video restora-tion, which introduces a grouped spatial-temporal shift for efficient and effective temporal feature aggregation 2) Our framework surpasses state-of-the-art methods with much fever FLOPs on both video deblurring and video denoising tasks, demonstrating its generalization capability. 2.