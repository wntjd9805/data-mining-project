Abstract
Despite the broad interest in meta-learning, the gener-alization problem remains one of the significant challenges in this field. Existing works focus on meta-generalization to unseen tasks at the meta-level by regularizing the meta-loss, while ignoring that adapted models may not general-ize to the task domains at the adaptation level. In this pa-per, we propose a new regularization mechanism for meta-learning – Minimax-Meta Regularization, which employs inverted regularization at the inner loop and ordinary reg-ularization at the outer loop during training. In particular, the inner inverted regularization makes the adapted model more difficult to generalize to task domains; thus, optimiz-ing the outer-loop loss forces the meta-model to learn meta-knowledge with better generalization. Theoretically, we prove that inverted regularization improves the meta-testing performance by reducing generalization errors. We conduct extensive experiments on the representative scenarios, and the results show that our method consistently improves the performance of meta-learning algorithms. 1.

Introduction
Meta-learning has been proven to be a powerful paradigm for extracting well-generalized knowledge from previous tasks and quickly learning new tasks [47]. It has received increasing attention in many machine learning set-tings such as few-shot learning [10, 45, 46, 50] and robust learning [27, 39, 42], and can be deployed in many practical applications [7, 21, 29, 54]. The key idea of meta-learning is to improve the learning ability of agents through a learning-to-learn process.
In recent years, optimization-based al-gorithms have emerged as a popular approach for realiz-ing the learning-to-learn process in meta-learning [10, 28].
These methods formulate the problem as a bi-level opti-mization problem and have demonstrated impressive per-*Equal contributions
†Corresponding authors formance across various domains, leading to significant at-tention from the research community. The primary focus of our paper is to further advance this line of research.
The training process of meta-learning takes place at two levels [10, 19]. At the inner-level, a base model, which is initialized using the meta-model’s parameters, adapts to each task by taking gradient descent steps over the support set. At the outer-level, a meta-training objective is opti-mized to evaluate the generalization capability of the initial-ization on all meta-training tasks over the query set, help-ing to ensure that the model is effectively optimized for the desired goal. With this learning-to-learn process, the final trained meta-model could be regarded as the model with good initialization to adapt to new tasks.
Despite the success of meta-learning, the additional level of learning also introduces a new source of potential over-fitting [36], which poses a significant challenge to the gen-eralization of the learned initialization. This generalization challenge is twofold: first, the meta-model must general-ize to unseen tasks (meta-generalization); and second, the adapted model must generalize to the domain of a specific task, which we refer to as adaptation-generalization. As the primary objective of meta-learning is to achieve strong performance when adapting to new tasks, the ability of the meta-model to generalize well is critical. Recent works aim to address the meta-generalization problem by meta-regularizations, such as constraining the meta-initialization space [52], enforcing the performance similarity of the meta-model on different tasks [20], and augmenting meta-training data [33, 36, 51]. These approaches are verified to enhance generalization to unseen tasks. However, they do not address the problem of adaptation-generalization to the data distribution of meta-testing tasks.
To address this issue, we propose Minimax-Meta Regu-larization, a novel regularization mechanism that improves both adaptation-generalization and meta-generalization.
Specifically, our approach particularly employs inverted regularization at to hinder the adapted model’s generalizability to the task domain. This forces the the inner-level
meta-model to learn hypotheses that better generalize to the task domains, which improves adaptation-generalization.
Meanwhile, we use ordinary regularization at the outer-level to optimize the meta-model’s generalization to new tasks, which helps meta-generalization. By improving both adaptation-generalization and meta-generalization simulta-neously, our method results in a more robust and effective meta-learning regularization mechanism.
Theoretically, we prove that under certain assumptions, if we add L2-Norm as the regularization term to the inner-level loss function, the inverted regularization will reduce the generalization bound of MAML, while the ordinary reg-ularization will increase the generalization bound. In terms of total test error, which includes both generalization error and training bias caused by regularization, the inverted L2-Norm also reduces the total test error when the reg parame-ter is selected within a negative interval. These results sug-gest that the regularization at the inner-level should be in-verted. As it has been verified that ordinary regularization at the outer-level helps the meta-generalization, our theory im-plies that the proposed Minimax-Meta Regularization helps both meta-generalization and adaptation-generalization.
We conduct experiments on the few-shot classification problem for MAML [10] with different regularization types (ordinary/inverted) at the inner- and outer-level. The results demonstrate the efficacy of Minimax-Meta Regularization, and support the theoretical results that regularization at the inner-level improves test performance only when it’s in-verted. Additionally, we empirically verify that Minimax-Meta regularization can be applied with different types of regularization terms (norm/entropy), implying the flexibil-ity for applying the proposed method in practice. 2.