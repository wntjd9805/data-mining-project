Abstract
Counterfactual explanations and adversarial attacks have a related goal: flipping output labels with minimal perturbations regardless of their characteristics. Yet, ad-versarial attacks cannot be used directly in a counterfac-tual explanation perspective, as such perturbations are per-ceived as noise and not as actionable and understandable image modifications. Building on the robust learning liter-ature, this paper proposes an elegant method to turn adver-sarial attacks into semantically meaningful perturbations, without modifying the classifiers to explain. The proposed approach hypothesizes that Denoising Diffusion Probabilis-tic Models are excellent regularizers for avoiding high-frequency and out-of-distribution perturbations when gen-erating adversarial attacks. The paper’s key idea is to build attacks through a diffusion model to polish them. This al-lows studying the target model regardless of its robustifi-cation level. Extensive experimentation shows the advan-tages of our counterfactual explanation approach over cur-rent State-of-the-Art in multiple testbeds. 1.

Introduction
The research branch of explainable artificial intelligence has yielded remarkable results, gradually opening the ma-chine learning black boxes. The production of counter-factual explanations (CE) has become one of the promis-ing pipelines for explainability, especially in computer vi-sion [25, 28, 49, 54]. As a matter of fact, CE are an intuitive way to expose how an input instance can be minimally mod-ified to steer the desired change in the model’s output. More precisely, CE answers the following: what does X have to change to alter the prediction from Y to Y ′? From a user perspective, these explanations are easy to understand since they are concise and illustrated by examples. Henceforth, companies have adopted CE as an interpretation methodol-ogy to legally justify the decision-making of machine learn-ing models [60]. To better appreciate the potential of CE, one may consider the following scenario: a client goes to a photo booth to take some ID photos, and the system claims the photos are invalid for such usage. Instead of performing random attempts to abide by the administration criteria, an approach based on CE could provide visual indications of what the client should fix.
The main objective of CE is to add minimalistic semantic changes in the image to flip the original model’s prediction.
Yet, these generated explanations must accomplish several objectives [28,49,60]. A CE must be valid, meaning that the
CE has to change the prediction of the model. Secondly, the modifications have to be sparse and proximal to the input data, targeting to provide simple and concise explanations.
In addition, the CE method should be able to generate di-verse explanations. If a trait is the most important for a cer-tain class among other features, diverse explanations should change this attribute most frequently. Finally, the semantic changes must be realistic. When the CE method inserts out-of-distribution artifacts in the input image, it is difficult to interpret whether the flipping decision was because of the inserted object or because of the shifting of the distribution, making the explanation unclear.
Adversarial attacks share a common goal with CE: flip-ping the classifier’s prediction. For traditional and non-robust visual classifiers, generating these attacks on input instances creates imperceptible noise. Even though it has been shown that it contains meaningful changes [24] and that adversarial noise and counterfactual perturbations are related [13, 23], adversarial attacks have lesser value. In-deed, the modifications present in the adversaries are unno-ticeable by the user and leave him with no real feedback.
Contrary to the previous observations, many papers (e.g.,
[47]) evidenced that adversarial attacks toward robust clas-sifiers generate semantic changes in the input images. This has led works [51, 70] to explore robust models to produce data using adversarial attacks. In the context of counterfac-tual explanations, this is advantageous [5, 52] because the optimization will produce semantic changes to induce the flipping of the label.
Then two challenges arise when employing adversarial attacks for counterfactual explanations. On the one hand, when studying a classifier, we must be able to explain its behavior regardless of its characteristics. So, a naive ap-plication of adversarial attacks is impractical for non-robust models. On the other hand, according to [57], robustify-ing the classifier yields an implicit trade-off by lowering the clean accuracy, as referred by the adversarial robustness community [10], a particularly crucial trait for high-stakes areas such as the medical field [40].
The previous remarks motivate our endeavor to mix the best of both worlds. Hence, in this paper, we propose robus-tifying brittle classifiers without modifying their weights to generate CE. This robustification, obtained through a filter-ing preprocessing leveraging diffusion models [19], allows us to keep the performance of the classifier untouched and unlocks the production of CE through adversarial attacks.
We summarize the novelty of our paper as follows: (i)
We propose Adversarial Counterfactual Explanations, ACE in short, a novel methodology based on adversarial attacks to generate semantically coherent counterfactual explana-tions. (ii) ACE performs competitively with respect to the other methods, beating previous state-of-the-art methods in multiple measurements along multiple datasets. (iii) Fi-nally, we point out some defects of current evaluation met-rics and propose ways to remedy their shortcomings. (iv)
To show a use case of ACE, we study ACE’s meaningful and plausible explanations to comprehend the mechanisms of classifiers. We experiment with ACE findings producing actionable modifications in real-world scenarios to flip the classifier decision.
Our code and models are available on GitHub. 2.