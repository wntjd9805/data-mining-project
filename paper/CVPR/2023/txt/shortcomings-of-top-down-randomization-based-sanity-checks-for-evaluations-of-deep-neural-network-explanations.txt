Abstract
While the evaluation of explanations is an important step towards trustworthy models, it needs to be done carefully, and the employed metrics need to be well-understood. Specif-ically model randomization testing can be overinterpreted if regarded as a primary criterion for selecting or discard-ing explanation methods. To address shortcomings of this test, we start by observing an experimental gap in the rank-ing of explanation methods between randomization-based sanity checks [1] and model output faithfulness measures (e.g. [20]). We identify limitations of model-randomization-based sanity checks for the purpose of evaluating explana-tions. Firstly, we show that uninformative attribution maps created with zero pixel-wise covariance easily achieve high scores in this type of checks. Secondly, we show that top-down model randomization preserves scales of forward pass activations with high probability. That is, channels with large activations have a high probility to contribute strongly to the output, even after randomization of the network on top of them. Hence, explanations after randomization can only be expected to differ to a certain extent. This explains the observed experimental gap. In summary, these results demonstrate the inadequacy of model-randomization-based sanity checks as a criterion to rank attribution methods. 1.

Introduction
Parallel to the progressively astounding performances of machine learning techniques, especially deep learning methods, in solving even the most complex tasks, the trans-parency, trustworthiness, and lack of interpretability of these techniques has increasingly been called into ques-tion [11, 14, 15]. As potential solutions to these issues, a vast number of XAI methods have been developed in re-cent years [21], that aim to explain a model’s behavior, for instance, by (locally) attributing importance scores to fea-tures of singular input samples, indicating how (much) these features influence a specific model decision [6, 22, 25, 27].
However, the scores obtained for different attribution map methods tend to differ significantly, and the question arises how well each explains model decisions. This is generally not answered easily, as there are a number of desirable prop-erties proposed to be fulfilled by these attributions, such as localization on relevant objects [4, 5, 30] or faithfulness to the model output [2, 8, 20], among others, with several quantitative tests having been proposed for each.
In parallel to these empirical evaluations, several works have proposed that explanations should fulfill a certain num-ber of ‘axioms’ or ‘unit tests’ [1, 12, 16, 27], which need to hold universally for a method to be considered good or valid.
We place our focus on the model-randomization-based sanity checks [1], which state that the explanation should be sensi-tive to a random permutation of parameters at one or more layers in the network. Specifically, the authors proposed to apply measures such as Structural Similarity Index Measure (SSIM) [28] between attribution maps obtained from the original model and a derived model for which the top-layers are randomized. The idea is to require that methods used to compute attribution maps should exhibit a large change when the neural network model — i.e., its defining/learned parameter set — is randomized from the top. The authors of [1,23] suggested to discard attribution map methods which perform poorly under this test — i.e., have a high SSIM mea-sure between attributions obtained with the original and the randomized model — under the assumption that those XAI methods are not affected by the model’s learned parameters.
However, we observe a significant experimental gap be-tween top-down randomization checks when used as an eval-uation measure, and occlusion-based evaluations of model faithfulness such as region perturbation [20]. Concretely,
Guided Backpropagation (GB) [25] and Layer-wise Rel-evance Propagation (LRP) [6] exhibit low randomization scores under the first type of measure and yet clearly out-perform several gradient-based methods in occlusion-based evaluations. We are interested to resolve this discrepancy.
We identify two shortcomings of top-down randomiza-tion checks when used as a measure of explanation quality.
Firstly, we show that uninformative attribution maps created with zero pixel-wise covariance — e.g., attribution maps gen-erated from random noise — easily achieve high scores in top-down randomization checks. Effectively, this makes top-down randomization checks favor attribution maps which are affected by gradient shattering noise [7].
Secondly, we argue that the randomization-based sanity checks may always reward explanations that change under randomization, even when such randomizations do not affect the output of the model (and its invariances) significantly.
Such invariance to randomization may result, e.g., from the presence of skip connections in the model, but also due to the fact that randomization may be insufficient to strongly alter the spatial distribution of activations in adjacent layers, something that we explain by the multiplicity and redun-dancy of positive activation paths between adjacent layers in ReLU networks. In setups which optimize parameters of attribution methods while measuring top-down randomiza-tion this might lead to the selection of explainers with higher noise.
Along with our contributed theoretical insights and sup-porting experiments, the present note warns against an unre-flected use of model-randomization-based sanity checks as a sole criterion for selecting or dismissing a particular attri-bution technique, and proposes several directions to enable a more precise and informative use of randomization-based sanity checks for assessing how XAI performs on practical
ML models. 1.1.