Abstract
In this work, we tackle the problem of online camera-to-robot pose estimation from single-view successive frames of an image sequence, a crucial task for robots to inter-act with the world. The primary obstacles of this task are the robot’s self-occlusions and the ambiguity of single-view images. This work demonstrates, for the first time, the ef-fectiveness of temporal information and the robot structure prior in addressing these challenges. Given the succes-sive frames and the robot joint configuration, our method learns to accurately regress the 2D coordinates of the pre-defined robot’s keypoints (e.g. joints). With the camera in-trinsic and robotic joints status known, we get the camera-to-robot pose using a Perspective-n-point (PnP) solver. We further improve the camera-to-robot pose iteratively using the robot structure prior. To train the whole pipeline, we build a large-scale synthetic dataset generated with do-main randomisation to bridge the sim-to-real gap. The ex-tensive experiments on synthetic and real-world datasets and the downstream robotic grasping task demonstrate that our method achieves new state-of-the-art performances and outperforms traditional hand-eye calibration algorithms in real-time (36 FPS). Code and data are available at the project page: https://sites.google.com/view/sgtapose. 1.

Introduction
Camera-to-robot pose estimation is a crucial task in determining the rigid transformation between the camera space and robot base space in terms of rotation and trans-lation. Accurate estimation of this transformation enables robots to perform downstream tasks autonomously, such as grasping, manipulation, and interaction. Classic camera-to-robot estimation approaches, e.g. [11, 14, 33], typically involve attaching augmented reality (AR) tags as mark-ers to the end-effector and directly solving a homogeneous matrix equation to calculate the transformation. However, these approaches have critical drawbacks. Capturing mul-tiple joint configurations and corresponding images is al-*: Equal contributions. †: Corresponding author
Figure 1. Overview of the proposed SGTAPose. Given a tem-poral sequence of RGB frames and known robot structure priors, our method estimates the 2D keypoints (e.g., joints) of the robot and performs real-time estimation of the camera-to-robot pose by combining a Perspective-n-point (PnP) solver(left). This real-time camera-to-robot pose estimation approach can be utilised for vari-ous downstream tasks, such as robotic grasping(right). ways troublesome, and these methods cannot be used on-line. These flaws become greatly amplified when down-stream tasks require frequent camera position adjustment.
To mitigate this limitation of classic offline hand-eye cal-ibration, some recent works [19, 20] introduce vision-based methods to estimate the camera-to-robot pose from a single image, opening the possibility of online hand-eye calibra-tion. Such approaches significantly grant mobile and itin-erant autonomous systems the ability to interact with other robots using only visual information in unstructured envi-ronments, especially in collaborative robotics [21].
Most existing learning-based camera-to-robot pose esti-mation works [19, 21, 26, 30] focus on single-frame estima-tion. However, due to the ambiguity of the single-view im-age, these methods do not perform well when the robotic arm is self-occluded. Since the camera-to-robot pose is likely invariant during a video sequence and the keypoints are moving continually, one way to tackle this problem is
to introduce temporal information. However, a crucial tech-nical challenge of estimating camera-to-robot pose tempo-rally is how to fuse temporal information efficiently. To this end, as shown in Fig. 1, we propose Structure Prior
Guided Temporal Attention for Camera-to-Robot Pose es-timation (SGTAPose) from successive frames of an image sequence. First, we proposed robot structure priors guided feature alignment approach to align the temporal features in two successive frames. Moreover, we apply a multi-head-cross-attention module to enhance the fusion of features in sequential images. Then, after a decoder layer, we solve an initial camera-to-robot pose from the 2D projections of detected keypoints and their 3D positions via a PnP solver.
We lastly reuse the structure priors as an explicit constraint to acquire a refined camera-to-robot pose.
By harnessing the temporal information and the robot structure priors, our proposed method gains significant per-formance improvement in the accuracy of camera-to-robot pose estimation and is more robust to robot self-occlusion.
We have surpassed previous online camera calibration ap-proaches in synthetic and real-world datasets and show a strong dominance in minimising calibration error compared with traditional hand-eye calibration, where our method could reach the level of 5mm calibration errors via multi-frame PnP solving. Finally, to test our method’s capability in real-world experiments, we directly apply our predicted pose to help implement grasping tasks. We have achieved a fast prediction speed (36FPS) and a high grasping success rate. Our contributions are summarised as follows:
• For the first time, we demonstrate the remarkable performance of camera-to-robot pose estimation from successive frames of a single-view image sequence.
• We propose a temporal cross-attention strategy absorb-ing robot structure priors to efficiently fuse successive frames’ features to estimate camera-to-robot pose.
• We demonstrate our method’s capability of imple-menting downstream online grasping tasks in the real world with high accuracy and stability, even beyond the performance of classical hand-eye calibration. 2.