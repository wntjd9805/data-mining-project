Abstract
Both masked image modeling (MIM) and natural lan-guage supervision have facilitated the progress of trans-ferable visual pre-training. In this work, we seek the syn-ergy between two paradigms and study the emerging prop-erties when MIM meets natural language supervision. To this end, we present a novel masked visual Reconstruction
In Language semantic Space (RILS) pre-training frame-work, in which sentence representations, encoded by the text encoder, serve as prototypes to transform the vision-only signals into patch-sentence probabilities as semantically meaningful MIM reconstruction targets. The vision models can therefore capture useful components with structured in-formation by predicting proper semantic of masked tokens.
Better visual representations could, in turn, improve the text encoder via the image-text alignment objective, which is essential for the effective MIM target transformation. Ex-tensive experimental results demonstrate that our method not only enjoys the best of previous MIM and CLIP but also achieves further improvements on various tasks due to their mutual benefits. RILS exhibits advanced transferabil-ity on downstream classification, detection, and segmenta-tion, especially for low-shot regimes. Code is available at https://github.com/hustvl/RILS. 1.

Introduction
Learning transferable representation lies a crucial task in deep learning. Over the past few years, natural language processing (NLP) has achieved great success in this line of research [18, 45, 60]. To explore similar trajectories in the vision domain, researchers tend to draw upon the suc-cesses of NLP and have made tremendous progress: (i)
Inspired by the advanced model architecture [60] as well as self-supervised learning paradigm [18] in NLP, vision
Transformers (ViT) [21, 42] and masked image modeling
*This work was done while S. Yang was an intern at ARC Lab, Tencent
PCG. † X. Wang and Y. Ge are the corresponding authors.
Figure 1. Overview of our RILS. Recon and Contra represent masked reconstruction loss and image-text contrastive loss. Dur-ing pre-training, RILS learns to perform masked image modeling and image-text contrastive simultaneously. Masked predictions and corresponding targets are transformed to probabilistic distri-butions in language space by leveraging text features as semantic-rich prototypes. Under such a scheme, both objective are unified and achieve mutual benefits from each other. Vision encoder ob-tains the ability to capture meaningful and fine-grained context in-formation, sparking decent transfer capacity. (MIM) [3, 28] open a new era of self-supervised visual representation learning, and show unprecedented transfer-ability on various tasks, especially on fine-grained tasks such as object detection [22, 39]. (ii) Inspired by the great scalability brought by leveraging web-scale collections of texts as training data in NLP [5, 48, 49, 72], CLIP [47] and
ALIGN [35] bring such a principle to vision and manifest the immense potential of leveraging natural language super-vision for scalable visual pre-training. Strong transferability of pre-trained visual models on low-shot regimes ensues. It also facilitates diverse applications by extracting contextu-alized image or text features [26, 50, 52].
The remarkable attainment achieved by these two re-search lines pushes us to ponder: Is it possible to unify both masked image modeling and natural language supervision to pursuit better visual pre-training? A straightforward way towards this goal is to simply combine masked image mod-eling (MIM) with image-text contrastive learning (ITC) for multi-task learning. Although the na¨ıve combination is fea-sible to inherit the above advantages, we find it remains un-satisfactory due to the mutual benefits between MIM and
ITC have not yet been fully explored. Motivated by this, we develop RILS, a tailored framework to seek the synergy of masked image modeling and language supervision.
The core insight of RILS is to perform masked visual re-construction in language semantic space. Specifically, in-stead of reconstructing masked patches in the standalone vi-sion space (e.g., raw pixel [28,66], low-level features [3,62] or high-level perceptions [12, 34, 63, 75]), we map patch features to a probabilistic distribution over a batch of text features as the reconstruction target, which is enabled by
ITC that progressively aligns the image and text spaces.
The text features serve as semantically rich prototypes and probabilistic distributions explicitly inject the semantic in-formation onto each image patch. The MIM objective is formulated as a soft cross-entropy loss to minimize the KL divergence between text-injected probabilistic distributions of masked vision tokens and their corresponding targets.
The visual model optimized by our language-assisted re-construction objective, in turn, improves ITC with better vi-sual representations that capture fine-grained local contexts. the two objec-tives(i.e., MIM and ITC) complement each other and form a unified solution for transferable and scalable visual pre-training. Note that a lot of works [34, 63, 75] have mani-fested the importance of semantic information in the recon-struction target of MIM objectives. However, it is abstract to pursue such a semantically rich space with visual-only signals due to its unstructured characteristics [29]. Thanks to natural language supervision, this issue is alleviated by performing masked reconstruction in language space.
Under such a working mechanism,
Extensive experiments on various downstream tasks demonstrate that our design enjoys the best of both worlds. With a vanilla ViT-B/16 as the vision model and 25-epoch pre-training on 20 million image-text pairs,
RILS achieves 83.3% top-1 accuracy when fine-tune on
ImageNet-1K [15], +1.2% and +0.6% better than the
MAE [28] and CLIP [47] counterparts. Advanced perfor-mance can be consistently acquired when transfer to fine-grained tasks such as detection and segmentation. More-over, our approach exhibits promising out-of-the-box ca-pability under an extremely low-shot regime. RILS also image demonstrates superior performance on zero-shot classification and image-text retrieval. On ImageNet-1K benchmark, RILS obtains 45.0% zero-shot classification ac-curacy, +4.7%/ + 3.4% higher than CLIP [47]/SLIP [43] under the same training recipe. Compelling results of RILS imply the promising capacity in the unification of MIM and language supervision. 2.