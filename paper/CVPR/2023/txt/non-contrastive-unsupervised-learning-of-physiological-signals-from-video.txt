Abstract
Subtle periodic signals such as blood volume pulse and respiration can be extracted from RGB video, enabling non-contact health monitoring at low cost. Advancements in remote pulse estimation – or remote photoplethysmogra-phy (rPPG) – are currently driven by deep learning solu-tions. However, modern approaches are trained and evalu-ated on benchmark datasets with ground truth from contact-PPG sensors. We present the first non-contrastive unsuper-vised learning framework for signal regression to mitigate the need for labelled video data. With minimal assumptions of periodicity and finite bandwidth, our approach discov-ers the blood volume pulse directly from unlabelled videos.
We find that encouraging sparse power spectra within nor-mal physiological bandlimits and variance over batches of power spectra is sufficient for learning visual features of periodic signals. We perform the first experiments utilizing unlabelled video data not specifically created for rPPG to train robust pulse rate estimators. Given the limited induc-tive biases and impressive empirical results, the approach is theoretically capable of discovering other periodic signals from video, enabling multiple physiological measurements without the need for ground truth signals. 1.

Introduction
Camera-based vitals estimation is a rapidly growing field enabling non-contact health monitoring in a variety of set-tings [23]. Although many of the signals avoid detection from the human eye, video data in the visible and infrared ranges contain subtle intensity changes caused by physio-logical oscillations such as blood volume and respiration.
Significant remote photoplethysmography (rPPG) research for estimating the cardiac pulse has leveraged supervised deep learning for robust signal extraction [7, 18, 30, 38, 51, 52]. While the number of successful approaches has rapidly increased, the size of benchmark video datasets with simul-taneous vitals recordings has remained relatively stagnant.
Robust deep learning-based systems for deployment re-quire training on larger volumes of video data with di-verse skin tones, lighting, camera sensors, and movement.
However, collecting simultaneous video and physiologi-cal ground truth with contact-PPG or electrocardiograms (ECG) is challenging for several reasons. First, many hours of high quality videos is an unwieldy volume of data. Sec-ond, recording a diverse subject population in conditions representative of real-world activities is difficult to conduct in the lab setting. Finally, synchronizing contact measure-ments with video is technically challenging, and even con-tact measurements used for ground truth contain noise.
Fortunately, recent works find that contrastive unsuper-vised learning for rPPG is a promising solution to the data scarcity problem [13, 42, 45, 50]. We extend this research line into non-contrastive unsupervised learning to discover periodic signals in video data. With end-to-end unsuper-vised learning, collecting more representative training data to learn powerful visual features is much simpler, since only video is required without associated medical information.
In this work, we show that non-contrastive unsupervised learning is especially simple when regressing rPPG signals.
We find weak assumptions of periodicity are sufficient for learning the minuscule visual features corresponding to the blood volume pulse from unlabelled face videos. The loss functions can be computed in the frequency domain over batches without the need for pairwise or triplet compar-isons. Figure 1 compares the proposed approach with su-pervised and contrastive unsupervised learning approaches.
This work creates opportunities for scaling deep learning models for camera-based vitals and estimating periodic or quasi-periodic signals from unlabelled data beyond rPPG.
Our novel contributions are: 1. A general framework for physiological signal estima-tion via non-contrastive unsupervised learning (SiNC) by leveraging periodic signal priors. 2. The first non-contrastive unsupervised learning method for camera-based vitals measurement. 3. The first experiments and results of training with non-rPPG-specific video data without ground truth vitals.
Source code to replicate this work is available at https:
//github.com/CVRL/SiNC-rPPG.
Figure 1. Overview of the SiNC framework for rPPG compared with traditional supervised and unsupervised learning. Supervised and contrastive losses use distance metrics to the ground truth or other samples. Our framework applies the loss directly to the prediction by shaping the frequency spectrum, and encouraging variance over a batch of inputs. Power outside of the bandlimits is penalized to learn invariances to irrelevant frequencies. Power within the bandlimits is encouraged to be sparsely distributed near the peak frequency. 2.