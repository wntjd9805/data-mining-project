Abstract
Image generation relies on massive training data that can hardly produce diverse images of an unseen category according to a few examples.
In this paper, we address this dilemma by projecting sparse few-shot samples into a continuous latent space that can potentially generate in-finite unseen samples. The rationale behind is that we aim to locate a centroid latent position in a conditional
StyleGAN, where the corresponding output image on that centroid can maximize the similarity with the given sam-ples. Although the given samples are unseen for the con-ditional StyleGAN, we assume the neighboring latent sub-space around the centroid belongs to the novel category, and therefore introduce two latent subspace optimization objectives. In the first one we use few-shot samples as pos-itive anchors of the novel class, and adjust the StyleGAN to produce the corresponding results with the new class label condition. The second objective is to govern the genera-tion process from the other way around, by altering the cen-*Equal Contributions.
†Corresponding authors. troid and its surrounding latent subspace for a more pre-cise generation of the novel class. These reciprocal opti-mization objectives inject a novel class into the StyleGAN latent subspace, and therefore new unseen samples can be easily produced by sampling images from it. Extensive ex-periments demonstrate superior few-shot generation perfor-mances compared with state-of-the-art methods, especially in terms of diversity and generation quality. Code is avail-able at https://github.com/chansey0529/LSO. 1.

Introduction
Recent advances in generative models [3, 5, 8, 11, 19, 36] allow synthesizing of high-quality and realistic images with diverse styles. However, the success of these models re-lies heavily on large-scale data. Preparing new data for a novel class is costly, so it is natural to raise a question, “can we generate high-quality images with a glance at a few im-ages?” This leads to the few-shot image generation prob-lem, where the model is required to generate a novel cate-gory with only a few images available. Unfortunately, since the extreme low-shot setting can easily cause catastrophic
over-fitting, few-shot image generation is still challenging.
Existing methods commonly suppose that the seen mod-els have implicit generalization ability towards unseen cate-gories. Based on this assumption, task-specific optimization is adopted to seek proper initial parameters, which better generalize to the downstream tasks [6, 25]. Testing phase generation is another solution, which skips integrating the information of unseen category into model weights. Never-theless, the generated images are either with a lot of class-specific information distortion [7] or fail to restore the de-tailed features, such as textures [10, 44]. The main assump-tion of this line of research in model generalization ability is false, and therefore the model trained on seen data can-not extract out-of-domain unseen-specific features without adaptation, e.g., generating a spotted dog via glancing on a golden retriever, which significantly limits their practical usage in real-world scenarios. As a consequence, a key fac-tor to the success of few-shot synthesis is to expose the sam-ples of unseen classes to the model.
One of the major obstacles is the sparsity of the unseen samples. Traditional generative networks require model-ing the continuous distribution for generating diverse im-ages with unseen-specific features. However, the discrete data points under the few-shot setting make the model ill-informed about the inner structure of the unseen distribu-tion. On the other hand, the pretrained latent spaces of
Style-series models [17–19, 43] are shown to be semanti-cally interpretable and continuous. This property ideally fits our problem. Once the proper latent locations of unseen samples are found, we can complement the marginal region with the hidden semantic information and form a subspace for the unseen category. In this way, diverse unseen images can be generated via sampling from the new subspace.
Based on the above insights, we proposed a novel la-tent subspace optimization framework for few-shot image generation. The key idea is to search for the optimal sub-distribution of unseen using latent anchor localization, and then align the sub-distribution with the input unseen distri-bution using latent subspace refinement. To obtain an un-seen correlated semantic region in the latent space, we first locate the subspace of the unseen category by faithful an-chor optimization. Specifically, the latent codes of the un-seen category are served as reliable latent subspace indica-tors by inverting the available unseen images into the latent space. Based on these anchors, the coarse centroid of the unseen distribution is pulled to the hypothetical point using a subspace localization loss.
Subsequently, due to the semantic deficiency of few-shot images, distributional shift exists between the resulting dis-tribution of our subspace and the real unseen distribution.
To mitigate semantic misalignment, we propose to refine the latent subspace of unseens. We employ an adversar-ial training scheme to inject the unseen correlated features into the generator. However, the guidance of the adversar-ial game easily leads to over-emphasis on transferring the low-level features, ignoring the learning of unseen seman-tics (e.g., fails to generate a wolf but a wolf-like dog). Thus, the generated images may belong to a completely different semantic category, though they contain similar textures with the few-shot examples. To preserve the unseen-specific se-mantic, we further restrict the latent subspace by a semantic stabilization loss. Once the StyleGAN and its subspace are properly optimized, our framework is able to generate di-verse and high-quality unseen images. We compare to state-of-the-art methods extensively on different datasets, and we show significant superiority over them.
In summary, the contribution of this paper is fourfold:
• We delve into few-shot image generation from a novel perspective of exploring the continuity of the latent space for discovering unseen category.
• We propose a novel latent subspace optimization framework to model the distribution of unseen sam-ples, while injecting category-specific features into the generated images.
• Experimental results show that our approach achieves state-of-the-art performances on three datasets, largely reducing the FID scores by 7.58, 4.37, and 0.98 on
Flowers, AnimalFaces, and VGGFaces respectively while gaining diversity on most datasets.
• We extend our model to other subfields like image edit-ing and high-resolution image generation with few-shot setting. Additionally, we explore the potential of our framework in few-shot incremental generation. 2.