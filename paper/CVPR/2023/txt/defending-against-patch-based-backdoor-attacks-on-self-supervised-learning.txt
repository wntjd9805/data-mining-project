Abstract
Recently, self-supervised learning (SSL) was shown to be vulnerable to patch-based data poisoning backdoor at-tacks. It was shown that an adversary can poison a small part of the unlabeled data so that when a victim trains an SSL model on it, the final model will have a back-door that the adversary can exploit. This work aims to defend self-supervised learning against such attacks. We use a three-step defense pipeline, where we first train a model on the poisoned data. In the second step, our pro-posed defense algorithm (PatchSearch) uses the trained model to search the training data for poisoned samples and removes them from the training set. In the third step, a final model is trained on the cleaned-up training set.
Our results show that PatchSearch is an effective defense.
As an example, it improves a model’s accuracy on im-ages containing the trigger from 38.2% to 63.7% which is very close to the clean model’s accuracy, 64.6%. More-over, we show that PatchSearch outperforms baselines and state-of-the-art defense approaches including those using additional clean, trusted data. Our code is available at https://github.com/UCDvision/PatchSearch 1.

Introduction
Self-supervised learning (SSL) promises to free deep learning models from the constraints of human supervision.
It allows models to be trained on cheap and abundantly available unlabeled, uncurated data [22]. A model trained this way can then be used as a general feature extractor for various downstream tasks with a small amount of labeled data. However, recent works [8, 44] have shown that un-curated data collection pipelines are vulnerable to data poi-soning backdoor attacks.
Data poisoning backdoor attacks on self-supervised learning (SSL) [44] work as follows. An attacker intro-duces “backdoors” in a model by simply injecting a few carefully crafted samples called “poisons” in the unlabeled
*Corresponding author <atejankar@ucdavis.edu>. Work done while interning at Meta AI. training dataset. Poisoning is done by pasting an attacker chosen “trigger” patch on a few images of a “target” cat-egory. A model pre-trained with such a poisoned data be-haves similar to a non-poisoned/clean model in all cases ex-cept when it is shown an image with a trigger. Then, the model will cause a downstream classifier trained on top of it to mis-classify the image as the target category. This types of attacks are sneaky and hard to detect since the trigger is like an attacker’s secret key that unlocks a failure mode of the model. Our goal in this work is to defend SSL models against such attacks.
While there have been many works for defending su-pervised learning against backdoor attacks [34], many of them directly rely upon the availability of labels. There-fore, such methods are hard to adopt in SSL where there are no labels. However, a few supervised defenses [5, 28] can be applied to SSL with some modifications. One such de-fense, proposed in [28], shows the effectiveness of applying a strong augmentation like CutMix [55]. Hence, we adopt i-CutMix [33], CutMix modified for SSL, as a baseline. We show that i-CutMix is indeed an effective defense that addi-tionally improves the overall performance of SSL models.
Another defense that does not require labels was pro-posed in [44]. While this defense successfully mitigates the attack, its success is dependent on a significant amount of clean, trusted data. However, access to such data may not be practical in many cases. Even if available, the trusted data may have its own set of biases depending on its sources which might bias the defended model. Hence, our goal is to design a defense that does not need any trusted data.
In this paper, we propose PatchSearch, which aims at identifying poisoned samples in the training set without ac-cess to trusted data or image labels. One way to identify a poisoned sample is to check if it contains a patch that be-haves similar to the trigger. A characteristic of a trigger is that it occupies a relatively small area in an image (≈ 5%) but heavily influences a model’s output. Hence, we can use an input explanation method like Grad-CAM [45] to high-light the trigger. Note that this idea has been explored in supervised defenses [14, 16]. However, Grad-CAM relies on a supervised classifier which is unavailable in our case.
Figure 1. Illustration of PatchSearch. SSL has been shown to group visually similar images in [4, 47], and [44] showed that poisoned images are close to each other in the representation space. Hence, the very first step (a) is to cluster the training dataset. We use clustering to locate the trigger and to efficiently search for poisoned samples. The second step locates the candidate trigger in an image with Grad-CAM (b) and assigns a poison score to it (c). The third step (d) searches for highly poisonous clusters and only scores images in them. This step outputs a few highly poisonous images which are used in the fourth and final step (e) to train a more accurate poison classifier.
This is a key problem that we solve with k-means cluster-ing. An SSL model is first trained on the poisoned data and its representations are used to cluster the training set. SSL has been shown to group visually similar images in [4, 47], and [44] showed that poisoned images are close to each other in the representation space. Hence, we expect poi-soned images to be grouped together and the trigger to be the cause of this grouping. Therefore, cluster centers pro-duced by k-means can be used as classifier weights in place of a supervised classifier in Grad-CAM. Finally, once we have located a salient patch, we can quantify how much it behaves like a trigger by pasting it on a few random images, and counting how many of them get assigned to the cluster of the patch (Figure 1 (c)). A similar idea has been explored in [14], but unlike ours, it requires access to trusted data, it is a supervised defense, and it operates during test time.
One can use the above process of assigning poison scores to rank the entire training set and then treat the top ranked images as poisonous. However, our experiments show that in some cases, this poison detection system has very low precision at high recalls. Further, processing all images is redundant since only a few samples are actually poisonous.
For instance, there can be as few as 650 poisons in a dataset of 127K samples (≈ 0.5%) in our experiments. Hence, we design an iterative search process that focuses on finding highly poisonous clusters and only scores images in them (Figure 1 (d)). The results of iterative search are a few but highly poisonous samples. These are then used in the next step to build a poison classifier which can identify poisons more accurately. This results in a poison detection system with about 55% precision and about 98% recall in average.
In summary, we propose PatchSearch, a novel de-fense algorithm that defends self-supervised learning mod-els against patch based data poisoning backdoor attacks by efficiently identifying and filtering out poisoned samples.
We also propose to apply i-CutMix [33] augmentation as a simple but effective defense. Our results indicate that Patch-Search is better than both the trusted data based defense from [44] and i-CutMix. Further, we show that i-CutMix and PatchSearch are complementary to each other and com-bining them results in a model with an improved overall performance while significantly mitigating the attack. 2.