Abstract 1.

Introduction
Over the last few decades, many aspects of human life have been enhanced with virtual domains, from the advent of digital assistants such as Amazon’s Alexa and Apple’s
Siri to the latest metaverse efforts of the rebranded Meta.
These trends underscore the importance of generating pho-torealistic visual depictions of humans. This has led to the rapid growth of so-called deepfake and talking-head gen-eration methods in recent years. Despite their impressive results and popularity, they usually lack certain qualitative aspects such as texture quality, lips synchronization, or res-olution, and practical aspects such as the ability to run in real-time. To allow for virtual human avatars to be used in practical scenarios, we propose an end-to-end framework for synthesizing high-quality virtual human faces capable of speaking with accurate lip motion with a special empha-sis on performance. We introduce a novel network utiliz-ing visemes as an intermediate audio representation and a novel data augmentation strategy employing a hierarchical image synthesis approach that allows disentanglement of the different modalities used to control the global head mo-tion. Our method runs in real-time, and is able to deliver superior results compared to the current state-of-the-art.
With the metaverse gaining traction in the media and the rapid virtualization of many aspects of life, virtual humans are becoming increasingly popular. Propped up by techno-logical advances in hardware, GPU computing, and deep learning, digitization of humans has become a very active research topic.
One challenging problem is creating virtual avatars that attempt to faithfully recreate a specific human indistinguish-able from their real counterparts. This entails a perfect vi-sual recreation, with high quality textures that recreate the minutia that make a human visually realistic, e.g., hair that appears to be composed of individual strands, clothes that appear made out of fabric, and pores or vellus hair on the skin. Recent advances in neural rendering have achieved considerable success in creating very convincing images that are able to visually recreate such fine details. Addi-tionally, many use-cases involving virtual humans require them to appear on large screens, e.g., human-size displays in airport halls, hotels, and billboards. For such use-cases, it is vital for the virtual humans to be rendered in a relatively high resolution.
Unfortunately, rendering quality alone is not sufficient to recreate a convincing virtual human that can be interacted with. Just as important as the visual fidelity is the fidelity of the motion. Specifically, for speech-driven avatars, the
way the avatar talks, opens its mouth, and moves its lips is crucial. The human perception system is very sensitive to even the slightest mistake when it comes to the face, specif-ically the appearance of mouth and lips during speech. In order for a virtual avatar to be believable, its mouth and lip motion must match the speech a human listener hears.
In this work, we introduce a novel training regime that utilizes 1-dimensional audio features such as visemes [9] or wav2vec 2.0 [1]. These are higher-level features com-pared to traditional features such as MFCC [23] which re-quire more complex encoders. This enables us to efficiently synthesize talking heads with high-quality rendering and lip motion. However, this alone is insufficient for creat-ing an avatar suitable for the aforementioned use-cases. In order to have more control over the avatar, it needs to be conditioned on multiple constraints in addition to speech, such as head pose or body shape. With speech represented as a 1-dimensional vector of visemes, head pose repre-sented as rotation angles, and body shape represented as a 2-dimensional outline, we quickly run into a problem with different modalities among our data representation.
Training deep neural networks on multimodal data is a difficult problem, especially when the different modalities are innately correlated. The network then tends to overfit on the modality that is easier to learn from, and this kind of overfitting leads to very poor performance at testing time.
While there exist many techniques to mitigate traditional overfitting such as reducing number of parameters, regular-ization, etc., they are less effective when the overfitting is caused by multiple modalities. To break the correlations between these modalities, we introduce a novel data aug-mentation strategy: for every given lip shape, we use a gen-erative oracle network to synthesize photorealistic images of the same lip shapes in a variety of head poses. This re-quires a generative network capable of producing images that are as high-resolution as our training data to preserve sharpness in important areas such as teeth.
Even with modern hardware and the latest research, gen-erative neural networks still struggle with generating high-resolution images, and the resolution usually comes at the cost of quality. When increasing the resolution and the complexity of what the network is asked to generate, the generative network often loses the semantic quality or other qualitative aspects of the result images. Increasing the net-work’s capacity alone by adding more learnable parameters does not necessarily help and can introduce noise. To allevi-ate this, we propose a hierarchical approach to generating a high-quality synthetic image of a talking face that preserves the quality in the mouth region via high-resolution supervi-sion.
In order to reproduce a believable interaction with a virtual avatar, inference speed is paramount. Our paper presents an efficient framework for creating high-quality virtual artificial humans in real-time where each of the aforementioned concerns is addressed. The contribution of our works is:
• A data augmentation method to disentangle audio and visual modalities so that the whole framework can be trained end-to-end.
• A hierarchical outpainting approach which allows for generation of high-resolution synthetic data.
• An end-to-end framework that utilizes 2-encoder-2-decoder neural network architecture and leverages syn-thetic data. 2.