Abstract
We address the problem of learning person-specific facial priors from a small number (e.g., 20) of portrait photos of the same person. This enables us to edit this specific person’s fa-cial appearance, such as expression and lighting, while pre-serving their identity and high-frequency facial details. Key to our approach, which we dub DiffusionRig, is a diffusion model conditioned on, or “rigged by,” crude 3D face models estimated from single in-the-wild images by an off-the-shelf estimator. On a high level, DiffusionRig learns to map sim-plistic renderings of 3D face models to realistic photos of a given person. Specifically, DiffusionRig is trained in two stages: It first learns generic facial priors from a large-scale face dataset and then person-specific priors from a small portrait photo collection of the person of interest. By learn-ing the CGI-to-photo mapping with such personalized priors,
† Work done during an internship at Adobe.
DiffusionRig can “rig” the lighting, facial expression, head pose, etc. of a portrait photo, conditioned only on coarse 3D models while preserving this person’s identity and other high-frequency characteristics. Qualitative and quantitative experiments show that DiffusionRig outperforms existing approaches in both identity preservation and photorealism.
Please see the project website: https://diffusionrig.github.io for the supplemental material, video, code, and data. 1.

Introduction
It is a longstanding problem in computer vision and graph-ics to photorealistically change the lighting, expression, head pose, etc. of a portrait photo while preserving the person’s identity and high-frequency facial characteristics. The dif-ficulty of this problem stems from its fundamentally under-constrained nature, and prior work typically addresses this with zero-shot learning, where neural networks were trained on a large-scale dataset of different identities and tested on a new identity. These methods ignore the fact that such
generic facial priors often fail to capture the test identity’s high-frequency facial characteristics, and multiple photos of the same person are often readily available in the person’s personal photo albums, e.g., on a mobile phone. In this work, we demonstrate that one can convincingly edit a person’s fa-cial appearance, such as lighting, expression, and head pose, while preserving their identity and other high-frequency fa-cial details. Our key insight is that we can first learn generic facial priors from a large-scale face dataset [19] and then finetune these generic priors into personalized ones using around 20 photos capturing the test identity.
When it comes to facial appearance editing, the natural question is what representation one uses to change lighting, expression, head pose, hairstyle, accessories, etc. Off-the-shelf 3D face estimators such as DECA [9] can already ex-tract, from an in-the-wild image, a parametric 3D face model that comprises parameters for lighting (spherical harmonics), expression, and head pose. However, directly rendering these physical properties back into images yields CGI-looking re-sults, as shown in the output columns of Figure 1. The reasons are at least three-fold: (a) The 3D face shape esti-mated is coarse, with mismatched face contours and misses high-frequency geometric details, (b) the assumptions on reflectance (Lambertian) and lighting (spherical harmonics) are restrictive and insufficient for reproducing the reality, and (c) 3D morphable models (3DMMs) simply cannot model all appearance aspects including hairstyle and accessories.
Nonetheless, such 3DMMs provide us with a useful rep-resentation that is amenable to “appearance rigging” since we can modify the facial expression and head pose by sim-ply changing the 3DMM parameters as well as lighting by varying the spherical harmonics (SH) coefficients.
On the other hand, diffusion models [15] have recently gained popularity as an alternative to Generative Adversarial
Networks (GANs) [11] for image generation. Diff-AE [33] further shows that when trained on the autoencoding task, diffusion models can provide a latent space for appearance editing. In addition, diffusion models are able to map pixel-aligned features (such as noise maps in the vanilla diffusion model) to photorealistic images. Although Diff-AE is capa-ble of interpolating from, e.g., smile to no smile, after seman-tic labels are used to find the direction to move towards, it is unable to perform edits that require 3D understanding and that cannot be expressed by simple binary semantic labels.
Such 3D edits, including relighting and head pose change, are the focus of our work.
To combine the best of both worlds, we propose Diffusion-Rig, a model that allows us to edit or “rig” the appearance (such as lighting and head pose) of a 3DMM and then pro-duce a photorealistic edited image conditioned on our 3D edits. Specifically, DiffusionRig first extracts rough physical properties from single portrait photos using an off-the-shelf method [9], performs desired 3D edits in the 3DMM space, and finally uses a diffusion model [15] to map the edited
“physical buffers” (surface normals, albedo, and Lambertian rendering) to photorealistic images. Since the edited im-ages should preserve the identity and high-frequency facial characteristics, we first train DiffusionRig on the CelebA dataset [27] to learn generic facial priors so that Diffusion-Rig knows how to map surface normals and the Lambertian rendering to a photorealistic image. Note that because the physical buffers are coarse and do not contain sufficient identity information, this “Stage 1 model” provides no guar-antee for identity preservation. At the second stage, we finetune DiffusionRig on a tiny dataset of roughly 20 images of one person of interest, producing a person-specific diffu-sion model mapping physical buffers to photos of just this person. As discussed, there are appearance aspects not mod-eled by the 3DMM, including but not limited to hairstyle and accessories. To provide our model with this additional in-formation, we add an encoder branch that encodes the input image into a global latent code (“global” in contrast to phys-ical buffers that are pixel-aligned with the output image and hence “local”). This code is chosen to be low-dimensional in the hope of capturing just the aspects not modeled by the 3DMM, such as hairstyle and eyeglasses.
In summary, our contributions are:
• A deep learning model for 3D facial appearance editing (that modifies lighting, facial expression, head pose, etc.) trained using just images with no 3D label,
• A method to drive portrait photo generation using diffu-sion models with 3D morphable face models, and
• A two-stage training strategy that learns personalized facial priors on top of generic face priors, enabling edit-ing that preserves identity and high-frequency details. 2.