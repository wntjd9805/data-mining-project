Abstract
In this paper, we efficiently transfer the surpassing rep-resentation power of the vision foundation models, such as
ViT and Swin, for video understanding with only a few train-able parameters. Previous adaptation methods have simul-taneously considered spatial and temporal modeling with a unified learnable module but still suffered from fully lever-aging the representative capabilities of image transformers.
We argue that the popular dual-path (two-stream) architec-ture in video models can mitigate this problem. We pro-pose a novel DUALPATH adaptation separated into spatial and temporal adaptation paths, where a lightweight bottle-neck adapter is employed in each transformer block. Espe-cially for temporal dynamic modeling, we incorporate con-secutive frames into a grid-like frameset to precisely imi-tate vision transformers’ capability that extrapolates rela-tionships between tokens. In addition, we extensively inves-tigate the multiple baselines from a unified perspective in video understanding and compare them with DUALPATH.
Experimental results on four action recognition benchmarks prove that pretrained image transformers with DUALPATH can be effectively generalized beyond the data domain. 1.

Introduction
Recognizing when, where, and what happened is a fun-damental capability in the human cognition system to un-derstand our natural world. The research for video under-standing inspires such capability for machine intelligence to comprehend scenes over time flow. Over the last decade, the development of deep neural networks [10,33,66,73] has contributed towards advances in video understanding.
Vision Transformer (ViT) [17] has recently emerged, making an upheaval in the research field of computer vision.
ViT and its variants [16, 46, 72, 78] have demonstrated re-markable generalizability and transferability of their repre-∗ Equal contributions.
† Corresponding author.
Official code: https://github.com/park-jungin/DualPath
This research was supported by the National Research Founda-tion of Korea (NRF) grant funded by the Korea government (MSIP) (NRF2021R1A2C2006703).
Figure 1. Performance comparison on the Kinetics-400 [38] dataset. We depict the action recognition performance (vertical axis, %) with respect to the number of trainable parameters (hori-zontal axis). The size of circles indicates GFLOPs for inference. sentations with scaled-up foundation models [34, 53, 63, 69, 77, 79] and large-scale web-collected image data (e.g. JFT-3B [79], LAION-5B [58]). To capitalize on well-trained vi-sual foundation models, finetuning entire parameters of the pretrained models with task-specific objectives has been the most popular transfer technique. However, it requires high-quality training data and plenty of computational resources to update the whole parameters for each downstream task, making overwhelming efforts for training. While partial finetuning [28], which trains additional multilayer percep-tron (MLP) layers to the top of the model, has also been widely used for affordable training costs, unsatisfactory per-formance has been pointed out as a problem.
Most transfer recently, parameter-efficient learning (PETL) methods [25, 26, 31, 32, 41, 61] have been proposed as an alternative to finetuning in the natural language pro-cessing area to adapt the large-scale language model, such as GPT series [8, 54, 55] and T5 [56], for each task. They have successfully attained comparable or even surpassing performance to full-tuning parameters by learning a small number of extra trainable parameters only while keeping the original parameters of the pretrained model frozen. Thanks to their effectiveness and simplicity, they have been ex-tended to vision models by applying prompt-based meth-ods [3, 35] and adapter-based methods [11, 49, 64]. They have efficiently adapted pretrained models to downstream tasks with significantly reduced tuning parameters, but most of these works mainly focus on transferring image models to image tasks [3, 11, 35, 49] and vision-language models to vision-language tasks [64]. Inspired by the advances of the prior arts, we raise two conceivable questions: (1) Is it possible to transfer the parameters of the image foundation model to another video domain? (2) Is it also possible the transferred model performs comparably to the carefully de-signed video models that take the spatiotemporal nature of the video into account?
While image models have demonstrated strong spatial context modeling capabilities [34, 53, 63, 79], video trans-former models [1, 47, 57, 76] require a more complex archi-tecture (e.g. 539 vs 48912 GFLOPs [57]) with a large num-ber of parameters (e.g. 84M vs 876M parameters [76]) than
ViT for temporal context reasoning. Therefore, the chal-lenge in transferring image models for video understanding is to encode the temporal context of videos while leverag-ing the discriminative spatial context of the pretrained im-age models. A naive solution is to finetune image models on a video dataset by directly applying previous prompt-/adapter-based approaches [3, 11, 35, 49]. However, these approaches inevitably ignore the temporal context in videos because they bridge only the spatial contexts between image and video data.
In this paper, we propose a novel adapter-based dual-path parameter efficient tuning method for video under-standing, namely DUALPATH, which consists of two dis-tinct paths (spatial path and temporal path). For both paths, we freeze the pretrained image model and train only ad-ditional bottleneck adapters for tuning. The spatial path is designed to encode the spatial contexts that can be in-ferred from the appearance of individual frames with the minimum tuning of the pretrained image model. To re-duce the computation burden, we sparsely use the frames with a low frame rate in the spatial path. The tempo-ral path corresponds to the temporal context that should be encoded by grasping the dynamic relationship over sev-eral frames sampled with a high frame rate. Especially for two reasons, we construct a grid-like frameset that con-sists of consecutive low-resolution frames as an input of the temporal path: (i) preventing computational efficiency loss caused by calculating multiple frames simultaneously; (ii) precisely imitating the ViT’s ability for extrapolating global dependencies between input tokens. To compare our DUALPATH with existing methods broadly, we imple-ment several baselines with a unified perspective on re-cent domain-specific PETL approaches [11, 35, 49]. Ex-tensive experiments on several action recognition bench-marks [23, 38, 39, 42] demonstrate the effectiveness and high efficiency of our DUALPATH, achieving comparable and even better performance than the baselines and prior video models [1, 7, 18, 22, 40, 43, 47, 57, 68, 76]. We achieve these results with extremely low computational costs for both training and inference, as demonstrated in Fig. 1. 2.