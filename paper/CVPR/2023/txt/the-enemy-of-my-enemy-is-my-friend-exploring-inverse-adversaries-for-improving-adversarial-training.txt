Abstract
Although current deep learning techniques have yielded superior performance on various computer vision tasks, yet they are still vulnerable to adversarial examples. Adversar-ial training and its variants have been shown to be the most effective approaches to defend against adversarial exam-ples. A particular class of these methods regularize the dif-ference between output probabilities for an adversarial and its corresponding natural example. However, it may have a negative impact if a natural example is misclassified. To circumvent this issue, we propose a novel adversarial train-ing scheme that encourages the model to produce similar output probabilities for an adversarial example and its “in-verse adversarial” counterpart. Particularly, the counter-part is generated by maximizing the likelihood in the neigh-borhood of the natural example. Extensive experiments on various vision datasets and architectures demonstrate that our training method achieves state-of-the-art robustness as well as natural accuracy among robust models. Further-more, using a universal version of inverse adversarial ex-amples, we improve the performance of single-step adver-sarial training techniques at a low computational cost. 1.

Introduction
Deep learning has achieved revolutionary progress in nu-merous computer vision tasks [24, 40, 55] and has emerged as a promising technique for fundamental research in mul-tiple disciplines [31, 35, 52]. However, a well-established study has demonstrated that Deep Neural Networks (DNNs) are extremely vulnerable to adversarial examples [42], which are indistinguishable from natural examples in hu-In other words, a visually undetectable per-man vision. turbation to the original example can lead to a significant
*Corresponding Author (a) Natural Training (b) Adversarial Training [25]
Figure 1. Average accuracy under different attack strengths for two networks trained on natural and adversarial samples. We rank test examples based on the cross-entropy loss value in increasing order and divide them into two equal halves. Note that the negative
ϵ denotes the strength of inverse adversarial perturbation. (a) Nat-urally trained models are extremely susceptible to perturbations. (b) For adversarially trained models, the adversarial effect is ex-acerbated on examples that are more possibly to be misclassified.
The green line corresponds to natural examples. disruption of the inference result of DNNs. The impercep-tibility of these tailored examples also makes them easy to bypass manual verification [3, 15], posing a potential secu-rity threat to the safety of deep learning-based applications.
Various defense methods have been proposed to improve adversarial robustness of DNNs [21,46,48]. As the primary defense method, adversarial training [10, 25, 42] improves intrinsic network robustness via adaptively augmenting ad-versarial examples into training examples. State-of-the-art adversarial training methods mainly focus on the distribu-tion alignment between natural and adversarial examples to preserve the consistency of the DNN prediction [7, 44, 53].
However, there still exists an undesirable decrease in the standard accuracy for adversarially trained models due to limited data and restricted model capacity. The misclassifi-cation of natural examples can further undermine the distri-bution alignment during adversarial training.
The natural intuition is that: adversarial examples corre-sponding to misclassified natural examples are more likely to be misclassified. In other words, adversarial examples exhibit higher loss values compared to their corresponding natural examples. Contrary to adversaries that are harm-ful to DNNs, we introduce inverse adversarial examples1 that are created via minimizing the objective function as an inverse procedure of adversary generation. Specifically, in-verse adversarial examples are beneficial to DNNs, which can be more possibly to be correctly classified. To sup-port this claim, we study the accuracy of trained classifi-cation models on two groups of samples (see Figure 1). We present the accuracy of adversarial examples and their in-verse counterparts under different attack strengths. Even a small adversarial perturbation can induce a drastic accu-racy decrease for the naturally trained model. For the ad-versarially trained model, the robust accuracy of examples with higher loss values (Bottom 50%) suffers from a heavier drop than that of examples with lower loss values (Top 50%) under larger attack strengths. This indicates that the adver-sarial counterparts of low-confidence or even misclassified examples are also misclassified. Therefore, the distribution alignment [7, 44, 53] between two misclassified examples might have an unnecessary or even harmful effect on the adversarial robustness establishment.
In this paper, to mitigate the unnecessary or even harm-ful matching manner between misclassified examples, we propose a novel adversarial training framework based on an inverse version of adversarial examples, dubbed Inverse Ad-versarial Training (IAT), which implicitly bridges the dis-tribution gap between adversarial examples and the high-likelihood region of their belonging classes. Adversarial ex-amples of a certain category can thus be pulled closer to the high-likelihood region instead of their original examples.
Specifically, we propose an inverse procedure of the stan-dard adversary generation to reach the high-likelihood re-gion. The generated inverse adversaries can also be viewed as the rectification of original examples for reducing pre-diction errors. Considering the multi-class decision surface and computational cost, we further design a class-specific inverse adversary generation paradigm as opposed to the instance-wise version. Furthermore, we establish a momen-tum mechanism for the prediction of inverse adversaries to stabilize the training process. A one-off version of our in-verse adversarial training is also proposed for improving time efficiency.
Comprehensive experiments demonstrate the superiority of our method in comparison with state-of-the-art adversar-ial training approaches. We also show that our method can be adapted to larger models with extra generated data for robustness enhancement. Besides, the robustness of single-step adversarial training methods can be further improved at a low cost by incorporating our method. 1The formal definition will be given in the following sections.
The main contribution of this paper can be summarized as follows:
• By analyzing the unnecessary, or even harmful, align-ment between misclassified examples, we propose a novel adversarial training framework based on the in-verse version of adversarial examples, which promotes the aggregation of adversarial examples to the high-likelihood region of their belonging classes.
• Based on our Inverse Adversarial Training (IAT) paradigm, we design a class-specific universal inverse adversary generation method to mitigate the individ-ual bias of different examples with high efficiency.
We also propose a one-off strategy to reduce compu-tational costs with a negligible performance loss.
• Extensive experiments demonstrate the effectiveness and generalizability of our method compared to state-of-the-art adversarial training methods. Our method can also be combined with single-step adversarial training methods as a plug-and-play component for boosting robustness at a low cost.