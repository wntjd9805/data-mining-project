Abstract
Recent work has shown that deep vision models tend to be overly dependent on low-level or “texture” features, leading to poor generalization. Various data augmentation strategies have been proposed to overcome this so-called texture bias in DNNs. We propose a simple, lightweight adversarial augmentation technique that explicitly incen-tivizes the network to learn holistic shapes for accurate prediction in an object classification setting. Our augmen-tations superpose edgemaps from one image onto another image with shuffled patches, using a randomly determined mixing proportion, with the image label of the edgemap im-age. To classify these augmented images, the model needs to not only detect and focus on edges but distinguish between relevant and spurious edges. We show that our augmenta-tions significantly improve classification accuracy and ro-bustness measures on a range of datasets and neural ar-chitectures. As an example, for ViT-S, We obtain absolute gains on classification accuracy gains up to 6%. We also obtain gains of up to 28% and 8.5% on natural adversarial and out-of-distribution datasets like ImageNet-A (for ViT-B) and ImageNet-R (for ViT-S), respectively. Analysis us-ing a range of probe datasets shows substantially increased shape sensitivity in our trained models, explaining the ob-served improvement in robustness and classification accu-racy. 1.

Introduction
A growing body of research catalogues and analyzes ap-parent failure modes of deep vision models. For instance, work on texture bias [1,7,12] suggests that image classifiers are overdependent on textural cues and fail against simple (adversarial) texture substitutions. Relatedly, the idea of simplicity bias [25] captures the tendency of deep models to use weakly predictive “simple” features such as color or texture, even in the presence of strongly predictive com-plex features. In psychology & neuroscience, too, evidence
*Work done at Google Research India.
Figure 1. Comparison of the models on robustness and shape-bias. The shape factor gives the fraction of dimensions that en-code shape cues [17]. Backbone(T) denotes texture shape debiased (TSD) models [21].
In comparison, ELEAS denoted by Back-bone(E) is more shape biased and shows better performance on
ImageNet-C and ImageNet-A datasets. suggests that deep networks focus more on “local” features rather than global features and differ from human behav-ior in related tasks [19]. More broadly speaking, there is a mismatch between the cognitive concepts and associated world knowledge implied by the category labels in image datasets such as Imagenet and the actual information con-tent made available to a model via one-hot vectors encod-ing these labels. In the face of under-determined learning problems, we need to introduce inductive biases to guide
Figure 2. Representative performance comparison of our model, with the ‘Debiased’ models (TSD [21]) on ImageNet-A, R, C, and
Sketch datasets. The models trained using ELEAS show improved performance on out-of-distribution robustness datasets. The large performance improvement on the ImageNet-A dataset indicates better robustness to natural adversarial examples. the learning process. To this end, Geirhos et al. [7] pro-posed a data augmentation method wherein the texture of an image was replaced with that of a painting through styl-ization. Follow-on work improved upon this approach by replacing textures from other objects (instead of paintings) and teaching the model to separately label the outer shape and the substituted texture according to their source image categories [21]. Both these approaches discourage overde-pendence on textural features in the learned model; how-ever, they do not explicitly incentivize shape recognition.
We propose a lightweight adversarial augmentation technique ELEAS (Edge Learning for Shape sensitivity) that is designed to increase shape sensitivity in vision mod-els. Specifically, we augment a dataset with superposi-tions of random pairs of images from the dataset, where one image is processed to produce an edge map, and the other is modified by shuffling the location of image patches within the image. The two images are superposed us-ing a randomly sampled relative mixing weight (similar to
Mixup [30]), and the new superposed image is assigned the label of the edgemap image. ELEAS is designed to specif-ically incentivize not only edge detection but shape sensi-tivity: 1) classifying the edgemap image requires the model to extract and exploit edges – the only features available in the image, 2) distinguishing the edgemap object cate-gory from the superposed shuffled image requires the model to distinguish the overall edgemap object shape (relevant edges) from the shuffled image edges (irrelevant edges, less likely to be “shape like”). We perform extensive experi-ments over a range of model architectures, image classi-fication datasets, and probe datasets, comparing ELEAS against recent baselines. Figure 1 provides a small visual sample of our findings and results; across various models, a measure of shape sensitivity [17] correlates very strongly with measures of classifier robustness [11, 12] (see Results for more details), validating the shape sensitivity inductive bias. In addition, for a number of model architectures, mod-els trained with ELEAS significantly improve both mea-sures compared to the previous SOTA data augmentation approach [21].
Summing up, we make the following contributions:
• We propose an adversarial augmentation technique,
ELEAS, designed to incentivize shape sensitivity in vision models. Our augmentation technique is lightweight (needing only an off-the-shelf edge detec-tion method) compared to previous proposals that re-quire expensive GAN-based image synthesis [7, 21].
• In experiments, ELEAS shows increased shape sen-sitivity on a wide range of tasks designed to probe this property. Consequently, we obtain increased ac-curacy in object classification with a 6% improvement on ImageNet-1K classification accuracy for ViT-Small among others.
• ELEAS shows high generalizability and out of dis-tribution robustness with 14.2% improvement in
ImageNet-C classification performance and 5.89% in-crease in shape-bias for Resnet152. 2.