Abstract
Deep Neural Networks (DNNs) are rather restrictive in long-tailed data, since they commonly exhibit an under-representation for minority classes. Various remedies have been proposed to tackle this problem from different perspec-tives, but they ignore the impact of the density of Back-bone Features (BFs) on this issue. Through representation learning, DNNs can map BFs into dense clusters in fea-ture space, while the features of minority classes often show sparse clusters. In practical applications, these features are discretely mapped or even cross the decision boundary re-sulting in misclassification. Inspired by this observation, we propose a simple and generic method, namely Feature Clus-ters Compression (FCC), to increase the density of BFs by compressing backbone feature clusters. The proposed FCC can be easily achieved by only multiplying original BFs by a scaling factor in training phase, which establishes a lin-ear compression relationship between the original and mul-tiplied features, and forces DNNs to map the former into denser clusters. In test phase, we directly feed original fea-tures without multiplying the factor to the classifier, such that BFs of test samples are mapped closer together and do not easily cross the decision boundary. Meanwhile, FCC can be friendly combined with existing long-tailed methods and further boost them. We apply FCC to numerous state-of-the-art methods and evaluate them on widely used long-tailed benchmark datasets. Extensive experiments fully ver-ify the effectiveness and generality of our method. Code is available at https://github.com/lijian16/FCC. 1.

Introduction
Recently, Deep Neural Networks (DNNs) have achieved considerable success in a variety of visual tasks, such as ob-ject detection [5,12] and visual recognition [18,38]. Despite with ingenious networks and powerful learning capabilities, the great success is inseparable from large-scale balanced
*Corresponding author
Figure 1. (a) DNNs can map backbone features into different clus-ters, while minority classes are mapped into sparse clusters com-pared to majority classes. The sparsity causes boundary points mapped far from their clusters or even cross the boundary. (b) FCC can compress original features compared with multiplied features, which makes these features are mapped closer together. Because the decision boundary remains unchanged in test phase, boundary points will be brought back within the boundary. datasets, such as ImageNet [8]. However, datasets collected from real-world scenarios normally follow imbalanced and long-tailed distributions, in which few categories (majority classes) occupy most of the data while many categories (mi-nority classes) are under-represented [15]. DNNs trained on such datasets commonly exhibit a bias towards over-represented majority classes and produce low recognition accuracy for minority classes [19, 24, 33].
A number of remedies have been proposed to deal with this problem from different perspectives. For instance, re-sampling methods aim to balance the data distribution by designing different sampling strategies [13, 23]. Re-weighting methods attempt to alleviate the dominance of majority classes by assigning different weights to each class
[1, 3, 21]. Two-stage training methods divide the vanilla training procedure into imbalanced learning and balanced fine-tuning [11, 19]. More recently, multi-expert networks have been proposed to employ multiple models to learn rep-resentation from different aspects [32, 37]. These methods show the outstanding performance on long-tailed recogni-tion, but they ignore the impact of the density of Backbone
Features (BFs) on this issue.
DNNs primarily consist of the backbone networks and classifiers [10, 18]. The former, similar to unsupervised clustering algorithms [9, 20], can map BFs into different clusters by linear and non-linear operations (e.g., convolu-tion kernel and activation functions). The latter is similar to SVM [2], which can draw the decision boundary based on these clusters for recognition [7, 14]. DNNs trained on long-tailed datasets tend to map BFs of majority classes into dense clusters, but those of minority classes are mapped into sparse clusters, in which feature points are far from each other [41, 42]. In practical application, due to the sparsity,
BFs of real samples cannot be mapped close enough to each other and are scattered in feature space, such that these fea-ture points, especially boundary points that located at the margin of clusters, easily cross the decision boundary re-sulting in poor performance, as shown in Fig. 1a.
In view of this observation, we propose a Feature Clus-ters Compression (FCC) to increase the density of BFs by compressing backbone feature clusters. Our method can be easily achieved by only multiplying original BFs by a spe-cific scaling factor τ (τ > 1) and feeding the multiplied features to the classifier for training, which will establish a linear compression relationship among the original and multiplied features. As the multiplied features are mapped into clusters in training process, this relationship will force the backbone to map the original features into denser clus-ters, thereby improving the density. In test phase, we di-rectly feed the original BFs without multiplying the factor
τ to the trained classifier, these features are mapped closer together, and because the decision boundary remains con-sistent with training phase, boundary points will be brought back within the decision boundary resulting in performance improvement, as shown in Fig. 1b. We notice that the input of the classifier is different in training and test phases, and this problem will be discussed in Sec. 3.2.
As reported by [38], different long-tailed methods might hurt each other when they are employed inappropriately.
For instance, applying re-sampling and re-weighting meth-ods simultaneously might obtain similar or even worse ac-curacy than using them alone since they both try to enlarge the influence of minority classes. But our FCC is a generic method which only focuses on optimizing BFs without con-flicting with other modules (e.g., sampling strategies and loss functions), and it can be friendly combined with exist-ing long-tailed methods and further boost them. Our contri-butions can be summarized as follows:
• We tackle long-tailed visual recognition from a novel perspective of increasing the density of BFs, which makes features mapped into denser clusters and bound-ary points brought back within the decision boundary.
• We propose a Feature Clusters Compression (FCC) to improve the density of BFs, and it can be easily achieved and friendly combined with existing long-tailed methods to boost them.
• Extensive experiments demonstrate that FCC applied to existing methods achieves significant performance improvement and state-of-the-art results on four popu-lar datasets, including CIFAR-10-LT, CIFAR-100-LT,
ImageNet-LT and iNaturalist 2018. 2.