Abstract
Recent transformer-based solutions have shown great success in 3D human pose estimation. Nevertheless, to cal-culate the joint-to-joint affinity matrix, the computational cost has a quadratic growth with the increasing number of joints. Such drawback becomes even worse especially for pose estimation in a video sequence, which necessitates spatio-temporal correlation spanning over the entire video.
In this paper, we facilitate the issue by decomposing cor-relation learning into space and time, and present a novel
Spatio-Temporal Criss-cross attention (STC) block. Tech-nically, STC first slices its input feature into two partitions evenly along the channel dimension, followed by perform-ing spatial and temporal attention respectively on each par-tition. STC then models the interactions between joints in an identical frame and joints in an identical trajectory simulta-neously by concatenating the outputs from attention layers.
On this basis, we devise STCFormer by stacking multiple
STC blocks and further integrate a new Structure-enhanced
Positional Embedding (SPE) into STCFormer to take the structure of human body into consideration. The embedding function consists of two components: spatio-temporal con-volution around neighboring joints to capture local struc-ture, and part-aware embedding to indicate which part each joint belongs to. Extensive experiments are conducted on
Human3.6M and MPI-INF-3DHP benchmarks, and supe-rior results are reported when comparing to the state-of-the-art approaches. More remarkably, STCFormer achieves to-date the best published performance: 40.5mm P1 error on the challenging Human3.6M dataset. 1.

Introduction 3D human pose estimation has attracted intensive re-search attention in CV community due to its great poten-*This work is supported by the National Natural Science Foundation of
China under Grants 61932009.
Figure 1. Modeling spatio-temporal correlation for 3D human pose estimation by (a) utilizing spatio-temporal attention on all joints in the entire video, (b) separating the framework into two steps that respectively capture spatial and temporal context, and (c) our Spatio-Temporal Criss-cross attention (STC), i.e., a two-pathway block that models spatial and temporal information in parallel. In the visualization of receptive field, the covered joints of each attention strategy is marked as red nodes. tial in numerous applications such as human-robot inter-action [20, 43], virtual reality [11] and motion prediction
[27, 28]. The typical monocular solution is a two-stage pipeline, which first extracts 2D keypoints by 2D human pose detectors (e.g., [7] and [41]), and then lifts 2D coordi-nates into 3D space [31]. Despite its simplicity, the second stage is an ill-posed problem which lacks the depth prior, and suffers from the ambiguity problem.
To mitigate this issue, several progresses propose to ag-gregate the temporal cues in a video sequence to promote pose estimation by grid convolutions [15,26,35], graph con-volutions [4, 47] and multi-layer perceptrons [6, 21]. Re-cently, Transformer structure has emerged as a dominant ar-chitecture in both NLP and CV fields [8,24,45,49], and also demonstrated high capability in modeling spatio-temporal correlation for 3D human pose estimation [13, 22, 23, 25, 48, 52, 54]. Figure 1(a) illustrates a straightforward way to exploit the transformer architecture for directly learning spatio-temporal correlation between all joints in the entire video sequence. However, the computational cost of calcu-lating the joint-to-joint affinity matrix in the self-attention has a quadratic growth along the increase of number of frames, making such solution unpractical for model train-ing. As a result, most transformer structures employ a two-step alternative, as shown in Figure 1(b), which encodes spatial information for each frame first and then aggregates the feature sequence by temporal transformer. Note that we take spatial transformer as the frame encoder as an example in the figure. This strategy basically mines the correlation across frame-level features but seldom explores the relation between joints across different frames.
In this paper, we propose a novel two-pathway attention mechanism, namely Spatio-Temporal Criss-cross attention (STC), that models spatial and temporal information in par-allel, as depicted in Figure 1(c). Concretely, STC first slices the input joint features into two partitions evenly with re-spect to the channel dimension. On each partition, a Multi-head Self-Attention (MSA) is implemented to encapsulate the context along space or time axis. In between, the space pathway computes the affinity between joints in each frame independently, and the time pathway correlates the identi-cal joint moving across different frames, i.e., the trajectory.
Then, STC recombines the learnt contexts from two path-ways, and mixes the information across channels by Multi-Layer Perceptrons (MLP). By doing so, the receptive field is like a criss cross of spatial and temporal axes, and the com-putational cost is O(T 2S) + O(T S2). That is much lower than O(T 2S2) of fully spatio-temporal attention, where T and S denote the number of frames and joints, respectively.
By stacking multiple STC blocks, we devise a new ar-chitecture â€” STCFormer for 3D human pose estimation.
Furthermore, we delve into the crucial design of positional embedding in STCFormer in the context of pose estimation.
The observations that joints in the same body part are either highly relevant (static part) or not relevant but containing moving patterns (dynamic part) motivate us to design a new
Structure-enhanced Positional Embedding (SPE). SPE con-sists of two embedding functions for the static and dynamic part, respectively. A part-aware embedding is to describe the static part by indicating which part each joint belongs to, and a spatio-temporal convolution around neighboring joints aims to capture dynamic structure in local window.
We summarize the main contributions of this work as follows. First, STC is a new type of decomposed spatio-temporal attention for 3D human pose estimation in an eco-nomic and effective way. Second, STCFormer is a novel transformer architecture by stacking multiple STC blocks and integrating the structure-enhanced positional embed-ding. Extensive experiments conducted on Human3.6M and
MPI-INF-3DHP datasets demonstrate that STCFormer with much less parameters achieves superior performances than the state-of-the-art techniques. 2.