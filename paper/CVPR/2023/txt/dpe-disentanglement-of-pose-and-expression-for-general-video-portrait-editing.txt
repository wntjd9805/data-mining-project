Abstract
One-shot video-driven talking face generation aims at producing a synthetic talking video by transferring the facial motion from a video to an arbitrary portrait image. Head pose and facial expression are always entangled in facial motion and transferred simultaneously. However, the entan-glement sets up a barrier for these methods to be used in video portrait editing directly, where it may require to modify the expression only while maintaining the pose unchanged.
One challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expres-*Corresponding Authors. sions. Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. But 3DMMs are not accurate enough to capture facial details due to the limited number of Blend-shapes, which has side effects on motion transfer. In this paper, we introduce a novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data, which consists of a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. The two generators render the
modified latent codes to images, respectively. Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints. Eval-uations demonstrate our method can control pose or expres-sion independently and be used for general video editing.
Code: https://github.com/Carlyx/DPE 1.

Introduction
Talking face generation has seen tremendous progress in visual quality and accuracy over recent years. Literature can be categorized into two groups, i.e., audio-driven [23] and video-driven [16]. The former focuses on animating an unseen portrait image or video with a given audio. The latter aims at animating with a given video. Talking face generation has a variety of meaningful applications, such as digital human animation, film dubbing, etc. In this work, we target video-driven talking face generation.
Recently, most methods [16, 26, 36, 39, 44] endeavor to drive a still portrait image with a video from different per-spectives, i.e., one-shot talking face generation. But only a few [19, 21, 30] make effort to reenact the portrait in a video with another talking video, i.e., video portrait editing. This is a more challenging task because edited faces are required to paste back to the original video and temporal dynamics need to be maintained. Several methods [19, 28] provide personal-ized solutions to this challenge by training a model on the videos of a specific person only. However, the learned model cannot generalize to other identities as the personalized train-ing heavily overfits the facial motion of the specific person and the background. For general video portrait editing, there-fore, resorting to the generalization property of one-shot talking face generation might be a feasible solution.
One-shot methods can transfer facial motion from a driv-ing face to a source one, resulting in that the edited face mimics the head pose and facial expression* of the driving one. The facial motion consists of entangled pose motion and expression motion, which are always transferred simul-taneously in previous methods. However, the entanglement makes those methods unable to transfer pose or expression independently. Since the input to the processing network is always the cropped face rather than the full original image, if the pose is modified along with the expression, the paste-back operation can cause noticeable artifacts around the crop boundary, e.g., twisted neck and inconsistent background.
Consequently, most one-shot methods face this obstacle pre-venting their application to general video portrait editing.
One challenge to disentangle pose and expression is the lack of paired data, such as the same pose but different ex-pressions, or vice versa. In the literature, there are only a few exceptions that can get rid of this limitation, e.g., PIRen-*Note that facial expression here differs from emotion. derer [25] and StyleHEAT [41], which are based on 3D
Morphable Models (3DMMs) [3], a predefined parametric representation that decomposes expression, pose, and iden-tity. However, the 3DMM-based methods heavily depend on the decoupling accuracy of the 3DMM parameters, which is far from satisfactory to reconstruct facial details due to the limited number of Blendshapes. Besides, optimization-based 3DMM parameter estimation is not efficient while learning-based estimation will introduce more errors.
In this work, we propose a novel self-supervised dis-entanglement framework to decouple pose and expression, breaking through the limitation of paired data without us-ing 3DMMs. Our framework has a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where coupled pose and expression motion in a latent code can be disentangled by a network. Then, pose or expression transfer can be per-formed by directly adding the latent code of a source face with the disentangled pose or expression motion code of a driving face. Finally, the two generators render modified latent codes to images. More importantly, to accomplish the disentanglement without paired data, we introduce a bidirec-tional cyclic training method with well-designed constraints.
Specifically, given a source face S and a driving one D, we transfer the expression and pose from D to S sequentially, resulting in two synthetic faces, S′ and S′′. Since there is no paired data, no supervision is provided for S′. To tackle the missing supervision, we exchange the role of S and D to transfer the pose and expression motion from S to D, resulting in D′ and D′′. The distance between D′ and S′ is one constraint for learning. However, it is still not enough for disentangling pose and expression. Then, we discover another core constraint, i.e., face reconstruction. When S and D are the same, S′ and D′ are exactly the same as S and
D, respectively. More analyses will be presented in Sec. 3.
Our main contributions are three-fold:
• We propose a self-supervised disentanglement frame-work to decouple pose and expression for independent motion transfer, without using 3DMMs and paired data.
• We propose a bidirectional cyclic training strategy with well-designed constraints to achieve the disentangle-ment of pose and expression.
• Extensive experiments demonstrate that our method can control pose or expression independently, and can be used for general video editing. 2.