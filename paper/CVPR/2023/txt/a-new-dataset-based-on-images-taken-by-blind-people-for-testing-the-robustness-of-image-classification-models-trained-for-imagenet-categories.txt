Abstract
Our goal is to improve upon the status quo for design-ing image classification models trained in one domain that perform well on images from another domain. Comple-menting existing work in robustness testing, we introduce the first dataset for this purpose which comes from an au-thentic use case where photographers wanted to learn about the content in their images. We built a new test set us-ing 8,900 images taken by people who are blind for which we collected metadata to indicate the presence versus ab-sence of 200 ImageNet object categories. We call this dataset VizWiz-Classification. We characterize this dataset and how it compares to the mainstream datasets for evalu-ating how well ImageNet-trained classification models gen-eralize. Finally, we analyze the performance of 100 Ima-geNet classification models on our new test dataset. Our fine-grained analysis demonstrates that these models strug-gle on images with quality issues. To enable future exten-sions to this work, we share our new dataset with evalua-tion server at: https://vizwiz.org/tasks-and-datasets/image-classification. 1.

Introduction
A common approach for designing computer vision so-lutions is to leverage large-scale datasets to train algorithms.
Yet, for many real-world applications, it is not only ineffi-cient to curate such training datasets but also challenging or infeasible. To address this problem, robustness testing was recently introduced with the focus of improving the perfor-mance of models trained for one domain on a test set in a different domain. In this paper, we focus on robustness test-ing for the image classification problem.
To date, progress with classification robustness testing has been possibly largely because of numerous publicly-available test datasets with distribution shifts from the orig-inal domain. While such datasets have been beneficial in catalyzing progress, they are limited in that they originate from contrived settings. For example, ImageNet-C [15] consists of real images with synthetically generated corrup-tions to assess model robustness for corrupted images. Yet, as shown in prior work [3], images curated from contrived settings can lack the diversity of challenges that emerge in real-world applications. A consequence of this lack of di-versity in test datasets is that algorithm developers do not receive feedback about whether their methods generalize to the range of plausible real-world vision challenges.
We address the above gap for robustness testing by intro-ducing a new test set for image classification. It consists of 8,900 images taken by people who are blind who were au-thentically trying to learn about images they took with their mobile phone cameras. For each image, we asked crowd-workers to indicate which from 200 object categories were present. We call the resulting dataset VizWiz-Classification.
Examples demonstrating how labelled images in our new dataset compare to those in a related robustness testing dataset are shown in Figure 1. We next analyze how our dataset compares to six existing robustness testing datasets and benchmark the performance of 100 modern image clas-sification models on this dataset to highlight challenges and opportunities that emerge for the research community.
Success on our new dataset could benefit real-world ap-plications today. Already, a growing number of blind people are sharing their images with services such as Microsoft’s
Seeing AI, Google’s Lookout, and TapTapSee, which rec-ognize a small number of object categories. Success could broaden such benefits to a longer tail of categories includ-ing those underrepresented in the developing world where it can be laborious/infeasible to collect large, labeled datasets, especially from such a specific population as people who are blind. More generally, our new dataset challenge will encourage developing algorithms that handle a larger diver-sity of real-world challenges. This could benefit applica-tions with similar challenges such as robotics and wearable lifelogging. Finally, image classification is a precursor for many downstream tasks and so we expect progress on our dataset to enable progress on downstream tasks such as ob-ject detection, segmentation, and tracking.
Figure 1. Example labelled images from our new VizWiz-Classification dataset, ImageNet [7], and ImageNet-C [15], where each has the label of “Table lamp”. When comparing our dataset to these prior works, (1) our images were taken by blind people who wanted to learn about their environment, whereas ImageNet images were collected from the Internet and ImageNet-C images consist of ImageNet images that were synthetically corrupted and (2) our images can have multiple labels (e.g. also includes a “Lampshade”), while ImageNet and
ImageNet-C permit only a single label, which can lead to issues for prediction models when multiple categories are present in an image. 2.