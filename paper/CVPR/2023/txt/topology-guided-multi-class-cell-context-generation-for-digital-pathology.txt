Abstract
In digital pathology, the spatial context of cells is impor-tant for cell classification, cancer diagnosis and prognosis.
To model such complex cell context, however, is challeng-ing. Cells form different mixtures, lineages, clusters and holes. To model such structural patterns in a learnable fashion, we introduce several mathematical tools from spa-tial statistics and topological data analysis. We incorporate such structural descriptors into a deep generative model as both conditional inputs and a differentiable loss. This way, we are able to generate high quality multi-class cell layouts for the first time. We show that the topology-rich cell layouts can be used for data augmentation and improve the perfor-mance of downstream tasks such as cell classification. 1.

Introduction
Deep learning has advanced our learning ability in digital pathology. Deep-learning-based methods have achieved im-pressive performance in various tasks including but not lim-ited to: cell detection and classification [2,23,24,52], nuclei instance segmentation [8, 18, 19, 21, 26, 32–34, 42, 51], sur-vival prediction and patient outcome [1,28,30,49], interpre-tation of multiplex immunohistochemistry and immunoflu-orescence imagery [3, 14–16] and many others.
Despite the rapid progress in recent years, pathology image analysis is still suffering from limited observations.
The available annotated images are still scarce relative to the highly heterogeneous and complex tumor microenviron-ment driven by numerous biological factors. The limitation in training data constraints a learning algorithm’s predic-tion power. To this end, one solution is to train generative models that can generate realistic pathology images to aug-ment existing data. Generative models have been proposed to help learning methods in various tasks such as nuclei seg-mentation [7, 21], survival prediction [44] and cancer grade prediction [47].
Generating pathology images usually involves two steps: (1) generating spatial layout of cells and (2) filling in stains and textures inside and outside cell nuclei masks. Most ex-Figure 1. Overview of our multi-class cell context generator. isting methods only focus on the second step. They either generate random cell positions [21] or directly copy nuclei masks from existing images [17]. These methods miss the opportunity to learn the rich cell spatial context carrying critical information about cancer biology.
Spatial context includes how different types of cells (tu-mor, lymphocyte, stromal, etc) are distributed around each other, as well as how they form different structural patterns such as clusters, holes and lineages. Plenty of evidence have demonstrated the importance of spatial context in can-cer diagnosis and prognosis [31, 53]. One good example is the clinical significance of tumor infiltrating lymphocytes (TILs), i.e., lymphocytes residing within the border of in-vasive tumors [37–40]. The spatial distribution of stromal cells in the vicinity of tumor has been shown to be directly related to cancer outcomes [35,53]. Tumor budding, i.e., the presence of isolated or small clusters of tumor cells at the invasive tumor front, is a prognosis biomarker associated with an increased risk of lymph node metastasis in colorec-tal carcinoma and other solid malignancies [29]. In prostate cancer tissue samples, plenty of loopy cellular structures are formed corresponding to glands. Their integrity, known as the Gleason score, is a good indicator of cancer progres-sion [48].
Figure 2. Sample results from our cell layout generator. Our generated samples have similar spatial characteristics as the corresponding reference layouts.
Given the biological significance of cell spatial context, we hypothesize that being able to model and generate cell configurations will benefit various downstream tasks. To model the complex cell spatial context, the main challenge is the limited information one can rely on –coordinates and types of cells. This makes it hard for even powerful deep learning methods [27] to learn the underlying distribution.
To better model the spatial context, we argue that princi-pled mathematical machinery has to be incorporated into the deep learning framework. Formally, we introduce the classic K-function from spatial statistics [5], as well as the theory of persistent homology [13], to model the spatial dis-tribution of multi-class cells and their structural patterns.
These mathematical constructs have been shown to corre-late with clinical outcomes [4]. However, they have not been used in the generation of pathology images.
We incorporate these spatial topological descriptors into a deep generative model. Our generative model takes an in-put pathology image and generates a new cell layout with similar spatial and topological characteristics. To enforce the expected spatial characteristics, we propose a novel cell configuration loss based on the persistent homology and spatial statistics of input cell spatial configuration. The loss compares the generated and the reference cell configura-tions and match their topology in view of a topological mea-sure called persistence diagram. The loss enforces holes in the generated cell configuration to be one-to-one matched to holes in the reference cell configuration, i.e., having similar shapes and density.
A direct topological matching via persistence diagrams is agnostic of the cell type composition. This is undesirable; we do not want to match a tumor cell hole to a stromal cell hole. To this end, we also incorporate spatial statistics mea-sure, i.e., cross K-functions, into the loss. This way, holes composed of different types of cells are matched properly.
Using the generated cell spatial configuration, we generate the nuclei mask, staining and texture.
See Fig. 1 for an illustration of the generation pipeline.
Also see Fig. 2 for examples of the generated cell lay-outs. The generated cell layouts have very similar spatial and structural characteristics as the reference/input image.
This is not guaranteed with previous methods using ran-domly generated masks. In the experiment section, we pro-vide comprehensive comparisons to verify the benefit of our method. We will also show that the augmented images can be used to train downstream tasks such as cell classification.
To summarize, our contributions are as follows:
• We propose the first generative model to learn cell spa-tial context from pathology images.
• We introduce multi-class spatial context descriptors based on spatial statistics and topology. These descrip-tors are used as conditional input for the generator.
• We propose a novel cell configuration loss function to enforce the desired behavior of spatial distribution and topology. The loss matches holes of generated cell lay-out and holes of the reference cell layout, in shape, density, and cell type composition.
• We show that the generated layouts can be used to generate synthetic H&E images for data augmenta-tion. We show the efficacy of the augmentation data in downstream tasks such as cell classification.
Figure 3. Illustration of the filtration process of the distance transform map to obtain the persistence homology. Red dots are the tumor cells in the original image. The blue dots in the last figure (f) are the centers for the holes, the saddle points which are obtained once a hole dies or disappears.
We stress that the benefit of modeling cell spatial con-text is beyond data augmentation. Modeling the spatial context will provide the foundation for better understand-ing and quantifying the heterogeneous tumor microenviron-ment, and correlate with genomics and clinical outcomes.
This work is one step towards such direction. 2.