Abstract
Most of the artificial lights fluctuate in response to the grid’s alternating current and exhibit subtle variations in terms of both intensity and spectrum, providing the poten-tial to estimate the Electric Network Frequency (ENF) from conventional frame-based videos. Nevertheless, the perfor-mance of Video-based ENF (V-ENF) estimation largely re-lies on the imaging quality and thus may suffer from signif-icant interference caused by non-ideal sampling, motion, and extreme lighting conditions.
In this paper, we show that the ENF can be extracted without the above limita-tions from a new modality provided by the so-called event camera, a neuromorphic sensor that encodes the light in-tensity variations and asynchronously emits events with ex-tremely high temporal resolution and high dynamic range.
Specifically, we first formulate and validate the physical mechanism for the ENF captured in events, and then pro-pose a simple yet robust Event-based ENF (E-ENF) estima-tion method through mode filtering and harmonic enhance-ment. Furthermore, we build an Event-Video ENF Dataset (EV-ENFD) that records both events and videos in diverse scenes. Extensive experiments on EV-ENFD demonstrate that our proposed E-ENF method can extract more accu-rate ENF traces, outperforming the conventional V-ENF by a large margin, especially in challenging environments with object motions and extreme lighting conditions. The code and dataset are available at https://github.com/ xlx-creater/E-ENF.
Figure 1. Illustrations of (a) the processing pipeline for E-ENF extraction and comparative experimental results under (b) different recording environments, in terms of (c) recorded event streams and (d) extracted V-ENF and E-ENF. 1.

Introduction
Light flicker is a nuisance in video [33] but can be ex-ploited to detect and estimate the Electric Network Fre-*Equal contribution. † Corresponding authors.
The research was partially supported by the National Natural Sci-ence Foundation of China under Grants 62271354 and the Natural Sci-ence Foundation of Hubei Province, China under Grants 2022CFB084 and 2021CFB467. quency (ENF) [10, 11], which is the power line transmis-sion frequency in a grid, unlocking the potential of mul-timedia forensic verification [7, 9], electrical load monitor-ing [23,24], recording device identification [1,28], etc. This is enabled by the discovery that the ENF fluctuates slightly and randomly around the nominal value (50 or 60 Hz) and consistently across the entire intra-grid nodes [13].
Existing Video-based ENF (V-ENF) extraction methods attempt to restore the illumination fluctuations by averag-ing the pixel intensities of every frame over the video se-quences [11]. Based on this, V-ENF estimates can be im-proved by using a higher sample rate in the rolling shut-ter [26,28] or an advanced frequency estimator [3,6,14,19].
However, due to the inherent characteristics of conventional cameras, existing V-ENF extraction methods still face a se-ries of challenges, which are summarized as follows.
• Non-ideal sampling. For global shutter videos, the sample rate equals to the camera’s frame rate, usu-ally at 25 or 30 fps, which is smaller than the Nyquist rate to record light flicker, resulting in frequency alias-ing for ENF estimation [10]. Although rolling shutter can increase the sample rate through the line exposure mechanism, it also introduces the inter-frame idle time within which no signal can be sampled [28,31], violat-ing the uniform sampling condition.
• Motion. Motion in the video leads to the abrupt change of pixel intensity, which is inconsistent with the flicker pattern and produces strong interference. Note that motion can be caused not only by object move-ment (e.g., walking person) but also by camera move-ment (e.g., hand-held camera shake).
• Extreme lighting conditions.
Imaging quality of over- or under-exposed videos (even under good light-ing conditions) can be substantially degraded, in which the flicker information can be totally lost [23]. Mean-while, under extreme low-light conditions (e.g., night scene with insufficient lighting), one has to boost the
ISO for appropriate exposure, inevitably bringing in
ISO noises, which can severely deteriorate the ENF.
Generally, the actual content in video recordings can have diverse scenes and objects, which become interference with the task of ENF extraction from the content. Note that putting a video camera still against a white wall illu-minated by a light source is an ideal condition for V-ENF extraction [10, 26]. However, to the very opposite, real-world recordings hardly satisfy this condition. To date, re-searchers and practitioners are still working intensively to tackle the above problems [14, 27, 29].
In this paper, we show that the above-mentioned prob-lems can be effectively solved via the proposed use of the so-called event camera, a new modality for recording. The event camera is a neuromorphic sensor that encodes the light intensity variations and asynchronously emits events with extremely high temporal resolution and high dynamic range [8, 21].
Different from V-ENF, the proposed Event-based ENF (E-ENF) extraction approach collects the illumination changes and converts them into an event stream, based on which the ENF traces can be eventually estimated, as shown in Fig. 1(a). Thanks to the high temporal resolution and high dynamic range of the event camera, E-ENF can pro-vide a sufficient sample rate and extract reliable ENF traces even under harsh conditions, e.g., motion interference and extreme lighting, as shown in Fig. 1(b-d).
Since no prior work has focused on extracting ENF from events, we hereby attempt to provide the first proof-of-concept study theoretically and experimentally demonstrat-ing why the ENF can exist in event streams and how it can be reliably extracted therein, in comparison with the con-ventional V-ENF approach.
The main contributions of this paper are three-fold:
• Based on the event-sensing mechanism, we formulate the process of the ENF fluctuation, reflected by light flicker, being recorded and converted into an event stream, validating the ENF capture in events.
• We propose the first method that effectively extracts the ENF traces from events, featuring a uniform-interval temporal sampling algorithm, a majority-voting spatial sampling algorithm, and a harmonic se-lection algorithm, respectively.
• We construct and open-source the first Event-Video hybrid ENF Dataset (EV-ENFD), containing both events and videos recorded in real-world environments with motion and extreme lighting conditions. The code and dataset are available at https://github. com/xlx-creater/E-ENF. 2.