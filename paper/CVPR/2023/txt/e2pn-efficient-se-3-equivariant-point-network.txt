Abstract
This paper proposes a convolution structure for learn-ing SE(3)-equivariant features from 3D point clouds. It can be viewed as an equivariant version of kernel point convo-lutions (KPConv), a widely used convolution form to pro-cess point cloud data. Compared with existing equivari-ant networks, our design is simple, lightweight, fast, and easy to be integrated with existing task-specific point cloud learning pipelines. We achieve these desirable properties by combining group convolutions and quotient representa-tions. Specifically, we discretize SO(3) to finite groups for their simplicity while using SO(2) as the stabilizer subgroup to form spherical quotient feature fields to save computa-tions. We also propose a permutation layer to recover SO(3) features from spherical features to preserve the capacity to distinguish rotations. Experiments show that our method achieves comparable or superior performance in various tasks, including object classification, pose estimation, and keypoint-matching, while consuming much less memory and running faster than existing work. The proposed method can foster the development of equivariant models for real-world applications based on point clouds. 1.

Introduction
Processing 3D data has become a vital task today as de-mands for automated robots and augmented reality tech-nologies emerge. In the past decade, computer vision has significantly succeeded in image processing, but learning from 3D data such as point clouds is still challenging. An important reason is that 3D data presents more variations than 2D images in several aspects. For example, the rigid body transformations in 2D only have 3 degrees of freedom (DoF) with 1 for rotations. In 3D space, the DoF is 6, with 3 for rotations. The 2D translation equivariance is a key fac-tor in the success of convolutional neural networks (CNNs)
Figure 1. Our method achieves higher efficiency by working with smaller feature maps defined on S2′ × R3 rather than SO(3)′ × R3 (′ denotes discretization). R3 is omitted in the figure. The black arrows in each space represent elements. The top and bottom paths are equivalent, showing the relations among different representations. in image processing, but it is not enough for 3D tasks.
Generally speaking, equivariance is a property for a map such that given a transformation in the input, the output changes in a predictable way determined by the input trans-formation. It drastically improves generalization as the vari-ance caused by the transformations is captured via the net-work by design. Take CNNs as an example, the equiv-ariance property refers to the fact that a translation in the input image results in the same translation in the feature map output from a convolution layer. However, conven-tional convolutions are not equivariant to rotations, which becomes problematic, especially when we deal with 3D data where many rotational variations occur. In response, on the one hand, data augmentations with 3D rotations are frequently used. On the other hand, equivariant feature learning emerges as a research area, aiming to generalize the translational equivariance to broader transformations.
A lot of progress has been made in group-equivariant feature learning. The term group encompasses the 3D rota-tions and translations, which is called the special Euclidean 1
group of dimension 3, denoted SE(3), and also other more general types of transformations that represent certain sym-metries. While many methods have been proposed, equiv-ariant feature learning has not yet become the default strat-egy for 3D deep learning tasks. From our understanding, two major reasons hinder the broader application of equiv-ariant methods. First, networks dealing with continuous groups typically require specially designed operations not commonly used in neural networks, such as generalized
Fourier transform and Monte Carlo sampling. Thus, incor-porating them into general neural networks for 3D learning tasks is challenging. Second, for the strategy of working on discretized (finite) groups [6, 19], while the network struc-tures are simpler and closer to conventional networks, they usually suffer from the high dimensionality of the feature maps and convolutions, which causes much larger memory usage and computational load, limiting their practical use.
This work proposes E2PN, a convolution structure for processing 3D point clouds. Our proposed approach can enable SE(3)-equivariance on any network with the KP-Conv [35] backbone by swapping KPConv with E2PN. The equivariance is up to a discretization on SO(3). We leverage a quotient representation to save computational and mem-ory costs by reducing SE(3) feature maps to feature maps defined on S2 × R3 (where S2 stands for the 2-sphere).
Nevertheless, we can recover the full 6 DoF information through a final permutation layer. As a result, our proposed network is SE(3)-equivariant and computationally efficient, ready for practical point-cloud learning applications.
Overall, this work has the following contributions:
• We propose an efficient SE(3)-equivariant convolution structure for 3D point clouds.
• We design a permutation layer to recover the full
SE(3) information from its quotient space.
• We achieve comparable or better performance with significantly reduced computational cost than existing equivariant models.
• Our implementation is open-sourced at https:// github.com/minghanz/E2PN.
Readers can find preliminary introductions to some re-lated background concepts in the appendix. 2.