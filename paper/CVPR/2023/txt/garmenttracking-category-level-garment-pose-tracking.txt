Abstract
Garments are important to humans. A visual system that can estimate and track the complete garment pose can be useful for many downstream tasks and real-world applications.
In this work, we present a complete pack-age to address the category-level garment pose tracking task: (1) A recording system VR-Garment, with which users can manipulate virtual garment models in simula-tion through a VR interface. (2) A large-scale dataset VR-Folding, with complex garment pose configurations in ma-nipulation like flattening and folding. (3) An end-to-end online tracking framework GarmentTracking, which pre-dicts complete garment pose both in canonical space and task space given a point cloud sequence. Extensive ex-periments demonstrate that the proposed GarmentTrack-ing achieves great performance even when the garment has large non-rigid deformation. It outperforms the base-line approach on both speed and accuracy. We hope our proposed solution can serve as a platform for future re-search. Codes and datasets are available in https:// garment-tracking.robotflow.ai. 1.

Introduction
Garments are one of the most important deformable ob-jects in daily life. A vision system for garment pose estima-tion and tracking can benefit downstream tasks like MR/AR and robotic manipulation [7, 17]. The category-level gar-ment pose estimation task is firstly introduced in Garment-Nets [11], which aims to recover the full configuration of an unseen garment from a single static frame. Unlike the non-rigid tracking methods [9, 10, 16, 19, 27, 34, 35] which can only recover the geometry of the visible regions, pose es-timation task can also reconstruct the occluded parts of the object. Another line of works [14, 15, 21, 22, 28–30] (non-rigid 4D reconstruction which can reconstruct complete ob-† Cewu Lu is the corresponding author, the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, China and Shanghai Qi Zhi Institute. ject geometry) cannot be directly applied on garments, since they assume the object has a watertight geometry. In con-trast, garments have thin structures with holes.
In this paper, we propose a new task called Category-level Garment Pose Tracking, which extends the single-frame pose estimation setting in [11] to pose tracking in dynamic videos. Specifically, we focus on the pose track-ing problem in garment manipulation (e.g. flattening, fold-ing). In this setting, we do not have the priors of the human body like previous works for clothed humans [18,26,31,41].
Therefore, we must address the extreme deformation that manipulated garments could undergo.
To tackle the garment pose tracking problem, we need a dataset of garment manipulation with complete pose an-notations. However, such a dataset does not exist so far to the best of our knowledge. To build such a dataset, we turn to a VR-based solution due to the tremendous difficulty of garment pose annotation in the real world [10]. We first create a real-time VR-based recording system named VR-Garment. Then the volunteer can manipulate the garment in a simulator through the VR interface. With VR-Garment, we build a large-scale garment manipulation dataset called
VR-Folding. Compared to the single static garment config-uration (i.e. grasped by one point) in GarmentNets, our ma-nipulation tasks include flattening and folding, which con-tain much more complex garment configurations. In total, our VR-Folding dataset contains 9767 manipulation videos which consist of 790K multi-view RGB-D frames with full garment pose and hand pose annotations on four garment categories selected from the CLOTH3D [8] dataset.
With the VR-Folding dataset, we propose an end-to-end online tracking method called GarmentTracking to per-form category-level garment pose tracking during manipu-lation. For the garment pose modeling, we follow Garment-Nets [11] to adopt the normalized object coordinate space (NOCS) for each category. Nevertheless, tracking garment pose raises new challenges compared to single-frame pose estimation: (1) How to fuse inter-frame geometry and corre-spondence information? (2) How to make the tracking pre-diction robust to pose estimation errors? (3) How to achieve
Figure 1. The pipeline of our Virtual Realty recording system (VR-Garment). (a) A volunteer needs to put on a VR headset and VR gloves. (b) By following the guidance of a specially designed UI, the volunteer begins to collect data efficiently. (c) After recording, we re-render multi-view RGB-D images with Unity [6] and obtain masks and deformed garment meshes with NOCS labels. tracking in real-time? To address these challenges, we con-duct GarmentTracking in three stages, namely NOCS pre-dictor, NOCS refiner, and warp field mapper. Firstly, it pre-dicts per-frame features and fuses them for canonical co-ordinate prediction. Then it refines the predicted canonical coordinates and the geometry with a NOCS refiner to re-duce the accumulated errors. Finally, it maps the prediction in canonical space to the task space (i.e. coordinate frame of the input point cloud).
Since no previous work is designed for the tracking set-ting, we use GarmentNets [11] as a single-frame prediction baseline for comparison. We also perform extensive abla-tive experiments to reveal the efficacy of our design choices.
Finally, we collect real-world data on garment manipula-tion and show the qualitative results of our method. In our design, we avoid the computationally expensive Marching
Cubes [25] for reconstructing the canonical mesh frame by frame, so that we can achieve tracking at 15 FPS with an
RTX 3090 GPU (5 times faster than the baseline approach).
We summarize our contributions as follows: 1). We propose a VR-based garment manipulation recording system named VR-Garment. It can synchronize human operations into the simulator and collect garment manipulation data. 2). We propose a large-scale garment manipulation dataset named VR-Folding for pose tracking. During ma-nipulation, garments exhibit diverse configurations. 3). We propose a real-time end-to-end framework named
GarmentTracking for category-level garment pose track-ing. It can serve as a strong baseline for further research.
We also demonstrate its generalization ability to real-world garment recordings with models trained by simulated data. 2.