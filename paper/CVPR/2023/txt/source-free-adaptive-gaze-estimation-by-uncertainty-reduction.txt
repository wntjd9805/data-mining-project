Abstract source domain target domain averaged mean error((cid:28733)) (b)
ETH-XGaze MPII
EyeDiap
Gaze estimation across domains has been explored re-cently because the training data are usually collected under controlled conditions while the trained gaze estimators are used in nature and diverse environments. However, due to privacy and efﬁciency concerns, simultaneous access to an-notated source data and to-be-predicted target data can be challenging.
In light of this, we present an unsupervised source-free domain adaptation approach for gaze estima-tion, which adapts a source-trained gaze estimator to unla-beled target domains without source data. We propose the
Uncertainty Reduction Gaze Adaptation (UnReGA) frame-work, which achieves adaptation by reducing both sample and model uncertainty. Sample uncertainty is mitigated by enhancing image quality and making them gaze-estimation-friendly, whereas model uncertainty is reduced by minimiz-ing prediction variance on the same inputs. Extensive ex-periments are conducted on six cross-domain tasks, demon-strating the effectiveness of UnReGA and its components.
Results show that UnReGA outperforms other state-of-the-art cross-domain gaze estimation methods under both pro-tocols, with and without source data. The code is available at https://github.com/caixin1998/UnReGA. 1.

Introduction
Gaze encodes rich information about the attention and psychological factors of an individual. Techniques that use eye tracking to infer human intentions and understand hu-man emotions have found an increasingly wide utilization in
ﬁelds including human-computer interaction [20,35,36], af-fective computing [11], and medical diagnosis [21,46]. The most prevalent way to estimate human gaze is using com-mercial eye trackers, which suffer from high cost or custom invasive hardware. To overcome the limitation on devices and environments, researchers have made great progress on
*This work is partially supported by National Key RD Program of
China (No. 2018AAA0102405), National Natural Science Foundation of
China (No. 62176248). model uncertainty 0.66
<
.98  0.98 
<
< enhance 1.2 1.21 (a) 0.80 ↓ 0.99 ↓ unlabeled  target faffacfafacfffacfaacacececee cece  face  enhancer sampleee uncertainty  (c) 9 8 7 6 0 1
-0 0 2
-0 1 0 3
-0 2 0 4
-0 3 0 5
-0 4 0 6
-0 5 0 7
-0 6 0 8
-0 7 0 9
-0 8 (b) percentile of the uncertainty(%) 0 0 1
-0 9 gazezeggazgazggggggggggazeazeazee estimatororororrr 1 gaze ggazgagggggggggagg eazeazeee estimatorrrrr (cid:884)
………
…
……
… gazeggazgaggggga eazeazeee estimatorrrr (cid:1837) (cid:303)(cid:303) variance modelmodelmode uncertainty 
Figure 1. (a) The source-trained model shows high uncertainty on samples from different domains. (b) Statistics of errors and model uncertainty by the same gaze estimator on different samples. The error increases as the uncertainty grow. (c) To accomplish unsu-pervised source-free domain adaptation, the UnReGA reduces the sample uncertainty by enhancing the input images and reduces the model uncertainty by minimizing the prediction variance. appearance-based gaze estimation methods with the devel-opment of deep learning [4, 6, 12, 56, 57].
Notwithstanding the achievements, the appearance-based gaze estimators meet the most challenging problem that their performance drops signiﬁcantly when they are trained and tested on different domains, e.g., the domains with different subjects, image quality, background environ-ments, or illuminations. Usually, gaze estimators are trained on the data collected under controlled conditions where true gaze is feasible to be measured and recorded by the de-ployed devices. Then, these gaze estimators would be ap-plied under a much different and uncontrolled environment.
To adapt the source-data-trained model to the target data, researchers propose methods to narrow the gap between the different domains [16, 34, 42, 45]. Most of the methods re-quire data from both the source and target domains during the adaptation. However, in the application of gaze esti-mation, the source data is likely to be neither available nor efﬁcient during the adaptation. First, most gaze models are trained with face images which might be not accessi-ble due to privacy or bandwidth issues. Secondly, process-ing source data might not be computationally practical in real-time gaze estimation on the target domain. Therefore, we formulate gaze estimation as an unsupervised source-free domain adaptation problem, where we cannot access the source data when ﬁtting the model to the target.
To address the source-free domain adaptation issue, we propose to adapt the source-trained gaze estimators to the target domain by reducing both the sample uncertainty and model uncertainty on the unlabeled target data. Sample un-certainty captures noise inherent in the input images, such as sensor noise and motion blur, which is also referred to as aleatoric uncertainty [24]. Model uncertainty is determined by the inconsistency of predication or model perturbations, which is also referred to as epistemic uncertainty [15, 24].
We formulate it as the variance of different estimators’ pre-dictions on the same sample. We assume that reducing the two uncertainties helps to reduce the gaze estimator’s er-rors across different domains due to three observations: 1)
Estimators show high model uncertainty on samples that are distributed far away from the training data and show low uncertainty on the nearby samples [24, 28]. As shown in Fig. 1(a), the ETH-XGaze-trained estimator has average model uncertainties of 0.66, 0.98, and 1.21 on the samples from ETH-XGaze [53], MPIIGaze [57], EyeDiap [14], re-spectively. EyeDiap has the most different distribution from
ETH-XGaze and shows the highest model uncertainty. 2)
Reducing the sample uncertainty pulls together the source and target data, and accordingly reduces the estimator’s model uncertainty on target data. In Fig. 1(a), the model un-certainties on MPIIGaze/EyeDiap decrease when we reduce the sample uncertainty by image enhancement, because by doing this, we reduce the image quality discrepancy be-tween MPIIGaze/EyeDiap and ETH-XGaze. 3) Model un-certainty empirically shows a positive correlation with gaze estimation error in cross-domain scenarios. Fig. 1(b) plots how the errors change with model uncertainty. We train 10 gaze estimators from ETH-XGaze and then, for each sam-ple in MPIIGaze, we compute the model uncertainty and the mean error of the estimators’ predictions. We sort the sam-ples by the model uncertainty in ascending order and group them by every 10-th percentile. The height of each bar in
Fig. 1(b) denotes the averaged mean error over the samples within each group. As can be seen, the top 10 percent of the model uncertainty corresponds to the smallest error.
To this end, we propose an Uncertainty Reduction
Gaze Adaption (UnReGA) framework that accomplishes the source-free adaptation by minimizing both the sample and model uncertainty. As illustrated in Fig. 1(c), we ﬁrst transfer the input images into a gaze-estimation-friendly do-main by introducing a face enhancer to enhance input im-ages without changing the gaze. Rather than low-quality images, high-quality images convey more details about the eyes and contribute to less sample uncertainty and better generalization ability of the source-trained gaze estimators.
Next, we update an ensemble of source gaze estimators by minimizing the variance of their predictions on the unla-beled target data. Finally, we merge the updated estimators into a single model during inference. Our empirical exper-iments demonstrate that the updated estimator outperforms the not-adapted source estimator on the target domain.
Our contributions are summarized as: 1. We formulate gaze estimation as an unsupervised source-free domain adaptation problem and propose an Uncertainty Reduction Gaze Adaption (UnReGA) framework that adapts the trained model to target do-main without the source data by reducing both the sam-ple uncertainty and model uncertainty. 2. We propose the variance minimization and pseudo-label supervision mechanisms in UnReGA to address the adaptation issue without source data for regres-sion tasks, while most existing source-free adapta-tion methods are designed for classiﬁcation tasks. We validate the effectiveness of the two mechanisms in source-free adaptive gaze estimation. 3. We evaluate the efﬁcacy of UnReGA and its com-ponents on cross-domain gaze estimation tasks. Ex-tensive experiments show UnReGA outperforms other state-of-the-art cross-domain gaze estimation methods under both protocols, with and without source data. 2.