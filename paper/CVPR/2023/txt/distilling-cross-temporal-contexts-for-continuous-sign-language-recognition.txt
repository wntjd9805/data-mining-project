Abstract
Continuous sign language recognition (CSLR) aims to recognize glosses in a sign language video. State-of-the-art methods typically have two modules, a spatial percep-tion module and a temporal aggregation module, which are jointly learned end-to-end. Existing results in [9, 20, 25, 36] have indicated that, as the frontal component of the over-all model, the spatial perception module used for spatial feature extraction tends to be insufficiently trained. In this paper, we first conduct empirical studies and show that a shallow temporal aggregation module allows more thor-ough training of the spatial perception module. However, a shallow temporal aggregation module cannot well capture both local and global temporal context information in sign language. To address this dilemma, we propose a cross-temporal context aggregation (CTCA) model. Specifically, we build a dual-path network that contains two branches for perceptions of local temporal context and global temporal context. We further design a cross-context knowledge distil-lation learning objective to aggregate the two types of con-text and the linguistic prior. The knowledge distillation en-ables the resultant one-branch temporal aggregation mod-ule to perceive local-global temporal and semantic context.
This shallow temporal perception module structure facili-tates spatial perception module learning. Extensive exper-iments on challenging CSLR benchmarks demonstrate that our method outperforms all state-of-the-art methods. 1.

Introduction
Sign language is a visual language for deaf and hearing-impaired people for ease of communication. Because sign language has a different grammatical structure and ex-pression from natural spoken language, deaf and hearing-*Wanli Xue and Qing Guo are corresponding authors (xue-wanli@email.tjut.edu.cn and tsingqguo@ieee.org).
Figure 1. (a) is the common CSLR framework. (b) presents the generalization capability (i.e., information stored in weights (IIW)
[31]) of TAMs and SPMs of baseline, state-of-the-art methods and the proposed method. (c) is the performance of the state-of-the-art methods including word error rate, test time, and model size. impaired people hardly communicate normally with hear-ing people in daily life. To eliminate this communica-tion gap, continuous sign language recognition (CSLR) en-forces to recognition of various glosses from a sign lan-guage video. Due to the data collection and annotation being labor-intensive, CSLR benchmarks adopt a sentence-annotation manner for all sign language videos [1, 15, 33].
In recent years, there is a consensus among state-of-the-art methods on a baseline framework (See Fig. 1 (a)).
It is made up of a spatial perception module (SPM), and a temporal aggregation module (TAM) including two compo-nents for local and global temporal perception module, and the connectionist temporal classification (CTC) loss [8] for training. At present, these methods [9, 20, 25, 36] have per-ceived one limitation of this framework that the temporal aggregation module can lead to insufficiently trained spatial
perception module and affect the final accuracy. We use a recent interpretation method (i.e., the compression of infor-mation stored in weights (IIW) [31]) to measure the gener-alization capability of different neural modules. Fig. 1 (b) shows the IIWs of TAMs and SPMs of baseline and state-of-the-art works and infers their positive relation, i.e., low-generalization TAM (i.e., high IIW) usually leads to low-generalization SPM. We provide more studies in Sec. 3.
Y. Min et al. [20] measure the difference of correctly and incorrectly recognized results between auxiliary and pri-mary classifiers to evaluate model overfitting, and A. Hao et al. [9] visualized heatmaps of TAM’s self-similarity ma-trices to show what the local and global temporal perception learning. However, there are no straightforward quantitative studies to discuss the effects of TAM on SPM, and we do not know how significant the effects could be and have no idea about the desired temporal aggregation. In this paper, we extensively study the limitation and desirable properties of the temporal aggregation module in the CSLR framework via constructing a baseline framework and extensive empir-ical studies. We insight that a desired temporal aggregation module should be a shallow architecture to allow more ef-fective training of spatial perception module but also should be a deep one for a high temporal aggregation capability.
Whereas, it is quite challenging for the temporal aggrega-tion module to achieve these properties simultaneously.
To overcome this challenge, we propose the cross-temporal context aggregation (CTCA) that a shallow tempo-ral aggregation module has capable of incorporating local-global temporal contexts and the linguistic prior. Specifi-cally, we construct a dual-path network, which decouples the local and global perception modules and imposes a lin-guistic module in parallel. This architecture ensures the lo-cal context perception, global context perception, and lin-guistic prior extraction. Furthermore, we propose a cross-context knowledge distillation loss function to transfer the local temporal context and the linguistic prior to the global perception module. Notice that the spatial perception mod-ule can facilitate itself by receiving cross-context knowl-edge as supervision during distillation. Fig. 1(b) shows that both SPM and TAM in CTCA achieve higher generalization than the ones of baselines. Consequently, Fig. 1(c) delivers
CTCA’s superiority and it outperforms the state-of-the-art methods on WER, test time, and model size. 2.