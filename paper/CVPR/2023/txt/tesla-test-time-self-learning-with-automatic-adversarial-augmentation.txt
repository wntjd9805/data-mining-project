Abstract
Most recent test-time adaptation methods focus on only classification tasks, use specialized network architectures, destroy model calibration or rely on lightweight information from the source domain. To tackle these issues, this paper proposes a novel Test-time Self-Learning method with auto-matic Adversarial augmentation dubbed TeSLA for adapt-ing a pre-trained source model to the unlabeled streaming test data. In contrast to conventional self-learning meth-ods based on cross-entropy, we introduce a new test-time loss function through an implicitly tight connection with the mutual information and online knowledge distillation.
Furthermore, we propose a learnable efficient adversarial augmentation module that further enhances online knowl-edge distillation by simulating high entropy augmented im-ages. Our method achieves state-of-the-art classification and segmentation results on several benchmarks and types of domain shifts, particularly on challenging measurement shifts of medical images. TeSLA also benefits from several de-sirable properties compared to competing methods in terms of calibration, uncertainty metrics, insensitivity to model architectures, and source training strategies, all supported by extensive ablations. Our code and models are available at https://github.com/devavratTomar/TeSLA. 1.

Introduction
Deep neural networks (DNNs) perform exceptionally well when the training (source) and test (target) data fol-low the same distribution. However, distribution shifts are inevitable in real-world settings and propose a major chal-lenge to the performance of deep networks after deployment.
Also, access to the labeled training data may be infeasible at test time due to privacy concerns or transmission band-width. In such scenarios, source-free domain adaptation (SFDA) [1, 23, 25] and test-time adaptation (TTA) meth-ods [19, 28, 39] aim to adapt the pre-trained source model to the unlabeled distributionally shifted target domain while easing access to source data. While SFDA methods have access to all full target data through multiple training epochs
Figure 1. Knowledge distillation with adversarial augmenta-tions. (a) Easy images with confident soft-pseudo labels and (b)
Hard images with unconfident soft-pseudo labels are adversarially augmented and pushed to the uncertainty region (high entropy) near the decision boundary. The model is updated for (a) to match its output on the augmented views with non-augmented views of Easy test images using KL-Divergence Lkd ̸= 0, while not updated for (b) as Lkd ∼ 0 between Hard images and their augmented views. (offline setup), TTA methods usually process test images in an online streaming fashion and represent a more realistic domain adaptation. However, most of these methods are applied: (i) only to classification tasks, (ii) evaluated on the non-real-world domain shifts, e.g., the non-measurement shift; (iii) destroy model calibration—entropy minimizing with overconfident predictions [45] on incorrectly classified samples, and (iv) use specialized network architectures or rely on the source dataset feature statistics [28].
We address these issues by proposing a new test-time adaptation method with automatic adversarial augmentation called TeSLA, under which we further define realistic TTA protocols. Self-learning methods often supervise the model adaptation on the unlabeled test images using their predicted pseudo-labels. As the model can easily overfit on its own pseudo-labels, a weight-averaged teacher model (slowly up-dated by the student model) is employed for obtaining the pseudo-labels [41, 47]. The student model is then trained with cross-entropy (CE) loss between the one-hot pseudo-labels and its predictions on the test images. In this paper, we instead propose to minimize flipped cross-entropy between the student model’s predictions and the soft pseudo-labels (notice the reverse order) with the negative entropy of its marginalized predictions over the test images. In Sec. 3, we show that the proposed formulation is an equivalence to mutual information maximization implicitly corrected
by the teacher-student knowledge distillation via pseudo-labels, yielding performance improvement on various test-time adaptation protocols and compared to the basic CE optimization (cf. ablation in Fig. 5-(1)).
Motivated by teacher-student knowledge distillation, an-other central tenet of our method is to assist the student model during adaptation in improving its performance on the hard-to-classify (high entropy) test images. For this purpose, we propose learning automatic adversarial aug-mentations (see Fig. 1) as a proxy for simulating images in the uncertainty region of the feature space. The model is then updated to ensure consistency between predictions of high entropy augmented images and soft-pseudo labels from the respective non-augmented versions. Consequently, the model is self-distilled on Easy test images with confident soft-pseudo labels (Fig. 1a). In contrast, the model update on Hard test images is discarded (Fig. 1b), resulting in better class-wise feature separation.
In summary, our contributions are: (i) we propose a novel test-time self-learning method based on flipped cross-entropy (f-CE) through the tight connection with the mutual informa-tion between the model’s predictions and the test images; (ii) we propose an efficient plug-in test-time automatic adversar-ial augmentation module used for online knowledge distilla-tion from the teacher to the student network that consistently improves the performance of test-time adaptation methods, including ours; and (iii) TeSLA achieves new state-of-the-art results on several benchmarks, from common image corrup-tion to realistic measurement shifts for classification and segmentation tasks. Furthermore, TeSLA outperforms ex-isting TTA methods in terms of calibration and uncertainty metrics while making no assumptions about the network architecture and source domain’s information, e.g., feature statistics or training strategy. 2.