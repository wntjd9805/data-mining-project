Abstract
Sparsely annotated semantic segmentation (SASS) aims to learn a segmentation model by images with sparse labels (i.e., points or scribbles). Existing methods mainly focus on introducing low-level affinity or generating pseudo la-bels to strengthen supervision, while largely ignoring the inherent relation between labeled and unlabeled pixels. In this paper, we observe that pixels that are close to each other in the feature space are more likely to share the same class.
Inspired by this, we propose a novel SASS frame-work, which is equipped with an Adaptive Gaussian Mix-ture Model (AGMM). Our AGMM can effectively endow re-liable supervision for unlabeled pixels based on the distri-butions of labeled and unlabeled pixels. Specifically, we first build Gaussian mixtures using labeled pixels and their relatively similar unlabeled pixels, where the labeled pix-els act as centroids, for modeling the feature distribution of each class. Then, we leverage the reliable information from labeled pixels and adaptively generated GMM predic-tions to supervise the training of unlabeled pixels, achieving online, dynamic, and robust self-supervision. In addition, by capturing category-wise Gaussian mixtures, AGMM en-courages the model to learn discriminative class decision boundaries in an end-to-end contrastive learning manner.
Experimental results conducted on the PASCAL VOC 2012 and Cityscapes datasets demonstrate that our AGMM can establish new state-of-the-art SASS performance. Code is available at https://github.com/Luffy03/AGMM-SASS.
Figure 1. (a) Illustration of SASS task. (b) Different from existing
SASS frameworks, our AGMM leverages the reliable information of labeled pixels and generates GMM predictions for dynamic on-line supervision. f denotes the model, P and G represent segmen-tation and GMM predictions, respectively. Solid and dashed lines represent model propagation and supervision, respectively. 1.

Introduction
Semantic segmentation [2, 8, 42] aims to assign the corresponding pixel-wise semantic labels for a given im-age, which is a fundamental computer vision task. Pre-*Corresponding author vious deep learning based semantic segmentation meth-ods [3, 9, 43] trained on large amounts of data with accu-rate pixel-wise annotations have demonstrated outstanding achievements. However, collecting such dense annotations always requires cumbersome manual efforts, which heav-ily limits the development of semantic segmentation meth-ods. To reduce the cost of manual annotations, many re-Figure 2. (a) Observation of the inherent relation between the labeled and unlabeled pixels. (b) Category-wise performance on the PASCAL
VOC 2012 dataset. The black line, blue bar, and orange bar represent the IoU of all unlabeled pixels, unlabeled pixels that are similar to labeled pixels, and unlabeled pixels that are dissimilar to labeled pixels, respectively. σ is the variance of a class (Eq. 5). cent works [4,17,27,29,47,48,59] have been made towards sparsely annotated semantic segmentation (SASS), which learns segmentation models via sparse labels, i.e., points or scribbles, as shown in Fig. 1(a). The sparse annotations are cheap to obtain and also contain the least necessary category and location information. Thus, SASS has high research potential in terms of the trade-off between information and costs. i.e.,
The main challenge of SASS is the lack of information for supervision. Existing SASS methods can be roughly divided into three categories, low-level regulariza-tion [26,30,34,47,48], pseudo supervision [7,27,35,59,60], and consistency learning [19, 38], as shown in Fig. 1(b).
Specifically, the low-level regularization methods [26, 30, 34, 47, 48] focus on introducing the low-level affinity of the raw images for supervision. However, the low-level infor-mation is not reliable enough to be associated with the high-level semantics. Pseudo supervision [27, 35, 59, 60] aims to generate pseudo labels via training with sparse labels, and then uses these pseudo labels to learn a more robust segmentation model. However, it commonly requires time-consuming multi-stage training and the generated pseudo labels are always coarse and ambiguous, which significantly hinders the learning of unlabeled pixels. Consistency learn-ing [19, 38, 55] further proposes to learn consistent repre-sentations in the high-dimension feature space, but it cannot directly supervise the final predictions at the category level.
To solve these problems, we aim to address the SASS task with more reliable supervision. To this end, we argue that the reliable information of labeled pixels should be fur-ther exploited. Previous methods only employ the labeled pixels for partial cross-entropy supervision, while largely ignoring the inherent relation between labeled and unla-beled pixels. As illustrated in Fig. 2, we observe that the similarity between labeled and unlabeled pixels is highly as-sociated with the predictions of unlabeled pixels. As shown in Fig. 2(a), if an unlabeled pixel is similar to the labeled pixel in the feature space, its corresponding prediction is more likely to be consistent with the category of the labeled pixel. In Fig. 2(b), we calculate the distance d (see Eq. 6) between labeled and unlabeled pixels to measure the simi-larity, i.e., d < σ as similar and d > σ as not similar. It can be seen that the similarity between labeled and unla-beled pixels is highly associated with the accuracy of the predictions. To this end, we propose to explicitly leverage the similarity between the labeled and unlabeled pixels to generate supervision information. The key challenge is how to effectively model the similarity between the labeled and unlabeled pixels.
In this paper, we propose a novel Adaptive Gaussian
Mixture Model (AGMM) framework, which is realized by incorporating a GMM branch into the traditional segmenta-tion branch. Specifically, we assign the labeled pixels as the centroids of Gaussian mixtures, enabling us to model the data distribution of each class in the high-dimension fea-ture space. Each Gaussian mixture represents the distribu-tion of a class, which consists of the centered labeled pix-els and the relatively similar unlabeled pixels. In this way, we build a GMM to measure the feature similarity between labeled and unlabeled pixels, producing soft GMM predic-tions to supervise the unlabeled regions from a probabilis-tic perspective. The process of GMM formulation works in an adaptive manner, where the parameters of GMM are dynamically adapted to the input features, achieving end-to-end online self-supervision. The GMM branch is progres-sively optimized during training, enabling us to learn more
discriminative Gaussian mixtures adaptively.
There are three appealing advantages in our proposed
AGMM. First, by capturing category-wise Gaussian mix-tures for feature representations, we can learn discrimina-tive decision boundaries between different classes via very limited supervision. Second, AGMM pushes each unla-beled pixel into or away from specific category-wise Gaus-sian mixtures, which further enables an end-to-end con-trastive representation learning. Finally, we leverage the reliable information from labeled pixels to generate GMM predictions for the unlabeled pixels, achieving more reliable supervision.
We conduct experiments under the point- and scribble-supervised settings on two widely used datasets, i.e., PAS-CAL VOC 2012 [14] and Cityscapes [12].
It is worth noting that compared with existing SASS methods, our
AGMM does not require extra information for supervision
[19,26,30,34,50], multi-stage training [7,35,37,59,60], and time-consuming post-processing [27, 31, 50, 60]. Extensive experiments demonstrate that our AGMM outperforms the existing state-of-the-art SASS methods. 2.