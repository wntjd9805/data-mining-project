Abstract
Building animatable 3D models is challenging due to the need for 3D scans, laborious registration, and rigging. Re-cently, differentiable rendering provides a pathway to ob-tain high-quality 3D models from monocular videos, but these are limited to rigid categories or single instances. We present RAC, a method to build category-level 3D models from monocular videos, disentangling variations over in-stances and motion over time. Three key ideas are intro-duced to solve this problem: (1) specializing a category-level skeleton to instances, (2) a method for latent space regularization that encourages shared structure across a category while maintaining instance details, and (3) us-ing 3D background models to disentangle objects from the background. We build 3D models for humans, cats and dogs given monocular videos. Project page: https://gengshan-y.github.io/rac-www/. 1.

Introduction
We aim to build animatable 3D models for deformable object categories. Prior work has done so for targeted cat-egories such as people (e.g., SMPL [1, 30]) and quadruped animals (e.g., SMAL [4]), but such methods appear chal-lenging to scale due to the need of 3D supervision and reg-istration. Recently, test-time optimization through differ-entiable rendering [40, 41, 44, 56, 69] provides a pathway to generate high-quality 3D models of deformable objects and scenes from monocular videos. However, such models are typically built independently for each object instance or scene. In contrast, we would like to build category models that can generate different instances along with deforma-tions, given causally-captured video collections.
Though scalable, such data is challenging to leverage in practice. One challenge is how to learn the morpho-logical variation of instances within a category. For ex-ample, huskys and corgis are both dogs, but have dif-ferent body shapes, skeleton dimensions, and texture ap-pearance. Such variations are difﬁcult to disentangle from the variations within a single instance, e.g., as a dog artic-ulates, stretches its muscles, and even moves into differ-ent illumination conditions. Approaches for disentangling such factors require enormous efforts in capture and reg-istration [1, 5], and doing so without explicit supervision remains an open challenge.
Another challenge arises from the impoverished nature of in-the-wild videos: objects are often partially observ-able at a limited number of viewpoints, and input signals such as segmentation masks can be inaccurate for such “in-the-wild” data. When dealing with partial or impoverished video inputs, one would want the model to listen to the com-mon structures learned across a category – e.g., dogs have two ears. On the other hand, one would want the model to
Morphology β
θ n o i t a l u c i t r
A
Figure 2. Disentangling morphologies β and articulation θ. We show different morphologies (body shape and clothing) given the same rest pose (top) and bouncing pose (bottom). stay faithful to the input views.
Our approach addresses these challenges by exploiting three insights: (1) We learn skeletons with constant bone lengths within a video, allowing for better disentanglement of between-instance morphology and within-instance artic-ulation. (2) We regularize the unobserved body parts to be coherent across instances while remaining faithful to the in-put views with a novel code-swapping technique. (3) We make use of a category-level background model that, while not 3D accurate, produces far better segmentation masks.
We learn animatable 3D models of cats, dogs, and humans which outperform prior art. Because our models regis-ter different instances with a canonical skeleton, we also demonstrate motion transfer across instances. 2.