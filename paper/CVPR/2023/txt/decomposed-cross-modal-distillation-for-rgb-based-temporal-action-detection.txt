Abstract
Temporal action detection aims to predict the time inter-vals and the classes of action instances in the video. Despite the promising performance, existing two-stream models ex-hibit slow inference speed due to their reliance on compu-tationally expensive optical ﬂow.
In this paper, we intro-duce a decomposed cross-modal distillation framework to build a strong RGB-based detector by transferring knowl-edge of the motion modality. Speciﬁcally, instead of direct distillation, we propose to separately learn RGB and motion representations, which are in turn combined to perform ac-tion localization. The dual-branch design and the asymmet-ric training objectives enable effective motion knowledge transfer while preserving RGB information intact. In addi-tion, we introduce a local attentive fusion to better exploit the multimodal complementarity. It is designed to preserve the local discriminability of the features that is important for action localization. Extensive experiments on the bench-marks verify the effectiveness of the proposed method in en-hancing RGB-based action detectors. Notably, our frame-work is agnostic to backbones and detection heads, bring-ing consistent gains across different model combinations. 1.

Introduction
With the popularization of mobile devices, a signiﬁcant number of videos are generated, uploaded, and shared ev-ery single day through various online platforms such as
YouTube and TikTok. Accordingly, there arises the impor-tance of automatically analyzing untrimmed videos. As one of the major tasks, temporal action detection (or localiza-tion) [56] has attracted much attention, whose goal is to ﬁnd the time intervals of action instances in the given video. In recent years, a lot of efforts have been devoted to improving the action detection performance [28–30, 36, 37, 74, 79, 84].
Most existing action detectors take as input two-stream data consisting of RGB frames and motion cues, e.g., opti-cal ﬂow [21, 66, 78]. Indeed, it is widely known that differ-Figure 1. Comparison between conventional distillation and ours.
Framework
Method
Average mAP (%)
RGB+OF RGB
∆
Anchor-based G-TAD [74]
Anchor-free
AFSD [34]
Actionformer [80]
DETR-like
TadTR [42]
Proposal-free
TAGS [47] 41.5 52.4 62.2 56.7 52.8 26.9 −14.6 43.3 55.5
−9.1
−6.7 46.0 −10.7 47.9
−4.9
Table 1. Impact of motion modality. We measure the average mAP under the IoU thresholds of [0.3:0.7:0.1] on THUMOS’14. ent modalities provide complementary information [6, 24, 58, 69]. To examine how much two-stream action detec-tors rely on the motion modality, we conduct an ablative study using a set of representative models1. As shown in
Table 1, regardless of the framework types, all the mod-els experience sharp performance drops when the motion modality is absent, probably due to the static bias of video models [11, 27, 31, 32]. This indicates that explicit motion cues are essential for accurate action detection.
However, two-stream action detectors impose a cycle of dilemmas for real-world applications due to the heavy com-putational cost of motion modality. For instance, the most popular form of motion cues for action detection, TV-L1 optical ﬂow [66], is not real-time, taking 1.8 minutes to process a 1-min 224 × 224 video of 30 fps on a single
GPU [58]. Although cheaper motion clues such as temporal gradient [63, 70, 85] can be alternatives, two-stream models still exhibit inefﬁciency at inference by doubling the net-*Corresponding author 1Each model is reproduced by its ofﬁcial codebase.
work forwarding process. Therefore, it would be desirable to build strong RGB-based action detectors that can bridge the performance gap with two-stream methods.
To this end, we focus on cross-modal knowledge distilla-tion [12,16], where the helpful knowledge of motion modal-ity is transferred to an RGB-based action detector during training in order to improve its performance. In contrast to conventional knowledge distillation [19, 20, 46, 52] where the superior teacher guides the weak student, cross-modal distillation requires exploiting the complementarity of the teacher and student. However, existing cross-modal distil-lation approaches [12,13] fail to consider the difference and directly transfer the motion knowledge to the RGB model (Fig. 1a), as conventional distillation does. By design, the
RGB and motion information are entangled with each other, making it difﬁcult to balance between them. As a result, they often achieve limited gains without careful tuning.
To tackle the issue, we introduce a novel framework, named decomposed cross-modal distillation (Fig. 1b).
In detail, our model adopts the split-and-merge paradigm, where the high-level features are decomposed into appear-ance and motion components within a dual-branch design.
Then only the motion branch receives the distillation signal, while the other branch remains intact to learn appearance in-formation. For explicit decomposition, we adopt the shared detection head and the asymmetric objective functions for the branches. Moreover, we design a novel attentive fusion to effectively combine the multimodal information provided by the two branches. In contrast to existing attention meth-ods, the proposed fusion preserves local sensitivity which is important for accurate action detection. With these key components, we build a strong action detector that produces precise action predictions given only RGB frames.
We conduct extensive experiments on the popular bench-marks, THUMOS’14 [22] and ActivityNet1.3 [4]. Experi-mental results show that the proposed framework enables effective cross-modal distillation by separating the RGB and motion features. Consequently, our model largely im-proves the performance of RGB-based action detectors, ex-hibiting its superiority over conventional distillation. The resulting RGB-based action detectors effectively bridge the gap with two-stream models. Moreover, we validate our ap-proach by utilizing another motion clue, i.e., temporal gra-dient, which has been underexplored for action detection.
To summarize, our contributions are three-fold: 1) We propose a decomposed cross-modal distillation framework, where motion knowledge is transferred in a separate way such that appearance information is not harmed. 2) We de-sign a novel attentive fusion method that is able to exploit the complementarity of two modalities while sustaining the local discriminability of features. 3) Our method is gener-alizable to various backbones and detection heads, showing consistent improvements. 2.