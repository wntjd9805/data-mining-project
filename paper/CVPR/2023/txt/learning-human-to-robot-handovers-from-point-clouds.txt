Abstract
We propose the first framework to learn control policies for vision-based human-to-robot handovers, a critical task for human-robot interaction. While research in Embodied
AI has made significant progress in training robot agents in simulated environments, interacting with humans remains challenging due to the difficulties of simulating humans.
Fortunately, recent research has developed realistic simu-lated environments for human-to-robot handovers. Lever-aging this result, we introduce a method that is trained with a human-in-the-loop via a two-stage teacher-student frame-work that uses motion and grasp planning, reinforcement learning, and self-supervision. We show significant per-formance gains over baselines on a simulation benchmark, sim-to-sim transfer and sim-to-real transfer. Video and code are available at https://handover-sim2real.github.io. 1.

Introduction
Handing over objects between humans and robots is an important tasks for human-robot interaction (HRI) [35]. It
*This work was done during an internship at NVIDIA. allows robots to assist humans in daily collaborative activi-ties, such as helping to prepare a meal, or to exchange tools and parts with human collaborators in manufacturing set-tings. To complete these tasks successfully and safely, in-tricate coordination between human and robot is required.
This is challenging, because the robot has to react to human behavior, while only having access to sparse sensory inputs such as a single camera with limited field of view. There-fore, a need for methods that solve interactive tasks such as handovers purely from vision input arises.
Bootstrapping robot training in the real world can be un-safe and time-consuming. Therefore, recent trends in Em-bodied AI have focused on training agents to act and interact in simulated (sim) environments [11, 12, 19, 43, 45, 46, 51].
With advances in rendering and physics simulation, models have been trained to map raw sensory input to action out-put, and can even be directly transferred from simulation to the real world [2, 42]. Many successes have been achieved particularly around the suite of tasks of robot navigation, manipulation, or a combination of both. In contrast to these areas, little progress has been made around tasks pertained to HRI. This is largely hindered by the challenges in em-bedding realistic human agents in these environments, since
modeling and simulating realistic humans is challenging.
Despite the challenges, an increasing number of works have attempted to embed realistic human agents in simu-lated environments [6, 9, 16, 36–38, 48]. Notably, a recent work has introduced a simulation environment (“Handover-Sim”) for human-to-robot handover (H2R) [6]. To ensure a realistic human handover motion, they use a large motion capture dataset [7] to drive the movements of a virtual hu-man in simulation. However, despite the great potential for training robots, the work of [6] only evaluates off-the-shelf models from prior work, and has not explored any policy training with humans in the loop in their environment.
We aim to close this gap by introducing a vision-based learning framework for H2R handovers that is trained with a human-in-the-loop (see Fig. 1). In particular, we propose a novel mixed imitation learning (IL) and reinforcement learning (RL) based approach, trained by interacting with the humans in HandoverSim. Our approach draws inspira-tion from a recent method for learning polices for grasping static objects from point clouds [50], but proposes several key changes to address the challenges in H2R handovers.
In contrast to static object grasping, where the policy only requires object information, we additionally encode human hand information in the policy’s input. Also, compared to static grasping without a human, we explicitly take human collisions into account in the supervision of training. Fi-nally, the key distinction between static object grasping and handovers is the dynamic nature of the hand and object dur-ing handover. To excel on the task, the robot needs to react to dynamic human behavior. Prior work typically relies on open-loop motion planners [49] to generate expert demon-strations, which may result in suboptimal supervision for dynamic cases. To this end, we propose a two-stage training framework. In the first stage, we fix the humans to be sta-tionary and train an RL policy that is partially guided by ex-pert demonstrations obtained from a motion and grasp plan-ner. In the second stage, we finetune the RL policy in the original dynamic setting where the human and robot move simultaneously. Instead of relying on a planner, we propose a self-supervision scheme, where the pre-trained RL policy serves as a teacher to the downstream policy.
We evaluate our method in three “worlds” (see Fig. 1).
First, we evaluate on the “native” test scenes in Handover-Sim [6], which use the same backend physics simulator (Bullet [10]) as training but unseen handover motions from the simulated humans. Next, we perform sim-to-sim evalua-tion on the test scenes implemented with a different physics simulator (Isaac Gym [29]). Lastly, we investigate sim-to-real transfer by evaluating polices on a real robotic system and demonstrate the benefits of our method.
We contribute: i) the first framework to train human-to-robot handover tasks from vision input with a human-in-the-loop, ii) a novel teacher-student method to train in the setting of a jointly moving human and robot, iii) an em-pirical evaluation showing that our approach outperforms baselines on the HandoverSim benchmark, iv) transfer ex-periments indicating that our method leads to more robust sim-to-sim and sim-to-real transfer compared to baselines. 2.