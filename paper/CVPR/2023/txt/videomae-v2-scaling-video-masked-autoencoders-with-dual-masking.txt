Abstract
Scale is the primary factor for building a powerful foun-dation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens.
Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset.
Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on
V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner. 1.

Introduction
Effectively pre-training large foundation models [5] on huge amounts of data is becoming a successful paradigm in learning generic representations for multiple data modali-ties (e.g., language [6, 16], audio [13, 50], image [3, 22, 79], video [18, 63, 76], vision-language [27, 55]). These foun-dation models could be easily adapted to a wide range of downstream tasks through zero-shot recognition, linear
* : Equal contribution.
Figure 1. VideoMAE with dual masking. To improve the overall efficiency of computation and memory in video masked autoen-coding, we propose to mask the decoder as well and devise the dual masking strategy. Like encoder, we also apply a masking map to the deocoder and simply reconstruct a subset of pixel cubes se-lected by the running cell masking. The final reconstruction loss only applies for the invisible tokens dropped by the encoder. probe, prompt tuning, or fine tuning. Compared with the specialized model to a single task, they exhibit excellent generalization capabilities and have become the main driv-ing force for advancing many areas in AI.
For vision research, many efforts have been devoted to developing effective pre-trained models. Among them,
Transformer [65] with masked autoencoding [16] is be-coming a conceptually simple yet effective self-supervised visual learner (e.g., BEiT [3], SimMIM [79], MAE [22] for images, and MaskFeat [76], VideoMAE [63], MAE-ST [18] for videos). Meanwhile, based on the results in language models [6], scaling model capacity and data size is an important ingredients for its remarkable performance improvement. However, for pre-trained vision models, very few work [44] has tried to scale up this masked autoencoder pre-training to the billion-level models in image domain, partially due to the high data dimension and the high com-putational overhead. This issue is even more serious for scaling up video masked autoencoder pre-training owning to its extra time dimension and strong temporal variations.
Following the promising findings in languages and im-ages, we aim to study the scaling property of video
masked autoencoder (VideoMAE), and push its perfor-mance limit on a variety of video downstream tasks. We scale VideoMAE in both model and data. For model scal-ing, we try to instantiate the VideoMAE with vision trans-former (ViT) [17] having billion-level parameters (e.g., ViT-g [84]), and for data scaling, we hope to increase the pre-training dataset size to million-level to fully unleash the power of billion-level ViT model. However, to successfully train giant VideoMAE on such huge amounts of data and achieve impressive improvements on all considered down-stream tasks, we still need to carefully address a few issues.
First, we find computational cost and memory consump-tion is the bottleneck of scaling VideoMAE on the current
GPUs with limited memory. Although VideoMAE [63] has improved its pre-training efficiency and reduced its mem-ory consumption by employing the efficient asymmetric encoder-decoder architecture [22] (i.e., dropping large num-bers of tokens in encoder), it still fails to well support the billion-level video transformer pre-training. It takes more than two weeks to pre-train a ViT-g model with VideoMAE on 64 A100 GPUs. To further improve its pre-training ef-ficiency, we find video data redundancy can be used to not only mask a high portion of cubes in the encoder, but also drop some cubes in the decoder. This solution yields higher pre-training efficiency and creates a similarly challenging and meaningful self-supervised task. In practice, it will in-crease the pre-training batchsize and reduce the pre-training time by a third with almost no performance drop.
Second, MAE is still demanding for large data [80] and billion-level video transformer tends to overfit on relatively small data. Unlike images, the existing public video dataset is much smaller. For example, there are only 0.24M videos in the Kinetics400 dataset [28], while the ImageNet-22k dataset [15] has 14.2M images, let alone those publicly in-accessible image datasets such as JFT-3B [84]. Therefore, we need to come up with new ways to build a larger video pre-training dataset to well support the billion-level video transformer pre-training. We show that simply mixing the video datasets from multiple resources could produce an ef-fective and diverse pre-training dataset for VideoMAE and improve its downstream performance of pre-trained models.
Finally, it is still unknown how to adapt the billion-level pre-trained model by VideoMAE. Masked autoencod-ing is expected to learn invariant features that provide a fa-vored initialization for vision transformer fine-tuning [30].
However, directly fine-tuning billion-level pre-trained mod-els on a relatively small video dataset (e.g., 0.24M videos) might be suboptimal, as the limited labeled samples might lead to overfitting issue in fine-tuning.
In fact, in image domain, the intermediate fine-tuning technique [3, 44] has been employed to boost the performance of masked pre-trained models. We show that collecting multiple labeled video datasets and building a supervised hybrid dataset can act as a bridge between the large-scale unsupervised dataset and the small-scale downstream target dataset. Progressive fine-tuning of the pre-trained models through this labeled hybrid dataset could contribute to higher performance in the downstream tasks.
Based on the above analysis, we present a simple and efficient way to scale VideoMAE to billion-level ViT mod-els on a dataset containing million-level pre-training videos.
Our technical improvement is to introduce the dual mask-ing strategy for masked autoencoder pipeline as shown in
Figure 1.
In addition to the masking operation in en-coder, we propose to mask decoder as well based on the data redundancy prior in video. With this dual-masked
VideoMAE, we follow the intermediate fine-tuning in im-ages [3, 44], and use a progressive training pipeline to per-form the video masked pre-training on the million-level un-labeled video dataset and then post-pre-training on the la-beled hybrid dataset. These core designs contribute to an ef-ficient billion-level video autoencoding framework, termed as VideoMAE V2. Within this framework, we successfully train the first video transformer model with one billion pa-rameters, which attains a new state-of-the-art performance on a variety of downstream tasks, including action recog-nition [20, 28, 32, 57], spatial action detection [21, 34], and temporal action detection [26, 43]. 2.