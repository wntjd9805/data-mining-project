Abstract
Video-text retrieval is an emerging stream in both com-puter vision and natural language processing communi-ties, which aims to find relevant videos given text queries.
In this paper, we study the notoriously challenging task, i.e., Unsupervised Domain Adaptation Video-text Retrieval (UDAVR), wherein training and testing data come from dif-ferent distributions. Previous works merely alleviate the domain shift, which however overlook the pairwise mis-alignment issue in target domain, i.e., there exist no se-mantic relationships between target videos and texts. To tackle this, we propose a novel method named Dual Align-ment Domain Adaptation (DADA). Specifically, we first in-troduce the cross-modal semantic embedding to generate discriminative source features in a joint embedding space.
Besides, we utilize the video and text domain adaptations to smoothly balance the minimization of the domain shifts.
To tackle the pairwise misalignment in target domain, we propose the Dual Alignment Consistency (DAC) to fully ex-ploit the semantic information of both modalities in target domain. The proposed DAC adaptively aligns the video-text pairs which are more likely to be relevant in target do-main, enabling that positive pairs are increasing progres-sively and the noisy ones will potentially be aligned in the later stages. To that end, our method can generate more truly aligned target pairs and ensure the discriminability of target features. Compared with the state-of-the-art meth-ods, DADA achieves 20.18% and 18.61% relative improve-ments on R@1 under the setting of TGIF→MSR-VTT and
TGIF→MSVD respectively, demonstrating the superiority of our method. 1.

Introduction
Video-text retrieval enables users to search videos with a simple and natural language description. The de facto paradigm is to learn high-level visual-textual embeddings
*Corresponding author.
Figure 1.
Illustration of the proposed method. Previous meth-ods simply bring source and target features closer (blue and red ovals are overlapping each other), whereas inevitably mixing tar-get videos (red circles) and texts (red triangles) together, ignoring whether they are semantically relevant or not. Instead, our method exploits the semantic structures in target domain to adaptively gen-erate truly aligned video-text pairs (dotted circles) and ensure the discriminability of target data. Best viewed in color. by off-the-shelf feature extractors, and to measure semantic similarities in a joint embedding space [13, 42, 46, 63]. De-spite their thrilling success, the primary assumption is that training and testing data come from the same distribution, which whereas may not hold in real scenarios.
To alleviate the domain shift problem, Unsupervised Do-main Adaptation (UDA) has gained a lot of attention due to its efficient training without the need of supervision in target domain. UDA transfers knowledge from a labeled source domain to an unlabeled target domain [15, 33, 40, 41, 53], which has made remarkable progress in many fields, such as image classification [33, 56], autonomous driving [54, 55], medical image processing [35, 36], and video-based action recognition [50, 52]. However, these methods are originally designed for classification tasks, which might not be suit-able for the video-text retrieval.
Note that in UDA Video-text Retrieval (UDAVR), there label set for source and target do-exists no identical mains. The only supervision is the semantic relationship in source dataset, which is also the general setting for UDA
cross-modal tasks [4, 11, 62, 64]. To that end, some ap-proaches have been recently proposed [9, 17, 39], such as directly minimizing the distribution discrepancy [17], dis-tilling knowledge from the source domain [9], or introduc-ing pre-defined prototype assignments [39]. However, they overlook the pairwise misalignment issue in target domain, i.e., there exist no semantic relationships between target videos and texts. Merely alleviating the video and text do-main shifts is a sub-optimal solution, which fails to fully explore the semantic structures of target data, i.e., whether the video-text pair is semantically relevant or not. As illus-trated in Fig. 1, previous methods bring the learned source and target features close together, which whereas inevitably mixes up target videos and texts, ignoring whether they are a truly relevant pair or not. This will further induce less dis-criminative target features, and thus becomes the motivation of our work.
In this paper, we propose a novel method named Dual
Alignment Domain Adaptation (DADA) to tackle the pair-wise misalignment issue in target domain. We first in-troduce the cross-modal semantic embedding to generate discriminative source features in a joint embedding space, where semantically relevant pairs should lie close together and vice versa. To alleviate the domain shift, we further utilize a smooth adaptation procedure to balance the min-imization of distribution shifts between source and target domains. Last but not least, to tackle the pairwise misalign-ment in target domain, we propose a simple yet effective
Dual Alignment Consistency (DAC), which fully exploits the semantic information of both modalities in target do-main. The proposed DAC adaptively aligns the video-text pairs which are more likely to be relevant in target domain, enabling that (1) positive pairs are increasing progressively, (2) the noisy ones will potentially be aligned in the later stages and (3) the discriminability of target features. Ex-tensive experiments on several benchmarks demonstrate the superiority of our method.
The contributions of this paper are mainly threefold:
• To tackle the pairwise misalignment problem in
UDAVR task, we develop a novel method named Dual
Alignment Domain Adaptation (DADA) which fully exploits the semantic structures of target data.
• The proposed Dual Alignment Consistency (DAC) mechanism adaptively aligns the most similar videos and texts in target domain, ensure that the positive pairs are increasing progressively and the noisy ones are potentially aligned in later stages.
• Compared with the state-of-the-art methods, DADA achieves 20.18% and 18.61% relative improvements on R@1 under the setting of TGIF→MSRVTT and
TGIF→MSVD respectively, demonstrating the supe-riority of our method. 2.