Abstract
Recent advances in 3D object detection (3DOD) have obtained remarkably strong results for LiDAR-based mod-In contrast, surround-view 3DOD models based on els. multiple camera images underperform due to the neces-sary view transformation of features from perspective view (PV) to a 3D world representation which is ambiguous due to missing depth information. This paper introduces
X3KD, a comprehensive knowledge distillation framework across different modalities, tasks, and stages for multi-camera 3DOD. Specifically, we propose cross-task distil-lation from an instance segmentation teacher (X-IS) in the
PV feature extraction stage providing supervision without ambiguous error backpropagation through the view trans-formation. After the transformation, we apply cross-modal feature distillation (X-FD) and adversarial training (X-AT) to improve the 3D world representation of multi-camera features through the information contained in a LiDAR-based 3DOD teacher. Finally, we also employ this teacher for cross-modal output distillation (X-OD), providing dense supervision at the prediction stage. We perform extensive ablations of knowledge distillation at different stages of multi-camera 3DOD. Our final X3KD model outperforms previous state-of-the-art approaches on the nuScenes and
Waymo datasets and generalizes to RADAR-based 3DOD.
Qualitative results video at https://youtu.be/1do9DPFmr38. 1.

Introduction 3D object detection (3DOD) is an essential task in vari-ous real-world computer vision applications, especially au-tonomous driving. Current 3DOD approaches can be cate-gorized by their utilized input modalities, e.g., camera im-ages [28, 40, 46] or LiDAR point clouds [25, 55, 60], which dictates the necessary sensor suite during inference. Re-cently, there has been significant interest in surround-view
*These authors contributed equally to this work.
†Automated Driving, Qualcomm Technologies, Inc.
‡Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.
§Automated Driving, QT Technologies Ireland Limited
Figure 1. While previous approaches considered multi-camera 3DOD in a standalone fashion or with depth supervision, we pro-pose X3KD, a knowledge distillation framework using cross-modal and cross-task information by distilling information from
LiDAR-based 3DOD and instance segmentation teachers into dif-ferent stages (marked by red arrows) of the multi-camera 3DOD. multi-camera 3DOD, aiming to leverage multiple low-cost monocular cameras, which are conveniently embedded in current vehicle designs in contrast to expensive LiDAR scanners. Existing solutions to 3DOD are mainly based on extracting a unified representation from multiple cameras
[28,30,37,41] such as the bird’s-eye view (BEV) grid. How-ever, predicting 3D bounding boxes from 2D perspective-view (PV) images involves an ambiguous 2D to 3D transfor-mation without depth information, which leads to lower per-formance compared to LiDAR-based 3DOD [1, 28, 30, 55].
While LiDAR scanners may not be available in commer-cially deployed vehicle fleets, they are typically available in training data collection vehicles to facilitate 3D annotation.
Therefore, LiDAR data is privileged; it is often available
Model
LSS++
DS
GFLOPS mAP↑
NDS↑ (X-FD and X-AT) and output-stage (X-OD).
BEVDepth†
X3KD (Ours)
✗
✗
✓
✓
✓
✗
✓
✗
✓
✓ 298 298 316 316 316 32.4 33.1 34.9 35.9 39.0 44.9 44.9 47.0 47.2 50.5
Table 1. Analysis of BEVDepth† (re-implementation of [28]):
We compare the architectural improvement of a larger Lift-Splat-Shoot (LSS++) transform to using depth supervision (DS). during training but not during inference. The recently intro-duced BEVDepth [28] approach pioneers using accurate 3D information from LiDAR data at training time to improve multi-camera 3DOD, see Fig. 1 (top part). Specifically, it proposed an improved Lift-Splat-Shoot PV-to-BEV trans-form (LSS++) and depth supervision (DS) by projected Li-DAR points, which we analyze in Table 1. We observe that the LSS++ architecture yields significant improvements, though depth supervision seems to have less effect. This motivates us to find additional types of supervision to trans-fer accurate 3D information from LiDAR point clouds to multi-camera 3DOD. To this end, we propose cross-modal knowledge distillation (KD) to not only use LiDAR data but a high-performing LiDAR-based 3DOD model, as in Fig. 1 (middle part). To provide an overview of the effectiveness of cross-modal KD at various multi-camera 3DOD network stages, we present three distillation techniques: feature dis-tillation (X-FD) and adversarial training (X-AT) to improve the feature representation by the intermediate information contained in the LiDAR 3DOD model as well as output dis-tillation (X-OD) to enhance output-stage supervision.
For optimal camera-based 3DOD, extracting useful PV features before the view transformation to BEV is equally essential. However, gradient-based optimization through an ambiguous view transformation can induce non-optimal su-pervision signals. Recent work proposes pre-training the
PV feature extractor on instance segmentation to improve the extracted features [49]. Nevertheless, neural networks are subject to catastrophic forgetting [23] such that knowl-edge from pre-training will continuously degrade if not re-tained by supervision. Therefore, we propose cross-task in-stance segmentation distillation (X-IS) from a pre-trained instance segmentation teacher into a multi-camera 3DOD model, see Fig. 1 (bottom part). As shown in Table 1, our
X3KD framework significantly improves upon BEVDepth without additional complexity during inference.
To summarize, our main contributions are as follows:
• We propose X3KD, a KD framework across modali-ties, tasks, and stages for multi-camera 3DOD.
• Specifically, we introduce cross-modal KD from a strong LiDAR-based 3DOD teacher to the multi-camera 3DOD student, which is applied at multiple network stages in bird’s eye view, i.e., feature-stage
• Further, we present cross-task instance segmentation distillation (X-IS) at the PV feature extraction stage.
• X3KD outperforms previous approaches for multi-camera 3DOD on the nuScenes and Waymo datasets.
• We transfer X3KD to RADAR-based 3DOD and train
X3KD only through KD without using ground truth.
• Our extensive ablation studies on nuScenes and
Waymo provide a comprehensive evaluation of KD at different network stages for multi-camera 3DOD. 2.