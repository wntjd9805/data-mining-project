Abstract
Effective modeling of complex spatiotemporal dependencies in long-form videos remains an open problem. The recently proposed Structured State-Space Sequence (S4) model with its linear complexity offers a promising direction in this space. However, we demonstrate that treating all image-tokens equally as done by S4 model can adversely affect its efficiency and accuracy. To address this limitation, we present a novel Selective S4 (i.e., S5) model that employs a lightweight mask generator to adaptively select infor-mative image tokens resulting in more efficient and accu-rate modeling of long-term spatiotemporal dependencies in videos. Unlike previous mask-based token reduction meth-ods used in transformers, our S5 model avoids the dense self-attention calculation by making use of the guidance of the momentum-updated S4 model. This enables our model to efficiently discard less informative tokens and adapt to various long-form video understanding tasks more effec-tively. However, as is the case for most token reduction methods, the informative image tokens could be dropped in-correctly. To improve the robustness and the temporal hori-zon of our model, we propose a novel long-short masked contrastive learning (LSMCL) approach that enables our model to predict longer temporal context using shorter in-put videos. We present extensive comparative results using three challenging long-form video understanding datasets (LVU, COIN and Breakfast), demonstrating that our ap-proach consistently outperforms the previous state-of-the-art S4 model by up to 9.6% accuracy while reducing its memory footprint by 23%. 1.

Introduction
Video understanding is an active research area where a variety of different models have been explored including e.g., two-stream networks [19, 20, 52], recurrent neural net-works [3, 63, 72] and 3-D convolutional networks [59–61].
However, most of these methods have primarily focused on short-form videos that are typically with a few seconds in length, and are not designed to model the complex long-Illustration of long-form videos – Evenly sampled
Figure 1. frames from two long-form videos, that have long duration (more than 1 minute) and distinct categories in the Breakfast [36] dataset (grayscale frames are shown for better visualization). The video on top shows the activity of making scrambled eggs, while the one on the bottom shows the activity of making cereal. These two videos heavily overlap in terms of objects (e.g., eggs, saucepan and stove), and actions (e.g., picking, whisking and pouring). To effectively distinguish these two videos, it is important to model long-term spatiotemporal dependencies, which is also the key in long-form video understanding. term spatiotemporal dependencies often found in long-form videos (see Figure 1 for an illustrative example). The re-cent vision transformer (ViT) [14] has shown promising ca-pability in modeling long-range dependencies, and several variants [1,4,15,41,45,49,65] have successfully adopted the transformer architecture for video modeling. However, for a video with T frames and S spatial tokens, the complexity of standard video transformer architecture is O(S2T2), which poses prohibitively high computation and memory costs when modeling long-form videos. Various attempts [54,68] have been proposed to improve this efficiency, but the ViT pyramid architecture prevents them from developing long-term dependencies on low-level features.
In addition to ViT, a recent ViS4mer [29] method has tried to apply the Structured State-Spaces Sequence (S4) model [23] as an effective way to model the long-term video dependencies. However, by introducing simple mask-ing techniques we empirically reveal that the S4 model can have different temporal reasoning preferences for different downstream tasks. This makes applying the same image to-ken selection method as done by ViS4mer [29] for all long-form video understanding tasks suboptimal.
To address this challenge, we propose a cost-efficient adaptive token selection module, termed S5 (i.e., selective
S4) model, which adaptively selects informative image to-kens for the S4 model, thereby learning discriminative long-form video representations. Previous token reduction meth-ods for efficient image transformers [37, 42, 50, 66, 70, 71] heavily rely on a dense self-attention calculation, which makes them less effective in practice despite their theoret-ical guarantees about efficiency gains. In contrast, our S5 model avoids the dense self-attention calculation by lever-aging S4 features in a gumble-softmax sampling [30] based mask generator to adaptively select more informative im-age tokens. Our mask generator leverages S4 feature for its global sequence-context information and is further guided by the momentum distillation from the S4 model.
To further improve the robustness and the temporal pre-dictability of our S5 model, we introduce a novel long-short mask contrastive learning (LSMCL) to pre-train our model.
In LSMCL, randomly selected image tokens from long and short clips include the scenario that the less informative im-age tokens are chosen, and the representation of them are learned to match each other. As a result, the LSMCL not only significantly boosts the efficiency compared to the pre-vious video contrastive learning methods [17, 51, 64], but also increases the robustness of our S5 model when deal-ing with the mis-predicted image tokens. We empirically demonstrate that the S5 model with LSMCL pre-training can employ shorter-length clips to achieve on-par perfor-mance with using longer-range clips without incorporating
LSMCL pre-training.
We summarize our key contributions as the following:
• We propose a Selective S4 (S5) model that leverages the global sequence-context information from S4 features to adaptively choose informative image tokens in a task-specific way.
• We introduce a novel long-short masked contrastive learn-ing approach (LSMCL) that enables our model to be tol-erant to the mis-predicted tokens and exploit longer dura-tion spatiotemporal context by using shorter duration input videos, leading to improved robustness in the S5 model.
• We demonstrate that two proposed novel techniques (S5 model and LSMCL) are seamlessly suitable and effective for long-form video understanding, achieving the state-of-the-art performance on three challenging benchmarks. No-tably, our method achieves up to 9.6% improvement on
LVU dataset compared to the previous state-of-the-art S4 method, while reducing the memory footprint by 23%. 2.