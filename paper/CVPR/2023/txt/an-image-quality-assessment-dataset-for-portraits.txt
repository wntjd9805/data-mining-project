Abstract
Year after year, the demand for ever-better smartphone photos continues to grow, in particular in the domain of portrait photography. Manufacturers thus use perceptual quality criteria throughout the development of smartphone cameras. This costly procedure can be partially replaced by automated learning-based methods for image quality as-sessment (IQA). Due to its subjective nature, it is necessary to estimate and guarantee the consistency of the IQA pro-cess, a characteristic lacking in the mean opinion scores (MOS) widely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA) datasets pay little attention to the difficulty of cross-content assessment, which may de-grade the quality of annotations. This paper introduces
PIQ23, a portrait-specific IQA dataset of 5116 images of 50 predefined scenarios acquired by 100 smartphones, cov-ering a high variety of brands, models, and use cases. The dataset includes individuals of various genders and ethnic-ities who have given explicit and informed consent for their photographs to be used in public research. It is annotated by pairwise comparisons (PWC) collected from over 30 im-age quality experts for three image attributes: face detail preservation, face target exposure, and overall image qual-ity. An in-depth statistical analysis of these annotations allows us to evaluate their consistency over PIQ23. Fi-nally, we show through an extensive comparison with ex-isting baselines that semantic information (image context) can be used to improve IQA predictions. The dataset along with the proposed statistical analysis and BIQA algorithms are available: https://github.com/DXOMARK-Research/PIQ2023 1.

Introduction
Social media has made smartphones a vital tool for con-necting with people worldwide. Visual media, particularly portrait photography, has become a crucial aspect of shar-ing content on these platforms. Portrait photography serves numerous applications (e.g., advertisements, social media) and use cases (e.g., anniversaries, weddings). Capturing a high-quality portrait is a complex exercise that demands careful consideration of multiple factors, such as scene se-mantics, compositional rules, image quality, and other sub-jective properties [46].
Smartphone manufacturers strive to deliver the best vi-sual quality while minimizing production costs to rival pro-fessional photography. Achieving this requires implement-ing complex tuning and optimization protocols to calibrate image quality in smartphone cameras. These cameras intro-duce sophisticated non-linear processing techniques such as multi-image fusion or deep learning-based image enhance-ment [55], resulting in a combination of authentic (realis-tic) camera distortions. This makes traditional objective quality assessment [4, 16, 32, 40] that models digital cam-eras as linear systems unreliable [9]. Therefore, in addi-tion to objective measurements, the tuning process also in-cludes perceptual evaluations where cameras are assessed by image quality experts. This procedure requires shoot-ing and evaluating thousands of use cases, which can be costly, time-consuming, and challenging to reproduce. Au-tomatic image quality assessment (IQA) methods that try to mimic human perception of quality have been around for many years, in order to help in the tuning process [14, 36, 37, 39, 48, 57, 60, 64]. Blind IQA (BIQA), in particular, is a branch of IQA where image quality is evaluated without the need for undistorted reference images. Learning-based
BIQA methods [15, 24, 25, 27, 52, 59, 62, 67, 69] have shown good performance on authentic camera distortion datasets
[9, 13, 21, 56, 61, 70], annotated by subjective assessment of image quality. Annotating these datasets is considered an ill-posed problem, as the subjective opinions are not de-terministic, making it challenging to use BIQA methods as accurate quality measures. Therefore, there is a need to de-velop a quantitative and formal framework to evaluate and compare subjective judgments in an objective manner. In this paper, we rely on pairwise comparisons performed by image quality experts along a fixed and relevant set of at-tributes.
Multiple attributes, including target exposure, dynamic
Figure 1. (a) Scenes from the PIQ23 dataset. (b) Examples of the region of interest (ROI) used for different attribute comparisons. Top: overall quality; we use a resized version of the full image. Bottom: details & target exposure; we use an upscaled face area. (a) (b) range, color, sharpness, noise, and artifacts, define image quality [3]. Portrait images require additional considera-tions, such as skin tone, bokeh effect, face detail rendering, and target exposure on the face, which fall under the scope of portrait quality assessment (PQA) [40].
To the best of our knowledge, the problem of assessing the quality of a portrait image has received limited atten-tion. Most of the work on face IQA [49] has been directed towards improving face recognition systems and not as an independent topic. As far as we know, our paper introduces the first-of-its-kind, smartphone portrait quality dataset. We hope to create a new domain of application for IQA and to push forward smartphone portrait photography. Our contri-butions are the following:
• A new dataset, PIQ23, consisting of 5116 single por-trait images, taken using 100 smartphone devices from 14 brands, and distributed across 50 different natural scenes (scene = fixed visual content). We have ad-dressed the ethical challenges involved in creating such a dataset, by obtaining from each individual depicted in the dataset a signed and informed agreement, mak-ing it the only IQA dataset with such legal and ethical characteristics, as far as we know.
• A large IQA experiment controlled in a laboratory en-vironment with fixed viewing conditions. Using pair-wise comparisons (PWC) and following carefully de-signed guidelines, we gather opinions for each scene, from over 30 image quality experts (professional pho-tographers and image quality experts) on three at-tributes related to portrait quality: face detail preser-vation, face target exposure, and overall portrait image quality.
• An in-depth statistical analysis method that allows us to evaluate the precision and consistency of the labels as well as the difficulty of the IQA task. This is par-ticularly important given the fact that image quality la-bels are heavily affected by subjectivity, disagreement between observers, and the number of annotations.
• An extensive comparison between multiple BIQA models and a simple new method combining scene se-mantic information with quality features to strengthen image quality prediction on PIQ23. 2.