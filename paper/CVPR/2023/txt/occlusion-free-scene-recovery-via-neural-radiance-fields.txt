Abstract
Our everyday lives are filled with occlusions that we strive to see through. By aggregating desired background information from different viewpoints, we can easily elim-inate such occlusions without any external occlusion-free supervision. Though several occlusion removal methods have been proposed to empower machine vision systems with such ability, their performances are still unsatisfactory due to reliance on external supervision. We propose a novel method for occlusion removal by directly building a mapping between position and viewing angles and the corresponding occlusion-free scene details leveraging Neu-ral Radiance Fields (NeRF). We also develop an effective scheme to jointly optimize camera parameters and scene reconstruction when occlusions are present. An additional depth constraint is applied to supervise the entire optimiza-tion without labeled external data for training. The exper-imental results on existing and newly collected datasets validate the effectiveness of our method. Our project page: https://freebutuselesssoul.github.io/occnerf. 1.

Introduction
Neural Radiance Fields (NeRF) are capable of learning the scene representation implicitly from a set of 2D images, yet not every scene is favored by observers. Many unde-sirable occlusions in our world obscure details that are es-sential to our understanding of the world. In general, such obstructions range from water droplets and scribbles on a piece of glass, to fences or any objects occluding the de-sired scenes (e.g., a statue closer to the camera in a land-mark scene). How to apply computational methods to ex-clude them from the scene representation is of great interest.
Occlusion removal (e.g., [29]) is the direct solution to achieve this goal. However, explicit occlusion removal may
*Corresponding author. oversmooth essential details necessary for clearly observ-ing the desired background scenes.
In addition, current methods mainly depend on external occlusion-free super-vision (e.g., fence removal [4], raindrop removal [22]) to develop the reliable capability in removing certain types of occlusions. Once encountering a new scenario with unseen occlusion types beyond their training data, these methods might show degraded performances. To handle more di-verse types of occlusions, generic constraints from multiple viewpoints are widely adopted [4, 10, 12, 14, 19, 27, 29] via mimicking our human vision systems, who can easily piece together the desired background scenes by looking at them from different viewpoints. But the majority of these meth-ods just consider viewpoints as a prior in relation to spatial correlations. Their backbones still rely on external train-ing data with corresponding ground truth for optimization, which still does not fundamentally alleviate the difficulty of handling diverse occlusions in the real world.
An occlusion-free world can be progressively aggregated by seeing its occluded part from different viewing directions to reveal occlusions previously unobservable in each sin-gle perspective, as illustrated in the left part of Fig. 1 (the fan is occluded in the target view). Since NeRF [18] em-ploys an implicit representation to map viewpoints to pix-els, one may come to the naive solution of directly con-structing a NeRF which is optimized across multiple view-points. However, the vanilla NeRF [18] representing the scene as a whole is not able to treat occlusion and back-ground scenes distinctively, and as long as the occlusion remains static, NeRF is designed to faithfully reconstruct its presence. Meanwhile, many NeRF variants can de-compose the whole scene into different components (e.g.,
NeRF-W [17], Ha-NeRF [3], NeRFReN [7]), but they can-not handle the real-world static occlusions. This is because
NeRF-W [17] and Ha-NeRF [3] rely on the inconsistency of undesired components across different views to achieve such separation, which is difficult to be observed in a con-tinuous 3D world. On the other hand, NeRFReN [7] only works in separating the transmission and reflection compo-nents caused by semi-transparent planar glass, which is in-capable of handling opaque occlusions in the real world.
Another problem comes from NeRF’s reliance on cam-era parameters pre-computed by COLMAP [23], be-cause handcrafted features extracted and matched using
COLMAP [23] are for the whole scene, and are inca-pable in distinguishing between undesired occlusions and the desired background. When the features from occlu-sion dominate the matching process, the obtained camera parameters cannot faithfully model the spatial correlation of background scenes across multiple viewpoints. Besides,
COLMAP [23] is not a stable option for pose estimation in the real world [28]. The existence of occlusions may pre-vent it from working properly, making the occlusion-free scene representation infeasible.
In this paper, we aim at seeing through the occluded scenes by developing an occlusion-free scene representa-tion without considering specific occlusion types, based on which we can render any occlusion-free images from de-sired viewpoints. Our method first maps viewing angles and their corresponding scene details by leveraging NeRF.
We then introduce a depth constraint to probe the occluded areas by measuring the depth of occlusion and background, by assuming that occlusions are always in the foreground with closer distance. During the scene modeling process, a pose refinement scheme is further introduced to refine the camera pose with the features of the background scene. As outlined in Fig. 1, our pipeline contains three modules to achieve the above goals: 1) a scene reconstruction module to represent the whole scene using NeRF (with occlusions), 2) a cost volume construction module to gather information from neighboring views as guidance (to indicate where oc-clusions are), and 3) a selective supervision scheme to con-strain another NeRF on the desired background information (occlusions removed), and our contributions can be summa-rized as follows:
• an occlusion-free representation without relying on any external prior as supervisory knowledge;
• a joint optimization of pose refinement and scene re-construction by effective multi-view feature fusion;
• a selective supervision scheme to probe the occluded areas guided by the scene depth information.
Based on the experiments with a dataset containing diverse types of occlusions, the proposed method can eliminate oc-clusions including scribbles and water droplets on a piece of glass, fences, and even irregular-shaped statues without relying on any external supervisions. 2.