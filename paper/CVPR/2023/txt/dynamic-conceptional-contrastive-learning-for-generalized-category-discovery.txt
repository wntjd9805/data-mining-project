Abstract
Generalized category discovery (GCD) is a recently pro-posed open-world problem, which aims to automatically cluster partially labeled data. The main challenge is that the unlabeled data contain instances that are not only from known categories of the labeled data but also from novel categories. This leads traditional novel category discov-ery (NCD) methods to be incapacitated for GCD, due to their assumption of unlabeled data are only from novel categories. One effective way for GCD is applying self-supervised learning to learn discriminate representation for unlabeled data. However, this manner largely ignores un-derlying relationships between instances of the same con-cepts (e.g., class, super-class, and sub-class), which re-In this paper, sults in inferior representation learning. we propose a Dynamic Conceptional Contrastive Learn-ing (DCCL) framework, which can effectively improve clus-tering accuracy by alternately estimating underlying vi-sual conceptions and learning conceptional representation.
In addition, we design a dynamic conception generation and update mechanism, which is able to ensure consis-tent conception learning and thus further facilitate the opti-mization of DCCL. Extensive experiments show that DCCL achieves new state-of-the-art performances on six generic and fine-grained visual recognition datasets, especially on fine-grained ones. For example, our method significantly surpasses the best competitor by 16.2% on the new classes for the CUB-200 dataset. Code is available at https:
//github.com/TPCD/DCCL 1.

Introduction
Learning recognition models (e.g., image classification) from labeled data has been widely studied in the field of
In spite machine learning and deep learning [13, 18, 31]. of their tremendous success, supervised learning techniques
*Corresponding Author.
Figure 1. Diagram of the proposed Dynamic Conceptional Con-trastive Learning (DCCL). Samples from the conceptions should be close to each other. For example, samples from the same classes (bus) at the class level, samples belonging to the transportation (bus and bicycle) at the super-class level, and samples from trains with different colors at the sub-class level. Our DCCL potentially learns the underlying conceptions in unlabeled data and produces more discriminative representations. rely heavily on huge annotated data, which is not suit-able for open-world applications. Thus, the researchers recently have paid much effort on learning with label-imperfection data, such as semi-supervised learning [23, 33], self-supervised learning [12, 42], weakly-supervised learning [41,45], few-shot learning [32,38], open-set recog-nition [30] and learning with noisy labels [40], etc.
Recently, inspired by the fact that Humans can easily and automatically learn new knowledge with the guidance of previously learned knowledge, novel category discov-ery (NCD) [9, 11, 28, 44, 47] is introduced to automatically cluster unlabeled data of unseen categories with the help of knowledge from seen categories. However, the implemen-tation of NCD is under a strong assumption that all the un-labeled instances belong to unseen categories, which is not practical in real-world applications. To address this limi-tation, Vaze et al. [35] extend NCD to the generalized cat-egory discovery (GCD) [35], where unlabeled images are
from both novel and labeled categories.
GCD is a challenging open-world problem in that we need to 1) jointly distinguish the known and unknown classes and 2) discover the novel clusters without any anno-tations. To solve this problem, Vaze et al. [35] leverage the contrastive learning technique to learn discriminative repre-sentation for unlabeled data and use k-means [21] to obtain final clustering results. In this method, the labeled data are fully exploited by supervised contrastive learning. How-ever, self-supervised learning is applied to the unlabeled data, which enforces samples to be close to their augmen-tation counterparts while far away from others. As a con-sequence, the underlying relationships between samples of the same conceptions are largely overlooked and thus will lead to degraded representation learning. Intuitively, sam-ples that belong to the same conceptions should be similar to each other in the feature space. The conceptions can be regarded as: classes, super-classes, sub-classes, etc. For ex-ample, as shown in Fig. 1, samples of the same class should be similar to each other, e.g., samples of the bus, samples of the bicycle. In addition, in the super-classes view, classes of the transportation, e.g., Bus and Bicycle, should belong to the same concept. Hence, the samples of transportation should be closer than that of other concepts (e.g., animals).
Similarly, samples belong to the same sub-classes (e.g., red train) should be closer to that of other sub-classes (e.g., white train). Hence, embracing such conceptions and their relationships can greatly benefit the representation learning for unlabeled data, especially for unseen classes.
Motivated by this, we propose a Dynamic Conceptional
Contrastive Learning (DCCL) framework for GCD to ef-fectively leverage the underlying relationships between un-labeled data for representation learning. Specifically, our
DCCL includes two steps: Dynamic Conception Genera-tion (DCG) and Dual-level Contrastive Learning (DCL). In
DCG, we dynamically generate conceptions based on the hyper-parameter-free clustering method equipped with the proposed semi-supervised conceptional consolidation.
In
DCL, we propose to optimize the model with conception-level and instance-level contrastive learning objectives, where we maintain a dynamic memory to ensure compar-ing with the up-to-date conceptions. The DCG and DCL are alternately performed until the model converges.
We summarize the contributions of this work as follows:
• We propose a novel dynamic conceptional contrastive learning (DCCL) framework to effectively leverage the underlying relationships between unlabeled samples for learning discriminative representation for GCD.
• We introduce a novel dynamic conception generation and update mechanism to ensure consistent conception learning, which encourages the model to produce more discriminative representation.
• Our DCCL approach consistently achieves superior performance over state-of-the-art GCD algorithms on both generic and fine-grained tasks. 2.