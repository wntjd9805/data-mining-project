Abstract
To build Video Question Answering (VideoQA) systems capable of assisting humans in daily activities, seeking answers from long-form videos with diverse and complex events is a must. Existing multi-modal VQA models achieve promising performance on images or short video clips, especially with the recent success of large-scale multi-modal pre-training. However, when extending these meth-ods to long-form videos, new challenges arise. On the one hand, using a dense video sampling strategy is com-putationally prohibitive. On the other hand, methods rely-ing on sparse sampling struggle in scenarios where multi-event and multi-granularity visual reasoning are required.
In this work, we introduce a new model named M ulti-modal Iterative Spatial-temporal Transformer (MIST) to better adapt pre-trained models for long-form VideoQA.
Specifically, MIST decomposes traditional dense spatial-temporal self-attention into cascaded segment and region selection modules that adaptively select frames and image regions that are closely relevant to the question itself. Vi-sual concepts at different granularities are then processed efficiently through an attention module. In addition, MIST iteratively conducts selection and attention over multiple layers to support reasoning over multiple events. The exper-imental results on four VideoQA datasets, including AGQA,
NExT-QA, STAR, and Env-QA, show that MIST achieves state-of-the-art performance and is superior at efficiency.
The code is available at github.com/showlab/mist. 1.

Introduction
One of the ultimate goals of Video Question Answering (VideoQA) systems is to assist people in solving problems in everyday life [13, 27, 41], e.g., helping users find some-thing, reminding them what they did, and assisting them while accomplishing complex tasks, etc. To achieve such
*Currently at Google Brain.
†Corresponding author.
Figure 1. Main challenges of long-form VideoQA. The ques-tions for long-form VideoQA usually involve multi-event, multi-grained, and causality reasoning. functions, the systems should be able to understand and seek the answer from long-form videos with diverse events about users’ activities.
Compared to understanding and reasoning over short videos, many unique challenges arise when the duration of the video increases, as shown in Fig. 1: 1) Multi-event rea-soning. The long-form videos usually record much more events. The questions about these videos thus naturally re-quire the systems to perform complex temporal reasoning, e.g., multi-event reasoning (Q1 in Fig. 1), causality (Q3), etc. 2) Interactions among different granularities of visual concepts. The questions of short-clip videos usually involve the interactions of objects or actions that happened simulta-neously, while questions for long-form videos could involve more complex interactions of objects, relations, and events across different events, e.g., Q2 in Fig. 1.
Current vision-language methods [2, 7, 10, 24, 29, 31, 32, 51, 52] excel at QA over images or short clips span-ning several seconds. In other words, they excel at learn-ing multi-modal correspondences between a single cap-tion with one or few events. Their tremendous progress over these years is fueled by 1) pre-training on large-scale image-language [22, 37, 38] and short-clip-language
shown in Fig. 2. MIST comes from a simple finding that for long-form VideoQA, it is not necessary to consider the details of all events in a video, like what dense self-attention over all patches do. The model only needs to consider the general content of all events and focuses on the de-tails of a few question-related events. Thus, MIST de-composes dense joint spatial-temporal self-attention into a question-conditioned cascade segment and region selection module along with a spatial-temporal self-attention over multi-modal multi-grained features. The cascade selection reduces the computation cost and benefits the performance by focusing on the question-related segments and regions.
The self-attention over segments and image patches, bet-ter captures interactions among different granularities of vi-sual concepts. In addition, through iteratively conducting selection and self-attention, MIST can reason over multiple events and better perform temporal and causal reasoning.
We conduct experiments on several VideoQA datasets with relatively longer videos, AGQA [14], NExT-QA [44],
STAR [42], and Env-QA [11], with an average video du-ration varies from 12s to 44s. The experimental results show that our approach achieves state-of-the-art perfor-mance. Further ablation studies verify the effectiveness of the key components. Moreover, quantitative and qualitative results also show that our method provides higher efficiency and reasonable evidence for answering questions. 2.