Abstract
Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and ﬁnetuning. As an alternative to large models, we present SMALLCAP, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly in-troduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SMALLCAP can transfer to new domains without additional ﬁnetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SMALLCAP, trained only on COCO, has com-petitive performance on this benchmark, and also trans-fers to other domains without retraining, solely through re-trieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, which proves to be effective for a range of domains, including the nocaps benchmark, designed to test generalization to unseen visual concepts.1 1.

Introduction
The state-of-the-art in image captioning is deﬁned by in-creasingly large-scale models trained on increasingly large-scale datasets [11, 18, 39, 42]. Scaling up leads to higher computational demands for model pre-training and ﬁnetun-ing on downstream tasks. This becomes especially relevant when numerous model versions may be needed for different visual domains [1] and end-users in practical applications, e.g. image captioning for the visually impaired [10].
Some efforts have been made recently to reduce the cost of model training, e.g., ClipCap [25] and I-Tuning [22]. 1Code: https://github.com/RitaRamo/smallcap. (cid:54)(cid:50)(cid:55)(cid:36)(cid:3)(cid:47)(cid:68)(cid:85)(cid:74)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86) (cid:53)(cid:72)(cid:70)(cid:72)(cid:81)(cid:87)(cid:3)(cid:47)(cid:76)(cid:74)(cid:75)(cid:87)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86) (cid:38)(cid:85)(cid:82)(cid:86)(cid:86)(cid:16)(cid:68)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:71)(cid:76)(cid:80)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:76)(cid:87)(cid:92) (cid:39)(cid:72)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85)(cid:3)(cid:86)(cid:76)(cid:93)(cid:72) (cid:74) (cid:37)(cid:68)(cid:86)(cid:72) (cid:71)(cid:32)(cid:20)(cid:25) (cid:54)(cid:80)(cid:68)(cid:79)(cid:79)(cid:38)(cid:68)(cid:83) (cid:28)(cid:19) (cid:27)(cid:19) (cid:26)(cid:19) (cid:25)(cid:19) (cid:24)(cid:19) (cid:86) (cid:83) (cid:68) (cid:70) (cid:82) (cid:81) (cid:3) (cid:16) (cid:3) (cid:85) (cid:40) (cid:39) (cid:38) (cid:44) (cid:50)(cid:54)(cid:38)(cid:36)(cid:53) (cid:38)(cid:68)(cid:48)(cid:40)(cid:47) (cid:38)(cid:79)(cid:76)(cid:83)(cid:38)(cid:68)(cid:83) (cid:20) (cid:21) (cid:24) (cid:20)(cid:19) (cid:21) (cid:24) (cid:20)(cid:19)(cid:19) (cid:21) (cid:24) (cid:20)(cid:19)(cid:19)(cid:19) (cid:37)(cid:68)(cid:86)(cid:72) (cid:71)(cid:32)(cid:20)(cid:25) (cid:71)(cid:32)(cid:27) (cid:71)(cid:32)(cid:23) (cid:48)(cid:72)(cid:71)(cid:76)(cid:88)(cid:80) (cid:47)(cid:68)(cid:85)(cid:74)(cid:72) (cid:38)(cid:68)(cid:48)(cid:40)(cid:47) (cid:44)(cid:55)(cid:16)(cid:48)(cid:72)(cid:71)(cid:76)(cid:88)(cid:80) (cid:44)(cid:55)(cid:16)(cid:47)(cid:68)(cid:85)(cid:74)(cid:72) (cid:44)(cid:55)(cid:16)(cid:37)(cid:68)(cid:86)(cid:72) (cid:38)(cid:79)(cid:76)(cid:83)(cid:38)(cid:68)(cid:83) (cid:50) (cid:38) (cid:50) (cid:38) (cid:3) (cid:16) (cid:3) (cid:85) (cid:40) (cid:39) (cid:38) (cid:44) (cid:20)(cid:23)(cid:24) (cid:20)(cid:23)(cid:19) (cid:20)(cid:22)(cid:24) (cid:20)(cid:22)(cid:19) (cid:20)(cid:21)(cid:24) (cid:20)(cid:21)(cid:19) (cid:20)(cid:20)(cid:24) (cid:20)(cid:20)(cid:19) (cid:54)(cid:76)(cid:80)(cid:57)(cid:47)(cid:48) (cid:47)(cid:40)(cid:48)(cid:50)(cid:49) (cid:37)(cid:47)(cid:44)(cid:51) (cid:50)(cid:54)(cid:38)(cid:36)(cid:53) (cid:21) (cid:24) (cid:20)(cid:19) (cid:21) (cid:24) (cid:20)(cid:19)(cid:19) (cid:21) (cid:24) (cid:20)(cid:19)(cid:19)(cid:19) (cid:51)(cid:68)(cid:85)(cid:68)(cid:80)(cid:86)(cid:3)(cid:11)(cid:79)(cid:82)(cid:74)(cid:12)
Figure 1. SMALLCAP’s performance on the COCO dataset and on the out-of-domain split of the nocaps dataset, compared to other approaches in terms of number of trainable parameters. We can control the number of trainable parameters through the dimen-sionality of the cross-attention (d = dv = dk) and the size of the decoder. SMALLCAP is competitive to other lightweight models on COCO, and outperforms much larger models on nocaps.
These models use an off-the-shelf pre-trained vision en-coder and language decoder. The parameters of these pre-trained components are frozen and only a mapping between the two is trained for the task of image captioning. This results in a highly reduced number of trainable parameters ( 43M in each case) and faster training time. While these
⇠ models operate on a much more manageable scale from a research perspective, they can still be unsuitable for the aforementioned practical applications, as both models re-quire separate training for every use-case.
This work presents SMALLCAP, an image captioning model, prompted with captions retrieved from an external datastore of text, based on the input image. This formula-tion of image captioning enables a range of desirable prop-lightweight training, training-free domain transfer, erties: and exploitation of large data in a training-free fashion.
SMALLCAP is both light to train and highly effective (see Figure 1).2 It uses a pre-trained CLIP vision encoder
[29] and GPT-2 language model [31], which are frozen and linked through new cross-attention layers amounting to 7 million trainable parameters. Through retrieval, the model leverages external data and therefore has to store less in-formation within its weights (as demonstrated in Figure 6).
Trained on the common COCO benchmark [7], SMALLCAP performs on par with other lightweight-training models, de-spite an 83% reduction in number of trainable parameters.
SMALLCAP can also leverage data in a training-free manner. Once the model is trained, we can replace the datastore with either (i) captions from a new domain or (ii) a large and diverse collection of captions. In the ﬁrst case, which presents an alternative to ﬁnetuning, SMALL-CAP gains access to the style and concepts that charac-terize the new domain and can generate captions accord-ingly. In the second case, which presents an alternative to generalized pre-training, SMALLCAP gains access to gen-eral knowledge that it can apply to any domain. Our ex-periments show that SMALLCAP effectively leverages new knowledge accessed through a retrieval-based prompt, im-proving its performance on different datasets. This includes the challenging VizWiz dataset, where images are captioned for the visually impaired [10], and the nocaps challenge dataset with rarely-seen and unseen visual concepts [1].
SMALLCAP competes with other lightweight-training models on in-domain evaluations and outperforms them by a large margin out-of-domain. It overcomes a key limita-tion of previous models, which require explicit ﬁnetuning to adapt to new domains, and in this way attests to the po-tential of retrieval augmentation for multimodal tasks. 2.