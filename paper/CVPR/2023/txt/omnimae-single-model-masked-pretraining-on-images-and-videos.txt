Abstract benchmark, setting a new state-of-the-art.
Transformer-based architectures have become competi-tive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in iso-lation, having a common architecture suggests that one can train a single unified model for multiple visual modalities.
Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance com-pared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision
Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality repre-sentations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model ar-chitectures. In particular, we show that our single ViT-Huge model can be finetuned to achieve 86.6% on ImageNet and 75.5% on the challenging Something Something-v2 video
∗Equal technical contribution. 1.

Introduction
The Transformer architecture [78] is rapidly becoming competitive across the different visual modalities in Com-puter Vision, from images [24, 27, 55, 77], to 3D [57, 60, 89] and videos [2, 9, 27, 31, 32, 56]. This convergence toward a unified architecture naturally suggests that we should be able to train a single model that works across different vi-sual modalities. However, recent attempts to train unified models either lead to worse performance compared to single modality models [53], or require the use of an alternative architecture [33], namely the Swin Transformer [55], with inductive biases tailored towards vision tasks. While special-ized Transformer architectures for vision [27, 55, 56, 81] can offer better performance for visual modalities, they lose the generality and flexibility of the vanilla Transformer, making it harder to later model different domains like text, speech, 3D etc. in multi-modal architectures.
In this work, we train a single vanilla Transformer that works for both images and videos, as illustrated in Figure 1.
To this end, we leverage the findings of several recent works on the use of the masked pretraining [23] to greatly improve the training and performance of Transformers in the domain of images [6, 40, 80, 84], videos [76, 80, 82] or across text, audio and images [5]. We show that this masked pretraining is a viable strategy to pretrain a unified ‘omnivorous’ Trans-former across visual modalities. In particular, we consider the Masked Auto-Encoding (MAE) approach [40] to train an
Omnivorous visual encoder [33]. The resulting OmniMAE model learns from all the modalities with the same objective function and does not require any supervision.
Using a masked pretraining objective has several advan-tages over supervised objectives [33, 53] or discriminative self-supervised objectives [13, 17, 41]. First, as opposed to supervised objectives, a general-purpose unsupervised loss does not require any human labeling effort. As a result, it is robust to biases introduced by a predefined set of labels [35].
Moreover, it does not require a multi-head architecture to incorporate supervision from each of the label spaces cor-responding to each modality, which is hard to maintain and scale with new modalities. Second, although discriminative self-supervised methods produce superior frozen features compared to reconstruction objectives, they are non trivial to scale in model and data size [36]. Our masked pretraining objective is simple, efficient to train, and scales to different visual modalities with minimal changes.
Contributions. (1) We show that the simple Vision Trans-former architecture (ViT) [24] originally designed for images can naturally be applied on videos, and videos and images jointly. OmniMAE is a single ViT-based model for videos and images that outperforms architectures and models specif-ically designed for either modality. (2) Prior and concurrent work design self-supervised methods and architectures for either image or video and we find that these models do not transfer well across modalities. OmniMAE is the first single self-supervised model that achieves good performance on both modalities. (3) We show that our joint training using both images and videos enables us to use much higher mask-ing ratios than any prior work for training MAE. Since ViT can processes only the non-masked input, we train Omn-iMAE models with only 10% of image and 5% of video patches. This enables us to train large (650M parameter) models with a ∼ 7× and ∼ 11× reduction in compute and memory on images and videos. (4) Finally, we propose im-provements to the MAE training. We show that repeating samples in a mini-batch reduces dataloading (and thus train-ing) time without loss in final transfer performance. Sample replication is particularly useful for masked pretraining as the unmasked patches are different across sample replicas.
We also show that using a shallow shared decoder for videos and images leads to better performance while reducing the number of parameters by 2 − 4×. 2.