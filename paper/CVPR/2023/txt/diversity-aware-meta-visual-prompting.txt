Abstract
We present Diversity-Aware Meta Visual Prompt-ing (DAM-VP), an efficient and effective prompting method for transferring pre-trained models to downstream tasks with frozen backbone. A challenging issue in visual prompt-ing is that image datasets sometimes have a large data di-versity whereas a per-dataset generic prompt can hardly handle the complex distribution shift toward the original pretraining data distribution properly. To address this issue, we propose a dataset Diversity-Aware prompting strategy whose initialization is realized by a Meta-prompt. Specif-ically, we cluster the downstream dataset into small ho-mogeneity subsets in a diversity-adaptive way, with each subset has its own prompt optimized separately. Such a divide-and-conquer design reduces the optimization diffi-culty greatly and significantly boosts the prompting perfor-mance. Furthermore, all the prompts are initialized with a meta-prompt, which is learned across several datasets. It is a bootstrapped paradigm, with the key observation that the prompting knowledge learned from previous datasets could help the prompt to converge faster and perform bet-ter on a new dataset. During inference, we dynamically select a proper prompt for each input, based on the fea-ture distance between the input and each subset. Through extensive experiments, our DAM-VP demonstrates supe-rior efficiency and effectiveness, clearly surpassing previ-ous prompting methods in a series of downstream datasets for different pretraining models. Our code is available at: https://github.com/shikiw/DAM-VP. 1.

Introduction
With the increasing scale of training data and model size, the pretraining-finetuning paradigm has shown remarkable achievement in many areas, including natural language pro-cessing (NLP) [4,13] and computer vision (CV) [2,7,8,19].
*Corresponding author.
Figure 1. Relation between dataset diversity and the performance gain got by using prompting. The gain is the performance im-provement when compared with the linear-probing accuracy, un-der the head-tuning setting. Both previous methods get a large performance gain on low-diversity datasets, while failing to boost the transfer performance on high-diversity datasets.
However, fully finetuning a large pre-trained model for each small downstream task still has some problems in real-world usage. The most practical one is the storage and dis-tribution problem that we have to maintain an independent copy of the model for each task, which is quite expensive and inflexible, especially for increasing numbers of down-stream tasks [9].
To break the dilemma, many efforts [6,17,18,25,51] have been paid to efficiently transfer the given pre-trained models into a particular dataset. Prompting is an extensively studied method in the NLP area, which appends a few tokens before the input sequence to provide some task-specific knowledge to the pre-trained model, so that the model could adapt well on the downstream tasks without the fully-finetuning. In-spired by the success of prompting in NLP, some recent works [1, 26] propose visual prompting for vision models.
By adding some learnable noise onto the input image or ap-pending some learnable tokens to the model input sequence, the pre-trained models show promising results on different kinds of downstream tasks.
However, we argue that these methods ignore the diverse distribution property of the image dataset and using a sin-gle prompt for all the images in each dataset is not opti-mal. In Figure 1, we show the relationship between the gain from prompting and the diversity of the dataset. Here the gain represents the accuracy improvement compared with the linear probing setting. We find that both VP [1] and
VPT [26] improve the model accuracy by a large margin on the low-diversity dataset, but relatively small gains on the high-diversity datasets, which is intuitively sensible. For low-diversity datasets, such as the street view house num-ber dataset (SVHN) [37], all the images have similar content so a unified prompt is sufficient. On the contrary, when it comes to high-diversity datasets, such as the ImageNet [12] dataset, it covers very diverse classes from the wordnet and there is not any pre-defined relationship between the classes, so it is hard to use a single prompt to provide the prior for all the images, such as for “car” and “dog”.
Motivated by this observation, we propose our Diversity-Aware Meta Visual Prompting (DAM-VP). It has two core designs. Firstly, to provide a proper prompt for each image from high-diversity datasets, we propose a clustering-based prompt selection method. In detail, given a pre-trained vi-sual model and a downstream dataset, we use the off-the-shelf clustering method to cluster the feature of the down-stream data into several coarse-grained subsets, and guide each cluster to learn its own prompt separately. Based on the strong homogeneity of the same clustered data, the opti-mization of cluster-specific visual prompts can be greatly facilitated and the data commonalities can be also easily covered. Secondly, we argue the prompt across different clusters or datasets may have some shared pattern, from which the model can be adapted to a new dataset faster and get better performance. This motivates us to introduce a meta-learning-based method that learns a meta prompt and initializes the prompt of each cluster with it.
We conduct our experiments on datasets with different data diversity and evaluate the transfer performance with different pre-trained models. We report the performance on both the widely used head-tuning setting and a more chal-lenging head-freezing/missing setting. Our DAM-VP out-performs previous methods by a large margin, especially on high-diversity datasets. For example, with the ImageNet-22k pre-trained ViT-B model, DAM-VP gets 73.1% top-1 accuracy under the head-tuning setting on the diverse
DTD [10] dataset, surpassing previous methods VP [1] and
VPT [26] with +13.6% and +7.3% respectively. Mean-while, we find DAM-VP is quite efficient that with only 10 epoch tuning, it gets 85.7% average top-1 accuracy over the 10 datasets, comparable with previous methods that tunes 100 epochs (83.4% for VP [1] and 85.5% for VPT [26]).
Our contributions can be summarized as follows:
• We analyze the limitation of previous visual prompting methods, and point out that vision-suitable prompting should consider the dataset diversity.
• Accordingly, we propose a novel Diversity-Aware
Meta Visual Prompting (DAM-VP) method. It uses the divide-and-conquer idea by clustering high-diversity datasets into subsets and learning separate prompts for each subset, in cooperation with a meta-prompt learn-ing design.
• Through extensive experiments, our DAM-VP demon-strates superior performance, achieving SOTA perfor-mance in a series of downstream datasets for different pretraining models. 2.