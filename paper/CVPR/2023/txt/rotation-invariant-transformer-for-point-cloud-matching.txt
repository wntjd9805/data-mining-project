Abstract
The intrinsic rotation invariance lies at the core of matching point clouds with handcrafted descriptors. How-ever, it is widely despised by recent deep matchers that obtain the rotation invariance extrinsically via data aug-mentation. As the ﬁnite number of augmented rotations can never span the continuous SO(3) space, these meth-ods usually show instability when facing rotations that are rarely seen. To this end, we introduce RoITr, a Rotation-Invariant Transformer to cope with the pose variations in the point cloud matching task. We contribute both on the local and global levels. Starting from the local level, we introduce an attention mechanism embedded with Point
Pair Feature (PPF)-based coordinates to describe the pose-invariant geometry, upon which a novel attention-based encoder-decoder architecture is constructed. We further propose a global transformer with rotation-invariant cross-frame spatial awareness learned by the self-attention mech-anism, which signiﬁcantly improves the feature distinctive-ness and makes the model robust with respect to the low overlap. Experiments are conducted on both the rigid and non-rigid public benchmarks, where RoITr outperforms all the state-of-the-art models by a considerable margin in the low-overlapping scenarios. Especially when the rotations are enlarged on the challenging 3DLoMatch benchmark,
RoITr surpasses the existing methods by at least 13 and 5 percentage points in terms of Inlier Ratio and Registration
Recall, respectively. Code is publicly available 1. 1.

Introduction
The correspondence estimation between a pair of partially-overlapping point clouds is a long-standing task that lies at the core of many computer vision applica-tions, such as tracking [17, 18], reconstruction [22, 23, 39], pose estimation [19, 27, 50] and 3D representation learn-ing [15, 16, 46], etc. In a typical solution, geometry is ﬁrst encoded into descriptors, and correspondences are then es-tablished between two frames by matching the most similar descriptors. As the two frames are observed from different views, depicting the same geometry under different trans-1https://github.com/haoyu94/RoITr
Figure 1. Feature Matching Recall (FMR) on 3DLoMatch [19] and Rotated 3DLoMatch. Distance to the diagonal represents the robustness against rotations. Among all the state-of-the-art ap-proaches, RoITr not only ranks ﬁrst on both benchmarks but also shows the best robustness against the enlarged rotations. formations identically, i.e., the pose-invariance, becomes the key to success in the point cloud matching task.
Since the side effects caused by a global translation can always be easily eliminated, e.g., by aligning the barycen-ter with the origin, the attention naturally shifts to coping with the rotations. In the past, handcrafted local descrip-tors [11, 30, 31, 41] were designed to be rotation-invariant so that the same geometry observed from different views can be correctly matched. With the emergence of deep neu-ral models for 3D point analysis, e.g., multilayer percep-trons (MLPs)-based like PointNet [25, 26], convolutions-based like KPConv [6, 40], and the attention-based like
PointTransformer [33, 53], recent approaches [1, 7, 9, 10, 13, 19, 20, 27, 32, 48–51] propose to learn descriptors from raw points as an alternative to handcrafted features that are less robust to occlusion and noise. The majority of deep point matchers [7, 10, 19, 20, 27, 34, 48, 50–52] is sensitive to ro-tations. Consequently, their invariance to rotations must be obtained extrinsically via augmented training to ensure that the same geometry under different poses can be depicted similarly. However, as the training cases can never span the continuous SO(3) space, they always suffer from instabil-ity when facing rotations that are rarely seen during train-ing. This can be observed by a signiﬁcant performance drop under enlarged rotations at inference time. (See Fig. 1.)
There are other works [1, 9, 13, 32, 44] that only lever-age deep neural networks to encode the pure geometry with the intrinsically-designed rotation invariance. However, the intrinsic rotation invariance comes at the cost of losing global context. For example, a human’s left and right halves are almost identically described, which naturally degrades the distinctiveness of features. Most recently, RIGA [49] is proposed to enhance the distinctiveness of the rotation-invariant descriptors by incorporating a global context, e.g., the left and right halves of a human become distinguishable by knowing there is a chair on the left while a table on the right. However, it lacks a highly-representative geometry encoder since it relies on PointNet [25], which accounts for an ineffective local geometry description. Moreover, as de-picting the cross-frame spatial relationships is non-trivial, previous works [19, 27, 34, 50] merely leverage the contex-tual features in the cross-frame context aggregation, which neglects the positional information. Although RIGA pro-poses to learn a rotation-invariant position representation by leveraging an additional PointNet, this simple design is hard to model the complex cross-frame positional relationships and leads to less distinctive descriptors.
In this paper, we present Rotation-Invariant Transformer (RoITr) to tackle the problem of point cloud matching un-der arbitrary pose variations. By using Point Pair Fea-tures (PPFs) as the local coordinates, we propose an at-tention mechanism to learn the pure geometry regardless of the varying poses. Upon it, attention-based layers are further proposed to compose the encoder-decoder architec-ture for highly-discriminative and rotation-invariant geom-etry encoding. We demonstrate its superiority over Point-Transformer [53], a state-of-the-art attention-based back-bone network, in terms of both efﬁciency and efﬁcacy in
Fig. 8 and Tab. 4 (a), respectively. On the global level, the cross-frame position awareness is introduced in a rotation-invariant fashion to facilitate feature distinctiveness. We il-lustrate its signiﬁcance over the state-of-the-art design [27] in Tab. 4 (d). Our main contributions are summarized as:
• An attention mechanism designed to disentangle the geometry and poses, which enables the pose-agnostic geometry description.
• An attention-based encoder-decoder architecture that learns highly-representative local geometry in a rotation-invariant fashion.
• A global transformer with rotation-invariant cross-frame position awareness that signiﬁcantly enhances the feature distinctiveness. 2.