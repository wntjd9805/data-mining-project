Abstract
Considering the ill-posed nature, contrastive regular-ization has been developed for single image dehazing, introducing the information from negative images as a lower bound. However, the contrastive samples are non-consensual, as the negatives are usually represented dis-tantly from the clear (i.e., positive) image, leaving the so-lution space still under-constricted. Moreover, the inter-pretability of deep dehazing models is underexplored to-wards the physics of the hazing process. In this paper, we propose a novel curricular contrastive regularization tar-geted at a consensual contrastive space as opposed to a non-consensual one. Our negatives, which provide better lower-bound constraints, can be assembled from 1) the hazy image, and 2) corresponding restorations by other existing methods. Further, due to the different similarities between the embeddings of the clear image and negatives, the learn-ing difﬁculty of the multiple components is intrinsically im-balanced. To tackle this issue, we customize a curriculum learning strategy to reweight the importance of different negatives.
In addition, to improve the interpretability in the feature space, we build a physics-aware dual-branch unit according to the atmospheric scattering model. With the unit, as well as curricular contrastive regularization, we establish our dehazing network, named C2PNet. Ex-tensive experiments demonstrate that our C2PNet signiﬁ-cantly outperforms state-of-the-art methods, with extreme
PSNR boosts of 3.94dB and 1.50dB, respectively, on SOTS-indoor and SOTS-outdoor datasets. Code is available at https://github.com/YuZheng9/C2PNet. 1.

Introduction
As a common atmospheric phenomenon, haze noticeably degrades the quality of photographed images, severely lim-iting the performance of subsequent high-level visual tasks such as vehicle re-identiﬁcation [7] and scene understand-*Corresponding author (csyongdu@ouc.edu.cn).
)
B d (
R
N
S
P 39 38 37 36 35 34 33 (cid:3) (a) 36.39 (b) 37.89 (d) 38.19 (c) 33.96 (f) 38.25 (e) 38.57 (g) 38.68
Method
Rate  E:H:U (a) Baseline (b) (c) (d) (e) (f) (g) 0:0:0 1:0:0 0:0:1 0:1:0 1:2:0 1:0:2 1:1:1 0 3
#Negatives in the consensual contrastive space 1
Non-consensual Contrastive Space (cid:3)
Consensual Contrastive Space
Anchor
×
Positive(GT)
Negatives
Ultra-hard Negatives
Hard Negatives
Easy Negatives
Pull
Push
×
Push (under-constricted) (g) Rate E:H:U = 1:1:1
Figure 1. Upper panel: Examination for contrastive regularization based on three difﬁculty levels of the negatives in the consensual contrastive space. Lower panel: Illustration of contrastive samples in the consensual and non-consensual spaces. ing [35]. Similar to the emergence of other image restora-tion task solvers [12, 13, 39, 43], valid image dehazing tech-niques are required for handling vision-based applications.
Deep learning based methods have achieved tremendous success in single image dehazing and can be roughly cate-gorized into two classes: physics-free methods [5,10,17,24] and physics-aware methods [4,8,11,34]. Regarding the for-mer, most of them usually use ground-truth images with predicted restorations to enforce L1/L2 distance-based con-sistency and also involve various regularizations [29, 42] as additional constraints to cope with the ill-posed property.
Notice that all of those regularizations ignore the informa-tion from negative images as a lower bound, contrastive reg-ularization (CR) [40] is proposed to introduce different hazy  
images as negatives and the ground-truth image as the pos-itive and further uses contrastive learning [19, 20] to guar-antee a closed solution space. Moreover, it is shown that better performances can be achieved when using more neg-atives since diverse degraded patterns are included as cues.
However, the issue is that the contents of those negatives are distinct from the positive, and their embeddings may be too distant, leaving the solution space still under-constricted.
To remedy this issue, a natural idea is to use the negatives in the consensual contrastive space1 (see the lower panel in Fig. 1) as better lower-bound constraints, which can be easily assembled from the hazy input and the corresponding restorations by other existing methods. In such cases, the negatives can be “closer” to the positive than those in the non-consensual space since the diversity of such negatives is more associated with the haze (or haze residue) rather than any other semantics. However, an intrinsic dilemma arises when the embedding of a negative is too close to that of the positive, as its pushing force to an anchor (i.e., the prediction) may cancel out the pulling force of the positive.
Such a learning difﬁculty can confuse the anchor to move towards the positive, especially in the early training stage.
This intuition is further examined in the upper panel of
[33] as baseline (row (a)) and
Fig. 1. We use FFA-Net
SOTS-indoor [28] as the testing dataset to explore the im-pact of the negatives in the consensual space with diverse difﬁculty. Speciﬁcally, we deﬁne the difﬁculty of the neg-atives into three levels: easy (E), hard (H), and ultra-hard (U). We adopt the hazy input as the easy negative, and use a coarse strategy to distinguish between the latter two types, i.e., whether the PSNR of the negative is greater than 30.
First, in the single-negative case (row (b)-(d)), an interesting
ﬁnding is that using a hard sample as negative achieves the best performance compared to the other two settings, and using an ultra-hard negative is even worse than the base-line. This reveals that a “close” negative has the potential to promote the effectiveness of the dehazing model, but not the closer the better due to the learning difﬁculty. While in the multi-negative case2 (row (e)-(g)), we have observed that comprehensively covering negatives with different dif-ﬁculty levels, including ultra-hard samples, can lead to the best performance. It implies the negatives at different difﬁ-culty levels can all contribute to the training phase. These observations motivate us to explore how to wisely arrange the multiple negative pairs in a consensual space into the
CR during training.
Moving on to the realm of physics-aware deep models, 1In this space, the contents of the negatives are identical to the positive sample, except for the haze distribution. Here, we use the terms (non-)consensual contrastive space and (non-)consensual space interchangeably, and a negative in the consensual space is denoted as a consensual negative. 2We give each negative the same weight in the regularization under this case, and we omit the cases of E=0, which would drastically decrease the performance. We will discuss the reason for this in Sec. 3. most of them utilize the atmospheric scattering model [31, 32] in the raw space, without fully exploring the beneﬁ-cial feature-level information. PFDN [11] is the only work that attempts to express the physics model as a basic unit in the network. The unit is designed as a shared structure to predict the latent features corresponding to the atmo-spheric light and transmission map. Nevertheless, the for-mer is usually assumed to be homogeneous while the latter is non-homogeneous, and thus their features cannot be ap-proximated in the same way. Therefore, it is still an open problem how to accurately realize the interpretability of the feature space of the deep network using the physics model, which is another aspect we are interested in.
In this paper, we propose a curricular contrastive reg-ularization using hazy or restored images as negatives in the consensual space for image dehazing to address the ﬁrst issue.
Informed by our analysis, which suggests that the difﬁculty of consensual negatives can impact the effective-ness of the regularization, we present a curriculum learning strategy to arrange these negatives to mitigate learning am-biguity. Speciﬁcally, we split the negatives into three types (i.e., easy, hard, and ultra-hard) and assign different weights to corresponding negative pairs in CR. Meanwhile, the dif-ﬁculty levels of the negatives are dynamically adjusted as the anchor moves towards the positive in the representation space during training.
In this way, the proposed regular-ization can facilitate the dehazing models to be stably opti-mized in a more compact solution space.
We propose a physics-aware dual-branch unit (PDU) re-garding the second issue. The PDU approximates the fea-tures corresponding to the atmospheric light and the trans-mission map in dual branches, respectively considering the physical characteristics of each factor. The features of the latent clear image can thus be synthesized more precisely in line with the physics model. Finally, we establish C2PNet, our dehazing network that deploys PDUs into a cascaded backbone with curricular contrastive regularization.
In summary, our key contributions are as follows:
• We propose a novel C2PNet for haze removal that em-ploys curricular contrastive regularization and enforces physics-based prior in the feature space. Our method outperforms SOTAs in both synthetic and real-world scenarios. In particular, we achieve signiﬁcant PSNR boosts of 3.94dB and 1.50dB on the SOTS-indoor and
SOTS-outdoor datasets, respectively.
• The proposed regularization adopts a unique consen-sual negative-based approach for dehazing and incor-porates a self-contained curriculum learning strategy that dynamically calibrates the priority and difﬁculty levels of the negatives. It is also proven to enhance the performance of SOTAs as a generalized regularization technique, surpassing previous related strategies.
• With careful consideration of the characteristics of fac-tors involved, we built the PDU based on an unprece-dented expression of the physics model. This innova-tive design promotes feature transmission and extrac-tion in the feature space, guided by physics priors. 2.