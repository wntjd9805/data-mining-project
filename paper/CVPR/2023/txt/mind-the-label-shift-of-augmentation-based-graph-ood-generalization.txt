Abstract
Out-of-distribution (OOD) generalization is an impor-tant issue for Graph Neural Networks (GNNs). Recent works employ different graph editions to generate aug-mented environments and learn an invariant GNN for gen-eralization. However, the label shift usually occurs in aug-mentation since graph structural edition inevitably alters the graph label. This brings inconsistent predictive rela-tionships among augmented environments, which is harm-ful to generalization. To address this issue, we propose
LiSA, which generates label-invariant augmentations to fa-cilitate graph OOD generalization. Instead of resorting to graph editions, LiSA exploits Label-invariant Subgraphs of the training graphs to construct Augmented environments.
Specifically, LiSA first designs the variational subgraph generators to extract locally predictive patterns and con-struct multiple label-invariant subgraphs efficiently. Then, the subgraphs produced by different generators are col-lected to build different augmented environments. To pro-mote diversity among augmented environments, LiSA fur-ther introduces a tractable energy-based regularization to enlarge pair-wise distances between the distributions of en-vironments.
In this manner, LiSA generates diverse aug-mented environments with a consistent predictive relation-ship and facilitates learning an invariant GNN. Extensive experiments on node-level and graph-level OOD bench-marks show that LiSA achieves impressive generalization performance with different GNN backbones. Code is avail-able on https://github.com/Samyu0304/LiSA. 1.

Introduction
Learning from graph-structured data is a fundamental problem in various applications, such as 3D vision [50],
*Corresponding Author knowledge graph reasoning [65], and social network analy-sis [32]. Recently, the Graph Neural Networks (GNNs) [33] have become a de facto standard in developing deep learn-ing systems on graphs [10], showing superior performance on point cloud classification [18], recommendation system
[56], biochemistry [29] and so on. Despite their remarkable success, these models heavily rely on the i.i.d. assumption that the training and testing data are independently drawn from an identical distribution [9, 39]. When tested on out-of-distribution (OOD) graphs (i.e. larger graphs), GNN usu-ally suffers from unsatisfactory performances and unstable prediction results. Hence, handling the distribution shift for
GNNs has received increasing attention.
Many solutions have been proposed to explore the
OOD generalization problem in Euclidean space [47], such as invariant learning [3, 14, 38], group fairness [34], and distribution-robust optimization [49]. Recent works mainly resort to learning an invariant classifier that per-forms equally well in different training environments [3, 16, 37, 38]. However, the study of its counterpart problem for non-Euclidean graphs is comparatively lacking. One challenge is the environmental scarcity of graph-structured data [39, 53]. Inspired by the data augmentation literature
[48, 52], some pioneering works propose to generate aug-mented training environments by applying different graph edition policies to the training graphs [55,57]. After training in these environments, the GNN is expected to have better
OOD generalization ability. Nevertheless, the graph labels may change during the graph edition since they are sensi-tive to graph structural modifications. This causes the label shift problem of augmented graphs. For example, methods of graph adversarial attack usually seek to modify the graph structure to permute the model prediction [9]. Moreover, a small structural modification can drastically influence the biochemical property of molecule or protein graphs [30].
We formalize the impact of the label shift in augmen-tations on generalization using a unified structure equa-tion model [1]. Our analysis indicates that the label shift causes inconsistent predictive relationships among the aug-mented environments. This misguides the GNN to out-put a perturbed prediction rather than the invariant pre-diction, making the learned GNN hard to generalize (see
Section 3 for more details). Thus, it is crucial to gener-ate label-invariant augmentations for graph OOD general-ization. However, designing label-invariant graph edition is nontrivial or even brings extensive computation, since it requires learning class-conditional distribution for discrete and irregular graphs. In this work, we propose a novel label-invariant subgraph augmentation method, dubbed LiSA, for the graph OOD generalization problem. For an input graph,
LiSA first designs the variational subgraph generators to identify locally predictive patterns (i.e. important nodes or edges for the graph label) and generate multiple label-invariant subgraphs. These subgraphs capture prediction-relevant information with different structures, and thus con-struct augmented environments with a consistent predictive relationship. To promote diversity among the augmenta-tions, we propose a tractable energy-based regularization to enlarge the pair-wise distances between the distributions of augmented environments. With the augmentations pro-duced by LiSA, a GNN classifier is learned to be invariant across these augmented environments. The GNN predictor and variational subgraph generators are jointly optimized with a bi-level optimization scheme [61]. LiSA is model-agnostic and is flexible in handling both graph-level and node-level distribution shifts. Extensive experiments indi-cate that LiSA enjoys satisfactory performance gain over the baselines on 7 graph classification datasets and 4 node classification datasets. Our contributions are as follows:
• We propose a model-agnostic label-invariant subgraph augmentation (LiSA) framework to generate aug-mented environments with consistent predictive rela-tionships for graph OOD generalization.
• We propose the variational subgraph generator to dis-cover locally crucial patterns to construct the label-invariant subgraphs efficiently.
• To further promote diversity, we further propose an energy-based regularization to enlarge pair-wise dis-tances between the distributions of different aug-mented environments.
• Extensive experiments on node-level and graph-level tasks indicate that LiSA enjoys satisfactory perfor-mance gain over the baselines on various backbones. 2.