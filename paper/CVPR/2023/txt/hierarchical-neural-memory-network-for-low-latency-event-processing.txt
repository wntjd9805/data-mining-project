Abstract
This paper proposes a low latency neural network ar-chitecture for event-based dense prediction tasks. Conven-tional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper tem-poral scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dy-namic to static scene contents by propagating information from the fast to the slow memory modules. The architec-ture not only reduces the redundancy of conventional archi-tectures but also exploits long-term dependencies. Further-more, an attention-based event representation efficiently en-codes sparse event streams into the memory cells. We con-duct extensive evaluations on three event-based dense pre-diction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demon-strating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/ hmnet/. 1.

Introduction
Latency matters for many vision applications such as au-tonomous vehicles or UAVs, directly affecting their safety and reliability measures. Latency is also crucial for better user experience in time-sensitive vision applications such as augmented reality, where the latency of standard RGB cam-eras is insufficient. For example, a vehicle traveling 80km/h will move 74cm within a frame of a standard 30fps camera.
Event cameras have extremely low latency. Unlike stan-dard vision cameras that capture the intensity of all pixels at a fixed rate, event cameras asynchronously record intensity changes of individual pixels. This unique principle leads to extremely low latency (microseconds) and high temporal resolution, along with many other advantages such as high dynamic range and low power consumption. With such at-Figure 1. Schematic comparison between conventional and our methods. Our method adaptively processes dynamic and static scene contents using latent memories with variable rates. Our approach simultaneously processes fast-moving objects and static scene contents at low latency (See the right figure). tractive characteristics, event cameras are becoming popu-lar input devices for many dense prediction tasks such as semantic segmentation [1, 39], object detection [18, 30, 34], depth estimation [10, 26], and optical flow [11, 46].
Despite the emergence of low latency event-cameras, few works focused on low latency recognition models ded-icated to event-based dense prediction tasks. Instead, most previous works apply standard CNN architectures equipped with the recurrent modules as a backbone [10, 15, 30, 39], resulting in the same latency levels as frame-based mod-els. Another line of research applies Spiking Neural Net-works (SNNs) for event data. However, SNNs suffer from low accuracy due to the lack of established training prin-ciples [40, 41] or high latency due to long simulation time steps [44]. We need backbone architectures that best exploit low latency and high temporal resolution of event data.
To this end, we propose a Hierarchical Neural Memory
Network (HMNet) for low latency event processing. The key idea is to encode the scene contents at a proper tem-poral scale depending on their speed. For this purpose,
the proposed network builds multi-level latent memory with different operating rates (Fig. 1). The low-level memories run fast to quickly encode local and dynamic information, whereas high-level memories perform global reasoning on static information at a lower frequency. This design signif-icantly reduces the computational loads in contrast to con-ventional methods that runs the entire forward path every time. The paper also proposes an Event Sparse Cross Atten-tion (ESCA) that directly injects sparse event streams into dense memory cells with minimal information loss.
We conduct extensive evaluations on three event-based vision tasks (object detection, semantic segmentation, and monocular depth estimation) as well as event-image fusion task. Experimental results show that HMNet outperforms existing methods while reducing latency by 40%-50%. 2.