Abstract
In this paper, we present a new MTL framework that searches for structures optimized for multiple tasks with di-verse graph topologies and shares features among tasks.
We design a restricted DAG-based central network with read-in/read-out layers to build topologically diverse task-adaptive structures while limiting search space and time.
We search for a single optimized network that serves as multiple task adaptive sub-networks using our three-stage training process. To make the network compact and dis-cretized, we propose a ﬂow-based reduction algorithm and a squeeze loss used in the training process. We evaluate our optimized network on various public MTL datasets and show ours achieves state-of-the-art performance. An exten-sive ablation study experimentally validates the effective-ness of the sub-module and schemes in our framework. 1.

Introduction
Multi-task learning (MTL), which learns multiple tasks simultaneously with a single model has gained increasing attention [3, 13, 14]. MTL improves the generalization per-formance of tasks while limiting the total number of net-work parameters to a lower level by sharing representa-tions across tasks. However, as the number of tasks in-creases, it becomes more difﬁcult for the model to learn the shared representations, and improper sharing between less related tasks causes negative transfers that sacriﬁce the per-formance of multiple tasks [15,36]. To mitigate the negative transfer in MTL, some works [6, 25, 32] separate the shared and task-speciﬁc parameters on the network.
More recent works [21,29,38] have been proposed to dy-namically control the ratio of shared parameters across tasks using a Dynamic Neural Network (DNN) to construct a task adaptive network. These works mainly apply the cell-based architecture search [19, 27, 41] for fast search times, so that the optimized sub-networks of each task consist of ﬁxed or simple structures whose layers are simply branched, as shown in Fig. 1a. They primarily focus on ﬁnding branching
*Corresponding author patterns in speciﬁc aspects of the architecture, and feature-sharing ratios across tasks. However, exploring optimized structures in restricted network topologies has the potential to cause performance degradation in heterogeneous MTL scenarios due to unbalanced task complexity.
We present a new MTL framework searching for sub-network structures, optimized for each task across diverse network topologies in a single network. To search the graph topologies from richer search space, we apply Directed
Acyclic Graph (DAG) for the homo/heterogeneous MTL frameworks, inspired by the work in NAS [19, 27, 40]. The
MTL in the DAG search space causes a scalability issue, where the number of parameters and search time increase quadratically as the number of hidden states increases.
To solve this problem, we design a restricted DAG-based central network with read-in/read-out layers that allow our
MTL framework to search across diverse graph topologies while limiting the search space and search time. Our ﬂow-restriction eliminates the low-importance long skip connec-tion among network structures for each task, and creates the required number of parameters from O(N 2) to O(N ). The read-in layer is the layer that directly connects all the hidden states from the input state, and the read-out layer is the layer that connects all the hidden states to the last feature layer.
These are key to having various network topological rep-resentations, such as polytree structures, with early-exiting and multi-embedding.
Then, we optimize the central network to have compact task-adaptive sub-networks using a three-stage training pro-cedure. To accomplish this, we propose a squeeze loss and a
ﬂow-based reduction algorithm. The squeeze loss limits the upper bound on the number of parameters. The reduction algorithm prunes the network based on the weighted adja-cency matrix measured by the amount of information ﬂow in each layer. In the end, our MTL framework constructs a compact single network that serves as multiple task-speciﬁc networks with unique structures, such as chain, polytree, and parallel diverse topologies, as presented in Fig. 1b. It also dynamically controls the amount of sharing represen-tation among tasks.
(a) Graph representation of existing DNN-based methods (i).-(ii). for MTL and ours (iii). (b) Topologies of DAG (i). and its sub-graph (ii).
Figure 1. Graph representation of various neural networks. (a) Graph representation of existing dynamic neural network for multitask learning and ours. (b) Topologies of a completed Directed Acyclic Graph (DAG) and the output sub-graph of DAG structure.
The experiments demonstrate that our framework suc-cessfully searches the task-adaptive network topologies of each task and leverages the knowledge among tasks to make a generalized feature. The proposed method outperforms state-of-the-art methods on all common benchmark datasets for MTL. Our contributions can be summarized as follows:
• We present for the ﬁrst time an MTL framework that searches both task-adaptive structures and sharing pat-terns among tasks. It achieves state-of-the-art perfor-mance on all public MTL datasets.
• We propose a new DAG-based central network com-posed of a ﬂow restriction scheme and read-in/out lay-ers, that has diverse graph topologies in a reasonably restricted search space.
• We introduce a new training procedure that optimizes the MTL framework for compactly constructing vari-ous task-speciﬁc sub-networks in a single network. 2.