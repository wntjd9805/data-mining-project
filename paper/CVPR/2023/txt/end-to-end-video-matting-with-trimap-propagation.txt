Abstract
The research of video matting mainly focuses on temporal coherence and has gained significant improvement via neural networks. However, matting usually relies on user-annotated trimaps to estimate alpha values, which is a labor-intensive issue. Although recent studies exploit video object segmentation methods to propagate the given trimaps, they suffer inconsistent results. Here we present a more robust and faster end-to-end video matting model equipped with trimap propagation called FTP-VM (Fast
Trimap Propagation - Video Matting).
The FTP-VM combines trimap propagation and video matting in one model, where the additional backbone in memory matching is replaced with the proposed lightweight trimap fusion module. The segmentation consistency loss is adopted from automotive segmentation to fit trimap segmentation with the collaboration of RNN (Recurrent Neural Network) to improve the temporal coherence. The experimental results demonstrate that the FTP-VM performs competitively both in composited and real videos only with few given trimaps.
The efficiency is eight times higher than the state-of-the-art methods, which confirms its robustness and applicability in real-time scenarios. The code is available at https:
//github.com/csvt32745/FTP-VM . 1.

Introduction
Image matting aims to estimate the alpha value of each pixel for a target in the input image. Unlike general segmentation, which generates binary values, matting outputs values between 0 and 1, meaning the degree of transparency. The output alpha mattes can describe semi-transparent objects with precise details. As shown in Eq. (1), each pixel color
C of an image is composited by the foreground color F , background color B, and an alpha value α, where the background can be substituted to generate a matting dataset.
C = αF + (1 − α)B (1)
Given a frame like Fig. 1a, a trimap Fig. 1b is the common requirement for image matting, which divides the pixels (a) Memory frame (b) Memory trimap (c) Query frame (d) The result of RVM [32] (e) The result of OTVM [42] (f) The result of FTP-VM (ours)
Figure 1. An example of an in-street interview video. (d)
Automatic matting, (e) Trimap-propagation-based method (f) The proposed method. into three regions: foreground, background and unknown.
Matting methods adopt such information to solve alpha values of unknown (gray) regions.
Video matting extracts an alpha matte of each frame of the given video. The resulting alpha mattes can be used for background replacement, which is decisive for video applications such as video conferencing and visual effects.
It is intuitive to perform image matting on each frame of a video. However, severe flickering artifacts would occur in the resultant image sequence. In order to improve the robustness, considering spatial and temporal coherence is the main challenge for video matting as the temporal information helps infer the matting target from the previous frames. Another challenge is to provide a trimap for each frame, which is expected to be a massive cost to most users.
To tackle the above two issues, automatic matting and trimap propagation are addressed in this paper. Automatic trimaps, but matting captures specific targets without
the input videos with ambiguous scenarios highly affect the results.
Fig. 1 shows an example of an in-street interview video where an interviewee and passengers occasionally appear simultaneously. While the human matting method [32] attempts to capture all the people, the ambiguity caused by inconspicuous passengers results in unsatisfactory output Fig. 1d. Thus we opt for trimap propagation to select targets more stably as shown in Fig. 1f.
While performing trimap propagation, the user is required to provide a pair of so-called memory frame Fig. 1a and memory trimap Fig. 1b, and this information is utilized to propagate throughout the video.
Since we are predicting a sequence of three-class segmentation masks, the trimap propagation can be treated as video object segmentation. Recent studies [42, 47, 55] leverage
STM (Space-Time Memory network), [35] an emerging video object segmentation model, to produce the trimaps successfully. trimaps usually contain flickers and lead to unsatisfactory results. As
STM lacks temporal coherence, we append ConvGRUs the
[2] to the model previous approaches containing two full models make them unsuitable for interactive applications. We thus combine the two models into an end-to-end model to enhance the speed and performance. The main contributions of this work are summarized as follows. to improve stability. Moreover, the resultant
However,
• A novel end-to-end video matting model equipped with trimap propagation, called FTP-VM (Fast Trimap
Propagation - Video Matting), is proposed. FTP-VM is faster than the previous two-model methods by a large margin while preserving competitive performance in different scenarios. While the frame rate of the the proposed method previous methods is 5 FPS, reaches 40 FPS on an NVIDIA RTX 2080Ti GPU.
• A lightweight trimap fusion module is designed to replace an additional encoder in the STM-based model to make FTP-VM efficient and more powerful.
• Motivated by [38], the segmentation consistency loss from automotive segmentation is adapted to trimap segmentation. The final setting reaching the more satisfactory performance is determined by conducting comprehensive experiments. 2.