Abstract
This paper introduces a novel representation of volumet-ric videos for real-time view synthesis of dynamic scenes.
Recent advances in neural scene representations demon-strate their remarkable capability to model and render com-plex static scenes, but extending them to represent dynamic scenes is not straightforward due to their slow rendering speed or high storage cost. To solve this problem, our key idea is to represent the radiance field of each frame as a set of shallow MLP networks whose parameters are stored in 2D grids, called MLP maps, and dynamically predicted by a 2D CNN decoder shared by all frames. Represent-ing 3D scenes with shallow MLPs significantly improves the rendering speed, while dynamically predicting MLP pa-rameters with a shared 2D CNN instead of explicitly stor-ing them leads to low storage cost. Experiments show that the proposed approach achieves state-of-the-art rendering quality on the NHR and ZJU-MoCap datasets, while being efficient for real-time rendering with a speed of 41.7 fps for 512 × 512 images on an RTX 3090 GPU. The code is avail-able at https://zju3dv.github.io/mlp maps/. 1.

Introduction
Volumetric video captures a dynamic scene in 3D which allows users to watch from arbitrary viewpoints with im-mersive experience. It is a cornerstone for the next gener-ation media and has many important applications such as video conferencing, sport broadcasting, and remote learn-ing. The same as 2D video, volumetric video should be ca-pable of high-quality and real-time rendering as well as be-ing compressed for efficient storage and transmission. De-signing a proper representation for volumetric video to sat-isfy these requirements remains an open problem.
Traditional image-based rendering methods [1,12,25,74] build free-viewpoint video systems based on dense camera arrays. They record dynamic scenes with many cameras and then synthesize novel views by interpolation from in-put nearby views. For these methods, the underlying scene
∗Equal contribution. †Corresponding author.
Figure 1. The basic idea of dynamic MLP maps.
Instead of modeling the volumetric video with a big MLP network [26], we exploit a 2D convolutional neural network to dynamically gener-ate 2D MLP maps at each video frame, where each pixel storing the parameter vector of a small MLP network. This enables us to represent volumetric videos with a set of small MLP networks, thus significantly improving the rendering speed. representation is the original multi-view video. While there have been many multi-view video coding techniques, the storage and transmission cost is still huge which cannot satisfy real-time video applications. Another line of work
[11, 13] utilizes RGB-D sensors to reconstruct textured meshes as the scene representation. With mesh compression techniques, this representation can be very compact and en-able streamable volumetric videos, but these methods can only capture humans and objects in constrained environ-ments as reconstructing a high-quality renderable mesh for general dynamic scenes is still a very challenging problem.
Recent advances in neural scene representations [26, 33, 61] provide a promising solution for this problem. They represent 3D scene with neural networks, which can be ef-fectively learned from multi-view images through differen-tiable renderers. For instance, Neural Volumes [33] rep-resents volumetric videos with a set of RGB-density vol-umes predicted by 3D CNNs. Since the volume prediction easily consumes large amount of GPU memory, it strug-gles to model high-resolution 3D scenes. NeRF [37] in-stead represent 3D scenes with MLP networks regressing density and color for any 3D point, thereby enabling it to synthesize high-resolution images. DyNeRF [26] extends
NeRF to model volumetric videos by introducing a tempo-ral latent code as additional input of the MLP network. A major issue of NeRF models is that their rendering is gen-erally quite slow due to the costly network evaluation. To increase the rendering speed, some methods [17,61,75] uti-lize caching techniques to pre-compute a discrete radiance volume. This strategy typically leads to high storage cost, which is acceptable for a static scene, but not scalable to render a volumetric video of dynamic scenes.
In this paper, we propose a novel representation of volu-metric video, named dynamic MLP maps, for efficient view synthesis of dynamic scenes. The basic idea is illustrated in
Figure 1. Instead of modeling a volumetric video with a sin-gle MLP network, we represent each video frame as a set of small MLP networks whose parameters are predicted by a per-scene trained 2D CNN decoder with a per-frame latent code. Specifically, given a multi-view video, we choose a subset of views and feed them into a CNN encoder to obtain a latent code for each frame. Then, a 2D CNN decoder re-gresses from the latent code to 2D maps, where each pixel in the maps stores a vector of MLP parameters. We call these 2D maps as MLP maps. To model a 3D scene with the MLP maps, we project a query point in 3D space onto the MLP maps and use the corresponding MLP networks to infer its density and color values.
Representing 3D scenes with many small MLP networks decreases the cost of network evaluation and increases the rendering speed. This strategy has been proposed in pre-vious works [48, 49], but their networks need to be stored for each static scene, which easily consumes a lot of stor-age to represent a dynamic scene. In contrast to them, we use shared 2D CNN encoder and decoder to predict MLP parameters on the fly for each video frame, thereby effec-tively compressing the storage along the temporal domain.
Another advantage of the proposed representation is that
MLP maps represent 3D scenes with 2D maps, enabling us to adopt 2D CNNs as the decoder instead of 3D CNNs in
Neural Volumes [33]. This strategy leverages the fast infer-ence speed of 2D CNNs and further decreases the memory requirement.
We evaluate our approach on the NHR and ZJU-MoCap datasets, which present dynamic scenes with complex mo-tions. Across all datasets, our approach exhibits state-of-the-art performance in terms of rendering quality and speed, while taking up low storage. Experiments demonstrate that our approach is over 100 times faster than DyNeRF [26].
In summary, this work has the following contributions:
• A novel representation of volumetric video named dy-namic MLP maps, which achieves compact represen-tation and fast inference.
• A new pipeline for real-time rendering of dynamic scenes based on dynamic MLP maps.
• State-of-the-art performance in terms of the render-ing quality, speed, and storage on the NHR and ZJU-MoCap datasets. 2.