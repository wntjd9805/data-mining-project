Abstract
Position Embeddings (PEs), an arguably indispensable component in Vision Transformers (ViTs), have been shown to improve the performance of ViTs on many vision tasks.
However, PEs have a potentially high risk of privacy leak-age since the spatial information of the input patches is exposed. This caveat naturally raises a series of interesting questions about the impact of PEs on accuracy, privacy, pre-diction consistency, etc. To tackle these issues, we propose a
Masked Jigsaw Puzzle (MJP) position embedding method.
In particular, MJP first shuffles the selected patches via our block-wise random jigsaw puzzle shuffle algorithm, and their corresponding PEs are occluded. Meanwhile, for the non-occluded patches, the PEs remain the original ones but their spatial relation is strengthened via our dense absolute lo-calization regressor. The experimental results reveal that 1)
PEs explicitly encode the 2D spatial relationship and lead to severe privacy leakage problems under gradient inversion attack; 2) Training ViTs with the naively shuffled patches can alleviate the problem, but it harms the accuracy; 3) Under a certain shuffle ratio, the proposed MJP not only boosts the performance and robustness on large-scale datasets (i.e.,
ImageNet-1K and ImageNet-C, -A/O) but also improves the privacy preservation ability under typical gradient attacks by a large margin. The source code and trained models are available at https://github.com/yhlleo/MJP. 1.

Introduction
Transformers [38] demonstrated their overwhelming power on a broad range of language tasks (e.g., text classifi-cation, machine translation, or question answering [22, 38]), and the vision community follows it closely and extends it for vision tasks, such as image classification [7,37], object detec-tion [2, 51], segmentation [47], and image generation [3, 24].
Most of the previous ViT-based methods focus on design-ing different pre-training objectives [9, 11, 12] or variants of self-attention mechanisms [28, 39, 40]. By contrast, PEs
*Equal contribution. Email: {bin.ren, yahui.liu}@unitn.it
†Corresponding author. Email: wei.wang@bjtu.edu.cn
Figure 1. Low-dimensional projection of position embeddings from
DeiT-S [37]. (a) The 2D UMAP projection, it shows that reverse diagonal indices have the same order as the input patch positions. (b) The 3D PCA projection, it also shows that the position informa-tion is well captured with PEs. Note that the embedding of index 1 (highlighted in red) corresponds to the [CLS] embedding that does not embed any positional information. receive less attention from the research community and have not been well studied yet. In fact, apart from the attention mechanism, how to embed the position information into the self-attention mechanism is also one indispensable research topic in Transformer. It has been demonstrated that with-out the PEs, the pure language Transformer encoders (e.g.,
BERT [6] and RoBERTa [25]) may not well capture the meaning of positions [43]. As a consequence, the meaning of a sentence can not be well represented [8]. Similar phe-nomenon of PEs could also be observed in vision community.
Dosovitskiy et al. [7] reveals that removing PEs causes per-formance degradation. Moreover, Lu et al. [29] analyzed this issue from the perspective of user privacy and demon-strated that the PEs place the model at severe privacy risk since it leaks the clues of reconstructing sequential patches back to images. Hence, it is very interesting and necessary to understand how the PEs affect the accuracy, privacy, and consistency in vision tasks. Here the consistency means whether the predictions of the transformed/shuffled image are consistent with the ones of the original image.
To study the aforementioned effects of PEs, the key is to figure out what explicitly PEs learn about positions from input patches. To answer this question, we project the high-dimensional PEs into the 2D and 3D spaces using Uni-Figure 2. (a) The original input patches; (b) Totally random shuffled input patches; (c) Partially random shuffled input patches; (d) An overview of the proposed MJP. Note that we show the random shuffled patches and its corresponding unknow position embedding in green and the rest part in blue. DAL means the self-supervised dense absolute localization regression constraint. form Manifold Approximation & Projection (UMAP) [30] and PCA, respectively. Then for the first time, we visually demonstrate that the PEs can learn the 2D spatial relation-ship very well from the input image patches (the relation is visualized in Fig. 1). We can see that the PEs are distributed in the same order as the input patch positions. Therefore, we can easily obtain the actual spatial position of the input patches by analyzing the PEs. Now it explains why PEs can bring the performance gain for ViTs [7]. This is because the spatial relation in ViTs works similar as the inherent intrinsic inductive bias in CNNs (i.e., it models the local visual struc-ture) [45]. However, these correctly learned spatial relations are unfortunately the exact key factor resulting in the privacy leakage [29].
Based on these observations, one straightforward idea to protect the user privacy is to provide ViTs with the randomly transformed (i.e., shuffled) input data. The underlying intu-ition is that the original correct spatial relation within input patches will be violated via such a transformation. There-fore, we transform the previous visually recognizable input image patches x shown in Fig. 2(a) to its unrecognizable counterpart (cid:101)x depicted in Fig. 2(b) during training. The ex-perimental results show that such a strategy can effectively alleviate the privacy leakage problem. This is reasonable since the reconstruction of the original input data during the attack is misled by the incorrect spatial relation. However, the side-effect is that this leads to a severe accuracy drop.
Meanwhile, we noticed that such a naive transformation strategy actually boosts the consistency [31, 34, 44] albeit the accuracy drops. Note that here the consistency repre-sents if the predictions of the original and transformed (i.e., shuffled) images are consistent. Given the original input patches x and its corresponding transformed (i.e., shuffled) counterpart, we say that the predictions are consistent if arg max P (F(x)) = arg max P (F((cid:101)x)), where F refers to the ViT models, and P denotes the predicted logits.
These observations hint that there might be a trade-off solution that makes ViTs take the best from both worlds (i.e., both the accuracy and the consistency). Hence, we pro-pose the Masked Jigsaw Puzzle (MJP) position embedding method. Specifically, there are four core procedures in the
MJP: (1) We first utilize a block-wise masking method [1] to randomly select a partial of the input sequential patches; (2) Next, we apply jigsaw puzzle to the selected patches (i.e., shuffle the orders); (3) After that, we use a shared un-known position embedding for the shuffled patches instead of using their original PEs; (4) To well maintain the posi-tion prior of the unshuffled patches, we introduce a dense absolute localization (DAL) regressor to strengthen their spatial relationship in a self-supervised manner. We simply demonstrate the idea of the first two procedures in Fig. 2(c), and an overview of the proposed MJP method is available in
Fig. 2(d). In summary, our main contributions are:
• We demonstrate that although PEs can boost the accuracy, the consistency against image patch shuffling is harmed.
Therefore, we argue that studying PEs is a valuable re-search topic for the community.
• We propose a simple yet efficient Masked Jigsaw Puzzle (MJP) position embedding method which is able to find a balance among accuracy, privacy, and consistency.
• Extensive experimental results show that MJP boosts the accuracy on regular large-scale datasets (e.g., ImageNet-1K [32]) and the robustness largely on ImageNet-C [18],
-A/O [19]. One additional bonus of MJP is that it can improve the privacy preservation ability under typical gra-dient attacks by a large margin. 2.