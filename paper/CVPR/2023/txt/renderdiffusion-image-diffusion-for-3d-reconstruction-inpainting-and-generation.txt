Abstract
Diffusion models currently achieve state-of-the-art per-formance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruc-tion. In this paper, we present RenderDiffusion, the first dif-fusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and ren-ders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong induc-tive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervi-sion. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ,
ShapeNet and CLEVR datasets, showing competitive per-formance for generation of 3D scenes and inference of 3D scenes from 2D images. Additionally, our diffusion-based approach allows us to use 2D inpainting to edit 3D scenes.
Project page: https : / / github . com / Anciukevicius /
RenderDiffusion 1.

Introduction
Image diffusion models now achieve state-of-the-art per-formance on both generation and inference tasks. Com-pared to alternative approaches (e.g. GANs and VAEs), they are able to model complex datasets more faithfully, particu-larly for long-tailed distributions, by explicitly maximizing likelihood of the training data. Many exciting applications have emerged in only the last few months, including text-to-image generation [49, 55], inpainting [54], object inser-tion [3], and personalization [53].
However, in 3D generation and understanding, their suc-cess has so far been limited, both in terms of quality and diversity of the results. Some methods have successfully applied diffusion models directly to point cloud or voxel data [35, 67], or optimized a NeRF using a pre-trained dif-fusion model [48]. This limited success in 3D is due to two problems: first, an explicit 3D representation (e.g., voxels) leads to significant memory demands and affects conver-gence speed; and more importantly, a setup that requires ac-cess to explicit 3D supervision is problematic as 3D model repositories contain orders of magnitude fewer data com-pared to image counterparts—a particular problem for large diffusion models which tend to be more data-hungry than
GANs or VAEs.
In this work, we present RenderDiffusion – the first dif-fusion method for 3D content that is trained using only 2D images. Like previous diffusion models, we train our model to denoise 2D images. Our key insight is to incorporate a latent 3D representation into the denoiser. This creates an inductive bias that allows us to recover 3D objects while training only to denoise in 2D, without explicit 3D super-vision. This latent 3D structure consists of a triplane rep-resentation [8] that is created from the noisy image by an encoder, and a volumetric renderer [37] that renders the 3D representation back into a (denoised) 2D image. With the triplane representation, we avoid the cubic memory growth for volumetric data, and by working directly on 2D im-ages, we avoid the need for 3D supervision. Compared to latent diffusion models that work on a pre-trained latent space [5, 50], working directly on 2D images also allows us to obtain sharper generation and inference results. Note that
RenderDiffusion does assume that we have the intrinsic and extrinsic camera parameters available at training time.
We evaluate RenderDiffusion on in-the-wild (FFHQ,
AFHQ) and synthetic (CLEVR, ShapeNet) datasets and show that it generates plausible and diverse 3D-consistent scenes (see Figure 1). Furthermore, we demonstrate that it successfully performs challenging inference tasks such as monocular 3D reconstruction and inpainting 3D scenes from masked 2D images, without specific training for those tasks. We show improved reconstruction accuracy over a state-of-the-art method [8] in monocular 3D reconstruction that was also trained with only monocular supervision.
In short, our key contribution is a denoising architecture with an explicit latent 3D representation, which enables us to build the first 3D-aware diffusion model that can be trained purely from 2D images. 2.