Abstract
Visual Place Recognition (VPR) estimates the location of query images by matching them with images in a reference database. Conventional methods generally adopt aggre-gated CNN features for global retrieval and RANSAC-based geometric verification for reranking. However, RANSAC only employs geometric information but ignores other pos-sible information that could be useful for reranking, e.g. lo-cal feature correlations, and attention values. In this paper, we propose a unified place recognition framework that han-dles both retrieval and reranking with a novel transformer model, named R2Former. The proposed reranking module takes feature correlation, attention value, and xy coordi-nates into account, and learns to determine whether the image pair is from the same location. The whole pipeline is end-to-end trainable and the reranking module alone can also be adopted on other CNN or transformer back-bones as a generic component. Remarkably, R2Former significantly outperforms state-of-the-art methods on ma-jor VPR datasets with much less inference time and mem-ory consumption.
It also achieves the state-of-the-art on the hold-out MSLS challenge set and could serve as a sim-ple yet strong solution for real-world large-scale applica-tions. Experiments also show vision transformer tokens are comparable and sometimes better than CNN local fea-tures on local matching. The code is released at https:
//github.com/Jeff-Zilence/R2Former. 1.

Introduction
Visual Place Recognition (VPR) aims to localize query images from unknown locations by matching them with a set of reference images from known locations. It has great potential for robotics [47], navigation [41], autonomous driving [9], augmented reality (AR) applications. Previ-ous works [26, 53] generally formulate VPR as a retrieval problem with two stages, i.e. global retrieval, and rerank-ing. The global retrieval stage usually applies aggregation
† This work was done during the first author’s internship at ByteDance
Figure 1. An overview of conventional pipeline and our unified framework. Both our global retrieval and reranking modules are end-to-end trainable transformer layers, which achieve state-of-the-art performance with much less computational cost. methods (e.g. NetVLAD [3] and GeM [43]) on top of CNN (Convolutional Neural Network) to retrieve top candidates from a large reference database. While some works [3, 6] only adopt global retrieval, current state-of-the-art meth-ods [26, 53] conduct reranking (i.e. geometric verification with RANSAC [19]) on the top-k (e.g. k = 100) candidates to further confirm the matches, typically leading to a signif-icant performance boost. However, geometric information is not the only information that could be useful for rerank-ing, and task-relevant information could be learned with a data-driven module to further boost performance. Besides, the current reranking process requires a relatively large in-ference time and memory footprint (typically over 1s and 1MB per image), which cannot scale to real-world appli-cations with large QPS (Queries Per Second) and reference database (> 1M images).
Recently vision transformer [17] has achieved signifi-cant performance on a wide range of vision tasks, and has been shown to have a high potential for VPR [53] (Sec. 2).
However, the predominant local features [26, 53] are still based on CNN backbones, due to the built-in feature lo-cality with limited receptive fields. Although vision trans-former [17] considers each patch as an input token and natu-rally encodes local information, the local information might be overwritten with global information by the strong global correlation between all tokens in every layer. Therefore, it is still unclear how vision transformer tokens perform in local matching as compared to CNN local features in this field.
In this paper, we integrate the global retrieval and rerank-ing into one unified framework employing only transform-ers (Fig. 1), abbreviated as R2Former, which is simple, ef-ficient, and effective. The global retrieval is tackled based on the class token without additional aggregation modules
[3, 43] and the other image tokens are adopted as local fea-tures. Different from geometric verification which focuses on geometric information between local feature pairs, we feed the correlation between the tokens (local features) of a pair of images, xy coordinates of tokens, and their atten-tion information to transformer modules, so that the mod-ule can learn task-relevant information that could be use-ful for reranking. The global retrieval and reranking parts can be either trained in an end-to-end manner or tuned al-ternatively with a more stable convergence. The proposed reranking module can also be adopted on other CNN or transformer backbones, and our comparison shows that vi-sion transformer tokens are comparable to CNN local fea-tures in terms of reranking performance (Table 6).
Without bells and whistles, the proposed method out-performs both retrieval-only and retrieval+reranking state-of-the-art methods on a wide range of VPR datasets. The proposed method follows a very efficient design and ap-plies linear layers for dimension reduction: i.e. only 256 and 500 × 131 for global and local feature dimensions and only 32 for transformer dimension of reranking module, thus is significantly faster (> 4.7× QPS) with much less (< 22%) memory consumption than previous methods [26, 53]. Both the global and local features are extracted from the same backbone model only once, and the reranking of top-k can-didates is finished with only one forward pass by computing the reranking scores of all candidate pairs in parallel within one batch. The reranking speed can be further boosted by parallel computing on multiple GPUs with > 20× speedup over previous methods [26, 53]. We demonstrate that the proposed reranking module also learns to focus on good lo-cal matches like RANSAC [19]. We summarize our contri-butions as follows:
• A unified retrieval and reranking framework for place recognition employing pure transformers, which demon-strates that vision transformer tokens are comparable and sometimes better than CNN local features in terms of reranking or local matching.
• A novel transformer-based reranking module that learns to attend to the correlation of informative local feature pairs. It can be combined with either CNN or transformer backbones with better performance and efficiency than other reranking methods, e.g. RANSAC.
• Extensive experiments showing state-of-the-art perfor-mance on a wide range of place recognition datasets with significantly less (< 22%) inference latency and memory consumption than previous reranking-based methods. 2.