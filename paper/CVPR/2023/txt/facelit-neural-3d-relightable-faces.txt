Abstract 1.

Introduction
We propose a generative framework, FaceLit, capable of generating a 3D face that can be rendered at various user-defined lighting conditions and views, learned purely from 2D images in-the-wild without any manual annotation. Un-like existing works that require careful capture setup or hu-man labor, we rely on off-the-shelf pose and illumination estimators. With these estimates, we incorporate the Phong reflectance model in the neural volume rendering frame-work. Our model learns to generate shape and material properties of a face such that, when rendered according to the natural statistics of pose and illumination, produces photorealistic face images with multiview 3D and illumina-tion consistency. Our method enables photorealistic gener-ation of faces with explicit illumination and view controls on multiple datasets – FFHQ, MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware GANs on FFHQ dataset achieving an FID score of 3.5.
Learning a 3D generative model from 2D images has re-cently drawn much interest [13, 23, 32]. Since the intro-duction of Neural Radiance Fields (NeRF) [21], the quality of images rendered from a 3D model [6, 13] has improved drastically, becoming as photorealistic as those rendered by a 2D model [19]. While some of them [7,28] rely purely on 3D representations to deliver 3D consistency and pay the price of decreased photorealism, more recent work [6] has further shown that this can be avoided and extreme photo-realism can be obtained through a hybrid setup. However, even so, a shortcoming of these models is that the compo-nents that constitute the scene—the geometry, appearance, and the lighting—are all entangled and are thus not control-lable using user defined inputs.
Methods have been proposed to break this entangle-ment [2, 4, 35], however, they require multiview image col-lections of the scene being modeled and are thus inappli-corresponding authors: {anuragr, otuzel}@apple.com.
R work done while at Apple.
cable to images in-the-wild where such constraint cannot be satisfied easily. Boss et al. [3] loosens this constraint to images of different scenes, but they still require the same object to be seen from multiple views at the end. Moreover, these methods are not generative and therefore need to be trained for each object and cannot generate new objects.
For generative methods [6, 7, 13], geometry and illumina-tion remains entangled. In this work, we demonstrate that one does not require multiple views, and the variability and the volume of already existing datasets [16–18] are enough to learn a disentangled 3D generative model.
We propose FaceLit, a framework that learns a disentan-gled 3D model of a face, purely from images; see Fig. 1.
The high-level idea behind our method is to build a render-ing pipeline that is forced to respect physical lighting mod-els [25,26], similar to [35] but in a framework friendly to 3D generative modeling, and one that can leverage off-the-shelf lighting and pose estimators [10]. In more detail, we em-bed the physics-based illumination model using Spherical
Harmonics [26] within the recent generative Neural Volume
Rendering pipeline, EG3D [6]. We then simply train for re-alism, and since the framework has to then obey physics to generate realistic images, it naturally learns a disentangled 3D generative model.
Importantly, the way we embed physics-based render-ing into neural volume rendering is the core enabler of our method. As mentioned, to allow easy use of existing off-the-shelf illumination estimators [10], we base our method on Spherical Harmonics. We then model the diffuse and specular components of the scene via the Spherical Har-monic coefficients associated with the surface normals and the reflectance vectors, where the diffuse reflectance, the normal vectors and the material specular reflectance are generated by a neural network. While simple, our setup al-lows for effective disentanglement of illumination from the rendering process.
We show the effectiveness of our method using three datasets FFHQ [18], CelebA-HQ [16] and MetFaces [17] and obtain state-of-the-art FID scores among 3D aware gen-erative models. Furthermore, to the best of our knowledge, our method is the very first generative method that can gen-erate 3D faces with controllable scene lighting. Our code is available for research purposes at https://github. com/apple/ml-facelit/.
To summarize, our contributions are:
• we propose a novel framework that can learn a disentan-gled 3D generative model of faces from single views, with which we can render the face with different views and un-der various lighting conditions;
• we introduce how to embed an illumination model in the rendering framework that models the effects of diffuse and specular reflection.
• we show that our method can be trained without any man-ual label and simply with 2D images and an off-the-shelf pose/illumination estimation method.
• we achieve state-of-the-art FID score of 3.5 among 3D
GANs on the FFHQ dataset improving the recent work [6] by 25%, relatively. 2.