Abstract
Human spatial attention conveys information about the regions of visual scenes that are important for perform-ing visual tasks. Prior work has shown that the informa-tion about human attention can be leveraged to beneﬁt var-ious supervised vision tasks. Might providing this weak form of supervision be useful for self-supervised represen-tation learning? Addressing this question requires collect-ing large datasets with human attention labels. Yet, col-lecting such large scale data is very expensive. To address this challenge, we construct an auxiliary teacher model to predict human attention, trained on a relatively small la-beled dataset. This teacher model allows us to generate im-age (pseudo) attention labels for ImageNet. We then train a model with a primary contrastive objective; to this stan-dard conﬁguration, we add a simple output head trained to predict the attention map for each image, guided by the pseudo labels from teacher model. We measure the qual-ity of learned representations by evaluating classiﬁcation performance from the frozen learned embeddings as well as performance on image retrieval tasks (see supplementary material). We ﬁnd that the spatial-attention maps predicted from the contrastive model trained with teacher guidance aligns better with human attention compared to vanilla con-trastive models. Moreover, we ﬁnd that our approach im-proves classiﬁcation accuracy and robustness of the con-trastive models on ImageNet and ImageNet-C. Further, we
ﬁnd that model representations become more useful for im-age retrieval task as measured by precision-recall perfor-mance on ImageNet, ImageNet-C, CIFAR10, and CIFAR10-C datasets.
Figure 1. Illustration. A teacher model is trained to predict human spatial-attention from a small dataset. Then the model is used to provide attention labels for larger dataset, which are used as addi-tional targets for contrastive models. 1.

Introduction
Deep learning models have made signiﬁcant progress and obtained notable success on various vision tasks. De-spite these promising results, humans continue to perform better than deep learning models in many applications. A notable reason is that deep learning models have a tendency to learn “short-cuts”, i.e., giving signiﬁcance to physically meaningless patterns or exploiting features which are pre-dictive in some settings, but not causal [20]. Examples include focusing on less signiﬁcant features such as back-ground and textures [13]. These models yield representa-tions that are less generalizable and lead to models that are highly sensitive to small pixel modulations [42].
Human vision on the other hand is known to be much more robust and generalizable. One major difference be-tween human and machine vision is that humans tend to
⇤Equal technical contribution.
†Equal leadership and advising contribution
Correspondence to: junfenghe@google.com & gamaleldin@google.com
focus on speciﬁc regions in visual scene [45]. These lo-cations often reﬂect regions salient or useful to perform a speciﬁc vision task. Machines, instead, initially place equal signiﬁcance to all regions. A natural question is: will it be beneﬁcial if machine vision models is guided by human spatial attention?
Human spatial attention has been shown to beneﬁt com-puter vision models in supervised tasks, such as classiﬁca-tion [32]. Yet, it is still a question whether adding a form of weak supervision in the form of human spatial atten-tion could similarly beneﬁt self-supervised models that are trained end-to-end. Self-supervised models typically need a large amount of data to yield good representations. To test if training weakly supervised models with human spa-tial attention cues, we will need to collect a large volume of human spatial attention labels, which is a very expen-sive process that requires either using trackers to record eye movements [5,43,52] or asking humans to highlight regions that they attend to [25, 27]. This process is prohibitively te-dious and costly for datasets with millions of examples.
In this work, we test the hypothesis that a weak super-vision in the form of human spatial attention is beneﬁcial for representation learning for models trained with a con-trastive objective.
Inspired by knowledge distillation and self-training using teacher models [47, 49], we address the challenge of obtaining spatial attention labels on large scale image datasets by using machine pseudo-labeling. We train a teacher model on a set of limited ground truth human spa-tial attention labels, and use this teacher model to gener-ate spatial attention pseudo-labels for the large ImageNet benchmark. We are then able to utilize the generated spa-tial attention maps in the contrastive models, and discover that this approach yields representations that are highly pre-dictive of human spatial attention. Further, we ﬁnd that the learned representations are better as measured by higher ac-curacy and robustness on classiﬁcation downstream tasks, and higher precision and recall on image retrieval tasks. In-terestingly, we ﬁnd that the gains from using teacher mod-els to provide pseudo labels are larger than using the lim-ited ground truth human labels directly when training con-trastive models, and the gains are larger for contrastive mod-els than when applying same method to supervised models.
In summary, our contributions are as follows:
•
We create a dataset with spatial attention maps for the
ImageNet [37] benchmark by ﬁrst training a teacher model to predict human spatial attention labels from
Salicon dataset [25] and then use the model to label
ImageNet examples
•
We use spatial-attention labels from the teacher model as an additional prediction target to models trained
Trained teacher model is available at: https://github.com/google-research/google-research/tree/master/human attention/ with contrastive objective.
•
We ﬁnd that the proposed method can learn bet-ter representation, leading to better accuracy and ro-bustness for downstream classiﬁcation tasks (on Im-ageNet and ImageNet-C), and better performance on retrieval tasks (on ImageNet, ImageNet-C, CIFAR-10, and CIFAR10-C). 2.