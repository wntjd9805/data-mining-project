Abstract
Dataset distillation, also known as dataset condensation, aims to compress a large dataset into a compact synthetic one. Existing methods perform dataset condensation by as-suming a fixed storage or transmission budget. When the budget changes, however, they have to repeat the synthe-sizing process with access to original datasets, which is highly cumbersome if not infeasible at all. In this paper, we explore the problem of slimmable dataset condensation, to extract a smaller synthetic dataset given only previous condensation results. We first study the limitations of exist-ing dataset condensation algorithms on such a successive compression setting and identify two key factors: (1) the in-consistency of neural networks over different compression times and (2) the underdetermined solution space for syn-thetic data. Accordingly, we propose a novel training ob-jective for slimmable dataset condensation to explicitly ac-count for both factors. Moreover, synthetic datasets in our method adopt a significance-aware parameterization. The-oretical derivation indicates that an upper-bounded error can be achieved by discarding the minor components with-out training. Alternatively, if training is allowed, this strat-egy can serve as a strong initialization that enables a fast convergence. Extensive comparisons and ablations demon-strate the superiority of the proposed solution over existing methods on multiple benchmarks. 1.

Introductions
The success of deep learning is largely attributed to the enormous amount of training data [5,8,12,15,21,36,37,41, 50]. However, the massive data not only inevitably intro-duces heavy burdens on storage and transmission but also incommodes many applications that require training over datasets multiple times, such as hyper-parameter optimiza-tion [3, 9, 16, 29, 30] and neural architecture search [10, 24, 43, 49]. Moreover, it raises concerns on privacy and copyright if raw datasets are published and accessed di-*Corresponding Author.
Figure 1. Scenarios where slimmable dataset condensation is use-ful: (a) adapting to devices with different storage budgets, (b) con-tinual learning using a synthetic buffer with a fixed size, and (c) federated learning with synthetic data where a dynamic number of participants share the network bandwidth. rectly [7, 40, 51]. These issues can be largely alleviated by using smaller datasets containing only a few synthetic samples but with performance similar to the original ones.
How to compress a given real dataset into such a synthetic dataset is the main focus of dataset distillation, also known as dataset condensation (DC), whose concept is introduced by Wang et al. [45] and further developed by a series of following works recently [2,17,28,33,34,44,54,55,57,58].
Specifically, existing DC approaches work under a pre-defined storage budget, e.g., the number of images per class.
Although it has been demonstrated that most performance of original datasets can be recovered by the synthetic ones with only a few synthetic samples, the fixed storage budget does not take the variations of the storage budget into con-sideration. Some examples are shown in Fig. 1. On the one hand, different devices may have different storage and transmission resources. On the other hand, in applications
like continual learning [4,31,32,38,39,46,52] and federated learning [11, 13, 19, 25, 26, 42, 48, 59], the storage and trans-mission budgets may change on different occasions, since a replay buffer with a static memory size needs to take ac-count for more and more historical data, and the bandwidth allocated to each participant is smaller with the increas-ing number of participants, respectively. In these scenar-ios where it is necessary to adapt to different capacities on storage and transmission, or the budget is changed, exist-ing algorithms have to repeat the synthesizing process for the newly-defined budget with access to original datasets, which is largely cumbersome if not infeasible at all due to the lack of original data.
In this paper, we phrase the task of re-condensing a syn-thetic dataset, derived from dataset distillation per se, as slimmable dataset condensation (slimmable DC). In fact, it even remains unclear whether a valid synthetic dataset can be re-condensed from only previously condensed samples.
Unfortunately, we find that the answer is negative for exist-ing state-of-the-art methods [28, 33, 34, 58]. The basic idea of these methods is to optimize the validation error on real datasets for models trained by synthetic ones. Although the solution is effective for the original DC setting, it is not the case for slimmable DC. Specifically, we reveal that since the synthetic data for re-condensation are much less than the original ones, existing methods suffer from two main is-sues: (1) the performance is sensitive to the inconsistency of neural networks adopted on different occasions of compres-sion, and (2) solution spaces for re-condensed datasets be-come underdetermined, which triggers deviations in train-ing results and further leads to inferior performances.
To address these drawbacks, we propose to explicitly regulate the consistency between the training effects using synthetic datasets before and after a condensation step for slimmable DC. Specifically, the proposed objective is com-posed of two terms: first-order and infinity-order parame-ter matching, which are designed to explicitly account for the two aforementioned issues. The former encourages a unified embedding space over different training iterations, while the latter enforces the consistency of final training pa-rameters in such a space. Optimized with the proposed ob-jective function, we achieve favorable results for slimmable
DC: the performance of a further condensed dataset from a previously condensed one effectively approaches that ob-tained with access to the real dataset.
Moreover, for an efficient slimming procedure, we ex-plore a significance-aware synthetic dataset parameteriza-tion, which explicitly embeds a linear space with orthogonal bases and askew-distributed singular values during training.
Theoretical derivation indicates an upper-bounded error by discarding the minor components, i.e., bases with the small-est singular values. This strategy may serve as either a learning-free slimmable DC solution or a strong initializa-tion in learning-based settings to accelerate convergence.
We conduct extensive experiments on multiple bench-marks and applications, including continual learning and federated learning, and demonstrate the effectiveness of the proposed solution. Results suggest that our method out-performs all state-of-the-art baselines by a large margin on slimmable DC. Our contributions are summarized below:
• We introduce the task of slimmable dataset condensa-tion beyond the typical DC setting, which alleviates the dilemma of existing DC methods when the budget changes for storage or transmission.
• We delve into the limitations of existing algorithms for typical DC and propose a novel first-order and infinity-order matching-based training objective pertinently for slimmable DC.
• We propose a significance-aware synthetic dataset parameterization with a theoretical guarantee for learning-free slimmable DC or initialization to accel-erate convergence in learning-based settings.
• Experiments on multiple benchmarks and applications demonstrate the effectiveness of the proposed method and its superiority over state-of-the-art baselines. 2.