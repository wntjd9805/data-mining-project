Abstract
Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually-grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the im-ages via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original Vis-Dial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic di-alogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on Vis-Dial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual ad-versarial attacks. Finally, GST yields strong performance gains in the low-data regime. Code is available at https:
//github.com/gicheonkang/gst-visdial. 1.

Introduction
Recently, there has been extensive research towards de-veloping visually-grounded dialog systems [12, 13, 34, 36] due to their significance in many real-world applications (e.g., helping visually impaired person). Notably, Visual Di-alog (VisDial) [12] has provided a testbed for studying such systems, where a dialog agent should answer a sequence of image-grounded questions. For instance, the agent is ex-pected to answer open-ended questions like “What color is it?” and “How old does she look?”. This task requires a holistic understanding of visual information, linguistic se-*Equal contribution mantics in context (e.g., it and she), and most importantly, the grounding of these two.
Most of the previous approaches in VisDial [9, 10, 18, 20, 25, 26, 30, 31, 35, 49, 54, 55, 64, 67, 78, 84] have trained the dialog agents solely on VisDial data via supervised learning.
More recent studies [8,53,77] have employed self-supervised pre-trained models such as BERT [14] or ViLBERT [48] and finetuned them on VisDial data. The models are typi-cally pre-trained to recover masked inputs and predict the semantic alignment between two segments. This pretrain-then-transfer learning strategy has shown promising results by transferring knowledge from the models pre-trained on large-scale data sources [4, 71, 85] to VisDial.
Our research question is the following: How can the dia-log agent expand its knowledge beyond what it can acquire via supervised learning or self-supervised pre-training on the provided datasets? Some recent studies have shown that semi-supervised learning and pre-training have complemen-tary modeling capabilities in image [86] and text classifica-tion [16]. Inspired by them, we consider semi-supervised learning (SSL) as a way to address the above question.
Let us assume that large amounts of unlabeled images are available. SSL for VisDial can be applied to generate synthetic conversations for the unlabeled images and train the agent with the synthetic data. However, there are two critical challenges to this approach. First, the target output for VisDial (i.e., multi-turn visual QA data) is more complex than that of the aforementioned studies [16, 86]. Specifically, they have addressed the classification problems, yielding class probabilities as pseudo labels [39]. In contrast, SSL for VisDial should generate a sequence of pseudo queries (i.e., visual questions) and pseudo labels (i.e., corresponding answers) in natural language to train the answering agent.
It further indicates that the target output should be generated while considering the multimodal and sequential nature of the visual dialog task. Next, even if SSL yields synthetic dialogs via text generation, there may be noise, such as generating irrelevant questions or incorrect answers to given contexts. A robust training method is required to leverage such noisy synthetic dialog datasets.
In this paper, we study the above challenges in the con-text of SSL, especially self-training [6, 16, 21, 28, 32, 39, 44, 52, 60, 65, 72, 73, 79, 80, 86], where a teacher model trained on labeled data predicts the pseudo labels for unlabeled data. Then, a student model jointly learns on the labeled and the pseudo-labeled datasets. Unlike existing studies in self-training that have mainly studied uni-modal, discrimi-native tasks such as image classification [72, 80, 86] or text classification [16, 32, 52], we extend the idea of self-training to the task of multimodal conditional text generation.
To this end, we propose a new learning strategy, called
Generative Self-Training (GST), that artificially generates multi-turn visual QA data and utilizes the synthetic data for training. GST first trains the teacher model (answerer) and the visual question generation model (questioner) using
VisDial data. It then retrieves a set of unlabeled images from a Web image dataset, Conceptual 12M [7]. Next, the questioner and the teacher generate a series of visual QA pairs for the retrieved images. Finally, the student is trained on the synthetic and the original VisDial data. We also pro-pose perplexity-based data selection (PPL) and multimodal consistency regularization (MCR) to effectively train the student with the noisy dialog data. PPL is to selectively utilize the answers whose perplexity of the teacher is below a threshold. MCR encourages the student to yield consis-tent predictions when the perturbed multimodal inputs are given. As a result, GST successfully augments the synthetic
VisDial data (11.7M QA pairs), thus mitigating the need to scale up the size of the human-annotated VisDial data, which is prohibitively expensive and time-consuming.
Our key contributions are three-fold. First, we propose
Generative Self-Training (GST) that generates multi-turn visual QA data to leverage unlabeled Web images effectively.
Second, experiments show that GST achieves new state-of-the-art performance on VisDial v1.0 and v0.9 datasets.
We further demonstrate two important results: (1) GST is indeed effective when the human-annotated visual dialog data is extremely scarce (improving up to 11.09 absolute points on NDCG), and (2) PPL and MCR are effective when training the noisy synthetic dialog data. Third, to validate the robustness of GST, we evaluate our proposed method under three different visual and textual adversarial attacks, i.e., FGSM, coreference, and random token attacks. We observe that GST significantly improves the performance compared with the baseline models against all adversarial attacks, especially boosting NDCG scores from 21.60% to 45.43% in the FGSM attack [19]. 2.