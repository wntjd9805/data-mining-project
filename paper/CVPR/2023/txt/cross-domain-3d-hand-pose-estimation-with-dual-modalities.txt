Abstract
Recent advances in hand pose estimation have shed light on utilizing synthetic data to train neural networks, which however inevitably hinders generalization to real-world data due to domain gaps. To solve this problem, we present a framework for cross-domain semi-supervised hand pose estimation and target the challenging scenario of learning models from labelled multi-modal synthetic data and unlabelled real-world data. To that end, we propose a dual-modality network that exploits synthetic RGB and synthetic depth images. For pre-training, our network uses multi-modal contrastive learning and attention-fused super-vision to learn effective representations of the RGB images.
We then integrate a novel self-distillation technique during fine-tuning to reduce pseudo-label noise. Experiments show that the proposed method significantly improves 3D hand pose estimation and 2D keypoint detection on benchmarks. 1.

Introduction
Hand pose estimation supports a wide range of appli-cations, including sign language recognition [18, 19] and gesture-based interaction systems [1]. However, it is dif-ficult to obtain the large amounts of accurate ground truth labels required for training deep-learning-based hand pose estimation systems. Training models with synthetic data is one option, but such models exhibit a sim-to-real domain gap and generalize poorly to real-world settings. More so-phisticated synthesis can narrow this gap, but the perfor-mance drop is still noticeable [23].
This paper addresses the cross-domain pose estimation problem and focuses on a semi-supervised setting. We tar-get learning from labelled synthetic data and unlabelled real-world data for application to real-world data. Pre-training with synthetic data is common [14, 32]; surpris-ingly, only RGB synthetic images have been considered.
Yet when generating synthetic data, it is relatively easy to render multiple data modalities. For example, the RHD
*Equal contribution
Figure 1. Attention comparisons between RGB and depth maps.
Apart from the hand region, the RGB attention shows high acti-vation on the background. The depth map attention however pro-vides high activation only on the hand, confirming its strength to focus on task-relevant information. dataset [41] features both RGB images and depth maps.
Across the modalities, there are shared common visual cues relating to the underlying geometry or semantics that are task-relevant for hand pose estimation. To exploit such information during pre-training, a simple solution is to ap-ply pixel-wise ℓ1 alignment between feature maps of RGB and depth maps [37]. However, the large discrepancy be-tween RGB images and depth maps might cause the pixel-wise alignment to also focus on irrelevant regions related to the background. Fig. 1 shows the attention derived from
RGB versus depth maps and their overlays on the original
RGB input. We observe that the RGB attention is strong in task-related areas, i.e. the hand, but also on unrelated areas of the background. In contrast, the attention from the depth map is successfully localized on the hand.
To focus more on relevant regions, we propose a dual-modality network for RGB images and depth maps that improves RGB-based hand pose estimation via a multi-level alignment. Specifically, we design an attention mod-ule to apply information learned from depth maps to the
RGB image to produce attention-fused RGB features. The learned information is then aligned to RGB features by multi-modal supervision on the predictions from all modal-ities with ground truth. This limits the RGB encoder’s sen-sitivity to non-informative cues such as background or ir-relevant textures [10, 34]. At the feature space level, we ex-plore intra-modal and inter-modal contrastive learning for multi-modal data. Our work is the first to investigate su-pervised multi-modal contrastive learning on hand pose es-timation. In particular, contrastive learning between RGB features and attention-fused RGB features minimizes fea-ture discrepancies across modalities.
After pre-training, pseudo-labelling is commonly used to fine-tune on unlabelled data [4,20,22]. However, na¨ıvely generated pseudo-labels are inevitably noisy and deteriorate model performance. To handle noisy pseudo-labels, we de-sign a two-pass self-distillation procedure (see Fig. 2 (b)).
The RGB input is first applied through the multimodal de-coder to predict a depth map and pose. In a second pass, the same RGB input is applied together with the predicted depth map to estimate the pose. As the attention from depth maps activates RGB features in relevant regions, the two passes can be considered a refinement or denoising process.
The pose predicted from the RGB in the first pass is encour-aged to be consistent with the pose from the attention-fused version in the second pass. Such a procedure distills knowl-edge from within the network itself. The self-distillation also prevents the network from over-fitting to noisy samples that often have unstable predictions [17, 20].
In summary, we make the following contributions: 1. We propose a dual-modality network that learns from
RGB images and depth maps but is applicable to only RGB inputs for fine-tuning and inference. The network features a specially designed attention mod-ule that identifies geometric relationships common to
RGB and depth from stand-alone RGB images. 2. We propose the first supervised multi-modal con-trastive learning method based on fused features to minimize feature discrepancies across modalities. 3. We introduce a self-distillation procedure to exploit yet not over-fit to noisy pseudo-labels during fine-tuning. 4. The proposed method significantly improves the state-of-the-art by up to 16.0% and 14.8% for 2D keypoint detection and 3D keypoint estimation respectively. 2.