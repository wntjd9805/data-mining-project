Abstract
Input image
Concept bottleneck representation
Interpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI pro-vides a way to address this challenge, mostly by provid-ing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some re-cent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bot-tleneck Concept Learner (BotCL), which represents an im-age solely by the presence/absence of concepts learned through training over the target task without explicit super-vision over the concepts. It uses self-supervision and tai-lored regularizers so that learned concepts can be human-understandable. Using some image classiﬁcation tasks as our testbed, we demonstrate BotCL’s potential to rebuild neural networks for better interpretability 1. 1.

Introduction
Understanding the behavior of deep neural networks (DNNs) is a major challenge in the explainable AI (XAI) community, especially for medical applications [19,38], for identifying biases in DNNs [2, 18, 42], etc. Tremendous re-search efforts have been devoted to the post-hoc paradigm for a posteriori explanation [29, 33]. This paradigm pro-duces a relevance map to spot regions in the input image that interact with the model’s decision. Yet the relevance map only tells low-level (or per-pixel) relationships and does not explicitly convey any semantics behind the decision. Inter-pretation of relevance maps may require expert knowledge.
The concept-based framework [22, 37, 50] is inspired by the human capacity to learn a new concept by (subcon-sciously) ﬁnding ﬁner-grained concepts and reuse them in different ways for better recognition [24]. Instead of giv-ing per-pixel relevance, this framework offers higher-level
*Corresponding author. 1Code is avaliable at https://github.com/wbw520/BotCL and a simple demo is available at https://botcl.liangzhili.com/.
Cpt.1
Cpt.2
Cpt.3
Cpt.4
Cpt.5
Cpt.6
Dataset
Discovered concepts
Figure 1. Examples of concepts discovered by BotCL in Ima-geNet [10] and concepts in the input image. BotCL automatically discovers a set of concepts optimized for the target task and repre-sents an image solely with the presence/absence of concepts. relationships between the image and decision mediated by a limited number of concepts. That is, the decision is ex-plained by giving a set of concepts found in the image. The interpretation of the decision is thus straightforward once the interpretation of each concept is established.
Some works use concepts for the post-hoc paradigm for better interpretation of the decision [14, 50], while the link between the decision and concepts in the image is not ob-vious. The concept bottleneck structure [23] uses the pres-ence/absence of concepts as image representation (referred to as concept activation). The classiﬁer has access only to the concept activation, so the decision is strongly tied to the concepts. This bottleneck structure has become the main-stream of the concept-based framework [5, 20, 28, 31].
A major difﬁculty in this framework is designing a set of concepts that suits the target task. A promising approach is handcrafting them [4, 21, 48], which inherently offers bet-ter interpretability at the cost of extra annotations on the concepts. Recent attempts automatically discover concepts
[1, 13, 14, 46]. Such concepts may not always be consis-tent with how humans (or models) see the world [25, 47] and may require some effort to interpret them, but concept discovery without supervision is a signiﬁcant advantage.
Inspired by these works, we propose bottleneck concept learner (BotCL) for simultaneously discovering concepts and learning the classiﬁer. BotCL optimizes concepts for the given target image classiﬁcation task without supervi-sion for the concepts. An image is represented solely by the existence of concepts and is classiﬁed using them. We adopt a slot attention-based mechanism [26, 27] to spot the region in which each concept is found. This gives an extra signal for interpreting the decision since one can easily see what each learned concept represents by collectively show-ing training images with the detected concepts. Figure 1 shows examples from ImageNet [10]. BotCL discovers a predeﬁned number of concepts in the dataset, which are ex-empliﬁed by several images with attention maps. An image of Great White Shark is represented by the right part of mouth (Cpt.1) and ﬁns (Cpt.3). BotCL uses a single fully-connected (FC) layer as a classiﬁer, which is simple but enough to encode the co-occurrence of each concept and each class.
Contribution. For better concept discovery, we propose to use self-supervision over concepts, inspired by the re-cent success in representation learning [9, 16]. Our ablation study demonstrates that self-supervision by contrastive loss is the key. We also try several constraints on concepts them-selves, i.e., individual consistency to make a concept more selective and mutual distinctiveness for better coverage of various visual elements. These additional constraints regu-lar the training process and help the model learn concepts of higher quality. 2.