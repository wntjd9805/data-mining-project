Abstract
The success of deep learning based face recognition sys-tems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the dig-ital world. Existing methods for enhancing privacy fail to generate “naturalistic” images that can protect facial privacy without compromising user experience. We pro-pose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Sub-sequently, user-defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experi-ments demonstrate that faces generated by our approach have stronger black-box transferability with an absolute gain of 12.06% over the state-of-the-art facial privacy pro-tection approach under the face verification task. Finally, we demonstrate the effectiveness of the proposed approach for commercial face recognition systems. Our code is avail-able at https://github.com/fahadshamshad/Clip2Protect. 1.

Introduction
Deep learning based face recognition (FR) systems [43, 61] have found widespread usage in multiple applications,
Table 1. Comparison among different facial privacy protection methods w.r.t. the natural outputs, black box setting, experiments under face verification and identification tasks, unrestricted (se-mantically meaningful), and more flexible text guided adversaries.
Adv-Makeup [71] TIP-IM [70] AMT-GAN [22] Ours
Natural outputs
Black box
Verification
Identification
Unrestricted
Text guided
Yes
Yes
Yes
No
Yes
No
Partially
Yes
No
Yes
No
No
Partially
Yes
Yes
No
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes including security [63], biometrics [38], and criminal in-vestigation [45], outperforming humans in many scenar-ios [12, 48, 61]. Despite positive aspects of this technol-ogy, FR systems seriously threaten personal security and privacy in the digital world because of their potential to enable mass surveillance capabilities [1, 67]. For exam-ple, government and private entities can use FR systems to track user relationships and activities by scraping face im-ages from social media profiles such as Twitter, Linkedin, and Facebook [18, 20]. These entities generally use propri-etary FR systems, whose specifications are unknown to the public (black box model). Therefore, there is an urgent need for an effective approach that protects facial privacy against such unknown FR systems.
An ideal facial privacy protection algorithm must strike the right balance between naturalness and privacy protec-tion [70, 77].
In this context, “naturalness” is defined as the absence of any noise artifacts that can be easily per-ceived by human observers and the preservation of human-perceived identity. “Privacy protection” refers to the fact that the protected image must be capable of deceiving a black-box malicious FR system.
In other words, the pro-tected image must closely resemble the given face image and be artifact-free for a human observer, while at the same time fool an unknown automated FR system. Since fail-ure to generate naturalistic faces can significantly affect user experience on social media platforms, it is a necessary pre-condition for adoption of a privacy-enhancement algorithm.
Recent works exploit adversarial attacks [57] to conceal user identity by overlaying noise-constrained (bounded) ad-versarial perturbations on the original face image [6,53,74].
Since the adversarial examples are generally optimized in the image space, it is often difficult to simultaneously achieve naturalness and privacy [70]. Unlike noise-based methods, unrestricted adversarial examples are not con-strained by the magnitude of perturbation in the image space and have demonstrated better perceptual realism for human observers while being adversarially effective [3, 55, 68, 76].
Several efforts have been made to generate unrestricted adversarial examples that mislead FR systems (see Tab. 1) [22, 25, 39, 72]. Among these, adversarial makeup based methods [22, 72] are gaining increasing attention as they can embed adversarial modifications in a more natural way.
These approaches use generative adversarial networks [15] (GANs) to adversarially transfer makeup from a given ref-erence image to the user’s face image while impersonating a target identity. However, existing techniques based on adversarial makeup transfer have the following limitations: (i) adversarial toxicity in these methods hamper the perfor-mance of the makeup transfer module, thereby resulting in unnatural faces with makeup artifacts (see Fig. 1); (ii) the use of a reference image to define the desired makeup style affects the practicality of this approach; (iii) for every new target identity, these approaches require end-to-end retrain-ing from scratch using large makeup datasets; and (iv) most of these methods primarily aim at impersonation of the tar-get identity, whereas the desired privacy objective is dodg-ing, i.e., multiple images of the user’s face scraped from different social media sites must not match with each other.
To mitigate the above problems, we propose a new ap-proach to protect user facial privacy on online platforms (Sec. 3). The proposed approach aims to search for adver-sarial latent codes in a low-dimensional manifold learned by a generative model trained to generate face images
[2, 27]. Our main contributions are:
• Facial Privacy-protection Framework Using Ad-versarial Latent Codes: Given a face image, we pro-pose a novel two-step method to search for adversarial latent codes, which can be used by a generative model (e.g., StyleGAN) to produce face images with high visual quality that matches human-perceived identity, while deceiving black-box FR systems.
• Adversarial Makeup Transfer using Textual
Prompts: A critical component of the above frame-work is a technique for leveraging user-defined textual (makeup) prompts to traverse over the latent manifold of the generative model and find transferable adversar-ial latent codes. Our approach effectively hides attack information in the desired makeup style, without the need for any large makeup dataset or retraining of models for different target identities.
• Identity Preserving Regularization: We propose a regularizer that preserves identity-related attributes within the latent space of the generative model and en-sures that the protected face image visually resembles the original face.
Extensive experiments (Sec. 4.1) for both face verification and identification scenarios demonstrate the effectiveness of our approach against black-box FR models and online com-mercial facial recognition APIs (Sec. 4.2). Furthermore, we provide detailed ablative analysis to dissect the performance of different components of our approach (Sec. 4.3).
2.