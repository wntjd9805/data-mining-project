Abstract
Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style de-fined with a few reference images in a target domain. How-ever, the content feature extracted using a representative font might not be optimal. In light of this, we propose a con-tent fusion module (CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also al-lows to optimize the style representation vector of reference images through a lightweight iterative style-vector refine-ment (ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruc-tion loss (namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. We
*This work was done during an internship at Alibaba Group.
†Corresponding author. have evaluated our method on a dataset of 300 fonts with 6.5k characters each. Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. The source code can be found at https:// github.com/ wangchi95/ CF-Font. 1.

Introduction
Few-shot font generation aims to produce characters of a new font by transforming font images from a source do-main to a target domain according to just a few reference images.
It can greatly reduce the labor of expert design-ers to create a new style of fonts, especially for logographic languages that contain multiple characters, such as Chinese (over 60K characters), Japanese (over 50K characters), and
Korean (over 11K characters), since only several reference images need to be manually designed. Therefore, font gen-eration has wide applications in font completion for ancient books and monuments, personal font generation, etc.
Recently, with the rapid development of convolu-tional neural networks [22] and generative adversarial net-works [9] (GAN), pioneers have made great progress in
generating gratifying logographic fonts. Zi2zi [38] intro-duces pix2pix [14] method to generate complex charac-ters of logographic languages with high quality, but it can-not handle those fonts that do not appear in training (un-seen fonts). For the few-shot font generation, many meth-ods [3, 7, 31, 32, 34, 42, 47] verify that content and style dis-entanglement is effective to convert the style of a character in the source domain, denoted as source character, to the target style embodied with reference images of seen or un-seen fonts. The neural networks in these methods usually have two branches to learn content and style features respec-tively, and the content features are usually obtained with the character image from a manually-chosen font, denoted as source font. However, since it’s a difficult task to achieve a complete disentanglement between content and style fea-tures [17, 21], the choice of the font for content-feature en-coding influences the font generation results substantially.
For instance, Song and Kai are commonly selected as the source font [20, 28, 31, 42, 43, 47]. While such choices are effective in many cases, the generated images sometimes contain artifacts, such as incomplete and unwanted strokes.
The main contribution of this paper is a novel content feature fusion scheme to mitigate the influence of incom-plete disentanglement by exploring the synchronization of content and style features, which significantly enhances the quality of few-shot font generation. Specifically, we design a content fusion module (CFM) to take the content features of different fonts into consideration during training and in-ference. It is realized by computing the content feature of a character of a target font through linearly blending con-tent features of the corresponding characters in the auto-matically determined basis fonts, and the blending weights are determined through a carefully designed font-level dis-In this way, we can form a linear cluster tance measure. for the content feature of a semantic character, and explore how to leverage the font-level similarity to seek for an opti-mized content feature in this cluster to improve the quality of generated characters.
In addition, we introduce an iterative style-vector refine-ment (ISR) strategy to find a better style feature vector for font-level style representation. For each font, we average the style vectors of reference images and treat it as a learn-able parameter. Afterward, we fine-tune the style vector with a reconstruction loss, which further improves the qual-ity of the generated fonts.
Most font-generation algorithms [3, 20, 31, 32, 38, 42] choose L1 loss as the character image reconstruction loss.
However, L1 or L2 loss mainly supervises per-pixel accu-racy and is easily disturbed by the local misalignment of details. Hence, we employ a distribution-based projected character loss (PCL) to measure the shape difference be-tween characters. Specifically, by treating the 1D projec-tion of 2D character images as a 1D probability distribution,
PCL computes the distribution distance to pay more atten-tion to the global properties of character shapes, resulting in the large improvement of skeleton topology transfer results.
The CFM can be embedded into the few-shot font gen-eration task to enhance the quality of generated results. Ex-tensive experiments verify that our method, referred to as
CF-Font, remarkably outperforms state-of-the-art methods on both seen and unseen fonts. Fig. 1 reveals that our method can generate high-quality fonts of various styles. 2.