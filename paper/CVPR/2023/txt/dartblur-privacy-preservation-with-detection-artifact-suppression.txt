Abstract
Nowadays, privacy issue has become a top priority when training AI algorithms. Machine learning algorithms are expected to benefit our daily life, while personal informa-tion must also be carefully protected from exposure. Fa-cial information is particularly sensitive in this regard.
Multiple datasets containing facial information have been taken offline, and the community is actively seeking solu-tions to remedy the privacy issues. Existing methods for privacy preservation can be divided into blur-based and face replacement-based methods. Owing to the advantages of review convenience and good accessibility, blur-based based methods have become a dominant choice in prac-tice. However, blur-based methods would inevitably intro-duce training artifacts harmful to the performance of down-stream tasks. In this paper, we propose a novel De-artifact
Blurring (DartBlur) privacy-preserving method, which cap-italizes on a DNN architecture to generate blurred faces.
DartBlur can effectively hide facial privacy information while detection artifacts are simultaneously suppressed. We have designed four training objectives that particularly aim to improve review convenience and maximize detec-tion artifact suppression. We associate the algorithm with an adversarial training strategy with a second-order opti-mization pipeline. Experimental results demonstrate that
DartBlur outperforms the existing face-replacement method from both perspectives of review convenience and accessi-bility, and also shows an exclusive advantage in suppress-ing the training artifact compared to traditional blur-based methods. Our implementation is available at https:
//github.com/JaNg2333/DartBlur. 1.

Introduction
Computer vision (CV) technology has been influencing our daily life in many ways. However, successful CV mod-els often have to rely on large-scale datasets collected from real-world scenes, which raises concerning privacy issues.
*These authors contributed equally to this work.
†Lu Fang is the corresponding author (www.luvision.net).
Figure 1. Example faces and anonymized versions by existing methods and DartBlur. As presented, blur-based methods facil-itate review convenience, and face replacement-based methods may fail when the keypoint detector does not work as expected.
Best viewed in color.
The CV community has started to take privacy issues seriously. Existing privacy-preserving methods can be di-vided into blur-based methods (e.g., Block, Gaussian blur,
Pixelation) and face replacement-based methods (e.g., CIA-GAN [24], DeepPrivacy [13], DeIdGAN [18]). Blur-based methods are simple to implement but inevitably introduce additional noise and artifacts into the actual CV task [37].
For example, Gaussian blur patterns are easier to recog-nize. Therefore, face detectors trained on Gaussian blurred
In other words, the distance be-and after blurring. tween hard cases (in terms of recognition) and easy cases on clean data should be maintained in the feature space after blurring.
• Cycle Fidelity: Models trained on blurred data should produce good recognition results on clean testing data.
Given the above considerations, we propose a novel privacy-preserving model called De-artifact Blurring (Dart-Blur). DartBlur is a learnable U-Net model [28] that is fed with Gaussian blurred images and face bounding boxes as input, and outputs detection artifact-suppressed blurred im-ages without relying on other pretrained models like land-mark detection. We propose four training objectives, each specifically addressing the mentioned concerns above, and the implementation resorts to an adversarial training strat-egy with a second-order optimization pipeline. Example images anonymized by existing methods and DartBlur are presented in Figure 1.
The main contributions of this paper can be summarized as follows.
• We propose a new blur-based privacy preservation model DartBlur by taking into account the actual ac-cessibility of the model, review convenience, and de-tection artifact suppression simultaneously.
• DartBlur model is associated with four novel train-ing objectives that each directly addresses the desired properties. We also design an adversarial training strat-egy with a second-order optimization for model train-ing.
• We demonstrate that DartBlur can effectively protect personal privacy while suppressing detection artifacts on various benchmarks. 2.