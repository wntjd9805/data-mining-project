Abstract
Depth estimation is an important step in many computer vision problems such as 3D reconstruction, novel view syn-thesis, and computational photography. Most existing work focuses on depth estimation from single frames. When ap-plied to videos, the result lacks temporal consistency, show-ing flickering and swimming artifacts.
In this paper we aim to estimate temporally consistent depth maps of video streams in an online setting. This is a difficult problem as future frames are not available and the method must choose between enforcing consistency and correcting errors from previous estimations. The presence of dynamic objects fur-ther complicates the problem. We propose to address these challenges by using a global point cloud that is dynami-cally updated each frame, along with a learned fusion ap-proach in image space. Our approach encourages consis-tency while simultaneously allowing updates to handle er-rors and dynamic objects. Qualitative and quantitative re-sults show that our method achieves state-of-the-art quality for consistent video depth estimation. 1.

Introduction
Depth reconstruction is a long-standing, fundamental problem in computer vision. For decades the most popular depth estimation techniques were based on stereo match-ing [37] or structure-from-motion [36]. However, more re-cently the best results have come from learning-based ap-proaches [7]. As the overall reconstruction quality has im-proved, focus has shifted to new areas such as monocular es-timation [21,32,33], edge quality [58], and temporal consis-tency [20, 26]. The latter is particularly important for video applications in computational photography and virtual re-ality [3, 52] as inconsistent depth can cause objectionable flickering and swimming artifacts.
Consistent video depth estimation, however, remains a difficult problem as even the best method will suffer from unpredictable errors and imperfections based on scene con-tent, especially in textureless and specular regions. This difficulty is aggravated by many of the aforementioned ap-plications requiring online reconstruction: future frames are
Figure 1. Comparing monocular depth estimation on the ScanNet dataset across four frames. Clockwise from Top-left: Input RGB image, Ranftl et al.’s DPT [32], our result with a DPT backbone,
RGB-D sensor ground truth. not known beforehand and temporal consistency must be balanced with error-correction. Furthermore, the presence of dynamic objects — which are inherently inconsistent — adds an additional layer of complication. Luo et al. [26] address these problems by assuming all frames are known beforehand and fine-tuning their method for each input video. Other approaches seek an online solution by encod-ing consistency in network weights either through a training loss [21], by using recurrent architectures [8, 30, 56], or by conditioning it on the input [24]. Each method, however, ultimately relies on the raw output of a neural network be-ing consistent, which is difficult to achieve due to camera noise and aliasing in convolutional networks [14, 44, 57].
In this work, we propose the use of a global point cloud to encourage temporal consistency in online video depth es-timation. We demonstrate how to tackle the twin problems of handling dynamic objects and updating a static — and potentially erroneous — point cloud when future frames are not known. With quantitative and qualitative results, we show our approach significantly improves the temporal consistency of both stereo and monocular depth estimation without sacrificing spatial quality. In summary, our contri-butions are as follows:
Figure 2. We generate temporally consistent depth maps for each RGB video frame t by fusing the projected depth from a prior point cloud dt p to update dynamic regions, followed by spatial fusion with dt based p with the estimated depth dt. This is done by temporally fusing dt on confidence estimates of accuracy. The final result dt o is used to update the point cloud for the next frame. 1. We propose point cloud-based fusion for temporally consistent video depth estimation. 2. We present a three-stage approach to encourage con-sistency in online settings, while simultaneously al-lowing updates to improve the accuracy of reconstruc-tion and handle dynamic scenes. 3. We present an image-space approach to dynamics es-timation and depth fusion that is lightweight and has low runtime overhead. 2.