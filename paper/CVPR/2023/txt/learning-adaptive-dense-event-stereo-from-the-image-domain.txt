Abstract
Recently, event-based stereo matching has been studied due to its robustness in poor light conditions. However, existing event-based stereo networks suffer severe perfor-mance degradation when domains shift. Unsupervised do-main adaptation (UDA) aims at resolving this problem with-out using the target domain ground-truth. However, tradi-tional UDA still needs the input event data with ground-truth in the source domain, which is more challenging and costly to obtain than image data. To tackle this issue, we propose a novel unsupervised domain Adaptive Dense
Event Stereo (ADES), which resolves gaps between the dif-ferent domains and input modalities. The proposed ADES framework adapts event-based stereo networks from abun-dant image datasets with ground-truth on the source do-main to event datasets without ground-truth on the target domain, which is a more practical setup. First, we pro-pose a self-supervision module that trains the network on the target domain through image reconstruction, while an artifact prediction network trained on the source domain as-sists in removing intermittent artifacts in the reconstructed image. Secondly, we utilize the feature-level normalization scheme to align the extracted features along the epipolar line. Finally, we present the motion-invariant consistency module to impose the consistent output between the per-turbed motion. Our experiments demonstrate that our ap-proach achieves remarkable results in the adaptation ability of event-based stereo matching from the image domain. 1.

Introduction
Stereo matching [22, 41] is one of the most widely used methods for obtaining 3D information by establishing cor-respondences between stereo images. With considerable in-terest, learning-based stereo methods have achieved state-of-the-art performance in many benchmark datasets. How-ever, some challenges in stereo matching still exist due to the shortcoming of sensors (e.g., low dynamic range, motion blur due to large exposure time). Event cameras
[3] are novel sensors that asynchronously report per-pixel changes of intensity by imitating the human eye. Thanks
Figure 1. The proposed ADES framework for adaptive dense event stereo network. ADES aims to exploit the existing frame-based stereo dataset for learning the event stereo network. to the high dynamic range and low latency, the event cam-era can be considered as a promising sensor for depth es-timation, especially in driving scenarios. Recent works
[2,8,9,28,30,48,59] have attempted to utilize event cameras for stereo matching even under poor light conditions.
Despite advances in event stereo, most prior works [9, 28, 48] still experience a significant degradation in perfor-mance when domains shift. Unsupervised domain adap-tation (UDA) can resolve this problem without using the target domain ground-truth. When UDA is applied for event stereo domain adaptation, it still needs the input event data with ground-truth in the source domain. However, as mentioned in [32], accurate synchronization of events with high temporal resolution and other devices (e.g., Li-DAR) requires additional hardware and post-processing, so it is more challenging to obtain accurate ground truth than images. In this paper, we draw attention to large im-age datasets with ground-truth, which are easily accessible (e.g., DrivingStereo [57], SceneFlow [24] and KITTI [25]).
In this setup, abundant image data from diverse environ-ments helps the event stereo network improve generaliz-ability with high performance. To this end, as shown in
Fig. 1, we propose a novel Adaptive Dense Event Stereo (ADES), which adapts the stereo network from the source domain having image data with ground-truth to the target domain having event data without ground-truth. ADES re-solves gaps between the different domains and input modal-ities.
The proposed ADES framework consists of three com-ponents: smudge-aware self-supervision module, feature normalization, and motion-invariant consistency module.
The proposed smudge-aware self-supervision module lever-ages dense traits of images via image reconstruction on the event target domain. Image reconstruction using only the event is often interrupted by blurry artifacts, what we call a smudge, so the network cannot estimate the sharp and ac-curate disparity map. To predict the smudge effect in the target domain, we design the self-supervision pipeline on the source image domain to estimate and suppress the arti-fact area in the reconstructed image on the target domain.
In addition, we exploit the feature normalization be-fore generating the cost volume. Normalization scheme
[29, 43, 49, 58] was generally used in the domain adapta-tion between the image modalities. However, due to the characteristics of the event, it is not efficient to normalize over the entire pixel area. Since most of the events are trig-gered around an edge of objects, some regions (e.g., sky) have very sparse events. Therefore, vanilla normalization can mislead the values of features to shift to the values of the regions without events. While reducing the difference in features between the two domains, we apply a normal-ization along the epipolar line to take into account the char-acteristics of events and stereo matching.
Finally, we focus on the different motion of event cam-eras from the source and target domains, leading to a severe domain gap. Therefore, we present the motion-invariant consistency module to predict consistent disparity even if the camera motion changes to some extent. This module help the network to adapt the target domain and also reduces the gap from camera motion. To the best of our knowledge, our work is the first attempt to move from unpaired image domain to event domain for stereo matching. Our main con-tributions are summarized as below:
• Our work is the first that transfers the disparity estima-tion task from the rich image dataset with ground-truth to the event stream, resolving gaps between the differ-ent domains and input modalities.
• We propose a novel adaptive event stereo network,
ADES, containing the smudge-aware self-supervision module, feature normalization, and motion-invariant consistency module.
• Extensive experiments demonstrate that the ADES framework achieves significantly better performance than the prior works in the adaptation ability between the different domains and modalities for event stereo. 2.