Abstract
Sound source localization is a typical and challenging task that predicts the location of sound sources in a video.
Previous single-source methods mainly used the audio-visual association as clues to localize sounding objects in each image. Due to the mixed property of multiple sound sources in the original space, there exist rare multi-source approaches to localizing multiple sources simultaneously, except for one recent work using a contrastive random walk in the graph with images and separated sound as nodes.
Despite their promising performance, they can only handle a fixed number of sources, and they cannot learn compact class-aware representations for individual sources. To al-leviate this shortcoming, in this paper, we propose a novel audio-visual grouping network, namely AVGN, that can di-rectly learn category-wise semantic features for each source from the input audio mixture and image to localize multi-ple sources simultaneously. Specifically, our AVGN lever-ages learnable audio-visual class tokens to aggregate class-aware source features. Then, the aggregated semantic fea-tures for each source can be used as guidance to localize the corresponding visual regions. Compared to existing multi-source methods, our new framework can localize a flexible number of sources and disentangle category-aware audio-visual representations for individual sound sources.
We conduct extensive experiments on MUSIC, VGGSound-Instruments, and VGG-Sound Sources benchmarks. The results demonstrate that the proposed AVGN can achieve state-of-the-art sounding object localization performance on both single-source and multi-source scenarios. Code is available at https://github.com/stoneMo/
AVGN . 1.

Introduction
When we hear a dog barking, we are naturally aware of where the dog is in the room due to the strong correspon-dence between audio signals and visual objects in the world.
In the meanwhile, we are capable of separating individual sources from a mixture of multiple sources in the daily envi-*Corresponding author.
Figure 1. Comparison of our AVGN with state-of-the-art methods on single-source (Top Row) and multi-source (Bottom Row) sound localization on MUSIC [51], VGGSound-Instruments [23], and
VGG-Sound Sources [9] benchmarks. ronment. This human perception intelligence attracts many researchers to explore audio-visual joint learning for visual sound source localization.
Visual sound source localization is a typical and chal-lenging task that predicts the location of sound sources in a video. To tackle this problem, early single-source meth-ods [1, 9, 21, 30, 31, 39, 40] mainly used the audio-visual as-sociation as clues to localize sounding objects in the frame.
Typically, Attention10k [39] introduced a two-stream ar-chitecture for audio and images to localize a sound source in the image using an attention mechanism. Based on the attention, Afouras et al. [1] incorporated the optical flow for more accurate localization in a video. To explicitly learn discriminative audio-visual corresponding fragments,
LVS [9] proposed hard sample mining with a differentiable threshold-based contrastive loss, while HardPos [40] uti-lized hard positives from negative pairs in the loss. More recently, EZVSL [31] developed a multiple-instance con-trastive learning framework on the most aligned regions cor-responding to the audio. SLAVC [30] adopted momentum encoders and extreme visual dropout to address overfitting and silence issues in single-source sound localization. How-ever, those baselines are based on the single-source sound as input and they achieve worse performance for sound local-ization from mixtures. In this work, we will solve the prob-lem in our approach by extracting disentangled and compact
representations with learnable audio-visual class tokens as guidance for sound localization. visual class tokens in learning compact representations for sound source localization.
Since multiple sound sources are mixed in the origi-nal space, recent researchers have tried to explore diverse pipelines to localize multiple sources on frames from a sound mixture. This multi-source task requires the model to associate individual sources separated from the mixture with each frame. Qian et al. [38] leveraged a two-stage framework to capture cross-modal feature alignment be-tween sound and vision representations in a coarse-to-fine manner. DSOL [22] introduced a two-stage training frame-work to tackle with silence in category-aware sound source localization. More recently, Mix-and-Localize [23] pro-posed to use a contrastive random walk in the graph with images and separated sounds as nodes, where a random walker was trained to walk from each audio node to an image node with audio-visual similarity as the transition probability. Despite their promising performance, they can only handle a fixed number of sources and they cannot learn compact class-aware representations for individual sources.
In contrast, we can support a flexible number of sources as input and learn class-aware representations for each source.
The main challenge is that sounds are naturally mixed in the audio space. This inspires us to disentangle the indi-vidual semantics for each source from the mixture to guide source localization. To address the problem, our key idea is to disentangle individual source representation using audio-visual grouping for source localization, which is different from existing single-source and multi-source methods. Dur-ing training, we aim to learn audio-visual category tokens to aggregate category-aware source features from the sound mixture and the image, where separated high-level seman-tics for individual sources are learned.
To this end, we propose a novel audio-visual grouping network, namely AVGN, that can directly learn category-wise semantic features for each source from the input audio mixture and frame to localize multiple sources simultane-ously. Specifically, our AVGN leverages learnable audio-visual class tokens to aggregate class-aware source features.
Then, the aggregated semantic features for each source will serve as guidance to localize the corresponding visual re-gions. Compared to previous multi-source baselines, our new framework can support a flexible number of sources and shows the effectiveness of learning compact audio-visual representations with category-aware semantics.
Empirical experiments on MUSIC and VGGSound-Instruments benchmarks comprehensively demonstrate the state-of-the-art performance against previous single-source and multi-source baselines. In addition, qualitative visual-izations of localization results vividly showcase the effec-tiveness of our AVGN in localizing individual sources from mixtures. Extensive ablation studies also validate the im-portance of category-aware grouping and learnable audio-Our main contributions can be summarized as follows:
• We present a novel Audio-Visual Grouping Network, namely AVGN, to disentangle the individual seman-tics from sound mixtures and images to guide source localization.
• We introduce learnable audio-visual class tokens and category-aware grouping in sound localization to ag-gregate category-wise source features with explicit high-level semantics.
• Extensive experiments comprehensively demonstrate the state-of-the-art superiority of our AVGN over pre-vious baselines on both single-source and multi-source sounding object localization. 2.