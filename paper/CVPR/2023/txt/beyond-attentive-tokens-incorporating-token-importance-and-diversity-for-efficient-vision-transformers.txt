Abstract
Vision transformers have achieved significant improve-ments on various vision tasks but their quadratic interac-tions between tokens significantly reduce computational ef-ficiency. Many pruning methods have been proposed to re-move redundant tokens for efficient vision transformers re-cently. However, existing studies mainly focus on the token importance to preserve local attentive tokens but completely ignore the global token diversity. In this paper, we empha-size the cruciality of diverse global semantics and propose an efficient token decoupling and merging method that can jointly consider the token importance and diversity for to-ken pruning. According to the class token attention, we de-couple the attentive and inattentive tokens. In addition to preserving the most discriminative local tokens, we merge similar inattentive tokens and match homogeneous atten-tive tokens to maximize the token diversity. Despite its sim-plicity, our method obtains a promising trade-off between model complexity and classification accuracy. On DeiT-S, our method reduces the FLOPs by 35% with only a 0.2% accuracy drop. Notably, benefiting from maintaining the to-ken diversity, our method can even improve the accuracy of
DeiT-T by 0.1% after reducing its FLOPs by 40%. 1.

Introduction
Transformer [32] has become the most popular archi-tecture in both natural language processing and computer vision communities. Vision transformers (ViTs) [8] have achieved superior performance and outperformed standard
CNNs in different vision tasks such as image classifica-tion [12, 31, 34, 41], semantic segmentation [20, 22, 33, 36],
*Equal contribution.
†Corresponding authors.
Figure 1. The ImageNet accuracy and keep rate of the pruned
DeiT-S. (a) Importance-based method preserves attentive tokens based on the class token attention and masks all inattentive tokens; (b) Diversity-based method clusters similar tokens into a group and then combines tokens from the same group into a new token. (c) Incorporate method decouples and merges tokens to consider token importance and diversity simultaneously. and object detection [1, 5]. The most remarkable advantage of transformer is its ability to effectively capture long-range dependencies between patches in the input image through the self-attention mechanism [26]. However, quadratic in-teractions between tokens significantly degrade the compu-tational efficiency [39], which motivates many researches on exploring efficient transformers.
As one of the most direct and effective ways to reduce computational complexity, token pruning has been widely studied recently. Existing studies mainly focus on designing different importance-evaluating strategies to retain attentive tokens and prune inattentive tokens [10, 21, 24, 26, 38, 40].
In these importance-based works, DyViT [26] introduces
an extra module to estimate the importance of each token while EViT [21] reorganizes image tokens based on the class attention importance score. However, inspired by re-cent diversity-preserving studies in ViT variants [11,13–15, 28, 30], we argue that promoting token diversity is also cru-cial for token pruning. Though inattentive tokens like the image background and low-level textures are not directly related to the classification objects, they can increase the token diversity and improve the expressivity of the model.
As discussed in [35], image backgrounds (e.g., the grass and leaves in Fig. 2) can improve the classification accuracy due to their potential relations to foreground objects. To this end, we first investigate a diversity-based pruning strategy on DeiT-S [31] with different keep rates. Specifically, in-stead of highlighting the token importance, it directly clus-ters and combines similar tokens into a single one, hereby maximizing the token diversity. Surprisingly, as shown in
Fig. 1, such an intuitive strategy can achieve comparable and even better performance than SOTA importance-based pruning methods, especially at the low keep rate.
Despite its promising performance, the diversity-based strategy cannot retain original attentive tokens and may con-sequently weaken the discriminative ability of the model.
As shown in Fig. 2 (c), the most representative tokens, e.g., eyes and ears of the dog or beaks of two birds, contain crit-ical semantic information for classification tasks but cannot be preserved by the diversity-based strategy. To address this issue, we naturally tend to keep all these dominant tokens while maintaining the token diversity, as shown in Fig. 2 (d). In short, a satisfied pruning method should jointly take the token importance and diversity into account, such that the most important local information and the diverse global information can be preserved simultaneously.
Motivated by these above observations, in this paper, we propose a novel pruning method that incorporates the token importance and diversity through efficient token decoupling and merging. As shown in Figure 1 (c), we first decouple the origin token sequence into attentive and inattentive por-tions based on class token attention. Instead of discarding inattentive tokens completely, we apply a simplified density peak clustering algorithm [27] to efficiently cluster similar inattentive tokens and combine these tokens from the same group into a new one. In addition, unlike existing methods that preserve all attentive tokens, we design a straightfor-ward matching algorithm to fuse homogeneous attentive to-kens and improve the calculation efficiency further. In this way, we can effectively prune tokens while maximizing the preservation of token diversity. We conduct extensive to-ken pruning experiments to validate the effectiveness of our method. Despite its simplicity, our method achieves supe-rior pruning performance on ImageNet [6] for two different vision transformers, DeiT [31] and LV-ViT [17]. Our main contributions are summarized as follows:
Figure 2. Visualizations of pruning results of different methods on
ImageNet with DeiT-S. (a) Original image. (b) Importance-based method masks inattentive tokens. (c) Diversity-based method clus-ters similar tokens and visualizes the same group of tokens as one colour. (d) Our method preserves the most discriminative tokens, e.g., the heads of birds and dogs. In addition, we merge similar inattentive tokens and match homogeneous attentive tokens, e.g., the grass and leaves.
• To the best of our knowledge, we are the first to em-phasize the token diversity for pruning ViT. We also demonstrate its cruciality through numerical and em-pirical analysis.
• We propose a simple yet effective decoupling and merging method that can simultaneously preserve the most attentive local tokens and diverse global seman-tics without imposing extra parameters.
• Benefiting from incorporating token importance and diversity, our method achieves new SOTA performance on the trade-off between accuracy and FLOPs.
It can also be deployed to other token pruning methods, achieving excellent performance improvement. 2.