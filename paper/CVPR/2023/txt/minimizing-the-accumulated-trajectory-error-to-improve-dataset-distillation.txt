Abstract
Model-based deep learning has achieved astounding suc-cesses due in part to the availability of large-scale real-world data. However, processing such massive amounts of data comes at a considerable cost in terms of computations, stor-age, training and the search for good neural architectures.
Dataset distillation has thus recently come to the fore. This paradigm involves distilling information from large real-world datasets into tiny and compact synthetic datasets such that processing the latter ideally yields similar performances as the former. State-of-the-art methods primarily rely on learning the synthetic dataset by matching the gradients ob-tained during training between the real and synthetic data.
However, these gradient-matching methods suffer from the so-called accumulated trajectory error caused by the discrep-ancy between the distillation and subsequent evaluation. To mitigate the adverse impact of this accumulated trajectory error, we propose a novel approach that encourages the op-timization algorithm to seek a flat trajectory. We show that the weights trained on synthetic data are robust against the accumulated errors perturbations with the regularization towards the flat trajectory. Our method, called Flat Trajec-tory Distillation (FTD), is shown to boost the performance of gradient-matching methods by up to 4.7% on a subset of images of the ImageNet dataset with higher resolution images. We also validate the effectiveness and generalizabil-ity of our method with datasets of different resolutions and demonstrate its applicability to neural architecture search.
Code is available at .https://github.com/AngusDujw/FTD-distillation. 1.

Introduction
Modern deep learning has achieved astounding successes in achieving ever better performances in a wide range of
*Corresponding Author. † Equal Contribution.
Figure 1. The change of the loss difference LTTest (fθ) − LTTest (fθ∗ ), in which θ and θ∗ denote the weights optimized by synthetic dataset S and real dataset T , respectively. The gray line represents LTTest (fθ∗ ) and is associated with the gray y-axis of the plot with two y-axes. The lines indicated by “Evaluation” represent the networks that are initialized at epoch 0 and trained with the synthetic dataset for 50 epochs. The line indicated by
“Distillation” represents the network that is initialized at epochs 2, 4, . . . , 48 and trained with the synthetic dataset for 2 epochs. The former lines have much higher loss difference compared to the latter; this is caused by the accumulated trajectory error. And we try to minimize it in the evaluation phase, so that the loss difference line of our method is lower and tends to converge than that of MTT [1]. fields by exploiting large-scale real-world data and well-constructed Deep Neural Networks (DNN) [4, 5, 9]. Unfortu-nately, these achievements have come at a prohibitively high cost in terms of computation, particularly when it relates to the tasks of data storage, network training, hyperparameter tuning, and architectural search.
A series of model distillation studies [2, 14, 16, 22] has thus been proposed to condense the scale of models by dis-tilling the knowledge from a large-scale teacher model into a compact student one. Recently, a similar but distinct task, dataset distillation [1, 3, 26, 35, 36, 42–45, 47, 49, 50] has been considered to condense the size of real-world datasets.
This task aims to synthesize a large-scale real-world dataset into a tiny synthetic one, such that a model trained with the synthetic dataset is comparable to the one trained with the
real dataset. Dataset distillation can expedite model training and reduce cost. It plays an important role in some machine learning tasks such as continual learning [39, 48–50], neural architecture search [19,37,38,48,50], and privacy-preserving tasks [8, 13, 27], etc.
Wang et al. [45] was the first to formally study dataset distillation. The authors proposed a method DD that models a regular optimizer as the function that treats the synthetic dataset as the inputs, and uses an additional optimizer to update the synthetic dataset pixel-by-pixel. Although the performance of DD degrades significantly compared to train-ing on the real dataset, [45] revealed a promising solution for condensing datasets. In contrast to conventional methods, they introduced an evaluation standard for synthetic datasets that uses the learned distilled set to train randomly initialized neural networks and the authors evaluate their performance on the real test set. Following that, Such et al. [41] employed a generative model to generate the synthetic dataset. Nguyen et al. [34] reformulated the inner regular optimization of
DD into a kernel-ridge regression problem, which admits closed-form solution.
In particular, Zhao and Bilen [50] pioneered a gradient-matching approach DC, which learns the synthetic dataset by minimizing the distance between two segments of gra-dients calculated from the real dataset T , and the synthetic dataset S. Instead of learning a synthetic dataset through a bi-level optimization as DD does, DC [50] optimizes the syn-thetic dataset explicitly and yields much better performance compared to DD. Along the lines of DC [50], more gradient-matching methods have been proposed to further enhance
DC from the perspectives of data augmentation [48], feature alignment [44], and long-range trajectory matching [1].
However, these follow-up studies on gradient-matching methods fail to address a serious weakness that results from the discrepancy between training and testing phases.
In the training phase, the trajectory of the weights generated by S is optimized to reproduce the trajectory of T which commenced from a set of weights that were progressively updated by T . However, in the testing phase, the weights are no longer initialized by the weights with respect to T , but the weights that are continually updated by S in previous iterations. The discrepancy of the starting points of the train-ing and testing phases results in an error on the converged weights. Such inaccuracies will accumulate and have an ad-verse impact on the starting weight for subsequent iterations.
As demonstrated in Figure 1, we observe the loss difference between the weights updated by S and T . We refer to the error as the accumulated trajectory error, because it grows as the optimization algorithm progresses along its iterations.
The synthetic dataset S optimized by the gradient-matching methods is able to generalize to various starting weights, but is not sufficiently robust to mitigate the per-turbation caused by the accumulated trajectory error of the starting weights. To minimize this source of error, the most straightforward approach is to employ robust learning, which adds perturbations to the starting weights intentionally dur-ing training to make S robust to errors. However, such a robust learning procedure will increase the amount of infor-mation of the real dataset to be distilled. Given a fixed size of S, the distillation from the increased information results in convergence issues and will degrade the final performance.
We demonstrate this via empirical studies in subsection 3.2.
In this paper, we propose a novel approach to minimize the accumulated trajectory error that results in improved performance. Specifically, we regularize the training on the real dataset to a flat trajectory that is robust to the pertur-bation of the weights. Without increasing the information to be distilled in the real dataset, the synthetic dataset will enhance its robustness to the accumulated trajectory error at no cost. Thanks to the improved tolerance to the perturbation of the starting weights, the synthetic dataset is also able to ameliorate the accumulation of inaccuracies and improves the generalization during the testing phase. It can also be ap-plied to cross-architecture scenarios. Our proposed method is compatible with the gradient-matching methods and boost their performances. Extensive experiments demonstrate that our solution minimizes the accumulated error and outper-forms the vanilla trajectory matching method on various datasets, including CIFAR-10, CIFAR-100, subsets of the
TinyImageNet, and ImageNet. For example, we achieve per-formance accuracies of 43.2% with only 10 images per class and 50.7% with 50 images per class on CIFAR-100, com-pared to the previous state-of-the-art work from [1] (which yields accuracies of only 40.1% and 47.7% respectively). In particular, we significantly improve the performance on a subset of the ImageNet dataset which contains higher resolu-tion images by more than 4%. 2. Preliminaries and