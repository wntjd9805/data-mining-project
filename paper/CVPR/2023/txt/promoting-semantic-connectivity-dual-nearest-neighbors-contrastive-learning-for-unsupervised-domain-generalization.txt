Abstract
Domain Generalization (DG) has achieved great success in generalizing knowledge from source domains to unseen target domains. However, current DG methods rely heav-ily on labeled source data, which are usually costly and unavailable. Since unlabeled data are far more accessible, we study a more practical unsupervised domain generaliza-tion (UDG) problem. Learning invariant visual representa-tion from different views, i.e., contrastive learning, promises well semantic features for in-domain unsupervised learning.
However, it fails in cross-domain scenarios. In this paper, we first delve into the failure of vanilla contrastive learn-ing and point out that semantic connectivity is the key to
UDG. Specifically, suppressing the intra-domain connectiv-ity and encouraging the intra-class connectivity help to learn the domain-invariant semantic information. Then, we pro-pose a novel unsupervised domain generalization approach, namely Dual Nearest Neighbors contrastive learning with strong Augmentation (DN2A). Our DN2A leverages strong augmentations to suppress the intra-domain connectivity and proposes a novel dual nearest neighbors search strategy to find trustworthy cross domain neighbors along with in-domain neighbors to encourage the intra-class connectivity.
Experimental results demonstrate that our DN2A outper-forms the state-of-the-art by a large margin, e.g., 12.01% and 13.11% accuracy gain with only 1% labels for linear evaluation on PACS and DomainNet, respectively. 1.

Introduction
Deep learning methods have yielded prolific results in various tasks in recent years. However, they are tailored for experimental cases, where training and testing data share the same distribution. When transferred to practical applications, these methods perform poorly on out-of-distribution data due to domain shifts [27, 30]. To tackle this issue, domain generalization (DG) methods [25, 35] are proposed to learn
*Corresponding author: Wenrui Dai. †Equal contribution.
Figure 1. (a) t-SNE visualization of unsupervised features learned by SimCLR and our DN2A on PACS. (b) Grad-cam visualization of linear probing for SimCLR and ours with 10% labeled data. transferable knowledge from multiple source domains to generalize on unseen target domains. Despite the promising results of DG, they are restricted to supervised training with large amounts of labeled source data. However, large-scale labeled source data are often unavailable due to the laborious and expensive annotation capture, while unlabeled data are far more accessible. Thus, we study the more practical unsupervised domain generalization (UDG) [34] problem to learn domain-invariant features in an unsupervised fashion.
Recent advances in unsupervised learning prefer con-trastive learning (CL) [2, 14, 28, 32], which learns semantic representation by enforcing similarity over different augmen-tations of the same image. However, most CL methods are designed for i.i.d. datasets and can hardly accommodate the cross-domain scenario in UDG. As depicted in Fig. 1, vanilla
SimCLR fails to learn domain-invariant semantic features but learns domain-biased features. For further understand-ing, we dive into this phenomenon and propose the semantic connectivity for UDG to measure the intra-domain and intra-class similarity. From augmentation graph view [12, 31], semantic connectivity is the support overlap of augmented samples within the same semantic class. We further find that the degraded semantic connectivity is responsible for
the failure of vanilla CL in UDG, which is reflected in two folds, i.e., large intra-domain connectivity and small intra-class connectivity. Positive samples generated by standard augmentations under the i.i.d. hypothesis share too much domain-relevant information, which induces the model to learn domain-related features for alignment, resulting in large intra-domain connectivity. Moreover, it is hard to cap-ture domain invariance via handcrafted transformations due to significant distribution shifts across domains. For example, one can hardly transform a cat from sketch to photo. Small intra-class connectivity occurs in cross-domain scenarios.
To address these issues, we propose to suppress the intra-domain connectivity and enhance intra-class connectivity.
First, we leverage strong augmentations to generate positive samples with a small amount of shared information, where the domain nuisance information is suppressed. The sup-pressed domain-related information decreases intra-domain connectivity, and the learned unsupervised representation can achieve a higher degree of invariance against domain shifts. Besides, we employ cross domain nearest neighbors (NN) as positive samples to impose the domain invariance by enforcing the similarity between cross domain samples po-tentially belonging to the same category, which can increase the cross-domain intra-class connectivity. In addition, we improve cross domain NN by a dual NN strategy that further introduces in-domain NN as positives to overcome the intra-domain variances and increase the intra-domain intra-class connectivity. For cross-domain NNs, a direct search may result in many false matches, due to distribution shifts across domains. Since searching NN within a domain without dis-tribution shift is more accurate than across domains, we propose a novel Cross Domain Double-lock NN (CD2NN) search strategy that employs more accurate in-domain NN as a mediator to find more trustworthy cross domain neighbors for boosting the performance. For in-domain NN, since di-rect searching may fail to find sufficiently diverse samples to overcome intra-domain variances, we resort to more distinct cross domain NN as a mediator to find more diverse neigh-bors, namely In-domain Cycle NN (ICNN). Totally, our dual nearest neighbors, i.e., CD2NN and ICNN, can increase the intra-class connectivity for UDG. In a nutshell, contributions of this paper are summarized as:
• We propose a novel semantic connectivity metric to indicate the inherent problem of contrastive learning in
UDG, and propose a novel method DN2A to increase the semantic connectivity with theoretical guarantees.
• We propose to leverage strong augmentations to sup-press the intra-domain connectivity and use cross do-main neighbors as positive samples to increase intra-class connectivity by enforcing the similarity over cross domain samples potentially from the same category.
• We propose a novel cross domain double-lock near-est neighbors search strategy to find more trustwor-thy cross domain neighbors and improve it by a novel in-domain cycle nearest neighbors search strategy to further boost the semantic connectivity.
Experiments show our DN2A outperforms state-of-the-art methods by a large margin, e.g., 12.01% and 13.11% accuracy gains with only 1% labels for linear evaluation on
PACS and DomainNet, respectively. Besides, with less than 4% samples compared to ImageNet for training, our method outperforms ImageNet pretraining, showing a promising way to initialize models for the DG problem. 2.