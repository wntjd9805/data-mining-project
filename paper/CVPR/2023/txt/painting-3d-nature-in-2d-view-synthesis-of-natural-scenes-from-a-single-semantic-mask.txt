Abstract project website is https://zju3dv.github.io/paintingnature/.
We introduce a novel approach that takes a single seman-tic mask as input to synthesize multi-view consistent color images of natural scenes, trained with a collection of single images from the Internet. Prior works on 3D-aware image synthesis either require multi-view supervision or learning category-level prior for specific classes of objects, which are inapplicable to natural scenes. Our key idea to solve this challenge is to use a semantic field as the intermedi-ate representation, which is easier to reconstruct from an input semantic mask and then translated to a radiance field with the assistance of off-the-shelf semantic image synthe-sis models. Experiments show that our method outperforms baseline methods and produces photorealistic and multi-view consistent videos of a variety of natural scenes. The
∗Affiliated with the State Key Lab of CAD&CG, Zhejiang University.
†Corresponding author: Xiaowei Zhou. 1.

Introduction
Natural scenes are indispensable content in many appli-cations such as film production and video games. This work focuses on a specific setting of synthesizing novel views of natural scenes given a single semantic mask, which en-ables us to generate 3D contents by editing 2D semantic masks. With the development of deep generative models, 2D semantic image synthesis methods [24, 46, 61, 66] have achieved impressive advances. However, they do not con-sider the underlying 3D structure and cannot generate multi-view consistent free-viewpoint videos.
To address this problem, a straightforward approach is first utilizing semantics-driven image generator like
SPADE [46] to synthesize an image from the input semantic mask and then predicting novel views based on the gener-ated image. Although the existing single-view view syn-thesis methods [31, 34, 45, 52, 67, 70] achieve impressive rendering results, they typically require training networks on posed multi-view images. Compared to urban or indoor scenes, learning to synthesize natural scenes is a challeng-ing task as it is difficult to collect 3D data or posed videos of natural scenes for training, as demonstrated in [32], making the aforementioned methods not applicable. AdaMPI [17] designs a training strategy to learn the view synthesis net-work on single-view image collections. It warps images to random novel views and warps them back to the original view. An inpainting network is trained to fill the holes in disocclusion regions to match the original images. After training, the inpainting network is used to generate pseudo multi-view images for training a view synthesis network.
Our experimental results in Section 4.5 show that the in-painting network struggles to output high-quality image contents in missing regions under large viewpoint changes, thus limiting the rendering quality.
In this paper, we propose a novel framework for semantics-guided view synthesis of natural scenes by learn-ing prior from single-view image collections. Based on the observation that semantic masks have much lower complex-ity than images, we divide this task into two simpler sub-problems: we first generate semantic masks at novel views and then translate them to RGB images through SPADE.
For view synthesis of semantic masks, the input semantic mask is first translated to a color image by SPADE, and a depth map is predicted from the color image by a depth es-timator [50]. Then, the input semantic mask is warped to novel views using the predicted depth map and refined by an inpainting network trained by a self-supervised learning strategy on single-view image collections. Our experiments show that, in contrast to images, the novel view synthesis of semantic masks is much easier to learn by the network.
It is observed that semantic masks generated by the in-painting network tend to be view-inconsistent. As a result,
SPADE could generate quite different contents in these re-gions even when the inconsistency is minor between the se-mantic masks. Fig. 4 presents two examples. To solve this issue, we learn a neural semantic field to fuse and denoise these semantic masks for better multi-view consistency. Fi-nally, we translate the multi-view semantic masks to color images by SPADE and reconstruct a neural scene represen-tation for view-consistent rendering.
Extensive experiments are conducted on the LHQ dataset [58], a widely-used benchmark dataset for semantic image synthesis. The results demonstrate that our approach significantly outperforms baseline methods both qualita-tively and quantitatively. We also show that by editing the input semantic mask, our approach is capable of generating various high-quality rendering results of natural scenes, as shown in Fig. 1. 2.