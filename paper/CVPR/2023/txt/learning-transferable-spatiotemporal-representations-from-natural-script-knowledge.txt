Abstract
Pre-training on large-scale video data has become a common recipe for learning transferable spatiotemporal representations in recent years. Despite some progress, ex-isting methods are mostly limited to highly curated datasets (e.g., K400) and exhibit unsatisfactory out-of-the-box rep-resentations. We argue that it is due to the fact that they only capture pixel-level knowledge rather than spatiotem-poral semantics, which hinders further progress in video understanding.
Inspired by the great success of image-text pre-training (e.g., CLIP), we take the first step to ex-ploit language semantics to boost transferable spatiotem-poral representation learning. We introduce a new pre-text task, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR scripts by attending to learned video representations. We do not rely on descriptive cap-tions and learn purely from video, i.e., leveraging the natu-ral transcribed speech knowledge to provide noisy but use-ful semantics over time. Our method enforces the vision model to contextualize what is happening over time so that it can re-organize the narrative transcripts, and can seam-lessly apply to large-scale uncurated video data in the real world. Our method demonstrates strong out-of-the-box spa-tiotemporal representations on diverse benchmarks, e.g.,
+13.6% gains over VideoMAE on SSV2 via linear prob-ing. The code is available at https://github.com/
TencentARC/TVTS. 1.

Introduction
The aspiration of representation learning is to encode general-purpose representations that transfer well to di-verse downstream tasks, where self-supervised methodolo-gies [9, 25] dominate due to their advantage in exploiting large-scale unlabeled data. Despite significant progress in learning representations of still images [23, 43], the real world is dynamic and requires reasoning over time. In this paper, we focus on out-of-the-box spatiotemporal represen-tation learning, a more challenging but practical task to-wards generic video understanding, which aims to capture hidden representations that can be further used to conduct reasoning on broader tasks, e.g., classification and retrieval.
There have been various attempts at self-supervised pre-training on video data from discriminative learning ob-jectives [5, 8, 27] to generative ones [17, 49], where the core is context capturing in spatial and temporal dimen-sions. Though promising results are achieved when trans-ferring the pre-trained models to downstream video recog-nition [22, 33, 48] via fine-tuning, the learned representa-tions are still far away from out-of-the-box given the poor linearly probing results (see Figure 1(a)). Moreover, exist-ing works mostly develop video models on the highly cu-rated dataset with particular biases, i.e., K400 [31]. Their applicability in the real world is questioned given the ob-served performance drops when training on a larger but uncurated dataset, YT-Temporal [55]. We argue that, to address the above issue, the rich spatiotemporal semantics contained in the video itself should be fully exploited. But current video models generally exploit visual-only percep-tion (e.g., pixels) without explicit semantics.
Recently, the success of CLIP [43] has inspired the com-munity to learn semantically aware image representations that are better transferable to downstream tasks and scal-able to larger uncurated datasets. It provides a feasible so-lution for improving spatiotemporal representation learning but remains two key problems. (1) The vision-language contrastive constraints in CLIP mainly encourage the un-derstanding of static objects (noun contrast) and simple mo-tions (verb contrast), while how to enable long-range tem-Figure 1. (a) We evaluate the transferability of spatiotemporal representations via linear probing on four video recognition datasets [22, 31, 33, 48], where the state-of-the-art method [49] underperforms. It performs even worse when pre-trained with a large-scale uncurated dataset, YT-Temporal [55]. (b) We encourage complex temporal understanding and advanced spatiotemporal representation learning with a new pretext task of sorting transcripts. poral understanding with language supervision needs to be studied. (2) The quality of language supervision [47] is crit-ical to the final performance of CLIP, however, it is hard to collect large-scale video data with literal captions that care-fully describe the dynamic content over time. The ideal way for self-supervised learning is to learn useful knowledge purely from the data itself, which is also the philosophy followed by previous video pre-training methods [17, 49].
Fortunately, video data is naturally multi-modal with tran-scribed speech knowledge in the form of text (ASR), pro-viding time-dependent semantics despite some noise.
To facilitate spatiotemporal understanding in large-scale uncurated data under the supervision of inherent script knowledge, we introduce a new pretext task for video pre-training, namely, Turning to Video for Transcript Sorting (TVTS). Intuitively, people sort out the order of events by temporal reasoning. As illustrated in Figure 1(b), given several unordered transcripts, it is difficult to reorganize the narrative by merely understanding the literal semantics.
When the corresponding video is provided, it will be much easier to sort the transcripts by contextualizing what is hap-pening over time. Whereas in neural networks, the tem-poral inference is embedded in spatiotemporal representa-tions. Thus we believe that if the chronological order of transcripts can be correctly figured out via resorting to the correlated video representations, the video has been well understood.
Subsequently, we concatenate the encoded script repre-sentations and the video representations and perform self-attention to predict the actual orders of the shuffled tran-scripts by fully understanding the spatiotemporal seman-tics in the video. The order prediction is cast as a K-way classification task, where K is the number of transcripts.
The pretext task indirectly regularizes our model to prop-erly capture contextualized spatiotemporal representations to provide enough knowledge for transcript ordering.
The usage of language supervision is related to video-text alignment [4, 20] and multimodal representation learn-ing [18, 55] methods, however, we are completely differ-(1) Video-text alignment methods focus on retrieval ent. tasks and are devoted to associating the vision patterns with language concepts. They are generally single-frame bi-ased [34] and fail to encode strong out-of-the-box tempo-ral representations. (2) Multimodal representation learning methods aim to learn fused representations across modali-ties rather than vision-only spatiotemporal representations in our work. Moreover, different from our pretext task that aims to optimize spatiotemporal video representations,
[55] sorts video frames by taking the features of individual frames as inputs without temporal modeling, i.e., learning video representations only at the image level. As [55] points out, its ordering pretext task is not critical for downstream tasks (performance even drops) and primarily serves as an interface to query the model about temporal events.
We realize the pretext task of TVTS by performing joint attention among the encoded video spatiotemporal repre-sentations and the extracted ASR transcript representations.
Specifically, given an input video and its successive tran-scripts, we randomly shuffle the order of the sentences.
To summarize, our contributions are three-fold. (i) We exploit the rich semantics from script knowledge which is naturally along with the video, rendering a flexible pre-training method that can easily apply to uncurated video (ii) We introduce a novel pre-data in the real world.
text task for video pre-training, namely, Turning to Video for Transcript Sorting (TVTS). It promotes the capability of the model in learning transferable spatiotemporal video representations. (iii) We conduct comprehensive compar-isons with advanced methods. Our pre-trained model ex-hibits strong out-of-the-box spatiotemporal representations on downstream action recognition tasks, especially the rel-atively large-scale and the most challenging SSV2 [22]. We also achieve state-of-the-art performances on eight common video datasets in terms of fine-tuning. 2.