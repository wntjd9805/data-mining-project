Abstract
Animating virtual avatars to make co-speech gestures facilitates various applications in human-machine interac-tion. The existing methods mainly rely on generative adver-sarial networks (GANs), which typically suffer from noto-rious mode collapse and unstable training, thus making it difficult to learn accurate audio-gesture joint distributions.
In this work, we propose a novel diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to effectively capture the cross-modal audio-to-gesture asso-ciations and preserve temporal coherence for high-fidelity audio-driven co-speech gesture generation. Specifically, we first establish the diffusion-conditional generation process on clips of skeleton sequences and audio to enable the whole framework. Then, a novel Diffusion Audio-Gesture
Transformer is devised to better attend to the information from multiple modalities and model the long-term temporal dependency. Moreover, to eliminate temporal inconsistency, we propose an effective Diffusion Gesture Stabilizer with an annealed noise sampling strategy. Benefiting from the architectural advantages of diffusion models, we further incorporate implicit classifier-free guidance to trade off between diversity and gesture quality. Extensive experi-ments demonstrate that DiffGesture achieves state-of-the-art performance, which renders coherent gestures with bet-ter mode coverage and stronger audio correlations. Code is available at https://github.com/Advocate99/DiffGesture. 1.

Introduction
Making co-speech gestures is an innate human behavior in daily conversations, which helps the speakers to express their thoughts and the listeners to comprehend the mean-ings [10, 32, 38]. Previous linguistic studies verify that such non-verbal behaviors could liven up the atmosphere
*Equal contribution.
†Corresponding author.
Figure 1. Illustration of Conditional Generation Process in Co-Speech Gesture Generation. The diffusion process q gradually adds Gaussian noise to the gesture sequence (i.e., x0 sampled from the real data distribution). The generation process pθ learns to denoise the white noise (i.e., xT sampled from the normal distribution) conditioned on context information c. Note that xt denotes the corrupted gesture sequence at the t-th diffusion step. and improve mutual intimacy [7, 8, 21]. Therefore, ani-mating virtual avatars to gesticulate co-speech movements is crucial in embodied AI. To this end, recent researches focus on the problem of audio-driven co-speech gesture generation [16, 25, 30, 41], which synthesizes human upper body gesture sequences that are aligned to the speech audio.
Early attempts downgrade this task as a searching-and-connecting problem, where they predefine the correspond-ing gestures of each speech unit and stitch them together by optimizing the transitions between consecutive motions for coherent results [11,21,31]. In recent years, the compelling performance of deep neural networks has prompted data-driven approaches. Previous studies establish large-scale speech-gesture corpus to learn the mapping from speech audio to human skeletons in an end-to-end manner [4, 5, 25, 27, 30, 34, 39]. To attain more expressive results, Ginosar et al. [16] and Yoon et al. [41] propose GAN-based methods to guarantee realism by adversarial mechanism, where the discriminator is trained to distinguish real gestures from the synthetic ones while the generator’s objective is to fool the discriminator. However, such pipelines suffer from the inherent mode collapse and unstable training, making them difficult to capture the high-fidelity audio-conditioned gesture distribution, resulting in dull or unreasonable poses.
The recent paradigm of diffusion probabilistic models
provides a new perspective for realistic generation [19, 37], facilitating high-fidelity synthesis with desirable properties such as good distribution coverage and stable training com-pared to GANs. However, it is non-trivial to adapt existing diffusion models for co-speech gesture generation. Most existing conditional diffusion models deal with static data and conditions [35, 36] (e.g., the image-text pairs without temporal dimension), while co-speech gesture generation requires generating temporally coherent gesture sequences conditioned on continual audio clips. Further, the com-monly used denoising strategy in existing diffusion models samples independently and identically distributed (i.i.d.) noises in latent space to increase diversity. However, this strategy tends to introduce variation for each gesture frame and lead to temporal inconsistency in skeleton sequences.
Therefore, how to generate high-fidelity co-speech gestures with strong audio correlations and temporal consistency is quite challenging within the diffusion paradigm.
To address the above challenges, we propose a tailored
Diffusion Co-Speech Gesture framework to capture the cross-modal audio-gesture associations while maintain-ing temporal coherence for high-fidelity audio-driven co-speech gesture generation, named DiffGesture. As shown in Figure 1, we formulate our task as a diffusion-conditional generation process on clips of skeleton and audio, where the diffusion phase is defined by gradually adding noise to gesture sequence, and the generation phase is referred as a parameterized Markov chain with conditional context features of audio clips to denoise the corrupted gestures.
As we treat the multi-frame gesture clip as the diffusion latent space, the skeletons can be efficiently synthesized in a non-autoregressive manner to bypass error accumulation.
To better attend to the sequential conditions from multiple modalities and enhance the temporal coherence, we then devise a novel Diffusion Audio-Gesture Transformer archi-tecture to model audio-gesture long-term temporal depen-dency. Particularly, the per-frame skeleton and contextual features are concatenated in the aligned temporal dimension and embedded as individual input tokens to a Transformer block. Further, to eliminate the temporal inconsistency caused by the naive denoising strategy in the inference stage, we thus propose a new Diffusion Gesture Stabilizer module to gradually anneal down the noise discrepancy in the temporal dimension. Finally, we incorporate implicit classifier-free guidance by jointly training the conditional and unconditional models, which allows us to trade off between the diversity and sample quality during inference.
Extensive experiments on two benchmark datasets show that our synthesized results are coherent with stronger audio correlations and outperform the state-of-the-arts with superior performance on co-speech gesture generation. To summarize, our main contributions are three-fold: 1) As an early attempt at taming diffusion models for co-speech gesture generation, we formally define the diffusion and de-noising process in gesture space, which synthesizes audio-aligned gestures of high-fidelity. 2) We devise the Diffusion
Audio-Gesture Transformer with implicit classifier-free dif-fusion guidance to better deal with the input conditional information from multiple sequential modalities. 3) We pro-pose the Diffusion Gesture Stabilizer to eliminate temporal inconsistency with an annealed noise sampling strategy. 2.