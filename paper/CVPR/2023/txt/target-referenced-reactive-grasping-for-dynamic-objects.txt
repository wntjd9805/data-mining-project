Abstract
Reactive grasping, which enables the robot to success-fully grasp dynamic moving objects, is of great interest in robotics. Current methods mainly focus on the temporal smoothness of the predicted grasp poses but few consider their semantic consistency. Consequently, the predicted grasps are not guaranteed to fall on the same part of the same object, especially in cluttered scenes. In this paper, we propose to solve reactive grasping in a target-referenced setting by tracking through generated grasp spaces. Given a targeted grasp pose on an object and detected grasp poses in a new observation, our method is composed of two stages: 1) discovering grasp pose correspondences through an attentional graph neural network and selecting the one with the highest similarity with respect to the target pose; 2) refining the selected grasp poses based on target and histor-ical information. We evaluate our method on a large-scale benchmark GraspNet-1Billion. We also collect 30 scenes of dynamic objects for testing. The results suggest that our method outperforms other representative methods. Further-more, our real robot experiments achieve an average suc-cess rate of over 80 percent. Code and demos are available at: https://graspnet.net/reactive. 1.

Introduction
Reactive grasping is in great demand in the industry.
For instance, in places where human-robot collaboration is heavily required like factories, stress on laborers will be sig-nificantly relieved if robots can receive tools from humans and complete the harder work for laborers. Such a vision is based on reactive grasping.
On the contrary to static environments, in reactive grasp-ing, dynamic task setting poses new challenges for algo-rithm design. Previous research in this area mainly focuses
† Cewu Lu is the corresponding author, a member of Qing Yuan Re-search Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, Shanghai, China (a) (b)
Figure 1. (a) Classic reactive grasping guarantees the smoothness of the grasp poses but cannot predicts grasps on the same part of the hammer. (b) Our target-referenced reactive grasping takes se-mantic consistency into consideration. The generated grasp poses across frames are illustrated with blue grippers. on planning temporally smooth grasps [22, 42] to avoid wavy and jerky robot motion. Few of them pay attention to its semantic consistency. In short, given a targeted grasp at the first frame, we want the robot to grasp the same part of the object in the following frames. Additionally, it is not guaranteed that grasp predictions made by classical meth-ods fall on the same object in cluttered scenes. Hence, most of their experiments are conducted on single-object scenes.
Unlike previous works, this work is aimed at achieving tem-porally smooth and semantically consistent reactive grasp-ing in clutter given a targeted grasp. We refer to such a task setting as target-referenced reactive grasping as shown in
Fig.1. Note that despite robot handover is a major applica-tion scenario of reactive grasping, this work focuses on a more general task setting - dynamic object grasping.
A naive idea to solve this task is to generate reference grasp poses for the initial scene and consecutively track the object’s 6D pose. As the object moves, the initial grasp pose can be projected to a new coming frame based on the object’s 6D pose. Although such an idea seems to be natu-ral and valid, some bottleneck greatly degrades its viabil-ity. First of all, the solution to reactive grasping should be able to handle objects’ motion in real-time, meaning that it requires fast inference speed and immediate response to continuous environmental changes. However, 6D pose tracking may be time-consuming due to commonly-used in-stance segmentation [11,40]. Second, 6D pose tracking usu-ally requires objects’ prior knowledge, such as CAD mod-els [4,41], which is not always available in the real world as well or achieves only category-level generalization [37].
Different from tracking objects, we propose to track grasps by a two-stage policy instead. We also comply with the restriction that no prior knowledge of the ob-jects is allowed. Given a target grasp on a partial-view point cloud, we first discover its corresponding grasp among future frame’s detected grasp poses as coarse estimation.
These gasp poses can be given by an off-the-shelf grasp de-tector. Inspired by recent progress in local feature match-ing, which often uses image descriptors like SIFT [17] to describe interesting regions of images, we view grasp poses and their corresponding features as geometric descriptors on a partial-view point cloud. Based on such an assump-tion, we can simply estimate correspondences between two grasp sets from two different observation frames by match-ing the associated grasp features. Note that in opposition to classical local feature matching, features of the entire scene are also incorporated to help achieve global aware-ness. Furthermore, consecutive matching along an observa-tion sequence may lead to the accumulation of error, on top of the coarse estimation through correspondence matching, we further use a memory-augmented coarse-to-fine module which uses both target grasp features and historical grasp features to refine the grasp tracking results for better tem-poral smoothness and semantic consistency.
We conduct extensive experiments on two benchmarks to evaluate our method and demonstrate its effectiveness.
The results show that our method outperforms two repre-sentative baseline methods. We also conduct real robot ex-periments on both single-object scenes and cluttered scenes.
We report success rates of 81.25% for single-object scenes and 81.67% for multi-object scenes. 2.