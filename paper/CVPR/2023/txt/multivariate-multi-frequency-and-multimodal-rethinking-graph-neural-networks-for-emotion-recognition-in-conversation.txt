Abstract
Complex relationships of high arity across modality and context dimensions is a critical challenge in the Emotion
Recognition in Conversation (ERC) task. Yet, previous works tend to encode multimodal and contextual relation-ships in a loosely-coupled manner, which may harm re-lationship modelling. Recently, Graph Neural Networks (GNN) which show advantages in capturing data relations, offer a new solution for ERC. However, existing GNN-based
ERC models fail to address some general limits of GNNs, including assuming pairwise formulation and erasing high-frequency signals, which may be trivial for many applica-tions but crucial for the ERC task. In this paper, we pro-pose a GNN-based model that explores multivariate rela-tionships and captures the varying importance of emotion discrepancy and commonality by valuing multi-frequency signals. We empower GNNs to better capture the inherent relationships among utterances and deliver more sufﬁcient multimodal and contextual modelling. Experimental results show that our proposed method outperforms previous state-of-the-art works on two popular multimodal ERC datasets. 1.

Introduction
Human beings constantly express their feelings in ev-eryday communication. Emotion Recognition in Conver-sation (ERC) aims at enabling machines to detect interac-tive human emotions in a dialogue, utilizing multi-sensory data, including textual, visual and acoustic information
[5, 13, 18, 24]. Unlike traditional affective computing tasks that are performed on single modalities (e.g., text, speech or facial images) [12, 28, 32] or/and in non-conversational
∗Corresponding author: Jie Shao. This work is supported by the 61832001 and
National Natural Science Foundation of China (No.
No. 62276047), Natural Science Foundation of Sichuan Province (No. 2023NSFSC1972) and Science and Technology Program of Yibin Sanjiang
New Area (No. 2023SJXQYBKJJH001).
Figure 1. An example of multimodal dialogue (left) and the com-plex multivariate relationships of u3 and u6 (right). scenario [15, 23, 33], there exists a distinct and essential challenge in the ERC task - the complex multivariate re-lationships among multiple modalities and conversational context. In other words, the emotional dependencies of an utterance are usually of high arity, and involve multi-source information across both modality and context dimensions.
Figure 1 presents a sample of conversation between two speakers. Take the utterance u3 as an example. The visual and acoustic messages of utterance u3 (an expressionless face and a ﬂat tone) are ambiguous, but imply a veiled anger if coupled with the text. Moreover, the emotion behind u3 is also related to the preceding context u1 and u2. In partic-ular, the change from calling by nickname in u1 to calling by full name in u3 suggests an emotion shift caused by u2, since another speaker tries to make a joke with a pretended lightness. Therefore, the relationships in {u1, u2, u3} are complex and multivariate, and involve interdependencies across both modality and context dimensions.
Researchers have been exploring how to capture the complex relationships more effectively. Among existing
ERC models, a dominant paradigm is to capture contextual relationships with context-sensitive modules such as recur-rent unit or transformer, whilst modelling multimodal re-lationships through various fusion methods [4, 24, 25, 34].
Despite the advances, this paradigm tends to underrate mul-tivariate relationships among modalities and context, as it limits the natural interaction between loosely-coupled mul-timodal and contextual modelling.
More recently, Graph Neural Networks (GNNs) have shown great promise and yielded remarkable improvements in ERC, by revealing rich expressive power of mining struc-tural information and data relations [17, 18]. A routine so-lution is to construct a heterogeneous graph where each modality of an utterance is regarded as a node, and con-nected with other modalities of the same utterance as well as connected with the utterances in same modality in the same dialogue. Carefully-tweaked edge-weighting strategies usu-ally follow. On this basis, multimodal and contextual de-pendencies among utterances can be modelled simultane-ously through message passing, and thus deliver tighter en-tanglement and richer interaction. Powerful as these GNN-based methods are, they still suffer from two limitations: i) Insufﬁcient multivariate relationships.
Conven-tional GNNs assume pairwise relationships of ob-jects of interest, and can only offer an approximation of higher-order and multivariate relationships through multiple pairs [1, 10]. However, degeneration of those multivariate relationships into pairwise formulation may harm the expressiveness [20,30]. Therefore, com-plex multivariate relationships in ERC may not be suf-ﬁciently modelled by previous GNN-based methods. ii) Underestimated high-frequency information. It has been shown that the propagation rule of GNNs (i.e., ag-gregating and smoothing messages from neighbours) is an analogy to a ﬁxed low-pass ﬁlter [26, 31], and it is mainly low-frequency messages that ﬂow in the graph while the effects of high-frequency ones are much weakened. Moreover, Bo et al. [2] show that low-frequency messages, which retain the commonality of node features, perform better on assortative graphs (in which the linked nodes tend to have similar features and share the same label). In contrast, high-frequency information that mirrors discrepancy and inconsistency is more crucial on disassortative graphs. For ERC, the constructed graphs are in general highly disassorta-tive, where inconsistent emotional messages may exist among modalities (say being sarcastic) or short-term context. Hence, high-frequency information may pro-vide crucial guidance, which is however badly ignored by previous GNN-based ERC models, incurring bottle-neck of performance improvement.
To address these issues, in this work we propose
Multivariate Multi-frequency Multimodal Graph Neural
Network (M3Net), which aims to capture more sufﬁcient multivariate relationships among modalities and context, while beneﬁting from multi-frequency information within the graph. At the core of M3Net are two parallel compo-nents, multivariate propagation and multi-frequency propa-gation. Concretely, we ﬁrst construct a hypergraph neural network with edge-dependent node weights [7] for multi-variate propagation, in which each modality of an utterance is represented as a node. We construct multimodal and con-textual hyperedges, which can connect arbitrary number of nodes, and thus can naturally encode relationships of higher arity. Meanwhile, we model multi-frequency information upon an undirected GNN, by adapting a set of frequency ﬁl-ters [2, 8] to distil different frequency constituents from the node features. We adaptively integrate different frequency signals to capture the varying importance of emotion dis-crepancy and emotion commonality in the local neighbour-hood, so as to achieve adaptive information sharing pattern.
The effectiveness of our work is further demonstrated by extensive experimental studies on two popular multimodal
ERC datasets IEMOCAP [3] and MELD [27]. We show that M3Net outperforms previous state-of-the-art methods. 2.