Abstract
Domain adaptive panoptic segmentation aims to miti-gate data annotation challenge by leveraging off-the-shelf annotated data in one or multiple related source domains.
However, existing studies employ two separate networks for instance segmentation and semantic segmentation which lead to excessive network parameters as well as compli-cated and computationally intensive training and inference processes. We design UniDAformer, a unified domain adap-tive panoptic segmentation transformer that is simple but can achieve domain adaptive instance segmentation and semantic segmentation simultaneously within a single net-work. UniDAformer introduces Hierarchical Mask Calibra-tion (HMC) that rectifies inaccurate predictions at the level of regions, superpixels and pixels via online self-training on the fly. It has three unique features: 1) it enables uni-fied domain adaptive panoptic adaptation; 2) it mitigates false predictions and improves domain adaptive panoptic segmentation effectively; 3) it is end-to-end trainable with a much simpler training and inference pipeline. Exten-sive experiments over multiple public benchmarks show that
UniDAformer achieves superior domain adaptive panoptic segmentation as compared with the state-of-the-art.
Figure 1. Existing domain adaptive panoptic segmentation [20] adapts things and stuff separately with two isolated networks (for instance segmentation and semantic segmentation) and fuses their outputs to produce the final panoptic segmentation as in (a), leading to excessive network parameters as well as complicated and computationally intensive training and inference. Differently,
UniDAformer employs a single unified network to jointly adapt things and stuff as in (b), which involves much less parameters and simplifies the training and inference pipeline greatly. 1.

Introduction
Panoptic segmentation [30] performs instance segmenta-tion for things and semantic segmentation for stuff, which assigns each image pixel with a semantic category and a unique identity simultaneously. With the advance of deep neural networks [5, 17–19, 31, 43], panoptic segmen-tation [4, 7–9, 29, 30, 33, 35, 54, 55] has achieved very impressive performance under the supervision of plenty of densely-annotated training data. However, collecting densely-annotated panoptic data is prohibitively laborious and time-consuming [10,11,37] which has become one ma-jor constraint along this line of research. One alternative is
*Equal contribution, {jingyi.zhang, jiaxing.huang}@ntu.edu.sg.
†Corresponding author, shijian.lu@ntu.edu.sg. to leverage off-the-shelf labeled data from one or multiple source domains. Nevertheless, the source-trained models often experience clear performance drop while applied to various target domains that usually have different data dis-tributions as compared with the source domains [20].
Domain adaptive panoptic segmentation can mitigate the inter-domain discrepancy by aligning one or multiple la-beled source domains and an unlabeled target domain [20].
To the best of our knowledge, CVRN [20] is the only work that tackles domain adaptive panoptic segmentation chal-lenges by exploiting the distinct natures of instance segmen-tation and semantic segmentation. Specifically, CVRN in-troduces cross-view regularization to guide the two segmen-tation tasks to complement and regularize each other and achieves very impressive performance. However, CVRN re-Multi-branch Architecture
Unified Architecture
PSN [30]
Panoptic FCN [35]
MaskFormer [9]
DETR [4]
Supervised Setup
Adaptation Setup
Performance Drop mSQ 75.5 59.0
-16.5 mRQ 60.2 27.8
-32.4 mPQ mSQ mRQ mPQ mSQ mRQ mPQ mSQ mRQ mPQ 47.7 20.1
-27.6 79.7 47.5
-32.2 73.1 19.7
-53.4 59.6 15.8
-43.8 79.1 56.6
-22.5 62.6 19.2
-43.4 51.1 16.2
-34.9 79.1 56.4
-22.7 64.1 21.8
-42.3 51.9 18.3
-33.6
Table 1. Panoptic segmentation with traditional multi-branch architecture [30] and recent unified architectures [4, 9, 35]: The Supervised
Setup trains with the Cityscapes [10] and tests on the same dataset. The UDA Setup trains with the SYNTHIA [45] and tests on Cityscapes.
It can be seen that the performance drops between the two learning setups come more from mRQ than from mSQ consistently across different architectures. In addition, such a phenomenon is more severe for unified architectures. This demonstrates a clear false prediction issue in unified domain adaptive panoptic segmentation as mRQ is computed with false positives and false negatives. lies on a multi-branch architecture that adopts a two-phase pipeline with two separate networks as illustrated in Fig. 1 (a). This sophisticated design directly doubles network pa-rameters, slows down the training, and hinders it from being end-to-end trainable. It is desirable to have a unified panop-tic adaptation network that can effectively handle the two segmentation tasks with a single network.
We design a unified domain adaptive panoptic segmenta-tion transformer (UniDAformer) as shown in Fig. 1 (b). Our design is based on the observation that one major issue in unified panoptic adaptation comes from a severe false pre-diction problem. As shown in Table 1, most recent unified panoptic segmentation architectures [4, 9, 35] outperform traditional multi-branch architectures [30] by large margins under the supervised setup. However, the situation inverts completely under unsupervised domain adaptation setup.
Such contradictory results are more severe for the recog-nition quality in mRQ. This shows that the panoptic quality drop of unified architecture mainly comes from False Posi-tives (FP) and False Negatives (FN) as mRQ is computed from all predictions (True Positives, False Negatives and
False Positives) while the segmentation quality in mSQ is computed with True Positives (TP) only.
In the proposed UniDAformer, we mitigate the false prediction issue by introducing Hierarchical Mask Cali-bration (HMC) that calibrates inaccurate predictions at the level of regions, superpixels, and pixels. With the cor-rected masks, UniDAformer re-trains the network via an online self-training process on the fly. Specifically, HMC treats both things and stuff predictions as masks uniformly and corrects each predicted pseudo mask hierarchically in a coarse-to-fine manner, i.e., from region level that cali-brates the overall category of each mask to superpixel and pixel levels that calibrate the superpixel and pixels around the boundary of each mask (which are more susceptible to prediction errors). UniDAformer has three unique fea-tures. First, it achieves unified panoptic adaptation by treat-ing things and stuff as masks and adapting them uniformly.
Second, it mitigates the false prediction issue effectively by calibrating the predicted pseudo masks iteratively and progressively. Third, it is end-to-end trainable with much less parameters and simpler training and inference pipeline.
Besides, HMC introduces little computation overhead and could be used as a plug-in.
The contributions of this work can be summarized in three aspects. First, we propose UniDAformer that en-ables concurrent domain adaptive instance segmentation and semantic segmentation within a single network. It is the first end-to-end unified domain adaptive panoptic seg-mentation transformer to the best our knowledge. Sec-ond, we design Hierarchical Mask Calibration with online self-training, which allows to calibrate the predicted pseudo masks on the fly during self-training. Third, extensive ex-periments over multiple public benchmarks show that the proposed UniDAformer achieves superior segmentation ac-curacy and efficiency as compared with the state-of-the-art. 2.