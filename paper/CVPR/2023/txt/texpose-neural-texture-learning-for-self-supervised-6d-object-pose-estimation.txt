Abstract
In this paper, we introduce neural texture learning for 6D object pose estimation from synthetic data and a few unlabelled real images. Our major contribution is a novel learning scheme which removes the drawbacks of previous works, namely the strong dependency on co-modalities or additional reﬁnement. These have been previously necessary to provide training signals for convergence. We formulate such a scheme as two sub-optimisation problems on texture learning and pose learning. We separately learn to pre-dict realistic texture of objects from real image collections and learn pose estimation from pixel-perfect synthetic data.
Combining these two capabilities allows then to synthesise photorealistic novel views to supervise the pose estimator with accurate geometry. To alleviate pose noise and segmen-tation imperfection present during the texture learning phase, we propose a surfel-based adversarial training loss together with texture regularisation from synthetic data. We demon-strate that the proposed approach signiﬁcantly outperforms the recent state-of-the-art methods without ground-truth pose annotations and demonstrates substantial generalisation im-provements towards unseen scenes. Remarkably, our scheme improves the adopted pose estimators substantially even when initialised with much inferior performance. 1.

Introduction
For spatial interaction with objects, one needs an under-standing of the translation and rotation of targets within 3D space. Inferring these 6D object pose parameters from a single RGB image is a core task for 3D computer vision.
This visually retrieved information has a wide range of appli-cations in AR/VR [13, 33], autonomous driving [12, 30, 57], and robotic manipulation [24, 51, 53]. Noteworthy, accuracy and runtime have both recently made a huge leap forward thanks to deep learning [19, 23, 31, 39, 40]. Unfortunately, most of these methods heavily rely on a massive amount of labelled data for supervision to learn precise models with strong generalisation capabilities [16, 39, 53, 59]. However, it is very labor-intensive and time consuming to generate accurate annotations for pose data [15, 53]. Meanwhile, this process also easily suffers from labelling errors as precise annotation in 3D is highly challenging [14]. Therefore, most of the benchmarks have only few hundreds images, which
In fact, does not allow proper learning of large models. methods training with such low amount of real data tend to strongly overﬁt to the data domain and fail to generalise to new scenes [18].
As a consequence, many different approaches have been proposed in the literature to tackle this problem. The sim-plest solution is to employ cut-and-paste strategy to increase domain invariance [11, 26]. Nonetheless, this requires highly accurate manual annotations and most models tend to overﬁt to the original domain. Another alternative is to rely on a large amount of synthetically generated data to prevent overﬁtting. Though this process is relatively cheap and fast, rendered images can exhibit drastic visual discrepancy in comparison with real images even when advanced physically-based renderers [9] are used. A handful of approaches try to close the domain gap via the use of generative adversarial networks to translate the synthetic images into the real do-main [2, 58]. Unfortunately, these methods do not achieve promising results as the generated images are still easily distinguishable from real imagery. Notably, very recently a new line of work that proposes to self-supervise the pose estimator on real data has emerged. After training in sim-ulation they ﬁne-tune on the new datasets [46, 47]. While these methods achieve impressive results that are even on par with fully supervised methods, they still suffer from sig-niﬁcant performance drop when not leveraging additional supervisory signals such as depth data [46] or ground truth camera poses [25, 42]. In this work, we propose a novel way to conduct self-supervised 6D pose estimation that is free from any additional supervision sources and further yields state-of-the-art performance.
The core idea of previous attempts [44,46,47,54] to adapt pretrained pose estimators to the real domain is to conduct render-and-compare using a differentiable renderer. With
CAD models and poses given, a renderer can output several attributes of the current object pose (e.g. mask, colour, depth) which allows to reﬁne the pose through iterative comparison between the renderings and observations. However, when relying on 2D visual contents (mask and colour), such strate-gies tend to fail due to measured silhouette imperfection, lack of textures, and domain discrepancies. This undesirable behaviour is also discussed in [47].
Thus, different from the previous attempts that heavily rely on render-and-compare for self-supervision [46,47], we instead propose to regard realistic textures of the objects as an intermediate representation before conducting training for the pose estimator. Our approach is formulated as two interconnected sub-optimisation problems on texture learn-ing and pose learning. In the core, we ﬁrst learn realistic textures of objects from raw image collections, then syn-thesise training data to supervise the pose estimator with pixel-perfect labels and realistic appearance. The key chal-lenge of our proposed scheme lies in capturing accurate texture under noise introduced by poses initialised by a pre-trained pose estimator during supervision. To this end, in addition to leveraging synthetic data to establish geome-try priors, we learn robust supervision through adversarial training by conditioning synthesised colours on local sur-face information. Furthermore, we establish regularisation from synthetic textures to compensate segmentation artefacts during a texture-learning phase. We demonstrate that the proposed approach signiﬁcantly outperforms the recent state-of-the-art methods without ground-truth pose annotations and demonstrates substantial generalisation improvements towards unseen domains. Our method signiﬁcantly improves even difﬁcult objects with little variance in appearance and geometry through self-supervision. Impressively, Our ap-proach demonstrates a robust self-improving ability for the employed pose estimators even when initialised with much inferior pose estimates than stronger baselines [46].
To summarise, our main contributions are:
• We formulate a new learning scheme, TexPose, that decomposes self-supervision for 6D object pose into
Texture learning and Pose learning.
• We propose a surfel-conditioned adversarial training loss and a synthetic texture regularisation term to han-dle pose errors and segmentation imperfection during texture learning, further delivering self-improving abil-ity to the pose estimators.
• We show signiﬁcant improvements over recent strong baselines with additional supervision signals. Our pose estimators demonstrates a substantial generalisation ability even on unseen scenes. 2.