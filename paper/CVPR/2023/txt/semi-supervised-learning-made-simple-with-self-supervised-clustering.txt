Abstract
Self-supervised learning models have been shown to learn rich visual representations without requiring human annotations. However, in many real-world scenarios, labels are partially available, motivating a recent line of work on semi-supervised methods inspired by self-supervised prin-ciples.
In this paper, we propose a conceptually sim-ple yet empirically powerful approach to turn clustering-based self-supervised methods such as SwAV or DINO into semi-supervised learners. More precisely, we introduce a multi-task framework merging a supervised objective using ground-truth labels and a self-supervised objective relying on clustering assignments with a single cross-entropy loss.
This approach may be interpreted as imposing the clus-ter centroids to be class prototypes. Despite its simplicity, we provide empirical evidence that our approach is highly effective and achieves state-of-the-art performance on CI-FAR100 and ImageNet. 1.

Introduction
In recent years, self-supervised learning became the dominant paradigm for unsupervised visual representation learning. In particular, much experimental evidence shows that augmentation-based self-supervision [3, 8–10, 14–16, 18, 21, 29, 32, 35, 71] can produce powerful representa-tions of unlabeled data. Such models, although trained without supervision, can be naturally used for supervised downstream tasks via simple fine-tuning. However, the most suitable way to leverage self-supervision is perhaps by multi-tasking the self-supervised objective with a cus-tom (possibly supervised) objective. Based on this idea, the community has worked on re-purposing self-supervised methods in other sub-fields of computer vision, as for instance in domain adaptation [25], novel class discov-ery [31, 77], continual learning [30] and semi-supervised learning [4, 13, 19, 72].
*Enrico Fini and Pietro Astolfi contributed equally.
†Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
Figure 1. Schematic illustration of the motivation behind the pro-posed semi-supervised framework. (a) Self-supervised clustering methods like SwAV [15] and DINO [16] compute cluster proto-types that are not necessarily well aligned with semantic cate-gories, but they do not require labeled data. (b) Adding a lin-ear classifier provides class prototypes, but the labeled (and unla-beled) samples are not always correctly separated. (c) Fine-tuning can help separating labeled data. (d) Our framework learns clus-ter prototypes that are aligned with class prototypes thus correctly separating both labeled and unlabeled data.
One of the areas that potentially benefits from the advancements in unsupervised representation learning is semi-supervised learning. This is mainly due to the fact that in semi-supervised learning it is crucial to efficiently extract the information in the unlabeled set to improve the
classification accuracy on the labeled classes. Indeed, sev-eral powerful semi-supervised methods [6, 13, 61, 72] were built upon this idea.
In the self-supervised learning landscape, arguably the most successful methods belong to the clustering-based family, such as DeepCluster v2 [14], SwAV [15] and DINO
[16]. These methods learn representations by contrast-ing predicted cluster assignments of correlated views of the same image. To avoid collapsed solutions and group samples together, they use simple clustering-based pseudo-labeling algorithms such as k-means and Sinkhorn-Knopp to generate the assignments. A peculiar fact about this fam-ily of methods is the discretization of the feature space, that allows them to use techniques that were originally devel-oped for supervised learning. Indeed, similar to supervised learning, the cross-entropy loss is adopted to compare the assignments, as they represent probability distributions over the set of clusters.
In this paper, we propose a new approach for semi-supervised learning based on the simple observation that clustering-based methods are amenable to be adapted to a semi-supervised learning setting: the cluster prototypes can be replaced with class prototypes learned with supervision and the same loss function can be used for both labeled and unlabeled data. In practice, semi-supervised learning can be achieved by multi-tasking the self-supervised and super-vised objectives. This encourages the network to cluster unlabeled samples around the centroids of the classes in the feature space. By leveraging on these observations we pro-pose a new framework for semi-supervised methods based on self-supervised clustering. We experiment with two in-stances of that framework: Suave and Daino, the semi-supervised counterparts of SwAV and DINO. These meth-ods have several favorable properties: i) they are efficient at learning representations from unlabeled data since they are based on the top-performing self-supervised methods; ii) they extract relevant information for the semantic cate-gories associated with the data thanks to the supervised con-ditioning; iii) they are easy to implement as they are based on the multi-tasking of two objectives. The motivation be-hind our proposal is also illustrated in Fig. 1. As shown in the figure, our multi-tasking approach enables to compute cluster centers that are aligned with class prototypes thus correctly separating both labeled and unlabeled data.
Our contributions can be summarized as follows:
• We propose a new framework for semi-supervised learning based on the multi-tasking of a supervised ob-jective on the labeled data and a clustering-based self-supervised objective on the unlabeled samples;
• We experiment with two representatives of such frame-work: Suave and Daino, semi-supervised extensions of SwAV [15] and DINO [16]. These methods, while simple to implement, are powerful and efficient semi-supervised learners;
• Our methods outperform state-of-the-art approaches, often relying on multiple ad hoc components, both on common small scale (CIFAR100) and large scale (Im-ageNet) benchmarks, setting a new state-of-the-art on semi-supervised learning. 2.