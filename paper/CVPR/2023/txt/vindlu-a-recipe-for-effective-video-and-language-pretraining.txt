Abstract
The last several years have witnessed remarkable progress in video-and-language (VidL) understanding.
However, most modern VidL approaches use complex and specialized model architectures and sophisticated pretrain-ing protocols, making the reproducibility, analysis and com-parisons of these frameworks difficult. Hence, instead of proposing yet another new VidL model, this paper conducts a thorough empirical study demystifying the most important factors in the VidL model design. Among the factors that we investigate are (i) the spatiotemporal architecture design, (ii) the multimodal fusion schemes, (iii) the pretraining ob-jectives, (iv) the choice of pretraining data, (v) pretraining and finetuning protocols, and (vi) dataset and model scal-ing. Our empirical study reveals that the most important de-sign factors include: temporal modeling, video-to-text mul-timodal fusion, masked modeling objectives, and joint train-ing on images and videos. Using these empirical insights, we then develop a step-by-step recipe, dubbed VINDLU, for effective VidL pretraining. Our final model trained us-ing our recipe achieves comparable or better than state-of-the-art results on several VidL tasks without relying on ex-ternal CLIP pretraining. In particular, on the text-to-video retrieval task, our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains state-of-the-art video question-answering re-sults on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC and
TVQA. Our code and pretrained models are publicly avail-able at: https://github.com/klauscc/VindLU . 1.

Introduction
Fueled by the growing availability of video-and-text data [2, 8, 9, 24, 41, 43, 48] and advances in the Transformer model design [12, 54], the last few years have witnessed incredible progress in video-and-language (VidL) under-standing [26,31,40,64,75,80]. Since the initial transformer-based models for VidL, such as ClipBERT [26], the text-to-video retrieval accuracy has improved from 22.0%, 22.4%, and 21.3% on MSR-VTT [65], DiDeMo [1], and Activi-Figure 1. We present a recipe for effective video-language pre-training. Our recipe starts with image and text transformer en-coders trained on video-text pairs using a contrastive objective (VTC). We then progressively add more components to our frame-work while also studying the importance of each component along the way. Our final recipe includes the steps for (1) adding temporal attention, (2) injecting a multimodal fusion encoder, (3) incorpo-rating masked modeling pretraining objectives, (4) jointly training on images and videos, (5) using more frames during fine-tuning and inference, and lastly, (6) scaling up the data and the model. tyNet [23] to > 45% R@1 accuracy on all three of these datasets, thus, marking an extraordinary relative improve-ment of more than 100% in less than 2 years.
At the same time, the model architectures and pretrain-ing/finetuning protocols used by modern VidL approaches
Method
Model Design
Pretraining Data
#Frames
Temporal
Modeling
Multimodal
Fusion
Pretraining
Objectives
Dataset
Size Modality
PT
FT Eval
Joint Att. [5]
UniVL [39] 1D-Conv+TR
VideoCLIP [64]
Mean Pooling
ClipBert [26]
Temp. Attn [5]
Frozen [2]
Joint Attn
MERLOT [75]
Window Attn [37]
VIOLET [16]
Joint Attn
MV-GPT [47]
Token Rolling [55]
ALL-in-one [55]
Singularity [25]
Late Temp. Attn
LAVENDER [32] Window Attn [37]
OmniVL [57]
ATP [6]
CLIP4Clip [40]
ECLIPSE [34]
CLIP2TV [18]
CLIP-Hitchhiker [3]
CLIP-ViP [66]
Temp. Attn
✗
Late TR
Late TR
CLIP
Late Attn
Prompt Attn [66] 2-layer TR VTC+VTM+MLM+MFM+LM
✗
BERT
✗
RoBERTa
BERT 2-layer TR
ViT 3-layer TR
BERT 2×BERT
✗
✗
✗ 4-layer TR
✗
✗
VTC
MLM+VTM
ITC
VTC+MLM+FOM
VTC+VTM+MLM+MVM
MLM+LM
VTC+VTM+MLM
VTC+VTM+MLM
MLM
VTC+VTM+LM
VTC
VTC
VTC
VTC+VTM
VTC
VTC
HT
HT 136M 136M
COCO+VG 0.2M
V
V
I 180M
C5M
YT 5M I+V
V
YT+C5M 185M I+V
V
V 17M I+V
C17M+IN 30M I+V 17M I+V
HT
HT+W2
C17M 136M 172M
C17M
CLIP
CLIP
CLIP
CLIP
CLIP
CLIP
I
I 400M 400M 400M I+A 400M 400M 500M I+V
I
I 48 32 1 1 → 4 16 4
-3 1 → 4 4 1 → 8 1 1 1 1 1 48 48 32 32 16 16 4 4 16 16 5 5
--9 3 12 4 5 5 8 8 16 16 12 12 32 32 12 12 16 120 1 → 12 12 12
TR: Transformer; Late: Late fusion; Attn: Attention. V: Video; I: Image; A: Audio; 1 → 4: 1 frame for stage-1 training and 4 frames for stage-2.
VTC: Video-text contrastive; VTM: Video-text matching; MLM: Masked language modeling; MFM: Masked frame modeling; LM: Language modeling. HT: HowTo100M [41]; C5M, C17M: see supplementary; YT: YT-Temporal [75]; W2: WebVid-2M [2]; COCO: [33], VG: Visual
Genome [24]; IN: An internal dataset.
Table 1. An overview of the existing VidL methods. Significant differences exist among these methods, making it challenging to reproduce, analyze and compare these methods. This motivates us to answer the question “What are the key steps to build a highly performant VidL framework” by investigating various components in the VidL framework design. have become significantly more complex and specialized over the last several years. As a result, it is increasingly dif-ficult to reproduce, analyze and compare most recent VidL frameworks. For example, several recent approaches [25, 32, 66] propose new architectures, new initialization strate-gies, pretraining objectives, pretraining datasets, and opti-mization protocols. Due to the large computational cost of ablating all these factors, it is difficult to understand which components are critical to the success of the pro-posed frameworks. Similarly, the key success factors of many other recent VidL approaches [6, 16, 32, 57] are also often obfuscated, which hinders future research.
In Table 1, we illustrate the complexity of modern VidL frameworks by dissecting them along multiple dimensions, including temporal modeling schemes, multimodal fusion modules, pretraining objectives, the source of the pretrain-ing data, and the number of frames for pretraining, finetun-ing and inference. Based on this analysis, we observe that there exist significant differences among these VidL meth-ods. Unfortunately, it’s not clear which differences are im-portant for the overall VidL performance and which are not.
The recent METER [13] work studies a subset of these components in the context of image-language modeling.
However, their analysis is limited to images and, thus, ig-nores various aspects related to video modeling, such as spatiotemporal architecture design, video pretraining ob-jectives, video pretraining data, and video-specific finetun-ing/evaluation protocols such as the number of frames. As we will show in our experimental section, many of the find-ings presented in the image-based studies [13] do not hold for video. Beyond image-based analysis, we note that the concurrent work in [17] conducts an empirical study of
VidL transformers. However, unlike our work, which cov-ers a broad range of VidL design factors, their analysis is focused predominantly on masked visual modeling objec-tives, which we also study in this work.
Our main objective in this work is to answer the ques-tion “What are the key steps needed to build a highly per-formant VidL framework?” To do this, we conduct a thor-ough empirical study that demystifies the importance of var-ious VidL design choices and ultimately leads to a VidL framework that achieves state-of-the-art results on various
VidL benchmarks. Using our empirical insights, we then develop a step-by-step recipe for effective VidL pretrain-ing. Our recipe, dubbed VINDLU (VIdeo aND Language
Understanding), starts from a standard Vision Transformer (ViT) [12] and uses a simple progressive expansion scheme where at each step, we investigate a particular aspect of
VidL framework design (e.g., architecture, pretraining ob-jective, pretraining data, etc.), and choose the best perform-ing option. In particular, we study the following VidL de-sign components: (i) the spatiotemporal architecture design,
• Masked language modeling objective significantly im-proves performance (+6.2%) while masked video mod-eling objective brings an additional +1% improvement.
• Pretraining jointly on images and videos is beneficial (+2.7%). Also, contrary to prior methods [2,57], we find multi-stage training unnecessary.
• Pretraining with a small number of frames (e.g., 4) is suf-ficient and it can significantly reduce the computational cost of large-scale pretraining. Pretraining with more frames does not lead to a substantial performance boost.
• Compared to many recent CLIP-based [45] VidL ap-proaches [3, 40, 66], our recipe achieves comparable or even better performance with 20× less pretraining data. trained using our VINDLU recipe,
Our final model, achieves state-of-the-art results on several VidL bench-marks. task, our
Specifically, on the video retrieval method achieves 46.5%, 61.2%, 55.0% R@1 accuracy on MSR-VTT, DiDeMo, and ActivityNet outperforming the state-of-the-art by 7.8% and 6.1% on the latter two datasets. Also, our approach obtains state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC and TVQA, where we achieve top-1 ac-curacy of 44.7%, 44.6%, 97.1%, and 79.0% respectively.
We want to make it clear that, in this paper, we do not claim technical novelty behind any of the individual de-sign choices (i.e., different subsets of these design choices were already used by prior VidL methods as shown in Ta-ble 1).
Instead, our main contribution, which we believe might be equally if not more important than proposing yet another specialized or obfuscated VidL model, is to inves-tigate these components collectively and validate their im-portance. We also do not claim superiority over previous methods (despite better results). Due to the implementa-tion complexities of each method, fair and complete com-parisons are difficult and not our intent. Instead, we hope that our recipe for building an effective VidL framework will provide useful insights for future research on VidL un-derstanding. To enable the VidL community to build on our work, we release our code and pretrained models. 2.