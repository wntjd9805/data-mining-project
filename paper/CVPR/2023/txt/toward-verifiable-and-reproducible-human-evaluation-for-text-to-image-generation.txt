Abstract
Human evaluation is critical for validating the perfor-mance of text-to-image generative models, as this highly cognitive process requires deep comprehension of text and images. However, our survey of 37 recent papers reveals that many works rely solely on automatic measures (e.g.,
FID) or perform poorly described human evaluations that are not reliable or repeatable. This paper proposes a stan-dardized and well-defined human evaluation protocol to fa-cilitate verifiable and reproducible human evaluation in fu-ture works. In our pilot data collection, we experimentally show that the current automatic measures are incompatible with human perception in evaluating the performance of the text-to-image generation results. Furthermore, we provide insights for designing human evaluation experiments reli-ably and conclusively. Finally, we make several resources publicly available to the community to facilitate easy and fast implementations. 1.

Introduction
Text-to-image synthesis has seen substantial develop-ment in recent years. Several new models have been in-troduced with remarkable results. The majority of the works validate their models using automatic measures, such as FID [13] and recently proposed CLIPScore [12], even though many papers point out problems with these mea-sures. The most popular measure, FID, is criticized for misalignment with human perception [9]. For example, im-age resizing and compression hardly degrade the percep-tual quality but induce high variations in the FID score [28], while CLIPScore can inflate for a model trained to optimize text-to-image alignment in the CLIP space [27].
This empirical evidence of the misalignment of the au-tomatic measures motivates human evaluation of perceived quality. However, according to our study of 37 recent pa-pers, the current practices in human evaluation face signifi-Figure 1. Conventionally, researchers have used different proto-cols for human evaluation, and setup details are often unclear. We aim to build a standardized human evaluation. cant challenges in reliability and reproducibility. We mainly identified the following two problematic practices. Firstly, evaluation protocols vary significantly from one paper to another. For example, some employ relative evaluation by simultaneously showing annotators two or more samples for comparison, and others collect scores of individual samples based on certain criteria. Secondly, important details of ex-perimental configurations and collected data are often omit-ted. For example, the number of annotators who rate each sample is mostly missing, and sometimes even the number of assessed samples is not reported. These inconsistencies in the evaluation protocol make future analysis almost im-possible. We also find that recent papers do not analyze the quality of the collected human annotations. Therefore, we cannot assess how reliable the evaluation result is. It is also difficult to know which is a good way to evaluate text-to-image synthesis among various evaluation protocols.
The natural language generation (NLG) community has extensively explored human evaluation. Human evaluation is typically done in a crowdsourcing platform, and there are many well-known practices. Yet, quality control is an open challenge [16]. Recently, a platform for benchmarking mul-tiple NLG tasks was launched [18]. The platform offers a
web form where researchers can submit their model predic-tions. The platform automatically enqueues a human evalu-ation task on AMT, allowing a fair comparison for its users.
We address the lack of a standardized evaluation proto-col in text-to-image generation. To this end, we carefully design an evaluation protocol using crowdsourcing and em-pirically validate the protocol. We also provide recommen-dations for reporting a configuration and evaluation result.
We evaluate state-of-the-art generative models with our protocol and provide an in-depth analysis of collected hu-man ratings. We also investigate automatic measures, i.e.,
FID and CLIPScore, by checking the agreement between the measures and human evaluation. The results reveal the critical limitations of these two automatic measures.
Findings and resources Our main findings can be sum-marized as follows:
• Reliability of prior human evaluation is question-able. We provide insights about the required numbers of prompts and human ratings to be conclusive.
• FID is inconsistent with human perception. This is already known at least empirically, and our experiment supports it.
• CLIPScore is already saturated.
State-of-the-art generative models are already on par with authentic images in terms of CLIPScores.
These findings motivate us to build a standardized pro-tocol for human evaluation for better verifiability and re-producibility, facilitating to draw reliable conclusions. For continuous development, we open-source the following re-sources for the community.
• Implementation of human evaluation on a crowdsourc-ing platform, i.e., Amazon Mechanical Turk (AMT)1, which allows researchers to evaluate their generative models with a standardized protocol.
• Template for reporting human evaluation results. This template will enhance their transparency.
• Human ratings by our protocol on multiple datasets:
MS-COCO [23], DrawBench [30], and PartiPrompts
[37]. These per-image ratings, and not only their statis-tics, will facilitate designing automatic measures. 2.