Abstract
Existing vector quantization (VQ) based autoregressive models follow a two-stage generation paradigm that first learns a codebook to encode images as discrete codes, and then completes generation based on the learned code-book. However, they encode fixed-size image regions into fixed-length codes and ignore their naturally different in-formation densities, which results in insufficiency in impor-tant regions and redundancy in unimportant ones, and fi-nally degrades the generation quality and speed. More-over, the fixed-length coding leads to an unnatural raster-scan autoregressive generation. To address the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization VAE (DQ-VAE) which encodes image re-gions into variable-length codes based on their informa-tion densities for an accurate & compact code represen-tation. (2) DQ-Transformer which thereby generates im-ages autoregressively from coarse-grained (smooth regions with fewer codes) to fine-grained (details regions with more codes) by modeling the position and content of codes in each granularity alternately, through a novel stacked-transformer architecture and shared-content, non-shared position input layers designs. Comprehensive experiments on various generation tasks validate our superiorities in both effectiveness and efficiency. Code will be released at https : / / github . com / CrossmodalGroup /
DynamicVectorQuantization.
Figure 1. Illustration of our motivation. (a) Existing fixed-length coding ignores information densities, which results in insuffi-ciency in dense information regions like region ② and redundancy in sparse information regions like region ①, generating poor de-tails and inconsistent structure. Our information-density-based variable-length coding encodes accurately and produces rich de-tails and consistent structure. (b) Comparison of existing unnatu-ral raster-scan autoregressive generation order and our natural and more effective coarse-to-fine autoregressive generation order.
Error map: l1 loss of each 322 region between original images and recon-structions, higher (redder) worse. Existing examples are taken from [13]. 1.

Introduction
The vision community has witnessed the rapid progress of deep generative models, pushing image generation qual-ity to an unprecedented level. As a fundamental task, gen-erating realistic images from arbitrary inputs (e.g., class la-bels) can empower humans to create rich and diverse visual content and bring numerous real-world applications. Unify-*Zhendong Mao is the corresponding author. ing the realism of local details and the consistency of global structure is the eternal pursuit for all image generations.
Recently, vector quantization (VQ) [37] has been a foun-dation for various types of generative models as evidenced by numerous large-scale diffusion models like LDM [32], autoregressive models like DALL-E [30], etc. These mod-els follow a two-stage generation paradigm, i.e., the first stage learns a codebook by VQ to encode images as dis-crete codes, where each code represents a local visual pat-tern, while the second stage learns to generate codes of lo-cal regions and then restores to images. The importance lies in that the local details could be well encoded in the first stage and thus the second stage could effectively focus on global structure modeling, leading to better generation quality and scalability. Existing models mainly focus on the second stage to better generate codes for improving genera-tion quality, such as raster-scan autoregression [11, 30, 43], bi-direction [7, 24, 44], or diffusion [5, 14, 32]. Only a few works aim to improve the fundamental code representation itself in the first stage, including perceptual and adversar-ial loss for context-rich codebook [13], residual quantiza-tion [23], and more expressive transformer backbone [42], etc. Their commonality is that they all focus on encoding more information of all image regions together.
However, existing fundamental encoding works inher-ently fail to effectively encode image information for an accurate and compact code representation, because they ig-nore the naturally different information densities of dif-ferent image regions and encode fixed-size regions into fixed-length codes. As a result, they suffer from two lim-itations: (1) insufficient coding for important regions with dense information, which fails to encode all necessary in-formation for faithful reconstruction and therefore degrades (2) redundant the realism of local details in both stages. coding for unimportant ones with sparse information, bring-ing huge redundant codes that mislead the second stage to focus on the redundancy and therefore significantly hinder the global structure modeling on important ones. As shown in Figure 1(a), the fixed-length codes result in large recon-struction errors in important cheetah regions and produce poor local details (e.g., face, hair) in both stages. Mean-while, the fixed-length codes are overwhelmed for unimpor-tant background regions, which misleads the second stage to generate redundant background and inconsistent cheetah structure. Moreover, as shown in Figure 1(b), since all re-gions are encoded into fixed-length codes, there is no way for the second stage to distinguish their varying importance and thus results in an unnatural raster-scan order [13] for ex-isting autoregressive models [11, 23, 30, 42, 43], which fails to consider the image content for an effective generation.
To address this problem, inspired by the classical in-formation coding theorems [18, 33, 34] and their dynamic coding principle, we propose information-density-based variable-length coding for an accurate and compact code representation to improve generation quality and speed.
Moreover, we further propose a natural coarse-to-fine au-toregressive model for a more effective generation. Specif-ically, we propose a novel two-stage generation frame-(1) Dynamic-Quantization VAE (DQ-VAE) which work: first constructs hierarchical image representations of mul-tiple candidate granularities for each region, and then uses a novel Dynamic Grained Coding module to assign the most suitable granularity for each region under the con-straint of a proposed budget loss, matching the percent-age of each granularity to the desired expectation holis-tically. (2) DQ-Transformer which thereby generates im-ages autoregressively from coarse-grained (smooth regions with fewer codes) to fine-grained (details regions with more codes) to more effectively achieve consistent struc-tures. Considering the distribution of different granulari-ties varying, DQ-Transformer models the position and con-tent of codes in each granularity alternately through a novel stacked-transformer architecture. To effectively teach the difference between different granularities, we further design shared-content and non-shared-position input layers.
Technical contribution.
Our main contributions are summarized as follows:
Conceptual contribution. We point to the inherent in-sufficiency and redundancy in existing fixed-length cod-ing since they ignore information density. For the first time, we propose information-density-based variable-length coding for accurate & compact code representations. (1) We propose DQ-VAE to dynamically assign variable-length codes to regions based on their different information densities through a novel Dy-namic Grained Coding module and budget loss. (2) We pro-pose DQ-Transformer to generate images autoregressively from coarse-grained to fine-grained for the first time, which models the position and content of codes alternately in each granularity by stacked-transformer architecture with shared-content and non-shared position input layers design.
Experimental contribution. Comprehensive experi-ments on various generations validate our superiority, e.g., we achieve 7.4% quality improvement and faster speed compared to existing state-of-the-art autoregressive model on unconditional generation, and 17.3% quality improve-ment compared to existing million-level parameters state-of-the-art models on class-conditional generation. 2.