Abstract
We introduce a method for generating realistic pedes-trian trajectories and full-body animations that can be con-trolled to meet user-defined goals. We draw on recent ad-vances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only asso-ciated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target way-points, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based hu-manoid controller to form a closed-loop, full-body pedes-trian animation system capable of placing large crowds in a simulated environment with varying terrains. We fur-ther propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain.
Video results are available on the project page. 1.

Introduction
Synthesizing high-level human behavior, in the form of 2D positional trajectories, is at the core of modeling pedestrians for applications like autonomous vehicles, ur-ban planning, and architectural design. An important fea-ture of such synthesis is controllability – generating tra-∗Equal contribution jectories that meet user-defined objectives, edits, or con-straints. For example, a user may place specific waypoints for characters to follow, specify social groups for pedestri-ans to travel in, or define a social distance to maintain.
Attaining controllability is straightforward for algorith-mic or rule-based models of human behavior, since they have built-in objectives. In the simplest case, human tra-jectories can be determined by the shortest paths between control points [11], but more sophisticated heuristics have also been developed for pedestrians [2,14], crowds [22,46], and traffic [29, 53]. Unfortunately, algorithmically gener-ated trajectories often appear unnatural. Learning-based ap-proaches, on the other hand, can improve naturalness by mimicking real-world data. These methods often focus on short-term trajectory prediction using a single forward pass of a neural network [1, 10, 49, 61]. However, the ability to control these models is limited to sampling from an out-put trajectory distribution [34, 58] or using an expensive la-tent space traversal [45]. As a result, learning-based meth-ods can predict implausible motions such as collisions with obstacles or between pedestrians. This motivates another notion of controllability – maintaining realistic trajectories during agent-agent and agent-environment interactions.
In this work, we are particularly interested in using con-trollable pedestrian trajectory models for character anima-tion. We envision a simple interface where a user provides high-level objectives, such as waypoints and social groups, and a system converts them to physics-based full-body hu-man motion. Compared to existing kinematic motion mod-els [19, 27, 42], physics-based methods have the potential to produce high-quality motion with realistic subtle behav-iors during transitions, obstacle avoidance, traversing un-even terrains, etc. Although there exist physics-based ani-mation models [12, 27, 39–41, 57], controlling their behav-ior requires using task-specific planners that need to be re-trained for new tasks, terrains, and character body shapes.
We develop a generative model of trajectories that is data driven, controllable, and tightly integrated with a physics-based animation system for full-body pedestrian simulation (Fig. 1). Our method enables generating pedestrian trajec-tories that are realistic and amenable to user-defined objec-tives at test time. We use this trajectory generator as a plan-ner for a physics-based pedestrian controller, resulting in a closed-loop controllable pedestrian animation system.
For trajectory generation, we introduce a TRAjectory
Diffusion Model for Controllable PEdestrians (TRACE).
Inspired by recent successes in generating trajectories through denoising [9, 20, 64], TRACE generates the future trajectory for each pedestrian in a scene and accounts for the surrounding context through a spatial grid of learned map features that is queried locally during denoising. We leverage classifier-free sampling [17] to allow training on mixed annotations (e.g., with and without a semantic map), which improves controllability at test time by trading off sample diversity with compliance to conditioning. User-controlled sampling from TRACE is achieved through test-time guidance [7, 17, 18], which perturbs the output at each step of denoising towards the desired objective. We extend prior work [20] by introducing several analytical loss func-tions for pedestrians and re-formulating trajectory guidance to operate on clean trajectory outputs from the model [18], improving sample quality and adherence to user objectives.
For character animation, we develop a general-purpose
Pedestrian Animation ControllER (PACER) capable of driving physics-simulated humanoids with diverse body types to follow trajectories from a high-level planner. We focus on (1) motion quality: PACER learns from a small motion database to create natural and realistic locomotion through adversarial motion learning [40,41]; (2) terrain and social awareness: trained in diverse terrains with other hu-manoids, PACER learns to move through stairs, slopes, un-even surfaces, and to avoid obstacles and other pedestrians; (3) diverse body shapes: by training on different body types,
PACER draws on years of simulation experience to control a wide range of characters; (4) compatibility with high-level planners: PACER accepts 2D waypoints and can be a plug-in model for any 2D trajectory planner.
We demonstrate a controllable pedestrian animation sys-tem using TRACE as a high-level planner for PACER, the low-level animator. The planner and controller operate in a closed loop through frequent re-planning according to simulation results. We deepen their connection by guiding
TRACE with the value function learned during RL training of PACER to improve animation quality in varying tasks.
We evaluate TRACE on synthetic [2] and real-world pedes-trian data [3, 26, 38], demonstrating its flexibility to user-specified and plausibility objectives while synthesizing re-alistic motion. Furthermore, we show that our animation system is capable and robust with a variety of tasks, terrains, and characters. In summary, we contribute (1) a diffusion model for pedestrian trajectories that is readily controlled at test time through guidance, (2) a general-purpose pedestrian animation controller for diverse body types and terrains, and (3) a pedestrian animation system that integrates the two to drive simulated characters in a controllable way. 2.