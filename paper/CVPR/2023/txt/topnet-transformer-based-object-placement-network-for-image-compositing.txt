Abstract
We investigate the problem of automatically placing an object into a background image for image compositing.
Given a background image and a segmented object, the goal is to train a model to predict plausible placements (loca-tion and scale) of the object for compositing. The quality of the composite image highly depends on the predicted loca-tion/scale. Existing works either generate candidate bound-ing boxes or apply sliding-window search using global rep-resentations from background and object images, which fail to model local information in background images. How-ever, local clues in background images are important to de-termine the compatibility of placing the objects with certain locations/scales. In this paper, we propose to learn the cor-relation between object features and all local background features with a transformer module so that detailed infor-mation can be provided on all possible location/scale con-figurations. A sparse contrastive loss is further proposed to train our model with sparse supervision. Our new for-mulation generates a 3D heatmap indicating the plausibil-ity of all location/scale combinations in one network for-ward pass, which is > 10× faster than the previous sliding-window method. It also supports interactive search when users provide a pre-defined location or scale. The pro-posed method can be trained with explicit annotation or in a self-supervised manner using an off-the-shelf inpainting model, and it outperforms state-of-the-art methods signifi-cantly. User study shows that the trained model generalizes well to real-world images with diverse challenging scenes and object categories. 1.

Introduction
Object compositing [15, 25] is a common and important workflow for image editing and creation. The goal is to insert an object from an image into a given background im-age such that the resulting image appears visually pleasing and realistic. Conventional workflows in object composit-ing rely on manual object placement, i.e. manually deter-mining where the object should be placed (location) and in what size the object is placed (scale). However, man-ual placement does not fulfill the growing need for image creation for social sharing, advertising, education, etc., and
AI-assisted compositing with automatic object placement is more desirable for future image creation applications.
While there have been several works on learning-based ob-ject placement for specific scenes, general object placement with diverse scenes and objects still remains challenging with limited exploration, as it involves a deeper understand-ing of common sense, objects, and local details of scenes.
Inaccurate object placement could lead to poor compositing results, e.g. a person floating in the sky, a dog larger than buildings, etc.
Existing works [7,10,12,26,29] formulate the problem in very different ways, as shown in Fig. 1. [10,26] directly pre-dict multiple transformations or bounding boxes indicating the location and scale of the given objects. Such sparse pre-dictions recommend the top candidate placements for users, but they do not provide any information about other possi-ble locations and scales. They also fail to leverage the lo-cal clues in background images, as the bounding boxes are generated based on only global features. Another thread of works [12, 24] considers object placement as binary clas-sification, which evaluates the plausibility of input images and placement of bounding boxes instead of generating can-didate placements directly from input images. One recent work [29] utilizes a retrieval model to assess the plausibility of a given placement and evaluates a grid of locations and scales in a sliding-window manner. However, it requires multiple network forward passes to generate dense evalua-tion for one image, resulting in a slow inference speed.
In this paper, we propose TopNet, a Transformer-based
Object Placement Network for real-world object composit-ing applications. Different from previous works, TopNet formulates object placement as a dense prediction prob-lem: generating evaluation for a dense grid of locations and scales in one network forward pass. Given a back-ground image and an object, TopNet directly generates a 3D heatmap indicating the plausibility score of object lo-cation and scale, which is > 10× faster than the previous sliding-window method [29]. Previous works [26, 29] com-Figure 1. Comparison between different formulations for object placement. Our method provides a dense evaluation of possible loca-tions/scales in one network forward pass. bine background and foreground object features only at the global level, which fails to capture local clues for determin-ing the object location. We propose to learn the correlation between global foreground object feature and local back-ground features with a multi-layer transformer, leading to a more efficient and accurate evaluation of all possible place-ments. To train TopNet with sparse supervision where only one ground-truth placement bounding box is provided, we propose a sparse contrastive loss to encourage the ground-truth location/scale combination to have a relatively high score, while only minimizing the other combinations with the lowest score or a score higher than the ground-truth with a certain margin, thus preventing large penalty on other rea-sonable locations/scales. Once the 3D heatmap is predicted, top candidate placement bounding boxes can be generated by searching the local maximum in the 3D heatmap. The 3D heatmap also provides guidance for other possible loca-tions/scales which are not the best candidate. Experiments on a large-scale inpainted dataset (Pixabay [1]) and anno-tated dataset (OPA [12]) show the superiority of our ap-proach over previous methods. Our contributions are sum-marized as follows:
• A novel transformer-based architecture to model the cor-relation between object image and local clues from the background image, and generate dense object placement evaluation > 10× faster than previous sliding-window method [29].
• A sparse contrastive loss to effectively train a dense pre-diction network with sparse supervision.
• Extensive experiments on a large-scale inpainted dataset and annotated dataset with state-of-the-art performance. 2.