Abstract
Text-to-motion generation is an emerging and challeng-ing problem, which aims to synthesize motion with the same semantics as the input text. However, due to the lack of di-verse labeled training data, most approaches either limit to specific types of text annotations or require online optimiza-tions to cater to the texts during inference at the cost of ef-ficiency and stability. In this paper, we investigate offline open-vocabulary text-to-motion generation in a zero-shot learning manner that neither requires paired training data nor extra online optimization to adapt for unseen texts. In-spired by the prompt learning in NLP, we pretrain a motion generator that learns to reconstruct the full motion from the masked motion. During inference, instead of chang-ing the motion generator, our method reformulates the in-put text into a masked motion as the prompt for the motion generator to “reconstruct” the motion. In constructing the prompt, the unmasked poses of the prompt are synthesized by a text-to-pose generator. To supervise the optimization of the text-to-pose generator, we propose the first text-pose alignment model for measuring the alignment between texts and 3D poses. And to prevent the pose generator from over-fitting to limited training texts, we further propose a novel wordless training mechanism that optimizes the text-to-pose generator without any training texts. The comprehensive experimental results show that our method obtains a signif-icant improvement against the baseline methods. The code is available at https://github.com/junfanlin/ oohmg. 1.

Introduction
Motion generation has attracted increasing attention due to its practical value in the fields of virtual reality, video games, and movies. Especially for text-conditional motion generation, it can largely improve the user experience if the virtual avatars can react to the communication texts in real time. However, most current text-to-motion approaches
*Corresponding author: tian.qi1@huawei.com
Figure 1. Demonstrations of our OOHMG. Given an unseen open-vocabulary text (e.g., an object name “a basketball”, or a simile description “fly like a bird”, or a usual text “he walks”), OOHMG translates the text into the text-consistent pose, which is used to prompt the motion generator for synthesizing the motion. are trained on paired text-motion data with limited types of annotations, and thus could not well-generalize to unseen open-vocabulary texts.
To handle the open-vocabulary texts, recent works lever-age the powerful zero-shot text-image alignment ability of the pretrained model, i.e., CLIP [35], to facilitate the text-to-motion generation. Some works like MotionCLIP [42] use the CLIP text encoder to extract text features and learn a motion decoder to decode the features into motions. How-ever, they require paired text-motion training data and still could not handle texts that are dissimilar to the training texts. Instead of learning an offline motion generator with paired data, some works like AvatarCLIP [13] generate mo-tions for the given textual descriptions via online matching and optimization. Nevertheless, matching cannot generate new poses to fit diverse texts and online optimization is usu-ally time-consuming and unstable.
In this paper, we investigate filling the blank of offline open-vocabulary text-to-motion generation in a zero-shot learning manner. For convenience, we term our method as OOHMG which stands for Offline Open-vocabulary
Human Motion Generation.
The main philosophy of
culties in learning the text-to-pose generator: 1) what can associate diverse texts and poses to supervise the pose gen-erator, and 2) how to obtain diverse texts as the training in-puts. For difficulty 1, we build the first large-scale text-pose alignment model based on CLIP, namely TPA, that can effi-ciently measure the alignment between texts and 3D SMPL poses [27, 31] in the feature space. With TPA, the text-to-pose generator learns to generate poses for texts by maxi-mizing the text-pose alignments via gradient descent. As for difficulty 2, instead of collecting massive texts laboriously for training, we consider an extreme training paradigm, termed wordless training. Just as its name implies, word-less training only samples random training inputs from the latent space of texts. And we found that the optimized pose generator can well-generalize to real-world texts.
Overall, the contributions of OOHMG are as follows. 1)
We propose an offline open-vocabulary text-to-motion gen-eration framework, inspired by prompt learning, and 2) to supervise the training process of the text-to-pose generator, we propose the first text-pose alignment model, i.e., TPA, and 3) to endow the text-to-pose generator with the ability to handle open-vocabulary texts, we train the generator with the novel wordless training mechanism. 4) Extensive exper-iment results show that OOHMG is able to generate motions for open-vocabulary texts efficiently and effectively, and ob-tain clear improvement over the advanced baseline methods qualitatively and quantitatively. 2.