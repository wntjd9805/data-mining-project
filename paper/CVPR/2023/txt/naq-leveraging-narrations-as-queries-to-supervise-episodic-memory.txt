Abstract
Searching long egocentric videos with natural language queries (NLQ) has compelling applications in augmented reality and robotics, where a fluid index into everything that a person (agent) has seen before could augment human memory and surface relevant information on demand. How-ever, the structured nature of the learning problem (free-form text query inputs, localized video temporal window outputs) and its needle-in-a-haystack nature makes it both technically challenging and expensive to supervise. We in-troduce Narrations-as-Queries (NaQ), a data augmentation strategy that transforms standard video-text narrations into training data for a video query localization model. Vali-dating our idea on the Ego4D benchmark, we find it has tremendous impact in practice. NaQ improves multiple top models by substantial margins (even doubling their accu-racy), and yields the very best results to date on the Ego4D
NLQ challenge, soundly outperforming all challenge win-ners in the CVPR and ECCV 2022 competitions and top-ping the current public leaderboard. Beyond achieving the state-of-the-art for NLQ, we also demonstrate unique properties of our approach such as the ability to perform zero-shot and few-shot NLQ, and improved performance on queries about long-tail object categories. Code and mod-els: http://vision.cs.utexas.edu/projects/naq. 1.

Introduction
Human memory can fail us in day-to-day things in our visual experience. We misplace objects in the house (where is my passport?), we lose track of what tasks we have or have not done (did I add the salt already?), we forget where we did a given activity (where did I buy tickets last time?), we do not notice the state of an object in our environment (did I leave the garage door open?). First-person or “ego-centric” perception on a wearable camera could reduce that cognitive overload and provide us with a superhuman per-sonal episodic memory—by seeing what we see, and index-ing it in meaningful and easy-to-access ways.
This is the vision of the Natural Language Query (NLQ)
Figure 1. Episodic memory with natural language queries (NLQ) aims to search long egocentric videos to identify the temporal
“response” window revealing the answer to a free-form question about the camera wearer’s past visual experience. task in Ego4D’s Episodic Memory benchmark [16]. Given a natural language question and a long egocentric video, the
NLQ task requires identifying the precise temporal window in the camera wearer’s past video that reveals the answer.
See Figure 1. Such functionality could transform the every-day experience of an augmented reality user with always-on AR glasses. It could similarly play a role for a mobile household robot, whom a user may wish to query about its visual history (have you seen my keys?).
The NLQ challenge has attracted substantial attention in the research community over the last year [23, 24, 40] as have related video-language efforts for question answer-ing [31,34,35,37–39]. The technical challenges are striking.
Queries are free-form natural language, response windows are tiny slivers (a few seconds or less) within a long stretch of video, and wearable camera video is notoriously noisy with its quick head motions and limited field of view.
Today’s most successful methods embrace the visual-language aspect of the problem. In particular, inspired by the growing success of visual-linguistic embeddings [22, 25, 29, 33, 37], top competitors on NLQ perform large-scale pretraining on ⟨video clip, text description⟩ pairs mined from the Ego4D dataset’s provided narrations [23], which are timestamped play-by-play descriptions of the camera-methods, achieving sizeable improvements (e.g., 32% to 125% relative jumps in accuracy) across query types, met-rics, and methods. Notably, our gains hold even compared to existing methods such as EgoVLP [23] that use the same (or even more) narration annotations as our model, meaning that NaQ’s success can be attributed to good modeling, not more data. Moreover, NaQ even benefits video-language grounding on exocentric videos, i.e., it is beneficial to aug-ment its exocentric training with narrated egocentric videos.
To our knowledge, NaQ yields the very best results to date on the NLQ challenge, strongly outperforming all the chal-lenge winners from Ego4D CVPR’22 and ECCV’22, and topping the current public leaderboard. Beyond achieving state-of-the-art results, we perform a thorough analysis of the strengths and weaknesses of NaQ, and demonstrate use-ful properties such as benefits on long-tail object queries as well as zero-shot and few-shot NLQ. 2.