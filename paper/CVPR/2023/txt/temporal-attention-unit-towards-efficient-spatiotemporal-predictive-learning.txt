Abstract
Spatiotemporal predictive learning aims to generate fu-ture frames by learning from historical frames. In this pa-per, we investigate existing methods and present a general framework of spatiotemporal predictive learning, in which the spatial encoder and decoder capture intra-frame fea-tures and the middle temporal module catches inter-frame correlations. While the mainstream methods employ recur-rent units to capture long-term temporal dependencies, they suffer from low computational efficiency due to their unpar-allelizable architectures. To parallelize the temporal mod-ule, we propose the Temporal Attention Unit (TAU), which decomposes temporal attention into intra-frame statical at-tention and inter-frame dynamical attention. Moreover, while the mean squared error loss focuses on intra-frame errors, we introduce a novel differential divergence regu-larization to take inter-frame variations into account. Ex-tensive experiments demonstrate that the proposed method enables the derived model to achieve competitive perfor-mance on various spatiotemporal prediction benchmarks. 1.

Introduction
The last decade has witnessed revolutionary advances in deep learning across various supervised learning tasks such as image classification [34,52,87], object detection [73,75], computational biology [21, 22, 43, 83, 84], and etc. Despite significant breakthroughs in supervised learning, which re-lies on large-scale labeled datasets, the potential of unsuper-vised learning remains largely untapped. Self-supervised learning that designs pretext tasks to produce labels derived from the data itself is recognized as a subset of unsupervised
In the context of self-supervised learning, con-learning. trastive self-supervised learning [7, 8, 28, 33, 85, 86, 92, 104] predicts the noise contrastive estimation from predefined positive or negative pairs, and masked self-supervised learn-ing [17, 32, 45, 51, 57, 102] predicts the masked patches from the visible patches. Unlike these image-level self-*Equal contribution.
†Corresponding author. supervised learning, spatiotemporal predictive learning that the video-predicts future frames from past frames at level [6, 19, 27, 46, 60, 63].
Figure 1. Comparison of model architectures among common spa-tiotemporal predictive learning methods. Note that we denote 2D convolutional neural networks as Conv while 3D Conv means 3D convolutional neural networks.
Accurate spatiotemporal predictive learning can bene-fit broad practical applications in climate change [74, 77], human motion forecasting [91, 107], traffic flow predic-tion [18, 97], and representation learning [39, 71]. The significance of spatiotemporal predictive learning primarily lies in its potential of exploring both spatial correlation and temporal evolution in the physical world. Moreover, the self-supervised nature of spatiotemporal predictive learn-ing aligns well with human learning styles without a large amount of labeled data. Massive videos can provide a rich source of visual information, enabling spatiotemporal pre-dictive learning to serve as a generative pre-training strat-egy [35, 60] for feature representation learning towards di-verse downstream visual supervised tasks.
Most of the existing methods [2,3,9,11,12,19,24,29,41, 44,65,78,79,82,88,89,93–96,98,100,103] in spatiotempo-ral predictive learning employ hybrid architectures of con-volutional neural networks and recurrent units in which spa-tial correlation and time evolution can be learned, respec-Inspired by the success of long short-term mem-tively. ory (LSTM) [36] in sequential modeling, ConvLSTM [78] is a seminal work on the topic of spatiotemporal predic-tive learning that extends fully connected LSTM to con-volutional LSTM towards accurate precipitation nowcast-ing. PredRNN [95] is an admirable work that proposes
Spatiotemporal LSTM (ST-LSTM) units to model spatial appearances and temporal variations in a unified memory pool. This work provides insights on designing typical re-current units for spatiotemporal predictive learning and in-spires a series of subsequent works [4, 93, 96, 98]. E3D-LSTM [94] integrates 3D convolutional neural networks into recurrent units towards good representations with both short-term frame dependencies and long-term high-level re-lations. PhyDNet [29] introduces a two-branch architec-ture that involves physical-based PhyCells and ConvLSTMs for performing partial differential equation constraints in latent space. CrevNet [103] proposes an invertible two-way autoencoder based on flow [14, 15] and a condition-ally reversible architecture for spatiotemporal predictive learning. As shown in Figure 1 (a), we present a gen-eral framework consisting of the spatial encoder/decoder and the middle temporal module, abstracted from these methods. Though these spatiotemporal predictive learning methods have different temporal modules and spatial en-coders/decoders, they basically share a similar framework.
Based on the general framework, we argue that the tem-poral module plays an essential role in spatiotemporal pre-dictive learning. While the common choice of the tempo-ral module is the recurrent-based units, we explore a novel parallelizable attention module named Temporal Attention
Unit (TAU) to capture time evolution. The proposed tempo-ral attention is decomposed into intra-frame statical atten-tion and inter-frame dynamical attention. Furthermore, we argue that the mean square error loss only focuses on intra-frame differences, and we propose a differential divergence regularization that also cares about the inter-frame varia-tions. Keeping the spatial encoder and decoder as simple as 2D convolutional neural networks, we deliberately imple-ment our proposed TAU modules and surprisingly find the derived model achieves competitive performance as those recurrent-based models. This observation provides a new perspective to improve spatiotemporal predictive learning by parallelizable attention networks instead of common-used recurrent units.
We conduct experiments on various datasets with dif-ferent experimental settings: (1) Standard spatiotemporal predictive learning. Quantitative results on various datasets demonstrate our proposed method can achieve competitive performance on standard spatiotemporal predictive learn-ing. (2) Generalization ability. To verify the generaliza-tion ability, we train our model on KITTI and test it on the
Caltech Pedestrian dataset with different domains. (3) Pre-dict future frames with flexible lengths. We tackle the long-length frames by feeding the predicted frames as the input and find the performance is consistently well. Through the superior performance in the above three experimental set-tings, we demonstrate that our proposed model can provide a novel manner that learns temporal dependencies without recurrent units. 2.