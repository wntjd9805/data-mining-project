Abstract
Video anomaly detection (VAD) is a signiﬁcant computer vision problem. Existing deep neural network (DNN) based
VAD methods mostly follow the route of frame reconstruc-tion or frame prediction. However, the lack of mining and learning of higher-level visual features and temporal con-text relationships in videos limits the further performance of these two approaches. Inspired by video codec theory, we introduce a brand-new VAD paradigm to break through these limitations: First, we propose a new task of video event restoration based on keyframes. Encouraging DNN to infer missing multiple frames based on video keyframes so as to restore a video event, which can more effectively mo-tivate DNN to mine and learn potential higher-level visual features and comprehensive temporal context relationships in the video. To this end, we propose a novel U-shaped Swin
Transformer Network with Dual Skip Connections (USTN-DSC) for video event restoration, where a cross-attention and a temporal upsampling residual skip connection are in-troduced to further assist in restoring complex static and dynamic motion object features in the video. In addition, we propose a simple and effective adjacent frame differ-ence loss to constrain the motion consistency of the video sequence. Extensive experiments on benchmarks demon-strate that USTN-DSC outperforms most existing methods, validating the effectiveness of our method. 1.

Introduction
Video anomaly detection (VAD) is a hot but challenging research topic in the ﬁeld of computer vision. One of the most challenging aspects comes from the scarcity of anoma-lous samples, which hinders us from learning anomalous behavior patterns from the samples. As a result, it is hard for supervised methods to show their abilities, as unsuper-vised methods are by far the most widely explored direc-tion [1, 2, 4, 8, 10, 19, 25–27, 31, 33, 37, 41–43, 47–51]. To
†Corresponding authors.
N
N o o n n e e
N
N o o n n e e
N
N o o n n e e
N
N o o n n e e
N
N o o n n e e
N
N o o n n e e
I B B B
I B B B
P
P (a) video codec
B
B
BB
BB
I
I
I
I
I
I (b) video event restoration
I
I
Figure 1. Video codec vs. video event restoration. Compared to video decoding based on explicit motion information, our pro-posed video event restoration task encourages DNN to automat-ically mine and learn the implied spatio-temporal relationships from several frames with key appearance and temporal transition information to restore the entire video event. determine whether an abnormal event occurs under unsu-pervised, a general approach is to model a regular pattern based on normal samples, leaving samples that are incon-sistent with this pattern as anomalies.
Under the unsupervised framework, reconstruction and prediction-based approaches are two of the most represen-tative paradigms [10, 19] for VAD in the current era of deep neural networks (DNN). The reconstruction-based ap-proaches typically use deep autoencoder (DAE) to learn to reconstruct an input frame and regard a frame with large reconstruction errors as anomalous. However, the power-ful generalization capabilities of DAE enable well recon-struction of anomalous as well [8]. This is attributed to the simple task of reconstructing input frames, where DAE only memorizes low-level details rather than understand-ing high-level semantics [17]. The idea of the prediction-based approach assumes that anomalous events are un-predictable and then builds a model that can predict fu-ture frames according to past frames, and poor predictions of future frames indicate the presence of anomalies [19].
The prediction-based approaches further consider short-term temporal relationships to model appearance and mo-tion patterns. However, due to the appearance and motion variations of adjacent frames being minimal, the predic-tor can predict the future frame better based on the past few frames, even for anomalous samples. The inherent lack of capability of these two modeling paradigms in min-ing higher-level visual features and comprehensive tempo-ral context relationships limits their further performance im-provement.
In this paper, we aim to explore better methods for mining complex spatio-temporal relationships in the video.
Inspired by video codec theory [15, 18], we propose a brand-new VAD paradigm: video event restoration based on keyframes for VAD. In the video codec [18], three types of frames are utilized, namely I-frame, P-frame, and B-frame.
I-frame contains the complete appearance information of the current frame, which is called a keyframe. P-frame con-tains the motion difference information from the previous frame, and B-frame contains the motion difference between the previous and next frames. Based on these three types of frames containing explicit appearance and motion rela-tive relationships, the video can be decoded. Inspired by this process, our idea sprang up: It should be also theoreti-cally feasible if we give keyframes that contain only implicit relative relations between appearance and motion, and then encourage DNN to explore and mine the potential spatio-temporal variation relationships therein to infer the missing multiple frames for video event restoration. To motivate
DNN to explore and learn spatio-temporal evolutionary re-lationships in the video actively, we do not provide frames like P-frames or B-frames that contain explicit motion in-formation as input cues. This task is extremely challenging compared to reconstruction and prediction-based tasks, be-cause DNN must learn to infer the missing multiple frames based on keyframes only. This requires a strong regularity and temporal correlation of the event in the video for a better restoration. On the contrary, video restoration will be poor for anomalous events that are irregular and random. Under this assumption, our proposed keyframes-based video event restoration task can be exactly applicable to VAD. Fig. 1 compares the video codec and video event restoration task with an illustration.
To perform this challenging task, we propose a novel
U-shaped Swin Transformer Network with Dual Skip
Connections (USTN-DSC) for video event restoration based on keyframes. USTN-DSC follows the classic U-Net [36] architecture design, where its backbone consists of multiple layers of swin transformer (ST) [21] blocks. Fur-thermore, to cope with the complex motion patterns in the video so as to better restore the video event, we build dual skip connections in USTN-DSC. Speciﬁcally, we introduce a cross-attention and a temporal upsampling residual skip connection to further assist in restoring complex dynamic and static motion object features in the video. In addition, to ensure that the restored video sequence has the consistency of temporal variation with the real video sequence, we pro-pose a simple and effective adjacent frame difference (AFD) loss. Compared with the commonly used optical ﬂow con-straint loss [19], AFD loss is simpler to be calculated while having comparable constraint effectiveness.
The main contributions are summarized as follows:
• We introduce a brand-new video anomaly detection paradigm that is to restore the video event based on keyframes, which can more effectively mine and learn higher-level visual features and comprehensive tempo-ral context relationships in the video.
• We introduce a novel model called USTN-DSC for video event restoration, where a cross-attention and a temporal upsampling residual skip connection are in-troduced to further assist in restoring complex dynamic and static motion object features in the video.
• We propose a simple and effective AFD loss to con-strain the motion consistency of the video sequence.
• USTN-DSC outperforms most existing methods on three benchmarks, and extensive ablation experiments demonstrate the effectiveness of USTN-DSC. 2.