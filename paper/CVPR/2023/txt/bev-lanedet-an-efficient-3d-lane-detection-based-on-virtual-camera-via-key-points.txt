Abstract
Autonomous Driving Systems (ADS), vehicle self-control, localization, and map construction. 3D lane detection which plays a crucial role in vehicle routing, has recently been a rapidly developing topic in au-tonomous driving. Previous works struggle with practical-ity due to their complicated spatial transformations and in-ﬂexible representations of 3D lanes. Faced with the issues, our work proposes an efﬁcient and robust monocular 3D lane detection called BEV-LaneDet with three main contri-butions. First, we introduce the Virtual Camera that uniﬁes the in/extrinsic parameters of cameras mounted on differ-ent vehicles to guarantee the consistency of the spatial re-lationship among cameras. It can effectively promote the learning procedure due to the uniﬁed visual space. We sec-ondly propose a simple but efﬁcient 3D lane representation called Key-Points Representation. This module is more suit-able to represent the complicated and diverse 3D lane struc-tures. At last, we present a light-weight and chip-friendly spatial transformation module named Spatial Transforma-tion Pyramid to transform multiscale front-view features into BEV features. Experimental results demonstrate that our work outperforms the state-of-the-art approaches in terms of F-Score, being 10.6% higher on the OpenLane dataset and 4.0% higher on the Apollo 3D synthetic dataset, with a speed of 185 FPS. Code is released at https:
//github.com/gigo-team/bev_lane_det. 1.

Introduction
As one of the fundamental guarantees for autonomous driving, lane detection has recently received much attention from researchers. Robust lane detection in real-time is one of the foundations for advanced autonomous driving, which can provide substantial amounts of useful information for
*Corresponding author
This work was supported by the National Key Research and Devel-opment Project of New Generation Artiﬁcial Intelligence of China under
Grant 2018AAA0102504.
BEV output
Z 3D Head
STP
STP
Backbone
C a m e r a
V i r t u a l
Figure 1. End-to-end framework illustrated. The original image in bottom-right is transformed to an input image by the Virtual
Camera module. The input image is then encoded into front-view features by the backbone. Multiscale features from the backbone are put into the Spatial Transformation Pyramid (STP) to obtain
BEV features. Given the BEV features, the 3D Head called Key-Points Representation generates BEV output and Z, the height of
BEV lanes. At last, with the BEV output and Z, we can obtain 3D lanes. 2D lane detection methods have demonstrated remark-able performances [4, 18, 20, 22]. Moreover, their outputs are usually projected to the ﬂat ground plane by Inverse Per-spective Transformation (IPM) with the camera in/extrinsic parameters, and then curve ﬁtting is performed to obtain the
BEV lanes. However, the pipeline might cause other prob-lems in the actual driving process [1, 20] for challenging situations like uphill and downhill.
In order to overcome these problems, more recent meth-ods [3, 6, 8, 9, 14] have started to focus on the more com-plicated 3D lane perception domain. There are two signif-icant challenges in 3D lane detection: an efﬁcient spatial transformation module to obtain BEV features and a robust  
representation for 3D lane structures. The acquisition of
BEV features is heavily dependent on camera in/extrinsic parameters, and previous methods have chosen to incor-porate camera in/extrinsic parameters into the network to obtain BEV features. Moreover, unlike obstacles on the road, lane structures are slender and diverse. These meth-ods [3, 8, 9] carefully design the 3D anchor representation for lane structures with strong priors. However, they lack sufﬁcient ﬂexibility in some speciﬁc scenarios, as shown in
Figure 5. Moreover, the anchor-free method [6] proposes a 3D lane representation based on the hypothesis that a line segment in each predeﬁned tile is straight. This representa-tion is complicated and inaccurate.
Towards the issues, we introduce BEV-LaneDet, an ef-ﬁcient and real-time pipeline that achieves 3D lane detec-tion from a single image, as shown in Figure 1. Different from incorporating camera in/extrinsic parameters into the network to get BEV features, we establish a Virtual Cam-era, which is applied to images directly. The module uniﬁes the in/extrinsic parameters of front-facing cameras in differ-ent vehicles by the homography method [2] based on BEV.
This module guarantees the consistency of the spatial rela-tionship of front-facing cameras in different vehicles and re-duces variance in data distribution. Therefore, it can effec-tively promote the learning procedure due to the uniﬁed vi-sual space. We also propose a Key-Points Representation as our 3D lane representation. We demonstrate that it is a sim-ple but effective module to represent 3D lanes and is more expandable for complicated lane structures in some special scenarios. Moreover, the cost of computation and the chip’s friendliness are also crucial factors in autonomous driving.
Therefore, a light-weight and easy-to-deploy spatial trans-formation module based on MLP is our preference. Mean-while, inspired by FPN [17], we present the Spatial Trans-formation Pyramid, which transforms multiscale front-view features to BEV and provides robust BEV features for 3D lane detection. In our experiments, we perform extensive studies to conﬁrm that our BEV-LaneDet signiﬁcantly out-performs the state-of-the-art PersFormer [3] in terms of F-Score, being 10.6% higher on the OpenLane real-world test set [3] and 4.0% higher on the Apollo simulation test set [9] with a speed of 185 FPS.
In summary, our main contributions are three-fold: 1) Virtual Camera, a novel preprocessing module to unify the in/extrinsic parameters of cameras, ensuring data distri-bution consistency. 2) Key-Points Representation, a simple but effective representation of 3D lane structures. 3) Spa-tial Transformation Pyramid, a light-weight and easy-to-deploy architecture based on MLP to realize transformation from multiscale front-view features to BEV. Experiments demonstrate that our BEV-LaneDet achieves the state-of-the-art performance compared to other 3D lane detection algorithms. 2.