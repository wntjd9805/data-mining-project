Abstract
This paper presents a learning-based method for multi-view depth estimation from posed images. Our core idea is a “learning-to-optimize” paradigm that iteratively indexes a plane-sweeping cost volume and regresses the depth map via a convolutional Gated Recurrent Unit (GRU). Since the cost volume plays a paramount role in encoding the multi-view geometry, we aim to improve its construction both at pixel- and frame- levels. At the pixel level, we propose to break the symmetry of the Siamese network (which is typi-cally used in MVS to extract image features) by introducing a transformer block to the reference image (but not to the source images). Such an asymmetric volume allows the net-work to extract global features from the reference image to predict its depth map. Given potential inaccuracies in the poses between reference and source images, we propose to incorporate a residual pose network to correct the relative poses. This essentially rectifies the cost volume at the frame level. We conduct extensive experiments on real-world MVS datasets and show that our method achieves state-of-the-art performance in terms of both within-dataset evaluation and cross-dataset generalization. 1.

Introduction
Multi-view stereo (MVS) aims to recover dense 3D ge-ometry from multiple images captured from different view-points with calibrated cameras [28].
It is a fundamen-tal problem in computer vision and has wide applications ranging from autonomous driving [12, 55], remote sens-ing [3], augmented reality [50], to robotics [22]. Follow-ing the seminal MVSNet [59], many learning-based meth-ods [17, 39, 40, 52, 53, 58, 60] have been proposed, achiev-ing great improvements against their traditional counter-parts [5, 14, 19, 44], in terms of accuracy or efficiency.
Most of the learning-based MVS methods [17,39,40,52, 58, 60] rely on traditional plane-sweeping [14, 19] approach to generate a cost volume by comparing the CNN features of reference image and source images at several depth hy-potheses, and then apply 2D or 3D convolutional encoder-decoders to aggregate and regularize the cost volume. The 2D CNN methods [17] use multi-level features as skip con-nections to help decode the cost volume for depth regres-sion. Even though the skip connections improve the depth maps, they weaken the role of cost volume and the geome-try knowledge embedded therein to some extent. Hence, 2D
CNN methods suffer from degraded generalization when testing on unseen domains. The 3D CNN methods [31] use soft-argmin to regress the depth map as the expectation from the cost volume distribution, and hence cannot predict the best candidate but instead an averaged one when dealing with a flat or multi-modal distribution caused by textureless, repeated, or occluded regions, etc. To mitigate these prob-lems, we propose RIAV-MVS, a new paradigm to predict the depth via learning to recurrently index an asymmetric cost volume, obtaining improved accuracy and generaliza-tion. As depicted in Fig. 1, our RIAV-MVS features several nontrivial novel designs.
First, we learn to index the cost volume by approach-ing the correct depth planes per pixel via an index field (a grid of indices to identify the depth hypotheses), as shown in Fig. 1-(e). The proposed recurrent estimate of the index field enables the learning to be anchored at the cost volume domain. Specifically, it recurrently predicts the residual in-dex field in a descent direction of matching cost to retrieve cost values for the next iteration. The newly updated index field is used to directly index (i.e., sampling via linear in-terpolation) depth hypotheses to render a depth map, which is iteratively optimized to approach the ground truth depth, making the system end-to-end trainable.
Second, to facilitate the optimization, we propose to im-prove the cost volume at pixel- and frame- levels, respec-tively. At the pixel level, a transformer block is asymmet-rically applied to the reference view (but not to the source views). By capturing long-range global context via a trans-former and pixel-wise local features via CNNs, we build an asymmetric cost volume to store more accurate matching similarity cues. At the frame level, we propose a residual pose net to rectify the camera poses that are usually ob-tained via Visual SLAM [9, 16, 30] and inevitably contain noise. The rectified poses are used to more accurately back-ward warp the reference features to match the counterparts in source views.
Figure 1. Our pipeline versus RAFT [49] and IterMVS [52]. Our recurrent processing of a plane-sweep cost volume by the iteratively refined index field serves as a new design for multi-view depth estimation.
Our RIAV-MVS is depicted versus two related works
RAFT [49] and IterMVS [52] as in Fig. 1. First, our method is developed using RAFT’s GRU-based iterative optimiza-tion. However, RAFT operates an all-pair correlation vol-ume (no multi-view geometry constraints) for optical flow (Fig. 1-(a) and (c)), our method is proposed for multi-view depth estimation by constructing a plane-sweep cost vol-ume (Fig. 1-(b)). Second, IterMVS [52] iteratively predicts the depth and reconstructs a new plane-sweep cost volume using updated depth planes centered at the predicted depth (Fig. 1-(d)). Instead, as shown in Fig. 1-(e), our proposed index field serves as a new design that bridges the cost vol-ume optimization (i.e., by learning better image features via back-propagation) and the depth map estimation (i.e., by sampling sweeping planes).
It makes forward and back-ward learning differentiable. We conduct extensive exper-iments on indoor-scene datasets, including ScanNet [15],
DTU [27], 7-Scenes [20], and RGB-D Scenes V2 [32]. We also performed well-designed ablation studies to verify the effectiveness and the generalization of our approach. 2.