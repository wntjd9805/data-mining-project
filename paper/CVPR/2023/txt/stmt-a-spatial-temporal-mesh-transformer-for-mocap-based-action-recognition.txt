Abstract
We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing tech-niques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to di-rectly model the mesh sequences. The model uses a hier-archical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn non-local relationships in the spatial-temporal do-main. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierar-chical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/
STMT. 1.

Introduction
Motion Capture (MoCap) is the process of digitally recording the human movement, which enables the fine-†Equal Contribution. grained capture and analysis of human motions in 3D space
[40, 50]. MoCap-based human perception serves as key elements for various research fields, such as action recog-nition [15, 46–48, 50, 57], tracking [47], pose estimation
[1, 27], imitation learning [76], and motion synthesis [47].
Besides, MoCap is one of the fundamental technologies to enhance human-robot interactions in various practical sce-narios including hospitals and manufacturing environment
[22, 28, 41, 43, 45, 77]. For example, Hayes [22] classi-fied automotive assembly activities using MoCap data of humans and objects. Understanding human behaviors from
MoCap data is fundamentally important for robotics per-ception, planning, and control.
Skeleton representations are commonly used to model
MoCap sequences. Some early works [3, 29] directly used body markers and their connectivity relations to form a skeleton graph. However, the marker positions depend on each subject (person), which brings sample variances within each dataset. Moreover, different MoCap datasets usu-ally have different numbers of body markers. For exam-ple, ACCAD [48], BioMotion [64], Eyes Japan [15], and
KIT [42] have 82, 41, 37, and 50 body markers respec-tively. This prevents the model to be trained and tested on a unified framework. To use standard skeleton representa-tions such as NTU RGB+D [58], Punnakkal et al. [50] first used Mosh++ to fit body markers into SMPL-H meshes, and then predicted a 25-joint skeleton [33] from the mesh ver-tices [54]. Finally, a skeleton-based model [60] was used to perform action recognition. Although those methods achieved advanced performance, they have the following disadvantages. First, they require several manual steps to map the vertices from mesh to skeleton. Second, skeleton representations lose the information provided by original
MoCap data (i.e., surface motion and body shape knowl-edge). To overcome those disadvantages, we propose a mesh-based action recognition method to directly model dynamic changes in raw mesh sequences, as illustrated in
Figure 1.
Though mesh representations provide fine-grained body information, it is challenging to classify high-dimensional mesh sequences into different actions. First, unlike struc-tured 3D skeletons which have joint correspondence across frames, there is no vertex-level correspondence in meshes (i.e., the vertices are unordered). Therefore, the local con-nectivity of every single mesh can not be directly aggre-gated in the temporal dimension. Second, mesh repre-sentations encode local connectivity information, while ac-tion recognition requires global understanding in the whole spatial-temporal domain.
To overcome the aforementioned challenges, we pro-pose a novel Spatial-Temporal Mesh Transformer (STMT).
STMT leverages mesh connectivity information to build patches at the frame level, and uses a hierarchical trans-former which can freely attend to any intra- and inter-frame patches to learn spatial-temporal associations. The hierar-chical attention mechanism allows the model to learn patch correlation across the entire sequence, and alleviate the re-quirement of explicit vertex correspondence. We further de-fine two self-supervised learning tasks, namely masked ver-tex modeling and future frame prediction, to enhance the global interactions among vertex patches. To reconstruct masked vertices of different body parts, the model needs to learn prior knowledge about the human body in the spatial dimension. To predict future frames, the model needs to understand meaningful surface movement in the temporal dimension. To this end, our hierarchical transformer pre-trained with those two objectives can further learn spatial-temporal context across entire frames, which is beneficial for the downstream action recognition task.
We evaluate our model on common MoCap benchmark datasets. Our method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models.
The contributions of this paper are three-fold:
• We introduce a new hierarchical transformer architec-ture, which jointly encodes intrinsic and extrinsic rep-resentations, along with intra- and inter-frame atten-tion, for spatial-temporal mesh modeling.
• We design effective and efficient pretext tasks, namely masked vertex modeling and future frame prediction, to enable the model to learn from the spatial-temporal global context.
• Our model achieves superior performance compared to state-of-the-art point-cloud and skeleton models on common MoCap benchmarks. 2.