Abstract
Semi-supervised action recognition is a challenging but critical task due to the high cost of video annotations. Exist-ing approaches mainly use convolutional neural networks, yet current revolutionary vision transformer models have been less explored.
In this paper, we investigate the use of transformer models under the SSL setting for action recognition. To this end, we introduce SVFormer, which adopts a steady pseudo-labeling framework (i.e., EMA-Teacher) to cope with unlabeled video samples. While a wide range of data augmentations have been shown effec-tive for semi-supervised image classification, they generally produce limited results for video recognition. We there-fore introduce a novel augmentation strategy, Tube Token-Mix, tailored for video data where video clips are mixed via a mask with consistent masked tokens over the temporal axis. In addition, we propose a temporal warping augmen-tation to cover the complex temporal variation in videos, which stretches selected frames to various temporal dura-tions in the clip. Extensive experiments on three datasets
Kinetics-400, UCF-101, and HMDB-51 verify the advan-tage of SVFormer.
In particular, SVFormer outperforms the state-of-the-art by 31.5% with fewer training epochs under the 1% labeling rate of Kinetics-400. Our method can hopefully serve as a strong benchmark and encour-age future search on semi-supervised action recognition with Transformer networks. Code is released at https:
//github.com/ChenHsing/SVFormer. 1.

Introduction
Videos have gradually replaced images and texts on In-ternet and grown at an exponential rate. On video websites such as YouTube, millions of new videos are uploaded ev-ery day. Supervised video understanding works [4, 15, 17, 29, 34, 56, 70] have achieved great successes. They rely on
† Corresponding author.
Figure 1. Comparison of our method with the supervised baseline and previous state-of-the-art SSL method [64]. SVFormer signif-icantly outperforms previous methods under the case with very little labeled data. large-scale manual annotations, yet labeling so many videos is time-consuming and labor-intensive. How to make use of unlabeled videos that are readily available for better video understanding is of great importance [25, 38, 39].
In this spirit, semi-supervised action recognition [25, 40, 57] explores how to enhance the performance of deep learning models using large-scale unlabeled data. This is generally done with labeled data to pretrain the net-works [57,64], and then leveraging the pretrained models to generate pseudo labels for unlabeled data, a process known as pseudo labeling. The obtained pseudo labels are further used to refine the pretrained models. In order to improve the quality of pseudo labeling, previous methods [57, 62] use additional modalities such as optical flow [3] and tem-poral gradient [50], or introduce auxiliary networks [64] to supervise unlabeled data. Though these methods present promising results, they typically require additional training or inference cost, preventing them from scaling up.
Recently, video transformers [2,4,34] have shown strong results compared to CNNs [15, 17, 22]. Though great suc-cess has been achieved, the exploration of transformers on semi-supervised video tasks remains unexplored. While it sounds appealing to extend vision transformers directly to
SSL, a previous study shows that transformers perform sig-nificantly worse compared to CNNs in the low-data regime
due to the lack of inductive bias [54]. As a result, directly applying SSL methods, e.g., FixMatch [41], to ViT [13] leads to an inferior performance [54].
Surprisingly, in the video domain, we observe that
TimeSformer, a popular video Transformer [4], initialized with weights from ImageNet [11], demonstrates promising results even when annotations are limited [37]. This en-courages us to explore the great potential of transformers for action recognition in the SSL setting.
Existing SSL methods generally use image augmenta-tions (e.g., Mixup [67] and CutMix [66]) to speed up con-vergence under limited label resources. However, such pixel-level mixing strategies are not perfectly suitable for transformer architectures, which operate on tokens pro-duced by patch splitting layers.
In addition, strategies like Mixup and CutMix are particularly designed for image tasks, which fail to consider the temporal nature of video data. Therefore, as will be shown empirically, directly us-ing Mixup or CutMix for semi-supervised action recogni-tion leads to unsatisfactory performance.
In this work, we propose SVFormer, a transformer-based semi-supervised action recognition method. Concretely,
SVFormer adopts a consistency loss that builds two dif-ferently augmented views and demands consistent predic-tions between them. Most importantly, we propose Tube
TokenMix (TTMix), an augmentation method that is natu-rally suitable for video Transformer. Unlike Mixup and Cut-Mix, Tube TokenMix combines features at the token-level after tokenization via a mask, where the mask has consistent masked tokens over the temporal axis. Such a design could better model the temporal correlations between tokens.
Temporal augmentations in literatures (e.g. varying frame rates) only consider simple temporal scaling or shift-ing, neglecting the complex temporal changes of each part in human action. To help the model learn strong temporal dynamics, we further introduce the Temporal Warping Aug-mentation (TWAug), which arbitrarily changes the temporal length of each frame in the clip. TWAug can cover the com-plex temporal variation in videos and is complementary to spatial augmentations [10]. When combining TWAug with
TTMix, significant improvements are achieved.
As shown in Fig. 1, SVFormer achieves promising re-sults in several benchmarks. (i) We observe that the super-vised Transformer baseline is much better than the Conv-based method [22], and is even comparable with the 3D-ResNet state-of-the-art method [64] on Kinetics400 when (ii) SVFormer-S significantly trained with 1% of labels. outperforms previous state-of-the-arts with similar param-eters and inference cost, measured by FLOPs. (iii) Our method is also effective for the larger SVFormer-B model.
Our contributions are as follows:
• We are the first to explore the transformer model for semi-supervised video recognition. Unlike SSL for image recognition with transformers, we find that us-ing parameters pretrained on ImageNet is of great im-portance to ensure decent results for action recognition in the low-data regime.
• We propose a token-level augmentation Tube Token-Mix, which is more suitable for video Transformer than pixel-level mixing strategies. Coupled with Tem-poral Warping Augmentation, which improves tempo-ral variations between frames, TTMix achieves signif-icant boost compared with image augmentation.
• We conduct extensive experiments on three benchmark datasets. The performances of our method in two different sizes (i.e., SVFormer-B and SVFormer-S) outperform state-of-the-art approaches by clear mar-gins. Our method sets a strong baseline for future transformer-based works. 2.