Abstract
This paper proposes a novel method for vision-based metric cross-view geolocalization (CVGL) that matches the camera images captured from a ground-based vehicle with an aerial image to determine the vehicle’s geo-pose.
Since aerial images are globally available at low cost, they represent a potential compromise between two estab-lished paradigms of autonomous driving, i.e. using expensive high-definition prior maps or relying entirely on the sensor data captured at runtime.
We present an end-to-end differentiable model that uses the ground and aerial images to predict a probability distri-bution over possible vehicle poses. We combine multiple ve-hicle datasets with aerial images from orthophoto providers on which we demonstrate the feasibility of our method. Since the ground truth poses are often inaccurate w.r.t. the aerial images, we implement a pseudo-label approach to produce more accurate ground truth poses and make them publicly available.
While previous works require training data from the tar-get region to achieve reasonable localization accuracy (i.e. same-area evaluation), our approach overcomes this limi-tation and outperforms previous results even in the strictly more challenging cross-area case. We improve the previous state-of-the-art by a large margin even without ground or aerial data from the test region, which highlights the model’s potential for global-scale application. We further integrate the uncertainty-aware predictions in a tracking framework to determine the vehicle’s trajectory over time resulting in a mean position error on KITTI-360 of 0.78m.
1.

Introduction
Systems for autonomous driving require both a model of the vehicle’s environment as well as the location of the vehicle relative to the model. These systems either construct the full model during runtime (i.e. entirely online), or create some parts prior to runtime (i.e. partly offline). The latter methods typically construct high-definition maps of a region in advance (e.g. using lidar sensors) and localize the vehi-cle at runtime relative to that map [34]. While prior maps facilitate a high localization accuracy of the system, they are also expensive to construct and maintain. Online methods on the other hand create a model of the local environment using only the live sensor readings e.g. from lidar [52], cam-era [15] or both [26]. This avoids the need for expensive prior maps, but represents a more difficult task as the system has to predict both the spatial structure of the environment as well as its relative location within it.
Aerial images offer the potential to leverage the advan-tages of both approaches: They can be used as a prior map for localization, while also being affordable, globally available and up-to-date due to an established infrastructure of satel-lite and aerial orthophoto providers [1, 3]. We consider the problem of matching the sensor measurements of the vehicle against aerial images to determine the vehicle’s location on the images and thereby its geo-location.
Previous research in this area focuses on methods that cover large (e.g. city-scale) search regions [23,46], but suffer from low metric accuracy [57] insufficient for the navigation of autonomous vehicles. Since a prior pose estimate of the vehicle can be provided by global navigation satellite sys-tems (GNSS) or by tracking the vehicle continuously, several recent methods employ smaller search regions to achieve higher metric accuracy [13, 35].
Without access to three-dimensional lidar point clouds, a purely vision-based model has to bridge the gap between ground and aerial perspectives, for example by learning the transformation in a data-centric manner. We utilize a trans-former model that iteratively constructs a bird’s eye view (BEV) map of the local vehicle environment by aggregating information from the ground-level perspective views (PV).
The BEV refers to a nadir (i.e. orthogonal) view of the local vehicle environment. The final BEV map is matched with an aerial image to predict the relative vehicle pose with three de-grees of freedom (3-DoF), i.e. a two-dimensional translation and a one-dimensional rotation.
Our model outperforms previous approaches for the met-ric CVGL task on the Ford AV [6] and KITTI-360 [21] datasets and even surpasses related approaches utilizing lidar sensors in addition to camera input. It predicts a soft prob-ability distribution over possible vehicle poses (cf . Fig. 1) rather than a single pose which specifically benefits trackers that use the model predictions to determine the vehicle’s trajectory over time.
While previous works rely on the availability of training data from the target region to achieve reasonable localization accuracy, we address the strictly more challenging task of non-overlapping train and test regions. We further train and test the model on entirely different datasets that were cap-tured with different ground-based vehicles. Our evaluation demonstrates the generalization capabilities of our model under cross-area and cross-vehicle conditions and highlights the potential for global-scale application without fine-tuning on a new region or a new vehicle setup.
We collect multiple datasets from the autonomous driv-ing sector in addition to aerial images from several or-thophoto providers for our evaluation. Since the vehicle’s geo-locations do not always accurately match the correspond-ing aerial images, we compute new geo-registered ground truth poses for all datasets used in the work and filter out invalid samples via a data-pruning approach.
We publish the source code of our method online includ-ing a common interface for the different datasets. We also make the improved ground truth for all datasets publicly available. 1
In summary, our contributions are as follows: 1. We present a novel end-to-end trainable model for met-ric CVGL that requires only visual input and yields uncertainty-aware predictions. 2. We collect multiple vehicle datasets and aerial im-ages from several orthophoto providers for our eval-uation. We compute improved ground truth poses using a pseudo-label approach and filter out invalid samples via data-pruning. 3. Our method outperforms previous works by a large margin even under strictly more challenging cross-area and cross-vehicle settings. 2.