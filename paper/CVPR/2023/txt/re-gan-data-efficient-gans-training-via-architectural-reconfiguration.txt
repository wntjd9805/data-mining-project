Abstract 
Training  Generative  Adversarial  Networks  (GANs)  on  high-fidelity  images  usually  requires  a  vast  number  of  training images. Recent research on GAN tickets reveals  that  dense  GANs  models  contain  sparse  sub-networks  or 
"lottery tickets" that, when trained separately, yield better  results under limited data. However, finding GANs tickets  requires  an  expensive  process  of  train-prune-retrain.  In  this  paper,  we  propose  Re-GAN,  a  data-efficient  GANs  training that dynamically reconfigures GANs architecture  during training to explore different sub-network structures  time.  Our  method  repeatedly  prunes  in  unimportant connections to regularize GANs network and  regrows  them  to  reduce  the  risk  of  prematurely  pruning  important  connections.  Re-GAN  stabilizes  the  GANs  models  with  less  data  and  offers  an  alternative  to  the  existing  GANs  tickets  and  progressive  growing  methods. 
We  demonstrate  that  Re-GAN  is  a  generic  training  methodology  which  achieves  stability  on  datasets  of  varying sizes, domains, and resolutions (CIFAR-10, Tiny-ImageNet,  and  multiple  few-shot  generation  datasets)  as  well  as  different  GANs  architectures  (SNGAN,  ProGAN,  training  improves 
StyleGAN2  and  AutoGAN).  Re-GAN  also  performance when combined with the recent augmentation  approaches.  Moreover,  Re-GAN  requires  fewer  floating-point  operations  (FLOPs)  and  less  training  time  by  removing  the  unimportant  connections  during  GANs  training while maintaining comparable or even generating  higher-quality samples. When compared to state-of-the-art 
StyleGAN2, our method outperforms without requiring any  additional fine-tuning step. Code can be found at this link:  https://github.com/IntellicentAI-Lab/Re-GAN  1.

Introduction 
In  recent  years,  Generative  adversarial  networks  (GANs) [4]–[7] have made great strides in generating high-fidelity images. The GANs models serve as the backbone  of several vision applications, such as data augmentation 
[5], [8], [9], domain adaptation [10], [11], and image-to-image translation [14]–[16]. 
The success of the GANs methods largely depends on a  massive  quantity  of  diverse  training  data,  which  is  often  time-consuming and challenging to collect [17]. Figure 1  shows how the performance of the StyleGAN2 [18] model  drastically  declines  under  the  limited  training  data.  As  a                     
result, various new methods [1], [19], [20] have emerged  to  deal  with  the  problem  of  insufficient  data.  Dynamic  data-augmentation  [1],  [19]–[21]  fills  in  the  gap  and  stabilizes  GANs  training  with  less  data.  Very  recently, 
[22], [23] introduced the lottery ticket hypothesis (LTH) in 
GANs  (called  “GANs  tickets”),  a  complementary  to  the  existing  augmentation  techniques.  LTH  identifies  sparse  sub-networks (called “winning tickets”) that can be trained  in isolation to match or even surpass the performance of  unpruned  models.  [24]  demonstrated  that  an  identified 
GANs  ticket  can  be  used  as  a  sparse  structural  prior  to  alleviate the problem of limited data in GANs. However,  identifying these winning tickets requires many iterations  of a time-consuming and computationally expensive train-prune-retrain process. This results in high training time and  a  number  of  floating-point  operations  (FLOPs)  than  training a dense GANs models, such as StyleGAN2 [18]  and BigGAN [5]. In addition, these methods train a full-scale  model  before  pruning,  and  then,  after  the  pruning  process, they engage in an extra fine-tuning to improve the  performance. Given this perspective, we ask: 
Is  there  any  way  to  achieve  training  efficiency  w.r.to  both  data  and  computation  in  GANs  while preserving  or  even improving its performance? 
One  potential  solution  is  network  pruning  during  training, which can allow the exploration of different sub-network  structures  in  training-time.  Network  structure  exploration during training has shown to be effective in a  variety of domains [25], [26], and its properties have been  the subject of a significant amount of research [27], [28]. 
However,  network  pruning  is  never  introduced  to  GANs  training;  as  a  result,  the  investigation  of  different  sub-network  structures  exploration  during  GANs  training  remains mysterious.  
To address this gap in the literature, we investigate and  introduce the network pruning, i.e., connections, in GANs  training by dynamically reconfiguring GANs architecture  to allow the exploration of different sub-network structures  in  training  time,  dubbed  as  Re-GAN.  However,  on  the  other  hand,  it  is  common  knowledge  that  the  learning  capabilities  of  the  two  competing  networks—a  generator  (G)  and  a  discriminator  (D),  need  to  be  carefully  maintained equilibrium in their respective capabilities for  learning. Hence to build Re-GAN, the first question is: how  to  explore  different  network  structures  during  GANs  training? Network pruning during training regularizes the 
G to allow a robust gradient flow through G. This stabilizes  the GANs models under limited training data and improves  training efficiency. Re-GAN repeatedly prunes and grows  the connections during the training process to reduce the  risk  of  pruning  important  connections  prematurely  and  its  representational  prevent  capabilities  early  in  the  training  process.  As  a  result,  network  growing  provides  a  second  opportunity  to  the  model  from  losing  reinitialize  pruned  connections  by  reusing  information  from previously explored sub-network structures. 
Figure 2: Conventional GANs training has fixed connectivity  space.  Re-GAN  uses  network  pruning  and  growing  during  training to make connectivity space flexible that helps in the  propagation of robust gradients. Best viewed in color. 
The  second  question  is:  how  to  explore  different  sub-network structures in G and D simultaneously? On the one  hand, if we employ a pretrained D (or G) and prune solely  for G (or D), it can quickly incur an imbalance between the  capabilities  of  D  or  G  (particularly  in  the  early  stage  of  training),  resulting  in  slow  convergence.  While  it  is  possible  to  prune  for  G  and  D  simultaneously,  empirical  experiments show that doing so significantly degrades the  initial  unstable  GANs  in  highly  fluctuating training curves and, in many cases, a failure to  converge. As a trade-off, we propose expanding D as per  standard  GANs  training  while  applying  pruning  exclusively to G's architecture.  training,  resulting 
Additionally, our method is robust, working well with a  wide  range  of  GANs  architectures  (ProGANs  [29], 
SNGAN [30], StyleGAN2, and AutoGAN [31], [32]) and  datasets (CIFAR-10 [3], Tiny-ImageNet [33], Flickr Faces 
HQ [34], and many few-shot generation datasets). We find  that  exploring  different  sub-network  structures  during  training  accounts  for  a  significant  decrease  in  FID  score  compared to the vanilla DCGAN [35] architecture without  a pre-trained model or fine-tuning the pruned model (see 
Figure 2). Our method delivers higher performance in less  training time to state-of-the-art (SOTA) methods on most  available datasets without additional hyperparameters that  progressive  growing  method  introduces,  such  as  training  schedules and learning rates for different generation stages  (resolutions).  This  robustness  allows  the  Re-GAN  to  be  easily generalized on unseen datasets.  
To  the  best  of  our  knowledge,  Re-GAN  is  the  first  attempt  to  incorporate  network  pruning  during  GANs  training. Our technical innovations are as follows: 
•  We conduct the first in-depth study on taking a unified  approach  of  incorporating  pruning  in  GANs  training  without pre-training a large model or fine-tuning the  pruned model.  
•  Our  method  repeatedly  prunes  and  grows  the  connections during training to reduce the possibility of           
pruning important connections and helps the model to  maintain  its  representation  ability  early  in  training. 
Thus,  network  growing  gives  another  chance  to  reinitialize  pruned  connections  from  the  explored  network sub-structures.  
•  Extensive  experiments  are  conducted  across  a  wide  range  of  GANs  architectures  and  demonstrated  that 
Re-GAN  could  be  easily  applied  on  these  GANs  architectures  to  improve  their  performances,  both  in  regular and low-data regime setups. For example, for  the  identified  winning  GANs  ticket,  ProGAN  and 
StyleGAN2  on  full  CIFAR-10,  we  achieve  70.23%,  18.81%,  savings,  respectively, while improved generated sample quality  for both full and 10% training data. Re-GAN presents  a  viable  alternative  tickets  and  progressive  growing  techniques.  Additionally,  the  performance of Re-GAN is enhanced when integrated  with recent augmentation techniques.   training  FLOPs  the  GANs  and  19%  to  2.