Abstract
Convenient 4D modeling of human-object interactions is essential for numerous applications. However, monoc-ular tracking and rendering of complex interaction scenar-ios remain challenging. In this paper, we propose Instant-NVR, a neural approach for instant volumetric human-object tracking and rendering using a single RGBD cam-era. It bridges traditional non-rigid tracking with recent in-stant radiance ﬁeld techniques via a multi-thread tracking-rendering mechanism. In the tracking front-end, we adopt a robust human-object capture scheme to provide sufﬁ-cient motion priors. We further introduce a separated in-stant neural representation with a novel hybrid deforma-tion module for the interacting scene. We also provide an on-the-ﬂy reconstruction scheme of the dynamic/static ra-diance ﬁelds via efﬁcient motion-prior searching. More-over, we introduce an online key frame selection scheme and a rendering-aware reﬁnement strategy to signiﬁcantly improve the appearance details for online novel-view syn-thesis. Extensive experiments demonstrate the effective-ness and efﬁciency of our approach for the instant gen-eration of human-object radiance ﬁelds on the ﬂy, no-tably achieving real-time photo-realistic novel view synthe-sis under complex human-object interactions. Project page: https://nowheretrix.github.io/Instant-NVR/. 1.

Introduction
The accurate tracking and photo-realistic rendering for human-object interactions are critical for numerous human-centric applications like telepresence, tele-education or im-mersive experience in VR/AR. However, a convenient solu-tion from monocular input, especially for on-the-ﬂy setting, remains extremely challenging in the vision community.
Early high-end solutions [6, 9, 13, 18] require dense cameras for high-ﬁdelity reconstruction.
Recent ap-proaches [11, 12, 17, 46, 47, 59, 63] need less RGB or
RGBD video inputs (from 3 to 8 views) by using volu-*Equal Contribution.
Figure 1. Our Instant-NVR adopts a separated instant neural rep-resentation to achieve photo-realistic rendering for human-object interacting scenarios. metric tracking techniques [19, 32]. Yet, the multi-view setting is still undesirable for consumer-level daily usage.
Differently, the monocular method with a single handi-est commercial RGBD camera is more practical and at-tractive. For monocular human-object modeling, most ap-proaches [2, 15, 53, 57, 65, 66] track the rigid and skeletal motions of object and human using a pre-scanned template or parametric model. Besides, the monocular volumetric methods [32,41,43,58,64] obtain detailed geometry through depth fusion, while the recent advance [44] further extends it into the human-object setting. However, they fail to gen-erate realistic appearance results, restricted by the limited geometry resolution.
Recent neural rendering advances, represented by Neural
Radiance Fields (NeRF) [29], have recently enabled photo-realistic rendering with dense-view supervision. Notably, some recent dynamic variants of NeRF [21, 28, 50, 51, 55, 60, 67] obtain the compelling novel-view synthesis of hu-man activities even under monocular capturing. However, they rely on tedious and time-consuming per-scene training to fuse the temporal observations into the canonical space, thus unsuitable for on-the-ﬂy usage like telepresence. Only recently, Instant-NGP [30] enables fast radiance ﬁeld gener-ation in seconds, bringing the possibility for on-the-ﬂy ra-diance ﬁeld modeling. Yet, the original Instant-NGP can only handle static scenes. Few researchers explore the on-the-ﬂy neural rendering strategies for human-object interac-tions, especially for monocular setting.
In this paper, we present Instant-NVR – an instant neural volumetric rendering system for human-object interacting scenes using a single RGBD camera. As shown in Fig. 1,
Instant-NVR enables instant photo-realistic novel view syn-thesis via on-the-ﬂy generation of the radiance ﬁelds for both the rigid object and dynamic human. Our key idea is to bridge the traditional volumetric non-rigid tracking with instant radiance ﬁeld techniques. Analogous to the tracking-mapping design in SLAM, we adopt a multi-thread and tracking-rendering mechanism. The tracking front-end provides online motion estimations of both the performer and object, while the rendering back-end reconstructs the radiance ﬁelds of the interaction scene to provide instant novel view synthesis with photo-realism.
For the tracking front-end, we ﬁrst utilize off-the-shelf instant segmentation to distinguish the human and object from the input RGBD stream. Then, we adopt an efﬁcient non-rigid tracking scheme for both the performer and rigid object, where we adopt both embedded deformation [45] and SMPL [27] to model human motions. For the rendering back-end, inspired by Instant-NGP [30] we adopt a separate instant neural representation. Speciﬁcally, both the dynamic performer and static object are represented as implicit radi-ance ﬁelds with multi-scale feature hashing in the canonical space and share volumetric rendering for novel view syn-thesis. For the dynamic human, we further introduce a hy-brid deformation module to efﬁciently utilize the non-rigid motion priors. Then, we modify the training process of ra-diance ﬁelds into a key-frame based setting, so as to enable graduate and on-the-ﬂy optimization of the radiance ﬁelds within the rendering thread. For the dynamic one, we fur-ther propose to accelerate our hybrid deform module with a hierarchical and GPU-friendly strategy for motion-prior searching. Yet we observe that naively selecting key-frames with ﬁxed time intervals will cause non-evenly distribution of the captured regions of the dynamic scene.
It results in unbalanced radiance ﬁeld optimization and severe ap-pearance artifacts during free-view rendering. To that end, we propose an online key-frame selection scheme with a rendering-aware reﬁnement strategy. It jointly considers the visibility and motion distribution across the selected key-frames, achieving real-time and photo-realistic novel-view synthesis for human-object interactions.
To summarize, our main contributions include:
• We present the ﬁrst instant neural rendering system un-der human-object interactions from an RGBD sensor.
• We introduce an on-the-ﬂy reconstruction scheme for dynamic/static radiance ﬁelds using the motion priors through a tracking-rendering mechanism.
• We introduce an online key frame selection scheme and a rendering-aware reﬁnement strategy to signiﬁ-cantly improve the online novel-view synthesis. 2.