Abstract
We propose a new framework for conditional image syn-thesis from semantic layouts of any precision levels, ranging from pure text to a 2D semantic canvas with precise shapes.
More specifically, the input layout consists of one or more semantic regions with free-form text descriptions and ad-justable precision levels, which can be set based on the desired controllability. The framework naturally reduces to text-to-image (T2I) at the lowest level with no shape informa-tion, and it becomes segmentation-to-image (S2I) at the high-est level. By supporting the levels in-between, our framework is flexible in assisting users of different drawing expertise and at different stages of their creative workflow. We intro-duce several novel techniques to address the challenges com-ing with this new setup, including a pipeline for collecting training data; a precision-encoded mask pyramid and a text feature map representation to jointly encode precision level, semantics, and composition information; and a multi-scale guided diffusion model to synthesize images. To evaluate the proposed method, we collect a test dataset containing user-drawn layouts with diverse scenes and styles. Experi-mental results show that the proposed method can generate high-quality images following the layout at given precision, and compares favorably against existing methods. Project page https://zengxianyu.github.io/scenec/ 1.

Introduction
Recently, deep generative models such as StyleGAN
[24, 25] and diffusion models [9, 19, 49] have made a signif-icant breakthrough in generating high-quality images. Im-age generation and editing technologies enabled by these models have become highly appealing to artists and design-ers by helping their creative workflows. To make image generation more controllable, researchers have put a lot of effort into conditional image synthesis and introduced models using various types and levels of semantic input such as object categories, text prompts, and segmentation 1
Table 1. Difference from related conditional image synthesis works.
T2I: text to image, S2I: segmentation to image, ST2I: Scene-based text to image [13], Box2I: bounding box layout to image [50].
Setting Open-domain layout Shape control Sparse layout Coarse shape Level control
T2I
S2I
ST2I
Box2I
Ours
✓
✗
✗
✗
✓
✗
✓
✓
✗
✓
✗
✗
✗
✓
✓
✗
✗
✗
✗
✓
✗
✗
✗
✗
✓ maps etc. [23, 35, 36, 43, 44, 67].
However, existing models are not flexible enough to sup-port the full creative workflow. They mostly consider fixed-level semantics as the input [63], e.g. image-level text de-scriptions in text-to-image generation (T2I) [35, 39, 41, 43, 44], or pixel-level segmentation maps in segmentation-to-image generation (S2I) [23,36,67]. Recent breakthroughs on
T2I such as DALLE2 [39] and StableDiffusion [1,43] demon-strate extraordinary capabilities of generating high-quality results. They can convert a rough idea into visual messages to provide inspirations at the beginning of the creative pro-cess, but provide no further control over image composition.
On the other hand, S2I allows users to precisely control the image composition. As it is extremely challenging to draw a detailed layout directly, S2I is more useful for later cre-ative stages given initial designs. For real-world use cases, it is highly desirable to have a model which can generate images from not only pure text or segmentation maps, but also intermediate-level layouts with coarse shapes.
To this end, we propose a new unified conditional image synthesis framework to generate images from a semantic lay-out at any combination of precision levels. It is inspired by the typical coarse-to-fine workflow of artists and designers: they first start from an idea, which can be expressed as a text prompt or a set of concepts (Fig. 1 (a)), then tend to draw the approximate outlines and refine each object (Fig. 1 (a)-(d)). More specifically, we model a semantic layout as a set of semantic regions with free-form text descriptions. The layout can be sparse and each region can have a precision level to control how well the generated object should fit to the specified shape. The framework reduces to T2I when the layout is the coarsest (Fig. 1 (a)), and it becomes S2I when the layout is a segmentation map (Fig. 1 (d)). By adjusting the precision level, users can achieve their desired controlla-bility (Fig. 1 (a)-(d)). This framework is different from the existing works in many aspects, as summarized in Table 1.
This new setup comes with several challenges. First, it is non-trivial to encode open-domain layouts in image syn-thesis frameworks. Second, to handle hand-drawn layouts of varying precision, we need an effective and robust way to inject the precision information into the layout encod-ing. Third, there is no large-scale open-domain layout/image dataset. To generate high-quality images and generalize to novel concepts, a large and diverse training dataset is crucial.
We introduce several novel ideas to address these chal-lenges. First, we propose a text feature map representation for encoding a semantic layout. It can be seen as a spatial ex-tension of text embedding or generalization of segmentation masks from binary to continuous space. Second, we intro-duce a precision-encoded mask pyramid to model layout precision. Inspired by the classical image pyramid mod-els [2, 6, 47, 62], we relate shape precision to levels in a pyramid representation and encode precision by dropping out regions of lower precision levels. In other words, the l-th level of the mask pyramid is a sub-layout (subset of regions) consisting of semantic regions with precision level no less than l. By creating a text feature map for each sub-layout, we obtain a text feature pyramid as a unified representation of semantics, composition, and precision. Finally, we feed the text feature pyramid to a multi-scale guided diffusion model to generate images. We fulfill the need for training data by collecting them from two sources: (1) large-scale image-text pairs; (2) a relatively small pseudo layout/image dataset using text-based object detection and segmentation. With this multi-source training strategy, both text-to-image and layout-to-image can benefit from each other synergistically.
Our contributions are summarized as follows:
• A unified framework for diffusion-based image syn-thesis from semantic layouts with any combination of precision control.
• Novel ideas to build the model, including precision-encoded mask pyramid and pyramid text feature map representation, and multi-scale guided diffusion model, and training with multi-source data.
• A new real-world user-drawn layout dataset and ex-tensive experiments showing the effectiveness of our model for text-to-image and layout-to-image generation with precision control. 2.