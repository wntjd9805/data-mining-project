Abstract
Active depth sensing achieves robust depth estimation but is usually limited by the sensing range. Naively increas-ing the optical power can improve sensing range but in-duces eye-safety concerns for many applications, including autonomous robots and augmented reality. In this paper, we propose an adaptive active depth sensor that jointly opti-mizes range, power consumption, and eye-safety. The main observation is that we need not project light patterns to the entire scene but only to small regions of interest where depth is necessary for the application and passive stereo depth es-timation fails. We theoretically compare this adaptive sens-ing scheme with other sensing strategies, such as full-frame projection, line scanning, and point scanning. We show that, to achieve the same maximum sensing distance, the proposed method consumes the least power while having the shortest (best) eye-safety distance. We implement this adaptive sensing scheme with two hardware prototypes, one with a phase-only spatial light modulator (SLM) and the other with a micro-electro-mechanical (MEMS) mirror and diffractive optical elements (DOE). Experimental results validate the advantage of our method and demonstrate its capability of acquiring higher quality geometry adaptively.
Please see our project website for video results and code: https://btilmon.github.io/e3d.html. 1.

Introduction
Active 3D depth sensors have diverse applications in augmented reality, navigation, and robotics. Recently, these sensor modules are widely used in consumer products, such as time-of-flight (e.g., Lidar [15]), structured light (e.g.,
In addition, many computer
Kinect V1 [18]) and others. vision algorithms have been proposed to process the ac-quired data for downstream tasks such as 3D semantic un-derstanding [29], object tracking [17], guided upsampling in SLAM [24], etc.
*Work done during internship at Snap Research.
†Co-corresponding authors
Figure 1. Energy-efficient adaptive 3D sensing. (a) Depth sensing devices have three key optimization goals: minimizing the power consumption and eye-safety distance while maximiz-ing sensing distance. However, these goals are coupled to each other. (b) We propose a novel adaptive sensing method with an active stereo setup and a projector that can redistribute the light energy and project the pattern only to the required regions (e.g., textureless regions). (c) The proposed approach outperforms pre-vious methods including full-frame pattern systems (like Intel Re-alSense) and line-scanning systems (like Episcan3D [22]): When the maximum sensing distance is the same, the required power is much less and the eye-safety distance is also shorter.
Unlike stereo cameras that only sense reflected ambient light passively, active depth sensors illuminate the scene with modulated light patterns, either spatially, temporally, or both. The illumination encodings allow robust estima-tion of scene depths. However, this also leads to three shortcomings: First, active depth sensors consume optical power, burdening wearable devices that are on a tight power budget. Second, the number of received photons reflected back from the scene drops with inverse-square relationship
Figure 2. Method overview. Our system consists of a stereo-camera pair and an adaptive projector. The system first captures an image of the scene (Step 1) and then computes an attention map to determine the ROI (Step 2). This attention map is used to compute the control signal for the specific projector implementation (Step 3) such that light is only projected to the ROI (Step 4). A depth map is then computed from the new stereo images, which can be used for applications such as augmented reality (Step 5). to scene depth. The maximum sensing distance is thereby limited by the received signal-to-noise ratio (SNR). Third, strong, active light sources on the device may unintention-ally hurt the user or other people around. For consumer devices, this constraint can be as strict as ensuring safety when a baby accidentally stares at the light source directly.
Interestingly, these three factors are often entangled with each other. For example, naively increasing range by rais-ing optical power makes the device less eye-safe. An active 3d sensor would benefit from the joint optimization of these three goals, as illustrated in Fig. 1(a).
In this paper, we present an adaptive depth-sensing strat-egy. Our key idea is that the coded scene illumination need not be sent to the entire scene (Fig. 1(b)). Intuitively, by lim-iting the illumination samples, the optical power per sample is increased, therefore extending the maximum sensing dis-tance. This idea of adaptive sensing is supported by three observations: First, illumination samples only need to be sent to parts of a scene where passive depth estimation fails (e.g., due to lack of texture). Second, depth estimation is often application-driven, e.g., accurate depths are only needed around AR objects to be inserted into the scene. Fi-nally, for video applications, sending light to regions where depths are already available from previous frames is re-dundant. Based on these observations, we demonstrate this adaptive idea with a stereo-projector setup (i.e., active stereo [4, 9, 34]), where an attention map is computed from the camera images for efficient light redistribution.
To quantitatively understand the benefits of our ap-proach, we propose a sensor model that analytically char-acterizes various sensing strategies, including full-frame (e.g., RealSense [19]), line-scanning (e.g., Episcan3D [22]), point-scanning (e.g., Lidar [25]) and proposed adaptive sensing. We establish, for the first time, a framework that jointly analyzes the power, range, and eye-safety of differ-ent strategies and demonstrates that, for the same maximum sensing distance, adaptive sensing consumes the least power while achieving the shortest (best) eye-safety distance.
Note that the realization of scene-adaptive illumination is not trivial: Common off-the-shelf projectors simply block part of the incident light, which wastes optical power. We propose two hardware implementations for adaptive illumi-nation: One is inspired by digital holography, which uses
Liquid Crystal on Silicon (LCoS) Spatial Light Modula-tors (SLM) to achieve free-form light projection. The other implementation uses diffractive optical elements (DOE) to generate dot patterns in a local region of interest (ROI), which is directed to different portions of the scene by a micro-electro-mechanical (MEMS) mirror.
Our contributions are summarized as follows:
• Adaptive 3D sensing theory: We propose adaptive 3D sensing and demonstrate its advantage in a theo-retical framework that jointly considers range, power and eye-safety.
• Hardware implementation: We implement the pro-posed adaptive active stereo approach with two hard-ware prototypes based on SLM and MEMS + DOE.
• Experimental validation: Real-world experimental results validate that our sensor can adapt to the scene and outperform existing sensing strategies. 2.