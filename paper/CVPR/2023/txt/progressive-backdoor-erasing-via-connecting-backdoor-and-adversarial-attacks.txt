Abstract
Deep neural networks (DNNs) are known to be vulnera-ble to both backdoor attacks as well as adversarial attacks.
In the literature, these two types of attacks are commonly treated as distinct problems and solved separately, since they belong to training-time and inference-time attacks re-spectively. However, in this paper we find an intriguing con-nection between them: for a model planted with backdoors, we observe that its adversarial examples have similar be-haviors as its triggered images, i.e., both activate the same subset of DNN neurons. It indicates that planting a back-door into a model will significantly affect the model’s ad-versarial examples. Based on these observations, a novel
Progressive Backdoor Erasing (PBE) algorithm is proposed to progressively purify the infected model by leveraging un-targeted adversarial attacks. Different from previous back-door defense methods, one significant advantage of our ap-proach is that it can erase backdoor even when the clean extra dataset is unavailable. We empirically show that, against 5 state-of-the-art backdoor attacks, our PBE can effectively erase the backdoor without obvious performance degradation on clean samples and outperforms existing de-fense methods. 1.

Introduction
Deep neural networks (DNNs) have been widely adopted in many safety-critical applications (e.g., face recognition and autonomous driving), thus more attention has been paid to the security of deep learning. It has been demonstrated that DNNs are prone to potential threats in both their infer-ence as well as training phases. Inference-time attack (a.k.a. adversarial attack [5, 25]) aims to fool a trained model into making incorrect predictions with small adversarial pertur-bations. In contrast, training-time attack (a.k.a. backdoor attack [13]) attempts to plant a backdoor into a model in the training phase, so that the infected model would mis-classify the testing images as the target-label whenever a pre-defined trigger (e.g., several pixels) is embedded into them (i.e., triggered testing images). (a) For benign model. (b) For infected model.
Figure 1. Predicted labels v.s. Target-labels for 10, 000 randomly sampled adversarial examples from CIFAR-10, with respect to benign and infected models. (a) For a benign model, the predicted labels obey uniform distribution; (b) for infected models under
WaNet backdoor attack [20], its adversarial examples are highly likely to be classified as the target-label (the matrix diagonals).
Illustration of our observations. For benign models,
Figure 2. conducting an untargeted adversarial attack will make an image move close to any class (e.g., Class 0 or Class 2) in feature space.
But for infected models, adversarial attack will make it move close to the target-label class (e.g., Class l)
Due to the obvious differences between backdoor and adversarial attacks, they are often treated as two different problems and solved separately in the literature. But in this paper, we illustrate that there is an underlying connection between them, i.e., planting a backdoor into one model will significantly affect the model’s adversarial examples. More-over, based on such findings we propose a new method to defend against backdoor attacks by leveraging adversarial attack techniques (i.e., generating adversarial examples).
In particular, we observe that: for a model planted with backdoors, its adversarial examples have similar behaviors as its triggered images. This is significantly different from a benign model without backdoors. Specifically, for a benign model, the predicted class labels of its adversarial examples obey a uniform distribution, as shown in Fig.1a. However, for an infected model, we surprisingly observe that its ad-versarial examples are highly likely to be predicted as the backdoor target-label, as shown in Fig.1b. As we know, triggered images will also be predicted as the backdoor target-label by an infected model. Therefore, it means that adversarial examples have similar behaviors as its triggered images for an infected model. Particularly, these phenom-ena are present regardless of the target-label, the backdoor attack setting (i.e., all-to-one or all-to-all settings), and even for most trigger embedding mechanisms (e.g., adding [6], blending [3] or warping [20]).
To find the underlying reason of such phenomena, we measure the feature similarity of those adversarial images and triggered images. Briefly, we find that after planting a backdoor into one model, the features of adversarial images change significantly. Particularly, the features of adversar-ial image (cid:101)x′ are surprisingly very similar to that of triggered image xt, as illustrated in Fig.2 and Fig.3. It indicates that both the (cid:101)x′ and xt have similar behaviors, i.e., both ac-tivate the same subset of DNN neurons. Note that such connection between adversarial and backdoor attack could be leveraged to design backdoor defense methods.
Backdoor attacks made great advances in recent years, evolved from visible trigger [6] to invisible trigger [3, 16, 20], from poisoning label to clean-label attacks [1]. For ex-ample, WaNet [20] uses affine transformation as trigger em-bedding mechanism, which could significant improve the invisibility of trigger. In contrast, the research on backdoor defenses lag behind a little. Even for the state-of-the-art backdoor defense methods [12,14,17], most of them can be evaded by the advanced modern backdoor attacks. More-over, a clean extra dataset is often required by those defense methods to erase backdoor from infected models.
In this paper, we propose a new backdoor defense method based on the discovered connections between ad-versarial and backdoor attacks, which could not only de-fend against modern backdoor attacks but also work with-out a clean extra dataset. Specifically, at the beginning the training data (containing poisoning images) are randomly sampled to build an initial extra dataset. Next, we use them to purify the infected model by leveraging adversarial at-tack techniques. And then, the purified model is used to identify clean images from training data, which are used to update the extra dataset. With an alternating procedure, the infected model as well as the extra dataset are progressively purified. So, we call our approach Progressive Backdoor
Erasing (PBE).
Regarding how to purify the infected model, we gener-ate adversarial examples and use them to fine-tune the in-fected model. Since adversarial images could come from arbitrary class, such fine-tuning procedure works like asso-ciating triggered images to arbitrary class instead of just the target class, which breaks the foundation of backdoor at-tacks (i.e., building a strong correlation between a trigger pattern and a target-label [12]). That is why our approach can erase backdoor from infected models.
As for identifying clean images, since clean images have similar prediction results for both benign and infected mod-els, we could effectively identify them by using the previ-ously obtained purified model. Note that if a clean extra dataset is available, we can skip the step of purifying extra dataset, and only run the step of purifying model once.
A big advantage of our approach is that it does not need the clean extra dataset and it can progressively filter poi-soning training data to obtain clean data. In our approach, the purified model could help to obtain clean data, in return the obtained clean data could help to further purify model.
Thus, the alternating iterations could progressively improve each other. To the best of knowledge, our approach is the first work to defend against backdoor attack without a clean extra dataset.
Our main contributions are summarized as follows:
• We observe an underlying connection between back-door attacks and adversarial attacks, i.e., for an in-fected model, its adversarial examples have similar be-haviors as its triggered samples. And an theoretical analysis is given to justify our observation.
• According to our observations, we propose a progres-sive backdoor defense method, which achieves the state-of-the-art defensive performance, even when a clean extra dataset is unavailable. 2.