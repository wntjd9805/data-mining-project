Abstract
We present a novel paradigm for high-ﬁdelity face swap-ping that faithfully preserves the desired subtle geometry and texture details. We rethink face swapping from the per-spective of ﬁne-grained face editing, i.e., “editing for swap-ping” (E4S), and propose a framework that is based on the explicit disentanglement of the shape and texture of facial components. Following the E4S principle, our framework enables both global and local swapping of facial features, as well as controlling the amount of partial swapping spec-iﬁed by the user. Furthermore, the E4S paradigm is in-herently capable of handling facial occlusions by means of facial masks. At the core of our system lies a novel Re-gional GAN Inversion (RGI) method, which allows the ex-plicit disentanglement of shape and texture. It also allows face swapping to be performed in the latent space of Style-GAN. Speciﬁcally, we design a multi-scale mask-guided en-coder to project the texture of each facial component into regional style codes. We also design a mask-guided injec-tion module to manipulate the feature maps with the style
Work done when Zhian Liu was an intern at Tencent AI Lab
Equal contribution.
Corresponding author: nieyongwei@scut.edu.cn
†
⇤ codes. Based on the disentanglement, face swapping is re-formulated as a simpliﬁed problem of style and mask swap-ping. Extensive experiments and comparisons with current state-of-the-art methods demonstrate the superiority of our approach in preserving texture and shape details, as well as working with high resolution images. The project page is https://e4s2022.github.io 1.

Introduction
Face swapping aims at transferring the identity infor-mation (e.g., shape and texture of facial components) of a source face to a given target face, while retaining the identity-irrelevant attribute information of the target (e.g., expression, head pose, background, etc.). It has immense potential applications in the entertainment and ﬁlm produc-tion industry, and thus has drawn considerable attention in the ﬁeld of computer vision and graphics.
The ﬁrst and foremost challenge in face swapping is identity preservation, i.e., how to faithfully preserve the unique facial characteristics of the source image. Most existing methods [9, 24, 38] rely on a pre-trained 2D face recognition network [12] or a 3D morphable face model (3DMM) [7, 13] to extract the global identity-related fea-tures, which are then injected into the face generation pro-cess. However, these face models are mainly designed for classiﬁcation rather than generation, thus some informative and important visual details related to facial identity may not be captured. Furthermore, the 3D face model built from a single input image can hardly meet the requirement of ro-bust and accurate facial shape recovery. Consequently, re-sults from previous methods often exhibit the “in-between i.e., the swapped face resembles both the source effect”: and the target faces, which looks like a third person instead of faithfully preserving the source identity. A related prob-lem is skin color, where we argue that skin color is some-times an important aspect of the source identity and should be preserved, while previous methods will always maintain the skin color of the target face, resulting in unrealistic re-sults when swapping faces with distinct skin tones.
Another challenge is how to properly handle facial oc-clusion. In real applications, for example, it is a common situation that some face regions are occluded by hair in the input images. An ideal swapped result should maintain the hair from the target, meaning that the occluded part should be recovered in the source image. To handle occlusion, FS-GAN [25] designs an inpainting sub-network to estimate the missing pixels of the source, but their inpainted faces are blurry. A reﬁnement network is carefully designed in
FaceShifter [24] to maintain the occluded region in the tar-get; however, the reﬁnement network may bring back some identity information of the target.
To address the above challenges more effectively, we re-think face swapping from a new perspective of ﬁne-grained face editing, i.e., “editing for swapping” (E4S). Given that both the shape and texture of individual facial components are correlated with facial identity, we consider to disentan-gle shape and texture explicitly for better identity preserva-tion. Instead of using a face recognition model or 3DMMs to extract global identity features, inspired by ﬁne-grained face editing [23], we exploit component masks for local fea-ture extraction. With such disentanglement, face swapping can be transformed as the replacement of local shape and texture between two given faces. The locally-recomposed shapes and textures are then fed into a mask-guided gener-ator to synthesize the ﬁnal result. One additional advantage of our E4S framework is that the occlusion challenge can be naturally handled by the masks, as the current face parsing network [44] can provide the pixel-wise label of each face region. The generator can ﬁll out the missing pixels with the swapped texture features adaptively according to those labels. It requires no additional effort to design a dedicated module as in previous methods [24, 25].
The key to our E4S is the disentanglement of shape and texture of facial components. Recently, StyleGAN [18] has been applied to various image synthesis tasks due to its amazing performance on high-quality image generation, which inspires us to exploit a pre-trained StyleGAN for the disentanglement. This is an ambitious goal because current
GAN inversion methods [30, 33, 36] only focus on global attribute editing (age, gender, expression, etc.) in the global style space of StyleGAN, and provide no mechanism for local shape and texture editing.
W
W
To solve this, we propose a novel Regional GAN Inver-+ sion (RGI) method that resides in a new regional-wise r+. Speciﬁcally, we design a mask-space, denoted as guided multi-scale encoder to project an input face into the style space of StyleGAN. Each facial component has a set of style codes for different layers of the StyleGAN gener-ator. We also design a mask-guided injection module that uses the style codes to manipulate the feature maps in the generator according to the given masks.
In this way, the shape and texture of each facial component are fully disen-tangled, where the texture is represented by the style codes while the shape is by the masks. Moreover, this new in-version latent space supports the editing of each individual face component in shape and texture, enabling various ap-plications such as face beautiﬁcation, hairstyle transfer, and controlling the swapping extent of face swapping. To sum up, our contributions are:
• We tackle face swapping from a new perspective of
ﬁne-grained editing, i.e., editing for swapping, and propose a novel framework for high-ﬁdelity face swap-ping with identity preservation and occlusion handling.
• We propose a StyleGAN-based Regional GAN Inver-r+ space, sion (RGI) method that resides in a novel for the explicit disentanglement of shape and texture.
It simpliﬁes face swapping as the swapping of the cor-responding style codes and masks.
W
• The extensive experiments on face swapping, face edit-ing, and other extension tasks demonstrate the effec-tiveness of our E4S framework and RGI. 2.