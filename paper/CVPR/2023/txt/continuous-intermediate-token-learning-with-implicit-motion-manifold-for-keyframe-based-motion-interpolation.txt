Abstract
Deriving sophisticated 3D motions from sparse keyframes is a particularly challenging problem, due to continuity and exceptionally skeletal precision. The action features are often derivable accurately from the full series of keyframes, and thus, leveraging the global context with transformers has been a promising data-driven embedding approach. However, existing methods are often with inputs of interpolated intermediate frame for continuity using basic interpolation methods with keyframes, which result in a trivial local minimum during training. In this paper, we propose a novel framework to formulate latent motion manifolds with keyframe-based constraints, from which the continuous nature of intermediate token representations is considered. Particularly, our proposed framework consists of two stages for identifying a latent motion subspace, i.e., a keyframe encoding stage and an intermediate token generation stage, and a subsequent motion synthesis stage to extrapolate and compose motion data from manifolds.
Through our extensive experiments conducted on both the
LaFAN1 and CMU Mocap datasets, our proposed method demonstrates both superior interpolation accuracy and high visual similarity to ground truth motions. 1.

Introduction
Pose-to-pose keyframing is a fundamental principle of character animation, and animation processes often rely on key pose definitions to efficiently construct motions
[6, 11, 30]. In computer animation, keyframes are tempo-rally connected via interpolation algorithms, which derive intermediate pose attributes to produce smooth transitions between key poses. However, human motion is often com-plex and difficult to be effectively represented by sparse keyframe sequences alone. While this can be addressed
*Corresponding author.
Code available at: https://github.com/MiniEval/CITL
Figure 1. An example of motion interpolation by our method (first row), given the keyframes of a hopping motion (in blue), compared with the ground truth (second row). by producing denser sequences of key poses, this approach is laborious for animators, thereby increasing the cost of keyframed animation processes. Even with Motion Capture (MoCap) workflows, artists must often resort to keyfram-ing in order to clean artifacts, impose motion constraints, or introduce motion features irreplicable by motion capture performers.
Learning-based motion interpolation methods have re-cently been proposed as an acceleration of the keyframed animation process, by automatically deriving details within keyframe transitions as shown in Figure 1. Various ma-chine learning methods have been explored to enable more realistic interpolation solutions from high quality MoCap databases, e.g. by using recurrent networks [14, 15, 40] or transformer-based approaches [10, 26, 31]. Guiding data-driven interpolation with real motions is particularly attrac-tive for keyframe animation workflows, as realistic motions often require the greatest amount of keyframing, by virtue of their subtle motion details and physical constraints.
Naturally, as a sequence in-painting problem, motion interpolation can be formulated as a masked sequence-to-sequence task, which the recent popular transformer ap-proach is expected to learn effectively [4, 38, 42, 43]. How-ever, sequential learning of masked continuous attributes with transformers is largely impaired by the conventional masked tokens for intermediate data. A token is defined as an individual data element on the extendable axis of a
In cur-sequence, namely the temporal axis for motions.
rent sequence modelling formulations, a token is usually represented by a one-hot vocabulary vector to specify in-dividual words or masked elements, which poses a limita-tion on continuous attributes. Since continuous attributes can be assigned any real value, there exists no value by which a masking token can be defined without correspond-ing to an otherwise valid input. Previous approaches have employed transformer decoder-level mask tokens and lin-ear interpolation (LERP)-based tokens have been explored to work around this issue [10, 16, 31]. However, these ap-proaches have innate incompatibilities with the transformer architecture. Singular mask token representations, regard-less of their point of introduction, result in discontinuous hidden representations, which are antithetical to the evalua-tion of continuous motion data. On the other hand, the use of LERP as a pre- or post-processing step necessarily intro-duces an accurate starting estimate to the solution, which transformer models are prone to becoming over-reliant on
[24, 45]. To fully address these limitations, we propose a novel transformer-based framework that learns to model keyframe sequences into latent motion manifold represen-tations for intermediate tokens, which reflects the smooth and continuous nature of human motion.
As illustrated in Figure 2, our proposed framework incor-porates three stages with transformers to convert a keyframe sequence into a complete motion sequence: Stage-I is a keyframe encoding stage to formulate the overall motion patterns from the keyframe sequence into keyframe context tokens as a guidance for further modelling; Stage-II is an in-termediate token generation stage, where temporal indices are mapped into intermediate token representations with the keyframe context tokens, which serve as an implicit latent motion manifold constraint; and Stage-III, a motion synthe-sis stage, takes the obtained intermediate tokens by inject-ing them within the keyframe token sequence, and interpo-lating them to derive a refined motion sequence estimation.
With this framework, our transformer-based approach exhibits two key advantages over existing approaches that enable its high-quality motion interpolation: a) Manifold learning allows our framework to establish temporal con-tinuity in its latent representation space, and b) The la-tent motion manifold constrains our transformer model to concentrate its attention exclusively towards motion keyframes, as opposed to intermediate tokens derived from non-keyframe poses, such as those derived from LERP, thereby forcing a necessary alignment between the known and unknown tokens adaptively.
In addition, we identify an adverse link between con-tinuous features and normalisation methods with per-token re-centering. Specifically, layer normalisation (LayerNorm)
[1], which is commonly used in transformer architectures, constrains the biases of token features based on their in-dividual distributions. Though this is well-known to be effective with linguistic models [25, 42], continuous data inherently contain biases that should be leveraged at se-quence level. Therefore, we introduce a sequence-level re-centering (Seq-RC) technique, where positional pose at-tributes of keyframes are recentred based on their distribu-tion throughout a motion sequence, and root-mean-square normalisation (RMSNorm) [47] layers are then employed to perform magnitude-only normalisation. Though RM-SNorm was initially proposed as only a speedup to Lay-erNorm, our observations demonstrate that Seq-RC leads to superior performance in terms of accuracy and visual simi-larity to MoCap sequences.
In summary, our paperâ€™s key contributions are threefold: 1. We propose a novel transformer-based architecture consisting of three cooperative stages. It constrains the evaluation of unknown intermediate representations of continuous attributes to the guidance of keyframe con-text tokens in a learned latent manifold. 2. We devise sequence-level re-centering (Seq-RC) nor-malisation to effectively operate with real scalar at-tributes with minimal accuracy loss. 3. Extensive comparisons and ablation results obtained on LaFAN1 and CMU Mocap strongly demonstrate the superiority of our method over the state-of-the-art. 2.