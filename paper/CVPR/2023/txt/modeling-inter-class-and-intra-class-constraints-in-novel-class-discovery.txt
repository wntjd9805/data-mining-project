Abstract
Novel class discovery (NCD) aims at learning a model that transfers the common knowledge from a class-disjoint labelled dataset to another unlabelled dataset and discov-ers new classes (clusters) within it. Many methods, as well as elaborate training pipelines and appropriate objectives, have been proposed and considerably boosted performance on NCD tasks. Despite all this, we find that the existing methods do not sufficiently take advantage of the essence of the NCD setting. To this end, in this paper, we pro-pose to model both inter-class and intra-class constraints in NCD based on the symmetric Kullback-Leibler diver-gence (sKLD). Specifically, we propose an inter-class sKLD constraint to effectively exploit the disjoint relationship be-tween labelled and unlabelled classes, enforcing the sep-arability for different classes in the embedding space. In addition, we present an intra-class sKLD constraint to ex-plicitly constrain the intra-relationship between a sample and its augmentations and ensure the stability of the train-ing process at the same time. We conduct extensive exper-iments on the popular CIFAR10, CIFAR100 and ImageNet benchmarks and successfully demonstrate that our method can establish a new state of the art and can achieve signif-icant performance improvements, e.g., 3.5%/3.7% cluster-ing accuracy improvements on CIFAR100-50 dataset split under the task-aware/-agnostic evaluation protocol, over previous state-of-the-art methods. Code is available at https://github.com/FanZhichen/NCD-IIC. 1.

Introduction
Deep learning has made great progress and achieved re-markable results in many computer vision fields, especially in image classification [13, 16, 22, 25, 27]. Unfortunately, these successes of deep learning heavily rely on a large amount of fully labelled data for training. On the other hand, in many realistic scenarios, it is difficult to collect or to annotate such a large-scale dataset. To address this prob-lem, a new paradigm of novel class discovery (NCD) has
*Corresponding author been proposed and attracted increasing attention in recent years [8, 10, 11, 33, 35, 37].
The goal of NCD is to train a classification model on a labelled dataset and simultaneously transfer the latent com-mon knowledge to discover new classes (or clusters) in an-other unlabelled dataset. Different from semi-supervised learning [1, 2, 26, 36] that assumes the labelled and unla-belled datasets share the same label space, in the setting of NCD, the classes of the unlabelled dataset are disjoint with those of the labelled dataset, which is more challeng-In addition, NCD is also different from the generic ing. clustering [3, 4, 18, 31] in that an additional labelled dataset is available in NCD. In general, for the standard cluster-ing methods, the clustering results are not unique. That is to say, there may be multiple different and approximately correct results for a certain unlabelled dataset. In contrast, thanks to the available labelled dataset, NCD can eliminate the semantic ambiguity with the label guidance and finally makes the clustering be consistent with the real visual se-mantics [11]. Clearly, NCD is more realistic and more prac-tical than unsupervised clustering.
In general, the existing NCD methods can be roughly divided into two categories, i.e., two-stage based methods and single-stage based methods [20]. Most of the early methods are two-stage by using labelled and unlabelled data in different stages, such as KCL [14], MCL [15] and
DTC [11]. Typically, the two-stage based methods first learn an embedding network on the labelled set through su-pervised learning and then use it on the unlabelled set to discover new clusters with little modifications. In contrast, the latest methods are almost single-stage, such as, RS [10],
NCL [37], UNO [8], DualRank [35] and ComEx [33], which use both labelled and unlabelled data in a single stage at the same time. The single-stage based methods can learn the feature representation and discover novel classes simultaneously, iteratively updating the learned feature em-bedding network and clustering results during the train-ing process. Compared with the two-stage based meth-ods, the single-stage methods can make more effective use of the similarity between labelled and unlabelled classes to achieve a better knowledge transfer between these two datasets. Therefore, in this paper, we will mainly focus on
the single-stage direction of NCD.
However, we find that the current single-stage based
NCD methods do not sufficiently take advantage of the essence of the NCD setting, that is to say, overlooking the disjoint characteristic between the labelled and unlabelled classes. In this sense, on one hand, the labelled and unla-belled samples cannot be effectively separated, weakening the discriminability of the learned features. On the other hand, because the labelled data is learned under supervision while the unlabelled data has no supervision, i.e., an im-balanced learning process (learning with different supervi-sion strengths), it will make the learned feature representa-tions biased toward the labelled data. In addition, we notice that although some methods [10, 35] have used data aug-mentation to generate additional samples and gained sig-nificant performance improvements, they generally employ the mean squared error (MSE) as the consistency regular-ization, which cannot constrain the consistency well with a good generalization ability.
To address the above two issues, we propose to model both Inter-class and Intra-class Constraints (IIC for short) built on the symmetric Kullback-Leibler divergence (sKLD) for discovering the novel classes. To be specific, an inter-class sKLD constraint is proposed to explicitly learn to sep-arate different classes between labelled and unlabelled data, enhancing the discriminability of learned feature represen-tations. Moreover, an intra-class sKLD constraint is pre-sented to fully learn the intra-relationship between sam-ples and their augmentations. According to our experi-ments, such an intra-class sKLD constraint can also stable the training process in the training phase. We have con-ducted extensive experiments on three benchmarks, includ-ing CIFAR10 [21], CIFAR100 [21] and ImageNet [7], and show that the proposed two constraints can significantly and consistently outperform the existing novel class discovery methods by a large margin.
To summarize, our contributions are as follows:
• We propose a new inter-class Kullback-Leibler diver-gence constraint to sufficiently model the relationship between the labelled and unlabelled datasets to learn more discriminative feature representations, which is somewhat overlooked in the literature.
• We propose a new intra-class Kullback-Leibler diver-gence constraint to effectively exploit the relationship between a sample and its different transformations to learn invariant feature representations.
• We evaluate the proposed constraints on three bench-mark datasets for novel class discovery and obtain sig-nificant performance improvements over the state-of-the-art methods, which successfully demonstrates the effectiveness of the proposed method. 2.