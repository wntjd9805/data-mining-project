Abstract
Metric learning requires the identification of far-apart similar pairs and close dissimilar pairs during training, and this is difficult to achieve with unlabeled data because pairs are typically assumed to be similar if they are close. We present a novel metric learning method which circumvents this issue by identifying hard negative pairs as those which obtain dissimilar labels via label propagation (LP), when the edge linking the pair of data is removed in the affinity matrix.
In so doing, the negative pairs can be identified despite their proximity, and we are able to utilize this infor-mation to significantly improve LPâ€™s ability to identify far-apart positive pairs and close negative pairs. This results in a considerable improvement in semi-supervised metric learning performance as evidenced by recall, precision and
Normalized Mutual Information (NMI) performance met-rics on Content-based Information Retrieval (CBIR) appli-cations. 1.

Introduction
Image data have proliferated owing to ubiquitous cam-era ownership, widespread internet connectivity, and broad availability of software applications. The design of efficient
CBIR systems is imperative in managing information con-tent. Traditional image retrieval systems use textual annota-tions, but such an approach require manual labor which may be not be cost-effective. On the other hand, CBIR retrieves relevant content from an image query, alleviating the need for human annotation.
Metric learning learns a transformation mapping similar data closer than dissimilar ones. This makes it easier to cluster data [37] and also to shortlist similar data by their proximity to the query. As such, metric learning algorithms are at the cornerstone of most state-of-the-art CBIR designs
[23]. As the amount of data handled in CBIR systems is very large, there is interest in utilizing the vast quantities
* Work done while a student at UIUC. of readily available unlabeled data to improve the metric learning training process.
Recent advances have asserted the importance of retain-ing within-class variance in training generalizable represen-tations [27], particularly because the information encapsu-lated in representations which are irrelevant in discriminat-ing between training labels may be important in discrim-inating between unseen test classes. Since close positive pairs results in little loss, there is a need to identify far-apart positive pairs for training. The paper [43] uses the method from [9] to propagate labels along manifolds in order to identify such points, by assuming data which are nearest neighbor pairs but not mutual nearest neighbors as close dissimilar pairs. However this method does not uti-lize label information and may not accurately identify hard negatives [3, 17, 39, 40], which are pairs of data which are close but have different labels. Label propagation (LP) us-ing mis-identified edges would lead to inaccuracies in the model trained.
This paper proposes a novel method to identify hard neg-ative pairs in a semi-supervised setting. Specifically, given a few labeled points from each class and an abundance of un-labeled data, we propose to identify hard negative pairs as those which obtain dissimilar labels via LP when the edge linking the two elements of the pair is removed in the affin-ity matrix. We obtain the dissimilarity weights of edges under this assumption quickly and efficiently, without the costly use of repeated LP to calculate these weights.
We use this negative edge information in a novel mixed
LP algorithm which is able to utilize both positive and nega-tive edge information. Specifically, the method encourages data linked by positive edges to have the same pseudolabel and data linked by negative edges to have different pseu-dolabels. Like LP, our mixed LP optimization problem can be solved with conjugate gradient (CG), allowing it to scale to large datasets.
As our obtained pseudolabels capture information on far-apart positive pairs and close negative pairs, they yield a significant improvement in semi-supervised metric learn-ing. We showcase this in a CBIR setting under recall, pre-cision and Normalized Mutual Information (NMI) perfor-mance metrics. This shows that our method is able to more effectively rank the database in relation to the query and re-turn relevant articles near the top of this list. 2.