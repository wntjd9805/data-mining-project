Abstract
Understanding dense action in videos is a fundamen-tal challenge towards the generalization of vision models.
Several works show that compositionality is key to achiev-ing generalization by combining known primitive elements, especially for handling novel composited structures. Com-positional temporal grounding is the task of localizing dense action by using known words combined in novel ways in the form of novel query sentences for the actual grounding. In recent works, composition is assumed to be learned from pairs of whole videos and language embeddings through large scale self-supervised pre-training. Alternatively, one can process the video and language into word-level prim-itive elements, and then only learn fine-grained semantic correspondences. Both approaches do not consider the gran-ularity of the compositions, where different query granularity corresponds to different video segments. Therefore, a good compositional representation should be sensitive to different video and query granularity. We propose a method to learn a coarse-to-fine compositional representation by decomposing the original query sentence into different granular levels, and then learning the correct correspondences between the video and recombined queries through a contrastive rank-ing constraint. Additionally, we run temporal boundary prediction in a coarse-to-fine manner for precise ground-ing boundary detection. Experiments are performed on two datasets, Charades-CG and ActivityNet-CG, showing the superior compositional generalizability of our approach. 1.

Introduction
Over the last years, robust results have been shown for the detection of predefined, simpler action classes in video
[11, 38, 41, 48]. On the other hand, the detection of dense ac-tion, e.g. action contents described by a rich description, still poses a significant challenge due to the large diversity of pos-sible language descriptions and semantic associations [16].
* Work done while Lijin Yang was an Intern at Woven by Toyota.
Recent work in this area treats complex actions as monolithic events via end-to-end predictions [27, 44]. They produce a single label to densely describe a long video sequence, and they are difficult to scale up to more varied patterns.
Fundamentally, most complex actions consist of a series of simpler events, and thus a dense action can be treated as a composition of known event primitives [13, 25, 26]. Such a compositional representation can allow for a higher degree of model generalization. By learning from a finite number of action composites, and recombining their constituent event primitives in novel ways for unseen action descriptions, the representation can expand to large numbers of novel sce-narios that have not been observed in the original action space. In this case, the action description is not treated as a single label but as a language modality that allows to learn finer-grained video and language correspondence.
The task of temporal video grounding concentrates on the described setting. Given a video and an action-related query sentence, the model has to output the start and end times of the specific moment that semantically corresponds to the given query sentence.
On top of temporal grounding, Compositional Temporal
Grounding is a new task for testing whether the model can generalize to sentences that contain novel compositions of seen words. Several recent methods [12, 47, 50] encode both sentence and video segments into unstructured global repre-sentations and then devise specific cross-modal interaction modules to fuse them for final prediction. Unfortunately, such global representations fail to explicitly model video structure and language compositions, and fail for cases where higher granularity is required. Similar to the above approach of correspondence learning via global information, compos-ite representations learn from paired monolithic video and language respectively, through large-scale self-supervised pre-training [32]. In either case, the main challenge of utiliz-ing datasets of limited action size to achieve generalization towards novel actions remains challenging due to its combi-natorial complexity.
Differently to the global representation approaches,
VISA [17] introduces a compositional method toward the
Figure 1. Our proposed method of decomposition and reconstruction of masked words with a coarse-to-fine scheme. temporal grounding task. They parse video and language into several primitive elements, and then learn fine-grained semantic correspondences. Specifically, the composite rep-resentation is extracted from a graph that is structured with primitive elements, and these elements serve as a common representation for vision-language correspondence learning.
However, both the global and compositional approaches dis-regard the granularity of the action composition during the learning phase, and can have issues when generalizing to more varied action spaces.
Based on these considerations, we argue that a good com-posite representation should be sensitive to different levels of granularity of both the action and the query language.
We provide an overview of our idea in Fig. 1. Concretely, we propose to learn a composite representation in a coarse-to-fine manner, where we first decompose the whole query sentence into several simple phrases as subsentences, and then learn correspondences not only globally across query and video, but also between the subsentences and their re-lated primitive sequences from the video. Since there is no ground truth to relate subsentences and their corresponding sequences, we propose to learn correspondence by decom-posing and reconstructing them under weak supervision. For each subsentence as an anchor sample, we generate a tem-poral proposal and learn the positive representation through the mining of negative samples.
To structure the overall weak supervision, we mask the words in a given anchor query, and use the embedding of the positive and negative query with the related pseudo temporal segments from the video respectively to do the reconstruc-tion for the masked words. The negative sample is another subsentence but includes the words from the other query action description, to allow the model sensitive to the word level variation for composition. According to the fact that the negative subsentence contains the novel word compared to the positive anchor sub-sentence, we could rank the recon-struction quality according to the given query sub-sentence as a natural prior constraint during the training.
Additionally, the ground truth that links the temporal boundary to the global sentence provides supervision for the coarse compositional representation. The generated subsen-tences from the same global query are recombined into a new sentence that should be as informative as the original global query, and then this recombined sentence is used with the original query sentence to estimate the temporal bound-ary via supervised learning. The whole process is trained end-to-end.
Our contributions are summarized below: (i). We argue that a good compositional representation should be sensitive to action granularity of video and query language, and propose a coarse-to-fine decomposition ap-proach to this end. (ii). To learn a compositional representation without composite-level labels, we decompose events from global queries and learn primitive-level correspondences between video and language with a weakly-supervised contrastive ranking in form of word-masked reconstruction. (iii). The decomposed events are recombined to a novel query along with the original query for temporal boundary estimation supervised, jointly learned end-to-end way with a coarse-to-fine structure. (iv). Experiments on Charades-CG and ActivityNet-CG [17] demonstrate our method significantly outperforms existing methods about compositional temporal grounding tasks. 2.