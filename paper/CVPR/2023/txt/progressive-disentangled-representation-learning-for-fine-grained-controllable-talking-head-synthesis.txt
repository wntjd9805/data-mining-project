Abstract
We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them. To effectively disen-tangle each motion factor, we propose a progressive disen-tangled representation learning strategy by separating the factors in a coarse-to-fine manner, where we first extract unified motion feature from the driving signal, and then iso-late each fine-grained motion from the unified feature. We leverage motion-specific contrastive learning and regress-ing for non-emotional motions, and introduce feature-level decorrelation and self-reconstruction for emotional expres-sion, to fully utilize the inherent properties of each motion factor in unstructured video data to achieve disentangle-ment. Experiments show that our method provides high quality speech&lip-motion synchronization along with pre-cise and disentangled control over multiple extra facial mo-tions, which can hardly be achieved by previous methods. 1.

Introduction
Talking head synthesis is an indispensable task for cre-ating realistic video avatars and enables multiple applica-tions such as visual dubbing, interactive live streaming, and online meeting.
In recent years, researchers have made great progress in one-shot generation of vivid talking heads by leveraging deep learning techniques. Corresponding methods can be mainly divided into audio-driven talking
head synthesis and video-driven face reenactment. Audio-driven methods focus more on accurate lip motion syn-thesis from audio signals [9, 44, 52, 54]. Video-driven ap-proaches [50,59] aim to faithfully transfer all facial motions in the source video to target identities and usually treat these motions as a unity without individual control.
We argue that a fine-grained and disentangled control over multiple facial motions is the key to achieving lifelike talking heads, where we can separately control lip motion, head pose, eye motion, and expression, given correspond-ing respective driving signals. This is not only meaningful from the research aspect which is often known as the dis-entangled representation learning but also has a great im-pact on practical applications. Imagining in a real scenario, where we would like to modify the eye gaze of an already synthesized talking head, it could be costly if we cannot solely change it but instead ask an actor to perform a com-pletely new driving motion. Nevertheless, controlling all these factors in a disentangled manner is very challenging.
For example, lip motions are highly tangled with emotions by nature, whereas the mouth movement of the same speech can be different under different emotions. There are also insufficient annotated data for large-scale supervised learn-ing to disentangle all these factors. As a result, existing methods either cannot modify certain factors such as eye gaze or expression [75, 76], or can only change them alto-gether [26,32], or have difficulties providing precise control over individual factors [26].
In this paper, we propose Progressive Disentangled
Fine-Grained Controllable Talking Head (PD-FGC) for one-shot talking head generation with disentangled con-trol over lip motion, head pose, eye gaze&blink, and emo-tional expression1, where the control signal of lip motion comes from audios, and all other motions can be individu-ally driven by different videos. To this end, our intuition is to learn disentangled latent representation for each motion factor, and leverage an image generator to synthesize talk-ing heads taking these latent representations as input. How-ever, it is very challenging to disentangle all these factors given only in-the-wild video data for training. Therefore, we propose to fully utilize the inherent properties of each motion within the video data with little help of existing prior models. We design a progressive disentangled representa-tion learning strategy to separate each factor control in a coarse-to-fine manner based on their individual properties.
It consists of three stages: 1) Appearance and Motion Disentanglement. We first learn appearance and motion disentanglement via data aug-mentation and self-driving [6,75] to obtain a unified motion feature that records all motions of the driving frame mean-while excludes appearance information. It serves as a strong 1We define the emotional expression as the facial expression that ex-cludes speech-related mouth movement and eye gaze&blink. starting point for further fine-grained disentanglement. 2) Fine-Grained Motion Disentanglement. Given the unified motion feature, we learn individual motion repre-sentation for lip motion, eye gaze&blink, and head pose, via a carefully designed motion-specific contrastive learn-ing scheme as well as the guidance of a 3D pose estima-tor [14].
Intuitively, speech-only lip motion can be well separated via learning shared information between the uni-fied motion feature and the corresponding audio signal [75]; eye motions can be disentangled by region-level contrastive learning that focuses on eye region only, and head pose can be well defined by 3D rigid transformation. 3) Expression Disentanglement. Finally, we turn to the challenging expression separation as the emotional expres-sion is often highly tangled with other motions such as mouth movement. We achieve expression disentanglement via decorrelating it with other motion factors on a feature level, which we find works incredibly well. An image gen-erator is simultaneously learned for self-reconstruction of the driving signals to learn the semantically-meaningful ex-pression representation in a complementary manner.
In summary, our contributions are as follows: 1) We pro-pose a novel one-shot and fine-grained controllable talk-ing head synthesis method that disentangles appearance, lip motion, head pose, eye blink&gaze, and emotional ex-pression, by leveraging a carefully designed progressive disentangled representation learning strategy. 2) Motion-specific contrastive learning and feature-level decorrelation are leveraged to achieve desired factor disentanglement. 3)
Trained on unstructured video data with limited guidance from prior models, our method can precisely control di-verse facial motions given different driving signals, which can hardly be achieved by previous methods. 2.