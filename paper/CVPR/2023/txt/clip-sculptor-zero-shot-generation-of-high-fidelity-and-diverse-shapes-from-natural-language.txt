Abstract
Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and di-versity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for im-proved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a trans-former conditioned on CLIP’s image-text embedding space.
We also present a novel variant of classifier-free guid-ance, which improves the accuracy-diversity trade-off. Fi-nally, we perform extensive experiments demonstrating that
CLIP-Sculptor outperforms state-of-the-art baselines. 1.

Introduction
In recent years, there has been rapid progress in text-conditioned image generation [31, 32, 35], which has been driven by advances in multimodal understanding learned from web-scaled paired (text, image) data. These advances have led to applications in domains ranging from content creation [21, 22, 31] to human–robot interaction [40]. Un-fortunately, developing the analogue of a text-conditioned 3D shape generator is challenging because it is difficult to obtain (text, 3D shape) pairs at large scale. Prior work has attempted to address this problem by collecting text-shape paired data [2, 5, 9, 23, 26], but these approaches have been limited to a small number of object categories.
A promising way around this data bottleneck is to use weak supervision from large-scale vision/language mod-els such as CLIP [30]. One approach is to directly op-timize a 3D representation such that (text, image render) pairs are aligned when projected into the CLIP embedding space. Prior work has applied this approach to stylize 3D meshes [25, 41] and to create abstract “dreamlike” objects using neural radiance fields [18] or meshes [19]. However, neither of the aforementioned methods produce realistic ob-ject geometry, and they can require expensive optimization.
Another approach, more in line with text-to-image gener-ators [31, 32], is to train a conditional generative model.
The CLIP-Forge system [37] builds such a model without paired (text, shape) data by using rendered images of shapes at training time and leveraging the CLIP embedding space to bridge the modalities of image and text at inference time.
CLIP-Forge demonstrates compelling zero-shot generation
Table 1. High-level comparison between zero-shot text-to-shape generation methods. Inference time is calculated using a single
NVIDIA Tesla V100 GPU. from hierarchical VQ-VAEs [8, 33] and propose an archi-tecture of hierarchical discrete representations capable of generating high fidelity 3D shapes.
Method
Inference
Fidelity Diversity
DreamFields [18] > 24 hrs 30 min
Clip-Mesh [19] 6.41 ms
Clip-Forge [37] 0.91 sec
CLIP-Sculptor
Low
Low
Single
Single
Medium Medium
High
High abilities but produces low-fidelity shapes which do not cap-ture the full diversity of shapes found in the training distri-bution.
In this paper, we propose CLIP-Sculptor, a text-conditioned 3D shape generative model that outperforms the state of the art by improving shape diversity and fi-delity with only (image, shape) pairs as supervision. CLIP-Sculptor’s novelty lies in its multi-resolution, voxel-based conditional generation scheme. Without (text, shape) pairs,
CLIP-Sculptor learns to generate 3D shapes of common object categories from the text-image joint embedding of CLIP. To achieve high fidelity outputs, CLIP-Sculptor adopts a multi-resolution approach: it first generates a low-resolution latent grid representation that captures the se-mantics from text/image, then upscales it to a higher res-olution latent grid representation with a super-resolution model, and finally decodes the output geometry. To gen-erate diverse shapes, CLIP-Sculptor adopts discrete latent representations which are obtained using a vector quanti-zation scheme that avoids posterior collapse. To further improve shape fidelity and diversity, CLIP-Sculptor uses a masked transformer architecture. We additionally propose a novel annealed strategy for the classifier-free guidance [16].
To sum up, we make the following contributions:
• CLIP-Sculptor, a multi-resolution text-conditional shape generative model that achieves both high fidelity and diversity without the need for (text, shape) pairs.
• A novel variant of classifier-free guidance for gener-ative models, with an annealed guidance schedule to achieve better quality for a given diversity level. 2.