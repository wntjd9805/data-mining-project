Abstract
Due to the difficulty in collecting large-scale and per-fectly aligned paired training data for Under-Display Cam-era (UDC) image restoration, previous methods resort to monitor-based image systems or simulation-based methods, sacrificing the realness of the data and introducing domain gaps. In this work, we revisit the classic stereo setup for training data collection – capturing two images of the same scene with one UDC and one standard camera. The key idea is to “copy” details from a high-quality reference im-age and “paste” them on the UDC image. While being able to generate real training pairs, this setting is suscep-tible to spatial misalignment due to perspective and depth of field changes. The problem is further compounded by the large domain discrepancy between the UDC and normal images, which is unique to UDC restoration. In this paper, we mitigate the non-trivial domain discrepancy and spatial misalignment through a novel Transformer-based frame-work that generates well-aligned yet high-quality target data for the corresponding UDC input. This is made possi-ble through two carefully designed components, namely, the
Domain Alignment Module (DAM) and Geometric Align-ment Module (GAM), which encourage robust and accurate discovery of correspondence between the UDC and nor-mal views. Extensive experiments show that high-quality and well-aligned pseudo UDC training pairs are benefi-cial for training a robust restoration network. Code and the dataset are available at https://github.com/ jnjaby/AlignFormer. 1.

Introduction
Under-Display Camera (UDC) is an imaging system with cameras placed underneath a display.
It emerges as a promising solution for smartphone manufacturers to com-pletely hide the selfie camera, providing a notch-free view-ing experience on smartphones. However, the widespread
Figure 1. Domain and geometric misalignment in UDC. Stereo pairs (a) and (b) are captured by Under-Display Camera and high-end camera, respectively. The two images deviate significantly due to the color shift and severe degradation the UDC image.
Anaglyph (c) illustrates the large spatial displacement between
UDC and reference images despite a careful hardware setup and rough alignment. Our AlignFormer aligns the image pair and min-imizes the parallax. commercial production of UDC is prevented by poor imag-ing quality caused by diffraction artifacts. Such artifacts are unique to UDC, caused by the gaps between display pixels that act as an aperture. As shown in Figure 1(a), typical diffraction artifacts entail flare, saturated blobs, blur, haze, and noise. The complex and diverse distortions make the reconstruction problem extremely challenging.
Training a deep network end-to-end for UDC restora-tion has been found challenging due to the need for a large-scale dataset of real-world degraded images and their high-quality counterparts. Existing methods [29, 52] build datasets with a monitor-based imaging system. As dis-cussed in Feng et al. [3], such a paradigm is inherently lim-ited by the dynamic range and spatial resolution of the mon-itor. To address the problem, Feng et al. [3] present a syn-thetic dataset grounded on the imaging formation model [3].
Both datasets exhibit degradation that deviates from the ac-tual physical imaging process, leading to poor generaliz-ability to diverse real-world test cases.
To circumvent the hurdle in collecting real paired data, we opt for an alternative setup, i.e., to construct paired dataset with a stereo setting. Specifically, we capture two images of the same scene with one Under-Display Cam-era and one normal camera, denoted as UDC and Reference image, respectively. An example is shown in Figure 1(a-b)
The key challenge lies in two aspects. i) Domain discrep-ancy. The different camera configurations inevitably give rise to variations in illuminance and severe color inconsis-tency, especially under the presence of color shift and severe diffraction artifacts in the UDC image. ii) Geometric mis-alignment. The contents in the UDC image and reference image are misaligned due to different focal lengths and field of views (FOV).
Due to the unique nature of UDC restoration, existing solutions are not effective in addressing the two aforemen-tioned challenges. In particular, the low-level vision com-munity has made attempts on this stereo setup for super-resolution [1], deblurring [32], and learnable ISP [9].
In addition, Contextual loss [25] and CoBi loss [48] are de-vised to alleviate mild spatial misalignment. As shown in our experiments, those methods are less stable and robust due to the difficulty of reliable matching when one image is severely distorted. In particular, the over-exposed regions caused by diffraction require strong pixel-wise supervision to enforce constraints during the training.
The key idea of our solution is to generate high-quality and well-aligned pseudo pairs from the non-aligned stereo data (UDC and reference) to enable end-to-end training of a deep network. The challenge lies in solving the domain and spatial misalignment so that the process resembles ‘copy-ing’ details from the reference image selectively and then
‘pasting’ on the degraded image. To this end, we devise a simple yet effective Transformer-based framework, namely
AlignFormer, with a Domain Alignment Module (DAM) and a Geometric Alignment Module (GAM). The DAM is inspired by AdaIN [7], aiming to mitigate the domain dis-crepancy between the UDC and reference images, allowing more robust and accurate correspondence matching in the subsequent stage. The GAM establishes accurate dense cor-respondences through incorporating geometric cues in at-tention. Specifically, GAM can flexibly work with any off-the-shelf pre-trained optical flow estimators to build pixel-wise correspondence between the UDC and reference im-ages. The discovered correspondence then guides the sparse attention in our Transformer to search for the matching pix-els accurately and effectively within local regions.
Figure 1(d-e) show that AlignFormer produces well-aligned image pairs. The results of AlignFormer can serve as pseudo ground-truth data and one can easily train an im-age restoration network end-to-end with common training settings, i.e., using pixel losses such as L1 that assume exact spatial alignment, the perceptual loss [14], and the adversarial loss. Moreover, the constructed pseudo-paired dataset allows us to enjoy the merits of any advanced ar-chitectures of neural networks designed for image restora-tion problems. The generated data do not suffer from the limited dynamic range of spatial resolution as in previous monitor-based imaging systems. The data also experience a far lower domain gap than simulation-based approaches.
The main contributions are three-fold:
• We propose a data generation framework that is specif-ically designed for UDC. It presents a promising direc-tion beyond previous monitor-based and simulation-based data collection approaches, leading to improved generalizability of UDC image restoration.
• Our AlignFormer properly integrates optical flow guidance into up-to-date Transformer architectures.
• Experimental results demonstrate significant progress in practical UDC image restoration in real-world sce-narios. 2.