Abstract
Contrastive learning has recently demonstrated great potential for unsupervised pre-training in 3D scene un-derstanding tasks. However, most existing work ran-domly selects point features as anchors while building con-trast, leading to a clear bias toward background points that often dominate in 3D scenes. Also, object aware-ness and foreground-to-background discrimination are ne-glected, making contrastive learning less effective.
To tackle these issues, we propose a general foreground-aware feature contrast (FAC) framework to learn more effective point cloud representations in pre-training. FAC consists of two novel contrast designs to construct more effective and informative contrast pairs. The first is building positive pairs within the same foreground segment where points tend to have the same semantics. The second is that we prevent over-discrimination between 3D segments/objects and en-courage foreground-to-background distinctions at the seg-ment level with adaptive feature learning in a Siamese cor-respondence network, which adaptively learns feature cor-relations within and across point cloud views effectively.
Visualization with point activation maps shows that our contrast pairs capture clear correspondences among fore-ground regions during pre-training. Quantitative exper-iments also show that FAC achieves superior knowledge transfer and data efficiency in various downstream 3D se-mantic segmentation and object detection tasks. All codes, data, and models are available. 1.

Introduction 3D scene understanding is crucial to many tasks such as robot grasping and autonomous navigation [12, 21, 30].
However, most existing work is fully supervised which re-lies heavily on large-scale annotated 3D data that is often laborious to collect. Self-supervised learning (SSL), which allows learning rich and meaningful representations from large-scale unannotated data, has recently demonstrated great potential to mitigate the annotation constraint [1, 5].
It learns with auxiliary supervision signals derived from unannotated data, which are usually much easier to col-â€  Corresponding Authors.
Figure 1. Constructing informative contrast pairs matters in con-trastive learning: Conventional contrast requires strict point-level correspondence. The proposed method FAC takes both fore-ground grouping and foreground-background distinction cues into account, thus forming better contrast pairs to learn more informa-tive and discriminative 3D feature representations. lect. In particular, contrastive learning as one prevalent SSL approach has achieved great success in various visual 2D recognition tasks [6, 29].
Contrastive learning has also been explored for point cloud representation learning in various downstream tasks such as semantic segmentation [7, 18, 22, 42], instance seg-mentation [19, 20], and object detection [26, 44]. However, many successful 2D contrastive learning methods [6,14,46] do not work well for 3D point clouds, largely because point clouds often capture wide-view scenes which con-sist of complex points of many irregularly distributed fore-ground objects as well as a large number of background points. Several studies attempt to design specific contrast to cater to the geometry and distribution of point clouds.
For example, [22] employ max-pooled features of two aug-mented scenes to form the contrast, but they tend to over-emphasize holistic information and overlook informative
features about foreground objects. [19, 26, 42] directly use registered point/voxel features as positive pairs and treat all non-registered as negative pairs, causing many false con-trast pairs in semantics.
We propose exploiting scene foreground evidence and foreground-background distinction to construct more fore-ground grouping aware and foreground-background distinc-tion aware contrast for learning discriminative 3D represen-tations. For foreground grouping aware contrast, we first obtain regional correspondences with over-segmentation and then build positive pairs with points of the same re-leading to semantic coherent repre-gion across views, sentations.
In addition, we design a sampling strategy to sample more foreground point features while building contrast, because the background point features are often less-informative and have repetitive or homogeneous pat-terns. For foreground-background contrast, we first en-hance foreground-background point feature distinction, and then design a Siamese correspondence network that se-lects correlated features by adaptively learning affinities among feature pairs within and across views in both fore-ground and background to avoid over-discrimination be-tween parts/objects. Visualizations show that foreground-enhanced contrast guides the learning toward foreground re-gions while foreground-background contrast enhances dis-tinctions among foreground and background features effec-tively in a complementary manner, the two collaborating to learn more informative and discriminative representation as illustrated in Fig. 1.
The contributions of this work can be summarized in three aspects. First, we propose FAC, a foreground-aware feature contrast framework for large-scale 3D pre-training.
Second, we construct region-level contrast to enhance the local coherence and better foreground awareness in the learned representations. Third, on top of that, we de-sign a Siamese correspondence framework that can lo-cate well-matched keys to adaptively enhance the intra-and inter-view feature correlations, as well as enhance the foreground-background distinction. Lastly, extensive ex-periments over multiple public benchmarks show that FAC achieves superior self-supervised learning when compared with the state-of-the-art. FAC is compatible with the preva-lent 3D segmentation backbone network SparseConv [15] and 3D detection backbone networks including PV-RCNN,
PointPillars [25], and PointRCNN [36]. It is also applica-ble to both indoor dense RGB-D and outdoor sparse LiDAR point clouds. 2.