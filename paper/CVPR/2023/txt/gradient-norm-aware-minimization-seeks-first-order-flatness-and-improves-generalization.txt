Abstract
Recently, flat minima are proven to be effective for im-proving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the cur-rent definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate min-ima with low generalization error from those with high gen-eralization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flat-ness focusing on the maximal gradient norm within a per-turbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named
Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Ex-perimental results show that GAM improves the generaliza-tion of models trained with current optimizers such as SGD and AdamW on various datasets and networks. Further-more, we show that GAM can help SAM find flatter minima and achieve better generalization. The code is available at https://github.com/xxgege/GAM . 1.

Introduction
Current neural networks have achieved promising results in a wide range of fields [36, 53, 55, 67, 73–75, 78], yet they are typically heavily over-parameterized [2, 3]. Such heavy overparameterization leads to severe overfitting and poor generalization to unseen data when the model is learned simply with common loss functions (e.g., cross-entropy)
[27]. Thus effective training algorithms are required to limit the negative effects of overfitting training data and find gen-†Equal contribution, *Corresponding author (a) (b)
Figure 1. The comparison of the zeroth-order flatness (ZOF) and first-order flatness (FOF). Given a perturbation radius ρ, ZOF can fail to indicate generalization error both when there are multi-ple minima (1a) and a single minimum (1b) in the radius while
FOF remains discriminative. The height of blue rectangles in curly brackets is the value of ZOF and the height of gray trian-gles (which indicates the slope) is the value of FOF. In Figure 1a, when ρ is large and enough to cover multiple minima, ZOF can not measure the fluctuation frequency while FOF prefers the flat-ter valley which has a smaller gradient norm. When ρ is small and covers only a single minimum, the maximum loss in ρ can be misleading as it can be misaligned with the uptrend of loss. As shown in Figure 1b, ZOF prefers the valley on the right, which has a larger generalization error (the orange dotted line), while FOF prefers the left one. eralizable solutions.
Many studies try to improve model generalization by modifying the training procedure, such as batch normal-ization [26], dropout [24], and data augmentation [13, 68, 72]. Especially, some works discuss the connection be-tween the geometry of the loss landscape and general-ization [19, 22, 27]. A branch of effective approaches, sharpness-Aware Minimization (SAM) [19] and its variants
[16, 17, 34, 43, 48, 77], minimizes the worst-case loss within a perturbation radius, which we call zeroth-order flatness. It is proven that optimizing the zeroth-order flatness leads to lower generalization error and achieves state-of-the-art per-formance on various image classification tasks [19, 39, 80].
Optimizing the worst case, however, relies on a reason-able choice of perturbation radius ρ. As a prefixed hyper-parameter in SAM or a hyperparameter under parameter re-scaling in its variants, such as ASAM [39], ρ can not al-ways be a perfect choice in the whole training process. We show that the zeroth-order flatness may fail to indicate the generalization error with a given ρ. As in Figure 1a, when
ρ covers multiple minima, the zeroth-order flatness (SAM) can not measure the fluctuation frequency. When there is a single minimum within ρ, as in Figure 1b the observation radius is limited and the maximum loss in ρ can be mis-aligned with the uptrend of loss. So zeroth-order flatness can be misleading and the knowledge of loss gradient is re-quired for generalization error minimization.
To address this problem, we introduce first-order flat-ness, which controls the maximum gradient norm in the neighborhood of minima. We show that the first-order flat-ness is stronger than the zeroth-order flatness as the loss intensity of the loss fluctuation can be bounded by the max-imum gradient. When the perturbation radius covers mul-tiple minima, which we show is quite common in prac-tice, the first-order flatness discriminates more drastic jit-ters from real flat valleys, as in Figure 1a. When the per-turbation radius is small and covers only one minimum, the first-order flatness demonstrates the trend of loss gradient and can help indicate generalization error. We further show that the first-order flatness directly controls the maximal eigenvalue of Hessian of the training loss, which is a proper sharpness/flatness measure indicating the loss uptrend un-der an adversarial perturbation to the weights [31–33].
To optimize the first-order flatness in deep model training, we propose Gradient norm Aware Minimization (GAM), which approximates the maximum gradient norm with stochastic gradient ascent and Hessian-vector products to avoid the materialization of the Hessian matrix.
We summarize our contributions as follows.
• We present first-order flatness, which measures the largest gradient norm in the neighborhood of minima. We show that the first-order flatness is stronger than current zeroth-order flatness and it controls the maximum eigenvalue of
Hessian.
• We propose a novel training procedure, GAM, to simulta-neously optimize prediction loss and first-order flatness.
We analyze the generalization error and the convergence of GAM.
• We empirically show that GAM considerably improves model generalization when combined with current opti-mizers such as SGD and AdamW across a wide range of datasets and networks. We show that GAM further im-proves the generalization of models trained with SAM.
• We empirically validate that GAM indeed finds flatter op-tima with lower Hessian spectra. 2.