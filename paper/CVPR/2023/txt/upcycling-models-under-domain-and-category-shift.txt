Abstract
Source Domain
Target Domain Upcycling
Deep neural networks (DNNs) often perform poorly in the presence of domain shift and category shift. How to up-cycle DNNs and adapt them to the target task remains an important open problem. Unsupervised Domain Adapta-tion (UDA), especially recently proposed Source-free Do-main Adaptation (SFDA), has become a promising tech-nology to address this issue. Nevertheless, existing SFDA methods require that the source domain and target domain share the same label space, consequently being only ap-plicable to the vanilla closed-set setting. In this paper, we take one step further and explore the Source-free Univer-sal Domain Adaptation (SF-UniDA). The goal is to iden-tify “known” data samples under both domain and category shift, and reject those “unknown” data samples (not present in source classes), with only the knowledge from standard pre-trained source model. To this end, we introduce an innovative global and local clustering learning technique (GLC). Speciﬁcally, we design a novel, adaptive one-vs-all global clustering algorithm to achieve the distinction across different target classes and introduce a local k-NN cluster-ing strategy to alleviate negative transfer. We examine the superiority of our GLC on multiple benchmarks with differ-ent category shift scenarios, including partial-set, open-set, and open-partial-set DA. Remarkably, in the most challeng-ing open-partial-set DA scenario, GLC outperforms UMAD by 14.8% on the VisDA benchmark. The code is available at https://github.com/ispc-lab/GLC. 1.

Introduction
At the expensive cost of given large-scale labeled data and huge computation resources, deep neural networks (DNNs) have made remarkable progress in various tasks.
However, DNNs often generalize poorly to the unseen new domain under domain shift and category shift. How to upcycle DNNs and adapt them to target tasks is still a
*Equal Contribution
†Corresponding author: guangchen@tongji.edu.cn (cid:434)  (cid:165) 
Source data
Source model
Open-partial-set
Partial-set
Open-set
GLC 
Tech
Which type category shift are we  facing? We have no idea ...
Upcycled  target model
Figure 1. The illustration of Source-free Universal Domain Adap-tation (SF-UniDA). The goal is to realize model upcycling under both domain shift and category shift. It is extremely challenging as only one source closed-set model is provided as supervision rather than raw data. And we do not have any prior knowledge about category shift between domains in advance. long-standing open problem. In the last decade, many ef-forts have been devoted to unsupervised domain adaptation (UDA) [12, 16, 28, 40], which capitalizes on labeled source data and unlabeled target data in a transduction manner, and has achieved signiﬁcant success. Despite this, the access to source raw data is inefﬁcient and may violate the increas-ingly stringent data privacy policies [45]. Recently, Source-free Domain Adaptation (SFDA) [22, 35, 46] has become a promising technology to alleviate this issue, where only a pre-trained source model is provided as supervision rather than raw data. However, to avoid model collapse, most ex-isting methods [22, 35, 46] assume that the label space is identical across the source and target domain, thus being only applicable to vanilla closed-set scenarios.
In reality, target data may come from a variety of sce-narios. Therefore, it is too difﬁcult to hold such a strict assumption. For a better illustration, we suppose Ys and
Yt as the label space of source domain and target domain, respectively. In addition to the well-studied vanilla closed-set (Ys = Yt.), we often encounter several other situations, e.g., the partial-set (Ys ⊃ Yt), the open-set (Ys ⊂ Yt), and the open-partial-set (Ys ∩ Yt (cid:5)= ∅, Ys (cid:2) Yt, Ys (cid:3) Yt).
Currently, there have been several source data-dependent works [4,5,26,32,38,47] developed to target category shift.
However, methods devised for one situation are commonly
infeasible for others. In practice, the target domain is un-labeled and we cannot know which of these category shifts will occur in advance. Not to mention that the requirement to source raw data makes it inefﬁcient and potentially vi-olates data protection policies. To tackle these limitations, and handle those category shifts in a uniﬁed manner, in this paper, we take one step further and delve into the Source-free Universal Domain Adaptation (SF-UniDA). The goal is to upcycle the standard pre-trained source models identi-fying “known” data samples and rejecting those “unknown” data samples (not present in source classes) under domain and category shift. We conceptually present the SF-UniDA in Fig. 1. Note that, very few works [18,23] have studied the source-free model adaptation in open-partial-set scenarios.
Nevertheless, their approaches demand dedicated model ar-chitectures, greatly limiting their practical applications. SF-UniDA is appealing in view that model adaptation can be resolved only on the basis of a standard pre-trained closed-set model, i.e., without speciﬁed model architectures.
To approach such a challenging DA setting, we propose a simple yet generic technique, Global and Local Clustering (GLC). Different from existing pseudo-labeling strategies that focus on closed-set scenarios, we develop a novel one-vs-all global clustering based pseudo-labeling algorithm to achieve “known” data identiﬁcation and “unknown” data rejection. As we have no prior about the category shift, we utilize the Silhouettes [36] metric to help us realize adaptive global clustering. To avoid source private cate-gories misleading, we design a global conﬁdence statistics based suppression strategy. Although the global clustering algorithm encourages the separation of “known” and “un-known” data samples, we ﬁnd that some semantically in-correct pseudo-label assignments may still occur, leading to negative knowledge transfer. To mitigate this, we further introduce a local k-NN clustering strategy by exploiting the intrinsic consensus structure of the target domain.
We validate the superiority of our GLC via extensive experiments on four benchmarks (Ofﬁce-31 [37], Ofﬁce-Home [44], VisDA [34], and Domain-Net [33]) under var-ious category shift situations, including partial-set, open-set and open-partial-set. Empirical results show that GLC yields state-of-the-art performance across multiple bench-marks, even with stricter constraints.
Our contributions can be summarized as follows:
• To the best of our knowledge, we are the ﬁrst to exploit and achieve the Source-free Universal Domain Adap-tation (SF-UniDA) with only a standard pre-trained closed-set model.
• We propose a generic global and local clustering tech-nique (GLC) to address the SF-UniDA. GLC equips with an innovative global one-vs-all clustering algo-rithm to realize “known” and “unknown” data samples separation under various category-shift.
• Extensive experiments on four benchmarks under vari-ous category-shift situations demonstrate the superior-ity of our GLC technique. Remarkably, in the open-partial-set DA situation, GLC attains an H-score of 73.1% on the VisDA benchmark, which is 14.8% and 16.7% higher than UMAD and GATE, respectively. 2.