Abstract
Spiking Neural Networks (SNNs) have attracted signif-icant attention due to their energy-efficient properties and potential application on neuromorphic hardware.
State-of-the-art SNNs are typically composed of simple
Leaky Integrate-and-Fire (LIF) neurons and have become comparable to ANNs in image classification tasks on large-scale datasets. However, the robustness of these deep SNNs
In this paper, we first has not yet been fully uncovered. experimentally observe that layers in these SNNs mostly communicate by rate coding. Based on this rate coding property, we develop a novel rate coding SNN-specified attack method, Rate Gradient Approximation Attack (RGA).
We generalize the RGA attack to SNNs composed of LIF neurons with different leaky parameters and input encoding by designing surrogate gradients. In addition, we develop the time-extended enhancement to generate more effective adversarial examples. The experiment results indicate that our proposed RGA attack is more effective than the previous attack and is less sensitive to neuron hyperparameters. We also conclude from the experiment that rate-coded SNN composed of LIF neurons is not secure, which calls for exploring training methods for SNNs composed of complex neurons and other neuronal codings. Code is available at https://github.com/putshua/SNN attack RGA 1.

Introduction
As the third generation of artificial neural networks [47],
Spiking Neural Networks (SNNs) have gained more attrac-tion due to their spatio-temporal, discrete representation, and event-driven properties. These bio-inspired neural net-works borrow the characteristics of spiking representations and neuronal dynamics from biological brains [23, 75]. Un-like traditional Analog Neural Networks (ANNs), SNNs utilize spiking neurons as their essential components, which accumulate current over time, emit spikes when the mem-brane potential exceeds the threshold, and pass on informa-* Corresponding author tion through spike trains. The natural sparsity of the spike trains leads to the low power consumption of SNNs [59,69].
SNNs are competitive in real-world vision applications.
The development of neuromorphic computing [10, 11, 20, 54, 56, 76] has further magnified the advantages of low-power consumption properties of SNNs, so that they can be deployed in power-limited scenarios [8, 64], such as edge computing or mobile application. However, the training al-gorithms of SNNs are also improving. The most practical training methods are ANN-SNN conversion [7], supervised training [72], and hybrid training [57, 58].
When SNNs are applied to safety-critical systems, the reliability of SNNs should be a major concern. The adver-sarial attack is one of the most significant categories that threatens model security [24, 68]. Similar to ANNs, SNNs can also be fooled by crafting adversarial examples that are imperceptible to human eyes from gradient-based back-propagation [62], which may lead to catastrophic conse-quences when SNNs are deployed in safety-related scenar-ios. Nevertheless, SNNs are still considered to be more ro-bust than ANNs. This robustness comes from inherent neu-ral dynamics, such as forgetting historical information and discrete spikes [63]. Besides, the robustness of SNNs can be improved through special structural enhancements [9] or training techniques [37, 45, 71].
Effective attack examples of ANNs can be crafted from well-defined gradients on the activation functions [68]. For
SNNs, a common way to construct gradient-based attacks is by backpropagating through a surrogate function over discrete spikes. In this way, the gradient may suffer from explosion and vanishment in temporal and layer-by-layer communication [72]; at the same time, the membrane po-tential of all historical time steps needs to be saved when backpropagation, which requires a large amount of memory.
Currently, high-performance SNNs typically combine leaky integrate-and-fire models and rate-encoded inputs. While the rate coding scheme brings excellent performance to
SNN, it also exposes shortcomings. If the rate coding nature in SNN is considered, can we construct a more powerful at-tack? After all, the activation functions of many ANNs are inspired by the firing rate of biological neurons [52].
In this paper, we develop a novel Rate Gradient Approx-imation Attack (RGA) based on components of rate coding in high-performance SNNs. RGA attack is more effective than previously used attacks as it makes better use of the rate coding feature. We expect our work to provide bench-marks for SNN defense against adversarial attacks and in-spire future research for SNNs. The main contributions of this paper are:
• We observe that layers in SNN are mainly communi-cated by rate coding, either for converted SNN or for surrogate-trained SNN.
• We develop the Rate Gradient Approximation Attack based on rate coding and apply it to SNNs composed of different types of neurons and input codings. We further propose a time-extended variant to get more ef-fective adversarial examples.
• Experiments prove that the RGA attack outperforms the STBP attack and is less sensitive to neuron hyper-parameters. Based on the proposed attack, we com-pare the robustness of SNNs using different leaky pa-rameters with that of ANNs and manifest that the
SNNs composed of LIF neurons cannot provide strong enough security. This conclusion inspires further re-search on networks with more complex neurons. 2.