Abstract
Pointly supervised instance segmentation (PSIS) learns to segment objects using a single point within the object extent as supervision. Challenged by the non-negligible se-mantic variance between object parts, however, the single supervision point causes semantic bias and false segmenta-tion. In this study, we propose an AttentionShift method, to solve the semantic bias issue by iteratively decomposing the instance attention map to parts and estimating fine-grained semantics of each part. AttentionShift consists of two mod-ules plugged on the vision transformer backbone: (i) to-ken querying for pointly supervised attention map genera-tion, and (ii) key-point shift, which re-estimates part-based attention maps by key-point filtering in the feature space.
These two steps are iteratively performed so that the part-based attention maps are optimized spatially as well as in the feature space to cover full object extent. Experiments on PASCAL VOC and MS COCO 2017 datasets show that
AttentionShift respectively improves the state-of-the-art of by 7.7% and 4.8% under mAP@0.5, setting a solid PSIS baseline using vision transformer.
Figure 1.
Comparison of existing methods with the pro-posed AttentionShift. Upper: The class activation map (CAM) method [51] suffers partial activation for the lack of spatial constraint. The self-attention map generated by vision trans-former [16] encounters false and missed object parts. Lower: At-tentionShift literately optimizes part-based attention maps (indi-cated by key-points) by shifting the key-points in the feature space to precisely localize the full object extent. (Best viewed in color) 1.

Introduction
Instance segmentation is one of the most important vi-sion tasks with a wide range of applications in medical im-age processing [21, 23, 36], human machine interface [3, 7, 24] and advanced driver assistance system [14,37,41]. Nev-ertheless, this task requires great human efforts to annotate instance masks, particular in the era of big data. For exam-ple, it takes more than four years to create the ground-truth instance masks in the MS COCO dataset by a human anno-tator [29]. The large annotation cost hinders the deployment
*Equal Contribution (Liao: Idea, Experiment; Guo: Detection Base-line, Writing).
†Corresponding Author. of instance segmentation to real-world applications.
Pointly supervised instance segmentation (PSIS) [25, 26], where each instance is indicated by a single point, has been a promising approach to solve the annotation cost is-sue. Compared with the precise mask annotation, PSIS re-quires only about 20% annotation cost, which is comparable with the weakly supervised method, while the performance is far beyond the later [2].
Existing methods [25, 26] typically estimate a single pseudo mask for each instance and refine the estimated mask by training a segmentation model. However, such methods ignore the fact that the semantic variance between object parts is non-negligible. For example, a “dog head”
Figure 2. Flowchart of AttenttionShift. During training, the model first learns the fine-grained semantics by decomposing the instance attention map to parts (indicated by key-points) and performing key-point shift the feature space. It then takes the estimated key-points (each represents a part) as supervisions and querying their locations using the vision transformer. and a “dog tail” belongs to the same semantic of “dog”, but their appearances are quite different, Fig. 1 upper. With a single supervision point, these methods could estimate the semantic of “dog” as that of the most discriminative part (“dog tail” in middle of Fig. 1 upper) or that of the regions around the supervision point (“dog head” in right of Fig. 1 upper), which is termed as semantic bias. We make a statis-tical analysis on ViT and found only 33% of ViT attention maps can cover over 50% of foreground pixels. Despite its ability to model long-range feature dependencies, ViT ap-pears to remain vulnerable to the issue of semantic bias.
In this study, we propose AttentionShift to solve the se-mantic bias problem by estimating fine-grained semantics of multiple instance parts and learning an instance segmen-tor under the supervision of estimated fine-grained seman-tics, Fig. 1(lower). Considering that instance parts are un-available during training, AttentionShift adopts an iterative optimization procedure, which spatially decomposes each instance to parts based on key-points defined on mean fea-ture vectors and adaptively updates the key-points in a way like mean shift [11] in the feature space, Fig. 2.
Using the vision transformer (ViT) as the backbone, At-tentionShift consists of two steps: (i) token querying of point supervision for instance attention map generation. It takes advantage of the ViT to generate an instance attention map by matching the semantics and locations of patch to-kens with those of the supervision point. (ii) key-point shift which re-estimates the part-based attention map by key-point initialization and filtering in the feature space. These two steps are iteratively performed so that the part-based at-tention map, indicated by key-points, is optimized spatially as well as in the feature space to cover the full object extent.
We conduct experiments on commonly used PASCAL
VOC and the challenging MS COCO 2017 datasets. At-tentionShift respectively improves the state-of-the-art of by 7.7% and 4.8% under mAP@0.5, demonstrating the poten-tial to fill the performance gap between pointly-supervised instance segmentation and fully supervised instance seg-mentation methods.
The contributions of this paper are concluded as follows:
• We propose a part-based attention map estimation ap-proach for PSIS, which estimates fine-grained seman-tics of instances to alleviate the semantic bias problem in a systematic fashion.
• We represent object parts using key-points and lever-age AttentionShift to operate key-points in the feature space, providing a simple-yet-effective fashion to op-timize part-based attention maps.
• AttentionShift achieves state-of-the-art performance, setting a solid PSIS baseline using vision transformer. 2.