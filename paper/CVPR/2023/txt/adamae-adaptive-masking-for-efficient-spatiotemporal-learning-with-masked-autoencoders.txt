Abstract
Masked Autoencoders (MAEs) learn generalizable rep-resentations for image, text, audio, video, etc., by recon-structing masked input data from tokens of the visible data.
Current MAE approaches for videos rely on random patch, tube, or frame based masking strategies to select these tokens.
This paper proposes AdaMAE, an adaptive masking strategy for MAEs that is end-to-end trainable. Our adaptive mask-ing strategy samples visible tokens based on the semantic context using an auxiliary sampling network. This network estimates a categorical distribution over spacetime-patch tokens. The tokens that increase the expected reconstruction error are rewarded and selected as visible tokens, motivated by the policy gradient algorithm in reinforcement learning.
We show that AdaMAE samples more tokens from the high spatiotemporal information regions, thereby allowing us to mask 95% of tokens, resulting in lower memory require-ments and faster pre-training. We conduct ablation studies on the Something-Something v2 (SSv2) dataset to demon-strate the efficacy of our adaptive sampling approach and report state-of-the-art results of 70.0% and 81.7% in top-1 accuracy on SSv2 and Kinetics-400 action classification datasets with a ViT-Base backbone and 800 pre-training epochs. Code and pre-trained models are available at: https://github.com/wgcban/adamae.git. 1.

Introduction
Self-supervised learning (SSL) aims to learn transferable representations from a large collection of unlabeled data for downstream applications (e.g., classification and detection).
SSL is conducted in a two-stage framework [21], consisting of pre-training on an unlabeled dataset, and fine-tuning on a downstream task. Pre-training has shown to improve perfor-mance [3,11], convergence speed [11], and robustness [2,18], and reduce model overfitting [11, 14] on downstream tasks.
Recently, masked autoencoders (MAEs) [6, 11, 16, 17, 20, 48, 57] and contrastive learning [21, 39, 47] approaches are
Figure 1. Comparison of our adaptive masking with existing random patch [11], tube [48, 52], and frame [27, 37, 42] masking for masking ratio of 80%. Our adaptive masking approach selects more tokens from the regions with high spatiotemporal information while a small number of tokens from the background. mainly used for SSL. In MAEs, the input (image or video) is patchified and converted into a set of tokens. A small percentage (e.g., 5-10%) of these tokens, namely visible to-kens, are passed through a Vision Transformer (ViT) [8].
The resulting token embeddings are then concatenated with a learnable representation for masked tokens, and are fed into a shallow decoder transformer to reconstruct masked patches. On the other hand, contrastive learning takes two augmented views of the same input and pulls them together in the embedding space, while embeddings of different in-puts are pushed away [39]. MAEs have recently gained more attention over contrastive learning methods due to the inher-ent use of a high masking ratio, which enables simple and memory-efficient training.
Mask sampling techniques are critical to the success of
MAEs [11, 54]. Previous studies have investigated differ-ent sampling techniques that include random “patch” [11],
“tube” [54], and “frame” [54] masking (see Fig. 1). Random
patch sampling has shown to work well compared to its coun-terparts in some cases [11]. However, since not all tokens have equal information, assuming a uniform probability dis-tribution over all input tokens (for selection of visible tokens) is sub-optimal. In other words, with these random masking strategies, the visible tokens are sampled from redundant or low information regions instead of high information ones, hence resulting in inaccurate reconstructions. This inhibits
MAEs from learning meaningful representations, besides requiring a relatively larger number of training iterations compared to contrastive learning methods.
In this paper, we propose an adaptive sampling approach that simultaneously optimizes an MAE and an adaptive token sampling network. Our approach selects patches based on their spatiotemporal information. Unlike uniform random sampling, we first estimate the categorical distribution over all input tokens using an auxiliary network, and then sample visible tokens from that distribution. Since sampling is a non-differentiable operation, we propose an auxiliary loss for optimizing the adaptive token sampling network. Our solution is motivated by the REINFORCE algorithm [56], which comes under the family of policy gradient algorithms in Reinforcement Learning (RL) [23].
We empirically show that our adaptive token sampling network leads to sampling more tokens from high spatiotem-poral information regions compared to random masking tech-niques as shown in Fig. 1. This efficient token allocation also enables high masking ratios (i.e., 95%) for pre-training
MAEs. This ultimately reduces the GPU memory require-ments and expedites pre-training while improving accuracy on downstream tasks. In summary, our contributions are:
• We propose AdaMAE, a novel, adaptive, and end-to-end trainable token sampling strategy for MAEs that takes into account the spatiotemporal properties of all input tokens to sample fewer but informative tokens.
• We empirically show that AdaMAE samples more to-kens from high spatiotemporal information regions of the input, resulting in learning meaningful representa-tions for downstream tasks.
• We demonstrate the efficiency of AdaMAE in terms of performance and GPU memory against random “patch”,
“tube”, and “frame” sampling by conducting a thorough ablation study on the SSv2 dataset.
• We show that our AdaMAE outperforms state-of-the-art (SOTA) by 0.7% and 1.1% (in top-1) improvements on
SSv2 and Kinetics-400, respectively. 2.