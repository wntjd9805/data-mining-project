Abstract
Synthesizing high-fidelity complex images from text is challenging. Based on large pretraining, the autoregres-sive and diffusion models can synthesize photo-realistic im-ages. Although these large models have shown notable progress, there remain three flaws. 1) These models re-quire tremendous training data and parameters to achieve good performance. 2) The multi-step generation design slows the image synthesis process heavily. 3) The synthe-sized visual features are challenging to control and require delicately designed prompts. To enable high-quality, effi-cient, fast, and controllable text-to-image synthesis, we pro-pose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the powerful pretrained CLIP model both in the discriminator and generator. Specifically, we propose a
CLIP-based discriminator. The complex scene understand-ing ability of CLIP enables the discriminator to accurately assess the image quality. Furthermore, we propose a CLIP-empowered generator that induces the visual concepts from
CLIP through bridge features and prompts. The CLIP-integrated generator and discriminator boost training ef-ficiency, and as a result, our model only requires about 3% training data and 6% learnable parameters, achieving com-parable results to large pretrained autoregressive and diffu-sion models. Moreover, our model achieves ∼120×faster synthesis speed and inherits the smooth latent space from
GAN. The extensive experimental results demonstrate the excellent performance of our GALIP. Code is available at https://github.com/tobran/GALIP. 1.

Introduction
Over the last few years, we have witnessed the great suc-cess of generative models for various applications [4, 47].
Among them, text-to-image synthesis [3, 5, 16, 19–22, 26, 29, 30, 34, 43, 48, 50–53, 60] is one of the most appealing
*Corresponding Author
Figure 1. (a) Existing text-to-image GANs conduct adversarial training from scratch. (b) Our proposed GALIP conducts adver-sarial training based on the integrated CLIP model. applications. It generates high-fidelity images according to given language guidance. Owing to the convenience of lan-guage for users, text-to-image synthesis has attracted many researchers and has become an active research area.
Based on a large scale of data collections, model size, and pretraining, recently proposed large pretrained autore-gressive and diffusion models, e.g., DALL-E [34] and
LDM [36], show the impressive generative ability to syn-thesize complex scenes and outperform the previous text-to-image GANs significantly. Although these large pretrained generative models have achieved significant advances, they still suffer from three flaws. First, these models require tremendous training data and parameters for pretraining.
The large data and model size brings an extremely high computing budget and hardware requirements, making it inaccessible to many researchers and users. Second, the generation of large models is much slower than GANs.
The token-by-token generation and progressive denoising require hundreds of inference steps and make the generated results lag the language inputs seriously. Third, there is no intuitive smooth latent space as GANs, which maps mean-ingful visual attributes to the latent vector. The multi-step generation design breaks the synthesis process and scatters the meaningful latent space. It makes the synthesis process require delicately designed prompts to control.
To address the above limitations, we rethink Generative
Adversarial Networks (GAN). GANs are much faster than autoregressive and diffusion models and have smooth latent space, which enables more controllable synthesis. How-ever, GAN models are known for potentially unstable train-ing and less diversity in the generation [7]. It makes current text-to-image GANs suffer from unsatisfied synthesis qual-ity under complex scenes.
In this work, we introduce the pretrained CLIP [31] into text-to-image GANs. The large pretraining of CLIP brings two advantages. First, it enhances the complex scene un-derstanding ability. The pretraining dataset has many com-plex images under different scenes. Armed with the Vision
Transformer (ViT) [9], the image encoder can extract in-formative and meaningful visual features from complex im-ages to align the corresponding text descriptions after ade-quate pretraining. Second, the large pretraining dataset also enables excellent domain generalization ability. It contains various kinds of images, e.g., photos, drawings, cartoons, and sketches, collected from a variety of publicly available sources. The various images make the CLIP model can map different kinds of images to the shared concepts and en-able impressive domain generalization and zero-shot trans-fer ability. These two advantages of CLIP, complex scene understanding and domain generalization ability, motivate us to build a more powerful text-to-image model.
We propose a novel text-to-image generation framework named Generative Adversarial CLIPs (GALIP). As shown in Figure 1, the GALIP integrates the CLIP model [31] in both the discriminator and generator. To be specific, we pro-pose the CLIP-based discriminator and CLIP-empowered generator. The CLIP-based discriminator inherits the com-plex scene understanding ability of CLIP [31]. It is com-posed of a frozen ViT-based CLIP image encoder (CLIP-ViT) and a learnable mate-discriminator (Mate-D). The
Mate-D is mated to the CLIP-ViT for adversarial training.
To retain the knowledge of complex scene understanding in the CLIP-ViT, we freeze its weights and collect the pre-dicted CLIP image features from different layers. Then, the Mate-D further extracts informative visual features from collected CLIP features to distinguish the synthesized and real images. Based on the complex scene understanding ability of CLIP-ViT and the continuous analysis of Mate-D, the CLIP-based discriminator can assess the quality of generated complex images more accurately.
Furthermore, we propose the CLIP-empowered genera-tor, which exerts the domain generalization ability of CLIP
[31]. It is hard for the generator to synthesize complex im-ages directly. Some works employ sketch [11] and lay-out [21, 23] as bridge domains to alleviate the difficulty.
However, such a design requires additional labeled data.
Different from these works, the excellent domain general-ization of CLIP [31] motivates us that there may be an im-plicit bridge domain, which is easier to synthesize but can be mapped to the same visual concepts through the CLIP-ViT. Thus, we design the CLIP-empowered generator.
It is composed of a frozen CLIP-ViT and a learnable mate-generator (Mate-G). The Mate-G first predicts the implicit bridge features from text and noise. Then the bridge feature will be mapped to the visual concepts through CLIP-ViT.
Furthermore, we add some text-conditioned prompts to the
CLIP-ViT for task adaptation. The predicted visual con-cepts close the gap between text features and target images which enhances the complex image synthesis ability.
Overall, our contributions can be summarized as follows:
• We propose an efficient, fast, and more controllable model for text-to-image synthesis that can synthesize high-quality complex images.
• We propose the CLIP-based discriminator, which as-sesses the quality of complex images more accurately.
• We propose the CLIP-empowered generator, which syn-thesizes images based on text features and predicted CLIP visual features.
• Extensive experiments demonstrate that the proposed
GALIP can achieve comparable performance with large pertaining models based on significantly smaller compu-tational costs. 2.