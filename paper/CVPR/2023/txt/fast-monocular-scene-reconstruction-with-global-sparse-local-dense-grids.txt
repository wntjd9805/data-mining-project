Abstract
Indoor scene reconstruction from monocular images has long been sought after by augmented reality and robotics developers. Recent advances in neural field representa-tions and monocular priors have led to remarkable re-sults in scene-level surface reconstructions. The reliance on Multilayer Perceptrons (MLP), however, significantly limits speed in training and rendering.
In this work, we propose to directly use signed distance function (SDF) in sparse voxel block grids for fast and accurate scene recon-struction without MLPs. Our globally sparse and locally dense data structure exploits surfaces’ spatial sparsity, en-ables cache-friendly queries, and allows direct extensions to multi-modal data such as color and semantic labels. To apply this representation to monocular scene reconstruc-tion, we develop a scale calibration algorithm for fast geo-metric initialization from monocular depth priors. We apply differentiable volume rendering from this initialization to refine details with fast convergence. We also introduce effi-cient high-dimensional Continuous Random Fields (CRFs) to further exploit the semantic-geometry consistency be-tween scene objects. Experiments show that our approach faster in rendering while is 10 achieving comparable accuracy to state-of-the-art neural implicit methods. faster in training and 100
×
× 1.

Introduction
Reconstructing indoor spaces into 3D representations is a key requirement for many real-world applications, includ-ing robot navigation, immersive virtual/augmented reality experiences, and architectural design. Particularly useful is reconstruction from monocular cameras which are the most prevalent and accessible to causal users. While much re-search has been devoted to this task, several challenges re-main.
Conventional monocular reconstruction from multi-view
RGB images uses patch matching [34], which takes hours to reconstruct even a relatively small scene. Several 3D re-*CMU RI. Work done during the internship at NVIDIA.
†NVIDIA Research
Figure 1. Color and semantic scene reconstruction from our sys-tem with monocular images and learned monocular priors. construction methods [38,46] have demonstrated fast recon-struction by applying 3D convolutional neural networks to feature volumes, but they have limited resolution and strug-gle to generalize to larger scenes.
Recently, unified neural radiance fields [22] and neural implicit representations were developed for the purpose of accurate surface reconstruction from images [29, 43, 47].
While this was successfully demonstrated on single objects, the weak photometric constraint leads to poor reconstruc-tion and slow convergence for large-scale scenes. Guo et al. [14] and Yu et al. [49] improved the quality and con-vergence speed of neural field reconstruction on large-scale scenes by incorporating learned geometrical cues like depth and normal estimation [11, 31], however, training and eval-uation remain inefficient. This is primarily because these approaches rely on MLPs and feature grids [23] that encode the entire scene rather than concentrating around surfaces.
In contrast to MLPs, an explicit SDF voxel grid can be adaptively allocated around surfaces, and allows fast query and sampling. However, an efficient implementation of differentiable SDF voxel grids without MLPs is missing.
Fridovich-Keil and Yu et al. [12] used an explicit density and color grid, but is limited to rendering small objects.
Muller et al. [23] developed a feature grid with spatial hash-ing for fast neural rendering, but its backbone hash map is not collision-free, causing inevitable slow random access and inaccurate indexing at large scales. Dong et al. [10] pro-(a) COLMAP [34] (b) NeRF [22] (c) VolSDF [47] (d) NeuS [43] (e) ManhattanSDF [14] (f) MonoSDF-MLP [49] (g) MonoSDF-Grid [49] (h) Ours
Figure 2. Qualitative reconstruction comparison on ScanNet [7]. While being 10× faster in training, we achieve similar reconstruction results to state-of-the-art MonoSDF [49], with fine details (see Fig. 9). posed a collision-free spatially hashed grid following Niess-ner et al. [28], but lacks support for differentiable rendering.
Several practical challenges hinder the implementation of an efficient differentiable data structure: 1. a collision-free spatial hash map on GPU that supports one-to-one indexing from positions to voxels; 2. differentiable trilinear inter-polations between spatially hashed voxels; 3. parallel ray marching and uniform sampling from a spatial hash map.
Our approach: we address such challenges using a dif-ferentiable globally sparse and locally dense voxel grid. We transform a collision-free GPU hash map [35] to a differen-tiable tensor indexer [30]. This generates a one-to-one map between positions and globally sparse voxel blocks around approximate surfaces, and enables skipping empty space for efficient ray marching and uniform sampling. We fur-ther manage locally dense voxel arrays within sparse voxel blocks for GPU cache-friendly contiguous data query via trilinear interpolation. As a result, using explicit SDF grids leads to fast SDF gradient computation in a single forward pass, which can further accelerate differentiable rendering.
This new data structure presents a new challenge — we can only optimize grid parameters if they are allocated around surfaces. To resolve this, we make use of off-the-shelf monocular depth priors [11,31] and design a novel ini-tialization scheme with global structure-from-motion (SfM) constraints to calibrate these unscaled predicted depths. It results in a consistent geometric initialization via volumet-ric fusion ready to be refined through differentiable volume rendering.
We additionally incorporate semantic monocular pri-ors [17] to provide cues for geometric refinement in 3D. For instance, we use colors and semantics to guide the sharp-ening of normals around object boundaries, which in turn improves the quality of colors and semantics. We enforce these intuitive notions through our novel continuous Condi-tional Random Field (CRF). We use Monte Carlo samples on the SDF zero-crossings to create continuous CRF nodes and define pairwise energy functions to enforce local con-sistency of colors, normals, and semantics. Importantly, we define similarity in a high dimensional space that consists of coordinates, colors, normals, and semantics, to reject spa-tially close samples with contrasting properties. To make inference tractable, we follow Krahenbuhl et al. [16] and use variational inference, leading to a series of convolutions in a high-dimensional space. We implement an efficient per-mutohedral lattice convolution [1] using the collision-free
GPU hashmap to power the continuous CRF inference.
×
The final output of our system is a scene reconstruction with geometry, colors, and semantic labels, as shown in
Fig. 1. Experiments show that our method is 10 faster in training, 100 faster in inference, and has comparable accuracy measured by F-scores against state-of-the-art im-plicit reconstruction systems [14, 49]. In summary, we pro-pose a fast scene reconstruction system for monocular im-ages. Our contributions include:
• A globally sparse locally dense differentiable volumetric data structure that exploits surface spatial sparsity without an MLP;
×
• A scale calibration algorithm that produces consistent ge-ometric initialization from unscaled monocular depths;
• A fast monocular scene reconstruction system equipped with volume rendering and high dimensional continuous
CRFs optimization. 2.