Abstract
This paper considers the problem of open-vocabulary se-mantic segmentation (OVS), that aims to segment objects of arbitrary classes beyond a pre-defined, closed-set cat-egories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled image-text pairs for pre-training without using any mask annota-tions. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slot-attention based bind-ing module, then aligns the group tokens to correspond-ing caption embeddings. Second, we propose two proxy tasks for training, namely masked entity completion and cross-image mask consistency. The former aims to infer all masked entities in the caption given group tokens, that enables the model to learn fine-grained alignment between visual groups and text entities. The latter enforces consis-tent mask predictions between images that contain shared entities, encouraging the model to learn visual invariance.
Third, we construct CC4M dataset for pre-training by fil-tering CC12M with frequently appeared entities, which sig-nificantly improves training efficiency. Fourth, we per-form zero-shot transfer on four benchmark datasets, PAS-CAL VOC, PASCAL Context, COCO Object, and ADE20K.
OVSegmentor achieves superior results over state-of-the-art approaches on PASCAL VOC using only 3% data (4M vs 134M) for pre-training. 1.

Introduction
Semantic segmentation considers the problem of assign-ing class labels to each pixel in the image. It plays criti-cal roles in a wide range of real-world scenarios, including autonomous driving, computer-aided diagnosis and satellite image analysis, to name a few. Generally speaking, two lines of research dominate semantic segmentation, one way is to cluster the pixels into different groups and assign a
*Corresponding author
Figure 1. An illustration of open-vocabulary semantic segmen-tation. Models trained on closed-set classes (cat and dog) fail to segment novel class (fire hydrant). We train a visual encoder and a text encoder on web-collected image-text pairs without using any mask labels, and our model can segment arbitrary object classes. semantic label to each group; the other idea treats segmen-tation as pixel-wise classification, casting each pixel into one category. Despite tremendous progress, the scalability of existing approaches that rely on supervised training has been fundamentally limited: (1) costly annotation proce-dure. Extensive manual pixel-wise annotations are required for training segmentation models; (2) closed-set segmenta-tion. The model is restricted to segmenting objects from a closed-set of categories. Whenever a new dataset comes, the model requires re-training.
In this paper, our goal is to train an open-vocabulary semantic segmentation (OVS) model, by exploiting freely available image-caption pairs on Internet, as illustrated in Fig. 1. The recent work, for example, CLIP [41] and ALIGN [23] have demonstrated that a combination of large-scale image-caption pairs and simple noise con-trastive estimation can learn powerful image-text embed-dings from scratch, and show strong “zero-shot” general-ization abilities for open-vocabulary classification. Addi-tionally, GroupViT [53] extends the idea towards semantic segmentation by training a segmentation model with text supervision only. They perform hierarchical grouping of
visual tokens, which are then aligned to the correspond-ing text embeddings via a contrastive loss. However, the following issues remain challenging and unsolved: First, the captions only provide coarse, image-level descriptions, which are insufficient for training semantic segmentation models where fine-grained, pixel-wise supervision is usu-ally needed. Second, the diversity of web-collected data is large, that requires the model to learn visual invariance on objects of interest, with only weak supervision provided.
For instance, the visual appearance of two images with sim-ilar captions can be drastically different.
To tackle the above challenges, (i) we propose a transformer-based model for open-vocabulary semantic segmentation, dubbed as OVSegmentor, that can segment objects of arbitrary categories via zero-shot transfer, with only image-caption pairs for pre-training. Specifically, we introduce learnable group tokens to cluster image patches via a slot-attention [33] based binding module, and align the group tokens with corresponding caption embeddings.
Note that our model neither requires ground-truth masks for training nor additional re-training on target segmenta-tion datasets, substantially alleviating the annotation efforts and improving transfer efficiency; (ii) As for training on the image-caption dataset, we propose two proxy tasks, namely masked entity completion and cross-image mask consis-tency, the former trains the model to infer all the masked entities in the sentence given the group tokens, and the lat-ter enforces consistent mask prediction for images with the common entity. Both tasks have shown to be beneficial in learning entity-specific, fine-grained and visually invari-ant group semantics; (iii) We construct an image-caption dataset, termed as CC4M, by designing an automatic ap-proach to filter CC12M [7] with frequently appeared visual entities, significantly improving the training efficiency.
We pre-train the proposed OVSegmentor on our filtered image-caption dataset (CC4M), without using any man-ual segmentation masks whatsoever. The model is eval-uated on four segmentation benchmarks, PASCAL VOC 2012 [18], PASCAL Context [38], COCO Object [31], and
ADE20K [59] in a zero-shot manner, i.e., the model is directly evaluated on target datasets without any finetun-ing. Extensive experiments demonstrate that our model sur-passes existing models that are trained with full supervision and outperforms state-of-the-art self-supervised approaches on PASCAL VOC by using only 3% data (4M vs 134M) for pre-training, significantly improving the training efficiency. 2.