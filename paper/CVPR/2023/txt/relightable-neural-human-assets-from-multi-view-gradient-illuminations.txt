Abstract
Human modeling and relighting are two fundamental problems in computer vision and graphics, where high-quality datasets can largely facilitate related research.
However, most existing human datasets only provide multi-view human images captured under the same illumination.
Although valuable for modeling tasks, they are not read-ily used in relighting problems. To promote research in both fields, in this paper, we present UltraStage, a new 3D human dataset that contains more than 2,000 high-quality human assets captured under both multi-view and multi-illumination settings. Specifically, for each example, we provide 32 surrounding views illuminated with one white light and two gradient illuminations. In addition to regular multi-view images, gradient illuminations help recover de-tailed surface normal and spatially-varying material maps,
Inspired by re-enabling various relighting applications.
*Equal contribution.
†Corresponding author. cent advances in neural representation, we further interpret each example into a neural human asset which allows novel view synthesis under arbitrary lighting conditions. We show our neural human assets can achieve extremely high capture performance and are capable of representing fine details such as facial wrinkles and cloth folds. We also validate
UltraStage in single image relighting tasks, training neural networks with virtual relighted data from neural assets and demonstrating realistic rendering improvements over prior arts. UltraStage will be publicly available to the community to stimulate significant future developments in various hu-man modeling and rendering tasks. The dataset is available at https://miaoing.github.io/RNHA. 1.

Introduction
Multi-view stereo (MVS) and photometric stereo (PS) have long served as two complementary workhorses for re-covering 3D objects, human performances, and environ-ments [1,29]. Earlier MVS typically exploits feature match-ing and bundle adjustment to find ray correspondences
[13, 43, 82] and subsequently across varying viewpoints infer their corresponding 3D points [19, 72, 81]. More re-cent neural modeling approaches have emerged as a more effective solution by implicitly encoding both geometry and appearance using neural networks [58]. PS, in contrast, generally assumes a single (fixed) viewpoint and employs appearance variations under illumination changes to infer the surface normal and reflectance (e.g., albedo) [1, 20, 23– 25, 35, 53, 54, 62]. Shape recovery in PS essentially corre-sponds to solving inverse rendering problems [25] where recent approaches also move towards neural representa-tions to encode the photometric information [55, 56, 68, 90].
Most recently, in both MVS and PS neural representations have demonstrated reduced data requirement [48, 88] and increased accuracy [21, 22].
In the context of 3D human scanning, MVS and PS exhibit drastically different benefits and challenges, in synchronization, calibration, reconstruc-tion, etc. For example, for high-quality performance cap-ture, MVS has long relied on synchronized camera arrays but assumes fixed illumination. The most popular and per-haps effective apparatus is the camera dome with tens and even hundreds of cameras [50]. Using classic or neural rep-resentations, such systems can produce geometry with rea-sonable quality. However, ultra-fine details such as facial wrinkles or clothing folds are generally missing due to lim-ited camera resolutions, small camera baselines, calibration errors, etc [87].
In comparison, a typical PS capture system uses a sin-gle camera and hence eliminates the need for cross-camera synchronization and calibrations. Yet the challenges shift to synchronization across the light sources and between the lights and the camera, as well as calibrating the light sources. PS solutions are epitomized by the USC Light-Stage [15, 30, 87] that utilize thousands of light sources to provide controllable illuminations [23,25,53,54,79], with a number of recent extensions [2, 27, 34, 35, 41, 65, 73, 95, 96].
A key benefit of PS is that it can produce very high-quality normal maps significantly surpassing MVS reconstruction.
Further, the appearance variations can be used to infer sur-face materials and conduct high-quality appearance editing and relighting applications [2, 65, 79, 95, 96]. However, re-sults from a single camera cannot fully cover the complete human geometry. Nor have they exploited multi-view re-constructions as useful priors. A unified capture apparatus that combines MVS and PS reconstructions has the poten-tial to achieve unprecedented reconstruction quality, rang-ing from recovering ultra-fine details such as clothes folds and facial wrinkles to supporting free-view relighting in metaverse applications. In particular, the availability of a comprehensive MVS-PS dataset may enable new learning-based approaches for reconstruction [58,66,77,89,92,100], rendering [11, 58, 83, 86, 93, 98], and generation [18, 37–39, 71]. However, there is very little work or dataset available
Figure 2. Data overview. Here we show 5 examples in our dataset.
From front to back are the normal map, albedo map, color gradient illumination observation, and inverse color gradient illumination observation, respectively.
In total, UltraStage Dataset contains more than 2,000 human-centric scenes of a single person or multi-ple people with various gestures, clothes and interactions. For each scene, we provide 8K resolution images captured by 32 surround-ing cameras under 3 illuminations. See Supp. for more examples. to the public due to challenges on multiple fronts for con-structing such imaging systems.
To fill in the gap, we construct PlenOptic Stage Ultra, an emerging hardware system that conducts simultaneous
MVS and PS acquisition of human performances. PlenOp-tic Stage Ultra is built upon an 8-meter radius large-scale capture stage with 22,080 light sources to illuminate a per-former with controllable illuminations, as well as places 32 cameras on the cage to cover the 360◦ surrounding views of the object. We present detailed solutions to obtain accurate camera-camera and camera-light calibration and synchro-nization as well as conduct camera ISP correction and light-ing intensity rectification. For PS capture, PlenOptic Stage
Ultra adopts tailored illuminations [27, 53, 55, 57]: for each human model, we illuminate it with one white light and two directional gradient illuminations. This produces ex-tra high-quality surface normal and anisotropic reflectance largely missing in existing MVS human reconstruction. A direct result of our system is UltraStage, a novel human dataset that provides more than 2,000 human models under different body movements and with sophisticated clothing like frock or cheongsam, to be disseminated to the commu-nity.
We further demonstrate several neural modeling and ren-dering techniques [11, 58, 59, 93, 98, 99] to process the dataset for recovering ultra-fine geometric details, modeling surface reflectance, and supporting relighting applications.
Specifically, we show neural human assets achieve signifi-cantly improved rendering and reconstruction quality over purely MVS or PS based methods [6,19,20,25,53,72], e.g., they can render exquisite details such as facial wrinkles and cloth folds. To use the neural assets for relighting, we adopt albedo and normal estimation networks for in-the-wild hu-man full-body images and we show our dataset greatly en-hances relighting quality over prior art [36, 80]. The dataset as well as the processing tools are expected to stimulate significant future developments such as generation tasks in both shape and appearance.
The USC LightStage [25, 53] is a prime example of PS success used in award-winning films. Multiple generations of the LightStage [27, 53, 55, 57, 94] use gradient illumi-nation to obtain normal maps at high efficiency and use coarse geometry as the boundary for normal integration.
This geometry may originate from MVS [7, 25] or paramet-ric models [52]. UltraStage proposes a novel neural model-ing pipeline for human asset reconstruction, while The Re-lightables [27] uses expensive depth cameras and applies
Poisson reconstruction. One-Light-at-A-Time (OLAT) is an alternative to gradient illumination in PS, requiring ultra-fast camera synchronization with lights. Beyond recover-ing normal maps, OLAT enables photo-realistic relighting directly [65, 79, 95]. Zhang et al. [96] learns a 6D neural light transport function for conducting real-time portrait re-lighting. Total Relighting [65] produces a photo-realistic relighting effect by the detail normal and albedo, and using the Phong model as a prior.
However, PS systems based on either gradient illumi-nation or OLAT tend to be expensive to construct, espe-cially when combined with MVS. By now the only handful
MVS-PS integrated systems are rather small in scale and are not for full-body human captures. Recent neural ap-proaches [40, 91] combine MVS and PS for reconstruction, focusing on static objects under directional lighting. In con-trast, our approach leverages gradient illumination for effi-cient human performance capture and directly utilizes PS normal and albedo to enhance realism. Further, very few datasets have been publicly available whereas we set out to construct such a full-body capture system and produce rich data for the community.
Neural human assets. Our research aims to generate neu-ral representations from MVS-PS human capture results and provide raw data. While initial Neural Radiance Field (NeRF) [58] was under the MVS setting, recent advances integrate illuminations [3, 4, 75, 97]. Srinivasan et al. [75] trained a neural field from multi-view images under known varying illuminations for free-view rendering and relight-ing. Most works focus on human faces, with techniques like rendering human eyes with exceptional realism [47] and modeling light interactions for high-quality portrait re-lighting [78].
Neural human assets have enabled applications like single-image portrait relighting by inferring geometry, albedo, normal, and other attributes from images. To date, in-the-wild human full-body relighting techniques
[32,36,44,80] predominantly rely on synthetic training data, using diffuse or simple parametric surface reflectance mod-els, resulting in reduced realism. Even so, publicly available multi-view, multi-lighting human datasets are extremely scarce, and existing ones [32, 44, 80] contain limited va-rieties in human subjects, movements, clothing, and other factors. UltraStage contains 2,000 human models engag-Figure 3. System Overview. The PlenOptic Stage Ultra is an 8-meter lighting stage composed of 32 sounding cameras and 22,080 controllable light sources. It supports both MVS and PS capture settings, enabling high-quality geometry and material acquisition for large-scale subjects and objects. Note that a human is inside in the middle of the picture. 2.