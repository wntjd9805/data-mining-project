Abstract
Understanding scenes in movies is crucial for a variety of applications such as video moderation, search, and recom-mendation. However, labeling individual scenes is a time-consuming process. In contrast, movie level metadata (e.g., genre, synopsis, etc.) regularly gets produced as part of the film production process, and is therefore significantly more commonly available. In this work, we propose a novel contrastive learning approach that uses movie metadata to learn a general-purpose scene representation. Specifically, we use movie metadata to define a measure of movie sim-ilarity, and use it during contrastive learning to limit our search for positive scene-pairs to only the movies that are considered similar to each other. Our learned scene repre-sentation consistently outperforms existing state-of-the-art methods on a diverse set of tasks evaluated using multiple benchmark datasets. Notably, our learned representation offers an average improvement of 7.9% on the seven classi-fication tasks and 9.7% improvement on the two regression tasks in LVU dataset. Furthermore, using a newly collected movie dataset, we present comparative results of our scene representation on a set of video moderation tasks to demon-strate its generalizability on previously less explored tasks. 1.

Introduction
Automatic understanding of movie scenes is a challenging problem [53] [26] that offers a variety of downstream appli-cations including video moderation, search, and recommen-dation. However, the long-form nature of movies makes la-beling of their scenes a laborious process, which limits the effectiveness of traditional end-to-end supervised learning methods for tasks related to automatic scene understanding.
The general problem of learning from limited labels has been explored from multiple perspectives [51], among which contrastive learning [28] has emerged as a particu-larly promising direction. Specifically, using natural lan-guage supervision to guide contrastive learning [41] has shown impressive results specially for zero-shot image-classification tasks. However, these methods rely on image-text pairs which are hard to collect for long-form videos.
Another important set of methods within the space of con-trastive learning use a pretext task to contrast similar data-points with randomly selected ones [23] [9]. However, most of the standard data-augmentation schemes [23] used to de-fine the pretext tasks for these approaches have been shown to be not as effective for scene understanding [8].
To address these challenges, we propose a novel con-trastive learning approach to find a general-purpose scene representation that is effective for a variety of scene under-standing tasks. Our key intuition is that commonly avail-able movie metadata (e.g., co-watch, genre, synopsis) can be used to effectively guide the process of learning a gen-eralizable scene representation. Specifically, we use such movie metadata to define a measure of movie-similarity, and use it during contrastive learning to limit our search for positive scene-pairs to only the movies that are considered similar to each other. This allows us to find positive scene-pairs that are not only visually similar but also semantically relevant, and can therefore provide us with a much richer set of geometric and thematic data-augmentations compared to previously employed augmentation schemes [23] [8] (see
Figure 1 for illustration). Furthermore, unlike previous contrastive learning approaches that mostly focus on im-ages [23] [9] [13] or shots (§ 3 for definition) [8], our ap-proach builds on the recent developments in vision trans-formers [12] to allow using variable-length multi-shot in-puts. This enables our method to seamlessly incorporate the interplay among multiple shots resulting in a more general-purpose scene representation.
Using a newly collected internal dataset MovieCL30K containing 30, 340 movies to learn our scene representation, we demonstrate the flexibility of our approach to handle both individual shots as well as multi-shot scenes provided as inputs to outperform existing state-of-the-art results on diverse downstream tasks using multiple public benchmark datasets [53] [26] [42]. Furthermore, as an important practi-cal application of long-form video understanding, we apply our scene representation to another newly collected dataset
MCD focused on large-scale video moderation with 44, 581 video clips from 18, 330 movies and TV episodes contain-ing sex, violence, and drug-use activities. We show that learning our general-purpose scene representation is crucial
Figure 1. Approach Overview – We employ commonly available movie metadata (e.g., co-watch, genre, synopsis) to define movie similarities. The figure illustrates a pair of similar movies where movie similarity is defined based on co-watch information, i.e., viewers who watched one movie often watched the second movie as well. Our approach automatically selects thematically similar scenes from such similar movie-pairs and uses them to learn scene-level representations that can be used for a variety of downstream tasks. to recognize such age-appropriate video-content where ex-isting representations learned for short-form action recogni-tion or image classification are significantly less effective. 2.