Abstract
Although vision transformers (ViTs) have shown promis-ing results in various computer vision tasks recently, their high computational cost limits their practical applications.
Previous approaches that prune redundant tokens have demonstrated a good trade-off between performance and computation costs. Nevertheless, errors caused by prun-ing strategies can lead to significant information loss. Our quantitative experiments reveal that the impact of pruned tokens on performance should be noticeable. To address this issue, we propose a novel joint Token Pruning &
Squeezing module (TPS) for compressing vision transform-ers with higher efficiency. Firstly, TPS adopts pruning to get the reserved and pruned subsets. Secondly, TPS squeezes the information of pruned tokens into partial reserved to-kens via the unidirectional nearest-neighbor matching and similarity-based fusing steps. Compared to state-of-the-art methods, our approach outperforms them under all to-ken pruning intensities. Especially while shrinking DeiT-tiny&small computational budgets to 35%, it improves the accuracy by 1%-6% compared with baselines on ImageNet classification. The proposed method can accelerate the throughput of DeiT-small beyond DeiT-tiny, while its accu-racy surpasses DeiT-tiny by 4.78%. Experiments on various transformers demonstrate the effectiveness of our method, while analysis experiments prove our higher robustness to the errors of the token pruning policy. Code is available at https://github.com/megvii-research/TPS-CVPR2023. 1.

Introduction
The transformer architecture has become popular for var-ious natural language processing (NLP) tasks, and its im-proved variants have been adopted for many vision tasks.
Vision transformers (ViTs) [5] leverage the long-range de-*The first two authors contributed equally to this work
†Corresponding author
Figure 1. Comparisons between token pruning paradigm [25] (the 2nd row) and our joint Token Pruning & Squeezing (the 3rd row).
The context information, such as the sod in the examples, is help-ful for prediction but is discarded. Our method remits the informa-tion loss by squeezing the pruned tokens into reserved ones instead of naively dropping them, as indicated by the stacked patches. By this design, we could apply more aggressive token pruning with less performance drop. The example results are from the Ima-geNet1K [4], and we reduce the actual patches grid 14 × 14 to 7 × 7 for visualization clarity. pendencies of self-attention mechanisms to achieve excel-lent performance, often surpassing that of CNNs.
In ad-dition to the vanilla ViT architecture, recent studies [17, 31, 33] have explored hybrid ViT designs incorporating convolution layers and multi-scale architectures. Despite their excellent performance, transformers still require rel-atively high computational budgets. This is due to the quadratic computation and memory costs associated with token length. To address this issue, contemporary ap-proaches [8, 14, 16, 21, 25, 27, 35, 36] propose pruning re-dundant tokens. They trade acceptable performance degra-dation for a more cost-effective model. Knowledge distil-lation [11] and other techniques can further mitigate the re-sulting performance drop.
However, a steep drop in performance is inevitable as pruning tokens further increases because both essential sub-ject and auxiliary context information drop significantly, es-pecially when the number of reserved tokens is closely be-low 10. Aggressive token pruning could lead to incomplete subject and background context loss, causing the wrong pre-diction, as shown in Fig. 1. Specifically, the background tokens containing sod help recognize the input image as a lawn mower rather than a folding chair. Meanwhile, miss-ing subject tokens make the baseball indistinguishable from a rugby ball. To regain adequate information from pruned tokens, EViT [16] and Evo-ViT [35] propose aggregating pruned tokens as one, as shown in Fig. 2 (b). Still, they ne-glect the discrepancy among these tokens, leading to feature collapse and hindering more aggressive token pruning.
Towards more aggressive pruning, we argue that infor-mation in pruned tokens deserves better treatment. We did a toy experiment to answer what accuracy token pruning could achieve if it applied the reversed pruning policy in the first pruned transformer block as Fig. 3 shows. Taking dy-namicViT [25] as a case study, the performance of reversed policy is enough to bring extra accuracy complementary to the original one (denoted by bonus accuracy). Moreover, this phenomenon would become more significant as prun-ing continues (red line in Fig. 3.).
To conserve the information from the pruned tokens, we propose a Joint Token Pruning & Squeezing (TPS) module to accommodate more aggressive compression of
ViTs. TPS module utilizes a feature dispatch mechanism that squeezes essential features from pruned tokens into re-served ones, as shown in Fig. 2 (c). Firstly, based on the scoring result, the TPS module divides input tokens into two complementary subsets: the reserved and pruned sets. Sec-ondly, instead of discarding or collapsing tokens from the pruned set into a single one, we employ a unidirectional nearest-neighbor matching algorithm to dispatch each of them independently to the associated reserved token dubbed as the host token. This design reduces information loss without sacrificing computational efficiency. Subsequently, we apply a similarity-based fusing way to squeeze the fea-tures of matched pruned tokens into corresponding host to-kens while the non-selected reserved tokens remain identi-cal. This design reduces the context information loss while retaining a reasonable computation budget. We can easily achieve hardware-friendly constant shape inference when fixing the cardinality of the reserved token set. Furthermore, we introduce two flexible variants: the inter-block version dTPS and the intra-block version eTPS, which are essen-tially plug-and-play blocks for both vanilla ViTs and hybrid
ViTs.
We conduct extensive experiments on two datasets: Im-ageNet1K [4] and large fine-grained dataset iNaturalist 2019 [29] to prove our efficiency, flexibility, and robustness.
Firstly, experiments under different token pruning settings demonstrate the superior performance of our TPS while op-erating more aggressive compression compared with token pruning [25] and token reorganization [16]; further compar-isons with state-of-the-art transformers [8,13,20,28,31,36, 39, 40] show our promising efficiency. Secondly, we man-ifest the flexibility of our TPS by integrating it into popu-lar ViTs, including both vanilla ViTs and hybrid ViTs. Fi-nally, the evaluations under the random token selection pol-icy confirm the higher robustness of our TPS.
Overall, our contributions are summarized as follows:
• We propose the joint Token Pruning & Squeezing (TPS) and its two variants: dTPS and eTPS, to con-serve the information of discarded tokens and facilitate more aggressive compression of vision transformers.
• Extensive experiments demonstrate our higher perfor-mance compared with prior approaches. Especially while compressing GFLOPs of DeiT-small&tiny to 35%, our TPS outperforms baselines with accuracy improvements of 1%-6%.
• Broadest experiments applying our method to vanilla
ViTs and hybrid ViTs show our flexibility, while the analysis experiments prove that our TPS is more robust than token pruning and token reorganization. 2.