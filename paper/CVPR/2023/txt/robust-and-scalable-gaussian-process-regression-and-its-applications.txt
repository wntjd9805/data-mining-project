Abstract
This paper introduces a robust and scalable Gaussian process regression (GPR) model via variational learning.
This enables the application of Gaussian processes to a wide range of real data, which are often large-scale and contaminated by outliers. Towards this end, we employ a mixture likelihood model where outliers are assumed to be sampled from a uniform distribution. We next derive a variational formulation that jointly infers the mode of data, i.e., inlier or outlier, as well as hyperparameters by maximizing a lower bound of the true log marginal like-lihood. Compared to previous robust GPR, our formula-tion approximates the exact posterior distribution. The in-ducing variable approximation and stochastic variational inference are further introduced to our variational frame-work, extending our model to large-scale data. We ap-ply our model to two challenging real-world applications, namely feature matching and dense gene expression impu-tation. Extensive experiments demonstrate the superiority of our model in terms of robustness and speed. Notably, when matching 4k feature points, its inference is completed in milliseconds with almost no false matches. The code is at github.com/YifanLu2000/Robust-Scalable-GPR. 1.

Introduction
Gaussian processes (GPs) [31] are probably the primary non-parametric method for inference on latent functions.
They have a wide range of applications from biology [3] to computer vision [41]. A commonly used observation model for Gaussian process regression (GPR) is the Normal dis-tribution, which brings great convenience to the inference.
Unfortunately, a well-known limitation of the Gaussian ob-servation model is its sensitivity to outliers in data. As il-lustrated in Fig. 1 (b), a few outliers can drastically destroy the entire posterior regression result. This hinders the real-world applications of GPR for many domains, where out-liers are often inevitable. This paper intends to conquer the
GPR with outlier contaminated data.
The idea of robust regression is not new. Outlier detec-*Corresponding Author
Figure 1. Regression with our model. (a) Perform exact GPR from 100 inliers. (b) When there are only 6 outliers in the data, the exact GPR leads to completely wrong results. (c) By comparison, our model is able to recover the exact posterior even facing 100 outliers. (d) The feature matching result using our model. (e) The dense spatial gene expression imputation result using our model. tion has been extensively and systematically described in
[6,9,10,29]. In the context of GPR, many efforts tried to re-place the Gaussian likelihood with other distributions show-ing heavy-tail behaviors, including Student-t [16,21,28,30],
Laplace [22, 30], Gaussian mixture [8, 22, 27], and data-dependent noise model [17]. The challenge with these non-Gaussian likelihoods lies in the inference, which is analyti-cally intractable. To this end, many approximation schemes have been applied, despite having high computational com-plexity, e.g., Markov Chain Monte Carlo (MCMC) sam-pling and Expectation Propagation (EP) [22].
In this paper, we propose a more effective mixture like-lihood model, where uniform distribution accounts for the outliers and Gaussian for inliers.
In our formulation, the outliers are independent of the GP and do not affect the computation of the posterior GP, thereby allowing to tol-erate more outliers. We next introduce a variational method that jointly determines the modes of data (i.e., inlier or out-lier) as well as hyperparameters by maximizing a lower bound to the marginal likelihood. We highlight that the dif-ference between our variational formulation and pervious methods is that the modes of data now become variational parameters and are obtained by minimizing the Kullback-Leibler (KL) divergence between the variational and the true posterior distribution. Thus, the proposed formulation is less likely to overfit and is able to approximate the exact posterior GP only from inliers, as in Fig. 1 (c).
Inspired by [37], the sparse inducing variable approxi-mation is integrated into our variational framework, which retains the exact GP prior but performs posterior approx-imation, and reduces the time complexity from O(n3) to
O(nm2). By treating the inducing variables as global vari-ables [18], our variational model enjoys the acceleration by
Stochastic Variational Inference (SVI) [20].
It performs stochastic optimization from natural gradient and further decreases the time complexity to O(km2). This provides a guarantee for our model to scale to large-scale data.
We apply our robust GPR model to two real-world ap-plications, say feature matching and dense spatial gene ex-pression imputation, as illustrated in the Figs. 1 (d) and (e).
Extensive experiments demonstrate the superiority of our method on both numerical data and real applications.
To summarize, our contributions include the following. (i) We present a robust Gaussian process regression model, which uses variational learning to approximate the true ex-act posterior. (ii) We leverage inducing variables and SVI to adapt our model to large-scale data. (iii) Two applica-tions of our model are described. Extensive experimental validation demonstrates the superiority of our model. 2.