Abstract
Humans have the ability of recognizing visual semantics in an unlimited granularity, but existing visual recognition algorithms cannot achieve this goal. In this paper, we estab-lish a new paradigm named visual recognition by request (ViRReq1) to bridge the gap. The key lies in decompos-ing visual recognition into atomic tasks named requests and leveraging a knowledge base, a hierarchical and text-based dictionary, to assist task deﬁnition. ViRReq allows for (i) learning complicated whole-part hierarchies from highly incomplete annotations and (ii) inserting new concepts with minimal efforts. We also establish a solid baseline by in-tegrating language-driven recognition into recent seman-tic and instance segmentation methods, and demonstrate its ﬂexible recognition ability on CPP and ADE20K, two datasets with hierarchical whole-part annotations. 1.

Introduction
Visual recognition is one of the fundamental problems in computer vision. In the past decade, visual recognition algorithms have been largely advanced with the availability of large-scale datasets and deep neural networks [8, 13, 18].
Typical examples include the ability of recognizing 10,000s of object classes [6], segmenting objects into parts or even parts of parts [43], using natural language to refer to open-world semantic concepts [31], etc.
Despite the increasing recognition accuracy in standard benchmarks, we raise a critical yet mostly uncovered issue, namely, the unlimited granularity of visual recognition.
As shown in Figure 1, given an image, humans have the ability of recognizing arbitrarily ﬁne-grained contents from it, but existing visual recognition algorithms cannot achieve the same goal. Superﬁcially, this is caused by limited anno-tation budgets so that few training data is available for ﬁne-1We recommend the readers to pronounce ViRReq as /’virik/.
Figure 1. An illustration of unlimited granularity in visual recog-nition. Top: an example image from ADE20K [43] with instance-level and part-level annotations. Middle: more and more incom-plete annotations of instances, parts, and parts of parts. Bottom: as granularity goes ﬁner, higher uncertainty occurs in recogniz-ing the boundary (left) and semantic class (right) of objects and/or parts – here, green, blue and red texts indicate labeled, unlabeled (but deﬁned), and unlabeled (and undeﬁned) objects, respectively. grained and/or long-tailed concepts, but we point out that a more essential reason lies in the conﬂict between granular-ity and recognition certainty – as shown in Figure 1, when annotation granularity goes ﬁner, annotation certainty will inevitably be lower. This motivates us that the granular-ity of visual recognition shall be variable across instances and scenarios. For this purpose, we propose to interpret semantic annotations into smaller units (i.e., requests) and assume that recognition is performed only when it is asked, so that one can freely adjust the granularity according to object size, importance, clearness, etc.
In this paper, we establish a new paradigm named visual recognition by request (ViRReq). We only consider the segmentation task in this paper because it best ﬁts the need of unlimited granularity in the spatial domain. Compared to
the conventional setting, the core idea is to decompose vi-sual recognition into a set of atomic tasks named requests, each of which performs one step of recognition. Speciﬁ-cally, there are two request types, with the ﬁrst type decom-posing an instance into semantic parts and the second type segmenting an instance out from a semantic region. For the
ﬁrst type, a knowledge base is available as a hierarchical text-based dictionary to guide the segmentation procedure.
Compared to the existing paradigms of visual recogni-tion, ViRReq has two clear advantages. First, ViRReq nat-urally has the ability of learning complicated visual con-cepts (e.g., the whole-part hierarchy in ADE20K [43]) from highly incomplete annotations, while conventional methods can encounter several difﬁculties. Second, ViRReq allows a new visual concept (e.g., objects, parts, etc.) to be added by simply updating the knowledge base and annotating a few training examples. We emphasize that the change of knowl-edge base does not impact the use of existing training data as each sample is bound to a speciﬁc version of the knowl-edge base. This property, called data versioning, allows for incremental learning with all historical data available.
To deal with ViRReq, we build a query-based recogni-tion algorithm that (i) extracts visual features from the im-age, (ii) computes text embedding vectors from the request and knowledge base, and (iii) performs interaction between image and text features. The framework is highly modular and the main parts (e.g., feature extractors) can be freely re-placed. We evaluate ViRReq with the proposed recognition algorithm on two datasets, namely, the CPP dataset [5] that extends Cityscapes [3] with part-level annotations and the
ADE20K dataset [43] that offers a multi-level hierarchy of complicated visual concepts. We parse the annotations of each image into a set of requests for training, and deﬁne a new evaluation metric named hierarchical panoptic quality (HPQ) for measuring the segmentation accuracy. Thanks to the ability of learning from incomplete annotations, ViRReq can report part-aware segmentation accuracy on ADE20K, which, to the best of our knowledge, is the ﬁrst ever work to achieve the goal. In addition, ViRReq shows a promis-ing ability of open-domain recognition, including absorbing new visual concepts (e.g., objects, parts, etc.) from a few training samples and understanding anomalous or composi-tional concepts without training data.
In summary, the contribution of this paper is three fold: (i) pointing out the issue of unlimited granularity, (ii) estab-lishing the paradigm named visual recognition by request, and (iii) setting up a solid baseline for this new direction. 2. Preliminaries and Insights 2.1.