Abstract 1.

Introduction
Object models are gradually progressing from predict-ing just category labels to providing detailed descriptions of object instances. This motivates the need for large datasets which go beyond traditional object masks and pro-vide richer annotations such as part masks and attributes.
Hence, we introduce PACO: Parts and Attributes of Com-mon Objects.
It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. We provide 641K part masks an-notated across 260K object boxes, with roughly half of them exhaustively annotated with attributes as well. We design evaluation metrics and provide benchmark results for three tasks on the dataset: part mask segmentation, ob-ject and part attribute prediction and zero-shot instance de-tection. Dataset, models, and code are open-sourced at https://github.com/facebookresearch/paco.
∗ Equal contribution † Work done during internship at Meta AI
Today, tasks requiring fine-grained understanding of objects like open vocabulary detection [8, 14, 20, 51],
GQA [17], and referring expressions [3, 21, 32] are gaining importance besides traditional object detection. Represent-ing objects through category labels is no longer sufficient.
A complete object description requires more fine-grained properties like object parts and their attributes, as shown by the queries in Fig. 1.
Currently, there are no large benchmark datasets for common objects with joint annotation of part masks, ob-ject attributes and part attributes (Fig. 1). Such datasets are found only in specific domains like clothing [18, 47], birds [42] and pedestrian description [25]. Current datasets with part masks for common objects [2, 15, 50] are lim-ited in number of object instances with parts (59K for
ADE20K [2] Tab. 1). On the attributes side, there exists large-scale datasets like Visual Genome [23], VAW [35] and
COCO-attributes [34] that provide object-level attributes. 1
However, none have part-level attribute annotations.
In this work, we enable research for the joint task of ob-ject detection, part segmentation, and attribute recognition, by designing a new dataset: PACO. With video object de-scription becoming more widely studied as well [19], we construct both an image dataset (sourced from LVIS [13]) and a video dataset (sourced from Ego4D [11]) as part of
PACO. Overall, PACO has 641K part masks annotated in 77K images for 260K object instances across 75 object classes and 456 object-specific part classes. It has an order of magnitude more objects with parts, compared to recently introduced PartImageNet dataset [15]. PACO further pro-vides annotations for 55 different attributes for both objects and parts. We conducted user studies and multi-round man-ual curation to identify high-quality vocabulary of parts and attributes.
Along with the dataset, we provide three associated benchmark tasks to help the community evaluate its progress over time. These tasks include: a) part segmen-tation, b) attribute detection for objects and object-parts and c) zero-shot instance detection with part/attribute queries.
The first two tasks are aimed at benchmarking stand alone capabilities of part and attribute understanding. The third task evaluates models directly for a downstream task.
While building the dataset and benchmarks, we navigate some key design choices: (a) Should we evaluate parts and attributes conditioned on the object or independent of the objects (eg: evaluating “leg” vs. “dog-leg”, “red” vs. “red cup”)? (b) How do we keep annotation workload limited without compromising fair benchmarking?
To answer the first question, we observed that the same semantic part can visually manifest very differently in different objects (“dog-leg” vs “chair-leg”). This makes the parts of different objects virtually independent classes, prompting us to evaluate them separately. This also forces models to not just identify parts or attributes independently, but predict objects, parts and attributes jointly. This is more useful for downstream applications.
Next, to keep annotation costs limited, we can construct a federated dataset as suggested in LVIS [13]. For object de-tection, LVIS showed that this enables fair evaluation with-out needing exhaustive annotations for every image. How-ever, this poses a specific challenge in our setup. Object detection requires every region to be associated with only one label (object category), while we require multiple la-bels: object, part and attribute jointly. This subtle but im-portant difference, makes it non-trivial to extend definition and implementation of metrics from LVIS to our setup. We provide a nuanced treatment of missing labels at different levels (missing attribute labels vs. missing part and attribute labels) to handle this.
Our design choices allow us to use popular detection metrics: Average Precision and Average Recall for all our tasks. To facilitate calibration of future research models, we also provide benchmark numbers for all tasks using simple variants of mask R-CNN [16] and ViT-det [28]. 1.1.