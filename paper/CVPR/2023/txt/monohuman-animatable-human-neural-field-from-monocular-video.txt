Abstract
Animating virtual avatars with free-view control is cru-cial for various applications like virtual reality and digi-tal entertainment. Previous studies have attempted to uti-lize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation net-work into the NeRF to further model the dynamics of the hu-man neural field for animating vivid human motions. How-ever, such pipelines either rely on pose-dependent repre-sentations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically.
In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under ar-bitrary novel poses. Our key insight is to model the de-formation field with bi-directional constraints and explic-itly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation mod-ule, which creates a pose-independent generalizable defor-mation field by disentangling backward and forward defor-mation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a Forward
Correspondence Search module, which queries the corre-spondence feature of keyframes to guide the rendering net-work. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose set-tings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods. 1.

Introduction
Rendering a free-viewpoint photo-realistic view synthe-sis of a digital avatar with explicit pose control is an im-portant task that will bring benefits to AR/VR applica-tions, virtual try-on, movie production, telepresence, etc.
However, previous methods [32, 33, 58] usually require carefully-collected multi-view videos with complicated sys-tems and controlled studios, which limits the usage in gen-eral and personalized scenarios applications. Therefore, though challenging, it has a significant application value to directly recover and animate the digital avatar from a monocular video.
Previous rendering methods [33] can synthesize realis-tic novel view images of the human body, but hard to ani-mate the avatar in unseen poses. To address this, some re-cent methods deform the neural radiance fields (NeRF [27])
[32, 49] to learn a backward skinning weight of paramet-ric models depending on the pose or individual frame in-dex. They can animate the recovered human in novel poses with small variations to the training set. However, as the
blending weights are pose-dependent, these methods usu-ally over-fit to the seen poses of training data, therefore, lack of generalizability [49]. Notably, one category of ap-proaches [5, 21] solves this problem by learning a pose-independent forward blend weight in canonical space and using the root-finding algorithm to search the backward cor-respondence. However, the root-finding algorithm is time-consuming. [4] proposes to add a forward mapping network to help the learning of backward mapping, though with con-sistent constraint, their backward warping weights still are frame dependent. Some other works [16,32,42] leverage the blend weight from the template model like SMPL [25]. The accuracy of their deformation heavily hinges on the tem-plate model and always fails in the cloth parts as SMPL does not model it. How to learn an accurate and generaliz-able deformation field is still an open problem.
To reconstruct and animate a photo-realistic avatar that can generalize to unseen poses from the monocular video, we conclude three key observations to achieve generaliz-ability from recent studies: 1) The deformation weight field should be defined in canonical space and as pose-independent as possible [5, 48, 49]; 2) An ideal deformation field should unify forward and backward deformation to al-leviate ambiguous correspondence on novel poses; 3) Direct appearance reference from input observation helps improve the fidelity of rendering.
We propose MonoHuman, a novel framework that en-joys the above strengths to reconstruct an animatable digi-tal avatar from only monocular video. Concretely, we show how to learn more correct and general deformation from such limited video data and how to render realistic results from deformed points in canonical space. We first intro-duce a novel Shared Bidirectional Deformation module to graft into the neural radiance fields, which disentangles the backward and forward deformation into one shared skeletal motion and two separate residual non-rigid motions. The shared motion basis encourages the network to learn more general rigid transformation weights. The separate residual non-rigid motions guarantee the expressiveness of the accu-rate recovery of pose-dependent deformation. With the ac-curate learned deformation, we further build an observation bank consisting of information from sparse keyframes, and present a Forward Correspondence Search module to search observation correspondence from the bank which helps cre-ate high fidelity and natural human appearance, especially the invisible part in the current frame. With the above de-signs, our framework can synthesize a human at any view-point and any pose with natural shape and appearance.
To summarize, our main contributions are three-fold:
• We present a new approach MonoHuman that can syn-thesize the free viewpoint and novel pose sequences of a performer with explicit pose control, only requiring a monocular video as supervision.
• We propose the Shared Bidirectional Deformation module to achieve generalizable consistent forward and backward deformation, and the Forward Search
Correspondence module to query correspondence ap-pearance features to guide the rendering step.
• Extensive experiments demonstrate that our frame-work MonoHuman renders high-fidelity results and outperforms state-of-the-art methods. 2.