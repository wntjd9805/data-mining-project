Abstract
This paper tries to address a fundamental question in point cloud self-supervised learning: what is a good sig-nal we should leverage to learn features from point clouds without annotations? To answer that, we introduce a point cloud representation learning framework, based on geo-metric feature reconstruction. In contrast to recent papers that directly adopt masked autoencoder (MAE) and only predict original coordinates or occupancy from masked point clouds, our method revisits differences between im-ages and point clouds and identifies three self-supervised learning objectives peculiar to point clouds, namely cen-troid prediction, normal estimation, and curvature predic-tion. Combined, these three objectives yield an nontrivial self-supervised learning task and mutually facilitate mod-els to better reason fine-grained geometry of point clouds.
Our pipeline is conceptually simple and it consists of two major steps: first, it randomly masks out groups of points, followed by a Transformer-based point cloud encoder; sec-ond, a lightweight Transformer decoder predicts centroid, normal, and curvature for points in each voxel. We transfer the pre-trained Transformer encoder to a downstream pe-ception model. On the nuScene Datset, our model achieves 3.38 mAP improvment for object detection, 2.1 mIoU gain for segmentation, and 1.7 AMOTA gain for multi-object tracking. We also conduct experiments on the Waymo Open
Dataset and achieve significant performance improvements over baselines as well. 1 1.

Introduction
While object detection and segmentation from LiDAR point clouds have achieved significant progress, these mod-els usually demand a large amount of 3D annotations that are hard to acquire. To alleviate this issue, recent works ex-plore learning representations from unlabeled point clouds,
*Corresponding to: hangzhao@mail.tsinghua.edu.cn 1Our code is available at https://github.com/Tsinghua-MARS-Lab/GeoMAE.
Figure 1. Pixel value regression has been proved effective in masked autoencoder pre-training for images. We find this practice ineffective in point cloud pre-training and propose a set of geome-try aware prediction targets. such as contrastive learning [20, 46, 53], and mask model-ing [32, 51]. Similar to image-based representation learn-ing settings, these representations are transferred to down-stream tasks for weight initialization. However, the exist-ing self-supervised pretext tasks do not bring adequate im-provements to the downstream tasks as expected.
Contrastive learning based methods typically encode dif-ferent ‘views’ (potentially with data augmentation) of point clouds into feature space. They bring features of the same point cloud closer and make features of different point clouds ‘repel’ each other. Other recent works use masked modeling to learn point cloud features through self re-construction [32, 51]. That is, randomly sparsified point clouds are encoded by point cloud feature extractors, fol-lowed by a reconstruction module to predict original point clouds. These methods, when applied to point clouds, ig-nore the fundamental difference of point clouds from im-ages – point clouds provide scene geometry while images provide brightness. As shown in Figure 1, this modality disparity hampers direct use of methods developed in the image domain for point cloud domain, and thus calls for novel self-supervised objectives dedicated to point clouds.
Inspired by modeling and computational techniques in geometry processing, we introduce a self-supervised learn-ing framework dedicated to point clouds. Most importantly, we design a series of prediction targets which describe the fine-grained geometric features of the point clouds. These
geometric feature prediction tasks jointly drive models to recognize different shapes and areas of scenes. Concretely, our method starts with a point cloud voxelizer, followed by a feature encoder to transform each voxel into a feature token. These feature tokens are randomly dropped based on a pre-defined mask ratio. Similar to the original MAE work [18], visible tokens are encoded by a Transformer en-coder. Then a Transformer decoder reconstructs the fea-tures of the original voxelized point clouds. Finally, our model predicts point statistics and surface properties in par-allel branches.
We conduct experiments on a diverse set of large-scale point cloud datasets including nuScenes [4] and
Waymo [39]. Our setting consists of a self-supervised pre-training stage and a downstream task stage (3D detection, 3D tracking, segmentation), where they share the same point cloud backbone. Our results show that even with-out additional unlabeled point clouds, self-supervised pre-training with objectives proposed by this paper can signif-icantly boost the performance of 3D object detection. To summarize, our contributions are:
• We introduce geometry aware self-supervised objec-tives for point clouds pre-training. Our method lever-ages fine-grained point statistics and surface properties to enable effective representation learning.
• With our novel learning objectives, we achieve state-of-the-art performance compared to previous 3D self-supervised learning methods on a variety of down-stream tasks including 3D object detection, 3D/BEV segmentation, and 3D multi-object tracking.
• We conduct comprehensive ablation studies to under-stand the effectiveness of each module and learning objective in our approach. 2.