Abstract
Tracking by natural language specification aims to lo-cate the referred target in a sequence based on the nat-ural language description. Existing algorithms solve this issue in two steps, visual grounding and tracking, and ac-cordingly deploy the separated grounding model and track-ing model to implement these two steps, respectively. Such a separated framework overlooks the link between visual grounding and tracking, which is that the natural language descriptions provide global semantic cues for localizing the target for both two steps. Besides, the separated frame-work can hardly be trained end-to-end. To handle these issues, we propose a joint visual grounding and tracking framework, which reformulates grounding and tracking as a unified task: localizing the referred target based on the given visual-language references. Specifically, we propose a multi-source relation modeling module to effectively build the relation between the visual-language references and the test image.
In addition, we design a temporal modeling module to provide a temporal clue with the guidance of the global semantic information for our model, which ef-fectively improves the adaptability to the appearance vari-ations of the target. Extensive experimental results on
TNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that our method performs favorably against state-of-the-art al-gorithms for both tracking and grounding. Code is avail-able at https://github.com/lizhou-cs/JointNLT.
Figure 1. Illustration of two different frameworks for tracking by natural language specification. (a) The separated visual ground-ing and tracking framework, which consists of two independent models for visual grounding and tracking, respectively. (b) The proposed joint visual grounding and tracking framework, which employs a single model for both visual grounding and tracking. 1.

Introduction
Tracking by natural language specification [18] is a task aiming to locate the target in every frame of a sequence ac-cording to the state specified by the natural language. Com-pared with the classical tracking task [25, 33, 34, 41] using a bounding box to specify the target of interest, tracking by natural language specification provides a novel human-machine interaction manner for visual tracking. In addition, the natural language specification also has two advantages
âˆ—Corresponding authors: Zikun Zhou and Zhenyu He. for the tracking task compared to the bounding box specifi-cation. First, the bounding box only provides a static repre-sentation of the target state, while the natural language can describe the variation of the target for the long term. Sec-ond, the bounding box contains no direct semantics about the target and even results in ambiguity [32], but the natural language can provide clear semantics of the target used for assisting the tracker to recognize the target. In spite of the above merits, tracking by natural language specification has not been fully explored.
Most existing solutions [17,18,32,39] for this task could
be generally divided into two steps: (1) localizing the target of interest according to the natural language description in the first frame, i.e., visual grounding; (2) tracking the local-ized target in the subsequent frames based on the target state predicted in the first frame, i.e., visual tracking. Accord-ingly, many algorithms [17, 32, 39] are designed to incorpo-rate a grounding model and a tracking model, as shown in
Figure 1(a). Herein the grounding model performs relation modeling between the language and vision signal to localize the target, while the tracking model performs relation mod-eling between the template and search region to localize the target. The drawback of this framework is that the ground-ing model and the tracking model are two separate parts and work independently, ignoring the connections between the two steps. Besides, many of them [17, 32, 39] choose to adopt the off-the-shelf grounding model [38] or tracking model [16] to construct their framework, which means that the overall framework cannot be trained end-to-end.
The tracking model in most existing algorithms [17, 32, 39] predicts the target state only based on the template, overlooking the natural language description. By contrast, the tracking mechanism that considers both the target tem-plate and the natural language for predicting the target state has proven to have great potential [12, 13, 18, 31]. Such a tracking mechanism requires the tracking model to own the ability to simultaneously model the vision-language re-lation and the template-search region relation. Inspired by this tracking mechanism, we come up with the idea to build a joint relation modeling model to accomplish the above-mentioned two-step pipeline. Herein a joint relation model-ing model can naturally connect visual grounding and track-ing together and also can be trained end-to-end.
To this end, we propose a joint visual grounding and tracking framework for tracking by natural language spec-ification, as shown in Figure 1(b). Specifically, we look at these two tasks from a unified perspective and reformu-late them as a unified one: localizing the referred target according to the given visual-language references. For vi-sual grounding, the reference information is the natural lan-guage, while for visual tracking, the reference information is the natural language and historical target patch (usually called template). Thus, the crux of this unified task is to model the multi-source relations between the input refer-ences and the test image, which involve the cross-modality (visual and language) relation and the cross-time (histori-cal target patch and current search image) relation. To deal with this issue, we introduce a transformer-based multi-source relation modeling module, which is flexible enough to accommodate the different references for grounding and tracking, to model the above relations effectively. It allows our method to switch between grounding and tracking ac-cording to different inputs.
In addition, to improve the adaptability to the variations of the target, we resort to the historical prediction as they provide the temporal clue about the recent target appearance and propose a temporal modeling module to achieve this purpose. Considering that the natural language specification contains the global semantic information of the target, we use it as guidance to assist the temporal modeling module to focus on the target region instead of the noise in the previous prediction results.
To conclude, we make the following contributions: (1) we propose a joint visual grounding and tracking frame-work for tracking by natural language specification, which unifies tracking and grounding as a unified task and can ac-commodate the different references of the grounding and tracking processes; (2) we propose a semantics-guided tem-poral modeling module to provide a temporal clue based on historical predictions for our joint model, which improves the adaptability of our method to the appearance varia-tions of the target; (3) we achieve favorable performance against state-of-the-art algorithms on three natural language tracking datasets and one visual grounding dataset, which demonstrates the effectiveness of our approach. 2.