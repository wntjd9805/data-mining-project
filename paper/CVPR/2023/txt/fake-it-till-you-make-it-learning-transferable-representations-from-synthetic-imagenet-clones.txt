Abstract
Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification.
Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data for transfer.
Project page: https://europe.naverlabs.com/imagenet-sd 1.

Introduction
The rise of (shallow) machine learning [15, 85] and later deep learning [27, 46, 80] has entirely changed the landscape of computer vision research over the past few decades, shift-ing some of the focus from methods to the training data itself. Datasets, initially of hundreds of images and dozens of classes [22, 23], have grown in size and complexity, and started becoming contributions in their own right. They have been fueling the progress of computer vision as much as, if not more than, the methods themselves. ImageNet [17], and mainly its ImageNet-1K [71] subset of about 1 million an-notated images, has impacted the field in an unprecedented way. Yet, curating and annotating such a dataset comes at a very high money and labor cost.
The last couple of years have seen the rise of large and generic models, trained on data which is less curated but
Figure 1. ImageNet-1K vs ImageNet-1K-SD. The blue polygon shows the performance of a model trained on ImageNet-1K. The red polygon depicts the performance of one trained on ImageNet-1K-SD, i.e., only on synthetic data generated with Stable Diffusion [70] using the class names of ImageNet-1K. We report top-5 accuracy for ImageNet test sets, and average top-1 for transfer tasks. orders of magnitude larger. Those proved to be easily ap-plicable, either directly, or combined with a tailored model, to a wide range of computer vision transfer tasks [38, 42, 65].
They have also been used beyond prediction tasks, e.g., for text-conditioned image generation. Models such as DALL-E [66] or Stable Diffusion [70] have demonstrated impressive image generation ability. They produce fairly realistic syn-thetic images and exhibit a high degree of compositionality.
Such generative models are trained on billion-scale datasets [76] composed of noisy image-text pairs scraped from the internet. Although training such models is out of reach for most institutions, a few of them have been made available to the community. Given the remarkable ability of these generative models, it is only natural to ask provocative questions such as: Is there still a need for real images when training image prediction models?
In this paper we explore this question through one of the most iconic computer vision datasets, ImageNet [17]. We study to which extent this dataset can be entirely replaced
by synthetic images when learning deep models. For this, we assume that we are provided with a set of classes, and the Stable Diffusion [70] model a generator that can produce realistic images from a textual prompt.
Our task is to learn an image classification model from scratch using a dataset composed only of synthetic images.
We then evaluate the performance of this model on several datasets. First and foremost, we measure how well models and classifiers trained only on synthetic images recognize the training classes in real images from the standard ImageNet validation set. Then, we evaluate them on common datasets that test their resilience to domain shifts or adversarial ex-amples, still for the ImageNet training classes. Finally, we consider several transfer learning scenarios where we mea-sure the generalization performance of our models to novel classes. Fig. 1 summarizes the main results by comparing models trained on two equally sized set of images from the same set of classes, one real and one synthetic, on a number of these tasks. The gap is surprisingly narrow, especially for some of these scenarios.
To summarize, our contributions are threefold. First, we leverage Stable Diffusion [70] and generate synthetic
ImageNet clones, i.e., datasets with synthetic images for the
ImageNet classes, using class names as prompts. We analyse the generated images, highlight important issues, and propose class-agnostic alterations to the basic prompt that reduce semantic issues and increase diversity. Second, we train classification models using different ImageNet clones and show that they can achieve 91.7% and 70.3% top-5 accuracy on ImageNet-100 and ImageNet-1K respectively.
Finally, we evaluate the generalization capacity of our models. We show that their performance gap with models trained on real images is reduced when testing for resilience to domain shifts or adversarial examples. Moreover, we show that our models perform on par with models trained conventionally when testing on 15 transfer datasets. 2.