Abstract
Image-text matching, a bridge connecting image and language, is an important task, which generally learns a holistic cross-modal embedding to achieve a high-quality semantic alignment between the two modalities. How-ever, previous studies only focus on capturing fragment-level relation within a sample from a particular modal-ity, e.g., salient regions in an image or text words in a sentence, where they usually pay less attention to captur-ing instance-level interactions among samples and modal-ities, e.g., multiple images and texts.
In this paper, we argue that sample relations could help learn subtle dif-ferences for hard negative instances, and thus transfer shared knowledge for infrequent samples should be promis-ing in obtaining better holistic embeddings. Therefore, we propose a novel hierarchical relation modeling frame-work (HREM), which explicitly capture both fragment-and instance-level relations to learn discriminative and ro-bust cross-modal embeddings. Extensive experiments on
Flickr30K and MS-COCO show our proposed method out-performs the state-of-the-art ones by 4%-10% in terms of rSum. Our code is available at https://github.com/
CrossmodalGroup/HREM . 1.

Introduction
Image-text matching bridges the semantic gap between visual and textual modalities and is a fundamental task for various multi-modal learning applications, such as cross-modal retrieval [22] and text-to-image synthesis [17]. The critical challenge is accurately and efficiently learning cross-modal embeddings and their similarities for images and texts, to achieve a high-quality semantic alignment.
In general, existing image-text matching methods can be classified into two paradigms. The first embedding-based matching [4, 10, 20, 35] separately encodes the whole im-ages and texts into a holistic embedding space, then globally
*Corresponding author.
Figure 1. Illustration of our motivation. Sample relation modeling improves the holistic representation of cross-modal learning. Col-ors and shapes indicate different modalities and image-text pairs, respectively. Orange elements mark effective interactions: (a) The pipeline of the previous and our work, we add the cross-modal re-lation interaction between samples. (b) For the identical theme of
“surfer with surfboard”, specific behaviors exist subtle differences, like “hold/squat/ride on the surfboard” and “stare/break/wipe out the wave”. Our method distinguishes these hard negative samples (c) For similar themes under “man from semantic ambiguities. play a ball”, corresponding behaviors usually are semantic simi-lar, like ”play the hockey/cricket/polo” all need to “hit the ball” with “sticks/bats”. Our method improves learning embeddings on these infrequent samples with semantic scarcities for themselves. measures the semantic similarity of the two modalities. The second score-based matching [3,7,19,27] applies the cross-modal interaction between visual and textual local features, then learns a cumulative similarity score.
Recently, embedding-based methods have served as the mainstream solution owing to both accuracy and efficiency in image-text matching, which contains two steps as shown in Fig. 1 (a): (1) Capturing the intra-modal relation be-tween visual fragments (e.g., regional features) or textual
fragments (e.g., word features) independently, then enhanc-ing the semantic representation of local features. (2) Ag-gregating relation-enhanced local features of two modalities into the holistic embedding space. For the first step, most use the graph neural network [5, 20, 21] or attention mod-ule [35, 45, 46] to capture semantic relations and enhance the local features of two modalities, respectively. Some work further exploits the spatial relation [45] for visual re-gions or grammatical relation [28] for textual words. For the second step, they design pooling functions [4] or se-quence encoders [21] to aggregate local features and get holistic embeddings. Existing embedding-based methods follow the principle of separately encoding images and texts by two branches as Fig. 1 (a). In each branch, these meth-ods only focus on the fragment-level relation modeling and local features interaction within one sample, e.g., the region features inside one image (or the word features inside one text). In this way, the instance-level relation modeling and global embeddings interaction among different samples and modalities, e.g., holistic embeddings of multiple images and texts, are entirely overlooked.
Consequently, existing embedding-based methods di-rectly use global embeddings to compute loss function, e.g., hard negative triplet loss [10] on random mini-batch, which is insufficient to exploit the manifold structure of holistic embedding space [39]. First, they fail to learn subtle seman-tic discrepancies among different samples (e.g., similar be-haviors with an identical theme as shown in Fig. 1 (b)), then can not distinguish hard negative samples with semantic ambiguities because of the heterogeneity of visual and tex-tual semantics. Second, they are unable to transfer shared knowledge from diverse samples (e.g., different samples that contain similar behaviors with similar themes as shown in Fig. 1 (c)), then can not effectively learn on these infre-quent samples with semantic scarcities. Therefore, it is ex-pected that a framework should precisely capture the sample relationship to learn better cross-modal embeddings, while does not break the principle of embedding-based methods, i.e., independently encodes embeddings without modality interaction at the inference stage.
In doing so, we propose a Hierarchical RElation
Modeling framework (HREM) that, for the first time to our knowledge, explicitly captures both fragment-level and instance-level relations to learn holistic embeddings jointly.
Therefore, HREM learns not only contextual semantics among intra-modal fragments to enhance local features, but also the associated semantics among inter-modal instances to distinguish hard negative samples and improve learning on infrequent samples. As illustrated in Fig. 1 (a) and
Fig. 2, we propose a novel step (i.e., the “stage-three”) to exactly capture the semantic relationship of cross-modal samples. First, we propose a novel cross-embedding as-sociation graph, which explicitly identifies the connection relation and learns the relevance relation between batch samples with fragment-level semantic matching. Next, we propose two relation interaction mechanisms, which ex-plore inter-modal and intra-modal relations synchronously or asynchronously with our improved attention modules to obtain enhanced embeddings. Consequently, HREM only needs to capture the instance-level relation for training, then encode multi-modal embeddings independently at the in-ference stage, to achieve high accuracy and efficiency for image-text matching.
To summarize, the major contributions are as follows: (1) We propose a hierarchical relation modeling framework (HREM) for image-text matching. To the best of our knowl-edge, this is the first work that explicitly captures both fragment-level relations within modality and instance-level relations across modalities. (2) We propose a novel cross-embedding association graph by identifying the connection relation and learning the relevance relation. (3) We propose two relation interaction mechanisms to learn the relation-enhanced embeddings. (4) HREM outperforms all state-of-the-art methods for image-text retrieval on two widely used benchmarks, Flickr30K and MS-COCO, by 4%-10% rSum. 2.