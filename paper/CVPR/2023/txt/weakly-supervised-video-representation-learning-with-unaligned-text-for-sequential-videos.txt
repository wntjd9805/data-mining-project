Abstract
Sequential video understanding, as an emerging video understanding task, has driven lots of researchers’ atten-tion because of its goal-oriented nature. This paper studies weakly supervised sequential video understanding where the accurate time-stamp level text-video alignment is not provided. We solve this task by borrowing ideas from CLIP.
Specifically, we use a transformer to aggregate frame-level features for video representation and use a pre-trained text encoder to encode the texts corresponding to each action and the whole video, respectively. To model the corre-spondence between text and video, we propose a multi-ple granularity loss, where the video-paragraph contrastive loss enforces matching between the whole video and the complete script, and a fine-grained frame-sentence con-trastive loss enforces the matching between each action and its description. As the frame-sentence correspondence is not available, we propose to use the fact that video ac-tions happen sequentially in the temporal domain to gen-erate pseudo frame-sentence correspondence and super-vise the network training with the pseudo labels. Exten-sive experiments on video sequence verification and text-to-video matching show that our method outperforms base-lines by a large margin, which validates the effectiveness of our proposed approach. Code is available at https:
//github.com/svip-lab/WeakSVR. 1.

Introduction
A strong artificial intelligence (AI) system is expected to be able to learn knowledge from the open world in an em-bodied manner such that amounts of goal-oriented tasks are designed for reinforcement learning in the environment. In the area of video understanding, a great deal of pioneering
*Equal Contribution.
†Corresponding Author. work in video classification [55], action localization [53], and action segmentation [25] has been explored, laying the foundation for video understanding. Beyond these typi-cal video understanding tasks, sequential videos (such as
Fig. 1) that usually describe how to perform a task in a certain sequence of procedures can be regarded as a goal-oriented task. Solving this task is extremely promising for guiding intelligence to learn a task like humans. It makes performing sequential video representations a potentially critical part of the road to strong AI.
Some efforts have been made for video representation learning for sequential videos. e.g., [1, 17] learns a video representation in an instructive video. However, these meth-ods rely heavily on the annotations of temporal boundaries, i.e., the timestamps of sequential actions, which are usu-ally difficult to be obtained due to the time-consuming hu-man labeling in practice. A common but often overlooked scenario is that sequential videos usually occur accompa-nied with audio or text narrations, which show consistent steps with explanations. The rich text information describes the corresponding procedure in detail as shown in Fig. 1, but they are usually not aligned with videos. Therefore, a i.e., whether it is possible to directly learn question arises, the video representation with unaligned text and video in a weakly supervised manner.
With the popularity of visual-language tasks, multi-modal learning has attracted growing attention and has been explored in a variety of areas, e.g., image classification
[5, 49], object detection [41, 61], and video understanding
[59]. One of the most representative works is CLIP [42].
It has shown the potential of learning a powerful seman-tic representation from natural language supervision with a contrastive learning loss and the strong zero-shot gener-alization on the downstream tasks, such as text-video re-trieval [47, 57], action segmentation [63], multiple-choice videoQA [16, 57] and action step localization [4]. Video-CLIP [58] presents a contrastive learning approach to pre-Figure 1. Sequential Video. The samples come from CSV dataset. They describe two types of step schedule to accomplish the task of
”fix the test tube on the iron stand with iron clamp”. The upper process the step ”fix on the iron stand ” before the steps ”take up the test tube” and ”screw the iron clamp”. Diversely, the lower make the steps of ”take up the test tube” and ”screw the iron clamp” before the step
”fix on the iron stand ”. It can be seen that the order, time span and temporal location of sub-actions to accomplish the task are apparently different. train a unified model with video-text pairs, and [1] pro-poses a unified fully and timestamp-supervised framework for multi-model action segmentation. This provides us with an alternative for weakly supervised video representation learning. However, all these previous works are equipped with aligned texts and video frames [1], which is not exis-tent in our weakly supervised setting. Thus, it is intractable to directly adapt the existing multi-modal video representa-tion models to our task.
To overcome the unalignment issue between text and video and learn a satisfactory video representation, we propose a weakly supervised video representation learn-ing pipeline and introduce a multiple granularity contrastive loss to constrain the model, which takes full account of the pseudo temporal alignment between frames and sentences.
To be specific, we first extract video and text features from a CLIP-based vision-language model, and a global con-trastive loss is designed to constrain the complete video-It constrains that a video will be paragraph alignment. closer to the sequence of the texts describing it while far away from the rest of the texts, and vice versa. Secondly, we introduce a fine-grained contrastive learning loss, which en-courages the frame sequences of representations to be more similar to the neighbor sentence representations than the re-mote sentences in the same paragraph. The intuition be-hind this constraint comes from a basic idea: if the sj is the corresponding sentence for frame hi, the correspond-ing sentence for frame hi+1 is never before the sj in se-quence. Specifically, we take the probabilistic sample from the sentence-frame similarity metric. And we propose to ap-ply the differentiable Gumbel-Softmax [22] tricks to gener-ate predictions and propose three kinds of methods to gener-ate the pseudo-labels that are based on the temporal relation of sentences in the temporal domain: 1) maximum-index sorting; 2) Viterbi algorithm [15]; 3) splitting. Finally, we calculate the Info-NCE contrastive loss based on the pseudo labels in order to guide the network to focus on the fine-grained action matching in sequential videos.
To evaluate the effectiveness of our weakly supervised video representation method, we conduct extensive experi-ments on two downstream tasks: video verification in pro-cedures and text-to-video matching. The results of experi-ments show that our approach outperforms other baselines by a significant margin and also demonstrates the great gen-eralization of our model.
We summarize our contributions in three folds:
• We propose a novel weakly supervised video repre-sentation learning pipeline with unaligned text for se-quential videos, which can learn powerful and seman-tic video-text representations.
• We design multiple granularity contrastive learning loss, including coarse-grained loss and fine-grained loss. Notably, we propose a novel method to imple-ment the temporal alignment between frames and sen-tences.
• Our model also shows strong generalization ability to downstream tasks, such as video sequence verification for procedures in videos and text-to-video matching. 2.