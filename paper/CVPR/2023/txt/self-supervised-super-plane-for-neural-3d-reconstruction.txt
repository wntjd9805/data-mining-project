Abstract
Neural implicit surface representation methods show impressive reconstruction results but struggle to handle texture-less planar regions that widely exist in indoor scenes. Existing approaches addressing this leverage image prior that requires assistive networks trained with large-scale annotated datasets.
In this work, we introduce a self-supervised super-plane constraint by exploring the free geometry cues from the predicted surface, which can fur-ther regularize the reconstruction of plane regions without any other ground truth annotations. Specifically, we in-troduce an iterative training scheme, where (i) grouping of pixels to formulate a super-plane (analogous to super-pixels), and (ii) optimizing of the scene reconstruction net-work via a super-plane constraint, are progressively con-ducted. We demonstrate that the model trained with super-planes surprisingly outperforms the one using conventional annotated planes, as individual super-plane statistically oc-cupies a larger area and leads to more stable training. Ex-tensive experiments show that our self-supervised super-plane constraint significantly improves 3D reconstruction quality even better than using ground truth plane segmen-tation. Additionally, the plane reconstruction results from our model can be used for auto-labeling for other vision tasks. The code and models are available at https:
//github.com/botaoye/S3PRecon. 1.

Introduction
Reconstructing 3D scenes from multi-view RGB images is an important but challenging task in computer vision, which has numerous applications in autonomous driving, virtual reality, robotics, etc. Existing matching-based meth-ods [30, 31, 50] estimate per-view depth maps, which are then fused to construct 3D representation. However, they do not recover the depth of the scene in texture-less planar ar-eas well (such as walls, floors, and other solid color planes), which are abundant, especially for indoor scenes. Recent data-driven methods [22, 26, 34, 38] alleviate this problem to some extent by automatically learning geometric priors from large-scale training data, but they either require nu-Figure 1. Reconstruction and Plane Segmentation Results. Our method can reconstruct smooth and complete planar regions by employing the super-plane constraint and further obtain plane seg-mentation in an unsupervised manner. merous and expensive 3D supervision (e.g., depth [22, 38], normal [14], etc.) or lack fine-grain details [26, 34].
Recently, neural implicit representations have gained much attention and shown impressive reconstruction results without 3D supervision [39, 46, 47]. However, these meth-ods purely rely on the photo-consistency to construct the scene, which leads to texture-geometry ambiguities since there are different plausible interpretations to satisfy this objective. Several approaches address this problem by in-troducing additional priors obtained from trained models that can be considered as assistant networks. For instance,
ManhattanSDF [9] introduces the Manhattan assumption on the floor and wall regions, which are predicted by a seman-tic segmentation model. NeuRIS [37] and MonoSDF [48] adopt additional normal supervision acquired from normal prediction networks trained on large-scale labeled datasets.
Although these methods can regularize the reconstruction process on indoor scenes, they all rely on large-scale an-notated 2D or 3D datasets.
In addition, these pretrained geometric prediction networks are sensitive to different scenes and not friendly across different domains or datasets.
For example, MonoSDF [48] reports that different normal prediction networks significantly affect the reconstruction quality. Thus, a natural question arises: can we improve the
RGB-based reconstruction results on texture-less regions without any implicit supervision from assistant networks?
In this work, we propose a novel neural 3D reconstruc-tion framework with the Self-Supervised Super-Plane con-straint, called S3P, which does not require any labeled data or assistant networks. The intuition behind our approach is simple: the constructed results provide a free geometry cue, i.e., surface normal, which can be utilized to guide the reconstruction process of planar regions. Specifically, pix-els belonging to parallel planes tend to have similar normal directions (shown in Fig 3). We group pixels sharing sim-ilar normal values into the same cluster, which we call a super-plane (analogous to a group of pixels is called super-pixel). A super-plane constraint is then applied to force normal directions within the same super-plane to be con-sistent, thus constraining the reconstruction of large super-plane regions. Due to the ambiguity of the prediction, espe-cially at the early training stages, the grouped super-plane can be inaccurate and introduces noisy self-supervision sig-nals. Therefore, an automatic filtering strategy is further designed to compare the normals of each pixel with the es-timated super-plane normal, i.e., the average normal of all pixels belonging to the same super-plane, and outliers with large angular differences will be filtered out. We also detect the discontinuity of the surface according to the geometry and color features and mask out non-plane edge regions in the super-plane segmentation maps.
Notably, by using normals for grouping, multiple paral-lel planes will be grouped together, so that our super-planes are typically larger than individual planes. This property is particularly beneficial for volume rendering-based training process [25] since planes with more pixels will also have more stable and accurate averaged normals when limited pixels are sampled in each iteration. We experimentally verify this benefit: our super-plane constraint yields better results than adopting ground truth plane segmentation.
As a by-product, self-supervised plane segmentation of the reconstructed scene can be easily obtained from the super-plane masks by extracting connected components separated by the detected non-plane edge regions. Thus, our approach can be extended to reconstruct planes of a scene without ground truth supervision. It can also be applied to label new scenes automatically for applications that require such training data. The main contributions of this work are:
• We introduce a super-plane constraint for neural im-plicit scene reconstruction by first generating super-planes in an unsupervised manner and then performing automatic outlier filtering.
• Our super-plane segmentation method can be further extended to get unsupervised plane reconstruction re-sults, which can be used as auto-labeling.
• Experimental results show that our method signifi-cantly improves the reconstruction quality of texture-less planar regions, and the unsupervised plane recon-struction results are comparable to those from state-of-the-art supervised methods.
Methods
Explicit 3D supervision
Implicit 2D/3D supervision
Handle texture-less
Patch Match-based MVS
Data-driven MVS
NeuS [39], VolSDF [46]
ManhattanSDF [9]
NeuRIS [37], MonoSDF [48]
Ours
×
✓
×
×
×
×
×
×
×
✓(2D)
✓(3D)
×
×
✓
×
✓
✓
✓
Table 1. Comparison between different reconstruction meth-ods. Our method can handle texture-less planar regions without implicit supervision provided by assistant networks. 2.