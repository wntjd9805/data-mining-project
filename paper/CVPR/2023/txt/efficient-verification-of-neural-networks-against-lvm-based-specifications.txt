Abstract
The deployment of perception systems based on neu-ral networks in safety critical applications requires assur-ance on their robustness. Deterministic guarantees on net-work robustness require formal verification. Standard ap-proaches for verifying robustness analyse invariance to an-alytically defined transformations, but not the diverse and ubiquitous changes involving object pose, scene viewpoint, occlusions, etc. To this end, we present an efficient ap-proach for verifying specifications definable using Latent
Variable Models that capture such diverse changes. The ap-proach involves adding an invertible encoding head to the network to be verified, enabling the verification of latent space sets with minimal reconstruction overhead. We re-port verification experiments for three classes of proposed latent space specifications, each capturing different types of realistic input variations. Differently from previous work in this area, the proposed approach is relatively independent of input dimensionality and scales to a broad class of deep networks and real-world datasets by mitigating the ineffi-ciency and decoder expressivity dependence in the present state-of-the-art. 1.

Introduction
The deployment of perception systems based on neu-ral networks in safety-critical applications requires assur-ance on their performance, notably accuracy and robust-ness. Formal verification contributes to this requirement by providing provable and deterministic guarantees that a net-work meets a given specification. Typically, specifications are mathematically expressed constraints on the network’s intended input/output and may encode desirable proper-ties, such as robustness to noise (including adversarial at-tacks) [35], geometric changes [1,2], bias-field changes [14] and beyond.
While the above is useful, practical applications require robustness against diverse changes in a scene, including changes in the pose of objects, viewpoints, occlusions, etc.
Such changes cannot be efficiently mathematically defined, but may be encoded from data by using generative mod-els. For instance, [11, 12, 28, 34] use generative models to generate novel in-domain images for data augmentation, ad-versarial training or evaluating network generalisation; [34] additionally derives formal conditions for a latent space set to necessarily contain sufficient perturbations for it to be trusted for adversarial training and robustness checks. All these approaches either provide statistical robustness mea-sures, or generate attacks based on gradient-search, which is not guaranteed to find an attack if one exists.
Popular for network robustification and empirical eval-uation, latent space sets are seldom used as inputs for ver-ification due to the valid concern over the lack of mathe-matical guarantees on the completeness of the specifications they encode. Therefore, we reiterate that this work is most useful for changes that are difficult to mathematically de-fine. We additionally argue that formal verification of latent space-based specifications can be more valuable than their empirical evaluation. This is because the latent space is a continuous domain and countably infinite number of inputs can be mapped to and reconstructed from a latent space set.
Therefore, no amount of testing, or search in the latent space can provide guarantees against all the variations encoded in it. To the best of our knowledge, only [20,27] encode speci-fications in a latent space and propose architectures to verify them.
There are, however, two difficulties with verification in the latent space. The first concerns the scalability of verifi-cation methods; the second relates to the quality of recon-structions affecting the verification outcomes. In this paper, we focus on alleviating these two concerns. Specifically, we propose a novel, invertible encoder-based pipeline for verifying latent space sets, that lends two key benefits of:
• Computational efficiency and relative independence to in-put dimensionality,
• Verification outcomes’ independence to reconstructions, and precise counterexamples with high recall.
We focus our analysis on pose and attribute variations in vi-sion inference tasks, but the approach is likely extendable
to other variations, domains and tasks. Next, we recall key notions for network verification and discuss the existing rel-evant work, before presenting and validating our method in subsequent sections. 2.