Abstract
For augmented reality (AR), it is important that virtual assets appear to ‘sit among’ real world objects. The virtual element should variously occlude and be occluded by real matter, based on a plausible depth ordering. This occlu-sion should be consistent over time as the viewer’s camera moves. Unfortunately, small mistakes in the estimated scene depth can ruin the downstream occlusion mask, and thereby the AR illusion. Especially in real-time settings, depths in-ferred near boundaries or across time can be inconsistent.
In this paper, we challenge the need for depth-regression as an intermediate step.
We instead propose an implicit model for depth and use that to predict the occlusion mask directly. The inputs to our network are one or more color images, plus the known depths of any virtual geometry. We show how our occlusion predictions are more accurate and more temporally stable than predictions derived from traditional depth-estimation models. We obtain state-of-the-art occlusion results on the challenging ScanNetv2 dataset and superior qualitative re-sults on real scenes. 1.

Introduction
Augmented reality and digital image editing usually en-tail compositing virtual rendered objects to look as if they are present in a real-world scene. A key and elusive part of making this effect realistic is occlusion. Looking from a camera’s perspective, a virtual object should ap-pear partially hidden when part of it passes behind a real world object.
In practice this comes down to estimating, for each pixel, if the final rendering pipeline should dis-play the real world object there vs. showing the virtual ob-ject [53, 24, 36].
Typically, this per-pixel decision is approached by first estimating the depth of each pixel in the real world image
[19, 90, 36]. Obtaining the depth to each pixel on the vir-tual object is trivial, and can be computed via traditional graphics pipelines [33]. The final mask can be estimated by
Figure 1. We address the problem of automatically estimating oc-clusion masks to realistically place virtual objects in real scenes.
Our approach, where we directly predict masks, leads to more ac-curate compositing compared with Lidar-based sensors or tradi-tional state-of-the-art depth regression methods. comparing the two depth maps: where the real world depth value is smaller, the virtual content is occluded, i.e. masked.
We propose an alternative, novel approach. Given im-ages of the real world scene and the depth map of the virtual assets, our network directly estimates the mask for com-positing. The key advantage is that the network no longer has to estimate the real-valued depth for every pixel, and instead focuses on the binary decision: is the virtual pixel in front or behind the real scene here? Further, at infer-ence time we can use the soft output of a sigmoid layer to softly blend between the real and virtual, which can give visually pleasing compositions [43], compared with those created by hard thresholding of depth maps (see Figure 1).
Finally, temporal consistency can be improved using ideas from temporal semantic segmentation that were difficult to apply when regressing depth as an intermediate step.
We have three contributions: 1. We frame the problem of compositing a virtual object at a known depth into a real scene as a binary mask estimation problem. This is in contrast to previous ap-proaches that estimate depth to solve this problem. 2. We introduce metrics for occlusion evaluation, and find our method results in more accurate and visually pleasing composites compared with alternatives. 3. We show that competing approaches flicker, leading to jarring occlusions. By framing the problem as segmen-tation, we can use temporal smoothing methods, which result in smoother predictions compared to baselines. in
‘implicit depth’ model ultimately results state-of-the-art occlusions on the challenging ScanNetv2 dataset [17]. Further, if depths are needed too, we can com-pute dense depth by gathering multiple binary masks. Sur-prisingly, this results in state-of-the-art depth estimation.
Our 2.