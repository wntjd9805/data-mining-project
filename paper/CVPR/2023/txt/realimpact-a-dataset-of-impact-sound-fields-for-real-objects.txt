Abstract
Objects make unique sounds under different perturba-tions, environment conditions, and poses relative to the listener. While prior works have modeled impact sounds and sound propagation in simulation, we lack a standard dataset of impact sound fields of real objects for audio-visual learning and calibration of the sim-to-real gap. We present REALIMPACT, a large-scale dataset of real object impact sounds recorded under controlled conditions. RE-ALIMPACT contains 150,000 recordings of impact sounds of 50 everyday objects with detailed annotations, includ-ing their impact locations, microphone locations, contact force profiles, material labels, and RGBD images.* We make preliminary attempts to use our dataset as a reference to current simulation methods for estimating object impact sounds that match the real world. Moreover, we demon-strate the usefulness of our dataset as a testbed for acoustic and audio-visual learning via the evaluation of two bench-mark tasks, including listener location classification and vi-sual acoustic matching. 1.

Introduction
Object sounds permeate our everyday natural environ-ments as we both actively interact with them and passively perceive events in our environment. The sound of a drinking glass bouncing on the floor assuages our fear that the glass would shatter. The click made by a knife making contact with a cutting board assures us that we have diced cleanly through a vegetable. And listening to the sound a painted mug makes when we tap it informs us of whether it is made of ceramic or metal. What we perceive from sound comple-ments what we perceive from vision by reinforcing, disam-biguating, or augmenting it.
Understanding the cause-and-effect relationships in these sounds at a fine-grained level can inform us about an object’s material properties and geometry, as well as its contact and other environmental conditions. Capturing
*The project page and dataset are available at https : / / samuelpclarke.com/realimpact/ these relationships from real-world data can help us im-prove our models toward more realistic physical simula-tions, with applications in virtual reality, animation, and training learning-based frameworks in simulation.
The sounds we perceive from objects are the result of many intricate physical processes: they encode important properties about the object itself (e.g., geometry, material, mechanical properties), as well as the surrounding environ-ment (e.g., room size, other passive objects present, mate-rials of furniture in the room). More specifically, when a hard object is struck, it vibrates according to its mass and stiffness, and the shape of the object determines the mode shapes of the dominant vibration patterns (§3.1). Acous-tic waves are then emitted into the medium, typically air, bouncing around in the room and interacting with surround-ing objects and the room itself before reaching our ear or a microphone to be perceived as pressure fluctuations (§3.2).
Prior work has explored using physical simulation [26, 54] or learning-based methods [28, 29] to reconstruct the sound generation process virtually, as well as building 3D environments with simulated spatial audio for embodied audio-visual learning [7, 15, 18, 35, 42]. However, there has been little work on building physical apparatuses and fea-sible measurement process to quantify sounds made by the everyday objects, despite their importance and intimate re-lationship with our daily lives. As a result, the evaluations of the methods above are largely established on subjective metrics such as user studies.
To address this gap, we introduce REALIMPACT, a dataset containing 150k recordings of 50 everyday objects, each being struck from 5 distinct impact positions. For each impact point, we capture sounds at 600 field points to provide comprehensive coverage of the frequency com-ponents of the sounds and how they are distributed spa-tially. REALIMPACT thus provides all the inputs most cur-rent simulation frameworks needed to simulate each sound, while also providing the ground truth recording for compar-ison. We show that REALIMPACT can be used for various downstream auditory and audio-visual learning tasks, such as listener location classification (§5.2) and visual acous-tic matching (§5.3). These results demonstrate that sound
fields can help improve machine perception and understand-ing of the world, and motivate further studies of even more accurate simulation methodologies to reduce the sim-to-real gap for future applications.
We make three contributions. First, we design an auto-mated setup for collecting high-fidelity, annotated record-ings of sounds by controlled striking of everyday objects.
Second, using this setup, we acquire a large dataset of spatialized object sounds, REALIMPACT. Third, we moti-vate the utility of REALIMPACT by (a) using it to perform comparisons to results generated by current state-of-the-art sound simulation frameworks and (b) evaluating two bench-mark tasks for acoustic and audio-visual learning. 2.