Abstract
Holistic 3D scene understanding entails estimation of both layout configuration and object geometry in a 3D en-vironment. Recent works have shown advances in 3D scene estimation from various input modalities (e.g., images, 3D scans), by leveraging 3D supervision (e.g., 3D bounding boxes or CAD models), for which collection at scale is ex-pensive and often intractable. To address this shortcoming, we propose a new method to learn 3D scene priors of layout and shape without requiring any 3D ground truth. Instead, we rely on 2D supervision from multi-view RGB images.
Our method represents a 3D scene as a latent vector, from which we can progressively decode to a sequence of objects characterized by their class categories, 3D bounding boxes, and meshes. With our trained autoregressive decoder repre-senting the scene prior, our method facilitates many down-stream applications, including scene synthesis, interpola-tion, and single-view reconstruction. Experiments on 3D-FRONT and ScanNet show that our method outperforms state of the art in single-view reconstruction, and achieves state-of-the-art results in scene synthesis against baselines which require for 3D supervision. Project page: https:
//yinyunie.github.io/sceneprior-page/ 1.

Introduction
Understanding the geometric structures in real world scenes takes significant meaning in 3D computer vision. In recent years, many prominent approaches have been pro-posed to understand 3D scenes for different applications, e.g., scene synthesis [24, 28, 36, 48, 49, 52] and semantic re-construction [6,15,16,18–21,26,30,34,35,56]. These meth-ods have shown initial promise in estimating object layouts and shapes in 3D scenes, from RGB or RGB-D input data.
However, such methods have focused on leveraging full 3D supervision for layout and shape learning, in task-specific fashion. Such ground truth 3D scene geometry with semantic decomposition to objects and structures is difficult and expensive to acquire, requiring significant artist model-ing efforts [13] or expert annotations [1] of CAD modeling.
In contrast, 2D image and video acquisition is significantly
Figure 1. We learn 3D scene priors with 2D supervision. We model a latent hypersphere surface to represent a manifold of 3D scenes, characterizing the semantic and geometric distribution of objects in 3D scenes. This supports many downstream applica-tions, including scene synthesis, interpolation and single-view re-construction. more accessible. We thus propose to instead learn a gen-eral 3D scene prior from only multi-view 2D information.
This approach takes a unified perspective to 3D scene un-derstanding: our 3D scene prior is learned in a generaliz-able fashion from only multi-view 2D information, enabling application to multiple downstream tasks, including scene synthesis and single-view reconstruction.
In order to understand geometric 3D scene structure from only multi-view RGB images, we must bridge the domain gap between 3D geometry and RGB colors. To this end, we propose to leverage the 2D semantic domain: we learn our 3D scene prior by ensuring that our learned 3D scene structures and objects, when differentiably ren-dered, match the 2D semantic distribution of estimated 2D instance segmentations from the RGB images. This enables supervision of 3D object and layout structures guided by 2D information.
Our 3D scene prior is learned by optimizing for a map-ping from a latent space (parameterized as a hypersphere surface) to a manifold of 3D scenes, as shown in Fig. 1.
Each random vector sampled from the latent space is de-coded into a sequence of objects autoregressively using our permutation-invariant transformer. Each output object is characterized by their class category, 3D bounding box and mesh. By training this decoder, we encode the scene prior
into a latent space, sampling on which enables us to syn-thesize plausible 3D scenes. With such formulation, our method supports many other downstream tasks on different conditions, e.g., scene synthesis, interpolation and single-view reconstruction.
In summary, we present our contributions as follows:
• We present a new perspective on general 3D scene prior learning with only 2D supervision. We learn the prior distribution of both semantic object instance lay-outs and shapes via mapping from a latent space to the 3D scene space, by training from 2D multi-view data.
• We formulate scene prior learning as an autoregres-sive sequence decoding problem, and propose a novel permutation-invariant transformer corresponding with a 2D view loss to map a latent vector from a hyper-sphere surface to a sequence of semantic instances characterized by their class categories, 3D bounding boxes and meshes.
• Our learned 3D scene prior supports many downstream tasks: single-view scene reconstruction, scene synthe-sis and interpolation. Experiments on 3D-FRONT and
ScanNet show that our method matches state-of-the-art performance in scene synthesis against methods re-quiring 3D supervision, and outperforms state of the art in single-view reconstruction. 2.