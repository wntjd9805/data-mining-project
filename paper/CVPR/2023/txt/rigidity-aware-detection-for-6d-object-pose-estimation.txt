Abstract
Most recent 6D object pose estimation methods first use object detection to obtain 2D bounding boxes before actu-ally regressing the pose. However, the general object de-tection methods they use are ill-suited to handle cluttered scenes, thus producing poor initialization to the subsequent pose network. To address this, we propose a rigidity-aware detection method exploiting the fact that, in 6D pose esti-mation, the target objects are rigid. This lets us introduce an approach to sampling positive object regions from the entire visible object area during training, instead of naively drawing samples from the bounding box center where the object might be occluded. As such, every visible object part can contribute to the final bounding box prediction, yielding better detection robustness. Key to the success of our approach is a visibility map, which we propose to build using a minimum barrier distance between every pixel in the bounding box and the box boundary. Our results on seven challenging 6D pose estimation datasets evidence that our method outperforms general detection frameworks by a large margin. Furthermore, combined with a pose re-gression network, we obtain state-of-the-art pose estimation results on the challenging BOP benchmark. 1.

Introduction
Estimating the 6D pose of objects, i.e., their 3D rota-tion and 3D translation with respect to the camera, is a fun-damental computer vision problem with many applications in, e.g., robotics, quality control, and augmented reality.
Most recent methods [2, 5, 9, 25, 42, 45] follow a two-stage pipeline: First, they detect the objects, and then estimate their 6D pose from a resized version of the resulting de-tected image patches. While this approach works well in simple scenarios, its performance drops significantly in the presence of cluttered scenes. In particular, and as illustrated in Fig. 1, we observed this to be mainly caused by detection failures.
Specifically, most 6D pose estimation methods rely on standard object detection methods [10, 22, 37, 43, 44, 50], which were designed to handle significantly different scenes (a) General detection (b) Detection in 6D pose (c) Baseline detection results (d) Our detection results (e) Baseline pose results (f) Our pose results
Figure 1. The challenges of detection in 6D object pose. (a) The general detection scenario (COCO [29]) exhibits small occlusions. (b) The occlusion problem in 6D object pose, however, is much more severe, (c) making the general detection method [44] based on center-oriented sampling unreliable (glue) or fail completely (cat). (d) By contrast, our new detection strategy is effective in these challenging scenarios, (e,f) and provides significantly more robust 2D box initialization for the following 6D regression net-works [15], yielding more accurate pose estimates. than those observed in 6D object pose estimation bench-marks, typically with much smaller occlusions, as shown in
Fig. 1(a). Because of these smaller occlusions, standard de-tection methods make the assumption that the regions in the center of the ground-truth bounding boxes depict the object of interest, and thus focus on learning to predict the bound-ing box parameters from samples drawn from these regions only. However, as shown in Fig. 2, this is ill-suited to 6D pose estimation in cluttered scenes, where the center of the objects is often occluded by other objects or scene elements.
the camera, nowadays typically involves a pose regression network to establish 3D-to-2D correspondences [12, 15–17, 19, 20, 33]. These correspondences then act as input to a perspective-n-points solver (PnP) [24] to compute the final 6D object pose. The current state-of-the-art methods [2,5,9, 23,26,32,40,42,46] virtually all use a 2D object detector to allow the following pose regression networks to focus on a region of interest (RoI), thus yielding more accurate poses.
While this is effective when detection is successful, the pose accuracy deteriorates significantly in case of missing or inaccurate detections. In particular, 6D pose estimation frameworks typically use standard object detectors that, as shown in Figs. 1 and 2, often fail in cluttered scenes such as those of standard 6D pose estimation benchmarks as they were not designed to handle such situations. To handle this, we propose a rigidity-aware detection method that leverages the target properties. As shown by our results, it yields sig-nificant better RoIs for 6D object pose estimation.
Object detection, whose goal is to extract accurate 2D bounding boxes for all objects in a scene, has been widely studied in 2D computer vision. Existing methods follow one of two main strategies: two-stage or one-stage detec-tion. Two-stage detectors first employ a region proposal net-work [10, 37] to generate bounding box candidates, which are then processed by a classification and refinement net-work to remove false positives and adjust the bounding boxes position and size [3, 10, 37]. Although this strategy is accurate in general, it is costly and inefficient in practice.
One-stage detectors tackle this by replacing the region proposal network with a pre-defined set of anchors at every spatial location in the encoder’s final feature map [28, 34, 43]. Unfortunately, this suffers from the presence of many negative samples among the anchors. While this can be ad-dressed to some degree by FocalLoss [27, 28], early single-stage detectors did not reach the accuracy of two-stage ones.
This was addressed in [50] via a simple yet effective strategy to sample positive candidates in a one-stage detec-tor. Most recent detection methods follow similar strate-gies [8, 22, 31, 35, 36, 44, 51], and now achieve better accu-racy than two-stage methods while being more efficient.
Nevertheless, while these methods work well on stan-dard object detection benchmarks, they suffer from the heavy occlusions present in 6D pose estimation ones. Here, we therefore propose a new strategy dedicated to detecting rigid objects, and show that it outperforms standard detec-tors by a large margin in the context of 6D pose estimation. 3. Approach
Given an RGB image depicting rigid objects, our goal is to estimate the 2D bounding box of each potential target for the subsequent pose regression network. To address this, we propose to leverage the fact that, in the context of 6D object pose estimation, we observe rigid targets. In this section, (a) Baseline strategy (b) Our strategy (c) Detection results
Figure 2. Detecting rigid objects in cluttered scenes. (a)
The standard strategy [50] chooses positive samples (green cells) around the object center, thus suffering from occlusions. (b) In-stead, we propose to use a visibility-guided sampling strategy to discard the occluded regions and encourage the network to be su-pervised by all visible parts. The sampling probability is depicted by different shades of green. (c) Our method (green boxes) yields more accurate detections than the standard strategy (red boxes).
To handle this, we propose a detection approach that leverages the property that the target objects in 6D pose es-timation are rigid. For such objects, any visible parts can provide a reliable estimate of the complete bounding box.
We therefore argue that, in contrast with the center-based sampling used by standard object detectors, any, and only feature vectors extracted from the visible parts should be potential candidates of positive samples during training.
In principle, modeling the visibility could be achieved by annotating segmentation masks for all objects. This pro-cess, however, is cumbersome, particularly in the presence of occlusions by scene elements, and would limit the scal-ability of the approach.
Instead, we therefore propose to compute a probability of visibility based on a minimum barrier distance between any pixel in a bounding box and the box boundary. We then use this probability to guide the sampling of candidates during training, thus discard-ing the occluded regions and encouraging the network to be supervised by all visible parts. Furthermore, to leverage the reliability of local predictions from most visible parts during inference, we collect all candidate local predictions above a confidence threshold, and combine them by a sim-ple weighted average, yielding more robust detections.
We demonstrate the effectiveness of our method on seven challenging 6D object pose estimation datasets, on which we consistently and significantly outperform all detection baselines. Furthermore, combined with a 6D pose regres-sion network, our approach yields state-of-the-art object pose results. 2.