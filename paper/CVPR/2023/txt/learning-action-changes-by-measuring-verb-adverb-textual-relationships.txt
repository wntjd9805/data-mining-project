Abstract
The goal of this work is to understand the way actions are performed in videos. That is, given a video, we aim to predict an adverb indicating a modification applied to the action (e.g. cut “finely”). We cast this problem as a regres-sion task. We measure textual relationships between verbs and adverbs to generate a regression target representing the action change we aim to learn. We test our approach on a range of datasets and achieve state-of-the-art results on both adverb prediction and antonym classification. Fur-thermore, we outperform previous work when we lift two commonly assumed conditions: the availability of action la-bels during testing and the pairing of adverbs as antonyms.
Existing datasets for adverb recognition are either noisy, which makes learning difficult, or contain actions whose ap-pearance is not influenced by adverbs, which makes evalu-ation less reliable. To address this, we collect a new high quality dataset: Adverbs in Recipes (AIR). We focus on in-structional recipes videos, curating a set of actions that exhibit meaningful visual changes when performed differ-ently. Videos in AIR are more tightly trimmed and were manually reviewed by multiple annotators to ensure high la-belling quality. Results show that models learn better from
AIR given its cleaner videos. At the same time, adverb pre-diction on AIR is challenging, demonstrating that there is considerable room for improvement. 1.

Introduction
Learning how an action is performed in a video is an important step towards expanding our video understanding beyond action recognition. This task has been referred to as
“adverb recognition”, and has potential useful applications in robotics and retrieval. Consider a scenario where a robot handles fragile objects. In such case we would like to tell the robot to grasp objects gently to prevent it from break-ing things. Similarly, learning how actions are performed enables more sophisticated queries for retrieval.
Imagine learning a new recipe where a crucial step is stirring a mix-ture vigorously. It would be useful to find good examples showing how to whip a mixture in such a manner.
Figure 1. We aim to predict the way an action is performed in a video (right). Actions and their outcomes change in different ways when modified by adverbs, e.g. grinding coffee slowly does not have the same effect as grinding it coarsely (left). We learn to predict adverbs by recognising action changes in the video. We define action changes measuring verb-adverb textual relationships.
Adverb recognition is a challenging problem. Firstly, a single adverb can modify multiple actions in different ways.
For example, to grind coffee coarsely (see Figure 1) we need to set a certain granularity in a grinder. Conversely, to spray something coarsely we would just use a spraying can more sparingly. Secondly, compared to actions or objects, adverbs are much harder to identify. Though somewhat sub-jective [1, 16], the temporal boundaries of an action can be easily determined. The spatial extent of an object is also easy to recognise for a human. Adverbs are instead more abstract: we can see their effect, but we struggle to pinpoint the spatio-temporal location where the adverb is appearing.
Consider for example the act of spraying something slowly in Figure 1. We could say that slowly “is located” on the hand motions. At the same time, spraying something slowly could implicate a more even coating of a surface. That is, the outcome of an action and the end-state of the involved objects can also change according to the adverb. This makes it hard to label adverbs with visual annotations (e.g. bound-ing boxes), which makes the problem more challenging.
In fact, previous approaches [6,7] learn adverbs as action changes in a weakly supervised manner. The state-of-the-art approach [6] treats adverbs as learnable parameters that modify actions. Specifically, the action change is learnt dur-ing training by contrasting antonyms, i.e. opposite adverbs.
We show that learning adverbs in such manner can be dif-ficult and can limit the ability of the model to generalise.
We thus propose to define action changes by measuring dis-tances in a text embedding space, and aim to learn such change from the video through regression. We show that our approach achieves new state-of-the-art results through extensive experiments. We also lift two major assumptions made in previous work [6, 7]: the availability of action la-bels during testing and the pairing of opposite adverbs as antonyms. Our method achieves stronger performance es-pecially when the above assumptions are relaxed.
Besides being challenging, adverb recognition is also an under-explored domain and only few datasets are available.
Doughty and Snoek [7] addressed the problem of scarce an-notations, proposing a semi-supervised method that assigns pseudo-labels to boost recognition performance. Three datasets sourced from captioning benchmarks were also col-lected in [7]. These datasets offer a large number of clips, actions and adverbs. However, adverbs in these datasets appear to be descriptive rather than action modifiers, i.e. actions do not display a significant change when modified by the adverb. This is an issue when adverbs are mod-elled as action changes as in this work. We thus introduce a new dataset focusing on instructional videos where actions change considerably depending on the way they are car-ried out. We focus on cooking videos since in this domain action changes are prominent. Our Adverbs in Recipes (AIR) dataset was manually labelled and consists of over 7K videos, 10 adverbs and 48 actions.
To summarise our contributions: i) we propose a more effective approach to learn adverbs in videos. Our method achieves state-of-the-art results on multiple datasets and with fewer assumptions; ii) we introduce the AIR dataset for adverb recognition. Our focus on a domain where ac-tion changes are prominent and our careful manual anno-tation makes AIR more suitable for training and evaluating models for adverb understanding. We publicly release AIR and our code at github.com/dmoltisanti/air-cvpr23. 2.