Abstract
Existing video recognition algorithms always conduct different training pipelines for inputs with different frame numbers, which requires repetitive training operations and multiplying storage costs.
If we evaluate the model us-ing other frames which are not used in training, we ob-serve the performance will drop significantly (see Fig. 1), which is summarized as Temporal Frequency Deviation phenomenon. To fix this issue, we propose a general frame-work, named Frame Flexible Network (FFN), which not only enables the model to be evaluated at different frames to adjust its computation, but also reduces the memory costs of storing multiple models significantly. Concretely, FFN in-tegrates several sets of training sequences, involves Multi-Frequency Alignment (MFAL) to learn temporal frequency invariant representations, and leverages Multi-Frequency
Adaptation (MFAD) to further strengthen the representa-tion abilities. Comprehensive empirical validations us-ing various architectures and popular benchmarks solidly demonstrate the effectiveness and generalization of FFN (e.g., 7.08/5.15/2.17% performance gain at Frame 4/8/16 on Something-Something V1 dataset over Uniformer). Code is available at https://github.com/BeSpontaneous/FFN. 1.

Introduction
The growing number of online videos boosts the research on video recognition, laying a solid foundation for deep learning which requires massive data. Compared with im-age classification, video recognition methods need a series of frames to represent the video which scales the compu-tation. Thus, the efficiency of video recognition methods has always been an essential factor in evaluating these ap-proaches. One existing direction to explore efficiency is designing lightweight networks [9, 40] which are hardware friendly. Even if they increase the efficiency with an accept-able performance trade-off, these methods cannot make fur-ther customized adjustments to meet the dynamic-changing resource constraint in real scenarios. In community, there (a) Temporal Frequency Deviation phenomenon exists in various video recognition architectures. (b) Temporal Frequency Devia-tion phenomenon exists in different depths of deep networks.
Figure 1. Temporal Frequency Deviation phenomenon widely exists in video recognition. All methods are trained with high frame number and evaluated at other frames to compare with Sep-arated Training (ST) which individually trains the model at differ-ent frames on Something-Something V1 dataset. are two lines of research being proposed to resolve this is-sue. The first one is to design networks that can execute at various depths [10] or widths [37] to adjust the computa-tion from the model perspective. The other line of research considers modifying the resolutions of input data [15,34] to accommodate the cost from the data aspect. However, these methods are carefully designed for 2D CNNs, which may hinder their applications on video recognition where 3D
CNNs and Transformer methods are crucial components.
Different from image-related tasks, we need to sample multiple frames to represent the video, and the computa-tional costs will grow proportionally to the number of sam-pled frames. Concretely, standard protocol trains the same network with different frames separately to obtain multi-ple models with different performances and computations.
This brings challenges to applying these networks on edge devices as the parameters will be multiplied if we store all models, and downloading and offloading models to switch them will cost non-negligible time. Moreover, the same video may be sampled at various temporal rates on differ-ent platforms, employing a single network that is trained at a certain frame number for inference cannot resist the vari-ance of frame numbers in real scenarios.
Training the model with a high frame number (i.e., high temporal frequency) and directly evaluating it at fewer frames (i.e., low temporal frequency) to adjust the cost is a naive and straightforward solution. To test its effective-ness, we compare it with Separated Training (ST) which trains the model at different temporal frequency individu-ally and tests it with the corresponding frame. We conduct experiments on 2D-network TSM [18], 3D-network Slow-Fast [6] and Transformer-network Uniformer [16], and find obvious performance gaps between the inference results and
ST from Fig. 1, which means these methods will exhibit sig-nificantly inferior performance if they are not evaluated at the frame number used in training. Further, we conduct the same experiments on different depths of deep networks and a similar phenomenon appears. We denote this generally existing phenomenon as Temporal Frequency Deviation.
The potential reason for Temporal Frequency Devia-tion has been explored in Sec. 3 and briefly summarized as the shift in normalization statistics. To address this is-sue, we propose a general framework, named Frame Flexi-ble Network (FFN), which only requires one-time training, but can be evaluated at multiple frame numbers with great flexibility. We import several input sequences with differ-ent sampled frames to FFN during training and propose
Multi-Frequency Alignment (MFAL) to learn the temporal frequency invariant representations for robustness towards frame change. Moreover, we present Multi-Frequency
Adaptation (MFAD) to further strengthen the representation abilities of the sub-networks which helps FFN to exhibit strong performance at different frames during inference.
Although normalization shifting problem [36, 37] and resolution-adaptive networks [15, 34] have been studied, we stress that designing frame flexible video recognition frameworks to accommodate the costs and save parameters is non-trivial and has practical significance for the follow-ing reasons. First, prior works [15, 34] carefully analyzed the detailed structure of 2D convolutions in order to pri-vatize the weights for different scale images. While our method does not touch the specific design of the spatial-temporal modeling components and shares their weights for inputs with different frames. This procedure not only en-ables our method to be easily applied to various architec-tures (2D/3D/Transformer models), but also enforces FFN to learn temporal frequency invariant representations. Sec-ond, it is, indeed, a common practice to conduct Separated
Training (ST) in video recognition, which needs multiply-ing memory costs to store individual models, and the mod-els are hard to resist the variance in temporal frequency which limits their applications in actual practice. While
FFN provides a feasible solution to these challenges which significantly reduces the memory costs of storing multiple models and can be evaluated at different frames to adjust the cost with even higher accuracy compared to ST.
With the proposed framework, we can resolve Tempo-ral Frequency Deviation and enable these methods to adjust their computation based on the current resource budget by sampling different frames, trimming the storage costs of ST remarkably. Moreover, we provide a naive solution that en-ables FFN to be evaluated at any frame and increases its flexibility during inference. Validation results prove that
FFN outperforms ST even at frames that are not used in training. The contributions are summarized as follows:
• We reveal the phenomenon of Temporal Frequency
Deviation that widely exists in video recognition. It is detailedly analyzed and practically inspires our study.
• We propose a general framework Frame Flexible Net-work (FFN) to resolve Temporal Frequency Devia-tion. We design Multi-Frequency Alignment (MFAL) to learn temporal frequency invariant representations and present Multi-Frequency Adaptation (MFAD) to further strengthen the representation abilities.
• Comprehensive empirical validations show that FFN, which only requires one-shot training, can adjust its computation by sampling different frames and outper-form Separated Training (ST) at different frames on various architectures and datasets, reducing the mem-ory costs of storing multiple models significantly. 2.