Abstract
The appearance of an object can be fleeting when it transforms. As eggs are broken or paper is torn, their color, shape and texture can change dramatically, preserving vir-tually nothing of the original except for the identity itself.
Yet, this important phenomenon is largely absent from ex-isting video object segmentation (VOS) benchmarks. In this work, we close the gap by collecting a new dataset for
Video Object Segmentation under Transformations (VOST).
It consists of more than 700 high-resolution videos, cap-tured in diverse environments, which are 20 seconds long on average and densely labeled with instance masks. We adopt a careful, multi-step approach to ensure that these videos focus on complex object transformations, capturing their full temporal extent. We then extensively evaluate state-of-the-art VOS methods and make a number of important discoveries. In particular, we show that existing methods struggle when applied to this novel task and that their main limitation lies in over-reliance on static appearance cues.
This motivates us to propose a few modifications for the top-performing baseline that improve its capabilities by better modeling spatio-temporal information. More broadly, our work highlights the need for further research on learning more robust video object representations.
Nothing is lost or created, all things are merely transformed.
Antoine Lavoisier 1.

Introduction
Spatio-temporal cues are central in segmenting and tracking objects in humans, with static appearance play-ing only a supporting role [23, 27, 43].
In the most ex-treme scenarios, we can even localize and track objects de-fined by coherent motion alone, with no unique appearance whatsoever [20]. Among other benefits, this appearance-last approach increases robustness to sensory noise and enables object permanence reasoning [41]. By contrast, modern computer vision models for video object segmenta-tion [3, 11, 44, 64] operate in an appearance-first paradigm.
Figure 1. Video frames from the DAVIS’17 dataset [42] (above), and our proposed VOST (below). While existing VOS datasets feature many challenges, such as deformations and pose change, the overall appearance of objects varies little. Our work focuses on object transformations, where appearance is no longer a reliable cue and more advanced spatio-temporal modeling is required.
Indeed, the most successful approaches effectively store patches with associated instance labels and retrieve the clos-est patches to segment the target frame [11, 38, 44, 64].
What are the reasons for this stark disparity? While some are algorithmic (e.g., object recognition models being first developed for static images), a key reason lies in the datasets we use. See for instance the “Breakdance” sequence from the validation set of DAVIS’17 [42] in Figure 1: while the dancer’s body experiences significant deformations and pose changes, the overall appearance of the person remains constant, making it an extremely strong cue.
However, this example – representative of many VOS datasets – covers only a narrow slice of the life of an object.
In addition to translations, rotations, and minor deforma-tions, objects can transform. Bananas can be peeled, paper can be cut, clay can be molded into bricks, etc. These trans-formations can dramatically change the color, texture, and shape of an object, preserving virtually nothing of the orig-Figure 2. Representative samples from VOST with annotations at three different time steps (see video for full results). Colours indicate instance ids, with grey representing ignored regions. VOST captures a wide variety of transformations in diverse environments and provides pixel-perfect labels even for the most challenging sequences. inal except for the identity itself (see Figure 1, bottom and
Figure 2). As we show in this paper, tracking object identity through these changes is relatively easy for humans (e.g. la-belers), but very challenging for VOS models. In this work, we set out to fill this gap and study the problem of segment-ing objects as they undergo complex transformations.
We begin by collecting a dataset that focuses on these scenarios in Section 3. We capitalize on the recent large-scale, ego-centric video collections [13, 21], which contain thousands of examples of human-object interactions with activity labels. We carefully filter these clips to only in-clude major object transformations using a combination of linguistic cues (change of state verbs [19, 29]) and man-ual inspection. The resulting dataset, which we call VOST (Video Object Segmentation under Transformations), con-tains 713 clips, covering 51 transformations over 155 object categories with an average video length of 21.2 seconds.
We then densely label these videos with more than 175,000 masks, using an unambiguous principle inspired by spatio-temporal continuity: if a region is marked as an object in the first frame of a video, all the parts that originate from it maintain the same identity (see Figure 2).
Equipped with this unique dataset, we analyze state-of-the-art VOS algorithms in Section 4. We strive to in-clude a representative set of baselines that illustrates the majority of the types of approaches to the problem in the literature, including classical, first frame matching meth-ods [61], local mask-propagation objectives [26], alter-native, object-level architectures [3], and the mainstream memory-based models [11, 63–65]. Firstly, we observe that existing methods are indeed ill-equipped for segmenting ob-jects through complex transformations, as illustrated by the large (2.3-12.5 times) gap in performance between VOST and DAVIS’17 (see Table 2). A closer analysis of the results reveals the following discoveries: (1) performance of the methods is inversely proportional to their reliance on static appearance cues; (2) progress on VOST can be achieved by improving the spatio-temporal modeling capacity of exist-ing architectures; (3) the problem is not easily solvable by training existing methods on more data.
We conclude in Section 5 by summarizing the main chal-lenges associated with modeling object transformations.
We hope that this work will motivate further exploration into more robust video object representations. Our dataset, source code, and models are available at vostdataset.org. 2.