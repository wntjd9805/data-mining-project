Abstract
In this work, we study the black-box targeted attack prob-lem from the model discrepancy perspective. On the the-oretical side, we present a generalization error bound for black-box targeted attacks, which gives a rigorous theoreti-cal analysis for guaranteeing the success of the attack. We reveal that the attack error on a target model mainly de-pends on empirical attack error on the substitute model and the maximum model discrepancy among substitute models.
On the algorithmic side, we derive a new algorithm for black-box targeted attacks based on our theoretical analy-sis, in which we additionally minimize the maximum model discrepancy (M3D) of the substitute models when training the generator to generate adversarial examples. In this way, our model is capable of crafting highly transferable ad-versarial examples that are robust to the model variation, thus improving the success rate for attacking the black-box model. We conduct extensive experiments on the ImageNet dataset with different classification models, and our pro-posed approach outperforms existing state-of-the-art meth-ods by a significant margin. The code will be available at https://github.com/Asteriajojo/M3D. 1.

Introduction
Recently, researchers have shown that Deep Neural Net-works (DNNs) are highly vulnerable to adversarial exam-ples [9, 28, 36].
It has been demonstrated that by adding small and human-imperceptible perturbations, images can be easily misclassified by deep-learning models. Even worse, adversarial examples are shown may have transfer-ability, i.e., adversarial examples generated by one model can successfully attack another model with a high prob-ability [23, 28, 37]. Consequently, there is an increasing interest in developing new techniques to attack an unseen black-box model by constructing adversarial examples on
*The corresponding author a substitute model, which is also known as black-box at-tack [5, 6, 14–16, 20, 39, 40].
While almost all existing black-box attack works implic-itly assume the transferability of adversarial examples, the theoretical analysis of the transferability is still absent. To this end, in this work, we aim to answer the question of to what extent the adversarial examples generated on one known model can be used to successfully attack another un-seen model. In particular, we are specifically interested in the targeted attack task, i.e., constructing adversarial exam-ples that can mislead the unseen black-box model by out-putting a highly dangerous specified class. We first present a generalization error bound for black-box targeted attacks from the model discrepancy perspective, in which we reveal that the attack error on a target model depends on the attack error on a substitute model and the model discrepancy be-tween the substitute model and the black-box model. Fur-thermore, the latter term can be bounded by the maximum model discrepancy on the underlying hypothesis set, which is irrelevant to the unseen target model, making it possi-ble to construct adversarial examples by directly minimiz-ing this term and thus the generalization error.
Based on the generalization error bound, we then design a novel method called Minimizing Maximum Model Dis-crepancy (M3D) attack to produce highly transferable per-turbations for black-box targeted attack. Specifically, we exploit two substitute models which are expected to main-tain their model discrepancy as large as possible. At the same time, we train a generator that takes an image as in-put and generates an adversarial example to attack these two substitute models and simultaneously minimize the discrep-ancy between the two substitute models.
In other words, the generator and the two substitute models are trained in an adversarial manner to play a min-max game in terms of the model discrepancy.
In this way, the generator is ex-pected to generate adversarial examples that are robust to the variation of the substitute models, thus being capable of attacking the black-box target model successfully with a high chance.
We conduct extensive experiments on the ImageNet dataset using different benchmark models, where our M3D approach outperforms state-of-the-art methods by a signifi-cant margin on a wide range of attack settings. Especially, we show impressive improvements in the situations when the black-box model has a large model discrepancy from the substitute model, such as attacking the ResNet [11] model by crafting adversarial examples on a VGG [35] model. The main contributions of this paper are as follows:
• We present a generalization error bound for black-box targeted attacks based on the model discrepancy per-spective.
• We design a novel generative approach called Mini-mizing Maximum Model Discrepancy (M3D) attack to craft adversarial examples with high transferability based on the generalization error bound.
• We demonstrate the effectiveness of our method by strong empirical results, where our approach outper-forms the state-of-art methods by a significant margin. 2.