Abstract
We propose Equiangular Basis Vectors (EBVs) for clas-siﬁcation tasks. In deep neural networks, models usually end with a k-way fully connected layer with softmax to handle different classiﬁcation tasks. The learning objec-tive of these methods can be summarized as mapping the learned feature representations to the samples’ label space.
While in metric learning approaches, the main objective is to learn a transformation function that maps training data points from the original space to a new space where simi-lar points are closer while dissimilar points become farther apart. Different from previous methods, our EBVs generate normalized vector embeddings as “predeﬁned classiﬁers” which are required to not only be with the equal status be-tween each other, but also be as orthogonal as possible. By minimizing the spherical distance of the embedding of an in-put between its categorical EBV in training, the predictions can be obtained by identifying the categorical EBV with the smallest distance during inference. Various experiments on the ImageNet-1K dataset and other downstream tasks demonstrate that our method outperforms the general fully connected classiﬁer while it does not introduce huge addi-tional computation compared with classical metric learning methods. Our EBVs won the ﬁrst place in the 2022 DIGIX
Global AI Challenge, and our code is open-source and avail-able at https://github.com/NJUST-VIPGroup/
Equiangular-Basis-Vectors. 1.

Introduction
The pattern classiﬁcation ﬁeld developed around the end of the twentieth century aims to deal with the speciﬁc prob-lem of assigning input signals to two or more classes [56].
In recent years, deep learning models have brought break-throughs in processing image, video, audio, text, and other
*Corresponding author. This work was supported by National Key
R&D Program of China (2021YFA1001100), National Natural Science
Foundation of China under Grant (62272231), Natural Science Foundation of Jiangsu Province of China under Grant (BK20210340), the Fundamental
Research Funds for the Central Universities (No. NJ2022028), and CAAI-Huawei MindSpore Open Fund. (cid:2206) (cid:1876)(cid:1876)(cid:2869) (cid:1876)(cid:1876)(cid:2870)
… (cid:1876)(cid:1876)(cid:3031) (cid:1876)(cid:1860)(cid:2869) (cid:1876)(cid:1860)(cid:2870) (cid:1876)(cid:1860)(cid:2871)
… (cid:1876)(cid:1860)(cid:3015) (cid:1876)(cid:1858)(cid:2869)
… (cid:1876)(cid:1858)(cid:3030)
… (cid:1876)(cid:1858)(cid:3030)(cid:4593) (cid:1857)(cid:3033)(cid:3117) (cid:1857)(cid:3033)(cid:3278) (cid:1876)(cid:1872)(cid:2870) (cid:1876) (cid:1876)(cid:1872)(cid:3030) (cid:1876) (cid:1857)(cid:3033)(cid:3278)(cid:4594) (cid:1876)(cid:1872)(cid:3030)(cid:4593) (cid:1872)(cid:2869) (cid:963)(cid:3037) (cid:1872)(cid:3037) (cid:1876)(cid:1868)(cid:2870) (cid:1872)(cid:3030) (cid:963)(cid:3037) (cid:1872)(cid:3037) (cid:1876)(cid:1868)(cid:3030) (cid:1872)(cid:3030)(cid:4593) (cid:963)(cid:3037) (cid:1872)(cid:3037) (cid:1876)(cid:1868)(cid:3030)(cid:4593)
Add category  (cid:1855)(cid:1314) (a) Fully Connected Layers with Softmax
Positive Sample (cid:547) (cid:3549)(cid:2205)(cid:2869), (cid:3549)(cid:2205)(cid:2870) = (cid:547) (cid:3549)(cid:2205)(cid:2869), (cid:3549)(cid:2205)(cid:2871) = (cid:547) (cid:3549)(cid:2205)(cid:2870), (cid:3549)(cid:2205)(cid:2871) (cid:547) (cid:3549)(cid:2205)(cid:2869), (cid:3549)(cid:2205)(cid:3030)(cid:4593) = (cid:547) (cid:3549)(cid:2205)(cid:2869), (cid:3549)(cid:2205)(cid:2870)
Anchor 
Add category  (cid:1855)(cid:1314) g
Negative Sample
Add category (cid:1855)(cid:1314) with (cid:1865)(cid:4593) samples:  (cid:2281) (cid:1839)(cid:2871) (cid:1372) (cid:2281)(((cid:1839) + (cid:1865)(cid:1314))(cid:2871)) (cid:2871) p (cid:2871) (cid:3549)(cid:2205)(cid:2871) (cid:3549)(cid:2205)(cid:2869) (cid:3549)(cid:2205)(cid:2870) (cid:3549)(cid:2205)(cid:3030)(cid:4593) (b) Metric Learning (c) Equiangular Basis Vectors
Figure 1. Comparisons between typical classiﬁcation paradigms and our proposed Equiangular Basis Vectors (EBVs). (a) A general classiﬁer ends with k-way fully connected layers with softmax.
When adding more categories, the trainable parameters of the classi-ﬁer grow linearly. (b) Taking triplet embedding [60] as an example of classical metric learning methods, the complexity is O(M 3) when given M images and it will grows to O((M + m(cid:2))3) when adding a new category with m(cid:2) images. (c) Our proposed EBVs.
EBVs predeﬁne ﬁxed normalized vector embeddings for different categories and these embeddings will not be changed during the training stage. The trainable parameters of the network will not be changed with the growth of the number of categories while the complexity only grows from O(M ) to O(M + m(cid:2)). data [10, 19, 27, 58]. Aided by the rapid gains in hardware, deep learning methods today can easily overﬁt one million images [9] and easily overcomes the obstacle to the qual-ity of handcrafted features in previous pattern classiﬁcation tasks. Many approaches based on deep learning spring up like mushrooms and had been used to solve classiﬁcation problems in various scenarios and settings such as remote sensing [38], few-shot [52], long-tailed [72], etc.
Figure 1 illustrates two typical classiﬁcation paradigms.
Nowadays, a large amount of deep learning methods [38, 72] adopt a trainable fully connected layer with softmax as the classiﬁer. However, since the number of categories is ﬁxed, the trainable parameters of the classiﬁer rise as the number of categories becomes larger. For example, the memory consumption of a fully connected layer W ∈ Rd×N linearly scales up with the growth of the category number N and so is the cost to compute the matrix multiplication between the fully connected layer and the d-dimensional features. While some other methods based on classical metric learning [23, 30, 60–62] have to consider all the training samples and design positive/negative pairs then optimize a class center for each category, which requires a signiﬁcant amount of extra computation for large-scale datasets, especially for those pre-training tasks.
In this paper, we propose Equiangular Basis Vectors (EBVs) to replace the fully connected layer associated with softmax within classiﬁcation tasks in deep neural net-works. EBVs predeﬁne ﬁxed normalized vector embeddings with equal status (equal angles) which will not be changed during the training stage. Speciﬁcally, EBVs pre-set a d-dimensional unit hypersphere, and for each category in the classiﬁcation task, EBVs assign the category a d-dimensional normalized embedding on the surface of the hypersphere and we term these embedding as basis vectors. The spherical distance of each basis vector pair satisﬁes an artiﬁcially made rule to make the relationship between any two vec-tors as close to orthogonal as possible. In order to keep the trainable parameters of the deep neural networks constant with the growth of the category number N , we then propose the deﬁnition of EBVs based on Tammes Problem [55] and
Equiangular Lines [57] in Section 3.2.
The learning objective of each category in our proposed
EBVs is also different from previous classiﬁcation methods.
Compared with deep models that end with a fully connected layer to handle the classiﬁcation tasks [19, 27], the meaning of the parameter weights within the fully connected layer in
EBVs is not the relevance of a feature representation to a par-ticular category but a ﬁxed matrix which embed feature rep-resentations to a new space. Also, compared with regression methods [34,51], EBVs do not need to learn the uniﬁed repre-sentations for different categories and optimize the distance between the representation of input images and category cen-ters, which helps reduce the computational consumption for the extra uniﬁed representations learning. In contrast to clas-sical metric learning approaches [14, 23, 50, 61], our EBVs do not need to measure the similarity among different train-ing samples and constrain distance between each category, which will introduce a large amount of computational con-sumption for large-scale datasets. In our proposed method, the representations of different images learned by EBVs will be embedded into a normalized hypersphere and the learning objective is altered to minimize the spherical distance of the learned representations with different predeﬁned basis vectors. In addition, the spherical distance between each predeﬁned basis vector is carefully constrained so that there is no need to spend extra cost in the optimization of these basis vectors. To quantitatively prove both the effectiveness and efﬁciency of our proposed EBVs, we evaluate EBVs on diverse computer vision tasks with large-scale datasets, including classiﬁcation on ImageNet-1K, object detection on COCO, as well as semantic segmentation on ADE20K. 2.