Abstract
Generative models have shown great promise in syn-thesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF gener-ates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN ar-chitectures and introduce a novel progressive-scale patch discrimination approach during training. With several ex-periments, we demonstrate that the results produced by Sin-GRAF outperform the closest related works in both quality and diversity by a large margin. 1.

Introduction
Creating a new 3D asset is a laborious task, which often requires manual design of triangle meshes, texture maps, and object placements. As such, numerous methods were proposed to automatically create diverse and realistic varia-tions of existing 3D assets. For example, procedural model-ing techniques [11,27] produce variations in 3D assets given predefined rules and grammars, and example-based model-ing methods [13, 21] combine different 3D components to generate new ones.
With our work, we propose a different, generative strat-egy that is able to create realistic variations of a single 3D scene from a small number of photographs. Unlike existing 3D generative models, which typically require 3D assets as input [13, 51], our approach only takes a set of unposed im-ages as input and outputs a generative model of a single 3D scene, represented as a neural radiance field [31].
Our method, dubbed SinGRAF, builds on recent progress in unconditional 3D-aware GANs [5, 40] that train generative radiance fields from a set of single-view images.
However, directly applying these 3D GANs to our problem
*Equal contribution.
Project page: computationalimaging.org/publications/singraf/
Figure 1. SinGRAF generates different plausible realizations of a single 3D scene from a few unposed input images of that scene.
In this example, i.e., the “office 3” scene, we use 100 input im-ages, four of which are shown in the top row. Next, we visualize four realizations of the 3D scene as panoramas, rendered using the generated neural radiance fields. Note the variations in scene lay-out, including chairs, tables, lamps, and other parts, while staying faithful to the structure and style of the input images. is challenging, because they typically require a large train-ing set of diverse images and often limit their optimal oper-ating ranges to objects, rather than entire scenes. SinGRAF makes a first attempt to train a 3D generative radiance field for individual indoor 3D scenes, creating realistic 3D varia-tions in scene layout from unposed 2D images.
Intuitively, our method is supervised to capture the inter-nal statistics of image patches at various scales and generate 3D scenes whose patch-based projections follow the input image statistics. At the core of our method lies continuous-scale patch-based adversarial [14] training. Our radiance fields are represented as triplane feature maps [4, 43] pro-duced by a StyleGAN2 [22] generator. We volume-render our generated scenes from randomly sampled cameras with varying fields of view, to simulate the appearance of im-age patches at various scales. A scale-aware discrimina-tor is then used to compute an adversarial loss to the real and generated 2D patches to enforce realistic patch dis-tributions across all sampled views. Notably, our design of continuous-scale patch-based generator and discrimina-tor allows patch-level adversarial training without expen-sive hierarchical training [41, 51, 54]. During the training, we find applying perspective augmentations to the image patches and optimizing the camera sampling distribution to be important for high-quality scene generation.
The resulting system is able to create plausible 3D vari-ations of a given scene trained only from a set of unposed 2D images of that scene. We demonstrate our method on two challenging indoor datasets of Replica [47] and Matter-port3D [6] as well as a captured outdoor scene. We evaluate
SinGRAF against the state-of-the-art 3D scene generation methods, demonstrating its unique ability to induce realis-tic and diverse 3D generations. 2.