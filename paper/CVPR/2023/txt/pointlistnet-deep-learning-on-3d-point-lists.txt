Abstract tiveness of the proposed PointListNet.
Deep neural networks on regular 1D lists (e.g., natural languages) and irregular 3D sets (e.g., point clouds) have made tremendous achievements. The key to natural lan-guage processing is to model words and their regular or-der dependency in texts. For point cloud understanding, the challenge is to understand the geometry via irregular point coordinates, in which point-feeding orders do not matter.
However, there are a few kinds of data that exhibit both reg-ular 1D list and irregular 3D set structures, such as proteins and non-coding RNAs. In this paper, we refer to them as 3D point lists and propose a Transformer-style PointListNet to model them. First, PointListNet employs non-parametric distance-based attention because we find sometimes it is the distance, instead of the feature or type, that mainly deter-mines how much two points, e.g., amino acids, are corre-lated in the micro world. Second, different from the vanilla
Transformer that directly performs a simple linear transfor-mation on inputs to generate values and does not explicitly model relative relations, our PointListNet integrates the 1D order and 3D Euclidean displacements into values. We con-duct experiments on protein fold classification and enzyme reaction classification. Experimental results show the effec-1.

Introduction
The essence of deep learning is to capture the structure of a certain kind of data via artificial neural networks. Usu-ally, an element of data includes a position part and a feature part. According to the type of element position, data exhibit different structures. Various deep neural networks are pro-posed to model those structures and have made tremendous achievements.
For example, texts are 1D lists of words. As shown in
Fig. 1(a). The position of a word is its order in the text and the feature is the word itself. To capture the structure of texts or the dependency of words, 1D convolutional neu-ral networks (CNNs) [3, 30, 58], recurrent neural networks (RNNs) [9, 26, 39] and Transformers [13, 49] are widely used. A digital image can be seen as a 2D rectangular grid or matrix of pixels, as shown in Fig. 1(b). Each pixel has a 2D position and is associated with a feature of color or other attributes. In this case, 2D CNNs are usually used to model image structure [23, 33, 46]. Recently, Transformers are also employed for image understanding [15].
Recently, 3D point cloud/set processing is attracting
more and more attention from the deep learning community.
Different from texts or images, in which the orders of words or the positions of pixels are regular (words or pixels are dis-tributed uniformly in texts or images), the 3D coordinates of points are irregular (points are distributed unevenly in 3D Euclidean space), as shown in Fig. 1(c). To capture the irregular structure of point clouds, deep neural networks, such as multilayer perceptrons (MLPs) [42, 43, 45], convo-lutions [48, 56] and Transformers [22, 62], need to not only effectively exploit 3D coordinates for geometry understand-ing but also be invariant to permutations of the input set in point-feeding order.
Besides regular 1D lists of words, 2D grids of pixels and irregular 3D point sets, data may exhibit hybrid struc-tures. For example, proteins are made up of amino acids.
As shown in Fig. 1(d), those amino acids are linked by pep-tide bonds and form a chain. Therefore, proteins include a 1D list data structure. Because amino acids are arranged uniformly in the chains, the list structure is regular. In ad-dition to the 1D sequential order in the peptide chain, each amino acid is with a 3D coordinate, which specifies its spa-tial position in the protein. Those 3D coordinates describe a geometry structure. Similar to point clouds, the geome-try structure of proteins exhibits irregularity. Therefore, the data structure of proteins involves a regular 1D list and an irregular 3D set. In this paper, we refer to this data struc-ture as 3D point list. Point lists also exist in other polymers, such as non-coding RNAs. Because the function of proteins or non-coding RNAs is based on their structures, modeling 3D point lists can facilitate a mechanistic understanding of their function to life.
In this paper, we propose a Transformer-style network, named PointListNet, to capture the structure of 3D point lists. First, different from the vanilla Transformer [15, 49], which calculates self-attention by performing compu-tationally expensive matrix multiplication on inputs, our
PointListNet employs a simple non-parametric distance-based attention mechanism because we find sometimes it is mainly the distance, instead of the feature or type, that determines how much two elements, e.g., amino acids, are correlated in the micro world. Second, because structures are relative, which is independent of the absolute sequential order or the absolute Euclidean coordinate, our PointList-Net integrates the 1D order and 3D Euclidean displace-ments into values. This is substantially different from the vanilla Transformer that directly performs a simple linear transformation on absolute positional embeddings and input features to generate values, which does not explicitly model relative distance or direction. To evaluate PointListNet, we conduct experiments on protein fold classification and en-zyme reaction classification and achieve new state-of-the-art accuracy. The contributions of this paper are fivefold:
• Among the early efforts, we investigate a range of point cloud methods for protein modeling.
• We propose a Transformer-style network, i.e.,
PointListNet, for 3D point list modeling.
• We replace non-parametric self-attention with distance-based attention, which is more efficient and effective to achieve the correlation among microparticles in some cases.
• We integrate relative structure modeling into Trans-former and employ regular and irregular methods to capture the sequence and geometry structures, respec-tively.
• We conduct extensive experiments on two protein tasks and the proposed method significantly outper-forms existing methods. 2.