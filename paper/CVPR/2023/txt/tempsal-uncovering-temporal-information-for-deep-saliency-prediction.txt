Abstract
Deep saliency prediction algorithms complement the ob-ject recognition features, they typically rely on additional information such as scene context, semantic relationships, gaze direction, and object dissimilarity. However, none of these models consider the temporal nature of gaze shifts during image observation. We introduce a novel saliency prediction model that learns to output saliency maps in se-quential time intervals by exploiting human temporal atten-tion patterns. Our approach locally modulates the saliency predictions by combining the learned temporal maps. Our experiments show that our method outperforms the state- of-the- art models, including a multi- duration saliency model, on the SALICON benchmark and CodeCharts1k dataset.
Our code is publicly available on GitHub1. 1.

Introduction
Humans have developed attention mechanisms that al-low them to selectively focus on the important parts of a scene. Saliency prediction algorithms aim to computation-ally detect these regions that stand out relative to their sur-roundings. These predictions have numerous applications 1https://ivrl.github.io/Tempsal/ in image compression [37], image enhancement [51], im-age retargeting [1], rendering [43], and segmentation [28].
Since the seminal work of Itti et al. [18], many have de-veloped solutions using both handcrafted features [7] and deep ones [9,17,26,34,46,48]. Nowadays, employing deep neural networks is preferred in saliency prediction as they outperform bottom-up models. These methods typically de-pend on pre-trained object recognition networks to extract features from the input image [31].
In addition to these features, scene context [47], object co-occurrence [50], and dissimilarity [2] have been exploited to improve the saliency prediction. However, while these approaches model the scene context and objects, they fail to consider that humans dynamically observe scenes [49]. In neuroscience, the inhi-bition of return paradigm states that a suppression mecha-nism reduces visual attention towards recently attended ob-jects [39] and encourages selective attention to novel re-gions. Motivated by this principle, we develop a saliency prediction model that incorporates temporal information.
Fosco et al. [14] also exploit temporal information in saliency prediction, but they consider snapshots containing observations up to 0.5, 3, and 5 seconds, thus not leverag-ing saliency trajectory but rather saliency accumulation. By contrast, here, we model consecutive time slices, connect-ing our approach more directly with the human gaze and thus opening the door to automated visual appeal assess-ment in applications such as website design [32], advertise-ment [36] and infographics [13].
To achieve this, we show that when viewing images, hu-man attention yields temporally evolving patterns, and we introduce a network capable of exploiting this temporal in-formation for saliency prediction. Specifically, our model learns time-specific predictions and is able to combine them with a conventional image saliency map to obtain a tempo-rally modulated image saliency prediction.
We evaluate our method on the SALICON [20] and
CodeCharts1k [14] datasets which contain temporal infor-mation, unlike the other popular saliency datasets such as
CAT2000 [3] and MIT1003 [23]. By showing the bene-fits of estimating temporal saliency we hope to encourage the community to publish the temporal information of their data along with the final saliency maps. Note that exist-ing works typically collect saliency data by conducting psy-chophysical experiments [3, 14, 20, 23], and the attention data recorded during these experiments already includes temporal information. Therefore, no further experiments are required.
As evidenced by our experiments, using temporal in-formation boosts the accuracy of the baseline network, enabling us to consistently outperform the state-of-the-art models on the SALICON saliency benchmark. More-over, in the CodeCharts1k dataset we outperform the multi-duration model [14] in two out of three metrics.
We summarize our contributions as follows:
• We evidence the presence of temporally evolving pat-terns in human attention.
• We show that temporal information in the form of a saliency trajectory improves saliency prediction in nat-ural images, providing an investigation of the SALI-CON dataset for temporal attention shifts.
• We introduce a novel, saliency prediction model, called TempSAL, capable of simultaneously predict-ing conventional image saliency and temporal saliency trajectories.
• We propose a spatiotemporal mixing module that learns time-dependent patterns from temporal saliency maps. Our approach outperforms the state-of-the-art image saliency models that either do not consider tem-poral information or encode it in a cumulative manner. 2.