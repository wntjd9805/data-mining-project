Abstract
This paper tackles the problem of semi-supervised video object segmentation on resource-constrained devices, such as mobile phones. We formulate this problem as a dis-tillation task, whereby we demonstrate that small space-time-memory networks with finite memory can achieve com-petitive results with state of the art, but at a fraction of the computational cost (32 milliseconds per frame on a
Samsung Galaxy S22). Specifically, we provide a theo-retically grounded framework that unifies knowledge dis-tillation with supervised contrastive representation learn-ing. These models are able to jointly benefit from both pixel-wise contrastive learning and distillation from a pre-trained teacher. We validate this loss by achieving compet-itive J &F to state of the art on both the standard DAVIS and YouTube benchmarks, despite running up to ˆ5 faster, and with ˆ32 fewer parameters. 1.

Introduction
Video Object Segmentation (VOS) is a foundational task in computer vision, where the aim is to segment and track objects in a sequence of frames. VOS is the backbone of many applications such as video editing, autonomous driv-ing, surveillance, and augmented reality [55]. In this work, we focus on semi-supervised VOS (SVOS), where the mask annotations for only the initial frame are provided. SVOS is a notoriously difficult task, where one needs to model the motion and appearance changes under severe occlusion and drifts, while performing well in a class-agnostic manner.
SVOS approaches can be divided into three main meth-ods, namely, online fine-tuning [4, 20, 27, 36, 43, 53], ob-ject flow [12, 35, 40, 50, 61, 63], and offline matching [8, 21, 52, 64, 65]. Memory-based matching methods have shown impressive results in the last years. These approaches
[9, 10, 26, 29, 32, 34, 41, 47, 54, 57] leverage memory banks to encode and memorize previous frames. They find feature correspondences between the memory and the current frame
*Work done while being an intern at Samsung Research UK to predict the next object masks. Despite their good results, memory-networks tend to suffer from a trade-off between the memory usage and accuracy [26]. Storing a reduced number of frames [9,26,54], storing local segmentation fea-tures [37, 57] (i.e. features of only a part of the image), or segmenting only on keyframes [59], have been proposed as methods to tackle this tradeoff. Recent methods are now accurate and fast on high-end GPUs, but there is no method in the literature that is capable of retaining state-of-the-art results while performing in real-time on mobile phones.
Our work addresses real-time SVOS on resource-constrained devices. Specifically, we focus on memory-based networks and begin to bridge the gap between infinite memory and finite memory networks. In contrast with ex-isting methods that do so by architectural [26] and memory-bank design changes [9, 54], we adopt a novel approach that addresses the problem through knowledge distillation.
Specifically, we propose a pixel-wise representation distil-lation loss that aims to transfer the structural information of a large, infinite memory teacher to that of a smaller, finite memory student. We then show that using a sim-ple boundary-aware sampling strategy can improve training convergence. Finally, we provide a natural generalisation to encompass a supervised pixel-wise contrastive representa-tion objective. Using ground-truth labels as another venue for structural information, we interpolate between repre-sentation distillation and contrastive learning with a hyper-parameter, and use it as a unified loss. Our results on popu-lar SVOS datasets show that we are competitive with state-of-the-art [26], while running ˆ5 faster and having ˆ32 fewer parameters. Our model, without any pruning or quan-tisation, runs comfortably in real-time on a mobile device.
The summary of our main contributions is given as follows:
• We unify knowledge distillation and supervised con-trastive learning to design a novel representation-based loss that bridges the gap between the large, infinite memory models and those with small, finite memory.
Additionally, we show that a simple boundary-aware pixel sampling strategy can further improve the results and model convergence.
Figure 1. The proposed student-teacher framework using boundary-aware distillation. The teacher model utilises infinite memory, whereas the student model only retains memory from the previous and first frames/mask. Since the teacher has more memory, the task of distillation (equation 4 and 9) is then to learn features which are consistent across multiple frames. The auxiliary objective of contrastive representation learning encourages discriminative pixel-wise features (equation 7) around the boundary of objects.
• Using this unified loss, we show that a common net-work design can achieve results competitive to the state-of-the-art, while running up to ˆ5 faster and hav-ing ˆ32 fewer parameters.
• Without complex architectural or memory design changes, our proposed loss can unlock real-time per-formance (30FPS+) on mobile devices (i.e. Samsung
Galaxy S22) while retaining competitive performance with state-of-the-art. 2.