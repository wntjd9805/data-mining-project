Abstract 1.

Introduction
Modeling the 3D world from sensor data for simula-tion is a scalable way of developing testing and valida-tion environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difﬁcult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images – but still suffer limitations as they leverage either human-curated im-age datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from cam-era and LiDAR sensors to create realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling representation learning and generative model-ing into two stages with a learned tri-plane latent structure, inspired by recent advances in generative modeling of im-ages. To evaluate our approach, we construct a large-scale object-centric dataset containing over 520K images of ve-hicles and pedestrians from the Waymo Open Dataset, and a new set of 80K images of long-tail instances such as con-struction equipment, garbage trucks, and cable cars. We compare our model with existing approaches and demon-strate that it achieves state-of-the-art performance in quality and diversity for both generated images and geometries.
Learning to perceive, reason, and interact with the 3D world has been a longstanding challenge in the computer vision and robotics community for decades [2–9]. Mod-ern robotic systems [10–16] deployed in the wild are often equipped with multiple sensors (e.g. cameras, LiDARs, and
Radars) that perceive the 3D environments, followed by an intelligent unit for reasoning and interacting with the com-plex scene dynamics. End-to-end testing and validating these intelligent agents in the real-world environments are difﬁ-cult and expensive, especially in safety critical and resource constrained domains like autonomous driving.
On the other hand, the use of simulated data has pro-liferated over the last few years to train and evaluate the intelligent agents under controlled settings [17–27] in a safe, scalable and veriﬁable manner. Such developments were fueled by rapid advances in computer graphics, including rendering frameworks [28–30], physical simulation [31, 32] and large-scale open-sourced asset repositories [33–39]. A key concern is to create realistic virtual worlds that align in asset content, composition, and behavior with real distribu-tions, so as to give the practitioner conﬁdence that using such simulations for development and veriﬁcation can transfer to performance in the real world [40–48]. However, manual asset creation faces two major obstacles. First, manual cre-ation of 3D assets requires dedicated efforts from engineers and artists with 3D domain expertise, which is expensive and difﬁcult to scale [26]. Second, real-world distribution contains diverse examples (including interesting rare cases) and is also constantly evolving [49, 50].
∗Work done during an internship at Waymo. † Work done at Waymo.
Recent developments in the generative 3D modeling offer
new perspectives to tackle these aforementioned obstacles, as it allows producing additional realistic but previously unseen examples. A sub-class of these approaches, gener-ative 3D-aware image synthesis [51, 52], holds signiﬁcant promise since it enables 3D modeling from partial observa-tions (e.g. image projections of the 3D object). Moreover, many real-world robotic applications already capture, an-notate and update multi-sensor observations at scale. Such data thus offer an accurate, diverse, task-relevant, and up-to-date representation of the real-world distribution, which the generative model can potentially capture. However, ex-isting works use either human-curated image datasets with clean observations [53–58] or renderings from synthetic 3D environments [33, 36]. Scaling generative 3D-aware image synthesis models to the real world faces several challenges, as many factors are entangled in the partial observations.
First, bridging the in-the-wild images from a simple prior without 3D structures make the learning difﬁcult. Second, unconstrained occlusions entangle object-of-interest and its surroundings in pixel space, which is hard to disentangle in a purely unsupervised manner. Lastly, the above chal-lenges are compounded by a lack of effort in constructing an asset-centric benchmark for sensor data captured in the wild.
In this work, we introduce a 3D-aware generative trans-former for implicit neural asset generation, named GINA-3D (Generative Implicit Neural Assets). To tackle the real world challenges, we propose a novel 3D-aware Encoder-Decoder framework with a learned structured prior. Speciﬁcally, we embed a tri-plane structure into the latent prior (or tri-plane latents) of our generative model, where each entry is param-eterized by a discrete representation from a learned code-book [59,60]. The Encoder-Decoder framework is composed of a transformation encoder and a decoder with neural ren-dering components. To handle unconstrained occlusions, we explicitly disentangle object pixels from its surrounding with an occlusion-aware composition, using pseudo labels from an off-the-shelf segmenation model [61]. Finally, the learned prior of tri-plane latents from a discrete codebook can be used to train conditional latents sampling models [62]. The same codebook can be readily applied to various conditional synthesis tasks, including object scale, class, semantics, and time-of-day.
To evaluate our model, we construct a large-scale object-centric benchmark from multi-sensor driving data captured in the wild. We ﬁrst extract over 520K images of diverse variations for vehicles and pedestrians from Waymo Open
Dataset [14]. We then augment the benchmark with long-tail instances from real-world driving scenes, including rare ob-jects like construction equipment, cable cars, school buses and garbage trucks. We demonstrate through extensive ex-periments that GINA-3D outperforms the state-of-the-art 3D-aware generative models, measured by image quality, geometry consistency, and geometry diversity. Moreover, we showcase example applications of various conditional synthesis tasks and shape editing results by leveraging the learned 3D-aware codebook. To support future research along this direction, we intend to release the benchmark to support relevant research in the community. 2.