Abstract
We propose learning a depth covariance function with applications to geometric vision tasks. Given RGB images as input, the covariance function can be flexibly used to define priors over depth functions, predictive distributions given observations, and methods for active point selection.
We leverage these techniques for a selection of downstream tasks: depth completion, bundle adjustment, and monocular dense visual odometry. 1.

Introduction
Inferring the 3D structure of the world from 2D images is an essential computer vision task. In recent years, there has been significant interest in combining principled multi-ple view geometry with data-driven priors. Learning-based methods that predict geometry provide a prior directly over the latent variables, which avoids the ill-posed configura-tions of traditional methods. However, direct, overconfident priors may prevent realization of the true 3D structure when multi-view geometry is well-defined. For example, data-driven methods have shown tremendous promise in monoc-ular depth estimation, but often lack consistency when fus-ing information into 3D space.
Thus far, designing a unified framework that combines the best of learning and optimization methods has proven challenging. Recent data-driven methods have attempted to relax rigid geometric constraints by also predicting low-dimensional subspaces or per-pixel uncertainties. Fixing capacity during training is often wasteful or inflexible at test time, while per-pixel residual distributions typically explain away the limitations of the model instead of the relationship between errors. In reality, the 3D world is anywhere from simple to complex, and an ideal system should explicitly adapt its capacity and correlation based on the scene.
In this paper, rather than directly predicting geometry from images, we propose learning a depth covariance func-tion. Given an RGB image, our method predicts how the depths of any two pixels relate. To achieve this, a neural net-work transforms color information to a feature space, and
Figure 1. Example monocular reconstruction using the depth co-variance for bundle adjustment and dense depth prediction from three seconds (100 frames) of TUM data. Three representative images and the mesh from TSDF fusion of the depth predictions are shown. Each frame leverages the learned covariance function to model geometric correlation between pairs of scene points. a Gaussian process (GP) models a prior distribution given these features and a base kernel function. The distinction between image processing and the prior enables promoting locality and granting flexible capacity at test time. Locality avoids over-correlating pixels on distinct structures, while adaptive capacity permits tuning the complexity of our sub-space to the content of the viewed scene.
Learning this flexible, high-level prior allows for bal-ancing data-driven methods with test-time optimization that can be applied to a variety of geometric vision tasks. Fur-thermore, the covariance function is agnostic to the 3D rep-resentation as it does not directly learn a geometric output.
Depth maps may be requested by conditioning on observa-tions, but the prior may also be leveraged for inferring the desired latent 3D representation. In Figure 1, we illustrate depth covariance along with an example of bundle adjust-ment, dense depth prediction, and multi-view fusion.
In summary, our key contributions are:
• A framework for learning the covariance function by selecting a depth representation, a base kernel func-tion, and a scalable optimization objective
• Application of the prior to depth completion, bundle adjustment, and monocular dense visual odometry 2.