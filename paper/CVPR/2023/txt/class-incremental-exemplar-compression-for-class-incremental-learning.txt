Abstract
Exemplar-based class-incremental learning (CIL) [36] finetunes the model with all samples of new classes but few-shot exemplars of old classes in each incremental phase, where the “few-shot” abides by the limited memory bud-get.
In this paper, we break this “few-shot” limit based on a simple yet surprisingly effective idea: compressing exemplars by downsampling non-discriminative pixels and saving “many-shot” compressed exemplars in the mem-ory. Without needing any manual annotation, we achieve this compression by generating 0-1 masks on discrimina-tive pixels from class activation maps (CAM) [49]. We propose an adaptive mask generation model called class-incremental masking (CIM) to explicitly resolve two dif-ficulties of using CAM: 1) transforming the heatmaps of
CAM to 0-1 masks with an arbitrary threshold leads to a trade-off between the coverage on discriminative pix-els and the quantity of exemplars, as the total memory is fixed; and 2) optimal thresholds vary for different object classes, which is particularly obvious in the dynamic envi-ronment of CIL. We optimize the CIM model alternatively with the conventional CIL model through a bilevel opti-mization problem [40]. We conduct extensive experiments on high-resolution CIL benchmarks including Food-101,
ImageNet-100, and ImageNet-1000, and show that using the compressed exemplars by CIM can achieve a new state-of-the-art CIL accuracy, e.g., 4.8 percentage points higher than FOSTER [42] on 10-Phase ImageNet-1000. Our code is available at https://github.com/xfflzl/CIM-CIL. 1.

Introduction
Dynamic AI systems have a continual learning nature to learn new class data. They are expected to adapt to new classes while maintaining the knowledge of old classes, i.e., free from forgetting problems [31]. To evaluate this, the following protocol of class-incremental learning (CIL) was proposed by Rebuffi et al. [36]. The model training goes
Figure 1. The phase-wise training data in different methods. (a) iCaRL [36] is the baseline method using full new class data and few-shot old class exemplars. (b) Mnemonics [27] distills all training samples into few-shot exemplars without increasing their (c) MRDC [43] compresses each exemplar uniformly quantity. into a low-resolution image using JPEG [41]. (d) Our approach based on the proposed class-incremental masking (CIM) down-samples only non-discriminative pixels in the image. The legend shows the symbols of special images generated by the methods. through a number of phases. Each phase has new class data added and old class data discarded, and the resultant model is evaluated on the test data of all seen classes. A straightforward way to retain old class knowledge is keep-ing around a few old class exemplars in the memory and using them to re-train the model in subsequent phases. The number of exemplars is usually limited, e.g., 5 ∼ 20 exem-plars per class [12, 17, 25, 27, 36, 42, 44, 46, 48], as the total memory in CIL strictly budgeted, e.g., 2k exemplars.
This leads to a serious data imbalance between old and new classes, e.g., 20 per old class vs. 1.3k per new class (on
ImageNet-1000 [9]), as illustrated in Figure 1a. The train-ing is thus always dominated by new classes, and forgetting problems occur for old classes. Liu et al. [27] tried to miti-gate this problem by parameterizing and distilling the exem-plars, without increasing the number of them (Figure 1b).
Wang et al. [43] traded off between the quality and quantity of exemplars by uniformly compressing exemplar images with JPEG [41] (Figure 1c). As shown in Figure 1d, our approach is also based on image compression. The idea is to downsample only non-discriminative pixels (e.g., back-ground) and keep discriminative pixels (i.e., representative cues of foreground objects) as the original. In this way, we do not sacrifice the discriminativeness of exemplars when increasing their quantity. In particular, we aim for adaptive compression in dynamic environments of CIL, where the intuition is later phases need to be more conservative (i.e., less downsampling) as the model needs more visual cues to classify the increased number of classes.
To achieve selective and adaptive compression, we need the location labels of discriminative pixels. Without extra labeling, we automatically generate the labels by utilizing the model’s own “attention” on discriminative features, i.e., class activation maps (CAM) [49]. We take this method as a feasible baseline, and based on it, we propose an adaptive version called class-incremental masking (CIM). Specifi-cally, for each input image (with its class label), we use its feature maps and classifier weights (corresponding to its class label) to compute a CAM by channel-wise mul-tiplication, aggregation, and normalization. Then, we ap-ply hard thresholding to generate a 0-1 mask.1 We notice that when generating the masks in the dynamic environ-ments of CIL, the optimal hyperparameters (such as the value of hard threshold and the choice of activation func-tions) vary for different classes as well as in different incre-mental phases. Our adaptive version CIM tackles this by pa-rameterizing a mask generation model and optimizing it in an end-to-end manner across all incremental phases. In each phase, the learned CIM model adaptively generates class-and phase-specific masks. We find that the compressed ex-emplars based on these masks have stronger representative-ness, compared to using the conventional CAM.
Technically, we have two models to optimize, i.e., the
CIL model and the CIM model.2 These two cannot be optimized separately as they are dependent on computa-tion: 1) the CIM model compresses exemplars to input into the CIL model; 2) the two models share network pa-rameters. We exploit a global bilevel optimization prob-lem (BOP) [7, 40] to alternate their training processes at two levels. This BOP goes through all incremental train-ing phases. In particular, for each phase, we perform a lo-cal BOP with two steps to tune the parameters of the CIM model: 1) a temporary model is trained with the compressed exemplars as input; and 2) a validation loss on the uncom-1Note that we do not use mask labels to do image compression because storing them is expensive. Instead, we expand the mask to a bounding box, as elaborated in Section 4. 2Note that the CIM model is actually a plug-in branch in the CIL model, which is detailed in Section 4.2. pressed new data is computed and the gradients are back-propagated to optimize the parameters of CIM. To evalu-ate CIM, we conduct extensive experiments by plugging it in recent CIL methods,3 LUCIR [17], DER [46], and
FOSTER [42], on three high-resolution benchmarks, Food-101 [3], ImageNet-100 [17], and ImageNet-1000 [9]. We find that using the compressed exemplars by CIM brings consistent and significant improvements, e.g., 4.2% and 4.8% higher than the SOTA method FOSTER [42], respec-tively, in the 5-phase and 10-phase settings of ImageNet-1000, with a total memory budget for 5k exemplars. 2.