Abstract
Encoding videos as neural networks is a recently pro-posed approach that allows new forms of video processing.
However, traditional techniques still outperform such neu-ral video representation (NVR) methods for the task of video compression. This performance gap can be explained by the fact that current NVR methods: i) use architectures that do not efficiently obtain a compact representation of tem-poral and spatial information; and ii) minimize rate and distortion disjointly (first overfitting a network on a video and then using heuristic techniques such as post-training quantization or weight pruning to compress the model). We propose a novel convolutional architecture for video repre-sentation that better represents spatio-temporal information and a training strategy capable of jointly optimizing rate and distortion. All network and quantization parameters are jointly learned end-to-end, and the post-training opera-tions used in previous works are unnecessary. We evaluate our method on the UVG dataset, achieving new state-of-the-art results for video compression with NVRs. Moreover, we deliver the first NVR-based video compression method that improves over the typically adopted HEVC benchmark (x265, disabled b-frames, “medium” preset), closing the gap to autoencoder-based video compression techniques.
Figure 1. Compared to the previous state-of-the-art method, our approach produces sharper frames at lower bpp. Images are la-beled with PSNR @ bpp. 1.

Introduction
Lossy video compression is a Rate-Distortion (R-D) op-timization problem of the form min D +λR. Given a video, the encoder’s task is to minimize the number of bits required to represent it, R (Rate), while also minimizing any dis-tortion brought about by the compression, D (Distortion).
λ controls the trade-off between both and defines a Pareto frontier, where improvements in the distortion term come at an increased cost in the rate term. Traditionally, heuristic-based engineered video codecs are used to encode videos in efficient representations. Such codecs have gradually de-veloped into complex pipelines and have been established as powerful standards such as H.264 [26] and HEVC [30].
Inspired by the success of deep learning in many im-age processing tasks, over the past few years, video com-pression using neural networks has been the target of much research [8, 18]. Most of the proposed methods are based
on Encoder-Decoder neural network pairs, capable of trans-forming Groups of Frames (GoF) into latent representations and subsequently recovering them. These representations are quantized, entropy coded, and then stored/transmitted.
Commonly, the loss function optimized by these methods is theoretically grounded in information theory and structured as an R-D problem. D is some measure of the difference be-tween the original and decoded frames, and R is given by an estimate of the lower bound of the length of a bit sequence representing the latent vector.
More recently, Implicit Neural Representations (INRs) have emerged as alternatives to dense grids for repre-senting continuous signals. They have seen remarkable success in 3D scene reconstruction and shape represen-tation [22, 24], and have also been used for video rep-resentation [3, 7, 33].
In such a case, a video is inter-preted as a function f (x, y, t) = (R, G, B) which is ap-proximated by fitting a neural network to a set of samples
S = {(x, y, t), (R, G, B)}. The video is then effectively stored in the neural network’s parameters and can be recov-ered by performing forward passes. Interestingly, by using
INRs, video compression can be framed as a neural network compression problem.
Previous efforts in using INRs for video compression, however, have mostly treated the problems of representing the input signal with high fidelity and compressing it as mostly disjoint tasks. A network is first trained to minimize a distortion loss and is then put through some procedure to reduce its size, e.g., storing its weights in a 16-bit floating point format, quantization, or pruning [7, 10, 29]. Further-more, current architectures are not designed with parameter efficiency as a priority, suffering from an inefficient alloca-tion of parameters, which can be prohibitive for the task of video compression [7, 16].
Tackling the above issues, we propose a new compact convolutional architecture for video representation that pro-vides better R-D performance than previous works (see Fig-ure 1). In addition, drawing on information theory, we pro-pose a theoretically motivated R-D formulation for video compression with INRs that jointly minimizes rate and dis-tortion. We build on the work of Oktay et al. [23] and model the entropy of the neural network weights, allow-ing us to minimize it jointly with the distortion. Thus, our method learns weights that simultaneously provide high-fidelity representations of the original video and have low entropy. Applying entropy coding methods to the final weights produces a compressed video representation.
In summary, our main contributions are: 1. we propose a novel compact convolutional architecture for neural video representation, which results in better representation capacity than NeRV [7] and faster en-coding and decoding than E-NeRV [16]; 2. we formally define signal compression with INRs as an
R-D problem by modeling the entropy of the weights and using quantization-aware training (allowing end-to-end training and eliminating the need for post-training techniques such as pruning); 3. we show that such an entropy modeling can also im-prove other methods, e.g., NeRV; 4. we evaluate our method on the UVG [21] dataset, im-proving on the state-of-the-art results for video com-pression with INRs and outperforming DVC [17], a well-established neural video compression method. 2.