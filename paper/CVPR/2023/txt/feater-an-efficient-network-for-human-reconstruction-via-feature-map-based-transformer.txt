Abstract
Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D/3D hu-man pose estimation (2D/3D HPE) and human mesh recon-struction (HMR) tasks.
In these tasks, feature map rep-resentations of the human structural information are of-ten extracted first from the image by a CNN (such as HR-Net), and then further processed by transformer to predict the heatmaps for HPE or HMR. However, existing trans-former architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. Further-more, much of the performance benefit in recent HPE and
HMR methods has come at the cost of ever-increasing com-putation and memory needs. Therefore, to simultaneously address these problems, we propose FeatER, a novel trans-former design that preserves the inherent structure of fea-ture map representations when modeling attention while re-ducing memory and computational costs. Taking advan-tage of FeatER, we build an efficient network for a set of human reconstruction tasks including 2D HPE, 3D HPE, and HMR. A feature map reconstruction module is applied to improve the performance of the estimated human pose and mesh. Extensive experiments demonstrate the effective-ness of FeatER on various human pose and mesh datasets.
For instance, FeatER outperforms the SOTA method Mesh-Graphormer by requiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets. The project webpage is https://zczcwh.github.io/feater_page/. 1.

Introduction
Understanding human structure from monocular images is one of the fundamental topics in computer vision. The corresponding tasks of Human Pose Estimation (HPE) and
Human Mesh Reconstruction (HMR) have received a grow-ing interest from researchers, accelerating progress toward various applications such as VR/AR, virtual try-on, and AI coaching. However, HPE and HMR from a single image still remain challenging tasks due to depth ambiguity, oc-clusion, and complex human body articulation.
Figure 1. Generating heatmaps from joint coordinates.
With the blooming of deep learning techniques, Con-volutional Neural Network (CNN) [10, 32, 33] architec-tures have been extensively utilized in vision tasks and have achieved impressive performance. Most existing HPE and HMR models [13, 33] utilize CNN-based architectures (such as ResNet [10] and HRNet [33]) to predict fea-ture maps, which are supervised by the ground-truth 2D heatmap representation (encodes the position of each key-point into a feature map with a Gaussian distribution) as shown in Fig. 1. This form of output representation and supervision can make the training process smoother, and therefore has become the de facto process in HPE’s net-works [1, 33, 43].
Recently, the transformer architecture has been fruitfully adapted from the field of natural language processing (NLP) into computer vision, where it has enabled state-of-the-art performance in HPE and HMR tasks [23–25, 38, 47]. The transformer architecture demonstrates a strong ability to model global dependencies in comparison to CNNs via its self-attention mechanism. The long-range correlations be-tween tokens can be captured, which is critical for modeling the dependencies of different human body parts in HPE and
HMR tasks. Since feature maps concentrate on certain hu-man body parts, we aim to utilize the transformer architec-ture to refine the coarse feature maps (extracted by a CNN backbone). After capturing the global correlations between human body parts, more accurate pose and mesh can be ob-tained.
However, inheriting from NLP where transformers em-bed each word to a feature vector, Vision Transformer ar-chitectures such as ViT [8] can only deal with the flattened features when modeling attention. This is less than ideal for preserving the structural context of the feature maps dur-ing the refinement stage (feature maps with the shape of
[n, h, w] need to be flattened as [n, d], where d = h × w.
Here n is the number of feature maps, h and w are height and width of each feature map, respectively). Furthermore, another issue is that the large embedding dimension caused by the flattening process makes the transformer computa-tionally expensive. This is not suitable for real-world ap-plications of HPE and HMR, which often demand real-time processing capabilities on deployed devices (e.g. AR/VR headsets).
Therefore, we propose a Feature map-based trans-formER (FeatER) architecture to properly refine the coarse feature maps through global correlations of structural in-formation in a resource-friendly manner. Compared to the vanilla transformer architecture, FeatER has two advan-tages:
• First, FeatER preserves the feature map representation in the transformer encoder when modeling self-attention, which is naturally adherent with the HPE and HMR tasks.
Rather than conducting the self-attention based on flat-tened features, FeatER ensures that the self-attention is conducted based on the original 2D feature maps, which are more structurally meaningful. To accomplish this,
FeatER is designed with a novel dimensional decompo-sition strategy to handle the extracted stack of 2D feature maps.
• Second, this decompositional design simultaneously pro-vides a significant reduction in computational cost com-pared with the vanilla transformer 1. This makes FeatER more suitable for the needs of real-world applications.
Equipped with FeatER, we present an efficient frame-work for human representation tasks including 2D HPE, 3D HPE, and HMR. For the more challenging 3D HPE and
HMR portion, a feature map reconstruction module is inte-grated into the framework. Here, a subset of feature maps are randomly masked and then reconstructed by FeatER, en-abling more robust 3D pose and mesh predictions for in-the-1For example, there are 32 feature maps with overall dimension
[32, 64, 64]. For a vanilla transformer, without discarding information, the feature maps need to be flattened into [32, 4096]. One vanilla trans-former block requires 4.3G MACs. Even if we reduce the input size to
[32, 1024], it still requires 0.27G MACs. However, given the original in-put of [32, 64, 64], FeatER only requires 0.09G MACs. wild inference. We conduct extensive experiments on hu-man representation tasks, including 2D human pose estima-tion on COCO, 3D human pose estimation and human mesh reconstruction on Human3.6M and 3DPW datasets. Our method (FeatER) consistently outperforms SOTA methods on these tasks with significant computation and memory cost reduction (e.g. FeatER outperforms MeshGraphormer
[25] with only requiring 5% of Params and 16% of MACs). 2.