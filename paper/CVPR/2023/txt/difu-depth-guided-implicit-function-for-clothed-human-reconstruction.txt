Abstract implicit
Recently, function (IF)-based methods for clothed human reconstruction using a single image have received a lot of attention. Most existing methods rely on a 3D embedding branch using volume such as the skinned multi-person linear (SMPL) model, to compensate for the lack of information in a single image. Beyond the SMPL, which provides skinned parametric human 3D information, in this paper, we propose a new IF-based method, DIFu, that utilizes a projected depth prior containing textured
In particu-and non-parametric human 3D information. lar, DIFu consists of a generator, an occupancy prediction network, and a texture prediction network. The generator takes an RGB image of the human front-side as input, and hallucinates the human back-side image. After that, depth maps for front/back images are estimated and projected into 3D volume space. Finally, the occupancy prediction net-work extracts a pixel-aligned feature and a voxel-aligned feature through a 2D encoder and a 3D encoder, respec-tively, and estimates occupancy using these features. Note that voxel-aligned features are obtained from the projected depth maps, thus it can contain detailed 3D information such as hair and cloths. Also, colors of each query point are also estimated with the texture inference branch. The effectiveness of DIFu is demonstrated by comparing to re-cent IF-based models quantitatively and qualitatively. 1.

Introduction
In order to implement virtual reality and an immersive metaverse environment, a method of reconstructing a realis-tic human avatar is an important technology. In particular, if there are methods that can create a complete 3D model with only a single view image without specialized devices such
* Corresponding author.
† This work was done while the first author was pursuing his Master’s degree at Chungnam National University.
Project page is at https://eadcat.github.io/DIFu
Figure 1. (a) Front/back color images. (b) Parametric model vol-ume. (c) Depth maps. (d) Projected depth volume. as 3D scanning, it will be highly useful in various fields such as education, video conference, and entertainment.
Recently, there have been approaches to clothed human re-construction using a single-view image based on the im-plicit function (IF) [1,3,9,10,12,13,22,34,35,43,53]. While
IF-based methods have shown promising results thus far, their performance is limited in unobservable parts. Also, IF-based methods often produce over-smoothed results, partic-ularly in intricate areas such as clothing and hair. Without proper conditions for occluded parts, clothed human recon-struction is still an open and highly ill-posed problem.
To overcome the aforementioned issue, there are sev-eral attempts using parametric models [17, 23, 30, 44] to provide geometric patterns of the human. Leveraging these benefits, Zheng et al. [53] proposed the parametric model-conditioned implicit representation (PaMIR). Using the skinned multi-person linear (SMPL) voxel from pre-trained GCMR [19], PaMIR extracts 3D geometric features to overcome depth ambiguity. Also, Xiu et al. [43] pro-posed a method using the signed distance from the skinned model to the query points. Their approach helps approxi-mate the distance from the skinned model to the target sur-face. While the skinned model can provide global and pose information to condition occluded parts, it may struggle to estimate surface that is far from the skin, such as long hair or skirts. As shown in Figure 1-(b), significant discrepan-cies exist between the detailed surface shape of the skinned model and the target model. Considering that the parametric model-based methods are trained by losses on the sampled
query points, the over-smoothing becomes more severe.
Therefore, in this paper, we propose a new IF-based method using projected depth maps.
Specifically, our method uses a generator to make a back-side color image and front-/back-side depth maps from a front input image.
Then, we project the depth maps into 3D volume space as shown in Figure 1-(d). All information, including RGB im-ages, depth maps and projected depths are passed into the occupancy prediction network to predict the occupancy of each query point. The voxel-aligned features extracted from the 3D encoder in the occupancy prediction network are derived from projected depth maps rather than the SMPL model. As a result, they are more effective at conveying 3D information about the detailed surfaces of the target. To ob-tain the final 3D mesh, the marching cubes algorithm [24] is applied to these occupancies. Similar to the occupancy pre-diction network, we can estimate the colors of each query point via the texture inference network. 2.