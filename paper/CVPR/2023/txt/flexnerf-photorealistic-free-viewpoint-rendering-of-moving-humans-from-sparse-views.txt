Abstract
We present FlexNeRF, a method for photorealistic free-viewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empiri-cally demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is avail-able at: https://flex-nerf.github.io/. 1.

Introduction
Free-viewpoint rendering of a scene is an important problem often attempted under constrained settings: on subjects demonstrating simple motion carefully captured with multiple cameras [17, 19, 20]. However, photoreal-istic free-viewpoint rendering of moving humans captured from a monocular video still remains an unsolved challeng-ing problem, especially with sparse views.
Neural radiance fields (NeRF) have emerged as a pop-ular tool to learn radiance fields from images/videos for novel view-point rendering. Previous approaches assume multiple view-points and often fail on non-rigid human mo-tions. Human-specific NeRFs have recently become pop-ular for learning models using input videos [27, 40]. The current state-of-art approaches such as HumanNeRF [40] have shown impressive progress in this domain. However, there remain several challenges. Firstly, approaches such as HumanNeRF [40] utilize a pose prior and use a canoni-*Part of the work was done while the author was an intern at Amazon. cal configuration (e.g. T-pose) for optimization, which may be well outside the set of observed poses. The underlying optimization becomes challenging especially as the num-ber of observed views become sparse. In contrast, we se-lect a pose from the available set of poses as the canon-ical pose-configuration, similar to previous pose-free ap-proaches such as D-NeRF [29]. This enables best of both worlds; it becomes easier to learn a motion field mapping due to smaller deformations while using a pose prior.
In addition, having the canonical view in the training data pro-vides a strong prior for the optimization of the canonical pose itself. Finally, it allows us to optimize the canonical configuration with our novel pose-independent temporal de-formation. We demonstrate that this architectural change provides significantly better results compared to existing approaches [24, 40].
In addition, approaches such as HumanNeRF [40] de-pend on the estimated pose for the canonical configuration optimization. Errors in the initial pose estimation, for ex-ample, due to strong motion blur cause challenges in pose correction. The underlying assumption that the non-rigid motion is pose-dependent often fails in scenarios with com-plex clothing and accessories, hair styles, and large limb movements. Our proposed pose-independent temporal de-formation helps to supplement the missing information in its pose-dependent counterpart.
To this end, we introduce FlexNeRF, a novel approach for jointly learning a pose-dependent motion field and pose-independent temporal deformation within the NeRF frame-work for modeling human motions. Moreover, we intro-duce a novel cycle consistency loss in our framework, fur-ther capitalizing on the fact that our canonical pose corre-sponds to one of the captured frames. The consistency reg-ularizes the estimated deformation fields by mapping back and forth between each view and the canonical pose. More-over, the information content of any frame in a motion se-quence has a strong similarity to that of its neighbours.
Hence, we propose to utilize this contextual information present in a consecutive set of frames to aid learning by
imposing a temporal consistency loss. We additionally reg-ularize the training by adding a supplementary loss based on the segmentation masks. Our approach allows photo-realistic rendering of a moving human even when sparse views are available, by supplementing the pose-dependent motion field with additional information during learning: (i) pose-independent temporal deformation with ample pixel-wise correspondences beyond the (typically 24) pose point-correspondences, and (ii) consistency constraints/losses. In summary, our paper makes the following contributions:
• We propose a novel approach to learn pose-independent temporal deformation to complement the pose-dependent motion for modeling humans in video, using one of the views as the canonical view.
• We propose a novel cyclic-consistency loss to regular-ize the learned deformations.
• We propose a temporal-consistency loss to aid learn-ing with contextual information present in neighbour-ing frames, as well as to maintain consistency across consecutive rendered frames.
• Our approach outperforms the state-of-the-art ap-proaches, with significant improvement in case of sparse views. 2.