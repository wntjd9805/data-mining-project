Abstract
Predicting a high quality depth map from a single im-age is a challenging task, because it exists infinite pos-sibility to project a 2D scene to the corresponding 3D scene. Recently, some studies introduced multi-head at-tention (MHA) modules to perform long-range interaction, which have shown significant progress in regressing the depth maps. The main functions of MHA can be loosely summarized to capture long-distance information and re-port the attention map by the relationship between pixels.
However, due to the quadratic complexity of MHA, these methods can not leverage MHA to compute depth features in high resolution with an appropriate computational com-plexity. In this paper, we exploit a depth-wise convolution to obtain long-range information, and propose a novel trap attention, which sets some traps on the extended space for each pixel, and forms the attention mechanism by the fea-ture retention ratio of convolution window, resulting in that the quadratic computational complexity can be converted to linear form. Then we build an encoder-decoder trap depth estimation network, which introduces a vision transformer as the encoder, and uses the trap attention to estimate the depth from single image in the decoder. Extensive experi-mental results demonstrate that our proposed network can outperform the state-of-the-art methods in monocular depth estimation on datasets NYU Depth-v2 and KITTI, with sig-nificantly reduced number of parameters. Code is available at: https://github.com/ICSResearch/TrapAttention. 1.

Introduction
Depth estimation is a classical problem in computer vi-sion (CV) field and is a fundamental component for vari-ous applications, such as, scene understanding, autonomous driving, and 3D reconstruction. Estimating the depth map from a single RGB image is a challenge, since the same 2D scene can project an infinite number of 3D scenes. There-*Corresponding author
Figure 1.
Illustration of trap attention for monocular deep esti-mation. Note that trap attention can significantly enhance depth estimation, as evidenced by the clearer depth differences between the table/chairs and the background. fore, the traditional depth estimation methods [27, 28, 33] are often only suitable for predicting low-dimension, sparse distances [27], or known and fixed targets [28], which obvi-ously limits their application scenarios.
To overcome these constraints, many studies [1, 8, 9, 20] have employed the deep neural networks to directly obtain high-quality depth maps. However, most of these research focuses on improving the performance of depth estimation networks by designing more complex or large-scale mod-els. Unfortunately, such a line of research would render the depth estimation task a simple model scale problem with-out the trade-off between performance and computational budget.
Recently, several practitioners and researchers in monoc-ular depth estimation [3, 17, 45] introduced the multi-head attention (MHA) modules to perform the long-range inter-Figure 2. Overview of our trap depth estimation network, which includes an encoder and a decoder. TB, TI and BS denote the trap block, trap interpolation and block selection unit, respectively. TB is the basic block of our decoder, which decodes the depth feature from coarse to fine in five stages. TB consists of a depth-wise (DW) convolution layer, a trap attention (TA) unit and a convolution based MLP. The size of decoder depends on an arbitrary channel dimension (denoted as C). “⊕” denotes the addition operation. putational complexity of high resolution depth map for Ad-aBin or NeW CRFs is typically expensive, i.e., for an h × w image, its complexity is O(h2w2).
To reduce the computational complexity, in this work, we firstly exploit a deep-wise convolution layer to compute the long-distance information and then propose an attention mechanism, called trap attention, which leverages various manual traps to remove some features in extended space, and exploits a 3 × 3 convolution window to compute rela-tionship and attention map. As a result, the quadratic com-putational complexity O(h2w2) can be converted to linear form O(hw). As illustrated by the example in Figure 1, the proposed trap attention is highly effective for depth esti-mation, which can allocate more computational resource to-ward the informative features, i.e., edges of table and chairs, and output a refined depth map from coarse depth map.
Based on this attention mechanism, we finally build an encoder-decoder depth estimation network, which intro-duces a vision transformer as the encoder, and uses the trap attention to estimate the depth from single image in the de-coder. We can build our depth estimation network of differ-ent scales according to the depth estimation scene, which can obtain a balance between performance and computa-tional budget. Experimental results show that our depth estimation network outperform previous estimation meth-ods by remarkable margin on two most popular indoor and outdoor datasets, NYU [36] and KITTI [11], respectively. (a) t1 (b) t2 (c) t3 (d) t4
Figure 3. The curves for trap functions used in trap attention with-out the rounded operation. (a) and (d) are two similar curves. (b) has a higher frequency. (c) has a different initial phase with other curves. action, which have shown considerable progress in regress-ing the depth maps. Representative works of such methods are AdaBin [3] and NeW CRFs [45]. Nevertheless, due to the quadratic computational complexity of MHA, the com-Specifically, our model can obtain consistent predictions with sharp details on visual representations, and achieve the new state-of-the-art performance in monocular depth esti-mation, with only 35% parameters of the prior state-of-the-art methods.
In summary, our main contributions are as follows:
• We use a depth-wise convolution to capture long-distance information and introduce an extra attention mechanism to compute the relationships between fea-tures, which is an efficient alternative to MHA, result-ing in that the computational complexity is reduced from O(h2w2) to O(hw).
• We propose a novel attention mechanism that can al-locate more computational resource toward the infor-mative features, called trap attention, which is highly effective for depth estimation.
• We build an end-to-end trap network for monocular depth estimation, which can obtain the state-of-the-art performance on NYU and KITTI datasets, with signif-icantly reduced number of parameters. 2.