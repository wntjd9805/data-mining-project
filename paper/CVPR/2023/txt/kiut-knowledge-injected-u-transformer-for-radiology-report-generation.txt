Abstract
Radiology report generation aims to automatically gen-erate a clinically accurate and coherent paragraph from the X-ray image, which could relieve radiologists from the heavy burden of report writing. Although various image caption methods have shown remarkable performance in the natural image field, generating accurate reports for medical images requires knowledge of multiple modalities, including vision, language, and medical terminology. We propose a Knowledge-injected U-Transformer (KiUT) to learn multi-level visual representation and adaptively dis-till the information with contextual and clinical knowledge for word prediction. In detail, a U-connection schema be-tween the encoder and decoder is designed to model in-teractions between different modalities. And a symptom graph and an injected knowledge distiller are developed to assist the report generation. Experimentally, we outper-form state-of-the-art methods on two widely used bench-mark datasets: IU-Xray and MIMIC-CXR. Further experi-mental results prove the advantages of our architecture and the complementary benefits of the injected knowledge. 1.

Introduction
Radiology images (e.g., chest X-ray) play an indispens-able role in routine diagnosis and treatment, and the radiol-ogy reports of images are essential in facilitating later treat-ments. Getting a hand-crafted report is a time-consuming and error-prone process. Given a radiology image, only experienced radiologists can accurately interpret the image and write down corresponding findings. Therefore, auto-matically generating high-quality radiology reports is ur-gently needed to help radiologists eliminate the overwhelm-ing volume of radiology images. In recent years, radiology report generation has attracted much attention in the deep learning and medical domain. The encoder-decoder archi-tecture inspired by neural machine translation [34] has been widely adopted by most existing methods [17, 24, 40, 42]. (cid:12) Corresponding Author.
Figure 1. A transformer architecture with U-connection is adopted to generate reports from radiology images. The process involves injecting and distilling visual, clinical, and contextual knowledge
The color labels in the image and report represent the different ab-normal regions and their corresponding description, respectively.
With the recent advent of the attention mechanism, the ar-chitecture’s capability is greatly ameliorated.
Despite the remarkable performance, these models re-strained themselves in the methodology of image caption
[6, 7, 36, 39, 43], and suffer from such data biases: 1) the normal cases dominate the dataset over the abnormal cases; 2) the descriptions of normal regions dominate the entire report. Recently, some methods have been proposed to ei-ther alleviate case-level bias by utilizing posterior and prior knowledge [27] or relieve the region-level bias by distilling the contrastive information of abnormal regions [28].
Thus, in the medical field’s cross-modal task, a model needs to not only capture visual details of every abnor-mal region but also consider the interaction between the vi-sual and textual modalities among different levels. More-over, external clinical knowledge is required to achieve the radiologist-like ability in radiology image understand-ing and report writing. The external knowledge, e.g., the clinical entities and relationships, could be pre-defined by experts or mined from medical documents. However, di-rectly adopting the knowledge brings inconsistencies due to the heterogeneous context embedding space [21]. And too complex knowledge may be prone to distract the visual en-coder and divert the representation [27].
Instead of using external knowledge to augment the fea-ture extraction like previous approaches [27, 44], we pro-pose to introduce the injected knowledge in the final decod-ing stage. A graph with the clinical entities, i.e., symptoms and their relationships, is constructed under the guidance of professional doctors. These entities have homogeneous embedding space with the training corpus, and this signal could be injected smoothly with visual and contextual infor-mation. We further design the Injected Knowledge Distiller on top of the decoder to distill contributive knowledge from visual, contextual, and clinical knowledge.
Following these premises, we explore a novel framework dubbed as Knowledge-injected and U-Transformer (KiUT) to achieve the radiologist-like ability to understand the ra-diology images and write reports. As Fig. 1 shows, it con-sists of a Region Relationship Encoder and Decoder with U-connection architecture and Injected Knowledge Distiller.
Our contributions can be summarized as follows:
• We propose a novel model following the encoder-decoder architecture with U-connection that fully ex-ploits different levels of visual information instead of only one single input from the visual modality. In our experiments, the U-connection schema presents im-provement not only in radiology report generation but also in the natural image captioning task.
• Our proposed model injects clinical knowledge by constructing a symptom graph, combining it with the visual and contextual information, and distilling them when generating the final words in the decoding stage.
• The Region Relationship Encoder is developed to re-store the extrinsic and intrinsic relationships among image regions for extracting abnormal region features, which are crucial in the medical domain.
• We evaluate our approach on two public radiology report generation datasets, IU-Xray [8] and MIMIC-CXR [18]. KiUT achieves state-of-the-art perfor-mance on the two benchmark datasets. 2.