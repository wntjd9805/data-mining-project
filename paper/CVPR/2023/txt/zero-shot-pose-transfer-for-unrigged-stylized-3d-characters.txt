Abstract
Transferring the pose of a reference avatar to stylized 3D characters of various shapes is a fundamental task in com-puter graphics. Existing methods either require the stylized characters to be rigged, or they use the stylized character in the desired pose as ground truth at training. We present a zero-shot approach that requires only the widely available deformed non-stylized avatars in training, and deforms styl-ized characters of significantly different shapes at inference.
Classical methods achieve strong generalization by deform-ing the mesh at the triangle level, but this requires labelled correspondences. We leverage the power of local deforma-tion, but without requiring explicit correspondence labels.
We introduce a semi-supervised shape-understanding mod-ule to bypass the need for explicit correspondences at test time, and an implicit pose deformation module that deforms individual surface points to match the target pose. Further-more, to encourage realistic and accurate deformation of
*Work done during Jiashun Wang’s internship at NVIDIA. stylized characters, we introduce an efficient volume-based test-time training procedure. Because it does not need rig-ging, nor the deformed stylized character at training time, our model generalizes to categories with scarce annotation, such as stylized quadrupeds. Extensive experiments demon-strate the effectiveness of the proposed method compared to the state-of-the-art approaches trained with compara-ble or more supervision. Our project page is available at https://jiashunwang.github.io/ZPT/ 1.

Introduction
Stylized 3D characters , such as those in Fig. 1, are com-monly used in animation, movies, and video games. De-forming these characters to mimic natural human or animal poses has been a long-standing task in computer graphics.
Different from the 3D models of natural humans and an-imals, stylized 3D characters are created by professional artists through imagination and exaggeration. As a result, each stylized character has a distinct skeleton, shape, mesh
topology, and usually include various accessories, such as a cloak or wings (see Fig. 1). These variations hinder the process of matching the pose of a stylized 3D character to that of a reference avatar, generally making manual rigging a requirement. Unfortunately, rigging is a tedious process that requires manual effort to create the skeleton and skin-ning weights for each character. Even when provided with manually annotated rigs, transferring poses from a source avatar onto stylized characters is not trivial when the source and target skeletons differ. Automating this procedure is still an open research problem and is the focus of many recent works [2, 4, 24, 52]. Meanwhile, non-stylized 3D humans and animals have been well-studied by numerous prior works [35, 40, 54, 62, 68]. A few methods generously provide readily available annotated datasets [11, 12, 41, 68], or carefully designed parametric models [40, 51, 68]. By taking advantage of these datasets [12,41], several learning-based methods [7, 14, 35, 62, 67] disentangle and transfer poses between human meshes using neural networks. How-ever, these methods (referred to as “part-level” in the fol-lowing) carry out pose transfer by either globally deform-ing the whole body mesh [14, 22, 47, 67] or by transform-ing body parts [35, 48], both of which lead to overfitting on the training human meshes and fail to generalize to styl-ized characters with significantly different body part shapes.
Interestingly, classical mesh deformation methods [55, 56] (referred to as “local” in the following) can transfer poses between a pair of meshes with significant shape differences by computing and transferring per-triangle transformations through correspondence. Though these methods require manual correspondence annotation between the source and target meshes, they provide a key insight that by transform-ing individual triangles instead of body parts, the mesh de-formation methods are more agnostic to a part’s shape and can generalize to meshes with different shapes.
We marry the benefits of learning-based methods [7, 14, 35, 62, 67] with the classic local deformation approach [55] and present a model for unrigged, stylized character defor-mation guided by a non-stylized biped or quadruped avatar.
Notably, our model only requires easily accessible posed human or animal meshes for training and can be directly applied to deform 3D stylized characters with a signifi-cantly different shape at inference. To this end, we implic-itly operationalize the key insight from the local deforma-tion method [55] by modeling the shape and pose of a 3D character with a correspondence-aware shape understand-ing module and an implicit pose deformation module. The shape understanding module learns to predict the part seg-mentation label (i.e., the coarse-level correspondence) for each surface point, besides representing the shape of a 3D character as a latent shape code. The pose deformation module is conditioned on the shape code and deforms in-dividual surface point guided by a target pose code sampled from a prior pose latent space [50]. Furthermore, to encour-age realistic deformation and generalize to rare poses, we propose a novel volume-based test-time training procedure that can be efficiently applied to unseen stylized characters.
During inference, by mapping biped or quadruped poses from videos, in addition to meshes to the prior pose la-tent space using existing works [32, 51, 53], we can transfer poses from different modalities onto unrigged 3D stylized characters. Our main contributions are:
• We propose a solution to a practical and challenging task – learning a model for stylized 3D character de-formation with only posed human or animal meshes.
• We develop a correspondence-aware shape under-standing module, an implicit pose deformation mod-ule, and a volume-based test-time training procedure to generalize the proposed model to unseen stylized characters and arbitrary poses in a zero-shot manner.
• We carry out extensive experiments on both hu-mans and quadrupeds to show that our method pro-duces more visually pleasing and accurate deforma-tions compared to baselines trained with comparable or more supervision. 2.