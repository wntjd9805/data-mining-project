Abstract
When it comes to deploying deep vision models, the be-havior of these systems must be explicable to ensure confi-dence in their reliability and fairness. A common approach to evaluate deep learning models is to build a labeled test set with attributes of interest and assess how well it per-forms. However, creating a balanced test set (i.e., one that is uniformly sampled over all the important traits) is of-ten time-consuming, expensive, and prone to mistakes. The question we try to address is: can we evaluate the sensi-tivity of deep learning models to arbitrary visual attributes without an annotated test set?
This paper argues the case that Zero-shot Model Diag-nosis (ZOOM) is possible without the need for a test set nor labeling. To avoid the need for test sets, our system relies on a generative model and CLIP. The key idea is enabling the user to select a set of prompts (relevant to the prob-lem) and our system will automatically search for seman-tic counterfactual images (i.e., synthesized images that flip the prediction in the case of a binary classifier) using the generative model. We evaluate several visual tasks (classi-fication, key-point detection, and segmentation) in multiple visual domains to demonstrate the viability of our method-ology. Extensive experiments demonstrate that our method is capable of producing counterfactual images and offering sensitivity analysis for model diagnosis without the need for a test set. 1.

Introduction
Deep learning models inherit data biases, which can be accentuated or downplayed depending on the model’s ar-chitecture and optimization strategy. Deploying a computer vision deep learning model requires extensive testing and evaluation, with a particular focus on features with poten-tially dire social consequences (e.g., non-uniform behav-ior across gender or ethnicity). Given the importance of the problem, it is common to collect and label large-scale datasets to evaluate the behavior of these models across attributes of interest. Unfortunately, collecting these test
*Equal contribution.
Figure 1. Given a differentiable deep learning model (e.g., a cat/dog classifier) and user-defined text attributes, how can we de-termine the model’s sensitivity to specific attributes without us-ing labeled test data? Our system generates counterfactual images (bottom right) based on the textual directions provided by the user, while also computing the sensitivity histogram (top right). datasets is extremely time-consuming, error-prone, and ex-pensive. Moreover, a balanced dataset, that is uniformly distributed across all attributes of interest, is also typically impractical to acquire due to its combinatorial nature. Even with careful metric analysis in this test set, no robustness nor fairness can be guaranteed since there can be a mis-match between the real and test distributions [25]. This research will explore model diagnosis without relying on a test set in an effort to democratize model diagnosis and lower the associated cost.
Counterfactual explainability as a means of model diag-nosis is drawing the community’s attention [5,20]. Counter-factual images visualize the sensitive factors of an input im-age that can influence the model’s outputs. In other words, counterfactuals answer the question: “How can we modify the input image x (while fixing the ground truth) so that the model prediction would diverge from y to ˆy?”. The param-eterization of such counterfactuals will provide insights into identifying key factors of where the model fails. Unlike ex-isting image-space adversary techniques [4,18], counterfac-tuals provide semantic perturbations that are interpretable by humans. However, existing counterfactual studies re-On the other hand, quire the user to either collect uniform test sets [10], anno-tate discovered bias [15], or train a model-specific explana-tion every time the user wants to diagnose a new model [13]. recent advances in Contrastive
Language-Image Pretraining (CLIP) [24] can help to over-come the above challenges. CLIP enables text-driven ap-plications that map user text representations to visual man-ifolds for downstream tasks such as avatar generation [7], motion generation [37] or neural rendering [22, 30].
In the domain of image synthesis, StyleCLIP [21] reveals that text-conditioned optimization in the StyleGAN [12] latent space can decompose latent directions for image editing, allowing for the mutation of a specific attribute without dis-turbing others. With such capability, users can freely edit semantic attributes conditioned on text inputs. This paper further explores its use in the scope of model diagnosis.
The central concept of the paper is depicted in Fig. 1.
Consider a user interested in evaluating which factors con-tribute to the lack of robustness in a cat/dog classifier (target model). By selecting a list of keyword attributes, the user is able to (1) see counterfactual images where semantic vari-ations flip the target model predictions (see the classifier score in the top-right corner of the counterfactual images) and (2) quantify the sensitivity of each attribute for the tar-get model (see sensitivity histogram on the top). Instead of using a test set, we propose using a StyleGAN generator as the picture engine for sampling counterfactual images.
CLIP transforms user’s text input, and enables model diag-nosis in an open-vocabulary setting. This is a major advan-tage since there is no need for collecting and annotating im-ages and minimal user expert knowledge. In addition, we are not tied to a particular annotation from datasets (e.g., specific attributes in CelebA [16]).
To summarize, our proposed work offers three major im-provements over earlier efforts:
• The user requires neither a labeled, balanced test dataset, and minimal expert knowledge in order to evaluate where a model fails (i.e., model diagnosis). In addition, the method provides a sensitivity histogram across the attributes of interest.
• When a different target model or a new user-defined attribute space is introduced, it is not necessary to re-train our system, allowing for practical use.
• The target model fine-tuned with counterfactual im-ages not only slightly improves the classification per-formance, but also greatly increases the distributional robustness against counterfactual images. 2.