Abstract
Natural videos captured by consumer cameras often suf-fer from low framerate and motion blur due to the combi-nation of dynamic scene complexity, lens and sensor imper-fection, and less than ideal exposure setting. As a result, computational methods that jointly perform video frame in-terpolation and deblurring begin to emerge with the unreal-istic assumption that the exposure time is known and fixed.
In this work, we aim ambitiously for a more realistic and challenging task - joint video multi-frame interpolation and deblurring under unknown exposure time. Toward this goal, we first adopt a variant of supervised contrastive learn-ing to construct an exposure-aware representation from in-put blurred frames. We then train two U-Nets for intra-motion and inter-motion analysis, respectively, adapting to the learned exposure representation via gain tuning. We finally build our video reconstruction network upon the ex-posure and motion representation by progressive exposure-adaptive convolution and motion refinement. Extensive ex-periments on both simulated and real-world datasets show that our optimized method achieves notable performance gains over the state-of-the-art on the joint video ×8 interpo-lation and deblurring task. Moreover, on the seemingly im-plausible ×16 interpolation task, our method outperforms existing methods by more than 1.5 dB in terms of PSNR. 1.

Introduction
When capturing videos, shutter period (i.e., the inverse of the framerate) and exposure time are two major factors that we are able to manipulate for improved video qual-ity, compared to other confounding factors such as object motion in the scene, lens imperfection, and sensor limita-tions [3]. Particularly, a long shutter period corresponds to lower framerate, and a longer exposure time increases the possibility of introducing severer motion blur (see Fig. 1).
Often, we have good control over the shutter period in the
*Corresponding author: rendongweihit@gmail.com.
Figure 1. Visualization of motion blur under different exposure time and object motion. Frames from the left to right are obtained with increasing exposure time, leading to severer degree of motion blur. The situation may be worse if stronger motion is present, as illustrated by images in the second row. form of the framerate, which is, nevertheless, quite lim-ited in consumer cameras (e.g., 30 frames per second, or
FPS). This is not the case for exposure time, which may constantly and dynamically change depending on the video shooting environment, e.g., illumination and reflection con-ditions [25]. Therefore, video frame interpolation (also known as framerate up-conversion [5, 10, 15]) and video deblurring methods (under unknown exposure time) are crucial for improving the quality of low framerate blurred videos, and are widely applicable to video editing, video compression, and slow-motion video generation.
In literature, video frame interpolation [21,30,42,48] and video deblurring [9, 35] have long been treated as individ-ual problems and tackled separately with worth-celebrating successes. A straightforward approach to joint video frame interpolation and deblurring is to deploy deblurring meth-ods followed by frame interpolation. However, this type of cascaded methods usually cannot obtain satisfactory re-construction results, since algorithm-dependent deblurring artifacts would be propagated to and amplified in interpo-lated frames [41]. Similar situations will occur if we cas-cade video frame interpolation first [1]. This inspires re-cent work [1, 34, 41, 47] to cast video frame interpolation and deblurring (or super-resolution) as a joint and emerg-ing low-level vision problem. However, these methods as-sume known and fixed exposure time in the video degrada-tion model, which is unrealistic, especially when the auto-exposure mode is enabled. Thus, they are bound to general-ize poorly in the real-world.
Currently, there are two studies [25, 51] considering the setting of unknown exposure time. Zhang et al. [51] com-puted generic quadratic motion trajectories from consec-utive blurred (or estimated sharp) frames. They directly cascaded existing deblurring and frame interpolation meth-ods, suffering from the error propagation problem. Benefit-ing from additional event cameras [4], Kim [25] proposed an event-guided and end-to-end optimized video deblurring method under unknown exposure time, but event sensors have not been equipped on consumer cameras, restricting its practical applications.
In this work, we aim ambitiously for the more realis-tic and challenging task - joint video multi-frame interpola-tion and deblurring under unknown exposure time. Our pri-mary design philosophy is adaptive computation: we adapt our Video frame Interpolation and Deblurring method un-der Unknown Exposure time (VIDUE) to exposure-aware and motion-aware representations, where the computation of the motion-aware representation is further adapted to the exposure-aware representation. To achieve this, we first adopt a variant of supervised contrastive learning to ex-tract an exposure-aware representation from a low framerate blurred video. We then train two U-Nets: one is responsible for intra-motion analysis (e.g., assessing motion complexity within each frame); the other is responsible for inter-motion analysis (e.g., assessing motion continuity between frames).
We adapt the two U-Nets to the exposure-aware representa-tion via gain tuning [31] (also can be seen as a variant of sequence-and-excitation [16]). We last develop a video re-construction network with a U-Net-like structure, which en-ables exposure-adaptive convolution and motion refinement in a progressive fashion.
Extensive experiments on both synthetic and real datasets show that the proposed VIDUE consistently pro-duces higher-quality deblurred and ×8 interpolated frames both visually and in terms of standard quality metrics.
Moreover, VIDUE exhibits significant performance gains (≥ 1.5 dB) over the state-of-the-art on the seemingly im-plausible ×16 interpolation and deblurring task. 2.