Abstract
Supervised crowd counting relies heavily on costly man-ual labeling, which is difficult and expensive, especially in dense scenes. To alleviate the problem, we propose a novel unsupervised framework for crowd counting, named
CrowdCLIP. The core idea is built on two observations: 1) the recent contrastive pre-trained vision-language model (CLIP) has presented impressive performance on various downstream tasks; 2) there is a natural mapping between crowd patches and count text. To the best of our knowl-edge, CrowdCLIP is the first to investigate the vision-language knowledge to solve the counting problem. Specif-ically, in the training stage, we exploit the multi-modal ranking loss by constructing ranking text prompts to match the size-sorted crowd patches to guide the image encoder learning.
In the testing stage, to deal with the diversity of image patches, we propose a simple yet effective pro-gressive filtering strategy to first select the highly poten-tial crowd patches and then map them into the language space with various counting intervals. Extensive exper-iments on five challenging datasets demonstrate that the proposed CrowdCLIP achieves superior performance com-pared to previous unsupervised state-of-the-art counting methods. Notably, CrowdCLIP even surpasses some pop-ular fully-supervised methods under the cross-dataset set-ting. The source code will be available at https:// github.com/dk-liang/CrowdCLIP. 1.

Introduction
Crowd counting aims to estimate the number of peo-ple from images or videos in various crowd scenes, which has received tremendous attention due to its wide applica-tions in public safety and urban management [14, 42]. It is very challenging to accurately reason the count, especially
*Equal contribution. †Corresponding author.
Work done when Dingkang Liang was an intern at Baidu.
Figure 1. (a) The supervised methods require point-level annota-tions, which need heavy manual labor to label a large-scale dataset.
The proposed method transfers the vision-language knowledge to perform unsupervised crowd counting without any annotation; (b)
Crowd counting aims to calculate the number of human heads, while some crowd patches do not contain human heads, i.e., am-biguous patches. in dense regions where the crowd gathers.
The recent crowd counting methods [5, 18, 41, 62] at-tempt to regress a density map (Fig. 1(a)). To train such density-based models, point-level annotations are required, i.e., assigning a point in each human head. However, an-notating point-level object annotations is an expensive and laborious process. For example, the NWPU-Crowd [48] dataset, containing 5, 109 images, needs 30 annotators and 3, 000 human hours for the entire annotation process. To reduce the annotation cost, some weakly-supervised meth-ods [16, 19, 59] and semi-supervised methods [22, 28] are proposed, where the former usually adopts the count-level
annotation as supervision, and the latter uses a small frac-tion of fully-labeled images and massive unlabeled images for training. However, both weakly and semi-supervised methods still need considerable label costs, especially when annotating dense or blurry images.
Considering the above issues, a crowd counting model that can be trained without any labeled data is worth ex-ploring. So far, there is only one approach called CSS-CCNN [3] for pure unsupervised crowd counting. Based on the idea that natural crowds follow a power law dis-tribution, CSS-CCNN ensures the distribution of predic-tions is matched to the prior. Though the performance of CSS-CCNN is better than the random paradigm, there is a significant performance gap compared to the popu-lar fully supervised methods [4, 62]. Recently, Contrastive
Language-Image Pre-Training (CLIP) as a new paradigm has drawn increasing attention due to its powerful transfer ability. By using large-scale noisy image-text pairs to learn visual representation, CLIP has achieved promising perfor-mance on various downstream vision tasks (e.g., object de-tection [38], semantic segmentation [56], generation [10]).
Whereas, how to apply such a language-driven model to crowd counting has not been explored. Obviously, CLIP cannot be directly applied to the counting task since there is no such count supervision during the contrastive pre-training of CLIP.
A natural way to exploit the vision-language knowledge is to discretize the crowd number into a set of intervals, which transfers the crowd counting to a classification in-stead of a regression task. Then one can directly calcu-late the similarity between the image embedding from the image encoder and the text embedding from the text en-coder and choose the most similar image-text pair as the prediction count (called zero-shot CLIP). However, we re-veal that the zero-shot CLIP reports unsatisfactory perfor-mance, attributed to two crucial reasons: 1) The zero-shot
CLIP can not well understand crowd semantics since the original CLIP is mainly trained to recognize single-object images [38]; 2) Due to the non-uniform distribution of the crowd, the image patches are of high diversity while count-ing aims to calculate the number of human heads within each patch. Some crowd patches that do not contain human heads may cause ambiguity to CLIP, as shown in Fig. 1(b).
To relieve the above problems, in this paper, we propose
CrowdCLIP, which adapts CLIP’s strong vision-category correspondence capability to crowd counting in an unsu-pervised manner, as shown in Fig. 1(a). Specifically, first, we construct ranking text prompts to describe a set of size-sorted image patches during the training phase. As a re-sult, the image encoder can be fine-tuned to better capture the crowd semantics through the multi-modal ranking loss.
Second, during the testing phase, we propose a simple yet effective progressive filtering strategy consisting of three stages to choose high-related crowd patches. In particular, the first two stages aim to choose the high-related crowd patches with a coarse-to-fine classification paradigm, and the latest stage is utilized to map the corresponding crowd patches into an appropriate count. Thanks to such a progres-sive inference strategy, we can effectively reduce the impact of ambiguous crowd patches.
Extensive experiments conducted on five challenging datasets in various data settings demonstrate the effective-ness of our method. In particular, our CrowdCLIP signifi-cantly outperforms the current unsupervised state-of-the-art method CSS-CCNN [3] by 35.2% on the challenging UCF-QNRF dataset in terms of the MAE metric. Under cross-dataset validation, our method even surpasses some popular fully-supervised works [40, 62].
Our major contributions can be summarized as follows: 1) In this paper, we propose a novel unsupervised crowd counting method named CrowdCLIP, which innovatively views crowd counting as an image-text matching problem.
To the best of our knowledge, this is the first work to trans-fer vision-language knowledge to crowd counting. 2) We introduce a ranking-based contrastive fine-tuning strategy to make the image encoder better mine potential crowd se-mantics. In addition, a progressive filtering strategy is pro-posed to choose the high-related crowd patches for mapping to an appropriate count interval during the testing phase. 2.