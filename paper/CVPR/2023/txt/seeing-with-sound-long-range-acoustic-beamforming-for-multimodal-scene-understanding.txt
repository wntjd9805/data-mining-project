Abstract
Mobile robots, including autonomous vehicles rely heav-ily on sensors that use electromagnetic radiation like lidars, radars and cameras for perception. While effective in most scenarios, these sensors can be unreliable in unfavorable environmental conditions, including low-light scenarios and adverse weather, and they can only detect obstacles within their direct line-of-sight. Audible sound from other road users propagates as acoustic waves that carry information even in challenging scenarios. However, their low spatial resolution and lack of directional information have made them an overlooked sensing modality. In this work, we intro-duce long-range acoustic beamforming of sound produced by road users in-the-wild as a complementary sensing modal-ity to traditional electromagnetic radiation-based sensors.
To validate our approach and encourage further work in the field, we also introduce the first-ever multimodal long-range acoustic beamforming dataset. We propose a neural aper-ture expansion method for beamforming and demonstrate its effectiveness for multimodal automotive object detection when coupled with RGB images in challenging automotive scenarios, where camera-only approaches fail or are unable to provide ultra-fast acoustic sensing sampling rates. Data and code can be found here1. 1.

Introduction
Autonomous mobile robots of today predominantly rely on several electromagnetic (EM) radiation-based sensing modalities such as camera, radar and lidar for diverse scene understanding tasks, including object detection, semantic segmentation, lane detection, and intent prediction. The most promising approaches rely on fused data input from these camera, lidar and radar sensor configurations [7, 42, 50] and robust data-driven perception algorithms using convolutional neural networks or vision transformers. However, existing camera/radar/lidar stacks do not return signal for objects with low reflectance and in conditions where light-based sensors struggle, such as severe scattering due to fog. All existing
EM radiation-based sensor systems (active or passive) are fundamentally limited by the propagation of EM waves.
Acoustic waves are an alternative and complementary sensing modality that are not subject to these limita-tions. Every automotive vehicle generates noise due to engine/transmission, aerodynamics, braking, and contact with the road. Even electric vehicles are required by law to emit sound to alert pedestrians [36]. However, acoustic sensing is not without challenges. Spatially resolving the acoustic spectrum at meter wavelengths (e.g., a 1 kHz sound wave has a wavelength of about 35 cm in air) has limited existing approaches to low-resolution tracking of 3D spatial coordinates [11–13, 32, 44].
In this work, we show that acoustic sensing is comple-mentary to existing EM wave-based sensors, robust to chal-lenging scenarios, and achieves improved performance when combined with existing vision-only approaches. To this end, we captured a large multimodal dataset with a prototype vehicle equipped with a 1024 (32x32 grid) microphone array and a plethora of vision sensors, and had them labeled by human annotators, which we release as the first multimodal long-range beamforming dataset. To the best of our knowl-edge, there is no such large and diverse multimodal acoustic beamforming dataset, as also illustrated in Table 1. We ad-ditionally propose a neural acoustic beamforming method for small aperture microphone arrays via learned aperture expansion. The aperture-expanded beamforming maps re-cover spatial resolution typically lost in sound measurements, and facilitate fusion with visual inference tasks. We assess multimodal visual and acoustic vision tasks in diverse real-world driving scenarios. We validate that visual and acoustic signals can complement each other in challenging automo-tive scenarios and can enable future frame predictions at kHz frequencies. We also demonstrate that object detec-tion using vision and acoustic signals outperform that of vision-only signals in challenging low-light scenarios. Fur-thermore, we show the applicability of acoustic sensing in non-line-of-sight and partially occluded scenes where purely vision-based sensing fails. 1light.princeton.edu/seeingwithsound
Specifically, we make the following contributions:
Figure 1. We capture a large dataset of acoustic pressure signals at several frequencies from roadside noise using our prototype test vehicle (left). Of the available 250-5000 Hz frequency bands in the dataset, we visualize beamformed signals at the 4000 Hz octave band here (middle left). Using RGB-only result in missed and inaccurate detections at night (middle right). The complementary nature of acoustic signals, on the other hand, helps robustly detect the objects in challenging night scenarios (right).
Table 1. Existing beamforming works and datasets are limited to just a few hundred processed beamforming maps and a single RGB camera data. In stark contrast, our dataset is very large with acoustic signals captured at 40kHz frequency across 11 frequency bands in diverse urban scenarios.
Dataset
Michel et al. [35] Zunino et al. [51] Guidati [21]
Proposed
Ego Motion
Frequency Bands
Frequency Range
Processed Beamforming Frames
RGB Cameras
RGB Frames
Lidar Point Clouds
Annotated Frames
Static 1
✗
✗ 1
✗
✗
✗
Static 1 500 - 6400 Hz 151 1 151
✗
✗
Static 1
✗
✗
✗
✗
✗
✗
Dynamic 11 1 Hz - 20 kHz 42250 5 3.2 Mio 480,000 16,324
• We introduce long-range acoustic beamforming of road noise as a complementary sensing modality for auto-motive perception, and introduce the first annotated long-range acoustic beamforming dataset comprising of sound measurements from planar microphone array, lidar, RGB images, GPS and IMU data, in urban driving scenarios.
• We propose neural acoustic beamforming for small aper-ture microphone arrays via learned aperture expansion.
We validate that this beamforming approach can learn features with a spatial resolution that allows for fusion with existing RGB vision tasks.
• We validate that the proposed method complements ex-isting modalities and outperforms existing RGB-only and audio-only detection methods in challenging sce-narios with occlusion or poor lighting.
Scope As the proposed acoustic sensing modality relies on passive sound from traffic participants, beamforming measurements are fundamentally limited to sound-producing vehicles. Beamforming of quieter traffic participants such as pedestrians and bicycles is challenging. However, we show that infusing existing vision stacks with acoustic signals can enable robust scene understanding in challenging scenarios such as night scenes and under severe occlusion. 2.