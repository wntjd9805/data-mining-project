Abstract
ESPCN
SRResNet
ResNet18
MobileNet-v2
Neural network quantization is a popular approach for model compression. Modern hardware supports quanti-zation in mixed-precision mode, which allows for greater compression rates but adds the challenging task of search-ing for the optimal bit width. The majority of existing searchers find a single mixed-precision architecture. To select an architecture that is suitable in terms of perfor-mance and resource consumption, one has to restart search-ing multiple times. We focus on a specific class of methods that find tensor bit width using gradient-based optimization.
First, we theoretically derive several methods that were em-pirically proposed earlier. Second, we present a novel One-Shot method that finds a diverse set of Pareto-front architec-tures in O(1) time. For large models, the proposed method is 5 times more efficient than existing methods. We verify the method on two classification and super-resolution mod-els and show above 0.93 correlation score between the pre-dicted and actual model performance. The Pareto-front ar-chitecture selection is straightforward and takes only 20 to 40 supernet evaluations, which is the new state-of-the-art result to the best of our knowledge. 1.

Introduction
In recent years, neural network quantization [31] has be-come a popular hardware-friendly compression technique.
It is common to quantize linear and convolutional layer operands while leaving vector operands unchanged. Mod-ern algorithms achieve lossless quantization into fixed 8-bit integer values in many applications [45, 15, 40, 35, 49, 25, 5]. At higher compression rates, mixed-precision is often needed [22, 44]. For example, models often require 8-bit precision for the first and last layers, while the middle lay-ers can tolerate lower precision [15, 40]. In addition, the selected precision may depend on a quantized operation [7] or a hardware at hand [41]. This motivates many vendors to
) m ( e m
T i 10 0 50 0 200 0 250 0
EdMIPS
DNAS
GMPQ
One-Shot MPS
Figure 1. The searching time taken by each algorithm to dis-cover a single bit width architecture belonging to a Pareto front.
EdMIPS [7], DNAS [44], GMPQ [42], and One-Shot MPS (our) use a proxy dataset for ResNet-18 and MobileNet-v2. Bayesian
Bits [38] and HAQ [41] roughly take 100 times more searching time compared to our method. support mixed-precision models in hardware.
To attain the best mixed-precision performance, it is cru-cial to find an optimal precision for each matrix multipli-cation factor. Unfortunately, all possible bit width combi-nations cannot be examined since the search space scales exponentially with the number of multiplications. The in-ability to predict the influence of individual loss coefficients on the resulting compression rate furthermore exacerbates the difficulty of searching. Existing methods [44, 7, 38, 9] require multiple restarts of the searching process until a sat-isfactory bit width allocation is found. This results in O(N ) searching time, where N is the number of restarts.
The authors of EdMIPS [7] mention that “sometimes, it is mysterious why and how an architecture is found by Neu-ral Architecture Search (NAS)”. We answer this question in the context of EdMIPS and DNAS [44] methods. To do so, we simplify and generalize Bayesian Bits [38], where a variational inference (VI) approach is used to derive the loss function for a hierarchical supernet. Then, we demonstrate how the EdMIPS and DNAS loss functions can be derived.
Next, using our derivation, we propose a novel One-Shot Mixed-Precision Search (One-Shot MPS) method that finds a diverse set of Pareto-front architectures in O(1) time. We extend the commonly used supernet transforma-tion of a floating-point model with a set of trainable func-tions that predict the bit width probability depending on a hardware regularization parameter. For ImageNet [11], we observe at least 0.96 correlation score between child mod-els sampled from a One-Shot model and standalone fine-tuned models, cf . previous result attains a maximum score of 0.55 [16]. Such a high score allows one to plot a Pareto front of performance versus hardware resources using a lin-ear sweep over the regularization parameter, and select the most promising precision given hardware constraints be-fore fine-tuning. The Pareto-front architecture selection is straightforward and takes only 20 to 40 supernet evaluations while existing One-Shot methods require at least 1000 eval-uations [16, 10].
To sum up, our contribution is twofold. First, we pro-vide a theoretical derivation of the earlier empirically-found state-of-the-art searching methods. Second, we propose to augment a supernet with a bit width prediction model that allows searching for Pareto-front bit width combinations corresponding to different compression rates in a constant time. We validate the benefits of the approach on sev-eral widely-used models including mobile-friendly archi-tectures. To the best of our knowledge, the proposed pre-dictor is not described in the existing literature. 2.