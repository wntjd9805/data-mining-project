Abstract
End-to-end text spotting aims to integrate scene text de-tection and recognition into a unified framework. Deal-ing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although
Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. In this paper, we present DeepSolo, a simple DETR-like baseline that lets a single Decoder with Explicit Points Solo for text detec-tion and recognition simultaneously. Technically, for each text instance, we represent the character sequence as or-dered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Besides, we also introduce a text-matching cri-terion to deliver more accurate supervisory signals, thus enabling more efficient training. Quantitative experiments on public benchmarks demonstrate that DeepSolo outper-forms previous state-of-the-art methods and achieves better training efficiency. In addition, DeepSolo is also compati-ble with line annotations, which require much less annota-tion cost than polygons. The code is available at https:
//github.com/ViTAE-Transformer/DeepSolo. 1.

Introduction
Detecting and recognizing text in natural scenes, a.k.a. text spotting, has drawn increasing attention due to its wide range of applications [5, 34, 56, 59], such as autonomous driving [57] and intelligent navigation [7]. How to deal with the relationship between detection and recognition is a long-*Equal contribution. †Corresponding author. This work was done dur-ing Maoyuan Ye’s internship at JD Explore Academy.
Figure 1. Comparison of pipelines and query designs. TrEnc. (TrDec.): Transformer encoder (decoder). Char: character. standing problem in designing the text spotting pipeline and has a significant impact on structure design, spotting perfor-mance, training efficiency, and annotation cost, etc.
Most pioneering end-to-end spotting methods [16,25,28, 29,31,37,43,44,51] follow a detect-then-recognize pipeline, which first detects text instances and then exploits Region-of-Interest (RoI) based connectors to extract features within the detected area, finally feeds them into the following rec-ognizer (Fig. 1a). Although these methods have achieved great progress, there are two main limitations. 1) An extra connector for feature alignment is indispensable. Moreover, some connectors require polygon annotations, which are not applicable when only weak annotations are available. 2) Additional efforts are desired to address the synergy is-sue [17,62] between the detection and recognition modules.
In contrast, the segmentation-based methods [49, 53] try to isolate the two sub-tasks and complete spotting in a paral-lel multi-task framework with a shared backbone (Fig. 1b).
Nevertheless, they are sensitive to noise and require group-ing post-processing to gather unstructured components.
Recently, Transformer [47] has improved the perfor-mance remarkably for various computer vision tasks [9, 10, 23, 32, 33, 40, 46, 50, 54, 60], including text spotting [17, 21, 39, 61]. Although the spotters [21, 61] based on DETR [3]
can get rid of the connectors and heuristic post-processing, they lack efficient joint representation to deal with scene text detection and recognition, e.g., requiring an extra RNN module in TTS [21] (Fig. 1d) or exploiting individual Trans-former decoder for each sub-task in TESTR [61] (Fig. 1e).
The generic object query exploited in TTS fails to consider the unique characteristics of scene text, e.g., location and shape. While TESTR uses point queries with box positional prior that is coarse for point predicting, and the queries are different for detection and recognition, introducing unex-pected heterogeneity. Consequently, these designs have a side effect on the performance and training efficiency [55].
In this paper, we propose a novel query form based on explicit point representations of text lines. Built upon it, we present a succinct DETR-like baseline that lets a sin-gle Decoder with Explicit Points Solo (dubbed DeepSolo) for detection and recognition simultaneously (Fig. 1c and
Fig. 1f). Technically, for each instance, we first represent the character sequence as ordered points, where each point has explicit attributes of position, offsets to the top and bot-tom boundary, and category. Specifically, we devise top-K
Bezier center curves to fit scene text instances with arbitrary shape and sample a fixed number of on-curve points cov-ering characters in each text instance. Then, we leverage the sampled points to generate positional queries and guide the learnable content queries with explicit positional prior.
Next, we feed the image features from the Transformer en-coder and the point queries into a single Transformer de-coder, where the output queries are expected to have en-coded requisite text semantics and locations. Finally, we adopt several very simple prediction heads (a linear layer or
MLP) in parallel to decode the queries into the center line, boundary, script, and confidence of text, thereby solving de-tection and recognition simultaneously.
In summary, the main contributions are three-fold: 1)
We propose DeepSolo, i.e., a succinct DETR-like baseline with a single Transformer decoder and several simple pre-diction heads, to solve text spotting efficiently. 2) We pro-pose a novel query form based on explicit points sampled from the Bezier center curve representation of text instance lines, which can efficiently encode the position, shape, and semantics of text, thus helping simplify the text spotting pipeline. 3) Experimental results on public datasets demon-strate that DeepSolo is superior to previous representative methods in terms of spotting accuracy, training efficiency, and annotation flexibility. 2.