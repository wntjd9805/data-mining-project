Abstract
The success of multi-task learning (MTL) can largely be attributed to the shared representation of related tasks, al-lowing the models to better generalise. In deep learning, this is usually achieved by sharing a common neural net-work architecture and jointly training the weights. How-ever, the joint training of weighting parameters on mul-tiple related tasks may lead to performance degradation, known as negative transfer. To address this issue, this work proposes an evolutionary multi-tasking neural architecture search (EMT-NAS) algorithm to accelerate the search pro-cess by transferring architectural knowledge across mul-tiple related tasks.
In EMT-NAS, unlike the traditional
MTL, the model for each task has a personalised network thus offering the ca-architecture and its own weights, pability of effectively alleviating negative transfer. A fit-ness re-evaluation method is suggested to alleviate fluctu-ations in performance evaluations resulting from param-eter sharing and the mini-batch gradient descent training method, thereby avoiding losing promising solutions during the search process. To rigorously verify the performance of
EMT-NAS, the classification tasks used in the empirical as-sessments are derived from different datasets, including the
CIFAR-10 and CIFAR-100, and four MedMNIST datasets.
Extensive comparative experiments on different numbers of tasks demonstrate that EMT-NAS takes 8% and up to 40% on CIFAR and MedMNIST, respectively, less time to find competitive neural architectures than its single-task coun-terparts. 1.

Introduction
Many neural architecture search (NAS) algorithms [8, 16, 23, 56] have shown better performance on a specific task than manually designed deep neural networks [11, 14,
*Corresponding author.
Code: https://github.com/PengLiao12/EMT-NAS
Figure 1. Conceptual differences between our work and two main existing methodologies for two-task learning. (a) Existing multi-task NAS typically handles multiple tasks such as semantic segmentation and surface normal prediction on the same dataset, where the loss function contains losses for multiple tasks, resulting in a network architecture that shares a common set of weight pa-rameters. (b) Model-based transfer learning trains a network archi-tecture on a source dataset and then transfers it to a target dataset by fine-tuning the weights using a separate loss function. (c) EMT-NAS simultaneously optimises multiple classification tasks from different datasets using a separate loss function for each task, re-sulting in an individual network architecture and corresponding weights for each task. EMT-NAS aims to optimise each task sepa-rately while sharing the knowledge of good network architectures to facilitate the optimisation of each task. Best viewed in colour. 35, 51]. However, when the task (or dataset) changes, the
NAS algorithm needs to be run again to find a new optimal network architecture for the new task, which is typical for single-task learning (STL).
By contrast, it has been shown that simultaneously learn-ing multiple related tasks, known as multi-task learning (MTL) is beneficial [3, 52]. One class of MTL focuses on designing or training a single network to jointly solve multiple tasks formed by scene understanding (the same
dataset) [38, 46], such as the NYU v2 dataset [37], the
CityScapes dataset [4], and the Taskonomy dataset [48].
These tasks consist of semantic segmentation, surface nor-mal prediction, depth prediction, keypoint detection and edge detection, which are known as multiple tasks that arise naturally [3]. Another typical scenario of MTL is inspired from human learning, where humans often transfer knowl-edge from one task to another related task, for example, the skills of playing squash and tennis could help each other
In this case, tasks may come from dif-to improve [52]. ferent datasets. More often than not, a pair of tasks from different datasets have a lower relatedness score than those from the same dataset [18]. This can be clearly shown if we analyse the task relatedness [1] between classification tasks from four medical datasets using the representation simi-larity analysis (RSA) [7]. The results of our analysis are presented in Fig. 2, from which we can see that the relat-edness score becomes especially low when the datasets are from different data modalities, implying that MTL on tasks from different datasets is more challenging.
MTL is able to improve the performance of learning mul-tiple related tasks by means of sharing common knowledge between tasks, including the network architecture and the weights of the model. Existing MTL designs one architec-ture for multiple tasks, although only a subset of that net-work architecture is practically used for each task. In other words, there are shares and differences between these sub-sets of model architectures [33]. In sharing the weights, if one or more tasks have a dominating influence in training the network weights, the performance of MTL may deteri-orate on some of the dominated tasks, which is called neg-ative transfer [40]. Therefore, this work aims to improve the performance of each task by finding a personalised net-work architecture for each task, thereby alleviating negative transfer resulting from the jointly trained weights.
Inspired by the successful research on multi-factorial evolutionary algorithms (MFEAs) [9], a recently proposed approach to knowledge transfer in evolutionary optimisa-tion, this work designs an algorithm to separately search for a personalised network architecture and corresponding weight parameters for each task in the same search space.
This way, knowledge transfer across different tasks can be achieved through crossover between these architectures to accelerate the search process on each task, provided that there are similarities between the multiple learning tasks.
Contributions: First, we propose an evolutionary multi-tasking NAS algorithm (EMT-NAS) for finding person-alised network architectures for different tasks (datasets) in the same search space. Architectural knowledge transfer is achieved by optimising the supernet parameters of each task separately and recommending neural network archi-tectures with good performance on their own task. Sec-ond, block-based crossover and bit-based mutation opera-Figure 2. Task similarity matrix. The relatedness scores be-tween four medical tasks are computed using ResNet-50 as a feature extractor. PathMNIST [44] is the tissue classifica-tion based on colorectal cancer histology slides, while Organ-MNIST {Axial,Coronal,Sagittal} [44] are organ classifications based on 3 types of 2D images formed by 3D computed to-mography (CT) images. We observe that the relatedness be-tween PathMNIST and OrganMNIST {Axial,Coronal,Sagittal}, respectively, than that between OrganM-smaller
NIST {Axial,Coronal,Sagittal}. are all tors are designed to accommodate the transformation from a continuous space to a discrete space. Finally, a fit-ness re-evaluation method is introduced to alleviate fitness fluctuations over the generations resulting from parameter sharing and the mini-batch gradient descent based train-ing to keep promising solutions. We present thorough experimental evaluations on different datasets, including medical classification benchmarks (PathMNIST, OrganM-NIST {Axial,Coronal,Sagittal}) and popular image classi-fication benchmarks (CIFAR-10, CIFAR-100, ImageNet), demonstrating the efficacy and the value of each compo-nent of EMT-NAS. Our results demonstrate that, unlike tra-ditional MTL, it is feasible to search for personalised net-work architectures for multiple tasks through architectural knowledge transfer, obtaining better performance and re-quiring less search time compared with single-task learning. 2.