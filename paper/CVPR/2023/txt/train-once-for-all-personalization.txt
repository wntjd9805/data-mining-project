Abstract
We study the problem of how to train a “personalization-friendly” model such that given only the task descriptions, the model can be adapted to different end-users’ needs, e.g., for accurately classifying different subsets of objects. One baseline approach is to train a “generic” model for classi-fying a wide range of objects, followed by class selection. In our experiments, we however found it suboptimal, perhaps because the model’s weights are kept frozen without being personalized. To address this drawback, we propose Train-once-for-All PERsonalization (TAPER), a framework that is trained just once and can later customize a model for different end-users given their task descriptions. TAPER learns a set of “basis” models and a mixer predictor, such that given the task description, the weights (not the pre-dictions!) of the basis models can be on the fly combined into a single “personalized” model. Via extensive experi-ments on multiple recognition tasks, we show that TAPER consistently outperforms the baseline methods in achieving a higher personalized accuracy. Moreover, we show that
TAPER can synthesize a much smaller model to achieve comparable performance to a huge generic model, mak-ing it “deployment-friendly” to resource-limited end de-vices. Interestingly, even without end-users’ task descrip-tions, TAPER can still be specialized to the deployed con-text based on its past predictions, making it even more
“personalization-friendly”. 1.

Introduction
Recent years have witnessed multiple breakthroughs in visual recognition [10, 17, 23, 25, 36], thanks to the advance in deep learning and the accessibility to large datasets.
Specifically, existing works have shown the possibility to train a gigantic and versatile “generic” model capable of classifying a wide range of over tens of thousands of objects [22, 33], rendering the promising future towards general-purposed AI.
*Work done as a student researcher at Google Research.
Figure 1. Examples of personalization via task description. We train-once-for-all personalization. propose a useful formulation:
Our “personalization-friendly” framework TAPER can on the fly reply to each user’s request with a personalized model promptly conditioned on the task description only.
However, from an end-user’s perspective, we often do not need such a versatility at once. Instead, users more of-ten look for models that are specialized to their requests, e.g., for accurately classifying a few but frequently encoun-tered or safety-critical objects in their environments. Tak-ing ImageNet-1K [9] as an example, a ResNet-152 classi-fier [17] can achieve around 80% accuracy in recognizing each of the 1K objects, which, while exciting to the vision community, may sound terrible to a visually-impaired user who seeks to smoothly interact with a handful of everyday objects. A better solution for end-users is perhaps to con-struct “personalized” models dedicated to their needs, e.g., train a 20-way classifier for everyday objects to attain an ac-curacy closer to 100%. Importantly, a personalized model usually requires a smaller capacity/size than a generic one, making it easier to deploy to resource-limited devices.
Personalization is by no means a new concept. A na¨ıve way to achieve it is to retrain a new model upon request, using the corresponding data. Doing so, however, is hardly scalable from a service provider’s point of view: the com-putation for training simply grows linearly with the number of users and their requests. The training latency can also de-grade the user experience. Suppose the service provider has sufficient data and is capable of training a generic model, re-training may just sound superfluous: if the objects the end-user cares about are already seen in training the generic model, why bother training on them again for personaliza-tion? In this paper, we therefore ask:
Can we train a “personalization-friendly” model such that after deployed, it can be easily specialized and rapidly condensed based on the end-user’s task description, without further training?
To begin with, we investigate a fairly simple idea, which is to train a (large) generic model, followed by class se-lection for personalization — chopping off the classes that are not of the user’s interest from the classification head.
While extremely straightforward without further training, this idea can already boost the aforementioned ResNet-152 to 95% accuracy on recognizing 20 classes. Nevertheless, this approach does not condense the model for computa-tion and memory efficiency. One may resolve this problem by training a smaller generic model like ResNet-18, whose size is roughly 1 5 of ResNet-152. However, with limited ca-pacity, ResNet-18 after class selection can only attain 92% accuracy on classifying 20 classes. We hypothesize if we can somehow personalize the backbone weights as well, the model will be able to better utilize its capacity to tackle the shrunken scope of end-users’ tasks.
To address these deficiencies while keeping the per-sonalization process simple, we propose Train-once-for-All PERsonalization (TAPER), a novel framework that is trained just once and can later head-to-toe customizes a condensed model on the fly for different end-users and re-quests, given their task descriptions.
At the core of TAPER is a set of shareable “basis” mod-els inspired by [5, 12], and a “mixer” predictor. The basis models have the same neural network architecture, each of which is expected to capture a certain specialty and there-fore can be smaller in size than a large generic model. The mixer predictor then takes the user’s task description (e.g.,
“Classify bicycle, pedestrian, tree, obstacle for me.”) as input, and produces coefficients to linearly combine the weights (not predictions!) of the basis models, condensing them into a “personalized” model on the fly. As TAPER adapts to users by predicting corresponding coefficients, not by adjusting the bases, it requires no retraining and enjoys parameter efficiency (e.g., for cloud services). Moreover, since the resulting personalized model is just like a basis model in size, it enjoys computation and memory efficiency during inference and is suitable for edge deployment.
We introduce a stage-wise training procedure to effec-tively learn the bases and the mixer predictor. We found that na¨ıve end-to-end training for optimizing personalized accuracy often results in inferior bases that either general-ize poorly or are not specialized. We thus dedicate each stage to one desired property, starting with training each basis to generically classify all classes, followed by special-izing them to different but fixed portions of data. The final stage then jointly refines the bases, together with learning the mixer predictor, to synthesize classifiers for randomly sampled tasks on the fly to optimize personalized accuracy.
We validate TAPER on three visual recognition datasets, including ImageNet [9], iNaturalist [39], and Domain-Net [31], each of which captures a different personalization scenario. TAPER consistently outperforms the baselines in achieving a higher personalized accuracy. For instance, on ImageNet, TAPER is able to synthesize a ResNet-18 to achieve 96% accuracy on classifying 20 classes, 4% higher than ResNet-18 with class selection. The accuracy is even higher than ResNet-152 with class selection while using 1 5
Interestingly, even without end-users’ of the model size. task descriptions, we show that TAPER can still be “self-specialized” to the deployed environment conditioned on its past predictions. Most importantly, none of these im-provements require further training, making TAPER truly
“personalization-friendly.” 2.