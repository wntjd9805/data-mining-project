Abstract
Recently, numerous end-to-end optimized image com-pression neural networks have been developed and proved themselves as leaders in rate-distortion performance. The main strength of these learnt compression methods is in powerful nonlinear analysis and synthesis transforms that can be facilitated by deep neural networks. However, out of operational expediency, most of these end-to-end methods adopt uniform scalar quantizers rather than vector quantiz-ers, which are information-theoretically optimal. In this pa-per, we present a novel Lattice Vector Quantization scheme coupled with a spatially Adaptive Companding (LVQAC) mapping. LVQ can better exploit the inter-feature depen-dencies than scalar uniform quantization while being com-putationally almost as simple as the latter. Moreover, to improve the adaptability of LVQ to source statistics, we couple a spatially adaptive companding (AC) mapping with
LVQ. The resulting LVQAC design can be easily embedded into any end-to-end optimized image compression system.
Extensive experiments demonstrate that for any end-to-end
CNN image compression models, replacing uniform quan-tizer by LVQAC achieves better rate-distortion performance without significantly increasing the model complexity. 1.

Introduction
In the past five years, the research on end-to-end CNN image compression has made steady progress and led to the birth of a new class of image compression methods
[1, 3, 4, 8, 21, 25, 26, 34–36, 40–44]. The CNN compres-sion can now match and even exceed the rate-distortion performance of the previous best image compression meth-ods [6,33,37,45], which operate in the traditional paradigm of linear transform, quantization and entropy coding.
The advantages of the CNN approach of data compres-sion come from the nonlinearity of its analysis and syn-thesis transforms of the autoencoder architecture, the end-to-end joint optimization of the nonlinear transforms, uni-form quantization of the latent space and conditional en-tropy coding (context-based arithmetic coding) of quantized features.
Apparently, using uniform scalar quantizer in the above
CNN image compression framework is motivated by oper-ational expediency more than other considerations. Only at very high bit rate uniform quantization can approach the rate-distortion optimality [15].
It is very difficult to di-rectly adopt and optimize a vector quantizer (VQ) in the end-to-end CNN architecture design for data compression, because VQ is a discrete decision process and it is not com-patible with variational backpropagation that is necessary to the end-to-end CNN training. In [1], Agustsson et al. tried to circumvent the difficulty by a so-called soft-to-hard vec-tor quantization scheme. Their technique is a soft (continu-ous) relaxation of discrete computations of VQ and entropy so that their effects can be approximated in the end-to-end training. However, in [1] the quantization centers are opti-mized along with the other modules, which make the whole system quite cumbersome and more difficult to train.
In this paper, we p ropose a novel Lattice Vector Quantiza-tion scheme coupled with a spatially Adaptive Companding (LVQAC) mapping. LVQ can better exploit the inter-feature dependencies than scalar uniform quantization while be-ing computationally almost as simple as the latter. Even if the features to be compressed are statistically independent,
LVQ is still a more efficient coding strategy than scalar uni-form quantization. This is because the former offers a more efficient covering of high-dimensional space than the lat-ter, as proven by the theory of sphere packings, lattices and groups [11]. Moreover, to improve the adaptability of LVQ to source statistics, we couple a spatially adaptive compand-ing mapping with LVQ. The resulting LVQAC design is computationally as simple and as amenable to the end-to-end training of the CNN compression model as in the orig-inally proposed framework of [3].
Consequently, for any end-to-end CNN image com-pression models, replacing uniform quantizer by LVQAC achieves better rate-distortion performance without sig-the simpler nificantly increasing the model complexity;
the context-sensitive entropy model, the greater the per-formance gain. For instance, the checkerboard context model [18] has recently been proposed to speed up the de-coding process, but it hurts the R-D performance slightly.
By incorporating LVQAC into the end-to-end compression
CNN with a checkerboard context model, we can achieve the same performance as the counterpart CNN using a far more expensive auto-regressive context model. This is be-cause uniform scalar quantization needs a more complex model to compensate for its coding inefficiency, whereas
LVQAC does not. 2. Overview of Learned Image Compression and