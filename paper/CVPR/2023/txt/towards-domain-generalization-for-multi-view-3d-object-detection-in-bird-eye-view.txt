Abstract
Multi-view 3D object detection (MV3D-Det) in Bird-Eye-View (BEV) has drawn extensive attention due to its low cost and high efficiency. Although new algorithms for camera-only 3D object detection have been continu-ously proposed, most of them may risk drastic performance degradation when the domain of input images differs from that of training. In this paper, we first analyze the causes of the domain gap for the MV3D-Det task. Based on the covariate shift assumption, we find that the gap mainly at-tributes to the feature distribution of BEV, which is deter-mined by the quality of both depth estimation and 2D im-age’s feature representation. To acquire a robust depth pre-diction, we propose to decouple the depth estimation from the intrinsic parameters of the camera (i.e. the focal length) through converting the prediction of metric depth to that of scale-invariant depth and perform dynamic perspective augmentation to increase the diversity of the extrinsic pa-rameters (i.e. the camera poses) by utilizing homography.
Moreover, we modify the focal length values to create mul-tiple pseudo-domains and construct an adversarial train-ing loss to encourage the feature representation to be more domain-agnostic. Without bells and whistles, our approach, namely DG-BEV, successfully alleviates the performance drop on the unseen target domain without impairing the accuracy of the source domain. Extensive experiments on
Waymo, nuScenes, and Lyft, demonstrate the generalization and effectiveness of our approach. 1.

Introduction 3D object detection, aiming at localizing objects in the 3D space, is critical for various applications such as au-*Shuo Wang and Xinhai Zhao contributed equally. This work was done when Shuo Wang was an intern at Huawei Noah’s Ark Lab.
†Corresponding author. (a) Baseline (b) DG-BEV
Figure 1. Qualitative comparisons between BEVDepth and the proposed DG-BEV. The red and blue bounding boxes represent ground truth and detected results on the target domain respectively.
Depth-shift is shown in green arrows. Our approach can detect correct 3D results on unknown domains. tonomous driving [6, 38], robotic navigation [2], and vir-tual reality [32], etc. Despite the remarkable progress of
LiDAR-based methods [17,30,33], camera-based 3D object detection in Bird-Eye-View (BEV) [14, 19, 21] has drawn increasing attention in recent years due to its rich semantic information and low cost for deployment.
However, most of the detectors assume that the training and testing data are obtained in the same domain which may be hardly guaranteed in realistic scenarios. Thus, tremen-dous performance degradation will appear when the domain of the input image shifts. For example, nuScenes [3] and
eters of cameras (e.g. camera poses) also play an important role in camera-based depth estimation, which is often ig-nored in previous works. Instead, we introduce homogra-phy learning to dynamically augment the image perspec-tives by simultaneously adjusting the imagery data and the camera pose.
Moreover, since domain-agnostic feature representations are favored for better generalization, we propose to build up multiple pseudo-domains by modifying the focal length values of camera intrinsic parameters in the source domain and construct an adversarial training loss to further enhance the quality of feature representations. In summary, the main contributions of this paper are:
• We present a theoretical analysis on the causes of the domain gap in MV3D-Det. Based on the covariate shift assumption, we find the gap lies in the feature distribution of BEV, which is determined by the depth estimation and 2D image feature jointly.
• We propose DG-BEV, a domain generalization method to alleviate the domain gap from both of the two per-spectives mentioned above.
• Extensive experiments on various public datasets, in-cluding Waymo, nuScenes, and Lyft, demonstrate the generalization and effectiveness of our approach.
• To the best of our knowledge, this is the first systematic study to explore a domain generalization method for multi-view 3D object detectors. 2.