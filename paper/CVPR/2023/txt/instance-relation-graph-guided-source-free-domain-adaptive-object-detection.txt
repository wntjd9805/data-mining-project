Abstract
Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the issue of domain shift. Speciﬁcally,
UDA methods try to align the source and target representa-tions to improve generalization on the target domain. Fur-ther, UDA methods work under the assumption that the source data is accessible during the adaptation process.
However, in real-world scenarios, the labelled source data is often restricted due to privacy regulations, data transmis-sion constraints, or proprietary data concerns. The Source-Free Domain Adaptation (SFDA) setting aims to alleviate these concerns by adapting a source-trained model for the target domain without requiring access to the source data.
In this paper, we explore the SFDA setting for the task of adaptive object detection. To this end, we propose a novel training strategy for adapting a source-trained object de-tector to the target domain without source data. More pre-cisely, we design a novel contrastive loss to enhance the target representations by exploiting the objects relations for a given target domain input. These object instance rela-tions are modelled using an Instance Relation Graph (IRG) network, which are then used to guide the contrastive repre-sentation learning. In addition, we utilize a student-teacher to effectively distill knowledge from source-trained model to target domain. Extensive experiments on multiple ob-ject detection benchmark datasets show that the proposed approach is able to efﬁciently adapt source-trained object detectors to the target domain, outperforming state-of-the-art domain adaptive detection methods. Code and models are provided in https://viudomain.github.io/irg-sfda-web/. 1.

Introduction
In recent years, object detection has seen tremendous advancements due to the rise of deep networks [12, 42, 44, 45, 53, 79]. The major contributor to this success is the availability of large-scale annotated detection datasets
[10, 13, 15, 43, 73], as it enables the supervised training of deep object detector models. However, these models
Figure 1. Left: Supervised training of detection model on the source domain. Right: Source-Free Domain Adaptation where a source-trained model is adapted to the target domain in the ab-sence of source data with pseudo-label self-training and proposed
Instance Relation Graph (IRG) network guided contrastive loss. often have poor generalization when deployed in visual domains not encountered during training.
In such cases, most works in the literature follow the Unsupervised Do-main Adaptation (UDA) setting to improve generalization
[7, 14, 23, 24, 57, 62]. Speciﬁcally, UDA methods aim to minimize the domain discrepancy by aligning the feature distribution of the detector model between source and tar-get domain [9, 19, 28, 56, 59]. To perform feature align-ment, UDA methods require simultaneous access to the la-beled source and unlabeled target data. However in practi-cal scenarios, the access to source data is often restricted due to concerns related to privacy/safety, data transmis-sion, data proprietary etc. For example, consider a detec-tion model trained on large-scale source data, that performs poorly when deployed in new devices having data with dif-ferent visual domains.
In such cases, it is far more efﬁ-cient to transmit the source-trained detector model (∼500-1000MB) for adaptation rather than transmitting the source data (∼10-100GB) to these new devices [27,37]. Moreover, transmitting only source-trained model alleviates many pri-vacy/safety, data proprietary concerns as well [41, 47, 70].
Hence, adapting the source-trained model to the target do-main without having access to source data is essential in the case of practical deployment of detection models. This mo-tivates us to study Source-Free Domain Adaptation (SFDA) setting for adapting object detectors (illustrated in Fig. 1).
The SFDA is a more challenging setting than UDA.
Speciﬁcally, on top of having no labels for the target data,
(a) Prediction (b) Ground truth (a) Object predictions by Cityscapes-trained model
Figure 2. on the FoggyCityscapes image. (b) Corresponding ground truth.
Here, the proposals around the bus instance have inconsistent pre-dictions, indicating that instance features are prone to large shift in the feature space, for a small shift in the proposal location. the source data is not accessible during adaptation. There-fore, most SFDA methods for detection consider train-ing with pseudo-labels generated by source-trained model
[27, 40]. During our initial SFDA training experiments, we identiﬁed two key challenges. Firstly, noisy pseudo-labels generated by the source-trained model due to domain shift can result in suboptimal distillation of target domain information into the source-trained model [11, 46]. Sec-ondly, Fig. 2 shows object proposals for an image from
FoggyCityscapes, predicted by a detector model trained on
Cityscapes. Here, all the proposals have Intersection-over-Union>0.9 with respective ground-truth bounding boxes and each proposal is assigned a prediction with a conﬁdence score. Noticeably, the proposals around the bus instance have different predictions, e.g., car with 18%, truck with 93%, and bus with 29% conﬁdence. This indicates that the pooled features are prone to a large shift in the feature space for a small shift in the proposal location. This is because, the source-trained model representations would tend to be biassed towards source data, resulting in weak representa-tion for the target data. Therefore, we consider two major challenges in SFDA training: 1) Effectively distill target do-main information into source-trained model 2) Enhancing the target domain feature representations.
Motivated by [46], we utilize mean-teacher [61] frame-work to effectively distill of target domain knowledge into source-trained model. However, the key challenge of en-hancing the target domain feature representations remained.
To address this, we turned to contrastive representation learning (CRL) methods, has been shown to learn high-quality representations from images in an unsupervised manner [5, 6, 69]. CRL methods achieve this by forcing representations to be similar under multiple views (or aug-mentations) of an anchor image and dissimilar to all other images.
In classiﬁcation, the CRL methods assume that each image contains only one object. On the contrary, for object detection, each image is highly likely to have multiple object instances. Furthermore, the CRL train-ing also requires large batch sizes and multiple views to
Figure 3. (a) Class agnostic object proposals generated by Re-gion Proposal Network (RPN). (b) Cropping out RPN propos-als will provide multiple contrastive views of an object instance.
We utilize this to improve target domain feature representations through RPN-view contrastive learning. However as RPN pro-posals are class agnostic, it is challenging to form positive (same class)/negative pairs (different class), which is essential for CRL. learn high-quality representations, which incurs a very high
GPU/memory cost, as detection models are computation-ally expensive. To circumvent these issues, we propose an alternative strategy which exploits the architecture of the detection model like Faster-RCNN [54]. Interestingly, the proposals generated by the Region Proposal Network (RPN) of a Faster-RCNN essentially provide multiple views
In other for any object instance as shown in Fig. 3 (a). words, the RPN module provides instance augmentation for free, which could be exploited for CRL, as shown in
Fig. 3 (b). However, RPN predictions are class agnos-tic and without the ground-truth annotations for target do-main, it is impossible to know which of these proposals would form positive (same class)/negative pairs (different class), which is essential for CRL. To this end, we propose a Graph Convolution Network (GCN) based network that models the inter-instance relations for generated RPN pro-posals. Speciﬁcally, each node corresponds to a proposal and the edges represent the similarity relations between the proposals. This learned similarity relations are utilized to extract information regarding which proposals would form positive/negative pairs and are used to guide CRL. By doing so, we show that such graph-guided contrastive representa-tion learning is able to enhance representations for the target data. Our contributions are summarized as follows:
• We investigate the problem of source-free domain adap-tation for object detection and identify some of the major challenges that need to be addressed.
• We introduced an Instance Relation Graph (IRG) frame-work to model the relationship between proposals gener-ated by the region proposal network.
• We propose a novel contrastive loss which is guided by the IRG network to improve the feature representations for the target data.
• The effectiveness of the proposed method is evaluated on multiple object detection benchmarks comprising of visu-ally distinct domains. Our method outperforms existing source-free domain adaptation methods and many unsu-pervised domain adaptation methods.
2.