Abstract
We aim at advancing blind image quality assessment (BIQA), which predicts the human perception of image quality without any reference information. We develop a general and automated multitask learning scheme for BIQA to exploit auxiliary knowledge from other tasks, in a way that the model parameter sharing and the loss weighting are determined automatically. Specifically, we first describe all candidate label combinations (from multiple tasks) us-ing a textual template, and compute the joint probability from the cosine similarities of the visual-textual embed-dings. Predictions of each task can be inferred from the joint distribution, and optimized by carefully designed loss functions. Through comprehensive experiments on learn-ing three tasks - BIQA, scene classification, and distor-tion type identification, we verify that the proposed BIQA method 1) benefits from the scene classification and dis-tortion type identification tasks and outperforms the state-of-the-art on multiple IQA datasets, 2) is more robust in the group maximum differentiation competition, and 3) re-aligns the quality annotations from different IQA datasets more effectively. The source code is available at https:
//github.com/zwx8981/LIQE. 1.

Introduction
As a fundamental computational vision task, blind im-age quality assessment (BIQA) [63] aims to predict the visual quality of a digital image with no access to the underlying pristine-quality counterpart (if any).
In the age of deep learning, the development of BIQA can be mainly characterized by strategies to mitigate the conflict between the large number of trainable parameters and the
*Corresponding author. (a) (b) (c) (b) A dis-(a) A “parrots” image of pristine quality.
Figure 1. torted version of (a) by global Gaussian blurring. (c) A distorted
“cityscape” image by the same level of Gaussian blurring. Humans are able to “see through” the Gaussian blur, and recognize the two parrots in (b) with no effort, suggesting the internal representations for the task of visual recognition should be distortion-insensitive.
This makes it conceptually conflicting to BIQA, which relies on distortion-sensitive representations for quality prediction. small number of human quality annotations in the form of mean opinion scores (MOSs). When synthetic distor-tions (e.g., Gaussian noise and JPEG compression) are of primary concern, patchwise training [4], quality-aware pre-training [32, 37, 76], and learning from noisy pseudo-labels [2, 38, 67] are practical training tricks with less (or no) reliance on MOSs. Here the underlying assumptions are that 1) the pristine-quality images exist and are acces-sible, 2) the visual distortions can be simulated efficiently and automatically, and 3) full-reference IQA models [64] are applicable and provide adequate quality approxima-tions. However, all these assumptions do not hold when it comes to realistic camera distortion (e.g., sensor noise, motion blurring or a combination of both). A different set of training tricks have been explored, including trans-fer learning [17, 76], meta learning [80], and contrastive learning [40]. Emerging techniques that combine multiple datasets for joint training [78] and that identify informative samples for active fine-tuning [66] can also be seen as ways to confront the data challenge in BIQA.
In this paper, we aim to accomplish something in the
same spirit, but from a different multitask learning perspec-tive. We ask the key question:
Can BIQA benefit from auxiliary knowledge pro-vided by other tasks in a multitask learning setting?
This question is of particular interest because many high-level computer vision tasks (e.g., object recognition [8] and scene classification [5]), with easier-to-obtain ground-truth labels, seem to be conceptually conflicting to BIQA. This is clearly illustrated in Fig. 1. Humans are able to “see through” the Gaussian blur, and recognize effortlessly the two parrots in (b). That is, if we would like to develop computational methods for the same purpose, they should rely on distortion-insensitive features, and thus be robust to such corruptions. This is also manifested by the com-mon practice in visual recognition that treats synthetic dis-tortions as forms of data augmentation [16]. In stark con-trast, BIQA relies preferentially on distortion-sensitive fea-tures to quantify the perceptual quality of images of vari-ous semantic content. Ma et al. [37] proposed a cascaded multitask learning scheme for BIQA, but did not investigate the relationships between BIQA and high-level vision tasks.
Fang et al. [9] included scene classification as one task, but required manually specifying the parameters (i.e., computa-tions) to share across tasks, which is difficult and bound to be suboptimal.
Taking inspiration from recent work on vision-language pre-training [49], we propose a general and automated mul-titask learning scheme for BIQA, with an attempt to answer the above-highlighted question. Here, “automated” means that the model parameter sharing for all tasks and the loss weighting assigned to each task are determined automati-cally. We consider two additional tasks, scene classification and distortion type identification, the former of which is conceptually conflicting to BIQA, while the latter is closely related. We first summarize the scene category, distortion type, and quality level of an input image using a textual template. For example, Fig. 1 (c) may be described as “a photo of a cityscape with Gaussian blur artifacts, which is of bad quality.” We then employ the contrastive language-image pre-training (CLIP) [49], a joint vision and language model trained with massive image-text pairs, to obtain the visual and textual embeddings. The joint probability over the three tasks can be computed from the cosine similarities between the image embedding and all candidate textual em-beddings1. We marginalize the joint distribution to obtain the marginal probability for each task, and further convert the discretized quality levels to a continuous quality score using the marginal distribution as the weighting.
We supplement existing IQA datasets [7, 11, 17, 22, 27, 54] with scene category and distortion type labels, and jointly optimize the entire method on a combination of them by minimizing a weighted sum of three fidelity losses [58], where the loss weightings are adjusted automatically based on the training dynamics [31]. From extensive experimen-tal results, we arrive at a positive answer to the highlighted question: BIQA can indeed benefit from both scene clas-sification and distortion type identification. The resulting model, which we name Language-Image Quality Evaluator (LIQE), not only outperforms state-of-the-art BIQA meth-ods [13,55,76,78] in terms of prediction accuracy on multi-ple IQA datasets, but also exhibits improved generalizabil-ity in the group maximum differentiation (gMAD) competi-tion [35]. In addition, we provide quantitative evidence that
LIQE better realigns MOSs from different IQA datasets in a common perceptual scale [48]. 2.