Abstract
Pre-training of deep convolutional neural networks (DC-NNs) plays a crucial role in the field of visual sentiment anal-ysis (VSA). Most proposed methods employ the off-the-shelf backbones pre-trained on large-scale object classification datasets (i.e., ImageNet). While it boosts performance for a big margin against initializing model states from random, we argue that DCNNs simply pre-trained on ImageNet may excessively focus on recognizing objects, but failed to pro-vide high-level concepts in terms of sentiment. To address this long-term overlooked problem, we propose a sentiment-oriented pre-training method that is built upon human visual sentiment perception (VSP) mechanism. Specifically, we fac-torize the process of VSP into three steps, namely stimuli taking, holistic organizing, and high-level perceiving. From imitating each VSP step, a total of three models are sepa-rately pre-trained via our devised sentiment-aware tasks that contribute to excavating sentiment-discriminated represen-tations. Moreover, along with our elaborated multi-model amalgamation strategy, the prior knowledge learned from each perception step can be effectively transferred into a sin-gle target model, yielding substantial performance gains. Fi-nally, we verify the superiorities of our proposed method over extensive experiments, covering mainstream VSA tasks from single-label learning (SLL), multi-label learning (MLL), to label distribution learning (LDL). Experiment results demon-strate that our proposed method leads to unanimous improve-ments in these downstream tasks. Our code is released on https://github.com/tinglyfeng/sentiment_pretraining. 1.

Introduction
Visual sentiment analysis aims to understand the senti-ment embedded in an image, which gradually becomes a critical computer vision task that enables numerous applica-tions from opinion mining [45], entertainment assistance [5], to business intelligence [18]. Given an image, the main goal
* Equal contribution.
† Corresponding author. of VSA is to recognize the emotion induced by viewers, pro-viding either the categorical emotion states (CES) [9, 30] or dimensional emotion space (DES) [23, 41] representations.
Traditional methods proposed for VSA normally involve extracting sentiment-related hand-crafted features like line directions [48], textures and colors [30], etc. These features are then sent to a classifier e.g., a support vector machine (SVM) to predict the emotional states. However, due to af-fective gap [15], the low-level features can hardly meet the high-level attributes requirement of VSA, thus resulting in relatively unsatisfying performance.
Entering the deep learning era, DCNNs are now the dom-inant tools applied to various computer vision tasks, such as image classification, object detection, etc. Blessed with im-pressive high-level feature extraction capabilities, DCNNs have demonstrated superior advantages for modern VSA proven by a lot of milestone works [3, 50, 56]. Beneath the success, many may ignore one important factor that largely determines the performance of VSA, saying the pre-trained model. Due to the data-hungry nature of DCNNs, initializ-ing model parameters from models trained on large-scale datasets has been a go-to technique for most tasks to improve their generalization abilities. When it comes to VSA, the lack of data has been exacerbated by the arduous annota-tion process (every image needs to be annotated by multiple people due to the subjectivity of emotion), resulting in its especially heavy reliance on pre-training. In our experiments on FI dataset [57], the ResNet50 [16] pre-trained on Ima-geNet [8] outperforms the one trained from scratch by 20 percent in terms of accuracy, revealing the undeniable crucial role the pre-trained model plays in VSA.
Today’s deep models proposed for VSA are mostly ini-tialized from models pre-trained on ImageNet to achieve sat-isfactory performance [59]. However, different from many other computer vision tasks that mainly depend on objec-tive semantics, VSA requires a relatively higher level of understanding of an image. Therefore, pre-training only on
ImageNet which is specially designed for object classifica-tion may not be the best practice for VSA.
In this paper, we argue that the models pre-trained on Ima-Figure 1. Overview of our pre-training method. We split a CNN backbone into three stages, each of which is responsible for extracting features corresponding to a certain VSP step. To fully excavate sentiment-related knowledge in terms of each step, a total of three models are separately trained to perform our elaborated tasks shown at the bottom. geNet fail to achieve sentiment-related initial states to relieve the burden of learning sentiment representations from lim-ited data. Also, due to the psychological and physiological nature of VSA, we believe that only if we fully understand how human sentiment is internally constructed can we thor-oughly unveil the potential of VSA pre-training. Therefore, our proposed pre-training method is built upon human vi-sual sentiment perception mechanism. Summarized from numerous existing research in the field of psychology and neuroscience [24, 26], we factorize the process of VSP into three steps in chronological order: 1) Stimuli Taking (ST): the procedure starts with the retina receiving light signals composed of colors and textures [29]. 2) Holistic Organizing (HO): the second step taking place in the primary visual cor-tex (V1) of our brain is to construct a whole map determining the overall context and global organization of scene [10, 43]. 3) High-level Perceiving (HP): the other parts of our brain help us separate the main objects from ambient light and build our high-level awareness [13, 19, 39].
Inspired by these theories, we build our pre-training framework by instructing the DCNNs to mimic the behavior of humans. In this work, we separately perform three groups of pre-training tasks, each of which is corresponding to one
VSP step and is intentionally excavated the key sentiment features. To fully leverage the sentiment knowledge learned from pre-trained models, we then elaborate an amalgamation strategy to effectively distillate their abilities into a single target model. The amalgamation process is performed by squeezing the gap between the target model and sentiment-aware pre-trained models on both the logits and features at various levels. Moreover, the pre-trained models still par-ticipate in the whole downstream training, which further unleashes the potential learning abilities of DCNNs to ac-commodate the specialties of training data. We apply our method to multiple downstream VSA tasks including single-label learning, multi-label learning, and label distribution
Our contributions are three-fold. learning. Extensive experiments have demonstrated favor-able improvements from our proposed pre-training method. 1) We propose a sentiment-oriented pre-training method to separately train a total of three models, each of which is dedicated to mimick-ing the human sentiment perception mechanism through per-forming pre-training tasks. 2) We devise an amalgamation strategy to aggregate the sentiment-discriminated knowledge from pre-trained models into a single target model during training downstream tasks, yielding favorable performance gains. 3) We conduct extensive experiments on various back-bones and diverse VSA datasets. The experiment results demonstrate that our proposed method can unanimously im-prove the performance of a wide variety of VSA tasks. 2.