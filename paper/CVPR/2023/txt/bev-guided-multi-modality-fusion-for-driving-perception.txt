Abstract
Integrating multiple sensors and addressing diverse tasks in an end-to-end algorithm are challenging yet crit-ical topics for autonomous driving. To this end, we in-troduce BEVGuide, a novel Bird’s Eye-View (BEV) repre-sentation learning framework, representing the first attempt to unify a wide range of sensors under direct BEV guid-ance in an end-to-end fashion. Our architecture accepts in-put from a diverse sensor pool, including but not limited to Camera, Lidar and Radar sensors, and extracts BEV feature embeddings using a versatile and general trans-former backbone. We design a BEV-guided multi-sensor attention block to take queries from BEV embeddings and learn the BEV representation from sensor-specific features.
BEVGuide is efficient due to its lightweight backbone de-sign and highly flexible as it supports almost any input sen-sor configurations. Extensive experiments demonstrate that our framework achieves exceptional performance in BEV perception tasks with a diverse sensor set. Project page is at https://yunzeman.github.io/BEVGuide. 1.

Introduction
The recent research in Bird’s Eye-View (BEV) percep-tion and multi-sensor fusion has stimulated rapid progress for autonomous driving. The BEV coordinates naturally unify various downstream object-level and scene-level per-ception tasks, while joint learning with multiple sensors minimizes uncertainty, resulting in more robust and accu-rate predictions. However, existing work still exhibits fun-damental limitations. On the one hand, fusion strategies often necessitate explicit space transformations, which can be ill-posed and prone to errors. On the other hand, existing techniques utilizing BEV representations rely on ad-hoc de-signs and support a limited set of sensors (i.e., cameras and
Lidar). These constraints impede the evolution of a more general and flexible multi-sensor architecture for BEV 3D perception, which inspires the design of our work.
More specifically, as different sensors always lie in dif-ferent coordinate systems, prior approaches usually trans-form features of each sensor into the same space prior to
Figure 1. BEVGuide takes input from a sensor combination and learns BEV feature representation using a portable and gen-eral BEV-guided multi-sensor attention module.
In principle,
BEVGuide is able to take a wide variety of sensors and perform any BEV perception task. fusion. For example, some work prioritizes one sensor over another [1, 42, 43]. However, such fusion architectures tend to be inflexible and heavily reliant on the presence of the primary sensor – should the primary sensor be unavailable or malfunction, the entire pipeline collapses. Alternatively, other work transforms all sensors into the same space (3D or BEV space) using provided or estimated geometric con-straints [10, 22, 25]. Such methods usually require an ex-plicit depth estimation from camera images, which is sus-ceptible to errors due to the ill-posed nature of the image modality. Moreover, errors that arise during the transfor-mation process may propagate into subsequent feature fu-sion stages, ultimately impacting downstream tasks. Our approach seeks to streamline this process by employing the
BEV space to directly guide the fusion of multiple sensor feature maps within their native spaces.
Simultaneously, in addition to camera and Lidar sensors which bring about rich semantic information and 3D fea-tures respectively, we emphasize the integration of Radar sensors, which deliver unique velocity information and ro-bust signals in extreme weather conditions but have re-ceived considerably less attention in research compared with other sensing modalities. Among the limited literature that involves Radar learning, some work focuses on utiliz-ing the velocity measurement for prediction [39, 45], while
others treat Radar points as an additional 3D information source to aid detection and tracking [7, 10, 31]. Our method aims to accomplish perception tasks in the BEV space and analyze the synergy among all three types of sensors.
BEVGuide is a general and flexible multi-modality fu-sion framework designed for BEV perception. A paradigm of it is shown in Figure 1.
It accommodates any poten-tial sensor configurations within the sensor pool, including but not limited to camera, Lidar, and Radar sensors. For any new sensor, the core fusion block simply requires a sensor-specific feature embedding, which can be obtained from any backbone encoder, whether pretrained or yet to be trained. The fusion block consists of a BEV-guided sensor-agnostic attention module. We split the BEV space into small patches with position-aware embeddings, through which the model queries and fuses all sensor features to generate a unified BEV representation. By employing po-sitional encoding to encapsulate geometric constraints, we avoid error-prone explicit feature space transformations, en-abling the model to focus on positions of interest across sensors. Designed in this manner, the core fusion module is modality-agnostic and can potentially support any sen-sor configurations in real-world applications. We evaluate our model in BEV scene segmentation and velocity estima-tion tasks, where BEVGuide achieves leading results across various sensor configurations. Moreover, We observe that
BEVGuide exhibits great robustness in different weather and lighting conditions, facilitated by the inclusion of dif-ferent sensors.
The main contributions of this paper are as follows. (1) We propose BEVGuide, a comprehensive and versatile multi-modality fusion architecture designed for BEV per-ception. (2) We underscore the significance of Radar sen-sors in velocity flow estimation and BEV perception tasks in general, offering an insightful analysis in comparison with camera and Lidar sensors. (3) We present a map-guided multi-sensor cross-attention learning module that is gen-eral, sensor-agnostic, and easily extensible. (4) BEVGuide achieves state-of-the-art performance in various sensor con-figurations for BEV scene segmentation and velocity flow estimation tasks. And in principle, BEVGuide is compati-ble with a wide range of other BEV perception tasks. 2.