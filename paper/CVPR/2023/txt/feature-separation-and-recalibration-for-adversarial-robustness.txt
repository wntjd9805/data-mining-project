Abstract
Deep neural networks are susceptible to adversarial at-tacks due to the accumulation of perturbations in the fea-ture level, and numerous works have boosted model robust-ness by deactivating the non-robust feature activations that cause model mispredictions. However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional use-ful information for correct model predictions. To this end, we propose a novel, easy-to-plugin approach named Fea-ture Separation and Recalibration (FSR) that recalibrates the malicious, non-robust activations for more robust fea-ture maps through Separation and Recalibration. The Sep-aration part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activa-tions that are responsible for model mispredictions upon adversarial attack. The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions. Extensive experiments verify the superiority of FSR compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57% with small computational overhead. Codes are available at https://github.com/wkim97/FSR. 1.

Introduction
Despite the advancements of deep neural networks (DNNs) in computer vision tasks [2,10,20,39], they are vul-nerable to adversarial examples [19,43] that are maliciously crafted to subvert the decisions of these models by adding imperceptible noise to natural images. Adversarial exam-ples are also known to be successful in real-world cases, including autonomous driving [17] and biometrics [27, 41], and to be effective even when target models are unknown to the attacker [26, 31, 43]. Thus, it has become crucial to devise effective defense strategies against this insecurity.
To this end, numerous defense techniques have been pro-posed, including defensive distillation [38], input denois-ing [30], and attack detection [36, 54]. Among these meth-ods, adversarial training [19, 33], which robustifies a model by training it on a set of worst-case adversarial examples, has been considered to be the most successful and popular.
Even with adversarial training, however, small adversar-ial perturbations on the pixel-level accumulate to a much larger degree in the intermediate feature space and ruin the final output of the model [50]. To solve this problem, re-cent advanced methods disentangled and deactivated the non-robust feature activations that cause model mispredic-tions. Xie et al. [50] applied classical denoising techniques to deactivate disrupted activations, and Bai et al. [4] and
Yan et al. [55] deactivated channels that are irrelevant to correct model decisions. These approaches, however, in-evitably neglect discriminative cues that potentially lie in these non-robust activations. Ilyas et al. [22] have shown that a model can learn discriminative information from non-robost features in the input space. Based on this finding, we argue that there exist potential discriminative cues in the non-robust activations, and deactivating them could lead to loss of these useful information that can provide the model with better guidance for making correct predictions.
For the first time, we argue that with appropriate adjust-ment, the non-robust activations that lead to model mis-predictions could recapture discriminative cues for correct model decisions. To this end, we propose a novel Feature
Separation and Recalibration (FSR) module that aims to improve the feature robustness. We first separate the in-termediate feature map of a model into the malicious non-robust activations that are responsible for model mispre-dictions and the robust activations that still provide useful cues for correct model predictions even under adversarial attacks. Exploiting only the robust feature just like the ex-isting methods [4, 50, 55], however, could lead to loss of potentially useful cues in the non-robust feature. Thus, we recalibrate the non-robust activations to capture cues that provide additional useful guidance for correct model deci-sions. These additional cues can better guide the model to make correct predictions and thus boost its robustness.
Fig. 1 visualizes the attention maps [40] on the features of natural images by a naturally trained model (fnat) and the robust (f +), non-robust (f −), and recalibrated features ( ˜f −) on adversarial examples (x′) obtained from an adver-that these activations can be recalibrated to capture useful cues for correct model decisions.
• We introduce an easy-to-plugin Feature Separation and Recalibration (FSR) module, which separates non-robust activations from feature maps and recali-brates these feature units for additional useful cues.
• Experimental results demonstrate the effectiveness of our FSR module on various white- and black-box at-tacks with small computational overhead and verify our motivation that recalibration restores discrimina-tive cues in non-robust activations. 2.