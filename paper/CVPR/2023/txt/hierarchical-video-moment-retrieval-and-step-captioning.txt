Abstract
There is growing interest in searching for information from large video corpora. Prior works have studied rele-vant tasks, such as text-based video retrieval, moment re-trieval, video summarization, and video captioning in iso-lation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HIREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical infor-mation retrieval and visual/textual stepwise summarization from an instructional video corpus. HIREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans rel-evant to text query and breakdown of each moment into key instruction steps with caption and timestamps (totaling 8.6K step captions). Our hierarchical benchmark consists of video retrieval, moment retrieval, and two novel moment segmentation and step captioning tasks. In moment segmen-tation, models break down a video moment into instruction
In step caption-steps and identify start-end boundaries. ing, models generate a textual summary for each step. We also present starting point task-specific and end-to-end joint baseline models for our new benchmark. While the baseline models show some promising results, there still exists large room for future improvement by the community.1 1.

Introduction
Encouraged by the easy access to smartphones, record-ing software, and video hosting platforms, people are in-creasingly accumulating videos of all kinds. To fuel the
*equal contribution 1code and data: https://github.com/j-min/HiREST subsequent growing interest in using machine learning sys-tems to extract and summarize important information from these large video corpora based on text queries, progress has been made in video retrieval [2, 17, 18, 41, 42], moment re-trieval [10, 16, 17], video summarization [9, 24, 33, 34], and video captioning [13, 20, 41, 42]. Previous works have gen-erally focused on solving these tasks independently; how-ever, all these tasks share the common goal of retrieving in-formation from a video corpus, at different levels of scales and via different modalities. Hence, in this work, we intro-duce a new hierarchical benchmark that combines all four tasks to enable novel and useful real-world applications.
For example, a text-based search service that finds a rele-vant video from a large video corpus, extracts the most rel-evant moment from that video, segments the moment into important steps, and captions them for easy indexing and retrieval. To support this, we introduce HIREST, a hierar-chical instructional video dataset for a holistic benchmark of information retrieval from a video corpus (see Sec. 3).
HIREST consists of four annotations: 1) 3.4K pairs of text query about open-domain instructions (e.g., ‘how to make glow in the dark slime’) and videos, 2) relevant moment timestamps inside the 1.1K videos, where only a part of the video (< 75%) is relevant to the text query, 3) moment breakdown in several instructional steps with timestamps (7.6 steps per video, total 8.6K steps), and, 4) an manually curated English caption for each step (e.g. ‘pour shampoo in container’). We collect fine-grained step-wise annota-tions of HIREST in a two-step annotation process with on-line crowdworkers on instructional text-video pairs from the
HowTo100M [23] dataset (see Sec. 3.1). The instructional videos often come with clear step-by-step instructions, al-lowing fine-grained segmentation of the videos into short steps. While there are existing video datasets with step an-notations, they are based on a small number of predefined task names [36, 46] (thus step captions are not diverse), or are limited to a single topic (e.g. cooking [45]). HIREST covers various domains and provides diverse step captions with timestamps written by human annotators (see Table 1), presenting new challenging and realistic benchmarks for hi-Figure 1. Overview of four hierarchical tasks of our HIREST dataset (Sec. 3). 1) Video retrieval: find a video that is most relevant to a given text query. 2) Moment retrieval: choose the relevant span of the video, by trimming the parts irrelevant to the text query. 3) Moment segmentation: break down the span into several steps and identify the start-end boundaries of each step. 4) Step captioning: generate step-by-step textual summaries of the moment. erarchical video information retrieval.
Using the HIREST dataset, we benchmark four tasks: 1) video retrieval, 2) moment retrieval, 3) moment segmen-tation, and 4) step captioning (see Fig. 1 and Sec. 3.3). In the video retrieval task, models have to identify a video that is most relevant to a given text query. In the moment re-trieval task, models have to select the relevant span of the video, by trimming the parts irrelevant to the text query (blue boundary in Fig. 1). In the moment segmentation task, models have to break down the relevant portion into sev-eral instructional steps and identify the start-end boundaries of each step (green boundaries in Fig. 1). Finally, in the step captioning task, models have to generate step captions (e.g. ‘spray the warm water on carpet’) of the instructional steps. To provide good starting points to the community for our new task hierarchy, we show the performance of re-cent baseline models on HIREST. For baselines, we use strong models including CLIP [27], EVA-CLIP [8], Frozen-in-Time [2], BMT [13], and SwinBERT [20]. On all four tasks, we find that finetuning models on HIREST improve performance; however, there exists a large room to improve performance.
We summarize our contributions in this paper: 1) We present HIREST dataset and propose a new benchmark that covers hierarchy in information retrieval and visual/textual summarization from an instructional video corpus. 2) Un-like existing video datasets with step captions based on predefined task names or limited to a single topic, our
HIREST provides diverse, high-quality step captions with timestamps written by human annotators. 3) We provide a joint baseline model that can perform moment retrieval, moment segmentation, and step captioning with a single ar-chitecture. 4) We provide comprehensive dataset analyses and show experiments with baseline models for each task, where there is a large room to improve model performance.
We hope that HIREST can foster future work on end-to-end systems for holistic information retrieval and summariza-tion on large video corpus. In addition, our manually an-notated step captions can also be a good source for training and testing the step-by-step reasoning of large multimodal language models [40, 44]. 2.