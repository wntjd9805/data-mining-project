Abstract
The handling of long videos with complex and occluded sequences has recently emerged as a new challenge in the video instance segmentation (VIS) community. However, existing methods have limitations in addressing this chal-lenge. We argue that the biggest bottleneck in current ap-proaches is the discrepancy between training and inference.
To effectively bridge this gap, we propose a Generalized framework for VIS, namely GenVIS, that achieves state-of-the-art performance on challenging benchmarks without designing complicated architectures or requiring extra post-processing. The key contribution of GenVIS is the learning strategy, which includes a query-based training pipeline for sequential learning with a novel target label assign-ment. Additionally, we introduce a memory that effectively acquires information from previous states. Thanks to the new perspective, which focuses on building relationships between separate frames or clips, GenVIS can be ﬂexibly executed in both online and semi-online manner. We eval-uate our approach on popular VIS benchmarks, achieving state-of-the-art results on YouTube-VIS 2019/2021/2022 and
Occluded VIS (OVIS). Notably, we greatly outperform the state-of-the-art on the long VIS benchmark (OVIS), improv-ing 5.6 AP with ResNet-50 backbone. Code is available at https://github.com/miranheo/GenVIS. 1.

Introduction
Video Instance Segmentation (VIS) is the task of identi-fying, segmenting, and tracking all objects in videos simul-taneously. With the emergence of datasets containing long and complex sequences, the research community is taking a step towards real-world applications. While many papers have proposed solutions, the most notable performance im-provement has been achieved by recent online methods using image-based backbones [14, 34]. These results challenge the common belief that end-to-end semi-online or ofﬂine ap-proaches (i.e., [5, 13, 15, 30, 33, 38]) trained on longer video clips would better model long-range object relationships.
We hypothesize that reason behind this somewhat surpris-Frame
Clip & Video
Heuristic
Association
Heuristic-free
Association
Training
Inference (a) Online (b) Semi-Online & Offline (c) GenVIS (ours)
...
...
...
Figure 1. Comparison between current VIS paradigms and our approach. (a, b) While current methods use two separate paradigms based on the number of frames processed, we argue that the key challenge in processing real-world videos is building inter-clip assocications. (b) Our proposed GenVIS addresses this challenge and can operate effectively in both online and semi-online manner without requiring hand-crafted post-processing. ing result is the presence or absence of an object association scheme between frames or clips that can scale to long videos.
Recent VIS methods, regardless of the approach, are driven by powerful image-level detectors [6, 8], so detection and segmentation quality are already robust and comparable to each other. To operate robustly in long videos, what the VIS really needs to focus on is the long-range tracking quality.
Although semi-online and ofﬂine methods are suitable for tracking objects within clips, they need to associate objects between clips to infer long videos, which is usually achieved using simple heuristics such as IoU matching [1, 15].
While online methods are more robust than semi-online/ofﬂine VIS solutions in processing long videos, we still see a signiﬁcant room for improvement in their tracking approach. These methods only consider local contexts be-tween adjacent frames during training, while test videos can exceed hundreds of frames [27]. We believe that there is a better way to learn long-range temporal modeling that could fundamentally change the current VIS landscape.
In this paper, we argue that the biggest bottleneck in handling long videos is the discrepancy between the training and inference scenarios. Regardless of the previously deﬁned paradigms (e.g., how many frames a method processes at
once), we need to focus on how to train the model. As illustrated in Fig. 1, all previous methodologies use only a few frames or clips (e.g., one or two) for training, while real-world videos can have an unlimited length. Therefore, they have to handle typical long-range tracking scenarios (e.g., newborn objects and re-identiﬁcation) as exceptions through heuristics [34].
We introduce a Generalized VIS framework, namely
GenVIS (Fig. 1 (c)), that is designed to minimize the gap between training and inference of long videos. We take an existing ofﬂine VIS model (VITA [13]) as the backbone and applay a query-propagation method [32] for object associa-tion between clips. The essence of GenVIS is the novel train-ing strategy. By improving the training strategy of the base model, we achieve signiﬁcant gains of 5.1 AP (Occluded
VIS [27]) and 5.8 AP (YouTube-VIS 2022 Long Videos [36]) in long and challenging benchmarks, outperforming all the previous methods by a large margin.
Our ﬁrst proposal for improvement is to load multiple clips during training. Unlike previous semi-online/ofﬂine
VIS methods [1, 5, 15, 30, 33] that focus on placing multiple frames in a single clip to strengthen intra-clip tracking, we propose to prioritize inter-clip tracking by learning the tem-poral relationship through multiple consecutive clips. We believe that strong inter-clip reasoning is crucial for process-ing long videos in the real world. As videos must be split into multiple clips that ﬁt into GPU memory when process-ing long videos, inter-clip association is inevitable. In this work, we use relatively short clip lengths (e.g., 1 to 7), but load as many clips as possible (e.g., usually more than 5).
More importantly, we propose a new learning criterion that enables seamless association through multiple consecu-tive clips. Since we now deploy a sufﬁcient number of clips, we can effectively simulate various inference scenarios at training time, covering newborn objects and objects that dis-appear and reappear. Speciﬁcally, we propose the Uniﬁed
Video Label Assignment (UVLA) that allows unique object queries to detect newly-appeared object and to keep them consistent once matching identities are obtained. Our new learning criterion not only improves tracking performance but also removes all heuristics1 required to handle new ob-jects and re-identiﬁcation from the inference stage. In other words, our model infers videos exactly as it learned. With these two proposals in the learning strategy, our base model outperforms all the previous methods on long video VIS benchmarks [27] without additional network modules.
To further bridge the remaining gap between training and inference, we propose adopting a memory mechanism that stores previously decoded object queries. This mechanism is particularly useful for handling very long videos (or stream-ing video), where there is a limit to the number of clips that 1Previously, [32] need to determine whether a tracked query is valid or not with a conﬁdence threshold. can be loaded at once. To implement this, we add extra information for each object query by reading from its previ-ous states. The memory mechanism results in meaningful improvements with only a small computational overhead.
Despite its simple framework, GenVIS achieves state-of-the-art results on VIS benchmarks, outperforming previous methods on challenging long and complex video datasets (Occluded VIS [27] and YouTube-VIS 2022 [36]). Our method also demonstrates strong generalization capability under online and semi-online settings2. We provide addi-tional analysis of the training and inference settings, which can be useful for balancing accuracy and efﬁciency tradeoffs. 2.