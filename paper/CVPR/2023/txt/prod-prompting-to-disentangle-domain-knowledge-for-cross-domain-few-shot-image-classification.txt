Abstract
This paper considers few-shot image classification under the cross-domain scenario, where the train-to-test domain gap compromises classification accuracy. To mitigate the domain gap, we propose a prompting-to-disentangle (ProD) method through a novel exploration with the prompting mechanism. ProD adopts the popular multi-domain train-ing scheme and extracts the backbone feature with a stan-dard Convolutional Neural Network. Based on these two common practices, the key point of ProD is using the prompting mechanism in the transformer to disentangle the domain-general (DG) and domain-specific (DS) knowledge from the backbone feature. Specifically, ProD concatenates a DG and a DS prompt to the backbone feature and feeds them into a lightweight transformer. The DG prompt is learnable and shared by all the training domains, while the
DS prompt is generated from the domain-of-interest on the fly. As a result, the transformer outputs DG and DS features in parallel with the two prompts, yielding the disentangling effect. We show that: 1) Simply sharing a single DG prompt for all the training domains already improves generaliza-tion towards the novel test domain. 2) The cross-domain generalization can be further reinforced by making the DG prompt neutral towards the training domains. 3) When in-ference, the DS prompt is generated from the support sam-ples and can capture test domain knowledge through the prompting mechanism. Combining all three benefits, ProD significantly improves cross-domain few-shot classification.
For instance, on CUB, ProD improves the 5-way 5-shot ac-curacy from 73.56% (baseline) to 79.19%, setting a new state of the art. 1.

Introduction
Few-shot image classification aims to use very limited support samples to transfer the classifier from base train-ing classes to novel test classes [10, 27, 28, 32, 34], which meets the requirement in application scenarios when train-Figure 1. ProD flattens the CNN backbone feature into feature tokens and concatenates them with the DG and DS prompt. The
DG prompt is learnable and shared by all the training domains for general knowledge. In contrast, the DS prompt is generated from same-domain features and thus can capture novel test domain knowledge from the support images during the test. The output of the DG/DS prompt is respectively supervised with a global/local classification head during training and concatenated as the final representation for inference. ing data is scarce. However, besides the insufficient data, in real-world applications, another critical challenge is the cross-domain problem, i.e., there is usually a domain gap between the training set and the test set. This train-to-test domain gap further hinders the knowledge transfer between the training and test data, significantly compromising the classification accuracy [7, 12]. In this paper, we tackle the cross-domain problem for few-shot image classification.
Generally, there are two approaches for mitigating the domain gap, i.e., domain generalization, and domain adap-tation. Domain generalization improves the inherent gen-eralization ability of the learned feature and directly ap-In con-plies it to novel domains without further tuning. trast, the domain adaptation uses samples from the novel domain to fine-tune the already-learned feature. For few-shot image classification, the domain generalization ap-proach [14, 21, 29, 41] is more explored than the domain adaptation approach [12], because limited support samples hardly provide reliable clues for domain adaptation.
This paper proposes a prompting-to-disentangle (ProD) method through a novel exploration with the prompting mechanism. The prompting technique was first introduced in natural language processing and has become popular
It aims to switch the trans-in computer vision [16, 43]. former to different mapping functions without changing its parameters by using different prompts to condition (im-pact) the transformer. Compared with prior prompting tech-niques, our exploration is novel: with a single transformer, we simultaneously use two prompts to extract the domain-general (DG) knowledge and the domain-specific (DS) knowledge in parallel. Therefore, these two prompts switch a single transformer between two different outputs simul-taneously, i.e., DG and DS knowledge, yielding the so-called Prompting-to-Disentangle. The related work section (Sec.2.3) compares our method and the standard prompting mechanism.
Importantly, in ProD, both the DG and DS knowledge are beneficial, contrary to prior works [14, 19, 22] where the DS knowledge is harmful and discarded. The reason is: in ProD, the DS knowledge is not bound by the already-Instead, it can on-the-fly capture seen training domains. the novel domain knowledge from support samples through the prompting mechanism (as explained in the 3rd bene-fit below). Therefore, ProD benefits from the DS knowl-edge of the novel test domain. Specifically, as illustrated in Fig.1, ProD adopts the popular multi-domain training scheme [14, 29] and uses a Convolutional Neural Network (ResNet-10 [13]) to extract the backbone feature. After-ward, ProD flattens the backbone feature into multiple fea-ture tokens, concatenates the feature tokens with a DS and a
DG prompt, and feeds them into a lightweight transformer.
The DG prompt is learnable and shared by all the training domains, while the DS prompt is generated with backbone features from the domain-of-interest (i.e., the domain of the feature tokens) on the fly. In ProD, there are three key points for mitigating the domain gap: 1) Sharing a single prompt for all the training domains benefits cross-domain generalization.
In other words, we need no special design to obtain a DG prompt but only to share a single prompt with multiple domains. During train-ing, the output state of the DG prompt (i.e. the DG feature in Fig.1) is fed into a global classifier that contains the cate-gories from all the training domains. Inference with the DG feature improves classification accuracy. 2) The DG prompt can be further reinforced by making it neutral towards all the training domains. To this end, we enforce a simple constraint: the learned DG prompt should have identical (or close) distance toward all the training do-mains. This constraint reduces the bias toward any domain and enriches the domain-general knowledge, bringing an-other round of improvement. 3) The DS prompt can capture the DS knowledge from the domain-of-interest on the fly and thus makes the DS knowledge beneficial. Specifically, during training, given an input, we use features from the same domain to gener-ate a DS prompt. Correspondingly, the knowledge in the
DS prompt is from the input domain specifically rather than from all the training domains. Moreover, the output DS fea-ture is supervised by a local classifier, which contains only the categories in the current domain and thus avoids cross-domain interference. In the inference phase, we duplicate the DS prompt generation procedure onto the test domain, i.e., generating the DS prompt from the support samples.
Therefore, although the model remains unchanged, the on-the-fly DS prompt modifies the context of the model input and dynamically conditions the output to the test domain.
Such a prompting and conditioning effect can be viewed as a test-time adaptation without fine-tuning the model.
ProD concatenates the DG and DS features as the fi-nal representation for inference, therefore integrating the benefits of good generalization and fast adaptation. Con-sequently, ProD effectively mitigates the domain gap and improves cross-domain few-shot classification. For exam-ple, on CUB, ProD improves the 5-way 5-shot recognition accuracy from 73.56% to 79.19% on CUB dataset, setting a new state of the art.
Our contributions can be summed as follow:
• We propose a Prompting-to-Disentangling (ProD) method for cross-domain few-shot image classification.
ProD disentangles the domain-general (DG) and domain-specific (DS) knowledge through a novel exploration of the prompting mechanism.
• For the DG knowledge, we show that sharing and neu-tralizing a DG prompt for all the training domains bene-fits cross-domain generalization. For the DS knowledge, we condition model to the novel test domain through a DS prompt generated on-the-fly to replace fine-tuning.
• We conduct extensive experiments to validate the ef-fectiveness of ProD. Ablation studies show that both the DG and DS prompt in ProD are effective. 2.