Abstract challenging setups where the source and target domains are more distant. Project Page: yunqing-me.github.io/RICK.
Few-shot image generation (FSIG) learns to generate di-verse and high-fidelity images from a target domain using a few (e.g., 10) reference samples. Existing FSIG methods se-lect, preserve and transfer prior knowledge from a source generator (pretrained on a related domain) to learn the tar-get generator. In this work, we investigate an underexplored issue in FSIG, dubbed as incompatible knowledge transfer, which would significantly degrade the realisticness of syn-thetic samples. Empirical observations show that the issue stems from the least significant filters from the source gen-erator. To this end, we propose knowledge truncation to mitigate this issue in FSIG, which is a complementary op-eration to knowledge preservation and is implemented by a lightweight pruning-based method. Extensive experiments show that knowledge truncation is simple and effective, con-sistently achieving state-of-the-art performance, including 1.

Introduction
Over recent years, deep generative models [14,15,21,30, 52, 78] have made tremendous progress, enabling many in-triguing tasks such as image generation [5, 27–29], image editing [45, 63, 68, 80], and data augmentation [6, 12, 59].
In spite of their remarkable success, most research on gen-erative models has been focusing on setups with sizeable training datasets [24, 26, 59, 60, 69, 74], limiting its applica-tions in many domains where data collection is difficult or expensive [12, 13, 46] (e.g., medicine).
To address such problems, FSIG has been proposed re-cently [41, 50], which learns to generate images with ex-tremely few reference samples (e.g., 10 samples) from a tar-get domain. In these regimes, learning a generator to cap-ture the underlying target distribution is an undetermined
problem that requires some prior knowledge. The majority of existing state-of-the-art (SOTA) FSIG methods rely on transfer learning approaches [2, 23, 65, 71] to exploit prior knowledge (e.g., a source generator) learned from abundant data of a different but related source domain, and then trans-fer suitable source knowledge to learn the target generator with fine-tuning [9, 26, 41, 48–50, 64, 65, 74, 75, 77]. Differ-ent techniques have been proposed to effectively preserve useful source knowledge, such as freezing [48], regulariza-tion [41, 50, 77] and modulation [75] (details in Sec. 2).
Incompatible knowledge transfer. Despite the impres-sive improvement achieved by different knowledge preser-vation approaches [41,48,50,75,77], in this work, we argue that preventing incompatible knowledge transfer is equally crucial. This is revealed through a carefully designed in-vestigation, where such incompatible knowledge transfer is manifested in the presence of unexpected semantic fea-tures. These features are inconsistent with the target do-main, thereby degrading the realisticness of synthetic sam-ples. As illustrated in Figure 1, trees and buildings are in-compatible with the domain of Sailboat (as can be observed by inspecting the 10 reference samples). However, they appear in the synthetic images when applying the existing
SOTA methods [41, 75] with a source generator trained on
Church. This shows that the existing methods cannot effec-tively prevent the transfer of incompatible knowledge.
Knowledge truncation. Based on our observations, we propose Removing In-Compatible Knowledge (RICK), a lightweight filter-pruning based method to remove filters that encode incompatible knowledge (i.e., filters with least estimated importance for adaptation) during FSIG adapta-tion. While filter pruning has been applied extensively to achieve compact deep networks with reduced computation
[19, 57, 66], its application to prevent transfer of incompati-ble knowledge is underexplored. We note that our proposed knowledge truncation and pruning of incompatible filters are orthogonal and complementary with existing knowledge preservation methods in FSIG. In this way, our method ef-fectively removes the incompatible knowledge compared to prior works, and achieves noticeably improved quality (e.g.,
FID [20]) of generated images.
Our contributions can be summarized as follows:
‚ We explore the incompatible knowledge transfer for
FSIG, reveal that SOTA methods fail in handling this issue, investigate the underlying causation, and disclose the inade-quacy of fine-tuning in removal of incompatible knowledge.
‚ We propose knowledge truncation to alleviate incom-patible knowledge transfer, and realize it with a lightweight filter-pruning based method.
‚ Extensive experiments show that our method effec-tively removes incompatible knowledge and consistently improves the generative quality, including challenging se-tups where source and target domains are dissimilar. 2.