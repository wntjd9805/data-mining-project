Abstract
Talking head generation is to generate video based on a given source identity and target motion. However, cur-rent methods face several challenges that limit the quality and controllability of the generated videos. First, the gen-erated face often has unexpected deformation and severe distortions. Second, the driving image does not explicitly disentangle movement-relevant information, such as poses and expressions, which restricts the manipulation of dif-ferent attributes during generation. Third, the generated videos tend to have ﬂickering artifacts due to the inconsis-tency of the extracted landmarks between adjacent frames.
In this paper, we propose a novel model that produces high-ﬁdelity talking head videos with free control over head pose and expression. Our method leverages both self-supervised learned landmarks and 3D face model-based landmarks to model the motion. We also introduce a novel motion-aware multi-scale feature alignment module to effectively transfer the motion without face distortion. Furthermore, we en-hance the smoothness of the synthesized talking head videos with a feature context adaptation and propagation module.
We evaluate our model on challenging datasets and demon-strate its state-of-the-art performance. More information is available at https://yuegao.me/PECHead. 1.

Introduction
Talking head video generation is a process of synthesiz-ing a talking head video with a given source identity and target motion. This process is also called face reenactment when using a driving head to deﬁne the relative movement to the source identity [4]. This generation technique can
be used in various applications, including video conferenc-ing [51], movie effects [39], and entertainment [6]. Due to the rapid development of deep learning [20] and genera-tive adversarial networks (GAN) [17, 27], impressive works have been conducted on talking head generation [22, 51], face reenactment [2, 24, 40, 49, 57, 59, 61, 65, 66], and image animation [46, 47, 52, 67]. These works focus on animating objects beyond the face and head [45, 46].
Early works on talking head generation require multi-ple source and driving images to generate one result [4, 13].
Recent works focus on one-shot generation [51, 61, 66], i.e., using only one source frame to generate the target by transferring the pose information of one driving frame.
Currently, the mainstream works [45–47] follow a self-supervised learning pipeline. They mainly utilized the self-supervised learned landmarks to model the movement of the identity between the source and driving images. The learned landmarks pairs are ﬁrst detected from both source and driving images, and then the dense ﬂow ﬁeld is esti-mated from the two sets of learned landmarks to transform the source features and guide the reconstruction of the driv-ing image. To further improve the performance, recent ap-proaches propose to utilize additional information, such as 3D learned landmarks [51] and depth map [22], or enhance the model structure, for example, adopting the ﬂexible thin-plate spline transformation [14, 67], and representing the motion as a combination of latent vectors [52].
However, there are still many challenges with these methods. First, the generated face often has unexpected de-formation and severe distortions. The learned landmarks-based approaches [46, 51, 67], such as FOMM [46], which only utilizes the 2D learned landmarks without face shape constraints, produces frontalization results with apparent face distortions (see Fig. 1a). The predeﬁned landmarks-based methods [13,24,59,63] model the movement between the source and driving images only based on the predeﬁned facial landmarks, leading to the non-facial parts of the head (such as the hair and neck) are not well handled. Second, all the movement information needs to be obtained via one single driving image.
It is rare and difﬁcult to decouple and manipulate these movement-relevant information, in-cluding poses and expressions, when generating the new image. Third, in order to achieve smooth and natural move-ments in generated videos, prior methods [46, 47, 67] typi-cally incorporate techniques to smoothen the extracted land-marks learned between adjacent frames. However, the sen-sitivity and inconsistency of the extracted landmarks pose a challenge in achieving smoothness, resulting in generated videos that are prone to ﬂickering.
To address the above challenges, we propose the Pose and Expression Controllable Head model (PECHead), which can generate high-ﬁdelity video face reenactment re-sults and enable talking head video generation with full con-trol over head pose and expression. The proposed method
ﬁrst incorporates the learned landmarks and the predeﬁned face landmarks to model the overall head movement and the detailed facial expression changing in parallel. We uti-lize the single image-based face reconstruction model [12] to obtain the face landmarks and project them into 2D im-age space. This approach constrains the face to a physi-cally reasonable shape, thereby reducing distortion during motion transfer, as demonstrated in the last row of Fig. 1a.
In this work, we introduce the use of learned sparse land-marks for global motion and predeﬁned dense landmarks for local motion, with the Motion-Aware Multi-Scale Fea-ture Alignment (MMFA) module serving to align these two groups of features. Then we use different coefﬁcients as in-put conditions to control the estimation of both predeﬁned and learned landmarks, so that we can realize the head pose and expression manipulation (Fig. 1b). Moreover, inspired by the recent low-level video processing works [8, 33], we propose the Context Adaptation and Propagation (CAP) module to further improve the smoothness of the gener-ated video. Our proposed method is evaluated on multiple talking head datasets, and experimental results indicate that it achieves state-of-the-art performance, generating high-ﬁdelity face reenactment results and talking head videos with the ability to control the desired head pose and facial expression.
Our contributions can be summarized as follows:
• We propose a novel method, PECHead, that generates high-ﬁdelity face reenactment results and talking head videos. Our approach leverages head movements to control the estimation of learned and predeﬁned land-marks, enabling free control over the head pose and expression in talking head generation.
• We incorporate the learned and predeﬁned face land-marks for global and local motion estimation with the proposed Motion-Aware Multi-Scale Feature Align-ment module, which substantially enhances the quality of synthesized images.
• We introduce a video-based pipeline with the Con-text Adaptation and Propagation module to further im-prove the smoothness and naturalness of the generated videos.
• Extensive qualitative and quantitative results across several datasets demonstrate the superiority of the pro-posed framework for high-ﬁdelity video face reenact-ment and freely controllable talking head generation. 2.