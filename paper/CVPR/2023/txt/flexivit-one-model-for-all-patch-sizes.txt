Abstract
Vision Transformers convert images to sequences by slic-ing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but chang-ing the patch size typically requires retraining the model.
In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, mak-ing it possible to tailor the model to different compute bud-(cid:63)All authors made signiﬁcant technical contributions.
Lucas started and led the project. 1 Google Research, Brain Team. 3 work done at Google Brain, while being a PhD student at NYU. 2 Google Research.
1.

Introduction
Vision Transformers (ViTs) cut images into non-overlapping patches and perform all computations on to-kens created from these patches. This “patchiﬁcation” pro-cedure represents a signiﬁcant shift away from the previ-ously dominant convolutional neural network (CNN) ap-proach [32], where an image is processed with small lo-cal and typically overlapping ﬁlters. Patchiﬁcation has un-locked new capabilities, such as (random) dropping of im-age patch tokens [10, 20, 44, 53, 61], adding specialized to-kens for new tasks [54, 56] or mixing image tokens with tokens from other modalities [1, 38, 64].
Despite the importance of patchiﬁcation for ViT mod-els, the role of the patch size has received little attention.
While the original ViT paper [15] works with three patch sizes (32×32, 16×16, and 14×14 pixels), many follow-up works ﬁx the patch size at 16×16 pixels [54, 55, 65]. In this work, we show that the patch size provides a simple and effective lever to change the compute and predictive per-formance of a model, without changing model parametriza-tion. For example, a ViT-B/8 model achieves 85.6% top-1 accuracy on ImageNet1k with 156 GFLOPs and 85 M pa-rameters, while a ViT-B/32 model achieves only 79.1% ac-curacy with 8.6 GFLOPs and 87 M parameters. Despite the major difference in performance and compute, these models have essentially the same parametrization. However, stan-dard ViT models perform well only at the patch size that they have been trained at. Tuning the patch size therefore requires complete re-training of the model.
To overcome this limitation, we propose FlexiViT, a ﬂex-ible ViT which matches or outperforms standard ﬁxed-patch
ViTs across a wide range of patch sizes with no added cost.
To train FlexiViT, we randomize the patch size during train-ing, and resize the positional and patch embedding param-eters adaptively for each patch size, as shown in Figure 1.
These simple modiﬁcations are already sufﬁcient for strong performance, but we also propose a optimized resizing op-eration and a training procedure based on knowledge distil-lation which achieves even better results.
We demonstrate the efﬁciency of FlexiViT models in many downstream tasks, such as image classiﬁcation, trans-fer learning, panoptic and semantic segmentation, image-text retrieval and open-world recognition, and provide a general recipe for ﬂexifying existing ViT-based training se-tups. Furthermore, we show that ﬂexibility of the back-bone, i.e. strong performance across patch sizes, is often preserved even after ﬁne-tuning with a ﬁxed patch size.
We leverage this observation to perform resource-efﬁcient transfer learning: we ﬁnetune the model cheaply with a large patch size, but then deploy it with a small patch size for strong downstream performance. We further show that
ﬂexible patch size can be used to accelerate pre-training.
To explain the effectiveness of FlexiViT, we analyze the model’s representations. We ﬁnd that the representations are often similar across different patch sizes, especially in the deeper layers. Finally, we show that FlexiViT out-performs alternative architectural ways of controlling the performance-compute trade-off in ViT models. 2.