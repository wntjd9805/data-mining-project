Abstract
Modern Generative Adversarial Networks (GANs) gen-erate realistic images remarkably well. Previous work has demonstrated the feasibility of “GAN-classifiers” that are distinct from the co-trained discriminator, and operate on images generated from a frozen GAN. That such classifiers work at all affirms the existence of “knowledge gaps” (out-of-distribution artifacts across samples) present in GAN train-ing. We iteratively train GAN-classifiers and train GANs that
“fool” the classifiers (in an attempt to fill the knowledge gaps), and examine the effect on GAN training dynamics, output quality, and GAN-classifier generalization. We investigate two settings, a small DCGAN architecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA GAN architecture trained on high dimensional images (FFHQ).
We find that the DCGAN is unable to effectively fool a held-out GAN-classifier without compromising the output quality.
However, the StyleGAN2 can fool held-out classifiers with no change in output quality, and this effect persists over multiple rounds of GAN/classifier training which appears to reveal an ordering over optima in the generator parameter space. Finally, we study different classifier architectures and show that the architecture of the GAN-classifier has a strong influence on the set of its learned artifacts. 1.

Introduction
GAN [8] architectures like StyleGAN2 [17] generate high-resolution images that appear largely indistinguishable from real images to the untrained eye [14, 18, 24]. While there are many positive applications, the ability to generate large amounts of realistic images is also a source of concern given its potential application in scaled abuse and misin-formation. In particular, GAN-generated human faces are widely available (e.g., thispersondoesnotexist.com) and have been used for creating fake identities on the internet [12].
Detection of GAN-generated images is an active research area (see [9] for a survey of approaches), with some us-ing custom methods and others using generic CNN-based classifiers. Such classifiers are distinct from the discrimina-tor networks that are trained alongside the generator in the archetypal GAN setup. Given the adversarial nature of the training loss for GANs, the existence of the GAN-classifiers suggest consistent generator knowledge gaps (i.e., artifacts present across samples that distinguish generated images from those of the underlying distribution) left by discrimi-nators during training. Specialized classifiers [31] are able to detect images sampled from held-out GAN instances and even from held-out GAN architectures. These generalization capabilities imply that the knowledge gaps are consistent not only across samples from a GAN generator but across independent GAN generator instances.
In this work we modify the GAN training loss in order to fool a GAN-classifier in addition to the co-trained dis-criminator, and examine the effect on training dynamics and output quality. We conduct multiple rounds of training in-dependent pools (initialized differently) of GANs followed by GAN-classifiers, and gain new insights into the GAN optimization process. We investigate two different settings: in the first setting, we choose the low-dimensional domain of handwritten digits (MNIST [19]), using a small DCGAN
[25] architecture and a vanilla GAN-classifier architecture.
For the second setting, we choose a high-dimensional do-main of human faces (FFHQ [16]) with StyleGAN2 (SG2) as a SOTA GAN architecture, and three different GAN-classifier architectures (ResNet-50 [10], Inception-v3 [28], and MobileNetV2 [27]). Our findings in this paper are as follows:
Samples drawn from a GAN instance exhibit a space of
“artifacts” that are exploited by the classifiers, and this space is strongly correlated with those of other GAN generator instances. This effect is present in both the
DCGAN and SG2 settings.
Upon introducing the need to fool held-out classifiers, the DCGAN is unable to generate high quality outputs.
In the high dimensional setting, however, SG2 gener-•
•
•
ators can easily fool held-out trained classifiers, and move to a new artifact space. Strikingly, we find that the artifact space is correlated among the new popula-tion of generators as it was in the original population.
This correlation appears to persist in subsequent rounds as new classifiers are introduced that are adapted to the new artifact spaces.
•
•
MobileNetV2 classifier instances in the SG2 setting appear unable to learn all of the artifacts available for them to exploit. Instead, MobileNetV2 instances form clusters based on the subset of artifacts learned. We hypothesize this being an effect of classifier capacity.
An SG2 generator trained to reliably fool unseen classi-fier instances from a given architecture is not guaran-teed to fool classifiers from another architecture. There-fore, the artifacts learned by a given classifier depends strongly on the classifier’s architecture. 2.