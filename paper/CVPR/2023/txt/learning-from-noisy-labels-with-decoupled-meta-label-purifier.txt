Abstract
Training deep neural networks (DNN) with noisy labels is challenging since DNN can easily memorize inaccurate labels, leading to poor generalization ability. Recently, the meta-learning based label correction strategy is widely adopted to tackle this problem via identifying and correcting potential noisy labels with the help of a small set of clean validation data. Although training with purified labels can effectively improve performance, solving the meta-learning problem inevitably involves a nested loop of bi-level opti-mization between model weights and hyper-parameters (i.e., label distribution). As compromise, previous methods re-sort to a coupled learning process with alternating update.
In this paper, we empirically find such simultaneous opti-mization over both model weights and label distribution can not achieve an optimal routine, consequently limiting the representation ability of backbone and accuracy of cor-rected labels. From this observation, a novel multi-stage label purifier named DMLP is proposed. DMLP decou-ples the label correction process into label-free representa-tion learning and a simple meta label purifier, In this way,
DMLP can focus on extracting discriminative feature and label correction in two distinctive stages. DMLP is a plug-and-play label purifier, the purified labels can be directly reused in naive end-to-end network retraining or other ro-bust learning methods, where state-of-the-art results are obtained on several synthetic and real-world noisy datasets, especially under high noise levels. Code is available at https://github.com/yuanpengtu/DMLP. 1.

Introduction
Deep learning has achieved significant progress on vari-ous recognition tasks. The key to its success is the availabil-* Equal contribution.
† Corresponding author.
Figure 1. (a) Traditional coupled alternating update to solve meta label purification problem, and (b) the proposed DMLP method that decouples the label purification process into representation learning and a simple non-nested meta label purifier. ity of large-scale datasets with reliable annotations. Collect-ing such datasets, however, is time-consuming and expensive.
Easy ways to obtain labeled data, such as web crawling [31], inevitably yield samples with noisy labels, which is not appropriate to be directly utilized to train DNN since these complex models are vulnerable to memorize noisy labels [2].
Towards this problem, numerous Learning with Noisy La-bel (LNL) approaches were proposed. Classical LNL meth-ods focus on identifying the noisy samples and reducing their effect on parameter updates by abandoning [12] or assigning smaller importance. However, when it comes to extremely noisy and complex scenarios, such scheme struggles since there is no sufficient clean data to train a discriminative clas-sifier. Therefore, label correction approaches are proposed to augment clean training samples by revising noisy labels to underlying correct ones. Among them, meta-learning based approaches [9, 16, 25] achieve state-of-the-art performance via resorting to a small clean validation set and taking noisy labels as hyper-parameters, which provides sound guidance
toward underlying label distribution of clean samples. How-ever, such meta purification inevitably involves a nested bi-level optimization problem on both model weight and hyper-parameters (shown as Fig. 1 (a)), which is computa-tionally infeasible. As a compromise, the alternating update between model weights and hyper-parameters is adopted to optimize the objective [9, 16, 25], leading to a coupled solution for representation learning and label purification.
Empirical observation. Intuitively, alternate optimiza-tion over a large search space (model weight and hyper-parameters) may lead to sub-optimal solutions. To investi-gate how such approximation affects results in robust learn-ing, we conduct empirical analysis on CIFAR-10 [14] with recent label purification methods MLC [41] and MSLC [9], which consist of a deep model and a meta label correction network, and make observation as Fig. 2.
• Coupled optimization hinders quality of corrected la-bels. We first compare the Coupled meta corrector MLC with its extremely Decoupled variant where the model weights are first optimized for 70 epochs with noisy labels and get fixed, then labels are purified with the guidance of validation set. We adopt the accuracy of corrected label to measure the performance of purification. From Fig. 2 (a), we can clearly observe that compared with Decoupled counterpart, joint optimization yields inferior correction per-formance, and these miscorrection will reversely affect the representation learning in coupled optimization.
• Coupled optimization hinders representation ability.
We investigate the representation quality by evaluating the linear prob accuracy [6] of extracted feature in Fig. 2 (b).
We find the representation quality of Coupled training is much worse at the beginning, which leads to slow and un-stable representation learning in the later stage. To further investigate the effect on representation learning, we also resort to a well pretrained backbone with self-supervised learning [5] as initialization, recent research [40] shows pre-trained representation is substantially helpful for LNL frame-work. However, we find this conclusion does not strictly hold for coupled meta label correctors. As shown in Fig. 2 (c), by comparing the classification accuracy from classi-fier of MLC/MSLC, we observe the pretrained model only brings marginal improvement if model weights is still cou-pled with hyper-parameters. In contrast, when the weight of backbone is fixed and decoupled from the label purification and classifier, the improvement becomes more significant.
Decoupled Meta Purification. From the observation above, we find the decoupling between model weights and hyperparameters of meta correctors is essential to label ac-curacy and final results. Therefore, in this paper, we aim at detaching the meta label purification from representation learning and designing a simple meta label purifier which is more friendly to optimization of label distribution prob-lem than existing complex meta networks [9, 41]. Hence we propose a general multi-stage label correction strategy, named Decoupled Meta Label Purifier (DMLP). The core of DMLP is a meta-learning based label purifier, however, to avoid solving the bi-level optimization with a coupled solution, DMLP decouples this process into self-supervised representation learning and a linear meta-learner to fit un-derlying correct label distribution (illustrated as Fig. 1 (b)), thus simplifies the label purification stage as a single-level optimization problem. The simple meta-learner is carefully designed with two mutually reinforcing correcting processes, named intrinsic primary correction (IPC) and extrinsic aux-iliary correction (EAC) respectively. IPC plays the role of purifying labels in a global sense at a steady pace, while EAC targets at accelerating the purification process via looking ahead (i.e., training with) the updated labels from IPC. The two processes can enhance the ability of each other and form a positive loop of label correction. Our DMLP framework is flexible for application, the purified labels can either be directly applied for naive end-to-end network retraining, or exploited to boost the performance of existing LNL frame-works. Extensive experiments conducted on mainstream benchmarks, including synthetic (noisy versions of CIFAR) and real-world (Clothing1M) datasets, demonstrate the supe-riority of DMLP. In a nutshell, the key contributions of this paper include:
• We analyze the necessity of decoupled optimization for label correction in robust learning, based on which we propose DMLP, a flexible and novel multi-stage label purifier that solves bi-level meta-learning problem with a decoupled manner, which consists of representation learning and non-nested meta label purification;
• In DMLP, a novel non-nested meta label purifier equipped with two correctors, IPC and EAC is proposed.
IPC is a global and steady corrector, while EAC accelerates the correction process via training with the updated labels from IPC. The two processes form a positive training loop to learn more accurate label distribution;
• Deep models trained with purified labels from DMLP achieve state-of-the-art results on several synthetic and real-world noisy datasets across various types and levels of label noise, especially under high noise levels. Extensive ablation studies are provided to verify the effectiveness. 2.