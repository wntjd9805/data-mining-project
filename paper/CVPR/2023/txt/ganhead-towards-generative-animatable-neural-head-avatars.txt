Abstract
To bring digital avatars into people’s lives, it is highly demanded to efficiently generate complete, realistic, and animatable head avatars. This task is challenging, and it is difficult for existing methods to satisfy all the requirements at once. To achieve these goals, we propose GANHead (Generative Animatable Neural Head Avatar), a novel gen-erative head model that takes advantages of both the fine-grained control over the explicit expression parameters and the realistic rendering results of implicit representations.
Specifically, GANHead represents coarse geometry, fine-gained details and texture via three networks in canonical space to obtain the ability to generate complete and realistic
∗Corresponding author
Project page: https://wsj-sjtu.github.io/GANHead/ head avatars. To achieve flexible animation, we define the deformation filed by standard linear blend skinning (LBS), with the learned continuous pose and expression bases and
LBS weights. This allows the avatars to be directly ani-mated by FLAME [22] parameters and generalize well to unseen poses and expressions. Compared to state-of-the-art (SOTA) methods, GANHead achieves superior performance on head avatar generation and raw scan fitting. 1.

Introduction
How to efficiently generate photorealistic and animat-able head avatars without manual effort is an open prob-lem in computer vision and computer graphics, which has numerous applications in VR/AR, games, movies, and the metaverse. In these applications, it is desirable that the head
avatar models fulfill the following requirements: (1) Com-plete, i.e., the 3D model can cover the entire head including the frontal face, the back of head, and the hair region; (2)
Realistic, where the avatar is expected to display vivid tex-ture and detailed geometry; (3) Animatable, i.e., the avatar is supposed to be fully riggable over poses and expressions, and can be controlled with low-dimensional parameters; (4)
Generative model can be more flexibly applied to various downstream tasks, therefore, the head avatar model is pre-ferred to be generative rather than discriminative for large-scale content generation tasks.
We investigate the research on neural head avatars and summarize previous works in Tab. 1. 3D morphable models (3DMMs) built from registered meshes have been widely employed to model head avatars. Principal component anal-ysis (PCA) is applied to shape and texture, and novel sub-jects can be generated by sampling the coefficients of PCA bases. However, registering real-world raw scans to a tem-plate mesh with fixed topology is non-trivial, and it is diffi-cult to define a fixed topology for complex regions like hair.
As a result, most of these methods only model the facial region [3±6, 13, 15, 23, 39], while a few cover the full head without hair [2, 11, 22, 33, 36]. Moreover, the oversimplifi-cation of PCA makes the models lack of realism.
In parallel with explicit meshes, implicit representations have been utilized to approximate complex surfaces. Some discriminative models [14,19,24,31,49] successfully model the complete head geometry with realistic texture. How-ever, these methods can only be applied to the reconstruc-tion task, incapable of generating new samples. Mean-while, 3D-aware GANs based on implicit representations
[7, 8, 29, 48] can generate multi-view-consistent frontal face images. Nevertheless, the heads are still incomplete.
In addition, it is difficult to animate the neural head avatars generated by 3D-aware GANs. Recently, several implicit generative models [18, 41, 47, 50] achieve realistic and ani-matable head avatars. However, these models either cannot generate complete head with satisfactory geometry [18,50], or can only be animated implicitly via the learned latent codes [47], which is inconvenient and limits the general-ization ability to unseen poses and expressions.
It is natural to ask a question: can we build a model that can generate diverse realistic head avatars, and mean-while be compatible with the animation parameters of the common parametric face model (such as FLAME [22])?
In this work, we propose a generative animatable neural head avatar model, namely, GANHead, that simultaneously fulfills these requirements. Specifically, GANHead repre-sents the 3D head implicitly with neural occupancy func-tion learned by MLPs, where coarse geometry, fine-gained details and texture are respectively modeled via three net-works. Supervised with unregistered ground truth 3D head scans, all these networks are defined in canonical space via
Scheme
Explicit 3DMMs 3D-aware
GANs
Personalized
Avatars
Implicit
Head Models
Methods Complete Realistic Animatable Generative
[2, 3, 15, 40]
[7, 8, 29, 48]
[16, 49]
[14, 31, 44]
[50]
[18]
[47]
Ours
✗
✗
✓
✗
✗
✗
✓
✓
✗
✓
✓
✓
✓
✓
✓
✓
✓
✗
✓
△
△
✓
△
✓
✓
✓
✗
✗
✓
✓
✓
✓
Table 1. A summary of current head avatar methods. △ denotes that the head avatar can only be animated implicitly via the learned latent codes, and cannot generalize well to unseen expressions. auto-decoder structures that are conditioned by shape, detail and color latent codes, respectively. This framework allows
GANHead to achieve complete and realistic generation re-sults, while yielding desirable generative capacity.
The only remaining question is how to control the im-plicit representation with animation parameters? To answer this question, we extend the multi-subject forward skinning method designed for human bodies [9] to human faces, en-abling our framework to achieve flexible animation explic-itly controlled by FLAME [22] pose and expression param-eters. Inspired by IMAvatar [49], the deformation field in
GANHead is defined by standard vertex based linear blend skinning (LBS) with the learned pose-dependent corrective bases, the linear blend skinning weights, and the learned expression bases to capture non-rigid deformations. In this way, GANHead can be learned from textured scans, and no registration or canonical shapes are needed.
Once GANHead is trained, we can sample shape, de-tail and color latent codes to generate diverse textured head avatars, which can then be animated flexibly by
FLAME parameters with nice geometry consistency and pose/expression generalization capability. We compare our method with the state-of-the-art (SOTA) complete head generative models, and demonstrate the superiority of our method.
In summary, our main contributions are:
• We propose a generative animatable head model that can generate complete head avatars with realistic tex-ture and detailed geometry.
• The generated avatars can be directly animated by
FLAME [22] parameters, robust to unseen poses and expressions.
• The proposed model achieves promising results in head avatar generation and raw scan fitting compared with SOTA methods.
2.