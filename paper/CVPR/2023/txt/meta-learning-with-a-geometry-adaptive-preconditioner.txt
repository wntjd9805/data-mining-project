Abstract
Model-agnostic meta-learning (MAML) is one of the most successful meta-learning algorithms. It has a bi-level opti-mization structure where the outer-loop process learns a shared initialization and the inner-loop process optimizes task-specific weights. Although MAML relies on the stan-dard gradient descent in the inner-loop, recent studies have shown that controlling the inner-loop’s gradient descent with a meta-learned preconditioner can be beneficial. Existing preconditioners, however, cannot simultaneously adapt in a task-specific and path-dependent way. Additionally, they do not satisfy the Riemannian metric condition, which can enable the steepest descent learning with preconditioned gradient. In this study, we propose Geometry-Adaptive Pre-conditioned gradient descent (GAP) that can overcome the limitations in MAML; GAP can efficiently meta-learn a preconditioner that is dependent on task-specific param-eters, and its preconditioner can be shown to be a Rieman-nian metric. Thanks to the two properties, the geometry-adaptive preconditioner is effective for improving the inner-loop optimization. Experiment results show that GAP out-performs the state-of-the-art MAML family and precondi-tioned gradient descent-MAML (PGD-MAML) family in a variety of few-shot learning tasks. Code is available at: https://github.com/Suhyun777/CVPR23-GAP. 1.

Introduction
Meta-learning, or learning to learn, enables algorithms to quickly learn new concepts with only a small number of samples by extracting prior-knowledge known as meta-knowledge from a variety of tasks and by improving the gen-eralization capability over the new tasks. Among the meta-learning algorithms, the category of optimization-based meta-learning [8, 17, 20, 21, 48] has been gaining popularity
*Equal contribution.
†Corresponding author. due to its flexible applicability over diverse fields including robotics [55, 59], medical image analysis [40, 54], language modeling [37,42], and object detection [46,61]. In particular,
Model-Agnostic Meta-Learning (MAML) [20] is one of the most prevalent gradient-based meta-learning algorithms.
Many recent studies have improved MAML by adopting a Preconditioned Gradient Descent (PGD1) for inner-loop optimization [34, 36, 44, 49, 53, 57, 66]. In this paper, we collectively address PGD-based MAML algorithms as the
PGD-MAML family. A PGD is different from the ordinary gradient descent because it performs a preconditioning on the gradient using a preconditioning matrix P, also called a preconditioner. A PGD-MAML algorithm meta-learns not only the initialization parameter θ0 of the network but also the meta-parameter ϕ of the preconditioner P.
For the inner-loop optimization, P was kept static in most of the previous works (Figure 1(b)) [34, 36, 44, 57, 66]. Some of the previous works considered adapting the preconditioner P with the inner-step k (Figure 1(c)) [49] and some others with the individual task (Figure 1(d)) [53].
They achieved performance improvement by considering individual tasks and inner-step, respectively, and showed that both factors were valuable. However, both factors have not been considered simultaneously in the existing studies.
When a parameter space has a certain underlying struc-ture, there exists a Riemannian metric corresponding the pa-rameter space [3, 4]. If the preconditioning matrix is the Rie-mannian metric, the preconditioned gradient is known to be-come the steepest descent on the parameter space [2–4,6,27].
An illustration of a toy example is shown in Figure 2. The optimization path of an ordinary gradient descent is shown in Figure 2(a). Compared to the ordinary gradient descent, a preconditioned gradient descent with a preconditioner that does not satisfy the Riemannian metric condition can actually harm the optimization. For the example in Figure 2(b), the preconditioner affects the optimization into an undesirable direction and negatively affects the gradient descent. On the 1PGD in our work should not be confused with Projected Gradient
Descent [13].
(a) MAML (b) Non-adaptive P(ϕ) (c) Adaptive P(k; ϕ) (d) Adaptive P(Dtr
τ ; ϕ) (e) Adaptive P(θτ,k; ϕ)
Figure 1. Diagram of MAML and PGD-MAML family. For the inner-loop adaptation in each diagram, the dotted lines of the same color indicate that they use a common preconditioning matrix (preconditioner). (a) MAML adaptation: no preconditioner is used (i.e., P = I). (b) P(ϕ): a constant preconditioner is used in the inner-loop where the preconditioner’s meta-parameter ϕ is meta-learned. (c) P(k; ϕ): a constant preconditioner is used for each inner-step k. Preconditioner for each step is meta-learned, but P(k, ϕ) is not task-specific. (d)
P(Dtr
τ ; ϕ) is not dependent on k. (e) GAP adapts P(θτ,k; ϕ): a fully adaptive preconditioner is used where it is task-specific and path-dependent. Instead of saying ‘dependent on k’, we specifically say it is path-dependent because the exact dependency is on the task-specific parameter set θτ,k that is considerably more informative than k.
τ ; ϕ): a constant preconditioner is used for each task. Preconditioner for each task is meta-learned, but P(Dtr (a) Gradient Descent (GD) (b) Preconditioned GD (Non-Riemannian metric) (c) Preconditioned GD (Riemannian metric)
Figure 2. A toy example for illustrating the effect of Riemannian metric (see Supplementary Section A). When the curvature of the parameter space is poorly conditioned, (a) gradient descent can suf-fer from the difficulty of finding the solution, (b) a preconditioned gradient descent with a preconditioner that does not satisfy the
Riemannian metric condition can suffer further, and (c) a precondi-tioned gradient descent with a preconditioner that is a Riemannian metric can perform better. contrary, if the preconditioner is the Riemannian metric cor-responding the parameter space, the preconditioned gradient descent can become the steepest descent and can exhibit a better optimization behavior as shown in Figure 2(c). While the Riemannian metric condition (i.e., positive definiteness) is a necessary condition for steepest descent learning, the existing studies on PGD-MAML family did not consider constraining preconditioners to satisfy the condition for Rie-mannian metric.
In this study, we propose a new PGD method named
Geometry Aaptive Preconditioned gradient descent (GAP).
Specifically, GAP satisfies two desirable properties which have not been considered before. First, GAP’s precondi-tioner PGAP is a fully adaptive preconditioner that can adapt to the individual task (task-specific) and to the optimization-path (path-dependent). The full adaptation is made possible by having the preconditioner depend on the task-specific parameter θτ,k (Figure 1(e)). Second, we prove that PGAP is a Riemannian metric. To this end, we force the meta-parameters of PGAP to be positive definite. Thus, GAP guar-antees the steepest descent learning on the parameter space corresponding to PGAP. Owing to the two properties, GAP enables a geometry-adaptive learning in inner-loop optimiza-tion.
For the implementation of GAP, we utilize the Singular
Value Decomposition (SVD) operation to come up with our preconditioner satisfying the desired properties. For the recently proposed large-scale architectures, computational overhead can be an important design factor and we provide a low-computational approximation, Approximate GAP, that can be proven to asymptotically approximate the operation of GAP.
To demonstrate the effectiveness of GAP, we empiri-cally evaluate our algorithm on popular few-shot learning tasks; few-shot regression, few-shot classification, and few-shot cross-domain classification. The results show that GAP outperforms the state-of-the-art MAML family and PGD-MAML family.
The main contributions of our study can be summarized as follows:
• We propose a new preconditioned gradient descent method called GAP, where it learns a preconditioner that enables a geometry-adaptive learning in the inner-loop optimization.
• We prove that GAP’s preconditioner has two desir-able properties: (1) It is both task-specific and path-dependent (i.e., dependent on task-specific parameter
θτ,k). (2) It is a Riemannian metric.
• For large-scale architectures, we provide a low-computational approximation that can be theoretically shown to approximate the GAP method.
• For popular few-shot learning benchmark tasks, we empirically show that GAP outperforms the state-of-the-art MAML family and PGD-MAML family. 2.