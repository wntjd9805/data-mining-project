Abstract
Depth-from-defocus (DFD), modeling the relationship between depth and defocus pattern in images, has demon-strated promising performance in depth estimation. Re-cently, several self-supervised works try to overcome the difficulties in acquiring accurate depth ground-truth. How-ever, they depend on the all-in-focus (AIF) images, which cannot be captured in real-world scenarios. Such limi-tation discourages the applications of DFD methods. To tackle this issue, we propose a completely self-supervised framework that estimates depth purely from a sparse fo-cal stack. We show that our framework circumvents the needs for the depth and AIF image ground-truth, and re-ceives superior predictions, thus closing the gap between the theoretical success of DFD works and their applica-tions in the real world.
In particular, we propose (i) a more realistic setting for DFD tasks, where no depth or
AIF image ground-truth is available; (ii) a novel self-supervision framework that provides reliable predictions of depth and AIF image under the challenging setting. The proposed framework uses a neural model to predict the depth and AIF image, and utilizes an optical model to val-idate and refine the prediction. We verify our framework on three benchmark datasets with rendered focal stacks and real focal stacks. Qualitative and quantitative evaluations show that our method provides a strong baseline for self-supervised DFD tasks. The source code is publicly avail-able at https://github.com/Ehzoahis/DEReD. 1.

Introduction
Depth estimation is a fundamental task in computer vi-sion. Predicting depth from RGB images shows great po-* The contributions by Haozhe Si have been conducted and com-pleted during his internship at Shanghai AI Laboratory.
† Equal contribution.
‡ Corresponding author.
Figure 1. (a) A sparse focal stack from the NYUv2 dataset [17]. (b) The ground-truth depth map and all-in-focus (AIF) image. (c)
The depth map and AIF image estimated by our self-supervised framework from the sparse focal stack. tential in applications like autonomous cars or robots.
It has also been used as a powerful pretext training task in unsupervised downstream tasks including visual feature ex-traction [11, 20], semantic segmentation [2, 9], etc. How-ever, collecting sufficiently diverse datasets with per-pixel depth ground-truth for supervised learning is challenging.
To overcome this limitation, self-supervised depth estima-tion works exploit the consistency of 3D geometries within stereo pairs [5, 6] or monocular videos [7, 10], and have shown impressive results. Nevertheless collecting synchro-nized multi-view images or videos is resource-consuming.
Another clue for depth estimation is defocus. Defocus blur is a measurable property of images that is associated with the geometry of camera lens and the changes in depth.
Previous works [4, 22] prove that depth can be recovered from a set of images with different focus distances, i.e., a fo-cal stack, by observing their blurry amounts. With the help of deep learning methods, supervised DFD works [15, 27]
estimate depth in a data-driven way. Recent DFD meth-ods [8, 14, 24] claim they are freed from being supervised by the depth ground-truth. However, these works utilize the all-in-focus (AIF) image in model training, which is more of a theoretical concept than images captured by cam-era lens [13] in the real world. Existing works generally treat images taken with small apertures as the AIF image.
Such approximations inevitably contain regional blurriness and suffer from underexposure in short range. Thus, current
DFD methods have drawbacks in piratical applications.
In order to close the gap between theories and real-world applications, we design a more realistic setting for the DFD tasks: only focal stacks are provided in model training, while the availability of depth and AIF images ground-truth is deprived. This is a challenging task because we no longer have the direct/indirect optimization goal for the model. To tackle this challenge, we propose a self-supervised DFD framework, which constrains the predicted depth map and
AIF image to be accurate by precisely reconstructing fo-cal stacks. Specifically, our framework consists of a neu-ral model, DepthAIF-Net (DAIF-Net), which predicts the
AIF image and the depth map from the focal stack, and a phyisical-realistic optical model that reconstructs the input from the predicted AIF image and depth map. Since all im-ages in a focal stack are sharing the same depth and AIF im-age, and the physics model can deterministically map them to focal stacks, accurate depth maps and AIF images are a necessary and sufficient condition for precisely reconstruct-ing the input. Therefore, by assuring the consistency be-tween the input and the reconstructed focal stack, the pre-diction of depth map and AIF image can be intermediately improved. To the best of our knowledge, this is the first self-supervised DFD work that relieves the need for AIF images and depth ground-truth. Such improvements over previous supervised and indirectly-supervised works make our method more applicable in real scenarios.
Extensive experiments are conducted on both synthetic and real datasets. Results show that the proposed frame-work is on par with the state-of-the-art supervised/indirectly supervised depth-from-focus/defocus methods and has con-vincing visual quality, as shown in Figure 1. Additionally, we apply our model to the data in the wild, and demonstrate the ability of our framework to be applied in real scenarios.
Finally, we extend the training paradigm of our framework so that our model has the ability to be transferred between focal stacks with an arbitrary number of images.
Our contribution is three-fold:
• We design a more realistic and challenging scenario for the Depth-from-Defocus tasks, where only focal stacks are available in model training and evaluation.
• We propose the first completely self-supervised frame-work for DFD tasks. The framework predicts depth and AIF images simultaneously from a focal stacks and is supervised by reconstructing the input.
• Our framework performs favorably against the su-pervised state-of-the-art methods, providing a strong baseline for future self-supervised DFD tasks. 2.