Abstract
Recent LSS-based multi-view 3D object detection has made tremendous progress, by processing the features in
Brid-Eye-View (BEV) via the convolutional detector. How-ever, the typical convolution ignores the radial symmetry of the BEV features and increases the difficulty of the de-tector optimization. To preserve the inherent property of the BEV features and ease the optimization, we propose an azimuth-equivariant convolution (AeConv) and an azimuth-equivariant anchor. The sampling grid of AeConv is always in the radial direction, thus it can learn azimuth-invariant
BEV features. The proposed anchor enables the detection head to learn predicting azimuth-irrelevant targets. In ad-dition, we introduce a camera-decoupled virtual depth to unify the depth prediction for the images with different cam-era intrinsic parameters. The resultant detector is dubbed
Azimuth-equivariant Detector (AeDet). Extensive experi-ments are conducted on nuScenes, and AeDet achieves a 62.0% NDS, surpassing the recent multi-view 3D object de-tectors such as PETRv2 and BEVDepth by a large mar-gin. Project page: https://fcjian.github.io/ aedet. 1.

Introduction
In the field of autonomous driving, multi-view 3D ob-ject detection has been one of the most widely researched problems and received a lot of attention due to its low as-In the recent literature, sembly cost and high efficiency. such vision-based 3D object detector has made tremendous progress, especially for the Lift-Splat-Shoot (LSS) based methods [11,15]. They first transfer the image features from the image-view to Bird-Eye-View (BEV), and then process the BEV features via the convolutional backbone and detec-tion head similar to 2D object detection [6, 7, 28].
However, the BEV features in multi-view 3D object de-tection are significantly different from the image features in 2D object detection: (1) the BEV features from the same camera are naturally endowed with radial symmetry; (2) the cameras have different orientations in BEV, and the BEV
Figure 1. Illustration of the BEV features and predictions from the typical LSS-based detector BEVDepth [15] (top row) and our
AeDet (bottom row). The BEV features are the outputs of the
BEV backbone. Assume the six cameras capture the same imag-ing, and the detector takes the same imaging as the input of the six views. BEVDepth generates different features and predictions for the same bus in different azimuths, while AeDet yields almost the same feature and prediction for the same bus in different azimuths. features from the different cameras are also approximately with radial symmetry. Simply using the typical 2D back-bone and detection head to perform BEV perception (such as [11, 15]) ignores the inherent property of the BEV fea-tures and suffers from two limitations discussed as follows:
First, the BEV representation of the same imaging in different azimuths is inconsistent. The typical convolu-tion shares the kernel weights and adopts the same regular sampling grid at each location of the features. Such de-sign may destroy the radial symmetry of the BEV features, and thus is unfriendly to representation learning. To be spe-cific, assume the six cameras capture the same imaging of the object, and the detector takes the same imaging as the input of the six views. These images are then transferred to be the rotation-equivalent features in different azimuths from BEV. However, the sampling grid of the convolution is
translation-invariant and samples the inconsistent BEV fea-tures of the object in different azimuths (see Figure 3a for detailed demonstration). Consequently, as demonstrated in the ‘BEV feature’ column in Figure 1, the convolution of
BEVDepth learns different features in different azimuths, increasing the difficulty of the representation learning.
Second, the prediction targets of the same imaging in different azimuths are inconsistent. The typical detec-tion head predicts the object orientation and velocity along the Cartesian coordinates, which requires the detection head to predict different targets for the same imaging in differ-ent azimuths. Concretely, assume the different-view cam-eras capture the same imaging of the object at different mo-ments. After mapping the imaging from the image to BEV, the object would have different orientations and velocities along the Cartesian coordinates in different azimuths (see
Figure 3b for detailed demonstration). As a result, the de-tection head is required to predict different targets even for the same imaging in different azimuths, and inevitably in-creases the difficulty of the predictions.
To address the two limitations, we propose an Azimuth-equivariant Detector (AeDet) that aims to perform an azimuth-invariant BEV perception by modeling the prop-(1) Azimuth-erty of radial symmetry to the network: equivariant convolution. In contrast to the typical convo-lution that uses the same regular sampling grid at each loca-tion, we design an Azimuth-equivariant Convolution (Ae-Conv) to rotate the sampling grid according to the azimuth at each location. AeConv enables the sampling grid equiv-ariant to the azimuth and always in the radial direction of the camera. This allows the convolution to preserve the ra-dial symmetry of the BEV features and unify the represen-tation in different azimuths. (2) Azimuth-equivariant an-chor. To unify the prediction targets in different azimuths, we propose an azimuth-equivariant anchor. Specifically, different from the typical anchor (anchor point or anchor box) defined along the Cartesian coordinates, the azimuth-equivariant anchor is defined along the radial direction and equivariant to the azimuth. We predict both the bounding box and velocity along the new anchor and its orthogo-nal directions, yielding the same prediction target for the same imaging of the object in different azimuths. Thus the azimuth-equivariant anchor enables the detection head to learn predicting azimuth-irrelevant targets. Notably, Ae-Conv and the azimuth-equivariant anchor can work collabo-ratively to improve the consistency between the representa-tion learning and predictions, as shown in the ‘BEV feature’ and ‘Predictions’ columns of AeDet in Figure 1.
In addition, we introduce a camera-decoupled virtual depth to improve the depth prediction, and ease the opti-mization of the depth network. In specific, we decouple the camera’s intrinsic parameters from the depth network, en-abling the depth network to model the relationship between
In this way, the the image feature and the virtual depth. depth network only needs to learn to predict a universal vir-tual depth, regardless of the intrinsic parameters of different cameras. Finally, we map the virtual depth to the real depth according to the classic camera model.
To summarize, we make the following contributions: (1)
We design an azimuth-equivariant convolution to unify the representation learning in different azimuths, and extract the azimuth-invariant BEV features. (2) We propose a new azimuth-equivariant anchor to redefine the anchor along the radial direction and unify the prediction targets in differ-ent azimuths. (3) We introduce a camera-decoupled virtual depth to unify the depth prediction for the images captured by different cameras. (4) We conducted extensive experi-ments on nuScenes [1], where our AeDet significantly im-proves the accuracy of the object orientation (by 5.2%) and velocity (by 6.6%). AeDet achieves 62.0% NDS on the nuScenes test set, surpassing the recent multi-view object detectors such as BEVDepth [15] by a large margin. 2.