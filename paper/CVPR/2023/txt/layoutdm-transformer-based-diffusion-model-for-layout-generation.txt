Abstract
Automatic layout generation that can synthesize high-quality layouts is an important tool for graphic design in many applications. Though existing methods based on gen-erative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) have pro-gressed, they still leave much room for improving the qual-ity and diversity of the results. Inspired by the recent suc-cess of diffusion models in generating high-quality images, this paper explores their potential for conditional layout generation and proposes Transformer-based Layout Diffu-sion Model (LayoutDM) by instantiating the conditional de-noising diffusion probabilistic model (DDPM) with a purely transformer-based architecture.
Instead of using convo-lutional neural networks, a transformer-based conditional
Layout Denoiser is proposed to learn the reverse diffu-sion process to generate samples from noised layout data.
Benefitting from both transformer and DDPM, our Lay-outDM is of desired properties such as high-quality genera-tion, strong sample diversity, faithful distribution coverage, and stationary training in comparison to GANs and VAEs.
Quantitative and qualitative experimental results show that our method outperforms state-of-the-art generative models in terms of quality and diversity. 1.

Introduction
Layouts, i.e. the arrangement of the elements to be dis-played in a design, play a critical role in many applications from magazine pages to advertising posters to application interfaces. A good layout guides viewers’ reading order and draws their attention to important information. The se-mantic relationships of elements, the reading order, canvas space allocation and aesthetic principles must be carefully decided in the layout design process. However, manually arranging design elements to meet aesthetic goals and user-specified constraints is time-consuming. To aid the design of graphic layouts, the task of layout generation aims to
*Corresponding author. generate design layouts given a set of design components with user-specified attributes. Though meaningful attempts are made [1, 10, 11, 15, 18, 21, 23–25, 33, 44–46], it is still challenging to generate realistic and complex layouts, be-cause many factors need to be taken into consideration, such as design elements, their attributes, and their relationships to other elements.
Over the past few years, generative models such as
Generative Adversarial Networks (GANs) [9] and Varia-tional Auto-Encoders (VAEs) [20] have gained much at-tention in layout generation, as they have shown a great promise in terms of faithfully learning a given data dis-tribution and sampling from it. GANs model the sam-pling procedure of a complex distribution that is learned in an adversarial manner, while VAEs seek to learn a model that assigns a high likelihood to the observed data sam-ples. Though having shown impressive success in gener-ating high-quality layouts, these models have some limita-tions of their own. GANs are known for potentially unstable training and less distribution coverage due to their adversar-ial training nature [4, 5, 27], so they are inferior to state-of-the-art likelihood-based models (such as VAEs) in terms of diversity [28, 29, 34]. VAEs can capture more diversity and are typically easier to scale and train than GANs, but still fall short in terms of visual sample quality and sampling efficiency [22].
Recently, diffusion models such as denoising diffusion probabilistic model (DDPM) [14] have emerged as a pow-erful class of generative models, capable of producing high-quality images comparable to those of GANs. Importantly, they additionally offer desirable properties such as strong sample diversity, faithful distribution coverage, a station-ary training objective, and easy scalability. This implies that diffusion models are well suited for learning models of complex and diverse data, which also motivates us to ex-plore the potential of diffusion-based generative models for graphic layout generation.
Though diffusion models have shown splendid perfor-mance in high-fidelity image generation [8,14,35,39,41], it is still a sparsely explored area and provides unique chal-lenges to develop diffusion-based generative models for
layout generation. First, diffusion models often use con-volutional neural networks such as U-Net [36] to learn the reverse process to construct desired data samples from the noise. However, a layout is a non-sequential data structure consisting of varying length samples with discrete (classes) and continuous (coordinates) elements simultaneously, in-stead of pixels laid on a regular lattice. Obviously, convo-lutional neural networks are not suitable for layout denois-ing, which prevents diffusion models from being directly applied to layout generation. Second, the placement and sizing of a given element depend not only on its attributes (such as category label) but also on its relationship to other elements. How to incorporate the attributes knowledge and model the elements’ relationship in diffusion models is still an open problem. Since diffusion models are general frame-works, they leave room for adapting the underlying neural architectures to exploit the properties of the data.
Inspired by the above insights, by instantiating the con-ditional denoising diffusion probabilistic model (DDPM) with a transformer architecture, this paper proposes
Transformer-based Layout Diffusion Model (i.e., Lay-outDM) for conditional layout generation given a set of el-ements with user-specified attributes. The key idea is to use a purely transformer-based architecture instead of the commonly used convolutional neural networks to learn the reverse diffusion process from noised layout data. Benefit-ting from the self-attention mechanism in transformer lay-ers, LayoutDM can efficiently capture high-level relation-ship information between elements, and predict the noise at each time step from the noised layout data. Moreover, the attention mechanism also helps model another aspect of the data - namely a varying and large number of elements. Fi-nally, to generate layouts with desired attributes, LayoutDM designs a conditional Layout Denoiser (cLayoutDenoiser) based on a transformer architecture to learn the reverse dif-fusion process conditioned on the input attributes. Different from previous transformer models in the context of NLP or video, cLayoutDenoiser omits the positional encoding which indicates the element order in the sequence, as we do not consider the order of designed elements on a canvas in our setting. In comparison with current layout genera-tion approaches (such as GANs and VAEs), our LayoutDM offers several desired properties such as high-quality gener-ation, better diversity, faithful distribution coverage, a sta-tionary training objective, and easy scalability. Extensive experiments on five public datasets show that LayoutDM outperforms state-of-the-art methods in different tasks.
In summary, our main contributions are as follows:
• This paper proposes a novel LayoutDM to generate high-quality design layouts for a set of elements with user-specified attributes. Compared with existing meth-ods, LayoutDM is of desired properties such as high-quality generation, better diversity, faithful distribution coverage, and stationary training. To our best knowl-edge, LayoutDM is the first attempt to explore the po-tential of diffusion model for graphic layout generation.
• This paper explores a new class of diffusion models by replacing the commonly-used U-Net backbone with a transformer, and designs a novel cLayoutDenoiser to reverse the diffusion process from noised layout data and better capture the relationship of elements.
• Extensive experiments demonstrate that our method outperforms state-of-the-art models in terms of visual perceptual quality and diversity on five diverse layout datasets. 2.