Abstract
Action quality assessment (AQA) has become an emerg-ing topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets fo-cus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Dis-tinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 sec-onds. As for richness in annotations, LOGO includes for-mation labels to depict group information of multiple ath-letes and detailed annotations on action procedures. Fur-thermore, we propose a simple yet effective method to model relations among athletes and reason about the potential
* indicates the corresponding author. temporal logic in long-form videos. Specifically, we de-sign a group-aware attention module, which can be eas-ily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group informa-tion. To benchmark LOGO, we systematically conduct in-vestigations on the performance of several popular meth-ods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at https://github.com/shiyi-zh0408/LOGO. 1.

Introduction
Action quality assessment (AQA) is applicable to many real-world contexts where people evaluate how well a spe-cific action is performed such as sports events [13, 18, 29–
Table 1. Comparisons of LOGO and existing datasets of action quality assessment (upper part of the table) and group activity recognition (lower part of the table). Score indicates the score annotations; Action denotes action types and temporal boundaries; Act.Label indicates the action types of both individuals and groups. Bbox indicates bounding boxes for actors. Formation represents formation annotations.
Temp. indicates temporal boundary, Spat. indicates spatial localization.
Dataset
Duration
Avg.Dur.
Anno.Type
Samples Events Form.Anno. Year
MTL Dive [33]
UNLV Dive [31]
AQA-7-Dive [29]
MTL-AQA [30]
Rhythmic Gymnastics [53] 26h,23m,20s
FSD-10 [26]
FineDiving [50] 15m,54s 23m,26s 37m,31s 96m,29s
-3h,30m,0s
Collective Activity [7]
NCAA [35]
Volleyball [16]
FineGym [38]
Multisports [25]
-16h,9m,52s 2h,12m,1s 161h,1m,45s 18h,34m,40s 6.0s 3.8s 4.1s 4.1s 1m,35s 3-30s 4.2s
-4s 1.64s 45.7s 20.9s
Score
Score
Score
Action,Score
Score
Action,Score
Action,Score
Act.Label
Bbox,Act.Label
Bbox,Act.Label
Act.Label,Temp.
Act.Label,Temp.,Spat. 159 370 549 1412 1000 1484 3000 44 11436 4830 12685 3200
LOGO(Ours) 11h,20m,41s 3m,24s Action,Formation,Score 200 1 1 6 16 4
-30
---10 247 26
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗ 2014 2017 2019 2019 2020 2020 2022 2009 2016 2016 2020 2021
✓(15764) 33, 46], healthcare [28, 39, 55–58], art performances, mil-itary parades, and others. Due to the extensive applica-tion of AQA, many efforts have been made over the past few years. Although some existing works have achieved promising performances in several simple scenarios, the ap-plication of AQA in many situations is still difficult to im-plement. In the data-driven era, the richness of the dataset largely determines whether the model can be applied to a wide range of scenarios or not.
Inspired by this, we re-viewed the existing datasets in AQA and concluded that they are not rich enough for the following two reasons:
Simplicity of Scenarios. We argue that the complex-ity of the AQA application scenes is reflected in two as-pects, the number of people and the duration of the videos.
In snowboarding, for example, there is only one performer, while there are multiple actors in a military parade. In div-ing, the duration is only about 5 seconds, while in artis-tic swimming and dance performances, the duration of the action reaches several minutes. However, most existing datasets contain a single performer in each sample and many of them collect videos of 3-8s [29–31, 33, 50], which makes it difficult for existing methods to model complex scenes with more actors and longer duration. In such cases, just focusing on how well actions are performed by each individual may be insufficient. Relations among actors should be built, and the potential temporal logic in long-term videos should be modeled.
Coarse-grained Annotations. Though there have been some long-form video datasets in AQA [49, 53] which pro-vide more complex scenes, they typically contain the score as the only annotation. Such coarse-grained annotation makes it difficult for models to learn deeper information, es-pecially in more complex situations. Simply judging action quality via regressing a score for a long-term video could be confusing since we cannot figure out how the model de-termines whether an action is well-performed or not.
To address these issues, we propose a multi-person long-form video dataset, LOGO (short for LOng-form GrOup).
With 8 actors in each sample, LOGO contains videos with an average duration of 204.2s, much longer than most ex-isting datasets in AQA, making the scenes more complex.
Besides, as shown in Figure 1, LOGO contains fine-grained annotations including frame-wise action type labels and temporal boundaries of actions for each video. We also devise formation labels to depict relations among actors.
Specifically, we use a convex polygon to represent the for-mation actors perform, which reflects their position infor-mation and group information. In general, LOGO is distin-guished by its scenario complexity, while it also provides richer annotations compared to most existing datasets in
AQA [29–31, 33, 49, 50, 53].
Furthermore, we build a plug-and-play module, GOAT (short for GrOup-aware ATtention), to bridge the gap be-tween single-person short-sequence scenarios and multi-person long-sequence scenarios. Specifically, in the spa-tial domain, by building a graph for actors, we model the relations among them. The nodes of this graph represent actors’ features extracted from a CNN, and the edges repre-sent the relations among actors. Then we use a graph con-volution network (GCN) [19] to model the group features from the graph. The optimized features of the graph then serve as “queries” and “keys” for GOAT. In the temporal do-main, after feature extraction by the video feature backbone, the clip-wise features serve as “values” for GOAT. Instead of fusing the features simply using the average pooling as most previous works [44,50,52], GOAT learns the relations among clips and models the temporal features of the long-term videos based on the spatial information in every clip.
The contributions of this paper can be summarized as: (1) We construct the first multi-person long-form video dataset, LOGO, for action quality assessment. To the best of our knowledge, LOGO stands out for its longer aver-age duration, the larger number of people, and richer an-notations when compared to most existing datasets. Ex-perimental results also reveal the challenges our proposed dataset brings. (2) We propose a plug-and-play group-aware module, GOAT, which models the group information and the temporal contextual relations for input videos, bridg-ing the gap between single-person short-sequence scenarios and multi-person long-sequence scenarios. (3) Experimen-tal results demonstrate that our group-aware approach ob-tains substantial improvements compared to existing meth-ods in AQA and achieves the state-of-the-art. 2.