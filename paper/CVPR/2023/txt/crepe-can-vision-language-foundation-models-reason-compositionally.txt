Abstract
A fundamental characteristic common to both human vi-sion and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that—across 7 architec-tures trained with 4 algorithms on massive datasets—they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark,
CREPE, which measures two important aspects of compo-sitionality identified by cognitive science literature: system-aticity and productivity. To measure systematicity, CREPE consists of a test dataset containing over 370K image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate 325K, 316K, and 309K hard negative captions for a subset of the pairs. To test productivity, CREPE con-tains 17K image-text pairs with nine different complexities plus 278K hard negative captions with atomic, swapping and negation foils. The datasets are generated by repurpos-ing the Visual Genome scene graphs and region descriptions and applying handcrafted templates and GPT-3. For sys-tematicity, we find that model performance decreases con-sistently when novel compositions dominate the retrieval set, with Recall@1 dropping by up to 9%. For productivity, models’ retrieval success decays as complexity increases, frequently nearing random chance at high complexity. These results hold regardless of model and training dataset size. 1.

Introduction
Compositionality, the understanding that “the meaning of the whole is a function of the meanings of its parts” [11], is held to be a key characteristic of human intelligence.
In language, the whole is a sentence, made up of words.
In vision, the whole is a scene, made up of parts like objects, their attributes, and their relationships [31, 35].
*Equal contribution
Figure 1. We introduce CREPE, a benchmark to evaluate whether vision-language foundation models demonstrate two fundamental aspects of compositionality: systematicity and productivity. To eval-uate systematicity, CREPE utilizes Visual Genome and introduces three new test datasets for the three popular pretraining datasets:
CC-12M, YFCC-15M, and LAION-400M. These enable evaluating models’ abilities to systematically generalize their understanding to seen compounds, unseen compounds, and even unseen atoms.
To evaluate productivity, CREPE introduces examples of nine com-plexities, with three types of hard negatives for each.
Through compositional reasoning, humans can understand new scenes and generate complex sentences by combining known parts [6, 27, 30]. Despite compositionality’s impor-tance, there are no large-scale benchmarks directly evaluat-ing whether vision-language models can reason composition-ally. These models are pretrained using large-scale image-caption datasets [62, 64, 74], and are already widely applied for tasks that benefit from compositional reasoning, includ-ing retrieval, text-to-image generation, and open-vocabulary classification [10, 57, 60]. Especially as such models become ubiquitous “foundations” for other models [5], it is critical to understand their compositional abilities.
Previous work has evaluated these models using image-text retrieval [32,56,82]. However, the retrieval datasets used either do not provide controlled sets of negatives [45, 74] or study narrow negatives which vary along a single axis
Figure 2. An overview of the systematicity retrieval set generation process. First, a model’s image-caption training set is parsed to identify what atoms and compounds the model has seen. Then, an evaluation set is divided into three compositional splits according to whether the model has seen all the compounds (Seen Compounds), only all the atoms of the caption (Unseen Compounds), or neither (Unseen Atoms).
Finally, hard negative captions HN-ATOM and HN-COMP are generated for the hard negatives retrieval set DHN test . (e.g. permuted word orders or single word substitutions as negative captions) [21, 51, 65, 75]. Further, these analyses have also not studied how retrieval performance varies when generalizing to unseen compositional combinations, or to combinations of increased complexity.
We introduce CREPE (Compositional REPresentation
Evaluation): a new large-scale benchmark to evaluate two aspects of compositionality: systematicity and productivity (Figure 1). Systematicity measures how well a model is able to represent seen versus unseen atoms and their composi-tions. Productivity studies how well a model can compre-hend an unbounded set of increasingly complex expressions.
CREPE uses Visual Genome’s scene graph representation as the compositionality language [35] and constructs evaluation datasets using its annotations. To test systematicity, we parse the captions in three popular training datasets, CC-12M [8],
YFCC-15M [74], and LAION-400M [62], to identify atoms (objects, relations, or attributes) and compounds (combina-tions of atoms) present in each dataset. For each training set, we curate corresponding test sets containing 385K, 385K and 373K image-text pairs respectively, with splits checking generalization to seen compounds, unseen compounds, and unseen atoms. To test productivity, CREPE contains 17K image-text pairs split across nine levels of complexity, as defined by the number of atoms present in the text. Exam-ples across all datasets are paired with various hard negative types to ensure the legitimacy of our conclusions.
Our experiments—across 7 architectures trained with 4 training algorithms on massive datasets—find that vision-language models struggle at compositionality, with both systematicity and productivity. We present six key findings: first, our systematicity experiments find that models’ perfor-mance consistently drops between seen and unseen composi-tions; second, we observe larger drops for models trained on
LAION-400M (up to a 9% decrease in Recall@1); third, our productivity experiments indicate that retrieval performance degrades with increased caption complexity; fourth, we find no clear trend relating training dataset size to models’ com-positional reasoning; fifth, model size also has no impact; finally, models’ zero-shot ImageNet classification accuracy correlates only with their absolute retrieval performance on the systematicity dataset but not systematic generalization to unseen compounds or to productivity. 1 2.