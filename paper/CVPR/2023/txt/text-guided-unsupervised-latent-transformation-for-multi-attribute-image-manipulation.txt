Abstract
Great progress has been made in StyleGAN-based im-age editing. To associate with preset attributes, most ex-isting approaches focus on supervised learning for seman-tically meaningful latent space traversal directions, and each manipulation step is typically determined for an in-dividual attribute. To address this limitation, we propose a
Text-guided Unsupervised StyleGAN Latent Transformation (TUSLT) model, which adaptively infers a single transfor-mation step in the latent space of StyleGAN to simultane-ously manipulate multiple attributes on a given input image.
Speciﬁcally, we adopt a two-stage architecture for a latent mapping network to break down the transformation process into two manageable steps. Our network ﬁrst learns a di-verse set of semantic directions tailored to an input image, and later nonlinearly fuses the ones associated with the tar-get attributes to infer a residual vector. The resulting tightly interlinked two-stage architecture delivers the ﬂexibility to handle diverse attribute combinations. By leveraging the cross-modal text-image representation of CLIP, we can per-form pseudo annotations based on the semantic similarity between preset attribute text descriptions and training im-ages, and further jointly train an auxiliary attribute clas-siﬁer with the latent mapping network to provide semantic guidance. We perform extensive experiments to demonstrate that the adopted strategies contribute to the superior perfor-mance of TUSLT. 1.

Introduction
Visual attributes represent semantically meaningful fea-tures inherent in images, and attribute manipulation has ex-∗Corresponding author.
Figure 1. Visually comparing TUSLT with StyleFlow (supervised) and StyleCLIP (text-driven) in precisely manipulating multiple at-tributes and preserving irrelevant attributes. perienced great improvements, due to the advent of Gen-erative Adversarial Network [13] (GAN)-based generative models, e.g. StyleGAN [21, 22] and StarGAN [7, 8]. Re-cent works [15, 37, 43] have discovered that the latent space of StyleGAN possesses semantic disentanglement proper-ties, enabling a variety of image editing operations via la-tent transformations.
StyleGAN-based methods for image attribute manipula-tion typically involve a large number of manual annotations or well-trained attribute classiﬁers. Furthermore, the dis-covered semantic latent directions are associated with indi-vidual attributes. The editing on a target attribute is car-ried out by moving the latent code of an input image along one of the directions. For K target attributes, these model-s require K transformation steps to handle the translation.
As a result, they are not scalable to the increasing number of target attributes in multi-attribute transformation tasks.
As shown in Figure 1, we test a state-of-the-art supervised
Figure 2. Overview of the proposed model, TUSLT, consisting of two learnable components: an auxiliary attribute classiﬁer A trained on the CLIP-based labeled data, and a latent mapping network {Γ, Φ}. Γ infers latent directions {γ(1) w } for preset attributes, and
Φ transforms the target-related directions as indicated by mask M∆ into a residual vector γ∆ w , to which the initial latent code is added.
Precise multi-attribute transfer is allowed by such a single transformation step, and the generator G synthesizes a new image reﬂecting the target attributes under the guidance of A and CLIP encoders. w , . . . , γ(K) model, StyleFlow [2], and ﬁnd that multiple transformation steps lead to undesired deviation from the input image on irrelevant attributes. Compared to the state-of-the-art text-driven model, StyleCLIP [33], we can also achieve a better manipulation result by seeking a single latent transforma-tion step for the task.
More speciﬁcally, we propose a Text-guided Unsuper-vised StyleGAN Latent Transformation (TUSLT) model that supports simultaneous manipulation on multiple at-tributes. As shown in Figure 2, the key is to jointly learn a mapping network to infer the latent transformation and an auxiliary attribute classiﬁer to assess manipulation quality.
We employ the Contrastive Language-Image Pre-training (CLIP) model [34] to generate pseudo-labeled data by mea-suring the semantic similarities between attribute text de-scriptions and training images. Compared to CLIP, the jointly trained classiﬁer extracts domain-speciﬁc informa-tion to better characterize the differences among attributes.
This beneﬁts the mapping network to seek more suitable transformations, such that the synthesized images reﬂec-t target attributes. Further, we adopt a two-stage architec-ture for the mapping network: the earlier stage employs a prediction subnetwork to infer a set of semantic direction-s, and the latter stage operates on the resulting directions and nonlinearly fuses the target-related ones. The inter-mediate semantic directions are associated with preset at-tributes and tailored for the input image. This design allows us to deal with a wide range of attribute combinations in a single transformation step. We perform extensive experi-ments and provide both qualitative and quantitative results in diverse multi-attribute transformation tasks, showing the superiority of our model over the competing methods.
In summary, the main contributions of this work are giv-en as follows: (a) The existing image editing methods focus on discovering semantic latent directions associated with individual visual attributes, and a sequential manipulation process is thus needed for multi-attribute manipulation. In contrast, the proposed model infers a single step of latent s-pace walk to simultaneously manipulate multiple attributes. (b) Beneﬁting from the cross-modal text-image representa-tion of CLIP, we jointly train a latent mapping network with an auxiliary attribute classiﬁer, which leads to more precise attribute rendering without requiring additional manual an-notations. (c) Due to the two-stage nature, our latent map-ping network breaks down the challenging multi-attribute manipulation task into sub-tasks: inferring diverse seman-tic directions and integrating the target-related ones into a single transformation vector. This design gives our model interpretability and ﬂexibility in dealing with a variety of attribute combinations. 2.