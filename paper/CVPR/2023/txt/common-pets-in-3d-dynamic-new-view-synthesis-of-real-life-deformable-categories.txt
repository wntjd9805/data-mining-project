Abstract 1.

Introduction
Obtaining photorealistic reconstructions of objects from sparse views is inherently ambiguous and can only be achieved by learning suitable reconstruction priors. Ear-lier works on sparse rigid object reconstruction successfully learned such priors from large datasets such as CO3D. In this paper, we extend this approach to dynamic objects. We use cats and dogs as a representative example and introduce
Common Pets in 3D (CoP3D), a collection of crowd-sourced videos of approximately 4,200 distinct pets. CoP3D is one of the first large-scale datasets for benchmarking non-rigid 3D reconstruction “in the wild”. We also propose Tracker-NeRF, a method for learning 4D reconstruction from our dataset. At test time, given a small number of video frames of an unseen object, Tracker-NeRF predicts the trajecto-ries of its 3D points and generates new views, interpolating viewpoint and time. Results on CoP3D reveal significantly better non-rigid new-view synthesis performance than ex-isting baselines. The data will be available on the project webpage: https://cop3d.github.io/.
Advances in photorealistic reconstruction and new-view synthesis facilitate experiencing real-life objects and scenes in virtual and mixed reality. However, compared to standard 2D photography, capturing 3D content remains significantly more difficult. Methods based on neural radiance fields [22, 24, 27], signed distance functions [57], and other implicit representations [43] use deep neural networks to represent the geometry of a single scene [27]. They usually require hundreds of input views for high-quality reconstruction, and are often limited to rigid objects and static scenes. Instead, we would like users be able to capture 3D and 4D content as easily as taking an image or video with their smartphone, a setting that we call casual capture.
Casual capture requires reconstructing objects from only a few images, which is inherently ambiguous. This is partic-ularly true for dynamic content which requires to reconstruct not only shape and appearance, but also deformation. The ambiguity induced by deformations can be controlled via regularization [33, 47], but this can be too weak or too re-strictive to work well in all cases. Alternatively, one can
adopt parametric models of motion such as linear blend skin-ning [13,23,36,44,51,52], but these are difficult to generalise beyond a few object categories such as humans. Learning-based methods [2,9,11,29,38,58] address the reconstruction ambiguity by learning 3D priors for specific types of objects using large datasets such as CO3D [39]. With these priors, they can achieve plausible 3D reconstructions from a small number of images, but so far only for rigid objects.
In this paper, we wish to further extend data-driven ap-proaches to dynamic objects which change their shape during capture and thus require a 4D reconstruction. Our hypothe-sis is that dynamic objects can be reconstructed even from a monocular video, provided that one leverages priors learnt from a large collection of relevant videos.
In order to test this hypothesis, we first contribute a new crowd-sourced dataset, Common Pets in 3D (CoP3D).
CoP3D contains 4,200 videos of different cats and dogs. It contains more than 600,000 video frames with 3D camera tracking obtained using Structure-from-Motion (SfM) and foreground object masks for each frame. The videos are captured in the wild by non-experts, using smartphone cam-eras, and are thus representative of the casual capture setting.
Like CO3D, CoP3D can be used for single-sequence and category-wise reconstruction.
Our second contribution is Tracker-NeRF (TrackeRF), a new method for few-view reconstruction of dynamic ob-jects. The method is inspired by approaches such as Warped
Ray Embedding [10], PixelNeRF [58], and NeRFormer [38] in that it learns to synthesise new views by triangulating features extracted form the provided input views. Our key innovation is modelling the deformations of objects by infer-ring the 3D trajectory of each queried 3D point. Empirically, we show that TrackeRF outperforms previous approaches for new-view synthesis of dynamic objects. 2.