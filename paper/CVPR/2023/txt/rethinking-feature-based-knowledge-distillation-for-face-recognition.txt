Abstract
With the continual expansion of face datasets, feature-based distillation prevails for large-scale face recognition.
In this work, we attempt to remove identity supervision in student training, to spare the GPU memory from saving massive class centers. However, this naive removal leads to inferior distillation result. We carefully inspect the perfor-mance degradation from the perspective of intrinsic dimen-sion, and argue that the gap in intrinsic dimension, namely the intrinsic gap, is intimately connected to the infamous capacity gap problem. By constraining the teacher’s search space with reverse distillation, we narrow the intrinsic gap and unleash the potential of feature-only distillation. Re-markably, the proposed reverse distillation creates univer-sally student-friendly teacher that demonstrates outstand-ing student improvement. We further enhance its effective-ness by designing a student proxy to better bridge the intrin-sic gap. As a result, the proposed method surpasses state-of-the-art distillation techniques with identity supervision on various face recognition benchmarks, and the improve-ments are consistent across different teacher-student pairs. 1.

Introduction
Despite the unceasing emergence of larger and more powerful models for face recognition (FR), industrial de-ployment continues to demand for accurate and light-weight solutions. Among other compression techniques like pruning [27] and quantization [21], knowledge distillation (KD) has been proven to be effective in producing high-performing compact model from well-trained teacher. Un-like classic KD [17] and its variants [14, 24, 43, 44] who distill on logits, most of the existing works on FR distill on features [11, 13] or feature-relations [8, 20, 35]. One key
*Equal contribution. † Corresponding author.
IResNet18 (IR18) is distilled by four different teach-Figure 1. ers. Feature-only distillation (FO) shows performance degrada-tion comparing to feature-based distillation with ID supervision (FI). The proposed method (ReFO) significantly uplifts the perfor-mance of FO distillation. For both FI and FO, the student perfor-mance drops with larger teachers of lower intrinsic dimension. In line plot: student performance (%) on MR-all benchmark [9]. In bar plot: teacher’s intrinsic dimension (In.D). reason is that the massive and still growing number of iden-tities (IDs) in FR datasets, such as the 2 million IDs in Web-Face42M [45], make it too expensive to save extra teacher’s class centers for logits distillation.
The ground truth supervision from ID labels, which we call ID supervision, is still retained when training student models for better distillation results. Nonetheless, it is not only non-trivial to find the right balancing weight [15, 33], the obtained class centers are also not needed during infer-ence in an open-set FR problem. This motivates the com-plete removal of class centers in the student training for a number of benefits: 1) speed, the student distillation breaks free from the need of keeping any class center, providing further training speed-up with even lower GPU memory oc-cupancy; 2) access to unlabeled dataset, removing the de-pendency on ID labels conveniently opens the door to the vast quantity of unlabeled or uncleaned face images like
WebFace260M [45]; and 3) better focus on feature space, which is what really matters in an open-set problem. Hence, in this work, we are motivated to investigate feature distilla-tion for face recognition without ID supervision, which we call feature-only (FO) distillation.
The capacity gap problem is widely observed in various
KD applications [7, 19, 30, 37], where the student finds it increasingly difficult to learn from more powerful teacher due to larger mismatch in network capacity. In FO distilla-tion, the naive removal of ID supervision degrades student performance with more severe capacity gap problem. As shown in Fig. 1, comparing to the conventional feature dis-tillation with ID supervision (FI distillation), the IResNet18 (IR18) students trained by four other teachers all experience drops in performance when ID supervision is removed.
Pertinent works commonly agree that differing model sizes cause the capacity gap issue [7, 20, 30, 40]. Some remedies were proposed to mitigate the problem such as early stopping [7] and training teacher assistants as inter-mediate agents [30]. Liu et al. [26] further proved the im-portance of teacher-student structural compatibility. For a given teacher, their best student from Neural Architecture
Search outperformed other candidates of similar model size in the search space. However, recent works like [3, 32] showed that teachers of the same structure, same parameter size and comparable accuracy can also have differing dis-tillation results on the same student. Hence, there must be other factors contributing to the capacity gap problem other than model size and model structure.
In this work, we argue that the teacher-student gap in in-trinsic dimension, namely the intrinsic gap, plays a part.
The intrinsic dimension [2, 16, 36] of a feature space is the minimum number of variables needed to unambigu-ously describe all points in the feature space. Specifically for a model, lower intrinsic dimension is often associated with better generalization power and better performance for both general classification [2] and face recognition [16]. In
Fig. 1, as the teacher gets stronger with lower intrinsic di-mension, we observe a drop in student performance with wider intrinsic gap for both FI distillation and FO distilla-tion. If narrower intrinsic gap is related to better distillation result, can the capacity gap problem be mitigated by closing the intrinsic gap? This sparkles the idea that whether it is possible to narrow the intrinsic gap by raising teacher’s in-trinsic dimension for easier student-learning, neither chang-ing its model size nor model structure.
Firstly, we revisit FO distillation and point out the intrin-sic gap as another factor that could cause ineffective dis-tillation. Then a reverse distillation strategy is proposed to solve the problem by injecting knowledge about higher intrinsic dimensional feature space into the teacher train-ing. With reverse-distilled teachers, students trained with just FO distillation loss like mean-square-error (MSE) show performance on par or even better than competitors trained by sophisticatedly designed distillation loss with ID super-vision [20, 35]. The proposed method is thus fast and ver-satile, it can be online or offline and easily portable to unla-beled datasets. On top of that, we further improve the dis-tillation results by allowing the teacher to learn from more light-weight student proxies. This better closes the intrin-sic gap and we are able to obtain state-of-the-art (SOTA) student models on popular face recognition benchmarks.
To summarize, the contribution of this work includes:
• We reconsider the capacity gap issue in FO distillation and provide an alternative view from the perspective of the intrinsic dimension. The gap in the intrinsic di-mension between the teacher and the student is found to be related to the distillation performance.
• We propose a novel training scheme that narrows the teacher-student intrinsic gap via reverse distillation in the teacher training. Furthermore, we enhance its ef-fectiveness by designing light-weight student proxies as the reverse distillation targets. Students trained by the new teachers show consistent performance im-provement on FO distillation.
• Our method pushes the limit of FO distillation with easier-to-learn teacher. With only feature distillation loss, resulting students are shown to be superior than students trained by other SOTA distillation techniques with ID supervision. 2.