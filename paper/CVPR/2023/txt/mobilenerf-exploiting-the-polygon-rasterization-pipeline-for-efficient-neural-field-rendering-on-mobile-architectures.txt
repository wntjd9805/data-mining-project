Abstract
Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumet-ric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graph-ics hardware. This paper introduces a new NeRF repre-sentation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines.
The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Tradi-tional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables
NeRFs to be rendered with the traditional polygon rasteri-zation pipeline, which provides massive pixel-level paral-lelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones.
Project page: https://mobile-nerf.github.io 1.

Introduction
Neural Radiance Fields (NeRF) [33] have become a pop-ular representation for novel view synthesis of 3D scenes.
They represent a scene using a multilayer perceptron (MLP) that evaluates a 5D implicit function estimating the density and radiance emanating from any position in any direction, which can be used in a volumetric rendering framework to produce novel images. NeRF representations optimized to minimize multi-view color consistency losses for a set of posed photographs have demonstrated remarkable ability to reproduce fine image details for novel views.
One of the main impediments to wide-spread adoption of NeRF is that it requires specialized rendering algorithms that are poor match for commonly available hardware. Tra-ditional NeRF implementations use a volumetric rendering 3Work done while at Google.
Figure 1. Teaser – We present a NeRF that can run on a variety of common devices at interactive frame rates. algorithm that evaluates a large MLP at hundreds of sample positions along the ray for each pixel in order to estimate and integrate density and radiance. This rendering process is far too slow for interactive visualization.
Recent work has addressed this issue by “baking” NeRFs into a sparse 3D voxel grid [21, 51]. For example, Hed-man et al. introduced Sparse Neural Radiance Grids (SNeRG) [21], where each active voxel contains an opac-ity, diffuse color, and learned feature vector. Rendering an image from SNeRG is split into two phases: the first uses ray marching to accumulate the precomputed diffuse col-ors and feature vectors along each ray, and the second uses a light-weight MLP operating on the accumulated feature vector to produce a view-dependent residual that is added to the accumulated diffuse color. This precomputation and deferred rendering approach increase the rendering speed of
NeRF by three orders of magnitude. However, it still relies upon ray marching through a sparse voxel grid to produce the features for each pixel, and thus it cannot fully utilize the parallelism available in commodity graphics processing units (GPUs).
In addition, SNeRG requires a significant amount of GPU memory to store the volumetric textures, which prohibits it from running on common mobile devices.
In this paper, we introduce MobileNeRF, a NeRF that
can run on a variety of common mobile devices at inter-active frame rates. The NeRF is represented by a set of textured polygons, where the polygons roughly follow the surface of the scene, and the texture atlas stores opacity and feature vectors. To render an image, we utilize the classic polygon rasterization pipeline with Z-buffering to produce a feature vector for each pixel and pass it to a lightweight
MLP running in a GLSL fragment shader to produce the output color. This rendering pipeline does not sample rays or sort polygons in depth order, and thus can model only bi-nary opacities. However, it takes full advantage of the paral-lelism provided by z-buffers and fragment shaders in mod-ern graphics hardware, and thus is 10× faster than SNeRG with the same output quality on standard test scenes. More-over, it requires only a standard polygon rendering pipeline, which is implemented and accelerated on virtually every computing platform, and thus it runs on mobile phones and other devices previously unable to support NeRF visualiza-tion at interactive rates.
Contributions. In summary, MobileNeRF:
• Is 10× faster than the state-of-the-art (SNeRG), with the same output quality;
• Consumes less memory by storing surface textures in-stead of volumetric textures, enabling our method to run on integrated GPUs with limited memory and power;
• Runs on a web browser and is compatible with all devices we have tested, as our viewer is an HTML webpage;
• Allows real-time manipulation of the reconstructed ob-jects/scenes, as they are simple triangle meshes. 2.