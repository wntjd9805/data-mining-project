Abstract
This paper addresses the challenge of capturing perfor-mance for the clothed humans from sparse-view or monoc-ular videos. Previous methods capture the performance of full humans with a personalized template or recover the garments from a single frame with static human poses.
However, it is inconvenient to extract cloth semantics and capture clothing motion with one-piece template, while sin-gle frame-based methods may suffer from instable tracking across videos. To address these problems, we propose a novel method for human performance capture by tracking clothing and human body motion separately with a double-layer neural radiance ﬁelds (NeRFs). Speciﬁcally, we pro-pose a double-layer NeRFs for the body and garments, and track the densely deforming template of the clothing and body by jointly optimizing the deformation ﬁelds and the canonical double-layer NeRFs. In the optimization, we in-troduce a physics-aware cloth simulation network which can help generate physically plausible cloth dynamics and body-cloth interactions. Compared with existing method-s, our method is fully differentiable and can capture both the body and clothing motion robustly from dynamic videos.
Also, our method represents the clothing with an indepen-dent NeRFs, allowing us to model implicit ﬁelds of general clothes feasibly. The experimental evaluations validate its effectiveness on real multi-view or monocular videos. 1.

Introduction
Performance capture for clothed humans is one of the essential problems in the metaverse, and it not only cap-tures the inner human body motion but also recovers the outer clothing motion which has many promising applica-tions such as virtual try-on, video editing, and telepresence.
From sparse-view or monocular videos of a moving human
∗Corresponding author: wangkangkan@njust.edu.cn in general clothes, its goal is to recover the dynamic 3D shape sequence of the human body and clothing simultane-ously that are consistent with the observed frames in both human shape and motion. This is a very challenging prob-lem since the dynamic human could be with arbitrary mo-tions and with complex non-rigid cloth deformations, and the clothing in motion is difﬁcult to maintain physically plausible interactions with the body.
Previous systems [4, 21, 38, 39] reconstruct 3D clothed humans by using depth sensors or ﬁtting a personalized template [9, 10, 37] to the image observations (e.g., body joints and silhouettes). Only recovering one-piece geome-try which uniﬁes the human body and clothes, these systems fail to track the motion of the clothing and achieve clothing editing on 3D humans, which are the prerequisites in many
VR/AR applications like virtual dressing. On the contrary, cloth can be extracted and tracked from depth scans [26,40] accurately by ﬁtting pre-built cloth templates to the scans which have limited applications when 3D data are unavail-able. Existing garment estimation methods [3, 12, 43, 44] from color images require the person facing the camera and in static poses. When the human is moving and the cloth-ing is deforming, these methods may recover the 3D gar-ments unreliably. Recent methods [16, 31] track body and clothing motion simultaneously from videos, but they need to re-build cloth template for a new performer and the run-ning efﬁciency is very low due to online cloth simulation or computationally-exhaustive optimization, which prohibits them from being widely deployed for daily applications.
Recent works [20, 34, 41] adopt dynamic human NeRF-s to capture human motion and obtain impressive tracking results. By capturing the temporally-varying appearance in the videos, dynamic NeRFs [34] can provide dense photo-metric constraints to track the deforming geometry of the performer. However, they represent the human with a sin-gle NeRFs without modeling cloth, and the clothing motion cannot be extracted. In this paper, we aim to track the cloth-ing and body motion simultaneously with dynamic NeRFs.
However, this problem is rather challenging due to two ma-jor questions we need to solve: how to represent dynamic clothing and human body with NeRFs, and how to capture clothing and human body motion with plausible body-cloth interactions based on the implicit representation.
In this paper, we propose a novel method for clothed human performance capture with a double-layer NeRFs.
Speciﬁcally, a double-layer NeRFs is modeled for both the body and clothing in the template space, and transformed to the observation space with the corresponding deforma-tion ﬁelds, and the rendered images are then synthesized by composing the two dynamic NeRFs. We ﬁrst estimate the template shape in canonical frame and learn the geom-etry network supervised by the template geometry. In the rendering, we compose the double-layer NeRFs with the guidance of deformed body and clothing meshes. Then, by minimizing the difference between the rendered color and observed color, the deformation ﬁelds and the canonical N-eRFs are optimized jointly. The deformation ﬁeld is rep-resented as the inverse deformation of the template mesh, thus the densely deforming geometry of the template can be recovered simultaneously. In addition, we adopt a physics-aware network learnt from simulation data between various cloth types and humans to constrain the dynamic clothing and preserve physically plausible body-cloth interactions, resulting in realistic cloth geometry tracking. Compared to previous methods, our method is fully differentiable and can recover the realistic motion of both the clothing and body from dynamic videos with arbitrary human poses and com-plex cloth deformations. The experimental qualitative and quantitative results on datasets of DynaCap [8] and Deep-Cap [10] prove that the proposed approach can robustly and accurately capture the motion for clothed humans. In sum-mary, the primary contributions of our work include:
• We propose a double-layer NeRFs for dynamic hu-mans in general clothing, allowing us to model implicit humans with a variety of clothes (e.g., loose dresses).
• To the best of our knowledge, we propose the ﬁrst framework to capture clothing motion separately from the human body using the double-layer NeRFs, which provides dense appearance constraints on the geometry tracking and improves the robustness and accuracy.
• A differentiable physics-aware network is learnt for d-ifferent common garments and used to preserve physi-cally plausible cloth deformations in motion capture. 2.