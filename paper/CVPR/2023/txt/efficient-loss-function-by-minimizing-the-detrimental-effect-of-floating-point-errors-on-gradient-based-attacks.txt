Abstract
Attackers can deceive neural networks by adding hu-man imperceptive perturbations to their input data; this reveals the vulnerability and weak robustness of current deep-learning networks. Many attack techniques have been proposed to evaluate the model’s robustness. Gradient-based attacks suffer from severely overestimating the ro-bustness. This paper identifies that the relative error in cal-culated gradients caused by floating-point errors, including floating-point underflow and rounding errors, is a funda-mental reason why gradient-based attacks fail to accurately assess the model’s robustness. Although it is hard to elim-inate the relative error in the gradients, we can control its effect on the gradient-based attacks. Correspondingly, we propose an efficient loss function by minimizing the detri-mental impact of the floating-point errors on the attacks.
Experimental results show that it is more efficient and reli-able than other loss functions when examined across a wide range of defence mechanisms. 1.

Introduction
AI with deep neural networks (DNNs) as the core [24] has achieved great success in different research directions and has been widely used in many safety-critical systems, such as aviation [1, 8, 35], medical diagnosis [27, 49], self-driving [4,26,47] , etc. However, a severe problem with cur-rent DNNs is that they are vulnerable to adversarial attacks.
Adding human imperceptive perturbations to input data can mislead the model to output incorrect results [16, 45]. Such vulnerability poses a significant security risk to all DNN-based systems. Therefore, increasing the models’ robust-ness and providing efficient and reliable evaluation methods are becoming increasingly urgent and vital.
*Corresponding author. 1https://robustbench.github.io/
Figure 1. Comparison of the effectiveness of non-targeted and multi-targeted PGD 100 iterations attacks using various loss func-tions on the CIFAR-10 dataset. The best results were obtained from RobustBench1 using an ensemble attack with a minimum of 4900 iterations. The defence model is from [59].
Numerous defence strategies [6, 10, 13, 16, 29, 31, 55] to improve the model’s robustness and many attacks [2, 29, 33, 41, 50, 52, 54, 59] to evaluate these strategies have been proposed. Currently, the most efficient attack method is known as white-box attack, where the attacker has com-plete knowledge of the target defending strategy, includ-ing its model architecture, parameters, detail of the train-ing algorithm, dataset, etc. A typical example is Projec-tive gradient descent (PGD) [29] for evaluating the model’s robustness. However, studies have shown that PGD with cross-entropy (CE) loss fails to provide accurate evaluation results [6, 41] and significantly overestimates the model’s robustness(Figure 1). To enhance the evaluation accuracy, recent powerful evaluation methods were proposed to en-semble diverse attack algorithms [10, 30]. However, their performance is at the cost of high computational overhead. the failure of gradient-based attacks to gradient masking [15]. The core of gradient masking is that the calculated gradients are not
The research community attributes
necessarily efficient in guiding the generation of adversarial examples. The causes of inefficient gradients may be related to floating-point errors or other factors. This paper reveals that the relative error in the gradients caused by floating-point errors is one of the fundamental reasons for the failure of gradient-based attacks, and the relative error is directly affected by the value of the difference of the first and second largest in logits(DFSL). Although it is hard to eliminate the relative error in the gradients caused by floating-point errors completely, we can control its effect on the gradient-based attacks. Thus, we propose a new efficient loss function by minimizing the impact of floating-point errors (MIFPE) on gradient-based attacks. To summarize, our main contribu-tions are as follows:
• We provide a comprehensive demonstration of how the presence of floating-point errors leads to the failure of gradient-based attacks due to the relative error in cal-culated gradients.
• We show that the floating-point rounding errors may lead to attack’s failure due to growing relative errors as samples reach the classification boundary.
• We demonstrate that when the model’s robustness is overestimated primarily due to floating-point errors, the loss function discarding partial elements of logits causes performance degradation in the gradient-based attack.
• We show that minimizing the impact of floating-point errors can improve the efficiency and accuracy of gradient-based attacks.
• Empirical results demonstrate that MIFPE tends to outperform the competing loss functions in terms of attack performance and computational efficiency. 2. Preliminaries &