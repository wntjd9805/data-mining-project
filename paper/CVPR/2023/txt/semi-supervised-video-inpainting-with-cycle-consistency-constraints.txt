Abstract
Deep learning-based video inpainting has yielded promising results and gained increasing attention from re-searchers. Generally, these methods assume that the cor-rupted region masks of each frame are known and easily ob-tained. However, the annotation of these masks are labor-intensive and expensive, which limits the practical applica-tion of current methods. Therefore, we expect to relax this assumption by defining a new semi-supervised inpainting setting, making the networks have the ability of completing the corrupted regions of the whole video using the anno-tated mask of only one frame. Specifically, in this work, we propose an end-to-end trainable framework consisting of completion network and mask prediction network, which are designed to generate corrupted contents of the current frame using the known mask and decide the regions to be filled of the next frame, respectively. Besides, we introduce a cycle consistency loss to regularize the training parameters of these two networks. In this way, the completion network and the mask prediction network can constrain each other, and hence the overall performance of the trained model can be maximized. Furthermore, due to the natural existence of prior knowledge (e.g., corrupted contents and clear bor-ders), current video inpainting datasets are not suitable in the context of semi-supervised video inpainting. Thus, we create a new dataset by simulating the corrupted video of real-world scenarios. Extensive experimental results are reported to demonstrate the superiority of our model in the video inpainting task. Remarkably, although our model is trained in a semi-supervised manner, it can achieve compa-rable performance as fully-supervised methods. 1.

Introduction
Video inpainting aims to fill corrupted regions of the video with plausible contents, which is a promising yet
*Corresponding author
Figure 1. Existing methods perform video inpainting in a fully-supervised setting. The typical issue of such methods is the need to elaborately annotate the corrupted regions mi of each frame xi in the video (Fig.(a)), which is labor-intensive and expensive in real-world applications. In this paper, we formulate a new task: semi-supervised video inpainting, which only annotates the cor-rupted regions of one frame to complete the whole video (Fig.(b)).
Fig.(c) shows an example of semi-supervised video inpainting: the top row shows sample frames with the mask, where pink denotes the manually annotated mask of corrupted regions, and blue de-notes the mask of corrupted regions predicted by the model. The completed results (cid:98)yi are shown in the bottom row. challenging task in computer vision. It can benefit a wide range of practical applications, such as scratch restora-tion [1], undesired object removal [30], and autonomous driving [18], etc.
In essence, unlike image inpainting which usually learns on the spatial dimension, video in-painting task pays more attention to exploiting the tem-poral information. Naively using image inpainting meth-ods [13, 34, 42, 45] on individual video frame to fill cor-rupted regions will lose inter-frame motion continuity, re-sulting in flicker artifacts in the inpainted video.
Similar to the texture synthesis, traditional video inpaint-ing methods [5, 8, 23] attempt to generate missing contents to fill missing regions by searching and copying similar patches from known regions. Despite the authentic com-pletion results have been achieved, these approaches still meet grand challenges, such as the lack of high-level under-standing of the video [32] and high computational complex-ity [1, 16]. In fact, recently, deep learning-based video in-painting methods [1,4,11,16,17,26,32,41,50] have been no-ticed by many researchers and made promising progress in terms of the quality and speed. These methods usually ex-tract relevant features from the neighboring frames by con-volutional neural networks and perform different types of context aggregation to generate missing contents. Neverthe-less, these methods are only suitable in the scenarios where the corrupted region mask of each frame in the video is available (Fig.1(a)). Hence, when we resort to these meth-ods in real-world applications, annotating the corrupted re-gions of each frame in the video is required, which is labor-intensive and expensive, especially for long videos. For-mally, a naive combination of video object segmentation and video inpainting methods can be used to reduce anno-tation costs in a two-stage manner. In this way, we can first generate the masks for all video frames using a video ob-ject segmentation method, and then complete the missing regions with the fully-supervised video inpainting methods.
However, such a two-stage approach has some intuitive dis-advantages. One the one hand, since each module is learned individually and sometimes not well combined to maximize performance, the overall performance is sub-optimal. On the other hand, existing video object segmentation methods are unsatisfactory for segmentation results of the scratch re-gions similar to Fig.1(c), resulting in the inpainted video with critical errors. Therefore, to realize the goal of re-ducing annotation cost for video inpainting from scratch, in this paper, we introduce a new semi-supervised video in-painting setting that we can complete the corrupted regions of the whole video using the annotated mask of only one frame (Fig.1(b)) and train the network from end-to-end. In this way, compared with the conventional fully-supervised setting, the annotation cost can be greatly reduced, making video inpainting more convenient in practical application.
However, fulfilling the task of semi-supervised video in-painting is non-trivial and has some issues to be addressed.
On the one hand, except for one annotated known frame, there are no masks for other frames to indicate the corrupted regions in the proposed semi-supervised setting. To solve this problem, we decompose the semi-supervised video in-painting task into dual tasks: frame completion and mask prediction. Specifically, we first perform frame completion on the frame with corresponding given mask to obtain the completed frame using the designed frame completion net-work. Then, we feed the completed frame and the subse-quent frame into the proposed mask prediction network to generate the corrupted region mask of the subsequent frame.
Last, by iterating frame by frame, we can complete cor-rupted regions of each frame in the video. On the other hand, to precisely capture the accurate correspondence be-tween the completion network and mask prediction net-work, a cycle consistency loss is introduced to regularize the trained parameters. In addition, existing video inpainting datasets usually take black or noise pixels as the corrupted contents of the video frame. In fact, such a setting will in-troduce some specific prior knowledge (e.g., corrupted con-tents and clear borders) into the dataset, making it easy for the mask prediction network to distinguish corrupted re-gions from natural images.
In this way, existing datasets cannot realistically simulate complex real-world scenarios.
Hence, in our work, to effectively avoid the introduction of the above prior knowledge into the dataset, we use natural images as corrupted contents of the video frame and ap-ply iterative Gaussian smoothing [35] to extend the edges of corrupted regions. Experimental results demonstrate that our proposed method can achieve comparable inpainting re-sults as fully-supervised methods. An example result of our method is shown in Fig.1(c).
Our contributions are summarized as follows:
• We formulate a novel semi-supervised video inpaint-ing task that aims to complete the corrupted regions of the whole video with the given mask of one frame. To the best of our knowledge, this is the first end-to-end semi-supervised work in the video inpainting field.
• A flexible and efficient framework consisting of com-pletion network and mask prediction network is de-signed to solve the semi-supervised video inpainting task, where cycle consistency loss is introduced to reg-ularize the trained parameters.
• A novel synthetic dataset1 is tailored for the semi-supervised video inpainting task. which consists of 4,453 video clips. This dataset will be published to facilitate subsequent research and benefit other re-searchers. 2.