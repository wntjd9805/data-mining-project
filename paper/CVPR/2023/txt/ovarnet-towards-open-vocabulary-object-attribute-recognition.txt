Abstract
In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary sce-nario. To achieve this goal, we make the following con-tributions: (i) we start with a na¨ıve two-stage approach for open-vocabulary object detection and attribute classi-fication, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for se-mantic category and attributes; (ii) we combine all avail-able datasets and train with a federated strategy to fine-tune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs un-der weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowl-edge distillation, that performs class-agnostic object pro-posals and classification on semantic categories and at-tributes with classifiers generated from a text encoder; Fi-nally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recog-⋆ Equal contribution. † Corresponding author. nition of semantic category and attributes is complemen-tary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outper-form existing approaches that treat the two tasks indepen-dently, demonstrating strong generalization ability to novel attributes and categories. 1.

Introduction
Understanding the visual scene in terms of objects has been the main driving force for development in computer vision [46], for example, in object detection, the goal is to localise objects in an image and assign one of the pre-defined semantic labels to them, such as a ‘car’, ‘person’ or ‘bus’, despite tremendous success has been made by the community, such task definition has largely over-simplified our understanding of the visual world, as a visual object can often be characterised from many aspects other than seman-tic category, for example, a bus can be ‘yellow’ or ‘black’, a shirt can be ‘striped’ or ‘unpatterned’, learning attributes can thus complement category-level recognition, acquiring more comprehensive visual perception.
In the literature, numerous work has shown that under-standing the objects’ attributes can greatly facilitate object recognition and detection, even with few or no examples
of visual objects [5, 16, 21, 36, 45], for example, Farhadi et al. proposed to shift the goal of object recognition from
‘naming’ to ‘description’, which allows naming familiar objects with attributes, but also to say something about unfamiliar objects (“hairy and four-legged”, not just “un-known”) [5]; Lampert et al. considered the open-set ob-ject recognition, that aims to recognise objects by human-specified high-level description, e.g., arbitrary semantic at-tributes, like shape, color, or even geographic information, instead of training images [16]. However, the problem con-sidered in these seminal work tends to be a simplification from today’s standard, for example, attribute classification are often trained and evaluated on object-centric images under the close-set scenario, i.e., assuming the bounding boxes/segmentation masks are given [11, 25, 31], or some-times even the object category are known as a prior [22,25].
In this paper, we consider the task of simultaneously detecting objects and classifying the attributes in an open-vocabulary scenario, i.e., the model is only trained on a set of base object categories and attributes, while it is required to generalise towards ones that are unseen at training time, as shown in Fig. 1. Generally speaking, we observe three major challenges: First, in the existing foundation mod-els, e.g., CLIP [27] and ALIGN [13], the representation learned from image-caption pairs tends to bias towards ob-ject category, rather than attributes, which makes it suffer from feature misalignment when used directly for attribute recognition. We experimentally validate this conjecture by showing a significant performance drop in attribute recog-nition, compared to category classification; Second, there is no ideal training dataset with three types of annotations, ob-ject bounding boxes, semantic categories, and attributes; as far as we know, only the COCO Attributes dataset [24] pro-vides such a degree of annotations, but with a relatively lim-ited vocabulary size (196 attributes, 29 categories); Third, training all three tasks under a unified framework is chal-lenging and yet remains unexplored, i.e., simultaneously lo-calising (‘where’), classifying objects’ semantic categories and attributes (‘what’) under the open-vocabulary scenario.
To address the aforementioned issues, we start with a na¨ıve architecture, termed as CLIP-Attr, which first pro-poses object candidates with an offline RPN [30], and then performs open-vocabulary object attribute recognition by comparing the similarity between the attribute word em-bedding and the visual embedding of the proposal. To bet-ter align the feature between attribute words and propos-als, we introduce learnable prompt vectors with parent at-tributes on the textual encoder side and finetune the orig-inal CLIP model on a large corpus of the freely available image-caption datasets. To further improve the model effi-ciency, we present OvarNet, a unified framework that per-forms detection and attributes recognition at once, which is trained by leveraging datasets from both object detec-tion and attribute prediction, as well as absorbing knowl-edge from CLIP-Attr to improve the performance and ro-bustness of unseen attributes. As a result, our proposed
OvarNet, being the first scalable pipeline, can simultane-ously localize objects and infer their categories with visual attributes in an open-vocabulary scenario. Experimental re-sults demonstrate that despite only employing weakly su-pervised image-caption pairs for distillation, OvarNet out-performs previous the state-of-the-art on VAW [25], MS-COCO [18], LSA [26] and OVAD [3] datasets, exhibiting strong generalization ability on novel attributes and cate-gories. 2.