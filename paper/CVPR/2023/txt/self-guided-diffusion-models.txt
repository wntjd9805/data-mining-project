Abstract
Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for train-ing and is thus dependent on their availability and correct-ness. In this paper, we eliminate the need for such annota-tion by instead exploiting the flexibility of self-supervision signals to design a framework for self-guided diffusion mod-els. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annota-tion. Self-guided diffusion is simple, flexible and expected to profit from deployment at scale. 1.

Introduction
Diffusion models have recently enabled tremendous ad-vancements in many computer vision fields related to image synthesis, but counterintuitively this often comes with the cost of requiring large annotated datasets [49, 55]. For ex-ample, the image fidelity of samples from diffusion models can be spectacularly enhanced by conditioning on class la-bels [17]. Classifier guidance goes a step further and offers control over the alignment with the class label, by using the classifier gradient to guide the image generation [17].
Classifier-free guidance [28] replaces the dedicated classifier with a diffusion model trained by randomly dropping the condition during training. This has proven a fruitful line
â€ Equal contribution
Source code at: https://taohu.me/sgdm/.
Figure 1. Self-guided diffusion framework. Our method can leverage large and diverse image datasets without any annotations for training guided diffusion models. Starting from a dataset with-out ground-truth annotations, we apply a self-supervised feature extractor to create self-annotations. Using these, we train diffusion models with either self-labeled, self-boxed, or self-segmented guid-ance that enable controlled generation and improved image fidelity. of research for several other condition modalities, such as text [50, 55], image layout [53], visual neighbors [3], and image features [20]. However, all these conditioning and guidance methods require ground-truth annotations. In many domains, this is an unrealistic and too costly assumption. For example, medical images require domain experts to annotate very high-resolution data, which is infeasible to do exhaus-tively [45]. In this paper, we propose to remove the necessity of ground-truth annotation for guided diffusion models.
We are inspired by progress in self-supervised learning
[11,13], which encodes images into semantically meaningful latent vectors without using any label information. It usually does so by solving a pretext task [2, 21, 24, 69] on image-level to remove the necessity of labels. This annotation-free paradigm enables the representation learning to upscale to
larger and more diverse image datasets [19]. The holistic image-level self-supervision has recently been extended to more expressive dense representations, including bounding boxes (e.g., [41, 57]) and pixel-precise segmentation masks (e.g., [22, 72]). Some self-supervised learning methods even outperform supervised alternatives [11, 24]. We hypothesize that for diffusion models, self-supervision may also provide a flexible and competitive, possibly even stronger guidance signal than ground-truth labeled guidance.
In this paper, we propose self-guided diffusion models, a framework for image generation using guided diffusion without the need for any annotated image-label pairs, the detailed structure is shown in Figure 1. The framework en-compasses a feature extraction function and a self-annotation function, that are compatible with recent self-supervised learning advances. Furthermore, we leverage the flexibil-ity of self-supervised learning to generalize the guidance signal from the holistic image level to (unsupervised) lo-cal bounding boxes and segmentation masks for more fine-grained guidance. We demonstrate the potential of our pro-posal on single-label and multi-label image datasets, where self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annotation. 2.