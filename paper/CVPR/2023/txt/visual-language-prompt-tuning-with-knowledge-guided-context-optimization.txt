Abstract
Prompt tuning is an effective way to adapt the pretrained visual-language model (VLM) to the downstream task us-ing task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain speciﬁc textual knowledge. However, the speciﬁc textual knowledge is worse generalization to the unseen classes because it forgets the essential general tex-tual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided
Context Optimization (KgCoOp) to enhance the generaliza-tion ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that the forgetting about essen-tial knowledge can be alleviated by reducing the discrep-ancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy be-tween the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the Kg-CoOp upon the contrastive loss can make a discriminative prompt for both seen and unseen tasks. Extensive eval-uation of several benchmarks demonstrates that the pro-posed Knowledge-guided Context Optimization is an efﬁ-cient method for prompt tuning, i.e., achieves better perfor-mance with less training time. code. 1.

Introduction
With the help of the large scale of the image-text as-sociation pairs, the trained visual-language model (VLM) contains essential general knowledge, which has a bet-ter generalization ability for the other tasks. Recently, many visual-language models have been proposed, such as Contrastive Language-Image Pretraining (CLIP) [29],
Flamingo [1], ALIGN [18], etc. Although VLM is an ef-fective model for extracting the visual and text description, training VLM needs a large scale of high-quality datasets.
However, collecting a large amount of data for training a task-related model in real visual-language tasks is dif-ﬁcult. To address the above problem, the prompt tun-Table 1. Compared to existing methods, the proposed KgCoOp is an efﬁcient method, obtaining a higher performance with less training time.
Methods
Prompts
Accuracy
New
H
Base
Training-time
CLIP hand-crafted 69.34 74.22 71.70
-CoOp
ProGrad
CoCoOp
KgCoOp textual textual textual+visual textual 82.63 82.48 80.47 80.73 67.99 70.75 71.69 73.6 74.60 76.16 75.83 77.0 6ms/image 22ms/image 160ms/image 6ms/image ing [4] [10] [19] [22] [28] [30] [33] [38] has been proposed to adapt the pretrained VLM to downstream tasks, achiev-ing a fantastic performance on various few-shot or zero-shot visual recognization tasks.
The prompt tuning1 usually applies task-related textual tokens to embed task-speciﬁc textual knowledge for pre-diction. The hand-crafted template “a photo of a [Class]” in CLIP [29] is used to model the textual-based class em-bedding for zero-shot prediction. By deﬁning the knowl-edge captured by the ﬁxed (hand-crafted) prompts as the general textual knowledge2, it has a high generalization capability on unseen tasks. However, the general textual knowledge is less able to describe the downstream tasks due to not consider the speciﬁc knowledge of each task.
To obtain discriminative task-speciﬁc knowledge, Context
Optimization(CoOp) [41], Conditional Context Optimiza-tion(CoCoOp) [40], and ProGrad [42] replace the hand-crafted prompts with a set of learnable prompts inferred by the labeled few-shot samples. Formally, the discrimina-tive knowledge generated by the learned prompts is deﬁned as the speciﬁc textual knowledge. However, CoOp-based methods have a worse generalization to the unseen classes with the same task, e.g., obtaining a worse performance than
CLIP for the unseen classes (New), shown in Table 1.
As the speciﬁc textual knowledge is inferred from the 1In this work, we only consider the textual prompt tuning and do not involve the visual prompt tuning. 2Inspired from [17], ‘knowledge’in this work denotes the information contained in the trained model.
beddings is used to optimize the learnable prompts.
We conduct comprehensive experiments under base-to-new generalization setting, few-shot classiﬁcation, and do-main generalization over 11 image classiﬁcation datasets and four types of ImageNets. The evaluation shows that the proposed KgCoOp is an efﬁcient method: using the less training time obtains a higher performance, shown in Ta-ble 1. In summary, the proposed KgCoOp obtains: 1) higher performance: KgCoOp obtains a higher ﬁnal performance than existing methods. Especially, KgCoOp obtains a clear improvement on the New class upon the CoOp, CoCoOp, and ProGrad, demonstrating the rationality and necessity of considering the general textual knowledge. 2) less training time: the training time of KgCoOp is the same as CoOp, which is faster than CoCoOp and ProGrad. 2.