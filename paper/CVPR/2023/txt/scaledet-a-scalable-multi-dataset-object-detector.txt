Abstract
Multi-dataset training provides a viable solution for ex-ploiting heterogeneous large-scale datasets without extra annotation cost. In this work, we propose a scalable multi-dataset detector (ScaleDet) that can scale up its generaliza-tion across datasets when increasing the number of train-ing datasets. Unlike existing multi-dataset learners that mostly rely on manual relabelling efforts or sophisticated optimizations to unify labels across datasets, we introduce a simple yet scalable formulation to derive a unified se-mantic label space for multi-dataset training. ScaleDet is trained by visual-textual alignment to learn the label as-signment with label semantic similarities across datasets.
Once trained, ScaleDet can generalize well on any given upstream and downstream datasets with seen and unseen classes. We conduct extensive experiments using LVIS,
COCO, Objects365, OpenImages as upstream datasets, and 13 datasets from Object Detection in the Wild (ODinW) as downstream datasets. Our results show that ScaleDet achieves compelling strong model performance with an mAP of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on OpenImages, and 71.8 on ODinW, surpassing state-of-the-art detectors with the same backbone. 1.

Introduction
Major advances in computer vision have been driven by large-scale datasets, such as ImageNet [9] and Open-Images [22] for image classification, or Kinetics [6] and
ActivityNet [2] for video recognition. Large-scale datasets are crucial for training recognition models that generalize well. However, the collection of massive annotated datasets is costly and time-consuming. This is especially promi-nent in detection and segmentation tasks that require de-tailed annotations at the bounding box or pixel level. To exploit more training data without extra annotation cost, re-cent works unify multiple datasets to learn from more vi-sual categories and more diverse visual domains for detec-tion [38, 40, 47, 51] and segmentation [25, 37].
To train an object detector across multiple datasets, we need to tackle several challenges. First, multi-dataset train-Figure 1. Our scalable multi-dataset detector (ScaleDet) learns across datasets in a unified semantic label space by visual-textual alignment with label semantic similarities. At test time, ScaleDet can generalize on any given upstream and downstream dataset. ing requires unifying the heterogeneous label spaces across datasets, as label definitions are dataset-specific. The labels from two datasets may indicate the same or similar objects.
For example, “footwear” and “sneakers” are two different labels in OpenImages [24] and Objects365 [34], but refer to the same type of objects (see Figure 1). Second, the train-ing setups may be inconsistent among datasets, as different data sampling strategies and learning schedules are often re-quired for datasets of different sizes. Third, a multi-dataset model should perform better than single-dataset models on individual datasets. This is challenging due to the heteroge-neous label spaces, the domain discrepancy across datasets, and the risk of overfitting to the larger datasets.
To resolve the above challenges, existing work resorts to manually relabelings class labels [25], or training multiple dataset-specific classifiers with constraints to relate labels across datasets [51]. However, these methods lack scalabil-ity. The manual relabeling effort and the model complex-ity of training multiple classifiers grow rapidly as the num-ber of datasets increases. We overcome this limitation with
ScaleDet: a scalable multi-dataset detector (Figure 1). We propose two innovations: a scalable formulation to unify multiple label spaces, and a novel loss formulation to learn hard label and soft label assignments across datasets. While hard label assignment serves to disambiguate class labels in probability space, soft label assignment works as a reg-ularizer to relate class labels in semantic similarity space.
Unlike existing multi-dataset methods [25,37,38,40,47,51] that mostly generalize on seen datasets or seen classes, our method exploits vision-language learning to attain good generalization on both upstream and downstream datasets, where the downstream datasets can contain unseen classes and new domains. Our contributions are:
• We propose a novel scalable multi-dataset training recipe for object detection. Our method utilizes text embeddings to unify and relate labels with semantic similarities across datasets, and trains a single classifier via visual-textual alignment to learn hard label and soft label assignments.
• We conduct extensive experiments to demonstrate the compelling scalability and generalizability of ScaleDet in multi-dataset training. We show that ScaleDet can boost its performance as we increase the number of training datasets: LVIS [14], COCO [27], Objects365 [34] and
OpenImages [24] (Sec 4.2). Furthermore, we show that
ScaleDet achieves state-of-the-art performance on multi-ple benchmarks when compared to recent advanced de-tectors, e.g., Detic [49], UniDet [51] (Sec 4.3, Sec 4.4).
• We evaluate the transferablity of ScaleDet on the chal-lenging “Object Detection in the Wild” benchmark (which contains 13 datasets) [26] to demontrate its competitive generalizability on downstream datasets (Sec 4.5). 2.