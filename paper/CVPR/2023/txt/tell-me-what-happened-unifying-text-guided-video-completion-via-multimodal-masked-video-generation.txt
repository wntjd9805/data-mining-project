Abstract
Generating a video given the first several static frames is challenging as it anticipates reasonable future frames with temporal coherence. Besides video prediction, the ability to rewind from the last frame or infilling between the head and tail is also crucial, but they have rarely been explored for video completion. Since there could be different out-comes from the hints of just a few frames, a system that can follow natural language to perform video completion may
Inspired by this, we significantly improve controllability. introduce a novel task, text-guided video completion (TVC), which requests the model to generate a video from partial frames guided by an instruction. We then propose Multi-modal Masked Video Generation (MMVG) to address this
TVC task. During training, MMVG discretizes the video frames into visual tokens and masks most of them to perform video completion from any time point. At inference time, a single MMVG model can address all 3 cases of TVC, in-cluding video prediction, rewind, and infilling, by applying corresponding masking conditions. We evaluate MMVG in various video scenarios, including egocentric, animation, and gaming. Extensive experimental results indicate that
MMVG is effective in generating high-quality visual ap-pearances with text guidance for TVC. 1.

Introduction
Generative video modeling [15, 70, 84] has made great progress, which first succeeds in unconditional video gen-eration [40,64]. More recently, video prediction [28,36,47] has been trying the controllable setting, which anticipates the future by completing a video from the past frames or a static starting image [37, 97]. However, video prediction may produce various outcomes, which makes it difficult to meet human expectations. For the example in Fig. 1(a), the game agent can keep jumping to the right or move back and turn left. The limited guidance from only the first frame is
Figure 1. The introduced text-guided video completion (TVC) task. (a) Video prediction may have different outcomes without text guidance. (b) TVC performs video completion from the first frame (prediction), the last frame (rewind), or both (infilling), guided by the textual description. insufficient to tell the intention. For humans, language is the most straightforward way of communication. If a system can follow an instruction to accomplish video completion, it will significantly improve its controllability and make a vast application impact. On the other hand, compared with video prediction, video rewind and infilling have been rarely stud-ied [39, 79], but they are also crucial. Breaking the limita-tion of chronological guidance should make the visual guid-ance more flexible, leading to a general video completion.
We thus introduce a novel task, text-guided video com-pletion (TVC), where the partial frames and a given instruc-tion jointly guide the video generation. As illustrated in
Fig. 1(b), we consider three scenarios of video completion: prediction from the first frame, rewind from the last frame, and infilling between the head and tail. The missing (to-be-completed) event should follow the textual instruction.
Compared to generating content from scratch [43, 87], TVC requests models to understand the given visual and textual guidance before generation, which better mimics how hu-man imagines after seeing and listening in our daily lives.
To tackle TVC, we present Multimodal Masked Video
Generation (MMVG) to perform video completion. Specif-ically, we represent the video frames as discrete visual to-kens by temporal-aware VQGAN [54, 76]. One key chal-lenge is to deal with the video frames that are not presented in chronological (e.g., the last frame for rewind). Different from autoregressive models [23, 86] that only condition on the previous frames, MMVG carries out video completion in an encoder-decoder manner. Specifically, we propose a masking strategy that masks different parts of the video and feeds them as the input to the multimodal encoder with the instruction. As shown in Fig. 2, we allow MMVG to con-sider the visual hints from different time points, and the de-coder learns to produce the full target video. By varying the masking conditions (including the cases of only the first or last frame being accessible), a single MMVG can address all TVC tasks, including video prediction, rewind, and in-filling. Moreover, learning the recovery from partial frames also empowers MMVG with a strong temporal coherence, contributing to better generative video quality.
We consider videos in diverse scenarios for the TVC eval-uation. There are Kitchen [13], Flintstones [26], and MU-GEN [29] corresponding to the egocentric, animation, and gaming scenes. The model should generate videos such as performing kitchen activities in the first-person view, mak-ing characters act the assigned behavior, or imitating an agent playing game. All should be guided with the first/last (or both) frame(s) and controlled through the given human instructions. We also compare MMVG with previous meth-ods [23, 57, 94, 96] on UCF-101 [69] and BAIR [18] for the classic video generation/prediction tasks.
Experimental results demonstrate that instruction is nec-essary to make video completion controllable, MMVG can address all three TVC tasks, and our proposed masking strat-egy enhances the temporal modeling, which further benefits general video generation/prediction. In summary, our con-tributions are three-fold:
• We introduce TVC to generate a video from partial frames and control the temporal dynamics via natural language, where our video completion includes 3 cases: prediction, rewind, and infilling.
• We propose MMVG with an effecitve masking strategy to address all TVC tasks through a single training. ation/prediction. We believe TVC can become a new topic in vision-and-language research. 2.