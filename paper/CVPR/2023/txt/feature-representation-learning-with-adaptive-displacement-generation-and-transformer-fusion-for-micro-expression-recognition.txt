Abstract
Micro-expressions are spontaneous, rapid and subtle fa-cial movements that can neither be forged nor suppressed.
They are very important nonverbal communication clues, but are transient and of low intensity thus difficult to rec-ognize. Recently deep learning based methods have been developed for micro-expression (ME) recognition using fea-ture extraction and fusion techniques, however, targeted feature learning and efficient feature fusion still lack fur-ther study according to the ME characteristics. To address these issues, we propose a novel framework Feature Repre-sentation Learning with adaptive Displacement Generation and Transformer fusion (FRL-DGT), in which a convolu-tional Displacement Generation Module (DGM) with self-supervised learning is used to extract dynamic features from onset/apex frames targeted to the subsequent ME recogni-tion task, and a well-designed Transformer Fusion mecha-nism composed of three Transformer-based fusion modules (local, global fusions based on AU regions and full-face fu-sion) is applied to extract the multi-level informative fea-tures after DGM for the final ME prediction. The extensive experiments with solid leave-one-subject-out (LOSO) eval-uation results have demonstrated the superiority of our pro-posed FRL-DGT to state-of-the-art methods. 1.

Introduction
As a subtle and short-lasting change, micro-expression (ME) is produced by unconscious contractions of facial muscles and lasts only 1/25th to 1/5th of a second, as illus-trated in Figure 1, revealing a person’s true emotions under-neath the disguise [8, 35]. The demands for ME recognition technology are becoming more and more extensive [2, 18],
*Corresponding author.
Figure 1. A video sequence depicting the order of which onset, apex and offset frames occur. Sample frames are from a “surprise” sequence in CASME II. Our goal is to design a novel feature rep-resentation learning method based on an onset-apex frame pair for facial ME recognition. (Images from CASME II ©Xiaolan Fu) including multimedia entertainment, film-making, human-computer interaction, affective computing, business nego-tiation, teaching and learning, etc. Since MEs have invol-untary muscle movements with short duration and low in-tensity in nature, the research of ME is attractive but diffi-cult [1, 22]. Therefore, it is crucial and desired to extract robust feature representations to conduct ME analyses.
A lot of feature representation methods are already avail-able including those relying heavily on hand-crafted fea-tures with expert experiences [4, 11, 25] and deep learning techniques [33, 36, 40]. However, the performance of deep learning networks is still restricted for ME classification, mainly due to the complexity of ME and insufficient train-ing data [28, 47]. Deep learning methods can automatically extract optimal features and offer an end-to-end classifica-tion, but in the existing solutions, dynamic feature extrac-tion is only taken as a data preprocessing strategy. It is not integrated with the subsequent neural network, thus failing to adapt the generated dynamic features to a specific train-ing task, leading to redundancy or missing features. Such shortcoming motivates us to design a dynamic feature ex-tractor to adapt the subsequent ME recognition task.
In this paper, we propose a novel end-to-end feature rep-resentation learning framework named FRL-DGT, which is constructed with a well-designed adaptive Displacement
Generation Module (DGM) and a subsequent Transformer
Fusion mechanism composed of the Transformer-based lo-cal, global, and full-face fusion modules, as illustrated in
Figure 2, for ME feature learning to model global informa-tion while focusing on local features. Our FRL-DGT only requires a pair of images, i.e., the onset and the apex frames, which are extracted from the frame sequence.
Unlike the previous methods which extract optical flow or dynamic imaging features, our DGM and corresponding loss functions are designed to generate the displacement be-tween expression frames, using a convolution module in-stead of the traditional techniques. The DGM is involved in training with the subsequent ME classification module, and therefore its parameters can be tuned based on the feed-back from classification loss to generate more targeted dy-namic features adaptively. We shall emphasize that the la-beled training data for ME classification is very limited and therefore the supervised data for our DGM is insufficient.
To handle this case, we resort to a self-supervised learning strategy and sample sufficient additional random pairs of image sequence as the extra training data for the DGM, so that it is able to fully extract the necessary dynamic features adaptively for the subsequent ME recognition task.
Regarding fusing the dynamic features extracted from
DGM, we first adopt the AU (Action Unit) region partition-ing method from FACS (Facial Action Coding System) [49] to get 9 AUs, and then crop the frames and their displace-ments into blocks based on the 9 AUs and the full-face re-gion as input to the Transformer Fusion. We argue that the lower layers in the Transformer Fusion should encode and fuse different AU region features in a more targeted way, while the higher layers can classify MEs based on the infor-mation of all AUs. We propose a novel fusion layer with at-tentions as a linear fusion before attention mechanism [45], aiming at a more efficient and accurate integration of the embedding vectors. The fusion layers are interleaved with
Transformer’s basic blocks to form a new multi-level fusion module for classification to ensure it to better learn global information and long-term dependencies of ME.
To summarize, our main contributions are as follows:
• We propose a novel end-to-end network FRL-DGT which fully explores AU regions from onset-apex pair and the displacement between them to extract compre-hensive features via Transformer Fusion mechanism with the Transformer-based local, global, and full-face feature fusions for ME recognition.
• Our DGM is well-trained with self-supervised learn-ing, and makes full use of the subsequent classifica-tion supervision information in the training phase, so that the trained DGM model is able to generate more targeted ME dynamic features adaptively.
• We present a novel fusion layer to exploit the linear fusion before attention mechanism in Transformer for fusing the embedding features at both local and global levels with simplified computation.
• We demonstrate the effectiveness of each module with ablation study and the outperformance of the proposed
FRL-DGT to the SOTA methods with extensive exper-iments on three popular ME benchmark datasets. 2.