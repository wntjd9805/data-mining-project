Abstract
We introduce a novel framework for training deep stereo networks effortlessly and without any ground-truth. By leveraging state-of-the-art neural rendering solutions, we generate stereo training data from image sequences col-lected with a single handheld camera. On top of them, a
NeRF-supervised training procedure is carried out, from which we exploit rendered stereo triplets to compensate for occlusions and depth maps as proxy labels. This results in stereo networks capable of predicting sharp and detailed disparity maps. Experimental results show that models trained under this regime yield a 30-40% improvement over existing self-supervised methods on the challenging Middle-bury dataset, ﬁlling the gap to supervised models and, most times, outperforming them at zero-shot generalization. 1.

Introduction
Depth from stereo is one of the longest-standing research
ﬁelds in computer vision [38].
It involves ﬁnding pixels correspondences across two rectiﬁed images to obtain the disparity – i.e., their difference in terms of horizontal coor-dinates – and then use it to triangulate depth. After years of studies with hand-crafted algorithms [56], deep learning radically changed the way of approaching the problem [74].
End-to-end deep networks [50] rapidly became the domi-nant solution for stereo, delivering outstanding results on benchmarks [42, 55, 58] given sufﬁcient training data.
This latter requirement is the key factor for their suc-cess, but it is also one of the greatest limitations. Annotated data is hard to source when dealing with depth estimation since additional sensors are required (e.g., LiDARs), and thus represents a thick entry barrier to the ﬁeld. Over the years, two main trends have allowed to soften this problem: self-supervised learning paradigms [3, 23, 76] and the use of synthetic data [30, 41, 75]. Despite these advances, both approaches still have weaknesses to address.
Self-supervised learning: despite the possibility to train on any unlabeled stereo pair collected by any user – and potentially opening to data democratization – the use of self-supervised losses is ineffective at dealing with ill-posed stereo settings (e.g. occlusions, non-Lambertian surfaces, etc.). Albeit recent approaches soften the occlusions prob-lem [3], predictions are far from being as sharp, detailed and accurate as those obtained through supervised training.
Moreover, the self-supervised stereo literature [13, 29, 76] often focuses on well-deﬁned domains (i.e., KITTI) and rarely exposes domain generalization capabilities [3].
Synthetic data: although training on densely annotated synthetic images can guide the networks towards sharp, de-tailed and accurate predictions, the domain-shift that occurs when testing on real data dampens the full potential of the trained model. A large body of recent literature addressing zero-shot generalization [2, 16, 30, 35, 36, 87] proves how relevant the problem is. However, obtaining stereo pairs as realistic as possible requires signiﬁcant effort, despite syn-thetic depth labels being easily sourced through a graphics rendering pipeline. Indeed, modelling high-quality assets is crucial for mitigating the domain shift and requires ex-cellent graphics skills. While artists make various assets available, these are seldom open source and necessitate ad-ditional human labor to be organized into plausible scenes.
In short, in a world where data is the new gold, obtain-ing ﬂexible and scalable training samples to unleash the full potential of deep stereo networks still remains an open prob-In this paper, we propose a novel paradigm to ad-lem. dress this challenge. Given the recent advances in neural rendering [44, 45], we exploit them as data factories: we collect sparse sets of images in-the-wild with a standard, single handheld camera. After that, we train a Neural Ra-diance Field (NeRF) model for each sequence and use it to render arbitrary, novel views of the same scene. Speciﬁ-cally, we synthesize stereo pairs from arbitrary viewpoints by rendering a reference view corresponding to the real ac-quired image, and a target one on the right of it, displaced by means of a virtual arbitrary baseline. This allows us to generate countless samples to train any stereo network in a self-supervised manner by leveraging popular photomet-ric losses [23]. However, this na¨ıve approach would inherit the limitations of self-supervised methods [13,29,76] at oc-clusions, which can be effectively addressed by rendering a third view for each pair, placed on the left of the source view specularly to the other target image. This allows to compensate for the missing supervision at occluded regions.
Moreover, proxy-supervision in the form of rendered depth by NeRF completes our NeRF-Supervised training regime.
With it, we can train deep stereo networks by conducting a low-effort collection campaign, and yet obtain state-of-the-art results without requiring any ground-truth label – or not even a real stereo camera! – as shown on top of Fig. 1.
We believe that our approach is a signiﬁcant step towards democratizing training data. In fact, we will demonstrate how the efforts of just the four authors were enough to col-lect sufﬁcient data (roughly 270 scenes) to allow our NeRF-Supervised stereo networks to outperform models trained on synthetic datasets, such as [2, 16, 30, 35, 36, 87], as well as existing self-supervised methods [3, 79] in terms of zero-shot generalization, as depicted at the bottom of Fig. 1.
We summarize our main contributions as:
• A novel paradigm for collecting and generating stereo training data using neural rendering and a collection of user-collected image sequences.
• A NeRF-Supervised training protocol that combines rendered image triplets and depth maps to address oc-clusions and enhance ﬁne details.
• State-of-the art, zero-shot generalization results on challenging stereo datasets [55], without exploiting any ground-truth or real stereo pair. 2.