Abstract
This paper explores the tasks of leveraging auxiliary modalities which are only available at training to enhance multimodal representation learning through cross-modal
Knowledge Distillation (KD). The widely adopted mutual information maximization-based objective leads to a short-cut solution of the weak teacher, i.e., achieving the max-imum mutual information by simply making the teacher model as weak as the student model. To prevent such a weak solution, we introduce an additional objective term, i.e., the mutual information between the teacher and the auxiliary modality model. Besides, to narrow down the in-formation gap between the student and teacher, we further propose to minimize the conditional entropy of the teacher given the student. Novel training schemes based on con-trastive learning and adversarial learning are designed to optimize the mutual information and the conditional en-tropy, respectively. Experimental results on three popular multimodal benchmark datasets have shown that the pro-posed method outperforms a range of state-of-the-art ap-proaches for video recognition, video retrieval and emotion classification. 1.

Introduction
Multimodal learning has shown much promise in a wide spectrum of applications, such as video understand-ing [20, 39, 44], sentiment analysis [1, 19, 47] and medi-cal image segmentation [7, 49]. It is generally recognized that including more modalities helps the prediction accu-racy. For many real-world applications, while one has to make predictions based on limited modalities due to effi-ciency or cost concerns, it is usually possible to collect ad-ditional modalities at training. To enhance the representa-tion learned, several studies have thus considered to lever-age such modalities as a form of auxiliary information at training [11, 23, 29, 50]. Several attempts have explored to fill in the auxiliary modalities at test time, by employ-ing approaches such as Generative Adversarial Network
Figure 1.
Illustration of the difference between the MI-based method and the proposed AMID method. (a) MI-based methods maximize I(M ; S). (b) AMID maximizes I(M ; S) and I(M ; A), while minimizing H(M |S). (GAN) [4, 22]. However, such data generation-based ap-proaches introduce extra computation costs at inference.
As an alternative, cross-modal knowledge distillation is proposed, where a teacher model trained with both the tar-get modalities and the auxiliary modalities (denoted as full modalities hereafter), is employed to guide the training of a student model with target modalities only [10,11,16,23,45, 46]. In such a way, the student model directly learns to em-bed information about auxiliary modalities and involves no additional computational cost at inference. A typical prac-tice is to pre-train the teacher model from the full modality data, and fix it during the training. While such a setting enables the teacher to provide information-rich representa-tion [23, 46], it may not be favorable for the alignment of the teacher and student, due to the large information gap between them, especially at the early stage of training [32].
To better facilitate the knowledge transfer, online KD is proposed to jointly train the teacher and the student.
One common solution is to maximize the mutual informa-tion (MI) between the teacher and the student in a shared
space [42]. However, such an objective only focuses on maximizing the shared knowledge, in terms of percentage, from the teacher to the student, which is necessary but not sufficient to guarantee a complete knowledge transfer. As il-lustrated in Fig. 1(a), a short-cut solution is to simply make the teacher model capture less information, i.e., shrinking the entropy of the teacher.
Under the framework online KD, this paper explores to simultaneously achieve: 1) maintaining the representation capacity of the full modality teacher as much as possible, and 2) narrowing down the information gap between the teacher and the student. As Fig. 1(b) shows, in addition to maximizing the MI between the teacher and the student, maximizing the MI between the teacher and an additional auxiliary model may contribute to the former goal, while minimizing the conditional entropy of the teacher given the student may benefit the latter goal. We thus propose a new objective function consisting of all the above three terms.
To achieve the optimization of the MI, we further derive a new form of its lower bound by taking into account the supervision of the label, and propose a contrastive learning-based approach to implement it. To minimize the condi-tional entropy, an adversarial learning-based approach is in-troduced, where additional discriminators are used to distin-guish representations between the teacher and the student.
We name the proposed method Adversarial Mutual Infor-mation Distillation (AMID) hereafter.
To validate the effectiveness of AMID, extensive exper-iments on three popular multimodal tasks including video recognition, video retrieval and emotion classification tasks are conducted. The performance of these tasks are con-ducted on UCF51 [40], ActivityNet [20] and IEMOCAP [6] benchmark datasets, respectively. The results show that
AMID outperforms a range of state-of-the-art approaches on all three tasks. To summarize, the main contribution of this paper is threefold:
• We propose a novel cross-modal KD method, Adver-sarial Mutual Information Distillation (AMID), to pre-vent the teacher from losing information by maximiz-ing the MI between it and an auxiliary modality model.
• AMID maximally transfers information from the teacher to the student by optimizing the MI between them and minimizing the conditional entropy of the teacher given the student.
• To implement the joint optimization of the three objec-tive terms, a novel training approach that leverages the combined advantage of both contrastive learning and adversarial learning is proposed. 2.