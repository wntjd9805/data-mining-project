Abstract
Masked image modeling (MIM) has attracted much research attention due to its promising potential for learning scalable visual representations.
In typical approaches, models usually focus on predicting specific contents of masked patches, and their performances are highly related to pre-defined mask strategies. Intuitively, this procedure can be considered as training a student (the model) on solving given problems (predict masked patches). However, we argue that the model should not only focus on solving given problems, but also stand in the shoes of a teacher to produce a more challenging problem by itself. To this end, we propose
Hard Patches Mining (HPM), a brand-new framework for
MIM pre-training. We observe that the reconstruction loss can naturally be the metric of the difficulty of the pre-training task. Therefore, we introduce an auxiliary loss predictor, predicting patch-wise losses first and deciding where to mask next. It adopts a relative relationship learning strategy to prevent overfitting to exact reconstruction loss values. Experiments under various settings demonstrate the effectiveness of HPM in constructing masked images.
Furthermore, we empirically find that solely introducing the loss prediction objective leads to powerful representations, verifying the efficacy of the ability to be aware of where is hard to reconstruct.1 1.

Introduction
Self-supervised learning [6, 8, 9, 18, 20], with the goal of learning scalable feature representations from large-scale datasets without any annotations, has been a research hotspot in computer vision (CV). Inspired by masked 1Code: https://github.com/Haochen-Wang409/HPM
Figure 1. Comparison between conventional MIM pre-training paradigm and our proposed HPM. (a) Conventional approaches can be interpreted as training a student, where the model is only equipped with the ability to solve a given problem under some pre-defined mask strategies. (b) Our proposed HPM pre-training paradigm makes the model to be both a teacher and a student, with the extra ability to produce a challenging pretext task. language modeling (MLM) [4,11,44,45] in natural language processing (NLP), where the model is urged to predict masked words within a sentence, masked image modeling (MIM), the counterpart in CV, has attracted numerous interests of researchers [3, 13, 19, 26, 42, 61, 66, 69].
Fig. 1a illustrates the paradigm of conventional ap-proaches for MIM pre-training [3, 19, 67]. In these typical solutions, models usually focus on predicting specific contents of masked patches.
Intuitively, this procedure can be considered as training a student (i.e., the model) on solving given problems (i.e., predict masked patches).
To alleviate the spatial redundancy in CV [19] and produce
Figure 2. Visual comparison between reconstruction loss and discriminativeness on ImageNet validation set. We load the pre-trained
ViT-B/16 [14] provided by MAE [19]. For each tuple, we show the (a) input image, (b) patch-wise reconstruction loss averaged over 10 different masks, (c) predicted loss, and (d) masked images generated by the predicted loss (i.e., patches with top 75% predicted loss are masked). Red means higher loss while blue indicates the opposite. Discriminative parts tend to be hard to reconstruct. a challenging pretext task, mask strategies become critical, which are usually generated under pre-defined manners, e.g., random masking [19], block-wise masking [3], and uniform masking [29]. However, we argue that a difficult pretext task is not all we need, and not only learning to solve the
MIM problem is important, but also learning to produce challenging tasks is crucial. In other words, as shown in
Fig. 1b, by learning to create challenging problems and solving them simultaneously, the model can stand in the shoes of both a student and a teacher, being forced to hold a more comprehensive understanding of the image contents, and thus leading itself by generating a more desirable task.
To this end, we propose Hard Patches Mining (HPM), a new training paradigm for MIM. Specifically, given an input image, instead of generating a binary mask under a manually-designed criterion, we first let the model be a teacher to produce a demanding mask, and then train the model to predict masked patches as a student just like conventional methods. Through this way, the model is urged to learn where it is worth being masked, and how to solve the problem at the same time. Then, the question becomes how to design the auxiliary task, to make the model aware of where the hard patches are.
Intuitively, we observe that the reconstruction loss can be naturally a measure of the difficulty of the MIM task, which can be verified by the first two elements of each tuple in Fig. 2, where the backbone2 pre-trained by MAE [19] with 1600 epochs is used for visualization. As expected, we find that those discriminative parts of an image (e.g., object) are usually hard to reconstruct, resulting in larger losses. Therefore, by simply urging the model to predict reconstruction loss for each patch, and then masking those patches with higher predicted losses, we can obtain a more formidable MIM task. To achieve this, we introduce an auxiliary loss predictor, predicting patch-wise losses first and deciding where to mask next based on its outputs. To prevent it from being overwhelmed by the exact values of reconstruction losses and make it concentrate on the relative relationship among patches, we design a novel relative loss based on binary cross-entropy as the objective. We further evaluate the effectiveness of the loss predictor using a ViT-B under 200 epochs pre-training in Fig. 2. As the last two elements for each tuple in Fig. 2 suggest, patches with larger predicted losses tend to be discriminative, and thus masking these patches brings a challenging situation, where objects are almost masked. Meanwhile, considering the training evolution, we come up with an easy-to-hard mask generation strategy, providing some reasonable hints at the early stages.
Empirically, we observe significant and consistent im-provements over the supervised baseline and vanilla MIM 2https://dl.fbaipublicfiles.com/mae/visualize/ mae_visualize_vit_base.pth
pre-training under various settings. Concretely, with only 800 epochs pre-training, HPM achieves 84.2% and 85.8%
Top-1 accuracy on ImageNet-1K [49] using ViT-B and ViT-L, outperforming MAE [19] pre-trained with 1600 epochs by +0.6% and +0.7%, respectively. to mask the discriminative parts is crucial, which can not only guide the model in a more challenging manner, but also bring salient prior of input images, bootstrapping the performance on a wide range of downstream tasks hence. 2.