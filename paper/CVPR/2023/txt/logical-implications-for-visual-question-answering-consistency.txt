Abstract
Despite considerable recent progress in Visual Question
Answering (VQA) models, inconsistent or contradictory an-swers continue to cast doubt on their true reasoning ca-pabilities. However, most proposed methods use indirect strategies or strong assumptions on pairs of questions and answers to enforce model consistency. Instead, we propose a novel strategy intended to improve model performance by directly reducing logical inconsistencies. To do this, we in-troduce a new consistency loss term that can be used by a wide range of the VQA models and which relies on know-ing the logical relation between pairs of questions and an-swers. While such information is typically not available in
VQA datasets, we propose to infer these logical relations using a dedicated language model and use these in our pro-posed consistency loss function. We conduct extensive ex-periments on the VQA Introspect and DME datasets and show that our method brings improvements to state-of-the-art VQA models while being robust across different archi-tectures and settings. 1.

Introduction
Visual Questioning Answering (VQA) models have drawn recent interest in the computer vision community as they allow text queries to question image content. This has given way to a number of novel applications in the space of model reasoning [8, 29, 54, 56], medical diagno-sis [21,37,51,60] and counterfactual learning [1,2,11]. With the ability to combine language and image information in a common model, it is unsurprising to see a growing use of
VQA methods.
Despite this recent progress, however, a number of im-portant challenges remain when making VQAs more pro-ficient. For one, it remains extremely challenging to build
VQA datasets that are void of bias. Yet this is critical to ensure subsequent models are not learning spurious cor-relations or shortcuts [49]. This is particularly daunting in applications where domain knowledge plays an impor-tant role (e.g., medicine [15, 27, 33]). Alternatively, ensur-Figure 1. Top: Conventional VQA models tend to produce incon-sistent answers as a consequence of not considering the relations between question and answer pairs. Bottom: Our method learns the logical relation between question and answer pairs to improve consistency. ing that responses of a VQA are coherent, or consistent, is paramount as well. That is, VQA models that answer differ-ently about similar content in a given image imply inconsis-tencies in how the model interprets the inputs. A number of recent methods have attempted to address this using logic-based approaches [19], rephrashing [44], question gener-ation [18, 40, 41] and regularizing using consistency con-straints [47]. In this work, we follow this line of research and look to yield more reliable VQA models.
We wish to ensure that VQA models are consistent in an-swering questions about images. This implies that if multi-ple questions are asked about the same image, the model’s answers should not contradict themselves. For instance, if one question about the image in Fig. 1 asks “Is there snow on the ground?”, then the answer inferred should be consis-tent with that of the question “Is it the middle of summer?”
As noted in [43], such question pairs involve reasoning and perception, and consequentially lead the authors to define inconsistency when the reasoning and perception questions are answered correctly and incorrectly, respectively. Along this line, [47] uses a similar definition of inconsistency to
regularize a VQA model meant to answer medical diagno-sis questions that are hierarchical in nature. What is crit-ical in both cases, however, is that the consistency of the
VQA model depends explicitly on its answers, as well as the question and true answer. This hinges on the assump-tion that perception questions are sufficient to answer rea-soning questions. Yet, for any question pair, this may not be the case. As such, the current definition of consistency (or inconsistency) has been highly limited and does not truly reflect how VQAs should behave.
To address the need to have self-consistent VQA mod-els, we propose a novel training strategy that relies on logi-cal relations. To do so, we re-frame question-answer (QA) pairs as propositions and consider the relational construct between pairs of propositions. This construct allows us to properly categorize pairs of propositions in terms of their logical relations. From this, we introduce a novel loss function that explicitly leverages the logical relations be-tween pairs of questions and answers in order to enforce that VQA models be self-consistent. However, datasets typ-ically do not contain relational information about QA pairs, and collecting this would be extremely laborious and dif-ficult. To overcome this, we propose to train a dedicated language model that infers logical relations between propo-sitions. Our experiments show that we can effectively in-fer logical relations from propositions and use them in our loss function to train VQA models that improve state-of-the-art methods via consistency. We demonstrate this over two different VQA datasets, against different consistency methods, and with different VQA model architectures. Our code and data are available at https://github.com/ sergiotasconmorales/imp_vqa. 2.