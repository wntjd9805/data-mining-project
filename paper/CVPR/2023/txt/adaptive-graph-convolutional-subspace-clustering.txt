Abstract
Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace cluster-ing algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction meth-ods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint si-multaneously. And the graph-convolutional operator is up-dated iteratively and adaptively in our proposed algorithm.
Hence, we call the proposed method adaptive graph con-volutional subspace clustering (AGCSC). We claim that, by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace struc-ture of the original data set more faithfully. Finally, plenty of subspace clustering experiments prove our conclusions and show that AGCSC 1 outperforms some related methods as well as some deep models. 1.

Introduction
Subspace clustering has become an attractive topic in machine learning and computer vision fields due to its suc-cess in a variety of applications, such as image processing
[14, 48], motion segmentation [6, 17], and face clustering
[27]. The goal of subspace clustering is to arrange the high-dimensional data samples into a union of linear subspaces where they are generated [1, 25, 36]. In the past decades, different types of subspace clustering algorithms have been proposed [3, 11, 23, 37, 44]. Among them, spectral-type subspace clustering methods have shown promising perfor-mance in many real-world tasks. 1We present the codes of AGCSC and the evaluated algorithms on https://github.com/weilyshmtu/AGCSC.
Suppose a data matrix X = [x1; x2; · · · ; xn] ∈ Rn×d contains n data samples drawn from k subspaces, and d is the number of features. The general formulation of a spectral-type subspace clustering algorithm could be ex-pressed as follows: minC Ω(cid:0)Φ(X) − CΦ(X)(cid:1) + λΨ(C), (1) where Φ(·) is a function that is used to find the meaningful latent features for original data samples. It could be either a linear or a non-linear feature extraction method [26, 42, 45], or even a deep neural network [27]. C ∈ Rn×n is the re-construction coefficient matrix and Ψ(C) is usually some kind of constraint of C.
In addition, Ω(·) is a function to measure the reconstruction residual, and λ is a hyper-parameter. After C is obtained, an affinity graph A is de-fined as A = (|C| + |C⊤|)/2, where C⊤ is the transpose of
C. Then a certain spectral clustering, e.g. Normalized cuts (Ncuts) [33] is used to produce the final clustering results.
Classical spectral-type subspace clustering algorithms mainly focus on designing Ψ(C) to help C to carry certain characteristics and hope C can reveal the intrinsic struc-ture of the original data set. For example, sparse subspace clustering (SSC) [6] lets Ψ(C) = ∥C∥1, which makes C
In low-rank representation (LRR) [17], a sparse matrix.
Ψ(C) is the nuclear norm of C which helps discover the global structure of a data set. Least square regression (LSR)
[20] aims to find a dense reconstruction coefficient matrix by setting Ψ(C) = ∥C∥2
F . Block diagonal representation (BDR) [19] makes Ψ(C) a k-block diagonal regularizer to pursue a k-block diagonal coefficient matrix.
Recently, deep subspace clustering algorithms (DSCs) reported much better results than the classical spectral-type subspace clustering algorithms. The main difference be-tween (DSCs) and the classical spectral-type subspace clus-tering algorithms is that DSCs use deep auto-encoders to extract latent features from original data [21, 27, 29]. But it is pointed out that the success of DSCs may be attributed to the usage of an ad-hoc post-processing strategy [8]. Though the rationality of the existing DSCs is required further dis-cussion, some deep learning techniques are still worthy of being introduced into spectral-type subspace clustering al-gorithms.
In this paper, inspired by graph convolutional networks
[4, 12, 40], we explore the problem of using graph convo-lutional techniques to design the feature extraction function
Θ(·) and the constraint function Ψ(·) simultaneously. Dif-ferent from the existing graph convolution methods which need a predefined affinity graph, we apply the required co-efficient matrix C to construct a graph convolutional oper-ator. So the graph convolutional operator will be updated adaptively and iteratively in the proposed subspace clus-tering algorithm. Consequently, on one hand, by applying the adaptive graph convolutional operator, the aggregated feature representations of original data samples in the same subspace will be gathered closer, and those in different sub-spaces will be separated further. On the other hand, in the obtained coefficient matrix, the coefficients corresponding to different data samples will also have a similar charac-teristic to the samples’ feature representations, so it could reveal the intrinsic structure of data sets more accurately.
The overview pipeline of the proposed method is illustrated in Fig. 1.
Figure 1. The overview of the proposed method. The graph con-volutional operator S will be updated iteratively based on C. And the updated S will in turn affect the computation of C and feature aggregation. 2.