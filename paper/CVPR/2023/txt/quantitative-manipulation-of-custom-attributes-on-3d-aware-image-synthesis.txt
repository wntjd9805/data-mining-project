Abstract
While 3D-based GAN techniques have been successfully applied to render photo-realistic 3D images with a vari-ety of attributes while preserving view consistency, there has been little research on how to fine-control 3D im-ages without limiting to a specific category of objects of their properties. To fill such research gap, we propose a novel image manipulation model of 3D-based GAN repre-sentations for a fine-grained control of specific custom at-tributes. By extending the latest 3D-based GAN models (e.g., EG3D), our user-friendly quantitative manipulation model enables a fine yet normalized control of 3D manip-ulation of multi-attribute quantities while achieving view consistency. We validate the effectiveness of our proposed technique both qualitatively and quantitatively through var-ious experiments. 1.

Introduction
Recent advances in neural rendering [23,34,38] are mak-ing it easy to reproduce virtual 3D objects from real-world objects. Neural rendering approach is not fully scalable in practice since it heavily relies on input images and thereby can not fully represent every possible form, style, and state variation of all real and unreal objects. 3D generative ad-versarial networks (GANs) models, on the other hand, are more generalizable and extensible since these can not only reproduce 3D objects at scale but also allow easier configu-rations based on the user’s intention [5,32,37], making them more suitable for various 3D image synthesis tasks.
Image manipulation on the latent space of 2D GAN has been extensively studied in recent years [2, 28, 29, 33, 39].
StyleGAN2 [15] has been the dominant technique used due to its flexibility to represent different styles and disentan-gled latent spaces. More recently, 3D-based GAN [24,
†Work done during at LG Electronics
Figure 1. An example of quantitative image manipulation for the face tiredness attribute. Attributes expressed as complex facial features, such as tiredness, are not easy to define explicitly. Our method assigns user-defined attributes based on a small number of image samples, allowing quantitative manipulation of 3D objects according to the user’s desired state changes. 26, 32] for multi-view image synthesis using neural ren-dering [23, 25] has gained popularity. For instance, those models [5, 10, 27] equipped with StyleGAN2 modules can generate photo-realistic 3D images with a variety of at-tributes while preserving view consistency. Nevertheless, the existing works do not well explore a fine-grained ma-nipulation of custom attributes (e.g., capturing tiredness in a face, consisting of multiple and complex facial expres-sions, as shown in Figure 1) of 3D objects that are syn-thesized using 3D-based GAN models, and therefore it de-serves more thorough research. While there have been some attempts [29,33,39] to use the latent spaces generated by GAN models to manipulate generated and real images, these approaches mainly focus on 2D objects and they are not user-friendly because users need to individually deter-mine the appropriate manipulation scale for every use ac-cording to every specific intention.
Achieving view consistency during 3D image manipu-lation in the latent spaces is crucial to achieve the quanti-tative manipulation of custom attributes. Previously, each attribute in a multi-view image for an object was incon-sistently estimated across viewpoints [8, 16, 17]. We al-leviate such multi-view inconsistency problem by treating each attribute in each multi-view image as the same. That is, our 3D manipulation model is based on a 3D-based
GAN model like EG3D [5], also equipped with two oper-ators: 1) attribute quantifier that estimates the quantity of attribute to be edited, and 2) navigator that explores across the latent space to generate a manipulated image. Since the attribute quantifier guides the navigator, the manipula-tion quality of the navigator depends on the performance of quantifier. As quantifier, an off-the-shelf pre-trained regres-sion model [1, 22] for a specific attribute is often used. It is not always easy to construct the pre-trained quantifier, es-pecially for uncommon custom attributes. Hence, to better deal with custom attributes, our navigator manipulates the image by only assigning the target quantity without explor-ing the direction and scale of changes of the latent features.
The attribute quantifier is first trained on a small number of custom image samples and then evaluates a user-defined attribute as a normalized quantity in the range [0, 1]. Us-ing the quantifier, the navigator is then trained to generate and manipulate images corresponding to target custom at-tributes. We evaluate our approach in various attributes of 3D and 2D objects, including human faces, confirming that our method is qualitatively and quantitatively effective. 2.