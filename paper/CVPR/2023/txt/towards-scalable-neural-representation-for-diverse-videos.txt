Abstract
Implicit neural representations (INR) have gained in-creasing attention in representing 3D scenes and images, and have been recently applied to encode videos (e.g., NeRV [1],
E-NeRV [2]). While achieving promising results, existing
INR-based methods are limited to encoding a handful of short videos (e.g., seven 5-second videos in the UVG dataset) with redundant visual content, leading to a model design that fits individual video frames independently and is not efficiently scalable to a large number of diverse videos. This paper focuses on developing neural representations for a more practical setup – encoding long and/or a large number of videos with diverse visual content. We first show that instead of dividing videos into small subsets and encoding them with separate models, encoding long and diverse videos jointly with a unified model achieves better compression re-sults. Based on this observation, we propose D-NeRV, a novel neural representation framework designed to encode diverse videos by (i) decoupling clip-specific visual content from motion information, (ii) introducing temporal reason-ing into the implicit neural network, and (iii) employing the task-oriented flow as intermediate output to reduce spatial redundancies. Our new model largely surpasses NeRV and traditional video compression techniques on UCF101 and
UVG datasets on the video compression task. Moreover, when used as an efficient data-loader, D-NeRV achieves 3%-10% higher accuracy than NeRV on action recognition tasks on the UCF101 dataset under the same compression ratios. 1.

Introduction
Implicit neural representations (INR) have achieved great success in parameterizing various signals, such as 3D scenes [3–5], images [6, 7], audio [6], and videos [1, 2, 8–10].
The key idea is to represent signals as a function approx-imated by a neural network, mapping a reference coordi-nate to its corresponding signal value. Recently, INR has received increasing attention in image and video compres-Figure 1. Comparison of D-NeRV and NeRV when representing diverse videos. NeRV optimizes representation to every video independently while D-NeRV encodes all videos by a shared model. sion tasks [1, 2, 8, 11–15]. Compared with learning-based video compression techniques [16–18], INR-based methods (e.g., NeRV [1]) are more favorable due to simpler training pipelines and much faster video decoding speed.
While impressive progress has been made, existing INR-based methods are limited to encoding a single short video at a time. This prohibits the potential applications in most real-world scenarios, where we need to represent and compress a large number of diverse videos. A straightforward strategy for encoding diverse videos is to divide them into multiple subsets and model each of them by a separate neural network, as shown in Figure 1 (top). However, since this strategy is unable to leverage long-term redundancies across videos, it achieves inferior results compared to fitting all diverse videos with a single shared model. As shown in Figure 2, under the same compression ratio (bits per pixel), the performance of
NeRV is consistently better when fitting a larger number of
videos. This suggests that representing multiple videos by a single large model is generally more beneficial.
However, as observed empirically, the current design of
NeRV offers diminishing returns when scaling to large and diverse videos. We argue that the current coupled design of content and motion information modeling exaggerates the difficulty of memorizing diverse videos. To address this, we propose D-NeRV, a novel implicit neural representation that is specifically designed to efficiently encode long or a large number of diverse videos1. A representative overview of differences between D-NeRV and NeRV is shown in Figure 1.
When representing diverse videos, NeRV encodes each video into a separate model or simply concatenates multiple videos into a long video and encodes it, while our D-NeRV can represent different videos in a single model by conditioning on key-frames from each video clip.
Compared to NeRV, we have the following improvements.
First, we observe that the visual content of each video of-ten represents appearance, both background and foreground, which vary significantly among different videos, while the motion information often represents the semantic structure (e.g., similar motion for the same action class) and can be shared across different videos. Therefore, we decouple each video clip into two parts: clip-specific visual content and motion information, which are modeled separately in our method. Second, motivated by the vital importance of tem-poral modeling in video-related tasks, instead of outputting each frame independently, we introduce temporal reasoning into the INR-based network by explicitly modeling global temporal dependencies across different frames. Finally, con-sidering the significant spatial redundancies in videos, rather than predicting the raw pixel values directly, we propose to predict the task-oriented flow [19–22] as an intermedi-ate output, and use it in conjunction with the key-frames to get the final refined output. It alleviates the complexity of memorizing the same pixel value across different frames.
With these improvements, our D-NeRV significantly out-performs NeRV, especially when increasing the number of videos as shown in Figure 2. To summarize, our main contri-butions are as follows:
• We propose D-NeRV, a novel implicit neural represen-tation model, to represent a large and diverse set of videos as a single neural network.
• We conduct extensive experiments on video recon-struction and video compression tasks. Our D-NeRV consistently outperforms state-of-the-art INR-based methods (E-NeRV [2]), traditional video compres-sion approaches (H.264 [23], HEVC [24]), and the recent learning-based video compression methods (DCVC [18]). 1“Long videos" and “a large number of videos" are viewed as inter-changeable concepts in this paper because a long video can be obtained by concatenating a collection of diverse videos.
Figure 2. Comparison of D-NeRV and NeRV with fixed compres-sion ratio on UCF101. The size of circles indicates model sizes.
• We further show the advantage of D-NeRV on the ac-tion recognition task by its higher accuracy and faster decoding speed, and reveal its intriguing properties on the video inpainting task. 2.