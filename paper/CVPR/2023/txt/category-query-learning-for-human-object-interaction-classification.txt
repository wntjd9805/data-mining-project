Abstract
Unlike most previous HOI methods that focus on learn-ing better human-object features, we propose a novel and complementary approach called category query learning.
Such queries are explicitly associated to interaction cat-egories, converted to image specific category representa-tion via a transformer decoder, and learnt via an auxil-iary image-level classification task. This idea is motivated by an earlier multi-label image classification method, but is for the first time applied for the challenging human-object interaction classification task. Our method is sim-It is validated on three rep-ple, general and effective. resentative HOI baselines and achieves new state-of-the-art results on two benchmarks. Code will be available at https://github.com/charles-xie/CQL. 1.

Introduction
Human-Object Interaction (HOI) detection has attracted a lot of interests in recent years [3, 8–10, 21, 33]. The task consists of two sub-tasks. The first is human and object de-tection. It is usually performed by common object detection methods. The second is interaction classification of each human-object (HO) pair. This sub-task is very challenging due to the complex appearance variations in the interaction categories. See Fig. 1 for examples. It is the focus of most previous HOI methods, as well as this work. features,
Most previous HOI methods focus on learning bet-ter human-object including modeling relation and context via GNN [7, 29, 34, 37] or attention mecha-nism [8, 34, 46], decoupling localization and classification
[22, 41, 48], leveraging vision-language knowledge [6, 22] and introducing multi-scale feature to transformer [16].
However, for interaction classification they all adopt the simple linear classifier that performs the dot product of the
*Corresponding author.
†Work done during internship at MEGVII technology.
‡Work done while worked at MEGVII.
Figure 1. Interaction classification is inherently challenging. In (a), “fly” is semantically polysemic, resulting in different objects, poses and relative positions. In (b), when “hold” is associated with different objects, the appearance, scene background, and human poses are largely different. human-object feature and a static weight vector, which rep-resents an interaction category.
In this work, we propose a new approach that enhances the above paradigm and complements most previous HOI methods. It is motivated by the recent work Query2label
[25], a transformer-based classification network.
It pro-poses a new concept we call category-specific query. Unlike the queries in other transformer methods, each query is as-sociated to a specific and fixed image category during train-ing and inference. This one-to-one binding makes the query learn to model each category more effectively. The queries are converted to image specific category representations via a transformer decoder. This method achieves excellent per-formance on multi-label image classification task.
We extend this approach for human-object interaction classification. Essentially, our approach replaces traditional category representation as a static weight vector in previ-ous HOI methods with category queries learnt as described above. The same linear classifier is adopted. Such cate-gory queries are more effective, and adaptive for different images, giving rise to better modeling of the complex vari-ations in each interaction category. This is the crucial dif-ference between this work and a simple adaption of [25] to
HOI. Notably, this work is the first to address the category weight representation problem in the HOI community.
Note that our proposed category specific query is differ-ent and not related to those queries in other transformer-based HOI methods [4, 15, 33, 49]. Specifically, category queries extract image-level features as the category repre-sentation. The queries in other methods are human-object instance-level features and category-agnostic.
Our method is simple, lightweight and general. The overview is in Fig. 2. It is complementary to any off-the-shelf HOI method that provides human-object features. The modification of both inference and training is small. The in-curred additional cost is marginal.
In experiments, our approach is validated on three representative and strong HOI baseline methods, two transformer-based methods [22, 33] and a traditional two-stage method [42]. They are all significantly improved by our approach. New state-of-the-art results are obtained on two benchmarks. Specifically, we obtain 36.03 mAP on
HICO-DET. Comprehensive ablation studies and in-depth discussion are also provided to verify the effectiveness of implementation details in our approach. It turns out that our method is more effective on challenging images that con-tain more human-object instances, a property that is rarely discussed by previous HOI methods. 2.