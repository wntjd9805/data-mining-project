Abstract
Motion capture is a long-standing research problem. Al-though it has been studied for decades, the majority of re-search focus on ground-based movements such as walking, sitting, dancing, etc. Off-grounded actions such as climb-ing are largely overlooked. As an important type of action in sports and firefighting field, the climbing movements is challenging to capture because of its complex back poses, intricate human-scene interactions, and difficult global lo-calization. The research community does not have an in-depth understanding of the climbing action due to the lack of specific datasets. To address this limitation, we collect
CIMI4D, a large rock ClImbing MotIon dataset from 12 persons climbing 13 different climbing walls. The dataset consists of around 180,000 frames of pose inertial mea-surements, LiDAR point clouds, RGB videos, high-precision static point cloud scenes, and reconstructed scene meshes.
Moreover, we frame-wise annotate touch rock holds to fa-∗ Equal contribution.
† Corresponding author. cilitate a detailed exploration of human-scene interaction.
The core of this dataset is a blending optimization pro-cess, which corrects for the pose as it drifts and is af-fected by the magnetic conditions. To evaluate the merit of CIMI4D, we perform four tasks which include human pose estimations (with/without scene constraints), pose pre-diction, and pose generation. The experimental results demonstrate that CIMI4D presents great challenges to ex-isting methods and enables extensive research opportuni-ties. We share the dataset with the research community in http://www.lidarhumanmotion.net/cimi4d/. 1.

Introduction
Capturing human motions can benefit many downstream applications, such as AR/VR, games, movies, robotics, etc. However, it is a challenging and long-standing prob-lem [1,7,35,37,75] due to the diversity of human poses and complex interactive environment. Researchers have pro-posed various approaches to estimate human poses from images [15, 16, 19, 30, 67], point clouds [33], inertial mea-surement units (IMUs) [21, 69], etc. Although the prob-lem of human pose estimation (HPE) has been studied for decades [6, 58, 63], most of the existing solutions focus on upright frontal poses on the ground (such as walking, sitting, jumping, dancing and yoga) [54]. Different from daily activities (such as walking and running) that are on the ground, climbing is an activity off the ground with back poses, which is also an important type for sports [22, 50], entertainment [31, 32, 55], and firefighting.
Climbing is an activity that involves ascending geo-graphical objects using hands and feet, such as hills, rocks, or walls. Estimating the pose of a climbing human is chal-lenging due to severe self-occlusion and the human body’s closely contact with the climbing surface. These issues are primarily caused by complex human-scene interactions.
Moreover, understanding the climbing activities requires both accurate captures of the complex climbing poses and precise localization of the climber within scenes, which is especially challenging. Many pose/mesh estimation meth-ods are data-driven methods [24,45,54,65], relying on huge climbing motion data for training networks. So a large-scale climbing dataset is necessary for the holistic understanding of human poses. Publicly available human motion datasets are mostly in upright frontal poses [2, 36, 47], which are significantly different from climbing poses. Albeit some researchers collected RGBD-based climbing videos [4] or used marker-based systems [22], their data is private and the scale of dataset is very limited.
To address the limitations of current datasets and boost related research, we collect a large-scale multimodal climb-ing dataset, CIMI4D, under complex human-scene inter-action, as depicted in Fig. 1. CIMI4D consists of around 180,000 frames of time-synchronized and well-annotated
LiDAR point clouds, RGB videos, and IMU measurements from 12 actors climbing 13 rock-climbing walls. 12 ac-tors include professional athletes, rock climbing enthusi-asts, and beginners.
In total, we collect 42 rock climb-ing motion sequences, which enable CIMI4D to cover a wide diversity of climbing behaviors. To facilitate deep un-derstanding for human-scene interactions, we also provide high-quality static point clouds using a high-precision de-vice for seven rock-climbing walls. Furthermore, we anno-tate the rock holds (holds) on climbing walls and manually label the contact information between the human body and the holds. To obtain accurate pose and global trajectory of the human body, we devise an optimization method to an-notate IMU data, as it drifts over time [10,59] and is subject to magnetic conditions in the environment.
The comprehensive annotations in CIMI4D provide the opportunity for benchmarking various 3D HPE tasks.
In this work, we focus on four tasks: human pose estima-tion with or without scene constraints, human pose predic-tion and generation. To assess the effectiveness of existing methods on these tasks, we perform both quantitative and qualitative experiments. However, most of the existing ap-proaches are unable to capture accurately the climbing ac-tion. Our experimental results demonstrate that CIMI4D presents new challenges for current computer vision algo-rithms. We hope that CIMI4D could provide more opportu-nities for a deep understanding of human-scene interactions and further benefit the digital reconstruction for both.
In summary, our contributions can be listed as below:
• We present the first 3D climbing motion dataset,
CIMI4D, for understanding the interaction between complex human actions with scenes. CIMI4D consists of RGB videos, LiDAR point clouds, IMU measure-ments, and high-precision reconstructed scenes.
• We design an annotation method which uses multiple constraints to obtain natural and smooth human poses and trajectories.
• We perform an in-depth analysis of multiple methods for four tasks. CIMI4D presents a significant challenge to existing methods. 2.