Abstract
Recent advances on text-to-image generation have wit-nessed the rise of diffusion models which act as power-ful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the depen-dency among discrete words and meanwhile pursue com-plex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learning
Transformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image cap-tioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic in-formation. The rich semantics are further regarded as se-mantic prior to trigger the learning of Diffusion Trans-former, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer struc-tures are stacked to progressively strengthen the output sentence with better visional-language alignment and lin-guistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical se-quence training strategy is designed to guide the learn-ing of SCD-Net with the knowledge of a standard autore-gressive Transformer model. Extensive experiments on
COCO dataset demonstrate the promising potential of us-ing diffusion models in the challenging image captioning task. Source code is available at https://github. com/YehLi/xmodaler/tree/master/configs/ image_caption/scdnet. 1.

Introduction
As one of the fundamental tasks in vision and language field, image captioning aims to describe the interested se-mantics in a natural sentence. This task naturally connects
*The first two authors contributed equally to this work.
Figure 1. An illustration of (a) typical autoregressive model, (b) conventional diffusion model in [6], and (c) our semantic-conditional diffusion model in SCD-Net. computer vision and natural language processing by per-ceiving visual content in a scene and interpreting them in human language, simulating a basic capability of human in-telligence. The dominate force in current state-of-the-art techniques [8,11,15,26,27,33,34,40,45,52] is to capitalize on encode-decoder structure and frame the learning process in an autoregressive manner. In particular, an image encoder is responsible for encoding visual content as high-level se-mantics, and a sentence decoder learns to decode the se-quential sentence word-by-word (Fig.1 (a)). Nevertheless, such autoregressive progress only allows for unidirectional textual message passing, and typically relies on consider-able computational resources that scale quadratically w.r.t. the sentence length.
To alleviate this limitation, recent advances [12, 13, 32, 47] start to emerge as a non-autoregressive solution that enables bidirectional textual message passing and emits all words in parallel, leading to a light-weight and scal-able paradigm. However, these non-autoregressive ap-proaches are generally inferior to the autoregressive meth-ods. The performance degradation can be mostly attributed to the word repetition or omissions problems if leaving the sequential dependency under-exploited.
In addition, the indisposed sentence quality also makes it difficult to
upgrade non-autoregressive solution with powerful self-critical sequence learning [39] widely adopted in autore-gressive methods.
More recently, a superior generative module named dif-fusion model [16] has brought forward milestone improve-ments for visual content generation. This motivates a recent pioneering practice [6] to explore diffusion model for im-age captioning, pursuing non-autoregressive sentence gen-eration in a high degree of parallelism. It manages to enable continuous diffusion process to produce the discrete word sequence by representing each word as binary bits. Differ-ent from the typical discrete sentence generation in a single shot, such continuous diffusion process (Fig.1 (b)) can be represented as a parameterized Markov chain that gradu-ally adds Gaussian noise to the sentence. Each reverse state transition is thus learnt to recover the original sentence data from noise-augmented data via denoising. Despite show-ing comparable performances against basic autoregressive models, the severe problem of word repetition or omissions is still overlooked.
In an effort to mitigate this problem, we devise a new diffusion model based non-autoregressive paradigm, called
Semantic-Conditional Diffusion Networks (SCD-Net). Our launching point is to introduce the comprehensive semantic information of the input images into the continuous diffu-sion process, which act as semantic prior to guide the learn-ing of each reverse state transition (Fig.1 (c)). SCD-Net is henceforth able to encourage a better semantic alignment between visual content and the output sentence, i.e., allevi-ating the omission of semantic words. As a by-product, our
SCD-Net enables the powerful self-critical sequence learn-ing during the continuous diffusion process. Such sentence-level optimization of diffusion process strengthens the lin-guistical coherence of the output sentence, and thus allevi-ates word repetition issue.
Technically, our SCD-Net is composed of cascaded Dif-fusion Transformer structures that progressively enhance the output sentences. Each Diffusion Transformer ex-ploits the semantic-conditional diffusion process to learn
Transformer-based encoder-decoder. In particular, for each input image, Diffusion Transformer first retrieves the se-mantically relevant words by using an off-the-shelf cross-modal retrieval model. These semantic words are addi-tionally incorporated into the continuous diffusion process, targeting for constraining reverse state transition with the semantic condition. More importantly, we upgrade the semantic-conditional diffusion process with a new guided self-critical sequence learning strategy. This strategy el-egantly transfers the knowledge of a standard autoregres-sive Transformer model to our non-autoregressive Diffusion
Transformer through sentence-level reward, leading to a sta-bilized and boosted diffusion process.
In sum, we have made the following contributions: (I)
SCD-Net designs a novel semantic-conditional diffusion process for image captioning, pursuing a scalable non-autoregressive paradigm with better visual-language align-ment. (II) SCD-Net paves a new way to couple the con-tinuous diffusion process with a new guided self-critical se-quence learning. (III) SCD-Net has been properly analyzed and verified through extensive experiments on COCO, demonstrating its encouraging potential. 2.