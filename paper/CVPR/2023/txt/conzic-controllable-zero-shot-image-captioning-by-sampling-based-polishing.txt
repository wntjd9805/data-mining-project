Abstract
Zero-shot capability has been considered as a new rev-olution of deep learning, letting machines work on tasks without curated training data. As a good start and the only existing outcome of zero-shot image captioning (IC),
ZeroCap abandons supervised training and sequentially searches every word in the caption using the knowledge of large-scale pre-trained models. Though effective, its autoregressive generation and gradient-directed searching mechanism limit the diversity of captions and inference speed, respectively. Moreover, ZeroCap does not consider the controllability issue of zero-shot IC. To move forward, we propose a framework for Controllable Zero-shot IC, named ConZIC. The core of ConZIC is a novel sampling-based non-autoregressive language model named Gibbs-BERT, which can generate and continuously polish every word. Extensive quantitative and qualitative results demon-strate the superior performance of our proposed ConZIC for both zero-shot IC and controllable zero-shot IC. Espe-cially, ConZIC achieves about 5× faster generation speed than ZeroCap, and about 1.5× higher diversity scores, with accurate generation given different control signals. Our code is available at https://github.com/joeyz0z/ConZIC. 1.

Introduction
Image captioning (IC) is a visual-language task, which targets at automatically describing an image by generating a coherent sentence. By performing supervised learning on human-annotated datasets, such as MS-COCO [43], many methods [22, 33, 49, 50] have achieved impressive evalu-ation scores on metrics like BLEU [52], METEOR [7],
CIDERr [66], and SPICE [3]. However, these methods still lag behind human capability of zero-shot IC.
Specifically, those supervised methods extremely rely on well-designed image-captions pairs. However, it is likely
*Equal contribution.
†Corresponding authors (a) Examples of zero-shot image captioning. (b) Diversity of ConZIC.
Figure 1. The highlights of our proposed method. (a) shows two examples of zero-shot image captioning on several SOTA meth-ods. Specifically, GRIT [50] and ViTCAP [22] are two supervised methods without pre-trained models. ClipCap [49] is a super-vised method using pre-trained CLIP. GRIT, ViTCAP, and CLIP-Cap are firstly trained on MSCOCO and then do testing. ZeroCap
[65] is the zero-shot method without any training. (b) shows the diversity of our proposed ConZIC, which manifests two aspects: semantic (diverse words: different colors denoting different parts-of-speech) and syntactic (diverse sentence patterns). impossible to construct a large enough dataset, including paired images and high-quality captions covering various styles/contents. As a result, it is challenging for the machine to caption images that are outliers with respect to the train-ing distribution, which is common in real applications (see
examples in Fig. 1a). On the contrary, humans can perform
IC without any specific training, i.e., realizing zero-shot IC.
Because humans can integrate what they see, i.e., the image, and what they know, i.e., the knowledge.
Recently, large-scale pretraining models have shown a strong capability of learning knowledge from super-large-scale data, showing great potential in various downstream tasks [10,27,54,57, 63]. Equipped with the visual-language knowledge learned by CLIP [57] and linguistic knowledge from GPT-2 [58], ZeroCap [65] is the first and the only zero-shot IC method, which proposes a searching-based strategy and is free of training on extra supervised data. Specifically,
ZeroCap searches the caption words one by one and from left to right, guided by CLIP-induced score for image-text matching and GPT-2 word distribution for caption fluency.
ZeroCap is a good start and inspires us to explore how to search for the optimal caption in a better way. i) More flexible. ZeroCap utilizes GPT-2 to perform left-to-right autoregressive generation. Once a word is fixed, there is no chance to modify it when we move to the next position. In other words, such generation order is not flexi-ble enough to consider the full context information. ii) More efficient. The searching at every position is real-ized by iteratively updating the parameters of GPT-2, which is time-consuming, as shown in Fig. 3c. iii) More diverse.
IC is an open problem. Given an image, different persons may have different visual atten-tions [14] and language describing styles [24, 47, 73], thus resulting in diverse descriptions. ZeroCap employs beam search to generate several candidate sentences, which, how-ever, have similar syntactic patterns (see Appendix D). iv) More controllable. To endow captioning models with human-like controllability, e.g., sentiment, personality, a re-cent surge of efforts [12,19,24,47] resort to introducing ex-tra control signals as constraints of the generated captions, called Controllable IC. However, controllable zero-shot IC has not been explored yet.
Bearing all these four-aspect concerns in mind, we propose a novel framework for controllable zero-shot IC, named ConZIC, as shown in Fig. 2. Specifically, after ana-lyzing the relationship between Gibbs sampling and masked language models (MLMs, currently we use BERT) [11, 20, 70], we firstly develop a new language model (LM) called
Gibbs-BERT to realize the zero-shot IC by sampling-based search. Compared with autoregressive models, Gibbs-BERT has more a flexible generation order, bringing the self-correct capability by bidirectional attention with faster and more diverse generations. After integrating Gibbs-BERT with the CLIP that is used to evaluate the similar-ity between image and text, our proposed framework can perform zero-shot IC. By further introducing a task-specific discriminator for control signal into our framework, our proposed framework can perform controllable zero-shot IC.
The main contributions of this paper are:
• We propose to solve the controllable zero-shot IC task in a polishing way. By combining Gibbs sampling with a
MLM, we can randomly initialize the caption and then polish every word based on the full context (bidirectional information) in the caption.
• ConZIC is free of parameter updates, achieving about 5× faster generation speed than the SOTA method, ZeroCap.
• Equipped with Gibbs-BERT, ConZIC can perform flexi-ble searching, thus generating sentences with higher di-versity, as shown in Table. 1.
• To the best of our knowledge, ConZIC is the first control-lable zero-shot IC method. Four classes of controllable signals, including length, infilling, styles, and parts-of-speech, are evaluated in our experiments. 2.