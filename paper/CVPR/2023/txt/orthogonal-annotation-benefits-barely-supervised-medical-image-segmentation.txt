Abstract
Recent trends in semi-supervised learning have signifi-cantly boosted the performance of 3D semi-supervised med-ical image segmentation. Compared with 2D images, 3D medical volumes involve information from different direc-tions, e.g., transverse, sagittal, and coronal planes, so as to naturally provide complementary views. These com-plementary views and the intrinsic similarity among ad-jacent 3D slices inspire us to develop a novel annotation way and its corresponding semi-supervised model for effec-tive segmentation. Specifically, we firstly propose the or-thogonal annotation by only labeling two orthogonal slices in a labeled volume, which significantly relieves the bur-den of annotation. Then, we perform registration to ob-tain the initial pseudo labels for sparsely labeled volumes.
Subsequently, by introducing unlabeled volumes, we pro-pose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that exploits dense pseudo labels in early stage and sparse labels in later stage and meanwhile forces consistent output of two networks. Experimental results on three benchmark datasets validated our effectiveness in performance and efficiency in annotation. For example, with only 10 annotated slices, our method reaches a Dice up to 86.93% on KiTS19 dataset. Our code and models are available at https://github.com/HengCai-NJU/DeSCO. 1.

Introduction
Medical image segmentation is one of the most critical vision tasks in medical image analysis field. Thanks to the development of deep learning-based methods [8,11,28,32], segmentation performance has now been substantially im-proved. However, the current promising performance is at
*Corresponding author: Yinghuan Shi. Heng Cai, Shumeng Li,
Yinghuan Shi and Yang Gao are with the State Key Laboratory for
Novel Software Technology and National Institute of Healthcare Data
Science, Nanjing University, China. This work is supported by the
NSFC Program (62222604, 62206052, 62192783), CAAI-Huawei Mind-Spore (CAAIXSJLJJ-2021-042A), China Postdoctoral Science Founda-tion Project (2021M690609), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund.
Figure 1. The upper figure illustrates our annotation method, each volume with annotations is labeled with only two orthogonal slices. The lower figure shows the comparison between the effi-ciency and effectiveness of our orthogonal annotation and other manners, including conventional dense annotation and previous sparse annotation which labels slices in one plane. All trained on
LA [42] dataset with supervised setting. For sparse annotation and our orthogonal annotation, we train the models only on labeled voxels through partial cross-entropy and partial Dice loss. the cost of large-scale manually precisely labeled dataset, which is prohibitively expensive and laborious to achieve.
What’s worse, different radiologists might provide different annotations even for a same image. Therefore, exploring ways to alleviate the requirement of quantity or quality of manual annotation is highly demanded. Mainstream meth-ods typically follow two paradigms: 1) degrade annotation quality, i.e., weakly-supervised segmentation, and 2) reduce annotation quantity, i.e., semi-supervised segmentation.
Weakly-supervised segmentation methods usually utilize weak annotations, e.g., image-level label [16, 17], scrib-ble [20, 21], point [3] or partial slices [5, 18]. Unfor-tunately, most of them are either difficult to distinguish some fuzzy boundaries or with additional large computa-tional burden [15]. What’s more, weakly-supervised setting usually requires coarse annotation for every single image.
This is still a heavy burden for radiologists. Besides, most current methods originally developed for 2D segmentation could not directly utilize 3D volumetric information [9].
Different from these weakly-supervised methods, semi-supervised methods train segmentation models with a small amount of manually labeled data and a large amount of unlabeled data, which have achieved remarkable perfor-mance with an impressive deduction on demand for anno-tation [6, 19]. Despite their success, we notice most current semi-supervised segmentation methods still require full 3D annotation for each labeled volume. In fact, segmentation targets in adjacent slices of 3D volume are highly similar in both appearance and location, leading it redundant to la-bel every slice. Although the sparse annotation is discussed in recent work [18], we notice these conventional methods still neglect the complementary views between different di-rections in 3D volume.
It is known that 3D medical volumes naturally contains different directions (e.g., transverse, coronal planes) which provide complementary information from different views.
And recent trends in semi-supervised learning [7, 40] have revealed that learning from complementary view is indeed beneficial. Thus, we wonder whether a novel annotation method coupled with its corresponding model could be in-vestigated by introducing this complementary relation into 3D semi-supervised medical image segmentation.
In this paper, for labeled volume, we innovatively inves-tigate a novel sparse annotation way—orthogonal annota-tion, i.e., only to label two slices in its orthogonal direction (e.g., transverse and coronal direction in Figure 1). We be-lieve our annotation way has two merits: 1) it could largely force the model to learn from complementary views with two diversely initialized labeled slices, 2) it helps greatly re-duce the label costs with fully utilizing the inter-slice simi-larity. Following very recent work [18], we name the setting as Barely-supervised Segmentation.
To incorporate our orthogonal annotation, the most in-tuitive thought about training strategy of a segmentation model is that only the voxels on the labeled slices contribute to the training. However, directly learning from this sparse annotation is unstable and the training is apt to collapse (shown in Sec. 4). Thus, we apply registration to spread supervision signals from slice to volume, where the result of label propagation can serve as the dense pseudo label for training. By performing registration, we obtain two sets of pseudo labels for volumes from orthogonal directions.
Yet, the obtained pseudo labels are not promising enough to directly train a segmentation model using current exist-ing semi-supervised methods, which is mainly due to the accumulation of error in the registration process.
Therefore, to leverage 1) the volumes with inaccurate pseudo labels and 2) the rest unlabeled volumes, we propose a simple yet effective end-to-end framework namely Dense-Sparse Co-training (DeSCO), which consists two segmen-tation models of a same structure. At the beginning of training, the models mainly learn from dense pseudo labels with a learning preference on voxels with more confident pseudo labels, i.e., voxels near to registration source slice, and exploit unlabeled volumes through cross-supervision.
After the models have been improved through training, we gradually get rid of pseudo label until the supervised loss solely comes from sparse annotation. Meanwhile, the role of cross-supervision is gradually emphasized correspond-ingly. Because in the process of reaching consensus through cross-supervision, the mistake introduced by previous train-ing on inaccurate pseudo labels could be revised. Overall, our contributions are three folds:
• A new annotation way that only labels two orthogonal slices for a labeled 3D volume, which greatly reduces the annotation burden.
• A novel barely-supervised 3D medical image segmen-tation framework to steadily utilize our high-efficient sparse annotation with coupled segmentation method.
• A dense-sparse co-training paradigm to learn from dense pseudo label and sparse label while leveraging unlabeled volumes to reduce noise by reaching con-sensus through cross-supervision.
Extensive experiments on three public datasets validate that our barely-supervised method is close to or even bet-ter than its upper bound, i.e., semi-supervised methods with fully annotated labeled volumes. For example, on KiTS19, compared to Mean Teacher [36] that uses 320 labeled slices with a Dice of 84.98%, we only uses 10 labeled slices yet obtains a Dice of 86.93%. 2.