Abstract
Recently, deep Stereo Matching (SM) networks have shown impressive performance and attracted increasing at-tention in computer vision. However, existing deep SM networks are prone to learn dataset-dependent shortcuts, which fail to generalize well on unseen realistic dataset-s. This paper takes a step towards training robust models for the domain generalized SM task, which mainly focuses on learning shortcut-invariant representation from synthet-ic data to alleviate the domain shifts. Speciﬁcally, we pro-pose a Hierarchical Visual Transformation (HVT) network to 1) ﬁrst transform the training sample hierarchically in-to new domains with diverse distributions from three levels:
Global, Local, and Pixel, 2) then maximize the visual dis-crepancy between the source domain and new domains, and minimize the cross-domain feature inconsistency to capture domain-invariant features. In this way, we can prevent the model from exploiting the artifacts of synthetic stereo im-ages as shortcut features, thereby estimating the dispari-ty maps more effectively based on the learned robust and shortcut-invariant representation. We integrate our pro-posed HVT network with SOTA SM networks and evaluate its effectiveness on several public SM benchmark datasets.
Extensive experiments clearly show that the HVT network can substantially enhance the performance of existing SM networks in synthetic-to-realistic domain generalization. 1.

Introduction
Stereo Matching (SM) [7, 41, 44] aims to ﬁnd the match-ing correspondences between a given stereo image pair and then calculate the disparity for depth sensing in many ap-plications, such as robot navigation and autonomous driv-ing [1, 28]. Recently, it attracts increasing attention in the computer vision community [4, 27, 30, 42].
With the development of deep learning [6, 18, 34–38],
Convolutional Neural Network (CNN) based deep SM net-works have shown impressive performance beneﬁting from
∗Corresponding Author
Figure 1. Comparison of the cross-domain SM generalization.
Columns from left to right denote a sample image, ground truth disparities, the predicted disparities of the pretrained PSMNet model and our HVT-PSMNet model. Both models are trained on the synthetic SceneFlow [19] dataset and evaluated on the realistic datasets: Middlebury, ETH3D, KITTI 2012 and KITTI 2015. their strong ability of feature representation. However, due to the scarcity of sufﬁcient labeled realistic training data, existing state-of-the-art (SOTA) SM networks usually are trained on synthetic data, e.g. SceneFlow [19], which fail to generalize well to unseen realistic domains as shown in
Fig. 1. Generally, the generalizability of cross-domain deep
SM networks is mainly hindered by a critical issue: SM net-works usually learn superﬁcial shortcut features [5] from synthetic data to estimate the disparity. Speciﬁcally, such shortcut features mainly include two types of artifacts: con-sistent local RGB color statistics and overreliance on local chromaticity features, which are domain-sensitive and non-transferable to unseen domain. The semantic and structural features that are truly desirable are ignored by most existing
SM networks. Therefore, the key to addressing the chal-lenging cross-domain SM task is how to effectively learn the domain-invariant representations of the given stereo im-age pair for synthetic-to-realistic generalization.
Several attempts [3, 10, 15, 26, 45] have been made to minimize the synthetic-to-realistic domain gap and learn the domain-invariant representations for the SM task by either 1) exploiting labeled target-domain realistic data to
ﬁne-tune the SM network trained with synthetic data [3, 10]
or 2) jointly using the synthetic data and unlabeled target-domain realistic data to train domain adaptive SM network-s [15,26,45]. Despite their performance improvement on re-alistic data, these attempts only work well when the target-domain realistic data is provided during training and thus can not improve the out-of-distribution (OOD) generaliza-tion of SM networks, which are less practically useful in real-world scenarios.
In this work, we address the important but less explored challenging problem of single domain generalization in SM, where only the synthetic data is available for training. Con-sidering the fact that most existing SM networks are suscep-tible to exploiting shortcut cues in synthetic data instead of the semantic and structural correspondences, we propose to learn shortcut-invariant robust representation from synthet-ic SM image pairs for OOD generalization. Speciﬁcally, this paper presents a Hierarchical Visual Transformation (HVT) network to 1) ﬁrst transform the synthetic training sample hierarchically into new source domains with diverse distributions from three levels: Global, Local, and Pixel, 2) then maximize the image discrepancy between the synthet-ic source domain and new domains for signiﬁcantly alter-ing the original distribution, and minimize the cross-domain feature inconsistency to capture domain-invariant features.
In this way, we are able to prevent the model from ex-ploiting the artifacts of synthetic stereo images as shortcut features, thereby estimating the disparity maps more effec-tively based on the learned shortcut-invariant feature repre-sentation. Our basic idea is to diversify the distribution of training data and thus force the network to overlook the ar-tifacts from synthetic domain. Note that our proposed HVT network is simple and can be plug-and-play. We integrate
HVT with SOTA SM networks during training and evalu-ate its effectiveness on several challenging SM benchmark datasets. Extensive experiments clearly show that the HVT network can substantially enhance the performance of ex-isting SM networks in synthetic-to-realistic domain gener-alization without using any auxiliary data or features [17].
Our contributions can be brieﬂy summarized as follows:
• We devise a simple yet effective domain generalized SM framework. It leverages a hierarchical visual transforma-tion network to effectively diversify the distribution of training data which prevents the model from exploiting the artifacts in synthetic data as shortcuts.
• We formulate novel learning objectives that force the model to effectively optimize three complementary vi-sual transformations by maximizing domain discrepancy and minimizing feature inconsistency between synthetic domain and new domains, thereby facilitating the learn-ing of domain-invariant feature representation.
• Extensive experiments on four realistic SM datasets clearly demonstrate the effectiveness and robustness of our HVT network. The out-of-distribution generalization ability of four SOTA SM methods has been signiﬁcantly boosted, beneﬁting from our solution. 2.