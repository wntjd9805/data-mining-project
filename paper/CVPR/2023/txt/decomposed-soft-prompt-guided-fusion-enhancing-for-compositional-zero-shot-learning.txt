Abstract
Compositional Zero-Shot Learning (CZSL) aims to rec-ognize novel concepts formed by known states and objects during training. Existing methods either learn the combined state-object representation, challenging the generalization of unseen compositions, or design two classifiers to identify state and object separately from image features, ignoring the intrinsic relationship between them. To jointly eliminate the above issues and construct a more robust CZSL system, we propose a novel framework termed Decomposed Fusion with Soft Prompt (DFSP)1, by involving vision-language models (VLMs) for unseen composition recognition. Specif-ically, DFSP constructs a vector combination of learnable soft prompts with state and object to establish the joint rep-resentation of them. In addition, a cross-modal decomposed fusion module is designed between the language and image branches, which decomposes state and object among lan-guage features instead of image features. Notably, being fused with the decomposed features, the image features can be more expressive for learning the relationship with states and objects, respectively, to improve the response of un-seen compositions in the pair space, hence narrowing the domain gap between seen and unseen sets. Experimental results on three challenging benchmarks demonstrate that our approach significantly outperforms other state-of-the-art methods by large margins. 1.

Introduction
Given an unseen concept, such as green tiger, even though this is a nonexistent stuff humans have never seen, they may associate the known state green with an image of tiger immediately.
Inspired by this, Compositional Zero-Shot Learning (CZSL) is proposed with the purpose of
*Song Guo and Jingcai Guo are the corresponding authors 1Code is available at: https://github.com/Forest- art/
DFSP.git
Figure 1. The overview of DFSP. Our method aims to narrow the domain gap between seen and unseen compositions by fus-ing decomposed features fo and fs with image feature fv, while learn the joint representation between state and object in language branch. Being fused with the state and object features, image fea-ture can learn the response of them respectively and improve the sensitiveness of unseen compositions. equipping models with the ability to recognize novel con-cepts generated as humans do. Specifically, CZSL learns on visible primitive composed concepts (state and object) in the training phase, and recognizes unseen compositions in the inference phase.
Some prior algorithms [20, 26] design two classifiers to identify state and object separately, while these mod-els overlook the intrinsic relation between them. After the primitive concepts are obtained, the association between state and object could be established again through graph neural network (GNN) [24] or external knowledge compo-sitions [14]. Nevertheless, these are post-processing meth-ods and these classifiers are separated from image features with strong correlation, ignoring entanglement. Some other methods [28, 29] are to directly treat the combination as an entity, converting CZSL into a general zero-shot recognition problem. Generally, the visual features are projected into a shared semantic space and the distance between entities is
If too much optimized, such as Euclidean distance [44].
attention is paid to the composed concepts in the training stage, the model can not be generalized well to unseen com-positions, causing the domain gap between seen and unseen sets. In summary, these methods are all visual recognition models, which are limited by the strong entanglement of states and objects in image features.
In contrast, we focus on designing novel approaches based on vision-language models (VLMs) to cope with
CZSL challenges. Since state and object are two separate words in the text, they are less entangled in language fea-tures than image features and could be decomposed more easily and precisely. Certainly, state and object are also intrinsically linked in the text, such as ripe apple instead of old apple. Constructing the combination in the form of text can also establish the joint representation of state and object to pair with images. Meanwhile, the decomposed state and object features can also be independently associ-ated with the image feature, easing the excessive bias of the model towards seen compositions and enhancing the unseen response (shown in Fig. 1). To improve CZSL with VLMs, we design Decomposed Fusion with Soft Prompt (DFSP), an efficient framework aimed to both learn about the joint representation of primitive concepts and shrink the domain gap between seen and unseen composition sets, as shown in
Fig. 2. To be specific, DFSP is designed as a fully learnable soft prompt including prefix, state and object, which con-structs the joint representation between primitive concepts and can be fine-tuned well for new supervised tasks. We then design a decomposed fusion module (DFM) for state and object, which decomposes features extracted from text encoder, such as Bert [6], etc. Meanwhile, the decomposed language features and image features of DFSP interact with information in a cross-modal fusion module, which is cru-cial for learning high-quality language-aware visual repre-sentations. During the phase of fusion, the image can es-tablish separate relationships with the state and object, and then is paired with the composed prompt feature in the pair space, improving its response even for unseen compositions to shrink the domain gap.
Generally, this paper makes the following contributions:
• A novel framework named Decomposed Fusion with
Soft Prompt (DFSP) is proposed, which is based on vision-language paradigm aiming to cope with CZSL.
• The Decomposed Fusion Module is designed for
CZSL specifically, which decomposes the concepts of language features and fuses them with image features to improve the response of unseen compositions.
• We design a learnable soft prompt to construct the joint-representation of state and object, which can be more precisely decomposed than images.
• Extensive experiments demonstrate the effectiveness of DFSP, which greatly outperforms the state-of-the-art CZSL approaches on both closed-world and open-world. 2.