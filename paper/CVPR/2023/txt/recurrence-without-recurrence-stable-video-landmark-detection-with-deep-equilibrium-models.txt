Abstract
Cascaded computation, whereby predictions are recur-rently refined over several stages, has been a persistent theme throughout the development of landmark detection models. In this work, we show that the recently proposed
Deep Equilibrium Model (DEQ) can be naturally adapted to this form of computation. Our Landmark DEQ (LDEQ) achieves state-of-the-art performance on the challenging
WFLW facial landmark dataset, reaching 3.92 NME with fewer parameters and a training memory cost of O(1) in the number of recurrent modules. Furthermore, we show that DEQs are particularly suited for landmark detection in videos. In this setting, it is typical to train on still im-ages due to the lack of labelled videos. This can lead to a “flickering” effect at inference time on video, whereby a model can rapidly oscillate between different plausible so-lutions across consecutive frames. By rephrasing DEQs as a constrained optimization, we emulate recurrence at infer-ence time, despite not having access to temporal data at training time. This Recurrence without Recurrence (RwR) paradigm helps in reducing landmark flicker, which we demonstrate by introducing a new metric, normalized mean flicker (NMF), and contributing a new facial landmark video dataset (WFLW-V) targeting landmark uncertainty.
On the WFLW-V hard subset made up of 500 videos, our
LDEQ with RwR improves the NME and NMF by 10 and 13% respectively, compared to the strongest previously pub-lished model using a hand-tuned conventional filter. 1.

Introduction
The field of facial landmark detection has been fueled by important applications such as face recognition [49, 71], facial expression recognition [33, 37, 39, 43, 79], and face alignment [55,87,89]. Early approaches to landmark detec-tion relied on a statistical model of the global face appear-ance and shape [19, 20, 25], but this was then superseded by deep learning regression models [21,26,34,40,44,46,54,63, 68, 69, 73, 75, 77]. Both traditional and modern approaches have relied upon cascaded computation, an approach which starts with an initial guess of the landmarks and iteratively produces corrected landmarks which match the input face more finely. These iterations typically increase the train-ing memory cost linearly, and do not have an obvious stop-ping criteria. To solve these issues, we adapt the recently proposed Deep Equilibrium Model [9–11] to the setting of landmark detection. Our Landmark DEQ (LDEQ) achieves state-of-the-art performance on the WFLW dataset, while enjoying a natural stopping criteria and a memory cost that is constant in the number of cascaded iterations.
Furthermore, we explore the benefits of DEQs in land-mark detection from facial videos. Since obtaining land-mark annotation for videos is notoriously expensive, mod-els are virtually always trained on still images and ap-plied frame-wise on videos at inference time. When a sequence of frames have ambiguous landmarks (e.g., oc-cluded faces or motion blur), this leads to flickering land-marks, which rapidly oscillate between different possible configurations across consecutive frames. This poor tem-poral coherence is particularly problematic in applications where high precision is required, which is typically the case for facial landmarks. These applications include face trans-forms [1, 2], face reenactment [84], video emotion recogni-tion [33, 37, 39, 79], movie dubbing [28] or tiredness mon-itoring [35]. We propose to modify the DEQ objective at inference time to include a new recurrent loss term that en-courages temporal coherence. We measure this improve-ment on our new WFLW-Video dataset (WFLW-V), demon-strating superiority over traditional filters, which typically reduce flickering at the cost of reducing landmark accuracy.
Figure 1. (a) Common Stacked-Hourglass architecture [51], whereby each cascaded stage increases the memory cost and the number of backpropagation operations. (b) Our LDEQ model, which adapts an equilibrium model [9] to the landmark detection setting, enjoying a constant memory cost with the number of refining stages. At each stage, we compute landmark probability heatmaps, and encourage convergence to an equilibrium by lowering their entropy. We release our code here: https://github.com/polo5/LDEQ_RwR. 2.