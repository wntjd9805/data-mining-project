Abstract
Despite the prominent success of general object detec-tion, the performance and efficiency of Small Object Detec-tion (SOD) are still unsatisfactory. Unlike existing works that struggle to balance the trade-off between inference speed and SOD performance, in this paper, we propose a novel Scale-aware Knowledge Distillation (ScaleKD), which transfers knowledge of a complex teacher model to a compact student model. We design two novel modules to boost the quality of knowledge transfer in distillation for
SOD: 1) a scale-decoupled feature distillation module that disentangled teacher’s feature representation into multi-scale embedding that enables explicit feature mimicking of the student model on small objects. 2) a cross-scale assis-tant to refine the noisy and uninformative bounding boxes prediction student models, which can mislead the student model and impair the efficacy of knowledge distillation. A multi-scale cross-attention layer is established to capture the multi-scale semantic information to improve the student model. We conduct experiments on COCO and VisDrone datasets with diverse types of models, i.e., two-stage and one-stage detectors, to evaluate our proposed method. Our
ScaleKD achieves superior performance on general detec-tion performance and obtains spectacular improvement re-garding the SOD performance. 1.

Introduction
Object detection is a fundamental task that has been developed over the past twenty-year in the computer vi-sion community. Despite the state-of-the-art performance for general object detection having been conspicuously improved since the rise of deep learning, balancing the complexity-precision for small object detection is still an open question. Current works strive to refine feature fu-sion modules [9, 21], devise novel training schemes [32, 33]
*Corresponding author to explicitly train on small objects, design new neural archi-tectures [20,39] to better extract small objects’ features, and leverage increased input resolution to enhance representa-tion quality [1, 49]. However, these approaches struggle to balance detection quality on small objects with computa-tional costs at the inference stage.
The above reasons incentivize us to design a cost-free
In technique at test time to improve SOD performance. the spirit of the eminent success of knowledge distillation (KD) on image data [14], we explore distillation methods for SOD. Typically, knowledge distillation opts for a com-plex, high-performance model (teacher) that transfers its knowledge to a compact, low-performance model (student).
The student model can harness instructive information to enhance its representation learning ability. Nevertheless, unlocking this potential in SOD involves overcoming two challenges: 1) SOD usually suffers from noisy feature rep-resentations. Due to the nature of small objects, which gen-erally take over a small region in the whole image, the fea-ture representations of these small objects can be contami-nated by the background and other instances with relatively larger sizes. 2) Object detectors have a low tolerance for noisy bounding boxes on small objects. It is inevitable that teacher models make incorrect predictions. Usually, student models can extract informative dark knowledge [14, 28] from imperfect predictions from the teacher. However, in
SOD, small perturbations on the teacher’s bounding box can dramatically impair SOD performance on the student detec-tor (§3.2).
To this end, we propose Scale-aware Knowledge Distil-lation for small object detection (ScaleKD). Our proposed
ScaleKD consists of two modules, a Scale-Decoupled Fea-ture (SDF) distillation module and a Cross-Scale Assistant (CSA), to address the aforementioned two challenges corre-spondingly. The SDF is inspired by the crucial shortcoming of existing feature distillation methods, where the feature representations of objects with varying scales are coupled in a single embedding.
It poses difficulty for the student to mimic small objects’ features from the teacher model.
Figure 1. The overview of Scale-aware Knowledge Distillation. It consists of a Scale-Decoupled Feature distillation module and a Cross-Scale Assistant module to improve small object detection.
As a result, the proposed SDF aims to decouple a single-scale feature embedding into a multi-scale feature embed-ding. The multi-scale embedding is obtained by a paral-lel multi-branch convolutional block, where each branch deals with one scale. Our SDF allows the student model to better understand the feature knowledge from the per-spective of object scale. Furthermore, we propose a learn-able CSA to resolve the adverse effect of teachers’ noisy bounding box prediction on small objects. The CSA com-prises a multi-scale cross-attention module, where represen-tations from the teacher and student models are mapped into a single feature embedding. The multi-scale query-key pair projects the teacher’s features into multiple sizes, such that the fine-grained and low-level details can be preserved in
CSA, which helps to produce suitable bounding box super-vision for the student model.
We demonstrate the effectiveness of our approach on
COCO object detection and VisDrone datasets. The ex-periments are conducted on multiple types of detectors, in-cluding two-stage detectors, anchor-based detectors, and anchor-free detectors, and have proven the generalizability of our approach. Our work offers a practical approach for industrial application on SOD as well as introduces a new perspective on designing scale-aware KD modules to im-prove object detectors. We further extend our method on instance-level detection tasks, such as instance segmenta-tion and keypoint detection, demonstrating the superiority of our approach to dealing with small objects in vision tasks.
In summary, our contributions are the following:
• We propose Scale-Aware Knowledge Distillation (ScaleKD), a novel knowledge distillation framework to improve general detection and SOD performance without bringing extra computational costs at test time.
• Our proposed ScaleKD not only exceeds state-of-the-art KD for object detection methods on general de-tection performance but also surpasses existing ap-proaches on SOD by a large margin. Extended experi-ments on instance segmentation and keypoint detection further strength our method. 2.