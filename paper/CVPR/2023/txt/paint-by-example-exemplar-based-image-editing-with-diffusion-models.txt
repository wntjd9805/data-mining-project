Abstract pretrained models are available at https://github. com/Fantasy-Studio/Paint-by-Example.
Language-guided image editing has achieved great suc-cess recently. In this paper, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. How-ever, the naive approach will cause obvious fusing artifacts.
We carefully analyze it and propose a content bottleneck and strong augmentations to avoid the trivial solution of di-rectly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we de-sign an arbitrary shape mask for the exemplar image and leverage the classiﬁer-free guidance to increase the similar-ity to the exemplar image. The whole framework involves a single forward of the diffusion model without any itera-tive optimization. We demonstrate that our method achieves an impressive performance and enables controllable edit-ing on in-the-wild images with high ﬁdelity. The code and
*Work is done during the internship at Microsoft Research Asia.
†Corresponding author. 1.

Introduction
Creative editing for photos has become a ubiquitous need due to the advances in a plethora of social media plat-forms. AI-based techniques [37] signiﬁcantly lower the barrier of fancy image editing that traditionally requires specialized software and labor-intensive manual operations.
Deep neural networks can now produce compelling re-sults for various low-level image editing tasks, such as im-age inpainting [61, 68], composition [41, 67, 72], coloriza-tion [53,71,74] and aesthetic enhancement [7,11], by learn-ing from richly available paired data. A more challeng-ing scenario, on the other hand, is semantic image editing, which intends to manipulate the high-level semantics of im-age content while preserving image realism. Tremendous efforts [2, 5, 36, 49, 55, 57] have been made along this way, mostly relying on the semantic latent space of generative models, e.g., GANs [16, 28, 70], yet the majority of existing works are limited to speciﬁc image genres.
Recent large-scale language-image (LLI) models, based on either auto-regressive models [12, 69] or diffusion mod-els [19, 45, 50, 54], have shown unprecedented generative power in modeling complex images. These models en-able various image manipulation tasks [3, 22, 30, 52] previ-ously unassailable, allowing image editing for general im-ages with the guidance of text prompt. However, even the detailed textual description inevitably introduces ambiguity and may not accurately reﬂect the user-desired effects; in-deed, many ﬁne-grained object appearances can hardly be speciﬁed by the plain language. Hence, it is crucial to de-velop a more intuitive approach to ease ﬁne-grained image editing for novices and non-native speakers.
In this work, we propose an exemplar-based image edit-ing approach that allows accurate semantic manipulation on the image content according to an exemplar image provided by users or retrieved from the database. As the saying goes,
“a picture is worth a thousand words”. We believe images better convey the user’s desired image customization in a more granular manner than words. This task is completely different from image harmonization [20, 64] that mainly fo-cuses on color and lighting correction when compositing the foreground objects, whereas we aim for a much more com-plex job: semantically transforming the exemplar, e.g., pro-ducing a varied pose, deformation or viewpoint, such that the edited content can be seamlessly implanted according to the image context. In fact, ours automates the traditional image editing workﬂow where artists perform tedious trans-formations upon image assets for coherent image blending.
To achieve our goal, we train a diffusion model [24, 59] conditioned on the exemplar image. Different from text-guided models, the core challenge is that it is infeasible to collect enough triplet training pairs comprising source image, exemplar and corresponding editing ground truth.
One workaround is to randomly crop the objects from the input image, which serves as the reference when training the inpainting model. The model trained from such a self-reference setting, however, cannot generalize to real exem-plars, since the model simply learns to copy and paste the reference object into the ﬁnal output. We identify several key factors that circumvent this issue. The ﬁrst is to utilize a generative prior. Speciﬁcally, a pretrained text-to-image model has the ability to generate high-quality desired re-sults, we leverage it as initialization to avoid falling into the copy-and-paste trivial solution. However, a long time of ﬁnetuning may still cause the model to deviate from the prior knowledge and ultimately degenerate again. Hence, we introduce the content bottleneck for self-reference con-ditioning in which we drop the spatial tokens and only re-gard the global image embedding as the condition. In this way, we enforce the network to understand the high-level semantics of the exemplar image and the context from the source image, thus preventing trivial results during the self-supervised training. Moreover, we apply aggressive aug-mentation on the self-reference image which can effectively reduce the training-test gap.
We further improve the editability of our approach in two aspects. One is that our training uses irregular random masks so as to mimic the casual user brush used in practi-cal editing. We also prove that classiﬁer-free guidance [25] is beneﬁcial to boost both the image quality and the style resemblance to the reference.
The proposed method, Paint by Example, well solves semantic image composition problem where the reference is semantically transformed and harmonized before blend-ing into another image, as shown in Figure 1. Our method shows a signiﬁcant quality advantage over prior works in a similar setting. Notably, our editing just involves a single forward of the diffusion model without any image-speciﬁc optimization, which is a necessity for many real applica-tions. To summarize, our contributions are as follows:
• We propose a new image editing approach, Paint by Ex-ample, which semantically alters the image content based on an exemplar image. This approach offers ﬁne-grained control while being convenient to use.
• We solve the problem with an image-conditioned diffu-sion model trained in a self-supervised manner. We pro-pose a group of techniques to tackle the degenerate chal-lenge.
• Our approach performs favorably over prior arts for in-the-wild image editing, as measured by both quantitative metrics and subjective evaluation. 2.