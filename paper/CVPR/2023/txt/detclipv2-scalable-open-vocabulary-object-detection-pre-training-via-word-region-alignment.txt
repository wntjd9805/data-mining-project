Abstract
This paper presents DetCLIPv2, an efficient and scalable training framework that incorporates large-scale image-text pairs to achieve open-vocabulary object detection (OVD). Unlike previous OVD frameworks that typically rely on a pre-trained vision-language model (e.g., CLIP) or ex-ploit image-text pairs via a pseudo labeling process, Det-CLIPv2 directly learns the fine-grained word-region align-ment from massive image-text pairs in an end-to-end man-ner. To accomplish this, we employ a maximum word-region similarity between region proposals and textual words to guide the contrastive objective. To enable the model to gain localization capability while learning broad concepts, Det-CLIPv2 is trained with a hybrid supervision from detection, grounding and image-text pair data under a unified data formulation. By jointly training with an alternating scheme and adopting low-resolution input for image-text pairs, Det-CLIPv2 exploits image-text pair data efficiently and ef-fectively: DetCLIPv2 utilizes 13× more image-text pairs than DetCLIP with a similar training time and improves
†Corresponding xu.hang@huawei.com author: liangxd9@mail.sysu.edu.cn, performance. With 13M image-text pairs for pre-training,
DetCLIPv2 demonstrates superior open-vocabulary detec-tion performance, e.g., DetCLIPv2 with Swin-T backbone achieves 40.4% zero-shot AP on the LVIS benchmark, which outperforms previous works GLIP/GLIPv2/DetCLIP by 14.4/11.4/4.5% AP, respectively, and even beats its fully-supervised counterpart by a large margin. 1.

Introduction
Traditional object detection frameworks [6,35,36,57] are typically trained to predict a set of predefined categories, which fails to meet the demand of many downstream appli-cation scenarios that require to detect arbitrary categories (denoted as open-vocabulary detection, OVD). For exam-ple, a robust autonomous driving system requires accurate predictions for all classes of objects on the road [26]. Ex-tending traditional object detectors to adapt these scenar-ios needs tremendous human effort for extra instance-level bounding-box annotations, especially for rare classes. To obtain an open-vocabulary detector without the expensive annotation process, the central question we should ask is: where does knowledge about unseen categories come from?
Recent works [16,44,51] try to achieve open-vocabulary object detection by transferring knowledge from a pre-trained vision-language (VL) model [20, 33, 49]. E.g.,
ViLD [16] distills the CLIP’s [33] image embeddings of cropped proposals into the proposal features of a detection model. However, these solutions suffer from the domain gap problem: VL models are typically pre-trained with an image-level supervision using a fixed resolution input, which are not capable of recognizing objects with various scales in the detection task, especially for small objects.
Another line of work resorts to exploiting massive image-text pairs crawled from the Internet. To utilize the image-text pair data without instance-level annotation, ap-proaches [13, 14, 19, 27, 48, 54] generate pseudo-bounding-box labels following a self-training paradigm [40] or based on a pre-trained VL model [33]. However, their final per-formance is restricted by the quality of pseudo-labels pro-vided by a detector trained with limited human-annotated concepts or a VL model suffering from the aforementioned domain gap problem. Besides, using high-resolution inputs similar to detection data for massive image-text pairs will impose a huge computational burden on training, prevent-ing us from further scaling up image-text pairs.
To address the above issues, we present DetCLIPv2, an end-to-end open-vocabulary detection pre-training frame-work that effectively incorporates large-scale image-text pairs. DetCLILPv2 simultaneously learns localization ca-pability and knowledge of broad concepts without relying on a teacher model to provide pseudo labels. Specifically, we perform joint training with heterogeneous data from multiple sources, including detection [38], grounding [22] and image-text pairs [7, 39], under a unified data formula-tion. To enable image-text pairs without instance-level an-notations to facilitate learning of detection, inspired by [49], we employ an optimal matching-based set similarity be-tween visual regions and textual concepts to guide the con-trastive learning. By alternating different types of data for training, we enable a “flywheel effect”: learning from de-tection data provides accurate localization, which helps ex-tract representative regions for contrastive learning, while contrastive learning from image-text pairs helps recognize broader concepts, which further improves the localization of unseen categories. As the training goes on, the detector learns to locate and recognize increasingly rich concepts.
Furthermore, to relief the computation burden brought by large-scale image-text pairs, we adopt a low-resolution input for image-text pair data, which significantly improves the training efficiency. This is a reasonable design since the caption of image-text pair data typically describes only the main objects appearing in the image, which alleviates the necessity of high-resolution training.
Benefiting from the effective designs, DetCLIPv2 demonstrates superior open-vocabulary detection perfor-Figure 2. Different OVD training paradigms. (a) Distilling knowledge from a pre-trained VL model [16]. (b) Exploiting image-text pairs via pseudo labeling [27]. (c) Our end-to-end joint training eliminates complex multi-stage training schemes, allow-ing for mutual benefits in learning from different types of data. mance and promising scaling behavior. E.g., compared to the prior work DetCLIP [48], DetCLIPv2 is able to exploit 13× more image-text pairs while requiring only a similar training time. Using the vanilla ATSS [53] as the detector, DetCLIPv2 with Swin-T backbone achieves 40.4% zero-shot AP on the LVIS [17] benchmark, sur-passing previous works GLIP [27]/GLIPv2 [52]/DetCLIP
[48] by 14.4/11.4/4.5% AP, respectively. DetCLIPv2 also exhibits great generalization when transferring to down-stream tasks, e.g., it achieves SoTA fine-tuning performance on LVIS and ODinW13 [27]. We present a possibility of achieving open-world detection by incorporating large-scale image-text pairs and hope it will enlighten the commu-nity to explore a similar successful trajectory to CLIP [33]. 2.