Abstract
We propose Universal Document Processing (UDOP), a foundation Document AI model which uniﬁes text, im-age, and layout modalities together with varied task for-mats, including document understanding and generation.
UDOP leverages the spatial correlation between textual con-tent and document image to model image, text, and layout modalities with one uniform representation. With a novel
Vision-Text-Layout Transformer, UDOP uniﬁes pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and lay-out modalities via masked image reconstruction. To the best of our knowledge, this is the ﬁrst time in the ﬁeld of document AI that one model simultaneously achieves high-quality neural document editing and content customization.
Our method sets the state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA, across diverse data domains like ﬁnance reports, academic papers, and web-sites. UDOP ranks ﬁrst on the leaderboard of the Document
Understanding Benchmark.1 1.

Introduction
Document Artiﬁcial Intelligence studies information ex-traction, understanding, and analysis of digital documents, e.g., business invoices, tax forms, academic papers, etc. It is a multimodal task where text is structurally embedded in doc-uments, together with other vision information like symbols,
ﬁgures, and style. Different from classic vision-language research, document data have a 2D spatial layout: text con-tent is structurally spread around in different locations based on diverse document types and formats (e.g., invoices vs.
*Corresp. authors: ziyiyang@microsoft.com, mbansal@cs.unc.edu 1Code and models: https://github.com/microsoft/i-Code/tree/main/i-Code-Doc tax forms); formatted data such as ﬁgures, tables and plots are laid out across the document. Hence, effectively and efﬁciently modeling and understanding the layout is vital for document information extraction and content understand-ing, for example, title/signature extraction, fraudulent check detection, table processing, document classiﬁcation, and automatic data entry from documents.
Document AI has unique challenges that set it apart from other vision-language domains. For instance, the cross-modal interactions between text and visual modalities are much stronger here than in regular vision-language data, because the text modality is visually-situated in an image.
Moreover, downstream tasks are diverse in domains and paradigms, e.g., document question answering [45], lay-out detection [57], classiﬁcation [13], information extrac-tion [28], etc. This gives rises to two challenges: (1) how to utilize the strong correlation between image, text and lay-out modalities and unify them to model the document as a whole? (2) how can the model efﬁciently and effectively learn diverse vision, text, and layout tasks across different domains?
There has been remarkable progress in Document AI in recent years [1, 10–12, 15, 16, 24, 26, 29, 30, 36, 37, 48, 52–55].
Most of these model paradigms are similar to traditional vision-language frameworks: one line of work [1, 11, 29, 30, 36, 37, 52–55] inherits vision-language models that encode images with a vision network (e.g., vision transformer) and feed the encodings to the multimodal encoder along with text [17, 27, 44, 47]; another line of work uses one joint en-coder [22, 46] for both text and image [16]. Some models regard documents as text-only inputs [10, 12, 15, 26, 48]. In these works, the layout modality is represented as shallow positional embeddings, e.g., adding a 2D positional embed-ding to text embeddings. The strong correlation between modalities inherent in document data are not fully exploited.
Also to perform different tasks, many models have to use task-speciﬁc heads, which is inefﬁcient and requires manual design for each task.
To address these challenges, we propose Universal Docu-Text reconstruction with layout. <text_layout_0>  
Retail: Week of March 14, 1994
Visual text recognition. <text_0>  
<100><350><118><372>
</text_0> Week of March 14, 1994
Question answering.  
What is the date? 
Layout modeling. <layout_0> Ship
Date </layout_0> to Retail: Week of March 14, 1994
Layout analysis. Title
Masked image reconstruction.    Ship Date to Retail: Week of
March 14, 1994
Ship  
Date   to  
.....
OCR 
Text (0.1, 0.65, 0.11, 0.68)  (0.12, 0.65, 0.14, 0.68)  (0.16, 0.65, 0.18, 0.68) 
...
Bounding Boxes  (x0, y0, x1, y2)
Vision-Text-Layout  
Transformer
Unified
Encoder
Text-Layout
Decoder
Vision Decoder
Vision Task
Text Task
Mixed Task
Layout Task
<text_layout_0> Ship
Date <0><10><2><20>
<text_0> Ship Date
Week of March 14, 1994
<layout_0> <100><350><118><372>
Title <20><50><40><80>
Vision Outputs
Ship Date ...
Text Outputs
<100><350><118>
<372>...
Layout Outputs
Figure 1. UDOP uniﬁes vision, text, and layout through vision-text-layout Transformer and uniﬁed generative pretraining tasks including vision task, text task, layout task, and mixed task. We show the task prompts (left) and task targets (right) for all self-supervised objectives (joint text-layout reconstruction, visual text recognition, layout modeling, and masked autoencoding) and two example supervised objectives (question answering and layout analysis). ment Processing (UDOP), a foundation Document AI model that uniﬁes vision, text, and layout and different document tasks. Different from regarding image and document text as two separate inputs in previous works, in UDOP we propose to model them with the uniform layout-induced representa-tion (Sec. 3.1): in the input stage, we add embeddings of text tokens with the features of the image patch where the tokens are located. This simple and novel layout-induced representation greatly enhances the interaction between the text and vision modalities.
Besides the layout-induced representation, to form a uni-form paradigm for different vision, text, layout tasks, UDOP
ﬁrst builds a homogeneous vocabulary for texts and docu-ment layout that converts layout, i.e. bounding boxes, to discretized tokens. Second, we propose Vision-Text-Layout (VTL) Transformer, consisting of a modality-agnostic en-coder, text-layout decoder and vision decoder. VTL Trans-former allows UDOP to jointly encode and decode vision, text, and layout. UDOP unites all downstream tasks with a sequence-to-sequence generation framework.
Besides the challenges of modality uniﬁcation and task paradigms, another issue is previous works utilized self-supervised learning objectives that were originally designed for single-modality learning, e.g., masked language model-ing, or classical vision-language pretraining, e.g., contrastive learning. We instead propose novel self-supervised learning objectives designed to allow holistic document learning, in-cluding layout modeling, text and layout reconstruction, and vision recognition that account for text, vision and layout modeling together (Sec. 4). Besides sequential generation,
UDOP can also generate vision documents by leveraging masked autoencoders (MAE) [14] by reconstructing the doc-ument image from text and layout modalities. With such generation capacity, UDOP is the ﬁrst document AI model to achieve high-quality customizable, joint document editing and generation.
Finally, our uniform sequence-to-sequence generation framework enables us to conveniently incorporate all major document supervised learning tasks to pretraining, i.e., docu-ment layout analysis, information extraction, document clas-siﬁcation, document Q&A, and Table QA/NLI, despite their signiﬁcant differences in task and data format. In contrast, pretraining in previous document AI works is constrained to unlabeled data only (or using one single auxiliary super-vised dataset such as FUNSD [55]), while abundant labeled datasets with high quality supervision signals are ignored due to the lack of modeling ﬂexibility. Overall, UDOP is pretrained on 11M public unlabeled documents, together with 11 supervised datasets of 1.8M examples. Ablation study in Table 4 shows that UDOP only pretrained with the proposed self-supervised objectives exhibits great improve-ments over previous models, and adding the supervised data to pretraining further improves the performance.
We evaluate UDOP on FUNSD [18], CORD [34], RVL-CDIP [13], DocVQA [33], and DUE-Benchmark [2]. UDOP ranks the 1st place on the DUE-Benchmark leaderboard with 7 tasks, and also achieves SOTA on CORD, hence making
UDOP a powerful and uniﬁed foundation Document AI model for diverse document understanding tasks,
To summarize, our major contributions include: 1. Uniﬁed representations and modeling for vision, text and layout modalities in document AI. 2. Uniﬁed all document tasks to the sequence-to-sequence generation framework. 3. Combined novel self-supervised objectives with super-vised datasets in pretraining for uniﬁed document pretrain-ing. 4. UDOP can process and generate text, vision, and layout modalities together, which to the best of our knowledge is
ﬁrst one in the ﬁeld of document AI. 5. UDOP is a foundation model for Document AI, achiev-ing SOTA on 8 tasks with signiﬁcant margins.
2.