Abstract
Pre-training by numerous image data has become de-facto for robust 2D representations. In contrast, due to the expensive data processing, a paucity of 3D datasets severely hinders the learning for high-quality 3D features. In this paper, we propose an alternative to obtain superior 3D representations from 2D pre-trained models via Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised pre-training, we leverage the well learned 2D knowledge to guide 3D masked autoencoding, which recon-structs the masked point tokens with an encoder-decoder ar-chitecture. Specifically, we first utilize off-the-shelf 2D mod-els to extract the multi-view visual features of the input point cloud, and then conduct two types of image-to-point learn-ing schemes. For one, we introduce a 2D-guided masking strategy that maintains semantically important point tokens to be visible. Compared to random masking, the network can better concentrate on significant 3D structures with key spatial cues. For another, we enforce these visible to-kens to reconstruct multi-view 2D features after the decoder.
This enables the network to effectively inherit high-level 2D semantics for discriminative 3D modeling. Aided by our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning, achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to existing fully trained meth-ods. By further fine-tuning on on ScanObjectNN’s hard-est split, I2P-MAE attains the state-of-the-art 90.11% ac-curacy, +3.68% to the second-best, demonstrating supe-rior transferable capacity. Code is available at https:
//github.com/ZrrSkywalker/I2P-MAE. 1.

Introduction
Driven by huge volumes of image data [11, 40, 56, 61], pre-training for better visual representations has gained
† Corresponding author
Figure 1. Image-to-Point Masked Autoencoders. We leverage the 2D pre-trained models to guide the MAE pre-training in 3D, which alleviates the need of large-scale 3D datasets and learns from 2D knowledge for superior 3D representations. much attention in computer vision, which benefits a variety of downstream tasks [8, 29, 39, 55, 62]. Besides supervised pre-training with labels, many researches develop advanced self-supervised approaches to fully utilize raw image data via pre-text tasks, e.g., image-image contrast [5, 9, 10, 21, 28], language-image contrast [12, 53], and masked image modeling [3, 4, 18, 19, 27, 31, 42, 70]. Given the popularity of 2D pre-trained models, it is still absent for large-scale 3D datasets in the community, attributed to the expensive data acquisition and labor-intensive annotation. The widely adopted ShapeNet [6] only contains 50k point clouds of 55 object categories, far less than the 14 million ImageNet [11] and 400 million image-text pairs [53] in 2D vision. Though there have been attempts to extract self-supervisory signals for 3D pre-training [33, 41, 47, 64, 69, 76, 78, 83], raw point clouds with sparse structural patterns cannot provide suf-Figure 2. Comparison of (Left) Existing Methods [47, 78] and (Right) our I2P-MAE. On top of the general 3D MAE architec-ture, I2P-MAE introduces two schemes of image-to-point learn-ing: 2D-guided masking and 2D-semantic reconstruction.
Figure 3. Pre-training Epochs vs. Linear SVM Accuracy on
ModelNet40 [67]. With the image-to-point learning schemes, I2P-MAE exerts superior transferable capability with much faster con-vergence speed than Point-MAE [47] and Point-M2AE [78]. ficient and diversified semantics compared to colorful im-ages, which constrain the generalization capacity of pre-training. Considering the homology of images and point clouds, both of which depict certain visual characteristics of objects and are related by 2D-3D geometric mapping, we ask the question: can off-the-shelf 2D pre-trained mod-els help 3D representation learning by transferring robust 2D knowledge into 3D domains?
To tackle this challenge, we propose I2P-MAE, a
Masked Autoencoding framework that conducts Image-to-Point knowledge transfer for self-supervised 3D point cloud pre-training. As shown in Figure 1, aided by 2D semantics learned from abundant image data, our I2P-MAE produces high-quality 3D representations and exerts strong transfer-able capacity to downstream 3D tasks. Specifically, refer-ring to 3D MAE models [47, 78] in Figure 2 (Left), we first adopt an asymmetric encoder-decoder transformer [14] as our fundamental architecture for 3D pre-training, which takes as input a randomly masked point cloud and recon-structs the masked points from the visible ones. Then, to acquire 2D semantics for the 3D shape, we bridge the modal gap by efficiently projecting the point cloud into multi-view depth maps. This requires no time-consuming offline ren-dering and largely preserves 3D geometries from different perspectives. On top of that, we utilize off-the-shelf 2D models to obtain the multi-view 2D features along with 2D attention maps of the point cloud, and respectively guide the pre-training from two aspects, as shown in Figure 2 (Right).
Firstly, different from existing methods [47, 78] to ran-domly sample visible tokens, we introduce a 2D-guided masking strategy that reserves point tokens with more spa-In de-tial semantics to be visible for the MAE encoder. tail, we back-project the multi-view attention maps into 3D space as a spatial attention cloud. Each element in the cloud indicates the semantic significance of the correspond-ing point token. Guided by this, the 3D network can bet-ter focus on the visible critical structures to understand the global 3D shape, and also reconstruct the masked tokens from important spatial cues.
Secondly, in addition to the recovering of masked point tokens, we propose to concurrently reconstruct 2D seman-tics from the visible point tokens after the MAE decoder.
For each visible token, we respectively fetch its projected 2D representations from different views, and integrate them as the 2D-semantic learning target. By simultaneously re-constructing the masked 3D coordinates and visible 2D con-cepts, I2P-MAE is able to learn both low-level spatial pat-terns and high-level semantics pre-trained in 2D domains, contributing to superior 3D representations.
With the aforementioned image-to-point guidance, our
I2P-MAE significantly accelerates the convergence speed of pre-training and exhibits state-of-the-art performance on 3D downstream tasks, as shown in Figure 3. Learning from 2D ViT [14] pre-trained by CLIP [53], I2P-MAE, without any fine-tuning, achieves 93.4% classification ac-curacy by linear SVM on ModelNet40 [67], which has surpassed the fully fine-tuned results of Point-BERT [76] and Point-MAE [33]. After fine-tuning, I2P-MAE further achieves 90.11% classification accuracy on the hardest split of ScanObjectNN [63], significantly exceeding the second-best Point-M2AE [78] by +3.68%. The experiments fully demonstrate the effectiveness of learning from pre-trained 2D models for superior 3D representations.
Our contributions are summarized as follows:
• We propose Image-to-Point Masked Autoencoders (I2P-MAE), a pre-training framework to leverage 2D pre-trained models for learning 3D representations.
• We introduce two strategies, 2D-guided masking and 2D-semantic reconstruction, to effectively transfer the well learned 2D knowledge into 3D domains.
• Extensive experiments have been conducted to indicate the significance of our image-to-point pre-training. 2.