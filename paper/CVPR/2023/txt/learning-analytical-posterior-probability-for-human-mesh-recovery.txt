Abstract
Despite various probabilistic methods for modeling the uncertainty and ambiguity in human mesh recovery, their overall precision is limited because existing formulations for joint rotations are either not constrained to SO(3) or difficult to learn for neural networks. To address such an issue, we derive a novel analytical formulation for learn-ing posterior probability distributions of human joint rota-tions conditioned on bone directions in a Bayesian man-ner, and based on this, we propose a new posterior-guided framework for human mesh recovery. We demonstrate that our framework is not only superior to existing SOTA base-lines on multiple benchmarks but also flexible enough to seamlessly incorporate with additional sensors due to its
Bayesian nature. The code is available at https:// github.com/NetEase-GameAI/ProPose. 1.

Introduction
Human mesh recovery is a task of recovering body meshes and 3D joint rotations of human actors from im-ages, which has ubiquitous applications in animation pro-duction, sports analysis, etc. To achieve this goal, vari-ous approaches have been proposed in the computer vision community. Existing methods can be divided into two cate-gories, i.e., direct and indirect, respectively. Direct methods use neural networks to regress the rotations (e.g., axis an-gle [22], rotation matrix [34], 6D vector [29,37,74]) of each humanoid joint in an end-to-end way, while indirect meth-ods recover joint rotations based on some intermediately predicted proxies (e.g., 3D human keypoints [19, 36, 42], 2D heatmaps [52] or part segmentation [27]). However, both methods have obvious weaknesses. Generally, the es-timated poses from direct solutions are not so well-aligned with the images (Fig. 1(a)), because joint rotations are more difficult to regress compared with keypoints [19,36]. On the contrary, though indirect solutions tend to have better esti-mation precision, their performance heavily relies on the precision of the intermediate proxies and thus are vulner-able to noise and error in the predicted keypoints or part segmentation (Fig. 1(b)).
Figure 1. Comparisons of (a) the direct method [29], (b) the indi-rect method [36], and (c) our method.
To simultaneously achieve high precision and high ro-bustness, some probabilistic methods are developed, which, instead of seeking a unique solution, try to explicitly model the uncertainty of human poses by learning some kind of probability distribution. Prevalent ways of modeling the distribution include multivariate Gaussian distributions [48, 56], normalizing flows [31], and neural networks [51, 53].
In practice, these learned probability distributions can no-tably improve the estimation results in some extreme cases (e.g., under large occlusion), however, only minor differ-ences can be found in terms of the overall performances on large datasets. One reason is that these probability models cannot truly reflect the rotational uncertainty since they are not strictly constrained to SO(3). Recently, [55] proposes to adopt the matrix Fisher distribution over SO(3) [8, 25] to model the rotational uncertainty caused by depth ambi-guity. However, even with this mathematically-correct for-mulation, the actual performance does not improve much either, because the parameters of the matrix Fisher distribu-tion are not easy for deep neural networks to learn directly.
To address this problem, we propose a new learning-friendly and mathematically-correct formulation for learn-ing probability distributions for human mesh recovery. Our formulation is derived based on the facts that, (i) the joint rotations follow the matrix Fisher distribution over SO(3),
(ii) the unit directions of bones follow the von Mises-Fisher distribution [44], (iii) the bone direction can be viewed as the observation of joint rotation (i.e., the latent variable). It can be proven that the probability distributions of joint ro-tations conditioned on bone directions still follow the ma-trix Fisher distribution, which allows us to regress the pos-terior probability distribution of the 3D joint rotations in a Bayesian manner, and more importantly, in an analytical form. Moreover, we mathematically prove that the posterior probability of human joint rotations is more concentrated than the prior probability. Our experimental results demon-strate that such a characteristic makes the posterior proba-bility an easier form to learn (for neural networks) than its prior counterpart.
Apart from the theoretical contributions, we also pro-pose a new human mesh recovery framework that can uti-lize the learned analytical posterior probability. We demon-strate that this framework successfully achieves high preci-sion and high robustness at the same time, and outperforms existing SOTA baselines. Furthermore, our framework en-ables seamless integration with additional sensors that can yield directional/rotational observations (e.g., multi-view cameras, optical markers, IMUs) due to its Bayesian na-ture. Different from naive multi-sensor fusion algorithms (e.g., Kalman filter [21]) that typically perform fusion at the inference stage, our framework allows fusion in the training stage to learn the noise characteristics of sensors, and thus has the potential to produce better precision. We demon-strate that our fusion mechanism can achieve similar ef-fects to fusing the latent features from multiple sensor input branches, but is much more flexible since it does not require modification of the main backbone.
The key contributions of this paper are thereby:
• We derive a novel analytical formulation for learn-ing probability distributions for human joint rotations, and theoretically prove that such formulation allows the regression of posterior probability distribution in a
Bayesian manner.
• We propose a new framework for human mesh recov-ery by leveraging the learned analytical posterior prob-ability and show that this framework outperforms ex-isting SOTA baselines.
• We introduce a novel and flexible multi-sensor fusion mechanism that allows fusing different observations in the training stage. model [41, 54], optimization-based approaches [2, 9, 15, 51] fit the parameters via iteration while learning-based ap-proaches regress the parameters with neural networks. Our work follows the learning paradigm, therefore we here mainly review recent advances in learning-based methods.
Direct methods: Given images as input, this kind of ap-proach directly regresses the model parameters with neural networks. Different representations of rotation [22, 34, 74], supervision schemes [20, 29, 37] as well as temporal con-text [5, 13, 23, 26] are explored to improve performance.
However, the gap between the image space and the abstract parameter space of statistical models makes it difficult to generate well-aligned estimations.
Indirect methods:
Instead of regressing rotation repre-sentations from RGB images directly, plenty of works in-troduce proper intermediate or proxy representations, such as segmentation [27, 49, 68], IUV maps [64, 69, 70], key-points [6, 14, 36, 52] or surface landmarks [30, 32, 42], to guide the learning of neural networks efficiently. HybrIK
[36] decomposes the 3D rotation into solvable swings from 3D keypoints and extra predicted twists. PARE [27] learns to predict attention masks which are fused with image fea-ture maps to provide body part information. These solutions may achieve higher precision, but generating only determin-istic results and ignoring the uncertainty of estimation make them sensitive to noisy or erroneous proxy predictions.
Probabilistic methods: To deal with the uncertainty from occlusions or depth ambiguities, several works manage to produce multiple hypotheses [1] or a probability distri-I2L-MeshNet [48] predicts lixel-based 1D bution [53]. heatmaps for each human mesh vertex for uncertainty mod-eling. Sengupta et al. [56] assume simple multivariate
Gaussian distributions over the parameters of the human model. ProHMR [31] learns a distribution of plausible 3D poses represented by normalizing flows, which is more powerful and expressive than Gaussian distributions. Re-cently Sengupta et al. [55] further represent the essential distribution of human joint rotation over SO(3) by adopt-ing the matrix Fisher distribution [25], which can provide quantified uncertainty estimation. Despite a better explana-tion for ambiguities, the parameters of the above distribu-tion are not easy to learn, limiting their overall performance on complicated scenes. 2.