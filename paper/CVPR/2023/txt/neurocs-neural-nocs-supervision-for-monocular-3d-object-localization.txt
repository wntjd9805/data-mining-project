Abstract
Monocular 3D object localization in driving scenes is a crucial task, but challenging due to its ill-posed nature. Esti-mating 3D coordinates for each pixel on the object surface holds great potential as it provides dense 2D-3D geomet-ric constraints for the underlying PnP problem. However, high-quality ground truth supervision is not available in driving scenes due to sparsity and various artifacts of Li-dar data, as well as the practical infeasibility of collecting per-instance CAD models. In this work, we present Neu-rOCS, a framework that uses instance masks and 3D boxes as input to learn 3D object shapes by means of differentiable rendering, which further serves as supervision for learning dense object coordinates. Our approach rests on insights in learning a category-level shape prior directly from real driving scenes, while properly handling single-view ambi-guities. Furthermore, we study and make critical design choices to learn object coordinates more effectively from an object-centric view. Altogether, our framework leads to new state-of-the-art in monocular 3D localization that ranks 1st on the KITTI-Object [16] benchmark among published monocular methods. 1.

Introduction
Localization of surrounding vehicles in 3D space is an important problem in autonomous driving. While Lidar
[32, 59, 72] and stereo [32, 59, 72] methods have achieved strong performances, monocular 3D localization remains a challenge despite recent progress in the ﬁeld [4, 35, 43].
Monocular 3D object localization can be viewed as a form of 3D reconstruction, with the goal to estimate the 3D extent of an object from single images. While single-view 3D reconstruction is challenging due to its ill-posed nature, learned priors combined with differentiable rendering [65] have recently emerged as a powerful technique, which has the potential to improve 3D localization as well. Indeed, re-searchers [2, 31, 78, 79] have applied it as a means for object pose optimization in 3D localization. However, difﬁculties remain due to pose optimization being ambiguous under
Figure 1. NeurOCS learns category-level shape model in real driv-ing scenes to provide dense and clean NOCS supervision through differentiable rendering, leading to new state-of-the-art 3D object localization performance. challenging photometric conditions (such as textureless vehi-cle surfaces) and geometric occlusions ubiquitous in driving scenes. The question of how differentiable rendering may be best explored in 3D localization remains under-studied.
Our work proposes a framework to unleash its potential – in-stead of using differentiable rendering for pose optimization, we use it with annotated ground truth pose to provide high-quality supervision for image-based shape learning, leading to a new state-of-the-art in 3D localization performance.
Our framework relies on the machinery of Perspective-n-Point (PnP) [33] pose estimation, which uses 2D-3D con-straints to explicitly leverage geometric principles that lend itself well to generalization. In particular, learning 3D ob-ject coordinates for every visible pixel on the object surface, known as normalized object coordinate space (NOCS) [66], provides a dense set of constraints. However, despite NOCS-based pose estimation dominating indoor benchmarks [21], their use in real driving scenes has been limited primarily due to lack of supervision – it is nontrivial to obtain accurate per-instance reconstructions or CAD models in road scenes.
Lidar or its dense completion [23] are natural alternatives
as pseudo ground truth [7], but they are increasingly sparse or noisy on distant objects with reﬂective surfaces. Syn-thetic data is a potential source of supervision [78, 79], but is inherently restricted by domain gap to real scenes.
In this work, we propose NeurOCS that leverages neural rendering to obtain effective NOCS supervision. Firstly, we propose to learn category-level shape reconstruction from real driving scenes with object masks and 3D object boxes using Neural Radiance Field (NeRF) [44]. The shape re-construction is then rendered into NOCS maps that serve as the pseudo ground-truth for a network dedicated to regress
NOCS from images. Speciﬁcally, NeurOCS learns category-level shape representation as a latent grid [50, 51] with low-rank structure, consisting of a canonical latent grid plus several deformation bases to account for instance variations.
With single-view ambiguities handled by a KL regulariza-tion [26] and dense shape prior, we show that the NOCS supervision so obtained yields strong 3D localization per-formance, even when the shape model is trained without using Lidar data or any CAD models. Our NOCS supervi-sion is illustrated in Fig. 1 in comparison with Lidar and its dense completion. We also note that NeRF rendering is only required during training, without adding computational overhead to inference. We show NeurOCS is complementary to direct 3D box regression [43, 55], and their fusion further boosts the performance.
Further, we study crucial design choices in image-conditioned NOCS regression. For example, as opposed to learning NOCS in a scene-centric manner with the full image as network input, we learn in an object-centric view by crop-ping objects without scene context, which is demonstrated to especially beneﬁt the localization of distant or occluded objects. Our extensive experiments study key choices that en-able NeurOCS to achieve top-ranked accuracy for monocular 3D localization on the KITTI-Object benchmark [16].
In summary, our contributions include:
• We propose a framework to obtain neural NOCS supervi-sion through differentiable rendering of the category-level shape representation learned in real driving scenes.
• We drive the learning with deformable shape representa-tion as latent grids with careful regularizations, as well as effective NOCS learning from an object-centric view.
• Our insights translate to state-of-the-art performance which achieves a top rank in KITTI benchmark [16]. 2.