Abstract 1.

Introduction
We propose a method to reconstruct global human tra-jectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; meth-ods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven hu-man motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challeng-ing in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset
Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack.
Consider the video sequence in Figure 1. As human ob-servers, we can clearly perceive that the camera is following the athlete as he runs across the field. However, when this dynamic 3D scene is projected onto 2D images, because the camera tracks the athlete, the athlete appears to be at the center of the camera frame throughout the sequence — i.e. the projection only captures the net motion of the underlying human and camera trajectory. Thus, if we rely only on the person’s 2D motion, as many human video reconstruction methods do, we cannot recover their original trajectory in the world (Figure 1 bottom left). To recover the person’s 3D motion in the world (Figure 1 bottom right), we must also reason about how much the camera is moving.
We present an approach that models the camera motion to recover the 3D human motion in the world from videos in the wild. Our system can handle multiple people and re-constructs their motion in the same world coordinate frame, enabling us to capture their spatial relationships. Recovering the underlying human motion and their spatial relationships is a key step towards understanding humans from in-the-wild videos. Tasks, such as autonomous planning in environments with humans [50], or recognition of human interactions with
the environment [68] and other people [21, 37], rely on in-formation about global human trajectories. Current methods that recover global trajectories either require additional sen-sors, e.g. multiple cameras or depth sensors [11, 51], or dense 3D reconstruction of the environment [12, 31], both of which are only realistic in active or controlled capture settings. Our method acquires these global trajectories from videos in the wild, with no constraints on the capture setup, camera motion, or prior knowledge of the environment. Be-ing able to do this from dynamic cameras is particularly relevant with the emergence of large-scale egocentric video datasets [5, 9, 69].
To do this, given an input RGB video, we first estimate the relative camera motion between frames from the static scene’s pixel motion with a SLAM system [56]. At the same time, we estimate the identities and body poses of all detected people with a 3D human tracking system [45]. We use these estimates to initialize the trajectories of the humans and cameras in the shared world frame. We then optimize these global trajectories over multiple stages to be consistent with both the 2D observations in the video and learned priors about how human move in the world [46]. We illustrate our pipeline in Figure 2. Unlike existing works [11, 31], we optimize over human and camera trajectories in the world frame without requiring an accurate 3D reconstruction of the static scene. Because of this, our method operates on videos captured in the wild, a challenging domain for prior methods that require good 3D geometry, since these videos rarely contain camera viewpoints with sufficient baselines for reliable scene reconstruction.
We combine two main insights to enable this optimization.
First, even when the scene parallax is insufficient for accu-rate scene reconstruction, it still allows reasonable estimates of camera motion up to an arbitrary scale factor. In fact, in
Figure 2, the recovered scene structure for the input video is a degenerate flat plane, but the relative camera motion still explains the scene parallax between frames. Second, human bodies can move realistically in the world in a small range of ways. Learned priors capture this space of realistic human motion well. We use these insights to parameterize the camera trajectory to be both consistent with the scene parallax and the 2D reprojection of realistic human trajecto-ries in the world. Specifically, we optimize over the scale of camera displacement, using the relative camera estimates, to be consistent with the human displacement. Moreover, when multiple people are present in a video, as is often the case in in-the-wild videos, the motions of all the people further constrains the camera scale, allowing our method to operate on complex videos of people.
We evaluate our approach on EgoBody [69], a new dataset of videos captured with a dynamic (ego-centric) camera with ground truth 3D global human motion trajectory. Our approach achieves significant improvement upon the state-of-the-art method that also tries to recover the human mo-tion without considering the signal provided by the back-ground pixels [63]. We further evaluate our approach on
PoseTrack [1], a challenging in-the-wild video dataset origi-nally designed for tracking. To demonstrate the robustness of our approach, we provide the results on all PoseTrack validation sequences on our project page. On top of qualita-tive evaluation, since there are no 3D ground-truth labels in
PoseTrack, we test our approach through an evaluation on the downstream application of tracking. We show that the re-covered scaled camera motion trajectory can be directly used in the PHALP system [45] to improve tracking. The scaled camera enables more persistent 3D human registration in the 3D world, which reduces the re-identification mistakes. We provide video results and code at the project page. 2.