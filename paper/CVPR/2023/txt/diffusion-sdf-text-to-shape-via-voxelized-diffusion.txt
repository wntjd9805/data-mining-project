Abstract 1.

Introduction
With the rising industrial attention to 3D virtual mod-eling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue.
In this paper, we propose a new generative 3D model-ing framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape gen-eration, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To ad-dress this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate rep-resentations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net ar-chitecture that implants a local-focused inner network in-side the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations.
We extend our approach to further text-to-shape tasks in-cluding text-conditioned shape completion and manipula-tion. Experimental results show that Diffusion-SDF gen-erates both higher quality and more diversified 3D shapes that conform well to given text descriptions when compared to previous approaches. Code is available at: https:
//github.com/ttlmh/Diffusion-SDF.
â€ Corresponding author.
Exploring data representations for 3D shapes has been a fundamental and critical issue in 3D computer vision. Ex-plicit 3D representations including point clouds [38, 39], polygon meshes [17, 24] and occupancy voxel grids [9, 53] have been widely applied in various 3D downstream ap-plications [1, 35, 56]. While explicit 3D representations achieve encouraging performance, there are some primary limitations including not being suitable for generating wa-tertight surfaces (e.g. point clouds), or being subject to topo-logical constraints (e.g. meshes). On the other hand, im-plicit 3D representations have been widely studied more recently [3, 14, 37], with representative works including
DeepSDF [37], Occupancy Network [32] and IM-Net [8].
In general, implicit functions encode the shapes by the iso-surface of the function, which is a continuous field and can be evaluated at arbitrary resolution.
In recent years, numerous explorations have been con-ducted for implicit 3D generative models, which show promising performance on several downstream applications such as single/multi-view 3D reconstruction [23, 54] and shape completion [12, 34]. Besides, several studies have also explored the feasibility of directly generating novel 3D shapes based on implicit representations [15, 21]. How-ever, these approaches are incapable of generating specified 3D shapes that match a given condition, e.g. a short text describing the shape characteristics as shown in Figure 1.
Text-based visual content synthesis has the advantages of the flexibility and generality [41, 42]. Users may generate rich and diverse 3D shapes based on easily obtained natural language descriptors. In addition to generating 3D shapes directly based on text descriptions, manipulating 3D data with text guidance can be further utilized for iterative 3D synthesis and fine-grained 3D editing, which can be benefi-cial for non-expert users to create 3D visual content.
In the literature, there have been few attempts on the challenging task of text-to-shape generation based on im-plicit 3D representations [29, 34, 48]. For example, Au-toSDF [34] introduced a vector quantized SDF autoen-coder together with an autoregressive generator for shape generation. While encouraging progress has been made, the quality and diversity of generated shapes still requires improvement. The current approaches struggle to gener-ate highly diversified 3D shapes that both guarantee gen-eration quality and conform to the semantics of the input text. Motivated by the success of denoising diffusion mod-els in 2D image [13, 19, 36] and even explicit 3D point cloud [31, 58, 59] generation, we find that DMs achieve high-quality and highly diversified generation while being robust to model training. To this end, we aim to design an implicit 3D representation-based generative diffusion pro-cess for text-to-shape synthesis that can achieve better gen-eration flexibility and generalization performance.
In this paper, we propose the Diffusion-SDF framework for text-to-shape synthesis based on truncated signed dis-tance fields (TSDFs). Considering that 3D shapes share structural similarities at local scales, and the cubic data vol-ume of 3D voxels may lead to slow sampling speed for dif-fusion models, we propose a two-stage separated generation pipeline. First, we introduce a patch-based SDF autoen-coder that map the original signed distance fields into patch-independent local Gaussian latent representations. Sec-ond, we introduce the Voxelized Diffusion model that cap-tures the intra-patch information along with both patch-to-patch and patch-to-global relations. Specifically, we de-sign a novel UinU-Net architecture to replace the standard
U-Net [46] for the noise estimator in the reverse process.
UinU-Net implants a local-focused inner network inside the outer U-Net backbone, which takes into account the patch-independent prior of SDF representations to better re-construct local patch features from noise. Our work digs deeper into the further potential of diffusion model-based approaches towards text-conditioned 3D shape synthesis based on voxelized TSDFs. Experiments on the largest ex-isting text-shape dataset [6] show that our Diffusion-SDF approach achieves promising generation performance on text-to-shape tasks compared to existing state-of-the-art ap-proaches, in both qualitative and quantitative evaluations.
Strictly speaking, our approach employs a combined explicit-implicit representation in the form of voxelized signed distance fields. 2.