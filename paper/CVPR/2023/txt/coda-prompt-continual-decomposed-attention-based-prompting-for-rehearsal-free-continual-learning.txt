Abstract
Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel con-cepts from continuously shifting training data. Typical solu-tions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emer-gence of large-scale pre-trained vision transformer mod-els has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. How-ever, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sac-rificing new task accuracy, and inability to benefit from ex-panded parameter capacity. We instead propose to learn a set of prompt components which are assembled with input-conditioned weights to produce input-conditioned prompts, resulting in a novel attention-based end-to-end key-query scheme. Our experiments show that we outperform the cur-rent SOTA method DualPrompt on established benchmarks by as much as 4.5% in average final accuracy. We also outperform the state of art by as much as 4.4% accuracy on a continual learning benchmark which contains both class-incremental and domain-incremental task shifts, cor-responding to many practical settings. Our code is avail-able at https://github.com/GT-RIPL/CODA-Prompt 1.

Introduction
For a computer vision model to succeed in the real world, it must overcome brittle assumptions that the concepts it will encounter after deployment will match those learned a-priori during training. The real world is indeed dynamic and contains continuously emerging objects and categories.
Once deployed, models will encounter a range of differ-ences from their training sets, requiring us to continuously
Figure 1. Prior work [65] learns a pool of key-value pairs to select learnable prompts which are inserted into several layers of a pre-trained ViT (the prompting parameters are unique to each layer).
Our work introduces a decomposed prompt which consists of learnable prompt components that assemble to produce attention-conditioned prompts. Unlike prior work, ours is optimized in an end-to-end fashion (denoted with thick, green lines). update them to avoid stale and decaying performance.
One way to update a model is to collect additional train-ing data, combine this new training data with the old train-ing data, and then retrain the model from scratch. While this will guarantee high performance, it is not practical for large-scale applications which may require lengthy training times and aggressive replacement timelines, such as models for self-driving cars or social media platforms. This could incur high financial [26] and environmental [33] costs if the replacement is frequent. We could instead update the model by only training on the new training data, but this leads to a phenomenon known as catastrophic forgetting [46] where the model overwrites existing knowledge when learning the new data, a problem known as continual learning.
The most effective approaches proposed by the continual
learning community involve saving [50] or generating [55] a subset of past training data and mixing it with future task data, a strategy referred to as rehearsal. Yet many impor-tant applications are unable to store this data because they work with private user data that cannot be stored long term.
In this paper, we instead consider the highly-impactful and well-established setting of rehearsal-free continual learning [38, 56, 57, 65, 66]1, limiting our scope to strate-gies for continual learning which do not store training data. While rehearsal-free approaches have classically under-performed rehearsal-based approaches in this chal-lenging setting by wide-margins [57], recent prompt-based approaches [65, 66], leveraging pre-trained Vision trans-formers (ViT), have had tremendous success and can even outperform state-of-the-art (SOTA) rehearsal-based meth-ods. These prompting approaches boast a strong protection against catastrophic forgetting by learning a small pool of insert-able model embeddings (prompts) rather than mod-ifying vision encoder parameters directly. One drawback of these approaches, however, is that they cannot be opti-mized in an end-to-end fashion as they use a key and query to select a prompt index from the pool of prompts, and thus rely on a second, localized optimization to learn the keys because the model gradient cannot backpropagate through the key/query index selection. Furthermore, these meth-ods reduce forgetting by sacrificing new task accuracy (i.e., they lack sufficient plasticity). We show that expanding the prompt size does not increase the plasticity, motivating us to grow learning capacity from a new perspective.
We replace the prompt pool with a decomposed prompt that consists of a weighted sum of learnable prompt com-ponents (Figure 1). This decomposition enables higher prompting capacity by expanding in a new dimension (the number of components). Furthermore, this inherently en-courages knowledge re-use, as future task prompts will in-clude contributions from prior task components. We also introduce a novel attention-based component-weighting scheme, which allows for our entire method to be optimized in an end-to-end fashion unlike the existing works, increas-ing our plasticity to better learn future tasks. We boast sig-nificant performance gains on existing rehearsal-free con-tinual learning benchmarks w.r.t. SOTA prompting base-lines in addition to a challenging dual-shift benchmark con-taining both incremental concept shifts and covariate do-main shifts, highlighting our methodâ€™s impact and gener-ality. Our method improves results even under equivalent parameter counts, but importantly can scale performance by increasing capacity, unlike prior methods which quickly plateau. In summary, we make the following contributions: 1We focus on continual learning over a single, expanding classification head called class-incremental continual learning. This is different from the multi-task continual learning setting, known as task-incremental continual learning, where we learn separate classification heads for each task and the task label is provided during inference. [25, 61] 1. We introduce a decomposed attention-based prompt-ing for rehearsal-free continual learning, characterized by an expanded learning capacity compared to exist-ing continual prompting approaches. Importantly, our approach can be optimized in an end-to-end fashion, unlike the prior SOTA. 2. We establish a new SOTA for rehearsal-free continual learning on the well-established ImageNet-R [21, 65] and CIFAR-100 [32] benchmarks, beating the previous
SOTA method DualPrompt [65] by as much as 4.5%. 3. We evaluate on a challenging benchmark with dual distribution shifts (semantic and covariate) using the
ImageNet-R dataset, and again outperform the state of art, highlighting the real-world impact and generality of our approach. 2.