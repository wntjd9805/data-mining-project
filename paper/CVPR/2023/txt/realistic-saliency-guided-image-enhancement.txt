Abstract
Common editing operations performed by profes-sional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects.
These edits are challenging, requiring a delicate balance between manipulating the viewer’s attention while main-taining photo realism. While recent approaches can boast successful examples of attention attenuation or amplifica-tion, most of them also suffer from frequent unrealistic ed-its. We propose a realism loss for saliency-guided image en-hancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and ef-fectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations. 1.

Introduction
In everyday photography, the composition of a photo typically encompasses subjects on which the photographer intends to focus our attention, rather than other distracting things. When distracting things cannot be avoided, photog-raphers routinely edit their photos to de-emphasize them.
Conversely, when the subjects are not sufficiently visible, photographers routinely emphasize them. Among the most common emphasis and de-emphasis operations performed by professionals are the elementary ones: changing the sat-uration, exposure, or the color of each element. Although conceptually simple, these operations are challenging to ap-ply because they must delicately balance the effects on the viewer attention with photo realism.
To automate this editing process, recent works use saliency models as a guide [1, 2, 4, 8, 16, 17]. These saliency models [3, 7, 10, 14, 19] aim to predict the regions in the
image that catch the viewer’s attention, and saliency-guided image editing methods are optimized to increase or decrease the predicted saliency of a selected region. Optimizing solely based on the predicted saliency, however, often re-sults in unrealistic edits, as illustrated in Fig. 1. This issue results from the instability of saliency models under the im-age editing operations, as saliency models are trained on unedited images. Unrealistic edits can have low predicted saliency even when they are highly noticeable to human ob-servers, or vice versa. This was also noted by Aberman et al. [1], and is illustrated in Fig. 2.
Previous methods tried to enforce realism using adver-sarial setups [2, 4, 8, 17], GAN priors [1, 8], or cycle consis-tency [2] but with limited success (Fig. 1). Finding the exact point when an image edit stops looking realistic is challeng-ing. Rather than focusing on the entire image, in this work, we propose a method for measuring the realism of a local edit. To train our network, we generate realistic image ed-its by subtle perturbations to exposure, saturation, color or white balance, as well as very unrealistic edits by apply-ing extreme adjustments. Although our network is trained with only positive and negative examples at the extremes, we successfully learn a continuous measure of realism for a variety of editing operations as shown in Fig. 3.
We apply our realism metric to saliency-guided image editing by training the system to optimize the saliency of a selected region while being penalized for deviations from realism. We show that a combined loss allows us to enhance or suppress a selected region successfully while maintaining high realism. Our method can be also be applied to multiple regions in a photograph as shown in Fig. 1.
Evaluations with professional photographers and photo editors confirm our claim that we maintain high realism and succeed at redirecting attention in the edited photo. Further, our results are robust to different types of images including human faces, and are stable across different permutations of edit parameters. Taken together with our model size of 26Mb and run-time of 8ms, these results demonstrate that we have a more viable solution for broader use than the ap-proaches that are available for these tasks to date. 2.