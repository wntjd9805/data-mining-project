Abstract
The ability to discover abstract physical concepts and understand how they work in the world through observing lies at the core of human intelligence. The acquisition of this ability is based on compositionally perceiving the environ-ment in terms of objects and relations in an unsupervised manner. Recent approaches learn object-centric represen-tations and capture visually observable concepts of objects, e.g., shape, size, and location. In this paper, we take a step forward and try to discover and represent intrinsic physical concepts such as mass and charge. We introduce the PHYsi-cal Concepts Inference NEtwork (PHYCINE), a system that infers physical concepts in different abstract levels with-out supervision. The key insights underlining PHYCINE are two-fold, commonsense knowledge emerges with pre-diction, and physical concepts of different abstract levels should be reasoned in a bottom-up fashion. Empirical eval-uation demonstrates that variables inferred by our system work in accordance with the properties of the correspond-ing physical concepts. We also show that object representa-tions containing the discovered physical concepts variables could help achieve better performance in causal reasoning tasks, i.e., ComPhy. 1.

Introduction
Why do objects bounce off after the collision? Why do magnets attract or repel each other? Objects cover many complex physical concepts which define how they interact with the world [5]. Humans have the ability to discover abstract concepts about how the world works through just observation. In a preserved video, some concepts are obvi-ous in the visual appearances of objects, like location, size, velocity, etc, while some concepts are hidden in the behav-iors of objects. For example, intrinsic physical concepts
*Corresponding author.
⋆Equal contribution
Figure 1. Given a video from the ComPhy [5] dataset, PHYCINE decomposes the scene into multi-object representations that con-tain physical concepts of different abstraction levels, including vi-sual attributes, dynamics, mass, and charge. (shown in different colors). like mass and charge, are unobservable from static scenes and can only be discovered from the object dynamics. Ob-jects carrying the same or opposite charge will exert a re-pulsive or attractive force on each other. After a collision, the lighter object will undergo larger changes in its motion compared with the massive one. How can machines learn to reveal and represent such common sense knowledge?
We believe the answer lies in two key steps: decomposing the world into object-centric representations, and building a predictive model with abstract physical concepts as latent variables to handle uncertainty.
Object-centric representation learning [3,9,18,22,23,29] aims at perceiving the world in a structured manner to im-prove the generalization ability of intelligent systems and achieve higher-level cognition. VAE [13]-based models,
like IODINE [9] learn disentangled features that separate interpretable object properties (e.g., shape, color, location) in a common format. However, abstract concepts like mass and charge, can not be distilled by generative models since building object-level representations does not necessarily model these higher-level-abstracted physics. CPL [5] suc-cessfully learns high-level concepts with graph networks, but it relies on supervision signals from the ground-truth object-level concept labels. There are also several studies investigating the effectiveness of object-centric representa-tions in learning predictive models to solve predicting and planning tasks [11, 14, 23, 24]. Nevertheless, to our knowl-edge, there is no work yet trying to discover and represent object-level intrinsic physical concepts in an unsupervised manner.
The idea that abstract concepts can be learned through prediction has been formulated in various ways in cognitive science, neuroscience, and AI over several decades [19,20].
Intuitively, physics concepts such as velocity, mass, and charge, may emerge gradually by training the system to perform long-term predictions at the object representation level [16]. Through predictions at increasingly long-time scales, more and more complex concepts about how the world works may be acquired in a bottom-up fashion. In this paper, we focus on the main challenge: enabling the model to represent and disentangle the unfolded concepts.
We follow common sense: with a neural physics engine, if the prediction of an object trajectory fails, there must be physical concepts that have not been captured. Therefore, a latent variable that successfully models the uncertainty of prediction defines a new physical concept, and a better physical engine can be built. Following this idea, we cate-gorize physical concepts into three levels of abstraction: ex-trinsic concepts, dynamic concepts, and intrinsic concepts.
Firstly, the extrinsic properties (e.g., color, shape, material, size, depth, location) can be referred to as object contexts belonging to the lowest level of abstraction, and a percep-tion module can directly encode the contexts. Secondly, the dynamic properties (e.g., velocity) in the middle level are hidden in the temporal and spatial relationships of visual features and should be inferred from short-term prediction.
Thirdly, intrinsic properties like mass and charge can nei-ther be directly observed nor inferred from short-term pre-diction. They can only be inferred by analyzing the way how objects exert force on each other. For example, infer-ring mass needs the incorporation of a collision event, and inferring charge needs to observe the change of object dy-namics, which depends on a long-term observation or pre-diction.
In this work, we build a system called PHYsical Con-cepts Inference NEtwork (PHYCINE). In the system, there are features arranged in a bottom-up pyramid that represents physical concepts in different abstraction levels. These fea-tures cooperatively perform reconstruction and prediction to interpret the observed world. Firstly, the object context features reconstruct the observed image with a generative model. Secondly, object dynamics features predict the next-step object contexts by learning a state transition function.
Finally, the mass and charge features model the interaction between objects. PHYCINE uses a relation module to cal-culate pair-wise forces for all entities, and adaptively learn the variables that represent object mass and charge with proper regularization. During training, all representations are randomly initialized and iteratively refined using gradi-ent information about the evidence lower bound (ELBO) obtained by the current estimate of the parameters. The model can be trained using only raw input video data in an end-to-end fashion. As shown in Figure 1, taking a raw video as input, PHYCINE not only extracts extrinsic ob-ject contexts (i.e., size, shape, color, location, material), but also infers more abstract concepts (i.e., object dynamics, mass, and charge). In our experiments, we demonstrate the model’s ability to discover and disentangle physical con-cepts, which can be used to solve downstream tasks. We evaluate the learned representation on ComPhy, a causal reasoning benchmark.
Our main contributions are as follows: (i) We challenge a problem of physical concept discovery by observing raw videos and successfully discovering intrinsic concepts of velocity, mass, and charge. (ii) We introduce a framework
PHYCINE, a hierarchical object-centric predictive model that infers physical concepts from low (e.g., color, shape) to high (e.g., mass, charge) abstract levels, leading to dis-entangled object-level physical representations. (iii) We demonstrate the effectiveness of the representation learned by PHYCINE on ComPhy. 2.