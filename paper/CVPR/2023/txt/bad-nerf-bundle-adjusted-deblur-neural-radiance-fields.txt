Abstract
Neural Radiance Fields (NeRF) have received consider-able attention recently, due to its impressive capability in photo-realistic 3D reconstruction and novel view synthesis, given a set of posed camera images. Earlier work usually assumes the input images are of good quality. However, im-age degradation (e.g. image motion blur in low-light con-ditions) can easily happen in real-world scenarios, which would further affect the rendering quality of NeRF. In this paper, we present a novel bundle adjusted deblur Neural
Radiance Fields (BAD-NeRF), which can be robust to se-vere motion blurred images and inaccurate camera poses.
Our approach models the physical image formation process of a motion blurred image, and jointly learns the parameters of NeRF and recovers the camera motion trajectories dur-ing exposure time. In experiments, we show that by directly modeling the real physical image formation process, BAD-NeRF achieves superior performance over prior works on both synthetic and real datasets. Code and data are avail-able at https://github.com/WU-CVGL/BAD-NeRF. 1.

Introduction
Acquiring accurate 3D scene geometry and appearance from a set of 2D images has been a long standing problem
†Corresponding author. in computer vision. As a fundamental block for many vi-sion applications, such as novel view image synthesis and robotic navigation, great progress has been made over the last decades. Classic approaches usually represent the 3D scene explicitly, in the form of 3D point cloud [8, 52], tri-angular mesh [4, 5, 10] or volumetric grid [31, 45]. Recent advancements in implicit 3D representation by using a deep neural network, such as Neural Radiance Fields (NeRF)
[27], have enabled photo-realistic 3D reconstruction and novel view image synthesis, given well posed multi-view images.
NeRF takes a 5D vector (i.e. for spatial location and viewing direction of the sampled 3D point) as input and predicts its radiance and volume density via a multilayer perceptron. The corresponding pixel intensity or depth can then be computed by differentiable volume rendering
[19, 25]. While many methods have been proposed to fur-ther improve NeRF’s performance, such as rendering effi-ciency [11,28], training with inaccurate poses [20] etc., lim-ited work has been proposed to address the issue of training with motion blurred images. Motion blur is one of the most common artifacts that degrades images in practical appli-cation scenarios. It usually occurs in low-light conditions where longer exposure times are necessary. Motion blurred images would bring two main challenges to existing NeRF training pipeline: a) NeRF usually assumes the rendered
infinitesimal exposure time), motion image is sharp (i.e. blurred image thus violates this assumption; b) accurate camera poses are usually required to train NeRF, however, it is difficult to obtain accurate poses from blurred images only, since each of them usually encodes information of the motion trajectory during exposure time. On the other hand, it is also challenging itself to recover accurate poses (e.g., via COLMAP [41]) from a set of motion blurred images, due to the difficulties of detecting and matching salient key-points. Combining both factors would thus further degrade
NeRF’s performance if it is trained with motion blurred im-ages.
In order to address those challenges, we propose to in-tegrate the real physical image formation process of a mo-tion blurred image into the training of NeRF. We also use a linear motion model in the SE(3) space to represent the camera motion trajectory within exposure time. During the training stage, both the network weights of NeRF and the camera motion trajectories are estimated jointly. In partic-ular, we represent the motion trajectory of each image with both poses at the start and end of the exposure time respec-tively. The intermediate camera poses within exposure time can be linearly interpolated in the SE(3) space. This as-sumption holds in general since the exposure time is typ-ically small. We can then follow the real physical image formation model of a motion blurred image to synthesize the blurry images. In particular, a sequence of sharp im-ages along the motion trajectory within exposure time can be rendered from NeRF. The corresponding motion blurred image can then be synthesized by averaging those virtual sharp images. Both NeRF and the camera motion trajec-tories are estimated by minimizing the difference between the synthesized blurred images and the real blurred images.
We refer this modified model as BAD-NeRF, i.e. bundle adjusted deblur NeRF.
We evaluate BAD-NeRF with both synthetic and real datasets. The experimental results demonstrate that BAD-NeRF achieves superior performance compared to prior state of the art works (e.g. as shown in Fig. 1), by explicitly modeling the image formation process of the motion blurred image. In summary, our contributions are as follows:
• We present a photo-metric bundle adjustment formula-tion for motion blurred images under the framework of
NeRF, which can be potentially integrated with other vision pipelines (e.g. a motion blur aware camera pose tracker [21]) in future.
• We show how this formulation can be used to acquire high quality 3D scene representation from a set of mo-tion blurred images.
• We experimentally validate that our approach is able to deblur severe motion blurred images and synthesize high quality novel view images. 2.