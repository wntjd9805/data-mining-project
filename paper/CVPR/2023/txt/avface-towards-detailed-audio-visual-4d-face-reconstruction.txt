Abstract
In this work, we present a multimodal solution to the problem of 4D face reconstruction from monocular videos. 3D face reconstruction from 2D images is an under-constrained problem due to the ambiguity of depth. State-of-the-art methods try to solve this problem by leveraging visual information from a single image or video, whereas 3D mesh animation approaches rely more on audio. How-ever, in most cases (e.g. AR/VR applications), videos in-clude both visual and speech information. We propose AV-Face that incorporates both modalities and accurately re-constructs the 4D facial and lip motion of any speaker, with-out requiring any 3D ground truth for training. A coarse stage estimates the per-frame parameters of a 3D mor-phable model, followed by a lip refinement, and then a fine stage recovers facial geometric details. Due to the temporal audio and video information captured by transformer-based modules, our method is robust in cases when either modality is insufficient (e.g. face occlusions). Extensive qualitative and quantitative evaluation demonstrates the superiority of our method over the current state-of-the-art. 1.

Introduction
Reconstructing the 4D geometry of the human face has been a long standing research problem in computer vi-sion and graphics. Accurate spatio-temporal (4D) face re-construction has extensive applications in AR/VR, video the movie industry etc. games, virtual communication,
However, recovering the per-frame 3D head pose and fa-cial geometry from 2D images is an ill-posed problem due to the ambiguity of depth. Current approaches are largely based on 3D morphable models (3DMMs). Usually, they take a single image [18, 22] or video [60] as input and pre-dict the 3DMM parameters. Some have also tried to pre-dict additional geometric facial details, using the 3DMM fitting as prior [13, 18, 22]. However, most of these video-only methods are either speaker-specific [13,23,29], requir-ing to overfit to a specific speaker to recover their facial details, or fail to accurately capture fine details, like wrin-In addition, they cannot handle kles and lip movements. face occlusions, since they solely rely on the visual input.
On the other hand, audio-driven 3D mesh animation ap-proaches [17,21,52] learn better lip motion, but they require 4D ground truth scans for training, which are rare and ex-pensive to capture. Furthermore, such audio-only methods cannot capture any speaker-specific characteristics or fa-cial expressions, as they do not use any visual information.
There is limited work in audio-visual 4D face reconstruc-tion [1, 15, 44], but these methods also require 4D ground truth scans and do not recover any facial geometric details.
In this work, we propose AVFace that learns to recon-struct detailed 4D face geometry from monocular talking face videos, leveraging both audio and video modalities.
Without requiring any 3D ground truth scans, it can recover
accurate 4D facial and lip motion for any speaker. A coarse stage estimates a coarse geometry per frame, based on a 3DMM and using both image and speech features. Then, a SIREN MLP [58] further improves the lip position, by learning an implicit representation of the lip shape condi-tioned on speech. Finally, a fine stage recovers geometric facial details, guided by pseudo-ground truth face normals and producing a high-fidelity reconstruction of the input speaker’s face per frame. Due to the temporal audio and video information captured by transformer-based modules, our method is robust in cases when either modality is insuf-ficient (e.g. face occlusions). To better handle such hard cases that are frequent in talking face videos, we further fine-tune our coarse stage with synthetic face occlusions.
In brief, the contributions of our work are as follows:
• We propose AVFace, a novel audio-visual method for detailed 4D face reconstruction, that follows a coarse-to-fine optimization approach, trained only on monoc-ular talking face videos without any 3D ground truth.
• We introduce an audio-driven lip refinement network, and a fine stage guided by pseudo-ground truth face normals to accurately recover fine geometric details.
• Our temporal modeling, along with fine-tuning on syn-thetic face occlusions, makes our network robust to cases when either modality is insufficient. 2.