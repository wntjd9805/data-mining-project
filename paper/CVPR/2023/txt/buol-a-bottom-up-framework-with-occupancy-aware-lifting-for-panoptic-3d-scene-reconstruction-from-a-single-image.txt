Abstract
Understanding and modeling the 3D scene from a single image is a practical problem. A recent advance proposes a panoptic 3D scene reconstruction task that performs both 3D reconstruction and 3D panoptic segmentation from a single image. Although having made substantial progress, recent works only focus on top-down approaches that fill 2D instances into 3D voxels according to estimated depth, which hinders their performance by two ambiguities. (1) instance-channel ambiguity: The variable ids of instances in each scene lead to ambiguity during filling voxel chan-nels with 2D information, confusing the following 3D re-finement. (2) voxel-reconstruction ambiguity: 2D-to-3D lifting with estimated single view depth only propagates 2D information onto the surface of 3D regions, leading to ambi-guity during the reconstruction of regions behind the frontal view surface. In this paper, we propose BUOL, a Bottom-Up framework with Occupancy-aware Lifting to address the two issues for panoptic 3D scene reconstruction from a sin-gle image. For instance-channel ambiguity, a bottom-up framework lifts 2D information to 3D voxels based on de-terministic semantic assignments rather than arbitrary in-stance id assignments. The 3D voxels are then refined and grouped into 3D instances according to the predicted 2D instance centers. For voxel-reconstruction ambiguity, the estimated multi-plane occupancy is leveraged together with depth to fill the whole regions of things and stuff. Our method shows a tremendous performance advantage over state-of-the-art methods on synthetic dataset 3D-Front and real-world dataset Matterport3D. Code and models will be released. 1.

Introduction
Figure 1. Comparison of the feature lifting from 2D to 3D. (a)
General Top-down approaches: Feature lifting by depth with the two randomized instance assignments in the top-down framework.
The predicted 2D instance masks {i1, i2, i3} are lifted to only the surface of 3D instances at variable channels, such as {i1, i3, i6} or {i3, i6, i1}, which results in instance-channel ambiguity and (b) Our BUOL: Occupancy-voxel-reconstruction ambiguity. aware lifting with the deterministic semantic assignment in the bottom-up framework. The predicted 2D semantic category maps
{s1, s2, s6, s7} are lifted to the whole regions of things (s1, s2) and stuff (s6, s7), and the voxels are finally grouped into 3D in-stances {i1, i2, i3} by corresponding 2D instance centers.
Joint learning of 3D reconstruction and perception is a challenging and practical problem for various applica-tions. Existing works focus on combining 3D reconstruc-*Intern at Shanghai AI Laboratory.
â€ Corresponding author. tion with semantic segmentation [26, 27] or instance seg-mentation [11, 23, 28]. Recently, a pioneer work [6] unifies the tasks of 3D reconstruction, 3D semantic segmentation, and 3D instance segmentation into panoptic 3D scene re-construction from a single RGB image, which assigns a cat-egory label (i.e. a thing category with easily distinguishable edges, such as tables, or a stuff category with indistinguish-able edges, such as wall) [22] and an instance id (if the voxel belongs to a thing category) to each voxel in the 3D volume of the camera frustum.
Dahnert et al. [6] achieve this goal in a top-down pipeline that lifts 2D instance masks to channels of 3D voxels and predicts the panoptic 3D scene reconstruction in the follow-ing 3D refinement stage. Their method first estimates 2D instance masks and the depth map. The 2D instance masks are then lifted to fill voxel channels on the front-view sur-face of 3D objects using the depth map. Finally, a 3D model is adopted to refine the lifted 3D surface masks and attain panoptic 3D scene reconstruction results of all voxels.
After revisiting the top-down panoptic 3D scene recon-struction framework, we find two crucial limitations which hinder its performance, as shown in Figure 1(a). First, instance-channel ambiguity: the number of instances varies in different scenes. Thus lifting 2D instance masks to fill voxel channels can not be achieved by a determinis-tic instance-channel mapping function. Dahnert et al. [6] propose to utilize a randomized assignment that randomly assigns instance ids to the different channels of voxel fea-tures. For example, two possible random assignments are shown in Figure. 1(a), where solid and dashed arrow lines with the same color indicate a 2D mask is assigned to differ-ent voxel feature channels. This operator leads to instance-channel ambiguity, where an instance id may be assigned to an arbitrary channel, confusing the 3D refinement model. In addition, we experimentally discuss the impact of different instance assignments (e.g., random or sorted by category) on performance in Section 4. Second, voxel reconstruc-tion ambiguity: 2D-to-3D lifting with depth from a single view can only propagate 2D information onto the frontal surface in the camera frustum, causing ambiguity during the reconstruction of regions behind the frontal surface. As shown by dashed black lines in the right of Figure 1(a), the 2D information is only propagated to the frontal surface of initialized 3D instance masks, which is challenging for 3D refinement model to reconstruct the object regions behind the frontal surface accurately.
In this paper, we propose BUOL, a Bottom-Up frame-work with Occupancy-aware Lifting to address the above two ambiguities for panoptic 3D scene reconstruction from a single image.
For instance-channel ambiguity, our bottom-up framework lifts 2D semantics to 3D semantic voxels, as shown in Figure. 1(b). Compared to the top-down methods shown in Figure. 1(a), instance-channel ambigu-ity is tackled by a simple deterministic assignment map-ping from semantic category ids to voxel channels. The voxels are then grouped into 3D instances according to the predicted 2D instance centers. For voxel-reconstruction ambiguity, as shown in Figure. 1(b), the estimated multi-plane occupancy is leveraged together with depth by our occupancy-aware lifting mechanism to fill regions inside the things and stuff besides front-view surfaces for accurate 3D refinement.
Specifically, our framework comprises a 2D priors stage, a 2D-to-3D lifting stage, and a 3D refinement stage.
In the 2D priors stage, the 2D model predicts 2D semantic map, 2D instance centers, depth map, and multi-plane oc-cupancy. The multi-plane occupancy presents whether the plane at different depths is occupied by 3D things or stuff.
In the 2D-to-3D lifting stage, leveraging estimated multi-plane occupancy and depth map, we lift 2D semantics into deterministic channels of 3D voxel features inside the things and stuff besides the front-view surfaces. In the 3D refine-ment stage, we predict dense 3D occupancy in each voxel for reconstruction. Meanwhile, the 3D semantic segmen-tation is predicted for both the thing and stuff categories.
The 3D offsets towards the 2D instance centers are also estimated to identify voxels belonging to 3D objects. The ground truth annotations of 3D panoptic reconstruction, i.e., 3D instance/semantic segmentation masks and dense 3D oc-cupancy, can be readily converted to 2D instance center, 2D semantic segmentation, depth map, multi-plane occupancy, and 3D offsets for our 2D and 3D supervised learning. Dur-ing inference, we assign instance ids to 3D voxels occu-pied by thing objects based on 2D instance centers and 3D offsets, attaining final panoptic 3D scene reconstruction re-sults.
Extensive experiments show that the proposed bottom-up framework with occupancy-aware lifting outperforms prior competitive approaches. On the pre-processed 3D-Front [10] and Matterport3D [2], our method achieves
+11.81% and +7.46% PRQ (panoptic reconstruction qual-ity) over the state-of-the-art method [6], respectively. 2.