Abstract
The objective of this paper is an automatic Audio De-scription (AD) model that ingests movies and outputs AD in text form. Generating high-quality movie AD is challeng-ing due to the dependency of the descriptions on context, and the limited amount of training data available. In this work, we leverage the power of pretrained foundation mod-els, such as GPT and CLIP, and only train a mapping net-work that bridges the two models for visually-conditioned text generation.
In order to obtain high-quality AD, we make the following four contributions: (i) we incorporate context from the movie clip, AD from previous clips, as well as the subtitles; (ii) we address the lack of training data by pretraining on large-scale datasets, where visual or con-textual information is unavailable, e.g. text-only AD with-out movies or visual captioning datasets without context; (iii) we improve on the currently available AD datasets, by removing label noise in the MAD dataset, and adding char-acter naming information; and (iv) we obtain strong results on the movie AD task compared with previous methods. 1.

Introduction
That of all the arts, the most important for us is the cinema.
Vladimir Lenin
One of the long-term aims of computer vision is to un-derstand long-form feature films. There has been steady progress towards this aim with the identification of charac-ters by their face and voice [12, 15, 25, 29, 79], the recogni-tion of their actions and inter-actions [38,50,60,85], of their relationships [37], and 3D pose [61]. However, this is still a long way away from story understanding. Movie Audio De-scription (AD), the narration describing visual elements in movies, provides a means to evaluate current movie under-standing capabilities. AD was developed to aid visually im-paired audiences, and is typically generated by experienced annotators. The amount of AD on the internet is growing
∗: equal contribution. †: also at Google Research
Figure 1. Movie audio description (AD) consists of sentences describing movies for the visually impaired. Note how it is heav-ily influenced by various types of context – the visual frames, the previous AD, and the subtitles of the movie. due to more societal support for visually impaired commu-nities and its inclusion is becoming an emerging legal re-quirement.
AD differs from image or video captioning in several sig-nificant respects [67], bringing its own challenges. First,
AD provides dense descriptions of important visual ele-ments over time. Second, AD is always provided on a sep-arate soundtrack to the original audio track and is highly complementary to it. It is complementary in two ways: it does not need to provide descriptions of events that can be understood from the soundtrack alone (such as dialogue and ambient sounds), and it is constrained in time to intervals that do not overlap with the dialogue. Third, unlike dense video captioning, AD aims at storytelling; therefore, it typi-cally includes factors like a character’s name, emotion, and action descriptions.
In this work, our objective is automatic AD generation – a model that takes continuous movie frames as input and outputs AD in text form. Specifically, we generate text given a temporal interval of an AD, and evaluate its qual-ity by comparing with the ground-truth AD. This is a rela-tively unexplored task in the vision community with previ-ous work targeting ActivityNet videos [88], a very different domain to long-term feature films with storylines, and the
LSMDC challenge [68], where the descriptions and charac-ter names are treated separately.
As usual, one of the challenges holding back progress
is the lack of suitable training data. Paired image-text or video-text data that is available at scale, such as alt-text [63,72] or stock footage with captions [7], does not gen-eralize well to the movie domain [8]. However, collecting high-quality data for movie understanding is also difficult.
Researchers have tried to hire human annotators to describe video clips [21, 36, 90] but this does not scale well. Movie scripts, books and plots have also been used as learning sig-nals [12, 75, 97] but they do not ground on vision closely and are limited in number.
In this paper we address the AD and training data chal-lenges by – Spoiler Alert – developing a model that uses temporal context together with a visually conditioned gen-erative language model, while providing new and cleaner sources of training data. To achieve this, we leverage the strength of large-scale language models (LLMs), like
GPT [64], and vision-language models, like CLIP [63], and integrate them into a video captioning pipeline that can be effectively trained with AD data.
Our contributions are the following: (i) inspired by Clip-Cap [52] we propose a model that is effectively able to leverage both temporal context (from previously generated
AD) and dialogue context (in particular the names of char-acters) to improve AD generation. This is done by bridg-ing foundation models with lightweight adapters to inte-grate both types of context; (ii) we address the lack of large-scale training data for AD by pretraining components of our model on partially missing data which are typically avail-able in large quantities e.g. text-only AD without movie frames, or visual captioning datasets without multiple sen-tences as context; (iii) we propose an automatic pipeline for collecting AD narrations at scale using speaker-based sep-aration; and finally (iv) we show promising results on au-tomatic AD, as seen from both qualitative and quantitative evaluations, and also achieve impressive zero-shot results on the LSMDC multi-description benchmark comparable to the finetuned state-of-the-art. 2.