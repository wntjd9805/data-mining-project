Abstract
We present Galactic, a large-scale simulation and reinforcement-learning (RL) framework for robotic mobile manipulation in indoor environments. Specifically, a Fetch robot (equipped with a mobile base, 7DoF arm, RGBD camera, egomotion, and onboard sensing) is spawned in a home environment and asked to rearrange objects – by navigating to an object, picking it up, navigating to a target location, and then placing the object at the target location.
Galactic is fast. In terms of simulation speed (rendering
+ physics), Galactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which is 54x faster than Habitat 2.0 [55] (7699 SPS). More importantly, Galactic was designed to optimize the entire rendering+physics+RL interplay since any bottleneck in the interplay slows down training. In terms of simulation+RL speed (rendering + physics + inference + learning), Galactic achieves over 108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS).
These massive speed-ups not only drastically cut the wall-clock training time of existing experiments, but also unlock an unprecedented scale of new experiments. First, Galactic can train a mobile pick skill to >80% accuracy in under 16 minutes, a 100x speedup compared to the over 24 hours it takes to train the same skill in Habitat 2.0. Second, we use Galactic to perform the largest-scale experiment to date for rearrangement using 5B steps of experience in 46 hours, which is equivalent to 20 years of robot experience. This scaling results in a single neural network composed of task-agnostic components achiev-ing 85% success in GeometricGoal rearrangement, compared to 0% success reported in Habitat 2.0 for the same approach.
The code is available at github.com/facebookresearch/galactic . 1.

Introduction
The scaling hypothesis posits that as general-purpose neural architectures are scaled to larger model sizes and training experience ever increasingly sophisticated intelligent behavior
*Equal contribution emerges. These so-called ‘scaling laws’ appear to be driving many recent advances in AI, leading to massive improvements in computer vision [12, 42, 44] and natural language process-ing [3, 40]. But what about embodied AI? We contend that embodied AI experiments need to be scaled by several orders of magnitude to become comparable to the experiment scales of CV and NLP, and likely even further beyond given the multi-modal interactive long-horizon nature of embodied AI problems.
Consider one of the largest-scale experiments in vision-and-language: the CLIP [39] model was trained on a dataset of 400 million images (and captions) for 32 epochs, giving a total of approximately 12 Billion frames seen during training. In contrast, most navigation experiments in embodied AI involve only 100-500M frames of experience [5, 30, 68]. The value of large scale training in embodied AI was demonstrated by Wi-jmans et al. [60] by achieving near-perfect performance on the
PointNav task through scaling to 2.5 billion steps of experience.
Since then, there have been several other examples of scaling experiments to 1B steps in navigation tasks [41, 65]. Curiously, as problems become more challenging, going from navigation to mobile manipulation object rearrangement, the scale of experiments have become smaller. Examples of training scales in rearrangement policies include [19, 55, 59] training skills in Habitat 2.0 for 100-200M steps and [58] in Visual Room
Rearrangement for 75M steps. In this work, we demonstrate for the first time scaling training to 5 billion frames of experience for rearrangement in visually challenging environments.
Why is large-scale learning in embodied AI hard? Unlike in CV or NLP, data in embodied AI is collected through an agent acting in environments. This data collection process involves policy inference to compute actions, physics to update the world state, rendering to compute agent observations, and reinforcement learning (RL) to learn from the collected data.
These separate systems for rendering, physics, and inference are largely absent in CV and NLP.
We present Galactic, a large-scale simulation+RL framework for robotic mobile manipulation in indoor environments.
Specifically, we study and simulate the task of GeometricGoal
Res Sensors Train SPS
Sim SPS
Photoreal Physics 18,900 72
RGB
Arcade RL Sims
VizDoom [26, 37]
Physics-only Sims
Isaac Gym (Shadow Hand) [29]
Brax (Grasp) [16]
ADPL Humanoid [64]
EAI Sims iGibson [28, 51] –
RGB
AI2-THOR [11] 72
Megaverse [37] 42,700
RGB 100
× 128 64
RGB
LBS [48] 13,300 33,700
× 38,100
× 28
Device 1x RTX 3090
Device 1x A100 4x2 TPU v3 1x TITAN X
Device 1x GPU 128
Res Sensors Train SPS
N/A 150,000
N/A
N/A 1,000,000 10,000,000
N/A
N/A
N/A 40,960
Res. Sensors Train SPS
Sim SPS – 144,035
Sim SPS 8x RTX 2080TI 224x224
RGB 300
⇡ 2,860 327,000
× 1x RTX 3090 8x RTX 2080TI 1x RTX 3090 1x Tesla V100 8x Tesla V100 128 128×72 64 64×64 64×64
RGB 134,000 1,148,000
RGB
RGB 9,000 37,800 – –
Habitat 2.0 [55, 59] 128 RGBD 367
×
Galactic (Ours) 1,660 1x RTX 2080 Ti 128 8x RTX 2080 Ti 128×128 RGBD 1x Tesla V100 128×128 RGBD 8x Tesla V100 128×128 RGBD 1x Tesla V100 128×128 RGBD 14,807 8x Tesla V100 128×128 RGBD 108,806 1,243 128 945 7,699 2,790 17,465 54,966 421,194 6 3
Photoreal Physics 6 6 6 4 4 4
Photoreal Physics 4 4 6 4 4 4 4 3 6 4 4 4
Table 1. High-level throughput comparison of different simulators. Steps-per-second (SPS) numbers are taken from source publications, and we don’t control for all performance-critical variables including scene complexity and policy architecture. Comparisons should focus on orders of magnitude. We show Sim SPS (physics and/or rendering) and training SPS (physics and/or rendering, inference and learning) for various physics-only and Embodied AI simulators. We also describe VizDoom, an arcade simulator which has served as a classic benchmarks for RL algorithms due to its speed. The 3 for Megaverse and VizDoom represent physics for abstract, non-realistic environments. Among EAI simulators faster than the existing fastest simulator, Habitat 2.0 that support realistic environments (photorealism and realistic physics), Galactic is 80 (108,806 vs 1243 training SPS for 8 GPUs). Galactic’s training speed is comparable to LBS, Megaverse, and VizDoom, even though LBS doesn’t simulate physics and neither Megaverse nor VizDoom support realistic environments. We also compare to GPU-based physics simulators: while these are generally faster than Galactic, they entirely omit rendering, which significantly reduces their compute requirements. For Galactic, we observe near-linear scaling from 1 to 8 GPUs, with a 7.3x speedup.
⇥
Rearrangement [2], where a Fetch robot [43] equipped with a mobile base, 7DoF arm, RGBD camera, egomotion, and proprioceptive sensing must rearrange objects in the
ReplicaCAD [55] environments by navigating to an object, picking up the object, navigating to a target location, and then placing the object at the target location.
Galactic is fast. In terms of simulation speed (rendering
+ physics), Galactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which is 54x faster than Habitat 2.0 [55] (7699 SPS). More importantly, Galactic was designed to optimize the entire rendering+physics+RL interplay since any bottleneck in the interplay slows down training. In terms of simulation+RL speed (rendering + physics + inference + learning), Galactic achieves over 108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS).
Our key technical innovations are: (1) integration of
CPU-based batch physics with GPU-heavy batch rendering and inference, and (2) a new, approximate kinematic simulation targeted at EAI rearrangement tasks. Compared to a “one simulator, one environment, one process” paradigm, batching lower yields massive speedups due to memory savings, communication overhead, and greater parallelism. Meanwhile, we leverage our physics approximations and the reduced complexity of kinematic simulation to reduce our CPU compute and yield further speedups.
These massive speed-ups not only drastically cut the wall-clock training time of existing experiments, but also unlock an unprecedented scale of new experiments. First, Galactic can train a mobile pick skill to >80% accuracy in under 16 minutes, a 100x speedup compared to the over 24 hours it takes to train the same skill in Habitat 2.0. Second, we use Galactic to perform the largest-scale experiment to date for rearrangement using 5B steps of experience in 46 hours, which is equivalent to 20 years of robot experience (assuming 8 actions per second). This scal-ing results in a single neural network composed of task-agnostic components (CNNs and LSTMs) achieving 85% success in
GeometricGoal rearrangement. This is impressive performance because (1) the task is extremely long horizon (involving naviga-tion, picking, and placing), (2) the architecture is monolithic and has no mapping modules, task-planning, or motion-planning.
For context, Habitat 2.0 reported 0% success with a monolithic
RL baseline in GeometricGoal rearrangement. We find the learned policies are able to efficiently navigate, avoid distractor objects, and synchronize base and arm movement for greater efficiency. Finally, we also show that models trained in Galactic are somewhat robust to zero-shot sim2sim generalization, i.e.
can achieve 26% success when deployed in Habitat 2.0 despite differences in rendering, physics, and underlying controller. 2.