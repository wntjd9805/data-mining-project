Abstract
Camera-only 3D detection provides an economical so-lution with a simple configuration for localizing objects in 3D space compared to LiDAR-based detection systems.
However, a major challenge lies in precise depth estima-tion due to the lack of direct 3D measurements in the in-put. Many previous methods attempt to improve depth es-timation through network designs, e.g., deformable layers and larger receptive fields. This work proposes an or-thogonal direction, improving the camera-only 3D detec-tion by introducing multi-agent collaborations. Our pro-posed collaborative camera-only 3D detection (CoCa3D) enables agents to share complementary information with each other through communication. Meanwhile, we op-timize communication efficiency by selecting the most in-formative cues. The shared messages from multiple view-points disambiguate the single-agent estimated depth and complement the occluded and long-range regions in the single-agent view. We evaluate CoCa3D in one real-world dataset and two new simulation datasets. Results show that CoCa3D improves previous SOTA performances by 44.21% on DAIR-V2X, 30.60% on OPV2V+, 12.59% on
CoPerception-UAVs+ for AP@70. Our preliminary results show a potential that with sufficient collaboration, the cam-era might overtake LiDAR in some practical scenarios. We released the dataset and code. 1.

Introduction
As a fundamental task of computer vision, 3D object de-tection aims to localize objects in the 3D physical space given an agent’s real-time sensor inputs.
It is crucial in a wide range of applications, including autonomous driv-ing [10,33,43], surveillance systems [39], robotics [12] and unmanned aerial vehicles [7]. Depending on sensor setups, there are multiple technical solutions to realize 3D object detection. In this spectrum, one extreme emphasizes rais-ing the upper bound of detection performance, which uses
*Corresponding author.
Figure 1. Collaborative camera-only 3D detection can disam-biguate the single-view estimated depth, address the long-range and occlusion issues, and approach LiDAR in 3D detection. high-end LiDAR sensor [2, 3, 10, 21, 28, 42, 45] to collect precise 3D measurements. However, this approach is too expensive to scale up. The other extreme solution empha-sizes cost effectiveness, which tries to use thrifty sensor se-tups, e.g. only using cameras to detect 3D objects in real-time [1, 8, 9, 23, 25, 26, 30, 32, 41, 43]. However, camera-only 3D detection is significantly and consistently worse than LiDAR-based detection in most scenarios [19].
In this paper, we propose an orthogonal direction for im-proving camera-only 3D detection performances by intro-ducing multi-agent collaborations. Hypothetically, empow-ered by advanced communication systems, multiple agents equipped only with cameras could share visual information with each other. This would bring three outstanding ben-efits. First, different viewpoints from multiple agents can largely resolve the depth ambiguity issue in camera-only 3D detection, bridging the gap with expensive LiDARs on depth estimation. Second, multi-agent collaboration avoids inevitable limitations in single-agent 3D detection, such as occlusion and long-range issues, and potentially enables more holistic 3D detection; that is, detecting all the ob-jects existed in the 3D scene, including those beyond vi-sual range. Since LiDAR also suffers from limited field of view, this potentially enables collaborative cameras to out-perform LiDAR. Third, the total expense of a large fleet of vehicles is significantly reduced as cameras are much
cheaper than LiDAR. However, multi-agent collaboration also brings new challenges. Different from many multi-view geometry problems, here we also have to concern communication bandwidth constraints. Thus, each agent needs to select the most informative cues to share.
Following this design rationale, we propose a novel col-laborative camera-only 3D detection framework CoCa3D.
It includes three parts: i) single-agent camera-only 3D de-tection, which achieves basic depth estimation and 3D de-tection for each agent; ii) collaborative depth estimation, which disambiguates the estimated depths by promoting spatial consistency across multiple agents’ viewpoints; and iii) collaborative detection feature learning, which comple-ments detection features by sharing key detection messages with each other. Compared to recent collaborative percep-tion methods [11,14] that are dealing with LiDAR, CoCa3D specifically designs novel collaborative depth estimation to customize the task of camera-only 3D detection.
To evaluate CoCa3D, we conduct comprehensive ex-periments on one real-world dataset, DAIR-V2X [40], and two new simulation datasets, OPV2V+ and CoPerception-UAVs+, which are extended based on original OPV2V [37] and CoPerception-UAVs [6] with more collaborative agents that cover three types of agents (cars, infrastructures and drones). Our results show that i) with 10 collaborative agents, CoCa3D enables camera-only detectors to overtake
LiDAR-based detectors on OPV2V+; and ii) CoCa3D con-sistently outperforms previous works in the performance-bandwidth trade-off across multiple datasets by a large mar-gin, improving the previous SOTA performances by 30.60% on OPV2V+, 12.59% on CoPerception-UAVs+, 44.21% on
DAIR-V2X for AP@70. To sum up, our contributions are:
• We propose a novel collaborative camera-only 3D de-tection framework CoCa3D, which improves the detection ability of cameras with multi-agent collaboration, promot-ing more holistic 3D detection.
• We propose core communication-efficient collabora-tion techniques, which explore the spatially sparse yet crit-ical depth messages and tackle the depth ambiguity, occlu-sion, and long-range issues by fusing complementary infor-mation from different viewpoints, achieving more accurate and complete 3D representation.
• We expand two previous collaborative datasets with more agents, and conduct extensive experiments, validating that i) CoCa3D significantly bridges the performance gap between camera and LiDAR on OPV2V+ and DAIR-V2X; and ii) CoCa3D achieves the state-of-the-art performance-bandwidth trade-off. 2.