Abstract
Many perception systems in mobile computing, au-tonomous navigation, and AR/VR face strict compute con-straints that are particularly challenging for high-resolution input images. Previous works propose nonuniform downsam-plers that "learn to zoom" on salient image regions, reducing compute while retaining task-relevant image information.
However, for tasks with spatial labels (such as 2D/3D ob-ject detection and semantic segmentation), such distortions may harm performance. In this work (LZU), we "learn to zoom" in on the input image, compute spatial features, and then "unzoom" to revert any deformations. To enable ef-ﬁcient and differentiable unzooming, we approximate the zooming warp with a piecewise bilinear mapping that is invertible. LZU can be applied to any task with 2D spa-tial input and any model with 2D spatial features, and we demonstrate this versatility by evaluating on a variety of tasks and datasets: object detection on Argoverse-HD, se-mantic segmentation on Cityscapes, and monocular 3D ob-ject detection on nuScenes. Interestingly, we observe boosts in performance even when high-resolution sensor data is unavailable, implying that LZU can be used to "learn to up-sample" as well. Code and additional visuals are available at https://tchittesh.github.io/lzu/. 1.

Introduction
In many applications, the performance of perception sys-tems is bottlenecked by strict inference-time constraints.
This can be due to limited compute (as in mobile computing), a need for strong real-time performance (as in autonomous vehicles), or both (as in augmented/virtual reality). These constraints are particularly crippling for settings with high-resolution sensor data. Even with optimizations like model compression [4] and quantization [23], it is common practice to downsample inputs during inference.
However, running inference at a lower resolution undeni-ably destroys information. While some information loss is
†Now at Waymo.
‡Now at Nvidia.
Figure 1. LZU is characterized by "zooming" the input image, computing spatial features, then "unzooming" to revert spatial de-formations. LZU can be applied to any task and model that makes use of internal 2D features to process 2D inputs. We show visual examples of output tasks including 2D detection, semantic segmen-tation, and 3D detection from RGB images. unavoidable, the usual solution of uniform downsampling assumes that each pixel is equally informative towards the task at hand. To rectify this assumption, Recasens et al. [20] propose Learning to Zoom (LZ), a nonuniform downsampler that samples more densely at salient (task-relevant) image regions. They demonstrate superior performance relative to uniform downsampling on human gaze estimation and
ﬁne-grained image classiﬁcation. However, this formulation warps the input image and thus requires labels to be invariant to such deformations.
Adapting LZ downsampling to tasks with spatial labels is trickier, but has been accomplished in followup works for semantic segmentation (LDS [11]) and 2D object de-tection (FOVEA [22]). LDS [11] does not unzoom during learning, and so deﬁnes losses in the warped space. This necessitates additional regularization that may not apply to non-pixel-dense tasks like detection. FOVEA [22] does un-zoom bounding boxes for 2D detection, but uses a special purpose solution that avoids computing an inverse, making it inapplicable to pixel-dense tasks like semantic segmenta-tion. Despite these otherwise elegant solutions, there doesn’t seem to be a general task-agnostic solution for intelligent downsampling.
Our primary contribution is a general framework in which we zoom in on an input image, process the zoomed im-age, and then unzoom the output back with an inverse warp.
Learning to Zoom and Unzoom (LZU) can be applied to any network that uses 2D spatial features to process 2D spa-tial inputs (Figure 1) with no adjustments to the network or loss. To unzoom, we approximate the zooming warp with a piecewise bilinear mapping. This allows efﬁcient and differentiable computation of the forward and inverse warps.
To demonstrate the generality of LZU, we demonstrate performance a variety of tasks: object detection with Reti-naNet [17] on Argoverse-HD [14], semantic segmentation with PSPNet [29] on Cityscapes [7], and monocular 3D detection with FCOS3D [26] on nuScenes [2]. In our experi-ments, to maintain favorable accuracy-latency tradeoffs, we use cheap sources of saliency (as in [22]) when determining where to zoom. On each task, LZU increases performance over uniform downsampling and prior works with minimal additional latency.
Interestingly, for both 2D and 3D object detection, we also see performance boosts even when processing low reso-lution input data. While prior works focus on performance improvements via intelligent downsampling [20, 22], our results show that LZU can also improve performance by intelligently upsampling (suggesting that current networks struggle to remain scale invariant for small objects, a well-known observation in the detection community [18]). 2.