Abstract
Object compositing based on 2D images is a challeng-ing problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Fur-thermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in gen-erative models, in this work, we propose a self-supervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hol-listically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object’s characteristics, we introduce a content adaptor that helps to maintain categori-* Work done during internship at Adobe. cal semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the gen-erator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images. 1.

Introduction
Image compositing is an essential task in image editing that aims to insert an object from a given image into another image in a realistic way. Conventionally, many sub-tasks are involved in compositing an object to a new scene, in-cluding color harmonization [6, 7, 19, 51], relighting [52], and shadow generation [16, 29, 43] in order to naturally blend the object into the new image. As shown in Tab. 1, most previous methods [6, 7, 16, 19, 28, 43] focus on a sin-gle sub-task required for image compositing. Consequently, they must be appropriately combined to obtain a composite image where the input object is re-synthesized to have the
Method
Geometry
Light
Shadow View
✓
ST-GAN [28]
✗
SSH [19]
✗
DCCF [51]
✗
SSN [43]
✗
SGRNet [16]
GCC-GAN [5] ✓
✓
Ours
✗
✓
✓
✗
✗
✓
✓
✗
✗
✗
✓
✓
✗
✓
✗
✗
✗
✗
✗
✗
✓
Table 1. Prior works only focus on one or two aspects of object compositing, and they cannot synthesize novel views. In contrast, our model can address all perspectives as listed. color, lighting and shadow that is consistent with the back-ground scene. As shown in Fig. 1, results produced in this way still look unnatural, partly due to the viewpoint of the inserted object being different from the overall background.
Harmonizing the geometry and synthesizing novel views have often been overlooked in 2D image compositing, which require an accurate understanding of both the geome-try of the object and the background scene from 2D images.
Previous works [15, 21, 22] handle 3D object compositing with explicit background information such as lighting posi-tions and depth. More recently, [28] utilize GANs to esti-mate the homography of the object. However, this method is limited to placing furniture in indoor scenes. In this pa-per, we propose a generic image compositing method that is able to harmonize the geometry of the input object along with color, lighting and shadow with the background image using a diffusion-based generative model.
In recent years, generative models such as GANs [4, 10, 17, 20] and diffusion models [1, 14, 30, 32, 33, 38, 39] have shown great potential in synthesizing realistic images.
In particular, diffusion model-based frameworks are versa-tile and outperform various prior methods in image edit-ing [1, 23, 32] and other applications [11, 31, 35]. How-ever, most image editing diffusion models focus on using text inputs to manipulate images [1,3,9,23,33], which is in-sufficient for image compositing as verbal representations cannot fully capture the details or preserve the identity and appearance of a given object image. There have been re-cent works [23, 39] focusing on generating diverse contexts while preserving the key features of the object; however, these models are designed for a different task than object compositing. Furthermore, [23] requires fine-tuning the model for each input object and [39] also needs to be fine-tuned on multiple images of the same object. Therefore they are limited for general object compositing.
In this work, we leverage diffusion models to simulta-neously handle multiple aspects of image compositing such as color harmonization, relighting, geometry correction and shadow generation. With image guidance rather than text guidance, we aim to preserve the identity and appearance of the original object in the generated composite image.
Specifically, our model synthesizes a composite image given (i) a source object, (ii) a target background image, and (iii) a bounding box specifying the location to insert the object. The proposed framework consists of a con-tent adaptor and a generator module: the content adaptor is designed to extract a representation from the input object containing both high-level semantics and low-level details such as color and shape; the generator module preserves the background scene while improving the generation quality and versatility. Our framework is trained in a fully self-supervised manner and no task-specific labeling is required at any point during training. Moreover, various data aug-mentation techniques are applied to further improve the fi-delity and realism of the output. We evaluate our proposed method on a real-world dataset closely simulating real use cases for image compositing.
Our contributions are summarized as follows:
• We present the first diffusion model-based framework for generative object compositing that can handle mul-tiple aspects of compositing such as viewpoint, geom-etry, lighting and shadow.
• We propose a content adaptor module which learns a descriptive multi-modal embedding from images, en-abling image guidance for diffusion models.
• Our framework is trained in a self-supervised man-ner without any task-specific annotations, employing data augmentation techniques to improve the fidelity of generation.
• We collect a high-resolution real-world dataset for ob-ject compositing with diverse images, containing man-ually annotated object scales and locations. 2.