Abstract
Denoising is a crucial step for hyperspectral image (HSI) applications. Though witnessing the great power of deep learning, existing HSI denoising methods suffer from limitations in capturing the non-local self-similarity. Trans-formers have shown potential in capturing long-range de-pendencies, but few attempts have been made with specifi-cally designed Transformer to model the spatial and spec-tral correlation in HSIs.
In this paper, we address these issues by proposing a spectral enhanced rectangle Trans-former, driving it to explore the non-local spatial similar-ity and global spectral low-rank property of HSIs. For the former, we exploit the rectangle self-attention horizon-tally and vertically to capture the non-local similarity in the spatial domain. For the latter, we design a spectral enhancement module that is capable of extracting global underlying low-rank property of spatial-spectral cubes to suppress noise, while enabling the interactions among non-overlapping spatial rectangles. Extensive experiments have been conducted on both synthetic noisy HSIs and real noisy
HSIs, showing the effectiveness of our proposed method in terms of both objective metric and subjective visual quality.
The code is available at https://github.com/MyuLi/SERT. 1.

Introduction
With sufficient spectral information, hyperspectral im-ages (HSIs) can provide more detailed characteristics to distinguish from different materials compared to RGB im-ages. Thus, HSIs have been widely applied to face recog-nition [37, 38], vegetation detection [4], medical diagno-sis [43], etc. With scanning designs [2] and massive wave-bands, the photon numbers in individual bands are limited.
HSI is easily degraded by various noise. Apart from poor visual effects, such undesired degradation also negatively affects the downstream applications. To obtain better visual effects and performance in HSI vision tasks, denoising is a
†Equal Contribution, ∗Corresponding Author fundamental step for HSI analysis and processing.
Similar to RGB images, HSIs have self-similarity in the spatial domain, suggesting that similar pixels can be grouped and denoised together. Moreover, since hyperspec-tral imaging systems are able to acquire images at a nomi-nal spectral resolution, HSIs have inner correlations in the spectral domain. Thus, it is important to consider both spa-tial and spectral domains when designing denoising meth-ods for HSI. Traditional model-based HSI denoising meth-ods [10, 17, 21] employ handcrafted priors to explore the spatial and spectral correlations by iteratively solving the optimization problem. Among these works, total variation
[20, 21, 52] prior, non-local similarity [19], low-rank [8, 9] property, and sparsity [42] regularization are frequently uti-lized. The performance of these methods relies on the ac-curacy of handcrafted priors.
In practical HSI denoising, model-based methods are generally time-consuming and have limited generalization ability in diverse scenarios.
To obtain robust learning for noise removal, deep learn-ing methods [7, 35, 41, 49] are applied to HSI denoising and achieve impressive restoration performance. However, most of these works utilize convolutional neural networks for fea-ture extraction and depend on local filter response to sepa-rate noise and signal in a limited receptive field.
Recently, vision Transformers have emerged with com-petitive results in both high-level tasks [16, 39] and low-level tasks [1,13,50], showing the strong capability of mod-eling long-range dependencies in image regions. To di-minish the unaffordable quadratically computation cost to image size, many works have investigated the efficient de-sign of spatial attention [11, 46, 47]. Swin Transformer [28] splitted feature maps into shifted square windows. CSWin
Transformer [15] developed a stripe window across the fea-tures maps to enlarge the attention area. As HSI usually has large feature maps, exploring the similarity beyond the noisy pixel can cause unnecessary calculation burden.
Thus, how to efficiently model the non-local spatial simi-larity is still challenging for HSI denoising Transformer.
HSIs usually lie in a spectral low-rank subspace [9], which can maintain the distinguished information and sup-press noise. This indicates that the non-local spatial simi-larity and low-rank spectral statistics should be jointly uni-tized for HSI denoising. However, existing HSI denoising methods [24, 45] mainly utilize the low-rank characteris-tics through matrix factorization, which is based on a single
HSI and requires a long-time to solve. The global low-rank property in large datasets is hardly considered.
In this paper, we propose a Spectral Enhanced Rectangle
Transformerc (SERT) for HSI denoising. To reinforce model capacity with reasonable cost, we develop a multi-shape rectangle self-attention module to comprehensively explore the non-local spatial similarity. Besides, we ag-gregate the most informative spectral statistics to suppress noise in our spectral enhancement module, which projects the spatial-spectral cubes into low-rank vectors with the as-sistance of a global spectral memory unit. The spectral enhancement module also provides interactions between the non-overlapping spatial rectangles. With our proposed
Transformer, the spatial non-local similarity and global spectral low-rank properly are jointly considered to benefit the denoising process. Experimental results show that our method significantly outperforms the state-of-the-art meth-ods in both simulated data and real noisy HSIs.
Overall, our contributions can be summarized as follows:
• We propose a spectral enhanced rectangle Transformer for HSI denoising, which can well exploit both the non-local spatial similarity and global spectral low-rank property of noisy images.
• We present a multi-shape rectangle spatial self-attention module to effectively explore the comprehen-sive spatial self-similarity in HSI.
• A spectral enhancement module with memory blocks is employed to extract the informative low-rank vec-tors from HSI cube patches and suppress the noise. 2.