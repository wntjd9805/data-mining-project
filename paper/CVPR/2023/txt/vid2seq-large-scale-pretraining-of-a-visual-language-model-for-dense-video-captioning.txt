Abstract
In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The
Vid2Seq architecture augments a language model with spe-cial time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output se-quence. Such a unified model requires large-scale training data, which is not available in current annotated datasets.
We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sen-tence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pre-trained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning bench-marks including YouCook2, ViTT and ActivityNet Captions.
Vid2Seq also generalizes well to the tasks of video para-graph captioning and video clip captioning, and to few-shot settings. Our code is publicly available at [1]. 1.

Introduction
Dense video captioning requires the temporal localiza-tion and captioning of all events in an untrimmed video [45, 98, 127]. This differs from standard video captioning [62, 69, 79], where the goal is to produce a single caption for a given short video clip. Dense captioning is significantly more difficult, as it raises the additional complexity of lo-calizing the events in minutes-long videos. However, it also
*This work was done when the first author was an intern at Google. benefits from long-range video information. This task is potentially highly useful in applications such as large-scale video search and indexing, where the video content is not segmented into clips.
Existing methods mostly resort to two-stage ap-proaches [36, 45, 96], where events are first localized and then captioned. To further enhance the inter-task interac-tion between event localization and captioning, some ap-proaches have introduced models that jointly solve the two tasks [19,98,127]. However, often these approaches still re-quire task-specific components such as event counters [98].
Furthermore, they exclusively train on manually annotated datasets of limited size [34, 45, 126], which makes it diffi-cult to effectively solve the task. To address these issues, we take inspiration from recent sequence-to-sequence mod-els pretrained on Web data which have been successful on a wide range of vision and language tasks [3,10,12,101,113].
First, we propose a video language model, called
Vid2Seq. We start from a language model trained on Web text [77] and augment it with special time tokens that repre-sent timestamps in the video. Given video frames and tran-scribed speech inputs, the resulting model jointly predicts all event captions and their corresponding temporal bound-aries by generating a single sequence of discrete tokens, as illustrated in Figure 1 (right). Such a model therefore has the potential to learn multi-modal dependencies between the different events in the video via attention [90]. However this requires large-scale training data, which is not avail-able in current dense video captioning datasets [34,45,126].
Moreover, collecting manual annotations of dense captions for videos is expensive and prohibitive at scale.
Hence we propose to pretrain Vid2Seq by leveraging un-labeled narrated videos which are readily-available at scale.
To do this, we reformulate sentence boundaries of tran-scribed speech as pseudo event boundaries, and use the tran-scribed speech sentences as pseudo event captions. We then pretrain Vid2Seq with a generative objective, that requires predicting the transcribed speech given visual inputs, and a denoising objective, which masks spans of transcribed speech. Note that transcribed speech may not describe the video content faithfully, and is often temporally misaligned with the visual stream [31, 42, 70]. For instance, from the example in Figure 1 (left), one can understand that the grey skier has descended a slope from the last speech sentence which is said after he actually descended the slope. Intu-itively, Vid2Seq is particularly suited for learning from such noisy supervision as it jointly models all narrations and the corresponding timestamps in the video.
We demonstrate the effectiveness of our pretrained model through extensive experiments. We show the im-portance of pretraining on untrimmed narrated videos, the ability of Vid2Seq to use both the visual and speech modali-ties, the importance of the pretraining objectives, the benefit of joint caption generation and localization, as well as the importance of the language model size and the scale of the pretraining dataset. The pretrained Vid2Seq model achieves state-of-the-art performance on various dense video cap-tioning benchmarks [34, 45, 126]. Our model also excels at generating paragraphs of text describing the video: with-out using ground-truth event proposals at inference time, our model outperforms all prior approaches including those that rely on such proposals [49,75,124]. Moreover, Vid2Seq generalizes well to the standard task of video clip cap-tioning [8, 105]. Finally, we introduce a new few-shot dense video captioning setting in which we finetune our pre-trained model on a small fraction of the downstream train-ing dataset and show benefits of Vid2Seq in this setting.
In summary, we make the following contributions: (i) We introduce Vid2Seq for dense video captioning.
Given multi-modal inputs (transcribed speech and video),
Vid2Seq predicts a single sequence of discrete tokens that includes caption tokens interleaved with special time to-kens that represent event timestamps. (ii) We show that transcribed speech and corresponding timestamps in unla-beled narrated videos can be effectively used as a source (iii) Fi-of weak supervision for dense video captioning. nally, our pretrained Vid2Seq model improves the state of the art on three dense video captioning datasets (YouCook2,
ViTT, ActivityNet Captions), two video paragraph caption-ing benchmarks (YouCook2, ActivityNet Captions) and two video clip captioning datasets (MSR-VTT, MSVD), and also generalizes well to few-shot settings.
Our code implemented in Jax and based on the Scenic library [18] is publicly released at [1]. 2.