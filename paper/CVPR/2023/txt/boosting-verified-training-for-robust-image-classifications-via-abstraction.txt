Abstraction
Zhaodi Zhang 1,2,4, Zhiyi Xue 2, Yang Chen 2, Si Liu 3, Yueling Zhang 2, Jing Liu 1, Min Zhang 2 1 Shanghai Key Laboratory of Trustworthy Computing 2 East China Normal University 3 ETH Z¨urich 4 Chengdu Education Research Institute
Abstract
This paper proposes a novel, abstraction-based, certified training method for robust image classifiers. Via abstrac-tion, all perturbed images are mapped into intervals before feeding into neural networks for training. By training on intervals, all the perturbed images that are mapped to the same interval are classified as the same label, rendering the variance of training sets to be small and the loss landscape of the models to be smooth. Consequently, our approach significantly improves the robustness of trained models. For the abstraction, our training method also enables a sound and complete black-box verification approach, which is or-thogonal and scalable to arbitrary types of neural networks regardless of their sizes and architectures. We evaluate our method on a wide range of benchmarks in different scales.
The experimental results show that our method outperforms state of the art by (i) reducing the verified errors of trained models up to 95.64%; (ii) totally achieving up to 602.50x speedup; and (iii) scaling up to larger models with up to 138 million trainable parameters. The demo is available at https:// github.com/ zhangzhaodi233/ ABSCERT.git. 1.

Introduction
The robustness of image classifications based on neu-ral networks is attracting more attention than ever due to their applications to safety-critical domains such as self-driving [51] and medical diagnosis [43]. There has been a considerable amount of work on training robust neural networks against adversarial perturbations [1, 6–8, 10, 12, 28,32,56–59,62]. Conventional defending approaches aug-ment the training set with adversarial examples [5, 15, 27, 32, 37, 41, 54, 55]. They target only specific adversaries, de-pending on how the adversarial samples are generated [7], but cannot provide robustness guarantees [2, 11, 26].
Recent approaches attempt to train certifiably robust models with guarantees [2, 11, 14, 24, 26, 29, 49, 63]. They rely on the robustness verification results of neural networks in the training process. Most of the existing verification ap-proaches are based on symbolic interval propagation (SIP)
[11,14,14,24,26], by which intervals are symbolically input into neural networks and propagated on a layer basis. There are, however, mainly three obstacles for these approaches to be widely adopted: (i) adding the verification results to the loss function for training brings limited improvement to the robustness of neural networks due to overestimation intro-duced in the verification phase; (ii) they are time-consuming due to the high complexity of the verification problem per se, e.g., NP-complete for the simplest ReLU-based fully connected feedforward neural networks [19]; and (iii) the verification is tied to specific types of neural networks in terms of network architectures and activation functions.
To overcome the above obstacles, we propose a novel, abstraction-based approach for training verified robust im-age classifications whose inputs are numerical intervals.
Regarding (i), we abstract each pixel of an image into an interval before inputting it into the neural network. The in-terval is numerically input to the neural network by assign-ing to two input neurons its lower and upper bounds, re-spectively. This guarantees that all the perturbations to the pixel in this interval do not alter the classification results, thereby improving the robustness of the network. More-over, this imposes no overestimation in the training phase.
To address the challenge (ii), we use forward propagation and back propagation only to train the network without ex-tra time overhead. Regarding (iii), we treat the neural net-works as black boxes since we deal only with the input layer and do not change other layers. Hence, being agnostic to the actual neural network architectures, our approach can scale up to fairly large neural networks. Additionally, we iden-tify a crucial hyper-parameter, namely abstraction granu-larity, which corresponds to the size of intervals used for training the networks. We propose a gradient descent-based algorithm to refine the abstraction granularity for training a more robust neural network.
We implement our method in a tool called ABSCERT and assess it, together with existing approaches, on vari-ous benchmarks. The experimental results show that our approach reduces the verified errors of the trained neural networks up to 95.64%. Moreover, it totally achieves up to
602.50x speedup. Finally, it can scale up to larger neural networks with up to 138 million trainable parameters and be applied to a wide range of neural networks.
Contributions. Overall, we provide (i) a novel, abstraction-based training method for verified robust neural networks; (ii) a companion black-box verification method for certify-ing the robustness of trained neural networks; (iii) a tool im-plementing our method; and (iv) an extensive assessment of our method, together with existing approaches, on a wide range of benchmarks, which demonstrates our method’s promising achievements. 2. Robust Deep Neural Networks
Huber [17] introduces the concept of robustness in a broader sense: (i) the efficiency of the model should be rea-sonably good, and (ii) the small disturbance applied to the input should only have a slight impact on the result of the model. The problem of robustness verification has been for-mally defined by Lyu et al. [26]:
Definition 1 (Robustness verification) Robustness verifi-cation aims to guarantee that a neural network outputs con-sistent classification results for all inputs in a set which is usually represented as an lp norm ball around the clean im-age x: B(x, ϵ) = {x′| ∥x′ − x∥p ≤ ϵ}.
The problem of neural network robustness verification has intrinsically high complexity [65]. Many approaches rely on symbolic interval bound propagation to simplify the problem at the price of sacrificing the completeness
[18, 26, 63, 65]. Figure 1 illustratively compares the dif-ference of interval bound propagation (IBP) and SIP. Intu-itively, when we know the domain of each input, we can estimate the output range by propagating the inputs sym-bolically throughout the neural network. According to the output range, we can prove/disprove the robustness of the neural network. See [46] for more details.
To accelerate the propagation, the non-linear activation functions need to over-approximate using linear constraints, therefore rendering the output range overestimated. More hidden layers in a network imply a larger overestimation because the overestimation is accumulated layer by layer.
A large overestimation easily causes failure to verification.
Therefore, many efforts are being made to tighten the over-approximation [4,53,64]. Unfortunately, it has been proved there is a theoretical barrier [36, 44, 65]. (a) IBP (b) SIP
Figure 1. IBP and SIP [46]. these two sections, we also illustrate how varying network inputs can affect the loss function, which in turn contributes to the robustness of trained models. 3.1. Formulating the Training Problem
We solve the following training problem:
Given a training set and a testing set of images and a perturbation distance ϵ, our goal is to train an im-age classifier f that is provably robust on B(x, ϵ) for each image x in the set while f has a low verified er-ror on the testing set.
As the target image classifier f must be guaranteed ro-bust on B(x, ϵ), f returns the same classification results for all the perturbed images in B(x, ϵ). Our idea is to (i) map
B(x, ϵ) to an interval vector (I1, I2, . . . In) by an abstrac-tion function ϕ such that ϕ(B(x, ϵ)) = (I1, I2, . . . In), and (ii) train an image classifier f ′ which takes as input the inter-val vector and returns a classification result as x is labeled, i.e., arg maxy∈Y f ′(I1, I2, . . . , In) = ytrue with ytrue the ground truth label of x. The target image classifier f is a composition of f ′ and ϕ. Apparently, f is provably robust on B(x, ϵ) since all the images in B(x, ϵ) are mapped to (I1, I2, . . . , In) which is classified to ytrue by f ′.
To feed the training intervals to the neural network, the number of neurons in the input layer is doubled. As shown in Figure 2, the upper bound and lower bound of each train-ing interval are input to these neurons. Namely, the neurons in the input layer do not correspond to a pixel but the upper and lower bounds of each training interval. Any perturba-tion interval mapped to the same training interval will be fed into the neural network with the same upper and lower bounds. Other training parameters and settings are the same as the traditional training process. 3. The Abstraction-Based Training Approach 3.2. Abstraction of Perturbed Images
In this section, we present our abstraction-based method for training neural networks. We define the training prob-lem in Section 3.1. Section 3.2 and Section 3.3 give the ab-straction procedure and the training procedure. Throughout 3.2.1 From Perturbed Interval to Training Interval
We first introduce the abstraction function ϕ : I n → In, where I n is the set of all interval vectors, In is a finite set of
Algorithm 1: Abstraction of Perturbations ϕ.
Input
: I: perturbation interval vector; d: abstraction granularity
Output: I: training interval vector init, I; 1 Initialize I ′ 2 I ′ ← Divide(I ′ 3 for i in len(I) do 4 for j in len(I ′) do
// I ′ init, d); init is [-1,1], and I is an empty vector
// Get sub-intervals with size d
Figure 2. Training on numerical intervals. interval vectors on which neural networks are trained, and n denotes the size of the interval vectors. The norm ball
B(x, ϵ) of x under ϵ is essentially an interval vector in I n.
We call the elements in I n perturbation interval vectors.
Let [−1, 1] be the range of the complete input space. We divide [−1, 1] evenly into sub-intervals of size di and de-note the set of all the sub-intervals as Ii. Then, we obtain
In as the Cartesian product of all Ii, i.e., Πn
Ii. Because we train image classifiers on In, we call the elements in In training interval vectors. We use d to denote the integer vector (d1, d2, . . . , dn) and call it abstraction granularity.
In our abstraction process, all values in d are the same, so we use a constant d to represent it. i=1
Because perturbation interval vectors are infinite, it is impossible to enumerate all perturbation interval vectors for training. We abstract and map them onto a finite number of training interval vectors in In. The purpose of the ab-straction is to ease the follow-up robustness verification by restricting the infinite perturbation space to finite training space. In addition, the abstraction function is an element-wise operation. In such cases, we introduce our mapping function with an interval as an example.
In order to ensure that the perturbation interval can only be mapped to a unique training interval, we make the fol-lowing constraints on the mapping process: We first restrict the abstraction granularity d to be greater than or equal to twice the perturbation size ϵ, for the purpose of guarantee-ing that a perturbation interval has an intersection with at most two training intervals. We divide the mapping into three cases: (1) If a training interval contains the perturba-tion interval, the perturbation interval will be mapped to the training interval; (2) If the perturbation interval has an inter-section with two training intervals unequally, the perturba-tion interval will be mapped to the training interval with the larger coverage area; (3) If the perturbation interval has an intersection with two training intervals equally, the pertur-bation interval will be mapped to the training interval with a larger value. In this way, we map the perturbation interval on the unique training interval, and then we can train the neural network on these training intervals. 5 6 7 8 9 10 11 12 13 14 15 j) then if min(Ii) > max(I ′
Continue ; if Ii ∈ I ′ j then
Ii = I ′ j; else
// No intersection
// I ′ j contains Ii lenr = max(Ii) - min(I ′ lenl = max(I ′ j) - min(Ii); if lenr ≥ lenl then j+1);
Ii = I ′ j+1; else
Ii = I ′ j;
// Map to numerically larger
// Map to numerically smaller 16 Return I;
Algorithm 1 shows our abstraction function. Firstly, we initialize the complete input interval and an training inter-val vector (Line 1). Then, we obtain the sub-intervals which each size is d (Line 2). If a training interval contains the per-turbation interval, the perturbation interval will be mapped to the training interval (Lines 7-8). The perturbation inter-val at most intersects with two training intervals at the same time as d ≥ 2 ∗ ϵ. Let lenr be the size of the intersec-tion of the perturbation interval and the numerically larger training interval (Line 10). Let lenl be the size of the in-tersection of the perturbation interval and the numerically smaller training interval (Line 11). If lenr ≥ lenl, the per-turbation interval will be mapped to the numerically larger training interval (Lines 12-13); otherwise, it will be mapped to the numerically smaller training interval (Lines 14-15). 3.2.2 Effect of Abstraction on Input
We now explain that the abstraction process results in a smaller variance of training intervals.
In our training method, the value of each pixel is nor-malized to [−1, 1]. Considering the arbitrariness of the in-put value distribution, we assume that the input values are evenly distributed in [−1, 1]. We calculate the variance of input, and in such cases, the value we get is the maximum likelihood estimation of the actual value.
We calculate the variance of the input values with d representing abstraction granularity and I representing the training intervals of an image. Note that in conventional methods, it is equivalent to d = 0, while in our method, d
Algorithm 2: Abstraction-based Training: ABSTRAIN 3.3.2 Smoothing Loss Landscape
Input
: X: training data;
Y: ground-truth labels of training data;
ϵ: perturbation radius; d: abstraction granularity
Output: f ′: a neural network, ℓ: training loss 1 Initialize f ′, ℓ; 2 for (X, Y ) in (X, Y) do 3 for (x, y) in (X, Y ) do
I ← ϕ(B(x, ϵ), d) ;
ℓ ← ℓ+ L(f ′(I), y) ; 4 5
// Get training intervals.
// Accumulate the loss. f ′ ← Update(f ′, ℓ) ; 6 7 Return f ′, ℓ;
// Update the parameters in f ′. is a positive number. In this way, for an input image, the variance of it after the abstraction process is:
D(I) = E((I)2) − E(I)2 d 2
= (−1 + (cid:124)
)2 + (−1 +
) + . . . + (1 − 2 3d 2 (cid:123)(cid:122) 2/d items (1) d 2
)2 (cid:125)
= 4n2 − 1 6
.
In Equation 1, the variance of training interval is computed by the upper and lower bounds. For an abstraction-based trained neural network, a large abstraction granularity d im-plies a small n, and consequently the variance of the train-ing intervals becomes small. Apparently, intervals have a smaller variance than concrete pixel values. 3.3. Training on Intervals 3.3.1 The Training Method
When we get the training intervals, hyperparameter settings, such as the number of layers, the number of neurons in hid-den layers, the loss function during training, etc., are all the same as the traditional training methods except for the num-ber of neurons in the input layer.
Algorithm 2 shows the pseudo-code of the algorithm for training a neural network. The training dataset X, the ground-truth labels Y corresponding to the dataset, the per-turbation radius ϵ, and the abstraction granularity d are used as inputs. Firstly, a neural network is initialized. That is, the adjustable parameters of the neural network are initialized randomly (Line 1). For each image of training dataset X, the perturbation intervals are mapped to training intervals (Line 4). According to the current adjustable parameters, the cross-entropy loss is calculated (Line 5). Finally, the backward propagation is performed according to the cross-entropy loss, and the values of the adjustable parameters are updated (Line 6). A trained neural network is returned when the loss no longer decreases.
We illustrate that the small variance of input results in a smooth loss landscape during training. Loss landscape is the characterization of loss functions. For example, smooth loss landscape means that the size of the connected region around the minimum where the training loss remains low, showing convex contours in the center, while sharp loss landscape shows not convex but chaotic to that region [25].
We investigate the smoothness of the loss landscape from both theoretical and experimental perspectives. For a classi-fication problem, the loss function is usually cross-entropy loss. We use y to represent the ground-truth label, and ˆy to represent the prediction of the neural network. In such cases, we explore the relationship between I and loss func-tion.
L(y, ˆy) = CrossEntropyLoss(y, ˆy)
= c (cid:88) i=1 yi · (−log( ˆyi)) (2)
= −log(max(A · I + b)) where c represents the number of classes, yi is the one-hot encoding of labels, ytrue is the output value corresponding to the correct label, and A and b are the parameters of the neural network. For parameter space, a smooth loss land-scape means that when the value of trainable parameters of the neural network gradually deviates from the optimal value, the loss increases slowly with it. In other words, the first-order partial derivative of Equation 2 with respect to A and b should be a constant or a value with less variation.
The two partial derivatives are:
∂L(y, ˆy)
∂A
=
∂(−log(max(A · I + b)))
∂A
= −
I max(A · I + b)
∂L(y, ˆy)
∂b
=
∂(−log(max(A · I + b)))
∂b
= − 1 max(A · I + b) (3) (4) 1
A+b·(I)−1 .
With fixed A and b, we discuss the value of Equation 3 for different training intervals. If b = 0, we get a constant
− 1
A . Obviously, the loss landscape is smooth in this case.
If b ̸= 0, we write the derivative as −
In our method, the variance of I is small. That is, the value of I concentrates around a fixed value. Thus, the value of the derivative varies in a small range, and the loss landscape is smoother. For Equation 4, max(A · I + b) is large as it is the value corresponding to the ground-truth label. Hence,
Equation 4 is close to 0, and the loss landscape is smooth.
In summary, the small variance of I results in a smooth loss landscape. As an example, we utilize the tool in [25]
(a) d = 0 (b) d = 0.025
Figure 3. Visualization of DM-small’s loss landscape [63] trained when d = 0 and d = 0.025. to visualize in Figure 3 the loss landscape of two neural networks that are trained with d = 0 and d = 0.025, re-spectively. Figure 3 depicts that the neural network trained with d = 0.025 has a broader loss landscape than the one trained with d = 0, and the loss increases more smoothly in every direction. 3.3.3 The Robustness of Trained Neural Networks
Training on numerical intervals guarantees that all the per-turbed intervals that are mapped to the same numerical in-terval will have the same classification result. Intuitively, if a numerical interval represents more concrete values, even if the concrete value is slightly disturbed, these disturbed values will be mapped to the same numerical interval with a high probability. Consequently, the classification of the numerical interval according to the neural network still re-mains unchanged. In this sense, we say that the perturbation is dissolved by the abstraction, and therefore the neural net-work’s robustness is improved.
Theoretically, we show that the smooth loss landscape due to the abstraction in training contributes to the robust-ness of neural networks. As shown in Figure 3, the loss of
DM-small trained by ABSCERT increases slowly and uni-formly with the change of parameters, meaning that the growth rate of loss is slow in all directions of parameters’ change. However, neural networks trained by conventional methods have a steep slope, which means that there is a di-rection where the loss increases rapidly. Obviously, smooth loss landscape is helpful for the optimization in training phase to find a global optimum.
Moreover, in the process of our abstraction, an original pixel is first perturbed to a perturbed interval which is then mapped to a training interval for classification. This indi-cates that the loss is a constant because all the perturbed images of an image are mapped to a fixed set of training intervals, and the In in Equation 2 is never changed.
Neural networks trained by our abstraction-based train-ing method is more robust as the loss landscape is smooth in both parameter space and input space [61]. In particular, for parameter space, the model tends to find a global optimum in a reasonably efficient manner [25, 52]; for input space, the model is insensitive to the input perturbations [30].
Algorithm 3: Training with Granularity Tuning.
Input
: X: training data; Xtest: test data;
Y: ground-truth labels of training data;
Ytest: ground-truth labels of test data;
ϵ: perturbation radius; dstep: step size of abstraction granularity
Output: fout: verified robust neural network; dout: the best abstraction granularity 1 Initialize d, error, e, fout, dout; 2 while d ≥ 2 ∗ ϵ do 3
// e : verified error. (f, ℓ) ← ABSTRAIN(X, Y, ϵ, d); // ℓ: training loss. e ← VERIFY(f , Xtest, Ytest, ϵ, d); // Verification. if e < error then error ← e; fout ← f ; dout ← d;
I ← Φ(X, ϵ, d) ;
G, G ← ℓ′(I), ℓ′(I); if G ≤ 0 ∧ G ≥ 0 then
// Obtain bounds’ gradient,
// Map to training interval.
Break;
// Stop when G guides d to increase else d ← d − dstep;
// Decrease d and continue 4 5 6 7 8 9 10 11 12 13 14 15 Return fout, dout; 4. Formal Verification and Granularity Tuning
In this section we introduce a verification-based method for tuning the abstraction granularity to train robust models.
Due to the finiteness of In, verification procedure can be conducted in a black-box manner, which is both sound and complete. Based on verification results, we can tune the abstraction granularity to obtain finer training intervals. 4.1. Black-box Robustness Verification
We propose a black-box verification method VERIFY(·) for neural networks trained by our approach. Given a neural network f , a test set Xtest, Ytest, a perturbation distance ϵ and an abstraction granularity d, VERIFY(·) returns the ver-ified error on the set. The verification procedure is straight-forward. First, for each x ∈ Xtest, we compute the set I of training interval vectors of B(x, ϵ) using the same abstrac-tion function ϕ in Algorithm 1. Then, we feed each interval vector in I into f and check if the classification result is con-sistent with the ground-truth label of x. The verified error e is the ratio of the inconsistent cases in the test set.
Our verification method is both sound and complete due to the finiteness of I: f is robust on B(x, ϵ) if and only if f returns the same label on all the interval vectors in I as the one of x. Another advantage is that it treats f as a black box. Therefore, our verification method is orthogonal and scalable to arbitrary models.
4.2. Tuning Abstraction Granularity
When the verified error of a trained neural network is large, we can reduce it by tuning the abstraction granularity and re-train the model on the refined training set of interval vectors. We propose a gradient descent-based algorithm to explore the best d in the abstraction space.
Algorithm 3 shows the tuning and re-training process.
First, our algorithm initializes abstraction granularity d and verified error error (Line 1). The tuning process repeats until d < 2ϵ, which means that the size of a training in-terval is not less than that of the perturbed interval (Lines 2-15). Then a neural network and training loss toward train-ing intervals with d are obtained by calling the Abstraction-based Training function (Algorithm 2) (Line 3). Get the neural network’s verified errors on the test dataset (Line 4).
If the neural network’s verified errors are smaller, we save the neural network and abstraction granularity (Lines 5-8).
Next, training intervals are obtained (Line 9). Then the up-per and lower bounds’ gradient of the training intervals are obtained (Lines 10). If G ≤ 0 and G ≥ 0, the algorithm will terminate (Lines 11-12); otherwise, abstraction gran-ularity will be updated to a smaller granularity (Lines 13-14). When the algorithm terminates, we obtain the neural network with the lowest verified error. 5. Experiment
We have implemented both our training and verification approaches in a tool called ABSCERT. We evaluate AB-SCERT, together with existing approaches, on various pub-lic benchmarks with respect to both verified error and train-ing and verification time.
By comparing with the state of the art, our goal is to demonstrate that ABSCERT can train neural networks with lower verified errors (Experiment I), incurs less computa-tion overhead in both training and verification (Experiment
II), is applicable to a wide range of neural network architec-tures (Experiment III), and can scale up to larger models (Experiment IV). 5.1. Benchmarks and Experimental Setup
Competitors. We consider three state-of-the-art prov-ably robust training methods: LossLandscapeMatters [24],
LBP&Ramp [26], and AdvIBP [11]. All of them rely on linear approximation to train certifiably robust neural net-works, which minimizes the upper bound on the worst-case loss against l∞ perturbations. We use their predefined opti-mal hyper-parameters to train neural networks on different perturbations ϵ.
Datasets and Networks. We conduct our experiments on MNIST [23], CIFAR-10 [21], and ImageNet [9]. For
MNIST and CIFAR-10, we train and verify all three con-volutional neural networks (CNNs), i.e., DM-small, DM-medium, and DM-large [13, 63], and two fully connected neural networks (FNNs) with three and five layers, respec-tively. The batch size is set 128. We use cross-entropy loss function and Adam [20] optimizer to update the parameters.
The learning rate decreases following the values of the co-sine function between 0 and π after a warmup period during which it increases linearly between 0 and 1 [48].
For ImageNet, we use the AlexNet [22], VGG11
[38], Inception V1 [42], and ResNet18 [16] architectures, which are winners of the image classification competition
ILSVRC1. We use the same super-parameters as in their original experiments for training these networks.
Metrics. We use two metrics in our comparisons: (i) ver-ified error, which is the percentage of images that are not verified to be robust. We quantify the precision improve-ment by (e′ − e)/e′, with e and e′ the verified errors of neural networks trained by ABSCERT and the competitor, respectively; and (ii) time, which includes training and/or verification on the same neural network architecture with the same dataset. We compute speedup by t′/t, with t and t′ the execution time ABSCERT and the competitors, respec-tively. As the time for different perturbations ϵ is almost the same, we report the average time (in Table 3 and Table 5).
Experimental Setup. All experiments on MNIST and
CIFAR-10, as well as AlexNet were conducted on a work-station running Ubuntu 18.04 with one NVIDIA GeForce
RTX 3090 Ti GPU. All experiments on VGG11, Inception
V1 and ResNet18 were conducted on a workstation running
Windows11 with one NVIDIA GeForce RTX 3090 Ti GPU. 5.2. Experimental Results
Experiment I: Effectiveness. Table 1 shows the compari-son results of verified errors for DM-small, DM-medium, and DM-large. ABSCERT achieves lower verified errors than the competitors. On MNIST, we obtain up to 95.12%, 92.65%, and 95.64% improvements over LLM, LBP, and
AdvIBP on three neural network models (with ϵ = 0.4), re-spectively. On CIFAR-10, we achieve a 81.9% improve-ment to LLM with ϵ = 16/255, and all the improvements are above 49%. Note that the publicly available code of
LBP does not support CIFAR-10.
Another observation is that the improvement increases as
ϵ becomes larger. This implies that, under larger perturba-tions, the verified errors of the models trained by ABSCERT increase less slowly than those trained by the competing ap-proaches. This reflects that the models trained by ABSCERT are more robust than those trained by the three competitors.
As for the accuracy of the trained networks, Table 2 shows that our method achieves higher accuracy than the competitors for all datasets and models under the same per-turbations. Moreover, the decrease speed is much less than 1https://image-net.org/challenges/LSVRC/index.php
Table 1. Verified errors (%) of models trained by LossLandscapeMatters (LLM), LBP&Ramp (LBP), AdvIBP (AIBP), and ABSCERT (AC) on MNIST and CIFAR-10 datasets. “–” means that the publicly available code of LBP does not support the CIFAR-10 dataset.
Dataset
ϵ
MNIST 0.1 0.2 0.3 0.4
DM-small
DM-medium
DM-large
AC LLM Impr.(%) LBP Impr.(%) AIBP Impr.(%) AC LLM Impr.(%) LBP Impr.(%) AIBP Impr.(%) AC LLM Impr.(%) LBP Impr.(%) AIBP Impr.(%) 0.89 3.02 70.53 ↑ 4.09 78.24 ↑ 3.34 73.35 ↑ 0.69 2.67 74.16 ↑ 3.47 80.12 ↑ 3.62 80.94 ↑ 0.52 2.29 77.29 ↑ 2.93 82.25 ↑ 3.66 85.79 ↑ 0.94 6.04 84.44 ↑ 5.54 83.03 ↑ 5.96 84.23 ↑ 0.70 5.10 86.27 ↑ 4.73 85.20 ↑ 6.05 88.43 ↑ 0.61 4.38 86.07 ↑ 3.96 84.60 ↑ 5.89 89.64 ↑ 1.01 12.48 91.91 ↑ 8.11 87.55 ↑ 12.16 91.69 ↑ 0.77 11.75 93.45 ↑ 7.02 89.03 ↑ 9.61 91.99 ↑ 0.64 10.38 93.83 ↑ 6.14 89.58 ↑ 8.76 92.69 ↑ 1.22 20.51 94.05 ↑ 13.03 90.64 ↑ 20.69 94.10 ↑ 0.93 19.04 95.12 ↑ 11.59 91.98 ↑ 17.33 94.63 ↑ 0.77 15.71 95.10 ↑ 10.48 92.65 ↑ 17.68 95.64 ↑ 2/255 25.52 50.95 49.91 ↑ 4/255 25.52 61.90 58.77 ↑
CIFAR-10 6/255 25.52 68.36 62.67 ↑ 8/255 25.52 71.92 64.52 ↑ 16/255 26.61 78.13 65.94 ↑ – – – – – – – – – – 57.20 55.38 ↑ 16.40 49.83 67.09 ↑ 65.30 60.92 ↑ 16.40 61.46 73.32 ↑ 70.20 63.65 ↑ 16.70 67.28 75.18 ↑ 72.50 64.80 ↑ 16.93 70.54 76.00 ↑ 78.90 66.27 ↑ 17.16 78.27 78.08 ↑ – – – – – – – – – – 54.21 69.75 ↑ 13.81 48.20 71.35 ↑ 62.63 73.81 ↑ 13.81 61.22 77.44 ↑ 67.69 75.33 ↑ 13.88 66.99 79.28 ↑ 70.75 76.07 ↑ 13.88 70.35 80.27 ↑ 78.33 78.09 ↑ 14.12 78.03 81.90 ↑ – – – – – – – – – – 54.39 74.61 ↑ 61.95 77.71 ↑ 67.56 79.46 ↑ 70.72 80.37 ↑ 78.31 81.97 ↑
Table 2. Accuracy (%) of models trained by LLM, LBP, AIBP, and
AC (our method) on MNIST and CIFAR-10.
Dataset
MNIST
CIFAR-10
MNIST
CIFAR-10
MNIST
CIFAR-10 l l a m s
-M
D m u i d e m
M
D
-e g r a l
-M
D
ϵ 0.1 0.2 0.3 0.4 2/255 4/255 6/255 8/255 16/255 0.1 0.2 0.3 0.4 2/255 4/255 6/255 8/255 16/255 0.1 0.2 0.3 0.4 2/255 4/255 6/255 8/255 16/255
AC 99.11 99.06 98.99 98.78 74.48 74.48 74.48 74.48 73.39 99.31 99.30 99.23 99.07 83.60 83.60 83.30 83.07 82.84 99.48 99.39 99.36 99.23 86.19 86.19 86.12 86.12 85.88
LLM 98.43 97.15 94.38 94.46 64.70 55.07 49.29 44.34 32.58 98.76 98.13 95.04 93.60 66.00 55.09 48.38 41.19 33.65 98.95 98.41 95.90 96.14 62.50 56.99 49.58 43.26 33.03
LBP 96.63 96.94 96.65 96.65 – – – – – 97.37 97.36 97.35 97.36 – – – – – 97.79 97.79 97.77 97.79 – – – – –
AIBP 98.36 97.89 96.35 96.14 59.20 49.69 41.94 39.52 30.74 98.65 98.42 97.45 97.44 62.04 52.37 46.26 40.91 31.32 98.96 98.47 98.05 97.78 62.33 52.73 45.13 41.54 30.19 the one of the networks trained in competitors. Namely,
ABSCERT can better resist perturbations, and the models trained by ABSCERT have stronger robustness guarantees.
Experiment II: Efficiency. Table 3 shows the average training and verification time. ABSCERT consumes less training time than the competitors for all datasets and mod-els; in particular, compared to LLM, our method achieves up to 26.43x speedup on DM-large of MNIST. Addition-ally, LBP and AdvIBP can hardly be applied to CIFAR-10 (3200 epochs required), while ABSCERT runs smoothly (needs only 30 epochs). This indicates less time (and mem-ory) overhead for ABSCERT.
Regarding the verification overhead, ABSCERT achieves up to 602.5x speedup and is scalable to large models trained on CIFAR-10. That is mainly because our verification ap-proach treats the networks as black boxes thanks to the abstraction-based training method.
Experiment III: Applicability. We show that our approach is applicable to both CNNs and FNNs with various activa-tion functions, such as Sigmoid and Tanh. Table 4 shows the verified errors for both types of neural networks trained by our approach. We observe that the verified errors of those neural networks trained on MNIST (resp. CIFAR-10) datasets are all below 14% (resp. 66%), which are smaller than the benchmark counterparts verified in the work [40].
Experiment IV: Scalability. We show our method is scal-able with respect to training four larger neural network ar-chitectures: AlexNet, VGG11, Inception V1, and ResNet18 on ImageNet. Table 5 shows the verified errors and train-ing times. The number of trainable parameters varies from 11 million to 138 million. Compared with the reported er-rors and training times on those representative large mod-els [16, 22, 38], our approach achieves competitive perfor-mance. It is worth mentioning that the reported errors are computed on the testing sets but not verified because those models are too large and cannot be verified by existing veri-fication methods [19, 39, 40,45, 47, 64] due to the high com-putational complexity.
In contrast, ABSCERT can verify fairly large networks for its black-box feature. 6.