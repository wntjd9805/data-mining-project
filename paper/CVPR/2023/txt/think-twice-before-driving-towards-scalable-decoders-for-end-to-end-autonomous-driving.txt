Abstract
End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the en-coder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle’s future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the mas-sive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combina-tion of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process.
In this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capac-ity of the encoder; (2) increasing the capacity of the de-coder. Concretely, we first predict a coarse-grained fu-ture position and action based on the encoder features.
Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive ac-cordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the pre-dicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capac-ity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art perfor-mance in closed-loop benchmarks. Extensive ablation stud-ies demonstrate the effectiveness of each proposed module. 1.

Introduction
With the advance in deep learning, autonomous driv-ing has attracted attention from both academia and in-dustry. End-to-end autonomous driving [46, 48] aims to build a fully differentiable learning system that is able to map the raw sensor input directly to a control signal or a future trajectory. Due to its efficiency and ability to avoid cumulative errors, impressive progress has been achieved in recent years [3, 10, 12, 15, 16, 56]. State-of-the-art works [9,27,49,57,67,68] all adopt the encoder-decoder paradigm. The encoder module extracts information from raw sensor data (camera, LiDAR, Radar, etc.) and gener-ates a representation feature. Taking the feature as input, the decoder directly predicts way-points or control signals.
Under such a paradigm, the encoder does not have ac-cess to the intended behavior of the ego agent, which leaves the burden of finding out the safety-critical regions from the large perceptive field of massive sensor inputs and infer-ring about the future situations to the decoder. For example, when the ego vehicle is at the intersection, if it decides to go straight, it should check the traffic light across the road, which might consist of only several pixels. If it decides to go right, then it should check whether there are any agents on its potential route and think about how they would re-act to the ego vehicle’s action. Even worse, the decoder is usually several simple multi-layer perceptrons (MLP) or
GRUs while the encoder is a delicately designed combina-tion of the heavy ResNet or Transformer. Such unmatched resource-task division hampers the overall learning process.
To address the aforementioned issues, we design our new model based on two principles:
• Fully utilize the capacity of the encoder.
Instead of leaving all future-related tasks to the decoder, we should reuse the features from the encoder conditioned on the predicted decision.
• Extend the capacity of the decoder with dense su-pervision.
Instead of simply adding depth/width of
MLP which would cause severe overfit, we should en-large the encoder with prior structure and correspond-ing supervision so that it could capture the inherent driving logical reasoning.
To instantiate these two principles, we propose a cascaded decoder paradigm to predict the future action of the ego ve-hicle in a coarse-to-fine fashion as shown in Fig. 1. Con-cretely, (i) We first adopt an MLP similar to classical ap-proaches to generate the coarse future trajectory and action. (ii) We then retrieve features around the predicted future location from the encoder and further feed them into sev-eral convolutional layers to obtain goal-related scene fea-tures (we denote the module as Look Module and the fea-ture as Look Feature). This follows the intuition that human drivers would check their intended target to ensure safety and legitimacy. (iii) Inspired by the fact that human drivers would anticipate other agents’ future motion to avoid possi-ble collisions, we design a Prediction Module, which takes the coarse action and features of the current scene as input and generates future scene representation features (denoted as Prediction Feature). Considering the difficulty of ob-taining supervision of the future scene representation con-ditioned on the predicted action during open-loop imitation learning, we adopt the teach-forcing technique [2]: during training, we additionally feed samples with ground-truth action/trajectory into Prediction Module and supervise the corresponding Prediction Feature with ground-truth future scene. As for the target of the supervision, we choose fea-tures from Roach [77], an RL-based teacher network with privileged input, which contains decision-related informa-tion. (iv) Based on the Look Feature and Prediction Fea-ture, we predict the offset between the coarse prediction and ground-truth for refinement. The aforementioned process could be stacked cascadedly, which enlarges the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future.
We conducted experiments on two competitive closed-loop autonomous driving benchmarks with CARLA [21] and achieved state-of-the-art performance. We also con-ducted extensive ablation studies to demonstrate the effec-tiveness of the components of the proposed method.
In summary, our work has three-fold contributions: 1. We propose a scalable decoder paradigm for end-to-end autonomous driving, which, to the best of our knowledge, is the first to emphasize the importance of enlarging the capacity of the decoder in this field. 2. We devise a decoder module to look back to the safety critical areas and anticipate the future scene condi-tioned on the predicted action/trajectory, which injects spatial-temporal prior knowledge and dense supervi-sions into the training process. 3. We demonstrate state-of-the-art performance on two competitive benchmarks and conduct extensive abla-tion studies to verify the effectiveness of the proposed module.
We believe that the decoder (decision part) is equally im-portant as the encoder (perception part) in end-to-end au-tonomous driving. We hope our exploration could inspire further efforts in this line of study for the community. 2.