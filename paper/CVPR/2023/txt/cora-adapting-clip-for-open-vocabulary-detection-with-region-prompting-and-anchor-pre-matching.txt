Abstract
Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories be-yond the base categories on which the detector is trained.
Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detec-tor training: (1) the distribution mismatch that happens when applying a VL-model trained on whole images to region recognition tasks; (2) the difficulty of localizing objects of unseen classes. To overcome these obstacles, we propose CORA, a DETR-style framework that adapts
CLIP for Open-vocabulary detection by Region prompt-ing and Anchor pre-matching. Region prompting mitigates the whole-to-region distribution gap by prompting the re-gion features of the CLIP-based region classifier. Anchor pre-matching helps learning generalizable object localiza-tion by a class-aware matching mechanism. We evaluate
CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel classes, which outperforms the pre-vious SOTA by 2.4 AP50 even without resorting to extra training data. When extra training data is available, we train CORA+ on both ground-truth base-category annota-tions and additional pseudo bounding box labels computed by CORA. CORA+ achieves 43.1 AP50 on the COCO OVD benchmark and 28.1 box APr on the LVIS OVD benchmark.
The code is available at https://github.com/tgxs002/CORA. 1.

Introduction
Object detection is a fundamental vision problem that involves localizing and classifying objects from images.
Classical object detection requires detecting objects from a closed set of categories. Extra annotations and training are required if objects of unseen categories need to be detected.
It has attracted much attention on detecting novel categories without tedious annotations, or even detect object from new category, which is currently referred as open-vocabulary de-tection (OVD) [36].
Recent advances on large-scale vision-language pre-trained models, such as CLIP [30], enable new solutions for tackling OVD. CLIP learns a joint embedding space of im-ages and text from a large-scale image-text dataset, which shows remarkable capability on visual recognition tasks.
The general idea of applying CLIP for OVD is to treat it as an open-vocabulary classifier. However, there are two obstacles hindering the effective use of CLIP on tackling
OVD.
How to adapt CLIP for region-level tasks? One trivial solution is to crop regions and treat them as separate im-ages, which has been adopted by multiple recent works
[7, 14, 31, 35]. But the distribution gap between region crops and full images leads to inferior classification accu-racy. MEDet [7] mitigates this issue by augmenting the text feature with image features. However, it requires extra image-text pairs to prevent overfitting to so-called “base” classes that are seen during training. RegionCLIP [40] di-rectly acquires regional features by RoIAlign [17], which is more efficient but cannot generalize well to novel classes without finetuning. The finetuning is costly when adopting a larger CLIP model.
How to learn generalizable object proposals? ViLD [14],
OV-DETR [35], Object-Centric-OVD [31], Region-CLIP [40] need RPN or class-agnostic object detectors [29] to mine potential novel class objects. However, these RPNs are strongly biased towards the base classes on which they are trained, while perform poorly on the novel classes.
MEDet [7] and VL-PLM [39] identify this problem and adopt several handcrafted policies to rule out or merge low-quality boxes, but the performance is still bounded by the frozen RPN. OV-DETR [35] learns generalizable object localization by conditioning box regression on class name embeddings, but at the cost of efficiency issue induced by repetitive per-class inference.
In this work, we propose a new framework based on DE-tection TRansformers (DETR) [6] that incorporates CLIP
into detector training to achieve open-vocabulary detection without additional image-text data. Specifically, we use a
DETR-style object localizer for class-aware object localiza-tion, and the predicted boxes are encoded by pooling the in-termediate feature map of the CLIP image encoder, which are classified by the CLIP text encoder with class names.
However, there is a distribution gap between whole-image features from CLIP’s original visual encoder and the newly pooled region features, leading to an inferior classification accuracy. Thus, we propose Region Prompting to adapt the
CLIP image encoder, which boosts the classification perfor-mance, and also demonstrates better generalization capabil-ity than existing methods. We adopt DAB-DETR [26] as the localizer, in which object queries are associated with dy-namic anchor boxes. By pre-matching the dynamic anchor boxes with the input categories before box regression (An-chor Pre-Matching), class-aware regression can be achieved without the cost of repetitive per-class inference.
We validate our method on COCO [24] and LVIS v1.0 [16] OVD benchmarks. On the COCO OVD bench-mark, our method improves AP50 of novel categories over the previous best method [40] by 2.4 AP50 without train-ing on extra data, and achieves consistent gain on CLIP models of different scales. When compared under a fairer setting with extra training data, our method significantly outperforms the existing methods by 3.8 AP50 on novel categories and achieves comparable performance on base categories. On the LVIS OVD benchmark, our method achieves 22.2/28.1 APr with/w.o. extra data, which signif-icantly outperforms existing methods that are also trained with/w.o. extra data. By applying region prompting on the base classes of COCO, the classification performance on the novel classes is boosted from 63.9% to 74.1%, whereas other prompting or adaptation methods easily bias towards the base classes.
The contributions of this work are summarized as fol-lows: (1) Our proposed region prompting effectively miti-gates the gap between whole image features and region fea-tures, and generalize well in the open-vocabulary setting. (2) Anchor Pre-Matching enables DETR for generalizable object localization efficiently. (3) We achieve state-of-the-art performance on COCO and LVIS OVD benchmarks. vocabulary detector, which proposes conditional matching to solve the missing novel class problem in assignment, but at the cost of inefficient inference. RegionCLIP [40] propose a second-stage pre-training mechanism to adapt the CLIP model to encode region features, and demon-strates its capability on OVD and zero-shot transfer setting.
GLIP [21] jointly learns object localization and VL align-ment. Matthias et al. [15] proposes to finetune a VL aligned model for detection, while we fix the pre-trained VL model for better generalization towards novel categories.
Detection Transformers DETR [6] is an object detection architecture based on transformers that formulates object detection as a set-to-set matching problem, which greatly simplifies the pipeline. Several works address the slow convergence problem of DETR by architectural improve-ment [1,13,26,38] or special training strategies [8,18]. Zhu et al. [1] proposes multi-scale deformable attention mod-ule to efficiently aggregate information from multi-scale feature maps. Gao et al. [13] proposes to modulate the cross-attention in the transformer decoder by anchor box coordinates to accelerate the detector convergence. DAB-DETR [26] formulates the queries in DETR architecture as anchor boxes, which accelerates detector training. Chen et al. [38] proposes Group DETR, which adds auxiliary object queries during training to take advantage of one-to-many matching for faster convergence.
Prompt Tuning Prompting is originated from NLP, and it refers to prepending task instructions before the input sequence to give the language model the hint about the task [5]. Later works [22, 27] explores tuning continuous prompt vectors when few-shot data is available. VPT [19],
Visual Prompting [2, 3] explore prompting in the pixel space.
[20] and [25] prompts pre-trained model for video recognition tasks. [34] proposes class-aware visual prompt tuning to generalize the learned prompts to unseen cate-gories. Recent works demonstrate that prompt tuning is an effective and parameter-efficient way to adapt large-scale pre-trained models to downstream tasks. 3. Method 2.