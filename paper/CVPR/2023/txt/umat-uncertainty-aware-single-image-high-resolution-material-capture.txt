Abstract
We propose a learning-based method to recover normals, specularity, and roughness from a single diffuse image of a material, using microgeometry appearance as our primary cue. Previous methods that work on single images tend to produce over-smooth outputs with artifacts, operate at lim-ited resolution, or train one model per class with little room for generalization. In contrast, in this work, we propose a novel capture approach that leverages a generative network with attention and a U-Net discriminator, which shows out-standing performance integrating global information at re-duced computational complexity. We showcase the perfor-mance of our method with a real dataset of digitized textile materials and show that a commodity flatbed scanner can produce the type of diffuse illumination required as input to our method. Additionally, because the problem might be ill-posed –more than a single diffuse image might be needed to disambiguate the specular reflection– or because the train-ing dataset is not representative enough of the real distribu-tion, we propose a novel framework to quantify the model’s confidence about its prediction at test time. Our method is the first one to deal with the problem of modeling un-certainty in material digitization, increasing the trustwor-thiness of the process and enabling more intelligent strate-gies for dataset creation, as we demonstrate with an active learning experiment. 1.

Introduction
Virtual design, online marketplaces, product lifecycle workflows, AR/VR, videogames, . . . , all require lifelike digital representations of real-world materials (i.e., digital twins). Acquiring these digital copies is typically a cum-bersome and slow process that requires expensive machines and several manual steps, creating roadblocks for scalabil-ity, repeatability, and consistency. Among the many indus-tries requiring digital twins of materials, the fashion indus-try is in a critical position; facing the demand to digitize hundreds of samples of textiles in short periods, which can-not be achieved with the current technology.
Figure 1. Our method digitizes a material taking as input a sin-gle scanned image. Further, it returns a pixel-wise metric of un-certainty σBRDF, computed at test time through probabilistic sam-pling, proven useful for active learning. In the plot we compare the average deviations of the radiance of different renders in the blue crop w.r.t the ground truth (GT) of: 1) the distribution of the probabilistic samples of a model trained with 100% of the data; 2) the deterministic output of that model; 3) the output of a model trained using 40% of the training dataset, sampled by active learn-ing guided by σBRDF and; 4) a model trained using 40% of the training dataset, randomly sampled. The material at the bottom, for which the model shows a higher uncertainty, generates more varied renders and differs most from the ground truth.
In this context, casual capture systems for optical digiti-zation provide a promising path for scalability. These sys-tems leverage handheld devices (such as smartphones), one or more different illuminations, and learning-based priors to estimate the material’s diffuse and specular reflection lobes.
However, existing approaches present several drawbacks that make them unsuitable for practical digitization work-flows. Generative solutions [18, 63] typically produce unre-alistic artifacts. Despite recent attempts to improve tileabil-ity and controllability [73], these solutions are slow to train and to evaluate (requiring online optimization iterations), are limited in resolution, and present challenges for gener-alization (requiring one model per material class). Further, the fact that these methods build on perceptual losses –not pixel losses– to compare the input photo with the generated material entails extra difficulties when it comes to guaran-teeing the repeatability and consistency required for build-Figure 2. Scanner images vs fitted albedos. ing a digital inventory (i.e., color swatches, prints, or other variations). On the other hand, methods that build on dif-ferentiable node graphs [21] overcome the tileability and resolution limitations, yet, they share the problems derived from using perceptual losses and category-specific training.
In this work, we present UMat, a practical, scalable, and reliable approach to digitizing the optical appearance of textile material samples using SVBRDFs. Commonly used
SVBRDFs typically contain two reflection terms: a diffuse term, parameterized by an albedo image, and a specular one, parameterized by normals, specularity, and roughness.
Prior work typically estimates both components, which be-comes a very challenging problem, obtaining over-smooth outputs, and being prone to artifacts [7, 16, 74]. Instead, in this paper, we demonstrate that it is possible to provide ac-curate digitizations of materials leveraging as input a single diffuse image that acts as albedo and estimating the specular components using a neural network. Our key observation is to realize that most of the appearance variability of textile materials is due to its microgeometry and that a commodity flatbed scanner can approximate the type of diffuse illumi-nation that we require for the majority of textile materials (see Figure 2).
Nevertheless, single-image material estimation is still an ill-posed problem in our setting, as reflectance properties may not be directly observable from a single diffuse image.
To account for these non-directly observable properties, we propose a novel way to measure the model’s confidence about its prediction at test time. Leveraging Monte Carlo (MC) Dropout [10], we propose an uncertainty metric com-puted as the variance of sampling and evaluating multiple estimations for a single input in a render space. We show that this confidence directly correlates with the accuracy of the digitization, which helps identify ambiguous inputs, out-of-distribution samples, or under-represented classes.
Besides increasing the trustworthiness of the capture pro-cess, our confidence quantification enables smarter strate-gies for dataset creation, as we demonstrate with an active learning experiment.
We pose the estimation as an Image-to-Image Transla-tion problem (I2IT) that directly regresses roughness, spec-ular, and normals, from a single input image. Under the hood, our novel residual architecture has a single encoder enhanced with lightweight attention modules [45, 66] for improving global consistency and reducing artifacts, spe-cialized decoders for each target reflectance map, and a U-Net discriminator [57], which enhances generalization.
In summary, we present the following contributions:
• A novel material capture system which leverages the diffuse illumination provided by flatbed scanners for high-resolution, scalable and reliable digitizations.
• An attention-enhanced GAN model and training pro-cedure designed for maximizing accuracy and sharp-ness, and removing undesired artifacts.
• A generic uncertainty quantification framework for material capture algorithms which correlates with pre-diction error on a render space. 2.