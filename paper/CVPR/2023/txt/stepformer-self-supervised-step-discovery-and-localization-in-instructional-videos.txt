Abstract
Instructional videos are an important resource to learn procedural tasks from human demonstrations. However, the instruction steps in such videos are typically short and sparse, with most of the video being irrelevant to the pro-cedure. This motivates the need to temporally localize the instruction steps in such videos, i.e. the task called key-step localization. Traditional methods for key-step local-ization require video-level human annotations and thus do not scale to large datasets. In this work, we tackle the prob-lem with no human supervision and introduce StepFormer, a self-supervised model that discovers and localizes instruc-tion steps in a video. StepFormer is a transformer decoder that attends to the video with learnable queries, and pro-duces a sequence of slots capturing the key-steps in the video. We train our system on a large dataset of instruc-tional videos, using their automatically-generated subtitles as the only source of supervision. In particular, we super-vise our system with a sequence of text narrations using an order-aware loss function that filters out irrelevant phrases.
We show that our model outperforms all previous unsuper-vised and weakly-supervised approaches on step detection and localization by a large margin on three challenging benchmarks. Moreover, our model demonstrates an emer-gent property to solve zero-shot multi-step localization and outperforms all relevant baselines at this task. 1.

Introduction
Observing someone perform a task (e.g. cooking, assem-bling furniture or changing a tire) is a common approach for humans to acquire new skills. Instructional videos provide an excellent large-scale resource to learn such procedural activities for both humans and AI agents. Consequently, in-structional video datasets [24,34,41] have recently received significant attention and been used for various video under-standing tasks, e.g. [1, 5, 13, 23, 28, 38, 39]. A potential im-Figure 1. StepFormer for instruction step discovery and local-ization. StepFormer is a transformer decoder trained to discover instruction steps in a video, supervised purely from video subti-tles. At inference, it only needs the video to discover an ordered sequence of step slots and temporally localize them in the video. pediment with using instructional videos from the web is that they tend to be long and noisy, i.e. a limited number of frames in the video correspond to the instruction steps, while the remaining video segments are unrelated to the task (e.g. title frames, close-ups of people talking and product advertisements). Thus, a major challenge with instructional videos is filtering out the uninformative frames and focus-ing only on the task-relevant segments, i.e. the key-steps.
For instance, in a procedure of making a cake the key-steps could be “crack eggs”, “add sugar”, “add flour”, then “mix”, etc. As a result, many recent efforts tackle the problem of instruction key-step localization, e.g. [9, 10, 23, 24, 34, 41].
Most previous work aiming at temporally localizing key-steps from instructional videos rely on some form of su-pervision. Fully supervised approaches require start and end times of each step [34, 40]. Weakly supervised ap-proaches either rely on knowledge of steps present in the video in the form of a set [20], ordered steps transcript [2,9] or partially ordered steps captured with a flow graph [10].
Unsupervised approaches aim at directly detecting and lo-calizing key-steps without a priori knowledge of instruc-tion steps comprising videos [11, 15, 18, 30]. Conceptu-ally, these approaches are appealing for applications with large datasets, as they eschew the need for expensive and ambiguous labeling efforts. In practice, previous unsuper-vised approaches rely on knowing the video-level task label at training time [11, 18, 30], and thus are not fully unsuper-vised. Moreover, so far they have been only applied to small instructional video datasets (up to 3K videos), as their task-specific models are not designed to handle large databases.
As a result, state-of-the-art procedure learning methods are not deployable at scale and without human supervision.
To address these challenges, we present StepFormer, a novel self-supervised approach that simultaneously discov-ers and temporally localizes procedure key-steps in long untrimmed videos, as illustrated in Figure 1. StepFormer takes a video as input and outputs an ordered sequence of step slots, capturing instruction key-steps as they happen in the video. Notably, it does not rely on any human an-notations at training or inference time.
Instead, we train our model on a large instructional video dataset and use the accompanying narrations obtained from automated speech recognition (ASR) [29, 30] as the only source of supervi-sion. StepFormer is implemented as a transformer decoder with learnable input queries. Similar to the learnable ob-ject queries in DETR [3], StepFormer’s queries learn to at-tend to informative video segments and thus can be viewed as step proposals. To enforce the output step slots to fol-low the temporal order, we use an order-aware loss based on temporal alignment of the learned steps and video nar-rations. Since video narrations tend to be noisy and do not always describe visually groundable steps, we use a flexible sequence-to-sequence alignment algorithm, Drop-DTW [9], which allows for non-alignable narrations to be dropped. To localize the predicted step slots in the video, we explicitly use their learned temporal order. Precisely,
Drop-DTW aligns informative step slots with the video, and outputs start and end times for every detected step.
We train our system on HowTo100M [24], a large instructional video dataset with no human annotations.
For evaluation, we use three standard instructional videos benchmarks, i.e. CrossTask [41], ProceL [11] and COIN
[34]. Empirically, for unsupervised step localization, our self-supervised method outperforms all weakly- and un-supervised baselines on all three downstream datasets, with-out any dataset-specific adaption. Additionally, we demon-strate an emergent property of our model to perform zero-shot key-step localization from a text prompt (i.e. without finetuning on the target dataset), where it also outperforms all relevant baselines.
Contributions. The contributions of our paper are three-(i) We present StepFormer, a novel self-supervised fold. approach to key-step discovery and localization in instruc-tional videos. (ii) We model the temporal order of steps ex-plicitly, and use it to design effective training and inference (iii) We supervise StepFormer only with video methods. subtitles on a large instructional video dataset, and success-fully transfer the model to three downstream datasets with-out any finetuning. On all three datasets, StepFormer estab-lishes a new state of the art on unsupervised step localiza-tion, outperforming unsupervised and weakly-supervised baselines. We are commited to releasing our code. 2.