Abstract
Shadows in videos are difficult to detect because of the large shadow deformation between frames.
In this work, we argue that accounting for shadow deformation is essen-tial when designing a video shadow detection method. To this end, we introduce the shadow deformation attention trajectory (SODA), a new type of video self-attention mod-ule, specially designed to handle the large shadow defor-mations in videos. Moreover, we present a new shadow contrastive learning mechanism (SCOTCH) which aims at guiding the network to learn a unified shadow represen-tation from massive positive shadow pairs across differ-ent videos. We demonstrate empirically the effectiveness of our two contributions in an ablation study. Furthermore, we show that SCOTCH and SODA significantly outperforms existing techniques for video shadow detection. Code is available at the project page: https://lihaoliu-cambridge.github.io/scotch_and_soda/ 1.

Introduction
Shadow is an inherent part of videos, and they have an adverse effect on a wide variety of video vision tasks.
Therefore, the development of robust video shadow detec-tion techniques, to alleviate those negative effects, is of great interest for the community. Video shadow detection is usually formulated as a segmentation problem for videos, however and due to the nature of the problem, shadow de-tection greatly differs from other segmentation tasks such as object segmentation. For inferring the presence of shad-ows in an image, one has to account for the global content information such as light source orientation, and the pres-ence of objects casting shadows.
Importantly, in a given video, shadows considerably change appearance (deforma-tion) from frame to frame due to light variation and object motion. Finally, shadows can span over different back-grounds over different frames, making approaches relying
Figure 1. Overview of our SCOTCH and SODA framework. A MiT encoder extracts multi-scale features for each frame of the video (stage 1). Then, our deformation attention trajectory is applied to features individually to incorporate temporal information (stage 2). Finally, an MLP layer combines the multi-scale information to generate the segmentation masks (stage 3). The model is trained to contrast shadow and non-shadow features, by minimising our shadow contrastive loss with massive positive shadow pairs. on texture information unreliable.
Particularly, video shadow detection methods can be broadly divided into two main categories. The first category refers to image shadow detection (ISD) [9,15,35,36,43,46].
This family of techniques computes the shadow detec-tion frame by frame. Although computationally saving, these methods are incapable of handling temporal informa-tion. The second category refers to video shadow detection (VSD) [6, 9, 14, 16, 25]. These methods offer higher per-formance as the analysis involves spatial-temporal informa-tion. Hence, our main focus is video shadow detection.
State-of-the-art video shadow detection methods rely on deep neural networks, which are trained on large annotated datasets. Specifically, those methods are composed of three parts: (i) a feature extraction network that extracts spatial features for each frame of the video: (ii) a temporal ag-gregation mechanism [6, 14] enriching spatial features with information from different frames; and (iii) a decoder, that maps video features to segmentation masks. Additionally, some works enforce consistency between frames prediction by using additional training criterion [9,25]. We retain from these studies that the design of the temporal aggregation mechanism and the temporal consistency loss is crucial to the performance of a video shadow detection network, and we will investigate both of those aspects in this work.
The current temporal aggregation mechanisms available in the literature were typically designed for video tasks such as video action recognition, or video object segmenta-tion. Currently, the most widely used temporal aggregation mechanism is based on a variant of the self-attention mech-anism [1, 29, 32, 40, 41]. Recently, trajectory attention [29] has been shown to provide state-of-the-art results on video
Intuitively, trajectory attention aggregates in-processing. formation along the object’s moving trajectory, while ignor-ing the context information, deemed as irrelevant. However, shadows in videos are subject to strong deformations, mak-ing them difficult to track, and thus they might cause the trajectory attention to fail.
In this work, we first introduce the ShadOw Deformation
Attention trajectory (SODA), a spatial-temporal aggregation mechanism designed to better handle the large shadow de-formations that occur in videos. SODA operates in two steps.
First, for each spatial location, an associated token is com-puted between the given spatial location and the video, which contains information in every time-step for the given spatial location. Second, by aggregating every associated spatial token, a new token is yielded with enriched spatial deformation information. Aggregating spatial-location-to-video information along the spatial dimension helps the net-work to detect shape changes in videos.
Besides, we introduce the Shadow COnTrastive meCH-anism (SCOTCH), a supervised contrastive loss with massive positive shadow pairs aiming to drive our network to learn more discriminative features for the shadow regions in dif-ferent videos. Specifically, in training, we add a contrastive loss at the coarsest layer of the encoder, driving the fea-tures from shadow regions close together, and far from the features from the non-shadow region. Intuitively, this con-trastive mechanism drives the encoder to learn high-level representations of shadow, invariant to all the various fac-tors of shadow variations, such as shape and illumination.
In summary, our contributions are as follows:
• We introduce a new video shadow detection frame-work, in which we highlight: – SODA, a new type of trajectory attention that har-monise the features of the different video frames at each resolution. – SCOTCH, a contrastive loss that highlights a mas-sive positive shadow pairs strategy in order to make our encoder learn more robust high-level representations of shadows.
• We evaluate our proposed framework on the video shadow benchmark dataset ViSha [6], and compare with the state-of-the-art methods. Numerical and vi-sual experimental results demonstrate that our ap-proach outperforms, by a large margin, existing ones on video shadow detection. Furthermore, we provide an ablation study to further support the effectiveness of the technical contributions. 2.