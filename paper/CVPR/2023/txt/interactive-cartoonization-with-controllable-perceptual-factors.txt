Abstract
Cartoonization is a task that renders natural photos into cartoon styles. Previous deep cartoonization methods only have focused on end-to-end translation, which may hinder editability. Instead, we propose a novel solution with edit-ing features of texture and color based on the cartoon cre-ation process. To do that, we design a model architecture to have separate decoders, texture and color, to decouple these attributes. In the texture decoder, we propose a tex-ture controller, which enables a user to control stroke style and abstraction to generate diverse cartoon textures. We also introduce an HSV color augmentation to induce the networks to generate diverse and controllable color transla-tion. To the best of our knowledge, our work is the first deep approach to control the cartoonization at inference while showing profound quality improvement over to baselines. 1.

Introduction
Cartoons gain steep popularity in a recent, and the num-ber of cartoon creators have also increased. The univer-sal workflow of cartoon painters is as follows: character drawing, which is then composed into a background scene.
Post-processing such as shading is added afterward. Profes-sional tools [1, 3] provide helpful plugins to assist the artist.
Despite this, cartoon creation still remains an arduous task even for the more skilled creators.
We follow the observation that cartoon-styled scene gen-eration has received notable attention. Many artists con-vert real-world photographs into cartoon styles to utilize as a background scene, dubbed as image cartoonization. This allows creators to more focus on effective decisions in mak-ing cartoons, such as character generation. It is shown that deep learning-based cartoonization approach is able to pro-duce cartoon-stylized output with a prominent quality, that is possible to be utilized in real service production [5,6,19].
However, the previous deep methods skip the intermedi-ate procedures of cartoon-making processes, thus disabling the creators from controlling outputs. The artists follow a series of structured steps when creating a cartoon back-ground from a photo (Figure 2). 1) Color stylization, where the author changes the color both locally and globally. Sky synthesis is performed along with this procedure. 2) Texture stylization, where additional sketch lines are drawn, and fine details are selectively removed to achieve the different
(a) Source photo (b) Sky synthesis (c) Color stylization (d) Texture stylization (e) Post-processing
Figure 2. Example of the background making process. We visualize how the artist creates the background scene by the step-by-step procedure. Note that some steps can be skipped or changed in the order depending on the artist. ©Kawaii Studio levels of abstraction. 3) Post-processing, which includes lighting and image filters. Unfortunately, due to the end-to-end inference nature of the previous deep cartoonization methods, the artist has no control over the generation pro-cess. The creators may only intervene with a source photo (Figure 2a) or the final output (Figure 2e), which harms the usability of the cartoonization methods in artists’ workflow.
In this study, we present an effective approach to embed-ding interactivity in cartoonization. The proposed solution focuses on building a pipeline for more controllable texture and color. We define texture control as the manipulation of stroke thickness and abstractions. This concept can be utilized in many scenarios; the artist can abstract the de-tails of the far-distance scene to depict the natural perspec-tive or emphasize the details of the character. The creators can also choose to change the delicacy of the brushstroke to match the texture of the foreground objects when compos-ing the scene. As for color control, we aim to build a control system in which the creator freely manipulates arbitrary re-gions with the desired color. This is designed to assist the artist in the color stylization procedure (Figure 2c).
To obtain user controllability in cartoonization, we sep-arately build texture and color decoders to minimize inter-ference across the features (Figure 3). We also found that the decomposed architecture provides a robust and superb quality of texture stylization. For texture control, we in-vestigated the role of the receptive field and the target im-age resolution in the level of stroke thickness and abstrac-tion. Based on these observations, we present a texture controller, which adjusts the receptive field of the network through a dynamic replacement of the intermediate features.
For color control, we jointly train the color decoder in a su-pervised manner with the paired dataset that is built based on the proposed HSV augmentation. Throughout this train-ing strategy, the color module gains the ability to produce diverse colors. With the combination of the decoupled tex-ture and color modules, we achieve a two-dimension of con-trol space that can create a variety of cartoonized results upon user communication. Such a design also provides ro-bust and perceptually high-quality cartoonized outcomes.
To the best of our knowledge, our framework is the first approach that presents interactivity to deep learning-based
Figure 3. Model overview. Given a photo, CARTOONER estimates the stylized texture and color images, which are then composed for the final product. We design decomposed texture/color paths, a texture controller, and a multi-texture discriminator for interaction. cartoonization. Based on the proposed solution, we demon-strate application scenarios that permit user intentions to create cartoonized images with diverse settings. Extensive experiments demonstrate that the proposed solution outper-forms the previous cartoonization methods in terms of per-ceptual quality, while also being able to generate multiple images based on the user’s choices of texture and color. 2.