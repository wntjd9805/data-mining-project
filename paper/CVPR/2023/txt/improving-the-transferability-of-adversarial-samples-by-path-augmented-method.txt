Abstract
Deep neural networks have achieved unprecedented suc-cess on diverse vision tasks. However, they are vulnerable to adversarial noise that is imperceptible to humans. This phe-nomenon negatively affects their deployment in real-world scenarios, especially security-related ones. To evaluate the robustness of a target model in practice, transfer-based at-tacks craft adversarial samples with a local model and have attracted increasing attention from researchers due to their high efficiency. The state-of-the-art transfer-based attacks are generally based on data augmentation, which typically augments multiple training images from a linear path when learning adversarial samples. However, such methods se-lected the image augmentation path heuristically and may augment images that are semantics-inconsistent with the target images, which harms the transferability of the gen-erated adversarial samples. To overcome the pitfall, we propose the Path-Augmented Method (PAM). Specifically,
PAM first constructs a candidate augmentation path pool.
It then settles the employed augmentation paths during ad-versarial sample generation with greedy search. Further-more, to avoid augmenting semantics-inconsistent images, we train a Semantics Predictor (SP) to constrain the length of the augmentation path. Extensive experiments confirm that PAM can achieve an improvement of over 4.8% on av-erage compared with the state-of-the-art baselines in terms of the attack success rates. 1.

Introduction
Deep neural networks (DNNs) appear to be the state-of-the-art solutions for a wide variety of vision tasks [21,
*Corresponding author.
Figure 1. Illustration of how SIM and our PAM augment images (red dots) during the generation of adversarial samples. SIM only considers one linear path from the target image X to a baseline image X ′. Besides, SIM may augment images that are semantics-inconsistent with the target image. In contrast, our PAM augments images along multiple augmentation paths. We also constrain the length of the path to avoid augmenting images that are semantics-inconsistent with the target one. 28]. However, DNNs are vulnerable to adversarial sam-ples [47], which are elaborately designed by adding human-imperceptible noise to the clean image to mislead DNNs into wrong predictions. The existence of adversarial sam-ples causes negative effects on security-sensitive DNN-based applications, such as self-driving [27] and medical diagnosis [6, 7]. Therefore, it is necessary to understand the DNNs [15, 32, 33, 40] and enhance attack algorithms to better identify the DNN model’s vulnerability, which is the first step to improve their robustness against adversarial samples [23, 24, 42].
There are generally two kinds of attacks in the litera-ture [9]. One is the white-box attacks, which consider the white-box setting where attackers can access the architec-tures and parameters of the victim models. The other is the black-box attacks, which focus on the black-box situ-ation where attackers fail to get access to the specifics of
the victim models [10, 39]. Black-box attacks are more ap-plicable than the white-box counterparts to real-world sys-tems. There are two basic black-box attack methodolo-gies: the query-based [1, 2, 37] and the transfer-based at-tacks [38, 41]. Query-based attacks interact with the victim model to generate adversarial samples, but they may incur excessive queries. In contrast, transfer-based attacks craft adversarial samples with a local source model and do not need to query the victim model. Therefore, transfer-based attacks have attracted more attention recently because of their high efficiency [10, 39].
However, transfer-based attacks generally craft adversar-ial samples by employing white-box strategies like the Fast
Gradient Sign Method (FGSM) [11] to attack a local model, which often leads to limited transferability due to overfit-ting to the employed local model. Most existing solutions address the overfitting issue from the perspective of opti-mization and generalization, which regards the local model and the target image as the training data of the adversar-ial sample. Therefore, the transferability of the learned adversarial sample corresponds to its generalization abil-ity across attacking different models [20]. Such method-ologies to improve adversarial transferability can be cate-gorized into two groups. One is the optimizer-based ap-proaches [9, 20, 34, 36], which adopt more advanced opti-mizers to escape from poor local optima during the genera-tion of adversarial samples. The other is the augmentation-based methods [10,20,35,44], which resort to data augmen-tation and exploit multiple training images to learn a more transferable adversarial sample.
Current state-of-the-art augmentation-based attacks gen-erally apply a heuristics-based augmentation method. For example, the Scale-Invariant attack Method (SIM) [20] aug-ments multiple scale copies of the target image, while Ad-mix [35] augments multiple scale copies of the mixtures of the target image and the images from other categories. SIM exponentially augments images along a linear path from the target image to a baseline image, which is the origin. Ad-mix, in contrast, first augments the target image with the mixture of the target image and the images from other cate-gories. Then it also exponentially augments images along a linear path from the mixture image to the origin. Therefore, such methods only consider the image augmentation path to one baseline image, i.e., the origin. Besides, although they attempt to augment images that are semantics-consistent to the target image [20, 35], they fail to constrain the length of the image augmentation path, which may result in augment-ing semantics-inconsistent images.
To overcome the pitfalls of existing augmentation-based attacks, we propose a transfer-based attack called Path-Augmented Method (PAM). PAM proposes to augment im-ages from multiple image augmentation paths to improve the transferability of the learned adversarial sample. How-ever, due to the continuous space of images, the possible image augmentation paths starting from the target image are countless.
In order to cope with the efficiency prob-lem, we first select representative path directions to con-struct a candidate augmentation path pool. Then we settle the employed augmentation paths during adversarial sam-ple generation with greedy search. Furthermore, to avoid augmenting semantics-inconsistent images, we train a Se-mantics Predictor, which is a lightweight neural network, to constrain the length of each augmentation path.
The difference between our PAM and SIM is illustrated in Figure 1. During the generation of adversarial samples,
PAM augments images along multiple image augmentation paths from the target image to different baseline images, while SIM only augments images along a single image aug-mentation path from the target image to the origin. Be-sides, PAM constrains the length of the image augmenta-tion path to avoid augmenting images that are far away from the target image and preserve the semantic meaning of the target image. In contrast, SIM may augment images that are semantics-inconsistent with the target image due to the overlong image augmentation path.
To confirm the superiority of our PAM, we conduct ex-tensive experiments against both undefended and defended models on the ImageNet dataset. Experimental results show that our PAM can achieve an improvement of over 3.7% on average compared with the state-of-the-art baselines in terms of the attack success rates. We also evaluate the per-formance of the combination of PAM with other compatible attack methods. Again, experimental results confirm that our method can significantly outperform the state-of-the-art baselines by about 7.2% on average.
In summary, our contributions in this paper are threefold:
• We discover that the state-of-the-art augmentation-based attacks (SIM and Admix) actually augment training images from a linear path for learning adver-sarial samples. We argue that they suffer from limited and overlong augmentation paths.
• To address their pitfalls, We propose the Path-Augmented Method (PAM). PAM augments images from multiple augmentation paths during the genera-tion of adversarial samples. Besides, to make the aug-mented images preserve the semantic meaning of the target image, we train a Semantics Predictor (SP) to constrain the length of each augmentation path.
• We conduct extensive experiments to validate the ef-fectiveness of our methodologies. Experimental re-sults confirm that our approaches can outperform the state-of-the-art baselines by a margin of over 3.7% on average. Besides, when combined with other compati-ble strategies, our method can significantly surpass the state-of-the-art baselines by 7.2% on average.
2.