Abstract
Standard Unsupervised Domain Adaptation (UDA) meth-ods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably af-fects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweight-ing strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Further-more, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is pro-posed to identify and exclude negative pairs made of samples sharing the same class, even in presence of some noise in the pseudo-labels. Our method outperforms previous methods on three major benchmarks by a large margin. We set the new
SF-UDA state-of-the-art on VisDA-C and DomainNet with a performance gain of +1.8% on both benchmarks and on
PACS with +12.3% in the single-source setting and +6.6% in multi-target adaptation. Additional analyses demonstrate that the proposed approach is robust to the noise, which re-sults in significantly more accurate pseudo-labels compared to state-of-the-art approaches. 1.

Introduction
Deep learning methods achieve remarkable performance in visual tasks when the training and test sets share a similar distribution, while their generalisation ability on unseen data decreases in presence of the so called domain shift [18, 48].
Moreover, DNNs require a huge amount of labelled data to be trained on a new domain entailing a considerable cost for collecting and labelling the data. Unsupervised Domain
Adaptation (UDA) approaches aim to transfer the knowledge learned on a labelled source domain to an unseen target domain without requiring any target label [2, 13, 22, 57].
Common UDA techniques have the drawback of requiring access to source data while they are adapting the model to the target domain. This may not always be possible in many applications, i.e. when data privacy or transmission bandwidth become critical issues. In this work, we focus on the setting of Source-free adaptation (SF-UDA) [37, 55, 61, 66], where source data is no longer accessible during adaptation, but only unlabelled target data is available. As a result, standard UDA methods cannot be applied in the
SF-UDA setting, since they require data from both domains.
Recently, several SF-UDA methods have been proposed, focusing on generative models [36,43], class prototypes [37], entropy-minimisation [37, 61], self-training [37] and auxil-iary self-supervised tasks [55]. Yet, generative models re-quire a large computational effort to generate images/features in the target style [36], entropy-minimisation methods of-ten lead to posterior collapse [37] and the performance of self-training solutions [37] suffer from noisy pseudo-labels.
Self supervision and pseudo-labelling have also been intro-duced as joint SF-UDA strategies [55, 60], raising the issue of choosing a suitable pretext task and refining pseudo-labels.
For instance, Chen et al. [5] propose to refine predictions within a self-supervised strategy. Yet, their work does not take into account the noise inherent in pseudo-labels, which leads to equally weighting all samples without explicitly ac-counting for their uncertainty. This may cause pseudo-labels with high uncertainty to still contributing in the classification loss, resulting in detrimental noise overfitting and thus poor adaptation.
In this work, we propose a novel Source-free adaptation approach that builds upon an initial pseudo-labels assign-ment (for all target samples) performed by using the pre-trained source model, always assumed to be available. To obtain robustness against the noise that inevitably affects such pseudo-labels, we propose to reweight the classification loss based on the reliability of the pseudo-labels. We mea-sure it by estimating pseudo-labels uncertainty, after they
Figure 1. (a) We obtain the refined pseudo-label ˆyt (green circle with black outline) for the current sample by looking at pseudo-labels of neighbour samples. (b) Predictions from neighbours are used to estimate the uncertainty of ˆyt by computing the weight w through Entropy
ˆH. (c) A temporal queue Qe storing predictions at past T epochs, i.e. {e − T, ..., e − 1}, is used within the contrastive framework to exclude pairs of samples sharing the same class from the list of negative pairs (query, key). have been refined by knowledge aggregation from neigh-bours sample, as shown in Figure 1 (a). The introduced loss reweighting strategy penalises pseudo-labels with high uncertainty to guide the learning through reliable pseudo-labels. Differently from previous reweighting strategies, we reweight the loss by estimating the uncertainty of the refined pseudo-labels by simply analysing neighbours’ predictions, as shown in Figure 1 (b).
The process of refining pseudo-labels necessarily requires a regularised target feature space in which neighbourhoods are composed of semantically similar samples, possibly shar-ing the same class. With this objective, we exploit an aux-iliary self-supervised contrastive framework. Unlike prior works, we introduce a novel negative pairs exclusion strat-egy that is robust to noisy pseudo-labels, by leveraging past predictions stored in a temporal queue. This allows us to identify and exclude negative pairs made of samples belong-ing to the same class, even if their pseudo-labels are noisy, as shown in Figure 1 (c).
We benchmark our method on three major domain adap-tation datasets outperforming the state-of-the-art by a large margin. Specifically, on VisDA-C and DomainNet, we set the new state-of-the-art with 90.0% and 69.6% accuracy, with a gain of +1.8% in both cases, while on PACS we im-prove the only existing SF-UDA baseline [1] by +12.3% and
+6.6% in single-source and multi-target settings. Ablation studies demonstrate the effectiveness of individual compo-nents of our pipeline in adapting the model from the source to the target domain. We also show how our method is able to progressively reduce the noise in the pseudo-labels, better than the state-of-the-art.
To summarise, the main contributions of this work are:
• We introduce a novel loss re-weighting strategy that evaluates the reliability of refined pseudo-labels by es-timating their uncertainty. This enables our method to mitigate the impact of the noise in the pseudo-labels.
To the best of our knowledge, this is the first work that estimates the reliability of pseudo-labels after their refinement.
• We propose a novel negative pairs exclusion strategy which is robust to noisy pseudo-labels, being able to identify and exclude those negative pairs composed of same-class samples.
• We validate our method on three benchmark datasets, outperforming SOTA by a large margin, while also proving the potential of the approach in progressively refining the pseudo-labels.
The remainder of the paper is organized as follows. In
Section 2, we discuss related works in the literature. Sec-tion 3 describes the proposed method. Section 4 illustrates the experimental setup and reports the obtained results. Fi-nally, conclusions are drawn in Section 5 together with a discussion of limitations. 2.