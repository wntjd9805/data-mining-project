Abstract
Self supervision and natural language supervision have emerged as two exciting ways to train general purpose im-age encoders which excel at a variety of downstream tasks.
Recent works such as M3AE [31] and SLIP [63] have sug-gested that these approaches can be effectively combined, but most notably their results use small (<20M examples) pre-training datasets and don’t effectively reﬂect the large-scale regime (>100M samples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We ﬁnd that a combination of two state of the art approaches: masked auto-encoders, MAE [37] and contrastive language image pre-training, CLIP [68] provides a beneﬁt over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no beneﬁt (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training. 1.

Introduction
Large scale pretraining has become a powerful tool in the arsenal of computer vision researchers to produce state of the art results across a wider variety of tasks [39,88,95,98].
However, when pre-training on tens of millions to billions of images it is difﬁcult to rely on standard supervised meth-ods to train models, as datasets of this size often lack re-liable labels. In the presence of these massive but largely under-curated datasets, two general classes of methods to train general purpose image encoders have emerged: 1. Self Supervised techniques that learn visual represen-tations from the image data alone [11, 36] 2. Natural Language Supervised methods that utilize paired free-form text data to learn visual representa-tions [43, 69] proach1, a recent ﬂurry of work has introduced methods that combine both forms of supervision [31, 56, 64, 78] to vary-ing degrees of success. While each of these methods estab-lishes some regime where the additional supervision helps, none of these “joint-supervision” methods advance state of the art in any meaningful way. Additionally, to our knowl-edge none of these methods have shown comparative results at the scale many large scale vision models are currently trained at (>100M examples) [43,66,69,73,80,82,98]. Fur-thermore, methods that use both forms of supervision start with the presumption that the additional supervision is help-ful and either often lack clean ablations or lack evaluations in a “high accuracy” regime—leading to further confusion regarding whether a combination of these methods can ac-tually improve the state of the art. To clarify this issue, in this work, we investigate a simple question:
Does a combination of self supervision and natu-ral language supervision actually lead to higher quality visual representations?
In order to answer this, we ﬁrst introduce a straight-forward baseline approach that combines standard self su-pervision and language supervision techniques. We com-bine masked auto-encoders (MAE) and contrastive lan-guage image-pretraining (CLIP) to make MAE-CLIP. We then present a careful study of the performance of MAE,
M3AE, CLIP and MAE-CLIP across a wide variety of tasks in two distinct regimes: a “low-sample” 2 11.3 million example regime and a “high-sample” 1.4 billion example regime. We train self-supervised and language-supervised methods using the same pre-training datasets under the as-sumption that we have no knowledge about downstream tasks. Our experiments show: 1. In the low sample size regime, without changing the
ﬁnal pooling operation in the network, we observe a large performance improvement, namely 6% on Ima-geNet [18] and 4% on VTAB [105]. However, when 1Self supervised methods can learn representations without labels, but natural language supervision learns better representations. Natural lan-guage supervised methods rely on quality of captions 2We note that what low sample means has changed substantially over
Due to the unique strengths and weaknesses of each ap-the last few years 1
we modify the pooling operation, the improvement substantially decreases to around 1% on both Ima-geNet and VTAB. 2. In the high sample size regime, there is virtually no dif-ference in performance between MAE-CLIP and CLIP across ImageNet, VTAB, and VQA tasks.
We believe our work is the ﬁrst careful study of this form and contextualizes recent progress in both self-supervision and natural language supervision.
The rest of the paper is organized as follows: In Sec-tion 2, we cover related work in the areas of self supervision and natural language supervision. In Section 3, we give an overview of the baseline methods we study, MAE, M3AE,
CLIP and our new baseline MAE-CLIP. Then we present and analyse our small scale and large scale experimental
ﬁndings in Sections 4 and 5. Finally, we discuss potential explanations for our ﬁndings and some future work in 6. 2.