Abstract
The complicated architecture and high training cost of vi-sion transformers urge the exploration of post-training quan-tization. However, the heavy-tailed distribution of vision transformer activations hinders the effectiveness of previ-ous post-training quantization methods, even with advanced quantizer designs. Instead of tuning the quantizer to bet-ter fit the complicated activation distribution, this paper proposes NoisyQuant, a quantizer-agnostic enhancement for the post-training activation quantization performance of vision transformers. We make a surprising theoretical discovery that for a given quantizer, adding a fixed Uniform noisy bias to the values being quantized can significantly reduce the quantization error under provable conditions.
Building on the theoretical insight, NoisyQuant achieves the first success on actively altering the heavy-tailed activation distribution with additive noisy bias to fit a given quantizer.
Extensive experiments show NoisyQuant largely improves the post-training quantization performance of vision trans-former with minimal computation overhead. For instance, on linear uniform 6-bit activation quantization, NoisyQuant improves SOTA top-1 accuracy on ImageNet by up to 1.7%, 1.1% and 0.5% for ViT, DeiT, and Swin Transformer respec-tively, achieving on-par or even higher performance than previous nonlinear, mixed-precision quantization. 1.

Introduction
Inspired by the success of Self Attention (SA)-based transformer models in Natural Language Processing (NLP) tasks [31], recent researches make significant progress in applying transformer models to the field of computer vi-sion [3, 10, 22, 30]. In the meantime, the typical design of
* Equal contribution. (cid:0) Corresponding Author. transformer models induces large model sizes, high compu-tational consumption, and long training time. For instance, the widely used DeiT-Base model [30] contains 86M param-eters, logs 18G floating-point operations for a single input, and requires 300 epochs of training on the ImageNet dataset.
This leads to significant difficulties in hardware deployment.
In facing such difficulty, a number of compression and ac-celeration methods are applied to vision transformer models, including pruning [6, 39], quantization [24, 42], and neural architecture search [5], etc.
Among these methods, quantization appears as one of the most effective and widely applied ways [11]. Quan-tization process uses a predefined “quantizer” function to convert the continuous representation of weights and acti-vations into a small number of discrete symbols, therefore enabling low-precision representations for straightforward memory savings. For DNN models, the approximation er-ror made by the quantizer inevitably leads to performance drop. A series of work focuses on Quantization-Aware Train-ing (QAT) that finetunes the quantized model at low preci-sion [8, 9, 25, 37, 47]. However, given the high training cost and the complicated computation graph of vision transformer models, retraining the model at low precision could be costly and unstable [42]. Alternatively, Post-Training Quantization (PTQ) is preferable for vision transformers as it eliminates the need for re-training or finetuning the quantized model, instead only adjusts the design of the quantizer based on the full-precision pretrained model and a small set of sampled calibration data [1, 2, 23, 32, 36]. For example, linear quan-tizers [7, 29, 36, 38] reduce quantization error by shifting, clipping, and scaling the values to be quantized. Nonlinear quantizers [13, 42, 44] further adjust the width and location of each quantization bin to better fit the distribution.
Unfortunately, though progresses are made in designing better PTQ quantizers, it still appears to be significantly chal-lenging to quantize vision transformer models, especially for activation quantization. Transformer layers produce up
Figure 1. Overview of the NoisyQuant pipeline (orange box) with comparison to EasyQuant [36] (gray box). Histograms of input activations, quantized activations, and layer outputs are illustrated in the pipeline. Starting from the input activation X following a GELU function, where the green histogram is plotted in linear-scale and the gray in log-scale, NoisyQuant adds a fixed NoisyBias N sampled from a Uniform distribution onto X. Adding the noise before quantization flattens the peaks in the activation distribution to make it more friendly to quantization, as visualized in the linear/log-scaled histogram of X + N , and theoretically proved in Sec. 3.2. The noise is removed from the result of the fixed-point activation-weight multiplication with a denoising bias term, which we formulate in Sec. 3.3. The rightmost sub-figures illustrate the layer output distribution of EasyQuant and NoisyQuant. Compared to the output of the floating-point model (yellow), NoisyQuant output follows the distribution closely and achieves up to 60% output error reduction on some transformer layers, as shown in Sec. 4. The resulting PTQ performance improvement of the entire model is provided in Sec. 5. to millions of activation values with sophisticated distribu-tions. For instance, outputs of the GELU function [15] are asymmetrically distributed, with spikes in the histogram at some values and a long tail in a large range (see top-left of Fig. 1). Some linear projection layers also lead to signif-icantly large activation values that are sparsely distributed in a very long tail [23]. Consequently, low-precision PTQ on vision transformer suffers from performance degradation, even utilizing non-linear or mixed-precision quantizers at the cost of additional data communication and computation overheads [23, 43]. No linear uniform PTQ method achieves good performance on vision transformer models.
This paper provides a brand new perspective in dealing with the complicated vision transformer activation distribu-tion. Instead of adding more tricks in the quantizer design to fit the activation distribution, this work explores the potential of actively and cost-effectively altering the distribution be-ing quantized, making it more friendly to a given quantizer.
The pipeline of our proposed method is illustrated in Fig. 1.
Specifically, we make a surprising discovery that for any quantizer, the quantization error can be significantly reduced by adding a fixed noisy bias sampled from a Uniform distri-bution to the activation before quantization. We theoretically prove the condition when the quantization error reduction can be achieved. On this basis, we propose NoisyQuant, a plug-and-play, quantizer-agnostic enhancement on the post-training activation quantization of vision transformer models.
For each layer, we sample a Noisy Bias based on the input activation distribution following our theoretical analysis, and compute the corresponding denoising bias to retain the cor-rect output of the layer. At inference time, the Noisy Bias is added to the input activation before quantization, and the de-noising bias is removed from the layer output. This process significantly reduces the quantization error with minimal computation overhead. NoisyQuant leads to significant im-provement in the PTQ performance of state-of-the-art vision transformers. Applying NoisyQuant on top of a uniform linear quantization achieves on-par performance to SOTA mixed-precision, nonlinear PTQ methods [23, 42]. Adding
NoisyQuant on top of these nonlinear quantizers achieves further performance gain.
To the best of our knowledge, this paper makes the fol-lowing novel contributions:
• Theoretically shows the possibility and proves feasible conditions for reducing the quantization error of heavy-tailed distributions with a fixed additive noisy bias;
• Proposes NoisyQuant, a quantizer-agnostic enhance-ment for PTQ performance on activation quantization.
NoisyQuant achieves the first success in actively refin-ing the distribution being quantized to reduce quantiza-tion error following the theoretical results on additive noisy bias, with minimal computation overhead;
• Demonstrates consistent performance improvement by applying NoisyQuant on top of existing PTQ quantiz-ers. For 6-bit PTQ, NoisyQuant improves the ImageNet top-1 accuracy of uniform linear quantized vision trans-former models by up to 1.7%, and improves SOTA
nonlinear quantizer PTQ4ViT [43] by up to 0.7%. 2.