Abstract
Spatio-temporal video grounding aims to localize the aligned visual tube corresponding to a language query.
Existing techniques achieve such alignment by exploiting dense boundary and bounding box annotations, which can be prohibitively expensive. To bridge the gap, we inves-tigate the weakly-supervised setting, where models learn from easily accessible video-language data without anno-tations. We identify that intra-sample spurious correlations among video-language components can be alleviated if the model captures the decomposed structures of video and lan-guage data. In this light, we propose a novel framework, namely WINNER, for hierarchical video-text understand-ing. WINNER first builds the language decomposition tree in a bottom-up manner, upon which the structural attention mechanism and top-down feature backtracking jointly build a multi-modal decomposition tree, permitting a hierarchi-cal understanding of unstructured videos. The multi-modal decomposition tree serves as the basis for multi-hierarchy language-tube matching. A hierarchical contrastive learn-ing objective is proposed to learn the multi-hierarchy cor-respondence and distinguishment with intra-sample and inter-sample video-text decomposition structures, achieving video-language decomposition structure alignment. Exten-sive experiments demonstrate the rationality of our design and its effectiveness beyond state-of-the-art weakly super-vised methods, even some supervised methods. 1.

Introduction
Spatio-temporal video grounding (STVG) is a funda-mental task for video-language understanding [34, 41]. It aims to localize the spatio-temporal tube described by the
*Equal Contribution.
†Corresponding Authors. language query from the untrimmed video. The essence of grounding lies in the semantic alignment of video and lan-guage components [34, 35, 41, 55]. To achieve such align-ment, existing works fully exploit the fine-grained spatial-temporal annotations (e.g., temporal boundaries, and spa-tial bounding boxes). For example, the spatial and tempo-ral GCN modules in STGRN [55] require region-by-region frame-by-frame annotations for training convolutions.
Despite substantial research literature in this vein, the dense annotations (i.e., video-by-video temporal boundary and frame-by-frame spatial bounding boxes) necessary for effective alignment require tremendous labor costs, which are, however, not routinely available.
In addition, mas-sive video-language data without spatial-temporal annota-tions are easily accessible but without exploitation. To re-duce human labeling costs, we investigate the weakly su-pervised setting, where models learn to localize spatial-temporal tubes on easily accessible video-language data.
In the weakly-supervised setting, the absence of spatial-temporal annotations (i.e., the exactly matched temporal boundaries and spatial bounding boxes) drives the video-Language semantics language alignment problematic. might get spuriously correlated with unmatched visual com-ponents in the untrimmed video. For example, when we inspect the sample shown in Figure 1 at either the video-sentence or the object-word hierarchy (i.e., single-hierarchy understanding), the right-side woman can be hard to dis-tinguish from the target woman to be localized. We iden-tify that such spurious correlations could be alleviated if the model captures that the second woman is being pointed at when inspecting the sample at an intermediate video-language hierarchy. In this regard, we present a novel per-spective for grounding-oriented video disambiguation and understanding under the weakly-supervised setting, i.e., de-composing multiple aligned video-language hierarchies.
There are mainly two technical challenges to achieving such hierarchical alignment. On the one hand, the highly
correspondence between extracted visual cues and the multi-hierarchy language components.
Thanks to the multi-modal decomposition structure, the modality gap between video and text can be rela-tively narrowed in estimating the matching score between multi-hierarchy language components and potential spatial-temporal tube proposals. Upon the matching, we could build the language-grounded video decomposition tree, containing specific tubes as nodes, as revealed in Figure 1. A hierarchical contrastive learning objective is devised to recursively learn the desired correspondence and dis-tinguishment over intra-sample and inter-sample video-text decomposition trees, thus addressing the second challenge.
We conduct experiments on two widely used datasets for the weakly-supervised video object grounding task. Exper-imental results show that the WINNER model greatly out-performs the state-of-the-arts, and could achieve compara-ble performance with some supervised methods. Extended experiments including the ablation study and the case study further demonstrate the rationality of the model involved.
Our contributions are summarized as follows:
• We research the spatio-temporal video grounding task under the challenging weakly supervised setting. We present a novel perspective, i.e., hierarchical video-language decomposition and alignment, for alleviating spurious correlations brought by limited annotations.
• We propose a novel WINNER framework, which en-capsulates the structural attention and top-down back-tracking for multi-modal hierarchical understanding, and the multi-hierarchy intra-sample correspondence and inter-sample distinguishment learning.
• Experimental results demonstrate the rationality of our analysis and the effectiveness of WINNER. 2.