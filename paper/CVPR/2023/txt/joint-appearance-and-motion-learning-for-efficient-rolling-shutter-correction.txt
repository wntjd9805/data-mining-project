Abstract
Rolling shutter correction (RSC) is becoming increas-ingly popular for RS cameras that are widely used in com-mercial and industrial applications. Despite the promis-ing performance, existing RSC methods typically employ a two-stage network structure that ignores intrinsic infor-mation interactions and hinders fast inference. In this pa-per, we propose a single-stage encoder-decoder-based net-work, named JAMNet, for efﬁcient RSC. It ﬁrst extracts pyramid features from consecutive RS inputs, and then si-multaneously reﬁnes the two complementary information (i.e., global shutter appearance and undistortion motion
ﬁeld) to achieve mutual promotion in a joint learning de-coder. To inject sufﬁcient motion cues for guiding joint learning, we introduce a transformer-based motion embed-ding module and propose to pass hidden states across pyra-mid levels. Moreover, we present a new data augmenta-tion strategy “vertical ﬂip + inverse order” to release the potential of the RSC datasets. Experiments on various benchmarks show that our approach surpasses the state-of-the-art methods by a large margin, especially with a 4.7 dB PSNR leap on real-world RSC. Code is available at https://github.com/GitCVfb/JAMNet. 1.

Introduction
As commonly used image sensors in the automotive sec-tor and motion picture industry, CMOS sensors offer par-ticular beneﬁts, including low cost and simplicity in design
[15, 20, 42, 48]. The row-wise readout mechanism from top to bottom of electronic CMOS sensors, however, results in undesirable image distortions called the rolling shutter (RS) effect (also known as the jelly effect, e.g., wobble, skew) when a moving camera or object is in progress. Often, even a small camera motion causes visible geometric distortions in the captured RS image or video. Because of this, the RS effect inevitably becomes a hindrance to scene understand-ing and a nuisance in photography. As such, RS correction (RSC), as a way to make up for such deﬁciencies, is gradu-ally gaining more and more attention [5, 11, 27, 29, 58]. (cid:2) Equal contribution. † Corresponding author (daiyuchao@gmail.com).
Figure 1. Performance vs. Speed. Each circle represents the per-formance of a model in terms of FPS and PSNR on the Fastec-RS testing set [29] with 640 × 480 images using a 3090 GPU.
The radius of each circle denotes the model’s number of parame-ters. Our method achieves state-of-the-art performance with real-time inferences and smaller parameters compared with prior RSC methods, including DeepUnrollNet [29], SUNet [10], JCD [56],
AdaRSC [5], and Cascaded method (i.e., SUNet + DAIN [4]).
The RSC task aims to recover a latent distortion-free global shutter (GS) image corresponding to a speciﬁc ex-posure time between consecutive RS frames. The re-sulting RSC methods can be divided into traditional and deep learning-based ones. The traditional RSC methods
[1, 13, 26, 38, 40, 48, 50] usually rely on hand-designed prior assumptions, geometric constraints, and complex op-timization frameworks. Consequently, such processes are typically time-consuming and require complex parameter-tuning strategies for different scenarios, which restricts their real-world applications.
In contrast, convolutional neural networks have also been used to remove RS artifacts in re-cent years due to the considerable success in many com-puter vision tasks, such as [5, 9, 14, 29, 51, 60]. Particu-larly, RSC methods based on multiple consecutive RS im-ages have been heavily investigated [5, 10, 29, 56].
In general, these multi-image-based RSC approaches of-ten consist of a two-stage network design with two key elements: a motion estimation module and a GS frame synthesis module, as illustrated in Fig. 2 (a). The former is dedicated to estimating a pixel-wise undistortion ﬁeld, which is utilized to warp the RS appearance content to
the corresponding GS instance. The latter aims to fuse the contextual information in a coarse-to-ﬁne manner, ul-timately decoding the desired GS image. Although this two-stage idea sounds relatively straightforward, it suffers from several drawbacks. First, the two-stage RSC faces a classic “chicken-and-egg” problem: motion estimation and
GS frame synthesis are inextricably linked; a high-quality undistortion ﬁeld improves GS frame synthesis, and vice versa. Therefore, this step-by-step combination is not con-ducive to information interaction and joint optimization, re-sulting in a bottleneck for high-quality RSC. Second, the two modules are implemented by two independent encoder-decoders, ignoring the mutual promotion of these two key elements for RSC. Third, the two-stage network design in-evitably increases the model size and inference time, which greatly limits their efﬁcient deployment in practice.
To address these issues, we propose a novel single-stage solution for RS correction through Joint Appearance and Motion Learning (JAMNet). Our approach is a single encoder-decoder structure with coarse-to-ﬁne reﬁnements, as depicted in Fig. 2 (b), allowing the simultaneous learning of complementary GS appearance and undistortion motion information. After extracting hierarchical pyramid features, we design an efﬁcient decoder for simultaneous occlusion inference and context aggregation. It leverages a warping branch to estimate the undistortion ﬁeld to compensate for
RS geometric distortions, while a synthetic branch is used to progressively reﬁne the GS appearance, which forms a mutual promotion of complementary information. Among them, a hidden state is maintained to transmit additional cues across pyramid levels. Further, we propose to in-ject sufﬁcient motion priors into the network at the coars-est level via a transformer-based motion embedding mod-ule. Moreover, inspired by the imaging principle of RS data, we also develop a new data augmentation strategy, i.e. vertical ﬂip + inverse order, in the training process, to en-hance the robustness of RSC models. Extensive experimen-tal results demonstrate that our JAMNet signiﬁcantly out-performs state-of-the-art (SOTA) RSC methods, especially achieving a real-time inference speed, as shown in Fig. 1.
It is worth mentioning that our pipeline achieves a 4.7 dB
PSNR improvement on real-world RSC applications.
In a nutshell, our main contributions are summarized: 1) We propose a tractable single-stage architecture to jointly perform GS appearance reﬁnement and undistor-tion motion estimation for efﬁcient RS correction. 2) We develop a general data augmentation strategy, i.e., vertical ﬂip and inverse order, to maximize the explo-ration of the RS correction datasets. 3) Experiments show that our approach not only achieves
SOTA RSC accuracy, but also enjoys a fast inference speed and a ﬂexible and compact network structure.
Input RS
Undistortion
Field
Latent 
GS Frame (a) Two-stage RSC
Input RS
Latent GS Frame
Undistortion Field (b) Single-stage RSC
Figure 2. Different RSC paradigms. (a) The currently popular two-stage structure ﬁrst estimates the undistortion ﬁeld, and then completes GS recovery accordingly. (b) We propose a single-stage
RSC framework with a joint learning mechanism to estimate the undistortion ﬁeld and the latent GS frame at the same time. 2.