Abstract
We present
Iterative Vision-and-Language Naviga-tion (IVLN), a paradigm for evaluating language-guided agents navigating in a persistent environment over time. Ex-isting Vision-and-Language Navigation (VLN) benchmarks erase the agent’s memory at the beginning of every episode, testing the ability to perform cold-start navigation with no prior information. However, deployed robots occupy the same environment for long periods of time. The IVLN paradigm addresses this disparity by training and evaluating
VLN agents that maintain memory across tours of scenes that consist of up to 100 ordered instruction-following Room-to-Room (R2R) episodes, each deﬁned by an individual lan-guage instruction and a target path. We present discrete and continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours each in 80 indoor scenes. We
ﬁnd that extending the implicit memory of high-performing transformer VLN agents is not sufﬁcient for IVLN, but agents that build maps can beneﬁt from environment persistence, motivating a renewed focus on map-building agents in VLN. 1.

Introduction
Robots and virtual agents that persistently operate in hu-man spaces like homes should improve over time. For ex-ample, a smart vacuum told to clean the living room, which is down the hall past the guest bedroom should learn about both the living room and guest bedroom. Likewise, agents should be able to associate references in past instructions, such as guest bedroom, with spatial and visual information from the environment to understand future instructions.
Most work on language-guided, embodied agents per-forming navigation [3, 25] or household tasks [38] is episodic in nature—agent memory is erased before issu-ing each new instruction. In contrast, physical robots build maps [12,43,49] iteratively from visual observations [32,39] as an explicit form of long-term memory. Agents trained to
*Equal contributions. Correspondence: krantzja@oregonstate.edu 3University of Southern California 4Google Research perform language-guided navigation in simulation that are deployed on physical robots [2] fail to take advantage of the mapping-based strategies that facilitate robot navigation.
We propose Iterative Vision-and-Language Navigation (IVLN), in which an agent follows an ordered sequence of language instructions that conduct a tour of an indoor space.
Each tour is composed of individual episodes of language instructions with target paths. Agents can utilize memory to better understand future tour instructions. After just 10 episodes an agent has seen on average over 50% of the target path associated with the next language instruction in a tour.
While performing an IVLN tour, agents iteratively explore the environment, meaning regions irrelevant to task instruc-tions need not ever be visited. By conditioning exploration on language, IVLN enables rich semantic representations, e.g., unusual, novel, and scene-speciﬁc referents grounded during one episode can be reasoned about later.
We explore both a discrete VLN setting based on Room-to-Room [3] episodes and navigation graphs (IR2R) and a continuous simulation VLN-CE [25] setting (IR2R-CE). The markedly different action and visual observation spaces of these settings may require different memory mechanisms.
In the discrete setting, agents move on graph edges and observe clear, well-framed images. For IR2R, we extend a state-of-the-art transformer agent [11] that learns an implicit memory based on path history when interpreting instructions.
In the continuous setting, agents take motion actions while observing noisy images of a 3D environment reconstructed from discrete panorama images. For IR2R-CE, we propose an agent that builds and interprets an explicit semantic map.
In short, we deﬁne Iterative Vision-and-Language Navi-gation (IVLN), a paradigm for persistent VLN, and release
IR2R and IR2R-CE to study discrete and continuous navi-gation agents in the IVLN setting. We create initial agents for both benchmarks, including explicit mapping and im-plicit memory models for continuous navigation. Please see jacobkrantz.github.io/ivln for code and more details. 2.