Abstract
Prompt tuning has been employed as an efﬁcient way to adapt large vision-language pre-trained models (e.g. CLIP) to various downstream tasks in data-limited or label-limited settings. Nonetheless, visual data (e.g., images) is by de-fault prerequisite for learning prompts in existing methods.
In this work, we advocate that the effectiveness of image-text contrastive learning in aligning the two modalities (for training CLIP) further makes it feasible to treat texts as im-ages for prompt tuning and introduce TaI prompting.
In contrast to the visual data, text descriptions are easy to col-lect, and their class labels can be directly derived. Particu-larly, we apply TaI prompting to multi-label image recogni-tion, where sentences in the wild serve as alternatives to im-ages for prompt tuning. Moreover, with TaI, double-grained prompt tuning (TaI-DPT) is further presented to extract both coarse-grained and ﬁne-grained embeddings for enhanc-ing the multi-label recognition performance. Experimen-tal results show that our proposed TaI-DPT outperforms zero-shot CLIP by a large margin on multiple benchmarks, e.g., MS-COCO, VOC2007, and NUS-WIDE, while it can be combined with existing methods of prompting from im-ages to improve recognition performance further. The code is released at https://github.com/guozix/TaI-DPT. 1.

Introduction
Recent few years have witnessed rapid progress in large vision-language (VL) pre-trained models [1, 16, 19, 24, 33, 36] as well as their remarkable performance on downstream vision tasks. A VL pre-trained model generally involves data encoders, and it is becoming increasingly popular to exploit image-test contrastive loss [24] to align the embed-ding of images and texts into a shared space. When adapt-ing to downstream tasks in data-limited or label-limited set-tings, it is often ineffective to ﬁne-tune the entire model, due to its high complexity. Then, prompt tuning as a represen-tative parameter-efﬁcient learning paradigm has emerged as
*This work was done when Zixian Guo was a research intern at TAL.
Figure 1. A comparison between prompting from images and our text-as-image (TaI) prompting. (a) Prompting from images (e.g., [41]) uses labeled images of task categories to learn the text prompts. Instead, (b) our TaI prompting learn the prompts with easily-accessed text descriptions containing target categories. (c)
After training, the learned prompts in (a) or (b) can be readily ap-plied to test images. an efﬁcient way to adapt VL models to downstream tasks.
Albeit considerable achievements have been made, ex-isting prompt tuning methods generally require visual data to learn prompts (as shown in Fig. 1(a)). For example,
CoOp [41] learns from annotated images. CoCoOp [40] further introduces generalizable input-conditional prompts.
DualCoOp [28] adapts CLIP to multi-label recognition tasks by training pairs of positive and negative prompts with partial-labeled images. Nonetheless, the performance of these prompting methods may be limited when it is infeasi-ble to obtain sufﬁcient image data or annotate the required images.
In this paper, we advocate treating Texts as Images for prompt tuning, i.e., TaI prompting. It is considered feasible as the image encoder and text encoder in many pre-trained
VL models [16, 24] encode images and texts into a shared space. Given an image and its caption, the visual features produced by the image encoder will be close to the text fea-ture of the caption produced by the text encoder. There-fore, in addition to extracting visual features from images, it is also feasible to extract text features as alternatives form, for example, descriptive sentences and captions, for prompt tuning (see Fig. 1(b)). TaI prompting has several interesting properties and merits. Taking a downstream image recog-nition task as an example, given a set of object categories, one can easily crawl a large set of text descriptions that con-tain object names from these categories. Text descriptions are easily accessible in this way, and class labels can be di-rectly derived from text descriptions, which means, in con-trast to prompting from images, TaI prompting may suffer less from the data-limited and label-limited issues.
We use multi-label image recognition [8, 9, 11, 20, 35] to verify the effectiveness of our TaI prompting in this paper.
To begin with, we crawl the captions from the public image caption datasets (e.g., MS-COCO [20]) and localized narra-tives from object detection datasets (e.g., Open Images [18]) to form the training set of text descriptions. For any speciﬁc multi-label recognition task, we adopt a noun ﬁlter to map the nouns in the text descriptions to the corresponding ob-ject categories, and then only keep the text descriptions that contain one or more classes of target objects. To better cope with multi-label classiﬁcation, we introduce double-grained prompt tuning (i.e., TaI-DPT) which involves: (i) a set of global prompts to generate embeddings for classifying whole sentences or images, and (ii) a set of local prompts to extract embeddings for discriminating text tokens or image patches. Given a set of text descriptions, global and local prompts can be tuned by minimizing the ranking loss [14].
Note that, though these prompts are learned from text de-scriptions solely, they can be readily deployed to classify whole images as well as image patches during testing (see
Fig. 1(c)). Experimental results show that, without using any labeled images, our TaI prompting surpasses zero-shot
CLIP [24] by a large margin on multiple benchmarks, e.g.,
MS-COCO, VOC2007, and NUS-WIDE.
Moreover, when images are also available during train-ing, our TaI prompting can be combined with existing meth-ods of prompting from images to improve its performance.
In particular, given a few annotated images, our TaI-DPT can be integrated with CoOp as a prompt ensemble for im-proving classiﬁcation accuracy. With partially labeled train-ing data being provided, we may also combine TaI-DPT and
DualCoOp [28] to improve multi-label recognition accuracy consistently. Extensive results verify the effectiveness of our TaI-DPT in comparison to state-of-the-art. To sum up, the contributions of this work include:
• We propose Texts as Images in prompt tuning (i.e., TaI prompting) to adapt VL pre-trained models to multi-label image recognition. Text descriptions are easily accessible and, in contrast to images, their class labels can be directly derived, making our TaI prompting very compelling in practice.
• We present double-grained prompt tuning (i.e. TaI-DPT) to extract both coarse-grained and ﬁne-grained embeddings for enhancing multi-label image recogni-tion. Experiments on multiple benchmarks show that
TaI-DPT achieves comparable multi-label recognition accuracy against state-of-the-arts.
• The prompts learned by TaI-DPT can be easily com-bined with existing methods of prompting from images in an off-the-shelf manner, further improving multi-label recognition performance. 2.