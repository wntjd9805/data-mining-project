Abstract
Multiple near frontal-parallel planes based depth repre-sentation demonstrated impressive results in self-supervised monocular depth estimation (MDE). Whereas, such a repre-sentation would cause the discontinuity of the ground as it is perpendicular to the frontal-parallel planes, which is detri-mental to the identification of drivable space in autonomous driving. In this paper, we propose the PlaneDepth, a novel orthogonal planes based presentation, including vertical planes and ground planes. PlaneDepth estimates the depth distribution using a Laplacian Mixture Model based on orthogonal planes for an input image. These planes are used to synthesize a reference view to provide the self-supervision signal. Further, we find that the widely used resizing and cropping data augmentation breaks the or-thogonality assumptions, leading to inferior plane predic-tions. We address this problem by explicitly constructing the resizing cropping transformation to rectify the prede-fined planes and predicted camera pose. Moreover, we propose an augmented self-distillation loss supervised with a bilateral occlusion mask to boost the robustness of or-thogonal planes representation for occlusions. Thanks to our orthogonal planes representation, we can extract the ground plane in an unsupervised manner, which is impor-tant for autonomous driving. Extensive experiments on the KITTI dataset demonstrate the effectiveness and effi-ciency of our method. The code is available at https:
//github.com/svip-lab/PlaneDepth. 1.

Introduction
Monocular depth estimation (MDE) is an important task in computer vision and it has tremendous potential applica-tions, such as autonomous driving. However, the expensive data and labels acquisition process restricts the data scale in supervised MDE [3, 4, 9, 16, 26, 27]. Thus, researchers
*Corresponding Author. turn to solve the data constraints in supervised MDE with the self-supervised MDE framework by leveraging videos or stereo image pairs.
Most of the early works in self-supervised MDE leverage a regression module to estimate pixel-wise depth map [11, 12, 15, 29, 36, 40] and warp the reference view image to the target view based on the estimated depth. Then, a photo-metric consistency loss is used to guide the learning of the depth regression module. However, these methods usually encounter the local minimum issue because of the locality of bilinear interpolation on the reference view. To avoid this issue, rather than using simple depth regression, multiple frontal-parallel planes based depth representation is intro-duced where depth space is divided into a fixed number of frontal-parallel planes, and the depth network learns to clas-sify which predefined plane each pixel belongs to [13, 14].
It has been shown that such representation could produce much sharper depth on the edges of the object. However, they are insufficient to represent the ground because the ground plane is perpendicular to these predefined frontal-parallel planes. As shown in Fig. 1, such vertical depth planes only solution would lead to discontinuity on the ground, which is obviously detrimental to the identification of drivable space in autonomous driving. Further, photo-metric consistency loss is applied to the weighted compo-sition of each plane-warped image, which is sub-optimal as the combination of different weights may lead to exactly the same color image [30], resulting in ambiguous solutions for depth plane classification.
Considering the ground is perpendicular to the frontal-parallel planes, in this paper, we propose to leverage orthog-onal planes to represent the scene where the ground planes favor the depth estimation in the ground region. Further, we propose to model the depth as a mixture of Laplace distri-butions of orthogonal planes [38], where each Laplacian is centered at one plane. We compute the photometric consis-tency loss independently on the color image warped by each plane, resulting in a more deterministic and less ambiguous optimization objective compared with the weighted compo-Figure 1. Monocular Depth Estimation Results. The zoomed-in visualizations and bird-eye-view colored point cloud show that our method can predict continuous depth in the ground region while preserving the sharp edges of objects. Compared with PladeNet [13], our prediction has smoother depth in the ground region. Compared with DepthHint [40], our method addresses the occlusion problem, which can be seen in figure that our prediction have eliminated depth artifacts in the edges of street lights and cars. sition strategy mentioned above [13,14], consequently lead-ing to better depth estimation results, as shown in Fig. 1.
Moreover, we can extract the ground plane in an unsuper-vised manner thanks to our orthogonal planes representa-tion.
Resizing and cropping are widely used as a data augmen-tation strategy in the stereo training setting of MDE [13,14].
However, it would destroy the orthogonality of our prede-fined planes, leading to inferior plane distributions estima-tion. To remedy this issue, in this paper, we deeply ana-lyze the effects of resizing and cropping on the world co-ordinates system. We explicitly compute the resizing and cropping transformation and use it to rectify the predefined planes and the predicted camera rotation, which eases the learning of plane distributions. We further propose to use neural positional encoding (NPE) for the resizing and crop-ping parameter and incorporate it into our PlaneDepth net-work, which improves the robustness of network training.
The self-distillation strategy is commonly used to solve occlusion problems [13, 14] and improve the depth predic-tion results [29]. Post-processing is widely used to improve the final prediction [11], which can be used naturally to gen-erate more accurate self-distillation labels. In this paper, we propose to combine post-processing with self-distillation by using a bilateral occlusion mask to generate more accurate supervision of network training, which improves both accu-racy and efficiency of our method.
We summarize our contributions as follows: 1. We propose the PlaneDepth, a novel orthogonal plane-based monocular depth estimation network, which favours the representation of both vertical objects and ground. Such representation leads to a much smoother depth for ground regions and would facilitate the iden-tification of drivable regions. 2. The depth within the scene is modeled by a mixture of Laplacian distributions, and the depth classifica-tion problem is cast as the optimizing the mixture of
Laplace distribution, which avoids the ambiguity in color expectation based depth estimation and leads to more stable depth estimation. 3. An orthogonality-preserved data augmentation strat-egy is proposed, which improves the robustness of net-work training. 4. We combine post-processing with self-distillation by our augmented self-distillation, which improves both efficiency and accuracy. 2.