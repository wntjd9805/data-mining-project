Abstract
Mobile monocular 3D object detection (Mono3D) (e.g., on a vehicle, a drone, or a robot) is an important yet chal-lenging task. Existing transformer-based ofﬂine Mono3D models adopt grid-based vision tokens, which is subopti-mal when using coarse tokens due to the limited available computational power. In this paper, we propose an online
Mono3D framework, called MonoATT, which leverages a novel vision transformer with heterogeneous tokens of vary-ing shapes and sizes to facilitate mobile Mono3D. The core idea of MonoATT is to adaptively assign ﬁner tokens to ar-eas of more signiﬁcance before utilizing a transformer to enhance Mono3D. To this end, we ﬁrst use prior knowl-edge to design a scoring network for selecting the most important areas of the image, and then propose a token clustering and merging network with an attention mecha-nism to gradually merge tokens around the selected areas in multiple stages. Finally, a pixel-level feature map is re-constructed from heterogeneous tokens before employing a
SOTA Mono3D detector as the underlying detection core.
Experiment results on the real-world KITTI dataset demon-strate that MonoATT can effectively improve the Mono3D accuracy for both near and far objects and guarantee low latency. MonoATT yields the best performance compared with the state-of-the-art methods by a large margin and is ranked number one on the KITTI 3D benchmark. 1.

Introduction
Three-dimensional (3D) object detection has long been a fundamental problem in both industry and academia and enables various applications, ranging from autonomous to robotic manipulation and vehicles [17] and drones, augmented reality applications. Previous methods have achieved superior performance based on the accurate depth information from multiple sensors, such as LiDAR signal
[11,23,35,43,44,69] or stereo matching [9,10,21,34,37,57].
In order to lower the sensor requirements, a much cheaper, more energy-efﬁcient, and easier-to-deploy alternative, i.e.,
*Corresponding authors (cid:894)(cid:258)(cid:895)(cid:3)(cid:39)(cid:396)(cid:349)(cid:282)(cid:882)(cid:271)(cid:258)(cid:400)(cid:286)(cid:282)(cid:3)(cid:410)(cid:381)(cid:364)(cid:286)(cid:374)(cid:400)(cid:3)(cid:349)(cid:374)(cid:3)(cid:373)(cid:437)(cid:367)(cid:410)(cid:349)(cid:393)(cid:367)(cid:286)(cid:3)(cid:400)(cid:410)(cid:258)(cid:336)(cid:286)(cid:400)(cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:3) (cid:894)(cid:271)(cid:895)(cid:3)H(cid:286)(cid:410)(cid:286)(cid:396)(cid:381)(cid:336)(cid:286)(cid:374)(cid:286)(cid:381)(cid:437)(cid:400) (cid:410)(cid:381)(cid:364)(cid:286)(cid:374)(cid:400) (cid:349)(cid:374) (cid:373)(cid:437)(cid:367)(cid:410)(cid:349)(cid:393)(cid:367)(cid:286) (cid:400)(cid:410)(cid:258)(cid:336)(cid:286)(cid:400) (cid:3)
Figure 1. Illustration of (a) grid-based tokens used in traditional vi-sion transformers and (b) heterogeneous tokens used in our adap-tive token transformer (ATT). Instead of equally treating all image regions, our ATT distributes dense and ﬁne tokens to meaningful image regions (i.e., distant cars and lane lines) yet coarse tokens to regions with less information such as the background. monocular 3D object detection (Mono3D) has been pro-posed and made impressive progress. A practical online
Mono3D detector for autonomous driving should meet the following two requirements: 1) given the constrained com-putational resource on a mobile platform, the 3D bounding boxes produced by the Mono3D detector should be accu-rate enough, not only for near objects but also for far ones, to ensure, e.g., high-priority driving safety applications; 2) the response time of the Mono3D detector should be as low as possible to ensure that objects of interest can be instantly detected in mobile settings.
Current Mono3D methods, such as depth map based
[15, 29, 36], pseudo-LiDAR based [15, 29–31, 36, 54, 57], and image-only based [2, 3, 12, 22, 26, 28, 42, 48, 51, 64–67], mostly follow the pipelines of traditional 2D object de-tectors [41, 42, 48, 66] to ﬁrst localize object centers from heatmaps and then aggregate visual features around each object center to predict the object’s 3D properties, e.g., lo-cation, depth, 3D sizes, and orientation. Although it is con-ceptually straightforward and has low computational over-head, merely using local features around the predicted ob-ject centers is insufﬁcient to understand the scene-level ge-ometric cues for accurately estimating the depth of objects, making existing Mono3D methods far from satisfactory.
Recently, inspired by the success of transformers in natural language processing, visual transformers with long-range attention between image patches have recently been devel-oped to solve Mono3D tasks and achieve state-of-the-art (SOTA) performance [19, 64]. As illustrated in Figure 1 (a), most existing vision transformers follow the grid-based token generation method, where an input image is divided into a grid of equal image patches, known as tokens. How-ever, using grid-based tokens is sub-optimal for Mono3D applications such as autonomous driving because of the fol-lowing two reasons: 1) far objects have smaller size and less image information, which makes them hard to detect with coarse grid-based tokens; 2) using ﬁne grid-based tokens is prohibitive due to the limited computational power and the stringent latency requirement.
In this paper, we propose an online Mono3D frame-work, called MonoATT, which leverages a novel vision transformer with heterogeneous tokens of varying sizes and shapes to boost mobile Mono3D. We have one key obser-vation that not all image pixels of an object have equivalent signiﬁcance with respect to Mono3D. For instance, pixels on the outline of a vehicle are more important than those on the body; pixels on far objects are more sensitive than those on near objects. The core idea of MonoATT is to automat-ically assign ﬁne tokens to pixels of more signiﬁcance and coarse tokens to pixels of less signiﬁcance before utilizing a transformer to enhance Mono3D detection. To this end, as illustrated in Figure 1 (b), we apply a similarity compati-bility principle to dynamically cluster and aggregate image patches with similar features into heterogeneous tokens in multiple stages. In this way, MonoATT neatly distributes computational power among image parts of different impor-tance, satisfying both the high accuracy and low response time requirements posed by mobile Mono3D applications.
There are three main challenges in designing MonoATT.
First, it is essential yet non-trivial to determine keypoints on the feature map which can represent the most relevant infor-mation for Mono3D detection. Such keypoints also serve as cluster centers to group tokens with similar features. To tackle this challenge, we score image features based on prior knowledge in mobile Mono3D scenarios. Speciﬁcally, features of targets (e.g., vehicles, cyclists, and pedestrians) are more important than features of the background. More-over, more attention is paid to features of distant targets and the outline of targets. Then, a predeﬁned number of key-points with the highest scores are selected as cluster cen-ters to guide the token clustering in each stage. As a result, an image region with dense keypoints will eventually be as-signed with ﬁne tokens while a region with sparse keypoints will be assigned with coarse tokens.
Second, given the established cluster centers in each stage, how to group similar tokens into clusters and ef-fectively aggregate token features within a cluster is non-intuitive. Due to the local correlation of 2D convolu-tion, using naive minimal feature distance for token clus-tering would make the model insensitive to object outlines.
Furthermore, a straightforward feature averaging scheme would be greatly affected by noise introduced by outlier to-kens. To deal with these issues, we devise a token clustering and merging network. It groups tokens into clusters, taking both the feature similarity and image distance between to-kens into account, so that far tokens with similar features are more likely to be designated into one cluster. Then, it merges all tokens in a cluster into one combined token and aggregates their features with an attention mechanism.
Third, recovering multi-stage vision tokens to a pixel-level feature map is proved to be beneﬁcial for vision trans-formers [46, 62]. However, how to restore a regular image feature map from heterogeneous tokens of irregular shapes and various sizes is challenging. To transform adaptive to-kens of each stage into feature maps, we propose an efﬁcient multi-stage feature reconstruction network. Speciﬁcally, the feature reconstruction network starts from the last stage of clustering, gradually upsamples the tokens, and aggregates the token features of the previous stage. The aggregated tokens correspond to the pixels in the feature map one by one and are reshaped into a feature map. As a result, accu-rate 3D detection results can be obtained via a conventional
Mono3D detector using the enhanced feature map.
Experiments on KITTI dataset [17] demonstrate that our method outperforms the SOTA methods by a large margin.
Such a framework can be applied to existing Mono3D de-tectors and is practical for industrial applications. The pro-posed MonoATT is ranked number one on the KITTI 3D benchmark by submission. The whole suite of the code base will be released and the experimental results will be posted to the public leaderboard. We highlight the main contri-butions made in this paper as follows: 1) a novel online
Mono3D framework is introduced, leveraging an adaptive token transformer to improve the detection accuracy and guarantee a low latency; 2) a scoring network is proposed, which integrates prior knowledge to estimate keypoints for progressive adaptive token generation; 3) a feature recon-struction network is designed to reconstruct a detailed im-age feature map from adaptive tokens efﬁciently. 2.