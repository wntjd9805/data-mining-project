Abstract
In low-level video analyses, effective representations are important to derive the correspondences between video frames. These representations have been learned in a self-supervised fashion from unlabeled images or videos, us-ing carefully designed pretext tasks in some recent studies.
However, the previous work concentrates on either spatial-discriminative features or temporal-repetitive features, with little attention to the synergy between spatial and tempo-ral cues. To address this issue, we propose a spatial-then-temporal self-supervised learning method. Specifically, we firstly extract spatial features from unlabeled images via contrastive learning, and secondly enhance the features by exploiting the temporal cues in unlabeled videos via recon-structive learning. In the second step, we design a global correlation distillation loss to ensure the learning not to forget the spatial cues, and a local correlation distillation loss to combat the temporal discontinuity that harms the re-construction. The proposed method outperforms the state-of-the-art self-supervised methods, as established by the experimental results on a series of correspondence-based video analysis tasks. Also, we performed ablation studies to verify the effectiveness of the two-step design as well as the distillation losses. 1.

Introduction
Learning representations for video correspondence is a fundamental problem in computer vision, which is closely related to different downstream tasks, including optical flow estimation [10, 17], video object segmentation [3, 37], key-point tracking [53], etc. However, supervising such a rep-resentation requires a large number of dense annotations, which is unaffordable. Thus, most approaches acquire information from simulations [10, 33] or limited annota-tions [39, 55], which result in poor generalization in dif-This work was supported by the Natural Science Foundation of China under Grants 62022075 and 62036005, and by the Fundamental Research
Funds for the Central Universities under Contracts WK3490000005 and
WK3490000006. (Corresponding author: Dong Liu.)
Figure 1. Matching results given the query point. While temporal-repetitive features fail to handle video correspondence with dramatic appearance changes and deformations (the first two rows), spatial-discriminative features are incompetent to recognize the temporal repetition and are misled by distractors with similar appearance (the last two rows). The green/red bounding boxes in-dicate correct/wrong matching. (Zoom in for best view) ferent downstream tasks. Recently, self-supervised feature learning is gaining significant momentum. Several pretext tasks [15, 19, 24, 27, 48, 52] are designed, mostly concen-trating on either spatial feature learning or temporal feature learning for space-time visual correspondence.
With the objective of learning the representations that are invariant to the appearance changes, spatial feature learning provides video correspondence with discrimina-tive and robust appearance cues, especially when facing severe temporal discontinuity, i.e., occlusions, appearance changes, and deformations. Most recently, as mentioned in [50], the contrastive models [6, 15, 52] pre-trained on im-age data show competitive performance against dedicated methods for video correspondence. Thus, it is shown to be a better way for learning spatial representations in terms of quality and data efficiency, compared with the meth-ods [19, 27, 40, 47, 54, 59] using large-scale video datasets for training. Though getting promising results, as shown in
Figure 1, the learned spatial representations are misled by the distractors with similar appearance, which indicates the poor ability to recognize the temporal pattern.
In another line, temporal feature learning focuses on learning the temporal-repetitive features that occur consis-tently over time. With the temporal consistency assump-tion [2], the pixel repetition across video motivates recent studies to exploit the temporal cues via a reconstruction task [24, 30, 34, 58], where the query pixel in the target frame can be reconstructed by leveraging the information of adjacent reference frames within a local range. Then a reconstruction loss is applied to minimize the photometric error between the raw frame and its reconstruction. Nev-ertheless, temporal-repetitive features highly depend on the consistency of the pixels across the video. It thus can be easily influenced by the temporal discontinuity caused by the dramatic appearance changes, occlusions, and deforma-tions (see Figure 1).
In light of the above observation, we believe the video correspondence relies on both spatial-discriminative and temporal-repetitive features. Thus, we propose a novel spatial-then-temporal pretext task to achieve synergy be-tween spatial and temporal cues. Specifically, we firstly learn spatial features from unlabeled images via contrastive learning and secondly improve the features by exploiting the temporal repetition in unlabeled videos with frame re-construction. While such an implementation brings to-gether the advantages of spatial and temporal feature learn-ing, there are still some problems. First of all, the previous studies [18, 19, 27, 47, 54] propose to learn coarse-grained features for video correspondence. With the objective of reconstructive learning, the video frames need to be down-sampled to align with the coarse-grained features [24], re-sulting in severe temporal discontinuity for the pixels to be reconstructed. Thus, the frame reconstruction loss becomes invalid. Second, in the context of sequential training, di-rectly training with only new data and objective functions will degrade the discriminative features learned before.
To tackle the first problem, we firstly exploit temporal cues by frame reconstruction at different pyramid levels of the encoder. We observe that temporal repetition ben-efits from relatively smaller down-sampling rate. Hence, the model will learn better temporal-persistent features at the fine-grained pyramid level. To distillate the knowledge from it, we design a local correlation distillation loss that supports explicit learning of the final correlation map in the region with high uncertainty, which is achieved by taking the more fine-grained local correlation map as pseudo la-bels. This leads to better temporal representations on the coarse feature map. At the same time, we regard the model pre-trained in the first step as the teacher. Then a global correlation distillation loss is proposed to retain the spatial cues. Eventually, we can obtain better temporal represen-tations without losing the discriminative appearance cues acquired in the first step.
To sum up, our main contributions include: (i) We pro-pose a spatial-then-temporal pretext task for self-supervised video correspondence, which achieves synergy between spatially discriminative and temporally repetitive features. (ii) We propose the local correlation distillation loss to fa-cilitate the learning of temporal features in the second step while retaining the appearance sensitivity learned in the first step by the proposed global correlation distillation loss. (iii) We verify our approach in a series of correspondence-related tasks. Our approach consistently outperforms pre-vious state-of-the-art self-supervised methods and is even comparable with task-specific fully-supervised algorithms. 2.