Abstract
We propose an approach to estimate the number of sam-ples required for a model to reach a target performance.
We ﬁnd that the power law, the de facto principle to esti-mate model performance, leads to a large error when using a small dataset (e.g., 5 samples per class) for extrapola-tion. This is because the log-performance error against the log-dataset size follows a nonlinear progression in the few-shot regime followed by a linear progression in the high-shot regime. We introduce a novel piecewise power law (PPL) that handles the two data regimes differently. To estimate the parameters of the PPL, we introduce a ran-dom forest regressor trained via meta learning that general-izes across classiﬁcation/detection tasks, ResNet/ViT based architectures, and random/pre-trained initializations. The
PPL improves the performance estimation on average by 37% across 16 classiﬁcation and 33% across 10 detection datasets, compared to the power law. We further extend the
PPL to provide a conﬁdence bound and use it to limit the prediction horizon that reduces over-estimation of data by 76% on classiﬁcation and 91% on detection datasets. 1.

Introduction
More data translates to better performance, on aver-age, but higher cost. As data requirements scale, there is a natural desire to predict the cost to train a model and what performance it may achieve, as a function of cost, without training. Towards this goal, neural scaling laws [3, 4, 9, 19, 20, 40] have been proposed that suggest that the performance of a model trained on a given dataset size follows a linear ﬁt when plotted on a logarithmic scale (power law in linear scale).
In practice, however, the power law provides only a fam-ily of functions and its parameters must be ﬁtted to each speciﬁc case for performance prediction. A common use
*Corresponding author. † Work done at AWS.
Figure 1. Performance estimation curves using the powerlaw and piecewise power law (PPL) with estimated conﬁdence. The PPL reduces over-estimation of the power law from 12 in 1 step, and further to 1.2 in 2 steps using the estimated conﬁdence bounds to limit the prediction horizon n(1) in the ﬁrst step. to 1.9
⇥
⇥
⇥ case is one where a small initial dataset is made available and can be used to obtain small-scale performance statistics that are relatively inexpensive to obtain and can be used to
ﬁt the power law parameters. Then, the ﬁtted function is used to predict the performance for any dataset size train-ing through extrapolation. This approach is found empiri-cally to generalize across several datasets and deep learning models [40]. However, most applications of power law are done in the high-shot regime. The few-shot regime (e.g., 5 samples/class) is particularly useful given the prevalence of pre-trained initializations available for model training. In the few-shot regime, the performance curve exhibits a non-linear progression followed by a linear progression, see Fig-ure 1. Thus, data requirements based on the power law can lead to signiﬁcant errors incurring additional cost for ac-quiring data.
In this paper, we propose a piecewise power law (PPL) that models the performance as a quadratic curve in the few-shot regime and a linear curve in the high-shot regime in
the log-log domain, while ensuring continuity during the transition. To estimate the parameters of the PPL, we ﬁrst identify the switching point between the quadratic and lin-ear curves using PowerRF, a random forest regressor trained via meta-learning, and then use the performance statistics to ﬁt the remaining parameters. We show that our approach provides a better estimation of performance than the power law across several datasets, architectures, and initialization settings. We extend the PPL to provide a conﬁdence es-timate that is used to further reduce the error in data esti-mation. In Figure 1, the conﬁdence estimate controls the maximum number of samples in a step such that the PPL uses two steps to achieve the target performance with 1.2 over-estimation compared to 12 with the power law.
⇥
Our contributions are as follows. We propose an im-proved policy for predicting data size needed to achieve a target accuracy with three main innovations: (1) a piece-wise power law model (PPL) that approximates the per-formance error curve from low-to-high-shot regime, (2) a meta-learning approach to estimate the parameters of the
PPL, and (3) incorporating the conﬁdence interval of the estimates to limit the prediction horizon and reduce error in data estimation. We demonstrate the generalization of the
PPL and the meta-model on 16 classiﬁcation and 10 detec-tion datasets, improving the (a) performance estimates by 37% on classiﬁcation and 33% on detection datasets and (b) data estimates by 76% on classiﬁcation and 91% on de-tection datasets, with respect to the power law.
⇥ 2.