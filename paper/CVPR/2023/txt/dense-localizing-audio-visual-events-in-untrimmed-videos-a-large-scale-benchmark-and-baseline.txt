Abstract
Existing audio-visual event localization (AVE) handles manually trimmed videos with only a single instance in each of them. However, this setting is unrealistic as nat-ural videos often contain numerous audio-visual events with different categories. To better adapt to real-life ap-plications, in this paper we focus on the task of dense-localizing audio-visual events, which aims to jointly lo-calize and recognize all audio-visual events occurring in an untrimmed video. The problem is challenging as it re-quires fine-grained audio-visual scene and context under-standing. To tackle this problem, we introduce the first
Untrimmed Audio-Visual (UnAV-100) dataset, which con-tains 10K untrimmed videos with over 30K audio-visual events. Each video has 2.8 audio-visual events on aver-age, and the events are usually related to each other and might co-occur as in real-life scenes. Next, we formulate the task using a new learning-based framework, which is capable of fully integrating audio and visual modalities to localize audio-visual events with various lengths and cap-ture dependencies between them in a single pass. Extensive experiments demonstrate the effectiveness of our method as well as the significance of multi-scale cross-modal percep-tion and dependency modeling for this task. The dataset and code are available at https://unav100.github.io. 1.

Introduction
Understanding real-world scenes and events is inher-ently a multisensory perception process for humans [15,31].
However, for machines, how to integrate multi-modal in-formation, especially audio and visual ones, to facilitate comprehensive video understanding is still a challenging problem.
In recent years, with the introduction of many audio-visual datasets [7, 8, 11, 36], we have seen progress in
∗ Corresponding author
Figure 1. Different from the previous AVE task, dense-localizing audio-visual events involves localizing and recognizing all audio-visual events occurring in an untrimmed video. In real-life audio-visual scenes, there are often multiple audio-visual events that might be very short or long, and occur concurrently. The top and bottom examples are from the current AVE dataset [36] and our
UnAV-100 dataset, respectively. learning joint audio-visual representations [1, 27, 28], spa-tially localizing visible sound sources [7, 22] and tempo-rally localizing audio-visual events [39, 40, 46], etc. While the success of these algorithms is encouraging, they all fo-cus on manually trimmed videos that often just contain a single audio-visual instance/object in each of them. In par-ticular, audio-visual event localization (AVE) [36] aims to localize a single event that is both audible and visible at the same time in a short, trimmed video, as shown in the upper part of Fig. 1. The task setting is impractical as a real-life video is usually long, untrimmed and associated to multiple audio-visual events from different categories, and these events might have various duration and occur simulta-neously. For example, as illustrated at the bottom of Fig. 1, a man starts singing and other people accompany him on
trumpet and violin, and they pause several times along with the music. Therefore, we argue that it is necessary to re-examine and re-define the AVE task to better adapt to real-life audio-visual scenarios.
In this work, we conduct in-depth research starting from dataset construction to technical solutions. On the one hand, different from the existing AVE dataset [36] that only con-tains a single audio-visual event in each 10s trimmed video, we introduce a large-scale Untrimmed Audio-Visual (UnAV-100) dataset.
It consists of more than 10K untrimmed videos with over 30K audio-visual events covering 100 dif-ferent event categories. Our dataset spans a wide range of domains, including human activities, music performances, and sounds from animals, vehicles, tools, nature, etc. As the first audio-visual dataset built on untrimmed videos, UnAV-100 is quite challenging for many reasons. For instance, each video contains 2.8 audio-visual events on average (23 events maximum), and around 25% of videos have concur-rent events. Besides, the length of audio-visual events varies greatly from 0.2s to 60s. There are also rich temporal de-pendencies among events occurring in a video, e.g., people often clap when cheering, and rain is usually with thunder, etc. We believe that the UnAV-100 dataset, with its realistic complexity, can promote the exploration on comprehensive audio-visual video understanding.
On the other hand, facing such a complex real-life scene, current methods [36, 39–41, 46] formulate the AVE task as a single-label segment-level classification problem and can only identify one audio-visual event for each segment in a trimmed video. They fail to locate concurrent events and provide an exact temporal extent for each event in untrimmed videos. To address the above issues, we re-define AVE as an instance-level localization problem, called dense-localizing audio-visual events. We also present a new framework to flexibly recognize all audio-visual events in an untrimmed video and meanwhile regress their tempo-ral boundaries in a single pass. Firstly, the sound and its visual information are both critical to identify an audio-visual event, and the events can range across multiple time scales. Hence, we propose a cross-modal pyramid trans-former encoder that enables the model to fully integrate in-formative audio and visual signals and capture both very short as well as long audio-visual events. Secondly, with the observation that the events in a video are usually related to one another, we conduct temporal dependency modeling to learn such correlations, allowing the model to use con-text to localize events more correctly. Finally, we design a class-aware regression head for decoding temporal bound-aries of overlapping events, together with a classification head to obtain the final localization results. Extensive ex-periments demonstrate the effectiveness of our method, and show that it outperforms related state-of-the-art methods for untrimmed videos by a large margin.
Our contributions can be summarized as follows:
• We introduce a large-scale UnAV-100 dataset, as the first audio-visual benchmark based on untrimmed videos. There exist multiple audio-visual events in each video, and these events are usually related to one another and co-occur as in real-life scenes.
• We shift the AVE task to a more realistic setup of dense-localizing audio-visual events, and propose a new framework, allowing to flexibly recognize all audio-visual events in an untrimmed video and regress their temporal boundaries in a single pass.
• Extensive experiments demonstrate the significance of multi-scale cross-modal perception and dependency modeling for the task. Our method can achieve supe-rior performance over related state-of-the-art methods for untrimmed videos by a large margin. 2.