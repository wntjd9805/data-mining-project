Abstract
Reconstructing visual experiences from human brain ac-tivity offers a unique way to understand how the brain rep-resents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic ﬁ-delity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to recon-struct images from human brain activity obtained via func-tional magnetic resonance imaging (fMRI). More speciﬁ-cally, we rely on a latent diffusion model (LDM) termed
Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative perfor-mance. We also characterize the inner mechanisms of the
LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and differ-ent elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can recon-struct high-resolution images with high ﬁdelity in straight-* Corresponding author forward fashion, without the need for any additional train-ing and ﬁne-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientiﬁc perspective. Overall, our study proposes a promising method for reconstructing im-ages from human brain activity, and provides a new frame-work for understanding DMs. Please check out our web-page at https://sites.google.com/view/stablediffusion-with-brain/. 1.

Introduction
A fundamental goal of computer vision is to construct artiﬁcial systems that see and recognize the world as hu-man visual systems do. Recent developments in the mea-surement of population brain activity, combined with ad-vances in the implementation and design of deep neu-ral network models, have allowed direct comparisons be-tween latent representations in biological brains and ar-chitectural characteristics of artiﬁcial networks, providing important insights into how these systems operate [3, 8– 10, 13, 18, 19, 21, 42, 43, 54, 55]. These efforts have in-cluded the reconstruction of visual experiences (percep-tion or imagery) from brain activity, and the examination of potential correspondences between the computational processes associated with biological and artiﬁcial systems
[2, 5, 7, 24, 25, 27, 36, 44–46].
Reconstructing visual images from brain activity, such as that measured by functional Magnetic Resonance Imag-ing (fMRI), is an intriguing but challenging problem, be-cause the underlying representations in the brain are largely unknown, and the sample size typically associated with brain data is relatively small [17, 26, 30, 32].
In recent years, researchers have started addressing this task using deep-learning models and algorithms, including generative adversarial networks (GANs) and self-supervised learning
[2, 5, 7, 24, 25, 27, 36, 44–46]. Additionally, more recent studies have increased semantic ﬁdelity by explicitly using the semantic content of images as auxiliary inputs for re-construction [5, 25]. However, these studies require train-ing new generative models with fMRI data from scratch, or
ﬁne-tuning toward the speciﬁc stimuli used in the fMRI ex-periment. These efforts have shown impressive but limited success in pixel-wise and semantic ﬁdelity, partly because the number of samples in neuroscience is small, and partly because learning complex generative models poses numer-ous challenges.
Diffusion models (DMs) [11,47,48,53] are deep genera-tive models that have been gaining attention in recent years.
DMs have achieved state-of-the-art performance in several tasks involving conditional image generation [4,39,49], im-age super resolution [40], image colorization [38], and other related tasks [6, 16, 33, 41]. In addition, recently proposed latent diffusion models (LDMs) [37] have further reduced computational costs by utilizing the latent space generated by their autoencoding component, enabling more efﬁcient computations in the training and inference phases. An-other advantage of LDMs is their ability to generate high-resolution images with high semantic ﬁdelity. However, be-cause LDMs have been introduced only recently, we still lack a satisfactory understanding of their internal mecha-nisms. Speciﬁcally, we still need to discover how they rep-resent latent signals within each layer of DMs, how the la-tent representation changes throughout the denoising pro-cess, and how adding noise affects conditional image gen-eration.
Here, we attempt to tackle the above challenges by re-constructing visual images from fMRI signals using an
LDM named Stable Diffusion. This architecture is trained on a large dataset and carries high text-to-image genera-tive performance. We show that our simple framework can reconstruct high-resolution images with high semantic ﬁ-delity without any training or ﬁne-tuning of complex deep-learning models. We also provide biological interpretations of each component of the LDM, including forward/reverse diffusion processes, U-Net, and latent representations with different noise levels.
⇥
Our contributions are as follows: (i) We demonstrate that our simple framework can reconstruct high-resolution 512) images from brain activity with high seman-(512 tic ﬁdelity, without the need for training or ﬁne-tuning of complex deep generative models (Figure 1); (ii) We quan-titatively interpret each component of an LDM from a neu-roscience perspective, by mapping speciﬁc components to distinct brain regions; (iii) We present an objective interpre-tation of how the text-to-image conversion process imple-mented by an LDM incorporates the semantic information expressed by the conditional text, while at the same time maintaining the appearance of the original image. 2.