Abstract
Layout generation aims to synthesize realistic graphic scenes consisting of elements with different attributes in-cluding category, size, position, and between-element rela-tion. It is a crucial task for reducing the burden on heavy-duty graphic design works for formatted scenes, e.g., publi-cations, documents, and user interfaces (UIs). Diverse ap-plication scenarios impose a big challenge in unifying var-ious layout generation subtasks, including conditional and unconditional generation. In this paper, we propose a Lay-out Diffusion Generative Model (LDGM) to achieve such uniﬁcation with a single decoupled diffusion model. LDGM views a layout of arbitrary missing or coarse element at-tributes as an intermediate diffusion status from a com-pleted layout. Since different attributes have their individ-ual semantics and characteristics, we propose to decouple the diffusion processes for them to improve the diversity of training samples and learn the reverse process jointly to ex-ploit global-scope contexts for facilitating generation. As a result, our LDGM can generate layouts either from scratch or conditional on arbitrary available attributes. Exten-sive qualitative and quantitative experiments demonstrate our proposed LDGM outperforms existing layout genera-tion models in both functionality and performance. 1.

Introduction
Layout determines the placements and sizes of primi-tive elements on a page of formatted scenes (e.g., publica-tions, documents, UIs), which has critical impacts on how viewers understand and interact with the information in this page [13]. Layout generation is an emerging task of syn-thesizing realistic and attractive graphic scenes with prim-itive elements of different categories, sizes, positions, and
It is of high demands for reducing the burden relations. on heavy-duty graphic design works in diverse application
*This work was done when Mude Hui was an intern at MSRA.
Relation
Icon at  the top  of Text
Input
Category Left
Top Wide Height
Text
Icon
X2
Button
X3
Y2
Y3
W1
W2
W3
H1
H2
H3
Generation (Denoising)
Diffusion (Noise adding)
Target  UI
Target  layout
Output
Ico nIcon
Text
Button
Category Left
Top Wide Height
Text
Icon
X1
X2
Button
X3
Y1
Y2
Y3
W1
W2
W3
H1
H2
H2
Precise  Attribute
Coarse  Attribute
Element Relation
Missing Attribute
Figure 1. The layout generation tasks can be uniﬁed into a diffu-sion (noise-adding) process and a generation (denoising) process. scenarios. Recently, there have been some research works studying unconditional generation [1, 7, 10, 16, 28], condi-tional generation based on user speciﬁed inputs (e.g., el-ement types [12, 13, 15], element types and sizes [13, 16] or element relations [12, 15]), conditional reﬁnement based on coarse attributes [24], and conditional completion based on partially available elements [7], etc. However, none of them can cope with all these application scenarios simulta-neously. This imposes a big challenge in unifying various layout generation subtasks with a single model, including conditional generation upon various speciﬁed attributes and unconditional generation from scratch. Towards this goal, the prior work UniLayout [9] takes a further step by propos-ing a multi-task framework to handle six subtasks for lay-out generation with a single model. However, the supported subtasks are pre-deﬁned and could not cover all application scenarios, e.g., conditional generation based on speciﬁed el-ement sizes. Besides, it does not take into account the com-binational cases of several subtasks, e.g., the case wherein some elements have missing attributes to be generated while the others are with coarse attributes to be reﬁned in the same
layout simultaneously.
Generally, a layout comprises a series of elements with multiple attributes, i.e., category, position, size and between-element relation. Each element attribute has three possible statuses: precise, coarse or missing. Different lay-out generation subtasks supported by previous works are deﬁned as a limited number of cases where the attribute statuses are ﬁxed upon attribute types, as shown in Fig-ure 2. From a uniﬁed perspective, all missing or coarse attributes can be viewed as the corrupted results from their corresponding targets. With this key insight in mind, we innovatively propose to unify various forms of user inputs as intermediate statuses of a diffusion (corruption) process while modeling generation as a reverse (denoising) process.
Furthermore, attributes with different corruption degrees are likely to appear at once in user inputs. And different at-tributes have their own semantics and characteristics. These in fact impose a challenge for the diffusion process to cre-ate diverse training samples as comprehensive simulation for various user inputs.
In this work, we propose a de-coupled diffusion model LDGM to address this challenge.
The meaning of “decoupled” here is twofold: (i) we design attribute-speciﬁc forward diffusion processes upon the at-tribute types; (ii) we decouple the forward diffusion process with the reverse denoising process, wherein the forward processes are individual for different types of attributes, whereas the reverse processes are integrated into one to be jointly performed.
In this way, our proposed LDGM in-cludes not only attribute-aware forward diffusion processes for different attributes to ensure the diversity of generation results, but also a joint denoising process with fully mes-sage passing over the global-scope elements for improving the generation quality. Our contributions can be summa-rized in the following:
• We present that various layout generation subtasks can be comprehensively uniﬁed with a single diffusion model.
• We propose the Layout Diffusion Generative Model (LDGM), which allows parallel decoupled diffusion pro-cesses for different attributes and a joint denoising pro-cess for generation with sufﬁcient global message passing and context exploitation. It conforms to the characteris-tics of layouts and achieves high generation qualities.
• Extensive qualitative and quantitative experiment results demonstrate that our proposed scheme outperforms exist-ing layout generation models in terms of functionality and performance on different benchmark datasets. 2.