Abstract
The matching of 3D shapes has been extensively stud-ied for shapes represented as surface meshes, as well as for shapes represented as point clouds. While point clouds are a common representation of raw real-world 3D data (e.g. from laser scanners), meshes encode rich and expres-sive topological information, but their creation typically re-quires some form of (often manual) curation. In turn, meth-ods that purely rely on point clouds are unable to meet the matching quality of mesh-based methods that utilise the ad-ditional topological structure.
In this work we close this gap by introducing a self-supervised multimodal learning strategy that combines mesh-based functional map regular-isation with a contrastive loss that couples mesh and point cloud data. Our shape matching approach allows to ob-tain intramodal correspondences for triangle meshes, com-plete point clouds, and partially observed point clouds, as well as correspondences across these data modalities. We demonstrate that our method achieves state-of-the-art re-sults on several challenging benchmark datasets even in comparison to recent supervised methods, and that our method reaches previously unseen cross-dataset generali-sation ability. Our code is available at https://github.com/ dongliangcao/Self-Supervised-Multimodal-Shape-Matching. 1.

Introduction
Matching 3D shapes, i.e. finding correspondences be-tween their parts, is a fundamental problem in computer vision and computer graphics that has a wide range of ap-plications [11, 16, 31]. Even though it has been studied for decades [56, 57], the non-rigid shape matching problem re-mains highly challenging. One often faces a large variabil-ity in terms of shape deformations, or input data with severe noise and topological changes.
With the recent success of deep learning, many learning-based approaches were proposed for 3D shape match-ing [17, 19, 28, 33]. While recent approaches demonstrate near-perfect matching accuracy without requiring ground truth annotations [8, 17], they are limited to 3D shapes rep-resented as triangle meshes and strongly rely on clean data.
Figure 1. Left: Our method obtains accurate correspondences for triangle meshes, point clouds and even partially observed point clouds. Right: Proportion of correct keypoints (PCK) curves and mean geodesic errors (scores in the legend) on the
SHREC’19 dataset [34] for meshes (solid lines) and point clouds (dashed lines). Existing point cloud matching methods (DPC [26], green line), or mesh-based methods applied to point clouds (Deep
Shells [17], red dashed line) are unable to meet the matching per-formance of mesh-based methods (solid lines).
In contrast, our method is multimodal and can process both meshes and point clouds, while enabling accurate shape matching with comparable performance for both modalities (blue lines).
Since point clouds are a common representation for real-world 3D data, many unsupervised learning approaches were specifically designed for point cloud matching [20, 26, 63]. These methods are often based on learning per-point features, so that point-wise correspondences are ob-tained by comparing feature similarities. The learned fea-tures were shown to be robust under large shape deforma-tions and severe noise. However, although point clouds commonly represent samples of a surface, respective topo-logical relations are not explicitly available and thus can-not effectively be used during training.
In turn, existing point cloud correspondence methods are unable to meet the matching performance of mesh-based methods, as can be seen in Fig. 1. Moreover, when applying state-of-the-art unsupervised methods designed for meshes (e.g. Deep
Shells [17]) to point clouds, one can observe a significant drop in matching performance.
In this work, we propose a self-supervised learning framework to address these shortcomings. Our method uses
Methods
Unsup. Mesh
Point Cloud
FM-based
FMNet [28]
GeomFMaps [13]
DiffFMaps [32]
DPFM [2] 3D-CODED [19]
IFMatch [55]
UnsupFMNet [21]
SURFMNet [46, 51]
Deep Shells [17]
ConsistFMaps [8]
CorrNet3D [63]
DPC [26]
Ours
✗
✗
✗
✗
✗
✗
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✗∗
✗∗
✓
✗∗
✓
✓
✗∗
✗∗
✗∗
✗∗
✓
✓
✓
✓
✓
✓
✓
✗
✗
✓
✓
✓
✓
✗
✗
✓
Partiality Robustness w.o. Refinement Required train data∗∗
✓
✓
✓
✓
✗
✗
✓
✓
✗
✓
✓
✓
✓
Small
Small
Moderate
Small
Large
Moderate
Small
Small
Small
Small
Large
Moderate
Small
✗
✗
✗
✓
✓
✓
✗
✗
✗
✓
✗
✗
✓
✗
✗
✓
✗
✓
✓
✗
✗
✗
✗
✓
✓
✓
Table 1. Method comparison. Our method is the first learning-based approach that combines a unique set of desirable properties.
∗ Methods are originally designed for meshes, directly applying them to point clouds leads to a large performance drop.
∗∗ Categorisation according to the amount of training data: Small (<1000), Moderate (≈5,000) and Large (>10,000). a combination of triangle meshes and point clouds (ex-tracted from the meshes) for training. We first utilise the structural properties of functional maps for triangle meshes as strong unsupervised regularisation. At the same time, we introduce a self-supervised contrastive loss between triangle meshes and corresponding point clouds, enabling the learn-ing of consistent feature representations for both modali-ties. With that, our method does not require to compute functional maps for point clouds at inference time, but di-rectly predicts correspondences based on feature similarity comparison. Overall, our method is the first learning-based approach that combines a unique set of desirable proper-ties, i.e. it can be trained without ground-truth correspon-dence annotations, is designed for both triangle meshes and point clouds (throughout this paper we refer to this as mul-timodal), is robust against noise, allows for partial shape matching, and requires only a small amount of training data, see Tab. 1. In summary, our main contributions are:
• For the first time we enable multimodal non-rigid 3D shape matching under a simple yet efficient self-supervised learning framework.
• Our method achieves accurate matchings for triangle meshes based on functional map regularisation, while ensuring matching robustness for less structured point cloud data through deep feature similarity.
• Our method outperforms state-of-the-art unsupervised and even supervised methods on several challenging 3D shape matching benchmark datasets and shows pre-viously unseen cross-dataset generalisation ability.
• We extend the SURREAL dataset [58] by SURREAL-PV that exhibits disconnected components in partial views as they occur in 3D scanning scenarios. 2.