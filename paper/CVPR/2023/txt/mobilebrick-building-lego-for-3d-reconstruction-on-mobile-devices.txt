Abstract
High-quality 3D ground-truth shapes are critical for 3D object reconstruction evaluation. However, it is difﬁcult to create a replica of an object in reality, and even 3D re-constructions generated by 3D scanners have artefacts that cause biases in evaluation. To address this issue, we in-troduce a novel multi-view RGBD dataset captured using a mobile device, which includes highly precise 3D ground-truth annotations for 153 object models featuring a diverse set of 3D structures. We obtain precise 3D ground-truth shape without relying on high-end 3D scanners by utilising
LEGO models with known geometry as the 3D structures for image capture. The distinct data modality offered by high-resolution RGB images and low-resolution depth maps cap-tured on a mobile device, when combined with precise 3D geometry annotations, presents a unique opportunity for fu-ture research on high-ﬁdelity 3D reconstruction. Further-more, we evaluate a range of 3D reconstruction algorithms on the proposed dataset. 1.

Introduction the key to enabling immersive and realistic Augment Re-ality applications. For instance, a virtual object would not blend in the physical environment realistically even if it is a few millimetres off because the lighting is reﬂected incor-rectly. Many approaches (e.g., Visual SLAM [13], Depth
Fusion [24], Multi-View Stereo [27]) have been proposed to address this problem. Neural ﬁelds have also emerged as a promising technique for 3D reconstruction [33, 39] and novel view synthesis [23]. Advances in most of these algo-rithms are made possible with reliable datasets for bench-marking.
However, building the exact 3D model of an object for benchmarking is extremely difﬁcult. Some datasets [17, 19, 29] resort to high-end 3D scanners to create pseudo-ground-truth models for evaluation, but these 3D models still suf-fer from artefacts due to noisy measurements, as shown in Fig. 2. Therefore, the DTU dataset [17], a widely-used dataset for multi-view stereo, is reluctant to call their recon-structed models ground truth, but chose the term “evalua-tion reference”. This deviation from the actual ground-truth shape can cause signiﬁcant biases when evaluating high-ﬁdelity object reconstruction, where millimetres matters.
High-ﬁdelity 3D object reconstruction from images has
It is also always been a “holy grail” in computer vision.
In addition to the lack of 3D ground-truth, most exist-ing datasets in multi-view reconstruction are not captured
Point problem with annotated keypoints. Second, we verify and reﬁne the alignment by maximising the overlap between the 3D model’s projections on a few sampled images and its observed location in those images. Third, we use bun-dle adjustment to reﬁne the camera poses of all images in the sequence such that they are consistent with the aligned object pose and to alleviate motion drift caused by ARKit.
Overall, our contributions in this paper are as follows:
• We propose MobileBrick, a large-scale dataset of 153 diverse object shapes focusing on detailed 3D object reconstruction with a unique data modality of high-resolution RGB images with low-resolution depth maps captured on a mobile device.
• We provide exact ground-truth 3D models by building the digital replica of each object and we design an ef-ﬁcient annotation pipeline to align the 3D models to image sequences.
• We demonstrate the usefulness of the proposed dataset by training and evaluating various methods on the tasks of multi-view surface reconstruction, novel view synthesis, and colour-guided depth enhancement. 2.