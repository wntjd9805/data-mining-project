Abstract
Fine-tuning large-scale pre-trained vision models to downstream tasks is a standard technique for achieving state-of-the-art performance on computer vision bench-marks. However, ﬁne-tuning the whole model with millions of parameters is inefﬁcient as it requires storing a same-sized new model copy for each task. In this work, we pro-pose LoRand, a method for ﬁne-tuning large-scale vision models with a better trade-off between task performance and the number of trainable parameters. LoRand gener-ates tiny adapter structures with low-rank synthesis while keeping the original backbone parameters ﬁxed, resulting in high parameter sharing. To demonstrate LoRand’s ef-fectiveness, we implement extensive experiments on object detection, semantic segmentation, and instance segmenta-tion tasks. By only training a small percentage (1% to 3%) of the pre-trained backbone parameters, LoRand achieves comparable performance to standard ﬁne-tuning on COCO and ADE20K and outperforms ﬁne-tuning in low-resource
PASCAL VOC dataset. 1.

Introduction
With the rapid development of computer vision, pa-rameters in deep models are surging. Giant models need to be trained with massive resources to achieve superior performance [3, 17, 47, 58], which is often unavailable to many academics and institutions.
“Pretrain & Finetun-ing” paradigm is widely used to alleviate this dilemma.
Teams with sufﬁcient computation resources utilise enor-mous datasets [2, 9, 40, 50] to train superior backbones
[4, 32, 40, 48] and optimise the models with ideal perfor-mances. Models pretrained in this way usually have a su-∗Corresponding author.
†Equal contribution.
Figure 1. Comparisons of trainable backbone parameters between our methods (red) and ﬁne-tuning (black). In COCO, we achieve advanced performances and outperform most existing backbones with only 0.9∼2.5M new backbone parameters (Cascade-RCNN is employed as the detector). The ﬁne-tuning paradigm produces massive redundant backbone parameters, whereas our approach saves over 97% of hardware resources with competitive perfor-mances. The sizes of the circles intuitively compare the number of trainable parameters. perior understanding of homogeneous data. After that, re-searchers with limited computational resources can trans-fer the understanding capabilities of the pre-trained models to downstream tasks with promising performances by ﬁne-tuning [1, 26, 46, 53].
However, the ﬁne-tuned model will produce a new set of parameters as large as the pre-trained model. New pa-rameters are independent of the pre-trained models and un-shareable, which are very hardware intensive for cloud ser-vice providers [23, 49]. Figure 1 compares the parameter quantities of some remarkable backbones and their perfor-mances on the COCO [28] dataset. Recent advances in natural language processing (NLP) [30, 38] show that large pre-trained models trained with rich data have strong gener-iments, we ﬁnd that the models perform better with sparser adapter structures. To improve the performance of Adapter
Tuning, we propose Low-Rank Adapter (LoRand) to re-duce the adapter parameters, as shown in Figure 2. LoRand sparsely parameterizes the matrices in adapters by low-rank synthesis. Speciﬁcally, the projection matrix of the fully-connected layer (FC) in LoRand is a product of multiple low-rank matrices, which reduces FC parameters by more than 80%. We implement extensive experiments on ob-ject detection (PASCAL VOC [14]), semantic segmentation (ADE20K [62]), and instance segmentation (MS COCO
[28]) to verify the capability of LoRand. Experimental re-sults show that LoRand-Tuning is comparable to ﬁne-tuning on multiple tasks with only 1.8% to 2.8% new backbone parameters, which suggests that the pre-trained backbone parameters can be fully shared. More interestingly, our method completely outperforms ﬁne-tuning on the PAS-CAL VOC dataset, illustrating that LoRand-Tuning can re-duce the impairment of ﬁne-tuning on pre-trained models in low-resource conﬁgurations. Our method demonstrates that the LoRand-Tuning paradigm can substantially save storage resources and achieve competitive performances on most dense prediction tasks. In summary, our contributions are three-fold:
• We demonstrate that visual pre-trained models are highly generalisable and shareable. With our training methods, new tasks require only a few trainable pa-rameters to achieve performances comparable to ﬁne-tuning, which can save massive hardware resources.
• We propose the LoRand structure for sparser adapters based on low-rank synthesis. We demonstrate that the backbone parameters in ﬁne-tuning are highly redun-dant, which can be replaced by 1.8% to 2.8% addi-tional parameters in LoRand.
• Extensive experiments on object detection, semantic segmentation, and instance segmentation show that
LoRand-Tuning can achieve remarkable performances and reduce massive new parameters in challenging dense prediction tasks. 2.