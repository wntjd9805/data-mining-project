Abstract
Speech-driven 3D facial animation has been widely stud-ied, yet there is still a gap to achieving realism and vividness due to the highly ill-posed nature and scarcity of audio-visual data. Existing works typically formulate the cross-modal mapping into a regression task, which suffers from the regression-to-mean problem leading to over-smoothed facial motions. In this paper, we propose to cast speech-driven facial animation as a code query task in a finite proxy space of the learned codebook, which effectively pro-motes the vividness of the generated motions by reducing the cross-modal mapping uncertainty. The codebook is learned by self-reconstruction over real facial motions and thus embedded with realistic facial motion priors. Over the discrete motion space, a temporal autoregressive model is employed to sequentially synthesize facial motions from the input speech signal, which guarantees lip-sync as well as plausible facial expressions. We demonstrate that our ap-proach outperforms current state-of-the-art methods both qualitatively and quantitatively. Also, a user study fur-ther justifies our superiority in perceptual quality. Code and video demo are available at https://doubiiu. github.io/projects/codetalker. 1.

Introduction 3D facial animation has been an active research topic for decades, as attributed to its broad applications in virtual re-ality, film production, and games. The high correlation be-tween speech and facial gestures (especially lip movements) makes it possible to drive the facial animation with a speech signal. Early attempts are mainly made to build the complex mapping rules between phonemes and their visual counter-part, which usually have limited performance [53,63]. With the advances in deep learning, recent speech-driven facial animation techniques push forward the state-of-the-art sig-nificantly. However, it still remains challenging to generate human-like motions.
* Corresponding Author.
As an ill-posed problem, speech-driven facial animation generally has multiple plausible outputs for every input.
Such ambiguity tends to cause over-smoothed results. Any-how, person-specific approaches [29, 49] can usually ob-tain decent facial motions because of the relatively consis-tent talking style, but have low scalability to general ap-plications. Recently, VOCA [10] extends these methods to generalize across different identities, however, they gen-erally exhibit mild or static upper face expressions. This is because VOCA formulates the speech-to-motion map-ping as a regression task, which encourages averaged mo-tions, especially in the upper face that is only weakly or even uncorrelated to the speech signal. To reduce the un-certainty, FaceFormer [16] utilizes long-term audio context through a transformer-based model and synthesizes the se-quential motions in an autoregressive manner. Although it gains important performance promotion, it still inherits the weakness of one-to-one mapping formulation and suf-fers from a lack of subtle high-frequency motions. Dif-ferently, MeshTalk [50] models a categorical latent space for facial animation that disentangles audio-correlated and audio-uncorrelated information so that both aspects could be well handled. Anyway, the employed quantization and categorical latent space representation are not well-suited for motion prior learning, rendering the training tricky and consequently hindering its performance.
We get inspiration from 3D Face Morphable Model (3DMM) [35], where general facial expressions are rep-resented in a low-dimensional space. Accordingly, we propose to formulate speech-driven facial animation as a code query task in a finite proxy space of the learned dis-crete codebook prior. The codebook is learned by self-reconstruction over real facial motions using a vector-quantized autoencoder (VQ-VAE) [57], which along with
In the decoder stores the realistic facial motion priors. contrast to the continuous linear space of 3DMM, com-binations of codebook items form a discrete prior space with only finite cardinality. Still, in the context of the de-coder, the code representation possesses high expressive-ness. Through mapping the speech to the finite proxy space, the uncertainty of the speech-to-motion mapping is signif-icantly attenuated and hence promotes the quality of mo-tion synthesis. Conceptually, the proxy space approximates the facial motion space, where the learned codebook items serve as discrete motion primitives.
Based on the learned discrete codebook, we pro-pose a code-query-based temporal autoregressive model for speech-conditioned facial motion synthesis, called
CodeTalker. Specifically, taking a speech signal as input, our model predicts the motion feature tokens in a temporal recursive manner. Then, the feature tokens are used to query the code sequence in the discrete space, followed by facial motion reconstruction. Thanks to the contextual modeling over history motions and cross-modal alignment, the pro-posed CodeTalker shows the advantages of achieving accu-rate lip motions and natural expressions. Extensive experi-ments show that the proposed CodeTalker demonstrates su-perior performance on existing datasets. Systematic studies and experiments are conducted to demonstrate the merits of our method over previous works. The contributions of our work are as follows:
• We model the facial motion space with discrete prim-itives in a novel way, which offers advantages to pro-mote motion synthesis realism against cross-modal un-certainty.
• We propose a discrete motion prior based temporal au-toregressive model for speech-driven facial animation, which outperforms existing state-of-the-art methods. 2.