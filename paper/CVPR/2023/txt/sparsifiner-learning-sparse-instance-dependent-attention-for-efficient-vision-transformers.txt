Abstract
Vision Transformers (ViT) have shown competitive advan-tages in terms of performance compared to convolutional neural networks (CNNs), though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT’s multi-head self-attention (MHSA) operations. However, such struc-tured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned seman-tic connections from a full attention mask.
In this work, we propose an approach to learn instance-dependent at-tention patterns, by devising a lightweight connectivity pre-dictor module that estimates the connectivity score of each pair of tokens. Intuitively, two tokens have high connectiv-ity scores if the features are considered relevant either spa-tially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often very sparse by nature and therefore provide the opportunity to reduce network FLOPs via sparse compu-tations. Equipped with the learned unstructured attention pattern, sparse attention ViT (Sparsifiner) produces a supe-rior Pareto frontier between FLOPs and top-1 accuracy on
ImageNet compared to token sparsity. Our method reduces 48% ∼ 69% FLOPs of MHSA while the accuracy drop is within 0.4%. We also show that combining attention and token sparsity reduces ViT FLOPs by over 60%. 1.

Introduction
Vision Transformers (ViTs) [15] have emerged as a dom-inant model for fundamental vision tasks, such as image classification [15], object detection [3], and semantic seg-mentation [6, 7]. However, scaling ViTs to a large number of tokens is challenging due to the quadratic computational complexity of multi-head self-attention (MHSA) [37].
This is particularly disadvantageous for large-scale vi-sion tasks because processing high-resolution and high-*Equal contribution.
Figure 1. Comparison of Sparsifiner and fixed attention patterns.
Twins [10] (a), Swin [24] (b), and Axial [18] (c) address quadratic
MHSA complexity using fixed attention patterns, which does not consider the instance-dependent nature of semantic information in images. To address this, we propose Sparsifiner (d): an efficient module for sparse instance-dependent attention pattern prediction. dimensionality inputs is desirable. For example, input modalities such as video frames and 3D point clouds have a large number of tokens even for basic use cases. New algo-rithms are needed to continue to scale ViTs to larger, more complex vision tasks.
Prior works have largely taken two approaches to im-prove the computational efficiency of ViTs: token pruning and using fixed sparse attention patterns in MHSA. To-ken pruning methods [29] reduce the number of tokens by a fixed ratio called the keep rate, but accuracy degrades quickly when pruning early layers in the network [17, 32, 33]. For example, introducing token pruning into shallower layers of EViT [17] causes a significant 3.16% top-1 accu-racy drop on ImageNet [14]. This issue is due to the restric-tion of pruning an entire token, which amounts to pruning an entire row and column of the attention matrix at once.
One way to alleviate this is to prune connectivity between individual tokens in the attention matrix. Existing meth-ods that take this attention matrix connectivity-pruning ap-proach use fixed sparse attention patterns [8]. For example, local and strided fixed attention patterns are used [8, 16], in combination with randomly-initialized connectivities [42].
However, such fixed attention patterns limit the capacity of the self-attention connections to a fixed subset of to-kens (Fig. 1). The fixed nature of these attention patterns is less effective compared to the direct communication be-tween tokens in full self-attention. For example, Swin trans-former [23, 24] has a limited the receptive field at shallower layers and needs many layers to model long-range depen-dencies. And BigBird [42] needs to combine multiple fixed attention patterns to achieve good performance. Rather, it is desirable to design sparse attention algorithms that mimic full self attention’s instance-dependent nature [37], thereby capturing the variable distribution of semantic information in the input image content.
To address these challenges, we propose a method called
Sparsifiner that learns to compute sparse connectivity pat-terns over attention that are both instance-dependent and unstructured. The instance-dependent nature of the atten-tion pattern allows each token to use its limited attention budget of nonzero elements more efficiently compared to fixed sparse attention patterns. For example, in attention heads that attend to semantic rather than positional con-tent [37, 38], tokens containing similar semantic informa-tion should be considered to have high connectivity scores despite their spatial distance. Similarly, nearby tokens with irrelevant semantic relations should have lower connectivity scores despite their spatial proximity. Furthermore, Spar-sifiner improves attention pattern flexibility compared to to-ken pruning by pruning individual connectivities instead of entire rows and columns of the attention matrix. This al-lows Sparsifiner to reduce FLOPs in the early layers of the network without incurring significant top-1 accuracy degra-dation (§4). By pruning individual connectivities dependent on image content, Sparsifiner generalizes prior approaches to sparsifying MHSA in ViTs, and in doing so, produces a favourable trade-off between accuracy and FLOPs.
Our contributions are the following:
• We propose a novel efficient algorithm called Spar-sifiner to predict instance-dependent sparse attention patterns using low-rank connectivity patterns. Our in-vestigation into instance-dependent unstructured spar-sity is to the best of our knowledge novel in the context of ViTs.
• We show that such learned unstructured attention spar-sity produces a superior Pareto-optimal tradeoff be-tween FLOPs and top-1 accuracy on ImageNet com-pared to token sparsity. Furthermore, we show that
Sparsifiner is complementary to token sparsity meth-ods, and the two approaches can be combined to achieve superior performance-accuracy tradeoffs.
• We propose a knowledge distillation-based approach for training Sparsifiner from pretrained ViTs using a small number of training epochs. 2.