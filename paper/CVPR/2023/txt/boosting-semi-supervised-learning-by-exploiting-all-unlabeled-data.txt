Abstract
Semi-supervised learning (SSL) has attracted enormous attention due to its vast potential of mitigating the depen-dence on large labeled datasets. The latest methods (e.g.,
FixMatch) use a combination of consistency regulariza-tion and pseudo-labeling to achieve remarkable successes.
However, these methods all suffer from the waste of compli-cated examples since all pseudo-labels have to be selected by a high threshold to filter out noisy ones. Hence, the ex-amples with ambiguous predictions will not contribute to the training phase. For better leveraging all unlabeled ex-amples, we propose two novel techniques: Entropy Mean-ing Loss (EML) and Adaptive Negative Learning (ANL).
EML incorporates the prediction distribution of non-target classes into the optimization objective to avoid competition with target class, and thus generating more high-confidence predictions for selecting pseudo-label. ANL introduces the additional negative pseudo-label for all unlabeled data to leverage low-confidence examples.
It adaptively allo-cates this label by dynamically evaluating the top-k per-formance of the model. EML and ANL do not introduce any additional parameter and hyperparameter. We inte-grate these techniques with FixMatch, and develop a sim-ple yet powerful framework called FullMatch. Extensive experiments on several common SSL benchmarks (CIFAR-10/100, SVHN, STL-10 and ImageNet) demonstrate that
FullMatch exceeds FixMatch by a large margin. Integrated with FlexMatch (an advanced FixMatch-based framework), we achieve state-of-the-art performance. Source code is available at https://github.com/megvii-research/FullMatch. 1.

Introduction
Semi-supervised learning (SSL) is proposed to leverage an abundance of unlabeled data to enhance the model’s per-formance when labeled data is limited [44]. Consistency
*Corresponding author. He is sponsored by Shanghai Sailing Program (23YF1410500). (a) (b)
Figure 1. Visualization of the experimental results on CIFAR-100 with 10000 labeled images. Evaluations are done every 1K itera-tions. (a) The increasing proportion of examples with pseudo-label when applying EML to FixMatch. (b) The number of negative pseudo-labels per sample and accuracy during the whole training process. “NPL” denotes negative pseudo-labels. regularization [1, 3, 23] and pseudo labeling [10, 12, 25] have shown significant ability for leveraging unlabeled data and thus they are widely used in SSL frameworks. Re-cently, FixMatch-based methods [21, 26, 31, 38] that com-bine the two technologies in a unified framework have they apply achieved noticeable successes. Specifically, weak-augmentation (e.g., only random flip and shift) to un-labeled data and obtain their predictions, and then corre-sponding one-hot pseudo-label is generated if the largest prediction confidence is beyond the predefined threshold (e.g., 0.95), finally it is used as a training target when in-putting the strongly-augmented examples (e.g., RandAug-ment [6], Cutout [8]).
However, the FixMatch-based methods still have a sig-nificant drawback that they rely on an extremely high threshold to produce accurate pseudo-labels, which results in ignoring a large number of unlabeled examples with am-biguous predictions, especially on the early and middle training stages. We can easily observe this phenomenon according to the blue curve in Fig. 1(a), which visualizes the proportion of samples with pseudo-labels at different training iterations when applying FixMatch [26] to CIFAR-100 [16] with 10,000 labels. It shows that the ratio of se-lected examples with pseudo-label is around 58% after 200k iterations and merely reaches 84% in the end. This moti-vates us to exploit more unlabeled data to boost the overall performance.
One intuitive solution is to assign pseudo-label for po-tential examples (i.e., the maximum confidence is close to the predefined threshold). We argue the competition between partial classes leads to failure to produce high-confidence prediction, while the unsupervised loss of Fix-Match (i.e, cross-entropy) only focus on the target class when training the examples with pseudo-label. Therefore, we propose a novel scheme to enhance confidence on tar-get class, namely Entropy Meaning Loss (EML). For exam-ples with pseudo-label, EML imposes additional supervi-sion on all non-target classes (i.e., classes which specify the absence of a specific label) to push their prediction close to a uniform distribution, thus preventing any class com-petition with the target class. Fig. 1(a) illustrates that Fix-Match equipped with EML can select more examples with the pseudo-label during the whole training process. Since
EML attempts to yield more low-entropy predictions to se-lect more examples with pseudo-label rather than tuning the threshold, it can be also applied to any dynamic-threshold methods (e.g., FlexMatch [38], Dash [34]).
Nevertheless, it is still impossible to leverage all un-labeled data by generating pseudo-labels with a threshold strategy. This motivates us to further consider how to uti-lize the low-confidence unlabeled examples without pseudo-label (i.e., the maximum confidence is far from the prede-fined threshold).
Intuitively, the prediction may get con-fused among the top classes, but it will be confident that the input does not belong to the categories ranked after these classes. Fig. 2 shows an inference result of FixMatch. The ground truth is “cat”, FixMatch is confused by several top classes (e.g., “dog”, “frog”) and make low-confidence pre-diction, however it shows highly confidence that some low-rank classes (e.g.,“airplane”, “horse”) are not ground truth class, thus we can safely assign negative pseudo-labels to these classes. Based on this insight, we propose a novel method named Adaptive Negative Learning (ANL). Specif-ically, ANL first calculate a k adaptively based on the pre-diction consistency, so that the accuracy of top-k is close to 1, and then regard the classes ranked after k as nega-tive pseudo-labels. Furthermore, if the example is selected a pseudo-label, ANL will shrink the range of non-target classes (i.e., EML only needs constrain the top-k classes ex-cept target class). Note that ANL is a threshold-independent scheme and thus can be applied on all unlabeled data. As shown in Fig. 1(b), our ANL’s rendered negative pseudo-labels are increasing as the model is optimized while keep-ing high accuracy. In summary, our method makes full use of the unlabeled dataset, which is hardly ever seen in mod-ern SSL algorithms.
To demonstrate the effectiveness of the proposed EML and ANL, we simply integrate EML and ANL to Fix-Figure 2. An example of inference result of FixMatch.
It can conclude that the input does not belong to these low-rank classes, such as airplanes, horse.
Match and exploit a new framework named FullMatch.
We conduct various experiments on CIFAR-10/100 [16],
SVHN [22], STL-10 [5] and ImageNet [7]. The results demonstrate that FullMatch surpasses the performance of
FixMatch with a large margin while the training cost re-mains similar to FixMatch. Moreover, our method can be easily adapted to other FixMatch-based algorithms and obtain further improvement. For example, by combining it with FlexMatch [38], we achieve state-of-the-art perfor-mance. To summarize, our key contributions include: 1) We introduce an additional supervision namely En-tropy Meaning Loss (EML) when training examples with pseudo-label, which enforces a uniform distribution of non-target classes to avoid them competition with target class and thus producing more high-confidence predictions. 2) We propose the Adaptive Negative Learning (ANL), a dynamic negative pseudo-labels allocation scheme, which renders negative pseudo-labels with very limited extra com-putational overhead for all unlabeled data, including the low-confidence ones. 3) We design a simple yet effective framework named
FullMatch by simply integrating FixMatch with the pro-posed EML and ANL, which leverages all unlabeled data and thus achieving remarkable gains on five benchmarks.
Furthermore, our method is shown to be orthogonal to other
FixMatch-based frameworks. Specifically, FlexMatch with our method, achieves state-of-the-art results. 2.