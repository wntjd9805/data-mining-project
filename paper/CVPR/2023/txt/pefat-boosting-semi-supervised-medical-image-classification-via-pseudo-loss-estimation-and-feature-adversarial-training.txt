Abstract
Pseudo-labeling approaches have been proven beneficial for semi-supervised learning (SSL) schemes in computer vi-sion and medical imaging. Most works are dedicated to finding samples with high-confidence pseudo-labels from the perspective of model predicted probability. Whereas this way may lead to the inclusion of incorrectly pseudo-labeled data if the threshold is not carefully adjusted. In addition, low-confidence probability samples are frequently
In disregarded and not employed to their full potential. this paper, we propose a novel Pseudo-loss Estimation and Feature Adversarial Training semi-supervised frame-work, termed as PEFAT, to boost the performance of multi-class and multi-label medical image classification from the point of loss distribution modeling and adversarial train-ing.
Specifically, we develop a trustworthy data selec-tion scheme to split a high-quality pseudo-labeled set, in-spired by the dividable pseudo-loss assumption that clean data tend to show lower loss while noise data is the oppo-site. Instead of directly discarding these samples with low-quality pseudo-labels, we present a novel regularization approach to learn discriminate information from them via injecting adversarial noises at the feature-level to smooth the decision boundary. Experimental results on three med-ical and two natural image benchmarks validate that our
PEFAT can achieve a promising performance and surpass other state-of-the-art methods. The code is available at https://github.com/maxwell0027/PEFAT. 1.

Introduction
Deep learning has achieved remarkable success in vari-ous computer vision tasks [4–6,13,15,19]. This success has
∗Equal contribution. †Y. Xia is the corresponding author. This work was supported in part by the National Natural Science Foundation of China under Grants 62171377, and in part by the Key Research and Development
Program of Shaanxi Province, China, under Grant 2022GY-084. (a) Illustration of traditional SSL methods (top) and our method (bottom). (b) Probability distribution of labeled data (left) and validation data (right), when using the warm-upped model on ISIC2018 dataset.
Figure 1. (a) shows the main difference between our method and other SSL methods, our method selects high-quality pseudo-labeled data by pseudo-loss estimation, and also injects feature-level adversarial noises for better unlabeled data mining; (b) in-dicates the phenomenon that clean pseudo-labeled set is hard to collect when using probability-based threshold, which is mainly attributed to the over-confident prediction. also made practical applications more accessible, including medical image analysis (MIA) [10, 21, 30, 31, 35, 39]. How-ever, unlike computer vision, annotating a large-scale med-ical image dataset requires expert knowledge and is time-consuming and costly. Alternatively, unlabeled data can be collected from clinical sites in a more available way, thereby mitigating the cost of data annotation by leveraging these unlabeled data.
Semi-supervised Learning (SSL) has drawn a lot of at-tention due to its superior performance, by only leveraging limited labeled data and a vast number of unlabeled data.
Under the SSL setting, it is critical to mine adequate infor-mation from unlabeled data. In the existing SSL methods, pseudo-labeling [14, 22, 29, 37, 38] and consistency regular-ization [12, 24, 25] are the mainstream. The former focuses on finding confident pseudo-labels for re-training, and the latter aims to improve the robustness of the model by keep-ing one logical distribution similar to the other.
However, most SSL methods encounter two issues. First, unreliable pseudo-labels are a problem with the threshold-selecting data method based on predicted probability as it often introduces numerous incorrect pseudo-labels due to confirmation bias. As illustrated in Figure 1b, unlabeled data with both correct and incorrect pseudo-labels follow similar probability distributions. Second, informative unla-beled samples are underutilized as unselected data with low probabilities typically cluster around the decision boundary.
Recent studies [1, 17] have found that neural networks tend to fit clean data first and then memorize noise data during training, resulting in lower loss for clean data and higher loss for noise data in early stages of training. Furthermore, some works [9, 25] have investigated the effects of adver-sarial training under semi-supervised settings, which show potential for learning from low-quality pseudo-labeled data.
All these findings pave the way towards solving the afore-mentioned problems.
In this paper, we propose a novel SSL method called
Pseudo-loss Estimation and Feature Adversarial Training (PEFAT) for multi-class and multi-label medical image classification. First, we introduce a new estimation scheme for reliable pseudo-labeled data selection from the perspec-tive of pseudo-loss distribution. It is motivated by our argu-ment that there is a dividable loss distribution between cor-rect and incorrect pseudo-labeled data. Specifically, we first warm up the model on training data with contrastive learn-ing, in order to learn unbiased representation. Then we set up a two-component Gaussian Mixture Model (GMM) [28] to learn prior loss distribution on labeled data.
In the procedure of pseudo-labeled data selection, we feed cross pseudo-loss to the fitted GMM and obtain the trustworthy pseudo-labeled data with posterior probability. Second, we propose a feature adversarial training (FAT) strategy that in-jects adversarial noises in the feature-level to smooth the de-cision boundary, aiming at for further utilizing the rest uns-elected but informative data. Although FAT is originally de-signed for the rest data, it can also be applied to the selected pseudo-labeled data. Based on the technics above, our PE-FAT successfully boosts the classification performance in
MIA from the point of trustworthy pseudo-labeled data se-lection and adversarial consistency regularization.
To summarize, our main contributions are three-fold. (1)
Different from previous works that select pseudo-labeled data with a probability threshold, we present a new se-lection approach from the perspective of the loss distribu-tion, which exhibits superior ability in high-quality pseudo-labeled data collection. (2) We propose a new adversarial regularization strategy to fully leverage the rest unlabeled but informative data, which benefits the model in decision boundary smoothing and better representation learning. (3)
Extensive experimental results on three public medical im-age datasets and two natural image datasets demonstrate the superiority of the proposed PEFAT, which significantly sur-passes other advanced SSL methods. 2.