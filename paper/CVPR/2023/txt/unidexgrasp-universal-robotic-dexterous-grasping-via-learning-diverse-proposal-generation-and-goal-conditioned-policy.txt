Abstract
In this work, we tackle the problem of learning universal robotic dexterous grasping from a point cloud observation under a table-top setting. The goal is to grasp and lift up ob-jects in high-quality and diverse ways and generalize across hundreds of categories and even the unseen.
Inspired by successful pipelines used in parallel gripper grasping, we split the task into two stages: 1) grasp proposal (pose) gen-eration and 2) goal-conditioned grasp execution. For the first stage, we propose a novel probabilistic model of grasp pose conditioned on the point cloud observation that fac-torizes rotation from translation and articulation. Trained
*Equal contribution.
†Corresponding author. on our synthesized large-scale dexterous grasp dataset, this model enables us to sample diverse and high-quality dex-terous grasp poses for the object point cloud. For the sec-ond stage, we propose to replace the motion planning used in parallel gripper grasping with a goal-conditioned grasp policy, due to the complexity involved in dexterous grasp-ing execution. Note that it is very challenging to learn this highly generalizable grasp policy that only takes re-alistic inputs without oracle states. We thus propose sev-eral important innovations, including state canonicaliza-tion, object curriculum, and teacher-student distillation. In-tegrating the two stages, our final pipeline becomes the first to achieve universal generalization for dexterous grasping, demonstrating an average success rate of more than 60% on thousands of object instances, which significantly out-performs all baselines, meanwhile showing only a minimal generalization gap. 1.

Introduction
Robotic grasping is a fundamental capability for an agent to interact with the environment and serves as a prerequi-site to manipulation, which has been extensively studied for decades. Recent years have witnessed great progress in developing grasping algorithms for parallel grippers
[8, 16, 17, 20, 49, 50] that carry high success rate on univer-sally grasping unknown objects. However, one fundamental limitation of parallel grasping is its low dexterity which lim-its its usage to complex and functional object manipulation.
Dexterous grasping provides a more diverse way to grasp objects and thus is of vital importance to robotics for func-tional and fine-grained object manipulation [2, 29, 38, 40, 52]. However, the high dimensionality of the actuation space of a dexterous hand is both the advantage that en-dows it with such versatility and the major cause of the diffi-culty in executing a successful grasp. As a widely used five-finger robotic dexterous hand, ShadowHand [1] amounts to 26 degrees of freedom (DoF), in contrast with 7 DoF for a typical parallel gripper. Such high dimensionality magni-fies the difficulty in both generating valid grasp poses and planning the execution trajectories, and thus distinguishes the dexterous grasping task from its counterpart for paral-lel grippers. Several works have tackled the grasping pose synthesis problem [6, 28, 33, 49], however, they all assume oracle inputs (full object geometry and states). Very few works [9,38] tackle dexterous grasping in a realistic robotic setting, but so far no work yet can demonstrate universal and diverse dexterous grasping that can well generalize to unseen objects.
In this work, we tackle this very challenging task: learn-ing universal dexterous grasping skills that can generalize well across hundreds of seen and unseen object categories in a realistic robotic setting and only allow us to access depth observations and robot proprioception information.
Our dataset contains more than one million grasps for 5519 object instances from 133 object categories, which is the largest robotic dexterous grasping benchmark to evaluate universal dexterous grasping.
Inspired by the successful pipelines from parallel grip-pers, we propose to decompose this challenging task into two stages: 1) dexterous grasp proposal generation, in which we predict diverse grasp poses given the point cloud observations; and 2) goal-conditioned grasp execution, in which we take one grasp goal pose predicted by stage 1 as a condition and generates physically correct motion trajecto-ries that comply with the goal pose. Note that both of these two stages are indeed very challenging, for each of which we contribute several innovations, as explained below.
For dexterous grasp proposal generation, we devise a novel conditional grasp pose generative model that takes point cloud observations and is trained on our synthesized large-scale table-top dataset. Here our approach empha-sizes the diversity in grasp pose generation, since the way we humans manipulate objects can vary in many different ways and thus correspond to different grasping poses. With-out diversity, it is impossible for the grasping pose gen-eration to comply with the demand of later dexterous ma-nipulation. Previous works [22] leverages CVAE to jointly model hand rotation, translation, and articulations and we observe that such CVAE suffers from severe mode collapse and can’t generate diverse grasp poses, owing to its lim-ited expressivity when compared to conditional normaliz-ing flows [12, 13, 15, 23, 35] and conditional diffusion mod-els [5, 43, 46]. However, no works have developed normal-izing flows and diffusion models that work for the grasp pose space, which is a Cartesian product of SO(3) of hand rotation and a Euclidean space of the translation and joint angles. We thus propose to decompose this conditional generative model into two conditional generative models: a conditional rotation generative model, namely GraspIPDF, leveraging ImplicitPDF [34] (in short, IPDF) and a con-ditional normalizing flow, namely GraspGlow, leveraging
Glow [23]. Combining these two modules, we can sample diverse grasping poses and even select what we need ac-cording to language descriptions. The sampled grasps can be further refined to be more physically plausible via Con-tactNet, as done in [22].
For our grasp execution stage, we learn a goal-conditioned policy that can grasp any object in the way specified by the grasp goal pose and only takes realistic in-puts: the point cloud observation and robot proprioception information, as required by real robot experiments. Note that reinforcement learning (RL) algorithms usually have difficulties with learning such a highly generalizable pol-icy, especially when the inputs are visual signals without ground truth states. To tackle this challenge, we leverage a teacher-student learning framework that first learns an or-acle teacher model that can access the oracle state inputs and then distill it to a student model that only takes realis-tic inputs. Even though the teacher policy gains access to oracle information, making it successful in grasping thou-sands of different objects paired with diverse grasp goals is still formidable for RL. We thus introduce two critical inno-vations: a canonicalization step that ensures SO(2) equiv-ariance to ease the policy learning; and an object curricu-lum that first learns to grasp one object with different goals, then one category, then many categories, and finally all cat-egories.
Extensive experiments demonstrate the remarkable per-formance of our pipelines. In the grasp proposal generation stage, our pipeline is the only method that exhibits high di-versity while maintaining the highest grasping quality. The whole dexterous grasping pipeline, from the vision to pol-icy, again achieves impressive performance in our simula-tion environment and, for the first time, demonstrates a uni-versal grasping policy with more than 60% success rate and remarkably outperforms all the baselines. We will make the dataset and the code publicly available to facilitate future research. 2.