Abstract
Human-Object Interaction (HOI) detection, which lo-calizes and infers relationships between human and ob-jects, plays an important role in scene understanding. Al-though two-stage HOI detectors have advantages of high efficiency in training and inference, they suffer from lower performance than one-stage methods due to the old back-bone networks and the lack of considerations for the
HOI perception process of humans in the interaction clas-sifiers.
In this paper, we propose Vision Transformer based Pose-Conditioned Self-Loop Graph (ViPLO) to re-solve these problems. First, we propose a novel feature ex-traction method suitable for the Vision Transformer back-bone, called masking with overlapped area (MOA) module.
The MOA module utilizes the overlapped area between each patch and the given region in the attention function, which addresses the quantization problem when using the Vision
Transformer backbone. In addition, we design a graph with a pose-conditioned self-loop structure, which updates the human node encoding with local features of human joints.
This allows the classifier to focus on specific human joints to effectively identify the type of interaction, which is mo-tivated by the human perception process for HOI. As a re-sult, ViPLO achieves the state-of-the-art results on two pub-lic benchmarks, especially obtaining a +2.07 mAP perfor-mance gain on the HICO-DET dataset. 1.

Introduction
Human-Object Interaction (HOI) detection aims to lo-calize human and objects in the image, and classify interac-tions between them. A detected HOI instance is represented in the form of ⟨human, object, interaction⟩ triplet. HOI detection is steadily attracting attention in the computer vi-sion field, due to the high potential for use in the down-stream tasks such as action recognition [46], scene under-standing [44], and image captioning [47].
Existing HOI detectors can be divided into two cate-gories: two-stage methods and one-stage methods. The for-mer usually consists of three steps: 1) human and object detection with an off-the-shelf detector; 2) feature extrac-tion for human and objects with ROI-Pooling; and 3) pre-diction of the interaction between human and objects with the extracted features [3, 8, 12, 15, 16, 40]. One-stage HOI detection framework is first proposed in PPDM [26], which directly predicts HOI triplets based on interaction points
[26,42] and union boxes [21]. Recent studies based on one-stage structure use the two-branch transformer, which pre-dict HOI triplets with two sub-task decoders and matching processes [27, 53]. Nevertheless, these methods suffer from low training speed and large memory usage [50, 55].
In contrast, two-stage HOI detectors show high training speed using pre-trained object detectors. They are also ad-vantageous when the object bounding box is known in ad-vance from the image. For example, when analyzing HOI events of fixed target objects, we can efficiently detect inter-action using the fixed object regions without an extra object detection process. Moreover, interaction inference can be performed only for the desired human-object pair, which is also a big advantage compared to the one-stage methods in the application situation. Despite these advantages, two-stage methods have recently been less studied due to their low performance. In this paper, we focus on improving the performance of the two-stage approach, which can also en-joy the benefit of reduced computational and memory com-plexity. In particular, we improve the two important parts: feature extraction and interaction prediction.
Regarding the feature extraction part, we note that most of the two-stage methods rely on ResNet [14] as back-bone networks [15, 16, 49]. Recently, the Vision Trans-former (ViT) has emerged as a powerful alternative yield-ing state-of-the-art performance in various computer vision fields [34, 36, 38], and it also has a potential as an improved feature extractor for two-stage HOI detection. However, it is not straightforward to apply a conventional feature extrac-tion method (e.g., ROIAlign [13]) directly to a ViT back-bone due to the different shapes of the output feature maps of ResNet and ViT. Therefore, we propose a novel Masking
Figure 1. The process of HOI recognition by humans. Humans first localize each person and object (Step 1), identify interactiveness using the spatial relationship and human pose (Step 2), and finally focus on specific human joints to recognize the type of interaction (Step 3). with Overlapped Area (MOA) module for feature extraction using a ViT backbone. The MOA module resolves the spa-tial quantization problem using the overlapped area between each patch and the given region in the attention function. In addition, efficient computation in the MOA module keeps the inference speed similar to that of the ResNet backbone.
To improve the interaction prediction part, we refer to the human perception process of recognizing HOI. Previ-ous studies [1, 9, 10] show that the HOI perception of hu-mans considers information such as object identity, rela-tive positions, reach motions, manipulation motions, and context. Following this, we assume that HOI recognition by humans consists of three steps (Fig. 1): 1) Localize each human and object; 2) Identify interactiveness using the spatial relationship between human and object and hu-man pose information; 3) Focus on specific human joints to identify the type of interaction. Motivated by these, we design a pose-conditioned graph neural network for interac-tion prediction. For identifying the interactiveness part, pre-vious studies exclude non-interactive pairs with auxiliary networks [25, 30]. However, our model represents interac-tiveness with edge encoding, which is obtained by using the spatial human-object relationship and the human pose in-formation. We can expect more meaningful message pass-ing with interactiveness-aware edge encoding. For focusing on specific joints part, early methods utilize local pose in-formation with simple message passing or attention mecha-nisms [40, 54]. In contrast, we introduce pose-aware atten-tion mechanism using query (spatial features) / key (joint features) structure, resulting in human node encodings con-taining richer local information by a self-loop structure.
As a result, we propose Vision Transformer based Pose-Conditioned Self-Loop Graph (ViPLO), a novel two-stage
HOI detector. We demonstrate the effectiveness of our framework by achieving state-of-the-art results on two pub-lic benchmarks: HICO-DET [3] and V-COCO [11]. In ad-dition, we further evaluate and analyze our model design through extensive ablation studies. The contributions of this paper are as follows:
• We propose an effective two-stage HOI detector with a ViT backbone by introducing a novel MOA module, which is a suitable feature extraction module for ViT.
• We design the interaction classifier with a pose-conditioned self-loop graph structure, motivated by the human perception process.
• Our model outperforms the previous state-of-the-art results on a widely-used HOI benchmark. 2.