Abstract
Masked Autoencoders learn strong visual representa-tions and achieve state-of-the-art results in several inde-pendent modalities, yet very few works have addressed their capabilities in multi-modality settings. In this work, we fo-cus on point cloud and RGB image data, two modalities that are often presented together in the real world, and explore their meaningful interactions. To improve upon the cross-modal synergy in existing works, we propose Pi-MAE, a self-supervised pre-training framework that pro-motes 3D and 2D interaction through three aspects. Specif-ically, we first notice the importance of masking strategies between the two sources and utilize a projection module to complementarily align the mask and visible tokens of the two modalities. Then, we utilize a well-crafted two-branch MAE pipeline with a novel shared decoder to pro-mote cross-modality interaction in the mask tokens. Fi-nally, we design a unique cross-modal reconstruction mod-ule to enhance representation learning for both modali-ties. Through extensive experiments performed on large-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2), we discover it is nontrivial to interac-tively learn point-image features, where we greatly improve multiple 3D detectors, 2D detectors, and few-shot classi-fiers by 2.9%, 6.7%, and 2.4%, respectively. Code is avail-able at https://github.com/BLVLab/PiMAE. 1.

Introduction
The advancements in deep learning-based technology have developed many significant real-world applications,
*Equal Contribution.
†Corresponding Author.
Figure 1. With our proposed design, PiMAE learns cross-modal representations by interactively dealing with multi-modal data and performing reconstruction. such as robotics and autonomous driving. In these scenar-ios, 3D and 2D data in the form of point cloud and RGB im-ages from a specific view are readily available. Therefore, many existing methods perform multi-modal visual learn-ing, a popular approach leveraging both 3D and 2D infor-mation for better representational abilities.
Intuitively, the paired 2D pixels and 3D points present different perspectives of the same scene. They encode dif-ferent degrees of information that, when combined, may be-come a source of performance improvement. Designing a model that interacts with both modalities, such as geometry and RGB, is a difficult task because directly feeding them to a model results in marginal, if not degraded, performance, as demonstrated by [34].
In this paper, we aim to answer the question: how to design a more interactive unsupervised multi-modal learn-ing framework that is for better representation learning? To this end, we investigate the Masked Autoencoders (MAE) proposed by He et al. [20], which demonstrate a straightfor-ward yet powerful pre-training framework for Vision Trans-formers [10] (ViTs) and show promising results for inde-pendent modalities of both 2D and 3D vision [2, 14, 17, 63, 64]. However, these existing MAE pre-training objectives are limited to only a single modality.
While much literature has impressively demonstrated
MAE approaches’ superiority in multiple modalities, exist-ing methods have yet to show promising results in bridging 3D and 2D data. For 2D scene understanding among multi-ple modalities, MultiMAE [2] generates pseudo-modalities to promote synergy for extrapolating features. Unfortu-nately, these methods rely on an adjunct model for gener-ating pseudo-modalities, which is sub-optimal and makes it hard to investigate cross-modality interaction. On the other hand, contrastive methods for self-supervised 3D and 2D representation learning, such as [1, 6, 7, 34, 58], suffer from sampling bias when generating negative samples and aug-mentation, making them impractical in real-world scenar-ios [8, 22, 73].
To address the fusion of multi-modal point cloud and im-age data, we propose PiMAE, a simple yet effective pipeline that learns strong 3D and 2D features by increasing their interaction. Specifically, we pre-train pairs of points and images as inputs, employing a two-branched MAE learn-ing framework to individually learn embeddings for the two modalities. To further promote feature alignment, we de-sign three main features.
First, we tokenize the image and point inputs, and to cor-relate the tokens from different modalities, we project point tokens to image patches, explicitly aligning the masking re-lationship between them. We believe a specialized masking strategy may help point cloud tokens embed information from the image, and vice versa. Next, we utilize a novel symmetrical autoencoder scheme that promotes strong fea-ture fusion. The encoder draws inspiration from [31], con-sisting of both separate branches of modal-specific encoders and a shared-encoder. However, we notice that since MAE’s mask tokens only pass through the decoder [20], a shared-decoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders. Finally, for learning stronger features inspired by [15,67], PiMAE’s multi-modal reconstruction module tasks point cloud features to explic-itly encode image-level understanding through enhanced learning from image features.
To evaluate the effectiveness of our pre-training scheme, we systematically evaluate PiMAE with different fine-tuning architectures and tasks, including 3D and 2D ob-ject detection and few-shot image classification, performed on the RGB-D scene dataset SUN RGB-D [51] and Scan-netV2 [9] as well as multiple 2D detection and classification datasets. We find PiMAE to bring improvements over state-of-the-art methods in all evaluated downstream tasks.
Our main contributions are summarized as:
• To the best of our knowledge, we are the first to pro-pose pre-training MAE with point cloud and RGB modalities interactively with three novel schemes.
• To promote more interactive multi-modal learning, we novelly introduce a complementary cross-modal mask-ing strategy, a shared-decoder, and cross-modal recon-struction to PiMAE.
• Shown by extensive experiments, our pre-trained mod-els boost performance of 2D & 3D detectors by a large margin, demonstrating PiMAE’s effectiveness. 2.