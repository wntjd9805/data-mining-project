Abstract
Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone, V2C) task aims to generate speeches that match the speaker’s emotion presented in the video using the de-sired speaker voice as reference. V2C is more challeng-ing than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the vary-ing emotions and speaking speed presented in the video.
Unlike previous works, we propose a novel movie dub-bing architecture to tackle these problems via hierarchi-cal prosody modeling, which bridges the visual informa-tion to corresponding speech prosody from three aspects:
Specifically, we align lip move-lip, ment to the speech duration, and convey facial expres-sion to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by the psychology findings. Moreover, we design an emo-tion booster to capture the atmosphere from global video scenes. All these embeddings are used together to gener-ate mel-spectrogram, which is then converted into speech waves by an existing vocoder. Extensive experimental re-sults on the V2C and Chem benchmark datasets demon-strate the favourable performance of the proposed method.
The code and trained models will be made available at https://github.com/GalaxyCong/HPMDubbing. face, and scene. 1.

Introduction
Movie dubbing, also known as visual voice clone (V2C) [9], aims to convert a paragraph of text to a speech with both desired voice specified by reference audio and de-sired emotion and speed presented in the reference video as shown in the top panel of Figure 1. V2C is more challeng-ing than other speech synthesis tasks in two aspects: first,
†Corresponding author.
Figure 1. (a) Illustration of the V2C tasks. (b) To generate natural speech with proper emotions, we align the phonemes with lip mo-tion, estimate pitch and energy based on facial expression’s arousal and valence, and predict global emotion from video scenes. it requires synchronization between lip motion and gener-ated speech; second, it requires proper prosodic variations of the generated speech to reflect the speaker’s emotion in the video (i.e., the movie’s plot). These pose significant challenges to existing voice cloning methods.
Although significant progress has been made, exist-ing methods do not handle the challenges in V2C well.
Specifically, text-based dubbing methods [46–48, 54] con-struct speeches from given text conditioned on the differ-ent speaker embedding but do not consider audio-visual synchronization. On the other hand, lip-referred dubbing schemes [18,32,55] predict mel-spectrograms directly from a sequence of lip movements typically by encoder-decoder models. Due to high error rates in generated words, these methods can hardly guarantee high-quality results. Further-more, video-and-text based dubbing methods [17, 20, 32] focus on inferring speaker characters (e.g., age and gender).
However, these visual references usually do not convey tar-geted emotion well as intended in V2C.
An ideal dub should align well with the target charac-ter so that the audiences feel it is the character speaking instead of the dubber [7]. Thus, a professional dubber usu-ally has a keen sense of observing the unique characteris-In this tics of the subject and acts on voice accordingly. work, we address these issues with a hierarchical dubbing architecture to synthesize speech. Unlike previous methods, our model connects video representations to speech coun-terparts at three levels: lip, face, and scene, as shown in
Figure 1.
In this paper, we propose a hierarchical prosody model-ing for movie dubbing, which could keep the audio-visual sync and synthesis speech with proper prosody follow-ing the movie’s plot. Specifically, we first design a dura-tion alignment module that controls speech speed by learn-ing temporal correspondence via multi-head attention over phonemes and lip motion. Second, we propose an affective-display based Prosody Adaptor (PA), which learns affec-tive psychology computing conditioned on facial expres-sion and is supervised by corresponding energy and pitch in the target voice. In particular, we introduce arousal and valence features extracted from facial regions as emotion representations. This is inspired by the affective comput-ing method [51], which analyses the facial affect relying on dimensional measures, namely valence (how positive the emotional display is) and arousal (how calming or exciting the expression looks). Third, we exploit a scene-atmosphere based emotion booster, which fuses the global video rep-resentation with the above adapted hidden sequence and is supervised by the emotive state of the whole voice. The out-puts of these three modules are fed into a transformer-based decoder, which converts the speech-related representations into mel-spectrogram. Finally, we output the target speech waves from the mel-spectrogram via a powerful vocoder.
The contributions of this paper are summarized below:
• We propose a novel hierarchical movie dubbing archi-tecture to better synthesize speech with proper prosody by associating them with visual counterparts: lips, fa-cial expressions, and surrounding scenes.
• We design an affective display-based prosody adap-tor to predict the energy and pitch of speech from the arousal and valence fluctuations of facial regions in videos, which provides a fine-grained alignment with speakers’ emotions.
• Extensive experimental results demonstrate the pro-posed method performs well against state-of-the-art models on two benchmark datasets. 2.