Abstract
Current state-of-the-art approaches for few-shot action recognition achieve promising performance by conducting frame-level matching on learned visual features. However, they generally suffer from two limitations: i) the matching procedure between local frames tends to be inaccurate due to the lack of guidance to force long-range temporal percep-tion; ii) explicit motion learning is usually ignored, leading to partial information loss. To address these issues, we de-velop a Motion-augmented Long-short Contrastive Learn-ing (MoLo) method that contains two crucial components, including a long-short contrastive objective and a motion autodecoder. Specifically, the long-short contrastive objec-tive is to endow local frame features with long-form tem-poral awareness by maximizing their agreement with the global token of videos belonging to the same class. The motion autodecoder is a lightweight architecture to recon-struct pixel motions from the differential features, which ex-plicitly embeds the network with motion dynamics. By this means, MoLo can simultaneously learn long-range tempo-ral context and motion cues for comprehensive few-shot matching. To demonstrate the effectiveness, we evaluate
MoLo on five standard benchmarks, and the results show that MoLo favorably outperforms recent advanced meth-ods. The source code is available at https://github. com/alibaba-mmai-research/MoLo. 1.

Introduction
Recently, action recognition has achieved remarkable progress and shown broad prospects in many application fields [1, 5, 8, 37, 67]. Despite this, these successes rely heavily on large amounts of manual data annotation, which greatly limits the scalability to unseen categories due to the
∗ Intern at Alibaba DAMO Academy.
† Corresponding authors.
Figure 1.
Illustration of our motivation. We show that most existing metric-based local frame matching methods, such as
OTAM [4], can be easily perturbed by some similar co-existing video frames due to the lack of forced global context awareness during the support-query temporal alignment process. Example videos come from the commonly used SSv2 dataset [15]. high cost of acquiring large-scale labeled samples. To alle-viate the reliance on massive data, few-shot action recogni-tion [88] is a promising direction, aiming to identify novel classes with extremely limited labeled videos.
Most mainstream few-shot action recognition ap-proaches [4,21,44,74] adopt the metric-based meta-learning strategy [61] that learns to map videos into an appropriate feature space and then performs alignment metrics to pre-dict query labels. Typically, OTAM [4] leverages a deep network to extract video features and explicitly estimates an ordered temporal alignment path to match the frames of two videos. HyRSM [74] proposes to explore task-specific semantic correlations across videos and designs a bidirec-tional Mean Hausdorff Metric (Bi-MHM) to align frames.
Though these works have obtained significant results, there are still two limitations: first, existing standard metric-based techniques mainly focus on local frame-level alignment and are considered limited since the essential global informa-tion is not explicitly involved. As shown in Figure 1, lo-cal frame-level metrics can be easily affected by co-existing similar video frames. We argue that it would be beneficial to achieve accurate matching if the local frame features can predict the global context in few-shot classification; sec-ond, motion dynamics are widely regarded as a vital role in the field of video understanding [6, 22, 25, 42, 50, 70], while the existing few-shot methods do not explicitly ex-plore the rich motion cues between frames for the matching procedure, resulting in a sub-optimal performance. In the literature [5, 67], traditional action recognition works intro-duce motion information by feeding optical flow or frame difference into an additional deep network, which leads to non-negligible computational overhead. Therefore, an effi-cient motion compensation method should be introduced to achieve comprehensive few-shot matching.
Inspired by the above observations, we develop a motion-augmented long-short contrastive learning (MoLo) method to jointly model the global contextual information and motion dynamics. More specifically, to explicitly in-tegrate the global context into the local matching process, we apply a long-short contrastive objective to enforce frame features to predict the global context of the videos that be-long to the same class. For motion compensation, we de-sign a motion autodecoder to explicitly extract motion fea-tures between frame representations by reconstructing pixel motions, e.g., frame differences. In this way, our proposed
MoLo enables efficient and comprehensive exploitation of temporal contextual dependencies and motion cues for ac-curate few-shot action recognition. Experimental results on multiple widely-used benchmarks demonstrate that our
MoLo outperforms other advanced few-shot techniques and achieves state-of-the-art performance.
In summary, our contributions can be summarized as fol-lows: (1) We propose a novel MoLo method for few-shot action recognition, aiming to better leverage the global con-text and motion dynamics. (2) We further design a long-short contrastive objective to reinforce local frame features to perceive comprehensive global information and a motion autodecoder to explicitly extract motion cues. (3) We con-duct extensive experiments across five widely-used bench-marks to validate the effectiveness of the proposed MoLo.
The results demonstrate that MoLo significantly outper-forms baselines and achieves state-of-the-art performance. 2.