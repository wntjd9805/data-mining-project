Abstract
We introduce LAVILA, a new approach to learning video-language representations by leveraging Large Lan-guage Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and ﬁnetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the vi-sual information and text, and much higher diversity of text.
The video-language embedding learned contrastively with these narrations outperforms the previous state-of-the-art on multiple ﬁrst-person and third-person video tasks, both in zero-shot and ﬁnetuned setups. Most notably, LAVILA obtains an absolute gain of 10.1% on EGTEA classiﬁca-tion and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LAVILA trained with only half the narrations from the Ego4D dataset outperforms models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size. 1.

Introduction
Learning visual representation using web-scale image-text data is a powerful tool for computer vision. Vision-language approaches [31, 49, 80] have pushed the state-of-the-art across a variety of tasks, including zero-shot classi-ﬁcation [49], novel object detection [87], and even image generation [52]. Similar approaches for videos [4, 39, 46], however, have been limited by the small size of paired video-text corpora compared to the billion-scale image-text datasets [31, 49, 84]—even though access to raw video data has exploded in the past decade. In this work, we show it is possible to automatically generate text pairing for such videos by leveraging Large Language Models (LLMs), thus taking full advantage of the massive video data. Learning video-language models with these automatically generated annotations leads to stronger representations, and as Fig-ure 1 shows, sets a new state-of-the-art on six popular ﬁrst
*Work done during an internship at Meta.
CharadesEgo
Recognition (mAP) 36.1 32.1
EK-100
Multi-Instance
Retrieval (mAP) 50.9 45.0
EgoMCQ (intra-vid. acc.) 63.1 57.2
EGTEA
Recognition (mean acc.) 76.0 65.9 59.4 66.5
EK-100
Multi-Instance
Retrieval (nDCG) 50.5 51.0
EK-100 Recog-nition (top-1 action acc.) 54.3 82.7 61.5 88.1
HMDB-51
Recognition (linear probing mean acc.)
UCF-101
Recognition (linear probing mean acc.)
Figure 1. LAVILA sets a new state-of-the-art across a number of ﬁrst and third-person video understanding tasks (cf . Table 1 for details), by learning a video-language representation using super-vision from large language models as narrators.
LAVILA (Ours)
Previous SOTA and third-person video benchmarks.
Our method, called LAVILA:
Language-model augmented Video-Language pre-training, leverages pre-trained LLMs, e.g. GPT-2 [50], which encode within their weights a treasure trove of factual knowledge and conversational ability. As shown in Figure 2, we repurpose these LLMs to be “visually-conditioned narrators”, and
ﬁnetune on all accessible paired video-text clips. Once trained, we use the model to densely annotate thousands of hours of videos by generating rich textual descriptions.
This pseudo-supervision can thus pervade the entire video, in between and beyond the annotated snippets. Paired with another LLM trained to rephrase existing narrations,
LAVILA is able to create a much larger and more diverse set of text targets for video-text contrastive learning.
In addition to setting a new state-of-the-art as noted earlier, the stronger representation learned by LAVILA even outperforms prior work using only half the groundtruth annotations (Figure 5).
LAVILA’s strong performance can be attributed to a number of factors. First, LAVILA can provide temporally dense supervision for long-form videos, where the associ-C looks around the  open space.
C operates the phone.
Conventional 
Video-Language 
Representation Learning
Dual-Encoder Model
Dual-Encoder Model time
Human 
Narration / ASR
Large 
LM
LAVILA
NARRATOR
A lady walks past a car.
A woman converses with C.
A man walks towards a building
C walks on the pavement
C takes a selfie with the phone
Figure 2. LAVILA leverages Large Language Models (LLMs) to densely narrate long videos, and uses those narrations to train strong dual-encoder models. While prior work uses sparsely la-beled text by humans, or weakly aligned text transcribed from speech, LAVILA is able to leverage dense, diverse, and well-aligned text generated by a LLM. ated captions are either too sparse, or the video-level “Alt-Text” (in the case of web videos) does not describe all the nuanced activities happening in it. Second, the generated text is well-aligned with the visual input. Although prior work has leveraged automatic speech transcription on How-To videos [45] to automatically extract clips paired with text from the speech, such datasets have relatively poor alignment between the visual and textual content (≤ 50%, cf . [25, 45]), limiting the quality of the learned represen-tations. Third, LAVILA can signiﬁcantly expand annota-tions when only a little is available. For instance, videos of mundane day-to-day activities, especially from an egocen-tric viewpoint, could be very useful for assistive and aug-mented reality applications. Such videos, however, are rare on the internet, and hence do not readily exist with associ-ated web text. Recent work [24] instead opted to manually capture and narrate such video data. These narrations how-ever required signiﬁcant manual effort: 250K hours of an-notator time spent in narrating 3.6K hours of video. In con-trast, LAVILA is able to automatically narrate each video multiple times and far more densely, and hence learns much stronger representations.
We extensively evaluate LAVILA across multiple video-text pre-training datasets and downstream tasks to validate its effectiveness. Speciﬁcally, after being pre-trained on
Ego4D, the largest egocentric video datasets with narra-tions, LAVILA can re-narrate the whole dataset 10× over.
The resulting model learned on these expanded narrations sets a new state-of-the-art on a wide range of downstream tasks across challenging datasets, including multi-instance video retrieval on Epic-Kitchens-100 (5.9% absolute gain on mAP), multiple-choice question answering on Ego4D (5.9% absolute gain on intra-video accuracy), and action recognition on EGTEA (10.1% absolute gain on mean ac-curacy). It obtains gains both when evaluated for zero-shot transfer to the new dataset, as well as after ﬁne-tuning on that dataset. Similar gains are shown in third-person video data. When training LAVILA after densely re-narrating
HowTo100M, we outperform prior work on downstream ac-tion classiﬁcation on UCF-101 and HMDB-51. In a case study of semi-supervised learning, we show that our model, which only ever sees 50% of the human-labeled data, is ca-pable of outperforming the baseline model trained with all the narrations. Moreover, the gains progressively increase as we go to larger data regimes and larger backbones, sug-gesting the scalability of our method. 2.