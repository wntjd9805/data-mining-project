Abstract 1.

Introduction
Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth tran-sition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new do-main using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain pre-cise controllability over different levels of transformation.
The code is available at LeeDongYeun/FixNoise.
Image translation between different domains is a long-standing problem in computer vision [8, 9, 13, 20, 22, 24, 35, 52, 62]. Controllability in domain translation is impor-tant since it allows the users to set the desired properties.
Recently, several studies have shown promising results in domain translation using a pre-trained unconditional gen-erator, such as StyleGAN2 [27], and its fine-tuned version
[29, 30, 42, 48]. These studies implemented domain trans-lation by embedding an image from the source domain to the latent space of the source model and by providing the obtained latent code into the target model to generate a tar-get domain image. To preserve semantic correspondence between different domains, previous works commonly fo-cused on the hierarchical design of the unconditional gener-ator. They used several techniques like freezing [30] and swapping [42] layers or both [29].
In these approaches, users can control the degree of preserved source features by setting the number of freezing or swapping layers of the target model differently.
However, one of the notable limitations of the previous
methods is that they cannot control features across domains in a single model. Imagine morphing between two images x0 and x1. Previous methods approximated midpoints be-tween x0 and x1 by either building a new hybrid model by converting weights or training a new model. In these ap-proaches, each intermediate point is drawn from the output distribution of different models, which would produce in-consistent results. Moreover, getting an additional model for each intermediate point (image) also increases the com-putational cost. Another common limitation of these layer-based methods is that their control levels are discrete and restricted to the number of layers, which prevents fine-grain control.
In this paper, we introduce a new training strategy,
FixNoise, for cross-domain controllable domain translation.
To control features across domains in a single model, we argue that the source features should be preserved but dis-entangled with the target in the modelâ€™s inherited space. To this end, we focus on the fact that the noise input of Style-GAN2, which is added after each convolution, expands the functional space composed of the latent code expression. In other words, the feature space could be seen as a set of sub-spaces corresponding to each random noise. To preserve the source features only to a particular subset of the fea-ture space of the target model, we fix the noise input when applying a simple feature matching loss. The disentangled feature space allows our method to fine-grain control the preserved source features only in a single model without limited control steps through linear interpolation between the fixed and random noise. The extensive experiments demonstrate that our approach can generate more consistent and realistic results than existing methods on cross-domain feature control and also show better performance on domain translation qualitatively and quantitatively. 2.