Abstract 1.

Introduction
We present Instant Volumetric Head Avatars (INSTA), a novel approach for reconstructing photo-realistic digi-tal avatars instantaneously. INSTA models a dynamic neu-ral radiance field based on neural graphics primitives em-bedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that ob-serves the subject under different expressions and views.
While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In ad-dition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that IN-STA extrapolates to unseen poses. In quantitative and quali-tative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. Project website: https://zielon.github.io/insta/
For immersive telepresence in AR or VR, we aim for digital humans (avatars) that mimic the motions and facial expressions of the actual subjects participating in a meet-ing. Besides the motion, these avatars should reflect the human’s shape and appearance. Instead of prerecorded, old avatars, we aim to instantaneously reconstruct the subject’s look to capture the actual appearance during a meeting. To this end, we propose Instant Volumetric Head Avatars (IN-STA), which enables the reconstruction of an avatar within a few minutes (∼10 min) and can be driven at interactive frame rates. For easy accessibility, we rely on commodity hardware to train and capture the avatar. Specifically, we use a single RGB camera to record the input video. State-of-the-art methods that use similar input data to reconstruct a human avatar require a relatively long time to train, rang-ing from around one day [20] to almost a week [16,58]. Our approach uses dynamic neural radiance fields [16] based on neural graphics primitives [38], which are embedded around a parametric face model [25], allowing low training times and fast evaluation. In contrast to existing methods, we use a metrical face reconstruction [59] to ensure that the avatar
has metrical dimensions such that it can be viewed in an
AR/VR scenario where objects of known size are present.
We employ a canonical space where the dynamic neural radiance field is constructed. Leveraging the motion esti-mation employing the parametric face model FLAME [25], we establish a deformation field around the surface using a bounding volume hierarchy (BVH) [12]. Using this defor-mation field, we map points from the deformed space into the canonical space, where we evaluate the neural radiance field. As the surface deformation of the FLAME model does not include details like wrinkles or the mouth interior, we condition the neural radiance field by the facial expression parameters. To improve the extrapolation to novel views, we further leverage the FLAME-based face reconstruction to provide a geometric prior in terms of rendered depth maps during training of the NeRF [36]. In comparison to state-of-the-art methods like NeRFace [16], IMAvatar [58], or Neural Head Avatars (NHA) [20], our method achieves a higher rendering quality while being significantly faster to train and evaluate. We quantify this improvement in a series of experiments, including an ablation study on our method.
In summary, we present Instant Volumetric Head Avatars with the following contributions:
• a surface-embedded dynamic neural radiance field based on neural graphics primitives, which allows us to reconstruct metrical avatars in a few minutes instead of hours or days,
• and a 3DMM-driven geometry regularization of the dynamic density field to improve pose extrapolation, an important aspect of AR/VR applications. 2.