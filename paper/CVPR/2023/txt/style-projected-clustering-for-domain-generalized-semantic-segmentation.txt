Abstract
Existing semantic segmentation methods improve gen-eralization capability, by regularizing various images to a canonical feature space. While this process contributes to generalization, it weakens the representation inevitably. In contrast to existing methods, we instead utilize the differ-ence between images to build a better representation space, where the distinct style features are extracted and stored as the bases of representation. Then, the generalization to unseen image styles is achieved by projecting features to this known space. Specifically, we realize the style projec-tion as a weighted combination of stored bases, where the similarity distances are adopted as the weighting factors.
Based on the same concept, we extend this process to the decision part of model and promote the generalization of semantic prediction. By measuring the similarity distances to semantic bases (i.e., prototypes), we replace the common deterministic prediction with semantic clustering. Compre-hensive experiments demonstrate the advantage of proposed method to the state of the art, up to 3.6% mIoU improve-ment in average on unseen scenarios. Code and models are available at https://gitee.com/mindspore/ models/tree/master/research/cv/SPC-Net. 1.

Introduction
Domain generalization methods aim to promote the per-formance of model (trained on source datasets), when ap-plying it to unseen scenarios (target domains) [9, 19, 29, 36, 62, 74, 75]. Recently, domain generalization for semantic segmentation (DGSS) has attracted increasingly more at-tention due to the rise of safety-critical applications, such as autonomous driving [3, 12, 22, 45].
Existing DGSS methods improve the pixel-wise gen-eralization performance by learning domain-agnostic rep-*This work was done during W. Huang’s internship at Noah’s Ark Lab.
†Corresponding author
Figure 1. Illustration of instance normalization/whitening (IN/IW)
[5, 20, 40] and our proposed style projected clustering method.
IN and IW regularize image features from different domains to a canonical space (a-c). Our method builds style and semantic rep-resentation spaces based on the data from known domains (d). resentations [5, 16, 20, 25, 40, 42, 66, 72]. Researches in this line share the similar goal in general, that is to cap-ture the domain-invariant characteristics of object contents, and eliminates the domain-specific ones (i.e., image styles).
As two representatives, Instance Normalization (IN) [56] and Instance Whitening (IW) [17] regularize image fea-tures from different domains to a canonical space, as illus-trated in Fig. 1(a) and 1(b). Specifically, IN achieves center-level feature alignment via channel-wise feature normaliza-tion [33,40], and IW realizes uniform feature distribution by removing linear correlation between channels [5,41]. More-over, the combination of these two methods is proposed in [42] for a better generalization, as shown in Fig. 1(c).
Nevertheless, feature regularization inevitably weakens the representation capability, as a part of feature informa-tion is eliminated. Theoretically, it works under a strong assumption that the eliminated information is strictly the domain-specific ones. Yet in practice, the perfect disen-tanglement between image style and content is difficult to
achieve. It means that a part of content features will also be eliminated in the process of feature regularization, and thus degrades the segmentation performance.
Instead of seeking common ground by feature regular-ization, we aim to address DGSS in a different way. In this paper, we propose style projection as an alternative, which utilizes the features from different domains as bases to build a better representation space, as shown in Fig. 1(d). The motivation of style projection comes from a basic concept of generalization, that is to represent unseen data based on the known ones. Specifically, following the common practice, we adopt the statistics (i.e., mean and variance) of features in channel dimension to represent image styles. The im-age styles from source domains are iteratively extracted and stored as the bases of representation. Then, we project the style of given unseen images into this representation space to promote generalization. This projection process is im-plemented as a weighted combination of stored style bases, where the similarity distance between styles are adopted as the weighting factors, i.e., λ1 and λ2 shown in Fig. 1(d).
Based on the projected style features, we further devise the decision part of model, which is elaborated for semantic segmentation. Typically, existing methods learn a paramet-ric function to map pixel-wise features to semantic predic-tions. We replace this deterministic prediction with seman-tic clustering, where the class of each pixel is predicted by the minimal similarity distance to semantic bases, as shown in Fig. 1(d). Notably, it follows the same concept of style projection, that is to predict unseen data based on the known ones. More concretely, to facilitate the performance of se-mantic clustering, we propose a variant of contrastive loss to align the semantic bases of same classes and enhance dis-criminability between different classes.
We conduct comprehensive experiments on single- and multi-source settings to demonstrate the superior general-ization of our method over existing DGSS methods. In ad-dition, we visually analyze the effective representation of our proposed method for unseen images in both style and semantic spaces.
Contributions of this paper are summarized as follows:
• Beyond existing feature regularization methods, we propose style projected clustering, pointing out a new avenue to address DGSS.
• We propose style projection, which projects unseen styles into the style representation space built on known domains for a better representation.
• We propose semantic clustering to predict the class of each pixel in unseen images by the similarity distance to semantic bases, which further improves the general-ization capability for unseen domains.
• Our proposed method outperforms the current state of the arts on multiple DGSS benchmarks. 2.