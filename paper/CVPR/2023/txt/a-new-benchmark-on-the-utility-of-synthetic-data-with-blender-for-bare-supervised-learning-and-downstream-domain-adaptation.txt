Abstract
Deep learning in computer vision has achieved great success with the price of large-scale labeled training data.
However, exhaustive data annotation is impracticable for each task of all domains of interest, due to high labor costs and unguaranteed labeling accuracy. Besides, the uncon-trollable data collection process produces non-IID training and test data, where undesired duplication may exist. All these nuisances may hinder the verification of typical theo-ries and exposure to new findings. To circumvent them, an alternative is to generate synthetic data via 3D rendering with domain randomization. We in this work push forward along this line by doing profound and extensive research on bare supervised learning and downstream domain adapta-tion. Specifically, under the well-controlled, IID data set-ting enabled by 3D rendering, we systematically verify the typical, important learning insights, e.g., shortcut learning, and discover the new laws of various data regimes and net-work architectures in generalization. We further investigate the effect of image formation factors on generalization, e.g., object scale, material texture, illumination, camera view-point, and background in a 3D scene. Moreover, we use the simulation-to-reality adaptation as a downstream task for comparing the transferability between synthetic and real data when used for pre-training, which demonstrates that synthetic data pre-training is also promising to improve real test results. Lastly, to promote future research, we develop a new large-scale synthetic-to-real benchmark for image classification, termed S2RDA, which provides more signifi-cant challenges for transfer from simulation to reality. 1.

Introduction
Recently, we have witnessed considerable advances in various computer vision applications [16, 29, 38]. However, such a success is vulnerable and expensive in that it has been limited to supervised learning methods with abundant la-*Corresponding author. beled data. Some publicly available datasets exist certainly, which include a great mass of real-world images and ac-quire their labels via crowdsourcing generally. For example,
ImageNet-1K [9] is of 1.28M images; MetaShift [26] has 2.56M natural images. Nevertheless, data collection and annotation for all tasks of domains of interest are imprac-tical since many of them require exhaustive manual efforts and valuable domain expertise, e.g., self-driving and medi-cal diagnosis. What’s worse, the label given by humans has no guarantee to be correct, resulting in unpredictable label noise. Besides, the poor-controlled data collection process produces a lot of nuisances, e.g., training and test data aren’t independent identically distributed (IID) and even have du-plicate images. All of these shortcomings could prevent the validation of typical insights and exposure to new findings.
To remedy them, one can resort to synthetic data gen-eration via 3D rendering [10], where an arbitrary number of images can be produced with diverse values of imaging factors randomly chosen in a reasonable range, i.e., domain randomization [48]; such a dataset creation pipeline is thus very lucrative, where data with labels come for free. For image classification, Peng et al. [32] propose the first large-scale synthetic-to-real benchmark for visual domain adap-tation [30], VisDA-2017; it includes 152K synthetic images and 55K natural ones. Ros et al. [37] produce 9K synthetic cityscape images for cross-domain semantic segmentation.
Hinterstoisser et al. [19] densely render a set of 64 retail ob-jects for retail detection. All these datasets are customized for specific tasks in cross-domain transfer. In this work, we push forward along this line extensively and profoundly.
The deep models tend to find simple, unintended solu-tions and learn shortcut features less related to the seman-tics of particular object classes, due to systematic biases, as revealed in [14]. For example, a model basing its prediction on context would misclassify an airplane floating on water as a boat. The seminal work [14] emphasizes that short-cut opportunities are present in most data and rarely dis-appear by simply adding more data. Modifying the training data to block specific shortcuts may be a promising solution, e.g., making image variation factors consistently distributed
across all categories. To empirically verify the insight, we propose to compare the traditional fixed-dataset periodic training strategy with a new strategy of training with undu-plicated examples generated by 3D rendering, under the well-controlled, IID data setting. We run experiments on three representative network architectures of ResNet [18],
ViT [12], and MLP-Mixer [49], which consistently show obvious advantages of the data-unrepeatable training (cf.
Sec. 4.1). This also naturally validates the typical argu-ments of probably approximately correct (PAC) generaliza-tion [41] and variance-bias trade-off [11]. Thanks to the ideal IID data condition enabled by the well-controlled 3D rendering, we can also discover more reliable laws of var-ious data regimes and network architectures in generaliza-tion. Some interesting observations are as follows.
• Do not learn shortcuts! The test results on synthetic data without background are good enough to show that the synthetically trained models do not learn shortcut solutions relying on context clues [14].
• A zero-sum game. For the data-unrepeatable train-ing, IID and OOD (Out-of-Distribution [34, 55]) gen-eralizations are some type of zero-sum game w.r.t. the strength of data augmentation.
• Data augmentations do not help ViT much! In IID tests, ViT performs surprisingly poorly whatever the data augmentation is and even the triple number of training epochs does not improve much.
• There is always a bottleneck from synthetic data to
OOD/real data. Here, increasing data size and model capacity brings no more benefits, and domain adapta-tion [56] to bridge the distribution gap is indispensable except for evolving the image generation pipeline to synthesize more realistic images.
Furthermore, we comprehensively assess image varia-tion factors, e.g., object scale, material texture, illumina-tion, camera viewpoint, and background in a 3D scene. We then find that to generalize well, deep neural networks must learn to ignore non-semantic variability, which may appear in the test. To this end, sufficient images with different val-ues of one imaging factor should be generated to learn a robust, unbiased model, proving the necessity of sample di-versity for generalization [42, 48, 55]. We also observe that different factors and even their different values have uneven importance to IID generalization, implying that the under-explored weighted rendering [3] is worth studying.
Bare supervised learning on synthetic data results in poor performance in OOD/real tests, and pre-training and then domain adaptation can improve. Domain adaptation (DA)
[56] is a hot research area, which aims to make predictions for unlabeled instances in the target domain by transferring knowledge from the labeled source domain. To our knowl-edge, there is little research on pre-training for DA [24] (with real data). We thus use the popular simulation-to-real classification adaptation [32] as a downstream task, study the transferability of synthetic data pre-trained models by comparing with those pre-trained on real data like ImageNet and MetaShift. We report results for several representative
DA methods [7, 13, 40, 45, 47] on the commonly used back-bone, and our experiments yield some surprising findings.
• The importance of pre-training for DA. DA fails without pre-training (cf. Sec. 4.3.1).
• Effects of different pre-training schemes. Different
DA methods exhibit different relative advantages un-der different pre-training data. The reliability of exist-ing DA method evaluation criteria is unguaranteed.
• Synthetic data pre-training vs. real data pre-training. Synthetic data pre-training is better than pre-training on real data in our study.
• Implications for pre-training data setting. Big Syn-thesis Small Real is worth researching. Pre-train with target classes first under limited computing resources.
• The improved generalization of DA models. Real data pre-training with extra non-target classes, fine-grained target subclasses, or our synthesized data added for target classes helps DA.
Last but not least, we introduce a new, large-scale synthetic-to-real benchmark for classification adaptation (S2RDA), which has two challenging tasks S2RDA-49 and
S2RDA-MS-39. S2RDA contains more categories, more re-alistically synthesized source domain data coming for free, and more complicated target domain data collected from di-verse real-world sources, setting a more practical and chal-lenging benchmark for future DA research. 2.