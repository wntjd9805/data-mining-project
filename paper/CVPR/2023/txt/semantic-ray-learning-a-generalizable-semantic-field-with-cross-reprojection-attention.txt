Abstract
In this paper, we aim to learn a semantic radiance field from multiple scenes that is accurate, efficient and gener-alizable. While most existing NeRFs target at the tasks of neural scene rendering, image synthesis and multi-view re-construction, there are a few attempts such as Semantic-†Corresponding author.
NeRF that explore to learn high-level semantic understand-ing with the NeRF structure. However, Semantic-NeRF si-multaneously learns color and semantic label from a single ray with multiple heads, where the single ray fails to pro-vide rich semantic information. As a result, Semantic NeRF relies on positional encoding and needs to train one specific model for each scene. To address this, we propose Semantic
Ray (S-Ray) to fully exploit semantic information along the ray direction from its multi-view reprojections. As directly
performing dense attention over multi-view reprojected rays would suffer from heavy computational cost, we design a Cross-Reprojection Attention module with consecutive intra-view radial and cross-view sparse attentions, which decomposes contextual information along reprojected rays and cross multiple views and then collects dense connec-tions by stacking the modules. Experiments show that our
S-Ray is able to learn from multiple scenes, and it presents strong generalization ability to adapt to unseen scenes.
Project page: https://liuff19.github.io/S-Ray/. 1.

Introduction
Recently, Neural Radiance Field (NeRF) [32], a new novel view synthesis method with implicit representation, has taken the field of computer vision by storm [11]. NeRF and its variants [2, 32, 57, 59] adopt multi-layer perceptrons (MLPs) to learn continuous 3D representations and utilize multi-view images to render unseen views with fine-grained details. NeRF has shown state-of-the-art visual quality, pro-duced impressive demonstrations, and inspired many subse-quent works [4, 19, 20, 53, 57].
While the conventional NeRFs have achieved great suc-cess in low- and middle-level vision tasks such as neural scene rendering, image synthesis, and multi-view recon-struction [4, 12, 27, 35, 36, 42, 50], it is interesting to explore their more possibilities in high-level vision tasks and ap-plications. Learning high-level semantic information from 3D scenes is a fundamental task of computer vision with a wide range of applications [10, 13, 15, 33]. For example, a comprehensive semantic understanding of scenes enables intelligent agents to plan context-sensitive actions in their environments. One notable attempt to learn interpretable se-mantic understanding with the NeRF structure is Semantic-NeRF [61], which regresses a 3D-point semantic class to-gether with radiance and density. Semantic-NeRF shows the potential of NeRF in various high-level tasks, such as scene-labeling and novel semantic view synthesis.
However, Semantic-NeRF follows the vanilla NeRF by estimating the semantic label from a single ray with a new semantic head. While this operation is reasonable to learn low-level information including color and density, a single ray fails to provide rich semantic patterns – we can tell the color from observing a single pixel, but not its semantic la-bel. To deal with this, Semantic-NeRF heavily relies on po-sitional encoding to learn semantic features, which is prone to overfit the current scene and only applicable to novel views within the same scene [51]. As a result, Semantic-NeRF has to train one model from scratch for every scene independently or provides very limited novel scene general-ization by utilizing other pretrained models to infer 2D seg-mentation maps as training signals for unseen scenes. This significantly limits the range of applications in real-world scenarios.
In this paper, we propose a neural semantic represen-tation called Semantic Ray (S-Ray) to build a generaliz-able semantic field, which is able to learn from multiple scenes and directly infer semantics on novel viewpoints across novel scenes as shown in Figure 1. As each view provides specific high-level information for each ray re-garding of viewpoints, occlusions, etc., we design a Cross-Reprojection Attention module in S-Ray to fully exploit semantic information from the reprojections on multiple views, so that the learned semantic features have stronger discriminative power and generalization ability. While di-rectly performing dense attention over the sampled points on each reprojected ray of multiple views would suffer from heavy computational costs, we decompose the dense atten-tion into intra-view radial and cross-view sparse attentions to learn comprehensive relations in an efficient manner.
More specifically, for each query point in a novel view, different from Semantic-NeRF that directly estimates its se-mantic label with MLP, we reproject it to multiple known views.
It is worth noting that since the emitted ray from the query point is virtual, we cannot obtain the exact repro-jected point on each view, but a reprojected ray that shows possible positions. Therefore, our network is required to si-multaneously model the uncertainty of reprojection within each view, and comprehensively exploit semantic context from multiple views with their respective significance. To this end, our Cross-Reprojection Attention consists of an intra-view radial attention module that learns the relations among sampled points from the query ray, and a cross-view sparse attention module that distinguishes the same point in different viewpoints and scores the semantic contribution of each view. As a result, our S-Ray is aware of the scene prior with rich patterns and generalizes well to novel scenes. We evaluate our method quantitatively and qualitatively on syn-thetic scenes from the Replica dataset [46] and real-world scenes from the ScanNet dataset [7]. Experiments show that our S-Ray successfully learns from multiple scenes and generalizes to unseen scenes. By following Semantic-NeRF [61], we design competitive baselines based on the recent MVSNeRF [4] and NeuRay [27] architectures for generalizable semantic field learning. Our S-Ray signifi-cantly outperforms these baselines which demonstrates the effectiveness of our cross-reprojection attention module. 2.