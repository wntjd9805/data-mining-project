Abstract
In-context learning, as a new paradigm in NLP, allows the model to rapidly adapt to various tasks with only a hand-ful of prompts and examples. But in computer vision, the difficulties for in-context learning lie in that tasks vary sig-nificantly in the output representations, thus it is unclear how to define the general-purpose task prompts that the vi-*Equal contribution. Correspondence to xinlong.wang96@gmail.com.
This work is done when Wen Wang is an intern at BAAI. sion model can understand and transfer to out-of-domain tasks. In this work, we present Painter, a generalist model which addresses these obstacles with an “image”-centric solution, that is, to redefine the output of core vision tasks as images, and specify task prompts as also images. With this idea, our training process is extremely simple, which performs standard masked image modeling on the stitch of input and output image pairs. This makes the model capable of performing tasks conditioned on visible image patches.
Thus, during inference, we can adopt a pair of input and
output images from the same task as the input condition, to indicate which task to perform. Without bells and whistles, our generalist Painter can achieve competitive performance compared to well-established task-specific models, on seven representative vision tasks ranging from high-level visual understanding to low-level image processing. In addition,
Painter significantly outperforms recent generalist models on several challenging tasks. 1.

Introduction
Training one generalist model that can execute diverse tasks simultaneously, and even can perform a new task given a prompt and very few examples, is one important step closer to artificial general intelligence. In NLP, the emergence of in-context learning [2, 7, 18] presents a new path in this direction, which uses language sequences as the general interface and allows the model to rapidly adapt to various language-centric tasks with only a handful of prompts and examples.
Thus far, in computer vision, in-context learning is rarely explored and remains unclear how to achieve that. This can be attributed to the differences between two modalities. One difference is that NLP tasks mainly consist of language un-derstanding and generation, so their output spaces can be unified as sequences of discrete language tokens. But vision tasks are the abstractions of raw visual input to varied gran-ularity and angles. Thus, vision tasks vary significantly in output representations, leading to various task-specific loss functions and architecture designs. The second difference is that the output space of NLP tasks is even the same as the input. Thus, the task instruction and the example’s input/out-put, which are all language-token sequences, can be directly used as the input condition (also denoted as the task prompt), which can be processed straightforwardly by the large lan-guage model. However, in computer vision, it is unclear how to define general-purpose task prompts or instructions that the vision model can understand and transfer to out-of-domain tasks. Several recent attempts [6, 9, 10, 27, 35, 41] tackle these difficulties by following the solutions in NLP.
They more-or-less convert vision problems into NLP ones via discretizing the continuous output spaces of vision tasks, and using the language or specially-designed discrete tokens as the task prompts.
However, we believe that images speak in images, i.e., image itself is a natural interface for general-purpose visual perception. In this work, we address the above obstacles with a vision-centric solution. The core observation is that most dense-prediction vision problems can be formulated as image inpainting, i.e.:
Given an input image, prediction is to inpaint the desired but missing output “image”.
Thus, we need a representation of 3-channel tensor that appears as an “image” for the output of the vision tasks, and specify the task prompts using a pair of images. Here we showcase several representative vision tasks for train-ing, including depth estimation, human keypoint detection, semantic segmentation, instance segmentation, image de-noising, image deraining, and image enhancement, and unify their output spaces using a 3-channel tensor, a.k.a. “out-put image”. We carefully design the data format for each task, such as instance mask and per-pixel discrete labels of panoptic segmentation, per-pixel continuous values of depth estimation, and high-precision coordinates of pose estimation. Including more tasks is very straightforward, as we only need to construct new data pairs and add them to the training set, without modifications to either the model architecture or loss function.
Based on this unification, we train a generalist Painter model with an extremely simple training process. During training, we stitch two images from the same task into a larger image, and so do their corresponding output images.
Then we apply masked image modeling (MIM) on pixels of the output image, with the input image being the condition.
With such a learning process, we enable the model to per-form tasks conditioned on visible image patches, that is, the capability of in-context prediction with the visual signal as context.
Thus the trained model is capable of the in-context infer-ence. That is, we directly use the input/output paired images from the same task as the input condition to indicate which task to perform. Examples of in-context inference are illus-trated in Figure 1, consisting of seven in-domain examples (seven rows at top) and three out-of-domain examples (three rows at bottom). This definition of task prompts does not require deep understanding of language instructions as need by almost all previous approaches, and makes it very flexible for performing both in-domain and out-of-domain vision tasks.
Without bells and whistles, our model can achieve com-petitive performance compared to well-established task-specific models, on several fundamental vision tasks across high-level visual understanding to low-level image process-ing, namely, depth estimation on NYUv2 [37], semantic segmentation on ADE-20K [54], human keypoint detection on COCO [31], panoptic segmentation on COCO [31], and three low-level image restoration tasks. Notably, on depth estimation of NYUv2, our model achieves state-of-the-art performance, outperforming previous best results by large margins which have heavy and specialized designs on archi-tectures and loss functions. Compared to other generalist models, Painter yields significant improvements on several challenging tasks.
2.