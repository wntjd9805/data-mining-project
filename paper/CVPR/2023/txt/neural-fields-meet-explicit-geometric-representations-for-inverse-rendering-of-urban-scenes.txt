Abstract 1.

Introduction
Reconstruction and intrinsic decomposition of scenes from captured imagery would enable many applications such as relighting and virtual object insertion. Recent NeRF based methods achieve impressive ﬁdelity of 3D reconstruc-tion, but bake the lighting and shadows into the radiance
ﬁeld, while mesh-based methods that facilitate intrinsic de-composition through differentiable rendering have not yet scaled to the complexity and scale of outdoor scenes. We present a novel inverse rendering framework for large urban scenes capable of jointly reconstructing the scene geometry, spatially-varying materials, and HDR lighting from a set of posed RGB images with optional depth. Speciﬁcally, we use a neural ﬁeld to account for the primary rays, and use an explicit mesh (reconstructed from the underlying neural
ﬁeld) for modeling secondary rays that produce higher-order lighting effects such as cast shadows. By faithfully disentan-gling complex geometry and materials from lighting effects, our method enables photorealistic relighting with specular and shadow effects on several outdoor datasets. Moreover, it supports physics-based scene manipulations such as virtual object insertion with ray-traced shadow casting.
Reconstructing high ﬁdelity 3D scenes from captured im-agery is an important utility of scaleable 3D content creation.
However, for the reconstructed environments to serve as “dig-ital twins” for downstream applications such as augmented reality and gaming, we require that these environments are compatible with modern graphics pipeline and can be ren-dered with user-speciﬁed lighting. This means that we not only need to reconstruct 3D geometry and texture but also recover the intrinsic properties of the scene such as material properties and lighting information. This is an ill-posed, challenging problem oftentimes referred to as inverse render-ing [1].
Neural radiance ﬁelds (NeRFs) [34] have recently emerged as a powerful neural reconstruction approach that enables photo-realistic novel-view synthesis. NeRFs can be reconstructed from a set of posed camera images in a matter of minutes [14, 35, 43] and have been shown to scale to room-level scenes and beyond [45, 48, 55], making them an attractive representation for augmented/virtual reality and generation of digital twins. However, in NeRF, the intrinsic properties of the scene are not separated from the effect of in-cident light. As a result, novel views can only be synthesised
under ﬁxed lighting conditions present in the input images, i.e. a NeRF cannot be relighted [42]. tion that require spatially-varying lighting to cast shadows in a physically correct way.
While NeRF can be extended into a full inverse rendering formulation [3], this requires computing the volume render-ing integral when tracing multiple ray bounces. This quickly becomes intractable due to the underlying volumetric rep-resentation. Speciﬁcally, in order to estimate the secondary rays, the volumetric density ﬁeld of NeRF would have to be queried along the path from each surface point to all the light sources, scaling with O(nm) per point, where n de-notes the number of samples along each ray and m is the number of light sources or Monte Carlo (MC) samples in the case of global illumination. To restrict the incurred computa-tional cost, prior works have mostly focused on the single object setting and often assume a single (known) illumina-tion source [42]. Additionally, they forgo the volumetric rendering of secondary rays and instead approximate the direct/indirect lighting through a visibility MLP [42, 64].
In contrast to NeRF, the explicit mesh-based representa-tion allows for very efﬁcient rendering. With a known mesh topology, the estimation of both primary and secondary rays is carried out using ray-mesh intersection (O(m)) queries that can be efﬁciently computed using highly-optimized li-braries such as OptiX [39]. However, inverse rendering methods based on explicit mesh representations either as-sume a ﬁxed mesh topology [13], or recover the surface mesh via an SDF deﬁned on a volumetric grid [36] and are thus bounded by the grid resolution. Insofar, these methods have been shown to produce high-quality results only for the smaller, object-centric scenes.
In this work, we combine the advantages of the neural
ﬁeld (NeRF) and explicit (mesh) representations and pro-pose FEGR1, a new hybrid-rendering pipeline for inverse rendering of large urban scenes. Speciﬁcally, we represent the intrinsic properties of the scene using a neural ﬁeld and estimate the primary rays (G-buffer) with volumetric render-ing. To model the secondary rays that produce higher-order lighting effects such as specular highlights and cast shad-ows, we convert the neural ﬁeld to an explicit representa-tion and preform physics-based rendering. The underlying neural ﬁeld enables us to represent high-resolution details, while ray tracing secondary rays using the explicit mesh reduces the computational complexity. The proposed hybrid-rendering is fully differentiable an can be embedded into an optimization scheme that allows us to estimate 3D spatially-varying material properties, geometry, and HDR lighting of the scene from a set of posed camera images2. By modeling the HDR properties of the scene, our representation is also well suited for AR applications such as virtual object inser-1Abbreviation FEGR is derived from neural Fields meet Explicit
Geometric Representations and is pronounced as "ﬁgure". 2We can also integrate depth information, if available, to further con-strain the solution space.
We summarize our contributions as follows:
• We propose a novel neural ﬁeld representation that decom-poses scene into geometry, spatially varying materials, and HDR lighting.
• To achieve efﬁcient ray-tracing within a neural scene rep-resentation, we introduce a hybrid renderer that renders primary rays through volumetric rendering, and models the secondary rays using physics-based rendering. This en-ables high-quality inverse rendering of large urban scenes.
• We model the HDR lighting and material properties of the scene, making our representation well suited for down-stream applications such as relighting and virtual object insertion with cast shadows.
FEGR signiﬁcantly outperforms state-of-the-art in terms of novel-view synthesis under varying lighting conditions on the NeRF-OSR dataset [40]. We also show qualitative results on a single-illumination capture of an urban environment, collected by an autonomous vehicle. Moreover, we show the intrinsic rendering results, and showcase virtual object insertion as an application. Finally, we conduct a user study, in which the results of our method are signiﬁcantly preferred to those of the baselines. 2.