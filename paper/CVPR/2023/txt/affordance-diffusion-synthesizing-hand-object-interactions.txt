Abstract 1.

Introduction
Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned gen-eration for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (i.e., an articulated hand) with a given object. Given an
RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two-step generative approach: a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a
ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system al-lows us to predict descriptive affordance information, such as hand articulation and approaching orientation.
*Yufei was an intern at NVIDIA during the project.
Consider the bottles, bowls and cups shown in the left column of Figure 1. How might a human hand interact with such objects? Not only is it easy to imagine, from a single image, the types of interactions that might occur (e.g., ‘grab/hold’), and the interaction locations that might happen (e.g. ‘handle/body’), but it is also quite natural to hallucinate—in vivid detail— several ways in which a hand might contact and use the objects. This ability to predict and hallucinate hand-object-interactions (HOI) is critical to functional understanding of a scene, as well as to visual im-itation and manipulation.
Can current computer vision algorithms do the same? On the one hand, there has been a lot of progress in image gen-eration, such as synthesizing realistic high-resolution im-ages spanning a wide range of object categories [43, 73] from human faces to ImageNet classes. Newer diffusion models such as Dall-E 2 [65] and Stable Diffusion [66] can generate remarkably novel images in diverse styles. In fact, highly-realistic HOI images can be synthesized from simple text inputs such as “a hand holding a cup” [65, 66].
On the other hand, however, such models fail when con-ditioned on an image of a particular object instance. Given
an image of an object, it remains an extremely challeng-ing problem to generate realistic human object interaction.
Solving this problem requires (at least implicitly) an under-standing of physical constraints such as collision and force stability, as well as modeling the semantics and functional-ity of objects — the underlying affordances [19]. For ex-ample, the hand should prefer to grab the kettle handle but avoid grabbing the knife blade. Furthermore, in order to produce visually plausible results, it also requires modeling occlusions between hands and objects, their scale, lighting, texture, etc.
In this work, we propose a method for interaction syn-thesis that addresses these issues using diffusion models. In contrast to a generic image-conditioned diffusion model, we build upon the classic idea of disentangling where to inter-act (layout) from how to interact (content) [25,30]. Our key insight is that diverse interactions largely arise from hand-object layout, whereas hand articulations are driven by lo-cal object geometry. For example, a mug can be grasped by either its handle or body, but once the grasping location is determined, the placement of the fingers depends on the object’s local surface and the articulation will exhibit only subtle differences. We operationalize this idea by proposing a two-step stochastic procedure: 1) a LayoutNet that gener-ates 2D spatial arrangements of hands and objects, and 2) a ContentNet that is conditioned on the query object im-age and the sampled HOI layout to synthesize the images of hand-object interactions. These two modules are both implemented as image-conditioned diffusion models.
We evaluate our method on HOI4D and EPIC-KITCHEN [11, 48]. Our method outperforms generic im-age generation baselines, and the extracted hand poses from our HOI synthesis are favored in user studies against base-lines that are trained to directly predict hand poses. We also demonstrate surprisingly robust generalization ability across datasets, and we show that our model can quickly adapt to new hand-object-interactions with only a few ex-amples. Lastly, we show that our proposed method enables editing and guided generation from partially specified lay-out parameters. This allows us to reuse heatmap prediction from prior work [13, 56] and to generate consistent hand sizes for different objects in one scene.
Our main contributions are summarized below: 1) we propose a two-step method to synthesize hand-object in-teractions from an object image, which allows affordance information extracted from it; 2) we use inpainting tech-inuqes to supervise the model with paired real-world HOI and object-only images and propose a novel data augmenta-tion method to alleviate overfit to artifacts; and 3) we show that our approach generates realistic HOI images along with plausible 3D poses and generalizes surprisingly well on out-of-distribution scenes. 4) We also highlight several applica-tions that would benefit from such a method. 2.