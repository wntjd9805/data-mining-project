Abstract
Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream ap-proaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of
Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel
WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1)
We introduce the softmax function into GradCAM and ex-ploit the zero-shot ability of CLIP to suppress the confu-sion caused by non-target classes and backgrounds. Mean-while, to take full advantage of CLIP, we re-explore text in-puts under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we pro-pose a real-time class-aware attention-based affinity (CAA) module based on the inherent multi-head self-attention (MHSA) in CLIP-ViTs. 3) When training the final segmenta-tion model with the masks generated by CLIP, we introduced a confidence-guided loss (CGL) focus on confident regions.
Our CLIP-ES achieves SOTA performance on Pascal VOC 2012 and MS COCO 2014 while only taking 10% time of previous methods for the pseudo mask generation. Code is available at https://github.com/linyq2117/CLIP-ES. 1.

Introduction
Semantic segmentation [7,40] aims to predict pixel-level labels but requires labor-intensive pixel-level annotations.
*Equal contribution.
†Corresponding author.
Figure 1. Effect of the softmax function on GradCAM of CLIP.
The original GradCAM uses the logit (before the softmax) of the target class to compute gradient. We propose to compute gradient based on the probability (after the softmax). It can avoid confusion between the target class and background (the first two columns) and other object classes in the dataset (the last two columns).
Weakly supervised semantic segmentation (WSSS) is pro-posed to reduce the annotation cost. WSSS only requires weak supervision, e.g., image-level labels [2], bounding boxes [10, 33], points [4] or scribbles [31, 42]. The most commonly used one is WSSS with image-level annotations, which is the focus of our paper.
Previous WSSS approaches [24, 43, 46, 48] with image-level labels typically follow a three-stage framework. First, a classification model is trained on the specific dataset to generate initial CAMs (Class Activation Maps). Then, the initial CAMs are refined by the pixel affinity network [1, 2] or extra saliency maps [18, 41]. At last, the refined CAMs serve as the pseudo masks to train a semantic segmentation model. Obviously, this multi-stage framework is compli-cated as it needs to train multiple models at different stages, especially the separate classification model and affinity net-work in the first two stages. Although some end-to-end methods [3, 50] are proposed to improve efficiency, they tend to achieve poor performance compared to multi-stage methods. Therefore, it is a challenge to simplify the proce-dure of WSSS while maintaining its high performance.
Recently, the Contrastive Language-Image Pre-training (CLIP) [34], a model pre-trained on 400 million image-text pairs from the Internet to predict if an image and a text snip-pet are matched, has shown great success in the zero-shot classification. This dataset-agnostic model could transfer to unseen datasets directly. Besides, the powerful text-to-image generation ability of CLIP, i.e., DALL-E2 [35], in-dicates the strong relation between texts and corresponding components in the image. On the other hand, multi-head self-attention (MHSA) in ViT [12] reflects semantic affinity among patches and has the potential to substitute for affin-ity network. Motivated by these, we believe CLIP with ViT architecture could simplify the procedure of WSSS and lo-calize categories in the image through well-designed texts.
This paper proposes a new framework, CLIP-ES, to im-prove each stage in terms of efficiency and accuracy for
WSSS. In the first stage, the generated CAMs are usually redundant and incomplete. Most methods [43,45] are based on binary cross-entropy for multi-label classification. The loss is not mutually exclusive, so the generated CAMs suf-fer from confusion between foreground and non-target fore-ground categories, e.g., person and cow, or foreground and background categories, e.g., boat and water, as shown in
Fig. 1. The incompleteness stems from the gap between the classification and localization tasks, causing CAMs only focus on discriminative regions. To solve the confusion problems above, we introduce the softmax function into
GradCAM to make categories mutually exclusive and de-fine a background set to realize class-related background suppression. To get more complete CAMs and fully en-joy the merits inherited from CLIP, we investigate the ef-fect of text inputs in the setting of WSSS and design two task-specific text-driven strategies: sharpness-based prompt selection and synonym fusion.
In the second stage, instead of training an affinity net-work as in previous works, we leverage the attention ob-tained from the vision transformer. However, the attention map is class-agnostic, while the CAM is class-wise. To bridge this gap, we propose a class-aware attention-based affinity (CAA) module to refine the initial CAMs in real-time, which can be integrated into the first stage. Without fine-tuning CLIP on downstream datasets, our method re-tains CLIP’s generalization ability and is flexible to gener-ate pseudo labels for new classes and new datasets.
In the last stage, the pseudo masks from the refined
CAMs are viewed as ground truth to train a segmentation model in a fully supervised manner. However, the pseudo mask may be noisy and directly applied to training may mis-lead the optimization process. We proposed a confidence-guided loss (CGL) for training the final segmentation model by ignoring the noise in pseudo masks.
Our contributions are summarized as follows:
• We propose a simple yet effective framework for
WSSS based on frozen CLIP. We reveal that given only image-level labels, CLIP can perform remarkable semantic segmentation without further training. Our method can induce this potential of localizing objects that exists in CLIP.
• We introduce the softmax function into GradCAM and design a class-related background set to overcome cat-egory confusion problems. To get better CAMs, some text-driven strategies inherited from CLIP are explored and specially redesigned for WSSS.
• We present a class-aware attention-based affinity mod-ule (CAA) to refine the initial CAMs in real time, and introduce confidence-guided loss (CGL) to miti-gate the noise in pseudo masks when training the final segmentation model.
• Experiment results demonstrate that our framework can achieve SOTA performance and is 10x efficient than other methods when generating pseudo masks. 2.