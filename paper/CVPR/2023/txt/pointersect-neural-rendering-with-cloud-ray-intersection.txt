Abstract
We propose a novel method that renders point clouds as if they are surfaces. The proposed method is differentiable and requires no scene-specific optimization. This unique capabil-ity enables, out-of-the-box, surface normal estimation, ren-dering room-scale point clouds, inverse rendering, and ray tracing with global illumination. Unlike existing work that focuses on converting point clouds to other representations— e.g., surfaces or implicit functions—our key idea is to directly infer the intersection of a light ray with the underlying sur-face represented by the given point cloud. Specifically, we train a set transformer that, given a small number of local neighbor points along a light ray, provides the intersection point, the surface normal, and the material blending weights, which are used to render the outcome of this light ray. Lo-calizing the problem into small neighborhoods enables us to train a model with only 48 meshes and apply it to un-seen point clouds. Our model achieves higher estimation accuracy than state-of-the-art surface reconstruction and point-cloud rendering methods on three test sets. When ap-plied to room-scale point clouds, without any scene-specific optimization, the model achieves competitive quality with the state-of-the-art novel-view rendering methods. Moreover, we demonstrate ability to render and manipulate Lidar-scanned point clouds such as lighting control and object insertion. 1.

Introduction
Point clouds are abundant. They are samples of surfaces, and can be captured by sensors such as Lidar, continuous-wave time-of-flight, and stereo camera setups. Point-cloud representation provides a straightforward connection to the location of the surfaces in space, and thus is an intuitive primitive to represent geometry [15].
Despite being ubiquitous, a core limitation of point clouds is that they are non-trivial to render. Each point in the point cloud occupies no volume—one cannot render them into images as is. Therefore, existing methods either as-*Work done at Apple. Corresponding author: jenhao chang@apple.com (a) Illustration of the proposed method (b) Rendered results of a sphere represented by points
Figure 1. We propose pointersect, a novel method to perform cloud-ray intersection. (a) Instead of projecting points onto the sensor, suffering from holes, we trace rays from the sensor and estimate the intersection point p between a ray and the underlying surface represented by the points. We additionally estimate the surface normal ⃗n, and the convex combination weights of points near the ray to blend material or color, wj. (b) The capability to perform cloud-ray intersection enables us to render point clouds with the standard ray tracing method, i.e., path tracing. The result shows the effect of global illumination, e.g., the cast shadow and reflection. sign each point a volume-occupying shape, e.g., an ori-ented disk [36, 53], a sphere [28], or turn it into other shape representations like meshes [27] or implicit func-tions [14, 18, 22, 30, 35]. However, it is difficult to de-termine the ideal shape, e.g., the radius of spheres or disks, for rendering. Small shapes would cause holes, while large shapes would cause blobby renderings, producing artifacts in the rasterized images. While the artifacts can be alleviated
by finetuning the rasterized images with an additional neural rendering step, the operation often requires per-scene train-ing [3, 9, 28]. On the other hand, transforming point clouds into other shape representations complicates the pipeline and prevents gradients passing back to the point cloud through the new shape representation (in the case of inverse ren-dering). For example, turning point clouds into a mesh or a Signed Distance Function (SDF) [18, 30] would require any changes on the point cloud to trigger retraining of these representations, which would clearly be prohibitive.
Recent works [34, 51] raise new ideas to directly perform ray-casting on point clouds. For each scene, these methods first learn a feature embedding for each point; then they aggregate features near each camera ray to predict colors.
However, these methods require per-scene training since the feature embedding is scene-dependent. In our case, we aim for a solution that does not require scene-specific optimiza-tion and can be applied to any scene.
In this work, we propose pointersect, an alternative that can directly ray-trace point clouds by allowing one to use point clouds as surface primitives, as shown in Figure 1.
That is, we propose to train a neural network that provides the surface intersection point, surface normal, and material blending weights—the necessary information to render (or ray trace) a surface—given a point cloud and a query ray.
Implementing this idea requires paying attention to de-tails. A core observation is that the problem to find the intersecting surface point is SE(3) equivariant—any rigid transform on the input (i.e., the point cloud and the query ray) should result in the rigid transformation of the output (i.e., the intersection point and the surface normal). Naively training a neural network would require the network to learn this equivariance, which is non-trivial [5, 13, 45]. Instead, we opt to remove the need for learning this equivariance by canonicalizing the input according to the queried light ray.
In addition, pointersect should be invariant to the order in which the points are provided—we thus utilize a transformer to learn the set function.
It is important to note that finding intersection points be-tween rays and surfaces is a highly atomic and localized problem which can be solved only with local information.
Thus, we design our method to only consider nearby points, where the surface would have been, and how the surface texture and normal can be derived from these nearby points.
By constraining the input to be a small number (∼100) of neighboring points associated to a query ray, our method can be trained on only a handful of meshes, then be applied to unseen point clouds. As our experiments show, while only trained on 48 meshes, pointersect significantly improves the
Poisson surface reconstruction, a scene-specific optimiza-tion method, on three test datasets. We also demonstrate the generality and differentiability of pointersect on various ap-plications: novel-view synthesis on room-scale point clouds, inverse rendering, and ray tracing with global illumination.
Finally, we render room-scale Lidar-scanned point clouds and showcase the capability to directly render edited scenes, without any scene-specific optimization.
In short, our contributions are:
• We propose pointersect, a neural network performing the cloud-ray intersection operation. Pointersect is easy to train, and once learned, can be applied to unseen point clouds—we evaluate the same model on three test datasets.
• We demonstrate various applications with pointersect, in-cluding room-scale point cloud rendering, relighting, in-verse rendering, and ray tracing.
• We apply pointersect on Lidar-scanned point clouds and demonstrate novel-view synthesis and scene editing.
We encourage the readers to examine results and videos of novel-view rendering, relighting, and scene editing in the supplemental material and website (https://machinelearning. apple.com/research/pointersect). 2.