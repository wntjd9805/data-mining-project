Abstract
This paper shows that it is possible to learn models for monocular 3D reconstruction of articulated objects (e.g. horses, cows, sheep), using as few as 50-150 images la-beled with 2D keypoints. Our proposed approach involves training category-specific keypoint estimators, generating 2D keypoint pseudo-labels on unlabeled web images, and using both the labeled and self-labeled sets to train 3D re-construction models. It is based on two key insights: (1) 2D keypoint estimation networks trained on as few as 50-150 images of a given object category generalize well and generate reliable pseudo-labels; (2) a data selection mech-anism can automatically create a “curated” subset of the unlabeled web images that can be used for training – we evaluate four data selection methods. Coupling these two insights enables us to train models that effectively utilize web images, resulting in improved 3D reconstruction per-formance for several articulated object categories beyond the fully-supervised baseline. Our approach can quickly bootstrap a model and requires only a few images labeled with 2D keypoints. This requirement can be easily satisfied for any new object category. To showcase the practicality of our approach for predicting the 3D shape of arbitrary object categories, we annotate 2D keypoints on 250 giraffe and bear images from COCO in just 2.5 hours per category. 1.

Introduction
Predicting the 3D shape of an articulated object from a single image is a challenging task due to its under-constrained nature. Various successful approaches [14, 19] have been developed for inferring the 3D shape of humans.
These approaches rely on strong supervision from 3D joint locations acquired using motion capture systems. Similar breakthroughs for other categories of articulated objects, such as animals, remain elusive. This is primarily due to the scarcity of appropriate training data. Some works (such as
CMR [15]) learn to predict 3D shapes using only 2D labels for supervision. However, for most object categories even
Figure 1. Overview of the proposed framework.
It includes: (a) training a category-specific keypoint estimator with a limited labeled set S, (b) generating keypoints pseudo-labels on web im-ages, (c) automatic curation of web images to create a subset U ′, and (d) training a model for 3D shape prediction with images from
S and U ′. 2D labels are limited or non-existent. We ask: how can we learn models that predict the 3D shape of articulated ob-jects in-the-wild when limited or no annotated images are available for a given object category?
In this paper we propose an approach that requires as few as 50-150 images labeled with 2D keypoints. This labeled set can be easily and quickly created for any object category.
Our proposed approach is illustrated in Figure 1 and sum-marized as follows: (a) train a category-specific keypoint estimation network using a small set S of images labeled with 2D keypoints; (b) generate 2D keypoint pseudo-labels on a large unlabeled set U consisting of automatically ac-quired web images; (c) automatically curate U by creating a subset of images and pseudo-labels U ′ according to a se-lection criterion; (d) train a model for 3D shape prediction with data from both S and U ′.
A key insight is that current 2D keypoint estimators
[30, 34, 38] are accurate enough to create robust 2D key-point detections on unlabeled data, even when trained with a limited number of images. Another insight is that images from U increase the variability of several factors, such as
Figure 2. Given a small set S of images labeled with 2D keypoints, we train a 2D keypoint estimation netowrk hϕ and generate keypoint pseudo-labels on web images (set U). We select a subset of U to train a 3D shape predictor fθ. Two methods for data selection can be seen here: (a) CF-CM: an auxiliary 2D keypoint estimator gψ generates predictions on U and images with the smallest discrepancy between the keypoint estimates of hϕ and gψ are selected (criterion (c)); (b) CF-CM2: fθ is trained with samples from S and generates predictions on U. Images with the smallest discrepancy between the keypoint estimates of hϕ and fθ are selected (criterion (d)) to retrain fθ. camera viewpoints, articulations and image backgrounds, that are important for training generalizable models for 3D shape prediction. However, the automatically acquired web images contain a high proportion of low-quality images with wrong or heavy truncated objects. Naively using all web images and pseudo-labels during training leads to de-graded performance as can be seen in our experiments in
Section 4.2. While successful pseudo-label (PL) selection techniques [4, 5, 23, 29, 40] exist for various tasks, they do not address the challenges in our setting. These works investigate PL selection when the unlabeled images come from curated datasets (e.g. CIFAR-10 [20], Cityscapes [6]), while in our setting they come from the web. In addition, they eventually use all unlabeled images during training while in our case most of the web images should be dis-carded. To effectively utilize images from the web we inves-tigate four criteria to automatically create a “curated” sub-set that includes images with high-quality pseudo-labels.
These contain a confidence-based criterion as well as three consistency-based ones (see Figure 2 for two examples).
Through extensive experiments on five different articu-lated object categories (horse, cow, sheep, giraffe, bear) and three public datasets, we demonstrate that training with the proposed data selection approaches leads in considerably better 3D reconstructions compared to the fully-supervised baseline. Using all pseudo-labels leads to degraded perfor-mance. We analyze the performance of the data selection methods used and conclude that consistency-based selec-tion criteria are more effective in our setting. Finally, we conduct experiments with varying number of images in the labeled set S. We show that even with only 50 annotated instances and images from web, we can train models that lead to better 3D reconstructions than the fully-supervised models trained with more labels. 2.