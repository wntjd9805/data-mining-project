Abstract important
Predicting human gaze is in Human-Computer Interaction (HCI). However, to practically serve
HCI applications, gaze prediction models must be scalable, fast, and accurate in their spatial and temporal gaze pre-dictions. Recent scanpath prediction models focus on goal-directed attention (search). Such models are limited in their application due to a common approach relying on trained target detectors for all possible objects, and the availabil-ity of human gaze data for their training (both not scal-able). In response, we pose a new task called ZeroGaze, a new variant of zero-shot learning where gaze is pre-dicted for never-before-searched objects, and we develop a novel model, Gazeformer, to solve the ZeroGaze prob-lem.
In contrast to existing methods using object detec-tor modules, Gazeformer encodes the target using a natu-ral language model, thus leveraging semantic similarities in scanpath prediction. We use a transformer-based encoder-decoder architecture because transformers are particularly useful for generating contextual representations. Gaze-former surpasses other models by a large margin (19%– 70%) on the ZeroGaze setting. It also outperforms exist-ing target-detection models on standard gaze prediction for both target-present and target-absent search tasks. In ad-dition to its improved performance, Gazeformer is more than five times faster than the state-of-the-art target-present visual search model. Code can be found at https:// github.com/cvlab-stonybrook/Gazeformer/ 1.

Introduction
The prediction of human gaze behavior is a computer vi-sion problem with clear application to the design of more interactive systems that can anticipate a user’s attention.
In addition to addressing basic cognitive science questions, gaze modeling has applications in human-computer inter-action (HCI) and Augmented/Virtual Reality (AR/VR) sys-tems [2,12,24], non-invasive healthcare [35,43], visual dis-play design [18, 40], robotics [17, 25], education [3, 11, 39],
Figure 1. An example scenario where human scanpaths for only N categories such as “fork”, “cup” and “laptop” are available during training. In the traditional GazeTrain setting, the model is asked to predict scanpaths for only those N target categories it has seen during training, such as “fork”. However, in the ZeroGaze setting, the model is expected to predict scanpaths for target categories for which training scanpaths are not available, such as “knife”.
Gazeformer is overall superior since it is more scalable (better
ZeroGaze performance), more effective (better GazeTrain perfor-mance) and faster (higher inference speed) than previous methods. etc. It is likely that gaze modeling will be ubiquitous in all
HCI interfaces in the near future. Hence, there is a need for gaze prediction models that are reliable and effective yet efficient and capable of fast inference for use with edge devices (e.g., smartglasses, AR/VR headsets).
Our focus is on goal-directed behavior (not free view-ing), and specifically the prediction of gaze fixations as a person searches for a given target-object category (e.g., a clock or fork). Visual search is engaged in innumerable task-oriented human activities (e.g., driving) in both real and AR/VR environments. The gaze prediction problem for visual search takes an image input and outputs a sequence of eye movements conditioned on the target. Existing ar-chitectures for search fixation prediction use either panoptic segmentation maps [45] or object detection modules [8, 46] to encode specific search targets. However, the applicability of these methods is essentially limited to the relatively small number of target objects for which panoptic segmentation or object detector models can be realistically trained. For ex-ample, to predict the fixations in search for “pizza cutter”, which is an object category not included in the dataset used to train the backbone detector in [8], a new “pizza cutter” detector would need to be trained, meaning this and related methods do not scale beyond their training data. Relatedly, existing approaches [8, 45, 46] have required the laborious collection of large-scale datasets of search behavior for each target category (∼ 3000 scanpaths per category) included in the dataset, an approach that is unscalable to the innumer-able potential targets that humans search for in the wild.
We therefore introduce a new problem that we refer to as ZeroGaze, which is an extension of zero-shot learning to the gaze prediction problem. Under ZeroGaze, a model must predict the fixations that a person makes while search-ing for a target (e.g., a fork) despite the unavailability of search fixations (e.g., a person looking for a fork) for model training. A more challenging version of the problem is when there are no trained detectors for the target (e.g., a fork detector) either. This is in contrast to the traditional
“GazeTrain” setting (where the model has been trained on gaze behavior for all the categories that it encounters dur-ing inference). To address the ZeroGaze problem, we pro-pose Gazeformer, a novel multimodal model for scanpath prediction. Gazeformer is scalable because it does not re-quire training a backbone detector on the fixation behavior of people searching for specific object categories. We use a linguistic embedding of the target object from a language model (RoBERTa [31]) and a transformer encoder-decoder architecture [42] to model the joint image-target represen-tations and the spatio-temporal context embedded within it.
Given that recent language models can encode any object using any text string, they provide a powerful and scal-able solution to the representation of target categories for which no gaze data is available for training. Additionally,
Gazeformer decodes the fixation time-steps in parallel us-ing a transformer decoder, providing significantly faster in-ference than any other method, which is necessary for gaze tracking applications in HCI products (especially in wear-able edge devices). We show the advantages of our model in Fig. 1. The specific contributions of this study are: 1. We introduce the ZeroGaze task, where a model must predict the search fixation scanpath for a new target category without training on prior knowledge of the gaze behavior to that target. 2. We devise a novel multimodal transformer-based ar-chitecture called Gazeformer, which learns the interac-tion between the image input and the language-based semantic features of the target. 3. We propose a novel and effective scheme of fixation modeling that uses a set of Gaussian distributions in continuous image space rather than learning multino-mial probability distributions over patches, resulting in an intuitive distance-based objective function. 4. We achieve a new state-of-the-art in search scanpath prediction. It outperforms baselines, not only in our new ZeroGaze setting, but also in the traditional set-ting where models are trained on the gaze behavior (GazeTrain) and in target-absent search. Gazeformer also generalizes to unknown categories while being up to more than 5 times faster than previous methods. 2.