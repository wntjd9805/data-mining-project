Abstract
Implicit neural representations store videos as neural networks and have performed well for various vision tasks such as video compression and denoising. With frame in-dex or positional index as input, implicit representations (NeRV, E-NeRV, etc.) reconstruct video frames from fixed and content-agnostic embeddings. Such embedding largely limits the regression capacity and internal generalization for video interpolation.
In this paper, we propose a Hy-brid Neural Representation for Videos (HNeRV), where a learnable encoder generates content-adaptive embeddings, which act as the decoder input. Besides the input em-bedding, we introduce HNeRV blocks, which ensure model parameters are evenly distributed across the entire net-work, such that higher layers (layers near the output) can have more capacity to store high-resolution content and video details. With content-adaptive embeddings and re-designed architecture, HNeRV outperforms implicit meth-ods in video regression tasks for both reconstruction qual-ity (+4.7 PSNR) and convergence speed (16× faster), and shows better internal generalization. As a simple and effi-cient video representation, HNeRV also shows decoding ad-vantages for speed, flexibility, and deployment, compared to traditional codecs (H.264, H.265) and learning-based compression methods. Finally, we explore the effectiveness of HNeRV on downstream tasks such as video compression and video inpainting. 1.

Introduction
Given the massive amount of videos generated every day, storing and transferring them efficiently is a key task in computer vision and video processing. Even for modern storage systems, the space requirements of raw video data can be overwhelming. Despite storage becoming cheaper, network speeds and I/O processing remain a bottleneck and make transferring and processing videos expensive.
Traditional video codecs, such as H.264 [47] and
Figure 1. Top: hybrid neural representation with learnable and content-adaptive embedding (ours). Bottom: video regression for hybrid and implicit neural representations.
HEVC [41], rely on a manually-designed encoder and de-coder based on discrete cosine transform [3]. With the suc-cess of deep learning, many attempts [2, 7, 11, 23, 24, 27, 36, 37, 49] have been made to replace certain components of existing compression pipelines with neural networks.
Although these learning-based compression methods show high potential in terms of rate-distortion performance, they suffer from complex pipelines and expensive computation, not just to train, but also to encode and decode.
To address the complex pipelines and heavy computa-tion, implicit neural representations [6, 33, 35, 38, 40] have become popular due to their simplicity, compactness, and efficiency. These methods show great potential for visual data compression, such as COIN [8] for image compres-sion, and NeRV [4] for video compression. By representing videos as neural networks, video compression problems can
be converted to model compression problems, which greatly simplifies the encoding and decoding pipeline.
Implicit representation methods for video compression present a major trade-off: they embrace simplicity at the expense of generalizability. Given a frame index t as in-put, NeRV [4] uses a fixed position encoding function and a learnable decoder to reconstruct video frames from temporal embeddings. Another implicit representation, E-NeRV [22], takes a temporal embedding and spatial embed-ding to reconstruct video frames. Since the embeddings of
NeRV and E-NeRV are based on spatial and/or temporal in-formation only; without connection to the actual content of frames, they are content-agnostic. For decoding, NeRV-like models compute these embeddings using frame index alone, without access to the original frame. This is quite elegant for video compression, since instead of storing many frame embeddings, one would only need to store model weights and basic metadata (e.g., number of frames).
However, this comes with some major disadvantages.
Firstly, since embeddings are content-agnostic, and due to how the temporal embeddings are computed, there is no way to meaningfully interpolate between frames. Secondly, and more importantly, the positional embedding used by the fully-implicit models provides no visual prior and limits the regression capacity, since all the information needs to be learned by and stored in the video decoder.
In this paper, we propose a learnable encoder as a key component of hybrid neural representation for videos (HN-eRV, Figure 1 (top). Our proposed neural representation is a hybrid between implicit (network-centric) and explicit (embedding-centric) approaches since it stores videos in two parts: the tiny content-adaptive frame embeddings and a learned neural decoder. Besides the issue of content-agnostic embedding, prior work such as NeRV also suf-fers from an imbalance in the distribution of model parame-ters. In these decoders, later layers (closer to the output im-age) have much fewer parameters than earlier layers (closer to the embedding). This hinders NeRV’s ability to effec-tively reconstruct massive video content while preserving frame details. To rectify this, we introduce the HNeRV block, which increases kernel sizes and channel widths at later stages. With HNeRV blocks, we can build video de-coders with parameters that are more evenly distributed over the entire network. As a hybrid method, HNeRV improves reconstruction quality for video regression and boosts the convergence speed by up to 16× compared to implicit meth-ods, shown in Figure 1 (bottom). With content-adaptive em-beddings, HNeRV also shows much better internal general-ization (ability to encode and decode frames from the video that were not seen during training), and we verify this by frame interpolation results in Section 4.2.
HNeRV only requires a network forward operation for video decoding, which offers great advantages over tradi-tional codecs and prior deep learning approaches in terms of speed, flexibility, and ease of deployment. Additionally, most other video compression methods are auto-regressive and there is a high dependency on the sequential order of video frames. In contrast, there is no dependency on the sequential order of frames for HNeRV, which means it can randomly access frames efficiently to decode frames in par-allel. Such simplicity and parallelism make HNeRV a good codec for further speedups, like a special neural processing unit (NPU) chip, or parallel decoding with huge batches.
HNeRV is still viable for video compression, while also showing promising performance for video restoration tasks.
We design our encoder such that it can also be compressed; additionally, our HNeRV decoder blocks perform well in the model compression regime, such that HNeRV is com-petitive with state-of-the-art methods. We posit that neural representation can be robust to distortion in pixel space and therefore restore videos which have undergone distortions.
We verify this observation on the video inpainting task.
In summary, we propose a hybrid neural representa-tion for videos. With content-adaptive embedding and re-designed architecture, HNeRV shows much better video re-gression performance over implicit methods, in reconstruc-tion quality (+4.7 PSNR), convergence speed (16× faster), and internal generalization. As an efficient video codec,
HNeRV is easy to deploy, and is simple, fast, and flexible during video decoding. Finally, HNeRV shows good perfor-mance over downstream tasks like video compression and video inpainting. 2.