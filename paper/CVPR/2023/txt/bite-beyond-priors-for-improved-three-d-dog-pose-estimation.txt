Abstract
We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D train-ing data, this problem is challenging, and the best meth-ods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from mul-tiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side infor-mation. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We ex-ploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers sig-nificantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de/. 1.

Introduction
Capturing and modeling 3D animal shape and pose has many applications, ranging from biology and conservation to entertainment and virtual content creation. Cameras are a natural sensor to observe animals because they do not re-quire the animal to stand still, hold specific postures, be in physical contact, or otherwise cooperate. The study of an-imals using images has a long tradition, e.g. Muybridge’s famous “Horse in Motion” chronophotographs [30]. Still, expressive 3D models that can adapt to the individual shape and pose of an animal have only recently been cre-ated [44], following earlier work on 3D human shape and pose [1, 23, 38]. Here, we address the task of reconstruct-ing dogs in 3D from a single image. We focus on dogs as a representative animal species that features both large shape variability across different breeds and strong articulated de-formations typical of quadrupeds. Dogs are frequently pho-tographed, so images showing a wide range of poses, shapes and environments are readily available.
While, at first glance, modeling humans and modeling dogs may seem like similar problems, they present very dif-ferent technical challenges. For humans there is an enor-mous amount of existing 3D scan and motion capture data.
This data covers relevant pose and shape variations, which has made it possible to learn powerful, articulated models like SMPL [23] or GHUM [38].1 In contrast, 3D observa-tions of animals are difficult to acquire, and at present too scarce to train 3D statistical models that are equally expres-sive and cover all possible shapes and poses. The introduc-tion of SMAL [44], a parametric quadruped model learned from toy figurines, has enabled significant progress and has made it possible to reconstruct animals in 3D from images, including dogs [5, 21, 26]. However, SMAL is a generic model for multiple species from cats to hippos. While it can represent the varied body shapes of different animals, it cannot represent the distinctive and fine-grained character-istics of different dog breeds (e.g., the wide variety of ears).
To address this issue, we introduce the first dog-specific parametric model, termed D-SMAL, to accurately represent dogs.
An additional issue is that there is very little motion cap-ture data of dogs (unlike humans) and the data that does exist rarely captures sitting and lying poses. This makes it difficult for existing methods to infer dogs in such poses.
For example, if one learns a prior over 3D poses from ex-isting data, it will be biased to standing/walking poses. One could weaken this prior, using generic constraints, but then the estimation of pose is highly under-constrained. To ad-dress this problem, we exploit information about physical contact that has been neglected when modeling (land) an-imals: they are subject to gravity and therefore stand, sit or lie on the ground. We introduce ground contact infor-mation and show that we can exploit this to estimate com-plex dog poses, even in challenging cases with significant self-occlusion. While ground plane constraints have been exploited in human pose estimation, their potential bene-fit is larger for quadrupeds, simply because four legs mean more ground contact points, as well as more occluded body parts and larger non-rigid deformations when sitting or ly-ing down.
Another limitation of previous work is that the recon-struction pipelines are usually trained on 2D images, due to the difficulty of collecting 3D data (with paired 2D images).
As a result, they tend to predict shapes and poses that accu-rately match the image evidence when re-projected, but are distorted along the viewing direction. Viewed from a differ-ent angle, the 3D reconstruction may be inaccurate because, in the absence of paired data, there is not enough evidence to learn where to place more distant or even occluded body parts along the depth direction. Again, we find that mod-elling ground contact helps: to circumvent the manual re-construction (or synthesis) necessary to obtain paired 2D and 3D data, we revert to a weaker form of 3D supervision 1In fact, recent work on 3D humans deals with advanced aspects like clothing [36, 37] or multi-person scenarios [10]. and obtain ground contact labels. Specifically, we present real images to annotators and ask them to label whether the ground surface under the dog is flat and, if so, to also an-notate the ground contact points on the 3D animal. These labels are exploited not only for training: we found that the network can be trained to classify the surface and detect the contact points fairly reliably from a single image, such that they can also be used at test time.
We call our reconstruction system BITE, which we base on the current state-of-the-art model, BARC [26]. As an initial, coarse fitting stage we re-train BARC with our new
D-SMAL dog model. We then pass the resulting predictions to our newly developed refinement network, which we train with ground contact losses to refine the dog’s pose (as well as the camera parameters). Optionally, at test time, we can further exploit the ground contact loss to optimize the fit to the test image, fully automatically. This significantly im-proves reconstruction quality. With BITE, we obtain dogs that correctly stand on the (locally planar) ground, or are re-constructed realistically in sitting and lying postures, even though the training set for the BARC pose prior does not contain such poses (see Fig. 1).
Previous work on 3D dog reconstruction is evaluated by back-projecting to the image and measuring 2D residuals, thus projecting away errors in depth [5], or via subjective visual ratings [26]. To address the lack of objective 3D evaluations, we have created a novel, semi-synthetic dataset with 3D ground truth, by rendering 3D scans of real dogs from different viewing angles. We evaluate BITE as well as its main competitors on this new dataset, and show that
BITE indeed sets a new state of the art.
In summary, our contributions are: 1. We introduce D-SMAL, a novel, dog-specific 3D pose and shape model derived from SMAL. 2. We develop BITE, a neural model to refine 3D dog poses by encouraging plausible ground contact, while at the same time estimating the local ground plane. 3. We show that with that model it becomes possible to reconstruct dog poses far from those encoded in a (nec-essarily limited) prior. 4. We advance the state of the art for monocular 3D pose estimation on the challenging StanfordExtra dataset. 5. We put forward a new, semi-synthetic 3D test dataset based on scans of real dogs, with which we hope to encourage a move to true 3D evaluation. 2.