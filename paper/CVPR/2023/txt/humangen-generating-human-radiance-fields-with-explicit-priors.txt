Abstract
Recent years have witnessed the tremendous progress of 3D GANs for generating view-consistent radiance fields with photo-realism. Yet, high-quality generation of hu-man radiance fields remains challenging, partially due to the limited human-related priors adopted in existing meth-ods. We present HumanGen, a novel 3D human generation scheme with detailed geometry and 360◦ realistic free-view rendering. It explicitly marries the 3D human generation with various priors from the 2D generator and 3D recon-structor of humans through the design of “anchor image”.
We introduce a hybrid feature representation using the an-chor image to bridge the latent space of HumanGen with the existing 2D generator. We then adopt a pronged de-sign to disentangle the generation of geometry and appear-ance. With the aid of the anchor image, we adapt a 3D re-constructor for fine-grained details synthesis and propose a two-stage blending scheme to boost appearance genera-tion. Extensive experiments demonstrate our effectiveness for state-of-the-art 3D human generation regarding geome-try details, texture quality, and free-view performance. No-tably, HumanGen can also incorporate various off-the-shelf 2D latent editing methods, seamlessly lifting them into 3D. 1.

Introduction
We are entering an era where the boundaries of real and virtually generated worlds are dismissing. An epitome of this revolution is the recent rise of 3D-aware and photo-realistic image synthesis in the past several years [5, 6, 11, 16, 53, 63, 91], which combine 2D Generative Adversar-ial Networks (GANs) with neural volume rendering, like neural radiance fields (NeRFs) [43]. But such 3D GANs mainly focus on rigid contents like human/animal faces or
CAD models. The further 3D generation of us humans with photo-realism is more attractive, with numerous applica-tions in VR/AR or visual effects.
High-quality 3D human generative models should ide-ally generate 3D-aware humans with the following charac-teristics: (1) detailed geometry, (2) photo-realistic appear-Figure 1. The proposed HumanGen can generate 3D humans with fine-detailed geometry and appearance while seamlessly lifting various 2D latent editing tools into 3D. ance, and (3) even supporting 360◦ free-view rendering.
Yet, it remains extremely challenging, mainly due to the significantly higher diversity of human apparel and skele-tal pose. Only very recently, a few work explore auto-decoding [10] and 3D GANs [3, 22, 85] for human gener-ation by using the parametric human model like SMPL [39] as priors. But such parametric human prior lacks sufficient geometry details, and the adopted neural rendering in these methods does not guarantee that meaningful 3D geometry can be generated, further leading to appearance artifacts.
Besides, these 3D human generators are trained with lim-ited human datasets that lack diversity [68] or suffer from imbalanced viewing angles (most are front views) [13, 38].
In a nutshell, existing methods fail to fulfill all the afore-mentioned three characteristics for 3D human generation.
We observe that 3D human generation can benefit from more explicit priors from other research domains of human modeling, except for the SMPL prior adopted in existing methods. Specifically, with the recent large-scale dataset
SHHQ [13], the 2D human generators [29–31] achieve more decent synthesis results than the 3D ones. And var-ious downstream 2D editing tools are available by disentan-gling the latent spaces [55, 58, 65, 78]. These abilities of 2D generation and subsequent can significantly benefit the 3D human generation if their latent spaces can be bridged. Be-sides, recent advances in monocular 3D human reconstruc-tion [2, 60] have achieved more fine-grained geometry de-tails than the implicit geometry proxy in current 3D human generators. Yet, there lacks a well-designed mechanism to explicitly utilize the rich human priors from both 2D gener-ator and 2D reconstructor for 3D human generation.
In this paper, we present HumanGen – a novel neural scheme to generate high-quality radiance fields for 3D hu-mans from 2D images, as shown in Fig. 1. In stark contrast with existing methods that only use SMPL, our approach explicitly utilizes richer priors from the top-tier 2D gen-eration and 3D reconstruction schemes. As a result, our approach not only enables more realistic human genera-tion with detailed geometry and 360◦ free-view ability, but also maintains the compatibility to existing off-the-shelf 2D editing toolbox based on latent disentanglement.
Our key idea in HumanGen is to organically leverage a 2D human generator and a 3D human reconstructor as explicit priors into a 3D GAN-like framework. Specifi-cally, we first introduce a hybrid feature representation of the generative 3D space, which consists of the tri-plane fea-tures from EG3D [5] as well as a 2D photo-real human im-age (denoted as “anchor image”) generated through the pre-trained 2D generator. Note that we adopt separated Style-GAN2 [31] architectures to generate both the tri-plane fea-ture maps and the anchor image. But they share the same latent mapping network, so as to bridge and anchor the la-tent space of our 3D GAN to the pre-trained 2D human generator. Then, based on such hybrid representation, we design our 3D human generator into the pronged geometry and appearance branches. In the geometry branch, we ex-plicitly utilize a pre-trained 3D reconstructor PIFuHD [60] to extract pixel-aligned features from the anchor image and provide extra fine-grained geometry supervision for our Hu-manGen. Note that the original PIFuHD encodes geometry as an implicit occupancy field. Thus, we propose a geome-try adapting scheme to turn it into a generative version with signed distance field (SDF) output, so as to support efficient and high-resolution volume rendering with sphere tracing.
For the appearance branch, we propose to learn an appear-ance field and a blending field from both the pixel-aligned and tri-plane features. Note that [18,59] only use the pixel-aligned feature, thus we include the tri-plane features which
“sculpt” richer feature space for learning sharper texture.
Then, we adopt a two-stage blending scheme to fully use the rich texture information in the anchor image. For our GAN training procedure, we adopt similar training strategy like
EG3D [5] and introduce additional front and back consis-tency supervision to enhance the generated texture details.
Besides, we observe that existing 2D human generator
StyleGAN2 [31] trained on the large-scale SHHQ [13] can potentially generate diverse human images including side-views and even back-views. Thus, we train our HumanGan using an augmented dataset from SHHQ by using the pre-trained 2D generator to cover 360◦ viewing angles. Once trained, our HumanGen enables high-quality 3D human generation. As an additional benefit, it shares the same la-tent mapping with the 2D generated anchor image. Thus, using the anchor image, we can seamlessly upgrade off-the-shelf 2D latent editing methods into our 3D setting. We showcase various 3D effects via convenient anchor image editing. To summarize, our main contributions include:
• We present a novel 3D-aware human generation scheme, with detailed geometry and 360◦ more realis-tic free-view rendering than previous methods, achiev-ing significant superiority to state-of-the-arts.
• We propose a hybrid feature representation using an anchor image with shared latent space to bridge our 3D GAN with the existing 2D generator.
• We propose a pronged design for appearance/geometry branches, and adapt a 3D reconstructor to aid the ge-ometry branch for fine-grained details synthesis.
• We introduce an implicit blending field with two-stage blending strategy to generate high-quality appearance. 2.