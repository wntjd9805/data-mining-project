Abstract
Motivated by the increasing popularity of transformers in computer vision, in recent times there has been a rapid development of novel architectures. While in-domain per-formance follows a constant, upward trend, properties like robustness or uncertainty estimation are less explored— leaving doubts about advances in model reliability. Studies along these axes exist, but they are mainly limited to classifi-cation models. In contrast, we carry out a study on semantic segmentation, a relevant task for many real-world applica-tions where model reliability is paramount. We analyze a broad variety of models, spanning from older ResNet-based architectures to novel transformers and assess their relia-bility based on four metrics: robustness, calibration, mis-classification detection and out-of-distribution (OOD) de-tection. We find that while recent models are significantly more robust, they are not overall more reliable in terms of uncertainty estimation. We further explore methods that can come to the rescue and show that improving calibration can also help with other uncertainty metrics such as misclassi-fication or OOD detection. This is the first study on modern segmentation models focused on both robustness and uncer-tainty estimation and we hope it will help practitioners and researchers interested in this fundamental vision task1. 1.

Introduction
Humans tend to overestimate their abilities, a cogni-tive bias known as Dunning-Kruger effect [27]. Unfortu-nately, so do deep neural networks. Despite impressive per-formance on a wide range of tasks, deep learning models tend to be overconfident—that is, they predict with high-confidence even when they are wrong [19]. This effect is even more severe under domain shifts, where models tend to underperform in general [23, 40, 45].
While these vulnerabilities affect deep models in gen-eral, they are often studied for classification models and are
*https://europe.naverlabs.com 1Code available at https://github.com/naver/relis
Figure 1. Top: mIoU and ECE vs. domain shift. Errors are normalized with respect to the lowest error on the training dis-tribution (Cityscapes). We compare recent segmentation mod-els, both transformer-based (SETR [58], SegFormer [55] and Seg-menter [48]) and convolution-based (ConvNext [30]) with ResNet baselines (UPerNet [54] and DLV3+ [6]). All recent models (both transformers and CNNs) are remarkably more robust than ResNet baselines (whose lines in mIoU overlap), however, ECE increases sharply for all methods. Bottom: Sample images for each dataset. comparably less explored for semantic segmentation, a fun-damental task in computer vision that is key to many criti-cal applications such as autonomous driving and AI-assisted medical imaging. In those applications, domain shifts are more the rule than the exception (e.g., changes in weather for a self-driving car or differences across patients for a medical imaging system). Therefore, brittle performance and overconfidence under domain shifts are two important and challenging problems to address for a safe deployment of artificial intelligence systems in the real world.
With that in mind, we argue that a reliable model should i) be robust to domain shifts and ii) provide good uncer-tainty estimates. The core goal of this study is providing an answer to the following, crucial question: are state-of-the-art semantic segmentation models improving in terms of robustness and uncertainty estimation?
To shed light on this, we evaluate a large body of seg-mentation models, assessing their in-domain (ID) vs. out-of-domain (OOD) prediction quality (robustness) together with their calibration, misclassification detection and OOD detection (uncertainty estimation).
We argue that a study of this kind is crucial to under-stand whether research on semantic segmentation is moving in the right direction. Following the rise of transformer ar-chitectures in computer vision [4,15,29,50], several studies have compared recent self-attention and CNN-based classi-fication models in terms of robustness [2, 3, 30, 32, 36, 43] and predictive uncertainty [33, 44]. Yet, when it comes to semantic segmentation, prior studies [55, 59] only focused on robustness, using synthetic corruptions as domain shifts (e.g., blur, noise) [25]. In contrast, we consider natural, re-alistic domain shifts and study segmentation models both in terms of robustness and uncertainty, leveraging datasets captured in different conditions—see Fig. 1 (bottom).
Task-specific studies are important, since task-specific architectures and learning algorithms may carry different behaviors and some observations made for classification might not hold true when switching to segmentation. For instance, contrary to Minderer et al. [33], we observe that improvements in calibration are far behind those in robust-ness, see Fig. 1 (top). Furthermore, previous analyses only consider simple calibration approaches [19] while assessing model reliability; in contrast, we make a step forward and explore content-dependent calibration strategies [14, 17], which show promise to improve reliability out of domain.
Our analysis allows us individuating in which directions we are improving and in which we are lagging behind. This is the first work to systematically study robustness and un-certainty under domain shift for a large suite of segmen-tation models and we believe it can help practitioners and researchers working on semantic segmentation. We sum-marize our main observations in the following. i) Remarkable improvements in robustness, but poor in calibration. Under domain shifts, recent segmentation models perform significantly better (in terms of mIoU)— with larger improvements for stronger shifts. Yet, OOD calibration error increases dramatically for all models. ii) Content-dependent calibration [14] can improve OOD calibration, especially under strong domain shifts, where models are poorly calibrated. iii) Misclassification detection shows different model rank-ing in and out of domain. When tested in domain, recent models underperform the ResNet baseline. As the domain shift increases, recent models take the lead. iv) OOD detection is inversely correlated with perfor-mance. Indeed, a small ResNet-18 backbone performs best. v) Content-dependent calibration [14] can improve OOD detection and misclassification out of domain. We observe a significant increase in misclassification detection under strong domain shifts after improving calibration. We also observe improvements for OOD detection, albeit milder.
Sem. segm.
Robust performance
Uncertainty estimation
Natural shifts
OOD calib methods
Kamann et al. [25]
Bhojanapalli et al. [3]
Xie et al. [55]
Naseer et al. [36]
Bai et al. [2]
Minderer et al. [33]
Paul and Cheng [43]
Mao et al. [32]
Liu et al. [30]
Zhou et al. [59]
Pinto et al. [44]
Ours
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Table 1. Studies of recent architectures. While several prior works studied robustness and uncertainty of transformer- and
CNN- based classifiers, studies on segmentation limited to robust-ness. This is the first study assessing robustness and uncertainty of modern segmentation models. Moreover, we consider natural do-main shifts and are the only analysis to include content-dependent methods [14, 17] to improve calibration in OOD settings. 2.