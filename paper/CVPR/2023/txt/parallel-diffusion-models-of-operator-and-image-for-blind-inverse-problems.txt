Abstract
Diffusion model-based inverse problem solvers have demonstrated state-of-the-art performance in cases where the forward operator is known (i.e. non-blind). However, the applicability of the method to blind inverse problems
This work was supported by the National Research Foundation of Ko-rea under Grant NRF-2020R1A2B5B03001980, by the KAIST Key Re-search Institute (Interdisciplinary Research Group) Project, by the Insti-tute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2021-0-02068,
Artificial Intelligence Innovation Hub), and by the IITP grant funded by the Korea government(MSIT) (No. 2022-0-00984, Development of Arti-ficial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of Explanation). has yet to be explored.
In this work, we show that we can indeed solve a family of blind inverse problems by constructing another diffusion prior for the forward
Specifically, parallel reverse diffusion guided operator. by gradients from the intermediate stages enables joint optimization of both the forward operator parameters as well as the image, such that both are jointly estimated at the end of the parallel reverse diffusion procedure. We show the efficacy of our method on two representative tasks — blind deblurring, and imaging through turbulence — and show that our method yields state-of-the-art performance, while also being flexible to be applicable to general blind inverse problems when we know the functional forms. Code available: https://github.com/BlindDPS/blind-dps
1.

Introduction
Inverse problems subsume a wide set of important prob-lems in science and engineering, where the objective is to recover the latent image from the corrupted measurement, generated by the forward operator. Considering the taxon-omy, they can be split into two major categories — non-blind inverse problems, and blind inverse problems. The former considers the cases where the forward operator is known, and hence eases the problem. In contrast, the latter considers the cases where the operator is unknown, and thus the operator needs to be estimated together with the recon-struction of the latent image. The latter problem is consider-ably harder than the former problem, as joint minimization is typically much less stable.
In this work, we mainly focus on leveraging generative priors to solve inverse problems in imaging. Among many different generative model classes, diffusion models have established the new state-of-the-art.
In diffusion models, we define the forward data noising process, which gradually corrupts the image into white Gaussian noise. The genera-tive process is defined by the reverse of such process, where each step of reverse diffusion is governed by the score func-tion [53]. With the recent surge of diffusion models, it has been demonstrated in literature that diffusion models are not only powerful generative models, but also excellent gener-ative priors to solve inverse problems. Namely, one can ei-ther resort to iterative projections to the measurement sub-space [13, 53], or estimate posterior sampling [11] to ar-rive at feasible solutions that meet the data consistency. For both linear [13, 27, 53] and some non-linear [11, 51] inverse problems, guiding unconditional diffusion models to solve down-stream inverse problems were shown to have stronger performance even when compared to the fully supervised counterparts.
Nevertheless, current solvers are strictly limited to cases where the forward operator is known and fixed. For ex-ample, [11, 27] consider non-blind deblurring with known kernels. The problem now boils down to optimizing only for the latent image, since the likelihood can be computed robustly. Unfortunately, in real world problems, knowing the kernel exactly is impractical. It is often the case where the kernel is also unknown, and we have to jointly estimate the image and the kernel.
In such cases, not only do we need a prior model of the image, but we also need some proper prior model of the kernel [41, 55]. While conven-tional methods exploit, e.g. patch-based prior [55], sparsity prior [41], etc., they often fall short of accurate modeling of the distribution.
In this work, we aim to leverage the ability of diffusion models to act as strong generative priors and propose Blind-DPS (Blind Diffusion Posterior Sampling) — constructing multiple diffusion processes for learning the prior of each component — which enable posterior sampling even when the operator is unknown. BlindDPS starts by initializing both the image and the operator parameter with Gaussian noise. Reverse diffusion progresses in parallel for both models, where the cross-talk between the paths are enforced from the approximate likelihood and the measurement, as can be seen in Fig. 2. With our method, both the image and the kernel starts with a coarse estimation, gradually getting closer to the ground truth as t → 0 (see Fig. 1(c)).
In fact, our method can be thought of as a coarse-to-fine strategy naturally admitting a Gaussian scale-space rep-resentation [29, 36], which can be seen as a continuous generalization of the coarse-to-fine optimization strategy that most of the optimization-based methods take [41, 44].
Furthermore, our method is generally applicable to cases where we know the structure of the forward model a priori (e.g. convolution). To demonstrate the generality, we fur-ther show that our method can also be applied in imaging through turbulence. From our experiments, we show that the proposed method yields state-of-the-art performance while being generalizable to different inverse problems. 2.