Abstract
Visual place recognition (VPR) is usually considered as a specific image retrieval problem. Limited by existing training frameworks, most deep learning-based works can-not extract sufficiently stable global features from RGB im-ages and rely on a time-consuming re-ranking step to ex-ploit spatial structural information for better performance.
In this paper, we propose StructVPR, a novel training archi-tecture for VPR, to enhance structural knowledge in RGB global features and thus improve feature stability in a con-stantly changing environment. Specifically, StructVPR uses segmentation images as a more definitive source of struc-tural knowledge input into a CNN network and applies knowledge distillation to avoid online segmentation and in-ference of seg-branch in testing. Considering that not all samples contain high-quality and helpful knowledge, and some even hurt the performance of distillation, we partition samples and weigh each sample’s distillation loss to en-hance the expected knowledge precisely. Finally, StructVPR achieves impressive performance on several benchmarks using only global retrieval and even outperforms many two-stage approaches by a large margin. After adding ad-ditional re-ranking, ours achieves state-of-the-art perfor-mance while maintaining a low computational cost. 1.

Introduction
Visual place recognition (VPR) is a critical task in au-tonomous driving and robotics, and researchers usually re-gard it as an image retrieval problem [28, 30, 54]. Given a query RGB image from a robot, VPR aims to determine whether the robot has been to this place before and to iden-tify the corresponding images from a database. Extreme environmental variations are challenging to methods, espe-cially long-term changes (seasons, illumination, vegetation)
*Corresponding author.
†Supported by National Science Foundation of China (No. 62088102). (a) SEG is better than RGB (b) RGB is better than SEG
Figure 1. Examples of query images and ground truths. The marked number represents the recall performance of two pre-trained branches on ground truths. (a) shows the scene with il-lumination variation and seasonal changes, where segmentation images are more recognizable. (b) shows the scene with changing perspectives, where RGB images are more recognizable. and dynamic occlusions. Therefore, learning discriminative and robust features is essential to distinguish places.
There has been a commonly used two-stage strategy that retrieves candidates with global features and then re-ranks them through local descriptor matching, where re-ranking is time- and resource-consuming but dramatically improves the recall performance. The improvement is be-cause geometric verification based on local features pro-vides rich and explicit structural information, which has stronger robustness to VPR than appearance information in some aspects, such as shape, edge [3], spatial layout, and category [13]. Considering that segmentation (SEG) im-ages have rich structural information, we tried some empir-ical studies using RGB and SEG as network input for VPR.
We find that these two modalities have their advantages and disadvantages at the sample level, which means that better performance can be achieved if both modalities are appro-priately fused. As Figure 1 shows, both modalities have specific cases they are better at recognizing.
Based on the above discussion, we attempt to use the
SEG modality to enhance structural knowledge in global
RGB feature representation, achieving comparable perfor-mance to re-ranking while maintaining low computational
cost. Complementarity of the two modalities on samples, that is, some samples may contain harmful knowledge for
RGB, inspires us to perform knowledge enhancement selec-tively. Therefore, we propose a new knowledge distillation (KD) architecture, StructVPR, which can effectively dis-till the high-quality structural representations from the SEG modality to the RGB modality. Specifically, StructVPR uses RGB images and encoded segmentation label maps for separate pre-training with VPR loss, uses two pre-trained branches to partition samples, and then weights the distil-lation loss to selectively distill the high-quality knowledge of the pre-trained seg-branch into the final RGB network.
Note that the concept of “sample” is a sample pair with a query and a labeled positive. Compared with non-selective distillation methods [9] and previous selective distillation works [50], StructVPR can exactly mine those suitable sam-ples on which the teacher network performs good and better than the student and distinguish the importance of sample knowledge for KD. Moreover, overly refined segmentation is not helpful, and the importance of all semantic classes varies for VPR. Hence, we cluster original classes accord-ing to the sensitivity to objects in VPR and introduce prior information of labels via weighted one-hot encoding.
Our main contributions can be highlighted as follows: 1) The overall architecture avoids the computation and in-ference of segmentation during testing by distilling the high-quality knowledge from the SEG modality to the RGB modality, where segmentation images are pre-encoded into weighted one-hot label maps to extract structural infor-mation for VPR. 2) To the best of our knowledge, there is no previous work in VPR concerning selecting suitable samples for distillation. StructVPR forges a connection between sample partition with student network participat-ing and weighted knowledge distillation for each sample. 3) We perform comprehensive experiments on key bench-marks. StructVPR performs better than global methods and achieves comparable performance to many two-stage (global-local) methods. The consistent improvement in all datasets corroborates the effectiveness and robustness of StructVPR. Experimental results show that StructVPR achieves SOTA performance with low computational cost compared with global methods, and it is also competitive with most two-stage approaches [6,20]. StructVPR with re-ranking outperforms the SOTA VPR approaches (4.475% absolute increase on Recall@5 compared with the best base-line [51]). scriptors has achieved superior performances [5, 11, 24, 32].
Global descriptors can be generated by directly extracting
[8, 16, 38, 40, 58] or aggregating local descriptors. For ag-gregation, traditional methods have been incorporated into
CNN-based architectures [1, 31]. To achieve a good com-promise between accuracy and efficiency, a widely used ar-chitecture is to rank the database by global features, and then re-rank the top candidates [29, 43]. Many studies have verified its validity [42, 45, 46, 48].
Recently, many methods [35–37, 56, 57] try to intro-duce semantics into RGB features by using attention mech-anism or additional information. TransVPR [51] introduces the attention mechanism to guide models to focus on in-variant regions and extract robust representations. DAS-GIL [23] uses multi-task architecture with a single shared encoder to create global representation, and uses domain adaptation to align models on synthetic and real-world datasets. Based on [23], [34] focuses on filtering semantic information via an attention mechanism.
Knowledge Distillation.
It is an effective way to enrich models with knowledge distillation (KD) [17].
It extracts specific knowledge from a stronger model (i.e.,
“teacher”) and transfer to a weaker model (i.e., “student”) through additional training signals. There has been a large body of work on transferring knowledge with the same modality, such as model compression [4, 7, 21] and domain adaptation [2, 26]. However, the data or labels for some modalities might not be available during training or test-ing, so it is essential to distill knowledge between different modalities [39, 55]. [12, 22] generate a hallucination net-work to model depth information and enforce it for RGB descriptors learning. In this way, the student learns to simu-late a virtual depth that improves the inference performance.
In this work, we construct a weighted knowledge distilla-tion architecture to distill and enhance high-quality struc-tural knowledge into RGB features.
Nevertheless, these previous works enhance the tar-get model by transferring knowledge on each training sam-ple from the teacher model, rarely discussing the difference about knowledge among samples [14, 27]. Wang et al. [50] proposes to select suitable samples for distillation through analyzing the teacher network. Differently, our solution considers both pre-trained teacher and student network in sample partition and weight the distillation loss for samples. 3. Methodology 2.