Abstract
Model inversion (MI) attacks aim to infer and recon-struct private training data by abusing access to a model.
MI attacks have raised concerns about the leaking of sen-sitive information (e.g. private face images used in train-ing a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack perfor-mance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algo-rithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI.
In particular, our contributions are two-fold: 1) We ana-lyze the optimization objective of SOTA MI algorithms, ar-gue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts at-tack performance significantly. 2) We analyze “MI overfit-ting”, show that it would prevent reconstructed images from learning semantics of training data, and propose a novel
“model augmentation” idea to overcome this issue. Our proposed solutions are simple and improve all SOTA MI at-tack accuracy significantly. E.g., in the standard CelebA benchmark, our solutions improve accuracy by 11.8% and achieve for the first time over 90% attack accuracy. Our findings demonstrate that there is a clear risk of leak-ing sensitive information from deep learning models. We urge serious consideration to be given to the privacy im-plications. Our code, demo, and models are available at https://ngoc- nguyen- 0.github.io/re-thinking_model_inversion_attacks/. 1.

Introduction
Privacy of deep neural networks (DNNs) has attracted considerable attention recently [2, 3, 23, 31, 32]. Today,
DNNs are being applied in many domains involving pri-vate and sensitive datasets, e.g., healthcare, and security.
There is a growing concern of privacy attacks to gain knowl-edge of confidential datasets used in training DNNs. One
*Equal Contribution
†Corresponding Author important category of privacy attacks is Model Inversion (MI) [5, 8, 11, 12, 16, 36, 37, 39, 40] (Fig. 1). Given ac-cess to a model, MI attacks aim to infer and reconstruct fea-tures of the private dataset used in the training of the model.
For example, a malicious user may attack a face recognition system to reconstruct sensitive face images used in training.
Similar to previous work [5, 36, 39], we will use face recog-nition models as the running example.