Abstract
Event cameras sense the intensity changes asyn-chronously and produce event streams with high dynamic range and low latency. This has inspired research endeav-ors utilizing events to guide the challenging video super-resolution (VSR) task. In this paper, we make the first at-tempt to address a novel problem of achieving VSR at ran-dom scales by taking advantages of the high temporal res-olution property of events. This is hampered by the diffi-culties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpo-lation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specif-ically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks more explicit mo-tion information from the events near the queried times-tamp and generates the 2D features. Lastly, the Spatial-Temporal Implicit Representation (STIR) module recovers the SR frame in arbitrary resolutions from the outputs of these two modules.
In addition, we collect a real-world dataset with spatially aligned events and RGB frames. Ex-tensive experiments show that our method significantly sur-passes the prior-arts and achieves VSR with random scales, e.g., 6.5. Code and dataset are available at https:
//vlis2022.github.io/cvpr23/egvsr. 1.

Introduction
Video super-resolution (VSR) is a task of recovering high-resolution (HR) frames from successive multiple low-resolution (LR) frames. Unlike LR videos, HR videos con-tain more visual information, e.g., edge and texture, which
*These authors are co-first authors.
†These authors are co-second authors.
‡Corresponding author
Figure 1. (a) Our method learns implicit neural representations (INR) from the queried spatial-temporal coordinates (STF) and temporal features (TF) from RGB frames and events. (b) An ex-ample of VSR with random scale factors, e.g., 6.5, by our method. can be very helpful for many tasks, e.g., metaverse [52], surveillance [15] and entertainment [4]. However, VSR is a highly ill-posed problem owing to the loss of both spa-tial and temporal information, especially in the real-world scenes [5, 6, 26, 45]. Recently, deep learning-based algo-rithms have been successfully applied to learn the intra-frame correlation and temporal consistency from the LR frames to recover HR frames, e.g., DUF [19], EDVR [48],
RBPN [14], BasicVSR [5], BasicVSR++ [6]. However, due to the lack of inter-frame information, these methods are hampered by the limitations of modeling the spatial and temporal dependencies and may fail to recover HR frames in complex scenes.
Event cameras are bio-inspired sensors that can asyn-chronously detect the per-pixel intensity changes and gen-erate event streams with low latency (1us) and high dy-namic range (HDR) compared with the conventional frame-based cameras (140dB vs. 60dB) [35, 51]. This has sparked extensive research in reconstructing image/video from events [12, 29, 42, 44, 46, 53]. However, the recon-structed results are less plausible due to the loss of visual
details, e.g., structures, and textures. As a result, a recent work has utilized events for guiding VSR [18], trying to ‘in-ject’ energy from the event-based to the frame-based cam-eras. It leverages the high-frequency event data to synthe-size neighboring frames, so as to find correspondences be-tween consecutive frames. However, it only treats video frames in discrete ways with 2D arrays of pixels and up-samples them at a fixed up-scaling factor e.g., ×2 or ×4.
This causes inconvenience and inflexibility in the applica-tions of SR videos, which often require arbitrary resolu-tions, i.e., random scales.
Recently, some works tried to learn continuous image representations with arbitrary resolutions, e.g., LIIF [7], taking 2D queried coordinates and 2D features as input to learn an implicit neural representation (INR). VideoINR [8], on the other hand, decodes the LR videos into arbitrary spatial resolutions and frame rates by learning from the spatial coordinates and temporal coordinates, respectively.
However, it is still unclear how to leverage events to guide learning spatial-temporal INRs for VSR. This is hampered by two challenges. Firstly, although event data can ben-efit VSR with its high-frequency temporal and spatial in-formation, the large modality gap between the events and video frames makes it challenging to use INRs to represent the 3D spatial-temporal coordinates with event data. More-over, there lacks HR real-world dataset with spatially well-aligned events and frames.
In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking ad-vantage of the high-temporal resolution property of events.
Accordingly, we propose a novel framework that subtly in-corporates the spatial-temporal interpolation from events to
VSR in a unified framework, as shown in Fig. 1. Our key idea is to learn INRs from the queried spatial-temporal coor-dinates and features from both the RGB frames and events.
Our framework mainly includes three parts. The Spatial-Temporal Fusion (STF) branch learns the spatial-temporal information from events and RGB frames (Sec. 3.2). The shallow feature fusion and deep feature fusion are em-ployed to narrow the modality gap and fuse the events and
RGB frames into 3D global spatial-temporal representa-tions. Then, the Temporal Filter (TF) branch further un-locks more explicit motion information from events.
It learns the 2D event features from events nearing the queried timestamp (Sec. 3.3). With the features from the STF and TF branches, the Spatial-Temporal Implicit Represen-tation (STIR) module decodes the features and recovers SR frames with arbitrary spatial resolutions(Sec. 3.4). That is, given the arbitrary queried coordinates, we apply 3D sam-pling and 2D sampling to the fused 3D features and event data separately. Finally, the sampling features are added and fed into a decoder, and generate targeted SR frames. In addition, we collect a real-world dataset with a spatial reso-lution of 3264 × 2248, in which the events and RGB frames are spatially aligned. Extensive experiments on two real-world datasets show that our method surpasses the existing methods by 1.3 dB.
In summary, the main contributions of this paper are five-fold: (I) Our work serves as the first attempt to address a non-trivial problem of learning INRs from events and RGB frames for VSR at random scales. (II) We propose the STF branch and the TF branch to model the spatial and tempo-ral dependencies from events and RGB frames. (III) We propose the STIR module to reconstruct RGB frames with arbitrary spatial resolutions. (IV) We collect a high-quality real-world dataset with spatially aligned events and RGB frames. (V) Our method significantly surpasses the existing methods and achieves SR with random-scales, e.g., 6.5. 2.