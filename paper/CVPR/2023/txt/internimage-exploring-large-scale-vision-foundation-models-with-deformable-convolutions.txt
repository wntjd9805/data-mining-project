Abstract
Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale
CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and train-ing data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable con-volution as the core operator, so that our model not only has the large effective receptive field required for down-stream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternIm-age reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like
ViTs. The effectiveness of our model is proven on challeng-ing benchmarks including ImageNet, COCO, and ADE20K.
It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on
ADE20K, outperforming current leading CNNs and ViTs. 1.

Introduction
With the remarkable success of transformers in large-scale language models [3–8], vision transformers (ViTs) [2, 9–15] have also swept the computer vision field and are becoming the primary choice for the research and prac-tice of large-scale vision foundation models. Some pio-neers [16–20] have made attempts to extend ViTs to very large models with over a billion parameters, beating convo-lutional neural networks (CNNs) and significantly pushing the performance bound for a wide range of computer vision tasks, including basic classification, detection, and segmen-* equal contribution, (cid:66) corresponding author (qiaoyu@pjlab.org.cn)
Figure 1. Comparisons of different core operators. (a) shows the global aggregation of multi-head self-attention (MHSA) [1], whose computational and memory costs are expensive in down-(b) limits the stream tasks that require high-resolution inputs. range of MHSA into a local window [2] to reduce the cost. (c) is a depth-wise convolution with very large kernels to model long-range dependencies. (d) is a deformable convolution, which shares similar favorable properties with MHSA and is efficient enough for large-scale models. We start from it to build a large-scale CNN. tation. While these results suggest that CNNs are inferior to ViTs in the era of massive parameters and data, we ar-gue that CNN-based foundation models can also achieve comparable or even better performance than ViTs when equipped with similar operator-/architecture-level designs, scaling-up parameters, and massive data.
To bridge the gap between CNNs and ViTs, we first summarize their differences from two aspects: (1) From the operator level [9, 21, 22], the multi-head self-attention
(MHSA) of ViTs has long-range dependencies and adap-tive spatial aggregation (see Fig. 1(a)). Benefiting from the flexible MHSA, ViTs can learn more powerful and robust representations than CNNs from massive data. (2) From the architecture view [9, 22, 23], besides MHSA, ViTs con-tain a series of advanced components that are not included in standard CNNs, such as Layer Normalization (LN) [24], feed-forward network (FFN) [1], GELU [25], etc. Although recent works [21, 22] have made meaningful attempts to in-troduce long-range dependencies into CNNs by using dense convolutions with very large kernels (e.g., 31×31) as shown in Fig. 1 (c), there is still a considerable gap with the state-of-the-art large-scale ViTs [16, 18–20, 26] in terms of per-formance and model scale.
In this work, we concentrate on designing a CNN-based foundation model that can efficiently extend to large-scale parameters and data. Specifically, we start with a flexible convolution variant—deformable convolution (DCN) [27, 28]. By combining it with a series of tailored block-level and architecture-level designs similar to transformers, we design a brand-new convolutional backbone network, termed InternImage. As shown in Fig. 1, different from recently improved CNNs with very large kernels such as 31×31 [22], the core operator of InternImage is a dynamic sparse convolution with a common window size of 3×3, (1) whose sampling offsets are flexible to dynamically learn ap-propriate receptive fields (can be long- or short-range) from given data; (2) the sampling offsets and modulation scalars are adaptively adjusted according to the input data, which can achieve adaptive spatial aggregation like ViTs, reduc-ing the over-inductive bias of regular convolutions; and (3) the convolution window is a common 3×3, avoiding the optimization problems and expensive costs caused by large dense kernels [22, 29].
With the aforementioned designs, the proposed Intern-Image can efficiently scale to large parameter sizes and learn stronger representations from large-scale training data, achieving comparable or even better performance to large-scale ViTs [2, 11, 19] on a wide range of vision tasks.
In summary, our main contributions are as follows: (1) We present a new large-scale CNN-based founda-tion model—InternImage. To our best knowledge, it is the first CNN that effectively scales to over 1 billion parameters and 400 million training images and achieves comparable or even better performance than state-of-the-art ViTs, showing that convolutional models are also a worth-exploring direc-tion for large-scale model research. (2) We successfully scale CNNs to large-scale settings by introducing long-range dependencies and adaptive spa-tial aggregation using an improved 3×3 DCN operator, and explore the tailored basic block, stacking rules, and scaling strategies centered on the operator. These designs make ef-fective use of the operator, enabling our models to obtain
Figure 2. Performance comparison on COCO of different backbones. The proposed InternImage-H achieves a new record 65.4 box AP on COCO test-dev, significantly outperforming state-of-the-art CNNs and large-scale ViTs. the gains from large-scale parameters and data. (3) We evaluate the proposed model on representative vision tasks including image classification, object detec-tion, instance and semantic segmentation, and compared it with state-of-the-art CNNs and large-scale ViTs by scal-ing the model size ranging from 30 million to 1 billion, the data ranging from 1 million to 400 million. Specifi-cally, our model with different parameter sizes can consis-tently outperform prior arts on ImageNet [30]. InternImage-B achieves 84.9% top-1 accuracy trained only on the
ImageNet-1K dataset, outperforming CNN-based counter-parts [21, 22] by at least 1.1 points. With large-scale pa-rameters (i.e., 1 billion) and training data (i.e., 427 million), the top-1 accuracy of InternImage-H is further boosted to 89.6%, which is close to well-engineering ViTs [2, 19] and hybrid-ViTs [20]. In addition, on COCO [31], a challeng-ing downstream benchmark, our best model InternImage-H achieves state-of-the-art 65.4% box mAP with 2.18 billion parameters, 2.3 points higher than SwinV2-G [16] (65.4 vs. 63.1) with 27% fewer parameters as shown in Fig. 2. 2.