Abstract
Existing referring understanding tasks tend to involve the detection of a single text-referred object. In this paper, we propose a new and general referring understanding task, termed referring multi-object tracking (RMOT). Its core idea is to employ a language expression as a semantic cue to guide the prediction of multi-object tracking. To the best of our knowledge, it is the ﬁrst work to achieve an arbitrary number of referent object predictions in videos. To push forward RMOT, we construct one benchmark with scalable expressions based on KITTI, named Refer-KITTI. Speciﬁ-cally, it provides 18 videos with 818 expressions, and each expression in a video is annotated with an average of 10.7 objects. Further, we develop a transformer-based architec-ture TransRMOT to tackle the new task in an online manner, which achieves impressive detection performance and out-performs other counterparts. The Refer-KITTI dataset and the code are released at https://referringmot.github.io. 1.

Introduction
Recently, referring understanding [5, 17, 33, 55], inte-grating natural language processing into scene perception, has raised great attention in computer vision community.
It aims to localize regions of interest in images or videos under the instruction of human language, which has many applications, such as video editing and autonomous driving.
For referring understanding, several signiﬁcant benchmarks have been published. Flickr30k [53], ReferIt [15], and Re-fCOCO/+/g [55] have greatly encouraged the development of image-based referring tasks. More datasets (e.g., Lin-gual OTB99 [19], Cityscapes-Ref [42], Talk2Car [5], Refer-DAVIS17 [17], and Refer-Youtube-VOS [38]) are further
∗Equal contribution. †Corresponding author: Jianbing Shen. This work was supported in part by the FDCT grant SKL-IOTSC(UM)-2021-2023, the Grant MYRG-CRG2022-00013-IOTSC-ICI, and the Start-up
Research Grant (SRG) of University of Macau (SRG2022-00023-IOTSC).
‡The work is done during the internship at MEGVII Technology.
Figure 1. Representative examples from RMOT. The expression query can refer to multiple objects of interest (a), and captures the short-term status with accurate labels (b). proposed to cover the application in videos.
Despite these advanced progress, previous benchmarks have two typical limitations. First, each expression tends to correspond to only one target. However, many objects have the same semantics in an open world, i.e., one sin-gle expression could refer to multiple objects. From this side, existing datasets lack ﬂexible simulation on the multi-object scenarios, causing referring understanding tasks far from satisfactory. Second, the given expression may only describe part of frames for the video referring task, mak-ing the correspondence inaccurate. For example, given the expression ‘the car which is turning’, we have to predict the overall trajectory even if the car has ﬁnished the turn-ing action. Obviously, a single expression cannot cover all short-term status of one target. Overall, existing datasets fail to provide an accurate evaluation under the situations of multiple referent targets and temporal status variances.
To address these problems, we propose a novel video un-derstanding task guided by the language description, named referring multi-object tracking (RMOT). Given a language expression as a reference, it targets to ground all semanti-cally matched objects in a video. Unlike previous tasks, our proposed RMOT is much closer to the real environment, as each expression can involve multiple objects. For in-stance, the expression query ‘the cars in the right’ corre-sponds to one object at the 20th frame but two objects at the
Figure 2. More examples of Refer-KITTI. It provides high-diversity scenes and high-quality annotations referred to by expressions. 40th frame (see Fig. 1 (a)). The phenomenon indicates that
RMOT focuses more on ﬁnding the matched targets so that the referent number can be ﬂexibly changed. In addition, the temporal status variances are also considered in RMOT.
As shown in Fig. 1(b), the given example shows the cars can be detected only when they start the turning action, and the tracking will be ended if they ﬁnish the activity.
To speed up the development of RMOT, we construct a new benchmark, i.e., Refer-KITTI, concerning the trafﬁc scenes. It is developed from the public KITTI [9] dataset.
Compared to existing referring understanding datasets, it has three distinguishing characteristics: i) High ﬂexibility with referent objects. The number of objects described by each expression range from 0 to 105, with 10.7 on average. ii) High temporal dynamics. The temporal status of targets covers a longer time with more frames (varying in 0 400 frames), and the temporal variance of targets is accurately captured using our labeling tool. iii) Low labeling cost with identiﬁcation spread. We provide an effortless tool to anno-tate a target tracklet using only two clicks.
∼
Although RMOT has a more ﬂexible referring setting, it brings additional challenges: multi-object prediction and cross-frame association. Towards this end, we propose an end-to-end differentiable framework for RMOT. Our model builds upon the recent DETR framework [3], enhanced by powerful cross-modal reasoning and cross-frame conjunc-tion. It has an encoder-decoder architecture. Speciﬁcally, we design an early-fusion module in the encoder to densely integrate visual and linguistic features, followed by a stack of deformable attention layers for further reﬁning the cross-modal representations. In the decoder, query-based embed-dings interact with the cross-modal features to predict ref-erent boxes. To track multi-objects, similar to MOTR [57], we decouple the object queries into track query for tracking objects of previous frames and detect query for predicting the bounding boxes of new-born objects.
In summary, our contributions are three-fold. First, we propose a new task for referring multi-objects, called re-Dataset
Video Images 26,711
-RefCOCO [55] 19,992
-RefCOCO+ [55]
-26,711
RefCOCOg [55]
✓ 9,217
Talk2Car [5]
✓ 59,238
VID-Sentence [4]
Refer-DAVIS17 [17] ✓ 4,219
✓ 93,869
Refer-YV [38]
✓ 6,650
Refer-KITTI
Instances per-expression
Temporal ratio per-expression 1 1 1 1 1 1 1 10.7 1 1 1
-1 1 1 0.49
Table 1. Comparison of Refer-KITTI with existing datasets.
Refer-YV is short for Refer-Youtube-VOS. The temporal ratio rep-resents the average ratio of referent frames covering the entire video sequence. ‘-’ means unavailable. ferring multi-object tracking (RMOT). It tackles limitations in the existing referring understanding tasks and provides multi-object and temporally status-variant circumstances.
Second, we formulate a new benchmark, Refer-KITTI, to help the community to explore this new ﬁeld in depth. As far as we know, it is the ﬁrst dataset specializing in an ar-bitrary number of object predictions. Third, we propose an end-to-end framework built upon Transformer, termed as TransRMOT. With powerful cross-modal learning, it pro-vides impressive RMOT performance on Refer-KITTI com-pared to hand-crafted RMOT methods. 2.