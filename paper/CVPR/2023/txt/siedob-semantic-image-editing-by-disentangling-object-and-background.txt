Abstract
Semantic image editing provides users with a flexible tool to modify a given image guided by a correspond-In this task, the features of the ing segmentation map. foreground objects and the backgrounds are quite differ-ent. However, all previous methods handle backgrounds and objects as a whole using a monolithic model. Con-sequently, they remain limited in processing content-rich images and suffer from generating unrealistic objects and texture-inconsistent backgrounds. To address this issue, we propose a novel paradigm, Semantic Image Editing by Disentangling Object and Background (SIEDOB), the core idea of which is to explicitly leverages several hetero-geneous subnetworks for objects and backgrounds. First,
SIEDOB disassembles the edited input into background regions and instance-level objects. Then, we feed them into the dedicated generators. Finally, all synthesized parts are embedded in their original locations and utilize a fusion network to obtain a harmonized result. More-over, to produce high-quality edited images, we propose some innovative designs, including Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch Discrim-inator, and Style-Diversity Object Generator, and inte-grate them into SIEDOB. We conduct extensive experi-ments on Cityscapes and ADE20K-Room datasets and ex-hibit that our method remarkably outperforms the base-lines, especially in synthesizing realistic and diverse objects and texture-consistent backgrounds. Code is available at https://github.com/WuyangLuo/SIEDOB. 1.

Introduction
Semantic image editing has recently gained significant traction due to its diverse applications, including adding, al-tering, or removing objects and controllably inpainting, out-painting, or repainting images. Existing methods have made impressive progress benefiting from Generative Adversar-B Corresponding author
Figure 1. Existing methods struggle to deal with the compound of backgrounds and several overlapping objects in a complex scene.
They generate distorted objects and texturally inconsistent back-grounds. The proposed method can cope well with this input. ial Networks (GAN) [5, 14] and have demonstrated promis-ing results in relatively content-simple scenes, such as land-scapes. However, they still suffer from inferior results for content-rich images with multiple discrepant objects, such as cityscapes or indoor rooms. This paper aims to improve editing performance in complex real-world scenes.
When image editing experts deal with a complex task, they decompose the input image into multiple indepen-dent and disparate elements and handle them with different skills. For example, when editing a photo with a congested road, they process the cars and pedestrians individually and then fill in the background. Inspired by this spirit, we pro-pose a heterogeneous editing model, which mimics the ex-perience of human experts and explicitly splits the com-pound content into two distinct components: