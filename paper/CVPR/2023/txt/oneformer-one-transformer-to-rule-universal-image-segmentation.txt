Abstract
Universal Image Segmentation is not a new concept.
Past attempts to unify image segmentation include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architec-tures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance.
Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies seg-mentation with a multi-task train-once design. We first pro-pose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, in-stance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference.
Thirdly, we propose using a query-text contrastive loss dur-ing training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model out-performs specialized Mask2Former models across all three segmentation tasks on ADE20k, Cityscapes, and COCO, de-spite the latter being trained on each task individually. We believe OneFormer is a significant step towards making im-age segmentation more universal and accessible. 1.

Introduction
Image Segmentation is the task of grouping pixels into multiple segments. Such grouping can be semantic-based (e.g., road, sky, building), or instance-based (objects with well-defined boundaries). Earlier segmentation ap-proaches [6,19,32] tackled these two segmentation tasks in-dividually, with specialized architectures and therefore sep-arate research effort into each. In a recent effort to unify se-mantic and instance segmentation, Kirillov et al. [23] pro-posed panoptic segmentation, with pixels grouped into an
amorphous segment for amorphous background regions (la-beled “stuff”) and distinct segments for objects with well-defined shape (labeled “thing”). This effort, however, led to new specialized panoptic architectures [9] instead of unify-ing the previous tasks (see Fig. 1a). More recently, the research trend shifted towards unifying image segmenta-tion with new panoptic architectures, such as K-Net [47],
MaskFormer [11], and Mask2Former [10]. Such panop-tic/universal architectures can be trained on all three tasks and obtain high performance without changing architecture.
They do need to, however, be trained individually on each task to achieve the best performance (see Fig. 1b). The individual training policy requires extra training time and produces different sets of model weights for each task. In that regard, they can only be considered a semi-universal approach. For example, Mask2Former [10] is trained for 160K iterations on ADE20K [13] for each of the semantic, instance, and panoptic segmentation tasks to obtain the best performance for each task, yielding a total of 480k iterations in training, and three models to store and host for inference.
In an effort to truly unify image segmentation, we pro-pose a multi-task universal image segmentation framework (OneFormer), which outperforms existing state-of-the-arts on all three image segmentation tasks (see Fig. 1c), by only training once on one panoptic dataset. Through this work, we aim to answer the following questions: (i) Why are existing panoptic architectures [10,11] not suc-cessful with a single training process or model to tackle all three tasks? We hypothesize that existing methods need to train individually on each segmentation task due to the absence of task guidance in their architectures, making it challenging to learn the inter-task domain differences when trained jointly or with a single model. To tackle this chal-lenge, we introduce a task input token in the form of text:
“the task is {task}”, to condition the model on the task in focus, making our architecture task-guided for training, and task-dynamic for inference, all with a single model. We uniformly sample {task} from {panoptic, instance, semantic} and the corresponding ground truth during our joint training process to ensure our model is unbiased in terms of tasks. Motivated by the ability of panoptic [23] data to capture both semantic and instance information, we derive the semantic and instance labels from the cor-responding panoptic annotations during training. Conse-quently, we only need panoptic data during training. More-over, our joint training time, model parameters, and FLOPs are comparable to the existing methods, decreasing train-ing time and storage requirements up to 3×, making image segmentation less resource intensive and more accessible. (ii) How can the multi-task model better learn inter-task and inter-class differences during the single joint training pro-cess? Following the recent success of transformer frame-works [2,10,17,18,21,30,46] in computer vision, we formu-late our framework as a transformer-based approach, which can be guided through the use of query tokens. To add task-specific context to our model, we initialize our queries as repetitions of the task token (obtained from the task input) and compute a query-text contrastive loss [33, 43] with the text derived from the corresponding ground-truth label for the sampled task as shown in Fig. 2. We hypothesize that a contrastive loss on the queries helps guide the model to be task-sensitive and reduce category mispredictions.
We evaluate OneFormer on three major segmentation datasets: ADE20K [13], Cityscapes [12], and COCO [27], each with all three segmentation tasks. OneFormer sets the new state of the arts for all three tasks with a single jointly trained model. To summarize, our main contributions are:
• We propose OneFormer, the first transformer-based multi-task universal image segmentation framework that needs to be trained only once with a single univer-sal architecture, a single model, and on a single dataset to outperform existing frameworks across the seman-tic, instance, and panoptic segmentation tasks, despite the latter need to be trained separately on each task.
• OneFormer uses a task-conditioned joint training strat-egy, uniformly sampling different ground truth do-mains (semantic, instance, or panoptic) by deriving all GT labels from panoptic annotations to train its multi-task model. Thus, OneFormer actually achieves the orignial unification goal of panoptic segmenta-tion [23].
• We validate OneFormer through extensive experi-ments on three major benchmarks: ADE20K [13],
Cityscapes [12], and COCO [27]. OneFormer sets a new state-of-the-art performance on all three segmen-tation tasks compared with methods using the standard
Swin-L [30] backbone and improves even more with new ConvNeXt [31] and DiNAT [17] backbones. 2.