Abstract
Recent advance in 2D CNNs has revealed that large ker-nels are important. However, when directly applying large convolutional kernels in 3D CNNs, severe difficulties are met, where those successful module designs in 2D become surprisingly ineffective on 3D networks, including the pop-ular depth-wise convolution. To address this vital chal-lenge, we instead propose the spatial-wise partition con-volution and its large-kernel module. As a result, it avoids the optimization and efficiency issues of naive 3D large ker-nels. Our large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D tasks of semantic seg-mentation and object detection.
It achieves 73.9% mIoU on the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance further boosts to 74.2% NDS with a simple multi-modal fusion.
In addition, LargeKernel3D can be scaled to 17×17×17 kernel size on Waymo 3D object detection. For the first time, we show that large kernels are feasible and essential for 3D visual tasks. Our code and models is available at github.com/dvlab-research/LargeKernel3D. 1.

Introduction 3D Sparse convolutional neural networks (CNNs) have been widely used as feature extractors in 3D tasks, e.g., se-mantic segmentation [9,24] and object detection [55,65,75].
The advantages of efficiency and convenient usage en-sure its important role in various applications, such as autonomous driving and robotics. However, 3D sparse
CNNs are recently challenged by transformer-based meth-ods [45,46,79], mainly from the aspect of building effective receptive fields. Both global and local [21,45] self-attention mechanisms are able to capture context information from a large spatial scope. 2D Vision Transformers (ViTs) also emphasize their advantages in modeling long-range depen-dencies [20, 42, 51]. In contrast, common 3D sparse CNNs are limited in this regard. It is because the receptive fields
*Equal Contribution. of default 3D sparse CNN are constrained by small kernel sizes and spatial disconnection of sparse features (due to the property of submanifold sparse convolution [25]).
Literature about 2D CNNs [18, 43, 62] presents a series of methods, combined with large kernels, to enlarge the re-ceptive fields and model capacity. ConvNeXt [43] employs 7×7 depth-wise convolution as a strong design, combing with other training techniques to challenge its Swin Trans-former counterpart [42]. RepLKNet [18] pursues extremely large kernel sizes of 31×31 to boost the performance of dif-ferent tasks. To ensure the effectiveness of RepLKNet [18], additional factors, including depth-wise convolution, are also required. Other work [27] also emphasizes the im-portance of depth-wise convolution. Due to differences be-tween 3D and 2D tasks, these methods, however, are found not a good solution for 3D sparse CNNs.
We first analyze the difficulties of 3D large-kernel CNN design in two aspects. The first challenge is efficiency. It is easy to understand that 3D convolution is with the cubic kernel size and computation increases fast. For example, the model size increases 10+ times when kernels change from 3×3×3 to 7×7×7. The second difficulty exists in the optimization procedure. 3D datasets may contain only thousands of scenes, which cannot match 2D image bench-marks [15, 40] in terms of scales.
In addition, 3D point clouds or voxels are sparse, instead of dense images. Thus, it might be insufficient to optimize the proliferated parame-ters of large kernels and leads to over-fitting.
In this paper, we propose spatial-wise partition convo-lution as the 3D large-kernel design. It is a new family of group convolution by sharing weights among spatially ad-jacent locations, rather than depth-wise convolution [29] of channel-level groups. As shown in Fig. 1, spatial-wise parti-tion convolution remaps a large kernel (e.g., 7×7) as a small one (e.g., 3×3) via grouping spatial neighbors, while the ab-solutely large spatial size remains unchanged. With regard to the efficiency issue, it occupies few model sizes to keep parameters the same as those of small kernels. Moreover, it takes less latency, compared with plain large kernel coun-terparts. As for the optimization challenge, weight-sharing among spatial dimensions gives parameters more chance to update and overcome the over-fitting issue.
Figure 1. Sparse convolutions with different kernels. Small-kernel sparse convolution gathers features in a local area.
It is efficient but discards sufficient information flow due to feature disconnection and the small scope. Large-kernel sparse convolution is capable of capturing long-range information, at the price of a large number of parameters and computation. Our proposed spatial-wise partition convolution uses large kernel sizes, and shares weights among local neighbors for efficiency. We show 2D features for the simplicity sake.
To increase the detail-capturing ability of large ker-nels [18], we introduce position embeddings for spatial-wise group convolution. It makes notable effects for large kernel sizes. We name the proposed block as spatial-wise large-kernel convolution (SW-LK Conv). We compare the efficiency between plain 3D submanifold sparse convolu-tion and ours, as shown in Tab. 1. Both parameters and latency of the baseline increases dramatically, as its kernel size becomes larger, while ours is far more efficient.
SW-LK Conv can readily replace plain convolution lay-ers in existing 3D convolutional networks. We establish large-kernel backbone networks LargeKernel3D on existing 3D semantic segmentation [9] and object detection [16, 75] networks. It achieves notable improvement upon state-of-the-art methods [9, 16, 75], with a small model complex-ity overhead. Extensive experiments validate our effective-ness on large-scale benchmarks, including ScanNetv2 [13], nuScenes [4], and Waymo [57].
For object detection,
LargeKernel3D achieves 72.8% NDS on nuScenes, rank-ing 1st on the nuScenes LIDAR leaderboard. Without bells and whistles, it further improves to 74.2% NDS in a simple voxel-wise multi-modal fusion manner. More importantly, it is scalable to 17×17×17 kernel sizes on the large-scale
Waymo 3D object detection.
We visualize the Effective Receptive Fields (ERFs) of plain 3D CNNs and our LargeKernel3D in Fig. 2. It shows that deep small-kernel networks are also constrained by lim-ited ERFs, since sparse features are spatially disconnected.
Note that our large-kernel networks elegantly resolve this issue. For the first time, we show that large-kernel CNN designs become effective on essential 3D visual tasks. 2.