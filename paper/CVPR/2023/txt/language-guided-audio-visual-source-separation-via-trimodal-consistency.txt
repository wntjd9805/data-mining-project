Abstract
We propose a self-supervised approach for learning to perform audio source separation in videos based on natu-ral language queries, using only unlabeled video and au-dio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the correspond-ing components of the audio waveform, all without access to annotations during training. To overcome this chal-lenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. Dur-ing inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone.
We demonstrate the effectiveness of our self-supervised ap-proach on three audio-visual separation datasets, includ-ing MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised approaches despite not using object detectors or text labels during training. Our project page including publicly available code can be found at https://cs-people.bu.edu/rxtan/projects/VAST. 1.

Introduction
Our everyday audiovisual world is composed of many visible sound sources, often with multiple sources layering on top of one another. For example, consider the video of the guitar and cello musicians playing together in Fig. 1.
The two instruments have distinct timbres, and the musi-cians play non-unison, but complementary melodies. De-spite hearing both instruments simultaneously, humans have an innate ability to identify and isolate the melody of a sin-gle source object. In this paper, we define the corresponding machine task as follows: given a natural language query that selects a sounding object, such as “person playing a guitar”, separate its sound source from the input audio waveform and localize it in the input video, without any supervision.
This task is challenging. First, there is no approach for
Figure 1. We propose to separate and localize audio sources based on a natural language query, by learning to align the modalities on completely unlabeled videos. In comparison, prior audio-visual sound separation approaches require object label supervision. associating the linguistic description of a sound-emitting object to its visual features and the corresponding compo-nents of the audio waveform without access to annotations during training. Existing audio-visual methods [5, 14, 41] do not generalize to natural language queries due to their dependence on discrete object class labels. Second, an ideal solution would jointly identify and localize sound-emitting objects in videos as well as separate the corre-sponding components in the audio waveform without strong supervision. Although prior audio-visual work has demon-strated the benefits of aligning relevant object regions in the video with their corresponding sounds [5, 14], these ap-proaches require strong supervision including object label 1
and bounding box annotations (see Fig. 1 top). Overcoming these challenges would enable important downstream appli-cations including holistic video understanding [33], embod-ied AI [6], and bidirectional audio-to-video retrieval [38].
To address these challenges, we make the following contributions. First, we propose Video-Audio Separation through Text (VAST), a self-supervised approach that lever-ages large vision-language “foundation” models [20, 26] to provide pseudo-supervision for learning the alignment be-tween the three modalities: audio, video and natural lan-guage. Our key insight is to learn a strong transitive rela-tion from audio to natural language using vision as an inter-mediary modality, while preserving the alignment between the visual and natural language modalities embodied by the foundation models. However, just using the visual represen-tations of these foundation models in existing AV separation approaches does not preserve the transitive relationships be-tween the three modalities (Sec. 4.1).
Our second contribution introduces two novel multi-modal alignment objectives that encourage the learnt audio representations to encode the semantics of captions and in-fer the latent transitive relation between the three modali-ties. While natural language can express a large and var-ied range of visual concepts for audio separation in videos, the absence of captions in unlabeled videos during training poses a significant challenge in our self-supervised formu-lation. To learn the transitive alignment, we adapt a founda-tion model to extract latent captions from unlabeled videos.
Intuitively, the latent captions are representations that ex-press the visual concepts present in the videos. Third, we introduce a Multiple Instance Learning formulation to learn to perform audio separation at the video region level since we do not have prior information on relevant objects or their locations in the videos during training.
Finally, we demonstrate the effectiveness of our pro-posed VAST approach through extensive evaluations on the audio source separation task on the SOLOS [24], MU-SIC [41], and AudioSet [15] datasets. We show that our self-supervised approach outperforms strongly-supervised state-of-the-art approaches without using labels during training by leveraging the capability of vision-language foundation models. More importantly, we demonstrate that
VAST learns to use language queries for audio separation despite not training with ground-truth language supervision. 2.