Abstract
Generating a high-quality High Dynamic Range (HDR) image from dynamic scenes has recently been extensively studied by exploiting Deep Neural Networks (DNNs). Most
DNNs-based methods require a large amount of train-ing data with ground truth, requiring tedious and time-consuming work. Few-shot HDR imaging aims to generate satisfactory images with limited data. However, it is dif-ficult for modern DNNs to avoid overfitting when trained on only a few images.
In this work, we propose a novel semi-supervised approach to realize few-shot HDR imag-ing via two stages of training, called SSHDR. Unlikely pre-vious methods, directly recovering content and removing ghosts simultaneously, which is hard to achieve optimum, we first generate content of saturated regions with a self-supervised mechanism and then address ghosts via an itera-tive semi-supervised learning framework. Concretely, con-sidering that saturated regions can be regarded as mask-ing Low Dynamic Range (LDR) input regions, we design a
Saturated Mask AutoEncoder (SMAE) to learn a robust fea-ture representation and reconstruct a non-saturated HDR image. We also propose an adaptive pseudo-label selection strategy to pick high-quality HDR pseudo-labels in the sec-ond stage to avoid the effect of mislabeled samples. Ex-periments demonstrate that SSHDR outperforms state-of-the-art methods quantitatively and qualitatively within and across different datasets, achieving appealing HDR visual-ization with few labeled samples. 1.

Introduction
Standard digital photography sensors are unable to cap-ture the wide range of illumination present in natural scenes, resulting in Low Dynamic Range (LDR) images that often
*† The first three authors contributed equally to this work. This work was partially supported by NSFC (U19B2037, 61901384), Natural Science
Basic Research Program of Shaanxi (2021JCW-03, 2023-JC-QN-0685), and National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology. Corresponding author: Yu Zhu.
Figure 1. The proposed method generates high-quality images with few labeled samples when compared with several methods. suffer from over or underexposed regions, which can dam-age the details of the scene. High Dynamic Range (HDR) imaging has been developed to address these limitations.
This technique combines several LDR images with different exposures to generate an HDR image. While HDR imaging can effectively recover details in static scenes, it may pro-duce ghosting artifacts when used with dynamic scenes or hand-held camera scenarios.
Historically, various techniques have been suggested to address such issues, such as alignment-based methods
[3,10,27,37], patch-based methods [8,15,24], and rejection-based methods [5, 11, 19, 20, 35, 40]. Two categories of rigid alignment (e.g., alignment-based approaches exist: homographies) that fail to address foreground motions, and non-rigid alignment (e.g., optical flow) that is error-prone.
Patch-based techniques merge similar regions using patch-level alignment and produce superior results, but suffer from high complexity. Rejection-based methods aim to eliminate misaligned areas before fusing images, but may result in a loss of information in motion regions.
As Deep Neural Networks (DNNs) become increas-ingly prevalent, the DNN-based HDR deghosting methods
[9, 33, 36] achieve better visual results compared to tradi-tional methods. However, these alignment approaches are error-prone and inevitably cause ghosting artifacts (see Fig-ure 1 Kalantari’s results). AHDR [31, 32] proposes spatial attention to suppress motion and saturation, which effec-tively alleviate misalignment problems. Based on AHDR,
ADNET [14] proposes a dual branch architecture using spatial attention and PCD-alignment [29] to remove ghost-ing artifacts. All these above methods directly learn the complicated HDR mapping function with abundant HDR ground truth data. However, it’s challenging to collect a large amount of HDR-labeled data. The reasons can be at-tributed to that 1) generating a ghost-free HDR ground truth sample requires an absolute static background, and 2) it is time-consuming and requires considerable manpower to do manual post-examination. This generates a new setting that only uses a few labeled data for HDR imaging.
Recently, FSHDR [22] attempts to generate a ghost-free
HDR image with only few labeled data. They use a prelim-inary model trained with a large amount of unlabeled dy-namic samples, and a few dynamic and static labeled sam-ples to generate HDR pseudo-labels and synthesize artificial dynamic LDR inputs to improve the model performance of dynamic scenes further. This approach expects the model to handle both the saturation and the ghosting problems simul-taneously, but it is hard to achieve under the condition of few labeled data, especially misaligned regions caused by saturation and motion (see Figure 1 FSHDR). In addition,
FSHDR uses optical flow to forcibly synthesize dynamic
LDR inputs from poorly generated HDR pseudo-labels, the errors in optical flow further intensify the degraded qual-ity of artificial dynamic LDR images, resulting in an appar-ent distribution shift between LDR training and testing data, which hampers the performance of the network.
The above analysis makes it very challenging to di-rectly generate a high-quality and ghost-free HDR image with few labeled samples. A reasonable way is to address the saturation problems first and then cope with the ghost-ing problems with a few labeled samples.
In this paper, we propose a semi-supervised approach for HDR deghost-ing, named SSHDR, which consists of two stages: self-supervised learning network for content completion and sample-quality-based iterative semi-supervised learning for deghosting. In the first stage, we pretrain a Saturated Mask
AutoEncoder (SMAE), which learns the representation of
HDR features to generate content of saturated regions by self-supervised learning. Specifically, considering that the saturated regions can be regarded as masking the short LDR input patches, inspired by [6], we randomly mask a high proportion of the short LDR input and expect the model to reconstruct a no-saturated HDR image from the remaining
LDR patches in the first stage. This self-supervised ap-proach allows the model to recover the saturated regions with the capability to effectively learn a robust represen-tation for the HDR domain and map an LDR image to an
HDR image.
In the second stage, to prevent the overfit-ting problem with a few labeled training samples and make full use of the unlabeled samples, we iteratively train the model with a few labeled samples and a large amount of
HDR pseudo-labels from unlabeled data. Based on the pretrained SMAE, a sample-quality-based iterative semi-supervised learning framework is proposed to address the ghosting artifacts. Considering the quality of pseudo-labels is uneven, we develop an adaptive pseudo-labels selection strategy to pick high-quality HDR pseudo-labels (i.e., well-exposed, ghost-free) to avoid awful pseudo-labels hamper-ing the optimization process. This selection strategy is guided by a few labeled samples and enhances the diver-sity of training samples in each epoch. The experiments demonstrate that our proposed approach can generate high-quality HDR images with few labeled samples and achieves state-of-the-art performance on individual and cross-public datasets. Our contributions can be summarized as follows:
• We propose a novel and generalized HDR self-supervised pretraining model, which uses mask strategy to recon-struct an HDR image and addresses saturated problems from one LDR image.
• We propose a sample-quality-based semi-supervised training approach to select well-exposed and ghost-free
HDR pseudo-labels, which improves ghost removal.
• We perform both qualitative and quantitative experi-ments, which show that our method achieves state-of-the-art results on individual and cross-public datasets. 2.