Abstract 1.

Introduction
Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a NeRF model to generate a 3D object. We adapt the score distilla-tion to the publicly available, and computationally efﬁcient,
Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoen-coder. As NeRFs operate in image space, a na¨ıve solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the NeRF to the latent space, result-ing in a Latent-NeRF. Analyzing our Latent-NeRF, we show that while Text-to-3D models can generate impressive re-sults, they are inherently unconstrained and may lack the ability to guide or enforce a speciﬁc 3D structure. To as-sist and direct the 3D generation, we propose to guide our
Latent-NeRF using a Sketch-Shape: an abstract geometry that deﬁnes the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-NeRF. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry.
Our experiments validate the power of our different forms of guidance and the efﬁciency of using latent rendering.
Text-guided image generation has seen tremendous suc-cess in recent years, primarily due to the breathtaking de-velopment in Language-Image models [25, 28, 36] and dif-fusion models [14, 21, 37–40]. These breakthroughs have also resulted in fast progression for text-guided shape gen-eration [9,29,53]. Most recently, it has been shown [35] that one can directly use score distillation from a 2D diffusion model to guide the generation of a 3D object represented as a Neural Radiance Field (NeRF) [30].
While Text-to-3D can generate impressive results, it is inherently unconstrained and may lack the ability to guide or enforce a 3D structure. In this paper, we show how to in-troduce shape-guidance to the generation process to guide it toward a speciﬁc shape, thus allowing increased control over the generation process. Our method builds upon two models, a NeRF model [30], and a Latent Diffusion Model (LDM) [39]. Latent Models, which apply the entire diffu-sion process in a compact latent space, have recently gained popularity due to their efﬁciency and publicly available pre-trained checkpoints. As score distillation was previously applied only on RGB diffusion models, we ﬁrst present two key modiﬁcations to the NeRF model that are better paired with guidance from a latent model. First, instead of repre-senting our NeRF in the standard RGB space, we propose a Latent-NeRF which operates directly in the latent space of the LDM, thus avoiding the burden of encoding a ren-dered RGB image to a latent space for each and every guid-ing step. Secondly, we show that after training, one can easily transform a Latent-NeRF back into a regular NeRF.
This allows further reﬁnement in RGB space, where we can also introduce shading constraints or apply further guidance from RGB diffusion models [40]. This is achieved by intro-ducing a learnable linear layer that can be optionally added to a trained Latent-NeRF, where the linear layer is initial-ized using an approximate mapping between the latent and
RGB values [45].
Our ﬁrst form of shape-guidance is applied using a coarse 3D model, which we call a Sketch-Shape. Given a
Sketch-Shape, we apply soft constraint during the NeRF optimization process to guide its occupancy based on the given shape. Easily combined with Latent-NeRF optimiza-tion, the additional constraint can be tuned to meet a desired level of strictness. Using a Sketch-Shape allows users to deﬁne their base geometry, where Latent-NeRF then reﬁnes the shape and introduces texture based on a guiding prompt.
We further present Latent-Paint, another form of shape-guidance where the generation process is applied directly on a given 3D mesh, and we have not only the structure but also the exact parameterization of the input mesh. This is achieved by representing a texture map in the latent space and propagating the guidance gradients directly to the tex-ture map through the rendered mesh. By doing so, we allow for the ﬁrst time to colorize a mesh using guidance from a pretrained diffusion model and enjoy its expressiveness.
We evaluate our different forms of guidance under a va-riety of scenarios and show that together with our latent-based guidance, they offer a compelling solution for con-strained shape and texture generation. 2.