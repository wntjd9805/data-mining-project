Abstract
Vision Transformers (ViTs) have achieved overwhelming success, yet they suffer from vulnerable resolution scalabil-ity, i.e., the performance drops drastically when presented with input resolutions that are unseen during training. We introduce, ResFormer, a framework that is built upon the seminal idea of multi-resolution training for improved per-formance on a wide spectrum of, mostly unseen, testing res-olutions. In particular, ResFormer operates on replicated images of different resolutions and enforces a scale con-sistency loss to engage interactive information across dif-ferent scales. More importantly, to alternate among vary-ing resolutions effectively, especially novel ones in testing, we propose a global-local positional embedding strategy that changes smoothly conditioned on input sizes. We con-duct extensive experiments for image classification on Im-ageNet. The results provide strong quantitative evidence that ResFormer has promising scaling abilities towards a wide range of resolutions. For instance, ResFormer-B-MR achieves a Top-1 accuracy of 75.86% and 81.72% when evaluated on relatively low and high resolutions respec-tively (i.e., 96 and 640), which are 48% and 7.49% better than DeiT-B. We also demonstrate, moreover, ResFormer is flexible and can be easily extended to semantic segmenta-tion, object detection and video action recognition. 1.

Introduction
The strong track record of Transformers in a multi-tude of Natural Language Processing [53] tasks has moti-vated an extensive exploration of Transformers in the com-puter vision community. At its core, Vision Transformers (ViTs) build upon the multi-head self-attention mechanisms for feature learning through partitioning input images into patches of identical sizes and processing them as sequences for dependency modeling. Owing to their strong capabil-â€ Corresponding author.
Note that we use resolution, scale and size interchangeably.
Figure 1. Comparisons between ResFormer and vanilla ViTs. Res-Former achieves promising results on a wide range of resolutions. ities in capturing relationships among patches, ViTs and their variants demonstrate prominent results in versatile vi-sual tasks, e.g., image classification [36, 50, 65, 70], object detection [4, 30, 55], vision-language modeling [25, 40, 54] and video recognition [3, 29, 37, 64].
While ViTs have been shown effective, it remains un-clear how to scale ViTs to deal with inputs with varying sizes for different applications. For instance, in image clas-sification, the de facto training resolution of 224 is com-monly adopted [36, 50, 51, 65]. However, among works in pursuit of reducing the computational cost of ViTs [39, 43], shrinking the spatial dimension of inputs is a popular strat-egy [6, 32, 56]. On the other hand, fine-tuning with higher resolutions (e.g., 384) is widely used [15, 36, 48, 51, 59, 62] to produce better results. Similarly, dense prediction tasks such as semantic segmentation and object detection also re-quire relatively high resolution inputs [1, 30, 35, 55].
Despite of the necessity for both low and high resolu-tions, limited effort has been made to equip ViTs with the ability to handle different input resolutions. Given a novel resolution that is different from that used during training, a common practice adopted for inference is to keep the patch size fixed and then perform bicubic interpolation on posi-tional embeddings directly to the corresponding scale. As shown in Sec. 3, while such a strategy is able to scale ViTs to relatively larger input sizes, the results on low resolutions
In addition, significant changes between plunge sharply. training and testing scales also lead to limited results (e.g.,
DeiT-S trained on a resolution of 224 degrades by 1.73% and 7.2% when tested on 384 and 512 respectively).
Multi-resolution training, which randomly resizes im-ages to different resolutions, is a promising way to accom-modate varying resolutions at test time. While it has been widely used by CNNs for segmentation [22], detection [24] and action recognition [58], generalizing such an idea to
ViTs is challenging and less explored. For CNNs, thanks to the stacked convolution design, all input images, regard-less of their resolutions, share the same set of parameters in multi-resolution training. For ViTs, although it is feasible to share parameters for all samples, bicubic interpolations of positional embeddings, which are not scale-friendly, are still needed when iterating over images of different sizes.
In this paper, we posit that positional embeddings of
ViTs should be adjusted smoothly across different scales for multi-resolution training. The resulting model then has the potential to scale to different resolutions during inference.
Furthermore, as images in different scales contain objects of different sizes, we propose to explore useful information across different resolutions for improved performance in a similar spirit to feature pyramids, which are widely used in hierarchical backbone designs for both image classifica-tion [24, 36] and dense prediction tasks [22, 23, 33].
To this end, we introduce ResFormer, which which takes in inputs as multi-resolution images during training and ex-plores multi-scale clues for better results. Trained in a sin-gle run, ResFormer is expected to generalize to a large span of testing resolutions.
In particular, given an image dur-ing training, ResFormer resize it to different scales, and then use all scales in the same feed-forward process. To encourage information interaction among different resolu-tions, we introduce a scale consistency loss, which bridges the gap between low-resolution and high-resolution features by self-knowledge distillation. More importantly, to facili-tate multi-resolution training, we propose a global-local po-sitional embedding strategy, which enforces parameter shar-ing and changes smoothly across different resolutions with the help of convolutions. Given a novel resolution at testing,
ResFormer dynamically generates a new set of positional embeddings and performs inference.
To validate the efficacy of ResFormer, we conduct com-prehensive experiments on ImageNet-1K [13]. We observe that ResFormer makes remarkable gains compared with vanilla ViTs which are trained on single resolution. Given the testing resolution of 224, ResFormer-S-MR trained on resolutions of 128, 160 and 224 achieves a Top-1 accuracy of 82.16%, outperforming the 224-trained DeiT-S [50] by 2.24% . More importantly, as illustrated in Fig. 1, Res-Former surpasses DeiT by a large margin on unseen res-olutions, e.g., ResFormer-S-MR outperforms DeiT-S by 6.67% and 56.04% when tested on 448 and 80 respec-tively. Furthermore, we also validate the scalability of Res-Former on dense prediction tasks, e.g., ResFormer-B-MR achieves 48.30 mIoU on ADE20K [72] and 47.6 APbox on
COCO [34]. We also show that ResFormer can be readily adapted for video action recognition with different sizes of inputs via building upon TimeSFormer [3]. 2.