Abstract
Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing mul-tiple source domains without re-training. Most existing
DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local represen-tations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based meth-ods have achieved promising results in supervised learn-ing tasks by learning global interactions among different patches of the image.
Inspired by this, in this paper, we first analyze the difference between CNN and MLP meth-ods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods.
Then, based on a recent lightweight MLP method, we ob-tain a strong baseline that outperforms most state-of-the-art CNN-based methods. The baseline can learn global structure representations with a filter to suppress structure-irrelevant information in the frequency space. Moreover, we propose a dynAmic LOw-Frequency spectrum Trans-form (ALOFT) that can perturb local texture features while preserving global structure features, thus enabling the fil-ter to remove structure-irrelevant information sufficiently.
Extensive experiments on four benchmarks have demon-strated that our method can achieve great performance im-provement with a small number of parameters compared to
SOTA CNN-based DG methods. Our code is available at https://github.com/lingeringlight/ALOFT/.
*Corresponding authors: Yinghuan Shi and Lei Qi. Work supported by
NSFC Program (62222604, 62206052, 62192783), CAAI-Huawei Mind-Spore (CAAIXSJLJJ-2021-042A), China Postdoctoral Science Founda-tion Project (2021M690609), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund.
Figure 1. Comparison of the SOTA CNN-based methods, the latest
MLP-like models, and our method on PACS. Among the SOTA
CNN-based and MLP-based methods, our method can achieve the best performance with a relatively small-sized network. 1.

Introduction
Most deep learning methods often degrade rapidly in performance if training and test data are from different dis-tributions. Such performance degradation caused by distri-bution shift (i.e., domain shift [3]) hinders the applications of deep learning methods in real world. To address this is-sue, unsupervised domain adaptation (UDA) assumes that the unlabeled target domain can be utilized during training to help narrow the potential distribution gap between source and target domains [12, 31, 57]. However, UDA methods cannot guarantee the performance of model on unknown target domains that could not be observed during training
[38, 48]. Since the target domain could not always be avail-able in reality, domain generalization (DG) is proposed as a more challenging yet practical setting, which aims to learn a model from observed source domains that performs well on arbitrary unseen target domains without re-training.
To enhance the robustness of model to domain shifts,
many DG methods intend to learn domain-invariant rep-resentations across source domains, mainly via adversarial learning [9, 63], meta-learning [5, 58], data augmentation
[15,24,44], etc. Existing DG works are primarily built upon convolution neural networks (CNNs). However, due to the local processing in convolutions, CNN models inherently learn a texture bias from local representations [2,29], which inevitably leads to their tendency to overfit source domains and perform unsatisfactorily on unseen target domains. To tackle this drawback, some pioneers propose to replace the backbone architecture of DG with transformer or MLP-like models, which can learn global representations with atten-tion mechanisms [18, 39, 61, 62]. Although these methods have achieved remarkable performance, few of them have analyzed how the differences between the MLP and CNN architectures affect the generalization ability of model in the DG task. These methods also suffer from excessive net-work parameters and high computational complexity, which hinders their applications in real-world scenarios.
In this paper, we first investigate the generalization abil-ity of several MLP methods in the DG task and conduct the frequency analysis [2] to compare their differences with
CNN methods. We observe that MLP methods are bet-ter at capturing global structure information during infer-ence, hence they can generalize better to unseen target do-mains than CNN methods. Based on the observation, we propose an effective lightweight MLP-based framework for
DG, which can suppress local texture features and empha-size global structure features during training. Specifically, based on the conventional MLP-like architecture [10, 35], we explore a strong baseline for DG that performs better than most state-of-the-art CNN-based DG methods. The strong baseline utilizes a set of learnable filters to adaptively remove structure-irrelevant information in the frequency space, which can efficiently help the model learn domain-invariant global structure features. Moreover, since the low-frequency components of images contain the most domain-specific local texture information, we propose a novel dy-nAmic LOw-Frequency spectrum Transform (ALOFT) to further promote the ability of filters to suppress domain-specific features. ALOFT can sufficiently simulate potential domain shifts during training, which is achieved by model-ing the distribution of low-frequency spectrums in differ-ent samples and resampling new low-frequency spectrums from the estimated distribution. As shown in Fig. 1, our framework can achieve excellent generalization ability with a small number of parameters, proving its superiority in DG.
Our contributions are summarized as follows:
• We analyze how the MLP-like methods work in DG task from a frequency perspective. The results indicate that MLP-like methods can achieve better generaliza-tion ability because they can make better use of global structure information than CNN-based methods.
• We propose a lightweight MLP-like architecture with dynamic low-frequency transform as a competitive al-ternative to CNNs for DG, which can achieve a large improvement from the ResNet with similar or even smaller network size as shown in Fig. 1.
• For dynamic low-frequency transform, we design two variants to model the distribution of low-frequency spectrum from element-level and statistic-level, re-spectively. Both variants can enhance the capacity of the model in capturing global representations.
We demonstrate the effectiveness of our method on four standard domain generalization benchmarks. The results show that compared to state-of-the-art domain generaliza-tion methods, our framework can achieve a significant im-provement with a small-sized network on all benchmarks. 2.