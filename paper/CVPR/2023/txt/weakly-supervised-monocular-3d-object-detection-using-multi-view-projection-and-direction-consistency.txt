Abstract
Monocular 3D object detection has become a main-stream approach in automatic driving for its easy applica-tion. A prominent advantage is that it does not need Li-DAR point clouds during the inference. However, most cur-rent methods still rely on 3D point cloud data for labeling the ground truths used in the training phase. This incon-sistency between the training and inference makes it hard to utilize the large-scale feedback data and increases the data collection expenses. To bridge this gap, we propose a new weakly supervised monocular 3D objection detec-tion method, which can train the model with only 2D labels marked on images. To be specific, we explore three types of consistency in this task, i.e. the projection, multi-view and direction consistency, and design a weakly-supervised architecture based on these consistencies. Moreover, we propose a new 2D direction labeling method in this task to guide the model for accurate rotation direction predic-tion. Experiments show that our weakly-supervised method achieves comparable performance with some fully super-vised methods. When used as a pre-training method, our model can significantly outperform the corresponding fully-supervised baseline with only 1/3 3D labels. 1.

Introduction
Monocular 3D object detection is a foundational re-search area in computer vision and plays an important role in autonomous driving systems. It aims to identify the ob-jects and estimate the 3D bounding boxes of the correspond-ing targets with a single image as input. Different from 3D point clouds detection methods like [42, 43, 47, 53, 61],
∗Equal contribution. †Corresponding author: Jianbing Shen. This work was supported in part by the FDCT grant SKL-IOTSC(UM)-2021-2023, the FDCT Grant 0123/2022/AFJ, the Grant MYRG-CRG2022-00013-IOTSC-ICI, and the Grant SRG2022-00023-IOTSC.
Figure 1. Illustration of the projection and multi-view con-sistency. (a) Only projection consistency cannot determine the accurate position of the target because projection loss has more than one optimal solution in the 3D space. For example, the two dashed boxes in 3D space produce the same projection loss be-cause they have the same projection in 2D space. (b) Constrained by the multi-view consistency, the optimal solution must be the common solution for two viewpoints, that is, the target location. monocular 3D detection models [28, 34, 48, 49, 65] alleviate the need of LiDAR sensors, making the self-driving system easier to be applied.
However, there is still a challenging problem that lim-its the application of 3D object detection with pure cam-era vision data. That is, the ground truth 3D boxes used in the training phases are usually labeled with 3D point clouds [2, 10, 45]. Recently, self-driving systems with pure camera vision inputs have become a new trend. But the feedback video clips captured by the production cars can-not be utilized to improve the 3D object detection mod-els because of the lack of training labels. Compared with the data from the data-collection cars, the feedback images from production cars have a larger scale diversity and con-tain more corner cases, which are crucial for improving the robustness of models.
In this paper, we propose a new weakly supervised train-ing method, which can train the 3D object detection models with only camera images and 2D labels, making it possible to utilize the feedback data from the production cars. To
achieve this goal, we exploit three types of consistency be-tween the 3D boxes and 2D images and fully utilize them for training the object detection models. The first is the pro-jection consistency. With the intrinsic matrix of a camera, a 3D box predicted by the models can be projected into the 2D image space, and the projected boxes should be con-sistent with the corresponding 2D boxes. Based on this, we propose a projection loss by minimizing the difference between the projected boxes and 2D ground truths. This criterion can guide the predicted 3D boxes into the projec-tion regions. However, only projection consistency cannot provide enough information to correct the errors in the 3D space, especially for the depth dimension, as shown in Fig. 1 (a). According to the perspective principle, multiple boxes in the 3D space can be projected into the same 2D box in the image. Thus, there is more than one optimal solution for the projection loss, and the errors caused by these boxes cannot be optimized by the projection loss.
Aiming at solving this limitation, we incorporate multi-view consistency into our method to minimize the errors in 3D space. The same object captured from different view-points would show different positions and shapes in the cor-responding 2D images. But in the 3D space, 3D bounding boxes belonging to the same object should be consistent, i.e. they should be of the same position, size and rotation angle in a certain coordinate system. Based on this, we construct the multi-view consistency by minimizing the discrepancy between the predicted bounding boxes of the same object from a different point of view. As shown in Fig. 1(b), pro-jection losses on the two viewpoints will constrain the pre-dictions into their projection regions, and the multi-view consistency will further guide the predictions to the com-mon optimal solutions of the two views, which are where the objects located. Notably, in our work, images paired from different viewpoints are only used for calculating the losses, and the models still only take monocular inputs in the evaluation phase.
The last consistency presented in this paper is the direc-tion consistency for guiding the prediction of the direction scale. In previous works [2] [45] [10], 3D rotation direction is labeled on point clouds by a vector from the center to the front of objects. To avoid the need for 3D LiDAR data, we propose a new labeling method named 2D direction label directly on pure camera images, indicating the 2D direction of the object in the images. The predicted 3D box rotation should be consistent with the direction in 2D space when they are projected, i.e. the direction consistency. Based on this consistency, we further design a 2D rotation loss for optimizing the rotation-scale estimation.
The proposed weakly supervision method is a general framework that can be integrated with most monocular 3D detection models. To show the efficiency of our method, we incorporate it with a representative model - DD3D [34] in this task, and evaluate it on the KITTI benchmark. Re-sults show that our method can achieve comparable per-formance with some fully supervised methods. Also, to demonstrate the application in real scenes, we collect a new dataset named ProdCars from production cars and evaluate the performance of our method on it.
In short, in this work, we propose a novel weakly su-pervised method for monocular 3D object detection, which only utilizes 2D labels as ground truth without depend-ing on 3D point clouds for labeling, making us the first to do so. Our approach incorporates projection consistency and multi-view consistency, which are used to design two consistency losses guiding the prediction of accurate 3D bounding boxes. Additionally, we introduce a new labeling method called 2D direction label, replacing the 3D rotation label in point clouds data and a direction consistency loss based on the new labels. Our experiments show that our proposed weakly supervised method achieves comparable performance with some fully supervised methods, and even with only 1/3 of the ground truth labels, our method out-performs corresponding fully supervised baselines, demon-strating its potential for improving models based on feed-back production data. 2.