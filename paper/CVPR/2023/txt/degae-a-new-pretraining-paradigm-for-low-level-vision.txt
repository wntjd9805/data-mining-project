Abstract
Self-supervised pretraining has achieved remarkable success in high-level vision, but its application in low-level vision remains ambiguous and not well-established. What is the primitive intention of pretraining? What is the core problem of pretraining in low-level vision? In this paper, we aim to answer these essential questions and establish a new pretraining scheme for low-level vision. Specifically, we examine previous pretraining methods in both high-level and low-level vision, and categorize current low-level vision tasks into two groups based on the difficulty of data acqui-sition: low-cost and high-cost tasks. Existing literature has mainly focused on pretraining for low-cost tasks, where the observed performance improvement is often limited. How-ever, we argue that pretraining is more significant for high-cost tasks, where data acquisition is more challenging. To learn a general low-level vision representation that can im-prove the performance of various tasks, we propose a new pretraining paradigm called degradation autoencoder (De-gAE). DegAE follows the philosophy of designing pretext task for self-supervised pretraining and is elaborately tai-lored to low-level vision. With DegAE pretraining, SwinIR achieves a 6.88dB performance gain on image dehaze task, while Uformer obtains 3.22dB and 0.54dB improvement on dehaze and derain tasks, respectively. 1.

Introduction
With the phenomenal success of self-supervised pre-training in natural language processing (NLP), a large num-ber of attempts have also been proposed in the field of com-puter vision [20,21,66,67]. The idea behind self-supervised pretraining is to learn a general visual representation by de-vising an appropriate pretext task that does not rely on any manual annotation. Owing to large-scale pretraining, mod-els with a voracious appetite for data can alleviate the over-fitting problem and achieve further improvement.
* Corresponding author. Email: chao.dong@siat.ac.cn.
Recently, referring to the philosophy of masked language modeling (MLM) in NLP [27, 51], masked image modeling (MIM) [20,67] has been proposed and proven to be extraor-dinarily effective in high-level vision tasks, e.g., image clas-sification, object detection, and image segmentation. How-ever, the notion of low-level vision pretraining is not yet well-established, due to the distinctions between high-level and low-level vision tasks. Specifically, the representative high-level vision tasks take fixed-size images as inputs and predict manually annotated labels as targets [15, 23], while most low-level vision methods accept low-quality (LQ) im-ages as inputs and produce high-quality (HQ) images as targets [31, 78]. More importantly, the annotation man-ner in low-level vision is quite different. To obtain LQ-HQ pairs, a wide range of tasks choose to synthesize in-put LQ images from collected HQ images, such as classi-cal super-resolution [11] and Gaussian denoise [77]. Based on the difficulty of paired-data acquisition, we can roughly categorize low-level vision tasks into two groups: 1) low-cost task: tasks with low-cost data acquisition (e.g., super-resolution), and 2) high-cost task: tasks with high-cost data acquisition (e.g., dehaze). This analysis is absent in exist-ing low-level vision literatures [4, 7, 34]. They only con-sider low-cost tasks and simply adopt a straightforward pre-training strategy that has the same objectives as the down-stream tasks. Such a pretraining paradigm lacks generality and only brings marginal improvement. In this paper, we claim that pretraining could potentially be more effective for high-cost tasks and that a new pretraining paradigm tai-lored to low-level vision would be highly beneficial.
To this end, we devise a novel pretraining paradigm for low-level vision. Since the goal of low-level vision is to process LQ images with various degradations, we propose a degradation autoencoder (DegAE) to achieve content-degradation disentanglement and generation. DegAE ac-cepts an input image with degradation D1 and a reference image with degradation D2.
It attempts to transfer the degradation D2 of the reference image to the input image, obtaining an output image with input image content, but with degradation D2, as described in Fig. 1. Through such a learning paradigm, the model is expected to learn both
convolutional neural networks (CNN) to perform super-resolution. Zhang et al. [77] proposed the first deep de-noise method DnCNN. DehazeNet [2] and MSCNN [54] were the forerunners in applying the deep learning-based method to image dehaze. For image motion deblur, Deblur-GAN [29] and DeblurGAN-v2 [30] leveraged generative adversarial learning to achieve more realistic results. After-wards, more advanced methods like SRN [60], SPAIR [49] and NAFNet [5] were proposed. For image derain, the rep-resentative methods include DerainNet [16], PreNet [53] and MSPFN [25]. A multitude of follow-up works have been proposed in low-level vision tasks and achieved con-tinuous improvements.
Vision Transformer. Transformer [62] has dominated the model design in natural language processing (NLP). Due to its powerful representation learning capabilities, many attempts have been made to explore Transformer in vari-ous vision tasks, such as image recognition [15], segmen-tation [59] and object detection [3]. Along with high-level vision tasks, Transformer-based methods are also deployed in low-level vision tasks. Based on ViT, Chen et al. [4] proposed IPT for image restoration. Liang et al. [38] pre-sented a stronger baseline model SwinIR based on Swin
Transformers [44]. To facilitate long-range pixel dependen-cies and multi-scale local-global representation learning,
Uformer [64] and Restormer [71] were proposed. They both adopted an encoder-decoder design to achieve higher effi-ciency, establishing new state-of-the-art baselines on vari-ous image restoration tasks.
Self-supervised Pretraining.
In the field of NLP, with the power of Transformer and billion-scale data, self-supervised pretraining has become a default option. The tacit recipe is to pretrain on a large corpus and then fine-tune on a smaller task-specific dataset. Masked language modeling and its variants have been proven successful for pretraining, e.g., GPTs [51, 52] and BERT [27]. As for computer vision, diverse pretext tasks also have been in-vented to learn visual representation, e.g., jigsaw puzzle solving [48], rotation prediction [18], instance discrimina-tion [21, 66]. Recently, masked image modeling (MIM) has been proposed for vision tasks, where the pioneer works are
MAE [20] and SimMIM [67]. Experiments show that MIM pretraining can learn abstract and discriminative representa-tions, achieving promising transfer learning results. As for low-level vision tasks, a few pretraining methods have been proposed [4,7,34,41]. For instance, IPT [4] proposed multi-task restoration pretraining and EDT [34] proposed multi-related-task restoration pretraining. However, the motiva-tion and paradigm of these pretraining methods are ambigu-ous, compared to the prevalent pretraining in high-level vi-sion. In this paper, we propose a novel pretraining paradigm tailored to low-level vision – degradation autoencoder (De-gAE), which is more general for various downstream tasks.
Figure 1. Example results of DegAE pretraining. For instance, given an input noise image and a reference blur image, DegAE attempts to transfer the blur degradation to the input image. More visual examples are illustrated in the supplementary file. natural image representations and degradation information, which are the key components in low-level vision. Our approach follows the philosophy of designing pretext task for self-supervised pretraining [20, 27]. Firstly, the pretext task should not depend on the downstream tasks, in order to achieve the generality and transferability of the pretrained representations. Secondly, the pretext task should be care-fully designed to exploit internal structures of data.
To validate the effectiveness of DegAE pretraining, we choose three representative backbone models (SwinIR [38],
Uformer [64] and Restormer [71]) to conduct experiments.
The results suggest that DegAE pretraining can signif-icantly improve the model performance. For example,
SwinIR yields a 6.88dB gain on image dehaze task (SOTS) and a 1.27dB gain on image derain task (Rain100L).
Uformer obtains 3.22dB and 0.54dB improvement on im-age dehaze and derain task (Test100). Restormer achieves 0.43dB performance improvement on image motion deblur task (GoPro), respectively. As expected, we also observe incremental tasks – SR and denoise tasks. We believe our efforts can help to bridge the gap between high-level and low-level vision tasks and improve the performance of various low-level vision tasks. improvement on low-cost 2.