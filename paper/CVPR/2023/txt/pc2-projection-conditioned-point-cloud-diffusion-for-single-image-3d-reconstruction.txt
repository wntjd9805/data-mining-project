Abstract
Reconstructing the 3D shape of an object from a single
RGB image is a long-standing problem in computer vision.
In this paper, we propose a novel method for single-image 3D reconstruction which generates a sparse point cloud via a conditional denoising diffusion process. Our method takes as input a single RGB image along with its camera pose and gradually denoises a set of 3D points, whose positions are initially sampled randomly from a three-dimensional
Gaussian distribution, into the shape of an object. The key to our method is a geometrically-consistent conditioning process which we call projection conditioning: at each step in the diffusion process, we project local image features onto the partially-denoised point cloud from the given camera pose. This projection conditioning process enables us to generate high-resolution sparse geometries that are well-aligned with the input image and can additionally be used to predict point colors after shape reconstruction. Moreover, due to the probabilistic nature of the diffusion process, our method is naturally capable of generating multiple different shapes consistent with a single input image. In contrast to prior work, our approach not only performs well on synthetic benchmarks but also gives large qualitative improvements on complex real-world data. Data and code are available at https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/.
1.

Introduction
Reconstructing the 3D structure of an object from a sin-gle 2D view is a long-standing computer vision problem.
Given more than one view, it is possible to reconstruct an object’s shape using the tools of multiple-view geometry, but in the case of a single view, the problem is highly ill-posed and requires prior understanding of the possible shapes and appearances of the object.
Despite the difficulty of this task, humans are adept at us-ing a range of monocular cues and prior knowledge to infer the 3D structure of common objects from single views. For example, when a person looks at a photograph of an object, they can easily imagine what the shape of the backside of the object may plausibly be. The ease with which humans perform this task, and its relative difficulty for computer vision systems, has attracted the attention of the research community for nearly half a century [4, 5, 15, 23]. More-over, given the prevalence of single-view RGB image data in the real world, the single-view reconstruction problem has practical applications in numerous areas.
A substantial body of research has emerged around recon-struction via end-to-end deep learning methods [10,13,17,19, 25, 33, 43, 44, 46, 47, 50–54, 56]. They are capable of predict-ing volumes from single images, yet many remain limited to low-resolution geometries (e.g. dense voxel grids). Some recent works [8, 11, 34, 55] utilize implicit representations and radiance fields, which are capable of rendering novel views with photographic quality but often suffer from other drawbacks, such as an inability to reconstruct a distribution of possible 3D shapes from a single input image.
In this work, we take inspiration from recent progress in the generative modeling of 2D images using denoising diffusion probabilistic models. In the domain of 2D images, diffusion models (e.g. Latent-Diffusion [35], SDEdit [24],
GLIDE [27], and DALL-E 2 [30]) can produce remarkably high-fidelity image samples either from scratch or when con-ditioned on textual inputs. They do so by learning complex priors over the appearances of common objects. We seek to bring these advances to the domain of 3D reconstruction to generate the shape of unseen regions of a 3D object.
In order to do so, we propose to use diffusion models for single-image 3D reconstruction. Our approach represents shapes as unstructured point clouds: we gradually denoise a randomly-sampled set of points into a target shape condi-tional on an input image and its corresponding viewpoint.
The key to our approach is a novel way of conditioning the diffusion process to produce 3D shapes which are geometri-cally consistent with the input images. Specifically, at each iteration of the diffusion process, we project image features directly onto the points of the partially-denoised point cloud.
When shape reconstruction is complete, we are also able to use the same projection conditioning to predict point colors.
Our method differs from prior work in three major as-pects: (1) our point cloud-based shape representation, (2) our projection-conditioned diffusion model, and (3) our use of projection conditioning to predict point colors in addi-tion to point locations. Although these may seem separate from one another, they are in fact directly linked: the order-agnostic and unstructured nature of point clouds naturally lends itself to the highly flexible nature of diffusion models.
Without the point cloud-based representation, our projection-conditioned diffusion approach would be limited to coarse voxel-based shape approximation.
Finally, due to its probabilistic nature, given a single input image, our model is able to generate multiple plausible 3D point clouds which are all consistent with the input. We leverage this property by introducing a novel filtering step in the sampling process, which helps to address the ill-posed nature of the single-view 3D reconstruction problem. Specif-ically, we generate multiple point clouds for a given input image and filter these point clouds according to how well they match the input mask. Filtering enables us to bene-fit from the diversity of our model’s reconstructions in an entirely automated fashion.
Experimentally, we not only perform competitively on the synthetic ShapeNet benchmark, but we also move be-yond synthetic data: we demonstrate high-quality qualitative single-view reconstruction results on multiple categories in the challenging, real-world Co3D [32] dataset.
We present this work as a first step toward using denoising diffusion models for large-scale 3D reconstruction. Given the success of scaling 2D diffusion models over the past two years, our work demonstrates that 3D diffusion models could represent a path toward large-scale 3D reconstruction of complex objects from single-view images. 2.