Abstract
Rotated bounding boxes drastically reduce output ambi-guity of elongated objects, making it superior to axis-aligned bounding boxes. Despite the effectiveness, rotated detectors are not widely employed. Annotating rotated bounding boxes is such a laborious process that they are not provided in many detection datasets where axis-aligned annotations are used instead. In this paper, we propose a framework that allows the model to predict precise rotated boxes only requiring cheaper axis-aligned annotation of the target dataset 1.
To achieve this, we leverage the fact that neural networks are capable of learning richer representation of the target domain than what is utilized by the task. The under-utilized representation can be exploited to address a more detailed task. Our framework combines task knowledge of an out-of-domain source dataset with stronger annotation and domain knowledge of the target dataset with weaker annotation. A novel assignment process and projection loss are used to en-able the co-training on the source and target datasets. As a result, the model is able to solve the more detailed task in the target domain, without additional computation overhead dur-ing inference. We extensively evaluate the method on various target datasets including fresh-produce dataset, HRSC2016 and SSDD. Results show that the proposed method consis-tently performs on par with the fully supervised approach. 1.

Introduction
Rotated detectors introduced in recent works [17, 20, 32] have received attention due to their outstanding performance for top view images [15, 33, 34]. They reduce the output am-biguity of elongated objects for downstream tasks making them superior to axis-aligned detectors in dense scenes with severe occlusions [18]. However, the rotated annotation is more expensive compared to axis-aligned annotation. Fur-Figure 1. KCR combines the task knowledge of a source dataset with stronger rotated annotation and the domain knowledge of the target dataset with weaker axis-aligned annotation, which enables the model to predict rotated detection on the target domain. thermore, popular 2D annotation tools such as Sagemaker
Groundtruth 2 and VGG app 3 do not support rotated bound-ing box annotations. As a result, many popular detection datasets only have axis-aligned annotations [3, 4, 11]. These problems reduces the potential scope of the implementation of rotated detectors. In this work, we introduce Knowledge
Combination to learn Rotated object detection, a training scheme that only requires cheaper axis-aligned annotation for the target dataset in order to predict rotated boxes.
Neural networks encode data into a latent space, which is then decoded to optimize the given task. The latent em-bedding is an abstract representation of the data, containing much richer information than the output [29]. Early works in deep learning show that the model implicitly learns to detect image features such as edges and corners [10, 12], which can be used for more detailed tasks if decoded properly. We believe decoding to a more precise task on the target do-main can be learnt via co-optimizing with a strongly labelled source dataset. We design a framework that combines task knowledge of rotated detection from a source dataset, and the domain knowledge of a disjoint class of objects in the target dataset with only axis-aligned annotation, as shown in Figure 1. This approach combines the advantage of both weakly-supervised learning and transfer learning.
We follow a design principal that the framework should
*Corresponding author. 1Code is available at: https://github.com/alanzty/KCR-Official 2https://aws.amazon.com/sagemaker/data-labeling/ 3https://www.robots.ox.ac.uk/ vgg/software/via/
maximize the target domain knowledge learnt by the model while minimizing the negative impact caused by weaker labels. This is achieved by co-training the source and tar-get dataset with projection losses and a novel assignment process. The design choices are validated through ablation studies. We conduct extensive experiments to demonstrate that our framework is robust to a large domain gap between source and target dataset. Therefore, box orientation can practically be learnt for free with KCR, due to the availabil-ity of free public source datasets such as DOTA [32] with rotated annotations. We show the efficacy of this method on a fresh-produce dataset with high density of objects and severe occlusions. The performance (AP50) gap between the pro-posed method, learning from weak axis-aligned boxes, and the fully-supervised model learning from strong rotated an-notation, reduces to only 3.2% for the challenging cucumber dataset. We apply the same framework to HRSC2016 [16] and SSDD [27] datasets to show that our method consis-tently performs on par with fully supervised models. The performance gap reduces to 1.0% for SSDD. We believe our approach can greatly increase the usage and impact of rotated object detectors. The source code will be publicly available for the community to save future annotation cost.
In summary, our main contributions are as follows: 1) We introduce a framework that combines task knowl-edge of a strongly labelled source dataset and domain knowledge of a weakly labelled target dataset. 2) We apply this method in 2D rotated detection task, en-abling the model to predict rotated bounding box with only axis-aligned annotation and verify the generality of the method with several datasets. 3) We demonstrate robustness of the framework to vari-ous domain gaps between source and target datasets.
Hence, box orientation can be learnt with no additional annotation cost in practical applications. 2.