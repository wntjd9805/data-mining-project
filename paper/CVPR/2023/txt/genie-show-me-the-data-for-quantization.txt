Abstract
Zero-shot quantization is a promising approach for de-veloping lightweight deep neural networks when data is in-accessible owing to various reasons, including cost and is-sues related to privacy. By exploiting the learned parame-ters (µ and σ) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowl-edge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be op-timized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the con-text of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantiza-tion scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Further-more, we propose a framework called GENIE that gener-ates data suited for quantization. With the data synthesized by GENIE, we can produce robust quantized models with-out real datasets, which is comparable to few-shot quanti-zation. We also propose a post-training quantization algo-rithm to enhance the performance of quantized models. By combining them, we can bridge the gap between zero-shot
∗Equal contribution. Correspondence to: dragwon.jeon@samsung.com and few-shot quantization while significantly improving the quantization performance compared to that of existing ap-proaches. In other words, we can obtain a unique state-of-the-art zero-shot quantization approach. The code is available at https://github.com/SamsungLabs/
Genie. 1.

Introduction
Quantization is an indispensable procedure for deploy-ing models in resource-constrained devices such as mobile phones. By representing tensors using a lower bit width and maintaining a dense format of tensors, quantization reduces a computing unit to a significantly smaller size compared to that achieved by other approaches (such as pruning and low-rank approximations) and facilitates massive data par-allelism with vector processing units. Most early studies uti-lized quantization-aware training (QAT) schemes [8, 23] to compress models, which requires the entire training dataset and takes as much time as training FP32 models. However, access to the entire dataset for quantizing models may not be possible in the real world or industry owing to a variety of reasons, including issues related to privacy preservation.
Thus, recent studies have emphasized post-training quanti-zation (PTQ) [12, 14, 17, 21] because it serves as a conve-nient method of producing high-quality quantized networks
Figure 2. Conceptual illustration of GENIE, which consists of two sub-modules: synthesizing data and quantizing models with only a small amount of unlabeled datasets or even in the absence of a dataset (including synthetic datasets). Be-cause PTQ can compress models within a few hours but shows comparable performance to QAT, PTQ is preferred over QAT in practical situations.
Zero-shot quantization (ZSQ) [4, 7, 19] is another re-search regime that synthesizes data to compress models without employing real datasets. Starting from DFQ [22], schemes for ZSQ gradually pay more attention to generat-ing elaborate replicas such that the distribution of interme-diate feature maps matches the statistics of the correspond-ing batch normalization layers. Although many studies have achieved significant advancement in regards to quantization in the absence of real data, most of them have relied on
QAT schemes that require task-specific loss, such as cross-entropy (CE) loss and Kullback–Leibler (KL) divergence
[16], which requires more than 10 hours to complete the quantization of ResNet-18 [10] on Nvidia V100.
Excluding the data used, ZSQ and few-shot quanti-zation1(FSQ) commonly utilize FP32-pre-trained models (teacher) to optimize quantized models (student) by distill-ing knowledge. It is possible that ZSQ and FSQ share the quantization algorithm regardless of whether the data are real or synthetic. We thus adopt an up-to-date PTQ scheme to ZSQ so that breaking away from the quantization scheme conventionally used in ZSQ and then completing quantiza-tion within a few hours. Based on the existing method, we propose a framework called GENIE2 that distill data suited for model quantization. We also suggest a novel quantiza-tion scheme, which is a sub-module of GENIE and avail-able for both FSQ and ZSQ. As in Figure 2, GENIE con-sists of two sub-modules: synthesizing data (GENIE-D) and quantizing models (GENIE-M). By combining them, we bridge the gap between ZSQ and FSQ while taking an ultra-step forward from existing approaches. In other words, we achieve a state-of-the-art result that is unique among ZSQ approaches.
Our contributions are summarized as follows:
• First, we propose a scheme for synthesizing datasets by combining the approaches related to generation and dis-tillation to take advantage of both approaches. 1This refers to post-training quantization with few real data 2Data generation scheme suited for quantization
• Second, we suggest a method to substitute convolution of stride n (n > 1) by swing convolution. By applying randomness, various spatial information can be utilized when distilling datasets.
• Finally, we propose a new quantization scheme as a sub-module of GENIE (available for both FSQ and ZSQ), which is a simple but effective method that jointly op-timizes quantization parameters. 2.