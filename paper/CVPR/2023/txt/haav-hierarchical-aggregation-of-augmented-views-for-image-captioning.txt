Abstract
A great deal of progress has been made in image caption-ing, driven by research into how to encode the image using pre-trained models. This includes visual encodings (e.g. im-age grid features or detected objects) and more recently textual encodings (e.g. image tags or text descriptions of image regions). As more advanced encodings are available and incorporated, it is natural to ask: how to efficiently and effectively leverage the heterogeneous set of encodings? In this paper, we propose to regard the encodings as augmented views of the input image. The image captioning model en-codes each view independently with a shared encoder ef-ficiently, and a contrastive loss is incorporated across the encoded views in a novel way to improve their representation quality and the model’s data efficiency. Our proposed hier-archical decoder then adaptively weighs the encoded views according to their effectiveness for caption generation by first aggregating within each view at the token level, and then across views at the view level. We demonstrate significant performance improvements of +5.6% CIDEr on MS-COCO and +12.9% CIDEr on Flickr30k compared to state of the arts, and conduct rigorous analyses to demonstrate the im-portance of each part of our design. 1.

Introduction
A large amount of progress has been made in vision-and-language (VL) tasks such as image captioning [1, 8], visual question answering [16, 23], and image-text retrieval.
For these tasks, recent methods [31, 36, 45, 61] observe that encoding the input image by an object detector [42] pre-trained on Visual Genome [30] into a set of detected objects is not sufficient. To provide information complementary to detected objects, recent works proposed to encode an in-put image by different pre-trained models and into different modalities, and achieve substantial performance improve-ment by combining these heterogeneous encodings. For example, some works encode from the visual perspective (e.g. stronger object detector pre-trained on a larger vocabu-Figure 1. HAAV, Hierarchical Aggregation of Augmented Views, for image captioning at the step of predicting the word “sofa”. First, heterogeneous views such as detected objects [4], image grid fea-tures [45], and text descriptions [31] are generated from the input image by existing methods. We propose to regard these views as augmentations of the input image, and independently encode each view by a shared transformer encoder efficiently. A contrastive loss is incorporated to improve the representation quality of heteroge-neous views. Finally, our proposed hierarchical decoder models the effectiveness of each view and adaptively weigh them according to their effectiveness for predicting the current word “sofa”. lary and datasets [61] or global image features [24]), while other works encode from the textual perspective (e.g. image tags [36] and text descriptions of image regions [31]).
Given the great success of incorporating various heteroge-neous encodings or “views”, one research question emerges naturally: how to efficiently and effectively leverage these heterogeneous views for caption generation? For efficiency, three factors are particularly important: computation, pa-rameter count, and label efficiency. State-of-art VL and
image captioning models are typically a transformer encoder-decoder model [51], which has undesirable quadratic compu-tational complexity with respect to the input sequence size.
Therefore, as more views are incorporated, each represented by a sequence of tokens, we should carefully manage the computation and model size. Moreover, on the medium-scale MS-COCO image captioning benchmark [8] (∼0.6M training samples), we should take label efficiency into con-sideration when training the data-hungry [11] transformer model to avoid negative effects such as overfitting. For ef-fectiveness, different views contain some shared and some complementary information of the input image. Therefore, it is important to model the effectiveness of views and adap-tively weigh them according to their effectiveness for pre-dicting each word. Take image captioning in Figure 1 as an example, when predicting the current word “sofa”, for the incomplete caption “black bags sitting on top of a ? ”, if say, the view of detected objects fails to detect sofa in the input image, the captioning model should down-weigh the less effective view of detected objects and rely on other more effective views that properly encode the information about sofa. With these considerations in mind, we propose
HAAV, Hierarchical Aggregation of Augmented Views. In
HAAV, given a set of heterogeneous views of the input im-age from existing works such as detected objects [4], image grid features [45], and text descriptions [31], we propose to (1) regard heterogeneous views as augmentations of the input image, and (2) devise a hierarchical decoder layer to account for the effectiveness of heterogeneous views.
For (1), by regarding views as augmentations, we natu-rally choose to use a shared transformer encoder to encode each view independently. Compared to methods that con-catenate all views into a long sequence as input [31, 36, 61], where the computational complexity scales up quadratically with respect to the number of views, our method scales up linearly. Compared to models that use entire models per view [10, 22, 26, 43] or methods that encode each view with unshared encoders [2, 3, 20, 34], our method is more parame-ter efficient. Furthermore, data augmentation increases data diversity and thus improves data efficiency, which is particu-larly important for training data-hungry transformer models.
Last but not least, by regarding heterogeneous views gener-ated by different pre-trained models as augmentations of the input image, we incorporate a contrastive loss in a novel way to help representation learning of heterogeneous views and increase data efficiency [6, 15, 17]. Different from how other
VL methods [25,34,41,57,59] incorporate a contrastive loss, our formulation does not require annotated pairs (e.g. human annotated image-caption pairs in MS-COCO or image-text pairs scraped from the internet [44]) and can work with unlabeled image-only data to achieve better performance.
Also crucially, for (2) we devise a hierarchical decoder layer, which modifies the standard transformer decoder layer by introducing two-tiered cross-attention modules. The hi-erarchical decoder first aggregates within each view at the token level and then aggregates across views at the view level. By introducing this hierarchical aggregating struc-ture, we can better model the effectiveness of views and adaptively weigh them according to their effectiveness. For example, in Section 4 Experiment, we show that if we add noise to a certain view or mask out a prominent region of the input image, the proposed hierarchical decoder indeed down-weigh the noised and masked view when generating words and captions.
To sum up, in this paper, given a set of heterogeneous views of the input image from existing works, we focus on how to efficiently and effectively leverage these views and make the following contributions: (1) regard heterogeneous views as augmentations of the input image and propose a novel use of contrastive loss to improve computation, param-eter, and data efficiency; (2) devise a hierarchical decoder layer to model the effectiveness of each view and weigh each view accordingly for caption generation; (3) achieve significant improvement of +5.6% CIDEr on MS-COCO over state of the art, and achieve comparable or often better performance compared with methods using large-scale trans-former pre-training even though we do not do so; and (4) provide thorough ablations and rigorous analyses to validate our proposed method for efficiency and effectiveness. 2.