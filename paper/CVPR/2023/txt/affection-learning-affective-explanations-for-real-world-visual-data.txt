Abstract
In this work, we explore the space of emotional reac-tions induced by real-world images. For this, we first in-troduce a large-scale dataset that contains both categorical emotional reactions and free-form textual explanations for 85,007 publicly available images, analyzed by 6,283 annota-tors who were asked to indicate and explain how and why they felt when observing a particular image, with a total of 526,749 responses. Although emotional reactions are subjective and sensitive to context (personal mood, social status, past experiences) – we show that there is significant common ground to capture emotional responses with a large support in the subject population. In light of this observa-tion, we ask the following questions: i) Can we develop neural networks that provide plausible affective responses to real-world visual data explained with language? ii) Can we steer such methods towards producing explanations with varying degrees of pragmatic language, justifying different emotional reactions by grounding them in the visual stimu-lus? Finally, iii) How to evaluate the performance of such methods for this novel task? In this work, we take the first steps in addressing all of these questions, paving the way for more human-centric and emotionally-aware image anal-ysis systems. Our code and data are publicly available at https://affective-explanations.org. 1.

Introduction
A central goal of computer vision has been to gain a se-mantic understanding of visual stimuli [17, 78]. But what exactly do we mean by this understanding? The vast ma-jority of existing image analysis systems focus solely on image content [17]. Although models aimed at objective image analysis and captioning have achieved unprecedented success during the past years [70, 73], they largely ignore the more subtle and complex interactions that might exist between the image and its potential viewer.
In this work, our primary goal is to take a step toward a more viewer-centered understanding going beyond factual image analysis by incorporating the effect that an image might have on a viewer. To capture this effect, we argue that emotional responses provide a fundamental link between the visual world and human experience. We thus aim to understand what kinds of emotions a given image can elicit to different viewers and, most importantly, why?.
Emotion perception and recognition are influenced by and integrate many factors, from neurophysiological to cul-tural, from previous subjective experiences to social and even political context [41]. Thus, capturing and potentially reproducing plausible emotional responses to visual stimuli is significantly more challenging than standard image analy-sis, as it also involves an inherently subjective perspective, which is at the core of perception and consciousness [26].
To proceed with the goal of establishing a novel approach to affective analysis of real-world images, we leverage the fact that free-form language provides the simplest access to emotional expressions [60]. Thus, inspired by recent ad-vances in affective captioning of art-works [7], we study emotional responses induced by real-world visual data in conjunction with human-provided explanations. This ap-proach links emotions with linguistic constructs, which cru-cially are easier to curate at scale compared to other me-dia (e.g., fMRI scans). Put together, our work expands on the recent effort of Achlioptas et al. [7] by considering a visio-linguistic and emotion analysis across a large set of real-world images, not only restricted to visual art.
Our main contributions to this end are two-fold: first, we curate a large-scale collection of 526,749 explanations justifying emotions experienced at the sight of 85,007 dif-ferent real-world images selected from five public datasets.
The collected explanations are given by 6,283 annotators spanning many different opinions, personalities, and tastes.
The resulting dataset, which we term Affection, is very rich in visual and linguistic variations, capturing a wide vari-ety of both the underlying real-world depicted phenomena and their emotional effect. Second, we perform a linguistic and emotion-centric analysis of the dataset and, most impor-tantly, use it to produce deep neural listeners and speakers trained to comprehend, or generate plausible samples of visually grounded explanations for emotional reactions to images. Despite the aforementioned subjectivity and thus the more challenging nature of these tasks compared to purely descriptive visio-linguistic tasks (e.g., COCO-based caption-ing [18]), our methods appear to learn common biases of how people react emotionally, e.g., the presence of a shark is much more likely to raise fear than the presence of a peace-fully sleeping dog. Such common sense expectations are well captured in Affection, which is why we believe even black-box approaches like ours show promising results.
Finally, we explore variants of trained affective neural captioning systems, which allow some control on both the captured emotion and the level of factual visual details that are used when providing an explanation (e.g., ‘The sky looks beautiful’ to ‘The blue colors of the sky and the sea in this sunset make me happy’). Interestingly, we demonstrate that the pragmatic variant demonstrates richer and more diverse language across different images.
In summary, this work introduces new task, termed Af-fective Explanation Captioning (AEC) for real-world im-ages. To tackle AEC we release a new large-scale datased,
Affection, capturing 526,749 emotional reactions and expla-nations. We then design a variety of components, including, neural speakers that enable affective captioning, with vari-ous degrees of pragmatic and emotional control over their generations. Finally, all our neural speakers show strong per-formance on emotional Turing tests, where humans find their humans find their generations ∼60%-65% of the time likely to be uttered by other humans supporting rich discriminative references contained in Affection’s explanations. 2.