Abstract
Recent semi-supervised semantic segmentation methods combine pseudo labeling and consistency regularization to enhance model generalization from perturbation-invariant training.
In this work, we argue that adequate supervi-sion can be extracted directly from the geometry of fea-ture space.
Inspired by density-based unsupervised clus-tering, we propose to leverage feature density to locate sparse regions within feature clusters defined by label and pseudo labels. The hypothesis is that lower-density fea-tures tend to be under-trained compared with those densely gathered. Therefore, we propose to apply regularization on the structure of the cluster by tackling the sparsity to in-crease intra-class compactness in feature space. With this goal, we present a Density-Guided Contrastive Learning (DGCL) strategy to push anchor features in sparse regions toward cluster centers approximated by high-density posi-tive keys. The heart of our method is to estimate feature density which is defined as neighbor compactness. We de-sign a multi-scale density estimation module to obtain the density from multiple nearest-neighbor graphs for robust density modeling. Moreover, a unified training framework is proposed to combine label-guided self-training and density-guided geometry regularization to form complementary su-pervision on unlabeled data. Experimental results on PAS-CAL VOC and Cityscapes under various semi-supervised settings demonstrate that our proposed method achieves state-of-the-art performances.
The project is available at https://github.com/Gavinwxy/DGCL. 1.

Introduction
Semantic segmentation, as an essential computer vision task, has seen significant advances along with the rise of deep learning [4, 30, 46]. Nevertheless, training segmenta-tion models requires massive pixel-level annotations which can be time-consuming and laborious to obtain. Therefore,
*Corresponding author.
Figure 1. Illustration of feature density within clusters. (a) Pixel-level features of 5 classes extracted from PASCAL VOC 2012 [12] with prediction confidence over 0.95. (b) Feature density esti-mated by the averaged distance of 16 nearest neighbor features. semi-supervised learning is introduced in semantic segmen-tation and is drawing growing interest. It aims to design a label-efficient training scheme with limited annotated data to allow better model generalization by leveraging addi-tional unlabeled images.
The key in semi-supervised semantic segmentation lies in mining extra supervision from unlabeled samples. Re-cent studies focus on learning consistency-based regular-ization [13, 29, 32, 51] and designing self-training pipelines
[15, 19, 43, 44]. Inspired by the advances in representation learning [17,24], another line of works [28,40,47–49] intro-duce contrastive learning on pixel-level features to enhance inter-class separability. Though previous works have shown effectiveness, their label-guided learning scheme solely re-lies on classifier knowledge, while the structure information of feature space is under-explored. In this work, we argue that effective supervision can be extracted from the geome-try of feature clusters to complement label supervision.
Feature density, measured by local compactness, has shown its potential to reveal feature patterns in unsuper-vised clustering algorithms such as DBSCAN [11]. The density-peak assumption [33] states that cluster centers are more likely located in dense regions. Inversely, features in
sparse areas tend to be less representative within the clus-ter, so they require extra attention. The sparsity exists even within features that are confidently predicted by the classi-fier. As shown in Fig. 1, pixel-level features are extracted on labeled and unlabeled images from 5 classes in PASCAL
VOC 2012 dataset, all with high prediction confidence. Ev-ident density variation still exists within each cluster, indi-cating varying learning difficulty among features, which the classifier fails to capture.
In this work, we propose a learning strategy named
Density-Guided Contrastive Learning (DGCL) to mine ef-fective supervision from cluster structure of unlabeled data.
Specifically, we initialize categorical clusters based on la-beled features and enrich them with unlabeled features which are confidently predicted. Then, sparsity hunting is conducted in each in-class feature cluster to locate low-density features as anchors. Meanwhile, features in dense regions are selected to approximate the cluster centers and serve as the positive keys. Then, feature contrast is applied to push the anchors toward their positive keys, explicitly shrinking sparse regions to enforce more compact clusters.
The core of our method is feature density estimation. We measure local density by the average distance between the target feature and its nearest neighbors. For robust esti-mation, categorical memory banks are proposed to break the limitation on mini-batch, so in-class density can be es-timated in a feature-to-bank style where class distribution can be approximated globally. When building the nearest neighbor graph, densities estimated by fewer neighbors tend to focus on the local region, which prevents capturing true cluster centers. On the other hand, graphs with too many neighbors cause over-smoothed estimation, which harms accurate sparsity mining. Therefore, we propose multi-scale nearest neighbor graphs to determine the final density by combining estimations from graphs of different sizes.
We evaluate the proposed method on PASCAL VOC 2012 [12] and Cityscapes [8] under various semi-supervised settings, where our approach achieves state-of-the-art per-formances. Our contributions are summarized as follows:
• We propose a density-guided contrastive learning strat-egy to tackle semi-supervised semantic segmentation by mining effective supervision from the geometry in feature space.
• We propose a multi-scale density estimation module combined with dynamic memory banks to capture fea-ture density robustly.
• We propose a unified learning framework consisting of label-guided self-training and density-guided fea-ture learning, in which two schemes complement each other. Experiments show that our method achieves state-of-the-art performances. 2.