Abstract
We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan-guage. In the original Localized Narratives [36], annota-tors speak and move their mouse simultaneously on an im-age, thus grounding each word with a mouse trace segment.
However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Local-ized Narratives, capturing even complex events involving multiple actors interacting with each other and with sev-eral passive objects. We annotated 20k videos of the OVIS,
UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models.
Our annotations are available at https://google. github.io/video-localized-narratives/. 1.

Introduction
Vision and language is a very active research area, which experienced much exciting progress recently [1, 28, 38, 39, 48]. At the heart of many developments lie datasets con-necting still images to captions, while grounding some of the words in the caption to regions in the image, made with a variety of protocols [6,20,23,25,29,34,36,44,46,56]. The recent Localized Narratives (ImLNs) [36] offer a particu-larly attractive solution: the annotators describe an image with their voice while simultaneously moving their mouse over the regions they are describing. Speaking is natural and efﬁcient, resulting in long captions that describe the whole scene. Moreover, the synchronization between the voice and mouse pointer yields dense visual grounding for every word. Yet, still images only show one instant in time. Anno-tating videos would be even more interesting, as they show entire stories, featuring a ﬂow of events involving multiple actors and objects interacting with each other.
Directly extending ImLNs to video by letting the anno-tator move their mouse and talk while the video is playing would lead to a “race against time”, likely resulting in fol-lowing only one salient object. In this paper, we propose a better annotation protocol which allows the annotator to tell the story of the video in a calm environment (Fig. 1-3).
The annotators ﬁrst watch the video carefully, identify the main actors (“man”, “ostrich”), and select a few represen-tative key-frames for each actor. Then, for each actor sep-arately, the annotators tell a story: describing the events it is involved in using their voice, while moving the mouse on the key-frames over the objects and actions they are talking about. The annotators mention the actor name, its attributes, and especially the actions it performs, both on other ac-tors (e.g., “play with the ostrich”) and on passive objects (e.g., “grabs the cup of food”). For completeness, the an-notators also brieﬂy describe the background in a separate step (bottom row). Working on keyframes avoids the race against time, and producing a separate narration for each ac-tor enables disentangling situations and thus cleanly captur-ing even complex events involving multiple actors interact-ing with each other and with several passive objects. As in
ImLN, this protocol localizes each word with a mouse trace segment. We take several additional measures to obtain ac-curate localizations, beyond what was achieved in [36].
We annotated the OVIS [37], UVO [47], and Oops [12] datasets with Video Localized Narratives (VidLNs). These datasets cover a general domain, as opposed to only cook-ing [10, 62] or ﬁrst-person videos [16]. Moreover, these videos contain complex scenes with interactions between multiple actors and passive objects, leading to interesting stories that are captured by our rich annotations.
In to-tal our annotations span 20k videos, 72k actors, and 1.7 million words. On average, each narrative features tran-scriptions for 3.5 actors, totaling 75.1 words, including 23.0 nouns, 9.5 verbs, and 8.5 adjectives (Fig. 4). Our analysis demonstrates the high quality of the data. The text descrip-tions almost always mention objects/actions that are actu-ally present in the video, and the mouse traces do lie inside their corresponding object in most cases.
The richness of our VidLNs makes them a strong ba-sis for several tasks. We demonstrate this by creating new benchmarks for the tasks of Video Narrative Grounding (VNG) and Video Question Answering (VideoQA). The new VNG task requires a method to localize the nouns in
Figure 1. The ﬁve steps of the annotation process for VidLNs. We are able to cleanly describe even complex interactions and events by disentangling the storylines of different actors.
Figure 2. An example VidLN annotation. Each row shows the story of the video from the perspective of a different actor. Each mouse trace segment drawn on selected key-frames localizes the word highlighted in the same color. More examples are in the supplement. an input narrative with a segmentation mask on the video frames. This is a complex task that goes beyond open-vocabulary semantic segmentation [14], as often the text has multiple identical nouns that need to be disambiguated us-ing the context provided by other words. We construct two
VNG benchmarks on OVIS and UVO, with a total of 8k videos involving 45k objects. We also build baseline mod-els that explicitly attempt this disambiguation and report ex-periments showing it’s beneﬁcial on this dataset, while also highlighting these new benchmarks are far from solved.
For VideoQA, we construct a benchmark on the Oops dataset, consisting of text-output questions (e.g., “What is the person in the blue suit doing?”), where the answer is free-form text (e.g., “paragliding”), and location-output questions (e.g., “Where is the woman that is wearing a black and white dress?”), where the answer is a spatio-temporal location in the video. The benchmark features a total of 62k questions on 9.5k videos. Answering many of these ques-tions requires a deep understanding of the whole video. We also implement baseline methods to establish initial results on this task. 2.