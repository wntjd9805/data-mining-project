Abstract
Controllability, generalizability and efﬁciency are the major objectives of constructing face avatars represented by neural implicit ﬁeld. However, existing methods have not managed to accommodate the three requirements si-multaneously. They either focus on static portraits, restrict-ing the representation ability to a speciﬁc subject, or suffer from substantial computational cost, limiting their ﬂexibil-ity. In this paper, we propose One-shot Talking face Avatar (OTAvatar), which constructs face avatars by a general-ized controllable tri-plane rendering solution so that each personalized avatar can be constructed from only one por-trait as the reference. Speciﬁcally, OTAvatar ﬁrst inverts a portrait image to a motion-free identity code. Second, the identity code and a motion code are utilized to modulate an efﬁcient CNN to generate a tri-plane formulated volume, which encodes the subject in the desired motion. Finally, volume rendering is employed to generate an image in any view. The core of our solution is a novel decoupling-by-inverting strategy that disentangles identity and motion in the latent code via optimization-based inversion. Beneﬁt-ing from the efﬁcient tri-plane representation, we achieve controllable rendering of generalized face avatar at 35 FPS on A100. Experiments show promising performance of cross-identity reenactment on subjects out of the training set and better 3D consistency. The code is available at https://github.com/theEricMa/OTAvatar. 1.

Introduction
Neural rendering has achieved remarkable progress and promising results in 3D reconstruction. Thanks to the dif-ferentiability in neuron computation, the neural rendering
*Equal contribution.
†Corresponding author.
Single 
Reference
OTAvatar
ID Motion
Convs
Figure 1. OTAvatar animation results. The source subjects in
HDTF [43] dataset are animated by OTAvatar using a single por-trait as the reference. We use the pose and expression coefﬁcients of 3DMM to represent motion and drive the avatar. Note that these subjects are not included in the training data of OTAvatar. methods bypass the expensive cost of high-ﬁdelity digi-tal 3D modeling, thus attracting the attention of many re-searchers from academia and industry. In this work, we aim to generate a talking face avatar via neural rendering tech-niques, which is controllable by driving signals like video and audio segments. Such animation is expected to inherit the identity from reference portraits, while the expression and pose can keep in sync with the driving signals.
The early works about talking face focus on expression animation consistent with driving signals in constrained frontal face images [5, 7, 27]. It is then extended to in-the-[18, 35, 46, 47]. Many wild scenes with pose variations
previous works are 2D methods, where the image warping adopted to stimulate the motion in 3D space is learned in 2D space [29, 31, 32, 36, 40]. These methods tend to col-lapse under large pose variation. In contrast, there are 3D methods that can address the pose problem, since the head pose can be treated as a novel view. Both explicit [11, 20] and implicit 3D representations [23] are introduced to face rendering [15, 17, 21, 22, 25, 28, 45]. However, these meth-ods either overﬁt to a single identity or fail to produce high-quality animation for different identities.
In this work, we propose a one-shot talking face avatar (OTAvatar), which can generate mimic expressions with good 3D consistency and be generalized to different iden-tities with only one portrait reference. Some avatar anima-tion examples are shown in Fig. 1. Given a single reference image, OTAvatar can drive the subject with motion signals to generate corresponding face images. We realize it un-der the framework of volume rendering. Current methods usually render static subjects [22, 28]. Although there are works [12, 15, 25, 26, 45] proposed to implement dynamic rendering for face avatars, they need one model per subject.
Therefore, the generalization is poor. HeadNeRF [17] is a similar work to us. However, its reconstruction results are unnatural in the talking face case.
Our method is built on a 3D generative model pre-trained on a large-scale face database to guarantee identity general-ization ability. Besides, we employ a motion controller to decouple the motion and identity in latent space when per-forming the optimization-based GAN inversion, to make the motion controllable and transferable to different identities.
The network architecture is compact to ensure inference ef-ﬁciency. In the utilization of our OTAvatar, given a single reference image, we ﬁx the motion code predicted by the controller and only optimize the identity code so that a new avatar of the reference image can be constructed. Such a disentanglement enables the rendering of any desired mo-tion by simply alternating the motion-related latent code to be that of the speciﬁc motion representation.
The major contributions of this work can be summarized as follows.
• We make the ﬁrst attempt for one-shot 3D face recon-struction and motion-controllable rendering by taming a pre-trained 3D generative model for motion control.
• We propose to decouple motion-related and motion-free latent code in inversion optimization by prompt-ing the motion fraction of latent code ahead of the op-timization using a decoupling-by-inverting strategy.
• Our method can photo-realistically render any identity with the desired expression and pose at 35FPS. The experiment shows promising results of natural motion and 3D consistency on both 2D and 3D datasets. 2.