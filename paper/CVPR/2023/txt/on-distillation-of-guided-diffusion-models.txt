Abstract
Classiﬁer-free guided diffusion models have recently been shown to be highly effective at high-resolution image genera-tion, and they have been widely used in large-scale diffusion
*Work partially done during an internship at Google frameworks including DALL
E 2, Stable Diffusion and Ima-· gen. However, a downside of classiﬁer-free guided diffusion models is that they are computationally expensive at infer-ence time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we pro-pose an approach to distilling classiﬁer-free guided diffusion models into models that are fast to sample from: Given a pre-trained classiﬁer-free guided model, we ﬁrst learn a sin-gle model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10, achieving
FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-ﬁdelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2-4 denoising steps. 1.

Introduction
·
Denoising diffusion probabilistic models (DDPMs) [4,37, 39, 40] have achieved state-of-the-art performance on image generation [22, 26–28, 31], audio synthesis [11], molecular generation [44], and likelihood estimation [10]. Classiﬁer-free guidance [6] further improves the sample quality of diffusion models and has been widely used in large-scale diffusion model frameworks including GLIDE [23], Stable
Diffusion [28], DALL
E 2 [26], and Imagen [31]. How-ever, one key limitation of classiﬁer-free guidance is its low sampling efﬁciency—it requires evaluating two diffusion models tens to hundreds of times to generate one sample.
This limitation has hindered the application of classiﬁer-free guidance models in real-world settings. Although distillation approaches have been proposed for diffusion models [33,38], these approaches are not directly applicable to classiﬁer-free guided diffusion models. To deal with this issue, we propose a two-stage distillation approach to improving the sampling efﬁciency of classiﬁer-free guided models. In the ﬁrst stage, we introduce a single student model to match the combined output of the two diffusion models of the teacher. In the sec-ond stage, we progressively distill the model learned from the ﬁrst stage to a fewer-step model using the approach intro-duced in [33]. Using our approach, a single distilled model is able to handle a wide range of different guidance strengths, allowing for the trade-off between sample quality and di-versity efﬁciently. To sample from our model, we consider existing deterministic samplers in the literature [33, 38] and further propose a stochastic sampling process.
Our distillation framework can not only be applied to stan-dard diffusion models trained on the pixel-space [4, 36, 39], but also diffusion models trained on the latent-space of an au-toencoder [28,35] (e.g., Stable Diffusion [28]). For diffusion models directly trained on the pixel-space, our experiments on ImageNet 64x64 and CIFAR-10 show that the proposed distilled model can generate samples visually comparable to that of the teacher using only 4 steps and is able to achieve comparable FID/IS scores as the teacher model using as few as 4 to 16 steps on a wide range of guidance strengths (see
Fig. 2). For diffusion model trained on the latent-space of an encoder [28, 35], our approach is able to achieve comparable visual quality to the base model using as few as 1 to 4 sam-fewer steps than the base model) pling steps (at least 10 on ImageNet 256 512, matching the 256 and LAION 512 performance of the teacher (as evaluated by FID) with only 2-4 sampling steps. To the best of our knowledge, our work is the ﬁrst to demonstrate the effectiveness of distillation for both pixel-space and latent-space classiﬁer-free diffusion models. Finally, we apply our method to text-guided image inpainting and text-guided image editing tasks [20], where we reduce the total number of sampling steps to as few as 2-4 steps, demonstrating the potential of the proposed framework in style-transfer and image-editing applications [20, 41].
⇥
⇥
⇥
Deterministic 1-step 8-step
! = 0
! = 1
! = 2
! = 4
Figure 2. Class-conditional samples from our two-stage (determin-istic) approach on ImageNet 64x64 for diffusion models trained on the pixel-space. By varying the guidance weight w, our distilled model is able to trade-off between sample diversity and quality, while achieving good results using as few as one sampling step. 2.