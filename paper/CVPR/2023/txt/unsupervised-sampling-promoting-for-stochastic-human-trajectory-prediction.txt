Abstract
The indeterminate nature of human motion requires tra-jectory prediction systems to use a probabilistic model to formulate the multi-modality phenomenon and infer a finite set of future trajectories. However, the inference processes of most existing methods rely on Monte Carlo random sam-pling, which is insufficient to cover the realistic paths with finite samples, due to the long tail effect of the predicted distribution. To promote the sampling process of stochastic prediction, we propose a novel method, called BOsampler
, to adaptively mine potential paths with Bayesian optimiza-tion in an unsupervised manner, as a sequential design strat-egy in which new prediction is dependent on the previously drawn samples. Specifically, we model the trajectory sam-pling as a Gaussian process and construct an acquisition function to measure the potential sampling value. This ac-quisition function applies the original distribution as prior and encourages exploring paths in the long-tail region. This sampling method can be integrated with existing stochastic predictive models without retraining. Experimental results on various baseline methods demonstrate the effectiveness of our method. The source code is released in this link. 1.

Introduction
Humans usually behave indeterminately due to intrinsic intention changes or external surrounding influences.
It requires human trajectory forecasting systems to formulate humans’ multimodality nature and infer not a single future state but the full range of plausible ones [16, 32].
Facing this challenge, many prior methods formulate stochastic human trajectory prediction as a generative prob-lem, in which a latent random variable is used to represent multimodality. A typical category of methods [10, 18, 46, 66] is based on generative adversarial networks (GANs), which generate possible future trajectories by a noise in the multi-modal distribution. Another category exploits the variational auto-encoder (VAE) [21,26,30,41,50] that uses the observed
*Authors contributed equally and are listed alphabetically by first name.
Figure 1. The comparison of different sampling methods. Monte
Carlo (MC) sampling generates trajectories by directly sampling from a prior distribution of latent variable z. Quasi-Monte Carlo (QMC) sampling uses a transformation from low-discrepancy se-quences to the prior distribution [36] to sample more uniformly than MC. Different from MC and QMC, BOsampler formulates the sampling process as a Gaussian Process and calculate the Gaus-sian posterior with existing samples to sample the next one, where sampling and posterior updating are iterative. history trajectories as a condition to learn the latent variable.
Beyond these two mainstream categories, other generative models are also employed for trajectory prediction, such as diffusion model [16], normalized flow [39], and even simple
Gaussian model [35, 43].
Instead of a single prediction, the inference process of these stochastic prediction methods produces a finite set of plausible future trajectories by Monte Carlo (MC) random sampling. However, the distributions are always uneven and biased, where the common choices like “go straight” are in high probability. In contrast, many other choices such as
“turn left/right” and “U-turn” are in low probability. Due to the long tail effect of predicted distribution, finite samples are insufficient to cover the realistic paths. For example, as shown in Figure 1, MC sampling tends to generate redundant trajectories with high probability but ignores the potential low-probability choice. To solve this problem, some meth-ods [4, 31] trained the model using an objective term to
increase the diversity of samples, e.g., maximizing the dis-tance among the predicted samples. Though improving the sampling diversity, these methods need to re-train the model by adding the loss term. It is timely-cost and may fail when only the model is given (the source data is inaccessible).
In this paper, we propose an unsupervised method to pro-mote the sampling process of stochastic prediction without accessing the source data. It is named BOsampler , which refines the sampling for more exploration via Bayesian opti-mization (BO). Specifically, we first formulate the sampling process as a Gaussian Process (GP), where the posterior is conditioned by previous sampling trajectories. Then, we de-fine an acquisition function to measure the value of potential samples, where the samples fitting the trained distribution well or away from existing samplings obtain high values.
By this acquisition function, we can encourage the model to explore paths in the long-tail region and achieve a trade-off between accuracy and diversity. As shown in Figure 1, we compare BOsampler with MC and another sampling method QMC [4], which first generates a set of latent vari-ables from a uniform space and then transfers it to prior distribution for trajectory sampling. Compared with them,
BOsampler can adaptively update the Gaussian posterior based on existing samples, which is more flexible. We high-light that BOsampler serves as a plug-and-play module that could be integrated with existing multi-modal stochastic predictive models to promote the sampling process without retraining. In the experiments, we apply the BOsampler on many popular baseline methods, including Social GAN [18],
PECNet [33], Trajectron++ [41], and Social-STGCNN [35], and evaluate them on the ETH-UCY datasets. The main contributions of this paper are summarized as follows:
• We present an unsupervised sampling prompting method for stochastic trajectory prediction, which mines potential plausible paths with Bayesian optimiza-tion adaptively and sequentially.
• The proposed method can be integrated with existing stochastic predictors without retraining.
• We evaluate the method with multiple baseline methods and show significant improvements. 2.