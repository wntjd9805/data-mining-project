Abstract 3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar ap-pearance patterns between 2 hands, ill-posed joint posi-tion mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture in-teracting hands’ local fine details and global articulated clues among joints jointly. To this end, A2J is evolved un-der Transformer’s non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over
A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better cap-ture joints’ articulation clues for resisting occlusion. Sec-ondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advance-ment in 2-hand case) and can also be applied to depth domain with strong generalization. The code is avaliable at https://github.com/ChanglongJiangGit/
A2J-Transformer.
†Yang Xiao is corresponding author(Yang Xiao@hust.edu.cn).
Figure 1. The main idea of A2J-Transformer. 3D anchors are uniformly set and act as local regressors to predict each hand joint. Meanwhile, they are also used as queries, and the interaction among them is established to acquire global context. 1.

Introduction 3D interacting hand pose estimation from a single RGB image can be widely applied to the fields of virtual reality, augmented reality, human-computer interaction, etc.. [32, 34, 37]. Although the paid efforts, it still remains as a challenging research task due to the main issues of serious self-occlusion and inter-occlusion towards hands [7, 12, 16, 22, 27], confusing similar appearance patterns between 2 hands [12, 19, 27], and the ill-posed characteristics of esti-mating 3D hand pose via monocular RGB image [7,16,28].
The existing methods can be generally categorized into model-based [1,2,21,29,30,35,39,41,48] and model-free [5, 7,12,17,19,22,26,27,29,43] groups. Due to model’s strong prior knowledge on hands, the former paradigm is over-all of more promising performance. However, model-based methods generally require complex personalized model cal-ibration, which is sensitive to initialization and susceptible
to trap in local minima [11, 12]. This is actually not pre-ferred by the practical applications. Accordingly, we focus on model-free manner in regression way. The key idea is that, for effective 3D interacting hand pose estimation the predictor should be well aware of joints’ local fine details and global articulated context simultaneously to resist oc-clusion and confusing appearance pattern issues. To this end, we propose to extend the SOTA depth-based single hand 3D pose estimation method A2J [43] to 3D interact-ing hand pose estimation task from a single RGB image. intuitively applying it
Although A2J’s superiority with ensemble local regres-sion, to our task cannot ensure promising performance, since it generally suffers from 3 main defects as below. First, the local anchor points for predicting offsets between them and joints lack interaction among each other. This leads to the fact that, joints’ global articulated clues cannot be well captured to resist occlusion.
Secondly, the anchor points within the certain spatial range share the same single-scale local convolution feature, which essentially limits the discrimination capacity on confusing visual patterns towards the interacting hands. Last, anchor points locate within 2D plane, which is not optimal for al-leviating the ill-posed 2D to 3D lifting problem with single
RGB image. To address these, we propose to extend A2J un-der Transformer’s non-local encoding-decoding framework to build A2J-Transformer, with anchor point-wise adaptive multi-scale feature learning and 3D anchor point setup.
Particularly, the anchor point within A2J is evolved as the learnable query under Transformer framework. Each query will predict its position offsets to all the joints of the 2 hands. Joint’s position is finally estimated via fusing the prediction results from all queries in a linear weight-ing way. That is to say, joint’s position is determined by all the queries located over the whole image of global spa-tial perspective. Meanwhile, the setting query number is flexible, which is not strictly constrained by joint number as in [12]. Thanks to Transformer’s non-local self-attention mechanism [40], during feature encoding stage the queries can interact with each other to capture joints’ global ar-ticulated clues, which is essentially beneficial for resisting self-occlusion and inter-occlusion. Concerning the specific query, adaptive local feature learning will be conducted to extract query-wise multi-scale convolutional feature based
Resnet-50 [14]. Compared with A2J’s feature sharing strat-egy among the neighboring anchor points, our proposition can essentially facilitate query’s pattern fitting capacity both for accurate joint localization and joint’s hand identity ver-ification. In summary, each query will be of strong local-global spatial awareness ability to better fit interacting hand appearance pattern. Meanwhile to facilitate RGB-based 2D to 3D hand pose lifting problem, the queries will be set within the 3D space instead of 2D counterpart as in
A2J [43]. In this way, each query can directly predict its 3D position offset between the joints, which cannot be ac-quired by A2J. Overall, A2J-Transformer’s main research idea is shown in Fig. 1.
Compared with the most recently proposed model-free method [12] that also addresses 3D interacting hand pose estimation using Transformer, our proposition still takes some essential advantages. First, joint-like keypoint detec-tion is not required. Secondly, query number is not strictly constrained to be equal to joint number to facilitate pattern fitting capacity. Thirdly, our query locates within 3D space instead of 2D counterpart.
The experiments on the challenging Interhand 2.6M [29] dataset verify that, our approach can achieve the state-of-the-art model-free performance (3.38mm MPJPE advance-ment in 2-hand case) for 3D interacting hand pose estima-tion from a single RGB image. And, it significantly outper-forms A2J by large margins (i.e., over 5mm on MPJPE). In addition, experiments on HANDS2017 dataset [46] demon-strate that A2J-Transformer can also be applied to depth do-main with promising performance.
Overall, the main contributions of this paper include:
• For the first time, we extend A2J from depth domain to RGB domain to address 3D interacting hand pose estima-tion from a single RGB image with promising performance;
• A2J’s anchor point is evolved with Transformer’s non-local self-attention mechanism with adaptive local feature learning, to make it be aware of joints’ local fine details and global articulated context simultaneously;
• Anchor point is proposed to locate within 3D space to facilitate ill-posed 2D to 3D hand pose lifting problem based on monocular RGB information. 2.