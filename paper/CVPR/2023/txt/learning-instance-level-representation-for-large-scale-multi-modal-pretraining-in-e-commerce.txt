Abstract
This paper aims to establish a generic multi-modal foundation model that has the scalable capability to mas-sive downstream applications in E-commerce. Recently, large-scale vision-language pretraining approaches have achieved remarkable advances in the general domain.
However, due to the significant differences between natu-ral and product images, directly applying these frameworks for modeling image-level representations to E-commerce will be inevitably sub-optimal. To this end, we propose an instance-centric multi-modal pretraining paradigm called
ECLIP in this work. In detail, we craft a decoder archi-tecture that introduces a set of learnable instance queries to explicitly aggregate instance-level semantics. Moreover, to enable the model to focus on the desired product in-stance without reliance on expensive manual annotations, two specially configured pretext tasks are further proposed.
Pretrained on the 100 million E-commerce-related data,
ECLIP successfully extracts more generic, semantic-rich, and robust representations. Extensive experimental results show that, without further fine-tuning, ECLIP surpasses existing methods by a large margin on a broad range of downstream tasks, demonstrating the strong transferability to real-world E-commerce applications. 1.

Introduction
Nowadays, the flourishing growth of E-commerce has brought great convenience to people’s daily life. And a wide range of product-based application tasks has subsequently emerged, such as item classification [18, 29], product re-trieval [7, 35], commodity recommendation [21, 28], and so on. Compared to developing individual task-specific mod-els, building a general-purpose foundation model that works for massive E-commercial applications simultaneously can enhance applicability and reduce training costs.
*Corresponding Author.
Figure 1. Domain difference between natural and product im-ages. For natural images, it is the frequent case that most pixels are semantically correlated to the textual sentence. However, in
E-commerce, such correlation is much more sparse (e.g., “frying pan” or ”coffee machine” only occupy small portions of the entire images). Moreover, images for a product are often provided in a group from multiple sources such as (a) advertisement videos, (b) product pages, (c) customer comments (see the bottom examples).
Recent developments in vision-language pretraining (VLP) [9, 12, 17, 20, 31, 34] have demonstrated remarkable advances in diverse VL downstream tasks. Profiting from large-scale image-text pairs, these methods are able to learn generic multimodal representations that are reused across various tasks. In E-commerce scenario, the related data nat-urally contains cross-modal information to describe a cor-responding product. Motivated by the tremendous success achieved by VL modeling, several approaches [4,33,35,37] have made attempts at designing a commerce-specific mul-timodal representation learning paradigm. They imitate the existing VLP methods (e.g., CLIP [20], VilBERT [17]) to learn the image-level representations of the product via pre-training on abundant commerce image-text pairs.
Though promising results have been achieved, directly applying these VLP methods in the general domain to E-commerce still suffers from inherent deficiencies. The prop-erties of natural and product images appear to be dramati-cally different. Given a natural image-text pair, almost ev-ery pixel in the natural image is mentioned by the corre-sponding textual description. In contrast, as shown in Fig-ure 1, in a real E-commerce scenario, the images are mostly product-oriented. Only very few instances are related to the product description. Simply treating the whole image as a monolithic entity to perform cross-modal alignment with text will inevitably confound the foreground and noisy background. Hence, to establish a foundation model that generalizes well to diverse E-commerce applications, it is of great significance to learn the product-related instance-level representation. With this goal in mind, a crucial challenge needs to be addressed: How can we enable the model to fo-cus on the product instance in the presence of background interference?
A straightforward way to tackle this problem would be to resort to object-level human annotations, but it is labori-ous and infeasible to scale on larger data from the Internet.
In this work, we strive to derive the capability of ground-ing product instances from uncurated data. Our motiva-tion is built on the natural characteristics of E-commerce data itself. As illustrated in Figure 1, a product usually has multiple image samples from different sources (e.g., mer-chant, customer comments, attached advertisement videos, etc.). Although the appearance of these samples may be diverse due to the changes of camera view or scenes, they all include the identical product entity. This fact strongly spurs us to pursue an instance-centric multi-modal learning paradigm by leveraging such explicit correlation.
The proposed pretraining framework, dubbed as ECLIP (E for “E-commerce”), employs two separate encoders to embed the images and texts of products. Our key idea is to develop a decoder architecture built upon the above-mentioned encoders, which aims to aggregate the instance-centric product representations without additional hand-crafted annotation. Inspired by [1,15,30], the decoder intro-duces a set of learnable tokens that we refer to as instance query. At each decoder block, these instance queries are updated via interacting with the encoded visual features.
Through the stack of multiple blocks, they will gradually probe the potential product instance from the entire image.
Moreover, each instance query is conditioned on a concrete text or image called multi-modal prompt. Such a design renders it dedicated to a particular instance type indicated by the content of its associated prompt. Therefore, by spec-ifying the content of multi-modal prompt, the decoder can adaptively discover the corresponding instance. During pre-training, there is only one positive prompt for a given sam-ple. The rest are negative ones sampled from other products.
To effectively optimize the generated instance represen-inter-product tations, we newly craft two pretext tasks: and intra-product multi-modal learning. The first one is in charge of pulling the representations of the identical product closer to each other and pushing away the unmatched ones.
It is noteworthy that the appearance of the positive image samples varies a lot except for the presented product. Bring-ing their representations closer than negative pairs in the feature space will implicitly encourage the instance query to focus on the visual region that corresponds to the desired product. The second one aims to ensure that only positive queries can aggregate the semantics of the foreground in-stance, rather than negative ones. Coupling these two novel pretext tasks together, we find that the whole framework is capable of learning a generic product representation. Our core contributions can be summarized as follows: (1) We propose ECLIP, an effective and simple multi-modal representation learning paradigm in the E-commerce scenario. Going beyond regular global representations, it can successfully obtain instance-centric product representa-tions via a decoder architecture. (2) By fully exploiting the natural characteristics of E-commerce data and the proposed pretext tasks, ECLIP ob-tains the fine-grained alignment capability to ground the de-sired product instance (see Figure 4a) without reliance on any manual annotation. (3) Pre-trained on large-scale product data, the resulting foundation model can seamlessly generalize to downstream
E-commerce applications. Comprehensive experimental re-sults further demonstrate the superiority of ECLIP: without any fine-tuning, it achieves substantial improvements over the existing state-of-the-art methods on diverse real-world
E-commerce tasks. 2.