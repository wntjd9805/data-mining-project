Abstract
Dominant pre-training works for image-text retrieval adopt “dual-encoder” architecture to enable high effi-ciency, where two encoders are used to extract image and text representations and contrastive learning is employed for global alignment. However, coarse-grained global alignment ignores detailed semantic associations between image and text.
In this work, we propose a novel proxy task, named Visual-Language Error Modeling (ViLEM), to inject detailed image-text association into “dual-encoder” model by “proofreading” each word in the text against the corresponding image. Specifically, we first edit the image-paired text to automatically generate diverse plausible neg-ative texts with pre-trained language models. ViLEM then enforces the model to discriminate the correctness of each word in the plausible negative texts and further correct the wrong words via resorting to image information. Further-more, we propose a multi-granularity interaction frame-work to perform ViLEM via interacting text features with both global and local image features, which associates lo-cal text semantics with both high-level visual context and multi-level local visual information. Our method surpasses state-of-the-art “dual-encoder” methods by a large margin on the image-text retrieval task and significantly improves discriminativeness to local textual semantics. Our model can also generalize well to video-text retrieval. 1.

Introduction
Pre-training vision-language models on massive image-text pairs to learn transferable representations for image-text retrieval has attracted a lot of attention in recent
* Equal contribution. † Corresponding author.
Figure 1. Illustration of image-text contrastive learning (ITC) and visual-language error modeling (ViLEM). ITC learns image-text global alignment by distinguishing paired data from unpaired data.
ViLEM establishes detailed image-text association via discrimi-nating and correcting wrong words in plausible negative texts. years. Previous dominant methods [11, 29, 38] adopt “dual-encoder” architecture to enable efficient retrieval, where two separate encoders are used to extract image and text representations. They learn a joint image-text embedding space via constraining the coarse-grained alignment be-tween global image and text features. However, the coarse-grained alignment constraint ignores the capture of detailed image and text semantics, and associations between them, impeding the performance improvement of image-text re-trieval.
Humans achieve accurate image-text matching by care-fully discriminating whether there exists semantic diver-gence between image and text, i.e., determining whether each word can be precisely grounded to the image, which requires a comprehensive perception of each modality and well association between them. Humans can also elimi-nate semantic divergence effortlessly by correcting text er-rors through their powerful semantic association capability.
Inspired by these, we propose a novel proxy task, named
Visual-Language Error Modeling (ViLEM), for image-text retrieval. As shown in Figure 1, compared with image-text contrastive learning for global alignment, ViLEM enforces the model to discriminate and eliminate the local seman-tic divergence by “proofreading” plausible negative texts against image information, which enhances fine-grained se-mantic perception and establishes detailed image-text asso-ciation. Collaborating with image-text contrastive learning,
ViLEM significantly improves the retrieval performance of
“dual-encoder” architecture.
ViLEM is divided into two sub-tasks: text error detec-tion and text error correction. Given an image and a plau-sible negative text, the goal of error detection is training the model to exhaustively discriminate the correctness of each word in the form of binary classification. Meanwhile, error correction enforces the model to predict the correct words for the wrong ones from a fixed vocabulary under the condition of image information. However, finding plau-sible negative text for images and obtaining corresponding labels of error detection and correction requires high human annotation costs. Thus, we propose to automatically con-struct plausible negative texts and corresponding labels with a pre-trained language model BERT [12], where we exploit its rich linguistic knowledge to edit the image-paired texts and generate local text errors. The generated errors can be related to objects, actions, scenes, relationships, etc. (as shown in Figure 1), with which the model can learn various fine-grained semantics. The detection and correction labels can also be obtained by comparing generated negative texts with image-paired texts.
To further leverage ViLEM’s ability to establish seman-tics associations, we propose a multi-granularity interaction framework to enable effective interaction between visual and textual encoders while maintaining high retrieval effi-ciency. Specifically, global visual features and local visual features are both fully exploited for text error detection and correction. For global visual features, we inject them into the local text representations to provide visual conditions for discriminating and correcting text errors, which asso-ciates local text information with high-level visual context and enhances the discriminativeness to fine-grained text se-mantics. For local visual features, we employ additional cross-attention modules to adaptively aggregate them into word-related visual concepts for error detection and cor-rection, which establishes the association between detailed text semantics with multi-level local visual information and facilitates fine-grained image-text alignment. The cross-attention modules will be removed in the inference, intro-ducing no additional computation cost and parameters com-pared with vanilla “dual-encoder”.
The contributions of this work are listed as follows: (1) We introduce a novel proxy task, Visual-Language
Error Modeling (ViLEM), to inject detailed seman-tic association between images and texts into “dual-encoder” architecture. (2) We propose a multi-granularity interaction framework to further leverage the ability of ViLEM while main-taining the high retrieval efficiency, which enhances the capture of fine-grained semantics and associates lo-cal text semantics with both high-level visual context and multi-level local visual information. (3) The extensive experimental results show that our method surpasses previous state-of-the-art “dual-encoder” methods by a large margin on the image-text retrieval task and significantly improves the dis-criminativeness to local text semantics. Moreover, our model can also generalize well to video-text retrieval. 2.