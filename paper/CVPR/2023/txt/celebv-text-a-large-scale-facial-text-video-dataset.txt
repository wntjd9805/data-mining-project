Abstract 1.

Introduction
Text-driven generation models are flourishing in video generation and editing. However, face-centric text-to-video generation remains a challenge due to the lack of a suitable dataset containing high-quality videos and highly relevant texts. This paper presents CelebV-Text, a large-scale, di-verse, and high-quality dataset of facial text-video pairs, to facilitate research on facial text-to-video generation tasks.
CelebV-Text comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts gen-erated using the proposed semi-automatic text generation strategy. The provided texts are of high quality, describ-ing both static and dynamic attributes precisely. The supe-riority of CelebV-Text over other datasets is demonstrated via comprehensive statistical analysis of the videos, texts, and text-video relevance. The effectiveness and potential of CelebV-Text are further shown through extensive self-evaluation. A benchmark is constructed with representa-tive methods to standardize the evaluation of the facial text-to-video generation task. All data and models are publicly available1.
*Equal contribution. 1Project page: https://celebv-text.github.io
Text-driven video generation has recently garnered sig-nificant attention in the fields of computer vision and com-puter graphics. By using text as input, video content can be generated and controlled, inspiring numerous applications in both academia and industry [5,34,43,47]. However, text-to-video generation still faces many challenges, particularly in the face-centric scenario where generated video frames often lack quality [18, 34, 37] or have weak relevance to in-put texts [2,4,39,67]. We believe that one of the main issues is the absence of a well-suited facial text-video dataset con-taining high-quality video samples and text descriptions of various attributes highly relevant to videos.
Constructing a high-quality facial text-video dataset poses several challenges, mainly in three aspects. 1) Data collection. The quality and quantity of video samples largely determine the quality of generated videos [11, 45, 48, 60]. However, obtaining such a large-scale dataset with high-quality samples while maintaining a natural distribu-tion and smooth video motion is challenging. 2) Data an-notation. The relevance of text-video pairs needs to be en-sured. This requires a comprehensive coverage of text for describing the content and motion appearing in the video, such as light conditions and head movements. 3) Text gen-eration. Producing diverse and natural texts are non-trivial.
Manual text generation is expensive and not scalable. While auto-text generation is easily extensible, it is limited in nat-uralness.
To overcome the challenges mentioned above, we care-fully design a comprehensive data construction pipeline that includes data collection and processing, data annotation, and semi-auto text generation. First, to obtain raw videos, we follow the data collection steps of CelebV-HQ, which has proven to be effective in [66]. We introduce a minor modification to the video processing step to improve the video’s smoothness further. Next, to ensure highly relevant text-video pairs, we analyze videos from both temporal dy-namics and static content and establish a set of attributes that may or may not change over time. Finally, we propose a semi-auto template-based method to generate texts that are diverse and natural. Our approach leverages the advan-tages of both auto- and manual-text methods. Specifically, we design a rich variety of grammar templates as [10,52] to parse annotation and manual texts, which are flexibly com-bined and modified to achieve high diversity, complexity, and naturalness.
With the proposed pipeline, we create CelebV-Text, a Large-Scale Facial Text-Video Dataset, which includes 70, 000 in-the-wild video clips with a resolution of at least 512 × 512 and 1, 400, 000 text descriptions with 20 for each clip. As depicted in Figure 1, CelebV-Text consists of high-quality video samples and text descriptions for realistic face video generation. Each video is annotated with three types of static attributes (40 general appearances, 5 detailed ap-pearances, and 6 light conditions) and three types of dy-namic attributes (37 actions, 8 emotions, and 6 light direc-tions). All dynamic attributes are densely annotated with start and end timestamps, while manual-texts are provided for labels that cannot be discretized. Furthermore, we have designed three templates for each attribute type, resulting in a total of 18 templates that can be flexibly combined. All attributes and manual-texts are naturally described in our generated texts.
CelebV-Text surpasses existing face video datasets [11] in terms of resolution (over 2 times higher), number of sam-ples, and more diverse distribution. In addition, the texts in
CelebV-Text exhibit higher diversity, complexity, and natu-ralness than those in text-video datasets [19, 66]. CelebV-Text also shows high relevance of text-video pairs, validated by our text-video retrieval experiments [17]. To further ex-amine the effectiveness and potential of CelebV-Text, we evaluate it on a representative baseline [19] for facial text-to-video generation. Our results show better relevance be-tween generated face videos and texts when compared to a state-of-the-art large-scale pretrained model [26]. Fur-thermore, we show that a simple modification of [19] with text interpolation can significantly improve temporal coher-ence. Finally, we present a new benchmark for text-to-video generation to standardize the facial text-to-video generation task, which includes representative models [5, 19] on three text-video datasets. follows: 1) We propose CelebV-Text, the first large-scale facial text-video dataset with high-quality videos, as well as rich and highly-relevant texts, to facilitate research in fa-cial text-to-video generation. 2) Comprehensive statistical analyses are conducted to examine video/text quality and diversity, as well as text-video relevance, demonstrating the superiority of CelebV-Text. 3) A series of self-evaluations are performed to demonstrate the effectiveness and poten-tial of CelebV-Text. 4) A new benchmark for text-to-video generation is constructed to promote the standardization of the facial text-to-video generation task. 2.