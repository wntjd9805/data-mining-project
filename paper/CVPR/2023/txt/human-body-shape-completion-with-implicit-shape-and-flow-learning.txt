Abstract 1.

Introduction
In this paper, we investigate how to complete human body shape models by combining shape and ﬂow estimation given two consecutive depth images. Shape completion is a challenging task in computer vision that is highly under-constrained when considering partial depth observations.
Besides model based strategies that exploit strong priors, and consequently struggle to preserve ﬁne geometric de-tails, learning based approaches build on weaker assump-tions and can beneﬁt from efﬁcient implicit representations.
We adopt such a representation and explore how the motion
ﬂow between two consecutive frames can contribute to the shape completion task.
In order to effectively exploit the
ﬂow information, our architecture combines both estima-tions and implements two features for robustness: First, an all-to-all attention module that encodes the correlation be-tween points in the same frame and between corresponding points in different frames; Second, a coarse-dense to ﬁne-sparse strategy that balances the representation ability and the computational cost. Our experiments demonstrate that the ﬂow actually beneﬁts human body model completion.
They also show that our method outperforms the state-of-the-art approaches for shape completion on 2 benchmarks, considering different human shapes, poses, and clothing.
The inference of human body shape information from depth observations has become a standard problem in com-puter vision. Depth sensors are now common and enable the digitization of humans using every day devices such as tablets or mobile phones, in turn opening a way to new con-sumer applications that build on this ability, e.g. virtual try on or avatar applications. Solving the problem efﬁciently is however difﬁcult given human body observations that are, by construction, incomplete with a single frame. Consider-ing several frames over time, as often available, can how-ever improve the body shape estimation, provided that tem-poral consistency is effectively exploited. In this paper we consider how to build complete human shape models given these partial depth observations. Particularly we investigate how the combination of shape and motion ﬂow estimations can beneﬁt such shape completion tasks.
Different strategies for shape completion have been ex-plored that exploit various priors over human shapes. Para-metric body models such as SMPL [24] can be used as in [5, 31, 34]. The strong prior assumed with a paramet-ric model ensures spatially and temporally coherent human shape predictions. However these predictions are inherently restricted to limited shape spaces. The preservation of the geometric details that can be present in depth maps, e.g.
face attributes or cloth wrinkles, is arduous. Other strate-gies build on weaker priors, relying on learning to char-acterize shapes and their completion. Early contributions in this respect [9, 40, 52] explore encoder-decoder network architectures with 3D convolutions and successfully pre-dict complete distance ﬁelds in explicit voxel grids. They were subsequently extended to implicit representations that can provide continuous 3D shape functions such as occu-pancy [7,28], or distance ﬁelds [8,32], with limited memory costs compared to explicit voxel representations. Further-more, temporal features provided by depth map sequences can also be accounted for with implicit representations that then become spatio-temporal [58]. Yet, without explicit cor-respondences over time, learning based methods can only partially exploit temporal consistency.
Such correspondences are encoded in the motion ﬁeld between the input depth maps. This ﬁeld, the scene ﬂow, is traditionally estimated pixel-wise as an extension of the 2D optical ﬂow. Recent learning-based strategies [22, 50] generally focus on observed points only and do not target shape estimation nor completion. Closer to this objective,
OFlow [30] proposes a 4D model that combines shape and
ﬂow information in an implicit continuous representation.
While the method can account for point clouds, it does not easily extend to shape completion with depth maps. Fur-thermore the shape and ﬂow are estimated independently whereas we advocate a combined estimation of both.
To this aim, we propose a learning-based approach that considers two consecutive depth images as input and esti-mates a continuous complete representation of both shape occupancy (SDF) and motion as implicit functions, lever-aging their representational advantages demonstrated for both problems independently. Our experiments show that such a combined estimation beneﬁts the shape completion task with results that outperform existing works on standard datasets. The proposed approach is pyramidal and considers image features that are extracted in a coarse to ﬁne manner, preserving both local and more global shape properties. In addition, with the aim to enforce consistency in both spatial and temporal domains, we take inspiration from the scene
ﬂow work [47] and introduce an all-to-all attention mech-anism that accounts for spatial and temporal correlations between points in the two frames considered. Comprehen-sive ablation tests demonstrate the individual contributions of the pyramidal framework and attention mechanisms. Ex-periments were conducted on DFAUST [6] and CAPE [26] with both undressed and dressed humans. We provide com-parisons with the state-of-the-art approaches for both shape and ﬂow estimations and show consistent shape completion improvements with our method.
Method
SceneFlow [22, 47, 50] 4DComplete [19]
STIF [58]
NPMs [31]
OFlow [30]
Ours
Shape
Completion (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
Continuous Continuous
Flow Rep.
Shape Rep. (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
Detail
Preservation (cid:55) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51)
Scene
Represent.
Points
Voxels
Implicit
Para. model
Implicit
Implicit
Table 1. Classiﬁcation of related methods with respect to their abilities to: handle partial inputs; provide continuous shape and
ﬂow representations; preserve geometric details in the observa-tions. 2.