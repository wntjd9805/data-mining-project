Abstract
Despite its importance for federated learning, continu-ous learning and many other applications, on-device train-ing remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consump-tion required during training by the back-propagation al-gorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradi-ent map, thus significantly reducing the computational com-plexity and memory consumption of back propagation dur-ing training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g.,
MobileNet, DeepLabV3, UPerNet) and devices (e.g., Rasp-berry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, com-pared to SOTA, we achieve up to 19× speedup and 77.1% memory savings on ImageNet classification with only 0.1% accuracy loss. Finally, our method is easy to implement and deploy; over 20× speedup and 90% energy savings have been observed compared to highly optimized baselines in
MKLDNN and CUDNN on NVIDIA Jetson Nano. Conse-quently, our approach opens up a new direction of research with a huge potential for on-device training.1 1.

Introduction
Existing approaches for on-device training are neither efficient nor practical enough to satisfy the resource con-straints of edge devices (Figure 1). This is because these methods do not properly address a fundamental problem in on-device training, namely the computational and memory complexity of the back-propagation (BP) algorithm. More precisely, although the architecture modification [6] and layer freezing [18, 20] can help skipping the BP for some layers, for other layers, the complexity remains high. Gra-dient quantization [4, 7] can reduce the cost of arithmetic operations but cannot reduce the number of operations (e.g., multiplications); thus, the speedup in training remains lim-ited. Moreover, gradient quantization is not supported by existing deep-learning frameworks (e.g., CUDNN [9],
MKLDNN [1], PyTorch [25] and Tensorflow [2]). To en-able on-device training, there are two important questions must be addressed:
• How can we reduce the computational complexity of back propagation through the convolution layers?
• How can we reduce the data required by the gradient computation during back propagation?
In this paper, we propose gradient filtering, a new research direction, to address both questions. By addressing the first question, we reduce the computational complexity of train-ing; by addressing the second question, we reduce the mem-ory consumption.
In general, the gradient propagation through a convolu-tion layer involves multiplying the gradient of the output variable with a Jacobian matrix constructed with data from either the input feature map or the convolution kernel. We aim at simplifying this process with the new gradient filter-ing approach proposed in Section 3. Intuitively, if the gradi-ent map w.r.t. the output has the same value for all entries, then the computation-intensive matrix multiplication can be greatly simplified, and the data required to construct the Ja-cobian matrix can be significantly reduced. Thus, our gra-dient filtering can approximate the gradient w.r.t. the output by creating a new gradient map with a special (i.e., spatial) structure and fewer unique elements. By doing so, the gra-dient propagation through the convolution layers reduces to cheaper operations, while the data required (hence memory) for the forward propagation also lessens. Through this fil-tering process, we trade off the gradient precision against the computation complexity during BP. We note that gradi-ent filtering does not necessarily lead to a worse precision, i.e., models sometimes perform better with filtered gradi-ents when compared against models trained with vanilla BP. 1Code: https://github.com/SLDGroup/GradientFilter-CVPR23
In summary, our contributions are as follows:
• We propose gradient filtering, which reduces the com-putation and memory required for BP by more than two orders of magnitude compared to the exact gradi-ent calculation.
• We provide a rigorous error analysis which shows that the errors introduced by the gradient filtering have only a limited influence on model accuracy.
• Our experiments with multiple CNN models and com-puter vision tasks show that we can train a neural net-work with significantly less computation and memory costs, with only a marginal accuracy loss compared to baseline methods. Side-by-side comparisons against other training acceleration techniques also suggest the effectiveness of our method.
• Our method is easy to deploy with highly optimized deep learning frameworks (e.g., MKLDNN [1] and
CUDNN [9]). Evaluations on resource-constrained edge (Raspberry Pi and Jetson Nano) and high-performance devices (CPU/GPU) show that our method is highly suitable for real life deployment.
The paper is organized as follows. Section 2 reviews rel-evant work. Section 3 presents our method in detail. Sec-tion 4 discusses error analysis, computation and memory consumption. Experimental results are presented in Section 5. Finally, Section 6 summarizes our main contributions.
Figure 1. Matrix of orthogonal directions for on-device training.
“Arch” is short for “architecture”. Our approach opens up a new direction of research for on-device training for EdgeAI.
In contrast to the prior work, our method opens up a new research direction. More precisely, we reduce the number of computations and memory consumption required for train-ing a single layer via gradient filtering. Thus, our method can be combined with any of the methods mentioned above.
For example, in Section H in the Supplementary, we illus-trate how our method can work together with the gradient quantization methods to enable a higher speedup. 2.