Abstract
Knowledge Distillation (KD) aims at distilling the knowledge from the large teacher model to a lightweight student model. Mainstream KD methods can be divided into two categories, logit distillation, and feature distilla-tion. The former is easy to implement, but inferior in per-formance, while the latter is not applicable to some prac-tical circumstances due to concerns such as privacy and safety. Towards this dilemma, in this paper, we explore a stronger logit distillation method via making better uti-lization of logit outputs. Concretely, we propose a sim-ple yet effective approach to logit distillation via multi-level prediction alignment. Through this framework, the prediction alignment is not only conducted at the instance level, but also at the batch and class level, through which the student model learns instance prediction, input corre-lation, and category correlation simultaneously.
In addi-tion, a prediction augmentation mechanism based on model calibration further boosts the performance. Extensive ex-periment results validate that our method enjoys consis-tently higher performance than previous logit distillation methods, and even reaches competitive performance with mainstream feature distillation methods. Code is avail-able at https://github.com/Jin-Ying/Multi-Level-Logit-Distillation. 1.

Introduction
The last few decades have witnessed the prosperity of deep learning in computer vision tasks, such as image clas-sification [3, 7, 14, 26], object detection [21], and segmen-tation [25, 37]. However, due to their overwhelming large model size, many deep models rely heavily on computation and storage resources, which makes it nearly impossible to deploy them in some practical scenarios, such as mobile de-vices. Towards this challenge, Knowledge Distillation [10] (KD) was introduced to reduce model capacity. Concretely,
*(cid:0)Corresponding author.
Figure 1. Problem Setting. Feature distillation methods utilize features in the intermediate layers as well as logit outputs. On the contrary, logit distillation methods conduct knowledge distillation merely with logit outputs. the KD framework consists of one teacher model (large) and one student model (small). The main objective of KD is to distill the knowledge in the teacher model to the light-weight student model, which can be readily deployed. Var-ious KD methods [1, 8, 19, 22, 27] have been proposed and proved to be effective.
Mainstream KD methods fall into two lines of work, 1) logit distillation and 2) feature distillation. Logit distilla-tion conveys knowledge from the teacher model to the stu-dent merely on the logit level. The earliest KD method [10], which distills knowledge by reducing the divergence of pre-dictions, is an example of logit distillation method. To-wards better utilization of teacher knowledge, recent re-searches [1, 22] shed light on the intermediate layers in the teacher model, conducting distillation by matching feature distributions as well as logit outputs among the teacher and student model. These methods are coined feature distilla-tion methods. We compare feature distillation with logit distillation in Figure 1.
Utilizing the feature knowledge in intermediate layers, feature distillation methods are more likely to reach supe-rior performance. However, in some real-world applica-tions, the intrinsic architecture of the teacher model is invis-Table 1. Comparison of different methods. Compared with previous logit distillation and feature distillation methods, our method conducts prediction alignment at the instance level, batch level and category level simultaneously.
Method
Instance-level Alignment Batch-level Alignment Category-level Alignment
Logit Distillation (Previous)
Feature Distillation
Logit Distillation (Ours) (cid:33) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) ible due to commercial, privacy, and safety concerns, mak-ing these methods invalid under such circumstances. For example, when launching adversarial attacks [5], it is much easier for hackers to recover the train data when all inter-mediate layers in the model are available. Such a leak of data may cause highly negative influences on financial or medical applications.
Facing such a dilemma, we pay attention to logit distil-lation, which does not need to have access to the features in the intermediate layers. To mitigate the performance gap between logit distillation and feature distillation, we shall make better utilization of logit outputs. Concretely, in this paper, we propose multi-level logit distillation, a sim-ple yet effective approach to absorb more information from logit outputs. We propose a multi-level alignment to reduce the divergence of predictions between the teacher and stu-dent at the instance, batch, and class level. Through this alignment, the student model absorbs knowledge from the teacher model not only in instance-level prediction, but in batch-level input correlation and class-level category corre-lation as well. We compare our multi-level logit distillation method with previous methods in Table 1. The previous logit distillation merely conducts instance-level alignment, and the feature distillation incorporates batch-level align-ment. On the contrary, our method implements alignment at multiple levels with logit outputs alone. In addition, we also introduce a prediction augmentation based on model calibration. It enables the student model to learn from more diverse predictions, pushing the performance of our method to a higher level.
Extensive experiment results on mainstream benchmarks validate that our method surpasses previous logit distillation methods, in both homogenous and heterogeneous network knowledge distillation settings. Meanwhile, our method also reaches competitive performance over previous feature distillation methods, proving that our method excels at uti-lizing logit outputs. 2.