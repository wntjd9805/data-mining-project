Abstract 1.

Introduction
The intimate entanglement between objects affordances and human poses is of large interest, among others, for behavioural sciences, cognitive psychology, and Computer
Vision communities.
In recent years, the latter has de-veloped several object-centric approaches: starting from items, learning pipelines synthesizing human poses and dy-namics in a realistic way, satisfying both geometrical and functional expectations. However, the inverse perspective is significantly less explored: Can we infer 3D objects and their poses from human interactions alone? Our investiga-tion follows this direction, showing that a generic 3D hu-man point cloud is enough to pop up an unobserved ob-ject, even when the user is just imitating a functionality (e.g., looking through a binocular) without involving a tan-gible counterpart. We validate our method qualitatively and quantitatively, with synthetic data and sequences acquired for the task, showing applicability for XR/VR.
Complex interactions with the world are among the unique skills distinguishing humans from other living be-ings. Even though our perception might be imperfect (we cannot hear ultrasonic sounds or see ultraviolet light [49]), our cognitive representation is enriched with a functional perspective, i.e., potential ways of interacting with objects or, as introduced by Gibson and colleagues, the affordance of the objects [19]. Several behavioural studies confirmed the centrality of this concept [2, 9, 43], which plays a fun-damental role also for kids’ development [2]. Computer
Vision is well-aware that the function of an object com-plements its appearance [21], and exploited this in tasks like human and object reconstruction [62]. Previous litera-ture approaches the interaction analysis from an object per-spective (i.e., given an object, analyze the human interac-tion) [8,22,63], building object-centric priors [73], generat-ing realistic grasps given the object [13, 34], reconstructing hand-object interactions [8, 22, 63]. Namely, objects induce functionality, so a human interaction (e.g., a mug suggests
Our
Human-Centric
Object-Centric
Previous Work
Figure 2. Human-Centric vs. Object-Centric inference. A com-mon perspective in the prior work is to infer affordances or human pose starting from an object. We explore the inverse, Human-Centric perspective of the human-object interaction relationship: given the human, we are interested in predicting the object. a drinking action; an handle a grasping one). For the first time, our work reverts the perspective, suggesting that ana-lyzing human motion and behaviour is naturally a human-centric problem (i.e., given a human interaction, what kind of functionality is it suggesting, Fig. 2). Moving the first step in this new research direction, we pose a fundamen-tal question: Can we infer 3D objects and their poses from human interactions alone?
At first sight, the problem seems particularly hard and significantly under-constrained since several geometries might fit the same action. However, the human body com-plements this information in several ways: physical rela-tions, characteristic poses, or body dynamics serve as valu-able proxies for the involved functionality, as suggested by
Fig. 1. Such hints are so powerful that we can easily imag-ine the kind of object and its location, even if such an object does not exist. Furthermore, even if the given pose might fit several possible solutions, our mind naturally comes to the most natural one indicated by the observed behaviour.
It also suggests that solely focusing on the contact region (an approach often preferred by previous works) is insuf-ficient in this new viewpoint. The reification principle of
Gestalt psychology [37] highlights that ”the whole” arises from ”the parts” and their relation. Similarly, in Fig. 3, the hand grasps in B) pop up a binocular in our mind because we naturally consider the relationship with the other body parts.
Finally, moving to a human-centric perspective in human-object interaction is critical for human studies and daily-life applications. Modern systems for AR/VR [24], and digital interaction [26] are both centered on humans, often manipulating objects that do not have a real-world counterpart. Learning to decode an object from human be-haviour enables unprecedented applications.
To answer our question, we deploy a first straightforward
A)
B)
Figure 3. Reification principle. Gestalt theory suggests our perception considers the whole before the single parts (A). This served as inspiration for our work: the object can arise consider-ing the body parts as a whole (B). and effective pipeline to ”pop up” a rigid object from a 3D human point cloud. Starting from the input human point cloud and a class, we train an end-to-end pipeline to in-fer object location. In case a temporal sequence of point clouds is available, we suggest post-processing to avoid jit-tering and inconsistencies in the predictions, showing the relevance of this information to handle ambiguous poses.
We show promising results on previously unaddressed tasks in digital and real-world scenes. Finally, our method allows us to analyze different features of human behaviour, high-light their contribution to object retrieval, and point to ex-citing directions for future works.
In summary, our main contributions are: 1. We formulate a novel problem, changing the perspec-tive taken by previous works in the field, and open to a yet unexplored research direction; 2. We introduce a method capable of predicting the object starting from an input human point cloud; 3. We analyze different components of the human-object relationship: the contribution of different pieces of in-teractions (hands, body, time sequence), the point-wise saliency of input points, and the confusion produced by objects with similar functions.
All the code will be made publicly available. 2.