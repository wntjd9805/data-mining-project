Abstract
Recent CLIP-guided 3D optimization methods, such as
DreamFields [19] and PureCLIPNeRF [24], have achieved impressive results in zero-shot text-to-3D synthesis. How-ever, due to scratch training and random initialization with-out prior knowledge, these methods often fail to generate accurate and faithful 3D structures that conform to the in-put text. In this paper, we make the first attempt to introduce explicit 3D shape priors into the CLIP-guided 3D optimiza-tion process. Specifically, we first generate a high-quality 3D shape from the input text in the text-to-shape stage as a 3D shape prior. We then use it as the initialization of a neu-ral radiance field and optimize it with the full prompt. To address the challenging text-to-shape generation task, we present a simple yet effective approach that directly bridges the text and image modalities with a powerful text-to-image diffusion model. To narrow the style domain gap between the images synthesized by the text-to-image diffusion model and shape renderings used to train the image-to-shape generator, we further propose to jointly optimize a learn-able text prompt and fine-tune the text-to-image diffusion model for rendering-style image generation. Our method,
*Work done during an internship at ARC Lab, Tencent PCG.
†Corresponding Author.
Dream3D, is capable of generating imaginative 3D con-tent with superior visual quality and shape accuracy com-pared to state-of-the-art methods. Our project page is at https://bluestyle97.github.io/dream3d/. 1.

Introduction
Text-to-3D synthesis endeavors to create 3D content that is coherent with an input text, which has the potential to benefit a wide range of applications such as animations, games, and virtual reality. Recently developed zero-shot text-to-image models [34, 44, 45, 48, 50] have made remark-able progress and can generate diverse, high-fidelity, and imaginative images from various text prompts. However, extending this success to the text-to-3D synthesis task is challenging because it is not practically feasible to collect a comprehensive paired text-3D dataset.
Zero-shot text-to-3D synthesis [19, 22, 24, 40, 51], which eliminates the need for paired data, is an attractive approach that typically relies on powerful vision-language models such as CLIP [58]. There are two main categories of this approach. 1) CLIP-based generative models, such as CLIP-Forge [51]. They utilize images as an intermediate bridge and train a mapper from the CLIP image embeddings of
ShapeNet renderings to the shape embeddings of a 3D shape generator, then switch to the CLIP text embedding as the in-put at test time. 2) CLIP-guided 3D optimization methods, such as DreamFields [19] and PureCLIPNeRF [24]. They continuously optimize the CLIP similarity loss between a text prompt and rendered images of a 3D scene representa-tion, such as neural radiance fields [1, 26, 32, 41]. While the first category heavily relies on 3D shape generators trained on limited 3D shapes and seldom has the capacity to ad-just its shape structures, the second category has more cre-ative freedom with the “dreaming ability” to generate di-verse shape structures and textures.
We develop our method building upon CLIP-guided 3D optimization methods. Although these methods can pro-duce remarkable outcomes, they typically fail to create pre-cise and accurate 3D structures that conform to the input text (Fig. 1, 2nd row)). Due to the scratch training and ran-dom initialization without any prior knowledge, these meth-ods tend to generate highly-unconstrained “adversarial con-tents” that have high CLIP scores but low visual quality.
To address this issue and synthesize more faithful 3D con-tents, we suggest generating a high-quality 3D shape from the input text first and then using it as an explicit “3D shape prior” in the CLIP-guided 3D optimization process. In the text-to-shape* stage, we begin by synthesizing a 3D shape without textures of the main common object in the text prompt. We then use it as the initialization of a voxel-based neural radiance field and optimize it with the full prompt.
The text-to-shape generation itself is a challenging task.
Previous methods [19, 51] are often trained on images and tested with texts, and use CLIP to bridge the two modali-ties. However, this approach leads to a mismatching prob-lem due to the gap between the CLIP text and image embed-ding spaces. Additionally, existing methods cannot produce high-quality 3D shapes.
In this work, we propose to di-rectly bridge the text and image modalities with a powerful text-to-image diffusion model, i.e., Stable Diffusion [48].
We use the text-to-image diffusion model to synthesize an image from the input text and then feed the image into an image-to-shape generator to produce high-quality 3D shapes. Since we use the same procedure in both training and testing, the mismatching problem is largely reduced.
However, there is still a style domain gap between the im-ages synthesized by Stable Diffusion and the shape render-ings used to train the image-to-shape generator. Inspired by recent work on controllable text-to-image synthesis [11,49], we propose to jointly optimize a learnable text prompt and fine-tune the Stable Diffusion to address this domain gap.
The fine-tuned Stable Diffusion can reliably synthesize im-ages in the style of shape renderings used to train the image-*Throughout this paper, we use the term “shape” to refer to 3D geomet-ric models without textures, while some works [7,28] also use this term for textured 3D models. to-shape module without suffering from the domain gap.
To summarize, 1) We make the first attempt to introduce the explicit 3D shape prior into CLIP-guided 3D optimiza-tion methods. The proposed method can generate more ac-curate and high-quality 3D shapes conforming to the corre-sponding text, while still enjoying the “dreaming” ability of generating diverse shape structures and textures (Fig. 1, 1st row). Therefore, we name our method “Dream3D” as it has both strengths. 2) Regarding text-to-shape genera-tion, we present a straightforward yet effective approach that directly connects the text and image modalities using a powerful text-to-image diffusion model. To narrow the style domain gap between the synthesized images and shape ren-derings, we further propose to jointly optimize a learnable text prompt and fine-tune the text-to-image diffusion model for rendering-style image generation. 3) Our Dream3D can generate imaginative 3D content with better visual quality and shape accuracy than state-of-the-art methods. Addition-ally, our text-to-shape pipeline can produce 3D shapes of higher quality than previous work. 2.