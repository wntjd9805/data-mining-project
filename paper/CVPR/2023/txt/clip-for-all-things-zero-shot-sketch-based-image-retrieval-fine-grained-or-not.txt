Abstract
In this paper, we leverage CLIP for zero-shot sketch based image retrieval (ZS-SBIR). We are largely inspired by recent advances on foundation models and the unparalleled generalisation ability they seem to offer, but for the first time tailor it to benefit the sketch community. We put forward novel designs on how best to achieve this synergy, for both the category setting and the fine-grained setting (“all”). At the very core of our solution is a prompt learning setup.
First we show just via factoring in sketch-specific prompts, we already have a category-level ZS-SBIR system that over-shoots all prior arts, by a large margin (24.8%) – a great testimony on studying the CLIP and ZS-SBIR synergy. Mov-ing onto the fine-grained setup is however trickier, and re-quires a deeper dive into this synergy. For that, we come up with two specific designs to tackle the fine-grained match-ing nature of the problem: (i) an additional regularisa-tion loss to ensure the relative separation between sketches and photos is uniform across categories, which is not the case for the gold standard standalone triplet loss, and (ii) a clever patch shuffling technique to help establishing instance-level structural correspondences between sketch-photo pairs. With these designs, we again observe signifi-cant performance gains in the region of 26.9% over previ-ous state-of-the-art. The take-home message, if any, is the proposed CLIP and prompt learning paradigm carries great promise in tackling other sketch-related tasks (not limited to ZS-SBIR) where data scarcity remains a great challenge.
Project page: https:// aneeshan95.github.io/ Sketch LVM/ 1.

Introduction
Late research on sketch-based image retrieval (SBIR)
[30, 49, 51, 52] had fixated on the zero-shot setup, i.e., zero-shot SBIR (ZS-SBIR) [16, 18, 68]. This shift had become inevitable because of data-scarcity problem plaguing the sketch community [5, 7, 33] – there are just not enough
It fol-sketches to train a general-purpose SBIR model. lows that the key behind a successful ZS-SBIR model lies with how best it conducts semantic transfer cross object mAP 0.72 0.58 0.52 0.7 0.6 0.5
Acc@1 (%) 28.68 22.6 35 30 25 20
Text SBIR Ours
SOTA Ours
Figure 1. Against existing (left) ZS-SBIR methods, we adapt CLIP model for ZS-SBIR (middle), and extend to a more practical yet challenging setup of FG-ZS-SBIR (right), via a novel prompt-based design. Our model surpasses prior arts by a high margin. categories and between sketch-photo modalities. Despite great strides made elsewhere on the general zero-shot lit-erature [60, 66, 75] however, semantic transfer [47] for ZS-SBIR had remained rather rudimentary, mostly using stan-dard word embeddings directly [16, 18, 73] or indirectly
[37, 62, 65].
In this paper, we fast track ZS-SBIR research to be aligned with the status quo of the zero-shot literature, and for the first time, propose a synergy between foundation models like CLIP [46] and the cross-modal problem of ZS-SBIR. And to demonstrate the effectiveness of this syn-ergy, we not only tackle the conventional category-level
ZS-SBIR, but a new and more challenging fine-grained instance-level [44] ZS-SBIR as well.
Our motivation behind this synergy of CLIP and ZS-SBIR is no different to the many latest research adapting
CLIP to vision-language pre-training [22], image and ac-tion recognition [47, 66] and especially on zero-shot tasks
[31, 39, 60, 75] – CLIP exhibits a highly enriched seman-tic latent space, and already encapsulates knowledge across a myriad of cross-modal data. As for ZS-SBIR, CLIP is therefore almost a perfect match, as (i) it already provides a rich semantic space to conduct category transfer, and (ii) it has an unparalleled understanding on multi-modal data, which SBIR dictates.
At the very heart of our answer to this synergy is that of prompt learning [29], that involves learning a set of con-tinuous vectors injected into CLIP’s encoder. This enables
CLIP to adapt to downstream tasks while preserving its gen-eralisability – a theme that we follow in our CLIP-adaption to ZS-SBIR (Fig. 1). More specifically, we first design two sets of visual prompts, one for each modality (sketch, photo). They are both injected into the initial layer insider the transformer of CLIP for training. While keeping the rest of CLIP frozen, these two prompts are trained over the gold standard triplet loss paradigm [69], on extracted sketch-photo features. Motivated by the efficacy of train-ing batch normalisation for image recognition [21], we ad-ditionally fine-tune a small subset of trainable parameters of every Layer Normalisation (LN) layer for additional per-formance gain. Furthermore, to enhance cross-category se-mantic transfer, we also resort to CLIP’s text encoder fur-ther cultivating its zero-shot potential. In particular, in ad-dition to the said visual prompts onto the CLIP’s image en-coder, we use handcrafted text prompts via templates like
‘photo of a [category]’ to its text encoder, during training.
The new fine-grained setting [13, 44] is however more tricky. Unlike the previous category-level setup, it poses two additional challenges (i) relative feature-distances be-tween sketch-photo pairs across categories are non-uniform, which is reflected in the varying triplet-loss margin [69] across categories at training [8], and (ii) apart from semantic consistency, fine-grained ZS-SBIR requires instance-level matching to be conducted [44], which dictates additional constraints such as structural correspondences.
It follows that for the first challenge, we propose a new regularisation term that aims at making the relative sketch-photo feature-distances uniform across categories, such that a single (global) margin parameter works across all of them.
Specifically, taking the distribution of relative distances for all sketch-positive-negative hard-triplets [69] in a category, we aim to minimise the KL-divergence [40] between every pair of distributions, which trains the model towards mak-ing such relative sketch-photo distances uniform across cat-egories. For the latter, we propose a clever patch shuffling technique, where equally divided patches of a sketch and its corresponding photo (n×n) are first shuffled following a random permutation order of patches. We then advocate that a shuffled sketch should be closer to a shuffled photo having the same permutation order, but far from that of a different permutation. Training this permutation-invariance imparts a broad notion of structural correspondences, thus helping in fine-grained understanding.
Summing up: (i) We for the first time adapt CLIP for
ZS-SBIR, (ii) We propose a novel prompt learning setup to facilitate the synergy between the two, (iii) We address both the conventional ZS-SBIR setting, and a new and more challenging fine-grained ZS-SBIR problem, (iv) We intro-duce a regularisation term and a clever patch shuffling tech-nique to address the fine-grained challenges. With our
CLIP-adapted model surpassing all prior arts by a large margin (Fig. 1), we hope to have shed some light to the sketch community on benefits such a synergy between foun-dation models and sketch-related tasks can bring. 2.