Abstract
Adopting contrastive image-text pretrained models like
CLIP towards video classification has gained attention due to its cost-effectiveness and competitive performance. How-ever, recent works in this area face a trade-off. Fine-tuning the pretrained model to achieve strong supervised performance results in low zero-shot generalization. Sim-ilarly, freezing the backbone to retain zero-shot capabil-ity causes significant drop in supervised accuracy. Be-cause of this, recent works in literature typically train sep-arate models for supervised and zero-shot action recog-In this work, we propose a multimodal prompt nition. learning scheme that works to balance the supervised and zero-shot performance under a single unified train-ing. Our prompting approach on the vision side caters for three aspects: 1) Global video-level prompts to model the data distribution; 2) Local frame-level prompts to provide per-frame discriminative conditioning; and 3) a summary prompt to extract a condensed video representation. Ad-ditionally, we define a prompting scheme on the text side to augment the textual context. Through this prompting scheme, we can achieve state-of-the-art zero-shot perfor-mance on Kinetics-600, HMDB51 and UCF101 while re-maining competitive in the supervised setting. By keep-ing the pretrained backbone frozen, we optimize a much lower number of parameters and retain the existing gen-eral representation which helps achieve the strong zero-shot performance. Our codes/models will be released at https://github.com/TalalWasim/Vita-CLIP.. 1.

Introduction
In the image classification domain, multimodal image-text pretrained models such as CLIP [58], ALIGN [31] and
Florence [75] have shown the capability of learning gener-alized representations. These models, trained on large-scale language-image pairs in a contrastive manner, have remark-able zero-shot capabilities and transfer well to a variety of downstream tasks. However, training a similar model for the task of video recognition is not feasible both in terms of gathering large-scale video-text pairs, which can suffer from alignment problems [30], and is also exponentially more computationally expensive due to multiple frames be-ing processed per video. Therefore, there has been a recent push in the research community to effectively adopt the pre-trained image-text models for the task of video recognition,
In this re-while maintaining their zero-shot capabilities. gard, existing methods can be divided into two categories.
Some take inspiration from recent prompt learning methods
[25, 32, 77, 81, 82] and propose a prompt learning scheme either on the text [36] or vision [55, 70] side, along with additional transformer layers for improved temporal learn-ing. Others prefer an end-to-end CLIP finetuning scheme for video tasks [51, 55, 70]. However, the problem with these methods is that they either fail to effectively leverage learning on both the text and vision sides [36, 55] or end up losing the zero-shot generalization of CLIP by finetun-ing the vision decoder [47] or the backbone [51, 55, 70]. In summary, the existing approaches can steer the model either towards good zero-shot generalization or better supervised learning on video tasks. Since real-world tasks require both supervised and zero-shot capabilities, our work investigates the following question: Can we develop a unified model for videos that performs well for both supervised learning and zero-shot generalization tasks?
In pursuit of the aforementioned question, we propose a multimodal prompting-based Video and text adaptive CLIP.
To effectively adapt the pretrained image-text CLIP model to videos, we consider two important aspects. Firstly, one needs to preserve the generalization capabilities of the orig-inal pretrained CLIP backbone and secondly, it must be able to effectively adapt to the video domain. In this regard, we propose to keep the entire backbone frozen and learn addi-tional lightweight modules to adapt the model for videos.
On this point, for the vision side, we aim to explicitly ex-ploit the temporal information in videos which is lacking in the frozen image model. Our approach models video infor-mation at three levels: first via global video-level prompts
t o h s o r e
Z 1 5
B
D
M
H 1
-p o
T t o h s o r e
Z 1 0 1
F
C
U 1
-p o
T 52 50 48 46 44 42 40 80 75 70 65 60 55
Vita-CLIP B/16 (OURS) (frozen backbone)
A5 [36] (ECCV’22) (frozen backbone)
XCLIP B/16 [55] (ECCV’22) (finetuned backbone)
ActionClip B/16 [70] (CORR’21) (finetuned backbone) 76 78 82 80
Top-1 K400 Supervised 84
Vita-CLIP B/16 (OURS) (frozen backbone)
A5 [6] (ECCV’22) (frozen backbone)
XCLIP B/16 [55] (ECCV’22) (finetuned backbone)
ActionClip B/16 [70] (CORR’21) (finetuned backbone) (a) Proposed Prompting Scheme 76 78 82 80
Top-1 K400 Supervised 84 (b) Zero-shot accuracy (HMDB51, UCF101) vs supervised accuracy (Kinetics-400).
Figure 1. An overview of the proposed prompting scheme (left) alongside the trade-off which we attempt to balance between su-pervised and zero-shot performance (right). (a) Our prompting approach adds learnable pa-rameters to learn visual and temporal infor-mation in videos at three levels: a summary prompt to learn a condensed representation of the video, video-level prompts to model global distribution shifts needed to adapt to video do-main and frame-level prompts to enrich local discriminative information in each frame. On the text side, we learn prompts to adapt the language representations for videos. (b) The trade-off plots showing zero-shot vs. super-vised performance comparison for ours and re-cent CLIP-based video approaches. Note that existing SoTA [55] trains two separate models for zero-shot and supervised settings while our method offers a unified model with the same training for both settings. that learn the overall distribution characteristics of video data e.g., motion and dynamics; secondly, inspired by [53], local frame-level prompts which model per frame discrimi-native information by directly conditioning on classification tokens of all frames; and thirdly by a summary prompt that distills the entire video sequence response in a single con-cise summary vector.
Additionally, to better model the textual context we pro-pose to use a learnable context on the text encoder. The rea-son why this is particularly important is that the textual in-formation is quite limited in the available video datasets. In-stead of having per-sample text descriptions, we are limited to using class labels as text descriptions. Inspired by [82], we propose a prompt learning method on the text side to better model the textual context and to augment the video class label descriptions. An overview of our method with the trade-off it seeks to balance is presented in Fig. 1. The main contributions of this work are as follows:
• We propose a multimodal prompting approach Vita-CLIP for videos that learns video and text-specific context vec-tors to efficiently adapt the image-text pretrained CLIP model to video recognition tasks.
• On the vision side, we explicitly model the temporal in-formation and the video data distribution. Our prompt learning method aggregates the discriminative informa-tion from each frame in a clip with every other frame, while also providing per-layer learning capacity to better capture the data distribution. On the language side, our approach learns complimentary semantic context to bet-ter adapt the language representations.
• We evaluate our approach on supervised as well as gener-alization tasks and demonstrate a sound balance between both aspects using a single unified model. Specifically, on zero-shot tasks, we obtain 4.0%, 3.0% and 2.2% gains over the recent SoTA X-CLIP [55] on HMDB-51, UCF-101, and Kinetics-600 datasets respectively. 2.