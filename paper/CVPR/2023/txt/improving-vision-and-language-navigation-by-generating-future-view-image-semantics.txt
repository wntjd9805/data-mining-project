Abstract
Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of naviga-ble locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, hu-mans will have an expectation of how the future environ-ment will look like, based on the natural language instruc-tions and surrounding views, which will aid correct naviga-tion. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent’s in-domain pre-training:
Masked Panorama Modeling (MPM), Masked Trajectory
Modeling (MTM), and Action Prediction with Image Gen-eration (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict miss-ing steps in the full trajectory (MTM), and generate the next view based on the full instruction and navigation his-tory (APIG), respectively. We then fine-tune the agent on the VLN task with an auxiliary loss that minimizes the dif-ference between the view semantics generated by the agent and the ground truth view semantics of the next step. Em-pirically, our VLN-SIG achieves the new state-of-the-art on both Room-to-Room dataset and CVDN dataset. We fur-ther show that our agent learns to fill in missing patches in future views qualitatively, which brings more interpretabil-ity over agents’ predicted actions. Lastly, we demonstrate that learning to predict future view semantics also enables the agent to have better performance on longer paths.1 1.

Introduction
In Vision-and-Language Navigation, the agent needs to navigate through the environment based on natural lan-1Code is available at https://github.com/jialuli-luka/
VLN-SIG.git.
Figure 1. Overview of our proposed method VLN-SIG. We obtain the semantics of an image with a pre-trained image tokenizer, and use codebook selection (Sec. 3.4) and patch semantic calculation (Sec. 3.3) to adapt it to efficient in-domain VLN learning. We pre-train the agent on three proxy tasks (Sec. 3.5) and fine-tune the agent using Action Prediction with Image Generation (APIG) as the additional auxiliary task (Sec. 3.6). guage instructions. Many datasets have been proposed to solve this challenging task [2, 5, 22, 33, 40]. Based on these datasets, previous works aim to strengthen the navigation model or agent from several aspects: understanding long and detailed instructions in different languages, inferring and locating target objects based on common knowledge, navigating in diverse and potentially unseen environments, and learning better alignment between the environment and the instructions. However, most previous work simplifies the navigation process as an action selection process, where at each time step, the agent picks the action from a set of pre-defined candidates. This simplification does not take advantage of human’s ability to expect what scene will be more likely to happen next during navigation. For exam-ple, given the instruction “Walk through the bedroom into the hallway. Turn right and wait in the kitchen doorway.”, before walking out of the bedroom, humans will expect the hallway to be a long passage with doors, and the kitchen probably contains objects like a kitchen island and sink.
Humans have these expectations based on common sense knowledge and use them to select candidates during navi-gation. Thus, in this paper, we explore whether AI agents
could also benefit from the ability to generate future scenes for action selection during navigation.
[21] first explores the useful idea of generating future scenes with high quality, based on history observations on an indoor Vision-and-Language Navigation dataset. Fur-thermore, they adapt their synthetic future scenes for VLN by replacing the original observations with the generated observations. However, their replacement method did not enhance the agents’ performance on the VLN task. The po-tential of using semantic information from generated future observations is still underexplored. Thus, in this paper, we aim to equip the agent with both the ability to predict future scenes and also benefit from learning semantics in future generated observations.
We propose VLN-SIG: Vision-and-Language Naviga-tion with Image Semantics Generation. As shown in Fig-ure 1, to first calculate the overall image semantics, we tokenize the view images into visual tokens with a pre-trained discrete variational autoencoder (dVAE). While the pre-trained dVAE has a large vocabulary of 8192, we pro-pose and compare static and dynamic codebook selection processes to optimize a subset of the codebook at one time during training. This helps the agent to learn the more im-portant tokens and focus on optimizing more difficult to-kens. Then, we propose and compare three ways based on mean value, block-wise weighted value, and sampling to represent the semantics of all patches for efficient image semantics learning. In the pre-training stage, we propose three tasks: Masked Panorama Modeling (MPM), Masked
Trajectory Modeling (MTM) and Action Prediction with
Image Generation (APIG). In Masked Panorama Modeling, the agent learns to predict the semantics of multiple miss-ing views in a full panorama. In Masked Trajectory Mod-eling, the agent learns to predict the semantics of multiple steps in the full trajectory. MPM and MTM together give the agent the ability to understand the semantics in each vi-sual token and learn to recognize the semantics contained
In Action Prediction with Image Genera-in each view. tion, the agent mimics the navigation process to predict the next step by generating the visual tokens contained in the next navigation view. This task enables the agent to imag-ine the next step view semantics before making actions.
We then fine-tune the agent on the step-by-step Vision-and-Language Navigation task. We further enhance agents’ abil-ity to predict future views by optimizing an auxiliary task during navigation, which minimizes the difference between predicted observations and the target observation. Though this task does not help the agent make navigation decision directly, the future visual semantics injected by this task helps the agent understand the environment better.
We conduct experiments on Room-to-Room (R2R) datasets [2] and Cooperative Vision-and-Dialog Navigation (CVDN) dataset [40]. Empirical results show that our pro-posed VLN-SIG outperforms the strong SotA agents [8,10] by a relative gain of 4.5% in goal progress (meters) on
CVDN test leaderboard, and 3% absolute gain in success rate on Room-to-Room test leaderboard. We further demon-strate that our proposed codebook selection methods and patch semantic calculation methods are crucial for learn-ing to generate image semantics with ablation studies. Be-sides, we show that our agent achieves better performance for longer paths. Lastly, we show that our agent learns to fill in missing patches in the future views, which brings more interpretability over agents’ predictions. 2.