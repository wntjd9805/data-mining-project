Abstract
Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet. 1.

Introduction
Diffusion models have seen rapid progress, setting state-of-the-art (SOTA) performance across a variety of image generation tasks. While most diffusion methods model 2D images, recent work [2, 14, 42, 86] has attempted to develop denoising methods for 3D shape generation. These 3D diffusion methods operate on discrete point clouds and, while successful, exhibit limited quality and resolution.
In contrast to 2D diffusion, which directly leverages the image as the target for the diffusion process, it is not directly obvious how to construct such 2D targets in the case of 3D diffusion. Interestingly, recent work on 3D-aware generative adversarial networks (GANs) (see Sec. 2 for an overview) has demonstrated impressive results for 3D shape generation using 2D generators. We build upon this idea of learning to generate triplane representations [6] that encode 3D scenes or
*Equal contribution.
Part of the work was done during an internship at Stanford.
Project page: https://jryanshue.com/nfd
Figure 1. Our method leverages existing 2D diffusion models for 3D shape generation using hybrid explicit–implicit neural repre-sentations. Top: triplane-based 3D shape diffusion process using our framework. Bottom: Interpolation between generated shapes. radiance fields as a set of axis-aligned 2D feature planes. The structure of a triplane is analogous to that of a 2D image and can be used as part of a 3D generative method that leverages conventional 2D generator architectures.
Inspired by recent efforts in designing efficient 3D GAN architectures, we introduce a neural field-based diffusion framework for 3D representation learning. Our approach follows a two-step process. In the first step, a training set of 3D scenes is factored into a set of per-scene triplane features and a single, shared feature decoder. In the second step, a 2D diffusion model is trained on these triplanes. The trained diffusion model can then be used at inference time to generate novel and diverse 3D scenes. By interpreting triplanes as multi-channel 2D images and thus decoupling generation from rendering, we can leverage current (and likely future)
SOTA 2D diffusion model backbones nearly out of the box.
Fig. 1 illustrates how a single object is generated with our framework (top), and how two generated objects—even with different topologies—can be interpolated (bottom).
Our core contributions are as follows:
Figure 2. Visualization of the denoising process. Here, we show examples of triplanes as they are iteratively denoised at inference, as well as the shapes we obtain by “decoding” the noisy triplanes with our jointly-learned MLP. By interpreting triplane features simply as multi-channel feature images, we build our framework around 2D diffusion models.
• We introduce a generative framework for diffusion on 3D scenes that utilizes 2D diffusion model backbones and has a built-in 3D inductive bias.
• We show that our approach is capable of generating both high-fidelity and diverse 3D scenes that outperform state-of-the-art 3D GANs. better at capturing local scene details. We adopt the expressive hybrid triplane representation introduced by Chan et al. [6].
Triplanes are efficient, scaling with the surface area rather than volume, and naturally integrate with expressive, fine-tuned 2D generator architectures. We modify the triplane represen-tation for compatibility with our denoising framework. 2.