Abstract
Most existing online knowledge distillation (OKD) tech-niques typically require sophisticated modules to produce diverse knowledge for improving students’ generalization ability. In this paper, we strive to fully utilize multi-model settings instead of well-designed modules to achieve a dis-tillation effect with excellent generalization performance.
Generally, model generalization can be reflected in the flat-ness of the loss landscape. Since averaging parameters of multiple models can find flatter minima, we are inspired to extend the process to the sampled convex combinations of multi-student models in OKD. Specifically, by linearly weighting students’ parameters in each training batch, we construct a Hybrid-Weight Model (HWM) to represent the parameters surrounding involved students. The supervi-sion loss of HWM can estimate the landscape’s curva-ture of the whole region around students to measure the generalization explicitly. Hence we integrate HWM’s loss into students’ training and propose a novel OKD frame-work via parameter hybridization (OKDPH) to promote flatter minima and obtain robust solutions. Considering the redundancy of parameters could lead to the collapse of HWM, we further introduce a fusion operation to keep the high similarity of students. Compared to the state-of-the-art (SOTA) OKD methods and SOTA methods of seek-ing flat minima, our OKDPH achieves higher performance with fewer parameters, benefiting OKD with lightweight and robust characteristics. Our code is publicly available at https://github.com/tianlizhang/OKDPH. 1.

Introduction
Deep learning achieves breakthrough progress in a va-riety of tasks by constructing a large capacity network
†Corresponding author pre-trained on massive data [6].
In order to apply high-parameterized models in the real-world scene with lim-ited resources, the knowledge distillation (KD) technique
[12] aims to obtain a compact and effective student model guided by a large-scale teacher model for model compres-sion. Based on the developments of KD, Zhang et al. [44] propose the concept of online knowledge distillation (OKD) to view all networks as students and achieve mutual learn-ing from scratch through peer teaching, liberating the dis-tillation process from the dependency on pre-trained teach-ers. Existing OKD methods mainly encourage students to acquire diverse and rich knowledge, including aggregating predictions [10, 33, 37], combining features [19, 22, 28], working with peers [39, 47], learning from group lead-ers [2], and receiving guidance from online teachers [39].
Nevertheless, these strategies focus on designing sophis-ticated architectures to exploit heterogeneous knowledge to enhance students’ generalization, but they lack explicit con-straints on generalization. The concept of generalization to deep models is the ability to fit correctly on previously un-seen data [25], which can be reflected by the flatness of the loss landscape [14, 18]. Flatness is the landscape’s local curvature, which is costly to direct calculate by the Hessian.
Considering the setting of multiple students in OKD, we uti-lize the theory of multi-model fusion in parameter space [9] to estimate the local curvature by the linear combination of students’ parameters (we call it a hybrid-weight model, which is expressed as HWM). More specifically, HWM is a stochastic convex combination of parameters of multiple students on different data augmentations, which can sample multiple local points on the landscape. Intuitively, HWM’s loss reflects the upper and lower bounds of the local region’s loss and estimates the curvature of the landscape. Minimiz-ing HWM’s loss flattens the region and forms a landscape with smaller curvature.
Based on the above observation, we propose a concise and effective OKD framework, termed online knowledge
distillation with parameter hybridization (OKDPH), to pro-mote flatter loss minima and achieve higher generaliza-tion. We devise a novel loss function for students’ train-ing that incorporates the standard Cross-Entropy (CE) loss and Kullback-Leibler (KL) divergence loss, but also a su-pervised learning loss from HWM. Specifically, HWM is constructed in each batch by linearly weighting multiple students’ parameters. The classification error of HWM ex-plicitly measures the flatness of the region around students on the loss landscape, reflecting their stability and general-ization. The proposed loss equals imposing stronger con-straints on the landscape, guiding students to converge in a more stable direction. For intuitive understanding, we vi-sualize the loss landscape of students obtained by different methods in Fig. 1. Our students converge to one broader and flatter basin (thus superior generalization performance), while the students obtained by DML [44] converge to differ-ent sharp basins, degrading the robustness and performance.
Unfortunately, directly hybridizing students’ parameters can easily lead to the collapse of HWM due to the high nonlinearity of deep neural networks [26, 31]. Therefore, we restrict the differences between students through inter-mittent fusion operations to ensure the high similarity of multi-model parameters and achieve effective construction of HWM. Concretely, at regular intervals, we hybridize the parameter of HWM with each student and, conversely, assign the hybrid parameter to the corresponding student.
This process shortens the distance between students, shown as very close loss trajectories of our students in Fig. 1. How-ever, it will not reduce diversity because students receive different types of data augmentation, and they can easily become diverse during training. Our method pulls students in the same direction, plays the role of strong regularization, and obtains one lightweight parameter that performs well in various scenarios. The solution derived from our method is expected to integrate the dark knowledge from multiple models while maintaining a compact architecture and can be competent for resource-constrained applications.
To sum up, our contributions are organized as follows:
• Inspired by the theory of multi-model fusion, we inno-vatively extend traditional weight averaging to an on-the-fly stochastic convex combination of students’ pa-rameters, called a hybrid-weight model (HWM). The supervision loss of HWM can estimate the curvature of the loss landscape around students and explicitly mea-sure the generalization.
• We propose a brand-new extensible and pow-erful OKD framework via parameter hybridiza-tion (OKDPH) for loss minima flattening, which flex-ibly adapts to various network architectures without modifying peers’ structures and extra modules. It is the first OKD work that manipulates parameters.
Figure 1. The loss landscape visualization of four students (Ours-S1 and Ours-S2 are obtained by our method, and DML obtains
DML-S1 and DML-S2), which are ResNet32 [11] trained by the same settings on CIFAR-10 [20]. Four students start from the ini-tial point (Red points in the center) and converge to three basins along different trajectories. The x-axis and y-axis represent the values of model parameters that PCA [23] obtains.
• Extensive experiments on various backbones demon-strate that our OKDPH can considerably improve the students’ generalization and exceed the state-of-the-art (SOTA) OKD methods and SOTA approaches of seeking flat minima. Further loss landscape visualiza-tion and stability analysis verify that our solution lo-cates in the region having uniformly low loss and is more robust to perturbations and limited data. 2.