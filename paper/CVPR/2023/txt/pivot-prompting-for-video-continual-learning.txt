Abstract
Modern machine learning pipelines are limited due to data availability, storage quotas, privacy regulations, and expensive annotation processes. These constraints make it difficult or impossible to train and update large-scale mod-els on such dynamic annotated sets. Continual learning di-rectly approaches this problem, with the ultimate goal of devising methods where a deep neural network effectively learns relevant patterns for new (unseen) classes, without significantly altering its performance on previously learned ones.
In this paper, we address the problem of contin-ual learning for video data. We introduce PIVOT, a novel method that leverages extensive knowledge in pre-trained models from the image domain, thereby reducing the num-ber of trainable parameters and the associated forgetting.
Unlike previous methods, ours is the first approach that ef-fectively uses prompting mechanisms for continual learning without any in-domain pre-training. Our experiments show that PIVOT improves state-of-the-art methods by a signifi-cant 27% on the 20-task ActivityNet setup. 1.

Introduction
Modern Deep Neural Networks (DNNs) are at the core of many state-of-the-art methods in machine vision [10, 20, 21, 38, 44, 46] tasks. To achieve their remarkable performance, most DNNs rely on large-scale pre-training
[11, 26, 36, 46], thereby enabling feature reuse in related downstream tasks [4]. However, adapting and fine-tuning a pre-trained DNN on a novel dataset commonly leads to catastrophic forgetting [16]. This phenomenon explains how the effectiveness of the fine-tuned DNNs drastically reduces in the original training distribution, in favor of in-creased performance on the downstream task.
This undesirable property has driven multiple research efforts in the area of Continual Learning (CL) [7, 8, 18, 25,
Figure 1. Performance improvement by each PIVOT com-ponent. We report the average accuracy on all tasks under the 10-task CIL on UCF101 and ActivityNet. We report the per-formance of basic CLIP, and then gradually equip it with other components: Spatial Prompting, Memory Buffer, Multi-modal
Contrastive Learning, Temporal Encoder, to finally reach our pro-posed PIVOT method. The addition of each proposed component generally boosts the performance on both datasets. Stars represent the upper bound performance on each benchmark. 29], yielding techniques that enable fine-tuning a DNN on a sequence of tasks while mitigating the performance drop along the intermediate steps. One of the most challenging scenarios for the study of CL is Class Incremental Learning (CIL), where the labels and data are mutually exclusive be-tween tasks, training data is available only for the current task, and there are no task identifiers on the validation step (i.e task boundaries are not available at test time). Such a setup requires learning one model that, despite the continu-ous adaptation to novel tasks, performs well on all the seen classes. Recent advances in mitigating catastrophic forget-ting rely on deploying episodic memory or regularization techniques [2, 8, 25, 37]. Nevertheless, most of this progress has been directed toward analyzing the catastrophic forget-ting of DNNs in the image domain. The works of [34,35,47] introduced the CIL setup to the video domain, in particular the action recognition task. Unlike its image counterpart,
video CIL requires careful modeling of the temporal infor-mation, making it an even more challenging setup. Despite the success in small-scale datasets (like UCF101), state-of-the-art methods have shown limited effectiveness on more challenging video test-beds built upon larger action tax-onomies, such as Kinetics and ActivityNet [47].
Currently, state-of-the-art methods for video CIL rely on temporal masking and feature distillation to mitigate catas-trophic forgetting [34,47]. In this paper, we take inspiration from recent advances in large-scale DNNs for zero-shot im-age classification [36] and learnable prompts for continual learning [50], and we propose a novel strategy for CIL in the video domain. We show that a zero-shot baseline pre-trained in the image domain already outperforms the best
CIL methods in the action recognition task*. Moreover, we show that this baseline can be significantly improved by enabling temporal reasoning and augmenting the modal-ity encoders with a novel prompting strategy. As a con-sequence, our proposed method, PIVOT, outperforms ev-ery other baseline, setting a new state-of-the-art in all the 3 datasets included in the challenging vCLIMB benchmark for video CIL [47]. Figure 1 summarizes the performance improvements of our approach.
Notably and following the core ideas of the vCLIMB
[47] benchmark, PIVOT does not rely on any in-distribution pre-training (a common feature of prompting methods for
CL [45,50]). Rather, it leverages the vast and general visual knowledge contained in the CLIP visual encoder (trained on massive amounts of paired static images and text) and maps that knowledge into a feature space suitable for video understanding in a continual learning setup.
Contributions. This paper proposes PIVOT (Prompt-Ing for Video cOnTinual learning), a novel strategy for con-tinual learning in the video domains that leverages large-scale pre-trained networks in the image domain. Our work brings the following contributions: (i) We show that a multi-modal classifier (Video-Text) mitigates catastrophic forget-ting while greatly increasing the final average CIL accuracy. (ii) We design the first prompt-based strategy for video CIL.
Our approach leverages image pre-training to significantly mitigate forgetting when learning a sequence of video ac-tion recognition tasks. (iii) We conduct extensive experi-mental analysis to demonstrate the effectiveness of PIVOT.
Our results show that PIVOT outperforms state-of-the-art methods in the challenging vCLIMB benchmark by 31%, 27%, and 17.2% in the 20-task setups of Kinetics, Activi-tyNet, and UCF101, respectively. 2.