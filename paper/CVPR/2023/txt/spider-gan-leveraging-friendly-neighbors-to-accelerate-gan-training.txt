Abstract
Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel ap-proach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efﬁcient by identifying closely related datasets, or a “friendly neighborhood” of the target distribu-tion, inspiring the moniker, Spider GAN. To deﬁne friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet and CelebA faces. Further, we demonstrate cas-cading Spider GAN, where the output distribution from a pre-trained GAN generator is used as the input to the subse-quent network. Effectively, transporting one distribution to another in a cascaded fashion until the target is learnt – a new ﬂavor of transfer learning. We demonstrate the efﬁcacy of the Spider approach on DCGAN, conditional GAN, PG-GAN, StyleGAN2 and StyleGAN3. The proposed approach achieves state-of-the-art Fréchet inception distance (FID) values, with one-ﬁfth of the training iterations, in compari-son to their baseline counterparts on high-resolution small datasets such as MetFaces, Ukiyo-E Faces and AFHQ-Cats. 1.

Introduction
Generative adversarial networks (GANs) [1] are designed to model the underlying distribution of a target dataset (with
∗Siddarth Asokan is funded by the Qualcomm Innovation Fellowship, and the Robert Bosch Center for Cyber-Physical Systems Ph.D. Fellowship.
∼
∼ underlying distribution pd) through a min-max optimiza-tion between the generator G and the discriminator D net-works. The generator transforms an input z pz, typically
Gaussian or uniform distributed, into a generated sample
G(z) pg. The discriminator is trained to classify samples drawn from pg or pd as real or fake. The optimal generator is the one that outputs images that confuse the discriminator.
Inputs to the GAN generator: The input distribution plays a deﬁnitive role in the quality of GAN output. Low-dimensional latent vectors have been shown to help dis-entangle the representations and control features of the tar-get being learnt [2, 3]. Prior work on optimizing the latent distribution in GANs has been motivated by the need to improve the quality of interpolated images. Several works have considered replacing the Gaussian prior with Gaussian mixtures, Gamma, non-parametric distributions, etc [4–9].
Alternatively, the GAN generator can be trained with the latent-space distribution of the target dataset, as learnt by variational autoencoders [10, 11]. However, such approaches are not in conformity with the low-dimensional manifold structure of real data. Khayatkhoei et al. [12] attributed the poor quality of the interpolates to the disjoint structure of data distribution in high-dimensions, which motivates the need for an informed choice of the input distribution.
GANs and image-to-image translation: GANs that accept images as input fall under the umbrella of image translation.
Here, the task is to modify particular features of an im-age, either within domain (style transfer) or across domains (domain adaptation). Examples for in-domain translation include changing aspects of face images, such as the ex-pression, gender, accessories, etc. [13–15], or modifying the illumination or seasonal characteristics of natural scenes [16].
On the other hand, domain adaptation tasks aim at transform-ing the image from one style to another. Common applica-tions include simulation to real-world translation [17–20], or translating images across styles of artwork [21–23]. While the supervised Pix2Pix framework [22] originally proposed training GANs with pairs of images drawn from the source and target domains, semi-supervised and unsupervised ex-(a) Classical GANs (b) Spider GAN
Figure 1. ( Color online) A comparison of design philosophies of the standard GANs and Spider GAN. (a) A prototypical GAN transforms high-dimensional Gaussian data, which is concentrated at the surface of hyperspheres in n-D, into an image distribution comprising a union of low-dimensional manifolds embedded in a higher-dimensional space. (b) The Spider GAN generator aims to learn a simpler transformation between two closely related data manifolds in an unconstrained manner, thereby accelerating convergence. tensions [23–28] tackle the problem in an unpaired setting, and introduce modiﬁcations such as cycle-consistenty or the addition of regularization functionals to the GAN loss to maintain a measure of consistency between images. Exist-ing domain-adaptation GANs [29, 30] enforce cross-domain consistency to retain visual similarity. Ultimately, these ap-proaches rely on enforcing some form of coupling between the source and the target via feature-space mapping. 2. The Proposed Approach: Spider GAN
We propose the Spider GAN formulation motivated by the low-dimensional disconnected manifold structure of data [12, 31–33]. Spider GANs lie at the cross-roads be-tween classical GANs and image-translation GANs. As opposed to optimizing the latent parametric prior, we hy-pothesize that providing the generator with closely related image source datasets, (dubbed the friendly neighborhood, leading to the moniker Spider GAN) will result in superior convergence of the GAN. Unlike image translation tasks, the
Spider GAN generator is agnostic to individual input-image features, and is allowed to discover implicit structure in the mapping from the source distribution to the target. Figure 1 depicts the design philosophy of Spider GAN juxtaposed with the classical GAN training approach.
The choice of the input dataset affects the generator’s abil-ity to learn a stable and accurate mapping. Intuitively, if the
GAN has to be trained to learn the distribution of street view house numbers (SVHN) [34], the MNIST [35] dataset proves to be a better initialization of the input space than standard densities such as the uniform or Gaussian. It is a well known result that, for a given mean and variance, the Gaussian has 1, 1] maximum entropy, while for a given support (say, [ when training with re-normalized images), the uniform distri-bution has maximum entropy [36]. However, image datasets are highly structured, and possess lower entropy [37]. There-fore, one could interpret the generative modeling of images using GANs as effectively one of entropy minimization [13].
−
We argue that choosing a low entropy input distribution that is structurally closer to the target would lead to a more ef-ﬁcient generator transformation, thereby accelerating the training process. Existing image-translation approaches aim to maintain semantic information, for example, translating a speciﬁc instance of the digit ‘2’ in the MNIST dataset to the SVHN style. However, the Spider GAN formulation neither enforces nor requires such constraints. Rather, it allows for an implicit structure in the source dataset to be used to learn the target efﬁciently. It is entirely possible for the Trouser class in Fashion-MNIST [38] to map to the digit
‘1’ in MNIST due to structural similarity. Thus, the scope of
Spider GAN is much wider than image translation. 2.1. Our Contributions
In Section 3, we discuss the central focus in Spider GANs: deﬁning what constitutes a friendly neighborhood. Prelimi-nary experiments suggest that, while the well known Fréchet inception distance (FID) [39] and kernel inception distance (KID) [40] are able to capture visual similarity, they are un-able to quantify the diversity of samples in the underlying manifold. We therefore propose a novel distance measure to evaluate the input to GANs, one that is motivated by elec-trostatic potential ﬁelds and charge neutralization between the (positively charged) target data samples and (negatively charged) generator samples [41, 42], named signed inception distance (SID) (Section 3.1). An implementation of SID atop the Clean-FID [43] backbone is available at https:
//github.com/DarthSid95/clean-sid. We iden-tify friendly neighborhoods for multiple classes of standard image datasets such as MNIST, Fashion MNIST, SVHN,
CIFAR-10 [44], Tiny-ImageNet [45], LSUN-Churches [46],
CelebA [47], and Ukiyo-E Faces [48]. We present exper-imental validation on training the Spider variant of DC-GAN [49] (Section 4) and show that it results in up to 30% improvement in terms of FID, KID and cumulative
SID of the converged models. The Spider framework is
lightweight and can be extended to any GAN architec-ture, which we demonstrate via class-conditional learning with the Spider variant of auxiliary classiﬁer GANs (AC-GANs) [50] (Section 4). The source code for Spider GANs built atop the DCGAN architecture are available at https:
//github.com/DarthSid95/SpiderDCGAN. We also present a novel approach to transfer learning using Spi-der GANs by feeding the output distribution of a pre-trained generator to the input of the subsequent stage (Section 5).
Considering progressively growing GAN (PGGAN) [51] and StyleGAN [52–54] architectures, we show that the cor-responding Spider variants achieve competitive FID scores in one-ﬁfth of the training iterations on FFHQ [14] and
AFHQ-Cats [30], while achieving state-of-the-art FID on high-resolution small-sized datasets such as Ukiyo-E Faces and MetFaces [53] (Section 5.1). The source code for implementing Spider StyleGANs is available at https:
//github.com/DarthSid95/SpiderStyleGAN. 2.2.