Abstract
We present an extension to masked autoencoders (MAE) which improves on the representations learnt by the model by explicitly encouraging the learning of higher scene-level features. We do this by: (i) the introduction of a perceptual similarity term between generated and real images (ii) in-corporating several techniques from the adversarial train-ing literature including multi-scale training and adaptive discriminator augmentation. The combination of these re-sults in not only better pixel reconstruction but also repre-sentations which appear to capture better higher-level de-tails within images. More consequentially, we show how our method, Perceptual MAE, leads to better performance when used for downstream tasks outperforming previous methods. We achieve 78.1% top-1 accuracy linear probing on ImageNet-1K and up to 88.1% when fine-tuning, with similar results for other downstream tasks, all without use of additional pre-trained models or data. 1.

Introduction
Self-supervision provides a powerful framework for training deep neural networks without relying on explicit supervision or labels where learning proceeds by predicting one part of the input data from another. Approaches based on denoising autoencoders [45], where the input is masked and the missing parts reconstructed, have shown to be ef-fective for pre-training in NLP with BERT [8], and more recently similar techniques have been applied for learning visual representations from images [1,4,17,30]. Such meth-ods effectively use image reconstruction as a pretext task on the basis that by learning to predict missing patches useful representations can be learnt for downstream tasks.
One challenge when applying such techniques to images is that, unlike language where words contain some level of semantic meaning by design, the pixels in images are natu-ral signals containing high-frequency variations. Therefore, image-based denoising autoencoders have been adapted to avoid learning trivial solutions to reconstruction based on local textures or patterns. BEiT [1] uses an intermediary codebook of patches such that pixels are not reconstructed directly, whilst MAE [17] masks a high proportion of the image to force the model to learn how to reconstruct whole scenes with limited context.
In this paper, we build upon MAE and ask how we can move beyond the implicit conditioning of high masking ra-tios to explicitly incorporate the learning of higher-order
‘semantic’ features into the learning objective. To do this, we focus on introducing scene-level information by adding a perceptual loss term [22]. This works by constraining feature map similarity with a second pre-trained network, a technique which has been shown empirically in the gener-ative modelling literature to improve perceptual reconstruc-tion quality [54]. In addition, this also provides a mecha-nism to incorporate relevant scene-level cues contained in the second network (which could be e.g. a strong ImageNet classifier or a pre-trained language-vision embedding).
One of the benefits of MAE is that it can rapidly learn strong representations using only self-supervision from the images in the target pre-training set. To maintain this prop-erty, we introduce a second idea: tying the features not with a separate network, but with an adversarial discriminator trained in parallel to distinguish between real and generated images. Both ideas combined result in not only a lower re-construction error, but also learnt representations which bet-ter capture details of the scene layout and object boundaries (see Figure 3) without either explicit supervision or the use of hand-engineered inductive biases.
Finally, we build on these results and show that tech-niques from the generative modelling literature such as multi-scale gradients [24] and adaptive discriminator aug-mentation [25] can lead to further improvements in the learnt representation, and this also translates into a further boost in performance across downstream tasks. We hypoth-esise that the issues that these methods were designed to overcome, such as mode collapse during adversarial train-ing and incomplete learning of the underlying data distribu-tion, are related to overfitting on low-level image features.
Our contributions can be summarized as follows: (i) we introduce a simple and self-contained technique to improve the representations learnt by masked image modelling based
on perceptual loss and adversarial learning, (ii) we per-form a rigorous evaluation of this method and variants, and set new state-of-the-art for masked image modelling with-out additional data for classification on ImageNet-1K, ob-ject detection on MS COCO and semantic segmentation on
ADE20K, (iii) we demonstrate this approach can further draw on powerful pre-trained models when available, re-sulting in a further boost in performance and (iv) we show our approach leads qualitatively to more ‘object-centric’ representations and stronger performance with frozen fea-tures (in the linear probe setting) compared to MAE. 2.