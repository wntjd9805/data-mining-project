Abstract 1.

Introduction
Building a robot that can understand and learn to inter-act by watching humans has inspired several vision prob-lems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environ-ment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact.
The structure of these behavioral affordances directly en-ables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learn-ing, exploration, goal-conditioned learning, and action pa-rameterization for reinforcement learning. We show the effi-cacy of our approach, which we call Vision-Robotics Bridge (VRB) across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild.
The meaning or value of a thing consists of what it affords... what we perceive when we look at objects are their affordances, not their qualities.
Imagine standing in a brand-new kitchen. Before taking even a single action, we already have a good understand-ing of how most objects should be manipulated. This un-derstanding goes beyond semantics as we have a belief of where to hold objects and which direction to move them in, allowing us to interact with it. For instance, the oven is opened by pulling the handle downwards, the tap should be turned sideways, drawers are to be pulled outwards, and light switches are turned on with a flick. While things don’t always work as imagined and some exploration might be needed, but humans heavily rely on such visual affordances of objects to efficiently perform day-to-day tasks across en-vironments [34, 35]. Extracting such actionable knowledge from videos has long inspired the vision community.
More recently, with improving performance on static datasets, the field is increasingly adopting a broader ‘active’ definition of vision through research in egocentric visual understanding and visual affordances from videos of human interaction. With deep learning, methods can now predict heatmaps of where a human would interact [38, 75] or seg-J.J. Gibson (1979)
⋆equal contribution
Figure 2. VRB Overview. First, we learn an actionable representation of visual affordances from human videos: the model predicts contact points and trajectory waypoints with supervision from future frames. For robot deployment, we query the affordance model and convert its outputs to 3D actions to execute. robot mentation of the object being interacted with [101]. Despite being motivated by the goal of enabling downstream robotic tasks, prior methods for affordance learning are tested pri-marily on human video datasets with no physical robot or in-the-wild experiments. Without integration with a robotic system, even the most basic question of how the affordance should be defined or represented remains unanswered, let alone evaluating its performance.
On the contrary, most learning approaches, whether imitation or reinforcement learning, approach a new task or a new environment tabula rasa. At best, the vi-sual representation might be pretrained on some dataset [65, 79, 91, 100, 115, 117]. However, visual representations are only a small part of the larger problem.
In robotics, es-pecially in continuous control, the state space complexity grows exponentially with actions. Thus, even with perfect perception, knowing what to do is difficult. Given an im-age, current computer vision approaches can label most of the objects, and even tell us approximately where they are but this is not sufficient for the robot to perform the task. It also needs to know where and how to manipulate the object, and figuring this out from scratch in every new environment is virtually impossible for all but the simplest of tasks. How do we alleviate this clear gap between visual learning and robotics?
In this paper, we propose to rethink visual affordances as a means to bridge vision and robotics. We argue that rich video datasets of humans interacting can offer a lot more actionable information beyond just replacing ImageNet as a pretrained visual encoder for robot learning. Particularly, human interactions are a rich source of how a wide range of objects can be held and what are useful ways to manipulate their state. However, several challenges hinder the smooth integration of vision and robotics. We group them into three parts. First, what is an actionable way to represent affor-dances? Second, how to learn this representation in a data-driven and scalable manner? Third, how to adapt visual af-fordances for deployment across robot learning paradigms?
To answer the first question, we find that contact points and post-contact trajectories are excellent robot-centric repre-sentations of visual affordances, as well as modeling the inherent multi-modality of possible interactions. We make effective use of egocentric datasets in order to tackle the sec-ond question. In particular, we reformulate the data to focus on frames without humans for predicting contact points and the post-contact trajectories. To extract free supervision for this prediction, we utilize off-the-shelf tools for estimating egomotion, human pose, and hand-object interaction. Fi-nally, we show how to seamlessly integrate these affordance priors with different kinds of robot learning paradigms. We call our approach Vision-Robotics Bridge (VRB) due to its core goal of bridging vision and robotics.
We evaluate both the quality of our affordances and their usefulness for 4 different robotic paradigms – imitation and offline learning, exploration, visual goal-reaching, and us-ing the affordance model as a parameterization for action spaces. These are studied via extensive and rigorous real-world experiments on physical robots which span across 10 real-world tasks, 4 environments, and 2 robot hardware plat-forms. Many of these tasks are performed in-the-wild out-side of lab environments (see Figure 1). We find that VRB outperforms other state-of-the-art human hand-object affor-dance models, and enables high-performance robot learning in the wild without requiring any simulation. Finally, we also observe that our affordance model learns a good visual representation for robotics as a byproduct. We highlight that all the evaluations are performed in the real world span-ning several hundred hours of robot running time which is a very large-scale evaluation in robotics.
2.