Abstract
Neural Radiance Field (NeRF) has recently emerged as a powerful representation to synthesize photorealistic novel views. While showing impressive performance, it relies on the availability of dense input views with highly accurate camera poses, thus limiting its application in real-world scenarios. In this work, we introduce Sparse Pose Adjust-ing Radiance Field (SPARF), to address the challenge of novel-view synthesis given only few wide-baseline input im-ages (as low as 3) with noisy camera poses. Our approach exploits multi-view geometry constraints in order to jointly learn the NeRF and reﬁne the camera poses. By relying on pixel matches extracted between the input views, our multi-view correspondence objective enforces the optimized scene and camera poses to converge to a global and geometrically accurate solution. Our depth consistency loss further en-courages the reconstructed scene to be consistent from any viewpoint. Our approach sets a new state of the art in the sparse-view regime on multiple challenging datasets. 1.

Introduction
Novel-view synthesis (NVS) has long been one of the most essential goals in computer vision. It refers to the task of rendering unseen viewpoints of a scene given a particu-lar set of input images. NVS has recently gained tremen-dous popularity, in part due to the success of Neural Radi-ance Fields (NeRFs) [30]. NeRF encodes 3D scenes with a multi-layer perceptron (MLP) mapping 3D point locations to color and volume density and uses volume rendering to synthesize images. It has demonstrated remarkable abilities for high-ﬁdelity view synthesis under two conditions: dense input views and highly accurate camera poses.
Both these requirements however severely impede the usability of NeRFs in real-world applications. For instance, in AR/VR or autonomous driving, the input is inevitably much sparser, with only few images of any particular object or region available per scene. In such sparse-view scenario,
NeRF rapidly overﬁts to the input views [11, 22, 32], lead-∗This work was conducted during an internship at Google.
Figure 1. Novel-view rendering from sparse images. We show the RGB (second row) and depth (last row) renderings from an unseen viewpoint under sparse settings (3 input views only). Even with ground-truth camera poses, NeRF [30] overﬁts to the training images, leading to degenerate geometry (almost constant depth).
BARF [24], which can successfully handle noisy poses when dense views are available, struggles in the sparse regime. Our approach SPARF instead produces realistic novel-view renderings with accurate geometry, given only 3 input views with noisy poses. ing to inconsistent reconstructions at best, and degenerate solutions at worst (Fig. 1 left). Moreover, the de-facto stan-dard to estimate per-scene poses is to use an off-the-shelf
Structure-from-Motion approach, such as COLMAP [37].
When provided with many input views, COLMAP can gen-erally estimate accurate camera poses. Its performance nev-ertheless rapidly degrades when reducing the number of views, or increasing the baseline between the images [55].
Multiple works focus on improving NeRF’s performance in the sparse-view setting. One line of research [6,53] trains conditional neural ﬁeld models on large-scale datasets. Al-ternative approaches instead propose various regularization on color and geometry for per-scene training [11, 19, 22, 32, 34]. Despite showing impressive results in the sparse sce-nario, all these approaches assume perfect camera poses as a pre-requisite. Unfortunately, estimating accurate camera poses for few wide-baseline images is challenging [55] and has spawned its own research direction [1, 7, 14–16, 28, 60],
hence making this assumption unrealistic.
Recently, multiple approaches attempt to reduce the de-pendency of NeRFs on highly accurate input camera poses.
They rely on per-image training signals, such as a photomet-ric [9, 24, 29, 48, 50] or silhouette loss [5, 23, 56], to jointly optimize the NeRF and the poses. However, in the sparse-view scenario where the 3D space is under-constrained, we observe that it is crucial to explicitly exploit the relation between the different training images and their underlying scene geometry, to enforce learning a global and geomet-rically accurate solution. This is not the case of previous works [5, 23, 24, 48, 50, 56], which hence fail to register the poses in the sparse regime. As shown in Fig. 1, middle for
BARF [24], it leads to poor novel-view synthesis quality.
We propose Sparse Pose Adjusting Radiance Field (SPARF), a joint pose-NeRF training strategy. Our ap-proach produces realistic novel-view renderings given only few wide-baseline input images (as low as 3) with noisy camera poses (see Fig. 1 right). Crucially, it does not as-sume any prior on the scene or object shape. We introduce novel constraints derived from multi-view geometry [17] to drive and bound the NeRF-pose optimization. We ﬁrst in-fer pixel correspondences relating the input views with a pre-trained matching model [43]. These pixel matches are utilized in our multi-view correspondence objective, which minimizes the re-projection error using the depth rendered by the NeRF and the current pose estimates. Through the explicit connection between the training views, the loss en-forces convergence to a global and geometrically accurate pose/scene solution, consistent across all training views.
We also propose the depth consistency loss to boost the ren-dering quality from novel viewpoints. By using the depth rendered from the training views to create pseudo-ground-truth depth for unseen viewing directions, it encourages the reconstructed scene to be consistent from any viewpoint. We extensively evaluate and compare our approach on the chal-lenging DTU [20], LLFF [38], and Replica [39] datasets, setting a new state of the art on all three benchmarks. 2.