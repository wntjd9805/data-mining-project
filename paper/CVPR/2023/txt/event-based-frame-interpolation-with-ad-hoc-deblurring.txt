Abstract
The performance of video frame interpolation is inher-ently correlated with the ability to handle motion in the in-put scene. Even though previous works recognize the utility of asynchronous event information for this task, they ignore the fact that motion may or may not result in blur in the input video to be interpolated, depending on the length of the exposure time of the frames and the speed of the motion, and assume either that the input video is sharp, restrict-ing themselves to frame interpolation, or that it is blurry, including an explicit, separate deblurring stage before in-terpolation in their pipeline. We instead propose a general method for event-based frame interpolation that performs deblurring ad-hoc and thus works both on sharp and blurry input videos. Our model consists in a bidirectional recur-rent network that naturally incorporates the temporal di-mension of interpolation and fuses information from the in-put frames and the events adaptively based on their tempo-ral proximity. In addition, we introduce a novel real-world high-resolution dataset with events and color videos named
HighREV, which provides a challenging evaluation setting for the examined task. Extensive experiments on the stan-dard GoPro benchmark and on our dataset show that our network consistently outperforms previous state-of-the-art methods on frame interpolation, single image deblurring and the joint task of interpolation and deblurring. Our code and dataset are available at https://github.com/
AHupuJR/REFID. 1.

Introduction
Video frame interpolation (VFI) methods synthesize in-termediate frames between consecutive input frames, in-creasing the frame rate of the input video, with wide ap-plications in super-slow generation [11, 13, 20], video edit-ing [27, 45], virtual reality [1], and video compression [40].
With the absence of inter-frame information, frame-based methods explicitly or implicitly utilize motion models such as linear motion [13] or quadratic motion [41]. However,
Figure 1. Our unified framework for event-based sharp and blurry frame interpolation. Red/blue dots: negative/positive events;
Curly braces: exposure time range. the non-linearity of motion in real-world videos makes it hard to accurately capture inter-frame motion with these simple models.
Recent works introduce event cameras in VFI as a proxy to estimate the inter-frame motion between consecutive frames. Event cameras [7] are bio-inspired asynchronous sensors that report per-pixel intensity changes, i.e., events, instead of synchronous full intensity images. The events are recorded at high temporal resolution (in the order of
µs) and high dynamic range (over 140 dB) within and be-tween frames, providing valid compressed motion informa-tion. Previous works [9, 36, 37] show the potential of event cameras in VFI, comparing favorably to frame-only meth-ods, especially in high-speed non-linear motion scenarios, by using spatially aligned events and RGB frames. These event-based VFI methods make the crucial assumption that the input images are sharp. However, this assumption is violated in real-world scenes because of the ubiquitous mo-tion blur. In particular, because of the finite exposure time of frames in real-world videos, especially of those cap-tured with event cameras that output both image frames and an event stream (i.e., Dynamic and Activate VIsion Sen-sor (DAVIS) [3])—which have a rather long exposure time and low frame rate, motion blur is inevitable for high-speed scenes. In such a scenario, where the reference frames for
VFI are degraded by motion blur, the performance of frame interpolation also degrades.
As events encode motion information within and be-tween frames, several studies [4, 18, 22] are carried out on
event-based deblurring in conjunction with VFI. However, these works approach the problem via cascaded deblurring and interpolation pipelines and the performance of VFI is limited by the image deblurring performance.
Thus, the desideratum in event-based VFI is robust per-formance on both sharp image interpolation and blurry im-age interpolation. Frame-based methods [12, 17, 17, 21, 30, 46] usually treat these two aspects as separate tasks. Dif-ferent from frames, events are not subject to motion blur.
No matter whether the frame is sharp or blurry, the corre-sponding events are the same. Based on this observation, we propose to unify the two aforementioned tasks into one problem: given two input images and a corresponding event stream, restore the latent sharp images at arbitrary times between the input images. The input images could be ei-ther blurry or sharp, as Fig. 1 shows. To solve this prob-lem, we first revisit the physical model of event-based de-blurring and frame interpolation. Based on this model, we propose a novel recurrent network, which can perform both event-based sharp VFI and event-based blurry VFI. The net-work consists of two branches, an image branch and an event branch. The recurrent structure pertains to the event branch, in order to enable the propagation of information from events across time in both directions. Features from the image branch are fused into the recurrent event branch at multiple levels using a novel attention-based module for event-image fusion, which is based on the squeeze-and-excitation operation [10].
To test our method on a real-world setting and moti-vated by the lack of event-based datasets recorded with high-quality event cameras, we record a dataset, HighREV, with high-resolution chromatic image sequences and cor-responding events. From the sharp image sequences, we synthesize blurry images by averaging several consecutive frames [19]. To our knowledge, HighREV has the highest event resolution among all publicly available event datasets.
In summary, we make the following contributions:
• We propose a framework for solving general event-based frame interpolation and event-based single im-age deblurring, which builds on the underlying phys-ical model of high-frame-rate video frame formation and event generation.
• We introduce a novel network for solving the above tasks, which is based on a bi-directional recurrent ar-chitecture, includes an event-guided channel-level at-tention fusion module that adaptively attends to fea-tures from the two input frames according to the tem-poral proximity with features from the event branch, and achieves state-of-the-art results on both synthetic and real-world datasets.
• We present a new real-world high-resolution dataset with events and RGB videos, which enables real-world evaluation of event-based interpolation and deblurring. 2.