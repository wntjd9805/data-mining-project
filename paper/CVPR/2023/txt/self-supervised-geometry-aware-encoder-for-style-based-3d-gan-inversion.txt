Abstract
StyleGAN has achieved great progress in 2D face recon-struction and semantic editing via image inversion and la-tent editing. While studies over extending 2D StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion framework is still missing, limiting the applica-tions of 3D face reconstruction and semantic editing.
In this paper, we study the challenging problem of 3D GAN inversion where a latent code is predicted given a single face image to faithfully recover its 3D shapes and detailed textures. The problem is ill-posed: innumerable composi-tions of shape and texture could be rendered to the cur-rent image. Furthermore, with the limited capacity of a global latent code, 2D inversion methods cannot preserve faithful shape and texture at the same time when applied to 3D models. To solve this problem, we devise an effective self-training scheme to constrain the learning of inversion.
The learning is done efficiently without any real-world 2D-3D training pairs but proxy samples generated from a 3D
GAN. In addition, apart from a global latent code that cap-tures the coarse shape and texture information, we augment the generation network with a local branch, where pixel-aligned features are added to faithfully reconstruct face de-tails. We further consider a new pipeline to perform 3D view-consistent editing. Extensive experiments show that our method outperforms state-of-the-art inversion methods in both shape and texture reconstruction quality. 1.

Introduction
This work aims to devise an effective approach for encoder-based 3D Generative Adversarial Network (GAN) inversion. In particular, we focus on the reconstruction of 3D face, requiring just a single 2D face image as the input.
In the inversion process, we wish to map a given image to the latent space and obtain an editable latent code with an encoder. The latent code will be further fed to a generator to reconstruct the corresponding 3D shape with high-quality shape and texture. Further to the learning of an inversion encoder, we also wish to develop an approach to synthesize 3D view-consistent editing results, e.g., changing a neutral expression to smiling, by altering the estimated latent code.
GAN inversion [50] has been extensively studied for 2D images but remains underexplored in the 3D world. Inver-sion can be achieved via optimization [1, 2, 41], which typi-cally provides a precise image-to-latent mapping but can be time-consuming, or encoder-based techniques [40, 47, 49], which explicitly learn an encoding network that maps an image into the latent space. Encoder-based techniques en-joy faster inversion, but the mapping is typically inferior to optimization. In this study, we extend the notion of encoder-based inversion from 2D images to 3D shapes.
Adding the additional dimension makes inversion more challenging beyond the goal of reconstructing an editable shape with detail preservation. In particular, 1) Recovering 3D shapes from 2D images is an ill-posed problem, where innumerable compositions of shape and texture could gen-erate identical rendering results. 3D supervisions are cru-cial to alleviate the ambiguity of shape inversion from im-ages. Though high-quality 2D datasets are easily accessi-ble, owing to the expensive cost of scans there is currently a lack of large-scale labeled 3D datasets. 2) The global latent code, due to its compact and low-dimensional nature, only captures the coarse shape and texture information. With-out high-frequency spatial details, we cannot generate high-fidelity outputs. 3) Compared with 2D inversion methods where the editing view mostly aligns with the input view, in 3D editing we expect the editing results to perform well over the novel views with large pose variations. There-fore, 3D GAN inversion is non-trivial task and could not be achieved by directly applying existing approaches.
To this end, we propose a novel Encoder-based 3D GAN invErsion framework, E3DGE, which addresses the afore-mentioned three challenges. Our framework has three novel components with a delicate model design. Specifically:
Learning Inversion with Self-supervised Learning - The first component focuses on the training of the inversion en-coder. To address the shape collapse of single-view 3D re-construction without external 3D datasets, we retrofit the generator of a 3D GAN model to provide us with diverse pseudo training samples, which can then be used to train our inversion encoder in a self-supervised manner. Specif-ically, we generate 3D shapes from the latent space W of a 3D GAN, and then render diverse 2D views from each 3D shape given different camera poses. In this way, we can generate many pseudo 2D-3D pairs together with the corre-sponding latent codes. Since the pseudo pairs are generated from a smooth latent space that learns to approximate a nat-1
ural shape manifold, they serve as effective surrogate data to train the encoder, avoiding potential shape collapse.
Local Features for High-Fidelity Inversion - The second component learns to reconstruct accurate texture details.
Our novelty here is to leverage local features to enhance the representation capacity, beyond just the global latent code generated by the inversion encoder. Specifically, in addi-tion to inferring an editable global latent code to represent the overall shape of the face, we further devise an hour-glass model to extract local features over the residuals details that the global latent code fails to capture. The local features, with proper projection to the 3D space, serve as conditions to modulate the 2D image rendering. Through this effective learning scheme, we marry the benefits of both global and local priors and achieve high-fidelity reconstruction.
Synthesizing View-consistent Edited Output - The third component addresses the problem of novel view synthesis, a problem unique to 3D shape editing. Specifically, though we achieve high-fidelity reconstruction through aforemen-tioned designs, the local residual features may not fully align with the scene when being semantically edited. More-over, the occlusion issue further degrades the fusion perfor-mance when rendering from novel views with large pose variations. To this end, we propose a 2D-3D hybrid align-ment module for high-quality editing. Specifically, a 2D alignment module and a 3D projection scheme are intro-duced to jointly align the local features with edited images and inpaint occluded local features in novel view synthesis.
Extensive experiments show that our method achieves 3D GAN inversion with plausible shapes and high-fidelity image reconstruction without affecting editability. Owing to the self-supervised training strategy with delicate global-local design, our approach performs well on real-world 2D and 3D benchmarks without resorting to any real-world 3D dataset for training. To summarize, our main contributions are as follows:
• We propose an early attempt at learning an encoder-based 3D GAN inversion framework for high-quality shape and texture inversion. We show that, with care-ful design, samples synthesized by a GAN could serve as proxy data for self-supervised training in inversion.
• We present an effective framework that uses local fea-tures to complement the global latent code for high-fidelity inversion.
• We propose an effective approach to synthesize view-consistent output with a 2D-3D hybrid alignment. 2.