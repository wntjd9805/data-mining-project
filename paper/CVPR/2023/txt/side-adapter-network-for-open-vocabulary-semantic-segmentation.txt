Abstract
This paper presents a new framework for open-vocabulary semantic segmentation with the pre-trained vision-language model, named Side Adapter Network (SAN). Our approach models the semantic segmentation task as a region recognition problem. A side network is attached to a frozen CLIP model with two branches: one for predicting mask proposals, and the other for predicting attention bias which is applied in the CLIP model to rec-ognize the class of masks. This decoupled design has the benefit CLIP in recognizing the class of mask proposals.
Since the attached side network can reuse CLIP features, it can be very light. In addition, the entire network can be trained end-to-end, allowing the side network to be adapted to the frozen CLIP model, which makes the predicted mask proposals CLIP-aware. Our approach is fast, accurate, and only adds a few additional trainable parameters. We evalu-ate our approach on multiple semantic segmentation bench-marks. Our method significantly outperforms other coun-terparts, with up to 18 times fewer trainable parameters and 19 times faster inference speed. Fig. 1 shows some visual-ization results on ImageNet. We hope our approach will serve as a solid baseline and help ease future research in open-vocabulary semantic segmentation. 1.

Introduction
Recognizing and segmenting the visual elements of any category is the pursuit of semantic segmentation. Mod-ern semantic segmentation methods [5, 7, 23] rely on large amounts of labeled data, but typically datasets often only consist of tens to hundreds of categories, and expensive data collection and annotation limit our possibilities to fur-ther expand the categories. Recently, large-scale vision-language models [17, 27, 36, 37], represented by CLIP [27], have enabled arbitrary category recognition at the image level, i.e., open-vocabulary image classification, and this great success encourages us to explore its adaptation in se-mantic segmentation.
Applying the CLIP model in open-vocabulary seman-tic segmentation is challenging because the CLIP model is trained by image-level contrastive learning. Its learned rep-resentation lacks the pixel-level recognition capability that is required for semantic segmentation. One solution [12,19] to remedy the granularity gap of representation is fine-tuning the model on the segmentation dataset. However, the data sizes of segmentation datasets are much less than the vision-language pre-training dataset, so the capability of fine-tuned models on open-vocabulary recognition is often compromised.
Modeling semantic segmentation as a region recogni-tion problem bypasses the above difficulties. Early at-tempts [9, 33] adopt a two-stage training framework. In the first stage, a stand-alone model is trained to generate a set of masked image crops as mask proposals. In the second stage, the vision-language pre-training model (e.g. CLIP) is used to recognize the class of masked image crops. How-ever, since the mask prediction model is completely inde-pendent of the vision-language pre-training model, it misses the opportunity to leverage the strong features of the vision-language pre-training model and the predicted masked im-age crops may be unsuitable for recognition, which leads to a heavy, slow, and low-performing model.
This work seeks to fully unleash the capabilities of the vision-language pre-training model in open vocabulary se-mantic segmentation. To reach this goal, we present a new framework (Fig. 2), called side adapter network (SAN). Its mask prediction and recognition are CLIP-aware because of end-to-end training, and it can be lightweight due to lever-aging the features of CLIP.
The side adapter network has two branches: one predict-ing mask proposals, and one predicting attention biases that are applied to the self-attention blocks of CLIP for mask class recognition. We show this decoupled design improves
Figure 2. Overview of our SAN. The red dotted lines indicate the gradient flow during training. In our framework, the frozen CLIP model still serves as a classifier, and the side adapter network gen-erates mask proposals and attention bias to guide the deeper layers of the CLIP model to predict proposal-wise classification logits.
During inference, the mask proposals and the proposal logits are combined to get final predictions through Matmul. the segmentation performance because the region used for
CLIP to recognize the mask may be different from the mask region itself. To minimize the cost of CLIP, we further present a single-forward design: the features of shallow
CLIP blocks are fused to SAN, and other deeper blocks are combined with attention biases for mask recognition. Since the training is end-to-end, the side adapter network can be maximally adapted to the frozen CLIP model.
With the aim of fairness and reproducibility, our study is based on officially released CLIP models. We focus on the released ViT CLIP models because the vision transformer has de facto substituted ConvNet as the dominant backbone in the computer vision community, and for conceptual con-sistency and simplicity, the side adapter network is also im-plemented by the vision transformer.
Accurate semantic segmentation needs high-resolution images, but the released ViT CLIP models are designed for low-resolution images (e.g. 224 Ã— 224) and directly ap-ply to high-resolution images giving a poor performance.
To alleviate the conflicts in input resolutions, we use low-resolution images in the CLIP model and high-resolution images in the side adapter network. We show this asym-metric input resolution is very effective.
In addition, we also explore only fine-tuning the positional embedding of the ViT model and note improvements.
We evaluate our method on various benchmarks. Fol-lowing the setting of previous works [22, 33], the COCO
Stuff [4] dataset is used for training, and Pascal VOC [11],
Pascal Context-59 [25], Pascal Context-459 [25], ADE20K-150 [41], and ADE20K-847 [41] are used for testing. With-out bells and whistles, we report state-of-the-art perfor-mance on all benchmarks: with the CLIP ViT-L/14 model, our method achieves 12.4 mIoU on ADE-847, 15.7 mIoU on PC-459, 32.1 mIoU on ADE-150, 57.7 mIoU on PC-59, and 94.6 mIoU on VOC. Compared to the previous best
method, our method has an average of +1.8 mIoU improve-ments on 5 datasets for ViT-B/16, and +2.3 mIoU improve-ments for ViT-L/14, respectively. By further applying en-semble trick, the average performance gap increases to +2.9 mIoU and +3.7 mIoU for ViT-B/16 and ViT-L/14.
Along with the excellent performance, our approach re-quires only 8.4M trainable parameters with 64.3 GFLOPs, which is only 13% and 20% of [10], 6% and less than 1% of [22], respectively. 2.