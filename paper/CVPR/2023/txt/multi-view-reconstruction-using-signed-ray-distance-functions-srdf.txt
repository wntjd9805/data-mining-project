Abstract
In this paper, we investigate a new optimization frame-work for multi-view 3D shape reconstructions. Recent differentiable rendering approaches have provided break-through performances with implicit shape representations though they can still lack precision in the estimated geome-tries. On the other hand multi-view stereo methods can yield pixel wise geometric accuracy with local depth pre-dictions along viewing rays. Our approach bridges the gap between the two strategies with a novel volumetric shape representation that is implicit but parameterized with pixel depths to better materialize the shape surface with consis-tent signed distances along viewing rays. The approach re-tains pixel-accuracy while beneﬁting from volumetric inte-gration in the optimization. To this aim, depths are opti-mized by evaluating, at each 3D location within the vol-umetric discretization, the agreement between the depth prediction consistency and the photometric consistency for the corresponding pixels. The optimization is agnostic to the associated photo-consistency term which can vary from a median-based baseline to more elaborate criteria, e.g. learned functions. Our experiments demonstrate the ben-eﬁt of the volumetric integration with depth predictions.
They also show that our approach outperforms existing ap-proaches over standard 3D benchmarks with better geome-try estimations. 1.

Introduction
Reconstructing 3D shape geometries from 2D image observations has been a core issue in computer vision for decades. Applications are numerous and range from robotics to augmented reality and human digitization, among others. When images are available in sufﬁcient numbers, multi-view stereo (MVS) is a powerful strategy that has emerged in the late 90s (see [58]). In this strategy, 3D geometric models are built by searching for surface
Figure 1. Reconstructions with various methods using 14 images of a model from BlendedMVS [70]. locations in 3D where 2D image observations concur, a property called photo-consistency. This observation con-sistency strategy has been later challenged by approaches in the ﬁeld that seek instead for observation ﬁdelity using differentiable rendering. Given a shape model that includes appearance information, rendered images can be compared to observed images and the model can thus be optimized. Differentiable rendering adapts to several shape representations including point clouds, meshes and, more recently, implicit shape representations. The latter can account for occupancy, distance functions or densities, which are estimated either directly over discrete grids or through continuous MLP network functions. Associated to differentiable rendering these implicit representations have provided state-of-the-art approaches to recover both the geometry and the appearance of 3D shapes from 2D images.
With the objective to improve the precision of the recon-structed geometric models and their computational costs, we investigate an approach that takes inspiration from dif-ferentiable rendering methods while retaining beneﬁcial as-pects of MVS strategies. Following volumetric methods we use a volumetric signed ray distance representation which we parameterize with depths along viewing rays, a rep-resentation we call the Signed Ray Distance Function or
SRDF. This representation makes the shape surface explicit with depths while keeping the beneﬁt of better distributed gradients with a volumetric discretization. To optimize this
shape representation we introduce an unsupervised differ-entiable volumetric criterion that, in contrast to differen-tiable rendering approaches, does not require color estima-tion. Instead, the criterion considers volumetric 3D samples and evaluates whether the signed distances along rays agree at a sample when it is photo-consistent and disagree other-wise. While being volumetric our proposed approach shares the following MVS beneﬁts: i) No expensive ray tracing in addition to color decisions is required; ii) The proposed approach is pixel-wise accurate by con-struction; iii) The optimization can be performed over groups of cameras deﬁned with visibility considerations. The lat-ter enables parallelism between groups while still en-forcing consistency over depth maps.
In addition, the volumetric scheme provides a testbed to compare different photo-consistency priors in a consistent way with space discretizations that do not depend on the estimated surface.
To evaluate the approach, we conducted experiments on real data from DTU Robot Image Data Sets [23], Blended-MVS [70] and on synthetic data from Renderpeople [3] as well as on real human capture data. Ablation tests demon-strate the respective contributions of the SRDF parametriza-tion and the volumetric integration in the shape reconstruc-tion process. Comparisons with both MVS and differential rendering methods also show that our method consistently outperforms state-of-the-art both quantitatively and qualita-tively with better geometric details. 2.