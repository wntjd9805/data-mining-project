Abstract
In this work, we revisit the weak-to-strong consistency framework, popularized by FixMatch from semi-supervised classification, where the prediction of a weakly perturbed im-age serves as supervision for its strongly perturbed version.
Intriguingly, we observe that such a simple pipeline already achieves competitive results against recent advanced works, when transferred to our segmentation scenario. Its success heavily relies on the manual design of strong data augmen-tations, however, which may be limited and inadequate to explore a broader perturbation space. Motivated by this, we propose an auxiliary feature perturbation stream as a supplement, leading to an expanded perturbation space. On the other, to sufficiently probe original image-level augmen-tations, we present a dual-stream perturbation technique, enabling two strong views to be simultaneously guided by a common weak view. Consequently, our overall Unified
Dual-Stream Perturbations approach (UniMatch) surpasses all existing methods significantly across all evaluation proto-cols on the Pascal, Cityscapes, and COCO benchmarks. Its superiority is also demonstrated in remote sensing interpre-tation and medical image analysis. We hope our reproduced
FixMatch and our results can inspire more future works. 1.

Introduction
Semantic segmentation aims to provide pixel-level pre-dictions to images, which can be deemed as a dense classi-fication task and is fundamental to real-world applications, e.g., autonomous driving. Nevertheless, conventional fully-supervised scenario [43, 73, 77] is extremely hungry for deli-cately labeled images by human annotators, greatly hinder-ing its broad application to some fields where it is costly and even infeasible to annotate abundant images. Therefore, semi-supervised semantic segmentation [56] has been pro-posed and is attracting increasing attention. Generally, it wishes to alleviate the labor-intensive process via leverag-*Corresponding author.
Figure 1. Comparison between state-of-the-art methods and our reproduced FixMatch [55] on the Pascal dataset. ing a large quantity of unlabeled images, accompanied by a handful of manually labeled images.
Following closely the research line of semi-supervised learning (SSL), advanced methods in semi-supervised se-mantic segmentation have evolved from GANs-based adver-sarial training paradigm [21, 47, 56] into the widely adopted consistency regularization framework [13, 19, 28, 29, 49, 61, 81] and reborn self-training pipeline [23, 27, 68, 70]. In this work, we focus on the weak-to-strong consistency regulariza-tion framework, which is popularized by FixMatch [55] from the field of semi-supervised classification, and then impacts many other relevant tasks [42, 45, 57, 62, 66, 67]. The weak-to-strong approach supervises a strongly perturbed unlabeled image xs with the prediction yielded from its correspond-ing weakly perturbed version xw, as illustrated in Figure 2a.
Intuitively, its success lies in that the model is more likely to produce high-quality prediction on xw, while xs is more effective for our model to learn, since the strong perturba-tions introduce additional information as well as mitigate confirmation bias [2]. We surprisingly notice that, so long as coupled with appropriate strong perturbations, FixMatch can indeed still exhibit powerful generalization capability in our scenario, obtaining superior results over state-of-the-art (SOTA) methods, as compared in Figure 1. Thus, we select this simple yet effective framework as our baseline.
Through investigation of image-level strong perturbations,
Method w/o any SP w/ CutMix w/ whole SP
# labeled images (10582 in total) 92 39.5 56.7 63.9 183 52.7 67.9 73.0 366 65.5 71.9 75.5 732 69.2 75.1 77.8 1464 74.6 78.3 79.2
Table 1. The importance of image-level strong perturbations (SP) to FixMatch on the Pascal dataset. w/o any SP: directly utilize hard label of xw to supervise its logits. w/ CutMix: only use CutMix [71] as a perturbation. w/ whole SP: strong perturbations contain color transformations from ST++ [68], together with CutMix. shared weak view. Such a minor modification even easily turns the FixMatch baseline into a stronger SOTA framework by itself. Intuitively, we conjecture that enforcing two strong views to be close to a common weak view can be regarded as minimizing the distance between these strong views. Hence, it shares the spirits and merits of contrastive learning [11,25], which can learn more discriminative representations and is proved to be particularly beneficial to our current task [40, 61]. We conduct comprehensive studies on the effectiveness of each proposed component. Our contributions can be summarized in four folds: we observe that they play an indispensable role in making the FixMatch a rather strong competitor in semi-supervised semantic segmentation. As demonstrated in Table 1, the performance gap between whether to adopt perturbations is extremely huge. Greatly inspired by these clues, we hope to inherit the spirit of strong perturbations from FixMatch, but also further strengthen them from two different perspectives and directions, namely expanding a broader perturbation space, and sufficiently harvesting original perturbations.
Each of these two perspectives is detailed in the following two paragraphs respectively.
Image-level perturbations, e.g., color jitter and CutMix
[71], include heuristic biases, which actually introduce ad-ditional prior information into the bootstrapping paradigm of FixMatch, so as to capture the merits of consistency reg-ularization. In case not equipped with these perturbations,
FixMatch will be degenerated to a naïve online self-training pipeline, producing much worse results. Despite its effective-ness, these perturbations are totally constrained at the image level, hindering the model to explore a broader perturbation space and to maintain consistency at diverse levels. To this end, in order to expand original perturbation space, we de-sign a unified perturbation framework for both raw images and extracted features. Concretely, on raw images, similar to
FixMatch, pre-defined image-level strong perturbations are applied, while for extracted features of weakly perturbed im-ages, an embarrassingly simple channel dropout is inserted.
In this way, our model pursues the equivalence of predictions on unlabeled images at both the image and embedding level.
These two perturbation levels can be complementary to each other. Distinguished from [33, 41], we separate different levels of perturbations into independent streams to avoid a single stream being excessively hard to learn.
On the other hand, current FixMatch framework merely utilizes a single strong view of each unlabeled image in a mini-batch, which is insufficient to fully exploit the manually pre-defined perturbation space. Considering this, we present a simple yet highly effective improvement to the input, where dual independent strong views are randomly sampled from the perturbation pool. They are then fed into the student model in parallel, and simultaneously supervised by their
• We notice that, coupled with appropriate image-level strong perturbations, FixMatch is still a powerful frame-work when transferred to the semantic segmentation scenario. A plainly reproduced FixMatch outperforms almost all existing methods in our current task.
• Built upon FixMatch, we propose a unified perturba-tion framework that unifies image-level and feature-level perturbations in independent streams, to exploit a broader perturbation space.
• We design a dual-stream perturbation strategy to fully probe pre-defined image-level perturbation space, as well as to harvest the merits of contrastive learning for discriminative representations.
• Our framework that integrates above two components, surpasses existing methods remarkably across all evalu-ation protocols on the Pascal, Cityscapes, and COCO.
Notably, it also exhibits strong superiority in medical image analysis and remote sensing interpretation. 2.