Abstract
Distilled student models in teacher-student architectures are widely considered for computational-effective deploy-ment in real-time applications and edge devices. However, there is a higher risk of student models to encounter ad-versarial attacks at the edge. Popular enhancing schemes such as adversarial training have limited performance on compressed networks. Thus, recent studies concern about adversarial distillation (AD) that aims to inherit not only prediction accuracy but also adversarial robustness of a ro-bust teacher model under the paradigm of robust optimiza-tion. In the min-max framework of AD, existing AD methods generally use fixed supervision information from the teacher model to guide the inner optimization for knowledge distil-lation which often leads to an overcorrection towards model smoothness. In this paper, we propose an adaptive adver-sarial distillation (AdaAD) that involves the teacher model in the knowledge optimization process in a way interacting with the student model to adaptively search for the inner results. Comparing with state-of-the-art methods, the pro-posed AdaAD can significantly boost both the prediction accuracy and adversarial robustness of student models in most scenarios. In particular, the ResNet-18 model trained by AdaAD achieves top-rank performance (54.23% robust accuracy) on RobustBench under AutoAttack. 1.

Introduction
Although demonstrating great success in dealing with large-scale data, deep neural networks (DNNs) are often over-parameterized in practice and require huge storage as well as computational cost [18, 22, 26]. In many real-time
*Corresponding author. applications, it is desirable to deploy lightweight models in mobile devices with limited resources for prompt inference results. Teacher-student architectures have been considered as a means of computational-effective and high-performing deployment in such applications [23, 29, 45]. Due to lim-ited budget when deploying at the edge, small (student) models are in general lack of sufficient protection mecha-nisms. Compared with large-scale models, however, they are more prone to the risk of being exposed to a potential attacker, e.g., who crafts adversarial attacks for malicious purpose [3, 21, 43]. Therefore, it is essential to improve ad-versarial robustness of small models against malicious at-tacks when applying them to real applications.
As a defense scheme, adversarial training (AT) has been studied and demonstrated effective in improving adversar-ial robustness for deep models [21, 24, 27, 32, 36]. Sev-eral studies have shown that AT is more effective on over-parameterized models with high capacity rather than on small models [27, 31, 48]. Recently, adversarial distilla-tion (AD) was proposed as an alternative scheme for im-proving adversarial robustness in teacher-student architec-tures [20, 28, 49, 50]. Like AT, AD can also be formulated as a min-max optimization problem. It aims to enable the student model to inherit not only the prediction accuracy but also the adversarial robustness from a robust teacher model under the paradigm of robust optimization.
Existing AD methods generally utilize teacher models to produce fixed soft labels to guide the distillation optimiza-tion process [20, 49, 50]. However, fitting a neighborhood region with a fixed label will inevitably impose an over-correction towards model smoothness, leading to a severe trade-off between accuracy and robustness [12,12,17]. Fur-thermore, these AD methods do not fully interact with the teacher models to minimize the prediction discrepancy be-tween student and teacher models, thereby limiting the pre-diction and robustness inherited by the student model.
In this paper, we propose adaptive adversarial distilla-tion (AdaAD) which fully involves a robust teacher model to adaptively search for more representative inner results in the knowledge distillation process. Specifically, in the in-ner optimization of AdaAD, we adaptively search for the points, representing the upper bound of the prediction dis-crepancy between the two models, as the inner results. And in outer optimization, we minimize the upper bound to per-In this way, we can enable the student form distillation. model to better inherit the prediction accuracy and adver-sarial robustness from the teacher model.
Our main contributions can be summarized as:
• We formulate a new AD objective by maximizing the prediction discrepancy between teacher and student models in the min-max framework, and provide de-tailed analysis to explain why the proposed method can achieve better distillation performance.
• We design an adaptive adversarial distillation scheme, namely AdaAD, that adaptively searches for optimal match points in the inner optimization. This enables a much larger search radius (also known as perturba-tion limit) in local neighborhoods, which significantly enhances the robustness of student models.
• Extensive experimental results verify that the perfor-mance of our method is significantly superior to that of the state-of-the-arts AT and AD methods in various scenarios. In particular, the ResNet-18 model trained over CIFAR-10 dataset by AdaAD achieves top-rank performance (54.23% robust accuracy) on the leader-board of RobustBench 1 under AutoAttack. 2.