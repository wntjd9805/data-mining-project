Abstract
Humans are proficient at continuously acquiring and in-tegrating new knowledge. By contrast, deep models for-get catastrophically, especially when tackling highly long task sequences. Inspired by the way our brains constantly rewrite and consolidate past recollections, we propose a novel Bilateral Memory Consolidation (BiMeCo) frame-work that focuses on enhancing memory interaction capa-bilities. Specifically, BiMeCo explicitly decouples model parameters into short-term memory module and long-term memory module, responsible for representation ability of the model and generalization over all learned tasks, re-spectively. BiMeCo encourages dynamic interactions be-tween two memory modules by knowledge distillation and momentum-based updating for forming generic knowledge to prevent forgetting. The proposed BiMeCo is parameter-efficient and can be integrated into existing methods seam-lessly. Extensive experiments on challenging benchmarks show that BiMeCo significantly improves the performance of existing continual learning methods. For example, com-bined with the state-of-the-art method CwD [55], BiMeCo brings in significant gains of around 2% to 6% while using 2x fewer parameters on CIFAR-100 under ResNet-18. 1.

Introduction
Acquiring knowledge in a continuous fashion is a key capability for a learning system to achieve human intelli-gence. To this end, Continual Learning (CL) is introduced to foster the network to learn a sequence of tasks incremen-tally with the aim of exploiting existing knowledge to adapt quickly to new tasks. In CL, the training data of different classes comes in a phase-by-phase manner, where the model is trained on new class data at each task and then evaluated on all learned old and new classes. During training, a small number of exemplars of old classes are allowed to be saved
Figure 1. Model Size vs. Average Incremental Accuracy. Tak-ing class incremental learning [45] as an example, we use two typ-ical settings on CIFAR-100 [39], where 50 classes are for the first task with later (a) 10 classes per task (6-task sequence) and (b) 2 classes per task (26-task sequence). BiMeCo significantly out-performs other CL methods with 1.51∼2.06x smaller parameters under ResNet-18/34/50/101 [26]. More details are in Appendix. in the memory buffer with a strict budget restriction. These limited exemplars mitigate forgetting of previously learned tasks to some extent, but lead to another pressing issue – the extreme data imbalance between new and old classes, making the model biased towards recently learned tasks.
A wide variety of learning algorithms have been pro-posed to tackle the above shortcomings, including approx-imating exemplars per class for model inference [51], rec-tifying the network predictions to alleviate the task bias [5, 61], and preserving the original model on old classes to pro-vide soft labels for resisting to activating drifting [29, 42].
Recent studies such as CwD [55] also show that merely im-proving the learned representations of the initial task can facilitate mitigating forgetting greatly.
Although the achievements in the literature are brilliant, these methods still struggle to prevent forgetting when tack-ling long task sequences. In Fig. 1, we provide an empir-ical evaluation of representative CL methods on CIFAR-100 [39]. Though achieving decent accuracy in the incre-mental setting of 6 tasks, surprisingly they encounter ev-ident damages when handling 26 tasks. A vivid exam-ple is CwD [55], compared with fine-tuning with exem-plars (Replay), it boosts accuracy by more than 17% on the 6-task sequence under various ResNet-type networks [26], but merely brings around 10% accuracy gains on the more challenging 26-task sequence.
As pointed out in [18, 45], increasing memory buffer sizes or model parameters typically can alleviate forgetting for tackling long task sequences. However, we argue these strategies could be sub-optimal, since a desirable continual learning system should be able to scale to a wide variety of incremental scenarios without the need for extra overheads.
In this work, we approach this problem from a rather different perspective, and we study how to sufficiently mine memory samples to form structured knowledge for all learned tasks. To this end, we present Bilateral Memory
Consolidation (BiMeCo) as a simple framework for con-tinual learning. Specifically, BiMeCo explicitly disentan-gles model parameters into two complementary and parallel parts: short-term memory module and long-term memory module. Short-term memory module focuses on the repre-sentation ability of the model and rapid adaptation to recent tasks to form working memory, while being guided by the long-term memory module to prevent forgetting by knowl-edge distillation. Concurrently, long-term memory mod-ule learns a few task-balance samples online to consolidate working memory, while inheriting strong expressiveness from the short-term memory module by momentum-based updating. The key to BiMeCo is encouraging dynamic in-teractions between different degrees of memories to pro-duce rich feature representations. This way, BiMeCo en-ables continually evoking previous knowledge while learn-ing from changing data streams.
In implementation, we present a simple module replacement to integrate BiMeCo with existing models in a parameter-efficient manner.
We demonstrate that the proposed BiMeCo is signifi-cantly superior to other CL methods in a variety of aspects with great parameter efficiency. Fig. 1 illustrates the quan-titative results of our BiMeCo on CIFAR-100 [39] of two incremental settings under four ResNet-type networks [26].
Specifically, integrated with LUCIR [29], BiMeCo brings in a gain of at least +2.58% average incremental accuracy while significantly reducing parameters over other meth-ods on the 6-task sequence under ResNet-18. BiMeCo also achieves a superior result compared with CwD [55], which brings +5.52% accuracy gains with the reduction of param-eters by 41.9% on the 26-task sequence under ResNet-101.
Notably, BiMeCo is flexible and can be compatible with ex-isting methods seamlessly with further performance gains.
The key contributions can be summarized as follow:
• The proposed BiMeCo decouples model parameters into two complementary memories, encouraging dy-namic interactions between different degrees of mem-ories to prevent forgetting for continual learning.
• Extensive experiments as well as comprehensive ab-lation studies demonstrate the effectiveness of our
BiMeCo on three challenging benchmarks under a wide variety of incremental settings. 2.