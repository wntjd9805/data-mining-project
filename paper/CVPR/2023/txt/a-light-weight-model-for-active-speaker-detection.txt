Abstract
Active speaker detection is a challenging task in audio-visual scenarios, with the aim to detect who is speaking in one or more speaker scenarios. This task has received con-siderable attention because it is crucial in many applica-tions. Existing studies have attempted to improve the per-formance by inputting multiple candidate information and designing complex models. Although these methods have achieved excellent performance, their high memory and computational power consumption render their application to resource-limited scenarios difficult. Therefore, in this study, a lightweight active speaker detection architecture is constructed by reducing the number of input candidates, splitting 2D and 3D convolutions for audio-visual feature extraction, and applying gated recurrent units with low computational complexity for cross-modal modeling. Ex-perimental results on the AVA-ActiveSpeaker dataset reveal that the proposed framework achieves competitive mAP per-formance (94.1% vs. 94.2%), while the resource costs are significantly lower than the state-of-the-art method, partic-ularly in model parameters (1.0M vs. 22.5M, approximately 23×) and FLOPs (0.6G vs. 2.6G, approximately 4×). Ad-ditionally, the proposed framework also performs well on the Columbia dataset, thus demonstrating good robustness.
The code and model weights are available at https:
//github.com/Junhua-Liao/Light-ASD. 1.

Introduction
Active speaker detection is a multi-modal task that aims to identify active speakers from a set of candidates in ar-bitrary videos. This task is crucial in speaker diariza-tion [7, 42], speaker tracking [28, 29], automatic video edit-ing [10, 20], and other applications, and thus has attracted considerable attention from both the industry and academia.
*Corresponding author
Figure 1. mAP vs. FLOPs, size ∝ parameters. This figure shows the mAP of different methods [1,2,19,23,37,45] on the benchmark and the FLOPs required to predict one frame containing three can-didates. The size of the blobs is proportional to the number of model parameters. The legend shows the size of blobs correspond-ing to the model parameters from 1 × 106 to 30 × 106.
Research on active speaker detection dates back more than two decades [8, 35]. However, the lack of reliable large-scale data has delayed the development of this field.
With the release of the first large-scale active speaker detec-tion dataset, AVA-ActiveSpeaker [33], significant progress has been made in this field [15, 37, 38, 40, 47], following the rapid development of deep learning for audio-visual tasks [22]. These studies improved the performance of ac-tive speaker detection by inputting face sequences of mul-tiple candidates simultaneously [1, 2, 47], extracting visual features with 3D convolutional neural networks [3, 19, 48], and modeling cross-modal information with complex atten-tion modules [9, 44, 45], etc, which resulted in higher mem-ory and computation requirements. Therefore, applying the existing methods to scenarios requiring real-time process-ing with limited memory and computational resources, such as automatic video editing and live television, is difficult.
This study proposes a lightweight end-to-end architec-ture designed to detect active speakers in real time, where improvements are made from the three aspects of: (a) Sin-gle input: inputting a single candidate face sequence with the corresponding audio; (b) Feature extraction: split-ting the 3D convolution of visual feature extraction into 2D and 1D convolutions to extract spatial and temporal infor-mation, respectively, and splitting the 2D convolution for audio feature extraction into two 1D convolutions to ex-tract the frequency and temporal information; (c) Cross-modal modeling: using gated recurrent unit (GRU) [6] with less calculation, instead of complex attention mod-ules, for cross-modal modeling. Based on the character-istics of the lightweight architecture, a novel loss function is designed for training. Figure 1 visualizes multiple met-rics of different active speaker detection approaches. The experimental results reveal that the proposed active speaker detection method (1.0M params, 0.6G FLOPs, 94.1% mAP) significantly reduces the model size and computational cost, and its performance is still comparable to that of the state-of-the-art method [23] (22.5M params, 2.6G FLOPs, 94.2% mAP) on the benchmark. Moreover, the proposed method demonstrates good robustness in cross-dataset test-ing. Finally, the single-frame inference time of the proposed method ranges from 0.1ms to 4.5ms, which is feasible for deployment in real-time applications.
The major contributions can be summarized as follows:
• A lightweight design is developed from the three as-pects of information input, feature extraction, and cross-modal modeling; subsequently, a lightweight and effective end-to-end active speaker detection framework is proposed. In addition, a novel loss func-tion is designed for training.
• Experiments on AVA-ActiveSpeaker [33], a bench-mark dataset for active speaker detection released by
Google, reveal that the proposed method is comparable to the state-of-the-art method [23], while still reducing model parameters by 95.6% and FLOPs by 76.9%.
• Ablation studies, cross-dataset testing, and qualitative analysis demonstrate the state-of-the-art performance and good robustness of the proposed method. 2.