Abstract
Referring image segmentation (RIS) aims to find a seg-mentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive.
To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leverag-ing the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that cap-tures global and local contextual information of an input im-age. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed instance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expres-sion while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed method outperforms several zero-shot baselines of the task and even the weakly supervised referring expression segmentation method with substantial margins. Our code is available at https://github.com/Seonghoon-Yu/Zero-shot-RIS. 1.

Introduction
Recent advances of deep learning has revolutionised com-puter vision and natural language processing, and addressed various tasks in the field of vision-and-language [4, 19, 27, 28, 36, 43, 50]. A key element in the recent success of the multi-modal models such as CLIP [43] is the contrastive image-text pre-training on a large set of image and text pairs.
It has shown a remarkable zero-shot transferability on a wide range of tasks, such as object detection [9, 10, 13], semantic segmentation [7, 12, 59, 63], image captioning [40], visual question answering (VQA) [47] and so on.
Despite its good transferability of pre-trained multi-modal models, it is not straightforward to handle dense prediction tasks such as object detection and image segmentation. A pixel-level dense prediction task is challenging since there
Figure 1. Illustrations of the task of referring image segmenta-tion and motivations of global-local context features. To find the grounded mask given an expression, we need to understand the relations between the objects as well as their semantics. is a substantial gap between the image-level contrastive pre-training task and the pixel-level downstream task such as se-mantic segmentation. There have been several attempts to re-duce gap between two tasks [44, 54, 63], but these works aim to fine-tune the model consequently requiring task-specific dense annotations, which is notoriously labor-intensive and costly.
Referring image segmentation is a task to find the specific region in an image given a natural language text describing the region, and it is well-known as one of challenging vision-and-language tasks. Collecting annotations for this task is even more challenging as the task requires to collect precise referring expression of the target region as well as its dense mask annotation. Recently, a weakly-supervised referring image segmentation method [48] is proposed to overcome this issue. However, it still requires high-level text expres-sion annotations pairing with images for the target datasets and the performance of the method is far from that of the supervised methods. To tackle this issue, in this paper, we fo-cus on zero-shot transferring from the pre-trained knowledge
of CLIP to the task of referring image segmentation.
Moreover, this task is challenging because it requires high-level understanding of language and comprehensive understanding of an image, as well as a dense instance-level prediction. There have been several works for zero-shot semantic segmentation [7, 12, 59, 63], but they cannot be directly extended to the zero-shot referring image segmenta-tion task because it has different characteristics. Specifically, the semantic segmentation task does not need to distinguish instances, but the referring image segmentation task should be able to predict an instance-level segmentation mask. In addition, among multiple instances of the same class, only one instance described by the expression must be selected.
For example, in Figure 1, there are two cats in the input image. If the input text is given by “a cat is lying on the seat of the scooter”, the cat with the green mask is the proper output. To find this correct mask, we need to understand the relation between the objects (i.e. “lying on the seat”) as well as their semantics (i.e. “cat”, “scooter”).
In this paper, we propose a new baseline of zero-shot refer-ring image segmentation task using a pre-trained model from
CLIP, where global and local contexts of an image and an ex-pression are handled in a consistent way. In order to localize an object mask region in an image given a textual referring expression, we propose a mask-guided visual encoder that captures global and local context information of an image given a mask. We also present a global-local textual encoder where the local-context is captured by a target noun phrase and the global context is captured by a whole sentence of the expressions. By combining features in two different context levels, our method is able to understand a comprehensive knowledge as well as a specific trait of the target object. Note that, although our method does not require any additional training on CLIP model, it outperforms all baselines and the weakly supervised referring image segmentation method with a big margin.
Our main contributions can be summarised as follows:
• We propose a new task of zero-shot referring image segmentation based on CLIP without any additional training. To the best of our knowledge, this is the first work to study the zero-shot referring image segmenta-tion task.
• We present a visual encoder and a textual encoder that integrates global and local contexts of images and sen-tences, respectively. Although the modalities of two encoders are different, our visual and textual features are dealt in a consistent way.
• The proposed global-local context features take full advantage of CLIP to capture the target object seman-tics as well as the relations between the objects in both visual and textual modalities.
• Our method consistently shows outstanding results com-pared to several baseline methods, and also outperforms the weakly supervised referring image segmentation method with substantial margins. 2.