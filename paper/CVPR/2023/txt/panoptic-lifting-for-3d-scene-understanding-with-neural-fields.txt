Abstract
We propose Panoptic Lifting, a novel approach for learn-ing panoptic 3D volumetric representations from images of in-the-wild scenes. Once trained, our model can render color images together with 3D-consistent panoptic segmen-tation from novel viewpoints. Unlike existing approaches which use 3D input directly or indirectly, our method re-quires only machine-generated 2D panoptic segmentation masks inferred from a pre-trained network. Our core con-tribution is a panoptic lifting scheme based on a neural field representation that generates a unified and multi-view con-sistent, 3D panoptic representation of the scene. To ac-count for inconsistencies of 2D instance identifiers across views, we solve a linear assignment with a cost based on the model’s current predictions and the machine-generated segmentation masks, thus enabling us to lift 2D instances to 3D in a consistent way. We further propose and ablate contributions that make our method more robust to noisy, machine-generated labels, including test-time augmenta-tions for confidence estimates, segment consistency loss, bounded segmentation fields, and gradient stopping. Ex-perimental results validate our approach on the challeng-ing Hypersim, Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6% in scene-level PQ over state of the art. 1.

Introduction
Robust panoptic 3D scene understanding models are key to enabling applications such as VR, robot navigation, or self-driving, and more. Panoptic image understanding – the task of segmenting a 2D image into categorical “stuff” areas and individual “thing” instances – has experienced tremen-dous progress over the past years. These advances can be at-tributed to continuously improved model architectures and the availability of large-scale labeled 2D datasets, leading to state-of-the-art 2D panoptic segmentation models [6,19,43] that generalize well to unseen images captured in the wild.
Single-image panoptic segmentation, unfortunately, is still insufficient for tasks requiring coherency and consis-tency across multiple views. In fact, panoptic masks often contain view-specific imperfections and inconsistent clas-sifications, and single-image 2D models naturally lack the ability to track unique object identities across views (see
Fig. 2). Ideally, such consistency would stem from a full, 3D understanding of the environment, but lifting machine-generated 2D panoptic segmentations into a coherent 3D panoptic scene representation remains a challenging task.
Project page: nihalsid.github.io/panoptic-lifting/
Recent works [11,18,40,45] have addressed panoptic 3D scene understanding from 2D images by leveraging Neu-ral Radiance Fields (NeRFs) [22], gathering semantic scene data from multiple sources. Some works [11, 40] rely on ground truth 2D and 3D labels, which are expensive and time-consuming to acquire. The work of Kundu et al. [18] instead exploits machine-generated 3D bounding box detec-tion and tracking together with 2D semantic segmentation, both computed using off-the-shelf models. However, this approach is limited by the fact that 3D detection models, when compared to 2D panoptic segmentation ones, strug-gle to generalize beyond the data they were trained on. This is in large part due to the large difference in scale between 2D and 3D training datasets. Furthermore, dependence on multiple pre-trained models increases complexity and intro-duces potentially compounding sources of error.
In this work we introduce Panoptic Lifting, a novel for-mulation which represents a static 3D scene as a panoptic radiance field (see Sec. 3.2). Panoptic Lifting supports ap-plications like novel panoptic view synthesis and scene edit-ing, while maintaining robustness to a variety of diverse in-put data. Our model is trained from only 2D posed images and corresponding, machine-generated panoptic segmenta-tion masks, and can render color, depth, semantics, and 3D-consistent instance information for novel views of the scene.
Starting from a TensoRF [4] architecture that encodes density and view-dependent color information, we intro-duce lightweight output heads for learning semantic and instance fields. The semantic field, represented as a small
MLP, is directly supervised with the machine-generated 2D labels. An additional segment consistency loss guides this supervision to avoid optima that fragment objects in the presence of label inconsistencies. The 3D instance field is modelled by a separate MLP, holding a fixed number of class-agnostic, 3D-consistent surrogate object identifiers.
Losses for both the fields are weighted by confidence esti-mates obtained by test-time augmentation on the 2D panop-tic segmentation model. Finally, we discuss specific tech-niques, e.g. bounded segmentation fields and stopping semantics-to-geometry gradients (see Sec. 3.3), to further limit inconsistent segmentations.
In summary, our contributions are:
• A novel approach to panoptic radiance field represen-tation that models the radiance, semantic class and in-stance id for each point in the space for a scene by directly lifting machine-generated 2D panoptic labels.
• A robust formulation to handle inherent noise and in-consistencies in machine-generated labels, resulting in a clean, coherent and view-consistent panoptic seg-mentations from novel views, across diverse data.
Figure 2. Predictions from state-of-the-art 2D panoptic segmen-tation methods such as Mask2Former [6] are typically noisy and inconsistent when compared across views of the same scene. Typ-ical failure modes include conflicting labels (e.g. sofa predicted as a bed above) and segmentations (e.g. labeled void above). Fur-thermore, instance identities are not preserved across frames (rep-resented as different colors). 2.