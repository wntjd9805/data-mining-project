Abstract
Next generation cellular networks will implement ra-dio sensing functions alongside customary communications, thereby enabling unprecedented worldwide sensing coverage outdoors. Deep learning has revolutionised computer vision but has had limited application to radio perception tasks, in part due to lack of systematic datasets and benchmarks dedicated to the study of the performance and promise of radio sensing. To address this gap, we present MaxRay: a synthetic radio-visual dataset and benchmark that facil-itate precise target localisation in radio. We further pro-pose to learn to localise targets in radio without supervision by extracting self-coordinates from radio-visual correspon-dence. We use such self-supervised coordinates to train a radio localiser network. We characterise our performance against a number of state-of-the-art baselines. Our results indicate that accurate radio target localisation can be au-tomatically learned from paired radio-visual data without labels, which is important for empirical data. This opens the door for vast data scalability and may prove key to re-alising the promise of robust radio sensing atop a unified communication-perception cellular infrastructure. Dataset will be hosted on IEEE DataPort. 1.

Introduction
Sixth-generation (6G) wireless networks are being de-signed from the ground up to support sensing at the physical layer [79]. Such a brand new capability in 6G networks marks a departure from communication-only functions, and aims to supply applications with sensing primitives atop a unified communication-perception infrastructure. Con-cretely, dense cellular deployments in urban settings (e.g., per lamppost) would allow for unprecedented radio cover-age, enabling a multitude of challenging perception tasks.
Examples include around-the-corner obstacle detection in support of autonomous driving and pedestrian and drone localisation, to name a few [2].
*Correspondence to alloulah@outlook.com
†Work done whilst at Bell Labs.
Figure 1. We train a radio localisation network by using com-monalities with vision to drive spatial attention. Without laborious manual annotations, we learn to suppress clutter and localise targets in radio heatmaps.
Training perception models for radio signals is a key challenge for network infrastructure vendors. Unlike vision and audio, radio signals are hard to label manually because they are not human interpretable. Typically, sparse radio signals have been paired with a groundtruth vision modality for reliable semantic and qualitative filtration via a cross-modal annotation flow [34, 56, 77, 85]. Recently, this radio-visual pairing has been shown to work in a self-supervised fashion [10], building on a wave of progress in vision self-supervised learning (SSL) [19,22,26,40–42,45,61,62,82,84].
Computer vision has traditionally benefited from syn-thetic datasets for: (a) content augmentation for enhanced generalisability [54, 81], or (b) closing the learning loop on out-of-distribution failure modes [68], e.g., in the context of autonomous driving [1]. Extrapolating from vision, it is also likely that synthetic data will play an important role towards realising robust radio sensing. However, radio per-ception tasks have yet to benefit from such publicly available datasets.
In this work we aim to support next-gen 6G percep-tion tasks, while championing a self-supervised radio-visual learning approach. Concretely, Fig. 1 captures the crux of our new machine learning proposition for radio sensing. We demonstrate how to automatically extract radio self-labels through cross-modal learning with vision. We then use such self-labels to train a downstream localiser network. We show
that our self-supervised localiser net enhances estimation in the radio domain compared to state-of-the-art. Our contribu-tions are:
• A synthetic dataset: We curate and synthesise radio-visual data for a new learning task designed for target detection and localisation in radio.
• A cross-modal SSL algorithm: We formulate a con-trastive radio-visual objective for label-free radio local-isation.
• Evaluation: We conduct numerous characterisations on synthetic and empirical data in order to validate our
SSL algorithm and expose its superior performance compared to state-of-the-art.
We discuss our dataset and algorithmic findings to galvanise machine learners’ interest in radio-visual learning research.
We hope to both facilitate and inform future research on this new cross-modal learning paradigm. 2.