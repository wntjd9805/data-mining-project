Abstract
Multimodal alignment facilitates the retrieval of in-stances from one modality when queried using another. In this paper, we consider a novel setting where such an align-ment is between (i) instruction steps that are depicted as assembly diagrams (commonly seen in Ikea assembly man-uals) and (ii) segments from in-the-wild videos; these videos comprising an enactment of the assembly actions in the real world. We introduce a supervised contrastive learning ap-proach that learns to align videos with the subtle details of assembly diagrams, guided by a set of novel losses. To study this problem and evaluate the effectiveness of our method, we introduce a new dataset: IAW—for Ikea assembly in the wild—consisting of 183 hours of videos from diverse fur-niture assembly collections and nearly 8,300 illustrations from their associated instruction manuals and annotated for their ground truth alignments. We define two tasks on this dataset: First, nearest neighbor retrieval between video segments and illustrations, and, second, alignment of in-struction steps and the segments for each video. Extensive experiments on IAW demonstrate superior performance of our approach against alternatives. 1.

Introduction
The rise of Do-It-Yourself (DIY) videos on the web has made it possible even for an unskilled person (or a skilled robot) to imitate and follow instructions to complete com-plex real world tasks [4, 23, 31]. One such task that is of-ten cumbersome to infer from instruction descriptions yet easy to imitate from a video is the task of assembling fur-niture from its parts. Often times the instruction steps in-volved in such a task are depicted in pictorial form, so that
*Supported by an ANU-MERL PhD scholarship agreement.
†Supported by Marie Sklodowska-Curie grant agreement No. 893465.
‡Supported by an ARC Future Fellowship No. FT200100421.
Figure 1. An illustration of video-diagram alignment between a
YouTube video (top) He0pCeCTJQM and an Ikea furniture man-ual (bottom) s49069795. they are comprehensible beyond the boundaries of language (e.g., Ikea assembly manuals). However, such instructional diagrams can sometimes be ambiguous, unclear, or may not match the furniture parts at disposal due to product variabil-ity. Having access to video sequences that demonstrate the precise assembly process could be very useful in such cases.
Unfortunately, most DIY videos on the web are created by amateurs and often involve content that is not necessarily related to the task at hand. For example, such videos may include commentary about the furniture being assembled, or personal assembly preferences that are not captured in the instruction manual. Further, there could be large collections of videos on the web that demonstrate the assembly pro-cess for the same furniture but in diverse assembly settings; watching them could consume significant time from the as-sembly process. Thus, it is important to have a mechanism that can effectively align relevant video segments against the instructions steps illustrated in a manual.
In this paper, we consider this novel task as a multimodal alignment problem [25, 27], specifically for aligning in-the-wild web videos of furniture assembly and the respective diagrams in the instruction manuals as shown in Fig. 1. In contrast to prior approaches for such multimodal alignment, which usually uses audio, visual, and language modalities,
our task of aligning images with video sequences brings in several unique challenges. First, instructional diagrams can be significantly more abstract compared to text and audio descriptions. Second, illustrations of the assembly process can vary subtly from step-to-step (e.g., a rectangle placed on another rectangle could mean placing a furniture part on top of another). Third, the assembly actions, while depicted in a form that is easy for humans to understand, can be incom-prehensible for a machine. And last, there need not be com-mon standard or visual language followed when creating such manuals (e.g., a furniture piece could be represented as a rectangle based on its aspect ratio, or could be marked with an identifier, such as a part number). These issues make automated reasoning of instruction manuals against their video enactments extremely challenging.
In order to tackle the above challenges, we propose a novel contrastive learning framework for aligning videos and instructional diagrams, which better suits the specifics of our task. We utilize two important priors—a video only needs to align with its own manual and adjacent steps in a manual share common semantics—that we encode as terms in our loss function with multimodal features computed from video and image encoder networks.
To study the task in a realistic setting, we introduce a new dataset as part of this paper, dubbed IAW for Ikea as-sembly in the wild. Our dataset consists of nearly 8,300 il-lustrative diagrams from 420 unique furniture types scraped from the web and 1,005 videos capturing real-world furni-ture assembly in a variety of settings. We used the Ama-zon Mechanical Turk to obtain ground truth alignments of the videos to their instruction manuals. The videos involve significant camera motions, diverse viewpoints, changes in lighting conditions, human poses, assembly actions, and tool use. Such in-the-wild videos offer a compelling setting for studying our alignment task within its full generality and brings with it a novel research direction for exploring the multimodal alignment problem with exciting real-world ap-plications, e.g., robotic imitation learning, guiding human assembly, etc.
To evaluate the performance of our learned alignment, we propose two tasks on our dataset: (i) nearest neighbor retrieval between videos and instructional diagrams, and (ii) alignment of the set of instruction steps from the manual to clips from an associated video sequence. Our experimental results show that our proposed approach leads to promising results against a compelling alternative, CLIP [27], demon-strating 9.68% improvement on the retrieval task and 12% improvement on the video-to-diagram alignment task. 2.