Abstract
Deep Learning-based Unsupervised Salient Object De-tection (USOD) mainly relies on the noisy saliency pseudo labels that have been generated from traditional handcraft methods or pre-trained networks. To cope with the noisy la-bels problem, a class of methods focus on only easy samples with reliable labels but ignore valuable knowledge in hard samples. In this paper, we propose a novel USOD method to mine rich and accurate saliency knowledge from both easy and hard samples. First, we propose a Confidence-aware
Saliency Distilling (CSD) strategy that scores samples con-ditioned on samples’ confidences, which guides the model to distill saliency knowledge from easy samples to hard sam-ples progressively. Second, we propose a Boundary-aware
Texture Matching (BTM) strategy to refine the boundaries of noisy labels by matching the textures around the pre-dicted boundaries. Extensive experiments on RGB, RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance. Code is available at www.github.com/moothes/A2S-v2. 1.

Introduction
Unsupervised Salient Object Detection (USOD) meth-ods aim to correctly localize and precisely segment salient objects simultaneously without using manual annotations.
Compared to the supervised methods, USOD methods can easily adapt to more practical scenarios (e.g., industrial or medical images) where a large number of labeled images may be very hard to collect. Moreover, USOD methods also can assist some related methods for other tasks, e.g., object recognition [16, 44] and object detection [11, 35]. However, diverse objects, complex backgrounds, and other challeng-*Corresponding author. is supported by the Key-This project
Area Research and Development Program of Guangdong Province (2019B010155003), the National Natural Science Foundation of China (U22A2095, 62072482, 62076258, 62206316), and the Guangdong NSF
Project (2022A1515011254). (a) Image (b) GT (c) RBD (d) HS (e) USPS (f) A2S (g) Activation (h) Ours
Figure 1. Saliency maps generated by USOD methods. Our result is generated based on the activation map of a deep network. ing conditions bring severe challenges to USOD methods.
Most Deep Learning-based (DL-based) methods [21, 29, 32, 55, 63, 66] base on the saliency cues extracted by tra-ditional SOD methods (Fig. 1-c and 1-d). These hand-crafted features related cues are employed as pseudo labels to train deep networks under certain constraints, e.g., bi-nary cross-entropy (BCE) loss. However, saliency cues by traditional methods usually shift away from target objects, especially in complex scenes. Moreover, conventional con-straints, such as BCE loss, works well on fully-supervised
SOD methods, but is suboptimal when fitting the noisy la-bels for unsupervised methods (Fig. 1-e). Recently, Zhou et al. [72] addressed the first issue by extracting saliency cues (Fig. 1-f) based on a unsupervisedly pre-trained net-work (Fig. 1-g) instead of using traditional methods. Dur-ing training, they focus on learning reliable saliency knowl-edge from easy samples, but ignore latent knowledge in hard samples. The main reason is that hard samples may be wrongly-labeled and corrupt the fragile saliency knowl-edge learned in the early training phase. Therefore, to lever-age hard samples, we argue that all samples should be em-ployed in a meaningful order (i.e., from high reliable to low reliable), which is crucial for mining accurate knowledge from noisy labels. Trained by such a strategy, the network can mine valuable knowledge from hard examples without corrupting the knowledge learned from easy samples.
Deep networks can learn to localize salient regions from
noisy labels [72], but still struggle to find the precise bound-aries of target objects. Generally, the appearance around saliency boundary has a similar texture as in saliency map.
Therefore, matching the textures between different maps can serve as a guidance for producing reasonable saliency boundaries. We will demonstrate that above strategies are applicable to multimodal data besides RGB image, includ-ing depth map, thermal image, and optical flow.
Based on the above analysis, we propose a novel frame-work to tackle the Unsupervised Salient Object Detection (USOD) tasks. Specifically, two strategies are proposed to mine saliency knowledge from noisy saliency labels. First, we propose a Confidence-aware Saliency Distilling (CSD) scheme that scores samples with noisy labels conditioned on samples’ confidences. Then, our CSD guides the net-work to learn saliency knowledge from easy samples to more complex ones progressively by employing an adaptive loss conditioned on the training progress. Second, we pro-pose a Boundary-aware Texture Matching (BTM) strategy to refine the saliency boundaries of noisy labels by matching the textures around the predicted boundaries. During train-ing, the predicted saliency boundaries are shifting toward surrounding edges in the appearance space of the whole im-age. Finally, guided by above two mechanisms, our method can produce high-quality pseudo labels to train general-ized saliency detectors. Extensive experiments on RGB,
RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art performance compared to existing USOD methods.
The main contributions of our novel USOD method are: 1. We propose a Confidence-aware Saliency Distilling (CSD) to mine rich and accurate saliency knowledge from noisy labels, which breaks through the limitation that existing methods cannot utilize hard samples. 2. We propose a Boundary-aware Texture Matching (BTM) to refine the boundary of the predicted saliency maps by matching textures in different spaces. 3. Extensive experiments on RGB, RGB-D, RGB-T and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance. 2.