Abstract
In this paper, our goal is to design a simple learning paradigm for long-tail visual recognition, which not only improves the robustness of the feature extractor but also alleviates the bias of the classifier towards head classes while reducing the training skills and overhead. We pro-pose an efficient one-stage training strategy for long-tailed visual recognition called Global and Local Mixture Con-sistency cumulative learning (GLMC). Our core ideas are twofold: (1) a global and local mixture consistency loss im-proves the robustness of the feature extractor. Specifically, we generate two augmented batches by the global MixUp and local CutMix from the same batch data, respectively, and then use cosine similarity to minimize the difference. (2)
A cumulative head-tail soft label reweighted loss mitigates the head class bias problem. We use empirical class fre-quencies to reweight the mixed label of the head-tail class for long-tailed data and then balance the conventional loss and the rebalanced loss with a coefficient accumulated by epochs. Our approach achieves state-of-the-art accuracy on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets.
Additional experiments on balanced ImageNet and CIFAR demonstrate that GLMC can significantly improve the gen-eralization of backbones. Code is made publicly available at https://github.com/ynu-yangpeng/GLMC. 1.

Introduction
Thanks to the available large-scale datasets, e.g., Im-ageNet [10], MS COCO [27], and Places [46] Database, deep neural networks have achieved dominant results in image recognition [15]. Distinct from these well-designed balanced datasets, data naturally follows long-tail distribu-tion in real-world scenarios, where a small number of head classes occupy most of the samples. In contrast, dominant
*Corresponding author
Figure 1. An overview of our GLMC: two types of mixed-label augmented images are processed by an encoder network and a pro-jection head to obtain the representation hg and hl. Then a predic-tion head transforms the two representations to output ug and ul.
We minimize their negative cosine similarity as an auxiliary loss in the supervised loss. sg(·) denotes stop gradient operation. tail classes only have a few samples. Moreover, the tail classes are critical for some applications, such as medical diagnosis and autonomous driving. Unfortunately, learning directly from long-tailed data may cause model predictions to over-bias toward the head classes.
There are two classical rebalanced strategies for long-tailed distribution, including resampling training data [7, 13, 35] and designing cost-sensitive reweighting loss func-tions [3, 20]. For the resampling methods, the core idea is to oversample the tail class data or undersample the head classes in the SGD mini-batch to balance training. As for
the reweighting strategy, it mainly increases the loss weight of the tail classes to strengthen the tail class. However, learning to rebalance the tail classes directly would damage the original distribution [45] of the long-tailed data, either increasing the risk of overfitting in the tail classes or sacri-ficing the performance of the head classes. Therefore, these methods usually adopt a two-stage training process [1,3,45] to decouple the representation learning and classifier fine-tuning: the first stage trains the feature extractor on the orig-inal data distribution, then fixes the representation and trains a balanced classifier. Although multi-stage training signifi-cantly improves the performance of long-tail recognition, it also negatively increases the training tricks and overhead.
In this paper, our goal is to design a simple learning paradigm for long-tail visual recognition, which not only improves the robustness of the feature extractor but also al-leviates the bias of the classifier towards head classes while reducing the training skills and overhead. For improving representation robustness, recent contrastive learning tech-niques [8,18,26,47] that learn the consistency of augmented data pairs have achieved excellence. Still, they typically train the network in a two-stage manner, which does not meet our simplification goals, so we modify them as an aux-iliary loss in our supervision loss. For head class bias prob-lems, the typical approach is to initialize a new classifier for resampling or reweighting training. Inspired by the cumula-tive weighted rebalancing [45] branch strategy, we adopt a more efficient adaptive method to balance the conventional and reweighted classification loss.
Based on the above analysis, we propose an efficient one-stage training strategy for long-tailed visual recogni-tion called Global and Local Mixture Consistency cumula-tive learning (GLMC). Our core ideas are twofold: (1) a global and local mixture consistency loss improves the ro-bustness of the model. Specifically, we generate two aug-mented batches by the global MixUp and local CutMix from the same batch data, respectively, and then use cosine sim-ilarity to minimize the difference. (2) A cumulative head-tail soft label reweighted loss mitigates the head class bias problem. Specifically, we use empirical class frequencies to reweight the mixed label of the head-tail class for long-tailed data and then balance the conventional loss and the rebalanced loss with a coefficient accumulated by epochs.
Our method is mainly evaluated in three widely used long-tail image classification benchmark datasets, which include CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets. Extensive experiments show that our approach outperforms other methods by a large margin, which ver-ifies the effectiveness of our proposed training scheme.
Additional experiments on balanced ImageNet and CIFAR demonstrate that GLMC can significantly improve the gen-eralization of backbones. The main contributions of our work can be summarized as follows:
• We propose an efficient one-stage training strategy called Global and Local Mixture Consistency cumu-lative learning framework (GLMC), which can effec-tively improve the generalization of the backbone for long-tailed visual recognition.
• GLMC does not require negative sample pairs or large batches and can be as an auxiliary loss added in super-vised loss.
• Our GLMC achieves state-of-the-art performance on three challenging long-tailed recognition bench-marks, including CIFAR10-LT, CIFAR100-LT, and
ImageNet-LT datasets. Moreover, experimental results on full ImageNet and CIFAR validate the effectiveness of GLMC under a balanced setting. 2.