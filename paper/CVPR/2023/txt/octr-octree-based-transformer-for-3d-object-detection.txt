Abstract
A key challenge for LiDAR-based 3D object detection is to capture sufﬁcient features from large scale 3D scenes es-pecially for distant or/and occluded objects. Albeit recent efforts made by Transformers with the long sequence mod-eling capability, they fail to properly balance the accuracy and efﬁciency, suffering from inadequate receptive ﬁelds or coarse-grained holistic correlations. In this paper, we pro-pose an Octree-based Transformer, named OcTr, to address this issue. It ﬁrst constructs a dynamic octree on the hier-archical feature pyramid through conducting self-attention on the top level and then recursively propagates to the level below restricted by the octants, which captures rich global context in a coarse-to-ﬁne manner while maintaining the computational complexity under control. Furthermore, for enhanced foreground perception, we propose a hybrid po-sitional embedding, composed of the semantic-aware po-sitional embedding and attention mask, to fully exploit se-mantic and geometry clues. Extensive experiments are con-ducted on the Waymo Open Dataset and KITTI Dataset, and
OcTr reaches newly state-of-the-art results. 1.

Introduction 3D object detection from point clouds has received ex-tensive attention during the past decade for its ability to pro-vide accurate and stable recognition and localization in au-tonomous driving perception systems. In this task, feature learning plays a very fundamental and crucial role; yet it is rather challenging due to not only the disordered and sparse nature of data sampling, but also to insufﬁcient acquisition under occlusion or at a distance. To address this issue, many methods have been proposed, which can be taxonomized into two major classes, i.e. grid-based and point-based. The former ﬁrst regularize point clouds into multi-view images or voxels and then apply 2D or 3D CNNs to build shape rep-*indicates the corresponding author.
Figure 1. Illustration of three sparsiﬁcation strategies of attention matrices. Fixed pattern (1) narrows receptive ﬁelds and set proxy (2) discards elaborate correlations. The proposed octree construc-tion (3) keeps the global receptive ﬁeld in a coarse-grained manner while maintaining ﬁne-grained representations. resentations [4, 52], while the latter directly conduct MLP based networks such as PointNet++ [33] and DGCNN [50] on original points for geometry description [32, 40, 42, 60].
Unfortunately, they fail to capture necessary context infor-mation through the small receptive ﬁelds in the deep mod-els, leading to limited results.
Witnessing the recent success of Transformers in NLP, many studies have investigated and extended such architec-tures for 3D vision [24, 29, 58, 61]. Transformers are re-puted to model long-range dependencies, delivering global receptive ﬁelds, and to be suitable for scattered inputs of ar-bitrary sizes. Meanwhile, in contrast to those static weights that are learned in convolutions, Transformers dynamically aggregate the input features according to the relationships
between tokens. Regarding the case in 3D object detection, compared to point-based Transformers [11,61], voxel-based ones show the superiority in efﬁciency. However, they tend to suffer heavy computations when dealing with large scale scenes because of the quadratic complexity of Transform-ers, with the underlying dilemma between the grid size and the grid amount in voxelization. Taking the KITTI dataset as an example, it is unrealistic for Transformers to operate on the feature map with the spatial shape of 200 5, which is commonly adopted in most of the detection heads
[38, 46, 52, 55]. 176
×
×
More recently, there have appeared an inﬂux of efﬁcient self-attention model variants that attempt to tackle long se-quences as input. They generally sparsify the attention ma-trix by ﬁxed patterns [7, 23, 34], learned patterns [21, 45] or a combination of them [1, 56]. Fixed patterns chunk the in-put sequence into blocks of local windows or dilation win-dows, whilst learned patterns determine a notion of token relevance and eliminate or cluster outliers. Speciﬁc to 3D object detection from point clouds, VoTr [24] modiﬁes self-attention with pre-deﬁned patterns including local windows and stride dilation ones in a sparse query manner, and the di-lation mechanism enlarges the receptive ﬁeld by sampling attending tokens in a radius. SST [9] splits input tokens into non-overlapping patterns in a block-wise way and enables window shifting to capture cross-window correlation. De-spite some improvements reported, they both only achieve bigger local receptive ﬁelds rather than the expected global ones, and computations still increase rapidly with the ex-pansion of receptive ﬁelds.
Another alternative on self-attention is to take advantage of a proxy memory bank which has the access to the entire sequence tokens [1, 2, 56]. By using a small number of in-duced proxies to compress the whole sequence, it diffuses the global context efﬁciently. VoxSet [12] adapts Set Trans-former [19] to 3D object detection and exploits an induced set to model a set-to-set point cloud translation. With the help of the compressed global proxies and Conv-FFN, it ob-tains a global receptive ﬁeld; nevertheless, as they admit, it is sub-optimal to set only a few latent codes as proxies for a large 3D scene, prone to impairing the representation of dif-ferent point cloud structures and their correlations. There-fore, there remains space for a stronger solution.
In this paper, we present a novel Transformer network, namely Octree-based Transformer (OcTr), for 3D object detection. We ﬁrstly devise an octree-based learnable sparse pattern, i.e. OctAttn, which meticulously and efﬁciently en-codes point clouds of scenes as shown in Fig. 1. The Oc-tAttn module constructs a feature pyramid by gathering and applies self-attention to the top level of the feature pyramid to select the most relevant tokens, which are deemed as the octants to be divided in the subsequent. When propagating to the level below, the key/value inputs are restricted by the octants from the top. Through recursively conducting this process, OctAttn captures rich global context features by a global receptive ﬁeld in a coarse-to-ﬁne manner while re-ducing the quadratic complexity of vanilla self-attention to the linear complexity. In addition, for better foreground per-ception, we propose a hybrid positional embedding, which consists of the semantic-aware positional embedding and at-tention mask, to fully exploit geometry and semantic clues.
Thanks to the designs above, OcTr delivers a competitive trade-off between accuracy and efﬁciency.
Our contribution is summarized in three-fold: 1. We propose OcTr for voxel-based 3D object detection, which efﬁciently learns enhanced representations by a global receptive ﬁeld with rich contexts. 2. We propose an octree-based learnable attention sparsi-ﬁcation scheme (OctAttn) and a hybrid positional em-bedding combining geometry and semantics. 3. We carry out experiments on the Waymo Open Dataset (WOD) and the KITTI dataset and report state-of-the-art performance with signiﬁcant gains on far objects. 2.