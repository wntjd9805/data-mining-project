Abstract
Fully test-time adaptation aims to adapt the network model based on sequential analysis of input samples dur-ing the inference stage to address the cross-domain per-formance degradation problem of deep neural networks.
We take inspiration from the biological plausibility learn-ing where the neuron responses are tuned based on a lo-cal synapse-change procedure and activated by competi-tive lateral inhibition rules. Based on these feed-forward learning rules, we design a soft Hebbian learning process which provides an unsupervised and effective mechanism for online adaptation. We observe that the performance of this feed-forward Hebbian learning for fully test-time adaptation can be significantly improved by incorporating a feedback neuro-modulation layer. It is able to fine-tune the neuron responses based on the external feedback gener-ated by the error back-propagation from the top inference layers. This leads to our proposed neuro-modulated Heb-bian learning (NHL) method for fully test-time adaptation.
With the unsupervised feed-forward soft Hebbian learning being combined with a learned neuro-modulator to capture feedback from external responses, the source model can be effectively adapted during the testing process. Experimen-tal results on benchmark datasets demonstrate that our pro-posed method can significantly improve the adaptation per-formance of network models and outperforms existing state-of-the-art methods. 1.

Introduction
Although deep neural networks have achieved great success in various machine learning tasks, their perfor-mance tends to degrade significantly when there is data shift [27, 55] between the training data in the source do-* Corresponding authors. main and the testing data in the target domain [40]. To ad-dress the performance degradation problem, unsupervised domain adaptation (UDA) [16, 38, 50] has been proposed to fine-tune the model parameters with a large amount of un-labeled testing data in an unsupervised manner. Source-free
UDA methods [33, 35, 67] aim to adapt the network model without the need to access the source-domain samples.
There are two major categories of source-free UDA methods. The first category needs to access the whole test dataset on the target domain to achieve their adaptation per-formance [35, 67]. Notice that, in many practical scenarios when we deploy the network model on client devices, the network model does not have access to the whole dataset in the target domain since collecting and constructing the test dataset on the client side is very costly. The second type of method, called fully test-time adaptation, only needs access to live streams of test samples [41, 64, 66], which is able to dynamically adapt the source model on the fly during the testing process. Existing methods for fully test-time adap-tation mainly focus on constructing various loss functions to regulate the inference process and adapt the model based on error back-propagation. For example, the TENT method
[66] updates the batch normalization module by minimizing an entropy loss. The TTT method [64] updates the feature extractor parameters according to a self-supervised loss on a proxy learning task. The TTT++ method [37] introduces a feature alignment strategy based on online moment match-ing. 1.1. Challenges in Fully Test-Time UDA
We recognize that most of the domain variations, such as changes in the visual scenes and image transformations or corruptions, are early layers of features in the semantic hierarchy [66]. They can be effectively captured and mod-eled by lower layers of the network model. From the per-spective of machine learning, early representations through the lower layer play an important role to capture the pos-terior distribution of the underlying explanatory factors for the observed input [1]. For instance, in deep neural network models, the early layers of the network tend to respond to corners, edges, or colors. In contrast, deeper layers respond to more class-specific features [72]. In the corruption test-time adaptation scenario, the class-specific features are al-ways the same because the testing datasets are the corrup-tion of the training domain. However, the early layers of models can be failed due to corruption.
Therefore, the central challenge in fully test-time UDA lies in how to learn useful early layer representations of the test samples without supervision. Motivated by this obser-vation, we propose to explore neurobiology-inspired Heb-bian learning for effective early-layer representation learn-ing and fully test-time adaptation. It has been recognized that the learning rule of supervised end-to-end deep neural network training using back-propagation and the learning rules of the early front-end neural processing in neurobiol-ogy are unrelated [28]. The responses of neurons in bio-logical neural networks are tuned by local pre-synaptic and post-synaptic activity, along with global variables that mea-sure task performance, rather than the specific activity of other neurons [69].
Figure 1. The feature map visualization after the first convolution layer obtained by different learning methods. 1.2. Hebbian Learning
Hebbian learning aims to learn useful early layer rep-resentations without supervision based on local synaptic plasticity rules, which is able to generate early representa-tions that are as good as those learned by end-to-end super-vised training with back-propagation [28, 52]. Drastically different from the current error back-propagation methods which require pseudo-labels or loss functions from the top network layers, Hebbian learning is a pure feed-forward adaptation process and does not require feedback from the distant top network layers. The responses of neurons are tuned based on a local synapse-change procedure and ac-tivated by competitive lateral inhibition rules [28]. Dur-ing the learning process, the strength of synapses under-goes local changes that are proportional to the activity of the pre-synaptic cell and dependent on the activity of the post-synaptic cell. It also introduces local lateral inhibition between neurons within a layer, where the synapses of hid-den units with strong responses are pushed toward the pat-terns that drive them, while those with weaker responses are pushed away from these patterns.
Existing literature has shown that early representa-tions learned by Hebbian learning are as well as back-propagation and even more robust in testing [28, 29, 52].
Figure 1 shows the feature maps learned by different meth-ods. The first row shows the original image. The second row shows the images in the target domain with significant image corruption. The third row shows the feature maps learned by the network model trained in the source domain for these target-domain images. The fourth row shows the feature maps learned by our Hebbian learning method. The last row (“oracle”) shows the feature maps learned with true labels. We can see that the unsupervised Hebbian learning is able to generate feature maps which are as good as those from supervised learning. 1.3. Our Major Idea
In this work, we observe that Hebbian learning, although provides a new and effective approach for unsupervised learning of early layer representation of the image, when directly applied to the network model, is not able to achieve satisfactory performance in fully test-time adaptation. First, the original hard decision for competitive learning is not suitable for fully test-time adaptation. Second, the Hebbian learning does not have an effective mechanism to consider external feedback, especially the feedback from the top net-work layers. We observe that, biologically, the visual pro-cessing is realized through hierarchical models considering a bottom-up early representation learning for the sensory input, and a top-down feedback mechanism based on pre-dictive coding [15, 56].
Motivated by this, in this work, we propose to develop a new approach, called neuro-modulated Hebbian learn-ing (NHL), for fully test-time adaptation. We first incor-porate a soft decision rule into the feed-forward Hebbian learning to improve its competitive learning. Second, we learn a neuro-modulator to capture feedback from exter-nal responses, which controls which type of feature is con-solidated and further processed to minimize the predictive error. During inference, the source model is adapted by the proposed NHL rule for each mini-batch of testing sam-ples during the inference process. Experimental results on benchmark datasets demonstrate that our proposed method can significantly improve the adaptation performance of network models and outperforms existing state-of-the-art methods.
1.4. Summary of Major Contributions mentation [21], object detection [41], etc.
To summarize, our major contributions include: (1) we identify that the major challenge in fully test-time adaptation lies in effective unsupervised learning of early layer representations, and explore neurobiology-inspired soft Hebbian learning for effective early layer representa-tion learning and fully test-time adaptation. (2) We develop a new neuro-modulated Hebbian learning method which combines unsupervised feed-forward Hebbian learning of early layer representation with a learned neuro-modulator to capture feedback from external responses. We analyze the optimal property of the proposed NHL algorithm based on free-energy principles [14, 15]. (3) We evaluate our pro-posed NHL method on benchmark datasets for fully test-time adaptation, demonstrating its significant performance improvement over existing methods. 2.