Abstract
We extend neural radiance ﬁelds (NeRFs) to dynamic large-scale urban scenes. Prior work tends to reconstruct single video clips of short durations (up to 10 seconds). Two reasons are that such methods (a) tend to scale linearly with the number of moving objects and input videos because a separate model is built for each and (b) tend to require su-pervision via 3D bounding boxes and panoptic labels, ob-tained manually or via category-speciﬁc models. As a step towards truly open-world reconstructions of dynamic cities, we introduce two key innovations: (a) we factorize the scene into three separate hash table data structures to efﬁciently encode static, dynamic, and far-ﬁeld radiance ﬁelds, and (b) we make use of unlabeled target signals consisting of
RGB images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most importantly, 2D optical ﬂow. Op-erationalizing such inputs via photometric, geometric, and feature-metric reconstruction losses enables SUDS to de-compose dynamic scenes into the static background, indi-vidual objects, and their motions. When combined with our multi-branch table representation, such reconstructions can be scaled to tens of thousands of objects across 1.2 million frames from 1700 videos spanning geospatial footprints of hundreds of kilometers, (to our knowledge) the largest dy-namic NeRF built to date. We present qualitative initial re-sults on a variety of tasks enabled by our representations, including novel-view synthesis of dynamic urban scenes, unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To compare to prior work, we also evaluate on KITTI and Virtual KITTI 2, surpassing state-of-the-art methods that rely on ground truth 3D bounding box annotations while being 10x quicker to train. 1.

Introduction
Scalable geometric reconstructions of cities have trans-formed our daily lives, with tools such as Google Maps and
Streetview [6] becoming fundamental to how we navigate and interact with our environments. A watershed moment
*Work done as an intern at Argo AI.
Figure 1. SUDS. We scale neural reconstructions to city scale by dividing the area into multiple cells and training hash table rep-resentations for each. We show our full city-scale reconstruction above and the derived representations below. Unlike prior meth-ods, our approach handles dynamism across multiple videos, dis-entangling dynamic objects from static background and modeling shadow effects. We use unlabeled inputs to learn scene ﬂow and semantic predictions, enabling category- and object-level scene manipulation. in the development of such technology was the ability to scale structure-from-motion (SfM) algorithms to city-scale footprints [4]. Since then, the advent of Neural Radiance
Fields (NeRFs) [33] has transformed this domain by allow-ing for photorealistic interaction with a reconstructed scene via view synthesis.
Recent works have attempted to scale such represen-tations to neighborhood-scale reconstructions for virtual drive-throughs [47] and photorealistic ﬂy-throughs [52].
However, these maps remain static and frozen in time.
This makes capturing bustling human environments—
complete with moving vehicles, pedestrians, and objects— impossible, limiting the usefulness of the representation.
Challenges. One possible solution is a dynamic NeRF that conditions on time or warps a canonical space with a time-dependent deformation [38]. However, reconstruct-ing dynamic scenes is notoriously challenging because the problem is inherently under-constrained, particularly when input data is constrained to limited viewpoints, as is typi-cal from egocentric video capture [20]. One attractive so-lution is to scale up reconstructions to many videos, per-haps collected at different days (e.g., by an autonomous ve-hicle ﬂeet). However, this creates additional challenges in jointly modeling ﬁxed geometry that holds for all time (such as buildings), geometry that is locally static but transient across the videos (such as a parked car), and geometry that is truly dynamic (such as a moving person).
SUDS. In this paper, we propose SUDS: Scalable Ur-ban Dynamic Scenes, a 4D representation that targets both scale and dynamism. Our key insight is twofold; (1) SUDS makes use of a rich suite of informative but freely avail-able input signals, such as LiDAR depth measurements and optical ﬂow. Other dynamic scene representations [27, 37] require supervised inputs such as panoptic segmentation la-bels or bounding boxes, which are difﬁcult to acquire with high accuracy for our in-the-wild captures. (2) SUDS de-composes the world into 3 components: a static branch that models stationary topography that is consistent across videos, a dynamic branch that handles both transient (e.g., parked cars) and truly dynamic objects (e.g., pedestrians), and an environment map that handles far-ﬁeld objects and sky. We model each branch using a multi-resolution hash table with scene partitioning, allowing SUDS to scale to an entire city spanning over 100 km2.
Contributions. We make the following contributions: (1) to our knowledge, we build the ﬁrst large-scale dynamic
NeRF, (2) we introduce a scalable three-branch hash table representation for 4D reconstruction, (3) we present state-of-the-art reconstruction on 3 different datasets. Finally, (4) we showcase a variety of downstream tasks enabled by our representation, including free-viewpoint synthesis, 3D scene ﬂow estimation, and even unsupervised instance seg-mentation and 3D cuboid detection. 2.