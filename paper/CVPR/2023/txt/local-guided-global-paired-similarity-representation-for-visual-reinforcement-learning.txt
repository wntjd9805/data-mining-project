Abstract
Recent vision-based reinforcement learning (RL) meth-ods have found extracting high-level features from raw pix-els with self-supervised learning to be effective in learning policies. However, these methods focus on learning global representations of images, and disregard local spatial struc-tures present in the consecutively stacked frames. In this paper, we propose a novel approach, termed self-supervised
Paired Similarity Representation Learning (PSRL) for effec-tively encoding spatial structures in an unsupervised manner.
Given the input frames, the latent volumes are first gener-ated individually using an encoder, and they are used to capture the variance in terms of local spatial structures, i.e., correspondence maps among multiple frames. This enables for providing plenty of fine-grained samples for training the encoder of deep RL. We further attempt to learn the global se-mantic representations in the action aware transform module that predicts future state representations using action vec-tors as a medium. The proposed method imposes similarity constraints on the three latent volumes; transformed query representations by estimated pixel-wise correspondence, pre-dicted query representations from the action aware transform model, and target representations of future state, guiding action aware transform with locality-inherent volume. Ex-perimental results on complex tasks in Atari Games and
DeepMind Control Suite demonstrate that the RL methods are significantly boosted by the proposed self-supervised learning of paired similarity representations. 1.

Introduction
Deep reinforcement learning (RL) has been an appealing tool for training agents to solve various tasks including com-plex control and video games [12]. While most approaches have focused on training RL agent under the assumption
This work was partly supported by IITP grant funded by the Ko-rea government (MSIT) (No.RS-2022-00155966, AI Convergence Inno-vation Human Resources Development (Ewha Womans University)) and the Mid-Career Researcher Program through the NRF of Korea (NRF-2021R1A2C2011624). † Corresponding author: dbmin@ewha.ac.kr that compact state representations are readily available, this assumption does not hold in the cases where raw visual ob-servations (e.g. images) are used as inputs for training the deep RL agent. Learning visual features from raw pixels only using a reward function leads to limited performance and low sample efficiency.
To address this challenge, a number of deep RL ap-proaches [1,10,38,40,43,44,46] leverage the recent advance of self-supervised learning which effectively extracts high-level features from raw pixels in an unsupervised fashion.
In [38, 46], they propose to train the convolutional encoder for pairs of images using a contrastive loss [24,50]. For train-ing the RL agent, given a query and a set of keys consisting of positive and negative samples, they minimize the contrastive loss such that the query matches with the positive sample more than any of the negative samples [38, 46]. While the parameters of the query encoder are updated through back-propagation using the contrastive loss [50], the parameters of the key encoder are computed with an exponential mov-ing average (EMA) of the query encoder parameters. The output representations of the query encoder are passed to the
RL algorithm for training the agent. These approaches have shown compelling performance and high sample efficiency on the complex control tasks when compared to existing image-based RL approaches [31, 33, 51].
While these approaches can effectively encode the global semantic representations of images with the self-supervised representation learning, there has been no attention on the local fine-grained structures present in the consecutively stacked images. Our key observation is that spatial defor-mation, i.e., the change in terms of the spatial structures across the consecutive frames, can provide plenty of local samples for training the RL agent. Establishing dense corre-spondence [19, 34, 39, 42, 55], which has been widely used for various tasks such as image registration and recognition in computer vision, can be an appropriate tool in modeling the local spatial deformation.
In this work, we propose a novel approach, termed self-supervised Paired Similarity Representation Learning (PSRL), that learns representations for deep RL by effec-tively encoding the spatial structures in a self-supervised
fashion. The query representations generated from an en-coder are used to predict the correspondence maps among the input frames. A correspondence aware transform is then applied to generate future representations. We further extend our framework by introducing the concept of fu-ture state prediction, originally used for action planning in
RL [8, 11], into the proposed action aware transform in order to learn temporally-consistent global semantic representa-tions. The proposed method is termed ‘Paired Similarity’ as it encodes both local and global information of agent obser-vations. More structured details of the terms are provided in the supplementary material due to lack of space. To learn the proposed paired similarity representation, we impose similarity constraints on the three representations; trans-formed query representations by the estimated pixel-wise correspondence, predicted query representations from the action aware transform module, and target representations of future state. When applying the paired similarity constraint, the prediction and projection heads of global similarity con-straint are shared with the local constraint head, inducing locality-inherent volume to guide the global prediction. Fi-nally, the well-devised paired similarity representation is then used as input to the RL policy learner.
We evaluate the proposed method with two challeng-ing benchmarks including Atari 2600 Games [31, 51] and
DMControl Suite [48], which are the common benchmarks adopted to evaluate the performance of recent sample-efficient deep RL algorithms. The proposed method com-petes favorably compared to the state-of-the-arts in 13 out of 26 environments on Atari 2600 Games and in 4 out of 6 tasks on DMControl Suite, in terms of cumulative rewards per episode.
We highlight our contributions as follows.
• While prior approaches place emphasis only on encod-ing global representations, our method takes advan-tage of spatial deformation to learn local fine-grained structures together, providing sufficient supervision for training the encoder of deep RL.
• We propose to impose the paired similarity constraints for visual deep RL by guiding the global prediction heads with locality-inherent volume.
• We introduce the action aware transform module to self-supervised framework to learn temporally-consistent instance discriminability by using action as a medium. 2.