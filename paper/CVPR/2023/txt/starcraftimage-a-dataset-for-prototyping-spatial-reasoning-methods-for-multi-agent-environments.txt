Abstract
Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or miss-ing data imputation are important for multiple applica-tions (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft
II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representa-tions for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplic-ity of these datasets, we construct a benchmark spatial rea-soning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully sum-marize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, includ-ing all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity:
Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com/. 1.

Introduction
Spatial tasks in multi-agent environments require rea-soning over both agents’ positions and the environmental context such as buildings, obstacles, or terrain features.
These complex spatial reasoning tasks have applications in autonomous driving, autonomous surveillance over sensor networks, or reinforcement learning (RL) as subtasks of the
‡DEVCOM Army Research Laboratory
†Corresponding Authors: Sean Kulinski skulinsk@purdue.edu and
David I. Inouye dinouye@purdue.edu.
Figure 1. Two samples (one per row) showing (Blue box/left) our 64 x 64 StarCraftHyper dataset which contains all unit IDs and corresponding values for both players (color for unit IDs denotes categorical unit ids), (Green box/middle) StarCraftCIFAR10 (32 x 32) which is easy to interpret where blue is player 1, red is player 2, and green are neutral units such as terrain or resources, and (Orange box/right) StarCraftMNIST (28 x 28) which are grayscale images further simplified to show player 1 as light-gray, player 2 as dark-gray, and neutral as medium-level shades of gray.
RL agent. For example, to predict a car collision, an au-tonomous driving system needs to reason about other cars, road conditions, road signs, and buildings. For autonomous surveillance over sensor networks, the system would need to reason over the positions of objects, buildings, and other agents to determine if a new agent is normal or abnormal or to impute missing sensor values. An RL system may want to predict the cumulative or final reward or impute miss-ing values given only an incomplete snapshot of the world state, i.e., partial observability. Yet, collecting large realistic datasets for these tasks is expensive and laborious.
Due to the challenge of collecting real-world data, prac-titioners have turned to (semi-)synthetic sources for creat-ing large clean datasets of photo-realistic images or videos
[11, 12, 24, 40]. For example, [11] leveraged the Grand
Theft Auto V game engine to collect a synthetic video dataset for pedestrian detection and tracking. [4] overlays aerial images with crowd simulations to provide a crowd density estimation dataset. Yet, despite near photo-realism, these prior datasets focus on simple multi-agent environ-ments (e.g., pedestrian-like simulations [11, 40]) and thus lack complex (or strategic) agent and object positioning. In sharp contrast to these prior datasets, human-based replays of the real-time strategy game StarCraft II capture complex strategic and naturally adversarial positioning of agents and objects (e.g., buildings and outposts). Indeed, the human player provides thousands of micro-commands that produce an overall intelligent and strategic positioning of agents and building units. The release of the StarCraft II API and
Python bindings [38] significantly reduces the barrier to us-ing this rich data source for multi-agent environments. Yet, the StarCraft II environment still requires significant over-head including game engine installation, looping through the game engine, understanding the API, etc. This greatly limits the broad adoption of this very rich source of multi-agent interactions as a benchmark dataset—in contrast to the classic and extremely easy-to-use MNIST [27] and CI-FAR10 [25] benchmark datasets that drove image classifi-cation research in the early years and continue to be used for prototyping new ML methods. In summary, prior multi-agent datasets either lack complex strategic behavior or re-quire significant implementation overhead.
To address these issues, we created StarCraftImage: a simplified image-based representation of human-played
StarCraft II matches to serve as a large-scale multi-agent spatial reasoning benchmark dataset that is as easy to use as MNIST and CIFAR10 while still exhibiting complex and strategic object positioning. As seen in Fig. 1, each image in StarCraftImage is akin to a detailed snapshot of the Star-Craft II minimap and includes the locations of all units (both moveable units and buildings), the units’ IDs, as well as im-portant metadata like which player won that match, player resource counts, the current map name, player ranking, etc. We made two key design decisions when developing
StarCraftImage. First, we chose to represent the matches by snapshot images that summarize a window of approxi-mately 10 seconds of gameplay rather than a video. This design choice was motivated both by the ease-of-use crite-ria (as images are easier to load and manipulate than videos) and by the goal of performing spatial rather than temporal reasoning tasks—though a video dataset for complex tem-poral reasoning is a natural direction for future work.
Our second design choice was to represent the matches via minimap-like images rather than photo-realistic render-ings of the game state. This choice was motivated by two reasons. First, minimap images are easy to use because they are small yet still represent of the whole environment. By using a minimap representation, we can encode the most crucial game information (unit types, recent troop move-ments, building locations, environmental features, etc.) in a naturally compact representation. Indeed, the minimap rep-resentation is critical for playing StarCraft II as evidenced by the following quote from the famous StarCraft II player
Day[9] (Sean Plott): “...the two most important things [are] the minimap and your money” [32]. Second, the minimap representation allows for us to have many diverse samples while still maintaining a small data footprint. The resul-tant smaller disk size allows for rapid prototyping via quick dataset downloads and swift data consumption. Compared to prior common spatial reasoning datasets, our proposed 3.6 million image dataset has a total disk size of 8.4Gb while the MOTSynth-MOT-CVPR22 dataset [11], which consists of 1.3 million images, has a disk size of 167Gb (20x larger, while containing half the number of samples). Ul-timately, we construct three different image representations with decreasing complexity: Hyperspectral images which give precise game state information by encoding the unit ids and last-seen timestamps at each spatial location (mimick-ing the hyperspectral geospatial representations), RGB im-ages that mimic CIFAR10, and grayscale images that mimic
MNIST. Thus, our dataset is compatible with common ML frameworks with minimal overhead or preprocessing effort.
Overall, we use 60k StarCraft II replays to create 3.6 mil-lion summary images (not multi-counting different repre-sentations) and corresponding metadata.
To demonstrate how multi-agent spatial reasoning tasks can be easily prototyped using StarCraftImage, we also pro-vide a series of benchmark tasks. We perform target identi-fication (i.e., determining unit type from only knowing unit locations) where the input is either an RGB or grayscale image and the target image is hyperspectral with each chan-nel corresponding to a unit type. We also perform more complex tasks such as map event prediction (i.e., game out-come and StarCraft race prediction) which serve as canon-ical image-level reasoning problems. To show how our image representations can be easily manipulated for other tasks (like Rotated MNIST [26] or Color MNIST [2]), we map missing data imputation as an image inpainting task using both simulated sensor network faults and the fog-of-war from the game engine. Ultimately, we hope to provide a large-scale and rich multi-agent spatial reasoning dataset that is very easy to use yet exhibits complex and strategic placement of agents for complex spatial reasoning applica-tions. We summarize our contributions as follows:
• We design and extract StarCraftImage as an easy-to-use multi-agent spatial reasoning dataset under three representations: 1) Hyperspectral images that encode all unit ids and lasts seen timestamps for each spatial location, 2) RGB images that mimic CIFAR10, and 3) grayscale images that mimic MNIST.
• We apply StarCraftImage on tasks such as target iden-tification, movement prediction, and more. We also propose several noise simulation models and discuss several task modifiers such as domain generalization.
• We publicly release the datasets with a permissive CC
Figure 2. An overview of our hyperspectral dataset from different perspectives. The raw image data is stored in texttt.png files using the bag-of-units representation. A logical view of the dataset is a (sparse) hyperspectral image with many channels that include unit information and visibility per player, resource information (neutral units), and map information. The bag-of-units representation enables processing this very high-dimensional dataset using dense matrices only and leveraging embedding layers that are often used for processing sequences of IDs; importantly, because the unit order does not matter, an order-invariant reduction such as max or sum should be used to arrive at a representation with a fixed number of embedding channels E.
BY 4.0 license. We also release the StarCraft II dataset extraction code and the relevant data loaders and mod-ules for using the data as a Python package with an
MIT license, and provide matainance as laid out in our dataset nutrition label: Table 4. 2. Dataset Extraction and Construction
In this section, we describe how we extract observational data from the simulated yet complex environment of the
StarCraft II (SC2) game. We then transform the raw data into the hyperspectral, CIFAR10, and MNIST formats that are readily usable in ML tools. 2.1. Extracting Raw Data From SC2 Replays
Due to SC2 being an almost entirely deterministic game, an SC2 replay file contains an entire list of actions from both players that can be used to re-simulate an entire match by passing the actions back to the SC2 game engine. Each replay file also contains metadata from the match such as: the length of the match, the map/arena the match took place in, and per-player statistics such as the match making rating (MMR) (which can be thought of as the skill level of that player), the actions-per-minute (APM) the player took, and whether that player won, lost, or tied the match. Addition-ally, Activison Blizzard (the maker of SC2) bundles large sets of these replays together as a Replay Pack for others to use. We used Replay Pack 3.16.1 - Pack 1 from [6].
To extract the game state, we used the PySC2 [38]
Python library developed for RL applications that interfaces with the SC2 game engine. PySC2 exposes the raw game state while re-simulating a match based on replay files.
Each raw game state consists of information such as the location, allegiance, size, unit type ID, and health of ev-ery unit (character, building, worker, solider, etc.) which currently exists for that specific frame (where a frame is a single unit of time in a game). The raw frame data also contains dynamic map information including the visibility for each player (the locations on the map that the player can see due to friendly units/scouts being in that area, ver-sus areas which are undiscovered and thus hidden) and the current creep state (which is a terrain feature consisting of purple slime in which most Zerg structures must be built and upon which Zerg units will move faster). However, since the
PySC2 interface was designed for interacting with StarCraft
II, it comes with a steep learning curve and a complex data representation – which greatly hinders our goal of having clean observed game states that can be represented in a stan-dard form. Thus, we use PySC2 to extract raw game state observations and process these into standard image formats. 2.2. StarCraftHyper: Construction and Processing of Hyperspectral Representation
Our most general format is a hyperspectral image for-mat where each channel represents information for each unit type for each player in SC2. To do this, we first use
PySC2 to extract raw frame data and for the f th frame ob-servation, we record the location of each unit present via
Hf [uP ID, xh, yh] = 1(uP ID, xh, yh) where 1 is an indica-tor function that returns 1 if a unit is present, else 0, uP ID is the player-specific unit ID (PID), and xh, yh is the spa-tial location of the unit. Since the raw data gives spatial information in raw game-map spatial coordinates, we must perform a coordinate transform to our square hyperspectral image coordinates: (xh, yh) =
. We also (cid:106) (xraw,yraw) (cid:107) max(xraw,yraw)
Figure 3. (Top) We embed the unit information of player 1, player 2, and neutral separately using an embedding of size 1. We then combine with other dense features (visibility for players and terrain info for neutral). Finally, we concatenate each output into a 3-channel 32x32 px RGB image where the neutral channel is down-weighted for visual clarity. (Bottom) We take the RGB color image, rescale the values of each channel, and overlay each channel into a single grayscale 28x28 px image where precedence is given to P1, then P2, and finally neutral or background. We use precedence combinations as linear combinations of the layers could lead to unit information being canceled. crop to only the playable area of the map. There are 170 unit IDs for Player1, 170 IDs for Player2 units, and 44 IDs for Neutral units, which is 384 P IDs (i.e., channels).
Next, to allow for video-like spatial movements, we form a stack of 255 consecutive hyperspectral images Hstack =
[Hf ] where f ∈ [0, 255].1 Given Hstack, which is a tensor with shape (255, 384, 64, 64), we simplify this from a video format to a static image format by collapsing the time axis to create the summarized hyperspectral image H. We do this by recording the frame index of the most recent frame where a unit was present for each (P ID, x, y) coordinate (i.e., H[c, x, y] = arg max f Hstack[f, c, x, y] ̸= 0) and if
Hstack[f, c, x, y] = 0 ∀f ∈ [0, 255], then H[c, x, y] = 0).
Another possibility would be to average over the window of frames instead of the last seen timestamp; however, the last seen timestamp enables simple visualization of movement via a ghosting-like effect, and the last seen timestamp pre-serves time information (albeit only a compressed amount).
While this condensed representation does have the trade-off that if a unit with the same P ID crosses the same (x, y) location more than once in a frame stack, only the last cross-ing will be recorded in H, this is rare and seems a reason-able trade-off for a much simpler representation. At this step, the non-zero entries of H are saved as the raw repre-sentation of our dataset—i.e., a sparse tensor representation.
We can compress H into a dense “bag-of-units” representa-tion, which is similar to representing a sequence of words by their IDs rather than by very high-dimensional one-hot vec-tors, but where the order of the IDs does not matter (hence, the term “bag” as in bag-of-words representations). Just as the number of words of a sentence can vary in NLP, the number of units at each location can vary. Therefore, as 1We chose 255 to ensure that the values fit in an unsigned 8-bit integer and captures roughly 10 seconds of real time. in processing word sequences, we pad the channels of the dense representation with zeros (representing no unit) up to the max number of units at any location (either in a sin-gle sample or in a batch of samples), denoted by k. Con-cretely, the bag-of-units representation collapses the chan-nel axis into k ID matrices and k timestamp matrices of size (64, 64), where the ID matrix contains the P ID of the units present at each (x, y) coordinate, the timestamp matri-ces contain the corresponding timestamp that the unit was last seen, and k is the max number of units present at one (x, y) location in H. This highly-compressed bag-of-units representation for the StarCraftHyper dataset can be seen in the top right of Fig. 2 and is the default representation for the StarCraftHyper dataset. 2.3. StarCraftCIFAR10 and StarCraftMNIST:
RGB and Grayscale Representations
To further simplify dataset usage and prototyping abil-ity, we develop datasets that mimic CIFAR10 and MNIST in terms of image size, number of channels, number of classes, and number of train/test samples as seen in (mid-dle) and (right) of Fig. 4. Thus, our StarCraftCIFAR10 and StarCraftMNIST datasets can be used for rapid initial prototyping of new spatial reasoning methods just as these ubiquitous datasets have been used for prototyping image classification. These can model situations where agent and building positions are known but the agent type is unknown (e.g., low resolution satellite images or a network of pres-sure sensors). One natural task is to infer unit types given only unit location information, which is discussed in more detail in future sections.
To construct StarCraftCIFAR10, we first follow the ap-proach in subsection 2.4 to subsample our StarCraftHyper dataset to 50,000 train windows and 10,000 test windows, to
Figure 4. (Left) Our 64 x 64 StarCraftHyper dataset contains all unit IDs and corresponding values for both players (color for unit IDs denotes categorical unit ids) where visibility is player specific but the terrain and pathing grid are shared (a few other layers are not shown, see appendix). (Middle) StarCraftCIFAR10 (32 x 32) is easy to interpret where blue is player 1, red is player 2, and green are neutral units which are usually just resources. (Right) MNIST (28 x 28) grayscale images are further simplified to show player 1 as white to white-gray, player 2 as black to black-gray, and neutral as shades of gray. match the dataset size of CIFAR10. To transform each hy-perspectral window into a CIFAR10 format, we separate H into player-specific images and follow the process shown in
Fig. 3 (top). To construct the StarCraftMNIST dataset, we similarly subsample from the full StarCraftHyper dataset, but to a size of 60,000 train and 10,000 test images as in
MNIST. We process the images in the manner seen at the top and bottom of Fig. 3, where the last step is a function that overlays the Vp1 , Vp2 , VN scaled maps on top of each other such that any non-zero elements of Vp2 will overwrite the nonzero elements of VN and nonzero Vp1 values will overwrite both. We decided to overwrite rather than aver-age because having a unit of player 1 and player 2 at the same location would average to a gray background value but that is in fact one of the most interesting locations. In the next section, we discuss the creation of the 10 classes for each window via a combination of the variables: Player 1 race, Player 2 race, and Player 1 outcome. and testing on the held out set. Additionally, for canonical class labels, we use the race of each player (Terran, Zerg, or
Protoss) and player 1’s outcome (Win and NotWin where
NotWin includes the rare Tie outcome) to split the over-all dataset into 18 classes (3 races for player 1, 3 races for player 2 and 2 outcomes). We chose these three variables (Player 1 outcome, Player 1 race, Player 2 race) because though outcome prediction is a canonical task, readily avail-able ground truth for race prediction with this dataset is akin to behavior or tactical strategy prediction, as unit type infor-mation is hidden in the StarCraftCIFAR10 and StarCraftM-NIST versions of the dataset. For StarCraftCIFAR10 and
StarCraftMNIST, we select only classes that have at least one player as Zerg (5 total) with both outcomes to get ex-actly 10 balanced classes to match the setup of CIFAR10 and MNIST—this could be done similarly for Terran and
Protoss but Zerg is the easiest to understand because some
Zerg-specific units are often spread across the battlefield. 2.4. Dataset Exploration and Analysis 3. Multi-Agent Spatial Reasoning Applications
All in all, the StarCraftImage dataset consists of 3,607,787 windows extracted from 60,000 replays which are readily available in three representations (examples in
Fig. 4). The image data for each window is stored as a .png file in the bag-of-units representation. The data can be ac-cessed via directly loading in the relevant .png file and metadata row, or more simply by using the corresponding
PyTorch dataset classes that we have developed (one class for each representation).
Jointly with the image data collection, we also aggre-gated relevant metadata for each window, such as the tem-poral location of the window in the overall match (e.g., 75th window of 130), which player won the match, the races of the players, the name of match’s map, etc. (for a full list of the metadata keys, please see Appendix D). This meta-data has many uses for filtering replays based on conditions for a specific application, e.g., training on a subset of maps
In this section, we list examples of spatial reasoning tasks on our datasets (e.g., global reasoning as a classifi-cation task). We will also discuss simple noise models that simulate more complex scenarios on top of the clean data representations. Finally, we discuss natural task modifiers such as domain generalization or adversarial contexts. In all cases, we aim for a compromise between realism and simplicity as this dataset is meant as an initial prototyping dataset for complex or strategic agent and object positioning rather than a fully realistic spatial reasoning dataset. Given space constraints, we provide demos of these tasks in the supplementary material both in Appendix F and as IPython notebooks in our code repository. 3.1. Spatial Reasoning Examples
Target identification (Image colorization) The goal here is to identify the unit type (e.g., marine unit) or af-filiation (player one, two or neutral) for every detected unit.
This can be seen as a setting where an image only shows if a unit exists in its field of view (e.g., an aerial photo from a UAV or a post-processed output from a LiDAR scanner).
For the task, we cast this problem as an image colorization problem in which the input is either a StarCraftCIFAR10 or
StarCraftMNIST and the target output is the corresponding
StarCraftHyper or StarCraftCIFAR10 image.
Movement prediction (Simplified Multi-Object Track-ing) Predicting what is going to happen next is clearly an important task especially in time-critical applications such as autonomous driving [12], disaster relief [1], or, more gen-erally, optical flow [3]. While we do not generally consider the time dimension after we summarize the window, for this task, we can use the metadata to create pairs of adja-cent window summary images where the input is the current summary image and the target output is the next summary image in the same match.
Predict final outcome or race (Classification) Spatial reasoning systems are often used to predict the global prop-erties of a system (e.g., crop yield predictions [31] or re-ward predictions for RL models [38]), which can be cast as classification. The most canonical task is to predict the final outcome of the game (i.e., which player will win), which re-quires reasoning over both fighting units and environmental factors such as buildings and resources (e.g., even if there is little movement/few fighting units in a window, a model can still predict who will win based off of who has the strongest base). Another canonical task for the simplified datasets
StarCraftCIFAR10 and StarCraftMNIST (which give only unit location information rather than unit type information) is to predict both players’ races, which requires recognizing the common placement configurations for each race.
Imputing missing data (Image inpainting) Another critical task in spatial reasoning is imputing missing values for areas that lack coverage due to occlusion, data collection failures, or adversarial attacks. [13, 39]. Here the input im-age is a corrupted version of a sample from one of the three datasets, and the target output image is the uncorrupted sample. Due to StarCraftImage’s simple minimap repre-sentation, simulating spatial corruptions (e.g., noisy mea-surements or partial observability) is simple to do–unlike in photo-realistic settings which would require editing the im-ages or videos to hide or remove information. In the next section, we go over examples of spatial corruption models. 3.2. Simulated Data Corruption Models or a hierarchical system where reasoning happens on a (po-tentially noisy) abstracted spatial representation. We can implement this as a type of salt and pepper noise where the salt noise can randomly add units to locations that do not have units (i.e., false positives) and each real unit could pos-sibly become missing (i.e., false negatives), as seen in the left of Fig. 5.
Heterogeneous partial observations (Image masking)
Here the images can be seen as the fusion of irregular het-erogeneous sensor networks. This can be simulated by pro-ducing a mask that is based on static sensor locations and detection ranges, see Fig. 5 (middle) for example. Further-more, detailed sensor models can be used to pre-process the masked observations to provide an accurate representation based on the type of sensor implemented at a particular lo-cation, e.g., acoustic sensors may only return a range of the unit relative to its position. Sensor faults as above could be implemented on top of this heterogeneous sensor network (e.g., masking over a set of sensors’ visible range). For ex-amples and benchmark results on such heterogeneous sen-sor placements with aggregation failure simulations, please see Appendix E.
Imprecise sensors (Blur) Low resolution imaging will yield imprecise unit locations. Thus, we can implement this noising process by performing blur operations on top of the original datasets. This corruption is simplest to apply to StarCraftCIFAR10 (e.g., Fig. 5, right) and StarCraftM-NIST via standard CV packages but could also be applied to StarCraftHyper (albeit with more computation). 3.3. Spatial Reasoning Task Modifiers
Robustness to distribution shift (Domain generalization)
A key challenge in applying ML to real-world settings is training a model in one context but applying it to another context [48]. This is known as the domain generalization problem in which the goal is to perform the task well on an unseen test domain [33]. The metadata that we provide can provide natural segmentations of the dataset into domains.
One of the most canonical examples of distribution shift in real-world settings is a change in the environment settings
[23]. While greatly simplified, we can simulate changes in location by splitting the dataset based on the SC2 map and holding out one or more maps for testing. Other excellent domain splits could be players’ MMR or APM, which cor-respond to their skill level and frequency of actions. Player two’s race (Terran, Protoss, or Zerg) is also another way to split the dataset into 9 domains such as Terran vs. Terran,
Protoss vs. Zerg, or Terran vs. Protoss.
Random additive noise This corruption model is relevant for settings where images are taken using noisy equipment
Robustness to adversarial attacks (Adversarial training)
While uncommon, adversarial attacks are a genuine concern
Figure 5. Three example noise corruption models which are simulated on top of the StarCraftCIFAR10 dataset, where (left) simulated ran-dom additive noise, (middle) simulates observations via a heterogeneous SN, and (right) simulates limited precision (blurry) observations. for reasoning methods, especially those which involve hu-mans such as autonomous driving. We can simulate this idea by applying adversarial training methods under differ-ent adversarial attack models such as L0 pixel-wise attacks
[36] for attacking individual units. The adversarial training literature already benchmarks using MNIST as a key diffi-cult example [30], and thus, these StarCraft datasets could be immediately relevant and provide a more realistic bench-mark for the adversarial training literature.
Equipment usage optimization (Active learning) Opti-mizing sensor location and power usage are key challenges in sensor networks [9, 14]. Following the simulated sensor network seen in the previous section, constrained power us-age could be framed as an active learning problem in which the algorithm can only query a fixed number of sensors for each prediction problem. For optimizing sensor location, the algorithm could attempt to determine where to place the next sensor (i.e., to uncover information at a certain lo-cation) to optimize the downstream task such as outcome classification. A more complex case is moving sensors from their original locations to another location under a budget on geographic movement (e.g., a sensor on a robotic device). 4. Benchmark Evaluations
While we point the reader to Appendix E, where we give full descriptions and results, here we introduce four bench-mark multi-agent spatial reasoning tasks, which incorporate training U-Net-based [35] ResNet [15] models. The four benchmark tasks consist of two tasks on target identifica-tion (given a 64x64 RGB image, predict the ID of each unit at each location) and two tasks for unit tracking (given hy-perspectral window k, predict what will happen in window k + 1). Both task sets consist of first training and evalu-ating on “clean” (unaltered) data. To highlight the extend-ability of StarCraftImage, we also perform both tasks on corrupted data that has been passed through a simulation of a noisy sensor network. The sensor network simulation consists of 50 imaging sensors with a radius of 5.5 pixels with different sensor placement methodologies (e.g., grid, random) and communication failures during sensor fusion (see Fig. 10 for details), and results in noisy training win-dows. From the results seen in Table 1, it is clear that this is a difficult problem, especially when reasoning over cor-Table 1. Benchmark Evaluations on Unit Type Identification and
Next Hyperspectral Window Prediction with clean data and simu-lated data corruptions.
Placement ⇒ Clean
Unit Identification (Acc)
Rand.
Grid
Unet-ResNet18
Unet-ResNet34
Unet-ResNet50 56.6% 40.3% 30.1% 3.97 58.5% 40.2% 30.8% 3.99 62.5% 44.0% 32.8% 4.00
Next Wind. (MSE)
Clean Grid Rand. 4.15 4.11 4.17 4.12 4.15 4.06 rupted samples, and hopefully future work can build upon these results. 5. Preliminary Real-World Experiment on
DOTA Satellite-Image Dataset
In this section, we explore whether performance on
StarCraftImage is predictive of performance on real-world datasets. To this end, we use a version of the DOTA dataset
[41], which is a benchmark dataset for multi-object detec-tion in satellite images, where the samples have been trans-formed to match a similar format to StarCraftImage, which we call DOTA-UnitID (see Fig. 6). This format is similar to the scenario when we may have remote sensing or a sen-sor network that can detect the presence of certain agents or buildings but may not know what they are (e.g., due to cloud cover only synthetic aperture radar data is available).
Figure 6. DOTA dataset examples, where the top row shows three original (input, annotations) pairs from the DOTA dataset [41], while the bottom row shows the three corresponding (input, label) pairs from our DOTA-UnitID dataset. The DOTA-UnitID task is to colorize the grayscale annotation mask.
In addition to the Unet-ResNet models seen above, we trained two state-of-the-art segmentation models, a
SegFormer transformer model [42] (12th place in the
CityScapes Test leaderboard [7]) and a Lawin transformer model [44] (3rd place in [7]) on the clean Unit Identification
task for both the StarCraftImage dataset and the DOTA-UnitID datasets. As seen in the second row of Table 2, the model ranking is the same for both the DOTA-UnitID and StarCraftImage-UnitID experiments across all models (e.g., the Unet-ResNet50 had the best unit accuracy across both datasets), thus providing preliminary evidence that per-formance improvements on our dataset will carry over to real-world datasets. We note that the transformer results are much below the results of the ResNet models. This is likely due to these larger models requiring longer training times than the CNN-based models. Despite this, these re-sults suggest that StarCraftImage is still a difficult dataset even for SOTA models.
Table 2. Unit-ID experiment results on clean data for StarCraftIm-age and Dota-UnitID. RX is short for a Unet-ResNet-X model.
Lawin [44]
Model 27.0%
SCII
DOTA 34.1%
SegFormer [42] R18 27.9% 35.0%
R34 56.6% 58.5% 62.5% 52.4% 52.8% 53.6%
R50 6.