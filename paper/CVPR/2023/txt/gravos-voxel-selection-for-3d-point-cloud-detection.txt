Abstract 1.

Introduction 3D object detection within large 3D scenes is challeng-ing not only due to the sparsity and irregularity of 3D point clouds, but also due to both the extreme foreground-background scene imbalance and class imbalance. A com-mon approach is to add ground-truth objects from other scenes. Differently, we propose to modify the scenes by re-moving elements (voxels), rather than adding ones. Our approach selects the "meaningful" voxels, in a manner that addresses both types of dataset imbalance. The approach is general and can be applied to any voxel-based detector, yet the meaningfulness of a voxel is network-dependent. Our voxel selection is shown to improve the performance of sev-eral prominent 3D detection methods. 3D object detection has gained an increasing importance in both industry and academia due to its wide range of appli-cations, including autonomous driving and robotics [8, 33].
LiDAR sensors are the de-facto standard acquisition devices for capturing 3D outdoor scenes (Fig. 1). They produce sparse and irregular 3D point clouds, which are used in a variety of scene perception and understanding tasks.
There are three prominent challenges in outdoor point cloud datasets for detection. The first is the small size of the datasets, in terms of the number of scenes. The second is the large number of points in the scene vs. the small number of points on the training examples (objects). A single scene might contain hundreds of thousands, or even millions, of points but only a handful of objects. The third is class imbalance, where some classes might contain significantly
more instances than the other classes. This often results in lower predictive accuracy for the infrequent classes [12, 18].
To handle the first challenge, it was proposed to enrich the dataset during training with global operations, applied to the whole point cloud, such as rotation along the Z-axis, random flips along the X-axis, and coordinate scaling [35, 37, 45].
We note that most methods that solve the second and the third challenges indirectly also solve this first challenge.
To solve the second challenge, it is suggested to augment the scene by local operations, applied to points belonging to individual objects [4, 5]. Local operations include random point drop out, frustum drop out, additive noise added to the object, intra-object mixing, and swapping regions between different objects.
To solve the third challenge, as well as the second one,
[24–26, 35] propose to add ground-truth objects from dif-ferent point-clouds to the scene, for training. This type of augmentation indeed mitigates the imbalance. However, it does not take into account the network architecture, though
Reuse et al. [23] show, through a series of experiments, that both local and global data augmentation for 3D object detec-tion strongly depends not only on the dataset, but also on the network architecture. Thus, network-dependent augmenta-tions are beneficial.
We present a novel network-dependent data modification approach that addresses the latter two challenges. (We ad-dress the first challenge indirectly, similarly to [24–26].) The key idea is to learn a subset of elements of the scene, which are meaningful for object detection. Inline with [23]’s ob-servation, meaningfulness is defined in the context of the network and not only of the scene. Considering only this subset as input will allow us not only to decrease the num-ber of elements in the scene, but also to increase the class balance within a given scene.
To realize this idea, we focus on SoTA detection networks that transform point clouds into voxels as a first step in their pipeline. The main reason behind transferring the input to voxels is reducing the size of the input, as only the occupied voxels are then processed. This enables these systems to work on extremely large scenes. Obviously, another benefit of voxels is the ability to impose structure on the input.
Generally speaking, in this voxel-based setup, meaningful voxels are those for which the model "struggles" to locate the objects. Hence, the gradients play a major role in deter-mining the meaningful voxels. Our approach is thus termed
Gradient-based Voxel Selection (GraVoS).
We show that when focusing only on the meaningful voxels and removing the non-informative ones, most of the discarded voxels belong to the scene background. Few of the removed voxels are associated with the prevalent classes and almost none are associated with the non-prevalent classes.
Our strategy may be contrasted with dropout augmenta-tion techniques, which reduce the number of elements ran-domly [4, 5].
Our distribution balancing is demonstrated in Fig. 1.
Given a point cloud (cyan), our selected subset of the mean-ingful voxels (salmon) contains significantly few points from the background (the points are voxel centers here), more points on the objects that belong to the Car class (the most prevalent class), and almost all the points on the objects that belong the Pedestrian and the Cyclist classes.
Our method is general and may be applied to different voxel-based networks. Furthermore, it comes at no addi-tional inference time cost. We show results on four SoTA net-works: SECOND [35], Part-A2 [26], Voxel R-CNN [6], and
CenterPoint [42] on the well-established KITTI dataset [7,8].
The performance of all networks improves, in particular when considering the difficult categories of Pedestrian and
Cyclist. For instance, the performance of [26] on the bench-mark’s moderate subset improves by 2.32% & 1.15% for the non-prevalent classes (Cyclist and Pedestrian), which consti-tutes an error reduction of 8.20% & 2.77%, respectively.
Hence, the main contributions of this paper are:
• A novel & generic "meaningful" voxel selection method, called Gradient-based Voxel Selection.
• A training procedure that uses the selected voxels to improve 3D detection without additional data. This procedure combines information from different stages of the model’s training.
• Demonstrating improved performance of four voxel-based SoTA detection methods, successfully coping both with the inherent class imbalance and with the foreground-background imbalance. 2.