Abstract
Recently, emotional talking face generation has received considerable attention. However, existing methods only adopt one-hot coding, image, or audio as emotion condi-tions, thus lacking flexible control in practical applications and failing to handle unseen emotion styles due to limited semantics. They either ignore the one-shot setting or the quality of generated faces. In this paper, we propose a more flexible and generalized framework. Specifically, we supple-ment the emotion style in text prompts and use an Aligned
Multi-modal Emotion encoder to embed the text, image, and audio emotion modality into a unified space, which inher-its rich semantic prior from CLIP. Consequently, effective multi-modal emotion space learning helps our method sup-port arbitrary emotion modality during testing and could generalize to unseen emotion styles. Besides, an Emotion-aware Audio-to-3DMM Convertor is proposed to connect the emotion condition and the audio sequence to struc-tural representation. A followed style-based High-fidelity
Emotional Face generator is designed to generate arbitrary high-resolution realistic identities. Our texture generator hierarchically learns flow fields and animated faces in a residual manner. Extensive experiments demonstrate the flexibility and generalization of our method in emotion con-trol and the effectiveness of high-quality face synthesis. 1.

Introduction
Talking face generation [13,38,46,58] is the task of driv-ing a static portrait with given audio. Recently, many works have tried to solve the challenges of maintaining lip move-ments synchronized with input speech contents and syn-thesizing natural facial motion simultaneously. However, most researchers ignore a more challenging task, emotional
*Corresponding authors audio-driven talking face generation, which is critical for creating vivid talking faces.
Some works have achieved significant progress in solv-ing the above task conditioned on emotion embedding.
However, there are three continuously critical issues: 1)
How to explore a more semantic emotion embedding to achieve better generalization for unseen emotions. Early efforts [41,47,55] adopt the one-hot vector to indicate emo-tion category, which could only cover the pre-defined la-bel and lacks semantic cues. Subsequently, EVP [19] dis-entangles emotion embedding from the audio, while GC-AVT [23] and EAMM [18] drive emotion by visual im-ages. However, tailored audio- and image-based emotion encoders show limited semantics and also struggle to handle unseen emotion styles. 2) Could we construct multi-modal emotion sources into a unified feature space to allow a more flexible and user-friendly emotion control. Existing meth-ods only support one specific modality as the emotion con-dition, while the desired modality is usually not available in practical applications. 3) How to design a high-resolution identity-generalized generator. Early works [19, 41, 47] are in identity-specific design, while recent works [18, 23] have started to enable one-shot emotional talking face genera-tion. However, as shown in Fig. 1(a), GC-AVT and EAMM fail to produce high-resolution faces due to the inevitable information loss in face embedding and the challenge of es-timating accurate high-resolution flow fields, respectively.
To address the aforementioned challenges, we first sup-plement the emotion styles in the text prompt inspired by the zero-shot CLIP-guided image manipulation [29, 39, 43], which could inherit rich semantic knowledge and conve-nient interaction after being encoded. As shown in Fig. 1(b), unseen emotions, e.g., Satisfied, could be flexibly speci-fied using the text description and precisely reflected on the source face. Furthermore, to achieve alignment among multi-modal emotion features, we introduce an Aligned
Multi-modal Emotion (AME) encoder to unify the text, im-Figure 1. (a) An illustrative comparison of GC-AVT [23], EAMM [18], and our approach. First, our method supports multi-modal emotion cues as input. As shown in (b), given a source face, an audio sequence, and diverse emotion conditions, our results fulfill synchronized lip movements with the speech content and emotional face with the desired style. Besides, benefiting from the effective multi-modal emotion space and rich semantics of CLIP, our method could generalize to unseen style marked in Red. Second, the hierarchical style-based generator with coarse-to-fine facial deformation learning helps us generalize to unseen faces in high resolution and provides more realistic details and precise emotion than GC-AVT and EAMM. Images are from the official attached results or released codes for fair comparisons. age, and audio emotion modality into the same domain, thus supporting flexible emotion control by multi-modal inputs, as depicted in Fig. 1(b). In particular, the fixed CLIP text and image encoders are leveraged to extract their embed-ding and a learned CLIP audio encoder guided by several losses to find the proper emotion representation of the given audio sequence in CLIP space.
To this end, we follow the previous talking face gener-ation methods [34] that rely on intermediate structural in-formation such as 3DMM, and propose an Emotion-aware
Audio-to-3DMM Convertor (EAC), to distill the rich emo-tional semantics from AME and project them to the facial structure. Specifically, we employ the Transformer [40] to capture the longer-term audio context and sufficiently learn correlated audio-emotion features for expression co-efficient prediction, which involves precise facial emotion and synchronized lip movement. Notably, a learned inten-sity token is extended to control the emotion intensity con-tinuously. Furthermore, to generate high-resolution realis-tic faces, we propose a coarse-to-fine style-based identity-generalized model, High-fidelity Emotional Face (HEF) generator, which integrates appearance features, geometry information, and a style code within an elegant design. As shown in Fig. 1(a), unlike the EAMM that predicts the flow field at a single resolution by an isolated process, we hier-archically perform the flow estimation in a residual manner and incorporate it with texture refinement for efficiency.
In summary, we make the following three contributions:
• We propose a novel AME that provides a unified multi-modal semantic-rich emotion space, allowing flexible emotion control and unseen emotion generalization, which is the first attempt in this field.
• We propose a novel HEF to hierarchically learn the facial deformation by sufficiently modeling the inter-action among emotion, source appearance, and drive geometry for the high-resolution one-shot generation.
• Abundant experiments are conducted to demonstrate the superiority of our method for flexible and gener-alized emotion control, and high-resolution one-shot talking face animation over SOTA methods. 2.