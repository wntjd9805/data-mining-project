Abstract
Generalized Zero-Shot Learning (GZSL) identifies un-seen categories by knowledge transferred from the seen domain, relying on the intrinsic interactions between vi-sual and semantic information. Prior works mainly lo-calize regions corresponding to the sharing attributes.
When various visual appearances correspond to the same attribute, the sharing attributes inevitably introduce se-mantic ambiguity, hampering the exploration of accurate
In this paper, we deploy semantic-visual interactions. the dual semantic-visual transformer module (DSVTM) to progressively model the correspondences between at-tribute prototypes and visual features, constituting a pro-gressive semantic-visual mutual adaption (PSVMA) net-work for semantic disambiguation and knowledge trans-ferability improvement. Specifically, DSVTM devises an instance-motivated semantic encoder that learns instance-centric prototypes to adapt to different images, enabling the recast of the unmatched semantic-visual pair into the matched one. Then, a semantic-motivated instance decoder strengthens accurate cross-domain interactions between the matched pair for semantic-related instance adaption, en-couraging the generation of unambiguous visual represen-tations. Moreover, to mitigate the bias towards seen classes in GZSL, a debiasing loss is proposed to pursue response consistency between seen and unseen predictions. The
PSVMA consistently yields superior performances against other state-of-the-art methods. Code will be available at: https://github.com/ManLiuCoder/PSVMA. 1.

Introduction
Generalized Zero-Shot Learning (GZSL) [35] aims to recognize images belonging to both seen and unseen cat-egories, solely relying on the seen domain data. Freed
*Corresponding author
Figure 1. The embedding-based models for GZSL. (a) The early embedding-based method. (b) Part-based methods via attention mechanisms. (c) Semantic-guided methods. (d) Our PSVMA. A,
S, F denote the category attribute prototypes, sharing attributes, and visual features, respectively. The PSVMA progressively per-forms semantic-visual mutual adaption for semantic disambigua-tion and knowledge transferability improvement. from the requirement of enormous manually-labeled data,
GZSL has extensively attracted increasing attention as a challenging recognition task that mimics human cognitive abilities [25]. As unseen images are not available during training, knowledge transfer from the seen to unseen do-mains is achieved via auxiliary semantic information (i.e., category attributes [15, 25], text descriptions [26, 38], and word embedding [31, 32, 40]).
Early embedding-based methods [2,3,45,51] embed cat-egory attributes and visual images and learn to align global visual representations with corresponding category proto-types, as shown in Fig. 1 (a). Nevertheless, the global information is insufficient to mine fine-grained discrimi-native features which are beneficial to capture the sub-tle discrepancies between seen and unseen classes. To solve this issue, part-based learning strategies have been leveraged to explore distinct local features. Some works
[27, 29, 33, 47, 48, 52] apply attention mechanisms to high-light distinctive areas, as shown in Fig. 1 (b). These meth-ods fail to develop the deep correspondence between vi-sual and attribute features, which results in biased recog-nition of seen classes. More recently, semantic-guided ap-proaches (see Fig. 1 (c)) are proposed to employ the shar-ing attribute and localize specific attribute-related regions
[8, 22, 30, 49, 50]. They establish interactions between the sharing attributes and visual features during localization, further narrowing the cross-domain gap. Actually, various visual appearances correspond to the same sharing attribute descriptor. For example, for the attribute descriptor “tail”, the visual presentations of a dolphin’s and rat’s tail exhibit differently. The above methods are suboptimal to build matched visual-semantic pairs and inclined to generate am-biguous semantic representations. Further, this semantic ambiguity can hamper cross-domain interactions based on unmatched visual-semantic pairs, which is detrimental to the knowledge transferring from seen to unseen classes.
To tackle this problem, we propose a progressive semantic-visual mutual adaption (PSVMA) network, as shown in Fig. 1 (d), to progressively adapt the sharing at-tributes and image features. Specifically, inspired by the powerful ability of the vision transformers (ViT) [14] to capture global dependencies, we apply ViT for visual em-bedding and extract image patch features for the interac-tion with semantic attributes. With the embedded visual and attribute features, we devise the dual semantic-visual transformer module (DSVTM) in PSVMA, which consists of an instance-motivated semantic encoder (IMSE) and a semantic-motivated instance decoder (SMID).
Concretely, in IMSE, we first perform instance-aware semantic attention to adapt the sharing attributes to var-ious visual features. Based on the interrelationship be-tween attribute groups, we further introduce attribute com-munication and activation to promote the compactness be-tween attributes.
In this way, IMSE recurrently converts the sharing attributes into instance-centric semantic fea-tures and recasts the unmatched semantic-visual pair into the matched one, alleviating the problem of semantic ambi-guity. Subsequently, SMID explores the cross-domain cor-respondences between each visual patch and all matched attributes for semantic-related instance adaption, providing accurate semantic-visual interactions. Combined with the refinement of the patch mixing and activation in SMID, the visual representation is eventually adapted to be unambigu-ous and discriminative. In addition, we design a novel de-biasing loss for PSVMA to assist the process of knowledge transfer by pursuing the distribution consistency of inferred scores, mitigating the common bias towards seen domains.
Consequently, PSVMA can effectively achieve semantic disambiguation and improve knowledge transferability by progressive semantic-visual mutual adaption, gaining more accurate inferences for both seen and unseen categories.
Our key contributions can be summarized as follows: (1)
We propose a progressive semantic-visual mutual adaption (PSVMA) network that deploys the dual semantic-visual transformer module (DSVTM) to alleviate semantic am-biguity and strengthen feature transferability through mu-tual adaption. (2) The sharing attributes are converted into instance-centric attributes to adapt to different visual im-ages, enabling the recast of the unmatched semantic-visual pair into the matched one. Furthermore, accurate cross-domain correspondence is constructed to acquire transfer-able and unambiguous visual features. (3) Extensive ex-periments over common benchmarks demonstrate the effec-tiveness of our PSVMA with superior performance. Partic-ularly, our method achieves 75.4% for the harmonic mean on the popular benchmark AwA2, outperforming previous competitive solutions by more than 2.3%. 2.