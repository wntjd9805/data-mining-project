Abstract
Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice.
Previous methods mainly adopt filter-level pruning to ac-celerate networks with scarce training samples. In this pa-per, we reveal that dropping blocks is a fundamentally su-perior approach in this scenario.
It enjoys a higher ac-celeration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recov-erability to measure the difficulty of recovering the com-pressed network. Our recoverability is efficient and effec-tive for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks us-ing only tiny sets of training images. PRACTISE outper-forms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k.
It also enjoys high generalization ability, working well under data-free or out-of-domain data settings, too. Our code is at https:
//github.com/DoctorKey/Practise. 1.

Introduction
In recent years, convolutional neural networks (CNNs) have achieved remarkable success, but they suffer from high computational costs. To accelerate the networks, many net-work compression methods have been proposed, such as network pruning [11,18,20,22], network decoupling [6,15] and network quantization [2, 7]. However, most previous methods rely on the original training set (i.e., all the train-ing data) to recover the model’s accuracy. But, to preserve data privacy and/or to achieve fast deployment, only scarce training data may be available in many scenarios.
For example, a customer often asks the algorithmic provider to accelerate their CNN models, but due to privacy
*J. Wu is the corresponding author. This research was partly sup-ported by the National Natural Science Foundation of China under Grant 62276123 and Grant 61921006.
Figure 1. Comparison of different compression schemes with only 500 training images. We propose dropping blocks for few-shot network acceleration. Our method (‘Block’) outperforms previ-ous methods dominantly for the latency-accuracy tradeoff. The
ResNet-34 model was compressed on ImageNet-1k and all laten-cies were tested on an NVIDIA TITAN Xp GPU. concerns, the whole training data cannot be available. Only the raw uncompressed model and a few training examples are presented to the algorithmic provider. In some extreme cases, not even a single data point is to be provided. The algorithmic engineers need to synthesize images or collect some out-of-domain training images by themselves. Hence, to learn or tune a deep learning model with only very few samples is emerging as a critical problem to be solved.
In this few-shot compression scenario, most previous works [1,12,30] adopt filter-level pruning. However, it can-not achieve a high acceleration ratio on real-world com-puting devices (e.g., on GPUs). To make compressed mod-els indeed run faster than the uncompressed models, lots of
FLOPs (number of floating point operations) are required to be reduced by filter-level pruning. And without the whole training dataset, it is difficult to recover the compressed model’s accuracy. Hence, previous few-shot compression
KD [10]
FSKD [12]
CD [1] MiR [30] BP (blocks) 44.5 45.3 56.2 64.1 66.5
Table 1. Top-1 validation accuracy (%) on ImageNet-1k for differ-ent compression schemes. ResNet-34 was accelerated by reducing 16% latency with 50 training images. Previous methods prune fil-ters with the ‘normal’ style. For the block-level pruning, we sim-ply remove the first k blocks and finetune the pruned network by back propagation, i.e., ‘BP (blocks)’ in this table. methods often exhibit a poor latency (wall-clock timing) vs. accuracy tradeoff.
In this paper, we advocate that we need to focus on latency-accuracy rather than FLOPs-accuracy, and reveal that block-level pruning is fundamentally superior in the few-shot compression scenario. Compared to pruning fil-ters, dropping blocks enjoys a higher acceleration ratio.
Therefore it can keep more capacity from the original model and its accuracy is easier to be recovered by a tiny train-ing set under the same latency when compared with filter pruning. Fig. 1 shows dropping blocks dominantly out-performs previous compression schemes for the latency-accuracy tradeoff. Table 1 further reports that an em-barrassingly simple dropping block baseline (i.e., finetune without any other processing) has already surpassed exist-ing methods which use complicated techniques. The base-line, ‘BP (blocks)’, simply removes the first few blocks and finetune the pruned network with the cross-entropy loss.
To further improve block pruning, we study the strat-egy for choosing which blocks to drop, especially when only scarce training samples are available. Several cri-teria [21, 31, 34] have been proposed for pruning blocks on the whole dataset. However, some [31, 34] require a large amount of data for choosing, whereas others [21] only evaluate the output difference before/after block re-moval.
In this paper, we notice that although dropping some blocks significantly changes the feature maps, they are easily recovered by end-to-end finetuning even with a tiny training set. So simply measuring the difference between pruned/original networks is not valid. To deal with these problems, a new concept namely recoverability is proposed in this paper for better indicating blocks to drop. And we propose a method to compute it efficiently, with only a few training images. At last, our recoverability is surprisingly consistent with the accuracy of the finetuned network.
Finally, we propose PRACTISE, namely Practical net-work acceleration with tiny sets of images, to effectively accelerate a network with scarce data. PRACTISE signifi-cantly outperforms previous few-shot pruning methods. For 22.1% latency reduction, PRACTISE surpasses the previous state-of-the-art (SOTA) method on average by 7.0% (per-centage points, not relative improvement) Top-1 accuracy on ImageNet-1k. It is also robust and enjoys high gener-alization ability which can be used on synthesized/out-of-domain images. Our contributions are:
• We argue that the FLOPs-accuracy tradeoff is a mis-leading metric for few-shot compression, and advocate that the latency-accuracy tradeoff (which measures real runtime on devices) is more crucial in practice. For the first time, we find that in terms of latency vs. accuracy, block pruning is an embarrassingly simple but powerful method—dropping blocks with simple finetuning has already surpassed previ-ous methods (cf. Table 1). Note that although dropping blocks is previously known, we are the first to reveal its great potential in few-shot compression, which is both a sur-prising and an important finding.
• To further boost the latency-accuracy performance of block pruning, we study the optimal strategy to drop blocks.
A new concept recoverability is proposed to measure the difficulty of recovering each block, and in determining the priority to drop blocks. Then, we propose PRACTISE, an al-gorithm for accelerating networks with tiny sets of images.
• Extensive experiments demonstrate the extraordinary performance of our PRACTISE. In both the few-shot and even the extreme data-free scenario, PRACTISE improves results by a significant margin. It is versatile and widely applicable for different network architectures, too. 2.