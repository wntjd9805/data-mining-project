Abstract
We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually cap-tured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive op-‡Part of the work was done while Andreas and Yu-Lun were interns at
Meta. timization significantly improves the robustness of the re-construction. For handling large unbounded scenes, we dy-namically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the TANKS AND TEMPLES dataset and our collected outdoor dataset, STATIC HIKES, show that our approach compares favorably with the state-of-the-art. 1.

Introduction
Dense scene reconstruction for photorealistic view syn-thesis has many critical applications, in
VR/AR (virtual traveling, preserving of important cultural for example,
artifacts), video processing (stabilization and special ef-fects), and mapping (real-estate, human-level maps). Re-cently, rapid progress has been made in increasing the fi-delity of reconstructions using radiance fields [22]. Unlike most traditional methods, radiance fields can model com-mon phenomena such as view-dependent appearance, semi-transparency, and intricate micro-details.
Challenges.
In this paper, we aim to create radiance field reconstructions of large-scale scenes that are acquired us-ing a single handheld camera since this is arguably the most practical way of capturing them outside the realm of pro-fessional applications.
In this setting, we are faced with two main challenges: (1) estimating accurate camera tra-jectory of a long path and (2) reconstructing the large-scale radiance fields of scenes. Resolving them together is dif-ficult because changes in observation can be explained by either camera motion or the radiance field’s ability to model view-dependent appearance. For this reason, many radiance field estimation techniques assume that the accurate poses are known in advance (typically fixed during the radiance field optimization). However, in practice, one has to use a separate method, such as Structure-from-Motion (SfM), for estimating the camera poses in a pre-processing step. Un-fortunately, SfM is not robust in the handheld video setting.
It frequently fails because, unlike radiance fields, it does not model view-dependent appearance and struggles in the ab-sence of highly textured features and in the presence of even slight dynamic motion (such as swaying tree branches).
To remove the dependency on known camera poses, sev-eral approaches propose jointly optimizing camera poses and radiance fields [11,17,41]. These methods perform well when dealing with a few frames and a good pose initializa-tion. However, as shown in our experiments, they have dif-ficulty estimating long trajectories of a video camera from scratch and often fall into local minima.
Our work.
In this paper, we propose a joint pose and radiance field estimation method. We design our method by drawing inspiration from classical incremental SfM al-gorithms and keyframe-based SLAM systems for improving the robustness. The core of our approach is to process the video sequence progressively using overlapping local radi-ance fields. More specifically, we progressively estimate the poses of input frames while updating the radiance fields. To model large-scale unbounded scenes, we dynamically in-stantiate local radiance fields. The increased locality and progressive optimization yield several major advantages:
• Our method scales to processing arbitrarily long videos without loss of accuracy and without hitting memory limitations.
• Increased robustness because the impact of misestima-tions is locally bounded.
• Increased sharpness because we use multiple radiance fields to model local details of the scene (see Figure 1
Represented space
Uncontracted space (a) NDC (b) Mip-NeRF360 (c) Ours
Figure 2. Space parameterization. (a) NDC, used by NeRF [22] for forward-facing scenes, maps a frustum to a unit cube volume.
While a sensible approach for forward-facing cameras, it is only able to represent a small portion of a scene as the frustum can-not be extended beyond a field of view of 120◦ or so without significant distortion. (b) Mip-NeRF360’s [4] space contraction squeezes the background and fits the entire space into a sphere of radius 2. It is designed for inward-facing 360 scenes and cannot scale to long trajectories. (c) Our approach allocates several radi-ance fields along the camera trajectory. Each radiance field maps the entire space to a [−2, 2] cube (Equation (5)) and, each having its own center for contraction (Equatione (7)), the high-resolution uncontracted space follows the camera trajectory and our approach can adapt to any camera path. and 2b).
We validate our method on the TANKS AND TEMPLES dataset. We also collect a new dataset STATIC HIKES of twelve outdoor scenes using four consumer cameras to eval-uate our method. These sequences are challenging due to long handheld camera trajectories, motion blur, and com-plex appearance.
Our contributions. We present a new method for re-constructing the radiance field of a large-scale scene, which contains the following contributions:
• We propose to progressively estimate the camera poses and radiance fields, leading to significantly improved robustness.
• We show that multiple overlapping local radiance fields improve visual quality and support modeling large-scale unbounded scenes.
• We contribute a newly collected video dataset that presents new challenges not covered by existing view synthesis datasets.
Limitations. Our work aims to synthesize novel views from the reconstructed radiance fields. While we jointly es-timate the poses in the pipeline, we do not perform global bundle adjustment and loop closure (i.e., not a complete
SLAM system). We leave this important direction for fu-ture work.
2.