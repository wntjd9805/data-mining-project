Abstract
This paper presents Scalable Semantic Transfer (SST), a novel training paradigm, to explore how to leverage the mutual benefits of the data from different label domains (i.e. various levels of label granularity) to train a powerful hu-man parsing network.
In practice, two common applica-tion scenarios are addressed, termed universal parsing and dedicated parsing, where the former aims to learn homoge-neous human representations from multiple label domains and switch predictions by only using different segmentation heads, and the latter aims to learn a specific domain pre-diction while distilling the semantic knowledge from other domains. The proposed SST has the following appealing benefits: (1) it can capably serve as an effective train-ing scheme to embed semantic associations of human body parts from multiple label domains into the human repre-sentation learning process; (2) it is an extensible semantic transfer framework without predetermining the overall rela-tions of multiple label domains, which allows continuously adding human parsing datasets to promote the training. (3) the relevant modules are only used for auxiliary training and can be removed during inference, eliminating the extra reasoning cost. Experimental results demonstrate SST can effectively achieve promising universal human parsing per-formance as well as impressive improvements compared to its counterparts on three human parsing benchmarks (i.e.,
PASCAL-Person-Part, ATR, and CIHP). Code is available at https://github.com/yangjie-cv/SST. 1.

Introduction
Human parsing, which aims to assign pixel-wise cate-gory predictions for human body parts, has played a criti-cal role in human-oriented visual content analysis, editing, and generation, e.g., virtual try-on [7], human motion trans-fer [14], and human activity recognition [8]. Prior methods
*Corresponding author. have chronically been dominated by developing powerful network architectures [11, 18, 24, 25, 32]. However, these network designs usually serve as general components for human parsing in a specific labeling system, and how to use the data from multiple label domains to further improve the accuracy of the parsing network is still an open issue.
Recent studies [12, 20] have proposed using multiple la-bel domains to train a universal parsing network by con-structing graph neural networks that learn semantic associ-ations from different label domains. Graphonomy [10, 20] investigates cross-domain semantic coherence through se-mantic aggregation and transfer on explicit graph structures.
Grapy-ML [12] extends this method by stacking three levels of human part graph structures, from coarse to fine granu-larity. Despite these graph-based methods consider prior human body structures, such as the spatial associations of human parts and the semantic associations of different la-bel domains as in Fig. 1-(a), they have limitations in their ability to dynamically add new label domains due to their dependence on pre-determined graph structures. Addition-ally, explicit graph reasoning incurs extra inference costs, making it impractical in some scenarios. Therefore, there is a need for a more general, scalable, and simpler manner to harness the mutual benefits of data from multiple label domains to enhance given human parsing networks.
In this work, we propose Scalable Semantic Transfer (SST), a novel training paradigm that builds plug-and-play modules to effectively embed semantic associations of hu-man body parts from multiple label domains into the given parsing network. The proposed SST scheme can be eas-ily applied to two common scenarios, i.e., universal pars-ing and dedicated parsing. Similar to [12, 20], universal parsing aims to enforce the parsing network to learn homo-geneous human representation from multiple label domains with the help of the SST scheme in the training phase. It achieves different levels of human parsing prediction for an arbitrary image in the inference phase, as illustrated in Fig-ure 1-(b). In contrast, dedicated parsing aims to optimize a parsing network for a specific label domain by employ-Figure 1. (a) The Motivation of SST: 1) Intra-Domain Hierarchical Semantic Structure (Upper): The spatial correlated two parts are connected by a white line; 2) Cross-Domain Semantic Mapping and Consistency (Lower): We regularize the semantic consistency (orange line) between original category representations (solid line) and mapped ones (dot line) for two datasets (green for dataset-1 and blue for dataset-2). (b) The Training and Inference Phase of Universal Parsing. (c) The Training and Inference Phase of Dedicated Parsing. ing the SST scheme to distill the semantic knowledge from other label domains in the training phase, and finally real-izes more accurate prediction for the specific label domain in the inference phase, as shown in Fig. 1-(c).
The proposed SST has several appealing benefits: (1)
Generality. It is a general training scheme that effectively incorporates prior knowledge of human body parts into the representation learning process of a given parsing network.
It enables the integration of both intra-domain spatial cor-relations and inter-domain semantic associations, making it suitable for training both universal and dedicated parsing networks. (2) Scalability. It is a scalable framework that enables the addition of new datasets for more effective train-ing. In practice, universal parsing enables the parsing net-work to acquire a more robust human representation by ex-panding the label domain pool, while dedicated parsing fa-cilitates the transfer of rich semantic knowledge from more datasets to enhance the parsing network for a specific label domain. (3) Simplicity. All modules in SST are plug-and-play ones that are only used for auxiliary training and can be dropped out during inference, eliminating unnecessary model complexity in real-world applications.
The main contributions are summarized as follows: 1) We propose a novel training paradigm called Scalable
Semantic Transfer (SST) that effectively leverages the benefits of data from different label domains to train powerful universal and dedicated parsing networks. 2) We introduce three plug-and-play modules in SST that incorporate prior knowledge of human body parts into the training process of a given parsing network, which can be removed during inference. 3) Extensive experiments demonstrate the effectiveness of SST on both universal and dedicated parsing tasks, achieving impressive results on three human parsing datasets compared with the counterparts. 2.