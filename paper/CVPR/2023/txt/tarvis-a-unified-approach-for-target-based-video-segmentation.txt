Abstract
BEFORE
Task-specific models
NOW
Task-specific targets
The general domain of video segmentation is currently fragmented into different tasks spanning multiple bench-marks. Despite rapid progress in the state-of-the-art, cur-rent methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily de-fined ‘targets’ in video. Our approach is flexible with re-spect to how tasks define these targets, since it models the latter as abstract ‘queries’ which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning differ-ent tasks, and can hot-swap between tasks during infer-ence without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic
Segmentation (VPS), Video Object Segmentation (VOS) and
Point Exemplar-guided Tracking (PET). Our unified, jointly trained model achieves state-of-the-art performance on 5/7 benchmarks spanning these four tasks, and competitive per-formance on the remaining two. Code and model weights are available at: https://github.com/Ali2500/TarViS 1.

Introduction
The ability to understand video scenes has been a long-standing goal of computer vision research because of wide-ranging applications in intelligent vehicles and robots.
Early approaches tackled simpler tasks involving contour-based [33, 39] and box-level tracking [21, 25, 40, 52], back-ground subtraction [20, 61], and motion segmentation [8, 49]. The deep learning boom then revolutionized the land-scape by enabling methods to perform pixel-precise seg-mentation on challenging, real-world videos.
In the past few years, a number of benchmarks have emerged, which evaluate how well methods can perform video segmenta-tion according to various task formulations. Over time, these tasks/benchmarks have ballooned into separate re-VIS
VPS
VOS
PET
TarViS
S
I
V
S
P
V
T
E
P
/
S
O
V
Figure 1. Predicted results from a jointly trained TarViS model for four different video segmentation tasks. search sub-communities. Although existing methods are rapidly improving the state-of-the-art for these benchmarks, each of them typically tackles only one narrowly-defined task, and generalizing them is non-trivial since the task def-inition is baked into the core approach.
We argue that this fragmentation is unnecessary be-cause video target segmentation tasks all require the same high-level capability, namely that of identifying, localizing and tracking rich semantic concepts. Meanwhile, recent progress on Transformer networks has enabled the wider
AI research community to move towards unified, multi-task architectures [1, 30, 31, 38, 58], because the attention op-eration [62] is well-suited for processing feature sets with arbitrary structure and data modality. These developments give us the opportunity to unify the fractured landscape of target-based video segmentation. In this paper, we propose
TarViS: a novel architecture which enables a single, unified model to be jointly trained for multiple video segmentation tasks. During inference, the same model can perform differ-ent tasks at runtime by specifying the segmentation target.
The core idea is that TarViS tackles the generic task of segmenting a set of arbitrary targets in video (defined as semantic classes or as specific objects). These targets are encoded as queries which, together with the video features, are input to a Transformer-based model. The model iter-atively refines these queries and produces a pixel-precise mask for each target entity. This formulation conceptually fuses all video segmentation tasks [3, 54, 66, 72] which fall under the umbrella of the above-mentioned generic task, be-cause they differ only in how the targets are defined. During both training and inference, TarViS can hot-swap between tasks at run-time by providing the desired target query set.
To demonstrate our generalization capability, we tackle (1) Video Instance Segmenta-four different tasks: tion (VIS) [54, 72], (2) Video Panoptic Segmentation (VPS) [35], (3) Video Object Segmentation [53], and (4)
Point Exemplar-guided Tracking [3] (PET). For VIS, the segmentation targets are all objects in the video belong-ing to a predefined set of classes. The target set for
VPS includes that for VIS, and additionally, a set of non-instantiable stuff semantic classes. For VOS, the targets are a specific set of objects for which the first-frame ground-truth mask is provided. PET is a more constrained version of VOS which only provides the location of a single point inside the object, rather than the full object mask.
Existing methods for these tasks lack generalization capability because task-specific assumptions are typically baked into the approach (see Sec. 2 and 3 for details). In contrast, TarViS can tackle all four tasks with a unified model because we encode the task-specific targets as a set of queries, thus decoupling the network architecture from the task definition. Moreover, our approach can theoreti-cally generalize further, e.g., one could potentially define the target set as all objects described by a given text prompt, though this is beyond the scope of this paper.
To summarize, our contributions are as follows: we pro-pose TarViS, a novel architecture that can perform any task requiring segmentation of a set of targets from video. For the first time, we are able to jointly train and infer a single model on a collection of datasets spanning the four afore-mentioned tasks (VIS, VPS, VOS, PET). Our experimental results show that TarViS performs competitively for VOS, and achieves state-of-the-art results for VIS, VPS and PET. 2.