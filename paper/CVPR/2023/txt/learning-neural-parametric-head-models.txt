Abstract 1.

Introduction
We propose a novel 3D morphable model for complete human heads based on hybrid neural fields. At the core of our model lies a neural parametric representation that dis-entangles identity and expressions in disjoint latent spaces.
To this end, we capture a person’s identity in a canonical space as a signed distance field (SDF), and model facial ex-pressions with a neural deformation field. In addition, our representation achieves high-fidelity local detail by intro-ducing an ensemble of local fields centered around facial anchor points. To facilitate generalization, we train our model on a newly-captured dataset of over 3700 head scans from 203 different identities using a custom high-end 3D scanning setup. Our dataset significantly exceeds compara-ble existing datasets, both with respect to quality and com-pleteness of geometry, averaging around 3.5M mesh faces per scan1. Finally, we demonstrate that our approach out-performs state-of-the-art methods in terms of fitting error and reconstruction quality. 1We will publicly release our dataset along with a public benchmark for both neural head avatar construction as well as an evaluation on a hidden test-set for inference-time fitting.
Human faces and heads lie at the core of human visual perception, and hence are key to creating digital replica of someones identity, likeliness, and appearance. In particular, 3D reconstruction of human heads from sparse inputs, such as point clouds, is central to a wide range of applications in the context of gaming, augmented and virtual reality, and digitization in our modern digital era. One of the most suc-cessful lines of research to address this challenging prob-lem are parametric face models, which represent both shape identities and expressions featuring a low-dimensional para-metric space. These Blendshape and 3D morphable models (3DMMs) have achieved incredible success, since they can be fitted to sparse inputs, regularize out noise, and provide a compact 3D representation. As a result, many practical settings could be realized, ranging from face tracking and 3D avatar creation to facial-reenactment applications [49].
Traditionally, 3DMMs, are based on a low-rank approx-imation of the underlying 3D mesh geometry. To this end, a template mesh with fixed topology is non-rigidly regis-tered to a series of 3D scans. From this template regis-tration, a 3DMM can be computed using dimensionality
Website: https://simongiebenhain.github.io/NPHM
reduction methods such as principal component analysis (PCA). The quality of the resulting parametric space de-pends strongly on the quality of 3D scans, their registra-tion, and the ability to disentangle identity and expression variations. While these PCA-based models exhibit excel-lent regularizing properties, their inherent limitation lies in their inability to represent local surface detail and the re-liance on a template mesh of fixed topology, which inhibits the representation of diverse hair styles.
In this work, we propose neural parametric head models (NPHM), which represent complete human head geometry in a canonical space using an SDF, and morph the resulting geometry to posed space using a forward deformation field.
By decoupling the human head representation into these two spaces, we are able to learn disentangled latent spaces – one of the core concepts of 3DMMs. Furthermore, we decompose the implicit geometry representation in canon-ical space into an ensemble of local MLPs. Each part is represented by a small MLP that operates in a local coordi-nate system centered around face keypoints. Additionally, we exploit face symmetry by sharing network weights of symmetric regions. This decomposition into separate parts imposes a strong geometry prior and helps to improve both generalization and provide higher levels of detail.
In order to train our model, we capture a new high-fidelity head dataset with a high-end capture rig, which is composed of over 3700 3D head scans from 203 different people. After rigidly aligning all scans in a canonical co-ordinate system, we train our identity network on scans in canonical expression. In order to train the deformation net-work, we non-rigidly register each scan against a template mesh, which we in turn use as training data for our neural deformation model. At inference time, we can then fit our model to a given input point cloud by optimizing for the la-tent code parameters for both expression and identity. In a series of experiments, we demonstrate that our neural para-metric model outperforms state-of-the-art models and can represent complete heads, including fine details.
In sum, our contributions are as follows:
• We introduce a novel 3D dataset captured with a high-end capture rig, including over 3700 3D scans of hu-man heads from 203 different identities.
• We propose a new neural-field-based parametric head representation, which facilitates high-fidelity local de-tails through an ensemble of local implicit models.
• We demonstrate that our neural parametric head model can be robustly fit to range data, regularize out noise, and outperform existing models. 2.