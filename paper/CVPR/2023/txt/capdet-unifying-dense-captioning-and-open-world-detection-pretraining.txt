Abstract
Beneﬁting from large-scale vision-language pre-training on image-text pairs, open-world detection methods have shown superior generalization ability under the zero-shot or few-shot detection settings. However, a pre-deﬁned cate-gory space is still required during the inference stage of ex-isting methods and only the objects belonging to that space will be predicted. To introduce a “real” open-world de-tector, in this paper, we propose a novel method named
CapDet to either predict under a given category list or di-rectly generate the category of predicted bounding boxes.
Speciﬁcally, we unify the open-world detection and dense caption tasks into a single yet effective framework by in-troducing an additional dense captioning head to gener-ate the region-grounded captions. Besides, adding the cap-tioning task will in turn beneﬁt the generalization of detec-tion performance since the captioning dataset covers more concepts. Experiment results show that by unifying the dense caption task, our CapDet has obtained signiﬁcant performance improvements (e.g., +2.1% mAP on LVIS rare classes) over the baseline method on LVIS (1203 classes).
Besides, our CapDet also achieves state-of-the-art perfor-mance on dense captioning tasks, e.g., 15.44% mAP on VG
V1.2 and 13.98% on the VG-COCO dataset. 1.

Introduction
Most state-of-the-art object detection methods [33, 34, 50] beneﬁt from a large number of densely annotated detec-tion datasets (e.g., COCO [27], Object365 [36], LVIS [12]).
However, this closed-world setting results in the model only being able to predict categories that appear in the training set. Considering the ubiquity of new concepts in real-world scenes, it is very challenging to locate and identify these new visual concepts. This predictive ability of new concepts in open-world scenarios has very important research value
*Equal contribution.
†Corresponding authors.
Figure 1. Comparison of the different model predictions under
OWD, OVD, and our setting. (a) OWD methods [14, 18, 48] are not able to describe the detailed category of the detected unknown objects and (b) the performance of OVD methods [8, 12, 41] usu-ally depends on the pre-deﬁned category list during the inference. (c) With the uniﬁcation of two pipelines of dense captioning and open-world detection pre-training, our CapDet can either predict under a given category list or directly generate the description of predicted bounding boxes. in real-world applications such as object search [29, 30], in-stance registration [45], and human-object interaction mod-eling [10].
Currently, the open world scenario mainly includes two tasks: open world object detection [18] (OWD) and open-vocabulary object detection [44] (OVD). Although the paradigms of OWD and OVD tasks are closer to the real world, the former cannot describe the speciﬁc concept of the detected unknown objects and requires a pre-deﬁned category list during the inference. Speciﬁcally, as shown
in Figure 1, previous OWD methods [14, 18, 48] would rec-ognize new concepts not in the predeﬁned category space as “unknown”. Further, another line of task OVD requires the model to learn a limited base class and generalize to novel classes. Compared to the zero-shot object detection (ZSD) proposed by [32], OVD allows the model to use ex-ternal knowledge, e.g., knowledge distillation from a large-scale vision-language pre-trained model [8, 12], image-caption pairs [44], image classiﬁcation data [49], grounding data [25, 41, 46]. With the external knowledge, OVD meth-ods show a superior generalization capacity to detect the novel classes within a given category space. However, as shown in Figure 1, when given an incomplete category list,
OVD can only predict the concepts that appear in the given category list, otherwise, there will be recognition errors, ( i.e., as illustrated in Figure 1 (b), the OVD methods prone to predict the “wall socket” as “remote”, since the latter is in the category list but not the former).
Thus, under the OVD setting, we mainly face the follow-ing two challenges: (i) it is difﬁcult to deﬁne a complete list of categories; (ii) low response values on rare categories often lead to recognition errors. This is mainly because we cannot exhaustively enumerate new objects in the real world, and secondly, it is difﬁcult to collect enough sam-ples for rare classes. However, the fact that rare objects in the real world, even some new objects that are unknown to humans, such as UFOs, do not prevent people from using natural language to describe it as “a ﬂying vehicle that looks like a Frisbee”.
Therefore, based on the above observations, in this pa-per, we consider a new setting that is closer to the open world and real scenes, i.e., we expect the model to both detect and recognize concepts in a given category list, and to generate corresponding natural language descriptions for new concepts or rare categories of objects. Early dense cap-tioning methods [9, 17] can locate salient regions in images and generate the region-grounded captions with natural lan-guage. Inspired by this, to address the challenges faced in the OVD setting, we propose to unify the two pipelines of dense captioning and open-world detection pre-training into one training framework, called CapDet. It empowers the model with the ability to both accurately detect and recog-nize common object categories and generate dense captions for unknown and rare categories by unifying the two train-ing tasks.
Speciﬁcally, our CapDet constructs a uniﬁed data for-mat for the dense captioning data and detection data. With the data uniﬁcation, CapDet further adopts a uniﬁed pre-training paradigm including open-world object detection and dense captioning pre-training. For open-world detec-tion pretraining, we treat the detection task as a semantic alignment task and adopt a dual encoder structure as [41] to locate and predict the given concepts list. The concepts list contains category names in detection data and region-grounded captions in dense captioning data. For dense cap-tioning pretraining, CapDet proposes a dense captioning head to take the predicted proposals as input to generate the region-grounded captions. Due to the rich visual con-cepts in the dense captioning data , the integration of dense captioning tasks will in turn beneﬁt the generalization of detection performance.
Our experiments show that the integration of few dense captioning data brings in large improvement in the object detection datasets LVIS, e.g., +2.7% mAP on LVIS. The uniﬁcation of dense captioning and detection pre-training gains an additional 2.3% increment on LVIS and 2.1% in-crement on LVIS rare classes. Besides, our model also achieves state-of-the-art performance on dense captioning tasks. Note that our method is the ﬁrst to unify dense cap-tioning and open-world detection pretraining.
To summarize, our contributions are three folds:
• We propose a novel open-vocabulary object detection framework CapDet, which cannot only detect and rec-ognize concepts in a given category list but also gen-erate corresponding natural language descriptions for new concept objects.
• We propose to unify the two pipelines of dense cap-tioning and open-world detection pre-training into one training framework. Both two pre-training tasks are beneﬁcial to each other.
• Experiments show that by uniﬁed dense captioning task and detection task, our CapDet gains signiﬁcant performance improvements on the open-vocabulary object detection task (e.g., +3.3% mAP on LVIS rare classes). Furthermore, our CapDet also achieves state-of-the-art performance on the dense captioning tasks, e.g., 15.44% mAP on Visual Genome (VG) V1.2 and 13.98% mAP on VG-COCO. 2.