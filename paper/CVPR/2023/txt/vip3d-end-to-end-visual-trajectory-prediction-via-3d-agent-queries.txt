Abstract
Perception and prediction are two separate modules in the existing autonomous driving systems. They interact with each other via hand-picked features such as agent bounding boxes and trajectories. Due to this separation, prediction, as a downstream module, only receives limited information from the perception module. To make matters worse, er-rors from the perception modules can propagate and accu-mulate, adversely affecting the prediction results.
In this work, we propose ViP3D, a query-based visual trajectory prediction pipeline that exploits rich information from raw videos to directly predict future trajectories of agents in a scene. ViP3D employs sparse agent queries to detect, track, and predict throughout the pipeline, making it the first fully differentiable vision-based trajectory prediction approach.
Instead of using historical feature maps and trajectories, useful information from previous timestamps is encoded in agent queries, which makes ViP3D a concise streaming pre-diction method. Furthermore, extensive experimental re-sults on the nuScenes dataset show the strong vision-based prediction performance of ViP3D over traditional pipelines and previous end-to-end models.1 1.

Introduction
An autonomous driving system should be able to per-ceive agents in the current environment and predict their future behaviors so that the vehicle can navigate the world
∗Equal contribution.
†Corresponding to: hangzhao@mail.tsinghua.edu.cn 1Code and demos are available on the project page: https:// tsinghua-mars-lab.github.io/ViP3D
safely. Perception and prediction are two separate modules in the existing autonomous driving software pipeline, where the interface between them is often defined as hand-picked geometric and semantic features, such as historical agent trajectories, agent types, agent sizes, etc. Such an interface leads to the loss of useful perceptual information that can be used in trajectory prediction. For example, tail lights and brake lights indicate a vehicle’s intention, and pedestrians’ head pose and body pose tell about their attention. This information, if not explicitly modeled, is ignored in the ex-isting pipelines. In addition, with the separation of percep-tion and prediction, errors are accumulated and cannot be mitigated in later stages. Specifically, historical trajectories used by trajectory predictors come from an upstream per-ception module, which inevitably contains errors, leading to a drop in the prediction performance. Designing a trajec-tory predictor that is robust to upstream output errors is a non-trivial task [60].
Recent works
[3], FaF [34],
PnPNet [30] propose end-to-end models for LiDAR-based trajectory prediction. They suffer from a couple of limita-tions: (1) They are not able to leverage the abundant fine-grained visual information from cameras; (2) these models use convolutional feature maps as their intermediate rep-resentations within and across frames, thus suffering from non-differentiable operations such as non-maximum sup-pression in object decoding and object association in multi-object tracking.
IntentNet such as
To address all these challenges, we propose a novel pipeline that leverages a query-centric model design to pre-dict future trajectories, dubbed ViP3D (Visual trajectory
Prediction via 3D agent queries). ViP3D consumes multi-view videos from surrounding cameras and high-definition maps, and makes agent-level future trajectory prediction in an end-to-end and concise streaming manner, as shown in
Figure 1. Specifically, ViP3D leverages 3D agent queries as the interface throughout the pipeline, where each query can map to (at most) an agent in the environment. At each time step, the queries aggregate visual features from multi-view images, learn agent temporal dynamics, model the relation-ship between agents, and finally produce possible future tra-jectories for each agent. Across time, the 3D agent queries are maintained in a memory bank, which can be initialized, updated and discarded to track agents in the environment.
Additionally, unlike previous prediction methods that uti-lize historical agent trajectories and feature maps from mul-tiple historical frames, ViP3D only uses 3D agent queries from one previous timestamp and sensor features from the current timestamp, making it a concise streaming approach.
In summary, the contribution of this paper is three-fold: 1. ViP3D is the first fully differentiable vision-based approach to predict future trajectories of agents for au-tonomous driving. Instead of using hand-picked fea-tures like historical trajectories and agent sizes, ViP3D leverages the rich and fine-grained visual features from raw images which are useful for the trajectory predic-tion task. 2. With 3D agent queries as interface, ViP3D explicitly models agent-level detection, tracking and prediction, making it interpretable and debuggable. 3. ViP3D is a concise model with high performance. It outperforms a wide variety of baselines and recent end-to-end methods on the visual trajectory prediction task. 2.