Abstract
Rigorously testing autonomy systems is essential for making safe self-driving vehicles (SDV) a reality. It requires one to generate safety critical scenarios beyond what can be collected safely in the world, as many scenarios happen rarely on our roads. To accurately evaluate performance, we need to test the SDV on these scenarios in closed-loop, where the SDV and other actors interact with each other at each timestep. Previously recorded driving logs provide
*Indicates equal contribution. a rich resource to build these new scenarios from, but for closed loop evaluation, we need to modify the sensor data based on the new scene configuration and the SDV’s deci-sions, as actors might be added or removed and the tra-jectories of existing actors and the SDV will differ from the original log.
In this paper, we present UniSim, a neural sensor simulator that takes a single recorded log captured by a sensor-equipped vehicle and converts it into a realistic closed-loop multi-sensor simulation. UniSim builds neural feature grids to reconstruct both the static background and dynamic actors in the scene, and composites them together
to simulate LiDAR and camera data at new viewpoints, with actors added or removed and at new placements. To better handle extrapolated views, we incorporate learnable priors for dynamic objects, and leverage a convolutional network to complete unseen regions. Our experiments show UniSim can simulate realistic sensor data with small domain gap on downstream tasks. With UniSim, we demonstrate, for the first time, closed-loop evaluation of an autonomy system on safety-critical scenarios as if it were in the real world. 1.

Introduction
While driving along a highway, a car from the left sud-denly swerves into your lane. You brake hard, avoiding an accident, but discomforting your passengers. As you replay the encounter in your mind, you consider how the scenario would have gone if the other vehicle had acceler-ated more, if you had slowed down earlier, or if you had instead changed lanes for a more comfortable drive. Hav-ing the ability to generate such “what-if” scenarios from a single recording would be a game changer for developing safe self-driving solutions. Unfortunately, such a tool does not exist and the self-driving industry primarily test their systems on pre-recorded real-world sensor data (i.e., log-replay), or by driving new miles in the real-world. In the former, the autonomous system cannot execute actions and observe their effects as new sensor data different from the original recording is not generated, while the latter is neither safe, nor scalable or sustainable. The status quo calls for novel closed-loop sensor simulation systems that are high fidelity and represent the diversity of the real world.
Here, we aim to build an editable digital twin of the real world (through the logs we captured), where existing ac-tors in the scene can be modified or removed, new actors can be added, and new autonomy trajectories can be exe-cuted. This enables the autonomy system to interact with the simulated world, where it receives new sensor obser-vations based on its new location and the updated states of the dynamic actors, in a closed-loop fashion. Such a simulator can accurately measure self-driving performance, as if it were actually in the real world, but without the safety hazards, and in a much less capital-intensive manner.
Compared to manually-created game-engine based virtual worlds [15, 62], it is a more scalable, cost-effective, realis-tic, and diverse way towards closed-loop evaluation.
Towards this goal, we present UniSim, a realistic closed-loop data-driven sensor simulation system for self-driving.
UniSim reconstructs and renders multi-sensor data for novel views and new scene configurations from a single recorded log. This setting is very challenging as the observations are sparse and often captured from constrained viewpoints (e.g., straight trajectories along the roads). To better handle ex-trapolation from the observed views, we propose a series of enhancements over prior neural rendering approaches. In particular, we leverage multi-resolution voxel-based neural fields to represent and compose the static scene and dy-namic agents, and volume render feature maps. To better handle novel views and incorporate scene context to reduce artifacts, a convolutional network (CNN) renders the fea-ture map to form the final image. For dynamic agents, we learn a neural shape prior that helps complete the objects to render unseen areas. We use this sparse voxel-based rep-resentations to efficiently simulate both image and LiDAR observations under a unified framework. This is very useful as SDVs often use several sensor modalities for robustness.
Our experiments show that UniSim realistically simu-lates camera and LiDAR observations at new views for large-scale dynamic driving scenes, achieving SoTA perfor-mance in photorealism. Moreover, we find UniSim reduces the domain gap over existing camera simulation methods on the downstream autonomy tasks of detection, motion fore-casting and motion planning. We also apply UniSim to aug-ment training data to improve perception models. Impor-tantly, we show, for the first time, closed-loop evaluation of an autonomy system on photorealistic safety-critical scenar-ios, allowing us to better measure SDV performance. This further demonstrates UniSim’s value in enabling safer and more efficient development of self-driving. 2.