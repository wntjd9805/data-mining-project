Abstract
Existing techniques for image-to-image translation com-monly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inabil-ity to handle multiple attributes per image. Recent truly-unsupervised methods adopt clustering approaches to eas-ily provide per-sample one-hot domain labels. However, they cannot account for the real-world setting: one sample may have multiple attributes. In addition, the semantics of the clusters are not easily coupled to human understanding.
To overcome these, we present LANguage-driven Image-to-image Translation model, dubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts for a dataset: the similarity between images and attributes indicates per-sample domain labels. This formulation naturally enables multi-hot labels so that users can specify the target domain with a set of attributes in language. To account for the case that the initial prompts are inaccurate, we also present prompt learning. We further present domain regularization loss that enforces translated images to be mapped to the corresponding domain. Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models. The code is available at github.com/KU-CVLAB/LANIT. 1.

Introduction
Unpaired image-to-translation frameworks aim to trans-late an image from one domain to another [21,31,46]. They are typically trained on many images with their respective domain labels. It has been widely studied for a decade in nu-merous computer vision applications, such as image manip-ulation [14,62,66], multi-modal translation [3,9,10,21,35], and instance-aware translation [6, 23, 30].
*Equal contribution
â€ Corresponding author
Conventional methods [9, 10, 21] required at least per-sample domain supervision based on the strict assumption that each image should be described by a one-hot domain label, which is frequently violated; e.g., a face can have multiple attributes.
In addition, annotating domain label for each image may become considerably labor-intensive and error-prone, and subjectivity may lead to inconsistency across different annotators.
To mitigate such a heavy reliance on per-sample domain annotation, some recent methods formulate the problem in a few-shot setting [35, 51] or semi-supervised learning set-ting [55]. More recently, TUNIT [3] and Kim et al. [29] present a framework for jointly learning domain cluster-ing and translation within a unified model, thus enabling a truly-unsupervised learning of image-to-image transla-tion. Although these methods ease the burden of per-sample domain annotation and show relatively competitive perfor-mance, they still inherit the limitation of using one-hot do-main labels to train the image translation model. In addi-tion, these truly-unsupervised learning methods only learn domain clusters that are dominant in the dataset, which sometimes do not reflect semantic meanings. Such limi-tations make users confused in understanding the semantic meaning of each learned domain cluster.
In this paper, to overcome the problems in per-sample-level and unsupervised learning methods, we propose using the dataset-level annotation for the first time as exemplified in Fig. 1. In practice, it could be much easier to list can-didate textual domain descriptions that describe images in the dataset. Using the candidate attributes given in texts for a dataset, we specify the target domain given a reference image according to its similarity to the attributes in vision-language embedding [49]. Then, a generator translates a source image to the target domain using the target style code of the reference image. As similarities naturally lead to a multi-hot label, we can aggregate the style codes of multiple domains to represent the style code for a joint domain. In addition, to account for the fact that the initial prompts may
(a) Per-sample-level (b) Unsupervised (c) Dataset-level (Ours)
Figure 1. Levels of supervision. For unpaired image-to-image translation, (a) conventional methods [10, 21, 35, 55] require at least per-sample-level domain supervision, which is often hard to collect. To overcome this, (b) unsupervised learning methods [3, 29] learn image translation model using a dataset itself without any supervision, but it shows limited performance and lacks the semantic understanding of each cluster, limiting its applicability. Unlike them, (c) we present a novel framework that requires a dataset with possible textual domain descriptions (i.e., dataset-level annotation), which achieves comparable or even better performance than previous methods. by CycleGAN [67], of which many variants were proposed, formulating uni-modal models [34, 60, 61] and multi-modal models [9, 10, 21, 31]. Although these methods overcome the inherent limitation of the paired setting, they still re-quire at least per-sample domain annotation, which is often labor intensive or even impossible to achieve in some cases.
In addition, this per-sample supervision does not consider that some images may require multi-hot domain labels such as facial attributes, e.g., blond and young.
To mitigate the reliance on per-sample supervision, FU-NIT [35] and CoCo-FUNIT [51] proposed a few-shot set-ting, but still required a few annotations. SEMIT [55] utilizes a pseudo labeling technique. S3GAN [42], Self-conditioned GAN [37], and [4] adopt clustering methods or a classifier. Recently, TUNIT [3] and Kim et al. [29] have proposed the truly-unsupervised frameworks with cluster-ing approach or prototype-based learning, which do not need any per-sample domain supervision. However, their performance was limited in that it is challenging to under-stand the semantic meaning of each pseudo domain, which in turn limits their applicability. Note that the aforemen-tioned methods also inherit the problem of the one-hot do-main labeling assumption.
Vision-Language Model in Image Manipulation. Many large-scale vision-language models [24, 49] have been pop-ularly adopted in computer vision problems. Especially,
CLIP [49] has been employed in image synthesis and ma-nipulation with GANs [1, 38, 47, 56]. For instance, Style-CLIP [47] and HairCLIP [56] manipulate the latent space of StyleGAN [27], through the guidance of CLIP, but they heavily rely on pre-trained StyleGAN. CLIP2StyleGAN ex-tends such an approach in an unsupervised manner that eliminates the need for target style descriptions.
More recently, some works [2,12,16,28,39,45,50] com-bine a diffusion model [18] with CLIP [49] for text-driven image manipulation with high fidelity. However, they show limited performance in realistically translating in the real image domain. Furthermore, they rely on pre-trained dif-Figure 2. Examples of semantic encoding. The existing unsuper-vised method [3] allows users to translate one of several clusters.
However, the learned clusters lack semantic meaning and some-times some attributes do not appear in any clusters. Unlike this, our framework can select and train the domains that have explicit semantic meaning, which is more applicable. be sometimes inaccurate, we also propose prompt learning and encourage the translated images to have corresponding labels with our proposed domain regularization loss.
Our proposed framework just requires some textual do-main descriptions for defining domains and it can ease the burden on the per-sample annotating process. Thanks to the proposed dataset-level annotation, users can explicitly define the semantic meaning of the target domain with tex-tual descriptions, as exemplified in Fig. 2. We evaluate our method on several standard benchmarks [7, 26, 35]. Exper-imental results show that the proposed model trained only with dataset-level supervision is comparable or even outper-forms the latest methods, including StarGAN2 [10] trained with per-sample supervision. We also provide intensive ab-lation studies and user study results. 2.