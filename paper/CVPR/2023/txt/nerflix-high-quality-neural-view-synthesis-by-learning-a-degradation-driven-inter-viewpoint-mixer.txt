Abstract
Neural radiance fields (NeRF) show great success in novel view synthesis. However, in real-world scenes, re-covering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality train-ing frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis qual-ity of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we de-*Equal contribution
†Corresponding author sign a NeRF-style degradation modeling approach and con-struct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for ex-isting deep neural networks. Moreover, beyond the degra-dation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge
NeRF models to entirely new levels and producing highly photo-realistic synthetic views. 1.

Introduction
Neural radiance fields (NeRF) can generate photo-realistic images from new viewpoints, playing a heated role in novel view synthesis. In light of NeRF’s [37] success, numerous approaches [2, 9, 11, 19, 34, 35, 38, 40, 41, 47, 53, 54, 59, 61, 65] along these lines have been proposed, contin-ually raising the performance to greater levels. In fact, one prerequisite of NeRF is the precise camera settings of the taken photos for training [22, 32, 61]. However, accurately calibrating camera poses is exceedingly difficult in prac-tice. Contrarily, the shape-radiance co-adaption issue [74] reveals that while the learned radiance fields can perfectly explain the training views with inaccurate geometry, they poorly generalize to unseen views. On the other hand, the capacity to represent sophisticated geometry, lighting, ob-ject materials, and other factors is constrained by the simpli-fied scene representation of NeRF [19, 77, 78]. On the basis of such restrictions, advanced NeRF models may nonethe-less result in notable artifacts (such as blur, noise, detail missing, and more), which we refer to as NeRF-style degra-dations in this article and are shown in Fig. 1.
To address the aforementioned limitations, numerous works have been proposed. For example, some studies, including [22, 59, 66, 70], jointly optimize camera param-eters and neural radiance fields to refine camera poses as precisely as possible in order to address the camera calibra-tion issue. Another line of works [19, 73, 77, 78] presents physical-aware models that simultaneously take into ac-count the object materials and environment lighting, as op-posed to using MLPs or neural voxels to implicitly encode both the geometry and appearance. To meet the demands for high-quality neural view synthesis, one has to carefully examine all of the elements when building complex inverse rendering systems. In addition to being challenging to op-timize, they are also not scalable for rapid deployment with hard re-configurations in new environments. Regardless of the intricate physical-aware rendering models, is it possi-ble to design a practical NeRF-agnostic restorer to directly enhance synthesized views from NeRFs?
In the low-level vision, it is critical to construct large-scale paired data to train a deep restorer for eliminating real-world artifacts [56, 72]. When it comes to NeRF-style degradations, there are two challenges: (1) sizable paired training data; (2) NeRF degradation analysis. First, it is un-practical to gather large-scale training pairs (more specifi-cally, raw outputs from well-trained NeRFs and correspond-ing ground truths). Second, the modeling of NeRF-style degradation has received little attention. Unlike real-world images that generally suffer from JPEG compression, sen-sor noise, and motion blur, the NeRF-style artifacts are complex and differ from the existing ones. As far as we know, no previous studies have ever investigated NeRF-style degradation removal which effectively leverages the ample research on image and video restoration.
In this work, we are motivated to have the first study on the feasibility of simulating large-scale NeRF-style paired data, opening the possibility of training a NeRF-agnostic restorer for improving the NeRF rendering frames. To this end, we present a novel degradation simulator for typ-ical NeRF-style artifacts (e.g., rendering noise and blur) considering the NeRF mechanism. We review the over-all NeRF rendering pipeline and discuss the typical NeRF-style degradation cases. Accordingly, we present three basic degradation types to simulate the real rendered artifacts of
NeRF synthetic views and empirically evaluate the distri-bution similarity between real rendered photos and our sim-ulated ones. The feasibility of developing NeRF-agnostic restoration models has been made possible by constructing a sizable dataset that covers a variety of NeRF-style degra-dations, over different scenes.
Next, we show the necessity of our simulated dataset and demonstrate that existing state-of-the-art image restoration frameworks can be used to eliminate NeRF visual artifacts.
Furthermore, we notice, in a typical NeRF setup, neigh-boring high-quality views come for free, and they serve as potential reference bases for video-based restoration with a multi-frame aggregation and fusion module. However, this is not straightforward because NeRF input views are taken from a variety of very different angles and locations, mak-ing the estimation of correspondence quite challenging. To tackle this problem, we propose a degradation-driven inter-viewpoint “mixer” that progressively aligns image contents at the pixel and patch levels. In order to maximize efficiency and improve performance, we also propose a fast view se-lection technique to only choose the most pertinent refer-ence training views for aggregation, as opposed to using the entire NeRF input views.
In a nutshell, we present a NeRF-agnostic restorer (termed NeRFLiX) which learns a degradation-driven inter-viewpoint mixer. As illustrated in Fig. 1, given NeRF syn-thetic frames with various rendering degradations, NeR-FLiX successfully restores high-quality results. Our con-tributions are summarized as
• Universal enhancer for NeRF models. NeRFLiX is powerful and adaptable, removing NeRF artifacts and restoring clearly details, pushing the performance of cutting-edge NeRF models to entirely new levels.
• NeRF rendering degradation simulator. We develop a NeRF-style degradation simulator (NDS), construct-ing massive amounts of paired data and aiding the training of deep neural networks to improve the quality of NeRF-rendered images.
• Inter-viewpoint mixer. Based on our constructed
NDS, we further propose an inter-viewpoint baseline that is able to mix high-quality neighboring views for more effective restorations.
• Training time acceleration. We show how NeRFLiX makes it possible for NeRF models to produce even better results with a 50% reduction in training time.
2.