Abstract 1.

Introduction
While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Fur-thermore, can we compose multiple new concepts together?
We propose Custom Diffusion, an efficient method for aug-menting existing text-to-image models. We find that only op-timizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (∼ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multi-ple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient.
Recently released text-to-image models [53, 57, 60, 79] have represented a watershed year in image generation. By simply querying a text prompt, users are able to generate images of unprecedented quality. Such systems can generate a wide variety of objects, styles, and scenes – seemingly
“anything and everything”.
However, despite the diverse, general capability of such models, users often wish to synthesize specific concepts from their own personal lives. For example, loved ones such as family, friends, pets, or personal objects and places, such as a new sofa or a recently visited garden, make for intriguing concepts. As these concepts are by nature personal, they are unseen during large-scale model training. Describing these concepts after the fact, through text, is unwieldy and unable to produce the personal concept with sufficient fidelity.
This motivates a need for model customization. Given the few user-provided images, can we augment existing text-to-image diffusion models with the new concept (for example, their pet dog or a “moongate” as shown in Figure 1)? The fine-tuned model should be able to generalize and compose them with existing concepts to generate new variations. This
poses a few challenges – first, the model tends to forget [12, 35, 52] or change [34, 41] the meanings of existing concepts: e.g., the meaning of “moon” being lost when adding the
“moongate” concept. Secondly, the model is prone to overfit the few training samples and reduce sampling variations.
Moreover, we study a more challenging problem, compo-sitional fine-tuning – the ability to extend beyond tuning for a single, individual concept and compose multiple concepts together, e.g., pet dog in front of moongate (Figure 1).
Improving compositional generation has been studied in re-cent works [40]. But composing multiple new concepts poses additional challenges, such as mixing unseen concepts.
In this work, we propose a fine-tuning technique, Custom
Diffusion for text-to-image diffusion models. Our method is computationally and memory efficient. To overcome the above-mentioned challenges, we identify a small subset of model weights, namely the key and value mapping from text to latent features in the cross-attention layers [5, 70].
Fine-tuning these is sufficient to update the model with the new concept. To prevent model forgetting, we use a small set of real images with similar captions as the target images.
We also introduce augmentation during fine-tuning, which leads to faster convergence and improved results. To inject multiple concepts, our method supports training on both simultaneously or training them separately and then merging.
We build our method on Stable Diffusion [1] and experi-ment on various datasets with as few as four training images.
For adding single concepts, our method shows better text alignment and visual similarity to the target images than con-current works and baselines. More importantly, our method can compose multiple new concepts efficiently, whereas con-current methods struggle and often omit one. Finally, our method only requires storing a small subset of parameters (3% of the model weights) and reduces the fine-tuning time (6 minutes on 2 A100 GPUs, 2 faster compared to concurrent works). Full version of the paper is available at https://arxiv.org/abs/2212.04488. 4
×
− 2.