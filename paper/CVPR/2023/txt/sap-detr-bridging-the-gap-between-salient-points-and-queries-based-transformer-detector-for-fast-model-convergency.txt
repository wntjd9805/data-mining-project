Abstract
Recently, the dominant DETR-based approaches apply central-concept spatial prior to accelerating Transformer detector convergency. These methods gradually refine the reference points to the center of target objects and imbue ob-ject queries with the updated central reference information for spatially conditional attention. However, centralizing ref-erence points may severely deteriorate queries’ saliency and confuse detectors due to the indiscriminative spatial prior.
To bridge the gap between the reference points of salient queries and Transformer detectors, we propose SAlient
Point-based DETR (SAP-DETR) by treating object detec-tion as a transformation from salient points to instance ob-jects. Concretely, we explicitly initialize a query-specific reference point for each object query, gradually aggregate them into an instance object, and then predict the distance from each side of the bounding box to these points. By rapidly attending to query-specific reference regions and the conditional box edges, SAP-DETR can effectively bridge the gap between the salient point and the query-based Trans-former detector with a significant convergency speed. Exper-imentally, SAP-DETR achieves 1.4× convergency speed with competitive performance and stably promotes the SoTA ap-proaches by ∼1.0 AP. Based on ResNet-DC-101, SAP-DETR achieves 46.9 AP. The code will be released at https:
//github.com/liuyang-ict/SAP-DETR. 1.

Introduction
Object detection is a fundamental task in computer vi-sion, whose target is to recognize and localize each ob-ject from input images. In the last decade, various detec-*This work was done when working as an intern at AI Lab, Lenovo
Research, Beijing, China.
†Corresponding author.
Figure 1. Comparison of SAP-DETR and DAB-DETR under 3-layer decoder model and 36-epoch training scheme. (a) Statistics of the query count in different classification score intervals. (b) and (c) Visualize the distribution of all reference points (pink) and highlight the top-20 classification score queries with their bounding boxes (blue) and reference points (red) in different decoder layers. (d) Visualize the outputs of positive queries and ground truth (red) during the training process, each query has a representative color for its reference point and bounding box. Best viewed in color. tors [6, 11, 14, 18, 20, 22] based on Convolutional Neural Net-works (CNNs), have received widespread attention and made significant progress. Recently, Carion et al. [2] proposed a new end-to-end paradigm for object detection based on the
Transformer [24], called DEtection TRansformer (DETR), which treats object detection as a problem of set prediction.
In DETR, a set of learnable positional encodings, namely object queries, are employed to aggregate instance features from the context image in Transformer Decoder. The predic-tions of queries are finally assigned to the ground truth via bipartite matching to achieve end-to-end detection.
Despite the promising results of DETR, its application is largely limited by considerably longer training time com-pared to conventional CNNs. To address this problem, many
variants attempted to take a close look at query paradigm and introduced various spatial priors for model convergency and efficacy. According to the type of spatial prior, they can be categorized into implicit and explicit methods. The implicit ones [5, 16, 31] attempt to decouple a reference point from the object query and make use of this spatial prior to attend to the image context features efficiently. The current state-of-the-arts (SoTAs) are dominated by the explicit ones [13, 25], which suggest to instantiate a position with spatial prior for each query, i.e., explicit reference coordinates with a center point or an anchor box. These reference coordinates serve as helpful priors and enable the queries to focus on their expected regions easily. For instance, Anchor DETR [25] in-troduced an anchor concept (center point with different box size patterns) to formulate the query position and directly regressed the central offsets of the bounding boxes. DAB-DETR [13] further stretched the center point to a 4D anchor box concept [cx, cy, w, h] to refine proposal bonding boxes in a cascaded manner. However, instantiating the query loca-tion as a target center may severely degrade the classification accuracy and convergency speed. As illustrated in Fig. 1, there exist many plausible queries [19] with high-quality classification scores (Fig. 1(a) within red box) and box In-tersection over Union (IoU, see the redundant blue boxes in Fig. 1(b) and (c)), which only brings a slight improvement on precision rate but inevitably confuses the detector on the positive query assignments when training with bipartite matching strategy. This is because the plausible predictions are considered in negative classification loss, which severely decelerates the model convergency. As shown in Fig. 1(b) and (c), the predefined reference point of the positive query may not be the nearest one to the center of the ground truth bounding box, and the reference points tend to be centralized or marginalized (cyan arrows in Fig. 1(b)), hence losing the spatial specificity. With further insight into the one-to-one label assignment during the training process, we find that the query, whose reference point is closest to the center point, also has a high-quality IoU, but it still exists a disparity with the positive query in the classification confidence. Therefore, we argue that such a centralized spatial prior may cause de-generation of target consistency in both classification and localization tasks, which leads to inconsistent predictions.
Furthermore, the mentioned central point-based variants also have difficulties in detecting occluded objects. For ex-ample, Fig. 1(d) shows that DAB-DETR detects the left baseman twice, while the query point in SAP-DETR is not necessarily the center point, so the query point of the bound-ing box for the occluded baseman is from a pixel on the occluded baseman on the top right area instead of from the left baseman. One solution for center-based method [25] is to predefine different receptive fields (similar to the scaling anchor box in YOLO [17]) for the position of each query.
However, increasing the diversity of the receptive fields for each position query is unsuitable for non-overlapped targets, as it still generates massive indistinguishable predictions for one position as same as other center-based models.
To bridge these gaps, in this paper, we present a novel framework for Transformer detector, called SAlient Point-based DETR (SAP-DETR), which treats object detection as a transformation from salient points to instance objects.
Instead of regressing the reference point to the target center, we define the reference point belonging to one positive query as a salient point, keep this query-specific spatial prior with a scaling amplitude, and then gradually update them to an in-stance object by predicting the distance from each side of the bounding box. Specifically, we tile the mesh-grid referenced points and initialize their center/corner as the query-specific reference point. To disentangle the reference sparsity as well as stabilize the training process, a movable strategy with scaling amplitude is applied for reference point adjustment, which prompts queries to consider their reference grid as the salient region to perform image context attention. By localizing each side of the bounding box layer by layer, such query-specific spatial prior enables compensation for the over-smooth/inadequacy problem during center-based de-tection, thereby vastly promoting model convergency speed.
Inspired by [5, 13, 16], we also take advantage of both Gaus-sian spatial prior and conditional cross-attention mechanism, and then a salient point enhanced cross-attention mecha-nism is developed to distinguish the salient region and other conditional extreme regions from the context image features.
We bridge the gap between salient points and query-based
Transformer detector by speedily attending to the query-specific region and other conditional regions. The extensive experiments have shown that SAP-DETR achieves superior convergency speed and performance. To the best of our knowledge, this is the first work to introduce the salient point based regression into end-to-end query-based Transformer detectors. Our contributions can be summarized as follows. 1) We introduce the salient point concept into query-based
Transformer detectors by assigning query-specific reference points to object queries. Unlike center-based methods, we restrict the reference location and define the point of the posi-tive query as the salient one, hence enlarging the discrepancy of query as well as reducing the redundant predictions (see
Fig. 1). Thanks to the efficacy of the query-specific prior, our SAP-DETR accelerates the convergency speed greatly, achieving competitive performance with 30% fewer train-ing epochs. The proposed movable strategy further boosts
SAP-DETR to a new SoTA performance. 2) We devise a point-enhanced cross-attention mechanism to imbue query with spatial prior based on both reference point and box sides for final specific region attention. 3) Evaluation over COCO dataset has demonstrated that
SAP-DETR achieves superior convergency speed and detec-tion accuracy. Under the same training settings, SAP-DETR
outperforms the SoTA approaches with a large margin. 2.