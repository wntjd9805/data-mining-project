Abstract
In this paper, we propose a novel hybrid representa-tion and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the mod-eling power of neural fields with the ease of use and in-herent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geom-etry and texture features on the vertices of a deformable body model, thus exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that admits fitting to unseen scans and sampling of realistic avatars with var-ied appearances and geometries. Furthermore, our repre-sentation allows local editing by swapping local features between 3D assets. To verify our method for avatar cre-ation and editing, we contribute a new high-quality dataset, dubbed CustomHumans, for training and evaluation. Our experiments quantitatively and qualitatively show that our method generates diverse detailed avatars and achieves bet-ter model fitting performance compared to state-of-the-art methods. Our code and dataset are available at https:
//ait.ethz.ch/custom-humans. 1.

Introduction 3D Avatars are an important aspect of many emerging applications such as 3D games or the Metaverse. Allowing for easy personalization of such avatars, holds the promise of increased user engagement. Traditionally, editing 3D as-sets requires knowledge of computer graphics tools and re-lies on standardized data formats to represent shapes and appearances. While methods for reconstruction or genera-tive modeling of learned avatars achieve impressive results, it is unknown how such neural avatars can be edited and customized. Thus, the goal of our work is to contribute a simple, yet powerful data-driven method for avatar creation and customization (Fig. 1): our method enables (a) the abil-ity to transfer partial geometric and appearance details be-tween 3D assets, and (b) the ability to author details via 2D-3D transfer. The resulting avatars (c) retain consistent local details when posed.
Figure 1. Creating locally editable avatars: Given an input avatar, (a) the avatar can be edited by transferring clothing geome-try and color details from existing, yet unseen 3D assets. (b) Users can customize clothing details such as logos and letters via draw-ing on 2D images. (c) The avatars retain local detail consistently under pose changes.
Existing methods do not allow for such capabilities.
While 3D generative models of articulated human bod-ies [5, 21, 25, 42, 75] leverage differentiable neural render-ing to learn from images, they cannot control local de-tails due to highly entangled color and geometry in the 2D supervision signal. Generative models trained on 3D data [9, 13, 36, 44, 45] can produce geometric details for surfaces and clothing. However, the diversity of generated samples is low due to the lack of high-quality 3D human scans and not all methods model appearance.
At the core of the issue lies the question of represen-tation: graphics tools use meshes, UV, and texture maps which provide consistent topologies under deformation.
However, human avatar methods that are built on mesh-based representations and linear blend skinning (LBS) are limited in their representational power with respect to chal-lenging geometry (e.g., puffy garments) and flexible topolo-gies (e.g., jackets), even with adaptations of additional dis-placement parameters [36] and mesh subdivision [66].
Inspired by the recent neural 3D representations [40, 62, 70, 72], we propose a novel hybrid representation for digi-tal humans. Our representation combines the advantages of consistent topologies of LBS models with the representa-tional power of neural fields. The key idea is to decompose the tasks of deformation consistency on one hand and lo-cal surface and appearance description on the other. For the former, we leverage existing parametric body models (e.g.,
SMPL [33] and SMPL-X [48]). For the latter, we leverage the fixed topology of the poseable mesh to store local fea-ture codebooks. A decoder, shared across subjects, is then conditioned on the local features to predict the final signed distance and color values. Since only local information [15] is exposed to the decoder, overfitting and memorization can be mitigated. We experimentally show that this is crucial for 3D avatar fitting and reposing.
Complementing this hybrid representation, we propose a training pipeline in the auto-decoding generative frame-work [9, 46, 52]. To this end, we jointly optimize multi-subject feature codebooks and the shared decoder weights via 3D reconstruction and 2D adversarial losses. The 3D losses help in disentangling appearance and geomet-ric information from the input scans, while the latter im-proves the perceptual quality of randomly generated sam-ples. To showcase the hybrid representation and the gen-erative model we implement a prototypical avatar editing workflow shown in Fig. 1.
Furthermore, to enable research on high-quality 3D avatars we contribute training data for generative 3D hu-man models. We record a large-scale dataset (more than 600 scans of 80 subjects in 120 garments) using a volumet-ric capture stage [11]. Our dataset consists of high-quality 3D meshes alongside accurately registered SMPL-X [48] models and will be made available for research purposes.
Finally, we assess our design decisions in detailed evalua-tions, both on existing and the proposed datasets.
In summary, our contributions are threefold: (a) a novel hybrid representation for 3D virtual humans that allows for local editing across subjects, (b) a generative pipeline of 3D avatars creation that allows for fitting to unseen 3D scans and random sampling, and (c) a new large-scale high-quality dataset of 3D human scans containing diverse sub-jects, body poses and garments. 2.