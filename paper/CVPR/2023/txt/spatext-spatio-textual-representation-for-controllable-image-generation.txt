Abstract
Recent text-to-image diffusion models are able to gener-ate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different re-gions/objects or their layout in a fine-grained fashion. Pre-vious attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present
SpaText — a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is anno-tated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual de-scription for each region in the image, we choose to lever-age the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual repre-sentation, and show its effectiveness on two state-of-the-art
In addi-diffusion models: pixel-based and latent-based. tion, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm.
Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control. 1.

Introduction
Imagine you could generate an image by dipping your digital paintbrush (so to speak) in a “black horse” paint, then sketching the specific position and posture of the horse, afterwards, dipping it again in a “red full moon” paint and sketching it the desired area. Finally, you want the entire image to be in the style of The Starry Night. Current state-of-the-art text-to-image models [51, 59, 72] leave much to
Project page is available at: https://omriavrahami.com/spatext 1
be desired in achieving this vision.
The text-to-image interface is extremely powerful — a single prompt is able to represent an infinite number of pos-sible images. However, it has its cost — on the one hand, it enables a novice user to explore an endless number of ideas, but, on the other hand, it limits controllability: if the user has a mental image that they wish to generate, with a specific layout of objects or regions in the image and their shapes, it is practically impossible to convey this in-formation with text alone, as demonstrated in Figure 2. In addition, inferring spatial relations [72] from a single text prompt is one of the current limitations of SoTA models.
Make-A-Scene [22] proposed to tackle this problem by adding an additional (optional) input to text-to-image mod-els, a dense segmentation map with fixed labels. The user can provide two inputs: a text prompt that describes the en-tire scene and an elaborate segmentation map that includes a label for each segment in the image. This way, the user can easily control the layout of the image. However, it suffers from the following drawbacks: (1) training the model with a fixed set of labels limits the quality for objects that are not in that set at inference time, (2) providing a dense segmenta-tion can be cumbersome for users and undesirable in some cases, e.g., when the user prefers to provide a sketch for only a few main objects they care about, letting the model infer the rest of the layout; and (3) lack of fine-grained con-trol over the specific characteristic of each instance. For ex-ample, even if the label set contains the label “dog”, it is not clear how to generate several instances of dogs of different breeds in a single scene.
In order to tackle these drawbacks, we propose a differ-ent approach: (1) rather than using a fixed set of labels to represent each pixel in the segmentation map, we propose to represent it using spatial free-form text, and (2) rather than providing a dense segmentation map accounting for each pixel, we propose to use a sparse map, that describes only the objects that a user specifies (using spatial free-form text), while the rest of the scene remains unspecified.
To summarize, we propose a new problem setting: given a global text prompt that describes the entire image, and a spatio-textual scene that specifies for segments of inter-est their local text description as well as their position and shape, a corresponding image is generated, as illustrated in
Figure 1. These changes extend expressivity by providing the user with more control over the regions they care about, leaving the rest for the machine to figure out.
Acquiring a large-scale dataset that contains free-form textual descriptions for each segment in an image is pro-hibitively expensive, and such large-scale datasets do not exist to the best of our knowledge. Hence, we opt to extract the relevant information from existing image-text datasets.
To this end, we propose a novel CLIP-based [49] spatio-textual representation that enables a user to specify for each
“at the beach”
SpaText
Stable Diffusion
DALL·E 2
“a white Labrador”
“a blue ball”
“a white Labrador at the beach puts its right arm above a blue ball without touching, while sitting in the bottom right corner of the frame”
Figure 2. Lack of fine-grained spatial control: A user with a specific mental image of a Labrador dog holding its paw above a blue ball without touching, can easily generate it with a SpaText representation (left) but will struggle to do so with traditional text-to-image models (right) [52, 56]. segment its description using free-form text and its posi-tion and shape. During training, we extract local regions using a pre-trained panoptic segmentation model [69], and use them as input to a CLIP image encoder to create our representation. Then, at inference time, we use the text de-scriptions provided by the user, embed them using a CLIP text encoder, and translate them to the CLIP image embed-ding space using a prior model [51].
In order to assess the effectiveness of our proposed rep-resentation SpaText, we implement it on two state-of-the-art types of text-to-image diffusion models: a pixel-based model (DALL·E 2 [51]) and a latent-based model (Stable
Diffusion [56]). Both of these text-to-image models em-ploy classifier-free guidance [33] at inference time, which supports a single conditioning input (text prompt). In order to adapt them to our multi-conditional input (global text as well as the spatio-textual representation), we demonstrate how classifier-free guidance can be extended to any multi-conditional case. To the best of our knowledge, we are the first to demonstrate this. Furthermore, we propose an ad-ditional, faster variant of this extension that trades-off con-trollability for inference time.
Finally, we propose several automatic evaluation metrics for our problem setting and use them along with the FID score to evaluate our method against its baselines. In addi-tion, we conduct a user-study and show that our method is also preferred by human evaluators.
In summary, our contributions are: (1) we address a new scenario of image generation with free-form textual scene control, (2) we propose a novel spatio-textual representa-tion that for each segment represents its semantic proper-ties and structure, and demonstrate its effectiveness on two state-of-the-art diffusion models — pixel-based and latent-based, (3) we extend the classifier-free guidance in diffusion models to the multi-conditional case and present an alter-native accelerated inference algorithm, and (4) we propose several automatic evaluation metrics and use them to com-pare against baselines we adapted from existing methods.
We also evaluate via a user study. We find that our method achieves state-of-the-art results. 2
2.