Abstract
Semantic perception is a core building block in au-tonomous driving, since it provides information about the drivable space and location of other traffic participants.
For learning-based perception, often a large amount of di-verse training data is necessary to achieve high perfor-mance. Data labeling is usually a bottleneck for developing such methods, especially for dense prediction tasks, e.g., se-mantic segmentation or panoptic segmentation. For 3D Li-DAR data, the annotation process demands even more effort than for images. Especially in autonomous driving, point clouds are sparse, and objects appearance depends on its distance from the sensor, making it harder to acquire large amounts of labeled training data. This paper aims at taking an alternative path proposing a self-supervised representa-tion learning method for 3D LiDAR data. Our approach exploits the vehicle motion to match objects across time viewed in different scans. We then train a model to max-imize the point-wise feature similarities from points of the associated object in different scans, which enables to learn a consistent representation across time. The experimental results show that our approach performs better than previ-ous state-of-the-art self-supervised representation learning methods when fine-tuning to different downstream tasks. We furthermore show that with only 10% of labeled data, a net-work pre-trained with our approach can achieve better per-formance than the same network trained from scratch with all labels for semantic segmentation on SemanticKITTI. 1 1.

Introduction
Semantic perception is essential for safe interaction be-tween autonomous vehicles and their surrounding. For learning-based perception, a massive amount of training data is usually required for training high-performance mod-els. However, the data annotation is the bottleneck of col-lecting such large training sets, especially for dense predic-tion tasks, such as semantic segmentation [60, 80], instance 1Code: https://github.com/PRBonn/TARL
Figure 1. Our pre-training (TARL) can reduce the amount of nec-essary labels during fine-tuning on SemanticKITTI [4, 22]. Our method requires only 10% of labels to surpass the network trained from scratch with the full dataset for semantic segmentation. For panoptic segmentation, our method requires only half of the labels. segmentation [46, 73], and panoptic segmentation [45, 79], which require fine-grained labels.
In the context of au-tonomous driving, recent approaches exploit 3D LiDAR data [20, 54, 55, 61], where the data annotation process is more complex than on 2D RGB images due to the sparsity of the point cloud and object appearance varying with its distance to the sensor.
Recent self-supervised representation learning meth-ods [10, 11, 13–15, 25, 27, 76] tackle the lack of annotated data with a pre-training step requiring no labels. Those methods use data augmentation to generate different views from one data sample and train the network to learn an embedding space able to have similar representations for the generated augmented views. Other approaches [52, 63, 65, 69, 78] propose optimizing the pixel embedding space to learn a dense representation suited to be fine-tuned to more fine-grained tasks. For 3D point cloud data, recent approaches [21, 42, 49, 56] focus on synthetic point clouds of single objects to learn a robust representation for object classification. Other approaches [31,36,50,67,74,75] target real-world data representation, such as LiDAR or RGB-D data, but fewer target autonomous driving scenarios.
In this paper, we propose a novel temporal association representation learning (TARL) method for 3D LiDAR data designed for autonomous driving data. We exploit the ve-hicle motion to extract different views of the same objects across time. Then, we compute point-wise features from the objects in the point cloud and use a Transformer en-coder [2] as a projection head to learn a temporal asso-ciation from the object representation, embedding the ob-jects dynamics. We conduct extensive experiments to com-pare our approach with state-of-the-art methods and show that our approach surpasses previous self-supervised pre-training methods [50, 67, 75] when fine-tuning to different downstream tasks and datasets [4, 8, 22, 64]. In summary, our key contributions are:
• We propose a novel self-supervised pre-training for 3D LiDAR data able to learn a robust and temporally-consistent representation by clustering together points from the same object viewed at different points in time.
• We achieve better performance than previous state-of-the-art methods on different downstream tasks, i.e., se-mantic segmentation, panoptic segmentation, and ob-ject detection.
• With our pre-training, we require only 10% of labels to surpass the network trained from scratch for seman-tic segmentation using the whole training set on Se-manticKITTI (see Fig. 1).
• Our self-supervised pre-training produces representa-tions more suited for transfer learning than supervised pre-training, achieving better performance when fine-tuning to a different dataset. 2.