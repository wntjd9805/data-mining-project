Abstract
Self-supervised learning in vision–language processing (VLP) exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior im-ages. This does not only introduce poor alignment be-tween the modalities but also a missed opportunity to ex-ploit rich self-supervision through existing temporal con-tent in the data.
In this work, we explicitly account for prior images and reports when available during both train-ing and fine-tuning. Our approach, named BioViL-T, uses a CNN–Transformer hybrid multi-image encoder trained jointly with a text model.
It is designed to be versatile to arising challenges such as pose variations and miss-ing input images across time. The resulting model excels on downstream tasks both in single- and multi-image se-tups, achieving state-of-the-art (SOTA) performance on (I) progression classification, (II) phrase grounding, and (III) report generation, whilst offering consistent improvements on disease classification and sentence-similarity tasks. We release a novel multi-modal temporal benchmark dataset,
MS-CXR-T, to quantify the quality of vision–language rep-resentations in terms of temporal semantics. Our experi-mental results show the advantages of incorporating prior images and reports to make most use of the data.
Figure 1. (a) Existing visual–language pre-training approaches
[9, 32, 81] often use only a single image for contrastive learning (e.g., InfoNCE [49]). (b) In such settings, discarding the temporal connectivity of images limits the alignment of image–text pairs as shown with the affinity matrix, leading to suboptimal pre-training and missed opportunity to create additional model supervision for free. (c, d) Our approach exploits this domain knowledge by learn-ing to incorporate a series of images and correlate them to reports, leading to pre-trained models that can generalise to a wider range of downstream tasks whilst achieving SOTA performance. 1.

Introduction
Self-supervision from image–text pairs has enabled the development of flexible general-purpose vision–language models both in the general domain [40, 53, 77] and for specialised domains such as biomedicine and radiology
∗These authors contributed equally.
†Corresponding author: ozan.oktay@microsoft.com
[9, 32, 81]. Vision–language processing (VLP) has shown that cross-modal supervision can provide a richer signal for training both image [19] and text [9] models. However, the success of VLP relies on paired samples sharing semantics, i.e., given an image and text pair, the text should describe the image with minimal extraneous detail [15, 16, 35].
In this regard, VLP in biomedicine and radiology poses a distinctive challenge, as reports routinely include compar-isons to prior imaging studies [3, 47, 57]. Without knowl-edge of this prior image1, temporal information in the text modality, e.g. “Pneumonia is improving”, could pertain to any image containing “Pneumonia”, producing ambiguity during contrastive training (Figure 1). Despite this, the ex-isting VLP work to date considers alignment between only single images and reports [9,32,46,81], going so far as to re-move temporal content from reports in training data to pre-vent ‘hallucinations’ in downstream report generation [54].
However, temporal information can provide complementary self-supervision, solely by exploiting existing structure, and without requiring any additional data.
In this work, we neither ignore nor remove temporal in-formation in the text modality, but explicitly account for it during pre-training. Rather than treating all image–report pairs in the dataset as independent, we exploit temporal cor-relations by making prior images available for comparison to a given report. To learn from this structure, we develop a temporal VLP pre-training framework named BioViL-T.
A core component is its new multi-image encoder that can handle the absence of prior images and potential spatial misalignment between images across time. BioViL-T takes into account prior images where available, removing cross-modal ambiguity as illustrated in Fig. 1. Linking multi-ple images during pre-training proves beneficial to both im-age and text models: we report state-of-the-art (SOTA) per-formance on both temporal image classification and report generation. In the latter case, we show that prefixing the prior report substantially increases performance, again re-flecting the value of prior information. We emphasise that the benefit is not restricted to temporal downstream tasks: our approach also achieves SOTA on non-temporal tasks of pneumonia detection [60] and phrase grounding [10], un-derscoring the value of a cleaner learning signal during VLP without needing to modify or add to the training dataset.
Our contributions can be summarised as follows:
• We introduce a novel pre-training framework called
BioViL-T. It leverages the temporal relationship of sam-ples to self-supervise VLP models, making commonly used biomedical VLP models (e.g., [9, 32, 81]) more ap-plicable to a wider range of downstream tasks without compromising performance on existing benchmarks.
• We develop a generic multi-image encoder that handles missing image inputs and incorporates longitudinal in-formation without requiring explicit image registration.
• We achieve SOTA results in chest X-ray (CXR) report generation, temporal image classification, and phrase grounding downstream benchmarks by accounting for prior context in self-supervised training and fine-tuning.
• We release a new multimodal benchmark dataset,
MS-CXR-T, curated by an expert radiologist. It enables 1In the MIMIC-CXR v2 dataset [36], around 40% of reports explicitly reference a previous image. See Appendix B for details. benchmarking of CXR VLP models in terms of tempo-ral semantics extracted from image and text data. 2.