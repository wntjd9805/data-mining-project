Abstract 1.

Introduction
Edit fidelity is a significant issue in open-world control-lable generative image editing. Recently, CLIP-based ap-proaches have traded off simplicity to alleviate these prob-lems by introducing spatial attention in a handpicked layer of a StyleGAN. In this paper, we propose CoralStyleCLIP, which incorporates a multi-layer attention-guided blend-ing strategy in the feature space of StyleGAN2 for obtain-ing high-fidelity edits. We propose multiple forms of our co-optimized region and layer selection strategy to demon-strate the variation of time complexity with the quality of edits over different architectural intricacies while preserv-ing simplicity. We conduct extensive experimental analysis and benchmark our method against state-of-the-art CLIP-based methods. Our findings suggest that CoralStyleCLIP results in high-quality edits while preserving the ease of use.
Controlling smooth semantic edits to photorealistic im-ages [1, 5, 34, 41] synthesized by well-known Generative
Adversarial Networks (GANs) [13, 18, 19] has become simplified with guidance from independently trained con-trastive models such as CLIP [36]. Using natural language as a rich medium of instruction for open-world image syn-thesis [39, 46–48] and editing [11, 12, 24, 26, 28, 45] has ad-dressed many drawbacks of previously proposed methods.
As first demonstrated by StyleCLIP [34], the require-ments for large amounts of annotated data [25] and manual efforts [14, 44] were considerably alleviated. Furthermore, the range of possible edits that were achievable significantly improved [34]. The underlying theme of related approaches involves CLIP-driven exploration [15,22,34] of the interme-diate disentangled latent spaces of the GANs.
jointly learn a global direction [22,34] in the W + space and limit the predicted areas of interest at every layer to seg-ments from a pre-trained segmentation network. Doing so reduces the learning complexity significantly (see Table 1), albeit with potential pitfalls discussed in Section 4.3. We mitigate these pitfalls with a jointly trained attention net-work where we relax the areas of interest at every layer to spatial masks predicted by the network (see Section 3.2).
As a result, the training time increases from a few minutes to about an hour while improving the quality of edits com-pared to the segment selection approach.
In summary, our contributions are as follows:
• We propose a novel multi-layer blending strategy that attends to features selectively at the appropriate Style-GAN layer with minimal hand-holding.
• A CORAL variant based on segment selection demon-strates high edit quality at a fraction of time cost.
• Through extensive empirical analysis, we find that
CORAL outperforms recent state-of-the-art methods and is better equipped to handle complex text prompts. 2.