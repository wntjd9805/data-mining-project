Abstract
In this paper, we propose a novel transfer-based tar-geted attack method that optimizes the adversarial pertur-bations without any extra training efforts for auxiliary net-works on training data. Our new attack method is pro-posed based on the observation that highly universal ad-versarial perturbations tend to be more transferable for targeted attacks. Therefore, we propose to make the per-turbation to be agnostic to different local regions within one image, which we called as self-universality.
Instead of optimizing the perturbations on different images, opti-mizing on different regions to achieve self-universality can get rid of using extra data. Specifically, we introduce a feature similarity loss that encourages the learned pertur-bations to be universal by maximizing the feature similar-ity between adversarial perturbed global images and ran-domly cropped local regions. With the feature similarity loss, our method makes the features from adversarial per-turbations to be more dominant than that of benign im-ages, hence improving targeted transferability. We name the proposed attack method as Self-Universality (SU) attack.
Extensive experiments demonstrate that SU can achieve high success rates for transfer-based targeted attacks. On
ImageNet-compatible dataset, SU yields an improvement of 12% compared with existing state-of-the-art methods. Code is available at https://github.com/zhipeng-wei/Self-Universality. 1.

Introduction
It has been demonstrated in recent works that adversar-ial examples have the properties of transferability, which means an adversarial example generated on one white-box model can be used to fool other black-box models
[3, 15, 27, 30, 33]. The existence of transferability brings convenience to performing black-box attacks, hence raising security concerns for deploying deep models in real-world
†Corresponding author. applications [14, 22, 28, 35]. Consequently, considerable re-search attention has been spent on improving the transfer-ability of adversarial examples for both non-targeted and targeted attacks [4, 29, 36].
Compared to non-targeted attacks, transfer-based tar-geted attacks are inherently much more challenging since the goal is to fool deep models into predicting the specific target class. The major difficulty of transfer-based targeted attacks is caused by the fact that the gradient directions from a source image to a target class are usually different among different DNNs [16]. Hence, transfer-based attack methods designed for non-targeted attacks typically work poorly for targeted attacks. To increase the transferabil-ity, previous studies make efforts in aligning the feature of the generated adversarial example with the feature distribu-tions of the targeted class, which are learned from class-specific auxiliary networks [7, 8] or generative adversarial networks [18]. However, these works assume that the train-ing dataset is available and require extra training efforts for auxiliary networks, making it hard to apply in real-world scenarios.
This paper investigates the problem of transfer-based tar-geted attacks. Specifically, we propose a new method that improves the transferability of adversarial examples in a more efficient way, i.e., without any training efforts for auxiliary networks to learn the feature distributions of the targeted class. Our method is proposed based on the ob-servation that more universal perturbations yield better at-tack success rates in targeted attacks. To this end, our goal is to enhance the universality of the generated adver-sarial perturbations, in order to improve its targeted trans-ferability. Note that existing universal adversarial pertur-bation (UAP) attacks [17] require optimizing the pertur-bations on an abundant of images to achieve universality, which is not applicable in our setting. To get rid of us-ing extra data and make transfer-based targeted attacks as convenient as non-targeted attacks, we propose to make the perturbation to be agnostic to different local regions within one image, which we called as self-universality. Then our method optimizes the self-universality of adversarial pertur-Figure 1. Overview of the proposed SU attack. The random cropping is applied to the given benign image to generate the local image patch. After cropping, the local patch is resized to the shape of the benign image. Then both benign and local adversarial images with the shared perturbations are input to a surrogate white-box CNN model. Finally, the gradients obtained from the classification loss and the feature similarity loss are used to optimize perturbations. bations instead. To be specific, in addition to classification loss, our Self-Universality (SU) attack method introduces a feature similarity loss that maximizes the feature similarity between adversarial perturbed global images and randomly cropped local regions to achieve self-universality.
In this way, our method makes the features from adversarial per-turbations to be more dominant than that of benign images, hence improving targeted transferability.
Figure 1 gives an overview of the proposed Self-Universality (SU) attack. SU firstly applies random crop-ping on benign images to obtain local cropped patches.
Then it resizes local patches to the same size with benign images. Consequently, global and local inputs with shared perturbations are input to the white-box model. Finally, ad-versarial perturbations are updated by minimizing the clas-sification loss (e.g., Cross Entropy) between inputs and the target class and maximizing the feature similarity loss (e.g.,
Cosine Similarity) of adversarial intermediate features be-tween local and global inputs. Benefiting from satisfying the prediction of the target class between global and lo-cal inputs and approximating adversarial intermediate fea-tures between the two, the proposed SU attack can generate perturbations with self-universality, thereby improving the cross-model targeted transferability. We briefly summarize our primary contributions as follows:
• Through experiments, we find that highly universal ad-versarial perturbations tend to be more transferable for targeted attacks, which brings new insight into the de-sign of transfer-based targeted attack methods.
• Based on the finding, we propose a novel Self-Universality (SU) attack method that enhances the uni-versality of adversarial perturbations for better targeted transferability without the requirement for extra data.
• We conduct comprehensive experiments to demon-strate that the proposed SU attack can significantly im-prove the cross-model targeted transferability of adver-sarial images. Notably, SU can be easily combined with other existing methods. 2.