Abstract
Driven by improved architectures and better representa-tion learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt [33], have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learn-ing techniques such as masked autoencoders (MAE) [14].
However, we found that simply combining these two ap-proaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder frame-work and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural im-provement results in a new model family called ConvNeXt
V2, which significantly improves the performance of pure
ConvNets on various recognition benchmarks, including
ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on Im-ageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data. 1.

Introduction
Building on research breakthroughs in earlier decades
[16,25,28,37,44], the field of visual recognition has ushered in a new era of large-scale visual representation learning.
Pre-trained, large-scale vision models have become essen-tial tools for feature learning and enabling a wide range of vision applications. The performance of a visual represen-tation learning system is largely influenced by three main factors: the neural network architecture chosen, the method
* Work done during an internship at FAIR.
â€  Corresponding author.
Figure 1. ConvNeXt V2 model scaling. The ConvNeXt V2 model, which has been pre-trained using our fully convolutional masked autoencoder framework, performs significantly better than the previous version across a wide range of model sizes. used for training the network, and the data used for training.
In the field of visual recognition, progress in each of these areas contributes to overall improvements in performance.
Innovation in neural network architecture design has consistently played a major role in the field of represen-tation learning. Convolutional neural network architec-tures (ConvNets) [16, 25, 28] have had a significant im-pact on computer vision research by allowing for the use of generic feature learning methods for a variety of visual recognition tasks [10, 15], rather than relying on manual feature engineering. In recent years, the transformer archi-tecture [44], originally developed for natural language pro-cessing, has also gained popularity due to its strong scaling behavior with respect to model and dataset size [7]. More recently, ConvNeXt [33] architecture has modernized tradi-tional ConvNets and demonstrated that pure convolutional models could also be scalable architectures. However, the most common method for exploring the design space for neural network architectures is still through benchmarking supervised learning performance on ImageNet. 1
In a separate line of research, the focus of visual repre-sentation learning has been shifting from supervised learn-ing with labels to self-supervised pre-training with pre-text objectives. Among many different self-supervised al-gorithms, masked autoencoders (MAE) [14] have recently brought success in masked language modeling to the vision domain and quickly become a popular approach for visual representation learning. However, a common practice in self-supervised learning is to use a predetermined architec-ture designed for supervised learning, and assume the de-sign is fixed. For instance, MAE was developed using the vision transformer [7] architecture.
It is possible to combine the design elements of archi-tectures and self-supervised learning frameworks, but do-ing so may present challenges when using ConvNeXt with masked autoencoders. One issue is that MAE has a specific encode-decoder design that is optimized for the sequence processing capabilities of transformers, which allows the compute-heavy encoder to focus on visible patches and thus reduce the pre-training cost. This design may not be com-patible with standard ConvNets, which use dense sliding windows. Additionally, if the relationship between the ar-chitecture and the training objective is not taken into con-sideration, it may be unclear whether optimal performance can be achieved. In fact, previous research has shown that training ConvNets with mask-based self-supervised learn-ing can be difficult [24], and empirical evidence suggests that transformers and ConvNets may have different feature learning behaviors that can affect representation quality.
To this end, we propose to co-design the network archi-tecture and the masked autoencoder under the same frame-work, with the aim of making mask-based self-supervised learning effective for ConvNeXt models and achieving re-sults similar to those obtained using transformers.
In designing the masked autoencoder, we treat the masked input as a set of sparse patches and use sparse con-volutions [12] to process only the visible parts. The idea is inspired by the use of sparse convolutions in processing large-scale 3D point clouds [5, 52]. In practice, we can im-plement ConvNeXt with sparse convolutions, and at fine-tuning, the weights are converted back to standard, dense layers without requiring special handling. To further im-prove the pre-training efficiency, we replace the transformer decoder with a single ConvNeXt block, making the entire design fully convolutional. We have observed mixed results with these changes: the learned features are useful and im-prove upon the baseline results, but the fine-tuning perfor-mance is still not as good as the transformer-based model.
We then conduct a feature space analysis of different training configurations for ConvNeXt. We identify a poten-tial issue of feature collapse at the MLP layer when training
ConvNeXt directly on masked input. To address this issue, we propose adding a Global Response Normalization layer to enhance inter-channel feature competition. This change is most effective when the model is pre-trained with masked autoencoders, suggesting that reusing a fixed architecture design from supervised learning may be suboptimal.
In summary, we introduce ConvNeXt V2 which demon-strates improved performance when used in conjunction with masked autoencoders. We have found that this model significantly improves the performance of pure ConvNets across various downstream tasks, including ImageNet clas-sification [37], COCO object detection [30] and ADE20K segmentation [55]. The ConvNeXt V2 models can be used in a variety of compute regimes and includes models of varying complexity: from an efficient 3.7M-parameter Atto model that achieves 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that reaches a state-of-the-art 88.9% accuracy when using IN-22K labels. 2.