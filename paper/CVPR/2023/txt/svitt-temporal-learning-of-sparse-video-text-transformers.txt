Abstract
Do video-text transformers learn to model temporal re-lationships across frames? Despite their immense capacity and the abundance of multimodal training data, recent work has revealed the strong tendency of video-text models to-wards frame-based spatial representations, while temporal reasoning remains largely unsolved. In this work, we iden-tify several key challenges in temporal learning of video-text transformers: the spatiotemporal trade-off from limited network size; the curse of dimensionality for multi-frame modeling; and the diminishing returns of semantic informa-tion by extending clip length. Guided by these findings, we propose SViTT, a sparse video-text architecture that per-forms multi-frame reasoning with significantly lower cost than na¨ıve transformers with dense attention. Analogous to graph-based networks, SViTT employs two forms of sparsity: edge sparsity that limits the query-key commu-nications between tokens in self-attention, and node spar-sity that discards uninformative visual tokens. Trained with a curriculum which increases model sparsity with the clip length, SViTT outperforms dense transformer baselines on multiple video-text retrieval and question answering bench-marks, with a fraction of computational cost. Project page: http://svcl.ucsd.edu/projects/svitt. 1.

Introduction
With the rapid development of deep neural networks for computer vision and natural language processing, there has been growing interest in learning correspondences across the visual and text modalities. A variety of vision-language pretraining frameworks have been proposed [12, 22, 29, 34] for learning high-quality cross-modal representations with weak supervision. Recently, progress on visual transform-ers (ViT) [5,16,32] has enabled seamless integration of both modalities into a unified attention model, leading to image-text transformer architectures that achieve state-the-art per-formance on vision-language benchmarks [1, 27, 44].
Progress has also occurred in video-language pretraining by leveraging image-text models for improved frame-based
*Work done during an internship at Intel Labs.
Figure 1. We propose SViTT, a sparse video-text transformer for efficient modeling of temporal relationships across video frames.
Top: Semantic information for video-text reasoning is highly lo-calized in the spatiotemporal volume, making dense modeling in-efficient and prone to contextual noises. Bottom: SViTT pur-sues edge sparsity by limiting query-key pairs in self-attention, and node sparsity by pruning redundant tokens from visual sequence. reasoning [4, 9, 18]. Spatial modeling has the advantage of efficient (linear) scaling to long duration videos. Per-haps due to this, single-frame models have proven surpris-ingly effective at video-text tasks, matching or exceeding prior arts with complex temporal components [9, 24]. How-ever, spatial modeling creates a bias towards static appear-ance and overlooks the importance of temporal reasoning in videos. This suggests the question: Are temporal dynamics not worth modeling in the video-language domain?
Upon a closer investigation, we identify a few key chal-lenges to incorporating multi-frame reasoning in video-language models. First, limited model size implies a trade-off between spatial and temporal learning (a classic example being 2D/3D convolutions in video CNNs [46]). For any given dataset, optimal performance requires a careful bal-ance between the two. Second, long-term video models typ-ically have larger model sizes and are more prone to over-fitting. Hence, for longer term video models, it becomes more important to carefully allocate parameters and control model growth. Finally, even if extending the clip length im-proves the results, it is subject to diminishing returns since the amount of information provided by a video clip does not grow linearly with its sampling rate. If the model size is not controlled, the computational increase may not jus-tify the gains in accuracy. This is critical for transformer-based architectures, since self-attention mechanisms have a quadratic memory and time cost with respect to input length. In summary, model complexity should be adjusted adaptively, depending on the input videos, to achieve the best trade-off between spatial representation, temporal rep-resentation, overfitting potential, and complexity. Since ex-isting video-text models lack this ability, they either attain a suboptimal balance between spatial and temporal modeling, or do not learn meaningful temporal representations at all.
Motivated by these findings, we argue that video-text models should learn to allocate modeling resources to the video data. We hypothesize that, rather than uniformly ex-tending the model to longer clips, the allocation of these re-sources to the relevant spatiotemporal locations of the video is crucial for efficient learning from long clips. For trans-former models, this allocation is naturally performed by pruning redundant attention connections. We then propose to accomplish these goals by exploring transformer spar-sification techniques. This motivates the introduction of a
Sparse Video-Text Transformer (SViTT) inspired by graph models. As illustrated in Fig. 1, SViTT treats video to-kens as graph vertices, and self-attention patterns as edges that connect them. We design SViTT to pursue sparsity for both: edge sparsity aims at reducing query-key pairs in attention module while maintaining its global reasoning ca-pability; node sparsity reduces to identifying informative to-kens (e.g., corresponding to moving objects or person in the foreground) and pruning background feature embeddings.
To address the diminishing returns for longer input clips, we propose to train SViTT with temporal sparse expansion, a curriculum learning strategy that increases clip length and model sparsity, in sync, at each training stage.
SViTT is evaluated on diverse video-text benchmarks from video retrieval to question answering, comparing to prior arts and our own dense modeling baselines. First, we perform a series of ablation studies to understand the bene-fit of sparse modeling in transformers. Interestingly, we find that both nodes (tokens) and edges (attention) can be pruned drastically at inference, with a small impact on test perfor-mance. In fact, token selection using cross-modal attention improves retrieval results by 1% without re-training.
We next perform full pre-training with the sparse mod-els and evaluate their downstream performance. We observe that SViTT scales well to longer input clips where the accu-racy of dense transformers drops due to optimization diffi-culties. On all video-text benchmarks, SViTT reports com-parable or better performance than their dense counterparts with lower computational cost, outperforming prior arts in-cluding those trained with additional image-text corpora.
The key contributions of this work are: 1) a video-text architecture SViTT that unifies edge and node sparsity; 2) a sparse expansion curriculum for training SViTT on long video clips; and 3) empirical results that demonstrate its temporal modeling efficacy on video-language tasks. 2.