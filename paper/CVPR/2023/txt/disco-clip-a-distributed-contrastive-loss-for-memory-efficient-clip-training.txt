Abstract
We propose DisCo-CLIP, a distributed memory-efficient
CLIP training approach, to reduce the memory consump-tion of contrastive loss when training contrastive learning models. Our approach decomposes the contrastive loss and its gradient computation into two parts, one to cal-culate the intra-GPU gradients and the other to compute the inter-GPU gradients. According to our decomposition, only the intra-GPU gradients are computed on the cur-rent GPU, while the inter-GPU gradients are collected via all reduce from other GPUs instead of being repeatedly computed on every GPU. In this way, we can reduce the
GPU memory consumption of contrastive loss computation from
N ), where B and N are the batch size and the number of GPUs used for training. Such a dis-tributed solution is mathematically equivalent to the orig-inal non-distributed contrastive loss computation, without sacrificing any computation accuracy. It is particularly ef-ficient for large-batch CLIP training. For instance, DisCo-CLIP can enable contrastive training of a ViT-B/32 model with a batch size of 32K or 196K using 8 or 64 A100 40GB
GPUs, compared with the original CLIP solution which re-quires 128 A100 40GB GPUs to train a ViT-B/32 model with a batch size of 32K. (B2) to ( B2
O
O 1.

Introduction
Vision-language representation learning from massive image-text pairs has recently attracted tremendous atten-tion for its great potential in many applications such as zero-shot classification and text-image retrieval. Represen-tative works include CLIP [27], ALIGN [17], Florence [47],
CoCa [46], and BASIC [26], which all leverage hundreds of millions or even billions of image-text pairs collected from the Web to learn a semantic-rich and language-aligned visual representation [22]. As the web-collected data in-evitably contain noises, CLIP [27] for the first time applies
*Corresponding author. contrastive learning on 400M image-text pairs, which im-plies a weak but more proper assumption about the data: the relevance between paired image and text is greater than that between unpaired image and text. For its demonstrated performance in CLIP, contrastive learning has been widely adopted in subsequent works. Accordingly, several image-text data sets with increasingly larger scales have also been developed and made publicly available, such as Concep-tual 12M [3], YFCC 100M [38], WIT 37.6M [37], LAION-400M [35], and LAION-5B [34].
The goal of contrastive learning in CLIP is to learn an alignment between image and text via two encoders. That is, it encourages paired image and text (called a positive pair) to be similar and meanwhile enforces unpaired im-age and text (called a negative pair) to be dissimilar. For any positive image-text pair, as there are normally unlim-ited number (up to the total number of images or texts in a data set) of negative image-text pairs, it is crucial to in-clude a sufficiently large number of negative pairs in a con-trastive loss to make the representation learning effective, as validated in all related works such as CLIP [27], Flo-rence [47], OpenCLIP [16], and BASIC [26]. Specifically,
BASIC shows that larger batch size, plus larger data set and larger model, theoretically lead to a better generalization performance.
However, a fundamental technical challenge in training a CLIP-like model is how to enlarge its batch size under the constraint of limited GPU memory. For instance, when the batch size is 65,536, the similarity matrix for all image-text pairs in the batch will cost about 16GB using Float32.
As the backbone part also consumes a significant portion of GPU memory, especially for large backbones such as
ViT-Large or ViT-Huge [9], scaling up batch size presents a great challenge, usually requiring hundreds of V100 or
A100 GPUs [16, 27, 47], which are inaccessible for most research scientists.
In this work, we develop a distributed solution called
DisCo-CLIP for constrastive loss computation, which can save a large amount of memory for contrastive loss and make CLIP training more memory-efficient. Our method
starts from a decomposition of the original contrastive loss.
Based on this decomposition, we divide the contrastive loss into two parts, one to calculate the intra-GPU loss and gradients, and the other one to calculate the inter-GPU loss and gradients. For a mini-batch on the n-th
GPU (hereinafter called its hosting GPU), its intra-GPU gradients are calculated on its hosting GPU, and its inter-GPU gradients are collected from other GPUs. DisCo is an exact solution, mathematically equivalent to the origi-nal non-distributed contrastive loss, but more memory- and computation-efficient. It can decrease the memory cost of
N ), where B and N are contrastive loss from the batch size and the number of GPUs. When N equals to 64, it means around 97% (see Sec. 4.1 for details) of the memory, and similarly the computational cost, in con-trastive loss can be saved. Thus, using DisCo in CLIP, we can enable contrastive training with a larger batch size. Us-ing 8 Nvidia A100 40GB GPUs, DisCo-CLIP can enable contrastive training of a ViT-B/32 model with a batch size of 32,768. Using 64 A100 40GB GPUs, DisCo-CLIP can train the same model with a larger batch size of 196K. (B2) to ( B2
O
O
We summarize our contributions in twofold.
• We propose a novel distributed contrastive loss solu-tion called DisCo for memory efficient CLIP training, which can significantly reduce the memory consump-tion of the contrastive loss computation. Such a solu-tion enables a larger batch size for contrastive training using the same computing resource without sacrificing any computation accuracy.
• We further validate that training with a larger batch size can further improve the performance of con-trastive learning models. 2.