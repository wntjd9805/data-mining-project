Abstract
This paper focuses on analyzing and improving the com-monsense ability of recent popular vision-language (VL) models. Despite the great success, we observe that existing
VL-models still lack commonsense knowledge/reasoning ability (e.g., “Lemons are sour”), which is a vital com-ponent towards artificial general intelligence. Through our analysis, we find one important reason is that exist-ing large-scale VL datasets do not contain much common-sense knowledge, which motivates us to improve the com-monsense of VL-models from the data perspective. Rather than collecting a new VL training dataset, we propose a more scalable strategy, i.e., “Data Augmentation with kNowledge graph linearization for CommonsensE capabil-ity” (DANCE). It can be viewed as one type of data aug-mentation technique, which can inject commonsense knowl-edge into existing VL datasets on the fly during training.
More specifically, we leverage the commonsense knowl-edge graph (e.g., ConceptNet) and create variants of text description in VL datasets via bidirectional sub-graph se-quentialization. For better commonsense evaluation, we further propose the first retrieval-based commonsense di-agnostic benchmark. By conducting extensive experiments on some representative VL-models, we demonstrate that our DANCE technique is able to significantly improve the commonsense ability while maintaining the performance on vanilla retrieval tasks. The code and data are available at https:// github.com/ pleaseconnectwifi/ DANCE. 1.

Introduction
Many vision-based problems in our daily life go beyond perception and recognition. For example, when we hear people say “It tastes sour”, we need to identify they are talk-ing about lemons on the table instead of the chocolate cake.
Therefore, it is essential for artificial general intelligence to
*This work was partially supported by a GRF grant (Project No. CityU 11216122) from the Research Grants Council (RGC) of Hong Kong.
Figure 1.
Illustration of the commonsense lacking problem of various popular VL-models, including CLIP [45] pre-trained with contrastive supervision, ViLT [24] with matching supervision, and
BLIP [28] with the both. The bar plots suggest the alignment scores of the images to the text. All models fail in retrieving the correct image with lemon (in blue). develop commonsense capability. Vision-Language (VL) models [45] recently show promising signals on mimick-ing the core cognitive activities of humans by understand-ing the visual and textual information in the same latent space [74]. However, we observed that VL-models, e.g.,
CLIP [45], still struggle when minor commonsense knowl-edge is needed. For example, as shown in figure 1, none of the existing models correctly identify the lemon with text input “It tastes sour”.
In this work, we take a step towards injecting the VL-models with commonsense capability. More specifically, we find one important reason for the commonsense lacking issue is that existing large-scale VL datasets do not con-tain much commonsense knowledge. On the one hand, reg-ular VL data, e.g., COCO [32] and CC 12M [9] contain more nouns and descriptive adjectives, with much fewer verbs and particles compared to regular texts. This distribu-tion difference suggests that it might be infeasible for VL-models to gain commonsense ability by purely enlarging the dataset, unlike language-only models [22, 44]. Also, other objectives like visual question answering or generation are
not widely applicable for training and have limited data size.
Inspired by the aforementioned findings, we propose
Data Augmentation with kNowledge graph linearization for
CommonsensE capability (DANCE). The main idea is to generate commonsense-augmented image-text pairs. To do so, one natural idea is to leverage the rich commonsense knowledge in knowledge graphs [5, 56]. However, it is not trivial to inject the knowledge into image-text pairs. On the one hand, structured data like graphs usually require spe-cific architectures [59, 75] to embed, which is troublesome.
On the other hand, if we associate the external knowledge with the text in the training stage, we will need the exter-nal knowledge-augmentation process in the inference stage as well to avoid domain shift [51]. This is not desirable, since the corresponding knowledge is usually not available for the inference tasks. To address these challenges, we first re-organize the commonsense knowledge graph into entries with (entity, relation, entity) format, and pair them to the images that contain one of the entities. We then hide the name of entities in that image with demonstrative pronouns, e.g., “this item”. The generated descriptions are in tex-tual form and therefore readily applicable for the training of most VL-models. More importantly, by forcing the model to memorize the relationships between entities in the train-ing stage, such data augmentation is not needed in the infer-ence stage. The data pair generation pipeline is automatic and scalable, leveraging the existing consolidated common-sense knowledge base and the large and various collections of image-language supervision.
In addition, existing VL commonsense evaluations are restricted to visual question answering and generation which are not a good fit or well received in the majority of
VL-models. Therefore, we propose a new diagnostic test set in a wider adaptable form, i.e., Image-Text and Text-Image
Retrieval, to achieve a fair evaluation of the pre-trained VL-models. The set is upgraded by neighborhood hard-negative filtering to further ensure data quality.
The effectiveness of the DANCE is validated by not only our diagnostic test set, but also the most popular vi-sual question answering benchmark for commonsense [36].
Moreover, we show the commonsense ability of the models trained with DANCE even generalize to unseen knowledge.
We show the potential of the new train strategy and the test dataset by deep content analysis and baseline performance measurements across various cutting-edge VL-models. We summarize our main findings and contributions as follows: 1. We propose a novel commonsense-aware training strategy DANCE, which is compatible with the most of VL-models. The inference stage needs no change. 2. We propose a new retrieval-based well-received com-monsense benchmark to analyze a suite of VL-models and discover weaknesses that are not widely known: commonsense easy for humans (83%) is hard for cur-rent state-of-the-art VL-models (<42%). 3. We conduct extensive experiments to demonstrate the effectiveness of the proposed strategy and diagnostic test set. The datasets and all the code will be made publicly available. 2.