Abstract mpg.de/projects/MoFusion/.
Conventional methods for human motion synthesis have either been deterministic or have had to struggle with the trade-off between motion diversity vs motion quality. In re-sponse to these limitations, we introduce MoFusion, i.e., a new denoising-diffusion-based framework for high-quality conditional human motion synthesis that can synthesise long, temporally plausible, and semantically accurate mo-tions based on a range of conditioning contexts (such as mu-sic and text). We also present ways to introduce well-known kinematic losses for motion plausibility within the motion-diffusion framework through our scheduled weighting strat-egy. The learned latent space can be used for several inter-active motion-editing applications like in-betweening, seed-conditioning, and text-based editing, thus, providing cru-cial abilities for virtual-character animation and robotics.
Through comprehensive quantitative evaluations and a per-ceptual user study, we demonstrate the effectiveness of Mo-Fusion compared to the state of the art on established benchmarks in the literature. We urge the reader to watch our supplementary video at https://vcai.mpi-inf. 1.

Introduction 3D human motion synthesis is an important generative computer vision problem that often arises in robotics, vir-tual character animation and video games and movie pro-duction (e.g., for crowd dynamics simulation). It saw im-pressive progress over the last years; several works recently tackled it with reinforcement learning [41, 60, 64], deep generative models [2, 42, 43, 50] or using deterministic ap-proaches [12, 29, 35]. Despite the progress, multiple open challenges remain, such as improving motion variability, enabling higher motion realism and enhancing synthesis fi-delity under user-specified conditioning. Under condition-ing, we understand influencing the model outputs according to a control signal (e.g., “walking counter-clockwise”).
The key goal of conditional human motion synthesis is to generate motions that semantically agree with the con-ditioning while exhibiting diversity for the same condition-ing signal. To facilitate the same, the recent state-of-the-art approaches have widely adopted generative techniques like conditional variational auto-encoders (CVAE) [17, 32,
42, 43], normalizing flows [2, 3], as well as GANs [19, 30].
Naturally, each of them has strengths and limitations. GAN-based synthesis methods suffer from mode-collapse, thus resulting in insufficient fidelity of synthesis, especially for less common input conditioning. On the other hand, meth-ods using CVAEs and normalizing flows typically have to deal with the trade-off between synthesis quality and the richness of the latent space (i.e., diversity) [3, 50].
The seminal works of Sohl-Dickstein et al. [20] and
Ho et al. [21] recently demonstrated the ability of Denoising
Diffusion Probabilistic Models (DDPM) to learn the under-lying data distribution while also allowing for diverse sam-pling. Recent works [37, 49, 52] exhibited remarkable ca-pabilities in the conditional synthesis of images and audio with high-frequency details while also allowing interactive applications like editing and inpainting. However, it has remained unclear how DDPM could be trained for such a problem with the temporal component as human motion synthesis.
Motivated by the recent advances in diffusion models, we propose MoFusion, i.e., a new approach for human mo-tion synthesis with DDPM. This paper shows that diffu-sion models are highly effective for this task; see Fig. 1 for an overview. Our proposal includes a lightweight 1D
U-Net network for reverse diffusion to reduce the rather long inference times. Furthermore, we demonstrate how domain-inspired kinematic losses can be introduced to dif-fusion framework during training, thanks to our time-varying weight schedule, which is our primary contribution.
The result is a new versatile framework for human motion synthesis that produces diverse, temporally and kinemati-cally plausible, and semantically accurate results.
We analyse DDPM for motion synthesis on two rele-vant sub-tasks: music-conditioned choreography genera-tion and text-conditioned motion synthesis. While most existing choreography generation methods produce repeti-tive (loopy) motions, and text-to-motion synthesis methods struggle with left-right disambiguation, directional aware-ness and kinematic implausibility, we show that MoFu-sion barely suffers from these limitations. Finally, formu-lating motion synthesis in a diffusion framework also af-fords us the ability to perform interactive editing of the syn-thesised motion. To that end, we discuss the applications of a pre-trained MoFusion, like motion forecasting and in-betweening.(both are important applications for virtual character animation). We show improvements in both the sub-tasks through quantitative evaluations on AIST++ [29] and HumanML3D [15] datasets as well as a user study. In summary, our core technical contributions are as follows:
• The first method for conditional 3D human motion synthesis using denoising diffusion models. Thanks to the proposed time-varying weight schedule, we in-corporate several kinematic losses that make the syn-thesised outputs temporally plausible and semantically accurate with the conditioning signal.
• Model conditioning on various signals, i.e., music and text, which is reflected in our framework’s architec-ture. For a music-to-choreography generation, our re-sults generalise well to new music and do not suffer from degenerate repetitiveness. 2.