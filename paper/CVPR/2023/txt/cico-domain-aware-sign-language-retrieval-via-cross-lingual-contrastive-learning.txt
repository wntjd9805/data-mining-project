Abstract
This work focuses on sign language retrieval—a recently proposed task for sign language understanding. Sign lan-guage retrieval consists of two sub-tasks: text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval. Dif-ferent from traditional video-text retrieval, sign language videos, not only contain visual signals but also carry abun-dant semantic meanings by themselves due to the fact that sign languages are also natural languages. Considering this character, we formulate sign language retrieval as a cross-lingual retrieval problem as well as a video-text re-trieval task. Concretely, we take into account the linguistic properties of both sign languages and natural languages, and simultaneously identify the fine-grained cross-lingual (i.e., sign-to-word) mappings while contrasting the texts and the sign videos in a joint embedding space. This pro-cess is termed as cross-lingual contrastive learning. An-other challenge is raised by the data scarcity issue—sign language datasets are orders of magnitude smaller in scale than that of speech recognition. We alleviate this issue by adopting a domain-agnostic sign encoder pre-trained on large-scale sign videos into the target domain via pseudo-labeling. Our framework, termed as domain-aware sign language retrieval via Cross-lingual Contrastive learning or CiCo for short, outperforms the pioneering method by large margins on various datasets, e.g., +22.4 T2V and +28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1 improvements on
PHOENIX-2014T dataset. Code and models are available at: https://github.com/FangyunWei/SLRT. 1.

Introduction
Sign languages are the primary means of communication used by people who are deaf or hard of hearing. Sign lan-guage understanding [1, 10, 12–15, 18, 32, 33, 61, 73] is sig-nificant for overcoming the communication barrier between
*Corresponding author. (a) Text-to-sign-video (T2V) retrieval. (b) Sign-video-to-text (V2T) retrieval.
Figure 1. Illustration of: (a) T2V retrieval; (b) V2T retrieval. the hard-of-hearing and non-signers. Sign language recog-nition and translation (SLRT) has been extensively studied, with the goal of recognizing the arbitrary semantic mean-ings conveyed by sign languages. However, the lack of available data significantly limits the capability of SLRT.
In this paper, we focus on developing a framework for a recently proposed sign language retrieval task [18]. Un-like SLRT, sign language retrieval focuses on retrieving the meanings that signers express from a closed-set, which can significantly reduce error rates in realistic deployment.
Sign language retrieval is both similar to and distinct from the traditional video-text retrieval. On the one hand, like video-text retrieval, sign language retrieval is also com-posed of two sub-tasks, i.e., text-to-sign-video (T2V) re-trieval and sign-video-to-text (V2T) retrieval. Given a free-form written query and a large collection of sign language videos, the objective of T2V is to find the video that best matches the written query (Figure 1a). In contrast, the goal of V2T is to identify the most relevant text description given a query of sign language video (Figure 1b).
On the other hand, different from the video-text retrieval, sign languages, like most natural languages, have their own grammars and linguistic properties. Therefore, sign lan-guage videos not only contain visual signals, but also carry
grained gestures and actions; (4) Sign language videos typ-ically contain hundreds of frames. It is necessary to build efficient algorithms to lower the training cost and fit the long videos as well as the intermediate representations into lim-ited GPU memory.
In this work, we concentrate on resolving the challenges listed above:
• We consider the linguistic rules (e.g., word order) of both sign languages and natural languages. We formulate sign language retrieval as a cross-lingual retrieval task as well as a video-text retrieval problem. While contrasting the sign videos and the texts in a joint embedding space as achieved in most vision-language pre-training frame-works [5, 44, 57], we simultaneously identify the fine-grained cross-lingual (sign-to-word) mappings between two types of languages via our proposed cross-lingual contrastive learning as shown in Figure 2.
• Data scarcity typically brings in the over-fitting issue. To alleviate this issue, we adopt transfer learning and adapt a recently released domain-agnostic sign encoder [61] pre-trained on large-scale sign-videos to the target do-main. Although this encoder is capable of distinguish-ing the fine-grained signs, direct transferring may be sub-optimal due to the unavoidable domain gap between the pre-training dataset and sign language retrieval datasets.
To tackle this problem, we further fine-tune a domain-aware sign encoder on pseudo-labeled data from target datasets. The final sign encoder is composed of the well-optimized domain-aware sign encoder and the powerful domain-agnostic sign encoder.
• In order to effectively model long videos, we decouple our framework into two disjoint parts: (1) a sign en-coder which adopts a sliding window on sign-videos to pre-extract their vision features; (2) a cross-lingual con-trastive learning module which encodes the extracted vi-sion features and their corresponding texts in a joint em-bedding space.
Our framework, called domain-aware sign language re-trieval via Cross-lingual Contrastive learning or CiCo for short, outperforms the pioneer SPOT-ALIGN [18] by large margins on various datasets, achieving 56.6 (+22.4) T2V and 51.6 (+28.0) V2T R@1 accuracy (improvement) on
How2Sign [19] dataset, and 69.5 (+13.7) T2V and 70.2 (+17.1) V2T R@1 accuracy (improvement) on PHOENIX-2014T [8] dataset. With its simplicity and strong perfor-mance, we hope our approach can serve as a solid baseline for future research. 2.