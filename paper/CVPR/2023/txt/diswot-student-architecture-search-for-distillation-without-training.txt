Abstract
Knowledge distillation (KD) is an effective training strat-egy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large ar-chitecture difference across the teacher-student pairs lim-its the distillation gains.
In contrast to previous adap-tive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work ﬁrst empirically show that the optimal model un-der vanilla training cannot be the winner in distillation.
Secondly, we ﬁnd that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with ﬁnal distil-lation performances. Thus, we efﬁciently measure similar-ity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm
In this way, our student architec-without any training. ture search for Distillation WithOut Training (DisWOT) sig-niﬁcantly improves the performance of the model in the distillation stage with at least 180× training acceleration.
Additionally, we extend similarity metrics in DisWOT as new distillers and KD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201 demonstrate that our technique achieves state-of-the-art results on different search spaces. Our project and code are available at https://lilujunai.github.io/DisWOT-CVPR2023/. 1.

Introduction
Despite the remarkable achievements of Deep Neural
Networks (DNNs) in numerous visual recognition tasks [52, 64–68], they usually lead to heavy costs of memory, com-putation, and power at model inference due to their large numbers of parameters. To address this issue, Knowledge
Distillation (KD) has been proposed as a means of trans-ferring knowledge from a high-capacity teacher model to a low-capacity target student model, providing a more op-*Corresponding author, † equal contribution, PD conducted main ex-periments, LL proposed ideas and led the project & writing.
Figure 1. Left: Ranking correlation of proxies in zero-cost NAS with vanilla and distillation accuracy. Right: Vanilla accuracy, dis-tillation accuracy, prediction scores of DisWOT for ResNet[7,1,3] and ResNet[3,3,3] on search space S0.
Figure 2. Left: KD [22], DisWOT, DisWOT† results for ResNet20 under different teachers. Right: Comparison of distill accuracy & training efﬁciency with other NAS methods on NAS-Bench-201. timal accuracy-efﬁciency trade-off during runtime [4, 7, 76].
The original KD method [22] utilizes the logit outputs of the teacher network as the source of knowledge. Subsequent studies [2, 21, 24, 26, 54, 61, 73] have focused on extract-ing informative knowledge based on intermediate feature representations. However, as the gap in capacity between students and teachers increases, existing KD methods are unable to improve results, particularly in tasks that depend on large-scale visual models such as VIT and GPT-3 [5, 16].
For example, as shown in Figure 2 (Left), the large teacher (e.g., ResNet110) lead to worse performance for the ﬁxed student than the relatively smaller one (e.g., ResNet56).
To solve this issue, adaptive KD methods have been pro-posed in terms of training paradigms (e.g., early stop [10]) and architectural adaptations (e.g., assistant teacher [43] and architecture search [40]), respectively. However, they are ineffective in improving distillation performance or in-volve enormous training costs in the additional model train-ing and search process. In sharp contrast to these methods, we tackle this challenging problem from a new perspective regarding training-free architecture search. To achieve this goal, we construct a search space S0 for ResNet-like mod-els with different depth conﬁgurations and obtain vanilla and distill performance for each candidate in S0 by indi-vidual training. Then, we evaluate the ranking correlation between predicted scores of training-free search methods and the actual performance of each student model. Surpris-ingly, as shown in Figure 1 (Left), there are common rank-ing correlation loss (10% ↓∼ 20% ↓) for these methods in predicting distillation accuracy than vanilla accuracy. To clarify this, we carefully analyze the disparities in vanilla and distillation performance for each model: (1) for overall search space, vanilla accuracy only preserves 85% correla-tions with actual distillation performance. (2) for a particu-lar instance, as shown in Figure 1 (Right), ResNet20 with 3 res-blocks in each stage (i.e., ResNet[3,3,3]) has more pa-rameters and better standalone performance but is weaker than ResNet[7,1,3] in the distillation process. Considering that ResNet[7,1,3] has more layers than ResNet20, we seek to understand the above phenomenon regarding the vanilla-distillation accuracy gap from the perspective of semantic matching [37]. ResNet[7,1,3] enjoys a larger effective re-ceptive ﬁeld and more excellent matched knowledge with teacher, resulting in signiﬁcant distillation gains. Encour-aged by this understanding, we strive to design a new zero-proxy regarding the semantic matching of teacher-student.
As a result, we ﬁnd that the similarity scores of feature se-mantics and sample relations can outperform conventional zero-cost NAS in predicting ﬁnal distillation accuracy (see the comparison of ranking correlation on search space S0 in
Table 8). As shown in Figure 1(Right), similarity scores are also consistent with distillation performance.
Drawing on the aforementioned observations, we intro-duce DisWOT, a simple yet effective training-free frame-work that ﬁnds the best student architectures for distilling the given teacher model. For better semantic matching in distillation, DisWOT leverages novel zero-cost metrics re-garding the feature semantics and sample relations to select better student model. For the feature semantic similarity metric, we remark that randomly initialized models can lo-calize objects well [6] and generate localization heatmaps via Grad-CAM [56] as reliable semantic information. Then, we measure the channel-wise similarity matrix of localiza-tion heatmaps and take the L2 distance of the similarity matrix for the teacher-student model as the metric. For in-put samples, different models have diverse abilities to dis-criminate their relationships. To improve relational knowl-edge matching ability, we use the L2 distance of sample-relation correlation matrix as a relation similarity metric.
Finally, we search for student architectures using an evo-lutionary algorithm with semantic and relations similarity metrics. Then, the distillation process is implemented be-tween the searched student and the pre-deﬁned teacher. In addition, we leverage these metrics directly as new distillers to enhance the student, as the DisWOT†. Equipped with our train-free search and distillation design, our DisWOT and DisWOT† framework signiﬁcantly improve the model’s accuracy-latency tradeoff in inference with at least 180× training acceleration.
In principle, our DisWOT use higher-order statistics of teacher-student models to optimize the student architecture to ﬁt a given teacher model. Its merits can be highlighted in three aspects: (1) In contrast to training-based student ar-chitecture search requires the individual or weight-sharing training, our DisWOT does not require the training of stu-dent models in the search phase. In addition, DisWOT is ef-ﬁcient to compute and easy to implement as it uses only the mini-batch data at initialization. (2) DisWOT is a teacher-aware search for distillation, which has better predictive dis-till accuracy than conventional NAS. (3) DisWOT exploits the distance of higher-order knowledge between the neural networks, bridging knowledge distillation and zero-proxy
NAS. We further demonstrate the competitive ranking corre-lation of DisWOT among 10 knowledge distances in KD as zero-proxy for predicting vanilla accuracy in NAS-Bench-201. We anticipate that our work on KD-based zero-proxy can offer some assistance in furthering research endeavors related to KD and NAS.
We conduct extensive experiments on CIFAR-100, Im-ageNet, and the NAS-Bench-201 [14] dataset, demonstrat-ing the superiority of our proposed approach. In contrast to experiments in traditional architectural search, we focus on ﬁnal distillation accuracy instead of the vanilla accuracy for the student. The results show that our DisWOT can achieve better accuracy than traditional Zero-shot NAS in the same search space. Besides, by switching to a larger space, our DisWOT can obtain new state-of-the-art architec-tures. For example, in the same ResNet-like search space, we signiﬁcantly improved 1.62% Top-1 accuracy over KD for ResNet50-ResNet18 pair under the same training set-tings. We also conducted comprehensive ablation studies to investigate how our method can use the predictability of zero-cost metrics to boost the distillation performance.
Main Contributions:
• By analyzing and exploring the discrepancy between teacher-student capability, we empirically show that their semantic similarities have a stronger correlation with the ﬁnal distillation accuracy. This motivates us to propose a new student architecture search for the
Distillation without Training (DisWOT) framework to reduce the teacher-student capability gap, which, to the best of our knowledge, is not achieved in the area of knowledge distillation.
• DisWOT proposes novel zero-cost metrics on similar-ity of feature semantics and sample relations and en-semble these metrics to select the optimal student via an evolutionary algorithm at the initial time. In the dis-tillation stage, DisWOT achieves state-of-the-art per-formances in multiple datasets and search spaces.
• We further expand 10 kinds of knowledge distances including DisWOT as new universal KD-based zero proxies, which enjoy competitive predictive power with actual performance of models. We hope that our contributions in this endeavor may aid to some degree in advancing future research on KD and NAS. 2.