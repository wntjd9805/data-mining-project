Abstract
Vision Transformers (ViTs), which made a splash in the field of computer vision (CV), have shaken the dominance of convolutional neural networks (CNNs). However, in the process of industrializing ViTs, backdoor attacks have brought severe challenges to security. The success of ViTs benefits from the self-attention mechanism. However, com-pared with CNNs, we find that this mechanism of capturing global information within patches makes ViTs more sensi-tive to patch-wise triggers. Under such observations, we delicately design a novel backdoor attack framework for
ViTs, dubbed BadViT, which utilizes a universal patch-wise trigger to catch the model’s attention from patches bene-ficial for classification to those with triggers, thereby ma-nipulating the mechanism on which ViTs survive to confuse itself. Furthermore, we propose invisible variants of BadViT to increase the stealth of the attack by limiting the strength of the trigger perturbation. Through a large number of ex-periments, it is proved that BadViT is an efficient backdoor attack method against ViTs, which is less dependent on the number of poisons, with satisfactory convergence, and is transferable for downstream tasks. Furthermore, the risks inside of ViTs to backdoor attacks are also explored from the perspective of existing advanced defense schemes. 1.

Introduction
Transformers, which are all-powerful in the field of nat-ural language processing (NLP) [6, 13, 57], have recently set off a wave of frenetic research in computer vision (CV).
Thanks to the self-attention mechanism, vision transform-ers (ViTs) have broken the perennial domination of convo-lutional neural networks (CNNs) [17], and have been de-*Corresponding author. veloped in hot areas like image classification [36, 53, 66], object detection [3, 7] and semantic segmentation [46, 64].
The architecture optimization researches of ViTs are also continuously improving the performance and efficiency
[26, 40, 45, 53], and providing vitality for advancing the de-ployment of ViTs in industry.
Unfortunately, manifold threats in deep learning pose se-vere challenges to ViTs. For instance, adversarial attacks
[10, 11, 25, 28, 31, 32, 49, 50, 76] confuse the deep model to make wrong predictions by adding subtle perturbations to the input. In addition, backdoor attacks are also extremely threatening to deep models [22, 27, 48, 51, 68]. More and more deep learning tasks are “outsourced” training or di-rectly fine-tuning on pre-trained models [22, 75], allowing attackers to implant backdoors into the model by establish-ing a strong association between the trigger and the attack
In response, a growing number of researchers behavior. have paid attention to the security of ViTs under adversarial attacks [1,4,20,39,44] and backdoor attacks [16,37,47,73].
However, previous ViT backdoor works have not system-atically compared with CNN to elucidate the vulnerability source of ViTs to backdoor attacks, and have not consid-ered balancing attack concealment and attack benefit, so that triggers can be easily detected by the naked eye.
To fill this gap, we systemically discuss the robustness of
ViTs and CNNs under basic backdoor attacks with differ-ent trigger settings and find that ViTs seem to be more vul-nerable to patch-wise triggers rather than image-blending triggers. Delving into the essence of ViTs, images are di-vided into patches as tokens to calculate attentions, which can capture more interaction information between patches at the global level than CNNs [44]. Thus the patch-wise perturbation has been shown to sufficiently affect the self-attention mechanism of ViTs [20] and make ViTs weaker learners than CNNs. Inspired by this, a natural and interest-ing question is whether backdoor attacks with the patch-wise trigger are resultful in ViTs. Accordingly, we pro-pose BadViT, a well-designed backdoor attack framework against ViTs. Through the optimization process, the univer-sal adversarial patch is generated as a trigger, which can be better caught by the model through the self-attention mech-anism of ViTs to tighten the connection between the trig-ger and the target class. To achieve invisible attacks, we limit the perturbation strength of the adversarial patch-wise trigger and adopt the blending strategy [12] instead of past-ing. Moreover, we adopt a ViTs backdoor defense Patch-Drop [16], and two state-of-the-art defenses in CNNs, Neu-ral Cleanse [59] and FinePruning [33], to explore the vul-nerability of ViTs against our BadViT.
Our main contributions are as follows:
• We explore the robustness of ViTs compared with
CNNs against backdoor attacks with different triggers.
• We propose our BadViT as well as its invisible version and verify the validity through abundant experiments.
• We show the effect of BadViTs under three advanced defense methods, and further discuss the characteris-tics of ViTs under backdoor attacks through patch pro-cessing, reverse engineering, and pruning.
We believe this paper will offer readers a new under-standing of the robustness of ViTs against backdoor attacks, and provide constructive insights into the follow-up ViTs system optimization and backdoor defense efforts. 2.