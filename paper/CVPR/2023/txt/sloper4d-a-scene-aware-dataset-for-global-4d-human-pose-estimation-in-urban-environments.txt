Abstract
We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the re-search of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects’ activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global transla-tions are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, result-ing in plausible and scene-natural 3D human poses. Even-*Corresponding author. tually, SLOPER4D consists of 15 sequences of human mo-tions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 m2 (up to 30,000 m2), including more than 100k
LiDAR frames, 300k video frames, and 500k IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban envi-ronments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant chal-lenges to existing methods and produces great research op-portunities. The dataset and code are released at http:
//www.lidarhumanmotion.net/sloper4d/. 1.

Introduction
Urban-level human motion capture is attracting more and more attention, which targets acquiring consecutive fine-grained human pose representations, such as 3D skeletons
and parametric mesh models, with accurate global locations in the physical world. It is essential for human action recog-nition, social-behavioral analysis, and scene perception and further benefits many downstream applications, including
Augmented/Virtual Reality, simulation, autonomous driv-ing, smart city, sociology, etc. However, capturing extra large-scale dynamic scenes and annotating detailed 3D rep-resentations for humans with diverse poses is not trivial.
Over the past decades, a large number of datasets and benchmarks have been proposed and have greatly promoted the research in 3D human pose estimation (HPE). They can be divided into two main categories according to the cap-ture environment. The first class usually leverages marker-based systems [16, 33, 45], cameras [14, 59, 60], or RGB-D sensors [13,64] to capture human local poses in constrained environments. However, the optical system is sensitive to light and lacks depth information, making it unstable in out-door scenes and difficult to provide global translations, and the RGB-D sensor has limited range and could not work outdoors. The second class [39, 49] attempts to take advan-tage of body-mounted IMUs to capture occlusion-free 3D poses in free environments. However, IMUs suffer from severe drift for long-term capturing, resulting in misalign-ments with the human body. Then, some methods exploit additional sensors, such as RGB camera [17], RGB-D cam-era [46, 57, 67], or LiDAR [27] to alleviate the problem and make obvious improvement. However, they all focus on
HPE without considering the scene constraints, which are limited in reconstructing human-scene integrated digital ur-ban and human-scene natural interactions.
To capture human pose and related static scenes si-multaneously, some studies use wearable IMUs and body-mounted camera [12] or LiDAR [5] to register the human in large real scenarios and they are promising for captur-ing human-involved real-world scenes. However, human pose and scene are decoupled in these works due to the ego view, where auxiliary visual sensors are used for collecting the scene data while IMUs are utilized for obtaining the 3D pose. Different from them, we propose a novel setting for human-scene capture with wearable IMUs and global-view
LiDAR and camera, which can provide multi-modal data for more accurate 3D HPE.
In this paper, we propose a huge scene-aware dataset for sequential human pose estimation in urban environments, named SLOPER4D. To our knowledge, it is the first urban-level 3D HPE dataset with multi-modal capture data, in-cluding calibrated and synchronized IMU measurements,
LiDAR point clouds, and images for each subject. More-over, the dataset provides rich annotations, including 3D poses, SMPL [32] models and locations in the world co-ordinate system, 2D poses and bounding boxes in the im-age coordinate system, and reconstructed 3D scene mesh.
In particular, we propose a joint optimization method for obtaining accurate and natural human motion representa-tions by utilizing multi-sensor complementation and scene constraints, which also benefit global localization and cam-era calibration in the dynamic acquisition process. Fur-thermore, SLOPER4D consists of over 15 sequences in 10 scenes, including library, commercial street, coastal run-way, football field, landscape garden, etc., with up to 30k m2 area size and 200 ∼ 1, 000m trajectory length for each sequence. By providing multi-modal capture data and di-verse human-scene-related annotations, SLOPER4D opens a new door to benchmark urban-level HPE.
We conduct extensive experiments to show the superior-ity of our joint optimization approach for acquiring high-quality 3D pose annotations. Additionally, based on our proposed new dataset, we benchmark two critical tasks: camera-based 3D HPE and LiDAR-based 3D HPE, as well as provide benchmarks for GHPE.
Our contributions are summarized as follows:
• We propose the first large-scale urban-level human pose dataset with multi-modal capture data and rich human-scene annotations.
• We propose an effective joint optimization method for acquiring accurate human motions in both local and global by integrating LiDAR SLAM results, IMU poses, and scene constraints.
• We benchmark two HPE tasks as well as a GHPE task on SLOPER4D, demonstrating its potential of promot-ing urban-level 3D HPE research. 2.