Abstract
Video Frame Interpolation (VFI) aims to generate in-termediate video frames between consecutive input frames.
Since the event cameras are bio-inspired sensors that only encode brightness changes with a micro-second temporal resolution, several works utilized the event camera to en-hance the performance of VFI. However, existing methods estimate bidirectional inter-frame motion fields with only events or approximations, which can not consider the com-plex motion in real-world scenarios.
In this paper, we propose a novel event-based VFI framework with cross-modal asymmetric bidirectional motion field estimation. In detail, our EIF-BiOFNet utilizes each valuable charac-teristic of the events and images for direct estimation of inter-frame motion fields without any approximation meth-ods. Moreover, we develop an interactive attention-based frame synthesis network to efficiently leverage the comple-mentary warping-based and synthesis-based features. Fi-nally, we build a large-scale event-based VFI dataset, ERF-X170FPS, with a high frame rate, extreme motion, and dynamic textures to overcome the limitations of previous event-based VFI datasets. Extensive experimental results validate that our method shows significant performance im-provement over the state-of-the-art VFI methods on vari-ous datasets. Our project pages are available at: https:
//github.com/intelpro/CBMNet 1.

Introduction
Video frame interpolation (VFI) is a long-standing prob-lem in computer vision that aims to increase the temporal resolution of videos. It is widely applied to various fields ranging from SLAM, object tracking, novel view synthe-sis, frame rate up-conversion, and video enhancement. Due to its practical usage, many researchers are engaged in en-hancing the performance of video frame interpolation. Re-cently, numerous deep learning-based VFI methods have been proposed and recorded remarkable performance im-provements in various VFI datasets. Specifically, numerous
Figure 1. Qualitative comparison on the warped frame of inter-frame motion fields. (b) and (c) estimate symmetrical inter-frame motion fields. (d) and (e) estimate asymmetric motion fields using only images and events, respectively. (f) Ours shows the best re-sults using cross-modal asymmetric bidirectional motion fields. motion-based VFI methods [3, 4, 8, 12, 22, 29, 30] are pro-posed thanks to the recent advance in motion estimation al-gorithms [13, 14, 16, 23, 39, 41]. For the inter-frame motion field estimation, the previous works [3, 12, 29] estimate the optical flows between consecutive frames and approximate intermediate motion fields [12, 29, 49] using linear [12, 29] or quadratic [49] approximation assumptions. These meth-ods often estimate the inaccurate inter-frame motion fields when the motions between frames are vast or non-linear, adversely affecting the VFI performance.
Event cameras, novel bio-inspired sensors, can cap-ture blind motions between frames with high temporal res-olutions since they asynchronously report the brightness changes of the pixels at the micro-second level. There-fore, recent event-based VFI methods [9, 19, 42, 43, 47, 51] have tried to leverage the advantages of the events to esti-mate the inter-frame motion field. However, they utilized only the stream of events to estimate motion fields [9,43] or used approximation methods [42, 47, 51], resulting in sub-optimal motion field estimation. The first reason is that the nature of events is sparse and noisy, and all the brightness changes are not recorded in the stream, leading to inaccu-rate results. Second, the approximation-based methods can not completely express the motion model of complex real-world motion fields. Lastly, the previous works do not fully take advantage of the modality of images providing dense visual information; the previous works do not consider a remarkable distinction between the two modalities, such as simply concatenating two modality features [42].
To solve the problem, we propose a novel EIF-BiOFNet(Event-Image Fusion Bidirectional Optical Flow
Network) for directly estimating asymmetrical inter-frame motion fields, elaborately considering the characteristics of the events and frames without the approximation meth-ods. Our EIF-BiOFNet leverage the following properties of events and images in the inter-frame motion field estima-tion. Since the events are triggered near the object edges and provide abundant motion trajectory information, the event-level motion fields tend to be accurately estimated near the motion boundaries. However, it can not give precise results in other areas where the events are not triggered. On the other hand, the image-based motion fields can utilize dense visual information, but they can not employ the trajectory of motion, unlike the events. The proposed EIF-BiOFNet elaborately supplements image-level and event-level mo-tion fields to estimate more accurate results. As a result, the
EIF-BiOFNet significantly outperforms previous methods, as shown in Fig. 1.
For the intermediate frame synthesis, recent event-based
VFI methods [9, 42, 43, 47] are built on CNNs. However, most CNN-based frame synthesis methods have a weak-ness in long-range pixel correlation due to the limited re-ceptive field size. To this end, we propose a new Inter-active Attention-based frame synthesis network to leverage the complementary warping- and synthesis-based features.
Lastly, we propose a novel large-scale ERF-X170FPS dataset with an elaborated beam-splitter setup for event-based VFI research community. Existing event-based VFI datasets have several limitations, such as not publicly avail-able train split, low frame rate, and static camera move-ments. Our dataset contains a higher frame rate (170fps), higher resolution (1440Ã—975), and more diverse scenes compared to event-based VFI datasets [42] where both train and test splits are publicly available.
In summary, our contributions are four fold: (I) We pro-pose a novel EIF-BiOFNet for estimating the asymmetric inter-frame motion fields by elaborately utilizing the cross-modality information. (II) We propose a new Interactive
Attention-based frame synthesis network that efficiently leverages the complementary warping- and synthesis-based features. (III) We propose a novel large-scale event-based
VFI dataset named ERF-X170FPS. (IV) With these whole setups, we experimentally validate that our method signif-icantly outperforms the SoTA VFI methods on five bench-mark datasets, including the ERF-X170FPS dataset. Specif-ically, we recorded unprecedented performance improve-ment compared to SoTA event-based VFI method [43] by 7.9dB (PSNR) in the proposed dataset. 2.