Abstract
We consider the generic problem of detecting low-level structures in images, which includes segmenting the ma-nipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit
Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embed-ding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individ-ual image, i.e., the features from frozen patch embeddings and the input’s high-frequency components. The proposed
EVP significantly outperforms other parameter-efficient tun-ing protocols under the same amount of tunable parame-ters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github. com/NiFangBaAGe/Explicit-Visual-Prompt. 1.

Introduction
Advances in image editing and manipulation algorithms have made it easy to create photo-realistic but fake pic-tures [31, 39, 63]. Detecting such manipulated regions be-comes an important problem due to its potential negative impact related to surveillance and crime [31]. Low-level structures are known to be beneficial to tampered region de-tection, i.e., resizing and copy-pasting will destroy the JPEG compression levels between the temper region and the host image [28, 50, 62], the noise level of the tempered region and the background is also different [76, 87]. Interesting, to segment the blurred pixels [67], shadowed regions [59], and concealed objects [15], low-level clues also play important
*Corresponding Author
Figure 1. We propose a unified method for four low-level structure segmentation tasks: camouflaged object, forgery, shadow and defo-cus blur detection (Top). Our approach relies on a pre-trained frozen transformer backbone that leverages explicit extracted features, e.g., the frozen embedded features and high-frequency components, to prompt knowledge. roles. These detection tasks are shown to be beneficial to numerous computer vision tasks, including auto-refocus [1], image retargeting [37], object tracking [55], etc.
Although all these tasks belong to low-level structure segmentation, they are typically addressed by domain-specific solutions with carefully designed network archi-tectures [8,87,90]. Moreover, the lack of large-scale datasets is often considered a major factor, which limits the perfor-mances [31].
In this work, we propose a solution to address the four tasks in a unified fashion. We take inspiration from recent
advances of prompting [2, 4, 33], which is a concept that ini-tially emerged in natural language processing (NLP) [3]. The basic idea is to efficiently adapt a frozen large foundation model to many downstream tasks with the minimum extra trainable parameters. As the foundation model has already been trained on a large-scale dataset, prompting often leads to better model generalization on the downstream tasks [3], especially in the case of the limited annotated data. Prompt-ing also significantly saves the storage of models since it only needs to save a shared basic model and task-aware promptings.
Our main insight is to tune the task-specific knowledge only from the features of each individual image itself be-cause the pre-trained base model contains sufficient knowl-edge for semantic understanding. This is also inspired by the effectiveness of hand-crafted image features, such as
SIFT [29], JPEG noise [50], resampling artifacts [62] in these tasks [28, 29, 46, 50, 62, 87].
Based on this observation, we propose explicit visual prompting (EVP), where the tuning performance can be hugely improved via the re-modulation of image features.
Specifically, we consider two kinds of features for our task.
The first is the features from the frozen patch embedding, which is critical since we need to shift the distribution of the original model. Another is high-frequency components of the input image since the pre-trained visual recognition model is learned to be invariant to these features via data augmen-tation. As shown in Figure 1, we take a model pre-trained on a large-scale dataset and freeze its parameters. Then, to adapt to each task, we tune the embedded features and learn an extra embedding for high-frequency components of each individual image.
In terms of experiments, we validate our approach on nine datasets of four tasks: forgery detection, shadow detec-tion, defocus blur detection as well as camouflaged object detection. Our simple and unified network achieves very competitive performance with the whole model fine-tuning and outperforms task-specific solutions without modifica-tion.
In summary, our main contributions are as follows:
• We design a unified approach that produces state-of-the-art performances for a number of tasks, including forgery detection, defocus blur detection, shadow de-tection, and camouflaged object detection.
• We propose explicit visual prompting (EVP), which takes the features from the frozen patch embedding and the input’s high-frequency components as prompting. It is demonstrated to be effective across different tasks and outperforms other parameter-efficient tuning methods.
• Our method greatly simplifies the low-level structure segmentation models as well as achieves comparable performance with well-designed SOTA methods. 2.