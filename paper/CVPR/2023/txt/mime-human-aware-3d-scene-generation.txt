Abstract 1.

Introduction
Generating realistic 3D worlds occupied by moving hu-mans has many applications in games, architecture, and synthetic data creation. But generating such scenes is ex-pensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effec-tively turning human movement into a “scanner” of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME pro-duces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data are available for research at https://mime.is.tue.mpg.de.
*This work was performed when C.P. H. was at the MPI-IS.
Humans constantly interact with their environment. They walk through a room, touch objects, rest on a chair, or sleep in a bed. All these interactions contain information about the scene layout and object placement. In fact, a mime is a performer who uses our understanding of such interactions to convey a rich, imaginary, 3D world using only their body motion. Can we train a computer to take human motion and, similarly, conjure the 3D scene in which it belongs?
Such a method would have many applications in synthetic data generation, architecture, games, and virtual reality. For example, there exist large datasets of 3D human motion like AMASS [38] and such data rarely contains information about the 3D scene in which it was captured. Could we take AMASS and generate plausible 3D scenes for all the motions? If so, we could use AMASS to generate training data containing realistic human-scene interaction.
To answer such questions, we train a new method called
MIME (Mining Interaction and Movement to infer 3D Envi-ronments) that generates plausible indoor 3D scenes based on 3D human motion. Why is this possible? The key in-tuitions are that (1) A human’s motion through free space indicates the lack of objects, effectively carving out regions of the scene that are free of furniture. And (2), when they are in contact with the scene, this constrains both the type and placement of 3D objects; e.g., a sitting human must be sitting on something, such as a chair, a sofa, a bed, etc.
To make these intuitions concrete, we develop MIME,
which is a transformer-based auto-regressive 3D scene gen-eration method that, given an empty floor plan and a human motion sequence, predicts the furniture that is in contact with the human. It also predicts plausible objects that have no contact with the human but that fit with the other objects and respect the free-space constraints induced by the human motion. To condition the 3D scene generation with human motion, we estimate possible contact poses using POSA [23] and divide the motion in contact and non-contact snippets (Fig. 2). The non-contact poses define free-space in the room, which we encode as 2D floor maps, by projecting the foot vertices onto the ground plane. The contact poses and cor-responding 3D human body models are represented by 3D bounding boxes of the contact vertices predicted by POSA.
We use this information as input to the transformer and auto-regressively predict the objects that fulfill the contact and free-space constraints; see Fig. 1.
To train MIME, we built a new dataset called 3D-FRONT
HUMAN that extends the large-scale synthetic scene dataset 3D-FRONT [18]. Specifically, we automatically populate the 3D scenes with humans; i.e., non-contact humans (a sequence of walking motion and standing humans) as well as contact humans (sitting, touching, and lying humans). To this end, we leverage motion sequences from AMASS [38], as well as static contact poses from RenderPeople [47] scans.
At inference time, MIME generates a plausible 3D scene layout for the input motion, represented as 3D bounding boxes. Based on this layout, we select 3D models from the 3D-FUTURE dataset [19] and refine their 3D placement based on geometric constraints between the human poses and the scene.
In comparison to pure 3D scene generation approaches like ATISS [46], our method generates a 3D scene that sup-ports human contact and motion while putting plausible ob-jects in free space. In contrast to Pose2Room [43] which is a recent pose-conditioned generative model, our method en-ables the generation of objects that are not in contact with the human, thus, predicting the entire scene instead of isolated objects. We demonstrate that our method can directly be applied to real captured motion sequences such as PROX-D
[22] without finetuning.
In summary, we make the following contributions:
• a novel motion-conditioned generative model for 3D room scenes that auto-regressively generates objects that are in contact with the human and do not occupy free-space defined by the motion.
• a new 3D scene dataset with interacting humans and free space humans which is constructed by populating 3D FRONT with static contact/standing poses from
RenderPeople and motion data of AMASS.
Figure 2. We divide input humans into two parts: contact humans and free-space humans. We extract the 3D bounding boxes for each contact human, and use non-maximum suppression on the 3D IoU to aggregate multiple humans in the same 3D space into a single contact 3D bounding box (orange boxes). We project the foot vertices of free-space humans on the floor plane, to get the 2D free-space mask (dark blue). 2.