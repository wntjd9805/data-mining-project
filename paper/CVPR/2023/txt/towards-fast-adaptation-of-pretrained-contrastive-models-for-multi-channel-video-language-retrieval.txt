Abstract
Multi-channel video-language retrieval require models to understand information from different channels (e.g. video+question, video+speech) to correctly link a video with a textual response or query. Fortunately, contrastive multimodal models are shown to be highly effective at align-ing entities in images/videos and text, e.g., CLIP [20]; text contrastive models are extensively studied recently for their strong ability of producing discriminative sentence embed-dings, e.g., SimCSE [5]. However, there is not a clear way to quickly adapt these two lines to multi-channel video-language retrieval with limited data and resources. In this paper, we identify a principled model design space with two axes: how to represent videos and how to fuse video and text information. Based on categorization of recent methods, we investigate the options of representing videos using contin-uous feature vectors or discrete text tokens; for the fusion method, we explore the use of a multimodal transformer or a pretrained contrastive text model. We extensively evalu-ate the four combinations on five video-language datasets.
We surprisingly find that discrete text tokens coupled with a pretrained contrastive text model yields the best perfor-mance, which can even outperform state-of-the-art on the iVQA and How2QA datasets without additional training on millions of video-text data. Further analysis shows that this is because representing videos as text tokens captures the key visual information and text tokens are naturally aligned with text models that are strong retrievers after the con-trastive pretraining process. All the empirical analysis es-tablishes a solid foundation for future research on afford-able and upgradable multimodal intelligence. 1.

Introduction
From retrieving a trending video on TikTok with natural language descriptions to asking a bot to solve your tech-nical problem with the question and a descriptive video,
AI agents handling multi-channel video-language retrieval-style tasks have been increasingly demanded in this post-social-media era. These tasks require the agent to fuse in-formation from multiple channels, i.e., video and text to retrieve a text response or return a multi-channel sample for a text query. To power such agents, a popular ap-proach [9, 10, 15, 34, 40] consists of two rounds of pre-(1) The 1st round is to obtain unimodal pre-training: trained models, such as visual-only encoders [3, 6, 17, 20] (e.g., S3D,CLIP) and text-only encoders [4,13,22,24] (e.g.,
BERT) (2) The 2nd round aims at pretraining on visual-text dataset - specifically, researchers leverage techniques like masked token modeling [9, 40] or contrastive learn-ing [10, 15, 33, 34] to align and fuse unimodal features from model pretrained in the 1st round.
Such methods achieve good performance on multi-channel retrieval-style tasks but they suffer from two ma-jor limitations: 1) huge amounts of data and computational resources are required for the second-round “pretraining”, which significantly limits the research exploration without such resources; 2) the domain of video data used in the second round “pretraining” has to be strongly correlated with downstream tasks [9], which may restrict such meth-ods from being generally applicable.
To alleviate such limitations, we study a novel problem: fast adaptation of pretrained contrastive models on multi-channel video-language retrieval under limited resources.
Specifically, we propose to adapt both contrastive multi-modal models [16,20] and contrastive text models [5,22] to enjoy their strong encoding ability and discriminative em-bedding space. There has been tremendous progress re-cently on large-scale contrastive multimodal models [16, 17, 20]. Through pretraining on millions of images/videos, these models are highly effective at encoding visual inputs and linking entities across modalities. Meanwhile, con-trastive text models [5, 22] have been also densely stud-ied to obtain discriminative sentence embeddings. These models are shown to perform well on challenging text re-trieval tasks such as semantic search [19], which requires
the model to understand both language and real-world knowledge [5, 22]. Such an ability allows the model to retrieve a text response or encode a text query in multi-channel video-language retrieval-style tasks. Thereby, once the video information is effectively incorporated, it elimi-nates the necessity of second-round “pretraining” on large scale multimodal datasets, enabling fast adaptation to any video-language retrieval-style tasks.
We first conduct a systematic analysis on potential model designs, as shown in Figure 1. We identify the model de-sign space with two design principles: how to represent the video information and how to fuse this video information with questions or other text such as speech. To represent video information, we could either adopt Continuous Fea-tures which is commonly used in existing work [18, 34], or project videos into unified Text Tokens [11] from vari-ous modalities. To fuse information from multiple channels, i.e., video and question/speech, there are two potential op-tions, namely, a Multimodal Transformer [34] or a Text
Transformer [18]. Hence, there are four combinations de-rived from this model design space, namely, Continuous
Features + Multimodal Transformer [34], Continuous
Features + Text Transformer, Text Tokens + Multimodal
Transformer, Text Tokens + Text Transformer.
Our exploration of this model design space results into a simple yet effective approach that allows fast adaptation of pretrained contrastive models, which first leverages con-trastive multimodal models to retrieve a sequence of Text
Tokens for the visual input and then feeds these tokens to-gether with other text to contrastive Text Transformer for answer retrieval. Its fast adaptation ability not only comes from the ability of linking entities across modalities of the contrastive multimodal model, but it also enjoys the natural alignment with contrastive text models to produce discrimi-native embeddings. To the best of our knowledge, this is the first proposal to adapt pretrained contrastive models in this manner, although each individual design choice may have been adopted previously. We further conduct in-depth anal-ysis to understand 1) the trade-off between data efficiency and accuracy, 2) the impact of pretrained contrastive text model, and 3) the possible limitation of this framework.
The contribution could be summarized three-fold:
• We identified the principled model design space for fast adaption of pretrained contrastive multimodal models and pretrained contrastive text models.
• We conducted extensive experiments on five video-language datasets, observed a consistent trend across these four variants, and even obtained state-of-the-art performance (e.g., 6.5% improvement on How2QA) with the proposed Text Tokens + Text Transformer variant without using millions of extra multimodal data samples, which is essential to democratize the community of video+language.
• The proposed Text Tokens + Text Transformer vari-ant scales significantly better than the other variants, w.r.t. the quality of pretrained text models. The code will be released at https://github.com/
XudongLinthu / upgradable - multimodal -intelligence to facilitate future research. 2.