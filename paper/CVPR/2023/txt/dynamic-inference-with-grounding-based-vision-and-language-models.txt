Abstract
Transformers have been recently utilized for vision and language tasks successfully. For example, recent image and language models with more than 200M parameters have been proposed to learn visual grounding in the pre-training step and show impressive results on downstream vision and language tasks. On the other hand, there exists a large amount of computational redundancy in these large models which skips their run-time efficiency. To address this prob-lem, we propose dynamic inference for grounding based vi-sion and language models conditioned on the input image-text pair. We first design an approach to dynamically skip multihead self-attention and feed forward network layers across two backbones and multimodal network. Addition-ally, we propose dynamic token pruning and fusion for two backbones. In particular, we remove redundant tokens at different levels of the backbones and fuse the image tokens with the language tokens in an adaptive manner. To learn policies for dynamic inference, we train agents using rein-forcement learning. In this direction, we replace the CNN backbone in a recent grounding-based vision and language model, MDETR, with a vision transformer and call it ViT-MDETR. Then, we apply our dynamic inference method to ViTMDETR, called D-ViTDMETR, and perform experi-ments on image-language tasks. Our results show that we can improve the run-time efficiency of the state-of-the-art models MDETR and GLIP by up to ∼ 50% on Referring Ex-pression Comprehension and Segmentation, and VQA with only maximum ∼ 0.3% accuracy drop. 1.

Introduction
Significant progress has been made in the development of image and language models attributed to: (1) emergence of transformers for different modalities [6,13], and (2) large scale pre-training paradigms [4, 14, 17, 24, 29, 43, 44]. In particular, with the very large scale pre-training of image and language models, large number of parameters and com-putations are allocated for processing input image-text pair.
Specifically, the number of parameters of recent vision and
Figure 1. Accuracy vs frames per second comparison of the large (Top Left) and small (Top Right) models and accuracy vs GLOPs comparison of the large (Bottom Left) and small (Bottom Right) models. D-ViTMDETR outperforms MDETR, GLIP and our ViT-MDETR model in both frames per second and GFLOPs metrics while maintaining high accuracy. language models can be more than 200M [14, 17, 44], re-sulting in low run-time efficiency. This problem with the single-modality transformers has been tackled by several studies before [27, 30, 37]. Such computational complex-ity is further amplified in multimodal networks often build-ing on multiple transformer models. As a result, reduc-ing the run-time complexity of the multimodal networks can be very beneficial for the downstream tasks. Exist-ing methods including pruning [20], knowledge distilla-tion [10, 33] and quantization [39] can potentially be ex-tended toward this goal. However, they show significant performance drop (≥ 1%) at ≥ 50% compression rates and these methods are mostly designed for parameter re-duction not for run-time speed. As a result, we propose dynamic inference with the large image and language mod-els mainly to achieve two goals: (1) drastically reduce run-time complexity, and (2) maintain their high accuracy. To-wards these goals, in the first step, we analyze the clas-sic architectural choices for the recent image and language models. A typical image and language model consists of a vision encoder (a CNN or a vision transformer), a text encoder (transformer), and a multimodal transformer (fu-sion network).
Inspired by MDETR [14], we build a vi-sion and language model consisting of vision and language transformer and DETR-like multimodal network and call it
ViTMDETR. The transformer modules consist of a multi-head self attention (MSA) and feed forward network (FFN) blocks which are experimentally found to be computation-ally expensive modules in inference-time. It is also known that computationally complexity of transformer MSA mod-ule goes up quadratically w.r.t number of tokens. The num-ber of tokens and the computational complexity gets further amplified with inclusion of multimodal inputs and related modules.
For these reasons, with our D-ViTMDETR model we propose to dynamically prune input tokens from multiple modalities across the transformer backbones and fuse vi-sion tokens adaptively with the text tokens to improve the accuracy. This way, we can reduce the complexity quadrat-ically. Additionally, we adaptively skip the computationally expensive MSA and FFN layers across the two backbones and the multimodal network to further improve run-time ef-ficiency. To learn dynamic policies, we train decision net-works using the policy-gradients based reinforcement learn-ing algorithm and distill the knowledge from ViTMDETR to better optimize D-ViTMDETR.
In this research work, our contributions are as below:
• We introduce an MDETR-inspired transformer-based model ViTMDETR for grounding based vision and language tasks.
• We propose a novel method to learn dynamic token pruning and fusion actions to reduce computational complexity using reinforcement learning. Addition-ally, we train the same agents to learn MSA and FFN layer skipping throughout our vision and language model to further reduce complexity.
• For better optimization, we align the representations and predictions of D-ViTMDETR with the representa-tions and predictions of the ViTMDETR model.
• We perform experiments with both our ViTMDETR and D-ViTMDETR models on several image and lan-guage benchmarks for Referring Expression Compre-hension (REC) and Segmentation (RES), and VQA tasks. With our dynamic model, D-ViTMDETR, we can improve the run-time efficiency of the state-of-the-art models MDETR [14] and GLIP [17] by up to
∼ 50% on Referring Expression Comprehension and
Segmentation, and VQA with only maximum ∼ 0.3% accuracy drop as seen in Figure 1. 2.