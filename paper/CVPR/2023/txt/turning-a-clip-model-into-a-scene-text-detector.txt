Abstract
The recent large-scale Contrastive Language-Image Pre-training (CLIP) model has shown great potential in various downstream tasks via leveraging the pretrained vision and language knowledge. Scene text, which contains rich textual and visual information, has an inherent connection with a model like CLIP. Recently, pretraining approaches based on vision language models have made effective progresses in the field of text detection. In contrast to these works, this paper proposes a new method, termed TCM, focusing on
Turning the CLIP Model directly for text detection without pretraining process. We demonstrate the advantages of the proposed TCM as follows: (1) The underlying principle of our framework can be applied to improve existing scene text detector. (2) It facilitates the few-shot training capability of existing methods, e.g., by using 10% of labeled data, we sig-nificantly improve the performance of the baseline method with an average of 22% in terms of the F-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene text detection methods, we further achieve promising domain adaptation ability. The code will be publicly released at https://github.com/wenwenyu/TCM. 1.

Introduction
Scene text detection is a long-standing research topic aiming to localize the bounding box or polygon of each text instance from natural images, as it has wide practical applications scenarios, such as office automation, instant translation, automatic driving, and online education. With the rapid development of fully-supervised deep learning technologies, scene text detection has achieved remarkable progresses. Although supervised approaches have made remarkable progress in the field of text detection, they require extensive and elaborate annotations, e.g., character-level, word-level, and text-line level bounding boxes, especially polygonal boxes for arbitrarily-shaped scene text. Therefore, it is very important to investigate text detection methods
*Equal contribution.
†Corresponding author.
Figure 1. Comparisons of different paradigms of using text knowl-edge for scene text detection. under small amount of labeled data, i.e., few-shot training.
Recently, through leveraging the pretrained vision and language knowledge, the large-scale Contrastive Language-Image Pretraining (CLIP) model [26] has demonstrated its significance in various downstream tasks. e.g., image classi-fication [53], object detection [5], and semantic segmenta-tion [12, 27, 43].
Compared to general object detection, scene text in natu-ral images usually presents with both visual and rich char-acter information, which has a natural connection with the
CLIP model. Therefore, how to make full use of cross-modal information from visual, semantic, and text knowledge to improve the performance of the text detection models re-ceives increasing attentions in recent studies. For exam-ples, Song et al. [29], inspired by CLIP, adopts fine-grained cross-modality interaction to align unimodal embeddings for learning better representations of backbone via carefully designed pretraining tasks. Xue et al. [46] presents a weakly supervised pretraining method to jointly learn and align vi-sual and partial textual information for learning effective visual text representations for scene text detection. Wan et al. [35] proposes self-attention based text knowledge mining to enhance backbone via an image-level text recognition pretraining tasks.
Different from these works, as shown in Figure 1, this paper focuses on turning the CLIP model for text detection without pretraining process. However, it is not trivial to incorporate the CLIP model into a scene text detector. The key is seeking a proper method to exploit the visual and semantic prior information conditioned on each image. In this paper, we develop a new method for scene text detec-tion, termed as TCM, short for Turning a CLIP Model into a scene text detector, which can be easily plugged to im-prove the scene text detection frameworks. We design a cross-modal interaction mechanism through visual prompt learning, which is implemented by cross-attention to recover the locality feature from the image encoder of CLIP to cap-ture fine-grained information to respond to the coarse text region for the subsequent matching between text instance and language. Besides, to steer the pretrained knowledge from the text encoder conditioned independently on different input images, we employ the predefined language prompt, learnable prompt, and a language prompt generator using simple linear layer to get global image information. In ad-dition, we design an instance-language matching method to align the image embedding and text embedding, which en-courages the image encoder to explicitly refine text regions from cross-modal visual-language priors. Compared to pre-vious pretraining approaches, our method can be directly finetuned for the text detection task without pretraining pro-cess, as elaborated in Fig. 1. In this way, the text detector can absorb the rich visual or semantic information of text from CLIP. We summarize the advantages of our method as follows:
• We construct a new text detection framework, termed as TCM, which can be easily plugged to enhance the existing detectors.
• Our framework can enable effective few-shot training capability. Such advantage is more obvious when using less training samples compared to the baseline detectors.
Specifically, by using 10% of labeled data, we improve the performance of the baseline detector by an average of 22% in terms of the F-measure on 4 benchmarks.
• TCM introduces promising domain adaptation ability, i.e., when using training data that is out-of-distribution of the testing data, the performance can be significantly improved. Such phenomenon is further demonstrated by a NightTime-ArT text dataset1, which we collected from the ArT dataset. 1NightTime-ArT Download Link
• Without pretraining process using specific pretext tasks,
TCM can still leverage the prior knowledge from the
CLIP model, outperforming previous scene text pre-training methods [29, 35, 46]. 2.