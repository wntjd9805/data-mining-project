Abstract
Image-text pretrained models, e.g., CLIP, have shown impressive general multi-modal knowledge learned from large-scale image-text data pairs, thus attracting increas-ing attention for their potential to improve visual represen-tation learning in the video domain. In this paper, based on the CLIP model, we revisit temporal modeling in the context of image-to-video knowledge transferring, which is the key point for extending image-text pretrained models to the video domain. We find that current temporal model-ing mechanisms are tailored to either high-level semantic-dominant tasks (e.g., retrieval) or low-level visual pattern-dominant tasks (e.g., recognition), and fail to work on the two cases simultaneously. The key difficulty lies in modeling temporal dependency while taking advantage of both high-level and low-level knowledge in CLIP model. To tackle this problem, we present Spatial-Temporal Auxiliary Net-work (STAN) – a simple and effective temporal modeling mechanism extending CLIP model to diverse video tasks.
Specifically, to realize both low-level and high-level knowl-edge transferring, STAN adopts a branch structure with decomposed spatial-temporal modules that enable multi-level CLIP features to be spatial-temporally contextual-ized. We evaluate our method on two representative video tasks: Video-Text Retrieval and Video Recognition. Exten-sive experiments demonstrate the superiority of our model over the state-of-the-art methods on various datasets, in-cluding MSR-VTT, DiDeMo, LSMDC, MSVD, Kinetics-400, and Something-Something-V2. Codes will be available at https://github.com/farewellthree/STAN 1.

Introduction
Recent years have witnessed the great success of image-text pretrained models such as CLIP [31]. Pretrained on these models learned over 400M image-text data pairs, transferable rich knowledge for various image understand-ing tasks. Similarly, video domains also call for a CLIP-like model to solve downstream video tasks. However, it is hard to get a pretrained model as powerful as CLIP in the video domain due to the unaffordable demands on computation re-sources and the difficulty of collecting video-text data pairs as large and diverse as image-text data. Instead of directly pursuing video-text pretrained models [16, 26], a potential alternative solution that benefits video downstream tasks is to transfer the knowledge in image-text pretrained models to the video domain, which has attracted increasing atten-tion in recent years [11, 12, 25, 28, 29, 40].
Extending pretrained 2D image models to the video do-main is a widely-studied topic in deep learning [4, 7], and the key point lies in empowering 2D models with the ca-pability of modeling temporal dependency between video frames while taking advantages of knowledge in the pre-trained models. In this paper, based on CLIP [31], we revisit temporal modeling in the context of image-to-video knowl-edge transferring, and present Spatial-Temporal Auxiliary
Network (STAN) – a new temporal modeling method that is easy and effective for extending image-text pretrained model to diverse downstream video tasks.
We find that current efforts on empowering CLIP with temporal modeling capability can be roughly divided into posterior structure based methods and intermediate struc-ture based methods as shown in Fig. 1(a). Posterior struc-ture based methods [11,12,25] employ a late modeling strat-egy, which take CLIP as a feature extractor and conduct temporal modeling upon the embeddings of video frames extracted independently from CLIP. Upon the highly se-mantic embeddings, though the structure is beneficial for transferring the well-aligned visual-language representation (i.e., high-level knowledge) to downstream tasks, it hardly captures the spatial-temporal visual patterns (i.e., low-level knowledge) among different frames, which is important for video understanding. As shown in Fig. 1(b), compared to the CLIP baseline that employs a naive mean pooling to aggregate the features of all frames to obtain a video rep-resentation, the performance improvement brought by the typical posterior structure, i.e. CLIP4clip-seqTrans [25] is trivial, especially on the video action recognition task where
Figure 1. (a) Illustration of temporal modeling with posterior structure (left), intermediate structure (middle) and our branch structure(right). (b) Performance comparison among the posterior structure based CLIP4clip-seqTrans [25] , intermediate structure based XCLIP [28] and our branch structure based STAN. We take the CLIP model with a naive mean pooling to aggregate the features of all frames into video representations as the baseline. We present the improvement brought by different methods over this baseline w.r.t. Recall@1 on MSRVTT for video-text retrieval and Top-1 accuracy on Kinetics-400 for video recognition. spatial-temporal visual patterns are crucial. different implementations.
In contrast to posterior structure based methods, inter-mediate structure based methods [4, 28, 29] strengthen the spatial-temporal modeling capability of CLIP via plugging temporal modeling modules directly between CLIP layers, and achieve 3.7% improvement over the baseline on the video action recognition task. However, we find that in-serting additional modules into CLIP would impact the pre-trained high-level semantic knowledge in the model, which only outperforms the baseline by 0.2% on the video-text retrieval tasks. Therefore, modeling temporal dependency while taking advantage of knowledge in different levels of representation is important for extending the CLIP model to the video domain.
Unlike the above methods, inspired by FPN [22] that introduces a branch network to strengthen multi-level rep-resentation learning for CNNs, our proposed STAN em-ploys a new branch structure outside of the visual back-bone, as shown in Fig. 1(a). Thanks to the branch structure,
STAN augments the features of video frames with spatial-temporal contexts at different CLIP output levels without affecting the forward-propagating of CLIP itself. Thus, it is able to take advantage of both high-level and low-level knowledge in the pretrained model simultaneously, and ef-fectively extends CLIP to diverse downstream video tasks.
STAN consists of multiple layers with a spatial-temporal separated design. Specifically, the layer operates spatial-temporal modeling via alternatively stacking two separate modules – an intra-frame module and a cross-frame module, which enables the layer to boost the performance of model via reusing the pretrained parameter of CLIP layers to ini-tialize the intra-frame spatial modules. We further investi-gate two instantiations of cross-frame modules, i.e., the self-attention-based module and 3D convolution based module, to facilitate the comprehensive understanding of STAN in
We evaluate our STAN on both the high-level semantic-dominant task (i.e., video-text retrieval) and low-level vi-sual pattern-dominant task (i.e.,, video recognition), trial-ing our methods from the two different perspectives. Ex-tensive experiments demonstrate our expanded models are generally effective on the two different tasks. For video-text retrieval, we surpass the CLIP4clip by +3.7%, +3.1%, and +2.1% R@1 on MSRVTT, DiDemo, and LSMDC.
For video recognition, we achieve competitive performance on Kinetics-400, with 88× fewer FLOPs than Swin3D-L
[24] and improve CLIP baseline by 20%+ on Something-Something-V2.
Our main contributions are summarized as: (1) we re-visit temporal modeling in the context of image-to-video knowledge transferring and figure out that the key challenge lies in modeling temporal dependency while taking advan-tage of both high-level and low-level knowledge; (2) we propose Spatial-Temporal Auxiliary Network (STAN) – a new branch structure for temporal modeling, which facil-itates representation learning of video frames with includ-ing spatial-temporal contexts at different levels and better transfer the pretrained knowledge in CLIP to diverse video tasks; (3) our method achieves competitive results on both video-text retrieval and video recognition tasks compared to
SOTA methods. 2.