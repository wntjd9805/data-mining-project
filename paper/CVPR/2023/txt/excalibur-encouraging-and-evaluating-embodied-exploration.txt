Abstract
Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world.
On the other hand, machines are either trained to learn pas-sively from static and ﬁxed datasets, or taught to complete speciﬁc goal-conditioned tasks. To encourage the develop-ment of exploratory interactive agents, we present the EX-CALIBUR benchmark. EXCALIBUR allows agents to ex-plore their environment for long durations and then query their understanding of the physical world via inquiries like:
“is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to reﬁne their knowledge, update their beliefs, and improve their performance on the questions. Our ex-periments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and
the headroom afforded to develop new innovative methods.
Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCAL-IBUR affords unique challenges in comparison to present-day benchmarks and represents the next frontier for embod-ied AI research. 1.

Introduction
Humans are active learners, acquiring knowledge of the physical world through intentional experiments with their bodies and senses. Children as young as a few months old learn about objects and their environment through observa-tion and interaction [6, 24]. This sensorimotor experience, as pointed out by Piaget [47], is critical in forming a funda-mental understanding of reality. This is the cognitive moti-vation for the creation of EXCALIBUR.
In contrast, machine learning models typically obtain knowledge by passively observing web-crawled, encyclo-pedic, or crowd-sourced static datasets [67]. This pas-sive approach has clear limitations. For instance, ground-ing physical concepts like heavy, large, and long requires moving beyond passive observation. To weigh an object, humans will often try to use different forces to move it.
To compare the sizes of objects, they move around and perceive the objects from different angles and distances.
Although large pre-trained models have made progress in aligning with the grounded world [41, 45], they still lack an embodied understanding of physical concepts [59].
Todays popular active, embodied-learning benchmarks in the Embodied AI community focus on directed task com-pletion. These include navigating to speciﬁed GPS coor-dinates [3], locating an object of a speciﬁed category [7], translating commands into low-level actions [5, 56], and in-specting a scene to answer a question about the presence or count of an object category [15, 25]. A more recent bench-mark, Room Rearrangement [62] requires agents to explore the scene, but the focus there is on navigation, observa-tion, and memorization. Progress on these benchmarks has been promising. We can now train agents that can compre-hend goal instructions reasonably well and complete simple tasks, particularly navigation heavy tasks. None of these benchmarks, however, explicitly probe how these models have learned to represent their environments, nor do they encourage the type of free-form, undirected, experimental, exploration performed by humans.
To encourage and evaluate the capacity of embodied agents to openly explore their environment and interact with objects within it, we present the EXCALIBUR1 benchmark.
EXCALIBUR is built using large procedurally generated 1Exploratory Curious Agents with Language Induced Embodied World
Understanding houses via ProcTHOR [18]. Each episode in EXCALIBUR consists of four phases as shown in Fig. 1. Phase I Explo-ration – The agent must navigate to and interact with objects in the environment. Importantly, the agent isn’t seeded with a goal and must instead perform open-ended exploration.
Interacting with objects takes place via physics-enabled arm manipulation. Phase II Question Answering – We probe the agent’s understanding of the physical world through natural language inquiries. Our questions go beyond simple prim-itive queries, e.g. regarding object existence, and include physical attributes (e.g. masses and materials) and visual at-tributes (e.g. colors and shapes). Phase III Reentering – This is a goal-directed phase, since the agent must interact with the environment to reﬁne its understanding of the world in response to questions asked in the previous stage. Phase
IV Reﬁned Question Answering – This phase repeats the inquiries made in Phase II to query if the agent was able to successfully acquire the required knowledge about its world after being provided the goal question set.
Our use of question-answering in this benchmark which focuses on interaction and exploration has several beneﬁts.
Natural language inquiries allow us to probe the agent’s un-derstanding of the world. They also provide a clear and ob-jective metric for EXCALIBUR. Further, they can serve as supervisory signals to encourage agents to interact with ob-jects and explore the world. Finally, the introduction of lan-guage opens the door to using pre-trained language models in future work, given the recent rise of their use for planning for embodied agents [2]. (1) It encourages open-ended exploration.
EXCALIBUR is the ﬁrst benchmark that offers the fol-lowing new avenues and challenges for Embodied AI re-search: (2)
Agents in EXCALIBUR have access to a rich interactive ac-tion space that covers navigation, arm-based manipulation, and grasping with different degrees of force. (3) The ques-tions in this benchmark move beyond existence and count-ing. They probe the agent on its abilities to learn phys-(4) Our task re-ical and visual attributes of the world. quires long-horizon planning and reasoning. Most embod-ied benchmarks today have maximum episode lengths of up to 250 steps. Our task has four phases that include an ex-ploration phase of 2500 steps. (5) Our task also evaluates the ability of an agent to reﬁne and improve the existing knowledge of its environment. This is an ability that hu-mans commonly showcase in their everyday experiences.
We present baselines using state-of-the-art Embodied AI neural models and learning methods. We also design a Vir-tual Reality interface to enable humans to navigate and in-teract with objects in ProcTHOR scenes in an immersive way. This allows for a more accurate human baseline mea-surement, which demonstrates that there remains substan-tial room for model improvement. Finally, in Sec. 5, we show that the failure patterns of models are distinct from
Figure 2. The action space of EXCALIBUR. The whole action space consists of two sets of actions: Navigation (left) and Manipulation (right). Navigation actions are used to move the agent (bottom left) and look at different angles (top left). Manipulation actions are used to move the arm (top right), grasp with force and open and close closets, drawers and fridges (which are implemented as high action which can be triggered when the gripper is close to the handles), and signal ﬁnishing the task (bottom right). All of the actions are discretized: angler motion are discretized into 15 degrees, linear motion are discretized into 0.05 meter for joints and 0.25 meter for base and force is discretized into 0.05 kilogram-force. those of humans. Humans are great at exploration, but fall short at memorization, while agents tend to succeed at an-swering questions that depend on memory but are poor ex-plorers – even when trained with popular exploration re-wards. Altogether, we ﬁnd that EXCALIBUR serves as a powerful and ﬂexible framework and environment for evaluating and building Exploratory Curious Agents with
Language Induced Embodied World Understanding. 2. EXCALIBUR
Consider the example depicted in Fig. 1: the embodied agent is spawned in the bedroom of a random house at a
It traverses the bedroom, living room, random position. kitchen, and bathroom, opens closets, fridges, and drawers, and picks up various objects. After 2,500 steps, the agent is asked 20 questions and answers some questions correctly and some incorrectly, e.g. “How many silver objects are heavier than the white egg in the kitchen?”. The agent then returns to the house and explores the scene again. This time it starts lifting silver objects in the room to estimate their weight. As the example reveals, EXCALIBUR encourages agents to openly explore their world in the ﬁrst phase but also evaluates their ability to perform goal-directed explo-ration once the questions become known. Natural language inquiries are used to ascertain what the agent has learned about its environment. We now present details about the
EXCALIBUR task, and contrast it to previous Embodied AI benchmarks in Sec. 6 and Tab. 3. 2.1. Task.
,
,
P
Q
H hH
An EXCALIBUR task is deﬁned as a triple
,
Pi ouse consists of a ﬂoor plan and objects in it, where a uestion set is a list of English question-answer pairs, a
Q that is osition is a 2D location on the ﬂoor of and a empty (i.e. at which the agent can be placed) along with an initial agent camera orientation. Each object in the house is deﬁned by its type, colors, materials (full list of object types, colors, and materials is in Appendix B), meshes (3D shape of the objects), location, size (width, depth, height), and weight (under 5kg). At the start of each episode, the
ﬂoor plan, objects (including colors, materials, sizes, and weights), questions, and agent spawn position are randomly sampled with the distribution speciﬁed in Appendix C.
H
Phases. The EXCALIBUR task consists of four phases: (I) exploration, (II) question answering, (III) reentering, and (IV) reﬁned question answering. In both (I) and (III), the agent may navigate throughout the house and manipulate objects. One difference between (I) and (III) is that the time steps in (I) are limited to 2,500, while the steps in (III) T3 are unlimited but used to discount the accuracy improve-ment in Eq. 1. In (II) the agent is asked 20 questions. This brings up another notable difference between (I) and (III). In (I), an agent must perform open-ended exploration, learning about objects and their relationships. In (III), its exploration is conditioned on its experience in (I), the goal questions and its own answers in (II), and it attempts to improve its answers in (IV). We denote the accuracy in Phase (II) and
Figure 3. Dataset contruction procedure. We generate the dataset in four steps (each in a pane). (1) We consider the procedurally generated
ﬂoor plans and houses generated with PROCTHOR. (2) We then randomize the sizes and weights of objects in the scene. (3) We then extract the scene graphs of objects and relations in the scenes. (4) Based on hand-crafted templates, we generate questions and ﬁlter out questions that can be answered without exploring the scenes.
Phase (IV) as Accexp and Accref.
Agents. The breadth of embodied experience results from the versatility of human bodies. With this in mind, the agent used in EXCALIBUR is the MANIPULATHOR arm agent of Ehsani et al. [21]. This agent has a dexterous 6 DOF
Kinova-inspired robotic arm, see Fig. 2. We extend their design by adding a force argument to grasping action.2 This is one step further towards more realistic manipulation and also empowers the agents to “feel” the weights of objects through interaction. Fig. 2 shows the available actions of the armed agent in Phase (I) and (III). The “Done” action signals that the agent wishes to end Phase (III), the number of time steps spent before which are counted as T3. At every timestep, the agent acts given egocentric RGB images (of size 800
Evaluation. We wish to evaluate two facets of exploration: (1) “how many questions can be answered with the knowl-edge acquired in Phase (I)?”, and (2) “how efﬁcient is the agent in reﬁning its answers in Phase (IV)?”. To deﬁne a uniﬁed metric measuring both facets, we propose the fol-lowing exploration score (ExQA): 600) as its observation.
⇥
ExQA
 = Accexp + (Accref  
Accexp) exp( kT3), (1)
  where we call k > 0 the energy coefﬁcient. ExQA reduces to Accref when k = 0 and reduces to Accexp as k
. Our choice of k thus determines how we prioritize accuracy af-ter exploration versus after answer reﬁnement. We choose
! 1 2Force feedback mechanisms are common in physical manipulators. a value for k that maximizes human performance, biasing models to uncover strategies of similar efﬁciency and efﬁ-cacy as we see in human demonstrations.3 2.2. Dataset Construction
The EXCALIBUR dataset is built upon PROCTHOR-10k, a dataset of 10,000 procedurally generated home en-vironments, each containing between 1-10 rooms [18]. For each PROCTHOR-10k home, we apply a variety of scene augmentations (e.g. randomizing object weight and sizes) and generate sets of challenging questions. We break our dataset generation process into four stages: randomization, scene graph generation, question generation, and ﬁltering.
We detail each stage below, see Fig. 3 for a visual overview.
Randomization. The diversity across PROCTHOR-10k houses is very large: objects placements, ﬂoor plans, ma-terials, Pare all randomized while respecting sensible con-straints common across real homes. Despite this diversity, we found that, without applying additional scene augmen-tations, many questions of interest become either trivial or answerable via commonsense. For instance, the weights of many objects in AI2-THOR (and thus in PROCTHOR-10k) are set uniformly across object categories. This means that a question such as “is the cup in the kitchen heavier than the bowl?”, may have a constant answer across all cups and bowls. Thus, without applying weight randomization, 3Empirically, we ﬁnd k which maximizes ExQA likelihood under a gaus-sian prior. The procedure for choosing an optimal value of k is described in App. F.
⇥
⇥
⇥
⇥ and 1.5 and 1.0 the agent may answer accurately without any exploration or object interaction. In EXCALIBUR, we apply two types of supplemental randomization to PROCTHOR-10k: object weight and size randomization. In particular, within each house, we uniformly sample the weights of pickupable (i.e. excluding large objects that cannot be held by the agent, e.g. a fridge) objects to be between 0.5 their start-ing values. Similarly, the size of pickupable objects (i.e. their scale) is randomized to be with 0.8 of their starting values. Note that we only downscale objects as this prevents potential collisions between nearby objects.
Scene Graph. Before moving to question generation, we
ﬁrst preprocess each house to produce a scene graph rep-resentation of the environment. This scene graph provides a compact summary of the objects in the house along with their relationships and attributes. In our formulation, rooms, objects, and agent are represented as nodes with edges be-tween nodes representing their relationships. These re-lationships include, for example, CONTAINEDBY, ADJA-CENTTO, ONTOPOF. A full listing of object relationships and node attributes can be found in the appendix.
Question Generation. To generate our question sets, we follow the process used to generate the single-image visual question answering (VQA) dataset CLEVR [31].
In par-ticular, we represent questions using functional programs whose answer values can be found by evaluating these programs upon the above described scene graph. As for
CLEVR, we design a collection of (11) question families, which can be composed and chained to generate questions.
This question generation process may produce degenerate or tautological questions, we prune these using the depth-ﬁrst approach employed when constructing CLEVR. De-tails of question generation procedure is in App. E.
%
Type n o i t a l e
R n o i t s e u
Q 78.8 12.3 8.9
Yes-no
Count
Query
Color
Material
CONTAINEDBY
ADJACENTTO
ONTOPOF
HEAVIERTHAN4
LARGERTHAN
In order to
Filtering. create questions that are challenging and whose answers are not overly biased to certain an-swers, we use extensive question to remove easy questions.
In particular, for each candidate question q, we compute the answer of q across all scenes, which produces a distribution over answers. We ﬁlter questions to only include those whose answer distribution is sufﬁciently balanced. For more details see, App. D.
Table 1. Dataset Distribution. 26.7 66.2 8.2 39.5 0.8 30.6 18.9
ﬁltering
The result of such a process is an underlying dataset with a range of difﬁcult questions of 3 different types and 7 kinds of physical properties and relations (Fig. 1) Different types of questions are evaluated in slightly different ways: Yes-no questions are evaluated by exact matching, count questions are answered correctly when the prediction is only different than the standard answer by 5%, and query questions match prediction and the standard answer order-agnostically. In this way, we use accuracy as an umbrella metric for all of the questions. There are four splits in EXCALIBUR: (1) a training set with 10k PROCTHOR scenes, (2) a validation and a test set with 1k PROCTHOR scenes each, and (3) an-other test set with 9 hand-crafted ARCHITECTHOR scenes5 for comparison between agents and humans. 3. Human Baseline with VR Interface
One challenge of comparing human performance fairly with that of our agents is that our agents are extensively trained on houses from our dataset while human annotators, on the other hand, are only exposed to a small handful of training episodes. It is therefore important to create a real-istic environment where real-life experience and knowledge can be easily transferred to the simulated environment. For this, we create a VR interface to EXCALIBUR and ask hu-man annotators to complete tasks while virtually embodied as the agent. In our experiments, human participants used the Meta Quest 2 VR headset6 and were evaluated using the same metric as our agents. Concretely, to make the expe-rience interactive and immersive, we ensured that our VR experience satisﬁed the following requirements.
Flexible Head Movement: The head movement of the
• human annotators is smoothly reﬂected as camera move-ment in the VR environment, so that the information-seeking behavior of the human annotators can be easily transferred to the simulated environment.
Intuitive Arm Movement: Human annotators should be
• able to intuitively manipulate the robotic, 6 DOF Kinova-like, the arm of the MANIPULATHOR agent used in EX-CALIBUR. As the robotic arm has greater degrees of free-dom than a human arm (ignoring human ﬁngers) this means that special attention must be paid to ensure that humans need not worry about the rotation of joints of the arm, but only the position and orientation of the gripper.
Gripping With Force: We leveraged the pressure on the
• grip button of the Meta Quest 2 controller to map it to the grasp force in the environment so annotators can use differ-ent magnitudes of forces to grip objects.
Open/Close: We also facilitated the user to open and
• close various objects in the VR environment, to make the experience more immersive and allow the user to explore the house in greater depth.
For more details on VR interface, training and evaluating 4HEAVIERTHAN includes LIGHTERTHAN, and LARGERTHAN includes
SMALLERTHAN, LONGERTHAN, and SHORTERTHAN 5One ARCHITECTHOR scene is used for training human annotators. 6https://www.meta.com/quest/products/quest-2/
human annotators, see App. A. 4. Reinforcement Learning Baselines
 
EXCALIBUR requires a model to actively plan, ex-plore the houses, manipulate objects, memorize its his-tory, and answer questions.
In this work, and as is com-mon across modern embodied benchmarks, we train re-inforcement learning models as our baselines. Recurrent neural networks (RNNs) are frequently used as generic models for encoding language instructions, historical ob-servations, and actions, into belief states for embodied agents [18, 21, 32, 62–64]. Following this prior work, we use a GRU [13] to encode the history of observations seen and actions taken by the agent to produce, at every time step t 0, a vector belief state bt corresponding to the output of the RNN at that timestep. We extend this practice by feed-ing the belief states as input to an actor-critic policy head as well as to a question answering module. To understand whether questions answering serves as a good stimulation for encourging exploration, we consider three training sig-nals: a (1) coverage-based reward, (2) QA reward, and (3)
QA cross-entropy loss. Our goal in the following exper-iments is to show that modern Embodied AI models and training techniques can achieve some level of success on
EXCALIBUR with the goal of inspiring future work to build upon these results.
Actor-critic policy The belief state is fed into an MLP with one hidden layer, which we call the actor-head, and decoded into logits, one logit for each discrete action avail-able to the agent (recall Sec. 2.1). By passing these log-its through a softmax we produce the agent’s policy (i.e. a distribution over agent actions). To enable training with
PPO [52,63], we also must produce an estimate of the value of the agent’s current state. To do this, we feed the belief state through another similar MLP, the critic-head, which returns a 1-dimensional output.
Question answering To make full use of existing large, pretrained, language models, we follow [60] and propose to convert belief states into continuous preﬁx tokens using a preﬁx generator MLP with two hidden layers f preﬁx
. These preﬁx tokens are preprepended to with the question tokens and fed into the encoder of pre-trained T5 [49]. We then use the, pretrained, T5 decoder module to produce a (distribu-tion over) natural-language answers to the given question.
Note that the T5 model has its parameters frozen and so is not trained in our experiments.
Featurizing agent observations We experiment with two different visual feature extractors for the agent’s egocen-tric RGB observations: (1) a pre-trained CLIP ResNet50 model [32, 48] and (2) a MaskRCNN [29] model ﬁnetuned on our training scenes. Visual features and an embedding of the agent’s last action are concatenated and passed as in-put to the above RNN. After Phase II, the agent additionally
✓
ProcTHOR Test Set
ArchitecTHOR Test Set
Accexp Accref
T3
ExQA Accexp Accref
T3
ExQA
Random
Language
QA
Novelty
Novelty+QA
Human w/o replay
Human w/ replay 41.7 53.5 58.5 54.2 58.7
--41.7 53.5 60.2 56.5 63.1
----131.2 99.6 203.2
--41.7 53.5 60.0 56.4 62.4
--39.1 49.2 52.4 49.9 53.5 63.6 81.3 39.1 49.2 56.0 54.5 56.3 87.1 94.3
--159.1 125.7 211.7 759.4 782.1 39.1 49.2 55.7 54.1 55.9 79.4 90.1
Table 2. Human and baseline performance across two test sets. We bold best metric values among AI systems. conditions the question embeddings from the T5 encoder as input to the RNN, which is also concatenated to observation and question embeddings.
Training Our training loss equals the unweightd sum of the standard PPO RL loss [52] and
LQA, a cross-entropy loss for question answering deﬁned as
T
LQA = t=1
X
X(q,a) 2Q log pT5(a
|
 
[f preﬁx
✓ (ht), f emb(q)]), (2) where pT5 is the probability of answer a produced by a T5 encoder-decoder, and f emb is the embedding layer of the T5 encoder, and is the set of question-answer pairs associ-ated with an episode.
Rewards We consider two kinds of rewards in this paper: (1) a QA reward and (2) a novelty-based reward. The QA reward is calculated by comparing the answers generated through beam search from T5 and the ground truth answers:
Q 1 rQA t =
I(a = T5t(q))
I(a = T5t 1(q))
,
 
 
✓ 2Q ⇣
|Q| X(q,a)
⌘ (3) where T5t(q) = T5(f decoder (ht), f emb(q)) denotes the out-put of the T5 model when using beam search decoding.
Note that rQA can only be non-zero when the agent’s an-1 and swer to a question changes between time steps t t. Our novelty reward encourages the agent to exhaustively navigate and observe novel objects, in particular, we let
  t rnovelty t
=
Oseen t  
Oall
Oseen 1 t
 
+
At
At  
 
Areachable 1
, (4) t where Oseen denotes the number of objects seen till time step t, Oall denotes number of objects in
, At denotes the area covered by time step t, and Areachable denotes the total reachable area in
H
.
H 5. EXCALIBUR Human and Agent Evaluation
To gain insight into the gap between humans’ and state-of-the-art embodied AI models’ performance on EXCAL-IBUR we ﬁrst must train such embodied models. To this
end, we train several variants of the reinforcement learning baseline described in Sec. 4 on the training split of EXCAL-IBUR. In particular, we train three variants denoted QA,
Novelty, and Novelty+QA; as suggested by their names, the
QA agent is only given the QA reward signal, the Novelty agent has access to the novelty reward, and the Novelty+QA is given the sum of both rewards at every timestep. For all of these agents, cross entropy loss is used for optimizing the preﬁx generator. Beyond these RL baselines, we also in-clude non-interactive Random and Language baselines; the
Random baseline simply chooses answers at random from among plausible answers when conditioned on the question type while the Language model is trained to answer ques-tions given only question text, which helps indentifying ar-tifects in question generation.
To make cross-model and human-agent comparisons we evaluate our embodied models on two test sets: (1) the procedurally generally PROCTHOR-10k testing scenes and (2) the set of, human-designed, ARCHITECTHOR test houses [18]. We evaluate humans only in the ARCHITEC-THOR houses as the ARCHITECTHOR test houses were meticulously crafted to closely imitate real-world houses and represent a smaller domain shift for human participants.
The results of these evaluations can be found in Table 2.
Among AI systems, we see that the Novelty+QA agent per-forms best across the Accexp, Accref, and ExQA metrics with the QA model close behind. This suggests that the nov-elty reward may provide only marginal beneﬁts and, indeed, the Novelty agent obtains results only slightly above those of the Langauge model which, at best, simply reproduces the biases in our question-answer pairs.
For our human evaluations, we consider two experimen-tal conditions Human w/o replay and Human w/ replay. In the Human w/ replay trials, unlike in Human w/o replay, hu-mans are allowed to view a video of their behavior in Phase
I and Phase III when answering questions in Phase II and IV, respectively. Hence participants in the Human w/ replay tri-als are relieved of the burden of needing to remember all of the details of their exploration. While humans outperform the AI systems in both experimental conditions, the gap be-tween AI and human performance is far narrower (gap of
+10.1 Accexp for Human w/o replay v.s. a gap of +27.8 for
Accexp Human w/o replay). This suggests that memoriza-tion is a signiﬁcant bottleneck for humans. Note that, in the
Human w/ replay condition, humans achieve an extremely high Accref value (94.3) showing clearly that EXCALIBUR is, in principle, solvable by intelligent systems.
Further analysis of our results as well as descriptive met-rics of agent exploration behavior can be found in App. H. 6.