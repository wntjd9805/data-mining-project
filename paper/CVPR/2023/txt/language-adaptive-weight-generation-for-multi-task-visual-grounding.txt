Abstract
Although the impressive performance in visual ground-ing, the prevailing approaches usually exploit the visual backbone in a passive way, i.e., the visual backbone ex-tracts features with fixed weights without expression-related hints. The passive perception may lead to mismatches (e.g., redundant and missing), limiting further performance im-provement.
Ideally, the visual backbone should actively extract visual features since the expressions already pro-vide the blueprint of desired visual features. The active perception can take expressions as priors to extract rel-evant visual features, which can effectively alleviate the
Inspired by this, we propose an active per-mismatches. ception Visual Grounding framework based on Language
Adaptive Weights, called VG-LAW. The visual backbone serves as an expression-specific feature extractor through dynamic weights generated for various expressions. Ben-efiting from the specific and relevant visual features ex-tracted from the language-aware visual backbone, VG-LAW does not require additional modules for cross-modal inter-action. Along with a neat multi-task head, VG-LAW can be competent in referring expression comprehension and seg-mentation jointly. Extensive experiments on four represen-tative datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame, validate the effectiveness of the proposed framework and demonstrate state-of-the-art performance. 1.

Introduction
Visual grounding (such as referring expression compre-hension [4, 23, 42, 45, 46, 48, 50], referring expression seg-mentation [6, 14, 17, 23, 32, 33, 44], and phrase grounding
[4, 23, 50]) aims to detect or segment the specific object
*corresponding author.
Figure 1. The comparison of visual grounding frameworks. (a)
The visual and linguistic backbone independently extracts fea-tures, which are fused through cross-modal interaction. (b) Ad-ditional designed modules are inserted into the visual backbone to modulate visual features using linguistic features. (c) VG-LAW can generate language-adaptive weights for the visual backbone and directly output referred objects through our designed multi-task head without additional cross-modal interaction modules. based on a given natural language description. Compared to general object detection [38] or instance segmentation [11], which can only locate objects within a predefined and fixed category set, visual grounding is more flexible and purpose-ful. Free-formed language descriptions can specify specific visual properties of the target object, such as categories, at-tributes, relationships with other objects, relative/absolute positions, and etc.
Due to the similarity with detection tasks, previous vi-sual grounding approaches [23, 33, 46, 50] usually follow the general object detection frameworks [1, 11, 37], and pay
Figure 2. Attention visualization of the visual backbone with dif-ferent weights. (a) input image, (b) visual backbone with fixed weights, (c) and (d) visual backbone with weights generated for
“white bird” and “right bird”, respectively. attention to the design of cross-modal interaction modules.
Despite achieving impressive performance, the visual back-bone is not well explored. Concretely, the visual backbone passively extracts visual features with fixed architecture and weights, regardless of the referring expressions, as illus-trated in Fig. 1 (a). Such passive feature extraction may lead to mismatches between the extracted visual features and those required for various referring expressions, such as missing or redundant features. Taking Fig. 2 as an exam-ple, the fixed visual backbone has an inherent preference for the image, as shown in Fig. 2 (b), which may be irrelevant to the referring expression “white bird”. Ideally, the visual backbone should take full advantage of expressions, as the expressions can provide information and tendencies about the desired visual features.
Several methods have noticed this phenomenon and pro-posed corresponding solutions, such as QRNet [45], and
LAVT [44]. Both methods achieve the expression-aware visual feature extraction by inserting carefully designed interaction modules (such as QD-ATT [45], and PWAN
[44]) into the visual backbone, as illustrated in Fig. 1 (b).
Concretely, visual features are first extracted and then ad-justed using QD-ATT (channel and spatial attention) or
PWAM (transformer-based pixel-word attention) in QR-Net and LAVT at the end of each stage, respectively. Al-though performance improvement with adjusted visual fea-tures, the extract-then-adjust paradigm inevitably contains a large number of feature-extraction components with fixed weights, e.g., the components belonging to the original visual backbone in QRNet and LAVT. Considering that the architecture and weights jointly determine the func-tion of the visual backbone, this paper adopts a simpler and fine-grained scheme that modifies the function of the visual backbone with language-adaptive weights, as illus-trated in Fig. 1 (c). Different from the extract-then-adjust paradigm used by QRNet and LAVT, the visual backbone equipped with language-adaptive weights can directly ex-tract expression-relevant visual features without additional feature-adjustment modules.
In this paper, we propose an active perception Visual
Grounding framework based on Language Adaptive
Weights, called VG-LAW. It can dynamically adjust the behavior of the visual backbone by injecting the informa-tion of referring expressions into the weights. Specifi-cally, VG-LAW first obtains the specific language-adaptive weights for the visual backbone through two successive processes of linguistic feature aggregation and weight gen-eration. Then, the language-aware visual backbone can extract expression-relevant visual features without manu-ally modifying the visual backbone architecture. Since the extracted visual features are highly expression-relevant, cross-modal interaction modules are not required for fur-ther cross-modal fusion, and the entire network architecture is more streamlined. Furthermore, based on the expression-relevant features, we propose a lightweight but neat multi-task prediction head for jointly referring expression com-prehension (REC) and referring expression segmentation (RES) tasks. Extensive experiments on RefCOCO [47],
RefCOCO+ [47], RefCOCOg [36], and ReferItGame [19] datasets demonstrate the effectiveness of our method, which achieves state-of-the-art performance.
The main contributions can be summarized as follows:
• We propose an active perception visual ground-ing framework based on the language adaptive weights, called VG-LAW, which can actively extract expression-relevant visual features without manually modifying the visual backbone architecture.
• Benefiting from the active perception of visual feature extraction, we can directly utilize our proposed neat but efficient multi-task head for REC and RES tasks jointly without carefully designed cross-modal inter-action modules.
• Extensive experiments demonstrate the effectiveness of our framework, which achieves state-of-the-art per-formance on four widely used datasets, i.e., RefCOCO,
RefCOCO+, RefCOCOg, and ReferItGame. 2.