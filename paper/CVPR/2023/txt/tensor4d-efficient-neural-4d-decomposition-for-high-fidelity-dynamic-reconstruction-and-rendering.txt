Abstract
We present Tensor4D, an efficient yet effective approach to dynamic scene modeling. The key of our solution is an efficient 4D tensor decomposition method so that the dy-namic scene can be directly represented as a 4D spatio-temporal tensor. To tackle the accompanying memory is-sue, we decompose the 4D tensor hierarchically by pro-jecting it first into three time-aware volumes and then nine compact feature planes.
In this way, spatial information over time can be simultaneously captured in a compact and memory-efficient manner. When applying Tensor4D for dy-namic scene reconstruction and rendering, we further fac-torize the 4D fields to different scales in the sense that struc-tural motions and dynamic detailed changes can be learned from coarse to fine. The effectiveness of our method is val-idated on both synthetic and real-world scenes. Extensive experiments show that our method is able to achieve high-quality dynamic reconstruction and rendering from sparse-view camera rigs or even a monocular camera. The code and dataset will be released at https://github.com/
DSaurus/Tensor4D. 1.

Introduction
High quality reconstruction and Photo-realistic render-ing of a dynamic scene from a set of input images is nec-essary for many applications such as AR/VR, 3D content production and entertainment. Traditional methods use classical mesh-based representation to reconstruct the dy-namic scenes, which, unfortunately, are prone to produce reconstruction errors and rendering artifacts when the scene contains thin structures, specular surfaces and topological changes [9, 19, 23, 26, 49].
Recent advances in neural rendering approaches, which learn scene representations in the form of neural radiance fields (NeRF), have shown impressive novel view syn-thesis of general static scenes given only multi-view im-ages [32]. They are immediately extended to dynamic scenes: some methods (e.g., NeRF-T) consider time as an additional input dimension to NeRF representation [56, 62], while other methods (e.g., D-NeRF) disentangle a dynamic scene into a canonical radiance field and a dynamic motion field [11,28,37,40,55]. Either way, learning a 4D function is one of the main cornerstones. Unfortunately, directly using
MLP to fit such a function often suffers from high time and computation cost, i.e., dozens of hours on high-end GPUs.
In fact, the aforementioned limitation also exists in con-ventional NeRF-based methods for static scenes, and re-searchers have proposed to use discrete data structures like voxel grids [65] or triplanes [7] to accelerate NeRF train-ing and rendering. However, these techniques are difficult to be extended to dynamic domains as introducing an addi-tional time dimension will exponentially increase memory footprint, hindering them from modeling high-quality ap-pearance details.
In this work, we pursue a dynamic scene representation that also utilizes explicit feature grids to accelerate network training while avoiding huge memory consumption when introducing an additional time dimension. To this end, we bypass the construction of a high resolution 4D tensor; in-stead, we propose to model a 4D field using hierarchical tri-projection decomposition. Our decomposition method extends the tri-projection in EG3D [7]. It firstly project a full 4D field into three time-aware volumes, each of which is then further decomposed into three feature planes.
In this way, we model the 4D field using only nine 2D feature planes, and we empirically find that although being highly compact, such a representation is powerful enough to rep-resent dynamic scenes containing complex motions. More-over, the usage of explicit data structure also allows us to design a coarse-to-fine strategy to further improve the per-formance of our method.
By utilizing and factorizing an explicit 4D tensor, our method enables both efficient reconstruction and compact representation of dynamic scenes. Besides, the decompo-sition scheme also introduces implicit constraints on the representation since only low-rank tensors can be approx-imated by a small number of lower-dimensional compo-nents. Such a constraint can serve as an inherent regulariza-tion when the input observation is limited, e.g., under sparse and fixed cameras setting or even monocular inputs. In this paper, we first apply our method for sparse-view dynamic reconstruction by adopting our Tensor4D decomposition to time-conditioned radiance fields in “NeRF-T”. In addition, our decomposition method can also be used for single-view dynamic reconstruction. This is achieved through decom-posing both the 4D dynamic motion field and the canonical radiance field in “D-NeRF”. With proper regularization, our system enables efficient and high-quality reconstruction of dynamic objects under both camera settings. 2.