Abstract
Self-driving vehicles rely on urban street maps for au-tonomous navigation. In this paper, we introduce Pix2Map, a method for inferring urban street map topology directly from ego-view images, as needed to continually update and expand existing maps. This is a challenging task, as we need to infer a complex urban road topology directly from raw image data. The main insight of this paper is that this problem can be posed as cross-modal retrieval by learning a joint, cross-modal embedding space for images and ex-isting maps, represented as discrete graphs that encode the topological layout of the visual surroundings. We conduct our experimental evaluation using the Argoverse dataset and show that it is indeed possible to accurately retrieve street maps corresponding to both seen and unseen roads solely from image data. Moreover, we show that our re-trieved maps can be used to update or expand existing maps and even show proof-of-concept results for visual localiza-tion and image retrieval from spatial graphs. 1.

Introduction
We propose Pix2Map, a method for inferring road maps directly from images. More precisely, given camera im-ages, Pix2Map generates a topological map of the visible surroundings, represented as a spatial graph. Such maps en-code both geometric and semantic scene information such as lane-level boundaries and locations of signs [52] and serve as powerful priors in virtually all autonomous vehi-cle stacks. In conjunction with on-the-fly sensory measure-ments from lidar or camera, such maps can be used for lo-calization [3] and path planning [39]. As map maintenance and expansion to novel areas are challenging and expen-sive, often requiring manual effort [38, 50], automated map maintenance and expansion has been gaining interest in the community [9, 10, 27, 32, 33, 35, 38, 40].
Why is it hard? To estimate urban street maps, we need to
*Work done while at Carnegie Mellon University.
Illustration of our proposed Pix2Map for cross-Figure 1. modal retrieval. Given unseen 360◦ ego-view images collected from seven ring cameras (left), our Pix2Map predicts the lo-cal street map by retrieving from the existing street map library (right), represented as an adjacency matrix. The local street maps can be further used for global high-definition map maintenance (top). learn to map continuous images from ring cameras to dis-crete graphs with varying numbers of nodes and topology in bird’s eye view (BEV). Prior works that estimate road topology from monocular images first process images using
Convolutional Neural Networks or Transformers to extract road lanes and markings [9] or road centerlines [10] from images. These are used in conjunction with recurrent neural networks for the generation of polygonal structures [12] or heuristic post-processing [33] to estimate a spatial graph in
BEV. This is a very difficult learning problem: such meth-ods need to jointly learn to estimate a non-linear mapping from image pixels to BEV, as well as to estimate the road layout and learn to generate a discrete spatial graph.
Pix2Map.
Instead, our core insight is to simply sidestep the problem of graph generation and 3D localization from monocular images by recasting Pix2Map as a cross-modal retrieval task: given a set of test-time ego-view images, we (i) compute their visual embedding and then (ii) retrieve a graph with the closest graph embedding in terms of co-sine similarity. Given recent multi-city autonomous vehi-cle datasets [13], it is straightforward to construct pairs of ego-view images and street maps, both for training and test-1
ing. We train image and graph encoders to operate in the same embedding space, making use of recent techniques for cross-modal contrastive learning [44]. Our key techni-cal contribution is a novel but simple graph encoder, based on sequential transformers from the language community (i.e., BERT [18]) that extract fixed-dimensional embeddings from street maps of arbitrary size and topology.
In fact, we find that even naive nearest-neighbor retrieval performs comparably to leading techniques for map gener-ation [9, 10], i.e., returning the graph paired with the best-matching image in the training set. Moreoever, we demon-strate that cross-modal retrieval via Pix2Map performs even better due to its ability to learn graph embeddings that reg-ularize the output space of graphs. In addition, cross-modal retrieval has the added benefit of allowing one to expand the retrieval graph library with unpaired graphs that lack camera data, leveraging the insight that retrieval need not be limited to the same (image, graph) training pairs used for learning the encoders. This suggests that Pix2Map can be further improved with augmented road graph topologies that capture potential road graph updates (for which paired visual data might not yet exist). Beyond mapping, we show pilot experiments for visual localization, and the inverse method, Map2Pix, which retrieves a close-matching image from an image library given a graph. While not our pri-mary focus, such approaches may be useful for generating photorealistic simulated worlds [40].
We summarize our main contributions as follows: We (i) show that dynamic street map construction from cam-eras can be posed as a cross-modal retrieval task and pro-pose an contrastive image-graph model based on this fram-ing. Building on recent advances in multimodal represen-tation learning, we train a graph encoder and an image en-coder with a shared latent space. We (ii) demonstrate em-pirically that this approach is effective and perform ablation studies to highlight the impacts of architectural decisions.
Our approach outperforms existing graph generation meth-ods from image cues by a large margin. We (iii) further show that it is possible to retrieve similar graphs to those in previously unseen areas without access to the ground truth graphs for those areas, and demonstrate the generalization ability to novel observations. 2.