Abstract
Dance is an important human art form, but creating new dances can be difficult and time-consuming.
In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise con-ditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality gener-ated by our method extensively through (1) multiple quanti-tative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website. 1.

Introduction
Dance is an important part of many cultures around the world: it is a form of expression, communication, and social interaction [29]. However, creating new dances or dance an-imations is uniquely difficult because dance movements are expressive and freeform, yet precisely structured by music.
In practice, this requires tedious hand animation or motion capture solutions, which can be expensive and impractical.
On the other hand, using computational methods to gener-ate dances automatically can alleviate the burden of the cre-ation process, leading to many applications: such methods can help animators create new dances or provide interac-tive characters in video games or virtual reality with real-istic and varied movements based on user-provided music.
In addition, dance generation can provide insights into the relationship between music and movement, which is an im-portant area of research in neuroscience [2].
Previous work has made significant progress using ma-chine learning-based methods, but has achieved limited suc-cess in generating dances from music that satisfy user con-straints. Furthermore, the evaluation of generated dances is subjective and complex, and existing papers often use quan-titative metrics that we show to be flawed.
In this work, we propose Editable Dance GEneration (EDGE), a state-of-the-art method for dance generation that creates realistic, physically-plausible dance motions based on input music. Our method uses a transformer-based dif-fusion model paired with Jukebox, a strong music feature extractor. This unique diffusion-based approach confers powerful editing capabilities well-suited to dance, includ-ing joint-wise conditioning and in-betweening. In addition to the advantages immediately conferred by the modeling choices, we observe flaws with previous metrics and pro-pose a new metric that captures the physical accuracy of ground contact behaviors without explicit physical model-ing. In summary, our contributions are the following:
1. We introduce EDGE, a diffusion-based approach for dance generation that combines state-of-the-art perfor-mance with powerful editing capabilities and is able to generate arbitrarily long sequences. EDGE im-proves on previous hand-crafted audio feature extrac-tion strategies by leveraging music audio representa-tions from Jukebox [5], a pre-trained generative model for music that has previously demonstrated strong per-formance on music-specific prediction tasks [3, 7]. 2. We analyze the metrics proposed in previous works and show that they do not accurately represent human-evaluated quality as reported by a large user study. 3. We propose a new approach to eliminating foot-sliding physical implausibilities in generated motions using a novel Contact Consistency Loss, and introduce Phys-ical Foot Contact Score, a simple new acceleration-based quantitative metric for scoring physical plausi-bility of generated kinematic motions that requires no explicit physical modeling.
This work is best enjoyed when accompanied by our demo samples. Please see the samples at our website. 2.