Abstract
Active learning selects informative samples for annota-tion within budget, which has proven efficient recently on object detection. However, the widely used active detection benchmarks conduct image-level evaluation, which is un-realistic in human workload estimation and biased towards crowded images. Furthermore, existing methods still per-form image-level annotation, but equally scoring all targets within the same image incurs waste of budget and redun-dant labels. Having revealed above problems and limita-tions, we introduce a box-level active detection framework that controls a box-based budget per cycle, prioritizes infor-mative targets and avoids redundancy for fair comparison and efficient application.
Under the proposed box-level setting, we devise a novel pipeline, namely Complementary Pseudo Active Strategy (ComPAS). It exploits both human annotations and the model intelligence in a complementary fashion: an efficient input-end committee queries labels for informative objects only; meantime well-learned targets are identified by the model and compensated with pseudo-labels. ComPAS con-sistently outperforms 10 competitors under 4 settings in a unified codebase. With supervision from labeled data only, it achieves 100% supervised performance of VOC0712 with merely 19% box annotations. On the COCO dataset, it yields up to 4.3% mAP improvement over the second-best method. ComPAS also supports training with the unlabeled pool, where it surpasses 90% COCO supervised perfor-mance with 85% label reduction. Our source code is pub-licly available at https://github.com/lyumengyao/blad. 1.

Introduction
Reducing the dependency on large-scale and well-annotated datasets for deep neural networks has received
*Corresponding Authors.
Figure 1. Active detection methods evaluated on VOC0712 un-der image-level (Left) and box-level (Right) settings. BoxCnt is our hack that simply queries potentially the most crowded images, which demonstrates that image-level evaluation is highly biased.
Methods marked with * have specialized detector architectures. a growing interest in recent years, especially for the detec-tion task, where the box-level annotation is highly demand-ing. Among data-efficient training schemes, active detec-tion methods [1, 4, 6, 7, 13, 20, 24, 35] iterate over detec-tor training, performance evaluation, informative image ac-quisition and human annotation. Despite recent progress, previous pool-based active detection methods still consider the subject of interest at the image-level: they conduct image-level evaluation, where the budget is controlled by the number of labeled images per cycle; afterwards, they perform exhaustive image-level annotation, where all in-stances of the same image are labeled. Such an image-level framework suffers from unfairness in model performance comparison and leads to a waste of annotation resources.
On the one hand, existing methods under the image-level evaluation assume equal budget for every image. How-ever, in real-world use cases, the workload of annotators
is measured by bounding boxes [10,23]. As the image-level budget fails to reflect actual box-based costs, active detec-tion methods are allowed to obscurely gain an advantage by querying box supervision as much as possible until the image-based budget is run out. In fact, according to our ex-periment shown in Fig. 1L, naively sampling potentially the most crowded images (dubbed as “BoxCnt”) can surpass all elaborately designed methods, demonstrating the unfair-ness of image-level evaluation. On the other hand, during human annotation, simply performing image-level exhaus-tive annotation is wasteful, since the informativeness of dif-ferent targets involved in the same image can vary sharply.
For example, a salient target of a common category might have been well-learned, whereas a distant or occluded vari-ant could be more informative. As a result, annotating all instances amongst the same image as equals leads to a waste of resources and redundant annotations (See Fig. 4).
After revealing the above problems and limitations, we propose a new box-level active detection framework to-wards fair comparison and non-redundant human annota-tion. For evaluation, our framework includes a more prac-tical and unbiased criterion that controls the amount of queried boxes per cycle, enabling competing methods to be assessed directly within realistic box-based budgets (as il-lustrated in Fig. 1R). Considering the annotation, we advo-cate a box-level protocol that prioritizes top-ranked targets for annotation and discards well-learned counterparts to avoid redundancy. Under the proposed framework, we de-velop a novel and efficient method namely Complementary
Pseudo Active Strategy (ComPAS). It seamlessly integrates human efforts with model intelligence in actively acquiring informative targets via an input-end committee, and mean-time remedying the annotation of well-learned counterparts using online pseudo-labels.
In consideration of the active acquisition, concentrating resources on the most informative targets makes box-level informativeness estimation crucial. Among active learn-ing strategies, multi-model methods, such as Ensemble [2] and MCDropout [9], have demonstrated superiority. Built upon a model-end ensemble, query-by-committee methods select the most controversial candidates based on the voting of model members to minimize the version space [26, 27].
However, directly adapting them to detection not only mul-tiplies the computational cost in the committee construc-tion, but complicates the detection hypothesis ensemble on the box-level. Therefore, to harness the power of diver-sity without a heavy computational burden, orthogonal to model-end ensembles, we construct an input-end committee during the sampling stage. Variations are drawn from ubiq-uitous data augmentations and applied to unlabeled candi-dates, among which each perturbation can be considered as a cheap but effective committee member towards ver-sion space minimization. When it comes to the box-level hypothesis ensemble, instead of performing pair-wise label assignment among all members [24], we reduce the ensem-ble burden by analyzing the disagreement between predic-tions of a reliable reference and other members. Then the disagreement is quantified for both classification and local-ization to exploit the rich information in annotations.
Later during box-level annotation, the oracle only yields labels for challenging, controversial targets, leaving consis-tent ones unlabeled. Those unlabeled targets would be con-sidered as the background class during the following train-ing cycles, which severely harms the performance and poses a new challenge. To compensate well-learned targets for missing annotation, we combine sparse ground truths with online pseudo-label generation, where in contrast to active sampling, confident model predictions are accepted as self-supervision signals. The proposed box-level pipeline sup-ports both labeled-only and mixed-supervision learning set-tings w/ or w/o the unlabeled image pool involved during training, which makes a fairer comparison with fully- and semi-supervised state-of-the-arts (SOTAs).
Our contributions can be summarized as follows:
• We propose a box-level active detection framework, where we control box-based budgets for realistic and fair evaluation, and concentrate annotation resources on the most informative targets to avoid redundancy.
• We develop ComPAS, a novel method that seamlessly integrates model intelligence into human efforts via an input-end committee for challenging target annotation and pseudo-labeling for well-learned counterparts.
• We provide a unified codebase with implementations of active detection baselines and SOTAs, under which the superiority of ComPAS is demonstrated via exten-sive experiments. 2.