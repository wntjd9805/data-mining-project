Abstract
In a noisy conversation environment such as a dinner party, people often exhibit selective auditory attention, or the ability to focus on a particular speaker while tuning out others. Recognizing who somebody is listening to in a con-versation is essential for developing technologies that can understand social behavior and devices that can augment human hearing by amplifying particular sound sources.
The computer vision and audio research communities have made great strides towards recognizing sound sources and speakers in scenes.
In this work, we take a step further by focusing on the problem of localizing auditory attention targets in egocentric video, or detecting who in a camera wearer’s field of view they are listening to. To tackle the new and challenging Selective Auditory Attention Localiza-tion problem, we propose an end-to-end deep learning ap-proach that uses egocentric video and multichannel audio to predict the heatmap of the camera wearer’s auditory at-tention. Our approach leverages spatiotemporal audiovi-sual features and holistic reasoning about the scene to make predictions, and outperforms a set of baselines on a chal-lenging multi-speaker conversation dataset. Project page: https://fkryan.github.io/saal 1.

Introduction
One of the primary goals of wearable computing devices like augmented reality (AR) glasses is to enhance human perceptual and cognitive capabilities. This includes help-ing people have natural conversations in settings with high noise level (e.g. coffee shops, restaurants, etc.) by selec-tively amplifying certain speakers while suppressing noise and the voices of background speakers. This desired ef-fect mirrors selective auditory attention (SAA), or humans’ ability to intentionally focus on certain sounds while tuning out others. People exercise SAA in everyday conversational settings; at restaurants people tune out the voices at adja-cent tables, and in group social settings, such as dining at
*Work done during internship at Meta.
Figure 1. We address the novel task of Selective Auditory Atten-tion Localization in multi-speaker environments: given egocentric video and multichannel audio, we predict which people, if any, the camera wearer is listening to. We show our model’s predicted heatmaps, with red bounding boxes denoting ground truth auditory attention targets and white boxes denoting non-attended speakers. a large table or socializing at a party, people often engage in conversations with smaller subsets of people while oth-ers converse in close proximity. Being able to determine which speaker(s) a person is selectively listening to is im-portant for developing systems that can aid communication in noisy environments and assist people with hearing loss.
While the computer vision community has made strides towards understanding conversational dynamics with large-scale datasets like Ego4D [37], AVA-ActiveSpeaker [78],
VoxConverse [24], and AMI [57], the problem of model-ing selective listening behavior has not yet been addressed.
In fact, determining auditory attention among competing sound signals has traditionally been approached using neu-rophysiological sensing [11, 50]. These approaches involve a controlled listening task, where competing audio signals
are played simultaneously and a listener selectively listens to one. Statistical models are then used to correlate brain activity and the attended sound signal. However, the sens-ing approach is obtrusive and currently not feasible for use in realistic conversational settings in daily life.
In this work, we approach modeling SAA from an ego-centric audiovisual perspective. The egocentric setting pro-vides a compelling lens to study social conversational dy-namics, as it captures both the audio and visual stimuli present in a scene and how the camera wearer orients their view in response to them. We hypothesize that the behav-ior of the wearer that is implicitly captured in the egocentric video and multichannel audio can facilitate the prediction of auditory attention targets. To this end, we introduce and for-mulate the novel task of Selective Auditory Attention Lo-calization (SAAL), which uses egocentric video and multi-channel audio to localize the target of auditory attention in egocentric video. We specifically target multi-speaker con-versation scenarios where the camera wearer must selec-tively attend to certain speaker(s) while tuning out others.
This challenging setting is representative of everyday noisy conversation environments and highlights the complexity of modeling auditory attention in naturalistic settings.
We propose a deep audiovisual video network for SAAL that leverages both appearance-based features from the ego-centric video stream and spatial audio information from multichannel audio. Our key insight is to extract a spa-tiotemporal feature representation from each modality and use a transformer on the fused features to reason holistically about the scene. Our contributions are:
• We introduce the novel problem of Selective Auditory
Attention Localization (SAAL) as an egocentric multi-modal learning task.
• We propose a new architecture for SAAL. Our model extracts spatiotemporal video and audio features and uses a transformer to refine selection of an attention target by reasoning globally about the scene.
• We evaluate our approach on a challenging multi-speaker conversation dataset and demonstrate our model’s superiority over intuitive baselines and ap-proaches based on Active Speaker Localization (ASL).
• We conduct thorough experiments to give insight into our multimodal architecture design, effective multi-channel audio and visual input representations, and the relationship between SAAL and ASL. 2.