Abstract 1.

Introduction
Sign language (SL) is the primary method of communica-tion for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool.
Replacing these with 3D avatars can aid learning and en-able AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are univer-sally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs.
Our method, SGNify, captures fine-grained hand pose, fa-cial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture sys-tem to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose-and shape-estimation methods on SL videos. A perceptual study shows that SGNify’s 3D reconstructions are signifi-cantly more comprehensible and natural than those of pre-vious methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de.
It is estimated that over 466 million people have dis-abling hearing loss [13] and more than 70 million people use sign language (SL) as their primary means of commu-nication [52]. Increasing use of digital communication mo-tivates research on capturing, understanding, modeling, and synthesizing expressive 3D SL avatars. Existing datasets and dictionaries used in SL recognition (SLR), translation (SLT), and production (SLP) are primarily limited to 2D video because the technology required to capture 3D move-ment is prohibitively expensive, requires expertise to oper-ate, and may limit the movements of the signer. Dictionar-ies of isolated signs are a core SL learning tool, and many
SLs have online 2D video dictionaries. The Deaf commu-nity is actively seeking 3D dictionaries of isolated signs to aid learning [40]. The current approach to creating such 3D signing dictionaries is fully manual, requiring an artist or a HamNoSys [21] expert, and the resulting avatars often move unnaturally [3]. We aim to automatically reconstruct expressive 3D signing avatars from monocular SL video, which we term Sign Language Capture (SLC). We focus on
SLC of isolated signs. 3D reconstruction of human pose and shape has received significant attention, but accurate 3D hand-pose estimation
remains challenging from in-the-wild video. Challenges include the high number of degrees of freedom present in hands [5], frequent occurrence of self-contact and self-occlusions [37, 46], low resolution, and motion blur caused by fast motions [50] that cause hand pose to be unrecogniz-able in many frames (see Fig. 1). To address these issues, we exploit the linguistic nature of SL itself to develop novel priors that help disambiguate hand poses in SL videos, lead-ing to accurate 3D reconstructions. This is a novel use of linguistic “side information” to improve 3D reconstruction.
Based on hand movements and poses, Battison [4] de-fines five linguistic classes that contain all SL signs. We build on that work to define eight classes and formalize these as mathematical priors on 3D hand shape. We com-bine Battison’s first two classes and place all one-handed signs in class 0, while two-handed signs are arranged in classes 1, 2, or 3, depending on how the non-dominant hand participates in the articulation of the sign. We then divide each of these four classes into two subclasses depending on whether the pose of the active hand(s) changes during the articulation of the sign. We introduce two class-dependent
SL linguistic constraints that capture 1) symmetry and 2) hand-pose invariance. Under Battison’s SL symmetry con-dition [4], when both hands actively move, the articulation of the fingers must be identical; the same is true for one class of two-handed signs in which only the dominant hand moves. We formalize this concept as a regularization term that encourages the pose of the two hands to be similar for such signs. Coupling the hand poses in this way effectively increases the image evidence for a pose, which improves estimates for challenging videos. Our invariance constraint uses the observation that hand pose is either static or transi-tions smoothly from one pose to another during the articu-lation of the sign; other significant changes to hand pose are not common in SL. Specifically, we extract a characteris-tic “reference pose sequence” (RPS) to describe each local hand pose during the sign articulation, and we penalize dif-ferences between the RPS and the estimated hand pose in each frame. These two priors of symmetry and hand-pose invariance are universally applicable to all sign languages.
The hands alone, however, are not sufficient to accu-rately reproduce SL. Information is conveyed holistically in SL through hand gestures, facial expressions, and upper-body movements in 3D space. To combine these, we use a 3D whole-body model, SMPL-X [41], that jointly models this information (see Fig. 1).
Our novel hand-pose constraints are formulated to be incorporated into the loss function for training a neu-ral network regressor or into the objective function of optimization-based methods.
In general, optimization-based methods are more computationally intensive but pro-duce more accurate results when limited training data is available, so we take this approach here and build on the
SMPLify-X method [41]. To appropriately incorporate our terms into the objective function, we need to know the class of the sign. We train a simple model that extracts features from the raw video and determines the class to which the depicted sign belongs. While SMPLify-X is a good foun-dation for the hands and body, we find that it does not cap-ture expressive facial motions well. Consequently, we use a more expressive face regressor, SPECTRE [19], to capture the face parameters. We call our method SGNify.
To quantitatively evaluate SGNify, we capture a native
German (DGS) signer with a frontal RGB camera synchro-nized with a 54-camera Vicon motion capture system and recover ground-truth meshes from the Vicon markers [34].
We run SGNify on the RGB video and compute 3D vertex-to-vertex (V2V) error between our resulting avatars and the ground-truth meshes. We find that SGNify reconstructs
SMPL-X meshes more accurately than the competition.
We conduct a perceptual evaluation in which we present proficient signers with a video of either an estimated
SMPL-X avatar or the real-person source video and task them with identifying the sign being performed. Partici-pants also rate their ease in recognizing the sign and the nat-uralness of the articulation. Our results show that SGNify reconstructs 3D signs that are as recognizable as the original videos and consistently more recognizable, easier to under-stand, and more natural than the existing state of the art. We also evaluate SGNify in a multi-view setting and on contin-uous signing videos. Despite not being designed for the latter, SGNify captures the meaning in continuous SL.
SGNify represents a step towards the automatic recon-struction of natural 3D avatars from sign-language videos.
Our key contribution is the introduction of novel linguistic priors that are universal and helpful to constrain the problem of hand-pose estimation from SL video. SGNify is designed to work on video from different SL dictionaries across lan-guages, backgrounds, people, trimming, image resolution, and framing, as visible in Sup. Mat. and in the video on our project page. This capability is critical to capture 3D signing at scale, which will enable progress on learning SL avatars. Our code and motion-capture dataset are available for research purposes at sgnify.is.tue.mpg.de. 2.