Abstract 1.

Introduction
Salient object detection (SOD) aims to mimic the human visual system (HVS) and cognition mechanisms to identify and segment salient objects. However, due to the complex-ity of these mechanisms, current methods are not perfect.
Accuracy and robustness need to be further improved, par-ticularly in complex scenes with multiple objects and back-ground clutter. To address this issue, we propose a novel approach called Multiple Enhancement Network (MENet) that adopts the boundary sensibility, content integrity, it-erative refinement, and frequency decomposition mecha-nisms of HVS. A multi-level hybrid loss is firstly designed to guide the network to learn pixel-level, region-level, and object-level features. A flexible multiscale feature enhance-ment module (ME-Module) is then designed to gradually aggregate and refine global or detailed features by chang-ing the size order of the input feature sequence. An iter-ative training strategy is used to enhance boundary fea-tures and adaptive features in the dual-branch decoder of
MENet. Comprehensive evaluations on six challenging benchmark datasets show that MENet achieves state-of-the-art results. Both the codes and results are publicly available at https://github.com/yiwangtz/MENet.
*The corresponding authors
Salient object detection (SOD) aims to identify the most visually conspicuous regions in an image that are consis-tent with the human visual system (HVS) and cognition mechanisms [9,13,40]. SOD can eliminate redundant infor-mation and improve computational performance for many high-level computer vision tasks, such as action recognition
[4, 60], image segmentation [3, 33], image captioning [52], object tracking [14], and video summary [58]. Fully con-volutional networks (FCNs) [25] based SOD models have been particularly effective at improving SOD performance in recent years [40]. However, accurately segmenting com-plex object boundaries remains a challenging task for SOD.
This is especially true when the geometry and/or boundaries of these objects are complex, or when scenes are chaotic or cluttered [9, 10], as shown in Fig. 1.
An intuitive solution for addressing this problem is to explore the mechanisms of the human vision system (HVS)
[55] and some of which have been used to improve SOD models, as described below. (i) A human tends to enhance recognition by alternating between viewing the entire object and the details of complex scenes, which has been utilized for various visual tasks [16, 17, 22, 36]. (ii) HVS is sen-sitive to both boundary/contour and structural information, so dual-branch feature refinement structures have been de-veloped to incorporate extra-edge information to enhance salient feature learning [11, 22, 24, 43, 47, 53, 57]. Some
geometrically complex objects.
Then, we propose an iterative training strategy to pro-gressively enhance features by alternately aggregating high-and low-level features to mimic HVS bottom-up and top-down refinement mechanisms. To produce high- and low-level features flexibly, we design a multiscale feature en-hancement module (ME-Module) as the core of each branch by leveraging atrous spatial pyramid pooling (ASPP) [34] and global-local attention [6].
In addition, we introduce the HVS holistic and continu-ous mechanism to loss function design. We present a multi-level hybrid loss, which evaluates the pixel-, region-, and object-level similarities between predicted saliency maps and ground-truth (GT) saliency maps. For pixel-level loss, we also use Binary Cross Entropy (BCE) [7] loss to ensure network accuracy and convergence speed. As for region-level loss, we divide a saliency map into four sub-regions of equal size and then calculate the sum of weighted re-gional similarities through SSIM and IoU. Then, inspired by SSIM and S-measure [5], an object-level loss is designed by the contrast and distribution statistics of the foreground between the GT map and the predicted map. A similar hy-brid loss is reported in BASNet [32], but it uses a simple combination of BCE, IoU, and SSIM for the whole saliency map without partitioning regions. Following is a summary of our contributions.
• We propose to leverage not only pixel-level but also region-level and object-level similarity measures in loss to increase prediction accuracy and integrity, and then design a multi-level hybrid loss to implement this proposal.
• We design a multiscale feature enhancement module (ME-Module) to mimic HVS bottom-up and top-down refinement mechanisms. ME-Module can gradually propagate and produce comprehensive global or de-tailed features by changing the size and order of the input features.
• We propose a novel Multiple Enhancement Network (MENet) for dealing with SOD in complex scenes by integrating multiple HVS mechanisms into the net-work structure and loss function. Specifically, a two-branch decoder equipped with ME-Modules is de-signed to incrementally refine the boundary and adap-tive features by an iterative training strategy and the proposed multilevel hybrid loss.
The results of quantitative and qualitative experiments on six datasets demonstrate MENet outperforms the state-of-the-art methods by a large margin, as shown in Fig. 1.
Figure 1. Illustration of MAE (left part) and some visual results (right part) for the proposed MENet with some recent state-of-the-art SOD methods: EDN [45], AADFNet [57], SAC [16], and
ICON [59]. Please refer to Sec.5 for detailed experimental set-tings. The MENet model achieves the lowest MAE score with the most precise and complete boundaries. structural similarity measurements (e.g., Structural Simi-larity Index (SSIM) [41]) and regional similarity measure-ments (e.g., Intersection over Union (IoU) [35] and Dice
[12]) are also adopted by SOD models [32, 42, 43, 47, 59] in the loss functions. (iii) Human vision is indeed holistic and continuous so that it perceives objects and scenes as or-ganized wholes [18], which are composed of parts that are meaningful and coherent in relation to each other. ICON
[59] proposes to improve the integrity from both macro-and micro-level perspectives by enhancing integrity infor-mation hidden in channels of features. EDN [45] employs a powerful down-sampling technique to learn a global view of the whole image effectively. (iv) According to the hu-man visual spatial frequency model [30], an image can be decomposed into or synthesized by high-spatial frequency and low-spatial frequency parts. As a starting point in this work, we intend to use the mechanisms outlined above to further improve SOD performance for complex scenes.
In this work, we propose a multi-enhancement network (MENet) that effectively integrates the above HVS mecha-nisms in a U-Net-like [37] encoder-decoder framework to produce more accurate SOD for complex scenes. Foremost,
MENet employs the image frequency decomposition idea to design a two-stream feature learning decoder for boundaries (high frequencies) and inner body regions (low frequen-cies). This setting is different from the existing two-branch (or edge-aware) methods [11, 22, 47, 51, 53, 56, 57] that use one branch for the boundary and the other one for the entire object, such as EGNet [53] and AFNet [11]. Particularly, there is no interaction between the intermediate features of the two branches of MENet, so it reduces the interference of inaccurate boundary information with global features. This is because boundary features need to be highly discrimi-native against the background, while global features need consistency and robustness. Although LDF [43] also learns internal regional features in one branch, its detailed map and body map cannot be computed accurately and efficiently for
Figure 2. Illustration of the overall architecture and the pipeline of the MENet. 2.