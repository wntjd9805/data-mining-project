Abstract
In semi-supervised medical image segmentation, there exist empirical mismatch problems between labeled and un-labeled data distribution. The knowledge learned from the labeled data may be largely discarded if treating labeled and unlabeled data separately or in an inconsistent man-ner. We propose a straightforward method for alleviating the problem − copy-pasting labeled and unlabeled data bidirectionally, in a simple Mean Teacher architecture. The method encourages unlabeled data to learn comprehensive common semantics from the labeled data in both inward and outward directions. More importantly, the consistent learn-ing procedure for labeled and unlabeled data can largely reduce the empirical distribution gap. In detail, we copy-paste a random crop from a labeled image (foreground) onto an unlabeled image (background) and an unlabeled image (foreground) onto a labeled image (background), re-spectively. The two mixed images are fed into a Student network and supervised by the mixed supervisory signals of pseudo-labels and ground-truth. We reveal that the sim-ple mechanism of copy-pasting bidirectionally between la-beled and unlabeled data is good enough and the experi-ments show solid gains (e.g., over 21% Dice improvement on ACDC dataset with 5% labeled data) compared with other state-of-the-arts on various semi-supervised medical image segmentation datasets. Code is avaiable at https:
//github.com/DeepMed-Lab-ECNU/BCP. 1.

Introduction
Segmenting internal structures from medical images such as computed tomography (CT) or magnetic resonance imaging (MRI) is essential for many clinical applications
[34]. Various techniques based on supervised learning for medical image segmentation have been proposed [4,13,45], which usually requires a large amount of labeled data. But, due to the tedious and expensive manual contouring process when labeling medical images, semi-supervised segmenta-*Corresponding Author.
Figure 1.
Illustration of the mismatch problem under semi-supervised leaning setting. Assume the training set is drawn from a latent distribution in (a). But the empirical distributions of small amount of labeled data and a large amount of unlabeled data are (b) and (c), respectively. It’s hard to use few labeled data to con-struct the precise distribution of the whole dataset. (d) By using our BCP, the empirical distributions of labeled and unlabeled fea-tures are aligned. (e) But other methods such as SSNet [35] or cross unlabeled data copy-paste cannot address the empirical dis-tribution mismatch issue. All distributions are kernel density esti-mations of voxels belonging to myocardium class in ACDC [2]. tion attracts more attention in recent years, and has become ubiquitous in the field of medical image analysis.
Generally speaking, in semi-supervised medical image segmentation, the labeled and unlabeled data are drawn from the same distribution, (Fig. 1 (a)). But in real-world scenario, it’s hard to estimate the precise distribution from labeled data because they are few in number. Thus, there always exists empirical distribution mismatch between a large amount of unlabeled and a very small amount of la-beled data [30] (Fig. 1(b) and (c)). Semi-supervised seg-mentation methods always attempt to train labeled and un-labeled data symmetrically, in a consistent manner. E.g., self-training [1,48] generates pseudo-labels to supervise un-labeled data in a pseudo-supervised manner. Mean Teacher based methods [40] adopt consistency loss to “supervise” unlabeled data with strong augmentations, in analogy with supervising labeled data with ground-truth. DTC [16] proposed a dual-task-consistency framework, applicable to
age (background). The Student network is supervised by the generated supervisory signal via bidirectional copy-pasting between the pseudo-labels of the unlabeled images from the Teacher network and the label maps of the la-beled image. The two mixed images help the network to learn common semantics between the labeled and unlabeled data bidirectionally and symmetrically. We compute the
Dice scores for labeled and unlabeled training set from
LA dataset [39] based on models trained by state-of-the-arts and our method, as shown in Fig. 2. Previous models which process labeled data and unlabeled data separately present strong performance gap between labeled and unla-beled data. E.g., MC-Net obtains 95.59% Dice for labeled data but only 87.63% for unlabeled data. It means previous models absorb knowledge from ground-truth well, but dis-card a lot when transferring to unlabeled data. Our method can largely decrease the gap between labeled and unlabeled data (Fig. 1(d)) in terms of their performances. It is also in-teresting to observe that Dice for labeled data of our BCP is lower than other methods, implying that BCP can mitigate the over-fitting problem to some extent.
We verify BCP in three popular datasets: LA [39],
Pancreas-NIH [21], and ACDC [2] datasets. Extensive ex-periments show our simple method outperforms all state-of-the-arts by a large margin, with even over 21% improve-ment in Dice on ACDC dataset with 5% labeled data. Abla-tion study further shows the effectiveness of each proposed module. Note that compared with the baseline e.g., VNet or UNet, our method does not introduce new parameters for training, while remaining the same computational cost. 2.