Abstract
Recently, the robustness of deep neural networks has drawn extensive attention due to the potential distribution shift between training and testing data (e.g., deep mod-els trained on high-quality images are sensitive to corrup-tion during testing). Many researchers attempt to make the model learn invariant representations from multiple corrupted data through data augmentation or image-pair-based feature distillation to improve the robustness.
In-spired by sparse representation in image restoration, we opt to address this issue by learning image-quality-independent feature representation in a simple plug-and-play manner, that is, to introduce discrete vector quantization (VQ) to re-move redundancy in recognition models. Specifically, we first add a codebook module to the network to quantize deep features. Then we concatenate them and design a self-attention module to enhance the representation. During training, we enforce the quantization of features from clean and corrupted images in the same discrete embedding space so that an invariant quality-independent feature representa-tion can be learned to improve the recognition robustness of low-quality images. Qualitative and quantitative experi-mental results show that our method achieved this goal ef-fectively, leading to a new state-of-the-art result of 43.1 % mCE on ImageNet-C with ResNet50 as the backbone. On other robustness benchmark datasets, such as ImageNet-R, our method also has an accuracy improvement of almost 2%. The source code is available at https://see. xidian.edu.cn/faculty/wsdong/Projects/
VQSA.htm 1.

Introduction
The past few years have witnessed the remarkable devel-opment of deep convolutional neural networks (DCNNs) in many recognition tasks, such as classification [9,20,31,46],
*Corresponding author.
Figure 1. The Grad-CAM [45] maps of different models on defo-cus blur images. (a) The clean images. (b) The maps of vanilla
ResNet50 [20] model on clean images. (c) and (d) show the maps of QualNet50 [29] and our proposed method on defocus blur im-ages with severity level 3. The results show that our method still can focus on the salient object area without being seriously af-fected by corruption. Best viewed in color. detection [43, 48] and segmentation [5, 19]. Although its performance has exceeded that of humans in some datasets, its robustness still lags behind [10, 15]. Many re-searchers [11, 21, 22, 53] have shown that the performance of deep models trained in high-quality data decreases dra-matically with low-quality data encountered during deploy-ment, which usually contain common corruptions, includ-ing blur, noise, and weather influence. For example, the vanilla ResNet50 model has an accuracy of 76 % in the clean ImageNet validation set, but its average accuracy is less than 30 % in the same dataset contaminated by Gaus-sian noise.
As shown in Fig. 1, the model’s attention map is seri-ously affected by image quality. That is, the model pays
close attention to an incomplete or incorrect area of the de-focus blur image, which results in a wrong prediction. Deep degradation prior (DDP) [55] also reveals that deep repre-sentation space degradation is the reason for the decrease in model performance, and finding a mapping between de-graded and clean features will certainly benefit downstream classification or detection tasks.
Generally, a simple and effective method is to consider these common corruptions and to use low-quality simulated images for augmented (fine-tuning) training. However, as described in [29], this method does not explicitly map low-quality features to high-quality features; instead, the model tends to learn the average distribution among corruptions, resulting in limited performance improvement. In addition, when the type of corruption is unknown during training, this approach is heavily based on well-designed augmentation strategies to improve generalization and robustness.
To enhance the feature representation of low-quality im-ages, another feasible solution is to use pairs of clean and degraded images for training and align the degraded fea-tures with the corresponding clean ones. Both DDP [55] and QualNet [29] use high-quality features extracted from clean images as supervision and improve low-quality fea-tures through a distillation-like approach [1, 25, 26, 58]. Al-though they learn the mapping relationship between low-and high-quality feature representation, paired images are time-consuming for training, and some irrelevant features that are not useful for recognition are also forcibly aligned, such as background, color, lighting, and other features which are affected by the image quality.
The vector quantization (VQ) process is essentially a special case in sparse representation, that is, the represen-tation coefficient is a one-hot vector [50]. Similarly to the application of sparse representation in image restoration
[13, 59], the compression-reconstruction learning frame-work of VQ is also conducive to the removal of noisy redun-dant information and learning essential features for recog-nition. Inspired by this, we propose to use VQ to bridge the gap between low- and high-quality features and learn a quality-independent representation. Specifically, we first add a vector quantizer (codebook) to the recognition model.
During the training process, due to the codebook update mechanism, both low- and high-quality features will be as-signed to the same discrete embedding space. Subsequently, since direct hard quantization (selecting and replacing) may lose some useful information, we choose to concatenate the quantized features with the original ones and use a self-attention module to further enhance the quality-independent representation and weaken irrelevant features. In summary, the main contributions of this paper are listed below.
• To the best of our knowledge, it is the first time that we propose to introduce vector quantization into the recognition model for learning quality-independent feature representation and improving the models’ ro-bustness on common corruptions.
• We concatenate the quantized feature vector with the original one and use the self-attention module to en-hance the quality-independent feature representation instead of direct replacement in the standard vector quantization method.
• Extensive experimental results show that our method has achieved higher accuracy on benchmark low-quality datasets than several current state-of-the-art methods. 2.