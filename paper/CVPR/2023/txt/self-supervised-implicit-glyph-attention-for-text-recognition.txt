Abstract
The attention mechanism has become the de facto mod-ule in scene text recognition (STR) methods, due to its capa-bility of extracting character-level representations. These methods can be summarized into implicit attention based and supervised attention based, depended on how the at-tention is computed, i.e., implicit attention and supervised attention are learned from sequence-level text annotations and or character-level bounding box annotations, respec-tively. Implicit attention, as it may extract coarse or even incorrect spatial regions as character attention, is prone to suffering from an alignment-drifted issue. Supervised at-tention can alleviate the above issue, but it is character category-speciﬁc, which requires extra laborious character-level bounding box annotations and would be memory-intensive when handling languages with larger character categories. To address the aforementioned issues, we pro-pose a novel attention mechanism for STR, self-supervised implicit glyph attention (SIGA). SIGA delineates the glyph structures of text images by jointly self-supervised text seg-mentation and implicit attention alignment, which serve as the supervision to improve attention correctness with-out extra character-level annotations. Experimental results demonstrate that SIGA performs consistently and signiﬁ-cantly better than previous attention-based STR methods, in terms of both attention correctness and ﬁnal recognition performance on publicly available context benchmarks and our contributed contextless benchmarks. 1.

Introduction
Scene text recognition (STR) aims to recognize texts from natural images, which has wide applications in hand-writing recognition [36, 47, 53], industrial print recogni-tion [12, 16, 32], and visual understanding [8, 23, 31].
Recently, attention-based models with encoder-decoder ar-chitectures are typically developed to address this task by attending to important regions of text images to extract
*Corresponding author.
"COPTER"
"COPTER"
"COPTER"
STR images 1 3 5
Category-independent  channels 2 4 6
STR images 5 3 16 15 18 20
Category-dependent  channels
Alignment
Unsupervised  method
STR images 1 3 5
Category-independent  channels 2 4 6 (a) Implicit attention (b) Supervised attention (c) Self-supervised implicit glyph attention (SIGA)  
Figure 1. Three difference supervised manners for STR. character-level representations. These methods can be sum-marized into implicit attention methods (a) and supervised attention methods (b) as shown in Figure 1, according to the annotation type used for supervising the attention.
Speciﬁcally, implicit attention is learned from sequence-level text annotations by computing attention scores across all locations over a 1D or 2D space. For example, the 1D se-quential attention weights [3, 39] are generated at different decoding steps to extract important items of the encoded sequence. The 2D attention weights [15, 50] are gener-ated by executing a cross-attention operation with the em-bedded time-dependent sequences and visual features on all spatial locations. However, implicit attention methods, which only extract coarse or even unaligned spatial regions as character attention, may encounter alignment-drifted at-tention. In contrast, supervised attention is learned from ex-tra character-level bounding box annotations by generating character segmentation maps. Although these supervised attention methods [20, 28, 30, 42] can alleviate the above is-sue, they rely on labour-intensive character-level bounding box annotations, and their attention maps with respect to character categories might be memory-intensive when the number of character categories is large.
To address the aforementioned issues, we propose a novel attention-based method for STR (Figure 1 (c)), self-supervised implicit glyph attention (SIGA). As brieﬂy shown in Figure 2, SIGA delineates the glyph structures of text images by jointly self-supervised text segmenta-tion and implicit attention alignment, which serve as the supervision for learning attention maps during training to improve attention correctness. Speciﬁcally, the glyph structures are generated by modulating the learned text foreground representations with sequence-aligned attention
vectors. The text foreground representations are distilled from self-supervised segmentation results according to the internal structures of images [18]; The sequence-aligned at-tention vectors are obtained by applying an orthogonal con-straint to the 1D implicit attention vectors [3]. They then serve as the position information of each character in a text image to modulate the text foreground representations to generate glyph pseudo-labels online.
By introducing glyph pseudo-labels as the supervision of attention maps, the learned glyph attention encourages the text recognition network to focus on the structural re-gions of glyphs to improve attention correctness. Differ-ent from supervised attention methods, the glyph attention maps bring no additional cost to enable character and de-coding order consistency when handling languages with larger character categories.
For recognizing texts with linguistic context, SIGA achieves state-of-the-art results on seven publicly available context benchmarks. We also encapsulate our glyph atten-tion module as a plug-in component to other attention-based methods, achieving average performance gains of 5.68% and 1.34% on SRN [50] and ABINet [15], respectively.
It is worth mentioning that SIGA shows its prominent superiority in recognizing contextless texts widely used in industrial scenarios (e.g., workpiece serial numbers [16] and identiﬁcation codes [32]). Speciﬁcally, we contribute two large-scale contextless benchmarks (real-world MPSC and synthetic ArbitText) with random character sequences that differ from legal words. Experiments demonstrate that
SIGA improves the accuracy of contextless text recognition by a large margin, which is 7.0% and 10.3% higher than
MGP-STR [44] on MPSC and ArbitText, respectively. In summary, the main contributions are as follows:
• We propose a novel attention mechanism for scene text recognition, SIGA, which is able to delineate glyph structures of text images by jointly self-supervised text segmentation and implicit attention alignment to improve attention correctness without character-level bounding box annotations.
• Extensive experiments demonstrate that the proposed glyph attention is essential for improving the perfor-mance of vision models. Our method achieves the state-of-the-art performance on publicly available con-text benchmarks and our contributed large-scale con-textless benchmarks (MPSC and ArbitText). 2.