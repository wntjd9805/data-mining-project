Abstract
Recently, with the advent of Metaverse and AI Gener-ated Content, cross-modal retrieval becomes popular with a burst of 2D and 3D data. However, this problem is challenging given the heterogeneous structure and semantic discrepancies. Moreover, imperfect annotations are ubiq-uitous given the ambiguous 2D and 3D content, thus in-evitably producing noisy labels to degrade the learning per-formance. To tackle the problem, this paper proposes a ro-bust 2D-3D retrieval framework (RONO) to robustly learn from noisy multimodal data. Specifically, one novel Robust
Discriminative Center Learning mechanism (RDCL) is pro-posed in RONO to adaptively distinguish clean and noisy samples for respectively providing them with positive and negative optimization directions, thus mitigating the nega-tive impact of noisy labels. Besides, we present a Shared
Space Consistency Learning mechanism (SSCL) to capture the intrinsic information inside the noisy data by minimizing the cross-modal and semantic discrepancy between com-mon space and label space simultaneously. Comprehen-sive mathematical analyses are given to theoretically prove the noise tolerance of the proposed method. Furthermore, we conduct extensive experiments on four 3D-model mul-timodal datasets to verify the effectiveness of our method by comparing it with 15 state-of-the-art methods. Code is available at https://github.com/penghu-cs/RONO. 1.

Introduction
Point-cloud retrieval (PCR) is fundamental and crucial for processing and analyzing 3D data [14], which could provide the direct technical support of the 3D data search engine, thus embracing compelling application prospects and practical value in the fields of robotics [7, 32], au-tonomous driving [23, 31], virtual/augmented reality [9], and medicine [29], etc. Different from 2D images, 3D point clouds could depict the internal architecture and ex-*Corresponding author: Peng Hu (penghu.ml@gmail.com). ternal appearance of objects from distinct views/modalities.
Hence, PCR is often accompanied by retrieving across di-verse modalities, termed 2D-3D cross-modal retrieval [19].
On the other hand, it is extremely expensive and labor-intensive to label such a huge amount of data points [17, 41], not to mention the additional challenges of the miss-ing color and texture of the point clouds. In order to reduce the labeling cost, we could utilize open source or low-cost annotation tools (e.g., point-cloud-annotation-tool [18], La-belHub, etc.), hence it will inevitably introduce label noise due to the non-expert annotation. However, almost all exist-ing works excessively rely on well-labeled data [19, 20, 44], thus making them vulnerable to noisy labels and leading to unavoidable performance degradation.
To address the aforementioned issues, we propose a robust 2D-3D retrieval framework (RONO) to robustly learn from noisy multimodal data as shown in Figure 1.
Our RONO framework consists of two mechanisms: 1) a novel Robust Discriminative Center Learning mechanism (RDCL) to robustly and discriminatively tackle clean and noisy samples, and 2) a Shared Space Consistency Learning mechanism (SSCL) to alleviate and even eliminate the het-erogeneity and semantic gaps across different modalities.
More specifically, RDCL is presented to adaptively di-vide the noisy data into clean and noisy samples based on the memorization effect of deep neural networks (DNNs)
[3], and then endowing them with positive and negative op-timization directions, respectively.
In brief, RDCL could compact the clean points to the corresponding category cen-ters while scattering the noisy ones apart away from the noisy centers in the common space, thus alleviating the interference of noisy labels.
In addition, our SSCL aims at mitigating the inherent gaps in the common space, i.e., the heterogeneity and semantic gaps. On the one hand, to bridge the heterogeneity gap across different modalities, our
SSCL enforces modality-specific samples from the same in-stance collapse into a single point in the common space, thus producing modality-invariant representations. On the other hand, our SSCL narrows the gap between the repre-sentation space and shared label space to explicitly elimi-Figure 1. The pipeline of our robust 2D-3D retrieval framework (RONO). First, modality-specific extractors project different modalities
{Xj, Yj}M j=1 into a common space. Second, our Robust Discriminative Center Learning mechanism (RDCL) is conducted in the common space to divide the clean and noisy data while rectifying the optimization direction of noisy ones, leading to robustness against noisy labels. Finally, RONO employs a Shared Space Consistency Learning mechanism (SSCL) to bridge the intrinsic gaps between common space and label space. To be specific, SSCL narrows the cross-modal gap by a Multimodal Gap loss (MG) while minimizing the semantic discrepancy between the common space and label space using a Common Representations Classification loss (CRC) Lcrc, thus endowing representations with modality-invariant discrimination. nate the semantic discrepancy, thus encapsulating common discrimination into common representations. Our main con-tributions can be summarized as follows: 2.