Abstract
Given only acoustic signals without any high-level in-formation, such as voices or sounds of scenes/actions, how much can we infer about the behavior of humans? Unlike existing methods, which suffer from privacy issues because they use signals that include human speech or the sounds of specific actions, we explore how low-level acoustic signals can provide enough clues to estimate 3D human poses by active acoustic sensing with a single pair of microphones and loudspeakers (see Fig. 1). This is a challenging task since sound is much more diffractive than other signals and therefore covers up the shape of objects in a scene. Accord-ingly, we introduce a framework that encodes multichan-nel audio features into 3D human poses. Aiming to capture subtle sound changes to reveal detailed pose information, we explicitly extract phase features from the acoustic sig-nals together with typical spectrum features and feed them into our human pose estimation network. Also, we show that reflected or diffracted sounds are easily influenced by subjects’ physique differences e.g., height and muscularity, which deteriorates prediction accuracy. We reduce these gaps by using a subject discriminator to improve accu-racy. Our experiments suggest that with the use of only low-dimensional acoustic information, our method outper-forms baseline methods. The datasets and codes used in this project will be publicly available. 1.

Introduction
The ability to capture human behavior, such as 3D poses, has many potential applications. Over the last decade, many different technologies have been proposed to infer human poses, including conventional cameras [4, 8], tran-sient light [16], radio frequency (RF) or WiFi measure-ments [22, 33]. However, the optical signals are easily oc-cluded and restricted by poor lighting conditions, such as
Method Modality
RGB-based [4, 8]
RF/WiFi-based [1, 22, 30, 33] RF/WiFi
RGB
Audio to joint [10, 21, 28]
Audio to hand micro gesture [20]
Ours
Audio
Audio
Audio
Occluded by
Any opaque objects
Metal, water
Soundproof room
Soundproof room
Soundproof room
Required semantics level
High (image required)
Low
High (speech required)
Low
Low
Invasiveness
Non-invasive
Non-invasive
Non-invasive
Invasive
Non-invasive
Table 1. Comparisons between existing pose estimation methods and our method. a dark room or a night road. RF/WiFi signals are also oc-cluded by water or metal. In addition, the use of wireless signals is often limited, as electronic devices that transmit signals must remain off during flights as well as in hospital rooms with sensitive electronic systems.
Audio signals, which exist everywhere in our world, have the potential to solve these fatal limitations. We can listen to sounds regardless of the lighting conditions, and acoustic signals do not affect electronic systems. If we use ultrasonic waves, which are outside of our audible range, we are not even aware of them. Moreover, since acoustic signals have a much longer wavelength (meter scale) than visible light (nanometer scale) and RF/WiFi signals (cen-timeter scale), the signals are less occluded.
Some very recent studies have used acoustic signals in cross-modal analyse with visual information, including scene geometry estimation [5, 26], action recognition [9], visual semantic segmentation [15], and even object under-standing [27]. Another line of studies uses acoustic signals for sensing humans, such as active hand gesture monitor-ing [20]. Papers that are more relevant to ours are those that infer human joints by converting human speech or music to gestures [10, 21, 28]. These methods use human speech as a clue for recovering human gestures/motions. However, these methods utilize semantics of sounds, for instance, hu-man voice, music, speech, or sounds of specific actions, which raise privacy issues.
In this paper, we define such signals with the semantics of sounds as “high-level sig-nals” and signals that do not include any of the semantics of sounds as “low-level signals.” So far, no methods have been proposed to capture whole human 3D poses given only low-level acoustic signals.
The previous studies raised the following three ques-tions. First, do low-level acoustic signals have enough in-formation to reconstruct whole 3D human poses? Second, what is the smallest set of hardware needed for the task?
And third, which ones lend themselves to effective infer-ence algorithms?
To answer these questions, this paper examines a new task, 3D human pose estimation from only low-level acous-tic signals. This is a challenging task. The wavelengths of acoustic signals are much longer than optical or RF/WiFi signals. While it could be advantageous for occlusion is-sues, a longer wavelength is diffractive, making it difficult to distinguish small pose changes. In this work, we explore a solution to this task with minimal equipment configura-tion using only a single ambisonics microphone (Fig. 1), as opposed to previous methods, which use high-definition
RGB(D) cameras and RF/WiFi signals from multiple trans-mitters and receivers. While we do use multiple channels, our microphone is located in a specific single position and has far fewer geometry clues to map the signals to human activities. Moreover, unlike most previous works that have utilized higher-level semantics, such as human speech, mu-sic, or a dog barking, our low-level signals do not represent any of this kind of information directly.
To capture human status effectively under such a se-vere condition, we propose a convolutional neural net-work (CNN)-based framework designed to employ multi-channel audio features as its inputs and directly output the predicted 3D body part joint locations. If humans oc-clude acoustic signals emitted from loudspeakers, this sub-tle “shifts” in arrival time of the incoming acoustic signals will occur. Our network model captures these small shifts by explicitly integrating phase features that represent the time difference of arrival (TDOA) and utilizing them to in-fer human behavior. Additionally, we discover that sounds reflected or diffracted on subjects’ bodies tend to be affected by their physique differences, such as height and muscular-ity. Our proposed dataset contains sound data of both men and women, and the physical features vary among subjects.
This difference causes our model to over-fit on each sub-ject’s physical characteristics and prevents it from general-izing well to unseen subjects. Therefore, we apply adver-sarial learning to this task using the subject discriminator’s prediction uncertainty and create subject-invariant features.
Since no previous method could tackle this task, there is no public dataset available. Therefore, to train our network, we set up an active acoustic-sensing system us-ing a single pair of ambisonics microphones and loud-speakers. Then, we actively record the sounds of a time-stretched pulse (TSP) signal emitted from the speaker, syn-chronized with motion capture (Mocap) data. These data were recorded in both (i) an anechoic room with little effect of reverberation and (ii) a classroom with a lot of noise.
To summarize, our contributions are as follows: (1) We are the first to tackle a new task: 3D human pose estimation given only low-level audio signals; (2) We describe a net-work architecture that directly maps acoustic features to 3D human poses; (3) We increase prediction accuracy by using
adversarial learning and creating human-physique-invariant features; (4) Since there are no previous methods to carry out this task, we describe how to create new datasets to train our network model; and (5) We conduct extensive experi-mentation and show the effectiveness of our method. 2.