Abstract
The recent detection transformer (DETR) has ad-vanced object detection, but its application on resource-constrained devices requires massive computation and memory resources. Quantization stands out as a solution by representing the network in low-bit parameters and op-erations. However, there is a significant performance drop when performing low-bit quantized DETR (Q-DETR) with existing quantization methods. We find that the bottle-necks of Q-DETR come from the query information distor-tion through our empirical analyses. This paper addresses this problem based on a distribution rectification distillation (DRD). We formulate our DRD as a bi-level optimization problem, which can be derived by generalizing the informa-tion bottleneck (IB) principle to the learning of Q-DETR.
At the inner level, we conduct a distribution alignment for the queries to maximize the self-information entropy. At the upper level, we introduce a new foreground-aware query matching scheme to effectively transfer the teacher informa-tion to distillation-desired features to minimize the condi-tional information entropy. Extensive experimental results show that our method performs much better than prior arts.
For example, the 4-bit Q-DETR can theoretically acceler-ate DETR with ResNet-50 backbone by 6.6× and achieve 39.4% AP, with only 2.6% performance gaps than its real-valued counterpart on the COCO dataset 1. 1.

Introduction
Inspired by the success of natural language processing (NLP), object detection with transformers (DETR) has been introduced to train an end-to-end detector via a transformer encoder-decoder [4]. Unlike early works [22, 31] that often employ convolutional neural networks (CNNs) and require post-processing procedures, e.g., non-maximum suppres-sion (NMS), and hand-designed sample selection, DETR treats object detection as a direct set prediction problem.
† Equal contribution.
∗ Corresponding author: bczhang@buaa.edu.cn 1 Code: https://github.com/SteveTsui/Q-DETR
Figure 1. The histogram of query values q (blue shadow) and cor-responding PDF curves (red curve) of Gaussian distribution [17], w.r.t the cross attention of different decoder layers in (a) real-valued DETR-R50, and (b) 4-bit quantized DETR-R50 (baseline).
Gaussian distribution is generated from the statistical mean and variance of the query values. The query in quantized DETR-R50 bears information distortion compared with the real-valued one.
Experiments are performed on the VOC dataset [9].
Despite this attractiveness, DETR usually has a tremen-dous number of parameters and float-pointing operations (FLOPs). For instance, there are 39.8M parameters taking up 159MB memory usage and 86G FLOPs in the DETR model with ResNet-50 backbone [12] (DETR-R50). This leads to an unacceptable memory and computation con-sumption during inference, and challenges deployments on devices with limited supplies of resources.
Therefore, substantial efforts on network compression have been made towards efficient online inference [7, 33, 43, 44]. Quantization is particularly popular for deploying on AI chips by representing a network in low-bit formats.
Yet prior post-training quantization (PTQ) for DETR [26] derives quantized parameters from pre-trained real-valued models, which often restricts the model performance in a sub-optimized state due to the lack of fine-tuning on the training data.
In particular, the performance drastically drops when quantized to ultra-low bits (4-bits or less). Al-ternatively, quantization-aware training (QAT) [25, 42] per-forms quantization and fine-tuning on the training dataset
tion knowledge distillation method (DRD). We find ineffec-tive knowledge transferring from the real-valued teacher to the quantized student primarily because of the information gap and distortion. Therefore, we formulate our DRD as a bi-level optimization framework established on the infor-mation bottleneck principle (IB). Generally, it includes an inner-level optimization to maximize the self-information entropy of student queries and an upper-level optimization to minimize the conditional information entropy between student and teacher queries. At the inner level, we conduct a distribution alignment for the query guided by its Gaussian-alike distribution, as shown in Fig. 1, leading to an explicit state in compliance with its maximum information entropy in the forward propagation. At the upper level, we introduce a new foreground-aware query matching that filters out low-qualified student queries for exact one-to-one query match-ing between student and teacher, providing valuable knowl-edge gradients to push minimum conditional information entropy in the backward propagation.
This paper attempts to introduce a generic method for
DETR quantization. The significant contributions in this paper are outlined as follows: (1) We develop the first QAT quantization framework for DETR, dubbed Q-DETR. (2)
We use a bi-level optimization distillation framework, ab-breviated as DRD. (3) We observe a significant performance increase compared to existing quantized baselines. 2.