Abstract
Classiﬁcation has been the focal point of research on ad-versarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmenta-tion. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these mod-els based on a proximal splitting to produce adversarial perturbations with much smaller ℓ∞ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian ap-proach, coupled with adaptive constraint scaling and mask-ing strategies. We demonstrate that our attack signiﬁcantly outperforms previously proposed ones, as well as classiﬁca-tion attacks that we adapted for segmentation, providing a
ﬁrst comprehensive benchmark for this dense task. 1.

Introduction
Research on white-box adversarial attacks has mostly fo-cused on classiﬁcation tasks, with several methods proposed over the years for ℓp-norms [8,9,20–22,27,30,32,35,36,42].
In contrast, the literature on adversarial attacks for segmenta-tion tasks has been much scarcer, with few works proposing attacks [13,33,39]. The lack of studies on adversarial attacks in segmentation may appear surprising because of the promi-nence of this computer vision task in many applications where a semantic understanding of image contents is needed.
In many safety-critical application areas, it is thus crucial to assess the robustness of the employed segmentation models.
Although segmentation is treated as a per-pixel classiﬁ-cation problem, designing adversarial attacks for semantic segmentation is much more challenging for several reasons.
First, from an optimization perspective, the problem of ad-versarial example generation is more difﬁcult. In a clas-siﬁcation task, producing minimal adversarial examples is a nonconvex constrained problem with a single constraint.
In a segmentation task, this optimization problem now has multiple constraints, since at least one constraint must be addressed for each pixel in the image. For a dataset such as Cityscapes [19], the images have a size of 2 048×1 024, resulting in more than 2 million constraints. Consequently, most attacks originally designed for classiﬁcation cannot be directly extended to segmentation. For instance, penalty methods such as C&W [9] cannot tackle multiple constraints since they rely on a binary search of the penalty weight.
Second, the computational and memory cost of gener-ating adversarial examples can be prohibitive. White-box adversarial attacks usually rely on computing the gradient of a loss w.r.t. the input. In segmentation tasks, the dense outputs result in high memory usage to perform a backward propagation of the gradient. For reference, computing the gradients of the logits w.r.t. the input requires ∼22 GiB of memory for FCN HRNetV2 W48 [38] on Cityscapes with a 2 048×1 024 image. Additionally, most recent classiﬁ-cation adversarial attacks require between 100 and 1 000 iterations, resulting in a run-time of up to a few seconds per image [34,36] (on GPU) depending on the dataset and model.
For segmentation, this increases to tens or even hundreds of seconds per image with larger models.
In this article, we propose an adversarial attack to pro-duce minimal adversarial perturbations w.r.t. the ℓ∞-norm, for deep semantic segmentation models. Building on Aug-mented Lagrangian principles, we introduce adaptive strate-gies to handle a large number of constraints (i.e. >106).
Furthermore, we tackle the nonsmooth ℓ∞-norm minimiza-tion with a proximal splitting instead of gradient descent. In particular, we show that we can efﬁciently compute the prox-imity operator of the sum of the ℓ∞-norm and the indicator function of the space of possible perturbations. This results in an adversarial attack that signiﬁcantly outperforms the
DAG attacks [39], in addition to several classiﬁcation attacks that we were able to adapt for segmentation. We propose a methodology to evaluate adversarial attacks in segmentation, and compare the different approaches on the Cityscapes [19] and Pascal VOC 2012 [23] datasets with a diverse set of archi-tectures, including well-known DeepLabV3+ models [11], the recently proposed transformer-based SegFormer [40], and the robust DeepLabV3 DDC-AT model from [41]. The
(a) Pascal VOC 2012: ∥δ∥∞ = 0.67/255 (b) Cityscapes: ∥δ∥∞ = 0.53/255
Figure 1. Untargeted adversarial examples for FCN HRNetV2 W48 on Pascal VOC 2012 and Cityscapes. In both cases, more than 99% of pixels are incorrectly classiﬁed. For each dataset, left is the original image and its predicted segmentation, middle is the ampliﬁed perturbation and the ground truth segmentation and right is the adversarial image with its predicted segmentation. For Pascal VOC 2012, the predicted classes are TV monitor (blue), person (beige) and chair (bright red). proposed approach yields outstanding performances for all models and datasets considered in our experiments. For in-stance, our attack ﬁnds untargeted adversarial perturbations with ℓ∞-norms lower than 1/255 on average for all models on
Cityscapes. With this attacker, we provide better means to as-sess the robustness of deep segmentation models, which has often been overestimated until now, as the existing attacks could only ﬁnd adversarial examples with larger norms. 2.