Abstract 1.

Introduction 3D scene understanding has seen significant advances in recent years, but has largely focused on object understanding in 3D scenes with independent per-object predictions. We thus propose to learn Neural Part Priors (NPPs), parametric spaces of objects and their parts, that enable optimizing to fit to a new input 3D scan with global scene consistency con-straints. The rich structure of our NPPs enables accurate, holistic scene reconstruction across similar objects in the scene. Both objects and their part geometries are charac-terized by coordinate field MLPs, facilitating optimization at test time to fit to input geometric observations as well as similar objects in the input scan. This enables more accu-rate reconstructions than independent per-object predictions as a single forward pass, while establishing global consis-tency within a scene. Experiments on the ScanNet dataset demonstrate that NPPs significantly outperforms the state-of-the-art in part decomposition and object completion in real-world scenes.
With the introduction of commodity RGB-D sensors (e.g.,
Microsoft Kinect, Intel RealSense, etc.), remarkable progress has been made in reconstruction and tracking to construct 3D models of real-world environments [9, 14, 37, 39, 52].
This has enabled construction of large-scale datasets of real-world 3D scanned environments [4, 11], enabling significant advances in 3D semantic segmentation [10, 13, 23], 3D se-mantic instance segmentation [18, 24, 26], and even part-level understanding of scenes [3]. The achieved 3D object recognition has shown impressive advances, but methods fo-cus on independent predictions per object in single forward passes, resulting in semantic predictions that are inconsistent between repeated objects in a scene, and/or geometric predic-tions that do not precisely match input observed geometry.
Simultaneously, recent advances in representing 3D shapes as continuous implicit functions represented with coordinate field MLPs have shown high-fidelity shape re-construction [6–8, 40, 47]. Such methods have focused on object-level reconstructions, whereas part-based understand-ing is fundamental to many higher-level scene understanding tasks (e.g., interactions often occur with object parts – sitting on the seat of a couch, opening a door with a handle, etc.).
We thus propose to learn Neural Part Priors (NPPs), op-timizable parametric shape and part spaces learned from synthetic data. These learned manifolds enable efficient traversal in latent spaces during inference to fit precisely to objects in real-world scanned scenes, while maintaining consistent part decompositions with similar objects in the scene. Our NPPs leverage the representation power of neural implicit functions encoded as coordinate-field MLPs, repre-senting both shape and part geometries of objects. A shape can then be represented by a set of latent codes for each of its parts, where each code decodes to predict the respective part segmentation and signed distance field representation of the part geometry. This representation enables effective test-time joint optimization over all parts of a shape by traversing through the part latent space to find the set of parts that best explain a real-world shape observation. Furthermore, as re-peated objects often appear in a scene under different partial observation patterns (resulting in inconsistent predictions when made independently for each object) we further opti-mize for part consistency between similar objects detected in a scene to produce scene-consistent part decompositions.
This allows us to reconstruct the holistic structure of a scene.
To fit real-world 3D scan data, we first perform object detection and estimate the part types for each detected object.
We can then optimize in test time jointly over the part codes for each shape to fit the observed scan geometry; we leverage a predicted part segmentation of the detected object and optimize jointly across the parts of each shape such that each part matches the segmentation, and their union fits the object. This joint optimization across parts produces a high-resolution part decomposition whose union represents the complete shape while fitting precisely to the observed real geometry. Furthermore, this optimization at inference time allows leveraging global scene information to inform our optimized part decompositions; in particular, we consider objects of the same predicted class with similar estimated geometry, and optimize them jointly, enabling more robust and scene-consistent part decompositions.
In summary, we present the following contributions:
• We propose to learn optimizable part-based latent priors for 3D shapes – Neural Part Priors, encoding part segmen-tation and part geometry into a latent space for the part types of each class category.
• Our learned, optimizable part priors enable test-time opti-mization over the latent spaces, enhanced with inter-shape part-based constraints, to fit partial, cluttered object ge-ometry in real-world scanned scenes, resulting in robust and precise semantic part completion.
• We additionally propose a scene-consistent optimization, enhanced with intra-shape constraints, jointly optimizing over similar objects that provides globally-consistent part decompositions for repeated object instances in a scene. 2.