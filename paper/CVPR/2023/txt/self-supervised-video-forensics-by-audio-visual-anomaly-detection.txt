Abstract
Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics. 1.

Introduction
Supervised learning underlies today’s most successful methods for image and video forensics. However, the diffi-culty of collecting large, labeled datasets that fully capture all of the possible manipulations that one might encounter in the wild places significant limitations on this approach. A longstanding goal of the forensics community has been to design methods that, instead, learn to detect manipulations using cues discovered by analyzing large amounts of real data through self-supervision [27, 47].
We propose a method that identifies manipulated video through anomaly detection. Our model learns how audio and visual data temporally co-occur by training on large amounts of real, unlabeled video. At test time, we can then flag videos that our model assigns low probability, such as those whose video and audio streams are inconsistent.
One might expect that this problem could be posed as simply detecting out-of-sync examples, such as by finding cases in which a speaker’s mouth does not open precisely at the onset of a spoken word. Unfortunately, videos in the wild are often “naturally” misaligned due to errors in encoding or recording, such as by having a single, consistent shift by a few frames [2, 23].
Instead, we pose the problem as detecting anomalies in what we call synchronization features: audio-visual features
Figure 1. Audio-visual anomaly detection. We identify fake videos by finding anomalies in their audio-visual features, using generative models trained entirely on real videos. In one variation of our model (shown here), we use the time delay between the two modalities as our feature set, i.e., temporal misalignment between each video frame and the audio stream. We learn the distribution of these sequences, then flag sequences with low probability. that are designed to convey the temporal alignment between vision and sound. We evaluate several feature sets, each extracted from a model that has been trained to temporally align audio and visual streams of a video [18, 23, 78]. In
Figure 1, we show one such feature set: the amount of time that each video frame appears to be temporally offset from its corresponding sound. To detect anomalies, we fit an autoregressive generative model [84, 99] to sequences of synchronization features extracted from real videos, and identify low probability examples.
A key advantage of our formulation is that it does not require any manipulated examples for training. It also does not require the speakers in the test set to already be present in the training set. This is in contrast to previous audio-visual forensics approaches, which either require finetuning on datasets of manipulated video [40], or which are based on verifying that the speaker’s voice matches previously observed examples [25].
We evaluate our model on videos that have manipulated a person’s speech and face, using datasets of lip-synced and audio-driven face reenactment videos, some of which are also manipulated by faceswap techniques. Our model obtains strong performance on the FakeAVCeleb [52] and KoDF [61] datasets, despite the fact that it is trained entirely on real examples obtained from other video datasets. Our model generalizes to other spoken languages without retraining and obtains robustness to a variety of postprocessing operations, such as compression and blurring. We show through our experiments that:
• Video forensics can be posed as an audio-visual anomaly detection problem.
• Synchronization features convey information about video manipulations.
• Our model can successfully detect fake videos, while train-ing solely on real videos.
• Our model generalizes to many types of image postpro-cessing operations and to speech videos from spoken lan-guages not observed during training. 2.