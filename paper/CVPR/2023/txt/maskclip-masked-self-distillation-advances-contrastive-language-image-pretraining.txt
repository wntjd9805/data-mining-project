Abstract
This paper presents a simple yet effective framework
MaskCLIP, which incorporates a newly proposed masked self-distillation into contrastive language-image pretraining.
The core idea of masked self-distillation is to distill repre-sentation from a full image to the representation predicted from a masked image. Such incorporation enjoys two vital beneﬁts. First, masked self-distillation targets local patch representation learning, which is complementary to vision-language contrastive focusing on text-related representa-tion. Second, masked self-distillation is also consistent with vision-language contrastive from the perspective of train-ing objective as both utilize the visual encoder for feature aligning, and thus is able to learn local semantics getting indirect supervision from the language. We provide specially designed experiments with a comprehensive analysis to vali-date the two beneﬁts. Symmetrically, we also introduce the local semantic supervision into the text branch, which further improves the pretraining performance. With extensive exper-iments, we show that MaskCLIP, when applied to various challenging downstream tasks, achieves superior results in linear probing, ﬁnetuning, and zero-shot performance with the guidance of the language encoder. Code will be release at https://github.com/LightDXY/MaskCLIP. 1.

Introduction
Vision-language (VL) contrastive learning [31, 51] has shown remarkable success in pretraining for various tasks.
With large-scale image-text pairs available on the Internet, the model composed of a simple dual encoder design learns
*Equal contribution, † Corresponding Author
†Work done during an internship at Microsoft Research Asia strong semantic prior by aligning between image and text.
The resulting visual encoder not only exhibits excellent lin-ear probing and ﬁnetuning performance, but also enables impressive zero-shot performance with the guidance of the language encoder, showing the generality of natural language and its ability to supervise a wide range of visual concepts.
Nonetheless, the associated language description, though providing richer information than mere class labels, still can hardly describe all the information in the corresponding image, as images are continuous signals with ﬁne-grained de-tails and complex semantics. As a result, the VL contrastive by aligning global representations may only focus on the text-described objects and ignore the rest which might be useful for downstream tasks.
In this paper, we are interested in how to fully leverage the image itself to facilitate the VL contrastive to further improve the transfer capability. (1) Firstly, the learned fea-ture representation shall characterize local patches, serving as a complementary for global representation in VL con-trastive. Inspired by the recent success of masked image modeling [4, 19, 26, 51, 60, 61] in learning patch representa-tions, we also randomly mask the input image with a large portion to force the visual encoder to focus on the remaining visible patches. (2) Secondly, the learned representation for local patches shall possess semantic meanings, being consis-tent with the global representation receiving semantic text supervision. We bring mean teacher self-distillation [25, 57] to supervise the learned patch representations with the vi-sual feature representations, enabling implicit supervision from natural language. The resulting objective is denoted as masked self-distillation where the student model and the teacher model come from the same neural networks and the knowledge is distilled from the full image (fed to the teacher model) to the masked image (fed to student model). To this end, we introduce MaskCLIP by incorporating masked self-distillation into VL contrastive to advance the transferable visual encoder.
There are several recent attempts [49, 68] also exploring the capability of the visual encoder under natural language supervision. The common approach is to introduce con-trastive learning or masked image modeling on the vision side together with contrastive language-image pretraining.
However, the performance indeed improves based on CLIP but does not as well as our masked self-distillation. We argue that (1) the contrastive learning objective based on central crop augmentation actually learns global representations for salient objects while lack of attention on the surrounding backgrounds [11]; and (2) masked image modeling usually needs to remap the learned representation to pixels [26] or discrete tokens [4]. Such low-level prediction target is inef-ﬁcient for semantic feature learning and thus also conﬂicts with high-level language supervision in VL contrastive. A brief illustration is presented in Figure 1. In the experiments, we conduct comprehensive ablations to analyze the differ-ence and provide numerical and visual evidence for better understanding.
Symmetrically, we argue that local semantic supervision on the text branch is also helpful for the text encoder and eventually beneﬁcial for zero-shot performance. So we intro-duce the same mask-data-modeling format supervision into the text branch as well. Different from images where the pixel is low-level signal, the words crafted by human beings are already highly semantic, so we use the tokenized word piece as the prediction target directly, following the well-studied mask language modeling method BERT. Meanwhile, to reduce the output conﬂicts between contrastive learning and mask language modeling, we introduce a small decoder for the mask language modeling branch.
We train our MaskCLIP on a subset of a publicly avail-able image-text pairs dataset, YFCC [58], and thoroughly evaluate the transfer ability of visual representations on sev-eral vision benchmarks: ImageNet-1K [17] for classiﬁcation,
ADE20K [69] for semantic segmentation, MS-COCO [40] for detection and segmentation, as well as a batch of other classiﬁcation benchmarks. When it comes to ImageNet-1K [17] classiﬁcation, MaskCLIP achieves +6.9%, +7.2%,
+1.3% higher than CLIP for zero-shot transfer, linear prob-ing, and ﬁnetuning respectively. For vision downstream tasks, we reach +2.7 mIoU on ADE20K [69] and +1.8 APb,
+1.4 APm on MS-COCO [40]. For vision-language tasks,
MaskCLIP achieves +6.1% average zero-shot accuracy on 20 datasets, and +17.2%, +12.8% rank@1 improvement on the Flickr30K [67] image-test retrieval. In the recent Im-age Classiﬁcation in the Wild challenge academic track, our
MaskCLIP gets the 1st result with 48.9% TOP-1 average accuracy, surpassing the second team with 3.4%.
In summary, the major contributions of this work are: 1. We present a novel vision-language pretraining framework MaskCLIP, by introducing masked self-distillation objective to facilitate VL contrastive for better transferable visual models. 2. We present extensive ablation studies on MaskCLIP variants and provide in-depth analysis numerically and visually to help understand how the proposed masked self-distillation assists VL contrastive. 3. We demonstrate our MaskCLIP on tens of benchmarks, showing the superiority under all three settings: zero-shot, linear probing, and ﬁnetuning. 2.