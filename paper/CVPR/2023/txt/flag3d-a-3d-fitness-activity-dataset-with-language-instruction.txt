Abstract
With the continuously thriving popularity around the world, fitness activity analytic has become an emerging re-search topic in computer vision. While a variety of new tasks and algorithms have been proposed recently, there are growing hunger for data resources involved in high-quality data, fine-grained labels, and diverse environments.
In this paper, we present FLAG3D, a large-scale 3D fit-ness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D hu-man pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to per-form a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. Extensive experiments and in-depth analysis show that FLAG3D con-tributes great research value for various challenges, such as cross-domain human action recognition, dynamic human mesh recovery, and language-guided human action genera-tion. Our dataset and source code are publicly available at https://andytang15.github.io/FLAG3D. 1.

Introduction
With the great demand of keeping healthy, reducing high pressure from working and staying in shape, fitness activity has become more and more important and popular during the past decades [22]. According to the statistics1, there are over 200,000 fitness clubs and 170 million club members all over the world. More recently, because of the high expense of coaches and out-breaking of COVID-19, increasing peo-ple choose to exclude gym membership and do the work-out by watching the fitness instructional videos from fitness apps or YouTube channels (e.g., FITAPP, ATHLEAN-X,
The Fitness Marshall, etc.). Therefore, it is desirable to ad-1https://policyadvice.net/insurance/insights/fitness-industry-statistics
Table 1. Comparisons of FLAG3D with the relevant datasets. FLAG3D consists of 180K sequences (Seqs) of 60 fitness activity categories (Cats). It contains both low-level features, including 3D key points (K3D) and SMPL parameters, as well as high-level language annotation (LA) to instruct trainers, sharing merits of multiple resources from MoCap system in laboratory (Lab), synthetic (Syn.) data by rendering software and natural (Nat.) scenarios. We evaluate various tasks in this paper, including human action recognition (HAR), human mesh recovery (HMR), and human action generation (HAG), while more potential applications like human pose estimation (HPE), repetitive action counting (RAC), action quality assessment and visual grounding could be explored in the future (see Section 5 for more details.).
Dataset
Subjs
Cats
Seqs
Frames
LA
K3D
SMPL
Resource
Task
PoseTrack [7]
Human3.6M [33]
CMU Panoptic [37]
MPI-INF-3DHP [57] 3DPW [96]
ZJU-MoCap [68]
NTU RGB+D 120 [51]
HuMMan [11]
HumanML3D [26]
KIT Motion Language [71]
HumanAct12 [28]
UESTC [35]
Fit3D [22]
EC3D [115]
Yoga-82 [95]
-11 8 8 7 6 106 1000
-111 12 118 13 4
-FLAG3D (Ours) 10+10+4
-17 5 8
-6 120 500
--12 40 37 3 82 60 550 839 65
-60 9 114k 400K 14K 3911 1191 25K
-362
-66K 3.6M 594K
>1.3M 51k
>1k
-60M
--90K
> 5M
> 3M
-29K 180K 20M
×
×
×
×
×
×
×
×
✓
✓
×
×
×
×
×
✓
×
✓
✓
✓
×
✓
✓
✓
✓
✓
×
✓
✓
✓
×
✓
×
---✓
✓
-✓
✓
-✓
-✓
-×
✓
Nat.
Lab
Lab
Lab+Nat.
Nat.
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Lab
Nat.
HPE
HAR,HPE,HMR
HPE
HPE,HMR
HMR
HAR,HMR
HAR,HAG
HAR,HMR
HAG
HAG
HAG
HAR,HAG
HPE,RAC
HAR
HAR,HPE
Lab+Syn.+Nat.
HAR,HMR,HAG vance current intelligent vision systems to assist people to perceive, understand and analyze various fitness activities.
In recent years, a variety of datasets have been proposed in the field [22, 95, 115], which have provided good bench-marks for preliminary research. However, these datasets might have limited capability to model complex poses, describe a fine-grained activity, and generalize to differ-ent scenarios. We present FLAG3D in this paper, a 3D
Fitness activity dataset with LAnGuage instruction. Fig-ure 1 presents an illustration of our dataset, which con-tains 180K sequences of 60 complex fitness activities ob-tained from versatile sources, including a high-tech MoCap system, professional rendering software, and cost-effective smartphones. In particular, FLAG3D advances current re-lated datasets from the following three aspects:
Highly Accurate and Dense 3D Pose. For fitness activ-ity, there are various poses within lying, crouching, rolling up, jumping etc., which involve heavy self-occlusion and large movements. These complex cases bring inevitable ob-stacles for conventional appearance-based or depth-based sensors to capture the accurate 3D pose. To address this, we set up an advanced MoCap system with 24 VICON cam-eras [5] and professional MoCap clothes with 77 motion markers to capture the trainers’ detailed and dense 3D pose.
Detailed Language Instruction. Most existing fitness activity datasets merely provide a single action label or phase for each action [95,115]. However, understanding fit-ness activities usually requires more detailed descriptions.
We collect a series of sentence-level language instructions for describing each fine-grained movement.
Introducing language would also facilitate various research regarding emerging multi-modal applications.
Diverse Video Resources. To advance the research di-rectly into a more general field, we collect versatile videos for FLAG3D. Besides the data captured from the expensive
MoCap system, we further provide the synthetic sequences with high realism produced by rendering software and the corresponding SMPL parameters.
In addition, FLAG3D also contains videos from natural real-world environments obtained by cost-effective and user-friendly smartphones.
To understand the new challenges in FLAG3D, we eval-uate a series of recent advanced approaches and set a benchmark for various tasks, including skeleton-based ac-tion recognition, human mesh recovery, and dynamic ac-tion generation. Through the experiments, we find that 1) while the state-of-the-art skeleton-based action recognition methods have attained promising performance with highly accurate MoCap data under the in-domain scenario, the re-sults drop significantly under the out-domain scenario re-garding the rendered and natural videos. 2) Current 3D pose and shape estimation approaches easily fail on some poses, such as kneeling and lying, owing to the self-occlusion.
FLAG3D provides accurate ground truth for these situa-tions, which could improve current methods’ performance in addressing challenging postures. 3) Motions generated by state-of-the-art methods appear to be visually plausible
and context-aware at the beginning. However, they cannot follow the text description faithfully as time goes on.
To summarize, our contributions are twofold: 1) We present a new dataset named FLAG3D with highly accu-rate and dense 3D poses, detailed language instruction, and diverse video resources, which could be used for multiple tasks for fitness activity applications. 2) We conduct vari-ous empirical studies and in-depth analyses of the proposed
FLAG3D, which sheds light on the future research of activ-ity understanding to devote more attention to the generality and the interaction with natural language. 2.