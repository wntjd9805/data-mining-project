Abstract
Open-set fine-grained retrieval is an emerging challenge that requires an extra capability to retrieve unknown sub-categories during evaluation. However, current works focus on close-set visual concepts, where all the subcategories are pre-defined, and make it hard to capture discrimina-tive knowledge from unknown subcategories, consequently failing to handle unknown subcategories in open-world sce-narios. In this work, we propose a novel Prompting vision-Language Evaluator (PLEor) framework based on the re-cently introduced contrastive language-image pretraining (CLIP) model, for open-set fine-grained retrieval. PLEor could leverage pre-trained CLIP model to infer the discrep-ancies encompassing both pre-defined and unknown subcat-egories, called category-specific discrepancies, and trans-fer them to the backbone network trained in the close-set scenarios. To make pre-trained CLIP model sensitive to category-specific discrepancies, we design a dual prompt scheme to learn a vision prompt specifying the category-specific discrepancies, and turn random vectors with cate-gory names in a text prompt into category-specific discrep-ancy descriptions. Moreover, a vision-language evaluator is proposed to semantically align the vision and text prompts based on CLIP model, and reinforce each other. In addi-tion, we propose an open-set knowledge transfer to transfer the category-specific discrepancies into the backbone net-work using knowledge distillation mechanism. Quantitative and qualitative experiments show that our PLEor achieves promising performance on open-set fine-grained datasets. 1.

Introduction
Open-set fine-grained retrieval (OSFR) attempts to build a well-generalized embedding space where the visual dis-crepancies among unknown subcategories are clearly re-It plays a vital role in numerous vision applica-flected.
*Corresponding author: hjli@dlut.edu.cn (a) a classification-based evaluator (e.g., [34, 43, 52, 64]) (b) a metric-based evaluator (e.g., [3, 20, 39, 46, 51, 63]) (c) a vision-language evaluator with open-world knowledge
Figure 1. Comparison on existing evaluators in open-set fine-grained retrieval. Although our PLEor (c) is trained in a close-set scenarios, similar with previous works (a) (b), it could mine the category-specific discrepancies using pre-trained CLIP model aided by vision and text prompts, and transfer the discrepancies encompassing both pre-defined and unknown subcategories to our model. This enables our model to procure in-depth understanding for unknown subcategories owing to distilling the knowledge with open-world visual concepts from CLIP model, improving retrieval performance eventually in open-set scenarios. tions from fashion industry, e.g., retrieval of diverse types of clothes [1, 31], to environmental conservation, e.g., retriev-ing endangered species [7,49,50]. As shown in Fig. 1(a)(b), existing works follow a close-set learning setting, where all the subcategories are pre-defined, and evaluate embeddings identifying the visually similar objects of pre-defined sub-categories. However, such evaluation focuses on closed-set visual concepts, limiting the model to a pre-defined list of subcategories, and is not generalizable when it comes to un-known subcategories unseen during training.
Fortunately, recent works [66, 67] using large-scale con-trastive language-image pretraining (CLIP) model [37] have shown great potentials in alleviating this limitation. As shown in Fig. 1(c), CLIP model is pretrained from scratch on a dataset of 400 moillion image-text pairs, which are au-tomatically collected from the publicly available sources on
the Internet. Based on this, CLIP model could associate much wider range of visual concepts in the images with their text descriptions, rather than a fixed set of pre-defined categories. Therefore, one question naturally arises: is it possible that we can effectively exploit the open-set visual concepts in CLIP model to solve OSFR task? It is already answered yes by recent studies exploring how to transfer the knowledge from CLIP model to other downstream tasks via prompt techniques [10, 16, 26, 37, 56, 66, 67]. However, their prompt strategies are tailored for capturing category-level semantic (e.g., dog and cat) rather than more detailed visual discrepancies for distinguishing fine-grained objects (e.g., different breeds of dogs). Therefore, how to effec-tively make pre-trained CLIP model sensitive to the visual discrepancies encompassing both pre-defined and unknown subcategories (termed as category-specific discrepancies), and transfer these discrepancies to the model trained in closed-set scenarios is worthy of investigation.
To this end, we design a novel prompting vision-language evaluator (PLEor) for OSFR, based on the power of recently introduced CLIP model. Technically, to make pre-trained CLIP model sensitive to category-specific dis-crepancy, we design a dual prompt scheme composed of vision prompt and text prompt for explicitly highlighting the category-specific discrepancies from the input perspec-tive. Concretely, the vision prompt specifies the category-specific discrepancies via parsing semantic features inferred by the backbone network. And the text prompt turns ran-dom vectors with category names into category-specific dis-crepancy descriptions. Meanwhile, a vision-language eval-uator is proposed to encourage pre-trained CLIP model to locate the category-specific descriptions in vision prompt and generate the category-specific visual semantics into text prompt. In this way, the OSFR task aided by the designed prompts is close to the solved task of pre-training CLIP model, thus making the CLIP model sensitive to category-specific discrepancy. Nevertheless, a non-negligible prob-lem is that the corporation of CLIP model and backbone network is quite complex, leading to very time consuming and memory demanding during evaluation. Thereby, we propose an open-set knowledge transfer module to transfer the category-specific discrepancies from CLIP model to the backbone network using knowledge distillation mechanism.
Our contributions are summarized as follows:
• A prompting vision-language evaluator, i.e., PLEor, is proposed. It can distill the knowledge with open-world visual concepts from CLIP model to alleviate the prob-lems behind open-set scenarios. To our best knowl-edge, we are the first to regard CLIP model as an eval-uator specifically for OSFR task.
• PLEor provides timely insights into the adaptation of pre-trained CLIP model adopting prompt learning, and crucially, demonstrates the effectiveness of a simple modification for inputs of CLIP model in OSFR.
• PLEor achieves new state-of-the-art results compared with classification-based and metric-based evaluators, which is significant gains of 8.0% average retrieval ac-curacy on three widely-used OSFR datasets. 2.