Abstract
Active domain adaptation (ADA) aims to improve the model adaptation performance by incorporating active learning (AL) techniques to label a maximally-informative subset of target samples. Conventional AL methods do not consider the existence of domain shift, and hence, fail to identify the truly valuable samples in the context of domain adaptation. To accommodate active learning and domain adaption, the two naturally different tasks, in a collabo-rative framework, we advocate that a customized learn-ing strategy for the target data is the key to the success of ADA solutions. We present Divide-and-Adapt (DiaNA), a new ADA framework that partitions the target instances into four categories with stratiﬁed transferable properties.
With a novel data subdivision protocol based on uncertainty and domainness, DiaNA can accurately recognize the most gainful samples. While sending the informative instances for annotation, DiaNA employs tailored learning strate-gies for the remaining categories. Furthermore, we pro-pose an informativeness score that uniﬁes the data parti-tioning criteria. This enables the use of a Gaussian mix-ture model (GMM) to automatically sample unlabeled data into the proposed four categories. Thanks to the “divide-and-adapt” spirit, DiaNA can handle data with large vari-ations of domain gap. In addition, we show that DiaNA can generalize to different domain adaptation settings, such as unsupervised domain adaptation (UDA), semi-supervised domain adaptation (SSDA), source-free domain adaptation (SFDA), etc. 1.

Introduction
Domain adaptation (DA) approaches strive to general-ize model trained on a labeled source domain to a target domain with rare annotation [5, 13, 16, 24] by coping with
†Corresponding author is Guanbin Li.
Source
Centroids
Target (1) Confident
Consistent (2) Uncertain
Consistent (3) Uncertain
Inconsistent (4) Confident
Inconsistent
Backpack
Pan
Fan
Bike
TV
For source model: 
Transferable centroids
Transferable margins
Need
Annotation
Challenging
Samples
Figure 1. The illustration of our proposed Divide-and-Adapt mechanism to divide target samples into different data subsets for customized learning. the domain disparity. Nevertheless, DA methods are signiﬁ-cantly outperformed by their supervised counterparts due to the scarceness of annotation as demonstrated in [4, 14, 27].
In practice, it is cost-effective to get a moderate amount of target samples labeled to boost the performance of do-main adaptation. Active learning (AL) approaches seek to select samples with uncertainty [9, 12, 29, 30] and di-versity [19, 25] to best beneﬁt the model, which properly matches the demand. However, previous AL methods as-sume that both the labeled and unlabeled data follow the same distribution, such a strategy may become ineffective to the DA scenarios where the target data suffer from do-main shift. The recently proposed active domain adaptation (ADA) [4, 22, 31] aims to resolve this issue by actively se-lecting the maximally-informative instances such that the performance of the transferred model can be best boosted with a limited annotation budget.
The key to the success of ADA is to strike a good balance between the highly coupled yet inherently different tasks: active learning and domain adaptation. The real-world tar-get data typically exhibit either of the two characteristics:
source-like or target-speciﬁc. While the former has simi-lar feature distribution with the source data, the latter tends to be the unique part of target domain and deviates greatly from the source distribution [4, 26, 38]. On one hand, to achieve domain adaptation, applying the same adaptation strategy to all target data equally cannot generalize well to scenarios with varying degrees of domain shift. This is particularly true when the gap between the source-like and target-speciﬁc data is unknown. On the other hand, in active learning tasks, the samples with a learning difﬁculty will be more likely to be selected for labeling. Nonetheless, with large domain gap, incorporating such difﬁcult samples in the adaptation task would hamper the learning of the adapta-tion model, making the training highly unstable. However, despite the impressive progress that has been made, none of the existing works has fully addressed the above issues.
In this work, we propose Divide-And-Adapt (DiaNA), a novel ADA framework that can scale to large variations of domain gaps while achieving cost-effective data label-ing with a signiﬁcant performance boost for domain adapta-tion. Our key observation is that customized learning strate-gies are vital for target data with different characteristics.
In particular, DiaNA divides the target data into four sub-sets with different levels of transferable properties (see Fig-ure 1), each of which is handled with a customized learning strategy. Unlike traditional AL methods that would sim-ply label the most uncertain data [29, 30, 37], we propose to withhold the most challenging samples (Figure 1 cate-gory (4)) for training the domain adaption models. Instead, the selected samples for active annotation would maintain a proper stimulus for the source model, providing informative domain knowledge without jeopardizing the training stabil-ity. The subdivision of target data is dynamically updated as the domain disparity is gradually mitigated with more la-beled data. Hence, the previous challenging samples could be classiﬁed as transferable in the later stage and exploited in the network training.
We introduce a novel protocol for subdividing the target samples for customized learning. In addition to the uncer-tainty of model prediction, we advocate that the consistency with the learned prior distribution, i.e. the domainness, is another key criterion for active domain adaptation [4, 26].
To this end, we divide the target data into four categories as shown in Figure 1 according to the domainness and uncer-tainty of the instances. We further propose that the samples with 1) being uncertain to the model and 2) having incon-sistent prediction with the label of its closest category pro-totype in the learned feature space (i.e. high domainness) are the most “proﬁtable” instances for bringing informative knowledge of target domain if annotated. Thereby, we iden-tify the uncertain inconsistent samples for labeling while applying tailored learning strategies for the remaining cate-gories to boost the selectivity of the sampling.
To avoid heuristic thresholding for data subdivision, we propose an automatic data sampling mechanism based on
Gaussian mixture model (GMM). In particular, we propose an informativeness function that incorporates the domain-ness and uncertainty in a uniﬁed scoring system. The com-puted informativeness score of the labeled data is used to train a four-component GMM model, which is then applied to sample the unlabeled target data into four categories.
We evaluate DiaNA over a large variety of domain shift scenarios on DomainNet [21], Ofﬁce-Home [28] and
CIFAR-10 [11]. Furthermore, the proposed sampling strat-egy of DiaNA can be generalized to various domain adap-tion problems with different supervision settings, including unsupervised domain adaptation (UDA), semi-supervised domain adaptation (SSDA), and source-free domain adap-tation (SFDA).
We summarize our contributions as follows:
• A general “divide-and-adapt” framework, coded Di-aNA, for active domain adaptation that can handle di-versiﬁed degrees of domain gaps while being able to generalize to different domain adaptation problems, in-cluding UDA, SSDA, and SFDA.
• A new target data partition strategy based on domain-ness and uncertainty that enables stratiﬁed learning to achieve more stable training, superior adaptation per-formance, and better generality.
• A novel informativeness scoring system and the cor-responding sampling paradigm based on GMM model for automatic data partitioning.
• New state-of-the-art performance over the mainstream public datasets in the task of active domain adaptation. 2.