Abstract
Visual anomaly classification and segmentation are vi-tal for automating industrial quality inspection. The fo-cus of prior research in the field has been on training custom models for each quality inspection task, which re-quires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation.
Recently CLIP, a vision-language model, has shown rev-olutionary generality with competitive zero-/few-shot per-formance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggre-gation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension Win-CLIP+, which uses complementary information from nor-mal images. In MVTec-AD (and VisA), without further tun-ing, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AU-ROC in zero-shot anomaly classification and segmentation while WinCLIP+ does 93.1%/95.2% (83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins. 1.

Introduction
Visual anomaly classification (AC) and segmentation (AS) classify and localize defects in industrial manufac-turing, respectively, predicting an image or a pixel as nor-mal or anomalous. Visual inspection is a long-tail problem.
The objects and their defects vary widely in color, texture, and size across a wide range of industrial domains, includ-ing aerospace, automobile, pharmaceutical, and electronics.
These result in two main challenges in the field.
First, defects are rare with wide range of variations, leading to a lack of representative anomaly samples in the
†Work done during an Amazon internship.
∗The authors contributed equally.
‡Work done as part of AWS AI Labs. 1few-shot and few-normal-shot are used interchangeably in our case.
Figure 1. Language guided zero-/one-shot1anomaly segmentation from WinCLIP/WinCLIP+. Best viewed in color and zoom in. training data. Consequently, existing works have mainly focused on one-class or unsupervised anomaly detection
[2,7,8,20,29,31,51,57], which only requires normal images.
These methods typically fit a model to the normal images and treat any deviations from it as anomalous. When hundreds or thousands of normal images are available, many methods achieve high-accuracy on public benchmarks [3, 8, 31]. But in the few-normal-shot regime, there is still room to improve performance [14, 32, 39, 57], particularly in comparison with the fully-supervised upper bound.
Second, prior work has focused on training a bespoke model for each visual inspection task, which is not scalable across the long-tail of tasks. This motivates our interest in zero-shot anomaly classification and segmentation. But many defects are defined with respect to a normal image.
For example, a missing component on a circuit board is most easily defined with respect to a normal circuit board with all components present. For such cases, at least a few normal images are needed. So in addition to the zero-shot case, we also consider the case of few-normal-shot anomaly classification and segmentation. Since only few normal images are available, there is no segmentation supervision for localizing anomalies, making this a challenging problem across the long-tail of tasks.
Figure 2. Motivation of language guided visual inspection. (a) Language helps describe and clarify normality and anomaly; (b) Aggregating multi-scale features helps identify local defects; (c) Normal images provide rich referencing content to visually define normality
Vision-language models [1, 18, 27, 36] have shown promise in zero-shot classification tasks. Large-scale train-ing with vision-language annotated pairs learns expressive representations that capture broad concepts. Without addi-tional fine-tuning, text prompts can then be used to extract knowledge from such models for zero-/few-shot transfer to downstream tasks including image classification [27], object detection [11] and segmentation [45]. Since CLIP is one of the few open-source vision-language models, these works build on top of CLIP, benefiting from its generalization abil-ity, and showing competitive low-shot performances in both seen and unseen objects compared to full supervision.
In this paper, we focus on zero-shot and few-normal-shot (1 to 4) regime, which has received limited atten-tion [14, 32, 39]. Our hypothesis is that language is perhaps even more important for zero-shot/few-normal-shot anomaly classification and segmentation. This hypothesis stems from multiple observations. First, “normal” and “anomalous” are states [17] of an object that are context-dependent, and lan-guage helps clarify these states. For example, “a hole in a cloth” may be a desirable or undesirable depending upon whether distressed fashion or regular fashion clothes are be-ing manufactured. Language can bring such context and specificity to the broad “normal” and “anomalous” states.
Second, language can provide additional information to dis-tinguish defects from acceptable deviations from normality.
For example, in Figure 2(a), language provides informa-tion on the soldering defect, while minor scratches/stains on background are acceptable. In spite of these advantages, we are not aware of prior work leveraging vision-language models for anomaly classification and segmentation. In this work, with the pre-trained CLIP as a base model, we show and verify our hypothesis that language aids zero-/few-shot anomaly classification/segmentation.
Since CLIP is one of the few open-source vision-language models, we build on top of it. Previously, CLIP-based meth-ods have been applied for zero-shot classification [27]. CLIP can be applied in the same way to anomaly classification, using text prompts for “normal” and “anomalous” as classes.
However, we find na¨ıve prompts are not effective (see Ta-ble 3). So we improve the na¨ıve baseline with a state-level word ensemble to better describe normal and anomalous states. Another challenge is that CLIP is trained to enforce cross-modal alignment only on the global embeddings of image and text. However, for anomaly segmentation we seek pixel-level classification and it is non-trivial to extract dense visual features aligned with language for zero-shot anomaly segmentation. Therefore, we propose a new Window-based
CLIP (WinCLIP), which extracts and aggregates the multi-scale features while ensuring vision-language alignment.
The multiple scales used are illustrated in Figure 2(b). To leverage normal images available in the few-normal-shot setting, we introduce WinCLIP+, which aggregates comple-mentary information from the language driven WinCLIP and visual cues from the normal reference images, such as the one shown in Figure 2(c). We emphasize that our zero-shot models do not require any tuning for individual cases, and the few-normal-only setup does not use any segmentation annotation, facilitating applicability across a broad range of visual inspection tasks. As a sample, Figure 1 illustrates
WinCLIP and WinCLIP+ qualitative results for a few cases.
To summarize, our main contributions are:
• We introduce a compositional prompt ensemble, which improves zero-shot anomaly classification over the na¨ıve CLIP based zero-shot classification.
• Using the pre-trained CLIP model, we propose Win-CLIP, that efficiently extract and aggregate multi-scale spatial features aligned with language for zero-shot anomaly segmentation. As far as we know, we are the first to explore language-guided zero-shot anomaly classification and segmentation.
• We propose a simple reference association method, which is applied to multi-scale feature maps for im-age based few-shot anomaly segmentation. WinCLIP+ combines the language-guided and vision-only methods for few-normal-shot anomaly recognition.
• We show via extensive experiments on MVTec-AD and VisA benchmarks that our proposed methods Win-CLIP/WinCLIP+ outperform the state-of-the-art meth-ods in zero-/few-shot anomaly classification and seg-mentation with large margins.
2.