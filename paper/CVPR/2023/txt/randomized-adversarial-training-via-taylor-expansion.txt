Abstract
In recent years, there has been an explosion of research into developing more robust deep neural networks against adversarial examples. Adversarial training appears as one of the most successful methods. To deal with both the ro-bustness against adversarial examples and the accuracy over clean examples, many works develop enhanced ad-versarial training methods to achieve various trade-offs be-tween them [19, 38, 80]. Leveraging over the studies [8, 32] that smoothed update on weights during training may help find flat minima and improve generalization, we suggest reconciling the robustness-accuracy trade-off from another perspective, i.e., by adding random noise into determinis-tic weights. The randomized weights enable our design of a novel adversarial training method via Taylor expansion of a small Gaussian noise, and we show that the new adversar-ial training method can flatten loss landscape and find flat minima. With PGD, CW, and Auto Attacks, an extensive set of experiments demonstrate that our method enhances the state-of-the-art adversarial training methods, boosting both robustness and clean accuracy. The code is available at https://github.com/Alexkael/Randomized-Adversarial-Training. 1.

Introduction
Trade-off between adversarial robustness and clean ac-curacy has recently been intensively studied [63, 68, 80] and demonstrated to exist [34, 58, 70, 77]. Many differ-ent techniques have been developed to alleviate the loss of clean accuracy when improving robustness, including data augmentation [1, 5, 29], early-stopping [60, 81], instance reweighting [3, 82], and various adversarial training meth-ods [11, 36, 42, 74, 80]. Adversarial training is believed to be the most effective defense method against adversarial at-tacks, and is usually formulated as a minimax optimization problem where the network weights are assumed determin-istic in each alternating iteration. Given the fact that both clean and adversarial examples are drawn from unknown distributions which interact with one another through the network weights, it is reasonable to relax the assumption that neural networks are simply deterministic models where weights are scalar values. This paper is based on a view, il-lustrated in Fig. 1, that randomized models enable the train-ing optimization to consider multiple directions within a small area and may achieve smoothed weights update – in a way different from checkpoint averaging [8, 32] – and ob-tain robust models against new clean/adversarial examples.
Building upon the above view, we find a way, drastically different from most existing studies in adversarial training, to balance robustness and clean accuracy, that is, embed-ding neural network weights with random noise. Whilst the randomized weights framework is not new in statistical learning theory, where it has been used in many previous works for e.g., generalization analysis [18, 52, 75], we hope to advance the empirical understanding of the robustness-accuracy trade-off problem in adversarial training by lever-aging the rich tool sets in statistical learning. Remarkably, it turns out adversarial training with the optimization over randomized weights can improve the state-of-the-art adver-sarial training methods over both the adversarial robustness and clean accuracy.
By modeling weights as randomized variables with an artificially injected weight perturbation, we start with an empirical analysis of flatness of loss landscape in Sec. 3.
We show that our method can flatten loss landscape and find flatter minima in adversarial training, which is generally re-garded as an indicator of good generalization ability. After the flatness analysis, we show how to optimize with ran-domized weights during adversarial training in Sec. 4. A
Figure 1. A conceptual illustration of decision boundaries learned via (a) adversarial training of TRADES and (b) our method. (a) shows that TRADES considers a deterministic model and optimizes the distance of adversarial data and boundary only through one direction. Our method in (b) takes into account randomized weights (perturbed boundaries) and optimizes the distance of adversarial data and boundary via multi directions in a small area. The boundary learned by our method can be smoother and more robust against new data. novel adversarial training method based on TRADES [80] is proposed to reconcile adversarial robustness with clean accuracy by closing the gap between clean latent space and adversarial latent space over randomized weights. Specifi-cally, we utilize Taylor series to expand the objective func-tion over weights, in such a way that we can deconstruct the function into Taylor series (e.g., zeroth term, first term, sec-ond term, etc). From an algorithmic viewpoint, these Taylor terms can thus replace the objective function effectively and time-efficiently. As Fig. 1 shows, since our method takes randomized models into consideration during training, the learned boundary is smoother and the learned model is more robust in a small perturbed area.
We validate the effectiveness of our optimization method with the first and the second derivative terms of Taylor se-ries. In consideration of training complexity and efficiency, we omit the third and higher derivative terms. Through an extensive set of experiments on a wide range of datasets (CIFAR-10 [40], CIFAR-100, SVHN [51]) and model ar-chitectures (ResNet [27], WideResNet [78], VGG [65],
MobileNetV2 [62]), we find that our method can further enhance the state-of-the-art adversarial training methods on both adversarial robustness and clean accuracy, con-sistently across the datasets and the model architectures.
Overall, this paper makes the following contributions:
• We conduct a pilot study of the trade-off between ad-versarial robustness and clean accuracy with random-ized weights, and offer a new insight on the smoothed weights update and the flat minima during adversarial training (Sec. 3).
• We propose a novel adversarial training method under a randomized model to smooth the weights. The key enabler is the Taylor series expansion (in Sec. 4) of the robustness loss function over randomized weights (deterministic weights with random noise), so that the optimization can be simultaneously done over the ze-roth, first, and second orders of Taylor series. In doing so, the proposed method can effectively enhance ad-versarial robustness without a significant compromise on clean accuracy.
• An extensive set of empirical results are provided to demonstrate that our method can improve both robust-ness and clean accuracy consistently across different datasets and different network architectures (Sec. 5). 2. Preliminaries
Basic Notation. Consider the classification task with the training set S = {s1, ..., sm} where m samples are drawn from the data distribution D. For notational convenience, we omit the label y of sample s. The adversarial exam-ple s′ is generally not in the natural dataset S, such that
||s′ − s||p ≤ ϵ where || · ||p is by default the ℓp-norm. Let fw(·) be a learning model parameterized by w, where w is the vectorization of weight matrix W. L(fw(s), y) is the cross-entropy loss between fw(s) and y with normalization.
Define L(fw(S), Y) := Es∈S [L(fw(s), y)].
Vanilla Adversarial Training. Adversarial training can be formulated as a minimax optimization problem [45] min w (cid:110) (cid:104)
E s∼D max s′:||s′−s||p≤ϵ
L(fw(s′), y)
, (1) (cid:105)(cid:111) where s′ is an adversarial example causing the largest loss within an ϵ-ball centered at a clean example s with respect to a norm distance. Adversarial training aims to learn a model with the loss of adversarial examples minimized, without caring about the loss of the clean examples.
TRADES-like Adversarial Training. Advanced ad-versarial training methods, such as TRADES [80], strike a trade-off between clean accuracy and adversarial robust-ness, by optimizing the following problem min w (cid:110) (cid:104)
E s∼D
L(fw(s), y) + max s′:||s′−s||p≤ϵ
L(fw(s), fw(s′))/λ (cid:105)(cid:111)
, where the first term contributes to clean accuracy, and the second term with hyperparameter λ can be seen as a regular-ization for adversarial robustness that balances the outputs of clean and adversarial examples.
Nevertheless, each adversarial example s′ is generated from a clean example s based on a specific weight w, and
Figure 2. Comparison of loss landscapes of TRADES trained model (the first row) and TRADES+1st+2nd (our method, the second row) trained model. Loss plots in each column are generated from the same original image randomly chosen from the CIFAR-10 test dataset.
Following the settings in [20], the z axis represents the loss, the x and y axes represent the magnitude of the perturbation added in the directions of sign∇sf (s) and Rademacher(0.5) respectively. We provide more empirical results in Appendix A. the weight w is obtained by optimizing both clean and ad-versarial data with SGD. This is a chick-and-egg problem.
A not-very-well-trained model w may generate some mis-leading adversarial data samples, which in turn drive the training process away from the optimum.
Inspired by the idea of model smoothing, we believe that the introduction of noise term into the training process may smooth the update of weights and thus robustify the mod-els. To elaborate on this, we introduce our method from the perspective of flatness of loss landscape in Sec. 3. This is followed by a formalization in Sec. 4, where a noise term is injected to network weight w during adversarial training to transform deterministic weights into randomized weights. 3. Motivation
Flatness is commonly thought to be a metric of standard generalization: the loss surface at the final learnt weights for well-generalizing models is generally “flat” [18, 21, 35, 52].
Moreover, [73] believed that a flatter adversarial loss land-scape reduces the generalization gap in robustness. This aligns with [28], where the authors demonstrated that the lo-cal Lipschitz constant can be used to explicitly quantify the robustness of machine learning models. Many empirical de-fensive approaches, such as hessian/curvature-based regu-larization [48], gradient magnitude penalty [71], smoothen-ing with random noise [44], or entropy regularization [33], have echoed the flatness performance of a robust model.
However, all the above approaches require significant com-putational or memory resources, and many of them, such as hessian/curvature-based solutions, may suffer from stan-dard accuracy decreases [26].
Stochastic weight averaging (SWA) [32] is known to find solutions that are far flatter than SGD, is incredibly sim-ple to implement, enhances standard generalization, and has almost no computational overhead. It was developed to guarantee weight smoothness by simply averaging various checkpoints throughout the training trajectory. SWA has been applied to semi-supervised learning, Bayesian infer-ence, and low-precision training. Especially, [8] used SWA for the first time in adversarial training in order to smooth the weights and locate flatter minima, and showed that it enhances the adversarial generalization.
Different from SWA, we propose a novel method (with details provided in Sec. 4) via Taylor expansion of a small
Gaussian noise to smooth the update of weights. This method regards the TRADES [80], a state-of-the-art adver-sarial training method, as a special case with only zero-th term, and is able to locate flatter minima. To demonstrate the latter, we visualize the loss landscape with respect to both input and weight spaces. As shown in Fig. 2, com-pared to the TRADES, our method significantly flattens the rugged landscape. In addition, it is worth noting that our empirical results (in Sec. 5) demonstrate that our method can further improve not only clean accuracy but also adver-sarial accuracy compared with other SOTA (includes SWA from [8]). 4. Methodology
Given a deterministic model of weight w, we consider an additive small Gaussian noise u to smooth w. Based on TRADES [80], we propose that the robust optimization problem can be further enhanced by (cid:104)
Eu min w
Es(L(fw+u(s), y))
+ Es max s′:||s′−s||p≤ϵ
KL(fw+u(s)||fw+u(s′))/λ (2) (cid:105)
, where the first term contributes to clean accuracy, and the second term with hyperparameter λ can be seen as a regu-Algorithm 1 Adversarial training with randomized weights
Input: minibatch {si}n i=1, network architecture parametrized by w, learning rate ηl, step size ηs, hyper-parameters η, λ, zero mean Gaussian u, number of iterations K in inner optimization.
Output: Robust network fw
Randomly initialize network fw, or initialize network with pre-trained configuration repeat
▶ Generate adversarial examples:
Sample u from u for i = 1 to n do s′ i ← si + 0.001 · N (0, I) for k = 1 to K do s′ i ← Π(ηssign(∇s′ operator. i end for
L(fw+u(si), fw+u(s′ i))) + s′ i), subject to ||si − s′ i||p ≤ ϵ, where Π is the projection end for
▶ Optimization: w ← w − ηl (cid:80)n i=1 ∇w (cid:104) 1 n (zeroth term) (first term) (second term)
L(fw(si), yi)
+L(gsi (w), gs′ (cid:0)L(g′
+ηEu si (cid:0)L(uT g′′
+ η
Eu si 2 i (w))/λ (w)T u, g′ s′ i (w)u, uT g′′ s′ i (w)T u)(cid:1) (w)u)(cid:1)(cid:105)
, where various optimization methods can be applied: zeroth term optimization (TRADES), zeroth + first terms opti-mization and zeroth + first + second terms optimization. until training converged larization for adversarial robustness with weights w and a small Gaussian noise u.
Following [17, 35], we let the Gaussian u be small enough to maintain the generalization performance on clean data. Then, we can extend Eq. (2) by replacing KL(·||·) with a multi-class calibrated loss L(·, ·) [80]: (cid:104)
Es(L(fw(s), y))
Eu min w
+ Es max s′:||s′−s||p≤ϵ
L(fw+u(s), fw+u(s′))/λ (3) (cid:105)
.
As the minimax optimization is entangled with expecta-tion of randomized weights, it is challenging to solve such optimization problem directly. Instead, we propose to solve this problem in an alternating manner between the inner maximization and outer minimization. (cid:16) s′ t+1 ← Π
For the inner maximization, with a given model weight w, we solve it by adopting the commonly used gradient-based approach (e.g., PGD) to generate the adversarial per-turbation. That is, at the t-th iteration, the adversarial per-turbation is produced iteratively by letting t + ηssign(cid:0)∇s′ s′ t))(cid:1)(cid:17) (4) until the maximum allowed iterations are reached, where Π is the projection operator, ηs is the step size and u is a sam-ple of u. To reduce complexity, we only use one sample u to generate adversarial example for each minibatch. Never-theless, sufficient samples of u can still be produced in one
L(fw+u(s), fw+u(s′ t epoch as there are many minibatches. As Fig. 1 shows, the adversarial example is generated by one model (boundary), then the optimization with randomized weights in multiple directions can be executed through the following minimiza-tion method.
For the outer minimization, when the optimal perturbed sample s′ is generated by the inner maximization, we solve the following problem (cid:104)
L(fw(s), y) + EuL(fw+u(s), fw+u(s′))/λ
Es (cid:105)
. min w (5)
This optimization problem is challenging due to the entan-glement between the deterministic weight w and the ran-dom perturbation u, which makes gradient updating much involved.
To resolve this issue, we decompose the two components in such a way that gradient updating and model averaging can be done separately. Let fw+u(s) = gs(w + u), by
Taylor expansion of gs(w + u) at w, we have s(w)T u + uT g′′ s (w) 2! gs(w + u) = gs(w) + g′ u (6)
+ Os(||u||2), s(w) = ∂gs(w)
∂w ∈ Rd×1 and g′′ where g′
∂w∂w ∈
Rd×d are the first and second derivative of the function gs(w) versus the weight w, respectively, and Os(||u||2) tends to be zero with respect to ||u||2. By such approxi-mation, the gradient computation will be solely done on the s (w) = ∂2gs(w)
deterministic part w, and model smoothing with averaged random variable u is done independently.
The intuition behind is the following. For the random-ized model w + u with a specific input s, the Taylor ap-proximation explicitly takes into account the deterministic model gs(w), the projection of its gradient onto the random s(w)T u, and the projection of its curvature onto direction g′ the subspace spanned by uT g′′ s (w) 2! u, capturing higher-order information of the loss function with respect to u.
To further simplify the computation, we minimize an up-per bound of Eq. (5) instead. Due to the local convexity of loss landscape, by Jensen’s inequality, we conclude that
L(cid:0) (cid:88) xi, (cid:88) (cid:1) ≤ yi (cid:88)
L(xi, yi). (7) i i i
Therefore, the second term in Eq. (5) can be upper bounded by
EuL(fw+u(s), fw+u(s′))
≤ L(gs(w), gs′(w)) + Eu (cid:0)L(uT g′′
+
Eu s (w)u, uT g′′ (cid:0)L(g′ s(w)T u, g′ s′(w)u)(cid:1), 1 2 s′(w)T u)(cid:1) (8) where the terms on the right-hand side are referred to as the zeroth, first, and second order Taylor expressions. In-stead of minimizing Eq. (5) directly, we minimize the up-per bound in Eq. (8), which is easier to compute in prac-tical models. Details of the optimization are given in Ap-pendix D.
The pseudo-code of the proposed adversarial train-ing method is presented in Algorithm 1. Note that the zeroth term optimization in Algorithm 1 is almost the same as TRADES [80], thus we use TRADES and AWP-TRADES [74] as (zeroth term optimization) baselines in our experiments in Sec. 5, and verify how much first and second terms optimization can improve. Note that we per-form both AWP and our method through applying random-ized weight perturbations on AWP updated weights.
For all of our experiments, we limit the noise variance of u to be small enough [17, 35] to maintain the training, e.g.,
σ = 0.01. For the zeroth term hyper-parameter 1/λ in Al-gorithm 1, we set 1/λ = 6 for all of our experiments, which is a widely-used setting for TRADES [53,74,80]. The num-ber of inner optimization iterations K is set to 10 as usual.
To make our method more flexible, we set η and η 2 as the first term and second term hyper-parameters, respectively, and analyze the sensitivity of η in Sec. 5.1.
The main novelty of our proposed method is two-fold: (1) Instead of considering a single model with a determin-istic weight w, we consider a model ensemble with w + u to smooth the update of weights. By averaging these mod-els during adversarial training, we come up with a robust model with smoothed classification boundary tolerant to po-tential adversarial weight perturbation. (2) When averaging over the ensemble of randomized models during training, we disentangle gradient from random weight perturbation so that the gradient updating can be computed efficiently.
In particular, we apply Taylor expansion at the mean weight w (i.e., the deterministic component) and approximate the cross-entropy loss function with the zeroth, first, and second
Taylor terms. In doing so, the deterministic and statistical components of w + u can be decomposed and then com-puted independently. 5. Empirical results
In this section, we first discuss the hyper-parameter sen-sitivity of our method, and then evaluate the robustness on benchmark datasets against various white-box, black-box attacks and Auto Attack with ℓ2 and ℓ∞ threat models.
Adversarial Training Setting. We train PreAct ResNet-18 [27] for ℓ∞ and ℓ2 threat models on CIFAR-10/100 [40] and SVHN [51]. In addition, we also train WideResNet-34-10 [78] for CIFAR-10/100, VGG16 and MobileNetV2 for
CIFAR-10, with ℓ∞ threat model. We adopt the widely used adversarial training setting [60]: for the ℓ∞ threat model,
ϵ = 8/255 and step size 2/255; for the ℓ2 threat model,
ϵ = 128/255 and step size 15/255. For normal adversarial training, the training examples are generated with 10 steps.
All models (except SVHN) are trained for 200 epochs us-ing SGD with momentum 0.9, batchsize 128, weight decay 5 × 10−4, and an initial learning rate of 0.1 that is divided by 10 at the 100th and 150th epochs. Except for setting the starting learning rate to 0.01 for SVHN, we utilize the same other settings. Simple data augmentations are used, such as 32 × 32 random crop with 4-pixel padding and ran-dom horizontal flip. We report the highest robustness that ever achieved at different checkpoints for each dataset and report the clean accuracy on the model which gets the high-est PGD-20 accuracy. We omit the standard deviations of 3 runs as they are very small (< 0.40%) and implement all models on NVIDIA A100.
Evaluation Setting. We evaluate the robustness with wihte-box attacks, black-box attacks and auto attack. For white-box attacks, we adopt PGD-20 [45] and CW-20 [4] (the ℓ∞ version of CW loss optimized by PGD-20) to eval-uate trained models. For black-box attacks, we gener-ate adversarial perturbations by attacking a surrogate nor-mal adversarial training model (with same setting) [55], and then apply these adversarial examples to the defense model and evaluate the performances. The attacking meth-ods for black-box attacks we have used are PGD-20 and
CW-20. For Auto Attack (AA) [10], one of the strongest attack methods, we adopt it through a mixture of differ-ent parameter-free attacks which include three white-box attacks (APGD-CE [10], APGD-DLR [10], and FAB [9]) and one black-box attack (Square Attack [2]). We provide more details about the experimental setups in Appendix B.
Table 1. First and Second Derivative terms optimization on CIFAR-10 with ℓ∞ threat model for PreAct ResNet18. Classification accuracy (%) on clean images and under PGD-20 attack, CW-20 attack and Auto Attack with different hyper-parameters η = 0.05, 0.1, 0.2, 0.3, 0.4.
We highlight the best results in bold.
Method 1st (0.05) 1st (0.1) 1st (0.2) 1st (0.3) 1st (0.4)
Clean 83.19 82.86 83.55 83.96 83.93
PGD-20 CW-20 AA 48.2 49.3 48.8 49.7 48.6 53.98 54.29 54.86 55.05 54.65 52.12 52.24 52.65 52.54 52.23
Method 1st+2nd (0.05) 1st+2nd (0.1) 1st+2nd (0.2) 1st+2nd (0.3) 1st+2nd (0.4)
Clean 83.25 83.47 84.13 84.27 84.14
PGD-20 CW-20 AA 48.4 49.7 50.3 48.9 49.6 51.93 52.28 52.53 51.80 51.56 54.07 54.42 54.91 54.36 54.38
Table 2. First and Second Derivative terms optimization on CIFAR-10/CIFAR-100 with ℓ∞ threat model for WideResNet, compared with current state-of-the-art. Classification accuracy (%) on clean images and under PGD-20 attack, CW-20 attack (ϵ = 0.031) and Auto Attack (ϵ = 8/255). The results of our methods are in bold. Note that ∗ is under PGD-40 attack and ∗∗ is under PGD-10 attack.
Dataset
CIFAR-10
ℓ∞
CIFAR-100
ℓ∞
Method
+ Ours (1st)
+ Ours (1st+2nd)
Architecture Clean 92.56
WRN-34-10
Lee et al. (2020) [42] 83.51
WRN-34-10
Wang et al. (2020) [72] 85.34
WRN-34-20
Rice et al. (2020) [60] 84.52
WRN-34-10
Zhang et al. (2020) [81] 86.43
WRN-34-20
Pang et al. (2021) [54] 86.01
WRN-34-20
Jin et al. (2022) [36] 85.29
Gowal et al. (2020) [24]
WRN-70-16 84.65
Zhang et al. (2019) [80] (0th) WRN-34-10 85.51
WRN-34-10 85.98
WRN-34-10 85.17
WRN-34-10 86.10
+ Ours (1st)
WRN-34-10 86.12
+ Ours (1st+2nd)
WRN-34-10 60.43
WRN-34-10
Cui et al. (2021) [11] 60.86
Gowal et al. (2020) [24]
WRN-70-16 60.22
Zhang et al. (2019) [80] (0th) WRN-34-10 63.01
WRN-34-10 62.93
WRN-34-10 60.38
WRN-34-10 63.98
WRN-34-10 64.71
WRN-34-10
+ Ours (1st)
+ Ours (1st+2nd)
+ Ours (1st)
+ Ours (1st+2nd)
Wu et al. (2020) [74] (0th)
Wu et al. (2020) [74] (0th)
PGD-20 CW-20 54.53 54.33
---57.93
-54.49 56.06 56.13 57.33 58.09 58.22 31.50
-28.93 29.44 29.61 30.78 31.63 31.41 59.75 58.31
--57.91∗∗ 61.12 58.22∗ 56.68 58.34 58.47 59.64 61.47 61.45 35.50 31.47∗ 32.11 33.26 33.36 34.09 35.36 35.73
AA 39.70 51.10 53.42 53.51 54.39 55.90 57.20 53.0 54.0 54.2 56.2 57.1 57.4 29.34 30.03 26.9 28.1 27.9 28.6 29.8 30.2 5.1. Sensitivity of hyper-parameter
In our proposed algorithm, the regularization hyper-parameter η is crucial. We use numerical experiments on
CIFAR-10 with PreAct ResNet-18 to illustrate how the reg-ularization hyper-parameter influences the performance of our robust classifiers. To develop robust classifiers for multi-class tasks, we use the gradient descent update for-mula in Algorithm 1, with L as the cross-entropy loss.
Note that, for the zeroth term hyper-parameter 1/λ in Algo-rithm 1, we set 1/λ = 6 for all of our experiments, which is a widely used setting for TRADES. We train the models with η = 0.05, 0.1, 0.2, 0.3, 0.4 respectively. All models are trained with 200 epochs and 128 batchsize.
In Tab. 1, we observe that as the regularization parame-ter η increases, the clean accuracy and robust accuracy al-most both go up first and then down. In particular, the ac-curacy under Auto Attack is sensitive to the regularization hyper-parameter η. Considering both robustness accuracy and clean accuracy, it is not difficult to find that η = 0.3 is the best for first term optimization and η = 0.2 is the best for first + second terms optimization. Thus in the following experiments, we set η = 0.3 for first term optimization and
η = 0.2 for first + second terms optimization as default. 5.2. Comparison with SOTA on WideResNet
In Tab. 2, we compare our method with state-of-the-art on WideResNet with CIFAR-10 and CIFAR-100. For ze-roth term optimization, we use TRADES [80] and AWP-TRADES [74] as baselines with 1/λ = 6. We also report other state-of-the-art, include AVMixup [42], MART [72],
[60], [81], [54], TRADES+LBGAT [11] and [24].
The results in Tab. 2 demonstrate that our method can both improve clean accuracy and robustness accuracy con-sistently over different datasets and models. Especially for
Table 3. Adversarial training across datasets on PreAct ResNet18 with ℓ2 threat model. Classification accuracy (%) on clean images and under PGD-20 attack and Auto Attack. The results of our methods are in bold.
Method
Wu et al. (2020) [74](0th)
+ Ours (1st+2nd)
CIFAR-10
PGD 72.08 72.85
Clean 87.05 88.41
AA 71.6 72.0
CIFAR-100
PGD 45.11 46.88
Clean 62.87 65.59
AA 41.8 42.4
SVHN
PGD 72.45 73.07
Clean 92.86 93.98
AA 63.8 64.5
Table 4. Adversarial training across datasets on PreAct ResNet18 with ℓ∞ threat model. Classification accuracy (%) on clean images and black-box attacks. Black-box adversarial examples are generated by a surrogate normal adversarial training model (of same setting) with
PGD-20 attack and CW-20 attack. The results of our methods are in bold.
Method
Wu et al. (2020) [74](0th)
+ Ours (1st+2nd)
CIFAR-10
PGD 61.79 62.56
Clean 82.78 84.27
CW 59.42 60.58
CIFAR-100
PGD 38.55 39.82
Clean 58.33 61.08
CW 36.70 37.84
SVHN
PGD 63.80 66.16
Clean 93.77 95.11
CW 59.54 63.59
Table 5. Adversarial training across VGG16, MobileNetV2 on CIFAR-10 with ℓ∞ threat model. Classification accuracy (%) on clean images and under PGD-20 attack, CW-20 attack and Auto Attack. The results of our methods are in bold.
Method
Zhang et al. (2019) [80] (0th)
+ Ours (1st+2nd)
Wu et al. (2020) [74] (0th)
+ Ours (1st+2nd)
Clean 79.78 80.99 78.46 80.31
VGG16
MobileNetV2
PGD-20 CW-20 AA 44.3 44.4 46.3 46.5 49.88 50.13 51.19 52.71 46.95 47.09 47.41 48.38
Clean 79.73 81.86 79.86 81.95
PGD-20 CW-20 AA 46.4 47.8 47.7 49.4 51.41 53.34 53.56 55.37 48.43 49.93 50.11 51.55
Table 6. Time consumption and GPU memory usage for WideRes-Net on CIFAR-10 with ℓ∞ threat model. We deploy each model on a single NVIDIA A100 with batchsize 128. The results of our methods are in bold.
Method
Zhang et al. (2019) [80] (0th)
+ Ours (1st)
+ Ours (1st+2nd)
WideResNet-34-10
Time/Epoch GPU Memory 1666s 2161s 2362s 12875MB 20629MB 26409MB
Auto Attack, our method can get 57.4% on CIFAR-10 and 30.2% on CIFAR-100 with WRN-34-10, even surpass the performance of WRN-70-16 from [24]. 5.3. Other empirical results
Adversarial training with ℓ2 threat model. For the ex-periments with ℓ2 threat model on CIFAR-10, CIFAR-100 and SVHN in Tab. 3, the results still support that our method can enhance the performance under clean data, PGD-20 at-tack and Auto Attack. For CIFAR-100, it can even increase about 3% on clean accuracy.
Robustness under black-box attacks. We train PreAct
ResNet18 for ℓ∞ threat model on CIFAR-10, CIFAR-100 and SVHN. The black-box adversarial examples are gen-erated by a surrogate normal adversarial training model (of same setting) with PGD-20 attack and CW-20 attack. Tab. 4 shows our method is also effective under black-box attacks.
For SVHN, ours can even obtain a significant improvement over the existing ones.
Robustness on other architectures. We train VGG16 and MobileNetV2 for ℓ∞ threat model on CIFAR-10. Our results in Tab. 5 show a comprehensive improvement under clean data and robustness accuracy. Particularly, for Mo-bileNetV2, ours can achieve an improvement greater than 1% and 2% under Auto Attack and clean data, respectively.
Comparison with SWA from [8]. The best PGD-20 accuracy and AA accuracy on CIFAR-10 for ResNet-18 from [8] are 52.14% and 49.44% respectively, whereas ours are 55.13% and 50.8% in Tab. 1 (with 1st+2nd (0.2)).
More empirical results are given in Appendix E. 5.4. Limitations and future work
We use some approximation methods, e.g., Eq. (8), to reduce the complexity of our adversarial training method.
The results in Tab. 6 show that though our method with
WideResNet-34-10 can work on a single NVIDIA A100, the growth of training time and GPU memory still cannot be ignored. In the future work, we plan to further reduce the complexity of our algorithm and apply the first and sec-ond terms optimization on larger datasets. 6.