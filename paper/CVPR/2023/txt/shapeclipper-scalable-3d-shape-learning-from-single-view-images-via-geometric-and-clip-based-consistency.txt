Abstract 1.

Introduction
We present ShapeClipper, a novel method that recon-structs 3D object shapes from real-world single-view RGB images.
Instead of relying on laborious 3D, multi-view or camera pose annotation, ShapeClipper learns shape re-construction from a set of single-view segmented images.
The key idea is to facilitate shape learning via CLIP-based shape consistency, where we encourage objects with simi-lar CLIP encodings to share similar shapes. We also lever-age off-the-shelf normals as an additional geometric con-straint so the model can learn better bottom-up reasoning of detailed surface geometry. These two novel consistency constraints, when used to regularize our model, improve its ability to learn both global shape structure and local ge-ometric details. We evaluate our method over three chal-lenging real-world datasets, Pix3D, Pascal3D+, and Open-Images, where we achieve superior performance over state-of-the-art methods.1 1Project website at: https : / / zixuanh . com / projects / shapeclipper.html
How can we learn 3D shape reconstruction from real-world images in a scalable way? Recent works achieved impressive results via learning-based approaches either with 3D [4, 9, 11, 28, 36, 41, 43–45, 48, 49, 53] or multi-view su-pervision [16, 19, 23, 25, 32, 42, 50, 51]. However, such su-pervised techniques cannot be easily applied to real-world scenarios, because it is expensive to obtain 3D or multi-view supervision at a large scale. To address this limitation, recent works relax the requirement for 3D or multi-view supervision [1, 8, 10, 14, 15, 17, 18, 22, 24, 29, 31, 38, 46, 55, 57]. These works only require single-view self-supervision, with some additionally using expensive viewpoint annota-tions [17, 18, 24, 38, 57]. Despite this signiﬁcant progress, most methods still suffer from two major limitations: 1) In-correct top-down reasoning, where the model only explains the input view but does not accurately reconstruct the full 3D object shape; 2) Failed bottom-up reasoning, where the model cannot capture low-level geometric details such as concavities. How can we address these limitations while also remaining scalable to a wide range of object types?
To improve top-down reasoning, our inspiration comes
Figure 2. CLIP-based semantic neighbors. Samples that have similar CLIP encodings often have similar shapes. Note the view-point variability in the neighbors. from the recent success of large-scale image-text modeling.
The most successful image-text models such as CLIP [33] are trained on a vast corpus of captioned images and are able to extract ﬁne-grained semantic features that correlate well with the language descriptions. CLIP further demonstrates a great generalization ability to images across various do-mains. Can we leverage such a powerful and generalizable model to learn 3D reconstruction in a real-world scenario?
We observe that natural language descriptions of images often contain geometry-related information (e.g. a round speaker, a long bench) and many nouns by themselves have characteristic shape properties (e.g. “desks” usually have four legs, and “benches” normally include a ﬂat sur-face). Motivated by this intrinsic connection between ob-ject shapes and language-based semantics, we examine the latent space of CLIP’s visual encoder. In our study, we ﬁnd (via k-NN queries) that objects with similar CLIP embed-dings usually share similar shapes (see Fig. 2 for an ex-ample). Another key characteristic we identify with CLIP embeddings is that they have some robustness to viewpoint changes, meaning that changes in viewpoint generally do not produce drastic changes in CLIP embeddings.
Inspired by these ﬁndings, we propose to learn shapes using a semantic-based shape consistency (SSC) constraint using CLIP. Speciﬁcally, we use CLIP’s semantic encodings as guidance to form pseudo multi-view image sets. For each image in the training set, we extract its CLIP embedding and ﬁnd images with the most similar semantics across the training set. We then leverage these retrieved images as ad-ditional supervision to the input view, as illustrated in Fig. 3.
This approach greatly beneﬁts global shape understanding, because each predicted shape is required to simultaneously explain a set of images instead of only explaining the single input image.
On the other hand, we address the limitation of poor
Semantic-based Shape Consistency (SSC) Con-Figure 3. straint. We ﬁnd the semantic neighbors of the input image across the training set and use these neighbors to regularize the shape learning. bottom-up geometric reasoning by constraining the sur-face normals of the predicted shapes. Common failure cases include noisy surface reconstruction and failed con-cavity modeling, which are extremely hard to learn even with multi-view supervision. Inspired by the recent success of large-scale 2.5D estimation that generalizes to various scenes [7, 34, 35], we propose to use off-the-shelf surface normals as additional geometric supervision for our task.
However, unlike scenes, off-the-shelf normals for object-centric images are much noisier due to occlusion, trunca-tion, and domain gaps. To mitigate this issue, we introduce a noise-tolerant optimization process via outlier dropout, which stabilizes the training and improves the overall re-construction performance.
Overall, our contributions are threefold:
• We propose a novel CLIP-based shape consistency regularization that greatly facilitates the top-down un-derstanding of object shapes.
• We successfully leverage off-the-shelf geometry cues to improve single-view object shape reconstruction for the ﬁrst time and handle noise effectively.
• We perform extensive experiments across 3 differ-ent real-world datasets and demonstrate state-of-the-art performance. 2.