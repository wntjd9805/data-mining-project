Abstract
The dilemma between plasticity and stability arises as a common challenge for incremental learning. In contrast, the human memory system is able to remedy this dilemma owing to its multi-level memory structure, which motivates us to pro-pose a Bilevel Memory system with Knowledge Projection (BMKP) for incremental learning. BMKP decouples the functions of learning and remembering via a bilevel-memory design: a working memory responsible for adaptively model learning, to ensure plasticity; a long-term memory in charge of enduringly storing the knowledge incorporated within the learned model, to guarantee stability. However, an emerging issue is how to extract the learned knowledge from the work-ing memory and assimilate it into the long-term memory. To approach this issue, we reveal that the parameters learned by the working memory are actually residing in a redundant high-dimensional space, and the knowledge incorporated in the model can have a quite compact representation under a group of pattern basis shared by all incremental learning tasks. Therefore, we propose a knowledge projection pro-cess to adaptively maintain the shared basis, with which the loosely organized model knowledge of working memory is projected into the compact representation to be remembered in the long-term memory. We evaluate BMKP on CIFAR-10,
CIFAR-100, and Tiny-ImageNet. The experimental results show that BMKP achieves state-of-the-art performance with lower memory usage1. 1.

Introduction
In an ever-changing environment, intelligent systems are expected to learn new knowledge incrementally without for-getting, which is referred to as incremental learning (IL)
*Corresponding author: Yangli-ao Geng (gengyla@bjtu.edu.cn). 1The code is available at https://github.com/SunWenJu123/BMKP
[8,18]. Based on different design principles, many incremen-tal learning methods have been proposed [16,19,24]. Among them, memory-based models are leading the way in perfor-mance and have attracted enormous attention [3, 17, 26, 33].
The core idea of memory-based methods is to utilize partial old-task information to guide the model to learn with-out forgetting [20]. As illustrated in Figure 1 (a), memory-based methods typically maintain a memory to store the old-task information. According to the type of stored informa-tion, memory-based methods further fall into two categories: rehearsal-based and gradient-memory-based. Rehearsal-based methods [3, 17, 28, 29, 39] keep an exemplar memory (or generative model) to save (or generate) old-task samples or features, and replay them to recall old-task knowledge when learning new tasks. However, the parameter fitting of new tasks has the potential to overwrite the old-task knowl-edge, especially when the stored samples are unable to ac-curately simulate old-task data distributions, leading to low stability. Gradient-memory-based methods [26, 33] maintain a memory to store the gradient directions that may interfere with the performance of old tasks, and only update the learn-ing model with the gradients that are orthogonal to the stored ones. Although the gradient directions restriction guarantees stability, this restriction may prevent the model from being optimized toward the right direction for a new task, which would result in low plasticity. Therefore, both of these two types of methods suffer the low plasticity or stability. The reason is that their model is in charge of both learning new task knowledge and maintaining old task knowledge. The limited model capacity will inevitably lead to a plasticity-stability trade-off in the face of a steady stream of knowledge, i.e., plasticity-stability dilemma [21].
In contrast, human brains are known for both high plas-ticity and stability for incremental learning, owing to its multi-level memory system [7]. Figure 1 (b) illustrates how a human brain works in the classic Aktinson-Shiffrin human
Figure 1. Diagram of memory-based incremental learning method (a), Atkinson-Shiffrin human memory model [30] (b), and the architecture of the proposed BMKP (c). memory model [30]. Inspired by the mechanism of human memory, this paper proposes a Bilevel Memory model with
Knowledge Projection (BMKP) for incremental learning. As illustrated in Figure 1 (c), BMKP adopts a bilevel-memory design, including a working memory (corresponds to the short-term memory of the human brain) and a long-term memory. The working memory is implemented as a neural network responsible for adaptively learning new knowledge and inference. The long-term memory is in charge of steadily storing all the learned knowledge. Similar to the human memory, this bilevel-memory structure endows BMKP with both high plasticity and stability by decoupling the functions of learning and remembering.
An emerging issue for this bilevel memory framework is how to extract the learned knowledge from the working memory and assimilate it into the long-term memory. In the working memory, the knowledge is represented as the trained parameters in a high-dimensional space, which we call Pa-rameter Knowledge Space (PKS). However, this space is usually overparameterized [6], implying that the knowledge representation in PKS is loosely organized. Therefore, in-stead of directly storing the learned parameters, we propose to recognize the underlying common patterns, and further utilize these patterns as the basis to represent the parameters.
Specifically, we define the space spanned by these pattern basis as the Core Knowledge Space (CKS), in which the knowledge can be organized in a quite compact form with-out loss of performance. Based on these two knowledge spaces, we propose a knowledge projection process to adap-tively maintain a group of CKS pattern basis shared by all incremental learning tasks, with which the loosely organized model knowledge in PKS can be projected into CKS to ob-tain the compact knowledge representation. The compact representation, instead of the raw model knowledge, is trans-ferred to the long-term memory for storing.
The contributions of this work are summarized as follows:
• Inspired by the multi-level human memory system, we propose a bilevel-memory framework for incremental learning, which benefits from both high plasticity and stability.
• We propose a knowledge projection process to project knowledge from PKS into compact representation in
CKS, which not only improves memory utilization effi-ciency but also enables forward knowledge transfer for incremental learning.
• A representation compaction regularizer (Eq. (4)) is designed to encourage the working memory to reuse previously learned knowledge, which enhances both the memory efficiency and the performance of BMKP.
• We evaluate BMKP on CIFAR-10, CIFAR-100, and
Tiny-ImageNet. The experimental results show that
BMKP outperforms most of state-of-the-art baselines with lower memory usage. 2.