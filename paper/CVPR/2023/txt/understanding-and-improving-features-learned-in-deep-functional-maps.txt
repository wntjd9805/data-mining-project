Abstract
Deep functional maps have recently emerged as a success-ful paradigm for non-rigid 3D shape correspondence tasks.
An essential step in this pipeline consists in learning feature functions that are used as constraints to solve for a functional map inside the network. However, the precise nature of the in-formation learned and stored in these functions is not yet well understood. Specifically, a major question is whether these features can be used for any other objective, apart from their purely algebraic role in solving for functional map matrices.
In this paper, we show that under some mild conditions, the features learned within deep functional map approaches can be used as point-wise descriptors and thus are directly com-parable across different shapes, even without the necessity of solving for a functional map at test time. Furthermore, informed by our analysis, we propose effective modifications to the standard deep functional map pipeline, which promote structural properties of learned features, significantly im-proving the matching results. Finally, we demonstrate that previously unsuccessful attempts at using extrinsic archi-tectures for deep functional map feature extraction can be remedied via simple architectural changes, which encourage the theoretical properties suggested by our analysis. We thus bridge the gap between intrinsic and extrinsic surface-based learning, suggesting the necessary and sufficient conditions for successful shape matching. Our code is available at https://github.com/pvnieo/clover. 1.

Introduction
Computing dense correspondences between 3D shapes is a classical problem in Geometry Processing, Computer
Vision, and related fields, and remains at the core of many tasks including statistical shape analysis [8, 52], registration
[80], deformation [7] or texture transfer [14] among others.
Since its introduction, the functional map (fmap) pipeline
[49] has become a de facto tool for addressing this prob-lem. This framework relies on representing correspondences as linear transformations across functional spaces, by en-coding them as small matrices using the Laplace-Beltrami
Figure 1. Left: Features learned in existing deep functional map pipelines are used in a purely algebraic manner as constraints for a linear system, thus lacking interpretability and clear geometric content. Right: We propose a theoretically-justified modification to this pipeline, which leads to learning robust and repeatable features that enable matching via nearest neighbor search. basis. Methods based on this approach have been success-fully applied with hand-crafted features [5, 67, 71] to many scenarios including near-isometric, [20, 30, 44, 61, 70], non-isometric [17, 32] and partial [12, 35, 36, 63, 76, 77] shape matching. In recent years, a growing body of literature has advocated improving the functional map pipeline by using deeply learned features, pioneered by [34], and built upon by many follow-up works [3, 16, 19, 29, 39, 40, 68, 69]. In all of these methods, the learned features are only used to constrain the linear system when estimating the functional maps inside the network. Thus, no attention is paid to their geometric nature, or potential utility beyond this purely algebraic role.
On the other hand, features learned in other deep match-ing paradigms are the main focus of optimization, and they represent either robust descriptive geometric features that are used directly for matching using nearest neighbor search [6, 13, 24, 33, 37, 78, 79], or as distributions that, at every point, are interpreted as vertex indices on some tem-plate shape [42, 46, 53], or a deformation field that is used to deform the input shape to match a template [26].
In contrast, feature (also known as “probe” [50]) func-tions, within deep functional maps are used purely as an opti-misation tool and thus the information learned and stored in these functions is not yet well-understood. In this work, we aim to show that features in deep functional maps networks can, indeed, have geometric significance and that, under cer-tain conditions, they are directly comparable across shapes, and can be used for matching simply via nearest neighbor search, see Fig. 1.
Specifically, we introduce the notion of feature complete-ness and show that under certain mild conditions, extracting a pointwise map from a functional map or via the nearest neighbor search between learned features leads to the same result. Secondly, we propose a modification of the deep functional map pipeline, by imposing the learned functional maps to satisfy the conditions imposed by our analysis. We show that this leads to a significant improvement in accu-racy, allowing state-of-the-art results by simply performing a nearest-neighbor search between features at test time. Fi-nally, based on our theoretical results, we also propose a modification of some extrinsic feature extractors [73, 75], which previously failed in the context of deep functional maps, which improve their overall performance by a sig-nificant margin. Since our theoretical results hold for the functional map paradigm in general, they can be incorpo-rated in any deep fmap method, hence improving previous methods, and making future methods more robust.
Overall, our contributions can be summarized as follows:
• We introduce the notions of feature completeness and basis aligning functional maps and use them to establish a theoretical result about the nature of features learned in the deep functional map framework.
• Informed by our analysis, we propose simple modifica-tions to this framework, which lead to state-of-the-art results in challenging scenarios.
• Based on our theoretical results, we propose a simple modification to some extrinsic feature extractors, that were previously unsuccessful for deep functional maps, improving their overall accuracy, and bridging the gap between intrinsic and extrinsic surface-based learning. 2.