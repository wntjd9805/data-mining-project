Abstract
Massive data corpora like WebText, Wikipedia, Concep-tual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today’s benchmarks. A notable omiss1ion within this fam-ily of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category.
We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, im-Correspondence to <mattd@allenai.org>. proving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Em-bodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new direc-tions for research and enable new applications across the field of AI. 1.

Introduction
Massive datasets have enabled and driven rapid progress in AI. Language corpora on the web led to large language models like GPT-3 [4]; paired image and text datasets like
Conceptual Captions [63] led to vision-and-language pre-trained models like VilBERT [42]; YouTube video datasets led to video capable models like Merlot-Reserve [82]; and massive multimodal datasets like WebImageText [65] and
LAION [61, 62] led to models like CLIP [55] and StableD-iffusion [59]. These leaps in dataset scale and diversity were triggered by moving from manually curated datasets to har-nessing the power of the web and its creative content.
In contrast to the datasets described above, the size of
the datasets we are feeding to our data-hungry deep learn-ing models in many other areas of research is simply not comparable. For instance, the number of 3D assets used in training generative 3D models is, maximally, on the or-der of thousands [22] and the simulators used to train em-bodied AI models typically have only between a few dozen to a thousand unique scenes [36, 39, 58, 67]. The startling advances brought about by developing large-scale datasets for images, videos, and natural language, demand that an equivalent dataset be built for 3D assets.
We present OBJAVERSE 1.0, a large scale corpus of high-quality, richly annotated, 3D objects; see Fig. 1. Objects in our dataset are free to use1 and sourced from Sketch-fab, a leading online platform for managing, viewing, and distributing 3D models.
In total, OBJAVERSE contains over 800K 3D assets designed by over 150K artists which makes this data large and diversely sourced. Assets not only belong to varied categories like animals, humans, and vehicles, but also include interiors and exteriors of large spaces that can be used, e.g., to train embodied agents.
OBJAVERSE is a universe of rich 3D data with detailed metadata that can support many different annotations to en-able new applications. With this remarkable increase in scale, we see an incredible opportunity for OBJAVERSE to impact research progress across domains. In this work, we provide promising results to answer three questions.
Can 3D vision benefit from a large-scale dataset?
First, as a 3D asset resource, OBJAVERSE can support the exciting field of 3D generative modeling. We use data ex-tracted from OBJAVERSE to train generative models for sin-gle and multiple categories using GET3D [22] and find that we are able to generate high-quality objects. Moreover, we find that our generated objects are found by human anno-tators to be more diverse than those generated by a model trained on ShapeNet objects in 91% of cases.
Can the diversity of 3D models help improve classi-cal 2D vision task performance? To answer this ques-tion, we use the diversity of OBJAVERSE to improve the performance of long tail instance segmentation models. In-stance segmentation data can be expensive to obtain ow-ing to the cost of annotating contours around objects. The recent LVIS dataset contains segmentation annotations for 1,230 categories but the task remains very challenging for present day models, particularly on tail categories that have few examples. We show that increasing the volume of data by leveraging a simple Copy+Paste augmentation method with OBJAVERSE assets can improve the performance of state-of-the-art segmentation methods.
We also use OBJAVERSE to build a benchmark for eval-uating the robustness of state-of-the-art visual classifica-tion models to perspective shifts. We render objects in
OBJAVERSE from random orientations, which is how one 1Creative Commons license might expect to see them in the real world, and test the ability of CLIP-style visual backbones to correctly classify these images. Our experiments show that current state-of-the-art models’ performance degrades dramatically in this setting when viewing objects from arbitrary views.
OBJAVERSE allows us to build benchmarks to test (and po-tentially train) for orientation robustness for a long tail dis-tribution of asset categories. Building such benchmarks is made uniquely possible by the scale and diversity of 3D as-sets in OBJAVERSE. This would simply not be feasible to create in the real world nor can they be generated from ex-isting 2D images.
Can a large-scale 3D dataset help us train perfor-mant embodied agents? We use assets in OBJAVERSE to populate procedurally generated simulated environments in
ProcTHOR [16] that are used to train Embodied AI agents.
This results in an orders of magnitude increase in the num-ber of unique assets available for use in ProcTHOR scenes (previously limited to AI2-THOR’s [36] asset library of a few thousand unique instances each assigned to one of 108 object categories). Using OBJAVERSE populated scenes en-ables open vocabulary object navigation from any text de-scription. In this paper, we provide quantitative results for navigating to 1.1K semantic object categories, roughly a 50x increase.
These findings represent just a small fraction of what can be accomplished using OBJAVERSE. We are excited to see how the research community will leverage OBJAVERSE to enable fast and exciting progress in 2D and 3D computer vision applications and beyond. 2.