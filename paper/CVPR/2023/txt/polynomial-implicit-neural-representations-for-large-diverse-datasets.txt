Abstract
Implicit neural representations (INR) have gained sig-nificant popularity for signal and image representation for many end-tasks, such as superresolution, 3D modeling, and more. Most INR architectures rely on sinusoidal positional encoding, which accounts for high-frequency information in data. However, the finite encoding size restricts the model’s representational power. Higher representational power is needed to go from representing a single given image to repre-senting large and diverse datasets. Our approach addresses this gap by representing an image with a polynomial function and eliminates the need for positional encodings. Therefore, to achieve a progressively higher degree of polynomial rep-resentation, we use element-wise multiplications between features and affine-transformed coordinate locations after every ReLU layer. The proposed method is evaluated quali-tatively and quantitatively on large datasets like ImageNet.
The proposed Poly-INR model performs comparably to state-of-the-art generative models without any convolution, nor-malization, or self-attention layers, and with far fewer train-able parameters. With much fewer training parameters and higher representative power, our approach paves the way for broader adoption of INR models for generative mod-eling tasks in complex domains. The code is available at https://github.com/Rajhans0/Poly_INR 1.

Introduction
Deep learning-based generative models are a very ac-tive area of research with numerous advancements in recent years [8, 13, 24]. Most widely, generative models are based on convolutional architectures. However, recent develop-ments such as implicit neural representations (INR) [29, 43] represent an image as a continuous function of its coordinate locations, where each pixel is synthesized independently.
Such a function is approximated by using a deep neural network. INR provides flexibility for easy image transforma-tions and high-resolution up-sampling through the use of a coordinate grid. Thus, INRs have become very effective for 3D scene reconstruction and rendering from very few train-ing images [3, 27–29, 56]. However, they are usually trained to represent a single given scene, signal, or image. Recently,
INRs have been implemented as a generative model to gener-ate entire image datasets [1,46]. They perform comparably to
CNN-based generative models on perfectly curated datasets
like human faces [22]; however, they have yet to be scaled to large, diverse datasets like ImageNet [7].
INR generally consists of a positional encoding module and a multi-layer perceptron model (MLP). The positional encoding in INR is based on sinusoidal functions, often re-ferred to as Fourier features. Several methods [29, 43, 49] have shown that using MLP without sinusoidal positional encoding generates blurry outputs, i.e., only preserves low-frequency information. Although, one can remove the po-sitional encoding by replacing the ReLU activation with a periodic or non-periodic activation function in the MLP
[6, 37, 43]. However, in INR-based GAN [1], using a peri-odic activation function in MLP leads to subpar performance compared to positional encoding with ReLU-based MLP.
Sitzmann et al. [43] demonstrate that ReLU-based MLP fails to capture the information contained in higher deriva-tives. This failure to incorporate higher derivative informa-tion is due to ReLU’s piece-wise linear nature, and second or higher derivatives of ReLU are typically zero. This can be further interpreted in terms of the Taylor series expansion of a given function. The higher derivative information of a function is included in the coefficients of a higher-order poly-nomial derived from the Taylor series. Hence, the inability to generate high-frequency information is due to the ineffec-tiveness of the ReLU-based MLP model in approximating higher-order polynomials.
Sinusoidal positional encoding with MLP has been widely used, but the capacity of such INR can be limiting for two reasons. First, the size of the embedding space is limited; hence only a finite and fixed combination of periodic func-tions can be used, limiting its application to smaller datasets.
Second, such an INR design needs to be mathematically co-herent. These INR models can be interpreted as a non-linear combination of periodic functions where periodic functions define the initial part of the network, and the later part is often a ReLU-based non-linear function. Contrary to this, classical transforms (Fourier, sine, or cosine) represent an image by a linear summation of periodic functions. However, using just a linear combination of the positional embedding in a neural network is also limiting, making it difficult to represent large and diverse datasets. Therefore, instead of using periodic functions, this work models an image as a polynomial function of its coordinate location.
The main advantage of polynomial representation is the easy parameterization of polynomial coefficients with MLP to represent large datasets like ImageNet. However, conven-tionally MLP can only approximate lower-order polynomials.
One can use a polynomial positional embedding of the form xpyq in the first layer to enable the MLP to approximate higher order. However, such a design is limiting, as a fixed embedding size incorporates only fixed polynomial degrees.
In addition, we do not know the importance of each polyno-mial degree beforehand for a given image.
Hence, we do not use any positional encoding, but we progressively increase the degree of the polynomial with the depth of MLP. We achieve this by element-wise multiplica-tion between the feature and affine transformed coordinate location, obtained after every ReLU layer. The affine pa-rameters are parameterized by the latent code sampled from a known distribution. This way, our network learns the re-quired polynomial order and represents complex datasets with considerably fewer trainable parameters. In particular, the key highlights are summarized as follows:
• We propose a Poly-INR model based on polynomial functions and design a MLP model to approximate higher-order polynomials.
• Poly-INR as a generative model performs compara-bly to the state-of-the-art CNN-based GAN model (StyleGAN-XL [42]) on the ImageNet dataset with 3−4× fewer trainable parameters (depending on output resolution).
• Poly-INR outperforms the previously proposed INR models on the FFHQ dataset [22], using a significantly smaller model.
• We present various qualitative results demonstrating the benefit of our model for interpolation, inversion, style-mixing, high-resolution sampling, and extrapolation. 2.