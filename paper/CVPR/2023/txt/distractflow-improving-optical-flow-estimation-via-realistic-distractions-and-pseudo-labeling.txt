Abstract
We propose a novel data augmentation approach, Dis-tractFlow, for training optical flow estimation models by introducing realistic distractions to the input frames. Based on a mixing ratio, we combine one of the frames in the pair with a distractor image depicting a similar domain, which allows for inducing visual perturbations congruent with natural objects and scenes. We refer to such pairs as distracted pairs. Our intuition is that using semantically meaningful distractors enables the model to learn related variations and attain robustness against challenging devia-tions, compared to conventional augmentation schemes fo-cusing only on low-level aspects and modifications. More specifically, in addition to the supervised loss computed between the estimated flow for the original pair and its ground-truth flow, we include a second supervised loss de-fined between the distracted pair’s flow and the original pair’s ground-truth flow, weighted with the same mixing ra-tio. Furthermore, when unlabeled data is available, we ex-tend our augmentation approach to self-supervised settings through pseudo-labeling and cross-consistency regulariza-tion. Given an original pair and its distracted version, we enforce the estimated flow on the distracted pair to agree with the flow of the original pair. Our approach allows increasing the number of available training pairs signifi-cantly without requiring additional annotations. It is agnos-tic to the model architecture and can be applied to training any optical flow estimation models. Our extensive evalua-tions on multiple benchmarks, including Sintel, KITTI, and
SlowFlow, show that DistractFlow improves existing mod-els consistently, outperforming the latest state of the art. 1.

Introduction
Recent years have seen significant progress in optical flow estimation thanks to the development of deep learning, e.g., [4,7,8,23]. Among the latest works, many focus on de-veloping novel neural network architectures, such as PWC-Net [29], RAFT [30], and FlowFormer [6]. Other stud-† Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
Figure 1. Existing augmentation schemes apply low-level vi-sual modifications, such as color jittering, block-wise random oc-clusion, and flipping, to augment the training data (top), while
DistractFlow introduces high-level semantic perturbations to the frames (bottom). DistractFlow can further leverage unlabeled data to generate a self-supervised regularization. Our training leads to more accurate and robust optical flow estimation models, espe-cially in challenging real-world settings. ies investigate how to improve different aspects of super-vised training [27], e.g., gradient clipping, learning rate, and training compute load. More related to our paper are those incorporating data augmentation during training (e.g., [30]), including color jittering, random occlusion, cropping, and flipping. While these image manipulations can effectively expand the training data and enhance the robustness of the neural models, they fixate on the low-level aspects of the images.
Since obtaining ground truth optical flow on real data is very challenging, another line of work investigates how to leverage unlabeled data. To this end, semi-supervised methods [9, 12] that utilize frame pairs with ground-truth flow annotations in conjunction with unlabeled data in train-ing have been proposed. For instance, FlowSupervisor [9] adopts a teacher-student distillation approach to exploit un-labeled data. This method, however, does not consider lo-calized uncertainty but computes the loss for the entire im-age between the teacher and student network.
In this paper, we present a novel approach, Distract-Flow, which performs semantically meaningful data aug-mentations by introducing images of real objects and nat-ural scenes as distractors or perturbations to training frame pairs. More specifically, given a pair of consecutive frames, we combine the second frame with a random image depict-ing similar scenarios based on a mixing ratio. In this way, related objects and scenes are overlaid on top of the orig-inal second frame; see Figure 1 for an example. As a re-sult, we obtain challenging yet appropriate distractions for the optical flow estimation model that seeks dense corre-spondences from the first frame to the second frame (and in reverse too). The original first frame and the composite sec-ond frame constitute a distracted pair of frames, which we use as an additional data sample in both supervised and self-supervised training settings. Unlike our approach, existing data augmentation schemes for optical flow training apply only low-level variations such as contrast changes, geomet-ric manipulations, random blocks, haze, motion blur, and simple noise and shapes insertions [28, 30]. While such augmentations can still lead to performance improvements, they are disconnected from natural variations, scene con-text, and semantics. As we shall see in our experimental validation, the use of realistic distractions in training can provide a bigger boost to performance.
Figure 1 provides a high-level outline of DistractFlow.
We apply DistractFlow in supervised learning settings us-ing the ground-truth flow of the original pair. Distracted pairs contribute to the backpropagated loss proportional to the mixing ratios used in their construction. Additionally, when unlabeled frame pairs are available, DistractFlow al-lows us to impose a self-supervised regularization by fur-ther leveraging pseudo-labeling. Given an unlabeled pair of frames, we create a distracted version. Then, we enforce the estimated flow on the distracted pair to match that on the original pair. In other words, the prediction of the original pair is treated as a pseudo ground truth flow for the dis-tracted pair. Since the estimation on the original pair can be erroneous, we further derive and impose a confidence map to employ only highly confident pixel-wise flow estimations as the pseudo ground truth. This prevents the model from reinforcing incorrect predictions, leading to a more stable training process.
In summary, our main contributions are as follows:
• We introduce DistractFlow, a novel data augmentation approach that improves optical flow estimation by uti-lizing distractions from natural images. Our method provides augmentations with realistic semantic con-tents compared to existing augmentation schemes.
• We present a semi-supervised learning scheme for op-tical flow estimation that adopts the proposed dis-tracted pairs to leverage unlabeled data. We compute a confidence map to generate uncertainty-aware pseudo labels and to enhance training stability and overall per-formance.
• We demonstrate the effectiveness of DistractFlow in supervised [6, 14, 30] and semi-supervised settings, showing that DistractFlow outperforms the very recent
FlowSupervisor [9] that require additional in-domain unlabeled data. 2.