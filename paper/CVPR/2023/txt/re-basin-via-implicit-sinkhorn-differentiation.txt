Abstract
The recent emergence of new algorithms for permuting models into functionally equivalent regions of the solution space has shed some light on the complexity of error sur-faces and some promising properties like mode connectivity.
However, ﬁnding the permutation that minimizes some ob-jectives is challenging, and current optimization techniques are not differentiable, which makes it difﬁcult to integrate into a gradient-based optimization, and often leads to sub-optimal solutions. In this paper, we propose a Sinkhorn re-basin network with the ability to obtain the transportation plan that better suits a given objective. Unlike the cur-rent state-of-art, our method is differentiable and, there-fore, easy to adapt to any task within the deep learning do-main. Furthermore, we show the advantage of our re-basin method by proposing a new cost function that allows per-forming incremental learning by exploiting the linear mode connectivity property. The beneﬁt of our method is com-pared against similar approaches from the literature un-der several conditions for both optimal transport and linear mode connectivity. The effectiveness of our continual learn-ing method based on re-basin is also shown for several com-mon benchmark datasets, providing experimental results that are competitive with the state-of-art. The source code is provided at https://github.com/fagp/sinkhorn-rebasin. 1.

Introduction
Despite the success of deep learning (DL) across many application domains, the loss surfaces of neural networks (NNs) are not well understood. Even for shallow NNs, the number of saddle points and local optima can increase expo-nentially with the number of parameters [4,13]. The permu-θB
θA
πP (θB) (a)
Naive
WM [2]
Sinkhorn
)
θ (
C
λ (b)
Figure 1. (a) Loss landscape for the polynomial approximation task [27]. θA and θB are models found by SGD. LMC suggests that re-basin the model θB would result in a functionally equiv-alent model πP (θB), with no barrier on its linear interpolation (1 − λ)θA + λπP (θB). (b) Comparison of the cost in the lin-ear path along λ before and after re-basin using weight matching (WM) [2] and our Sinkhorn. The dashed line in the ﬁgures corre-sponds with the naive path, and the solid line is the path after the proposed Sinkhorn re-basin. The blue line represents WM path.
tation symmetry of neurons in each layer allows the same function to be represented with many different parameter values of the same network. Symmetries imposed by these invariances help us to better understand the structure of the loss landscape [6, 11, 13].
Previous studies establish that minima found by Stochas-tic Gradient Descent (SGD) are not only connected in the network parameter’s space by a path of non-increasing loss, but also permutation symmetries may allow us to con-nect those points linearly with no detriment to the loss
[9, 11–13, 15, 24]. This phenomenon is often referred to as linear mode connectivity (LMC) [24]. For instance, Fig. 1a shows a portion of the loss landscape for the polynomial approximation task [27] using the method proposed by Li et al. [16]. θA and θB are two minima found by SGD in different basins with an energy barrier between the pair.
LMC suggests that if one considers permutation invariance, we can teleport solutions into a single basin where there is almost no loss barrier between different solutions [2, 11].
In literature, this mechanism is called re-basin [2]. How-ever, efﬁciently searching for permutation symmetries that bring all solutions to one basin is a challenging problem
[11]. Three main approaches for matching units between two NNs have been explored in the literature [2]. Some studies propose a data-dependent algorithm that associates units across two NNs by matching their activations [2, 26].
Since activation-based matching is data dependent, it helps to adjust permutations to certain desired kinds of classes or domains [26]. Instead of associating units by their activa-tions, one could align the weights of the model itself [2,26], which is independent of the dataset, and therefore the com-putational cost is much lower. Finally, the third approach is to iteratively adjust the permutation of weights. In par-ticular, Ainsworth et al. [2] have proposed alternating be-tween models alignment and barrier minimization using a
Straight-Through Estimator. Unfortunately, the proposed approaches so far are either non-differentiable [2, 11, 26] or computationally expensive [2], making the solution difﬁcult to be extended to other applications, with a different objec-tive. For instance, adapting those methods for incremental learning by including the algorithm for weight matching be-tween two models trained on different domains is not trivial because of the difﬁculties in optimizing new objectives.
In this work, inspired by [21], we relax the permutation matrix with the Sinkhorn operator [1], and use it to solve the re-basin problem in a differentiable fashion. To avoid the high cost for computing gradients in the proposal of
Mena et al. [21], we use the implicit differentiation algo-rithm proposed in [10], which has been shown to be more cost-effective. Our re-basin formulation allows deﬁning any differentiable objective as a loss function.
A direct application of re-basin is the merger of diverse models without signiﬁcantly degrading their performance
[2, 5, 12, 13, 28]. Applications like federate learning [2], ensembling [12], or model initialization [5] exploit such a merger by selecting a model in the line connecting the mod-els to be combined. To show the effectiveness of our ap-proach, we propose a new continual learning algorithm that combines models trained on different domains. Our con-tinual learning algorithm differs from previous state-of-art approaches [22] because it directly estimates a model at the intersection of previous and new knowledge, by exploiting the LMC property observed in SGD-based solutions.
Our main contribution can be summarized as follows: (1) Solving the re-basin for optimal transportation using implicit Sinkhorn differentiation, enabling better differen-tiable solutions that can be integrated on any loss. (2) A powerful way to use our re-basin method based on the
Sinkhorn operator for incremental learning by considering it as a model merging problem and leveraging LMC. (3) An extensive set of experiments that validate our method for: (i) ﬁnding the optimal permutation to transform a model to another one equivalent; (ii) linear mode connec-tivity, to linearly connect two models such that their loss is almost identical along the entire connecting line in the weights space; and (iii) learning new domains and tasks in-crementally while not forgetting the previous ones. 2.