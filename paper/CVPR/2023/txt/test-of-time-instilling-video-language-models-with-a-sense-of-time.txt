Abstract
Modelling and understanding time remains a challenge in contemporary video understanding models. With lan-guage emerging as a key driver towards powerful gener-alization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We estab-lish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adapta-tion recipe on top of one such model, VideoCLIP, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require vary-ing degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch. 1.

Introduction
Self-supervised pretraining at scale on multimodal web corpora tied with powerful architectures [107] has led to foundational models [12] for images [2, 49, 59, 83, 84] and videos [2, 6, 26, 109, 119, 126]. These models have enabled remarkable improvements on a plethora of downstream video-language tasks such as video-text retrieval, video question-answering, and action recognition. Given the cost and difficulty of video annotations, even for a small amount of downstream data, such foundational models are emerging as the de-facto backbone for zero-shot [119, 122, 127] and few-shot generalization [2]. However, it remains unclear if these video-language models capture essential properties of a video beyond what can be learned from static images, most notably: time.
Many before us have shown that existing video-language
Figure 1. Can you match the correct video-text pairs? Understand-ing the time order of events across video and language is necessary to be able to solve this task. See footnote on next page for answers. models [6, 57, 66, 119] can achieve impressive performance on several video benchmarks [22, 41, 120] without reli-ably encoding time [13, 56, 59]. For example, Buch et al. [13] show that a model that uses a single (carefully selected) frame often outperforms recent video-language models [57, 119] on standard video benchmarks such as
MSR-VTT [120]. Lei et al. [56] report similar findings with a single-frame pretraining approach. These findings hint at a lack of time awareness in video models. However, it re-mains unclear if these findings are caused, indeed, by the lack of time in video models or whether the benchmarks themselves do not mandate time awareness. Furthermore, there is no clear definition of what it means for a model to be time aware. In this paper, we strive to shed light on all these factors of time awareness in video-language models.
As a first step, we consider a simple notion of under-standing time, i.e., understanding temporal relations such as before and after [4]. Consider the task presented in Fig. 1. A time invariant model shall be able to associate (A) with (1)
or (2) and (B) with (3) or (4) based on static frames alone.
But to distinguish between (1) and (2), one needs to be able to understand time order and connect it across video and language1. Thus, the first question we ask in Section 3: do the representations learnt by foundational video-language models encode this sense of time? To reliably attribute lack of time awareness to models and not existing benchmarks, we design our own synthetic dataset to probe models for this sense of time. We create video-language pairs that show a sequence of two events. Then, we alter the order of events either in the text or the video and check if models can con-nect the order in video and language. We find that existing video-language models indeed struggle to associate the time order across video and language.
In light of these findings, the second question we ask in
Section 4 is: can we adapt a video-language model, with-out expensive re-training from scratch, to instill this sense of time? Towards this, we take inspiration from literature on understanding time in natural language, where there has been much work on developing time aware language mod-els [20, 36, 37, 130, 131]. Our objective is to instill time awareness in a video-language model without having to pre-train from scratch. To do that, we propose TACT: Temporal
Adaptation by Consistent Time-ordering based on two key components: (i) we artificially create samples that provide temporal signal, for example, by flipping the order of events in the video or the text, and (ii) we introduce a modified contrastive loss to learn time order consistency based on these samples. Instead of training from scratch, we adapt an existing video-language model, VideoCLIP [61], using the paradigm of post-pretraining on a small amount of video-text data [66, 121]. We demonstrate the effectiveness of
TACT in connecting the time order in video and language on four diverse real datasets in Section 5.
Finally, in line with the original motive of video-language models for zero-shot generalization, we evalu-ate in Section 6 our TACT-adapted model for three sets of tasks on six downstream datasets which require a vary-ing degree of time awareness. On tasks that need higher time awareness, with the appropriate choice of adaptation dataset, TACT outperforms a strong baseline that is based on post-pretraining on canonical clip-text pairs without con-sideration of time-order.
In summary, our contributions are: (i) We show that existing video-language models struggle to associate time order in video and language through controlled experi-ments on synthetic data and several evaluations on real datasets. (ii) Based on VideoCLIP [119], we propose TACT, a method for temporal adaptation using this time order con-sistency without having to pretrain from scratch. (iii) We demonstrate improved zero-shot generalizability of TACT-adapted models on tasks that require higher time awareness. 1Answers: (A)-(2), (B)-(1), (C)-(4), (D)-(3). 2.