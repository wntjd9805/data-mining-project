Abstract
Few-shot class incremental learning (FSCIL) aims to address catastrophic forgetting during class incremental learning in a few-shot learning setting. In this paper, we approach the FSCIL by adopting analytic learning, a tech-nique that converts network training into linear problems.
This is inspired by the fact that the recursive implemen-tation (batch-by-batch learning) of analytic learning gives identical weights to that produced by training on the en-tire dataset at once. The recursive implementation and the weight-identical property highly resemble the FSCIL setting (phase-by-phase learning) and its goal of avoiding catas-trophic forgetting. By bridging the FSCIL with the ana-lytic learning, we propose a Gaussian kernel embedded an-alytic learning (GKEAL) for FSCIL. The key components of GKEAL include the kernel analytic module which allows the GKEAL to conduct FSCIL in a recursive manner, and the augmented feature concatenation module that balances the preference between old and new tasks especially effec-tively under the few-shot setting. Our experiments show that the GKEAL gives state-of-the-art performance on several benchmark datasets. 1.

Introduction
Class-incremental learning (CIL) [20] can continuously absorbs new category knowledge in a phase-by-phase man-ner with data coming separately in each phase, after training a classiﬁcation network. This is important as data can be scattered at various times and locations in a non-identical independent way. The few-shot class incremental learning (FSCIL) [23] further imposes an inefﬁciency constraint on the data availability. That is, only a few data samples, i.e., few-shot, for each new class is allowed, leading to a more challenging incremental learning problem.
The major challenge for FSCIL follows from the CIL’s, namely the catastrophic forgetting. The performance on old
Figure 1. The resemblance between the analytic learning (recur-sive form) [33] and incremental learning. We want to build a bridge between these two ﬁelds to take advantage of the analytic learning for addressing the FSCIL. (base) tasks is tremendously discounted after learning new tasks. This is caused by the lack of training data for old tasks, tricking models to focus only on new tasks. The for-getting issue is also referred to as task-recency bias, in favor of newly learned tasks in prediction. The forgetting issue in
FSCIL manifests more quickly due to over-ﬁtting than that in the conventional CIL setting as the training samples be-come scarce for new tasks.
To handle the forgetting, conventional CIL sparks various contributions, which mainly include the Bias correction-based CIL [1, 9], Regularization-based CIL [11, 13] and Replay-based CIL [17, 20]. They work well in ad-dressing the catastrophic forgetting in CIL. However, the few-shot constraint in FSCIL renders the CIL solutions ob-solete (see [23] or our experiments). There have been sev-eral works [22,23,31] taking into account the few-shot con-straint, outperforming the conventional CIL. These FSCIL techniques take inspirations from existing CIL variants [22] or the few-shot learning angle (e.g., prototype-based [31]) to present catastrophic forgetting.
In this paper, inspired by analytic learning [33, 34]—a technique converting network training into linear problems—we approach the FSCIL in an unique angle by incorporating traditional machine learning techniques. The analytic learning allows the training to be implemented in a recursive manner where training data are scattered into multiple batches. Yet the weights trained recursively are identical to those trained by pouring the entire data in one go [33]. We may call this weight-invariant (or weight-identical) property. Such recursive form and its weight-invariant property highly resemble the incremental learning paradigm and its objective of avoiding (catastrophic) for-getting respectively (see Figure 1). Following this intuition, we propose a Gaussian kernel embedded analytic learning (GKEAL) for FSCIL. The GKEAL adopts traditional ma-chine learning tools such as least squares (LS) and matrix inverse to avoid forgetting. The key contributions are sum-marized as follows.
• We introduce GKEAL by treating the FSCIL as a re-cursive learning problem to avoid forgetting. We prove that the GKEAL in the FSCIL setting follows the same weight-invariant property as that in analytic learning.
• To bridge analytic learning into the FSCIL realm, the
GKEAL replaces the classiﬁer at a network’s ﬁnal layer with a kernel analytic module (KAM). The KAM contains a Gaussian kernel embedding process for extracting more discriminative feature, and an LS solution that allows the
GKEAL to learn new tasks in a recursive manner.
• To mitigate the data imbalance between the base and new tasks, an augmented feature concatenation (AFC) mod-ule is introduced, which effectively balances the network’s base-new task preference.
• Experiments on benchmark datasets show that the
GKEAL outperforms the state-of-the-art methods by a con-siderable margin. Ablation study is also provided, giving thorough analysis of the hyperparameters introduced, as well as strong supports to our theoretical claims. 2.