Abstract
Building a universal Video-Language model for solving various video understanding tasks (e.g., text-video retrieval, video question answering) is an open challenge to the ma-chine learning field. Towards this goal, most recent works build the model by stacking uni-modal and cross-modal fea-ture encoders and train it with pair-wise contrastive pre-text tasks. Though offering attractive generality, the resulted models have to compromise between efficiency and perfor-mance. They mostly adopt different architectures to deal with different downstream tasks. We find this is because the pair-wise training cannot well align and fuse features from different modalities. We then introduce Clover—a Corre-lated Video-Language pre-training method—towards a uni-versal Video-Language model for solving multiple video un-derstanding tasks with neither performance nor efficiency compromise.
It improves cross-modal feature alignment and fusion via a novel tri-modal alignment pre-training task. Additionally, we propose to enhance the tri-modal alignment via incorporating learning from semantic masked samples and a new pair-wise ranking loss. Clover estab-lishes new state-of-the-arts on multiple downstream tasks, including three retrieval tasks for both zero-shot and fine-tuning settings, and eight video question answering tasks.
Codes and pre-trained models will be released at https:
//github.com/LeeYN-43/Clover. 1.

Introduction
Video-Language pre-training (VidL) aims to learn gen-eralizable multi-modal models from large-scale video-text
*Equal Contribution in alphabetical order.
†Work done when interning at ByteDance Inc.
‡Corresponding Author. samples so as to better solve various challenging Video-Language understanding tasks, such as text-video retrieval
[1, 4, 38, 55] and video question answering [16, 47, 52]. Re-cent studies [9,11,23,24,49,56,58,61] have shown that VidL leads to significant performance improvement and achieves state-of-the-art results on various downstream text-video re-trieval and video question answering (VQA) benchmarks.
Though achieving encouraging performance, existing
VidL models mostly adopt different architectures to deal with different downstream tasks. For the text-video retrieval tasks, they [2, 10, 11, 13, 31, 39] typically use two individ-ual uni-modal encoders for processing video and text data separately, for the sake of retrieval efficiency. While for video question answering tasks, the models usually adopt the multi-modal joint encoder design to learn the associa-tion and interaction of different modalities.
Building a unified model capable of solving various
Video-Language tasks is a long-standing challenge for ma-chine learning research. A few recent works [9, 24] attempt to learn a unified VidL model for both tasks, which uses the multi-modal encoder to conduct text-video retrieval. How-ever, the model requires an exhaustive pair-wise compar-ison between the query texts and gallery videos. Given
N text queries and M category videos, the computation complexity of the multi-modal encoder model would be
O(N M ), which makes it infeasible for large-scale video-text retrieval applications. Another straightforward solu-tion is to simply combine the uni-modal and multi-modal encoders (Fig. 1 (a)), and perform the retrieval and VQA tasks through the uni-modal and multi-modal encoders re-spectively.
Its computation complexity for retrieval tasks is only O(N + M ). However, the experiment results in
Fig.1 (c) show that, without a carefully designed correlat-ing mechanism between these two types of encoders, the simple combination i.e., COMB yields a compromised per-formance compared with the models i.e. IND individually designed for retrieval and VQA.
In this work, we aim to address the above issues and build a unified pre-training model that attains high effi-ciency and performance simultaneously. We observe: (i) well aligning the features of the video and text from the same data pair is important for text-video matching; (ii) ef-fectively fusing video and text features into unified repre-sentations is critical for video-text understanding. However, existing pre-training strategies that rely on either simple su-pervised or contrastive pre-text tasks hardly achieve promis-ing feature alignment and fusion capability simultaneously.
Motivated by these observations, we develop a new VidL method from these two aspects. Specifically, we propose
Correlated Video-Language pre-training (Clover), a VidL method that not only unifies Video-Language alignment and fusion, but also makes them mutually boosted. The fused multi-modal representation contains richer context informa-tion than the uni-modal representations [11,35]. As an inter-mediate modality between video and text, the multi-modal representations are good anchors for cross-modality align-ment. Meanwhile, keeping the fused representation closer to the uni-modal representation containing consistent se-mantic information and away from the inconsistent one will enhance the learning of semantic information in the fused modality. Therefore, we propose the Tri-Modal Alignment (TMA) to get Video-Language alignment and fusion mutu-ally boosted, which takes the alignment between the multi-modal representation and text/video representations as an auxiliary objective. We note that since the tri-modal align-ment is well compatible with the classical pre-training tasks
[3, 7, 35] e.g., Masked Language Modeling, its computation overhead is negligible. To help the model maintain fine-grained discriminative capability while improving its gen-eralizability, we further introduce a pair-wise ranking loss that urges the model to be aware of the concept missing in masked samples compared to original samples.
Extensive experiments are conducted on multiple down-stream tasks, including three retrieval tasks with different experimental setups (i.e. zero-shot and fine-tune) and eight video question answering tasks. The results demonstrate that Clover is able to get the cross-modal fusion and align-ment capability mutually improved, and consistently out-performs current SOTAs on various downstream tasks. It achieves an average performance improvement of 4.9% and 8.7% Recall@10 score on the zero-shot and fine-tune set-tings of the three downstream retrieval datasets, while the average accuracy improvement over current SOTAs is 2.3% on the eight video question answering datasets.
In summary, we make the following contributions: (1) we introduce Clover, a pre-training method achieving uni-fied Video-Language alignment and fusion model that can be easily transferred to various downstream video under-standing tasks while attaining both high efficiency and per-formance; (2) we propose a novel tri-modal alignment pre-training task, which correlates the uni-modal encoder and multi-modal encoder to get them mutually boosted. 2.