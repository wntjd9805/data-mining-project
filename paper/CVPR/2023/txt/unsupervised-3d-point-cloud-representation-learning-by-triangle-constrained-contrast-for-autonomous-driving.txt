Abstract
Due to the difﬁculty of annotating the 3D LiDAR data of autonomous driving, an efﬁcient unsupervised 3D represen-tation learning method is important. In this paper, we de-sign the Triangle Constrained Contrast (TriCC) framework tailored for autonomous driving scenes which learns 3D un-supervised representations through both the multimodal in-formation and dynamic of temporal sequences. We treat one camera image and two LiDAR point clouds with different timestamps as a triplet. And our key design is the consistent constraint that automatically ﬁnds matching relationships among the triplet through “self-cycle” and learns represen-tations from it. With the matching relations across the tem-poral dimension and modalities, we can further conduct a triplet contrast to improve learning efﬁciency. To the best of our knowledge, TriCC is the ﬁrst framework that uniﬁes both the temporal and multimodal semantics, which means it utilizes almost all the information in autonomous driving scenes. And compared with previous contrastive methods, it can automatically dig out contrasting pairs with higher difﬁculty, instead of relying on handcrafted ones. Extensive experiments are conducted with Minkowski-UNet and Vox-elNet on several semantic segmentation and 3D detection datasets. Results show that TriCC learns effective repre-sentations with much fewer training iterations and improves the SOTA results greatly on all the downstream tasks. Code and models can be found at https://bopang1996.github.io/. 1.

Introduction
For the perception of autonomous driving, semantic seg-mentation and 3D object detection on point cloud are two fundamental tasks [18, 89]. At present, deep learning al-gorithms based on supervised learning have pushed their performances to the applicable level [37, 42, 83] with large
∗Equal contribution. † Cewu Lu is the corresponding author, the mem-ber of Qing Yuan Research Institute and MoE Key Lab of Artiﬁcial Intel-ligence, AI Institute, Shanghai Jiao Tong University, China, and Shanghai
Qi Zhi Institute.
Point features at  key frame (cid:1872) (cid:3397) (cid:884)
Point features at  key frame (cid:1872) (cid:3397) (cid:883)
Point features at  key frame (cid:1872)
Pixel features at  key frame (cid:1872)
Pixel features at  key frame (cid:1872) (cid:3397) (cid:883)
Triangle Consistent 
Constraint 2D RGB features 3D Point Cloud  features
Figure 1. Brief illustration of the Triangle Constrained Contrast (TriCC). In order to learn dense 3D point cloud representations from temporal information and multimodal semantics in one uni-ﬁed algorithm, TriCC is designed to adopt the triangle consistent constraint to automatically ﬁnd the pixel (point)-level matching relationships among each triangle (the red cycle) and learn effec-tive representations spanning temporal and modality dimensions, instead of relying on hand-crafted pre-deﬁned dense positive pairs that traditional contrastive learning methods adopt. Thus, it can unify all the semantics in one concise and compact algorithm. scale datasets [7, 47, 66, 86]. Nevertheless, in the super-vised process, obtaining annotations for these dense local-ization tasks is expensive and time-consuming. Thus, a mass of easy-collected perception data lacks efﬁcient uti-lization [7,86] and it needs much repetitive work to transfer supervised trained models to different scenes which makes the extension of applications difﬁcult. Based on this, in this paper, we build an unsupervised dense 3D point cloud representation learning framework tailored for autonomous driving scenes, aiming at achieving better performance on dense tasks with less annotated data.
Inspired by contrastive learning, a mainstream unsuper-vised framework, we choose to adopt the same discrimi-native route to build our algorithm. Compared with cur-rently popular generative models like MAE [17,28,90], dis-criminative methods are more propitious to temporal and multimodal information, and have better applicability for structures of point cloud models. Some previous works try to utilize contrastive learning in point cloud tasks. Point-Contrast [78] and PPKT [44] adopt afﬁne transformation and multimodal information respectively to get positive dis-crimination pairs. However, this kind of hand-crafted man-ner leads to many false negatives. STRL [34] learns seman-tics from video temporal frames. Though the positive pairs have more diversity, it learns global semantics which is not optimal for dense tasks. More importantly, these methods cannot learn from the complete multimodal & temporal in-formation since they model these two sources of semantics in two different ways which cannot extend to each other.
This makes them incapable of efﬁcient unsupervised learn-ing in autonomous driving scenarios where multimodal and temporal information is commonly available.
In this paper, we design a new unsupervised represen-tation learning framework: Triangle Constrained Contrast (TriCC) for the autonomous driving scene. It aims at get-ting rid of discriminative unsupervised algorithms’ demand on hand-crafted positive temporal dense pairs and replac-ing it with an elegant self-driven scheme so that TriCC can learn both the dense temporal semantics and multimodal se-mantics in one uniﬁed end-to-end model. Inspired by [74], the core idea of TriCC is to treat a camera image, a Li-DAR point cloud with timestamp t0, and a point cloud with nearby timestamp t1 as a triple-pair. Then it learns repre-sentations and gets dense matching relationships among the triplet through the designed triangle consistent constraint which forces the pixels or points to match back with them-selves after transition through the triplet-cycle as Fig. 1 shows. The implicitly learned dense relationships among the temporal dimension and multiple modalities are harder and more effective positive pairs for discrimination which are important for representation learning. Moreover, we de-sign the cycle shortcut technique and triplet contrast to en-hance the learning efﬁciency, allowing TriCC to learn better representations with half iterations of previous methods.
The proposed TriCC framework is effective and simple to implement. We adopt nuScenes dataset [7] to learn the point cloud representation and evaluate it on point cloud se-mantic segmentation and 3D object detection downstream tasks with several datasets and backbones. TriCC pushes all the performances to the new state-of-the-art even if it only adopts half of the pre-training iterations than baselines. In particular, on nuScenes semantic segmentation, TriCC rela-tively improves Res16UNet’s ﬁne-tuning performance with 1% annotations by 7.9% and on KITTI [20] 3D object de-tection with 5% annotations, it produces a 4% relative im-provements. We hope this new framework will provide the community with new insights. 2.