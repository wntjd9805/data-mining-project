Abstract
Multi-modality (MM) image fusion aims to render fused images that maintain the merits of different modalities, e.g., functional highlight and detailed textures. To tackle the chal-lenge in modeling cross-modality features and decomposing desirable modality-speciﬁc and modality-shared features, we propose a novel Correlation-Driven feature Decompo-sition Fusion (CDDFuse) network. Firstly, CDDFuse uses
Restormer blocks to extract cross-modality shallow features.
We then introduce a dual-branch Transformer-CNN feature extractor with Lite Transformer (LT) blocks leveraging long-range attention to handle low-frequency global features and
Invertible Neural Networks (INN) blocks focusing on ex-tracting high-frequency local information. A correlation-driven loss is further proposed to make the low-frequency features correlated while the high-frequency features un-correlated based on the embedded information. Then, the
LT-based global fusion and INN-based local fusion layers output the fused image. Extensive experiments demonstrate that our CDDFuse achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. We also show that CDDFuse can boost the performance in downstream infrared-visible se-mantic segmentation and object detection in a uniﬁed bench-mark. The code is available at https://github.com/
Zhaozixiang1228/MMIF-CDDFuse. 1.

Introduction
Image fusion is a basic image processing topic that aims to generate informative fused images by combining the im-portant information from source ones [47,75,78,79,87]. The fusion targets include digital [45,84], multi-modal [36,70,88]
*Corresponding author. (a) Existing MMIF methods vs. CDDFuse. The base and detail encoders are respon-sible for extracting global and local features, respectively. (b) CDDFuse (highlighted in yellow) achieves the state-of-the-art performance on
MSRS [57] and RoadScene [71] in eight metrics.
Figure 1. Workﬂow and performance comparison of our proposed
CDDFuse with existing MM image fusion approaches. and remote sensing [4, 76, 91] images, etc. The Infrared-Visible image Fusion (IVF) and Medical Image Fusion (MIF) are two challenging sub-categories of Multi-Modality Image
Fusion (MMIF), focusing on modeling the cross-modality features from all the sensors and aggregating them into the output images. Speciﬁcally, IVF targets fused images that preserve thermal radiation information in the input infrared images and detailed texture information in the input visible images. The fused images can avoid the shortcomings of visible images being sensitive to illumination conditions as well as the infrared images being noisy and low-resolution.
Downstream recognition tasks, e.g., multi-modal saliency detection [33, 52, 63], object detection [6, 34, 58] and se-mantic segmentation [37, 49–51] can then beneﬁt from the obtained clearer representations of scenes and objects in IVF images. Similarly, MIF aims to clearly exhibit the abnor-malities by fusing multiple medical imaging modalities to reveal comprehensive information to assist diagnosis and treatment [21].
Many methods have been developed to tackle the MMIF challenges in recent years [35, 41, 44, 55, 73, 74, 82]. A common pipeline that demonstrated promising results uti-lizes CNN-based feature extraction and reconstruction in an
Auto-Encoder (AE) manner [28, 29, 32, 88]. The workﬂow is illustrated in Fig. 1a. However, existing methods have three main shortcomings. First, the internal working mecha-nism of CNNs is difﬁcult to control and interpret, causing insufﬁcient extraction of cross-modality features. For ex-ample, in Fig. 1a, shared encoders in (I) and (II) cannot distinguish modality-speciﬁc features, while the private en-coders in (III) ignore features shared by modalities. Second, the context-independent CNN only extracts local informa-tion in a relatively small receptive ﬁeld, which can hardly extract global information for generating high-quality fused images [31]. Thus, it is still unclear whether the inductive biases of CNN are capable enough to extract features for all modalities. Third, the forward propagation of fusion networks often causes the loss of high-frequency informa-tion [42,89]. Our work explores a more reasonable paradigm to tackle the challenges in feature extraction and fusion.
First, we aim to add correlation restrictions to the ex-tracted features and limit the solution space, which improves the controllability and interpretability of feature extraction.
Our assumption is that, in the MMIF task, the input fea-tures of the two modalities are correlated at low frequen-cies, representing the modality-shared information, while the high-frequency feature is irrelevant and represents the unique characteristics of the respective modalities. Taking
IVF as an example, since infrared and visible images come from the same scene, the low-frequency information of the two modalities contains statistical co-occurrences, such as background and large-scale environmental features. On the contrary, the high-frequency information of the two modali-ties is independent, e.g., the texture and detail information in the visible image and the thermal radiation information in the infrared image. Therefore, we aim to facilitate the extraction of modality-speciﬁc and modality-shared features by increas-ing and decreasing the correlation between low-frequency and high-frequency features, respectively.
Second, from the architectural perspective, Vision Trans-formers [14, 38, 80] recently shows impressive results in computer vision, with self-attention mechanism and global feature extraction. However, Transformer-based methods are computationally expensive, which leaves room for further improvement with considering the efﬁciency-performance tradeoff of image fusion architectures. Therefore, we pro-pose integrating the advantages of local context extraction and computational efﬁciency in CNN and the advantages of global attention and long-range dependency modeling in
Transformer to complete the MMIF task.
Third, to solve the challenge of losing wanted high-frequency input information, we adopt the building block of
Invertible Neural networks (INN) [13]. INN was proposed with invertibility by design, which prevents information loss through the mutual generation of input and output features and aligns with our goal of preserving high-frequency fea-tures in the fused images.
To this end, we proposed the Correlation-Driven feature
Decomposition Fusion (CDDFuse) model, where modality-speciﬁc and modality-shared feature extractions are realized by a dual-branch encoder, with the fused image reconstructed by the decoder. The workﬂow is shown in Figs. 1a and 2.
Our contributions can be summarized in four aspects:
• We propose a dual-branch Transformer-CNN frame-work for extracting and fusing global and local features, which better reﬂects the distinct modality-speciﬁc and modality-shared features.
• We reﬁne the CNN and Transformer blocks for a better adaptation to the MMIF task. Speciﬁcally, we are the
ﬁrst to utilize the INN blocks for lossless information transmission and the LT blocks for trading-off fusion quality and computational cost.
• We propose a correlation-driven decomposition loss function to enforce the modality shared/speciﬁc feature decomposition, which makes the cross-modality base features correlated while decorrelates the detailed high-frequency features in different modalities.
• Our method achieves leading image fusion performance for both IVF and MIF. We also present a uniﬁed mea-surement benchmark to justify how the IVF fusion im-ages facilitate downstream MM object detection and semantic segmentation tasks. 2.