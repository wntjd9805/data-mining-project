Abstract
Dual-pixel photography is monocular RGB-D photography with an ultra-high resolution, enabling many applications in computational photography. However, there are still sev-eral challenges to fully utilizing dual-pixel photography.
Unlike the conventional stereo pair, the dual pixel exhibits a bidirectional disparity that includes positive and nega-tive values, depending on the focus plane depth in an im-age. Furthermore, capturing a wide range of dual-pixel disparity requires a shallow depth of field, resulting in a severely blurred image, degrading depth estimation perfor-mance. Recently, several data-driven approaches have been proposed to mitigate these two challenges. However, due to the lack of the ground-truth dataset of the dual-pixel disparity, existing data-driven methods estimate either in-verse depth or blurriness map.
In this work, we propose a self-supervised learning method that learns bidirectional disparity by utilizing the nature of anisotropic blur kernels in dual-pixel photography. We observe that the dual-pixel left/right images have reflective-symmetric anisotropic ker-nels, so their sum is equivalent to that of a conventional image. We take a self-supervised training approach with the novel kernel-split symmetry loss accounting for the phe-nomenon. Our method does not rely on a training dataset of dual-pixel disparity that does not exist yet. Our method can estimate a complete disparity map with respect to the focus-plane depth from a dual-pixel image, outperforming the baseline dual-pixel methods. 1.

Introduction
Dual-pixel is an image sensor technology that a single pixel has two photodiodes, while a pixel on the traditional image sensor has only a single photodiode. The dual-pixel is orig-inally invented to leverage the phase difference of two pho-todiodes for efficient autofocusing [1, 14, 27]. Nowadays, dual-pixel image sensors can be easily found in multiple camera platforms such as Canon EOS 5D Mark IV DSLR and Google Pixel phone cameras. The dual-pixel cameras make possible not only traditional autofocusing but also
Figure 1. Captured images from the traditional binocular stereo camera and dual-pixel camera, with a focus-plane depth at the middle object. Upper part of the image is a left image, and lower part is a right image from each camera. Arrows note parallax, which is unidirectional in stereo imaging and bidirectional in dual-pixel imaging. a wide range of interesting applications of RGB-D com-putational photography, such as depth estimation [9], re-focusing [29], deblurring [2, 34] or reflection removal [24].
The captured dual-pixel image can be separated into two images by gathering pixels from the left and ride sides of photodiodes, respectively. Although the baseline between dual-pixel photodiodes is short, these two images capture slightly different light fields, producing disparity by paral-lax.
One characteristic of the dual-pixel setup compared with a traditional binocular stereo is that it yields a bidirectional disparity shown in Figure 1, depending on the focus-plane depth that is the distance between a camera lens and the per-fect point of focus in an image (Figure 2). The dual-pixel disparity estimation should account for the direction of hor-izontal shift changes by the focus-plane depth. In addition, the magnitude of a dual-pixel disparity is proportional to the size of a circle of confusion. To guarantee enough disparity in dual-pixel imaging, optics with a large aperture are re-quired to obtain a shallow depth of field for a large circle of confusion, which is the area around the plane of focus that appears to be in focus. As a result, the disparity is estimated from a pair of blurry dual-pixel images caused by the shal-low depth of field, which raises another challenge. The con-ventional camera has an isotropic blur kernel when an im-age is out of focus (Figure 2). However, a dual-pixel camera produces anisotropic blur kernels, i.e., the kernel shapes of the left/right photodiodes are anisotropic and symmetrically
flipped [3,23]. It is impossible to learn these kernels explic-itly in a supervised manner since no training dataset with bidirectional disparity is available.
To address those challenges, several data-driven super-vised learning-based methods have been proposed [9, 21].
However, the main challenge in this supervised approach is that it is difficult to create a ground-truth disparity dataset specifically designed for a dual-pixel system unless it is produced by simulating complete light transport through camera optics with the variation of focus-plane depths in each image. Up to date, there is no bidirectional disparity dataset available for supervised learning. Due to this, these methods alternatively estimate either inverse depth [9, 21], or blurriness maps [34] only, which cannot directly esti-mate the disparity of a dual-pixel image without focus-plane depth information. And thus, only traditional optimization-based approaches [23, 29] have handled the bidirectional disparity estimation problem in dual-pixel photography. No learning-based approach tackles the bidirectional disparity problem in dual-pixel photography.
In this work, we propose a learning-based bidirectional disparity estimation method specially tailored for dual-pixel imaging. Our contribution consists of two main parts: (1) Assuming an isotropic blur kernel, we first pretrain a conventional stereo network using a stereo dataset with left-right inverted. (2) Based on our observation that the dual-pixel left/right images have reflective-symmetric anisotropic kernels that their sum has to be equivalent to that of a con-ventional image, we employ our novel self-supervised train-ing with the novel kernel-split symmetry loss accounting for the phenomenon. In addition, we demonstrate that our model represents more accurate 3D geometric relationships among objects with comprehensive evaluations. 2.