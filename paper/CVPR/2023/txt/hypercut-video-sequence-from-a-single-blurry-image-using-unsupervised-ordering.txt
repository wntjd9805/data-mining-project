Abstract
We consider the challenging task of training models for image-to-video deblurring, which aims to recover a se-quence of sharp images corresponding to a given blurry image input. A critical issue disturbing the training of an image-to-video model is the ambiguity of the frame or-dering since both the forward and backward sequences are plausible solutions. This paper proposes an effective self-supervised ordering scheme that allows training high-quality image-to-video deblurring models. Unlike previous methods that rely on order-invariant losses, we assign an explicit order for each video sequence, thus avoiding the order-ambiguity issue. Specifically, we map each video se-quence to a vector in a latent high-dimensional space so that there exists a hyperplane such that for every video se-quence, the vectors extracted from it and its reversed se-quence are on different sides of the hyperplane. The side of the vectors will be used to define the order of the corre-sponding sequence. Last but not least, we propose a real-image dataset for the image-to-video deblurring problem that covers a variety of popular domains, including face, hand, and street. Extensive experimental results confirm the effectiveness of our method. Code and data are avail-able at https://github.com/VinAIResearch/
HyperCUT.git 1.

Introduction
Motion blur artifacts occur when the camera’s shutter speed is slower than the object’s motion. This can be stud-ied by considering the image capturing process, in which the camera shutter is opened to allow light to pass to the camera sensor. This process can be formulated as: y = g (cid:18) 1
τ (cid:90) τ 0 (cid:19) (cid:32) x(t)dt
≈ g 1
N + 1
N (cid:88) k=0 (cid:33) xk
, (1)
Figure 1. We tackle the order ambiguity issue by forcing the frame sequence to follow a pre-defined order. To find such an order, we map the frame sequences into a high dimensional space so that they are separable. The side (left or right of the hyperplane) is used to define the order of the frame sequence. where y is the resulting image, x(t) is the signal captured by the sensor at time t, g is the camera response function, and
τ is the camera exposure time. For simplicity, we omit the camera response function in the notation. The image y can also be approximated by averaging N +1 uniform samples of the signal x, denoted as xk with k = 0, N . For long exposure duration or rapid movement, these samples can be notably different, causing motion blur artifacts.
Image deblurring seeks to remove the blur artifacts to im-prove the quality of the captured image. This task has many practical applications, and it has been extensively studied in the computer vision literature. However, existing methods often formulate the deblurring task as an image-to-image mapping problem, where only one sharp image is sought for a given blurry input image, even though a blurry image corresponds to a sequence of sharp images. The image-to-image approach can improve the aesthetic look of a blurry image, but it is insufficient for many applications, espe-cially the applications that require recovering the motion of objects, e.g., for iris or finger tracking. In this paper, we tackle the another important task of image-to-video deblur-ring, which we will refer to as blur2vid.
Image-to-video deblurring, however, is a non trivial task that requires learning a set of deblurring mapping functions
{fk} so that fk(y) ≈ xk. A naive approach is to minimize the squared difference between the predicted sharp image and the ground truth target, i.e.,
• We build a new dataset for the task, covering three cat-egories: street, face, and hand. This is the first real and large-scale dataset for image-to-video deblurring.
• We demonstrate two potential real-world applications of image-to-video deblurring. fk = argmin f
Ex,y||f (y) − xk||2 2. (2) 2.