Abstract
Effectively localizing an agent in a realistic, noisy setting is crucial for many embodied vision tasks. Visual Odome-try (VO) is a practical substitute for unreliable GPS and compass sensors, especially in indoor environments. While
SLAM-based methods show a solid performance without large data requirements, they are less flexible and robust w.r.t. to noise and changes in the sensor suite compared to learning-based approaches. Recent deep VO models, however, limit themselves to a fixed set of input modalities, e.g., RGB and depth, while training on millions of sam-ples. When sensors fail, sensor suites change, or modali-ties are intentionally looped out due to available resources, e.g., power consumption, the models fail catastrophically.
Furthermore, training these models from scratch is even more expensive without simulator access or suitable exist-ing models that can be fine-tuned. While such scenarios get mostly ignored in simulation, they commonly hinder a model’s reusability in real-world applications. We propose a Transformer-based modality-invariant VO approach that can deal with diverse or changing sensor suites of naviga-tion agents. Our model outperforms previous methods while training on only a fraction of the data. We hope this method opens the door to a broader range of real-world applica-tions that can benefit from flexible and learned VO models. 1.

Introduction
Artificial intelligence has found its way into many com-mercial products that provide helpful digital services. To in-crease its impact beyond the digital world, personal robotics and embodied AI aims to put intelligent programs into bod-ies that can move in the real world or interact with it [15].
One of the most fundamental skills embodied agents must learn is to effectively traverse the environment around them, allowing them to move past stationary manipulation tasks and provide services in multiple locations instead [40]. The ability of an agent to locate itself in an environment is vi-tal to navigating it successfully [12, 64]. A common setup is to equip an agent with an RGB-D (RGB and Depth)
*Work done on exchange at EPFL
Figure 1. An agent is tasked to navigate to a goal location us-ing RGB-D sensors. Because GPS+Compass are not available, the location is inferred from visual observations only. Neverthe-less, sensors can malfunction, or availability can change during test-time (indicated by ∼), resulting in catastrophic failure of the localization. We train our model to react to such scenarios by ran-domly dropping input modalities. Furthermore, our method can be extended to learn from multiple arbitrary input modalities, e.g., surface normals, point clouds, or internal measurements. camera and a GPS+Compass sensor and teach it to nav-igate to goals in unseen environments [2]. With extended data access through simulators [28, 39, 40, 47, 57], photo-realistic scans of 3D environments [7, 28, 46, 56, 58], and large-scale parallel training, recent approaches reach al-most perfect navigation results in indoor environments [55].
However, these agents fail catastrophically in more real-istic settings with noisy, partially unavailable, or failing
RGB-D sensor readings, noisy actuation, or no access to
GPS+Compass [6, 64].
Visual Odometry (VO) is one way to close this per-formance gap and localize the agent from only RGB-D observations [2], and deploying such a model has been shown to be especially beneficial when observations are noisy [12, 64]. However, those methods are not robust to any sensory changes at the test-time, such as a sensor fail-ing, underperforming, or being intentionally looped out.
In practical applications [43], low-cost hardware can also experience serious bandwidth limitations, causing RGB (3 channels) and Depth (1 channel) to be transferred at dif-ferent rates. Furthermore, mobile edge devices must bal-ance battery usage by switching between passive (e.g., RGB) and active (e.g., LIDAR) sensors depending on the specific episode. Attempting to solve this asymmetry by keeping
separate models in memory, relying on active sensors, or using only the highest rate modality is simply infeasible for high-speed and real-world systems. Finally, a changing sen-sor suite represents an extreme case of sensor failure where access to a modality is lost during test-time. These points demonstrate the usefulness of a certain level of modality in-variance in a VO framework. Those scenarios decrease the robustness of SLAM-based approaches [32] and limit the transferability of models trained on RGB-D to systems with only a subset or different sensors.
We introduce “optional” modalities as an umbrella term to describe settings where input modalities may be of lim-ited availability at test-time. Figure 1 visualizes a typical indoor navigation pipeline, but introduces uncertainty about modality availability (i.e. at test-time, only a subset of all modalities might be available). While previous approaches completely neglect such scenarios, we argue that explicitly accounting for “optional” modalities already during train-ing of VO models allows for better reusability on platforms with different sensor suites and trading-off costly or unre-liable sensors during test-time. Recent methods [12, 64] use Convolution Neural Network (ConvNet) architectures that assume a constant channel size of the input, which makes it hard to deal with multiple ”optional” modalities.
In contrast, Transformers [51] are much more amenable to variable-sized inputs, facilitating the training of models that can optionally accept one or multiple modalities [4].
Transformers are known to require large amounts of data for training from scratch. Our model’s data requirements are significantly reduced by incorporating various biases:
We utilize multi-modal pre-training [4, 17, 30], which not only provides better initializations but also improves perfor-mance when only a subset of modalities are accessible dur-ing test-time [4]. Additionally, we propose a token-based action prior. The action taken by the agent has shown to be beneficial for learning VO [35, 64] and primes the model towards the task-relevant image regions.
We introduce the Visual Odometry Transformer (VOT), a novel modality-agnostic framework for VO based on the
Transformer architecture. Multi-modal pre-training and an action prior drastically reduce the data required to train the architecture. Furthermore, we propose explicit modality-invariance training. By dropping modalities during train-ing, a single VOT matches the performance of separate uni-modal approaches. This allows for traversing different sen-sors during test-time and maintaining performance in the absence of some training modalities.
We evaluate our method on point-goal navigation in the
Habitat Challenge 2021 [1] and show that VOT outper-forms previous methods [35] with training on only 5% of the data. Beyond this simple demonstration, we stress that our framework is modality-agnostic and not limited to
RGB-D input or discrete action spaces and can be adapted to various modalities, e.g., point clouds, surface normals, gyroscopes, accelerators, compass, etc. To the best of our knowledge, VOT is the first widely applicable modality-invariant Transformer-based VO approach and opens up ex-citing new applications of deep VO in both simulated and real-world applications. We make our code available at github.com/memmelma/VO-Transformer. 2.