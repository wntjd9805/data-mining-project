Abstract
The high computational costs of video super-resolution (VSR) models hinder their deployment on resource-limited devices, e.g., smartphones and drones. Existing VSR mod-els contain considerable redundant filters, which drag down the inference efficiency. To prune these unimportant fil-ters, we develop a structured pruning scheme called Struc-tured Sparsity Learning (SSL) according to the properties of VSR. In SSL, we design pruning schemes for several key components in VSR models, including residual blocks, re-current networks, and upsampling networks. Specifically, we develop a Residual Sparsity Connection (RSC) scheme for residual blocks of recurrent networks to liberate prun-ing restrictions and preserve the restoration information.
For upsampling networks, we design a pixel-shuffle prun-ing scheme to guarantee the accuracy of feature channel-space conversion.
In addition, we observe that pruning error would be amplified as the hidden states propagate along with recurrent networks. To alleviate the issue, we design Temporal Finetuning (TF). Extensive experiments show that SSL can significantly outperform recent methods quantitatively and qualitatively. The code is available at https://github.com/Zj-BinXia/SSL. 1.

Introduction
Video super-resolution (VSR) aims to generate a high-resolution (HR) video from its corresponding low-resolution (LR) observation by filling in missing details.
With the popularity of intelligent edge devices such as smartphones and small drones, performing VSR on these devices is in high demand. Although a variety of VSR net-works [20, 24, 29, 44, 51] can achieve great performance, these models are usually difficult to be deployed on edge devices with limited computation and memory resources.
To alleviate this issue, we explore a new direction for effective and efficient VSR. To reduce the redundancy of
Conv kernels [4, 5, 36, 38] obtaining a more efficient VSR network, we develop a neural network pruning scheme for the VSR task for the first time. Since structured prun-ing [14, 23, 46, 57] (focusing on filter pruning) can achieve
*Corresponding Author an actual acceleration [41,46] superior to unstructured prun-ing [11,12] (focusing on weight-element pruning), we adopt structured pruning principle to develop our VSR pruning scheme. Given a powerful VSR network, our pruning scheme can find submodels under presetting pruning rate without significantly compromising performance.
Structured pruning is a general concept, and designing a concrete pruning scheme for VSR networks is challeng-ing. (1) Recurrent networks are widely used in VSR models to extract temporal features, consisting of residual blocks (e.g., BasicVSR [2] has 60 residual blocks). However, it is hard to prune the residual blocks because the skip and residual connections ought to share the same indices [23] (Fig. 1 (a)). As shown in Fig. 1 (b), quite a few structured pruning schemes [23,34] do not prune the last Conv layer of the residual blocks, which restricts the pruning space. Re-cently, as shown in Fig. 1 (c), ASSL [57] and SRPN [58] introduce regularization and prune the same indices on skip and residual connections to keep channel alignment (local pruning scheme,i.e., each layer pruning the same ratio of filters). However, ASSL and SRPN still cannot achieve the potential of pruning residual blocks on recurrent networks.
The recurrent networks take the previous output as later in-put (Fig. 2 (a)). This requires the pruned indices of the first and last Convs in recurrent networks to be the same.
But ASSL and SRPN cannot guarantee filter indices are aligned. Besides, many SR methods [45, 56] have shown that the information contained in front Conv layers can help the restoration feature extraction of later Conv layers. Thus, we design a Residual Sparsity Connection (RSC) for VSR recurrent networks, which preserves all channels of the in-put and output feature maps and selects the important chan-nels for operation (Fig. 1 (d)). Compared with other pruning schemes [57, 58], RSC does not require the pruned indices of the first and last Convs of recurrent networks to be the same, can preserve the information contained in all layers, and liberates the pruning space of the last Conv of the resid-ual blocks without adding extra calculations. Notably, RSC can prune residual blocks globally (i.e., the filters in various layers are compared together to remove unimportant ones).
Figure 1. Illustration of different schemes for pruning residual blocks of recurrent networks. (a) Structure of the residual block in the VSR network. (b) The residual block pruning schemes [7, 23, 42] do not prune the last Conv. (c) ASSL [57] and SRPN [58] prunes the same indices on skip and residual connections to keep channel alignment, which abandons some channels of input and output feature maps. (d)
RSC preserves all channels of input and output feature maps, which does not need to align the pruned indices on the first and last Convs in recurrent networks, can fully use restoration information, and can prune the first and last Convs of residual blocks without restrictions. (2) We observe that the upsampling network accounts for 22% of the total calculations in BasicVSR [2], which is necessary to be pruned to reduce redundancy. Since the pixel-shuffle [37] operation in VSR networks converts the channels to space, pruning the pixel-shuffle without any re-strictions would cause the channel-space conversion to fail.
Thus, we specially design a pixel-shuffle pruning scheme by taking four consecutive filters as the pruning unit for 2× (3) Furthermore, we observe that the error pixel-shuffle. of pruning VSR networks would accumulate with propaga-tion steps increasing along with recurrent networks, which limits the efficiency and performance of pruning. Thus, we further introduce Temporal Finetuning (TF) to constrain the pruning error accumulation in recurrent networks. Overall, our main contributions are threefold:
• Our work is necessary and timely. There is an urgent need to compress VSR models for deployment. To the best of our knowledge, we are one of the first to design a structured pruning scheme for VSR.
• We propose an integral VSR pruning scheme called
Structured Sparsity Learning (SSL) for various com-ponents of VSR models, such as residual blocks, re-current networks, and pixel-shuffle operations.
• We employ SSL to train VSR models, which surpass recent pruning schemes and lightweight VSR models. 2.