Abstract
To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do.
Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other hand, operating with a multi-finger robot hand will allow better approximation to human be-havior and enable the robot to operate on diverse articu-lated objects. To this end, we propose a new benchmark called DexArt, which involves Dexterous manipulation with
Articulated objects in a physical simulator. In our bench-mark, we define multiple complex manipulation tasks, and the robot hand will need to manipulate diverse articulated objects within each task. Our main focus is to evaluate the generalizability of the learned policy on unseen articulated objects. This is very challenging given the high degrees of freedom of both hands and objects. We use Reinforcement
*Equal contributions. Work done while interning at UC San Diego.
Learning with 3D representation learning to achieve gen-eralization. Through extensive studies, we provide new in-sights into how 3D representation learning affects decision making in RL with 3D point cloud inputs. More details can be found at https://www.chenbao.tech/dexart/. 1.

Introduction
Most tools and objects humans interact with are articu-lated objects. To allow household robots to facilitate our daily life, we will need to enable them to manipulate di-verse articulated objects with multi-finger hands as hu-mans do. However, learning dexterous manipulation re-mains a challenging task given the high Degree-of-Freedom (DoF) joints of the robot hands. While recent work has shown encouraging progress in using Reinforcement Learn-ing (RL) [1, 8, 29, 69] for dexterous manipulation, most re-search focuses on manipulating a single rigid object. The manipulation of diverse articulated objects not only adds
additional complexity with joint DoF, but also brings new challenges in generalizing to unseen objects in test time, which has been a major bottleneck for RL. This requires efforts on integrating 3D visual understanding and robot learning on a novel benchmark.
Recent proposed robotic manipulation benchmarks [7, 12, 34, 65] play important roles in robot learning algorithm development. For example, the MetaWorld [65] benchmark provides more than 50 tasks for evaluating RL algorithms.
However, each proposed MetaWorld task only focuses on one single object without considering generalization across object instances. To enable generalizability for the robots, the ManiSkill [19, 41] benchmark is proposed with diverse manipulation tasks and a large number of objects to manipu-late within each task. While this is encouraging, the use of a parallel gripper has limited the tasks the robot can perform, and the ways how the robot can operate. For example, it is very challenging for a parallel gripper to pick up a bucket using the handle.
In this paper, we propose a new benchmark for
Dexterous manipulation with diverse Articulated objects (DexArt). We introduce multiple tasks with a dexterous hand (the Allegro Hand) manipulating the articulated ob-jects in the simulation. For each task, instead of operating with a particular object, we provide a training set of diverse articulated objects and the goal is to generalize the policy to a different test set of articulated objects. To achieve such a generalization, we incorporate RL with generalizable visual representation learning: we adopt 3D point clouds as our observations and use a PointNet encoder [44] to extract vi-sual representations for decision making. The generalizabil-ity of the policy depends on the 3D structure understand-ing modeled by the PointNet encoder. We experiment and benchmark with different methods and settings, and provide four key observations as follows: (i) Training with more objects leads to better general-ization. For each task, we trained policies using varying numbers of objects for each task and tested them on the same set of unseen objects. We find training with more objects consistently achieves better success rates. Similar findings have been reported in studies on manipulation with parallel grippers (Generalist-Specialist Learning [24], Man-iSkill [41]). While this might not be surprising from the perception perspective, it does present more challenges for a single RL policy to work with different objects simultane-ously. It highlights the importance of learning generalizable visual representations for RL. (ii) Encoder with a larger capacity does not necessar-ily help. We experiment with different sizes of PointNet encoders, and we observe the simplest one with the least parameters achieves the best sample efficiency and success rate, whether the network is pre-trained or not. This is sur-prising from the vision perspective, but it is consistent with previous literature which shows RL optimization becomes much more challenging with large encoders [41]. (iii) Object part reasoning is essential. With multi-finger hand interacting with different object parts, our intuition is that object part recognition and reasoning can be essential for manipulation. To validate our intuition, we pre-train the
PointNet encoder with object part segmentation tasks. We show the object part pre-training can significantly improve sample efficiency and success rate compared to approaches without pre-training and with other pre-training methods. (iv) Geometric representation learning brings robust pol-icy. We evaluate the robustness of the policy under unseen camera poses. We find that the policy trained with partial point cloud is surprisingly resilient to variations in camera poses, which aligns with the previous studies that use com-plete point clouds in policies [32]. The accuracy remains consistent even with large viewpoint variation. This is par-ticularly useful for real robot applications as it is challeng-ing to align the camera between sim and real.
With the proposed baselines and detailed analysis among them, we hope DexArt benchmark provides a platform to not only study generalizable dexterous manipulation skill itself, but also study how visual perception can be improved to aim for better decision making. We believe the unifi-cation of perception and action, and studying them under
DexArt can create a lot of research opportunities. 2.