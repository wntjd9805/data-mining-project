Abstract
Existing face forgery detection models try to discriminate fake images by detecting only spatial artifacts (e.g., gener-ative artifacts, blending) or mainly temporal artifacts (e.g., flickering, discontinuity). They may experience significant performance degradation when facing out-domain artifacts.
In this paper, we propose to capture both spatial and tempo-ral artifacts in one model for face forgery detection. A simple idea is to leverage a spatiotemporal model (3D ConvNet).
However, we find that it may easily rely on one type of arti-fact and ignore the other. To address this issue, we present a novel training strategy called AltFreezing for more general face forgery detection. The AltFreezing aims to encourage the model to detect both spatial and temporal artifacts. It divides the weights of a spatiotemporal network into two groups: spatial-related and temporal-related. Then the two groups of weights are alternately frozen during the training process so that the model can learn spatial and temporal features to distinguish real or fake videos. Furthermore, we introduce various video-level data augmentation methods to improve the generalization capability of the forgery detec-tion model. Extensive experiments show that our framework outperforms existing methods in terms of generalization to unseen manipulations and datasets. 1.

Introduction
With the recent rapid development of face generation and manipulation techniques [30, 31, 45–49, 56], it has be-come very easy to modify and manipulate the identities or attributes given a face video. This brings many important and impressive applications for movie-making, funny video generation, and so on. However, these techniques can also be abused for malicious purposes, creating serious crisis of trust
*Equal contribution.
†Corresponding authors.
Figure 1. Illustration of AltFreezing training strategy in a build-ing block of the spatiotemporal network. The convolutional ker-nels of the spatiotemporal network are divided into two groups: temporal-based and spatial-based. Two groups of weights are alternately frozen during training. With the help of the alternate freezing (AltFreezing) strategy, our model can capture both spatial and temporal artifacts to distinguish between fake and real videos. and security in our society. Therefore, how to detect video face forgeries has become a hot research topic recently.
To date, one successful line of research [10, 32, 34, 39, 42, 44, 50] tries to discriminate fake images by detecting
“spatial” artifacts in the generated images (e.g., checkboard, unnaturalness, and characteristic artifacts underlying the generative model). While these methods achieve impressive results in searching spatial-related artifacts, they ignore the temporal coherence of a video and fail to capture “temporal” artifacts like flicking and discontinuity in the video face forgeries. Some recent works [25,43,54] notice this issue and try to address it by leveraging temporal clues. Although they achieve encouraging results in detecting unnatural artifacts at the temporal level, the resulting models are not sufficiently capable of finding spatial-related artifacts.
In this paper, we attempt to capture both spatial and tem-poral artifacts for general video face forgery detection. Gen-erally, a well-trained spatiotemporal network (3D ConvNet) has the capability of searching both spatial and temporal artifacts. However, we find that naïve training may cause it to easily rely on spatial artifacts while ignoring temporal artifacts to make a decision, causing a poor generalization
capability. This is because spatial artifacts are usually more obvious than temporal incoherence, naïvely optimizing a 3D convolutional network makes it easily rely on spatial artifacts.
So the question is how to enable the spatiotemporal net-work to capture both spatial and temporal artifacts. To this end, we propose a novel training strategy called AltFreez-ing. As shown in Fig. 1, the key idea is to alternately freeze spatial- and temporal-related weights during training. Specif-ically, a spatiotemporal network [9] is built upon 3D res-blocks, which consist of spatial convolution with kernel size as 1 × Kh × Kw and temporal convolution with kernel size as Kt × 1 × 1. These spatial and temporal convolutional ker-nels are responsible for capturing spatial- and temporal-level features, respectively. Our AltFreezing strategy encourages the two groups of weights to be updated alternately so that both spatial and temporal artifacts can be conquered.
Furthermore, we propose a set of video-level fake video argumentation methods for generating fake videos for train-ing. These methods could be divided into two groups. The first is fake clips that only involve temporal artifacts wherein we just randomly drop and repeat frames for real clips. The second is clips with only spatial artifacts that are obtained by blending a region from one real clip to another real clip.
These augmentation methods are the first to take the tempo-ral dimension into consideration and generate spatial-only and temporal-only fake videos. With these augmentations, the spatiotemporal model is further encouraged to capture both spatial and temporal artifacts.
Equipped with the above-mentioned two techniques, we achieve state-of-the-art performance in various challenging face forgery detection scenarios, including generalization capability to unseen forgeries, and robustness to various perturbations. We also provide a comprehensive analysis of our method to verify the effectiveness of our proposed framework.
Our main contributions are three-fold as follows.
• We propose to explore both spatial and temporal arti-facts for video face forgery detection. To achieve this, a novel training strategy called AltFreezing is proposed.
• We introduce video-level fake data augmentation meth-ods to encourage the model to capture a more general representation of different types of forgeries.
• Extensive experiments on five benchmark datasets in-cluding both cross-manipulation and cross-dataset eval-uations demonstrate that the proposed method sets new state-of-the-art performance. 2.