Abstract
Humans possess a versatile mechanism for extracting structured representations of our visual world. When look-ing at an image, we can decompose the scene into entities and their parts as well as obtain the dependencies between them. To mimic such capability, we propose Visual Depen-dency Transformers (DependencyViT) 1 that can induce vi-sual dependencies without any labels. We achieve that with a novel neural operator called reversed attention that can naturally capture long-range visual dependencies between image patches. Specifically, we formulate it as a depen-dency graph where a child token in reversed attention is trained to attend to its parent tokens and send information following a normalized probability distribution rather than gathering information in conventional self-attention. With such a design, hierarchies naturally emerge from reversed attention layers, and a dependency tree is progressively in-duced from leaf nodes to the root node unsupervisedly.
DependencyViT offers several appealing benefits. (i) En-tities and their parts in an image are represented by dif-ferent subtrees, enabling part partitioning from dependen-cies; (ii) Dynamic visual pooling is made possible. The leaf nodes which rarely send messages can be pruned with-out hindering the model performance, based on which we propose the lightweight DependencyViT-Lite to reduce the computational and memory footprints; (iii) DependencyViT works well on both self- and weakly-supervised pretraining paradigms on ImageNet, and demonstrates its effectiveness on 8 datasets and 5 tasks, such as unsupervised part and saliency segmentation, recognition, and detection. 1.

Introduction
Humans have a rich mental representation of our sur-rounding environments. When looking at an image (see
Figure 1(a)), we can recognize the scene and also can
*This work was done when Mingyu was visiting MIT. 1https://github.com/dingmyu/DependencyViT (a) (b)
Figure 1. (a) is an example of hierarchical dependency structure. (b) illustrates the dynamic pooling and information aggregation process of DependencyViT. quickly decompose it into hierarchical elements with de-pendencies, e.g., a laptop consisting of a screen and a key-board is placed on the table. This ability to construct depen-dencies between objects (and/or their parts) serves as the cornerstone of human intelligence, enabling us to perceive, interact, and reason about the world.
From the pre-deeplearning era, many classical image de-pendency parsing algorithms [25, 27, 66, 70, 81, 98] have been proposed. For example, Bayesian framework [70],
And-Or graph [27], and hierarchical probabilistic mod-els [25, 66] for parsing images into their constituent visual patterns. Apart from that, Capsule Network [40, 61] shows the potential to learn geometrically organized parts from images. After that, visual grounding methods [10, 18, 21, 23, 85] try to align the semantic meaning between visual objects and words to distill effective structures for the vi-sion branch from language. Similarly, human-object inter-action approaches [39] learn the relationships between two objects, e.g., a boy “holds” an ice cream, from manually annotated labels. Such methods struggle to learn hierarchi-cal visual structures, such as different parts of an object, unless exhaustive and time-consuming manual annotations are provided. Recently, vision-language (VL) grammar in-duction [72] proposes to extract shared hierarchical object
dependencies for both vision and language unsupervisedly from image-caption pairs. However, the above works suffer two key issues: 1) the parsing relies heavily on supervision from natural language or human annotations rather than the image itself, and 2) their parsed structures are object-level based on a pre-trained object detection model, like
Faster/Mask-RCNN [30, 58], hindering their generalizabil-ity in part-level and non-detector scenarios.
This paper answers a question naturally raised from the above issues: can we efficiently induce visual dependencies and build hierarchies from images without human annota-tions? Currently, visual parsing works mainly lie in seman-tic and instance segmentation. Unlike detector-based works that rely on pre-trained detectors, they parse the image at the pixel level, which is resource-intensive and costly. Inspired by vision transformers [22] that take image patches as input and leverage self-attention to perform interactions between patches, we propose to build a dependency tree at the patch level. Taking patches as basic elements and building a tree structure based on them has two benefits: 1) it unifies part-level and object-level dependencies, all of which are formu-lated into subtrees; 2) in the dependency structure, informa-tion can be aggregated from leaves to the parent (as shown in Figure 1(b)) to produce a hierarchy of representations for different parts and object along the path.
In practice, it is non-trivial to build the dependency tree with the standard transformer. Although the self-attention mechanism is designed to collect information dynamically from other patches, the number of attention heads con-straints the number of tokens that a patch can attend to. 1)
However, each parent could have an arbitrary number of children in a dependency tree, while each child only has one parent. Thus it’s more straightforward for a node to select its parent instead of selecting the child. 2) Further-more, the transformer treats each patch equally, it does not distinguish between root and leaf nodes. Contributions for different subtrees should be distinct.
Motivated by the above observations, in this work, we propose a dependency-inspired vision transformer, named
Visual Dependency Transformers (DependencyViT). We propose three innovations to the standard self-attention, as shown in Figure 2. Firstly, to form a root-centric depen-dency parser, we introduce a reversed self-attention mecha-nism by transposing the adjacency matrix. In this way, leaf nodes can send information to their parents and form hi-erarchical subtrees. Secondly, we propose a message con-troller to determine how a node or subtree sends messages.
Thirdly, a soft head selector is introduced to generate a unique dependency graph for each layer. As a result, self-attentions in DependencyViT naturally form a dependency tree parser. We did extensive studies in both supervised and self-supervised pretraining to show DependencyViT is ca-pable of capturing either object- or part-level dependencies.
Intuitively, dependency parsing should ease scene under-standing, as humans can understand complex scenes at a glance based on visual dependencies. Based on this, we further introduce a lightweight model DependencyViT-Lite by proposing a dynamic pooling scheme, reducing the com-putational cost largely. Within each subtree, we prune those leaf nodes with the least information received because they have sent information to their parent node. We show the pruned nodes can be retrieved by soft aggregations from their parents, preserving the model capability and dense representation capability.
We make three main contributions. (i) DependencyViT performs visual dependency parsing by reversed attention in self- or weakly-supervised manners. We demonstrate its effectiveness in both part-level and object-level parsing. (ii) We propose a visual dynamic pooling scheme for De-pendencyViT hence DependencyViT-Lite. The dependency tree can also be progressively built during the pruning pro-cess. (iii) Extensive experiments on both self- and weakly-supervised pretraining on ImageNet, as well as five down-stream tasks, show the effectiveness of DependencyViT. 2.