Abstract
Latent Diffusion Models (LDMs) enable high-quality im-age synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-latent space. Here, we apply the LDM dimensional paradigm to high-resolution video generation, a particu-larly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded im-age sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video
In particular, we validate our Video LDM on modeling. real driving videos of resolution 512 × 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an ef-ficient and expressive text-to-video model with resolution up to 1280 × 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/
*Equal contribution.
†Andreas, Robin and Tim did the work during internships at NVIDIA.
Figure 2. Temporal Video Fine-Tuning.
We turn pre-trained image diffusion mod-els into temporally consistent video gener-ators. Initially, different samples of a batch synthesized by the model are independent.
After temporal video fine-tuning, the sam-ples are temporally aligned and form co-herent videos. The stochastic generation process before and after fine-tuning is visu-alised for a diffusion model of a one-dim. toy distribution. For clarity, the figure cor-responds to alignment in pixel space.
In practice, we perform alignment in LDM’s latent space and obtain videos after ap-plying LDM’s decoder (see Fig. 3). We also video fine-tune diffusion model up-samplers in pixel or latent space (Sec. 3.4). 1.

Introduction
Generative models of images have received unprece-dented attention, owing to recent breakthroughs in the un-derlying modeling methodology. The most powerful mod-els today are built on generative adversarial networks [21, 38–40, 75], autoregressive transformers [15, 63, 105], and most recently diffusion models [10, 28, 29, 57, 58, 62, 65, 68, 79, 82]. Diffusion models (DMs) in particular have de-sirable advantages; they offer a robust and scalable train-ing objective and are typically less parameter intensive than their transformer-based counterparts. However, while the image domain has seen great progress, video modeling has lagged behind—mainly due to the significant computa-tional cost associated with training on video data, and the lack of large-scale, general, and publicly available video datasets. While there is a rich literature on video synthe-sis [1, 6, 8, 9, 17, 19, 22, 23, 32, 32, 37, 42, 44, 47, 51, 55, 59, 71, 78, 85, 91, 94, 97–99, 103, 106], most works, including previous video DMs [24, 31, 33, 93, 104], only generate rel-atively low-resolution, often short, videos. Here, we ap-ply video models to real-world problems and generate high-resolution, long videos. Specifically, we focus on two rel-evant real-world video generation problems: (i) video syn-thesis of high-resolution real-word driving data, which has great potential as a simulation engine in the context of au-tonomous driving, and (ii) text-guided video synthesis for creative content generation; see Fig. 1.
To this end, we build on latent diffusion models (LDMs), which can reduce the heavy computational burden when training on high-resolution images [65]. We propose Video
LDMs and extend LDMs to high-resolution video genera-tion, a particularly compute-intensive task. In contrast to previous work on DMs for video generation [24, 31, 33, 93, 104], we first pre-train our Video LDMs on images only (or use available pre-trained image LDMs), thereby allowing us to leverage large-scale image datasets. We then trans-form the LDM image generator into a video generator by introducing a temporal dimension into the latent space DM and training only these temporal layers on encoded image sequences, i.e., videos (Fig. 2), while fixing the pre-trained spatial layers. We similarly fine-tune LDM’s decoder to achieve temporal consistency in pixel space (Fig. 3). To further enhance the spatial resolution, we also temporally align pixel-space and latent DM upsamplers [29], which are widely used for image super resolution [43, 65, 68, 69], turning them into temporally consistent video super resolu-tion models. Building on LDMs, our method can generate globally coherent and long videos in a computationally and memory efficient manner. For synthesis at very high reso-lutions, the video upsampler only needs to operate locally, keeping training and computational requirements low. We ablate our method and test on 512 × 1024 real driving scene videos, achieving state-of-the-art video quality, and synthe-size videos of several minutes length. We also video fine-tune a powerful, publicly available text-to-image LDM, Sta-ble Diffusion [65], and turn it into an efficient and powerful text-to-video generator with resolution up to 1280 × 2048.
Since we only need to train the temporal alignment layers in that case, we can use a relatively small training set of cap-tioned videos. By transferring the trained temporal layers to differently fine-tuned text-to-image LDMs, we demonstrate personalized text-to-video generation for the first time. We hope our work opens new avenues for efficient digital con-tent creation and autonomous driving simulation.
Contributions. (i) We present an efficient approach for training high-resolution, long-term consistent video genera-tion models based on LDMs. Our key insight is to leverage pre-trained image DMs and turn them into video generators by inserting temporal layers that learn to align images in a temporally consistent manner (Figs. 2 and 3). (ii) We fur-ther temporally fine-tune super resolution DMs, which are ubiquitous in the literature. (iii) We achieve state-of-the-art high-resolution video synthesis performance on real driv-ing scene videos, and we can generate multiple minute long
framework [82] (see Figs. 2 and 3), but in practice a fixed discretization can be used [28]. The maximum diffusion time is generally chosen such that the input data is entirely perturbed into Gaussian random noise and an iterative gen-erative denoising process that employs the learned denoiser fθ can be initialized from such Gaussian noise to synthe-size novel data. Here, we use pτ ∼ U{0, 1000} and rely on a variance-preserving noise schedule [82], for which
τ = 1 − α2
σ2
τ (see Appendices F and H for details).
Latent Diffusion Models (LDMs) [65] improve in com-putational and memory efficiency over pixel-space DMs by first training a compression model to transform input im-ages x∼pdata into a spatially lower-dimensional latent space of reduced complexity, from which the original data can be reconstructed at high fidelity. In practice, this approach is implemented with a regularized autoencoder, which recon-structs inputs x via an encoder module E and a decoder D, such that the reconstruction ˆx=D(E(x))≈x (Fig. 3). To en-sure photorealistic reconstructions, an adversarial objective can be added to the autoencoder training [65], which is im-plemented using a patch-based discriminator [35]. A DM can then be trained in the compressed latent space and x in
Eq. (1) is replaced by its latent representation z=E(x). This latent space DM can be typically smaller in terms of pa-rameter count and memory consumption compared to cor-responding pixel-space DMs of similar performance. 3. Latent Video Diffusion Models
Here we describe how we video fine-tune pre-trained im-age LDMs (and DM upsamplers) for high-resolution video synthesis. We assume access to a dataset pdata of videos, such that x ∈ RT ×3× ˜H× ˜W , x ∼ pdata is a sequence of T
RGB frames, with height and width ˜H and ˜W . 3.1. Turning Latent Image into Video Generators
Our key insight for efficiently training a video genera-tion model is to re-use a pre-trained, fixed image generation model; an LDM parameterized by parameters θ. Formally, let us denote the neural network layers that comprise the image LDM and process inputs over the pixel dimensions as spatial layers li
θ, with layer index i. However, although such a model is able to synthesize individual frames at high quality, using it directly to render a video of T consecutive frames will fail, as the model has no temporal awareness.
We thus introduce additional temporal neural network lay-ers li
ϕ, which are interleaved with the existing spatial layers li
θ and learn to align individual frames in a temporally con-sistent manner. These L additional temporal layers {li i=1 define the video-aware temporal backbone of our model, and the full model fθ,ϕ is thus the combination of the spa-tial and temporal layers; see Fig. 4 for a visualization.
ϕ}L
We start from a frame-wise encoded input video E(x) = z ∈ RT ×C×H×W , where C is the number of latent channels
Figure 3. Top: During temporal decoder fine-tuning, we process video sequences with a frozen encoder, which processes frames independently, and enforce temporally coherent reconstructions across frames. We additionally employ a video-aware discrimina-tor. Bottom: in LDMs, a diffusion model is trained in latent space.
It synthesizes latent features, which are then transformed through the decoder into images. Note that the bottom visualization is for individual frames; see Fig. 2 for the video fine-tuning framework that generates temporally consistent frame sequences. videos. (iv) We transform the publicly available Stable Dif-fusion text-to-image LDM into a powerful and expressive text-to-video LDM, and (v) show that the learned temporal layers can be combined with different image model check-points (e.g., DreamBooth [66]). 2.