Abstract
Scale variation across object instances remains a key challenge in object detection task. Despite the remarkable progress made by modern detection models, this challenge is particularly evident in the semi-supervised case. While existing semi-supervised object detection methods rely on strict conditions to filter high-quality pseudo labels from network predictions, we observe that objects with extreme scale tend to have low confidence, resulting in a lack of positive supervision for these objects.
In this paper, we propose a novel framework that addresses the scale vari-ation problem by introducing a mixed scale teacher to im-prove pseudo label generation and scale-invariant learn-ing. Additionally, we propose mining pseudo labels using score promotion of predictions across scales, which bene-fits from better predictions from mixed scale features. Our extensive experiments on MS COCO and PASCAL VOC benchmarks under various semi-supervised settings demon-strate that our method achieves new state-of-the-art per-formance. The code and models are available at https:
//github.com/lliuz/MixTeacher. 1.

Introduction
The remarkable performance of deep learning on various tasks can largely be attributed to large-scale datasets with accurate annotations. However, collecting a large amount of high-quality annotations is infeasible as it is labor-intensive and time-consuming, especially for tasks with complicated annotations such as object detection [23, 30] and segmen-tation [5, 6]. To reduce reliance on manual labeling, semi-supervised learning (SSL) has gained much attention. SSL aims to train models on a small amount of labeled im-ages and a large amount of easily accessible unlabeled data.
† Corresponding Authors.
Figure 1. Detection results with input of regular 1× scale and 0.5× down-sampled scale images. We plot the precision and recall under different score thresholds for (a) all objects and (b) large objects in COCO val2017 with the same model but different input scales. Two examples of unlabeled images are given in (c).
Large scale inputs have clear advantages in overall metrics, but down-sampled images are more suitable for large objects.
Following extensive pioneering studies on semi-supervised image classification [2, 14, 32], several methods on semi-supervised object detection have emerged.
Most early studies on semi-supervised object detec-tion [13, 24, 33] can be considered as a direct extension of SSL methods designed for image classification, using a teacher-student training paradigm [2,32,35]. In these meth-ods, a teacher model generates pseudo bounding boxes and corresponding class predictions on unlabeled images, and the pseudo labels are used to train a student model. Despite the performance improvement from using a large amount of unlabeled data, these methods overlooked the characteris-tics of object detection to some extent, resulting in a huge
gap from the fully supervised counterpart.
Compared to image classification, object instances in de-tection tasks can vary in a wider range of scales. To address this challenge of detecting and localizing multiple objects across scales and locations, numerous works in object de-tection have been proposed, such as FPN [21], Trident [20], and SNIP [31]. However, the large scale variation brings new challenges in the semi-supervised context. In order to guarantee high precision, most existing semi-supervised ob-ject detection methods adopt strict conditions (e.g. score > 0.9) to filter out highly confident pseudo labels. Although this ensures the quality of pseudo labels, many objects with low confidence are wrongly assigned as background, es-pecially for those with extreme scales. As shown in Fig-ure 1 (c), inappropriate scales will lead to false negatives, which can mislead the network in semi-supervised learn-ing. We further observe the influence of the test scale of the images. Consistent with common sense, large-scale inputs have clear advantages in overall metrics, as shown in Fig-ure 1 (a). However, down-sampled images show a superior-ity for large objects, as shown in Figure 1 (b). This provides a new view to handle the scale variation issue.
It is worth mentioning that recent works have paid at-tention to the scale variation issue in semi-supervised ob-ject detection. As shown in Figure 2 (a) and (b), previ-ous methods have introduced an additional down-sampled view to encourage the model to make scale-invariant pre-dictions. Specifically, SED [10] proposes to distill predic-tions of class probability from the regular scale to the down-sampled scale and constrain consistent predictions of local-ization for all proposals in two scales. PseCo [17] adopts the same pseudo labels generated from the regular scale for both scales. However, these methods mainly focus on the consistency of predictions across scales, which indirectly improves the models with regularization. Moreover, they highly rely on the pseudo labels generated from the regular scale in the teacher network. The false negatives caused by inappropriate scales still remain in these methods.
Based on the above methods, which are equipped with an additional down-sampled view of unlabeled images, we propose to explicitly improve the quality of pseudo la-bels to handle the scale variation of objects. As shown in Figure 2 (c), we introduce a mixed-scale feature pyra-mid, which is built from the large-scale feature pyramid in the regular view and the small-scale feature pyramid in the down-sampled view. The mixed-scale feature pyramid is supposed to be capable of adaptively fusing features across scales, thus making better predictions in the teacher net-work. Furthermore, to avoid object instances missing in the pseudo labels due to low confidence scores, we propose to leverage the improvement of score as an indicator for min-ing pseudo labels from low confidence predictions. In sum-mary, the main contributions are as follows:
• We propose a semi-supervised object detection frame-work MixTeacher, in which high-quality pseudo labels are generated from a mixed scale feature pyramid.
• We propose a method for pseudo labels mining, which leverages the improvement of predictions as the indi-cator to mining the promising pseudo labels.
• Our method achieves state-of-the-art performance on
MS COCO and Pascal VOC benchmarks under various semi-supervised settings. 2.