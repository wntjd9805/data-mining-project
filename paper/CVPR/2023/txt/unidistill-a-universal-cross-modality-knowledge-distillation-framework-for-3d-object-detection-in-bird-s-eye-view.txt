Abstract
In the field of 3D object detection for autonomous driving, the sensor portfolio including multi-modality and single-modality is diverse and complex. Since the multi-modal methods have system complexity while the accuracy of single-modal ones is relatively low, how to make a trade-off between them is difficult.
In this work, we propose a universal cross-modality knowledge distillation framework (UniDistill) to improve the performance of single-modality detectors. Specifically, during training, UniDistill projects the features of both the teacher and the student detector into
Bird’s-Eye-View (BEV), which is a friendly representation for different modalities. Then, three distillation losses are calculated to sparsely align the foreground features, help-ing the student learn from the teacher without introducing additional cost during inference. Taking advantage of the similar detection paradigm of different detectors in BEV,
UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three distillation losses can filter the effect of misaligned background information and bal-ance between objects of different sizes, improving the dis-tillation effectiveness. Extensive experiments on nuScenes demonstrate that UniDistill effectively improves the mAP and NDS of student detectors by 2.0%∼3.2%. 1.

Introduction 3D object detection plays a critical role in autonomous driving and robotic navigation. Generally, the popular 3D detectors can be categorized into (1) single-modality detectors that are based on LiDAR [18, 33, 34, 42, 43] or camera [1, 13, 20, 24] and (2) multi-modality detec-tors [22, 30, 36, 37] that are based on both modalities. By fusing the complementary knowledge of two modalities,
*Equal Contribution
†Corresponding Author
Figure 1. Illustration of our proposed UniDistill. The characters in green and blue represent the data process of camera and LiDAR re-spectively. (a) and (b) show the procedure of two previous knowl-edge distillation methods, where the modalities of the teacher and the student are restricted. By contrast, our proposed UniDistill in (c) supports four distillation paths. multi-modality detectors outperform their single-modality counterparts. Nevertheless, simultaneously processing the data of two modalities unavoidably introduces extra net-work designs and computational overhead. Worse still, the breakdown of any modality directly fails the detection, hin-dering the application of these detectors.
As a solution, some recent works introduced knowledge distillation to transfer complementary knowledge of other modalities to a single-modality detector. In [6, 15, 46], as il-lustrated in Figure 1(a) and 1(b), for a single-modality stu-dent detector, the authors first performed data transforma-tion of different modalities to train a structurally identical
teacher. The teacher was then leveraged to transfer knowl-edge by instructing the student to produce similar features and prediction results. In this way, the single-modality stu-dent obtains multi-modality knowledge and improves per-formance, without additional cost during inference.
Despite their effectiveness to transfer cross-modality knowledge, the application of existing methods is limited since the modalities of both the teacher and the student are restricted. In [6], the modalities of the teacher and student are fixed to be LiDAR and camera while in [15,46], they are determined to be LiDAR-camera and LiDAR. However, the sensor portfolio in the field of 3D object detection results in a diverse and complex application of different detectors.
With restricted modalities of both the teacher and student, these methods are difficult to be applied in more situations, e.g., the method in [6] is not suitable to transfer knowledge from a camera based teacher to a LiDAR based student.
To solve the above problems, we propose a universal cross-modality knowledge distillation framework (UniDis-till) that helps single-modality detectors improve perfor-mance. Our motivation is based on the observation that the detectors of different modalities adopt a similar detection paradigm in bird’s-eye view (BEV), where after transform-ing the low-level features to BEV, a BEV encoder follows to further encode high-level features and a detection head produces response features to perform final prediction.
UniDistill takes advantage of the similarity to construct the universal knowledge distillation framework. As in Fig-ure 1(c), during training, UniDistill projects the features of both the teacher and the student detector into the unified
BEV domain. Then for each ground truth bounding box, three distillation losses are calculated to transfer knowl-edge: (1) A feature distillation loss that transfers the seman-tic knowledge by aligning the low-level features of 9 cru-cial points. (2) A relation distillation loss that transfers the structural knowledge by aligning the relationship between the high-level features of 9 crucial points. (3) A response distillation loss that closes the prediction gap by aligning the response features in a Gaussian-like mask. Since the aligned features are commonly produced by different detec-tors, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distilla-tion paths. Furthermore, the three losses sparsely align the foreground features to filter the effect of misaligned back-ground information and balance between objects of differ-ent scales, improving the distillation effectiveness.
In summary, our contributions are three-fold:
• We propose a universal cross-modality knowledge dis-tillation framework (UniDistill) in the friendly BEV domain for single-modality 3D object detectors. With the transferred knowledge of different modalities, the performance of single-modality detectors is improved without additional cost during inference.
• Benefiting from the similar detection paradigm in
BEV, UniDistill supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Moreover, three distillation losses are designed to sparsely align foreground features, fil-tering the effect of background information misalign-ment and balance between objects of different sizes.
• Extensive experiments on nuScenes demonstrate that
UniDistill can effectively improve the mAP and NDS of student detectors by 2.0%∼3.2%. 2.