Abstract
There is a growing demand of automatically synthesizing co-speech gestures for virtual characters. However, it re-mains a challenge due to the complex relationship between input speeches and target gestures. Most existing works fo-cus on predicting the next gesture that ﬁts the data best, however, such methods are myopic and lack the ability to plan for future gestures. In this paper, we propose a novel reinforcement learning (RL) framework called RACER to generate sequences of gestures that maximize the overall satisfactory. RACER employs a vector quantized varia-tional autoencoder to learn compact representations of ges-tures and a GPT-based policy architecture to generate co-herent sequence of gestures autoregressively. In particular, we propose a contrastive pre-training approach to calculate the rewards, which integrates contextual information into action evaluation and successfully captures the complex re-lationships between multi-modal speech-gesture data. Ex-perimental results show that our method signiﬁcantly out-performs existing baselines in terms of both objective met-rics and subjective human judgements. Demos can be found at https://github.com/RLracer/RACER.git. 1.

Introduction
Gesturing is important for human speakers to improve their expressiveness. It conveys the necessary non-verbal information to the audience, giving the speech a more emo-tional touch so that to enhance persuasiveness and credibil-ity. Similarly, in virtual world, high-quality 3D gesture ani-mations can make a talking character more vividly. For ex-ample, in human-computer interaction scenarios, vivid ges-tures performed by virtual characters can help listeners to concentrate and improve the intimacy between humans and
*Equal contribution.
†Corresponding author. characters [36]. Attracted by these merits, there has been a growing demand of automatically synthesizing high-quality co-speech gestures in computer animation.
However, automatically synthesizing co-speech gestures remains a challenge due to the complicated relationship be-tween speech audios and gestures. On one hand, a speaker may play different gestures when speaking the same words due to different mental and physical states. On the other hand, a speaker may also play similar gestures when speak-ing different words. Therefore, co-speech gesture synthe-sizing is inherently a “many-to-many” problem [21]. More-over, in order to ensure the overall ﬂuency and consistency, we must take into consideration both the contextual infor-mation and the subsequent effect upon playing a gesture
[23, 38]. Therefore, gesture synthesizing is rather a sequen-tial decision making problem than a simple matching be-tween speeches and gestures.
Compared with traditional rule-based approaches [35], data-driven gesture synthesis approaches [10,39] has shown many advantages, including the low development cost and the ability to generalize. Most existing data-driven ap-proaches consider gesture synthesis as a classiﬁcation task
[6, 26] or a regression task [17] in a deterministic way i.e. the same speech or text input always maps to the same ges-ture output. However, these models rely on the assumption that there exists a unique ground-truth label for each in-put sequence of speech, which contradicts to the “many-to-many” nature of the problem. As a consequence, they sac-riﬁce diversity and semantics of gestures and tend to learn some averaged gestures for any input speeches. Some other works adopt adversarial learning framework, where a dis-criminator is trained to distinguish between generated ges-tures and the recorded gestures in dataset [10,38]. Although they improve the generalizability of the gesture generator to some extent, they still fail to explore the essential relation-ship between speeches and gestures.
In order to address the above challenges, we propose
a novel Reinforcement leArning framework with Contra-sitive prE-trained Rewards (RACER) for generating high-quality co-speech gestures. RACER is trained in an of-ﬂine manner but can be used to generate the next gesture for a speaking character in real time. RACER consists of the following three components. Firstly, in order to ex-tract meaningful gestures from the inﬁnite action space,
RACER adopts a vector quantized variational autoencoder (VQ-VAE) [32] to learn compact gesture representations, which signiﬁcantly reduces the action space. Secondly, we construct the Q-value network using a GPT-based [28] model, which has natural advantages of generating coher-ence sequence of gestures. Thirdly, inspired by the con-trastive language-image pre-training (CLIP) [27] and recent advances of contrastive learning [30], we propose a con-trastive speech-gesture pre-training method to compute the rewards, which guide the RL agent to explore sophisticated relations between speeches and gestures. Note that these rewards evaluate the quality of gestures as a sequence. By contrast, in conventional supervised learning frameworks, the focus is on predicting only the next gesture, ignoring the quality of generated sequence as a whole. To sum up, the core contributions of this paper are as follows: (1) We formally model the co-speech gesture synthesis problem as a Markov decision process and propose a novel RL based approach called RACER to learn the optimal gesture synthesis policy. RACER can be trained in an ofﬂine manner and used to synthesis co-speech gestures in real time. (2) We introduce VQ-VAE to encode and quantize the motion segments to a codebook, which signiﬁcantly reduces the action space and facilitates the RL phase. (3) We propose a contrastive speech-gesture pre-training method to compute the rewards, which guide the RL agent to discover deeper relations from multi-modal speech-gesture data. (4) Extensive experimental results show that RACER outperforms the existing baselines in terms of both objective metrics and subjective human judgements.
This demonstrates the superiority and the potential of
RL in co-speech gesture synthesis tasks. 2.