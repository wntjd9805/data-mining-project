Abstract
Diffusion models have demonstrated impressive capabil-ity of text-conditioned image synthesis, and broader appli-cation horizons are emerging by personalizing those pre-trained diffusion models toward generating some special-ized target object or style. In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffu-sion model with a handful of images (e.g., less than 10), so that the fine-tuned model can generate high-quality im-ages of arbitrary objects in this style. Such extremely low-shot fine-tuning is accomplished by a novel toolkit of fine-tuning techniques, including text-to-image customized data augmentations, a content loss to facilitate content-style dis-entanglement, and sparse updating that focuses on only a few time steps. Our framework, dubbed Specialist Dif-fusion, is plug-and-play to existing diffusion model back-bones and other personalization techniques. We demon-strate it to outperform the latest few-shot personalization alternatives of diffusion models such as Textual Inversion
[7] and DreamBooth [24], in terms of learning highly so-phisticated styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of textual inversion to boost performance further, even on highly unusual styles. Our codes are available at: https://github.com/Picsart-AI-Research/
Specialist-Diffusion. 1.

Introduction
Image synthesis has received increasing attention, par-tially owing to the recent breakthroughs made by diffusion models [10, 23, 28, 30, 34]. Training a diffusion model re-quires gradually adding random noises with a sequence of diffusion steps, and learning to rebuild data from noises by reversing the steps. By running the diffusion process on a lower-dimensional latent space instead of the pixel space, the latent diffusion model [23] achieves competitive per-1The first two authors Lu and Tunanyan contributed equally.
Figure 1. Comparison of fine-tuning the pre-trained Stable Dif-fusion [23] model, using our Specialist Diffusion method versus two other methods, from left to right columns. Three rows rep-resent three different, rare styles (“Flat design”, “Fantasy”, and
“Food doodle”) that we hope to personalize the Stable Diffusion model to learn, using only a handful of samples (even less than 10). All examples are generated using the same text prompt except for the style identifier. As a method focused on representing new objects, DreamBooth [24] performs poorly when being applied to capturing styles. Textual inversion [7] achieves neat performance on some styles, but fails on more unusual styles such as “Flat de-sign”. Specialist Diffusion (rightmost) succeeds to capture those highly unusual, specialized, and sophisticated styles via few-shot tuning. Please see Sec. 4 for more dataset and experiment details. formance more efficiently. Diffusion models are capable of both unconditioned and conditioned image synthesis, the previous one generates samples from random noise (noise-to-image), while the latter takes a condition such as text to guide the generation. In this paper, we will focus on text-to-image synthesis for its broad application interests.
Diffusion models nowadays are able to synthesize high-quality images on a wide variety of sophisticated text
prompts. However, those models still have limited cover-ages on what they can generate - and their generated images are often found to miss highly specialized objects or unusual styles [19]. Practitioners are hence motivated to personal-ize those models on the target data of their interest. Notably, many target domains have fine granularity, such as selfies of one user, or a specialized design style; consequently, sam-ples available from such target domains would be limited and constitute few-shot scenarios.
There was little work on few-shot personalization with diffusion models until recently. Most conditional models focus on specific applications. Here we consider the gen-eral case of few-shot generation for unknown classes at test time. One naive solution is to fine-tune the model, but it can easily go sample-costly due to the enormous model size.
For example, the training of Waifu-diffusion [15], a model fine-tuned from Stable Diffusion v1.4 [23], took approxi-mately 10 days on 8 Nvidia A40 GPUs with 680K text-image pairs. Some initial attempts were made to reduce the sample complexity. D2C [26] shows that latent space and self-supervised learning lead to few-shot tuning with as few as 100 samples. [8] explicitly conditions the denois-ing diffusion dynamics on a support set of target domain samples, and takes only 5 samples to generate images from a new class. Despite the promise, they modified the model architecture by injecting an unconventional set-based vision transformer (ViT) [5] to aggregate new image patch infor-mation. Moreover, their evaluation was on unconditional image generation with the resolution of 32 × 32 or 64 × 64, and it remains unclear how their method can extend to text-to-image synthesis, or to generating high-resolution images.
Several latest works shed new light on the horizon of few-shot personalization. Among them, DreamBooth [24] fine-tunes the pre-trained diffusion model to bind a unique identifier with an unseen object. Textual Inversion [7], in comparison, learns to represent a new concept through a new token “word” in the embedding space, without fine-tuning the parameters of the pretrained model. Both meth-ods can achieve personalization with ∼ 5 images. How-ever, both DreamBooth and Textual inversion were mainly demonstrated to synthesize new unknown objects (such as a selfie) in various known styles or contexts; while their ex-tension to the complementary side, e.g., synthesizing known objects in new unknown styles, are under-explored and of-ten found to be unsatisfactory, despite the apparent demands of capturing unfamilar or personalized styles by artist users.
In this paper, we focus on fine-tuning a pre-trained text-to-image diffusion model, to learn a highly specialized un-seen style, using a handful of images. Our goal is to have the fine-tuned model generate high-quality images of arbitrary (known) objects in this (previously unknown) style. Our proposed solution, Specialist Diffusion, is a plug-and-play set of fine-tuning techniques that works with any diffusion backbone without altering their architectures, and can also be integrated with existing personalization methods such as [7,24]. Firstly, our customized augmentations are specif-ically designed for the text-to-image scenario, that augment not only the images but also the text prompts with prior lan-guage knowledge from the image augmentation. Secondly, to prevent the model from overfitting to the target style while losing generalization to various objects, we introduce a content loss to preserve the ability to generate specified content based on CLIP [21]. Besides, instead of updating all diffusion steps, we find that updating only a sparse sub-set of steps can significantly improve the few-shot training efficiency, with comparable or sometimes better fine-tuning performance. Our contributions are summarized as:
• Specialist Diffusion is a general, plug-and-play frame-work to fine-tune pre-trained diffusion models with few-shot samples (less than 10), to learn highly sophis-ticated and unusual styles, without bells and whistles.
• We propose a rich set of techniques including vari-ous customized data augmentations that augment both text prompts and images, a content loss for disentan-glement, and sparse diffusion step updating for both sample and computation efficiency.
• Specialist Diffusion not only improves upon the state-of-the-art diffusion model personalization methods such as Textual Inversion [7] and DreamBooth [24] (e.g., see Fig. 1), but also can be combined on top of them to jointly boost style personalization further. 2.