Abstract
Differentiable architecture search (DARTS) has signif-icantly promoted the development of NAS techniques be-cause of its high search efficiency and effectiveness but suf-fers from performance collapse.
In this paper, we make efforts to alleviate the performance collapse problem for
DARTS from two aspects. First, we investigate the expres-sive power of the supernet in DARTS and then derive a new setup of DARTS paradigm with only training Batch-Norm. Second, we theoretically find that random features dilute the auxiliary connection role of skip-connection in supernet optimization and enable search algorithm focus on fairer operation selection, thereby solving the performance collapse problem. We instantiate DARTS and PC-DARTS with random features to build an improved version for each named RF-DARTS and RF-PCDARTS respectively. Experi-mental results show that RF-DARTS obtains 94.36% test ac-curacy on CIFAR-10 (which is the nearest optimal result in
NAS-Bench-201), and achieves the newest state-of-the-art top-1 test error of 24.0% on ImageNet when transferring from CIFAR-10. Moreover, RF-DARTS performs robustly across three datasets (CIFAR-10, CIFAR-100, and SVHN) and four search spaces (S1-S4). Besides, RF-PCDARTS achieves even better results on ImageNet, that is, 23.9% top-1 and 7.1% top-5 test error, surpassing representative methods like single-path, training-free, and partial-channel paradigms directly searched on ImageNet. 1.

Introduction
Differentiable architecture search (DARTS) [27] has demonstrated both higher search efficiency and better search efficacy than early pioneering neural architecture search (NAS) [1,50,51] attempts in the image classification task. In the past few years, many following works further
*Equal contributions.This work is done during Yonggang Li’s in-ternship at MEGVII Technology. This work is supported by Science and Technology Innovation 2030-New Generation Artificial Intelligence (2020AAA0104401). improve DARTS by introducing additional modules, such as Gumbel-softmax [13], early stop criterion [23], auxiliary skip-connection [10], etc. We have witnessed tremendous improvements in the image recognition task, but it is getting farther away from exploring how DARTS works. Newly, in this work, we intend to demystify DARTS by disassembling key modules rather than make it more complex.
We overview the vanilla DARTS paradigm, and sum-mary three key modules, namely dataset, evaluation met-ric, and supernet as follows:
• Dataset. DARTS [27] searches on proxy dataset and then transfers to target dataset due to huge requirements for GPU memory.
PC-DARTS [36] proves that proxy datasets inhibit the effectiveness of
DARTS and directly searching on target dataset ob-tains more promising architectures. UnNAS [25] and
RLNAS [46] ablate the role of labels in DARTS, and further conclude that ground truth labels are not neces-sary for DARTS.
• Evaluation metric. DARTS [27] introduces architec-ture parameters to reflect the strengths of the candidate operations. PT-DARTS [32] suspects the effectiveness of architecture parameters and shows that the magni-tude of architecture parameters does not necessarily indicate how much the operation contributes to the su-pernet’s performance. FreeNAS [45] and TE-NAS [7] further put forward training-free evaluation metrics to predict the performance of candidate architectures.
• Supernet. DARTS encodes all candidate architectures in search space into the supernet. The search cell of supernet will change as the search space changes. R-DARTS [41] proposes four challenging search spaces
S1-S4 where DARTS obtains inferior performance than the Random-search baseline. R-DARTS attributes the failure of vanilla DARTS to the dominant skip-connections. Thus R-DARTS concludes that the topol-ogy of supernet has great influence on the efficacy of DARTS. P-DARTS [9] finds that the depth gap
(a) Results on CIFAR-10 (b) Results on CIFAR-100 (c) Results on ImageNet16-120
Figure 1. The correlation between the DARTS supernet performance (blue histograms) and the searched architecture performance (orange histograms) in NAS-Bench-201 [14] search space (best viewed in color). We directly searched on target datasets CIFAR-10, CIFAR-100, and ImageNet16-120 in three runs and plot average results. Histograms on three datasets reflect a consistent phenomenon. Supernet performance and the expressive power are positively correlated. However, supernet with higher performance can not search for better architectures, and supernet optimized with only BN has the best search effect. between search and evaluation architecture prevents
DARTS from achieving better search results.
Option (state)
A (unexplored)
B (explored)
C (unexplored)
D (unexplored)
Learnable modules in supernet
Convolution (cid:33) (cid:33) (cid:37) (cid:37)
BN Affine (cid:33) (cid:37) (cid:33) (cid:37)
Expressive
Power strong (cid:119) (cid:119) (cid:119) (cid:119) (cid:127) weak
Table 1. Enumerate four combinations of learnable modules in supernet. We keep the composition of supernet unchanged and just change the optimizable weights. (cid:33)means updating weights with the optimizer. (cid:37)means freezing weights at the initialization.
In this paper, we take a further step to investigate the su-pernet in DARTS from two respects. First, we ablate the expressive power of supernet in DARTS. Specifically, each convolution operation (like separable convolutions) consists of two learnable modules: convolution layers and Batch-Norm (BN) [21] layers. Each convolution layer is followed by a private BN layer. To study the expressive power in iso-lation, we thoroughly traverse four combinations (named A,
B, C, and D for simple) of learnable modules as shown in
Tab. 1. Existing DARTS variants [9, 27, 36] adopt option
B by default, which disables BN affine weights and only trains convolution weights during the supernet training. On the contrary, option A, C, and D are still unexplored, thus it is a mystery what will happen when supernet is equipped with different expressive power, that is, trained with option of A, C, and D. Hence, we make a sanity check between su-pernet performance and searched architecture performance across the above four combinations. As shown in Fig. 1, the relative ranking of supernet performance is A≈B>C>D, which is consistent with the ranking of supernet’s expres-sive power. However, the relative ranking of searched archi-tecture performance is C≫D>A≈B, which sounds count-intuitive. This result implies that the performance of su-pernet is not such significant and scaling random fea-tures with BN affine weights is good enough for archi-tecture search. Therefore, we propose a new extension of
DARTS with random features driven by the surprising re-sults of only training BN. Second, we explore the work-ing mechanism of random features by considering skip-connection roles in DARTS. Skip-connection in DARTS plays two roles [10]: 1) as a shortcut to help the optimization of supernet, and 2) a candidate operation for architecture search. Random features dilute the role of auxiliary con-nection that skip-connection plays in supernet training and enable DARTS to focus on fairer operation selection, thus implicitly solving performance collapse of DARTS [10,41].
Based on the versatility of supernet optimization, we arm popular DARTS [27] and PC-DARTS [36] with ran-dom features to build more effective algorithms RF-DARTS and RF-PCDARTS. On CIFAR-10, RF-DARTS obtains 94.36% test accuracy that is the nearest optimal re-sults (94.37%) in NAS-Bench-201. RF-DARTS achieves the state-of-the-art 24.0% top-1 test error on ImageNet when transferred from CIFAR-10 in DARTS search space.
RF-DARTS also performs robustly on CIFAR-10, CIFAR-100, and SVHN across S1-S4. RF-PCDARTS directly searches on ImageNet and achieves 23.9% top-1 test er-ror, which surpasses representative methods from single-path, training-free, and partial channel paradigms. Overall, comprehensive results reveal that the expressive power of
DARTS supernet is over-powerful, and random features is just perfect for DARTS. We hope these essential analyses and results will inspire new understandings for NAS.
2.