Abstract
Self-supervised monocular depth estimation that does not require ground truth for training has attracted attention in recent years. It is of high interest to design lightweight but effective models so that they can be deployed on edge devices. Many existing architectures beneﬁt from using heavier backbones at the expense of model sizes. This paper achieves comparable results with a lightweight architecture.
Speciﬁcally, the efﬁcient combination of CNNs and Trans-formers is investigated, and a hybrid architecture called
Lite-Mono is presented. A Consecutive Dilated Convolu-tions (CDC) module and a Local-Global Features Interac-tion (LGFI) module are proposed. The former is used to extract rich multi-scale local features, and the latter takes advantage of the self-attention mechanism to encode long-range global information into the features. Experiments demonstrate that Lite-Mono outperforms Monodepth2 by a large margin in accuracy, with about 80% fewer train-able parameters. Our codes and models are available at https://github.com/noahzn/Lite-Mono. 1.

Introduction
Many applications in the ﬁeld of robotics, autonomous driving, and augmented reality rely on depth maps, which represent the 3D geometry of a scene. Since depth sen-sors increase costs, research on inferring depth maps us-ing Convolutional Neural Networks (CNNs) from images emerged. With the annotated depth one can train a regres-sion CNN to predict the depth value of each pixel on a sin-gle image [10, 11, 22]. Lacking large-scale accurate dense ground-truth depth for supervised learning, self-supervised methods that seek supervisory signals from stereo-pairs of frames or monocular videos are favorable and have made great progress in recent years. These methods regard the depth estimation task as a novel view synthesis problem and minimize an image reconstruction loss [5,14,15,41,45].
*Corresponding Author
Figure 1. The proposed Lite-Mono has fewer parameters than
Monodepth2 [15] and R-MSFM [46], but generates more accu-rate depth maps.
The camera motion is known when using stereo-pairs of im-ages, so a single depth estimation network is adopted to pre-dict depth. But if only using monocular videos for training an additional pose network is needed to estimate the motion of the camera. Despite this, self-supervised methods that only require monocular videos are preferred, as collecting stereo data needs complicated conﬁgurations and data pro-cessing. Therefore, this paper also focuses on monocular video training.
In addition to increasing the accuracy of monocular training by introducing improved loss functions [15] and semantic information [5, 21] to mitigate the occlusion and moving objects problems, many works focused on design-ing more effective CNN architectures [17, 33, 39, 41, 46].
However, the convolution operation in CNNs has a local receptive ﬁeld, which cannot capture long-range global in-formation. To achieve better results a CNN-based model can use a deeper backbone or a more complicated archi-tecture [15, 28, 44], which also results in a larger model size. The recently introduced Vision Transformer (ViT) [8] is able to model global contexts, and some recent works ap-ply it to monocular depth estimation architectures [3, 35] to obtain better results. However, the expensive calculation of the Multi-Head Self-Attention (MHSA) module in a Trans-former hinders the design of lightweight and fast inference models, compared with CNN models [35].
This paper pursues a lightweight and efﬁcient self-supervised monocular depth estimation model with a hy-brid CNN and Transformer architecture. In each stage of the proposed encoder a Consecutive Dilated Convolutions (CDC) module is adopted to capture enhanced multi-scale local features. Then, a Local-Global Features Interaction (LGFI) module is used to calculate the MHSA and encode
global contexts into the features. To reduce the computa-tional complexity the cross-covariance attention [1] is cal-culated in the channel dimension instead of the spatial di-mension. The contributions of this paper can be summa-rized in three aspects.
• A new lightweight architecture, dubbed Lite-Mono, for self-supervised monocular depth estimation, is pro-posed. Its effectiveness with regard to the model size and FLOPs is demonstrated.
• The proposed architecture shows superior accuracy on the KITTI [13] dataset compared with competitive larger models. It achieves state-of-the-art with the least trainable parameters. The model’s generalization abil-ity is further validated on the Make3D [32] dataset.
Additional ablation experiments are conducted to ver-ify the effectiveness of different design choices.
• The inference time of the proposed method is tested on an NVIDIA TITAN Xp and a Jetson Xavier platform, which demonstrates its good trade-off between model complexity and inference speed.
The remainder of the paper is organized as follows. Sec-tion 2 reviews some related research work. Section 3 illus-trates the proposed method in detail. Section 4 elaborates on the experimental results and discussion. Section 5 con-cludes the paper. 2.