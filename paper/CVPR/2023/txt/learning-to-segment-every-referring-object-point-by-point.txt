Abstract
Referring Expression Segmentation (RES) can facili-tate pixel-level semantic alignment between vision and lan-guage. Most of the existing RES approaches require mas-sive pixel-level annotations, which are expensive and ex-haustive. In this paper, we propose a new partially super-vised training paradigm for RES, i.e., training using abun-dant referring bounding boxes and only a few (e.g., 1%) pixel-level referring masks. To maximize the transferabil-ity from the REC model, we construct our model based on the point-based sequence prediction model. We propose the co-content teacher-forcing to make the model explicitly associate the point coordinates (scale values) with the re-ferred spatial features, which alleviates the exposure bias caused by the limited segmentation masks. To make the most of referring bounding box annotations, we further pro-pose the resampling pseudo points strategy to select more accurate pseudo-points as supervision. Extensive experi-ments show that our model achieves 52.06% in terms of ac-curacy (versus 58.93% in fully supervised setting) on Re-fCOCO+@testA, when only using 1% of the mask anno-tations. Code is available at https://github.com/ qumengxue/Partial-RES.git. 1.

Introduction
Referring Expression Segmentation (RES) aims to gen-erate a segmentation mask for the object referred to by the language expression in the image. It allows pixel-level se-mantic alignment between language and vision, which is meaningful to many multi-modal tasks and can be applied to various practical applications, e.g., video/image editing with sentences. Benefiting from the development of deep learning techniques, significant progress [31, 9, 12, 2, 19,
*Work done during an internship at JD Explore Academy.
†Yao Zhao is the corresponding author.
Figure 1. Comparison of MDETR and SeqTR. On the left is the streamlined framework of the two, and on the right is their IoU performance on RefCOCOg@val. Best viewed in color. 20, 27, 54, 9] has been made in this field and achieved re-markable performance.
The success of current methods partially attributes to the large-scale training dataset with accurate pixel-level masks.
However, labeling masks for a huge amount of images is often costly in terms of both human effort and finance, and thus hardly be scaled up. Therefore, it is meaningful to explore a new training paradigm for RES where extensive pixel-level annotations are not necessary. It is well known that bounding box annotations are much cheaper and easier to be collected compared with pixel-level masks. Thus, we try to study this question: is it possible to learn an accurate
RES model using abundant box annotations and only a few mask annotations?
To answer this question, for the first time, we ex-plore a new partially supervised training paradigm for RES (Partial-RES). Intuitively, a naive pipeline for this partially supervised setting is to first train a Referring Expression
Comprehension (REC) model and then transfer it to the
RES task by fine-tuning on a limited number of data with mask annotations. However, due to the difference between
the REC and RES tasks, prevalent models (e.g., Mask R-CNN [14], MDETR [24]) have to simultaneously optimize two different heads, with one for detection prediction and the other for segmentation prediction. Such kind of struc-tures would lead to a severe issue during the fine-tuning stage, i.e., a randomly initialized mask head can not get well optimized given only a few mask annotations (hun-dreds of images). As shown in Figure 1, it is not easy to optimize MDETR with only 1% mask-annotated data even transferred from a well-trained REC model.
Recently, a contour-based method named SeqTR has been proposed for unifying REC and RES. The idea is to use a sequence model (usually a Transformer decoder) to sequentially generate contour points of the referred object.
The predictions are two points (top-left and bottom-right corner points of the bounding box) in the REC task while dozens of points (contour) are in the RES task. As shown in Fig. 1, both boxes and masks are converted to point se-quences and optimized using the same simple cross-entropy loss in SeqTR, which ensures the consistency between REC and RES. It incorporates two optimization heads into a uni-fied one, making the knowledge learned with four points (i.e., detection) be naturally transferred to predict multiple ones (i.e., segmentation). Inspired by SeqTR, we have the conjecture that sequence prediction might be a better solu-tion for partially supervised training.
In this paper, we investigate methods for achieving bet-ter partially supervised training for RES sequence predic-tion models. Sequence-to-sequence prediction models are typically trained with Teacher-Forcing (TF) [39], wherein the model utilizes the ground truth token as input to pre-dict the next token during the training stage. However, the model can only predict the next state by taking its previous output as input in inference, which is known as the expo-sure bias [34]. This issue is even more severe in the par-tially supervised setting due to the very limited ground truth data. Consequently, we introduce the Co-Content Teacher-Forcing (CCTF), which combines the ground truth point coordinates together with the spatial visual feature of the pointed row or column. In contrast to the previous sequence model SeqTR, our CCTF explicitly associates the point co-ordinates (scale values) with the referred spatial region, pro-viding a more natural approach for visual grounding.
Furthermore, we estimate the referring region via our proposed Point-Modulated Cross-Attention, to ensure the decoder attends to those region content while generating the point contour sequences. To fully utilize the data without mask annotation in Partial-RES, we retrain the model with the generated pseudo labels, and we present a Resampling
Pseudo Points (RPP) Strategy. Unlike most pseudo-label works that directly use the network’s predictions as labels, we select the appropriate pseudo masks with Dice coeffi-cient and then resample the predicted points in a uniform way to regularize the contour sequence labels.
With extensive experiments, our method displays signif-icant improvement compared to SeqTR [54] baselines on all three benchmarks, i.e., at an average of 3.5%, 2.4%, and 3.0% on 1%, 5% and 10% mask-labeled data. With 10% mask annotated data, our method achieves 97% of the fully supervised performance on RefCOCO+@val. We are also able to achieve 88% of the fully supervised performance only with 1% mask-labeled data on RefCOCO+@testA. 2.