Abstract
Inferring a meaningful geometric scene representation from a single image is a fundamental problem in computer vision. Approaches based on traditional depth map predic-tion can only reason about areas that are visible in the im-age. Currently, neural radiance fields (NeRFs) can capture true 3D including color, but are too complex to be gener-ated from a single image. As an alternative, we propose to predict an implicit density field from a single image. It maps every location in the frustum of the image to volumetric den-sity. By directly sampling color from the available views instead of storing color in the density field, our scene rep-resentation becomes significantly less complex compared to
NeRFs, and a neural network can predict it in a single for-ward pass. The network is trained through self-supervision from only video data. Our formulation allows volume ren-dering to perform both depth prediction and novel view syn-thesis. Through experiments, we show that our method is able to predict meaningful geometry for regions that are oc-cluded in the input image. Additionally, we demonstrate the potential of our approach on three datasets for depth pre-diction and novel-view synthesis. 1.

Introduction
The ability to infer information about the geometric structure of a scene from a single image is of high impor-tance for a wide range of applications from robotics to aug-mented reality. While traditional computer vision mainly focused on reconstruction from multiple images, in the deep learning age the challenge of inferring a 3D scene from merely a single image has received renewed attention.
Traditionally, this problem has been formulated as the task of predicting per-pixel depth values (i.e. depth maps).
One of the most influential lines of work showed that it is possible to train neural networks for accurate single-image depth prediction in a self-supervised way only from video sequences. [14â€“16, 29, 44, 51, 58, 59, 61] Despite these ad-vances, depth prediction methods are not modeling the true 3D of the scene: they model only a single depth value per pixel. As a result, it is not directly possible to obtain depth values from views other than the input view without con-sidering interpolation and occlusion. Further, the predicted geometric representation of the scenes does not allow rea-soning about areas that lie behind another object in the im-age (e.g. a house behind a tree), inhibiting the applicability of monocular depth estimation to 3D understanding.
Due to the recent advance of 3D neural fields, the related task of novel view synthesis has also seen a lot of progress.
Instead of directly reasoning about the scene geometry, the goal here is to infer a representation that allows rendering views of the scene from novel viewpoints. While geomet-ric properties can often be inferred from the representation, they are usually only a side product and lack visual quality.
Even though neural radiance field [32] based methods achieve impressive results, they require many training im-ages per scene and do not generalize to new scenes. To en-able generalization, efforts have been made to condition the neural network on global or local scene features. However, this has only been shown to work well on simple scenes, for example, scenes containing an object from a single cat-egory [43, 57]. Nevertheless, obtaining a neural radiance field from a single image has not been achieved before.
In this work, we tackle the problem of inferring a ge-ometric representation from a single image by generaliz-ing the depth prediction formulation to a continuous den-sity field. Concretely, our architecture contains an encoder-decoder network that predicts a dense feature map from the input image. This feature map locally conditions a density field inside the camera frustum, which can be evaluated at any spatial point through a multi-layer perceptron (MLP).
The MLP is fed with the coordinates of the point and the feature sampled from the predicted feature map by repro-jecting points into the camera view. To train our method, we rely on simple image reconstruction losses.
Our method achieves robust generalization and accu-rate geometry prediction even in very challenging outdoor scenes through three key novelties: 1. Color sampling. When performing volume render-ing, we sample color values directly from the input frames through reprojection instead of using the MLP to predict color values. We find that only predicting density drasti-cally reduces the complexity of the function the network has to learn. Further, it forces the model to adhere to the multi-view consistency assumption during training, leading to more accurate geometry predictions. 2. Shifting capacity to the feature extractor. In many previous works, an encoder extracts image features to con-dition local appearance, while a high-capacity MLP is ex-pected to generalize to multiple scenes. However, on com-plex and diverse datasets, the training signal is too noisy for the MLP to learn meaningful priors. To enable robust train-ing, we significantly reduce the capacity of the MLP and use a more powerful encoder-decoder that can capture the entire scene in the extracted features. The MLP then only evaluates those features locally. 3. Behind the Scenes loss formulation. The continuous nature of density fields and color sampling allow us to re-construct a novel view from the colors of any frame, not just the input frame. By applying a reconstruction loss between two frames that both observe areas occluded in the input frame, we train our model to predict meaningful geometry everywhere in the camera frustum, not just the visible areas.
We demonstrate the potential of our new approach in a number of experiments on different datasets regarding the aspects of capturing true 3D, depth estimation, and novel view synthesis. On KITTI [12] and KITTI-360
[26], we show both qualitatively and quantitatively that our model can indeed capture true 3D, and that our model achieves state-of-the-art depth estimation accuracy. On
RealEstate10K [45] and KITTI, we achieve competitive novel view synthesis results, even though our method is purely geometry-based. Further, we perform thorough ab-lation studies to highlight the impact of our design choices. 2.