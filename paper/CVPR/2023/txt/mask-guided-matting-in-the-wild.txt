Abstract
Mask-guided matting has shown great practicality com-pared to traditional trimap-based methods. The mask-guided approach takes an easily-obtainable coarse mask as guidance and produces an accurate alpha matte. To ex-tend the success toward practical usage, we tackle mask-guided matting in the wild, which covers a wide range of categories in their complex context robustly. To this end, we propose a simple yet effective learning framework based on two core insights: 1) learning a generalized matting model that can better understand the given mask guidance and 2) leveraging weak supervision datasets (e.g., instance segmentation dataset) to alleviate the limited diversity and scale of existing matting datasets. Extensive experimen-tal results on multiple benchmarks, consisting of a newly proposed synthetic benchmark (Composition-Wild) and ex-isting natural datasets, demonstrate the superiority of the proposed method. Moreover, we provide appealing results on new practical applications (e.g., panoptic matting and mask-guided video matting), showing the great generality and potential of our model. 1.

Introduction
Image matting aims to predict the opacity of ob-jects, which enables precise separation from surround-ing backgrounds. Due to the ill-posed nature of the task, many works [7, 13, 21, 27, 30, 48] have improved matting performance by relying on the manual guidance of a trimap. However, pixel-level annotation of fore-ground/background/unknown is extremely burdensome, re-stricting its usage in many practical applications such as im-age/video editing and film production. Recently, many ef-ficient alternatives for user guidance have been proposed, including trimap-free [15, 32], additional background im-ages [22, 34], scribble [43], and the user clicks [45].
Among them, the mask-guided approach [50] shows a great trade-off between performance and intensity of user interaction. It utilizes a coarse mask as guidance, which is much easier to obtain either manually or from off-the-shelf
Figure 1. Qualitative Comparisons of MGMatting [50] and
Ours in the wild. The mask guidance is overlaid on images with blue color. Best viewed zoomed in. segmentation models [2, 10]. With only the coarse spatial prior, the mask-guided matting model [50] shows compara-ble or even better performance than the trimap-based com-petitors [13,17,21,27,48] on synthetic Composition-1k [48] and a real-world human matting dataset [50]. However, de-spite the encouraging results, we see the previous state-of-the-art model [50] struggles to obtain desirable alpha matte in complex real-world scenes (see Fig. 1).
With this observation, we tackle mask-guided matting in the wild. Specifically, we formulate unique setups and emerging challenges of the new task as follows: (1) We aim to handle objects in their complex context, reflecting the characteristics of natural images. The previous method [50] evaluates their model on iconic-object images [1, 29] where only a single object is in the center. As the model can easily find the target object in such images, the model’s real instance discrimination ability is, in fact, veiled. On the contrary, in an ‘in-the-wild’ setting, it is crucial to pre-cisely localize the target object from the given coarse/noisy mask guidance (i.e. mask awareness). (2) Our model tar-gets to deal with diverse categories of objects in natural
images. Unlike most previous methods that improve gen-eralization performance at the expense of category-specific regime (e.g., limiting to humans [15] or animals [19]), we aim to understand distinctive matting patterns of vast cat-egories. (3) Limited data problem makes the new setting more complicated. Due to the labeling complexity, anno-tating alpha matte for objects in common scenes, e.g., the
COCO dataset [24], is infeasible. As a sidestep, previ-ous benchmarks [32, 48] extract the alpha matte and fore-ground colors from images with simple backgrounds. These are composited on various backgrounds [6, 24], and result-ing samples are used to train and evaluate matting models.
However, due to the inevitable composition artifacts, the models usually show limited generalization performance.
In that sense, how to train and evaluate the in-the-wild mat-ting model remains an open question.
Toward this goal, we propose a simple yet effective learning framework for a generalized mask-guided matting model. First, we investigate fundamental reasons for the poor generalization of the previous mask-guided matting model [50] and find that this is mainly from the training data generation process. Specifically, the previous compo-sition process includes instance merging data augmentation, which merges several foreground objects into a single ob-ject. While this augmentation is effective in the trimap-based methods [21, 30, 41], it implicitly makes a negative bias for the mask-guided matting model to ignore the guid-ance. Thus, the model struggles to localize the target objects in complex natural scenes. We alleviate the bias by propos-ing an instance-wise learning objective, where the model is supervised to segment one of multiple instances according to the guidance. By doing so, the model learns strong se-mantic representation regarding complex relations and soft transitions between objects. Despite the simplicity of the proposal, this greatly improves performance in the wild.
Second, we explore a practical solution to make the mask-guided model handle various categories of objects ro-bustly. Instead of scaling the matting dataset, we leverage a dataset with weak supervision [14, 46] (i.e., instance seg-mentation dataset [24]), as the coarse instance masks are easier to obtain over the diverse categories of objects. To effectively hallucinate the fine supervision signal with the weak localization guidance, we come up with a self-training framework [36, 37, 47]. Specifically, a pseudo label is gen-erated based on a weakly-augmented input (both image and instance mask annotation as guidance), which supervises the model prediction on a strongly augmented version of them. During self-training, the model is not only adapting to the in-the-wild scenario in a self-evolving manner but also being robust to noise in both image and guidance.
To verify the in-the-wild performance of mask-guided matting, we formally define an evaluation protocol involv-ing multiple sub-benchmarks: Composition-Wild, AIM-500 [20], COCO [24]. We first design an in-the-wild ex-tension of the popular synthetic Composition-1k bench-mark [48], namely Composition-Wild. We simulate com-plex real-world images by compositing multiple foreground objects. To bring valuable insight on the failure cases of the model, we design sub-metrics for Composition-Wild.
In addition, we use the AIM-500 dataset to establish quan-titative results on natural images (i.e., with no composition artifacts), although most images are iconic-object images with simple backgrounds. Finally, we provide qualitative outputs of our mask-guided matting model on the COCO dataset [24] which is one of the most representative in-the-wild datasets.
To summarize, we make the following main contribu-tions. 1) To our best knowledge, it is the first work to explore mask-guided matting in the wild. 2) We develop a simple yet effective learning framework leveraging both composited and weak-guidance images. 3) We design an evaluation setup for the new task. 4) We initiate several in-teresting extensions: video and panoptic matting. 2.