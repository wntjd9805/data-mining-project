Abstract
RENDERING
SCENE EDITING
In this work, we present I2-SDF, a new method for in-trinsic indoor scene reconstruction and editing using dif-ferentiable Monte Carlo raytracing on neural signed dis-tance fields (SDFs). Our holistic neural SDF-based frame-work jointly recovers the underlying shapes, incident ra-diance and materials from multi-view images. We intro-duce a novel bubble loss for fine-grained small objects and error-guided adaptive sampling scheme to largely improve the reconstruction quality on large-scale indoor scenes.
Further, we propose to decompose the neural radiance field into spatially-varying material of the scene as a neu-ral field through surface-based, differentiable Monte Carlo raytracing and emitter semantic segmentations, which en-ables physically based and photorealistic scene relighting and editing applications. Through a number of qualita-tive and quantitative experiments, we demonstrate the su-perior quality of our method on indoor scene reconstruc-tion, novel view synthesis, and scene editing compared to state-of-the-art baselines. Our project page is at https:
//jingsenzhu.github.io/i2-sdf. 1.

Introduction
Reconstructing 3D scenes from multi-view images is a fundamental task in computer graphics and vision. Neu-ral Radiance Field (NeRF) [16] and its follow-up research leverage multi-layer perceptions (MLPs) as implicit func-tions, taking as input the positional and directional coordi-nates, to approximate the underlying geometry and appear-ance of a 3D scene. Such methods have shown compelling and high-fidelity results in novel view synthesis. However, we argue that novel view synthesis itself is insufficient for scene editing such as inserting virtual objects, relighting
*Corresponding author.
Figure 1. I2-SDF. Left: State-of-the-art neural implicit surface representation method [42] fails in reconstructing small objects inside an indoor scene (e.g. lamps and chandeliers), which is re-solved by our bubbling method. Middle and Right: Our intrinsic decomposition and raytracing method enable photo-realistic scene editing and relighting applications. and editing surface materials with global illumination.
On the other hand, inverse rendering or intrinsic de-composition, which reconstructs and decomposes the scene into shape, shading and surface reflectance from single or multiple images, enables photorealistic scene editing pos-sibilities. It is a long-term challenge especially for large-scale indoor scenes because they typically exhibit complex geometry and spatially-varying global illumination appear-ance. As intrinsic decomposition is an extremely ill-posed task, a physically-based shading model will crucially af-fect the decomposition quality. Existing neural rendering methods [2, 18, 43, 46] rely on simple rendering algorithms (such as pre-filtered shading) for the decomposition and use a global lighting representation (e.g., spherical Gaussians).
Although these methods have demonstrated the effective-ness on object-level inverse rendering, they are inapplicable to complex indoor scenes. Moreover, indoor scene images are usually captured from the inside out and most lighting information has already presented inside the room. As a re-sult, the reconstructed radiance field already provides suffi-cient lighting information without the need of active, exter-nal capture lighting setup.
To tackle the above challenges, we propose I2-SDF, a new method to decompose a 3D scene into its underlying
shape, material, and incident radiance components using implicit neural representations. We design a robust two-stage training scheme that first reconstructs a neural SDF with radiance field, and then conducts raytracing in the SDF to decompose the radiance field into material and emission fields. As complex indoor scenes typically contain many fine-grained, thin or small structures with high-frequency details that are difficult for an implicit SDF function to fit, we propose a novel bubble loss and an error-guided adap-tive sampling scheme that greatly improve the reconstruc-tion quality on small objects in the scene. As a result, our approach achieves higher reconstruction quality in both geometry and novel view synthesis, outperforming previ-ous state-of-the-art neural rendering methods in complex indoor scenes. Further, we present an efficient intrinsic de-composition method that decomposes the radiance field into spatially-varying material and emission fields using surface-based, differentiable Monte Carlo raytracing, enabling var-ious scene editing applications.
In summary, our contributions include:
• We introduce I2-SDF1, a holistic neural SDF-based framework for complex indoor scenes that jointly re-covers the underlying shape, radiance, and material fields from multi-view images.
• We propose a novel bubble loss and error-guided adap-tive sampling strategy to effectively reconstruct fine-grained small objects inside the scene.
• We are the first that introduce Monte Carlo raytracing technique in scene-level neural SDF to enable photo-realistic indoor scene relighting and editing.
• We provide a high-quality synthetic indoor scene multi-view dataset, with ground truth camera pose and geometry annotations. 2.