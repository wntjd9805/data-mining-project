Abstract
Most previous co-salient object detection works mainly focus on extracting co-salient cues via mining the con-sistency relations across images while ignore explicit ex-ploration of background regions.
In this paper, we pro-pose a Discriminative co-saliency and background Mining
Transformer framework (DMT) based on several econom-ical multi-grained correlation modules to explicitly mine both co-saliency and background information and effec-tively model their discrimination. Specifically, we first pro-pose a region-to-region correlation module for introduc-ing inter-image relations to pixel-wise segmentation fea-tures while maintaining computational efficiency. Then, we use two types of pre-defined tokens to mine co-saliency and background information via our proposed contrast-induced pixel-to-token correlation and co-saliency token-to-token correlation modules. We also design a token-guided fea-ture refinement module to enhance the discriminability of the segmentation features under the guidance of the learned tokens. We perform iterative mutual promotion for the seg-mentation feature extraction and token construction. Exper-imental results on three benchmark datasets demonstrate the effectiveness of our proposed method. The source code is available at: https://github.com/dragonlee258079/DMT. 1.

Introduction
Unlike standard Salient Object Detection (SOD) [13,21– 23, 37, 45], which detects salient objects in a single im-age, Co-Salient Object Detection (CoSOD) aims to detect common salient objects across a group of relevant images.
It often faces the following two challenges: 1) The fore-ground (FG) in CoSOD refers to co-salient objects, which are inherently hard to detect since they should satisfy both the intra-group commonality and the intra-image saliency. 2) The background (BG) in CoSOD might contain com-†Corresponding author: liunian228@gmail.com. plex distractions, including extraneous salient objects that are salient but not ”common”, and similar concomitant ob-jects appearing in multiple images (e.g. performers often appear in guitar images). Such difficult distractors can eas-ily seduce CoSOD models to make false positive predic-tions. Therefore, the effective exploration of both FG and
BG and modeling their discrimination to precisely detect co-salient objects while suppressing interference from BG is crucial for CoSOD.
Although many works have achieved promising perfor-mance, most of them [14, 18, 25, 30, 33, 38, 40–42] devoted to the ingenious mining of FG while ignored the explicit exploration of BG. They mainly constructed positive rela-tions between co-salient objects but paid less attention to modeling negative ones between co-saliency regions and
BG. [16, 34] followed some SOD methods [4, 43] to in-corporate BG features for co-saliency representation learn-ing or contrast learning. However, these methods can be regarded as an univariate FG&BG modeling in which the essential optimization target is limited to FG and there is no explicit BG modeling, thus limiting the discriminative learning capability. To this end, this paper propose to con-duct a bivariate FG&BG modeling paradigm that explicitly models both FG and BG information and effectively facili-tates their discriminative modeling.
As for co-saliency (FG) information, most previous works [12, 16, 34, 41] detected them by exploring the inter-image similarity. Calculating the Pixel-to-Pixel (P2P) cor-relation between 3D CNN features in a group is widely used in many works [12, 16, 34] and has demonstrated its effec-tiveness. However, this method introduces heavy computa-tion burdens and hinders sophisticated relation modeling.
To alleviate this problem, we introduce economic multi-grained correlations among different images and the co-saliency and BG information, thus enabling modeling so-phisticated relations to extract accurate co-saliency as well as BG knowledge.
Specifically, we construct a Discriminative co-saliency following the and BG Mining Transformer (DMT)
paradigm of a semantic segmentation transformer archi-tecture, i.e. MaskFormer [5], which enables explicit co-saliency and BG modeling and the construction of multi-grained correlations. Using this architecture, we decom-pose the CoSOD modeling into two sub-paths, i.e. gener-ating pixel-wise segmentation feature maps and extracting category information with pre-defined co-saliency and BG detection tokens.
In the first sub-path, to efficiently and thoroughly mine the common cues within the image group, we propose a
Region-to-Region correlation (R2R) module to model the inter-image relation and plug it into each decoder layer. In the second sub-path, we transform the pixel-wise features into a co-saliency token and a BG token for each image, ab-stracting pixel-wise cues into high-level tokens. As such, we achieve sophisticated relation modeling among the to-kens and features while largely reducing the computational costs. Concretely, we propose an intra-image Contrast-induced Pixel-to-Token correlation (CtP2T) module to ex-tract the two tokens by considering the contrast relation between co-saliency and BG. Since the co-saliency tokens from CtP2T are separately learned on each image, we fur-ther design a Co-saliency Token-to-Token (CoT2T) correla-tion module to model their common relation.
After obtaining the tokens and pixel-wise features, the
MaskFormer [5] architecture adopts dot production be-tween them to obtain the final segmentation results. How-ever, such a scheme only achieves unidirectional informa-tion propagation, i.e. conveying information from the fea-ture maps to the tokens. We argue that the learned two tokens can also be used to improve the discriminability of the pixel-wise features, thus proposing our Token-Guided
Feature Refinement (TGFR) module as a reverse informa-tion propagation path. Concretely, we first use the tokens as guidance to distill co-saliency and BG features from the pixel-wise feature maps, and then enhance the discrim-inability of the segmentation features between the two de-tection regions.
In this way, the refined features become sensitive to both co-saliency and BG, reducing the affect of ambiguous distractors.
Finally, as shown in Figure 1, our DMT iteratively de-ploys CtP2T and CoT2T to leverage the segmentation fea-tures for updating the tokens, and then adopts TGFR to re-fine the corresponding decoder feature with the updated to-kens. As a result, the learning processes can be effectively promoted, thus obtaining more accurate CoSOD results.
In summary, our major contributions are as follows:
• We model CoSOD from the perspective of explicitly exploring both co-saliency and BG information and ef-fectively modeling their discrimination.
• We introduce several computationally economical multi-grained correlation modules, i.e. R2R, CtP2T,
CoT2T, for the inter-image and intra-image relations modeling.
• We propose a novel TGFR module to use the learned tokens as guidance to refine the segmentation fea-tures for enhancing their discriminability between co-saliency and BG regions.
• Experimental results demonstrate that our DMT model outperforms previous state-of-the-art results on three benchmark datasets. 2.