Abstract
Nerf-based Generative models have shown impressive capacity in generating high-quality images with consistent 3D geometry. Despite successful synthesis of fake identity images randomly sampled from latent space, adopting these models for generating face images of real subjects is still a challenging task due to its so-called inversion issue. In this paper, we propose a universal method to surgically fine-tune these NeRF-GAN models in order to achieve high-fidelity animation of real subjects only by a single image. Given the optimized latent code for an out-of-domain real image, we employ 2D loss functions on the rendered image to re-duce the identity gap. Furthermore, our method leverages explicit and implicit 3D regularizations using the in-domain neighborhood samples around the optimized latent code to remove geometrical and visual artifacts. Our experiments confirm the effectiveness of our method in realistic, high-fidelity, and 3D consistent animation of real faces on multi-ple NeRF-GAN models across different datasets. 1.

Introduction
Animating a human with a novel view and expression sequence from a single image opens the door to a wide range of creative applications, such as talking head synthe-sis [22, 34], augmented and virtual reality (AR/VR) [19], image manipulation [24, 32, 45], as well as data augmenta-tion for training of deep models [25, 42, 43]. Early works of image animation mostly employed either 2D-based image generation models [14, 26, 31, 37], or 3D parametric mod-els [4, 11, 40, 41] (e.g. 3DMM [6]), but they mostly suffer from artifacts, 3D inconsistencies or unrealistic visuals.
Representing scenes as Neural Radiance Fields (NeRF) [23] has recently emerged as a breakthrough
Project page:
NeRFInvertor_Homepage/ https : / / yuyin1 . github . io /
Figure 1. Image animation results of our method. NeRFInver-tor achieves 3D-consistent and ID-preserving animation (i.e. novel views and expressions) of real subjects given only a single image. approach for generating high-quality images of a scene in novel views. However, the original NeRF models [5,33,44] only synthesize images of a static scene and require exten-sive multi-view data for training, restricting its application to novel view synthesis from a single image. Several studies have shown more recent advances in NeRFs by extending it to generate multi-view face images with single-shot data even with controllable expressions [7, 8, 12, 27, 38, 46].
These Nerf-based Generative models (NeRF-GANs) are able to embed attributes of training samples into their latent
variables, and synthesize new identity face images with different expressions and poses by sampling from their latent space.
While animatable synthesis of fake identity images is im-pressive, it is still challenging to generate 3D-consistent and identity-preserving images of real faces. Specifically, cur-rent Nerf-GANs have difficulties to accurately translate out-of-domain images into their latent space, and consequently change identity attributes and/or introduce artifacts when applied to most real-world images. In order to synthesize real faces, the conventional method applies optimization algorithms to invert the input image to a latent code in a smaller (i.e. W) or an extended (i.e. W+) NeRF-GAN la-tent space. However, they both either have ID-preserving or artifacts issues as shown in Figure 2. The W space inver-sion, in particular, generates realistic novel views and clean 3D geometries, but suffers from the identity gap between the real and synthesized images. In contrast, the W+ space inversion well preserves the identity but commonly gener-ates an inaccurate 3D geometry, resulting in visual artifacts when exhibited from new viewpoints. Hence, it remains as a trade-off to have a 3D-consistent geometry or preserve identity attributes when inverting face images out of latent space distribution.
In this paper, we present NeRFInvertor as a universal inversion method for NeRF-GAN models to achieve high-fidelity, 3D-consistent, and identity-preserving animation of real subjects given only a single image. Our method is ap-plicable to most of NeRF-GANs trained for a static or dy-namic scenes, and hence accomplishes synthesis of real im-ages with both novel views and novel expressions (see Fig-ure 1). Since the real images are mostly out of the domain of NeRF-GANs latent space, we surgically fine-tune their generator to enrich the latent space by leveraging the single input image without degrading the learned geometries.
In particular, given an optimized latent code for the in-put image, we first use image space supervision to narrow the identity gap between the synthesized and input images.
Without a doubt, the fine-tuned model can be overfitted on the input image and well reconstruct the input in the origi-nal view. However, fine-tuning with just image space super-vision produces erroneous 3D geometry due to the insuffi-cient geometry and content information in a single image, resulting in visual artifacts in novel views. To overcome this issue, we introduce regularizations using the surround-ing samples in the latent space, providing crucial guidance for the unobserved part in the image space. By sampling la-tent codes from the neighborhood of optimized latent vari-ables with different poses and expressions, we enforce a novel geometric constraint on the density outputs of fine-tuned and original pretrained generators. We also further add regularizations on the rendered images of neighborhood samples obtained from the fine-tuned and pretrained genera-Figure 2. Trade-off between ID-preserving and removing ar-tifacts. Optimizing latent variables of Nerf-GANs for synthesis of a real face leads to a trade-off between identity-preserving and geometrical and visual artifacts. Specifically, W space inversion results in clean geometry but identity gap between real and gener-ated images, and W+ space inversion causes preserving of iden-tity attributes but inaccurate geometry and visual artifacts. tors. These regularizations help us to leverage the geometry and content information of those in-domain neighborhood samples around the input. Our experiments validate the ef-fectiveness of our method in realistic, high-fidelity, and 3D consistent animating of real face images.
The main contributions of this paper are as follows: 1. We proposed a universal method for inverting NeRF-GANs to achieve 3D-consistent, high-fidelity, and identity-preserving animation of real subjects given only a single image. 2. We introduce a novel geometric constraint by leverag-ing density outputs of in-domain samples around the input to provide crucial guidance for the unobserved part in the 2D space. 3. We demonstrate the effusiveness of our method on multiple NeRF-GAN models across different datasets. 2.