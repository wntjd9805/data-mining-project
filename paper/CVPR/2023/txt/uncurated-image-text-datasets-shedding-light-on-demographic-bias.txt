Abstract
The increasing tendency to collect large and uncurated datasets to train vision-and-language models has raised concerns about fair representations. It is known that even small but manually annotated datasets, such as MSCOCO, are affected by societal bias. This problem, far from being solved, may be getting worse with data crawled from the In-ternet without much control. In addition, the lack of tools to analyze societal bias in big collections of images makes addressing the problem extremely challenging.
Our first contribution is to annotate part of the Google
Conceptual Captions dataset, widely used for training vision-and-language models, with four demographic and two contextual attributes. Our second contribution is to conduct a comprehensive analysis of the annotations, focus-ing on how different demographic groups are represented.
Our last contribution lies in evaluating three prevailing image captioning, text-image vision-and-language tasks:
CLIP embeddings, and text-to-image generation, showing that societal bias is a persistent problem in all of them. https://github.com/noagarcia/phase 1.

Introduction
The training paradigm in vision-and-language models has shifted from manually annotated collections, such as
MS-COCO [30] and Visual Genome [27], to massive datasets with little-to-none curation automatically crawled from the Internet [17, 42, 43]. Figure 1 illustrates this ten-dency by comparing the size of paired image-text datasets over time. Whereas manually annotated datasets, widely used in the last decade, contained a few hundred thousand images each, the latest automatically crawled collections are composed of several million samples. This large amount of data has led to training some disruptive models in the field, such as CLIP [37] trained on 400 million image-text pairs; Imagen [41] trained on 860 million image-text pairs;
Flamingo [1] trained on 2.3 billion images and short videos paired with text; DALL-E 2 [38] trained on 650 million im-ages; or Stable Diffusion [39], trained on 600 million cap-Figure 1. Evolution of paired image-text datasets in terms of num-ber of samples (in million). Datasets scaled up with data auto-matically crawled from the Internet, reaching the current status in which models are trained with hundreds of millions of samples. tioned images. Those models have been shown to learn vi-sual and language representations that outperform the pre-vious state-of-the-art on tasks such as zero-shot classifica-tion [37] or text-to-image generation [38, 39].
Despite the impressive results on controlled benchmarks, a critical drawback arises: the larger the training set, the less control over the data. With toxic content easily acces-sible on the Internet, models trained under uncurated col-lections are more prone to learn harmful representations of the world, including societal bias, which results in mod-els performing differently for different sociodemographic groups [57]. The risk of obtaining unfair representations is high, as not only do models trained on biased datasets learn to reproduce bias but also amplify it by making predictions more biased than the original data [22, 53, 56]. This turns out to be harmful when, far from controlled research envi-ronments, models are used in the real-world [10].
Manually annotated datasets [16,30] have been shown to be affected by societal bias [21, 32, 60, 61], but the problem gets worse in automatically crawled datasets [8,9]. To over-come societal bias, fairness protocols must be included both in the dataset and in the model development phase. Data analysis [8, 9, 21, 32, 52, 58], evaluation metrics [22, 40, 53],
Table 1. Image-text datasets with annotations for bias detection.
Dataset
Image source
Annotation process
Labels
Images
Regions
Attributes
Zhao et al. [61]
MSCOCO [30] automatic (captions) image 33, 889
-gender
Zhao et al. [60]
MSCOCO [30] (val) crowd-sourcing region 15, 762 28, 315
PHASE
GCC [43] crowd-sourcing region 18, 889 35, 347 gender skin-tone age gender skin-tone ethnicity emotion activity and mitigation techniques [5, 11, 23, 54] are essential tools for developing fairer models, however, they require demo-graphic attributes, such as gender or skin-tone, to be avail-able. These annotations are currently scarce, and only exist for a few datasets and attributes [60, 61].
In this paper, we contribute to the analysis, evalua-tion, and mitigation of bias in vision-and-language tasks by annotating six types of demographic1 and contextual2 attributes in a large dataset: the Google Conceptual Cap-tions (GCC) [43], which was one of the first automati-cally crawled datasets with 3.3 million image-caption pairs.
We name our annotations PHASE (Perceived Human
Annotations for Social Evaluation), and we use them to conduct a comprehensive analysis of the distribution of de-mographic attributes on the GCC dataset. We complement our findings with experiments on three main vision-and-language tasks: image captioning, text-image embeddings, and text-to-image generation. Overall, we found that a dataset crawled from the internet like GCC presents big un-balances on all the demographic attributes under analysis.
Moreover, when compared against the demographic annota-tions in MSCOCO by Zhao et al. [60], GCC has bigger rep-resentation gaps in gender and skin-tone. As for the down-stream tasks, the three of them show evidence of different performance for different demographic groups. 2.