Abstract
Object-goal visual navigation aims at steering an agent toward an object via a series of moving steps. Previous works mainly focus on learning informative visual repre-sentations for navigation, but overlook the impacts of nav-igation states on the effectiveness and efficiency of nav-igation. We observe that high relevance among naviga-tion states will cause navigation inefficiency or failure for
In this paper, we present a History-existing methods. inspired Navigation Policy Learning (HiNL) framework to estimate navigation states effectively by exploring relation-ships among historical navigation states. In HiNL, we pro-pose a History-aware State Estimation (HaSE) module to alleviate the impacts of dominant historical states on the current state estimation. Meanwhile, HaSE also encour-ages an agent to be alert to the current observation changes, thus enabling the agent to make valid actions. Furthermore, we design a History-based State Regularization (HbSR) to explicitly suppress the correlation among navigation states in training. As a result, our agent can update states more ef-fectively while reducing the correlations among navigation states. Experiments on the artificial platform AI2-THOR (i.e., iTHOR and RoboTHOR) demonstrate that HiNL sig-nificantly outperforms state-of-the-art methods on both Suc-cess Rate and SPL in unseen testing environments. 1.

Introduction
Object-goal visual navigation is to direct an agent to move consecutively toward an object of a specific category.
Without knowing the environment map beforehand, at each navigation step, an agent first needs to represent its visual observations, then estimate its navigation states from the visual representations and the preceding states, and at last predict the corresponding action. Therefore, to achieve an effective and efficient navigation system, learning instruc-tive visual representations and navigation states is critical.
Prevailing visual navigation works [13, 14, 50] focus on
*Corresponding author (a) Demonstration of inefficient action predictions caused by highly-correlated navigation states. Our agent is stuck by an obstacle, i.e., low-profile sofa, and repeatedly predicts an invalid action, i.e., MoveAhead. (b) Demonstration of the correlation coefficients among navigation states trained in two manners. Navigation states estimated via LSTM are highly-relevant. In contrast, our HiNL produce low-correlated navigation states.
Figure 1. Motivation of our proposed History-inspired Navigation
Learning (HiNL) framework. extracting informative visual representations, while some methods [13, 49] adjust navigation policy during inference.
All these approaches commonly employ recurrent neural networks (e.g., LSTM) to estimate navigation states. How-ever, we observe that the navigation states of existing meth-ods [13, 14, 49] exhibit high relevance, as demonstrated in
Figure 1b, and the highly-correlated navigation states would lead to inefficient navigation policy (i.e., failure to respond to observation changes rapidly). For instance, as shown in
Figure 1a, an agent is stuck by the low-profile sofa and fails to take proper actions to circumvent the obstacle. Hence, we aim to endow an agent with the capability of updating its navigation states effectively while avoiding producing highly-correlated states.
In this work, we propose a History-inspired Navigation
Learning (HiNL) framework to obtain informative naviga-tion states by exploiting the relationships among historical navigation states. HiNL consists of two novel components: (i) a History-aware State Estimation (HaSE) module, and (ii) a History-based State Regularization (HbSR). Here, our
HaSE module is designed to generate a state that can be promptly updated according to visual observations. Specif-ically, HaSE first analyzes the correlations among historical navigation states and then eliminates the influence of dom-inant historical states on the current state estimation. As a result, an agent is able to predict navigation states which can dynamically react to the current visual observations and then make sensible navigation actions.
Furthermore, learning-based existing reinforcement object-goal navigation systems [13,14,49] often assume the navigation state transition exhibits the first-order Markov property. This would allow the emergence of high corre-lations among navigation states, leading to inferior naviga-tion policy. To address this issue, we introduce an explicit constraint on the correlation among all the states, namely
History-based State Regularization (HbSR). To be specific,
HbSR enforces to relevance (i.e., correlations) between a state and all its preceding states (except its previous state) to be low. Here, we do not constrain states of two consec-utive steps because temporally close states generally have relevance in practice considering the navigation continuity.
After training with our HbSR, the correlations among the navigation states become much lower (see Figure 1b). This pheromone also indicates HiNL effectively updates states.
Hence, our navigation system can respond to observation changes adaptively.
To demonstrate the superiority of HiNL, we conduct experiments in the widely-adopted artificial environment iTHOR [26] and RoboTHOR [11]. HiNL outperforms the state-of-the-art by a large margin. To be specific, we im-prove the Success Rate (SR) from 72.2% to 80.1% and Suc-cess weighted by Path Length (SPL) from 0.449 to 0.498 in iTHOR. Overall, our major contributions are summarized as follows:
• We propose a History-inspired Navigation Policy (HiNL) framework to effectively estimate navigation states by utilizing historical states.
• We design a History-aware State Estimation (HaSE) to eliminate dominant historical states in the current state estimation. Therefore, the agent reduces the impact of distant navigation states on the state estimation, and thus reacts dynamically to the observation changes.
• We introduce a History-based State Regularization (HbSR) to explicitly constrain the correlations among navigation states. By doing this, the agent can effec-tively update navigation states with low relevance. 2.