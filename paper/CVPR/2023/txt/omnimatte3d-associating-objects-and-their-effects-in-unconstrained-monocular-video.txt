Abstract
We propose a method to decompose a video into a back-ground and a set of foreground layers, where the background captures stationary elements while the foreground layers capture moving objects along with their associated effects (e.g. shadows and reflections). Our approach is designed for unconstrained monocular videos, with an arbitrary camera and object motion. Prior work that tackles this problem assumes that the video can be mapped onto a fixed 2D can-vas, severely limiting the possible space of camera motion.
Instead, our method applies recent progress in monocular camera pose and depth estimation to create a full, RGBD video layer for the background, along with a video layer for each foreground object. To solve the underconstrained decomposition problem, we propose a new loss formulation based on multi-view consistency. We test our method on challenging videos with complex camera motion and show significant qualitative improvement over current approaches. 1.

Introduction
Decomposing a video into meaningful layers (as show in
Fig. 1) is a long-standing and complex problem [42] that has seen a recent surge in progress with the application of deep neural networks [17, 24, 25, 46]. A challenging variant of layer decomposition is the omnimatte [25] task, which aims to separate an input video into a background and multiple foreground layers, each containing an object of interest along with its correlated effects such as shadows and reflections, thus enabling video editing applications such as object re-moval, background replacement and retiming. The original work on omnimatte [24, 25] introduced a self-supervised ap-proach for decomposing a video by assuming the background can be unwrapped onto a single static 2D background canvas using homography warping. Following work [17,46] relaxed the homography restriction, but maintained the necessity of unwrapping onto a 2D canvas.
This 2D modelling, however, limits the applicability of these methods to camera motions that have limited or zero parallax; intuitively, only panning camera motions are al-lowed. If the cameraâ€™s center of projection moves a sig-nificant distance over the video, 2D methods are unable to learn an accurate background and are forced to place the background detail in a foreground layer (Figure 2). Since camera parallax is quite common, 2D methods are severely restricted in practice.
To handle camera parallax, we exploit another recent line
that group objects with their correlated effects, allowing the user to perform editing tasks such as object removal and re-timing. However, as discussed in Section 1, these works use a constrained background model which limits their use case to panning camera motions only. Subsequent work [17, 46] relaxes this assumption using general image warps, instead of homographies, but still cannot handle significant parallax; moreover, their goal is to learn a per-frame mapping to global sprites to enable applying consistent edits (e.g. style transfer) rather than targeting object removal or video stabilization.
Monocular video depth. Multi-view stereo based meth-ods [23,40] incorporate motion estimation and multi-view re-construction to predict depth by combining information from multiple reference frames. Hybrid depth methods combine single-view depth estimates from a pretrained network with geometric reasoning to obtain coherent depth predictions.
With depth maps from monocular video as initialization, these methods use correspondence to enforce consistency by either assuming static regions [26] or by modeling the scene flow [49]. Recent methods simultaneously estimate depth maps and camera poses [19, 48], exploiting the depth prior to resolve camera motion ambiguities. We rely on Casual-SAM [48] to obtain camera poses and initial depth estimates as their method performs well on casually captured video.
Video completion and object removal. Patch-based meth-ods for video inpainting [7, 12, 14, 28, 43] complete the missing regions by borrowing pixels from other frames, rely-ing on correspondence estimated by homography or optical flow. Learning-based works have explored the use of 3D convolutions [5, 41], attention mechanisms [21, 30, 47] and flow-based methods [10, 45] to achieve more realistic and consistent inpainting. These methods assume that the entire region to inpaint is specified by an input mask, whereas our method jointly learns where to inpaint the background while separating foreground objects and their correlated effects.
NeRF for dynamic scenes. With the recent success of neu-ral fields [29,44] for view synthesis of static scenes, there has been interest in their adoption for dynamic scenes [11,22,27].
These works learn two NeRF models that decompose a scene into a static and dynamic region. However, these methods only support a single foreground layer and do not account for correlated effects such as shadows. ST-NeRF [15] proposed to learn separate NeRFs for each person to obtain an editable neural representation. Their method, however, requires each frame in the video to be captured from 16 different view-points simultaneously. Unlike NeRF methods, our method represents the video with multi-layer mesh geometry where object layers include associated effects, allowing editing as well as fast rerendering of the scene.
Figure 2. Existing methods fail on scenes with parallax. Top row: Omnimatte [25] fails due to inaccurate homography regis-tration, reconstructing the majority of the video in the foreground layer. Middle row: Neural-Atlas [17] incorrectly places trees in the foreground layer and struggles to inpaint the person region.
Bottom row: Our method obtains a clean background and captures the person and their shadow in the foreground layer. of work on estimating 3D camera position and depth maps from casually-captured video with moving objects [18, 26, 48, 49]. Instead of a 2D layer decomposition, we propose to learn a 3D background model that varies per-frame and includes inpainted RGB and depth in the regions occluded by the foreground. Allowing the background to vary per-frame, rather than assuming a global, static canvas, creates an ill-posed decomposition problem, as variation in each input pixel could be explained by a foreground object, background, or both. To resolve this ambiguity, we enforce that the background varies slowly over time with 3D multi-view constraints using dense depth and camera pose estimates [48].
As shown in Fig. 2, our model can successfully separate the layers obtaining a clean background RGB whilst capturing only the person and their shadow in the foreground layer.
Contributions. Our main contribution is a novel video decomposition method capable of accurately separating an input video with complex object and camera motion (e.g., parallax) into a background and object layers. To address the ill-posed decomposition problem, we design regulariza-tion terms based on multi-view consistency and smoothness priors. The resulting model produces clean decompositions of real-world videos where previous methods fail. 2.