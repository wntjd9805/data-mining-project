Abstract
Exploring dense matching between the current frame and past frames for long-range context modeling, memory-based methods have demonstrated impressive results in video object segmentation (VOS) recently. Nevertheless, due to the lack of instance understanding ability, the above approaches are oftentimes brittle to large appearance vari-ations or viewpoint changes resulted from the movement of objects and cameras. In this paper, we argue that instance understanding matters in VOS, and integrating it with memory-based matching can enjoy the synergy, which is in-tuitively sensible from the definition of VOS task, i.e., identi-fying and segmenting object instances within the video. To-wards this goal, we present a two-branch network for VOS, where the query-based instance segmentation (IS) branch delves into the instance details of the current frame and the
VOS branch performs spatial-temporal matching with the memory bank. We employ the well-learned object queries from IS branch to inject instance-specific information into the query key, with which the instance-augmented match-ing is further performed. In addition, we introduce a multi-path fusion block to effectively combine the memory readout with multi-scale features from the instance segmentation de-coder, which incorporates high-resolution instance-aware features to produce final segmentation results. Our method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6% and 87.1%), DAVIS 2017 test-dev (82.8%), and
YouTube-VOS 2018/2019 val (86.3% and 86.3%), outper-forming alternative methods by clear margins. 1.

Introduction
Video object segmentation aims to identify and seg-ment the specific objects in a video sequence, which has very broad applications, e.g., interactive video editing and
†Corresponding authors.
J&F-time curve of XMem [13], a state-of-the-art
Figure 1. memory-based VOS model and our proposed method. XMem will suffer from a distinct accuracy degradation when the appearance of the target object (e.g., pose of the dancing person) changes dra-matically compared to the reference frame. Comparatively, our approach is more robust to this challenging case. autonomous driving. This work focuses on the semi-supervised setting where the annotation of the first frame is given.
Starting from Space-Time Memory network (STM) [46], memory-based methods [13–15, 25, 36, 41, 47, 53, 58] have almost dominated this field due to their supe-rior performance and simplicity. STM [46] and its vari-ants [25,35,65] typically build a feature memory to store the past frames as well as corresponding masks, and perform dense matching between the query frame and the memory to separate targeted objects from the background.
Despite the prominent success achieved, there exists a non-negligible limitation for the above approaches, i.e., the object deformation and large appearance variations result-ing from the motion of camera and objects will inevitably give rise to the risk of false matches [13,15,46], thus making them struggle to generate accurate masks. We visualize the
J&F -time curve of XMem [13], a state-of-the-art memory-based VOS model, on a representative video from DAVIS 2017 in Figure 1. It can be seen that, when the target ob-Figure 2. A conceptual introduction on how humans address the VOS task. For the current frame of a video stream, humans first distinguish between different instances and then match them with the target object(s) in memory. ject undergoes a distinct pose change compared to the first reference frame, XMem [13] misidentifies another person wearing the same color as the foreground and suffers from drastic performance degradation.
In contrast, humans are capable of avoiding such mis-takes and achieving consistently accurate matching. This gap motivates us to reflect on how we humans resolve the
VOS task. Intuitively, given the current frame in a video sequence, humans typically first distinguish between differ-ent instances within it by identifying which instance each pixel belongs to. After that, the instance matching with the target object(s) in memory is conducted to obtain the final results (see Figure 2 for a conceptual illustration). In fact, this intuition is also consistent with the definition of VOS itself, i.e., identify (matching) and segmenting objects (in-stance understanding). Moreover, in the absence of instance understanding, it is theoretically difficult to generate accu-rate predictions for regions that are invisible in the reference frame by pure matching.
Inspired by this, we argue that instance understanding is critical to video object segmentation, which could be incor-porated with memory-based matching to enjoy the synergy.
More specifically, we aim to derive instance-discriminative features that are able to distinguish different instances.
Equipped with these features, we then perform semantic matching with the memory bank to effectively associate the target object(s) with specific instance(s) in current frame.
In this spirit, we present a two-branch network, ISVOS, for semi-supervised VOS, which contains an instance seg-mentation (IS) branch to delve into the instance details for the current frame and a video object segmentation branch that resorts to external memory for spatial-temporal match-ing. The IS branch is built upon a query-based instance seg-mentation model [10] and supervised with instance masks to learn instance-specific representations. Note that ISVOS is a generic framework and IS branch can be easily re-placed with more advanced instance understanding mod-els. The video object segmentation (VOS) branch, on the other hand, maintains a memory bank to store the features of past frames and their predictions. We compare the query key of the current frame and memory key1 from memory 1In this paper, we follow previous work [15,46] of which are compared bank for affinity calculation following [13, 15, 46, 53]. Mo-tivated by recent approaches that use learnable queries serv-ing as region proposal networks to identify instances in im-ages [10, 11, 20, 71], we employ object queries from the IS branch to inject instance-specific information into our query key, with which the instance-augmented matching is per-formed. After that, the readout features are produced by ag-gregating the memory value with the affinity matrix. More-over, in order to make use of the fine-grained instance de-tails reserved in high-resolution instance-aware features, we further combine the multi-scale features from the instance segmentation decoder with the memory readout through a carefully designed multi-path fusion block to finally gener-ate the segmentation masks.
We conduct experiments on the standard DAVIS [49, 50] and YouTube-VOS [67] benchmarks. The results demon-strate that our ISVOS can achieve state-of-the-art perfor-mance on both single-object (i.e., 92.6% in terms of J&F on DAVIS 2016 validation split) and multi-object bench-marks (i.e., 87.1% and 82.8% on DAVIS 2017 validation and test-dev split, 86.3% and 86.3% on YouTube-VOS 2018
& 2019 validation split) without post-processing. 2.