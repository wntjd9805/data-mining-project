Abstract
Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since train-ing on a similar scale for videos is infeasible, recent ap-proaches focus on the effective transfer of image-based
CLIP to the video domain. In this pursuit, new paramet-ric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overﬁt on the given task distribu-tion and lack in generalization aspect. This begs the follow-ing question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is gen-erally sufﬁcient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by fea-ture pooling and similarity matching with corresponding
*Equally contributing authors. text embeddings helps in implicitly modeling the temporal cues within ViFi-CLIP. Such ﬁne-tuning helps the model to focus on scene dynamics, moving objects and inter-object relationships. For low-data regimes where full ﬁne-tuning is not viable, we propose a ‘bridge and prompt’ approach that ﬁrst uses ﬁne-tuning to bridge the domain gap and then learns prompts on language and vision side to adapt
CLIP representations. We extensively evaluate this simple yet strong baseline on zero-shot, base-to-novel generaliza-tion, few-shot and fully supervised settings across ﬁve video benchmarks. Our code and pre-trained models are avail-able at https://github.com/muzairkhattak/ViFi-CLIP. 1.

Introduction
Pretrained vision-language (VL) models like CLIP [33] and ALIGN [16] have shown impressive zero-shot perfor-mance for many downstream vision applications including classiﬁcation [45, 48], detection [15, 35, 49] and segmenta-tion [22, 34]. These models are trained using millions of
image-text pairs sourced from the internet and offer unique representations with strong generalization and transfer ca-pabilities. However, such massive pretraining is laborious for videos due to the following reasons: 1) Aligned video-text data has a limited availability and the cost of preparing such data is monumental in contrast to image-text pairs that are readily available via internet sources [16]. 2) Videos are inherently complex and have large compute cost while the diverse appearance cues could be learned through image-text pairs with a much lower compute budget. Therefore, it is critical to devise methods effectively adapting pretrained image-language models for video-based tasks without for-getting the generic multi-modal learned representations.
Recent video-based approaches adopt CLIP represen-tations using additional learnable components for spatio-temporal modeling.
These components include self-attention layers for cross-frame communication [17], tex-tual or visual prompts [40] or dedicated video decoder mod-ules [30] that are learned while keeping the CLIP backbone frozen or adapting the CLIP encoders as well. However, these designs require modality-speciﬁc inductive biases to be modeled in the developed architectural modules and need careful design efforts to adapt CLIP suitably for videos. Ad-ditionally, while adapting CLIP for downstream video tasks, such approaches generally do not remain a winner across all settings. For example, zero-shot adapted approaches per-form lower in supervised settings, and supervised models score lower on zero-shot generalization tasks.
To address the above challenges, we frame the follow-ing two questions: 1) Does the adaptation of CLIP for videos using additional tunable parameters tamper its gen-eralization capacity? 2) Is a simple video-speciﬁc ﬁne-tuning sufﬁcient to bridge the modality gap between im-ages and videos? In our empirical analysis, we observe that ﬁne-tuning pretrained CLIP encoders along with the newly introduced temporal modeling components can hin-der the generalization capability of CLIP. Interestingly, a simple CLIP model when ﬁne-tuned on a video dataset can instill suitable video-speciﬁc adaptations within the regular
CLIP model and perform competitively to more complex approaches having video-speciﬁc components inbuilt.
Although existing works explore ﬁne-tuning of CLIP en-coders as a baseline, they undermine the potential of full
ﬁne-tuning of CLIP. However we note that, full ﬁne-tuning to achieve better visual-language alignment on videos im-proves synergy between temporal and language cues, and perform competitive to much sophisticated approaches de-veloped for videos (see Fig. 1). Towards understanding how this capacity is achieved by the regular CLIP model, we show that a simple frame-level late representation aggrega-tion before loss calculation allows the exchange of temporal cues within the video ﬁne-tuned CLIP.
While simple CLIP ﬁne-tuning performs competitively to more sophisticated approaches, it is not always feasible, especially on low-data regimes. Based on the ﬁnding that simple ﬁne-tuning can efﬁciently adapt CLIP for videos, we propose a two-stage ‘bridge and prompt’ approach for adapting CLIP for low-data regimes that ﬁrst ﬁne-tunes vanilla CLIP on videos to bridge the modality gap, followed by a vision-language prompt learning approach keeping the tuned CLIP frozen. The contributions of this work are,
• We formulate a simple but strong baseline, ViFi-CLIP (Video Finetuned CLIP), for adapting image-based
CLIP to video-speciﬁc tasks. We show that simple
ﬁne-tuning of CLIP is sufﬁcient to learn video-speciﬁc inductive biases, resulting in impressive performance on downstream tasks (Sec. 4).
• We conduct experiments on four different experimen-tal settings including zero-shot, base-to-novel general-ization, few-shot and fully-supervised tasks. We show better or competitive performance as compared to the state-of-the-art approaches (Secs. 3, 4).
• We show the effectiveness of our proposed ‘bridge and prompt’ approach to ﬁrst bridge the modality gap through ﬁne-tuning followed by prompt learning in both visual and language branches of the CLIP model for low-data regimes (Sec. 5). 2.