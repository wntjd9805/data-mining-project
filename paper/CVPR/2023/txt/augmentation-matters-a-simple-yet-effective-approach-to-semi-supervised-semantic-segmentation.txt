Abstract
Recent studies on semi-supervised semantic segmenta-tion (SSS) have seen fast progress. Despite their promising performance, current state-of-the-art methods tend to in-creasingly complex designs at the cost of introducing more network components and additional training procedures.
Differently, in this work, we follow a standard teacher-student framework and propose AugSeg, a simple and clean approach that focuses mainly on data perturbations to boost the SSS performance. We argue that various data aug-mentations should be adjusted to better adapt to the semi-supervised scenarios instead of directly applying these tech-niques from supervised learning. Specifically, we adopt a simplified intensity-based augmentation that selects a ran-dom number of data transformations with uniformly sam-pling distortion strengths from a continuous space. Based on the estimated confidence of the model on different un-labeled samples, we also randomly inject labelled infor-mation to augment the unlabeled samples in an adaptive manner. Without bells and whistles, our simple AugSeg can readily achieve new state-of-the-art performance on SSS benchmarks under different partition protocols1. 1.

Introduction
Supervised semantic segmentation studies [5, 6, 37, 53] have recently achieved tremendous progress, but their suc-cess depends closely on large datasets with high-quality pixel-level annotations. Delicate and dense pixel-level la-belling is costly and time-consuming, which becomes a sig-nificant bottleneck in practical applications with limited la-belled data. To this end, semi-supervised semantic segmen-tation (SSS) [27, 39] has been proposed to train models on less labelled but larger amounts of unlabeled data.
Consistency regularization [42, 43], the currently domi-nant fundamental SSS method, effectively incorporates the
*Corresponding authors dong@baidu.com).
Council (ARC DP200103223). (luping.zhou@sydney.edu.au, wangjing-This work is supported by Australian Research 1Code and logs: https://github.com/zhenzhao/AugSeg. 1
Figure 1. Comparison between current SOTAs and our simple
AugSeg on Pascal VOC 2012, using R101 as the encoder. training on unlabeled data into standard supervised learn-ing [16, 44]. It relies on the label-preserving data or model perturbations to produce the prediction disagreement on the same inputs, such that unlabeled samples can be lever-aged to train models even if their labeled information is un-known. Some studies in [17, 29, 50, 51] explored different data augmentations to benefit the SSS training while works in [7,16,46] mainly focused on various model perturbations to obtain competitive SSS performance. On top of these fundamental designs, recent state-of-the-art (SOTA) meth-ods aim to integrate extra auxiliary tasks [1, 47, 56, 57], e.g., advanced contrastive learning techniques, and more train-able modules [28, 30, 36, 38], e.g. multiple ensemble mod-els and additional correcting networks, to further improve the SSS performance. Despite their promising performance,
SSS studies along this line come at the cost of requiring more complex methods, e.g., extra network components or additional training procedures.
In this paper, we break the trend of recent SOTAs that combine increasingly complex techniques and propose
AugSeg, a simple-yet-effective method that focuses mainly on data perturbations to boost the SSS performance. Al-though various auto data augmentations [9, 10] and cutmix-related transformations [17, 52] in supervised learning have
Method
CCT [44]
ECS [38]
SSMT [26]
PseudoSeg [58]
CAC [31]
DARS [24]
PC2Seg [56]
C3-Semiseg [57]
ReCo [34]
CPS [7]
ST++ [50]
ELN [30]
USRN [20]
PSMT [36]
U2PL [47]
AugSeg (ours)
Augmentations
More Supervision
Pseudo-rectifying
SDA
FT
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
MBSL CT UCL UAFS ACN PR
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Table 1. Comparison of recent SSS algorithms in terms of
“Augmentations”, “More supervision”, and “Pseudo-rectifying” (sorted by their publication date). We explain the abbrevia-tions as follows. “SDA”: Strong data augmentations, including various intensity-based and cutmix-related augmentations, “FT”:
Feature-based augmentations, “MBSL”: multiple branches, train-ing stages, or losses, “CT”: Co-training, “UCL”: unsuper-vised contrastive learning, “UAFS”: uncertainty/attention filter-ing/sampling, “ACN”: additional correcting networks, “PR”: prior-based re-balancing techniques. Note that, branches of
“more supervision” and “pseudo-rectifying” typically require more training efforts. Differently, our method enjoys the best sim-plicity but the highest performance. been extensively utilized in previous SSS studies, we ar-gue that these augmentations should be adjusted precisely to better adapt to the semi-supervised training. On one hand, these widely-adopted auto augmentations are essen-tially designed for supervised paradigm and aim to search the optimal augmentation strategies from a predefined fi-nite discrete space. Their optimal objective is constant and clear across the training course. However, data perturba-tions in semi-supervised learning consist in generating pre-diction disagreement on the same inputs, without a constant and specific objective or a predefined discrete searching space. Thus, we simplify existing randomAug [10] and de-sign a highly random intensity-based augmentation, which selects a random number of different intensity-based aug-mentations and a random distortion strength from a contin-uous space. On the other hand, random copy-paste [18] among different unlabeled samples can yield effective data perturbations in SSS, but their mixing between correspond-ing pseudo-labels can inevitably introduce confirmation bias [3], especially on these instances with less confident predictions of the model. Considering the utilization effi-ciency of unlabeled data, we simply mix labeled samples with these less confident unlabeled samples in a random and adaptive manner, i.e., adaptively injecting labeled informa-tion to stabilize the training on unlabeled data. Benefiting from the simply random and collaborative designs, AugSeg requires no extra operations to handle the distribution is-sues, as discussed in [51].
Despite its simplicity, AugSeg obtains new SOTA perfor-mance on popular SSS benchmarks under various partition protocols. As shown in Figure 1, AugSeg can consistently outperform current SOTA methods by a large margin. For example, AugSeg achieves a high mean intersection-over-union (mIoU) of 75.45% on classic Pascal VOC 2012 us-ing only 183 labels compared to the supervised baseline of 59.10% and previous SOTA of 71.0% in [50]. We attribute these remarkable performance gains to our revision – that various data augmentations are simplified and adjusted to better adapt to the semi-supervised scenarios. Our main contributions are summarized as follows,
• We break the trend of SSS studies that integrate increasingly complex designs and propose AugSeg, a standard and simple two-branch teacher-student method that can achieve readily better performance.
• We simply revise the widely-adopted data augmenta-tions to better adapt to SSS tasks by injecting labeled information adaptively and simplifying the standard
RandomAug with a highly random design.
• We provide a simple yet strong baseline for future SSS studies. Extensive experiments and ablations studies are conducted to demonstrate its effectiveness. 2.