Abstract
Compared to query-based black-box attacks, transfer-based black-box attacks do not require any information of the attacked models, which ensures their secrecy. However, most existing transfer-based approaches rely on ensembling multiple models to boost the attack transferability, which is time- and resource-intensive, not to mention the difficulty of obtaining diverse models on the same task. To address this limitation, in this work, we focus on the single-model transfer-based black-box attack on object detection, utiliz-ing only one model to achieve a high-transferability ad-versarial attack on multiple black-box detectors. Specifi-cally, we first make observations on the patch optimization process of the existing method and propose an enhanced attack framework by slightly adjusting its training strate-gies. Then, we analogize patch optimization with regular model optimization, proposing a series of self-ensemble ap-proaches on the input data, the attacked model, and the ad-versarial patch to efficiently make use of the limited infor-mation and prevent the patch from overfitting. The experi-mental results show that the proposed framework can be ap-plied with multiple classical base attack methods (e.g., PGD and MIM) to greatly improve the black-box transferability of the well-optimized patch on multiple mainstream detec-tors, meanwhile boosting white-box performance. Our code is available at https://github.com/VDIGPKU/T-SEA. 1.

Introduction
With the rapid development of computer vision, deep learning-based object detectors are being widely applied to
*These authors contributed equally to this work.
†Corresponding author.
Figure 1. Model optimization usually augments the training data and drop out neurons to increase generalization, motivating us to propose self-ensemble methods for adversarial patch optimization.
Specifically, inspired by data augmentation in model optimization, we augment the data x and the model f via constrained data aug-mentation and model ShakeDrop, respectively. Meanwhile, in-spired by drop out in model optimization, we cut out the training patch τ to prevent it overfitting on specific models or images. many aspects of our lives, many of which are highly re-lated to our personal safety, including autonomous driving and intelligent security. Unfortunately, recent works [16,19, 34, 37] have proved that adversarial examples can success-fully disrupt the detectors in both digital and physical do-mains, posing a great threat to detector-based applications.
Hence, the mechanism of adversarial examples on detectors should be further explored to help us improve the robustness of detector-based AI applications.
In real scenes, attackers usually cannot obtain the de-tails of the attacked model, so black-box attacks naturally receive more attention from both academia and industry.
Generally speaking, black-box adversarial attacks can be classified into 1) query-based and 2) transfer-based. For the former, we usually pre-train an adversarial perturbation on the white-box model and then fine-tune it via the infor-mation from the target black-box model, assuming we can access the target model for free. However, frequent queries may expose the attack intent, weakening the covertness of the attack. Contrarily, transfer-based black-box attacks uti-lize the adversarial examples’ model-level transferability to attack the target model without querying and ensure the se-crecy of the attack. Thus, how to enhance the model-level transferability is a key problem of transfer-based black-box attacks. Most existing works apply model ensemble strate-gies to enhance the transferability among black-box models, however, finding proper models for the same task is not easy and training adversarial patch on multiple models is labori-ous and costly. To address these issues, in this work, we fo-cus on how to enhance the model-level transferability with only one accessible model instead of model ensembling.
Though the investigation of adversarial transferability is still in its early stage, the generalizability of neural networks has been investigated for a long time. Intuitively, the asso-ciation between model optimization and patch optimization can be established by shifting of formal definitions. Given the input pair of data x ∈ X , and label y ∈ Y, the clas-sical model learning is to find a parametric model fθ such that fθ(x) = y, while learning an adversarial patch treats the model fθ, the original optimization target, as a fixed input to find a hypothesis h of parametric patch τ to cor-rupt the trained model such that hτ (fθ, x) ̸= y. Hence, it is straight forward to analogize patch optimization with regular model optimization. Motivated by the classical ap-proaches for increasing model generalization, we propose our Transfer-based Self-Ensemble Attack (T-SEA), ensem-bling the input x, the attacked model fθ, and the adversarial patch τ from themselves to boost the adversarial transfer-ability of the attack.
Specifically, we first introduce an enhanced attack base-line based on [32]. Observing from Fig. 4 that the original training strategies have some limitations, we slightly adjust its learning rate scheduler and training patch scale to revise
[32] as our enhanced baseline (E-baseline). Then, as shown in Fig. 1, motivated by input augmentation in model op-timization (e.g., training data augmentation), we introduce the constrained data augmentation (data self-ensemble) and model ShakeDrop (model self-ensemble), virtually expand-ing the inputs of patch optimization (i.e., the input data x and the attacked model f ) to increase the transferability of the patch against different data and models. Meanwhile, motivated by the dropout technique in model optimization, which utilizes sub-networks of the optimizing model to overcome overfitting and thus increasing model generaliza-tion, we propose patch cutout (patch self-ensemble), ran-domly performing cutout on the training patch τ to over-come overfitting. Through comprehensive experiments, we prove that the proposed E-baseline and self-ensemble strate-gies perform very well on widely-used detectors with main-stream base attack methods (e.g., PGD [24], MIM [13]).
Our contributions can be summarized as the following:
• We propose a transfer-based black-box attack T-SEA, requiring only one attacked model to achieve a high adversarial transferability attack on object detectors.
• Observing the issues of the existing approach, we slightly adjust the training strategies to craft an en-hanced baseline and increase its performance.
• Motivated by approaches increasing generalization of deep learning model, we propose a series of strate-gies to self-ensemble the input data, attacked model, and adversarial patch, which significantly increases the model-level adversarial transferability without intro-ducing extra information.
• The experimental results demonstrate that the pro-posed T-SEA can greatly reduce the AP on multiple widely-used detectors on the black-box setting com-pared to the previous methods, while concurrently per-forming well with multiple base attack methods. 2.