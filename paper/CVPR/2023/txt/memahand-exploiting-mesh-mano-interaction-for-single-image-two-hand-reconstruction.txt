Abstract
Existing methods proposed for hand reconstruction tasks usually parameterize a generic 3D hand model or predict hand mesh positions directly. The parametric representa-tions consisting of hand shapes and rotational poses are more stable, while the non-parametric methods can predict more accurate mesh positions. In this paper, we propose to reconstruct meshes and estimate MANO parameters of two hands from a single RGB image simultaneously to utilize the merits of two kinds of hand representations. To fulfill this target, we propose novel Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and MANO parameters as two kinds of query tokens. MMIB consists of one graph residual block to aggregate local information and two transformer encoders to model long-range depen-dencies. The transformer encoders are equipped with differ-ent asymmetric attention masks to model the intra-hand and inter-hand attention, respectively. Moreover, we introduce the mesh alignment refinement module to further enhance the mesh-image alignment. Extensive experiments on the
InterHand2.6M benchmark demonstrate promising results over the state-of-the-art hand reconstruction methods. 1.

Introduction
Vision-based 3D hand analysis plays an important role in many applications such as virtual reality (VR) and aug-mented reality (AR). Two-hand reconstruction from a sin-gle RGB image is more challenging due to complex mutual interactions and occlusions. Besides, the skin appearance similarity makes it difficult for the network to align image features to the corresponding hand.
Previous hand reconstruction works can be divided into two categories, parametric methods [3, 7, 32, 33] and non-parametric methods [4–6, 10, 18–20]. Parametric meth-ods typically learn to regress pose and shape parameters of MANO model [23], where pose represents joint rota-* Equal contribution. † Corresponding author.
Figure 1. Comparison with the state-of-the-art method IntagHand
[16] for single-image two-hand reconstruction. The integration of parametric and non-parametric hand representations allows us to achieve better performance in hard cases such as severe occlusions and challenging viewpoints. tions in axis-angle representation and shape represents the coefficients of shape PCA bases. The MANO prior can yield plausible hand shapes from a single monocular image.
However, they can not produce fine-grained hand meshes due to their limited capacity.
With the rapid progress of graph convolutional network (GCN) and transformer techniques [10, 16, 18, 19], it is ob-served that direct mesh reconstruction can achieve state-of-the-art performance towards the Euclidean distances be-tween the ground truth vertices and the predicted vertices.
Nonetheless, the non-parametric methods are less robust in handling challenging viewpoints or severe occlusions.
In this paper, we introduce a novel single-image two-hand reconstruction method designed to predict mesh ver-tices positions and estimate MANO parameters simulta-neously to utilize the merits of two kinds of hand repre-sentations. The proposed Mesh-Mano interaction Hand reconstruction architecture (MeMaHand) consists of three modules: 1) the image encoder-decoder module, 2) the
mesh-mano interaction module, 3) and the mesh align-ment refinement module. To extract contextually meaning-ful image features, we pre-train a classical image encoder-decoder network on auxiliary tasks including hand seg-mentation, hand 2D joints and dense mapping encodings.
The low-resolution features encode more global knowledge, while the high-resolution features contain more local de-tails. Secondly, the mesh-mano interaction module stacks three mesh-mano interaction blocks (MMIBs) to transform the mesh vertices and MANO parameters queries initialized by the global image feature vector. We observe that the hand prior embedded in the MANO parameters is valuable for predicting stable hand meshes in challenging situations such as severe occlusions. MMIB consists of one graph residual block to aggregate local information and two trans-former encoders to model long-range dependencies. The transformer encoders are equipped with different asymmet-ric attention masks to model the intra-hand and inter-hand attention, respectively. Each MMIB is followed by an up-sampling operation to upsample the mesh vertices tokens in a coarse-to-fine manner. Finally, the mesh alignment re-finement module utilizes one MMIB to predict offsets for mesh vertices and MANO parameters to enhance mesh-image alignment. To improve the reliability of image ev-idence, we project mesh vertices predicted by the Mesh-Mano interaction module onto the 2D image plane. The ex-plicit mesh-aligned image features are concatenated to the transformer input tokens.
The whole network, including the pre-trained image fea-ture encoder-decoder, is jointly optimized such that the im-age features better adapt to our hand mesh reconstruction task. Benefiting from the mesh-mano interaction mecha-nism and mesh alignment refinement stage, extensive ex-periments demonstrate that our method outperforms exist-ing both parametric and non-parametric methods on Inter-Hand2.6M [22] dataset. In summary, the contributions of our approach are as follows:
• We propose MeMaHand to integrate the merits of para-metric and non-parametric hand representation. The mesh vertices and MANO parameters are mutually re-inforced to achieve better performance in mesh recov-ery and parameter regression.
• A mesh-image alignment feedback loop is utilized to improve the reliability of image evidence. Therefore, more accurate predictions are obtained by rectifying the mesh-image misalignment.
• Our method achieves superior performance on the
InterHand2.6M dataset, compared with both non-parametric and parametric methods. 2.