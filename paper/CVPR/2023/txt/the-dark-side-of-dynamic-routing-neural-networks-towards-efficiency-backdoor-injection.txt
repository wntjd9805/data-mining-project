Abstract
Recent advancements in deploying deep neural networks (DNNs) on resource-constrained devices have generated in-terest in input-adaptive dynamic neural networks (DyNNs).
DyNNs offer more efﬁcient inferences and enable the de-ployment of DNNs on devices with limited resources, such as mobile devices. However, we have discovered a new vul-nerability in DyNNs that could potentially compromise their efﬁciency. Speciﬁcally, we investigate whether adversaries can manipulate DyNNs’ computational costs to create a false sense of efﬁciency. To address this question, we pro-pose EfficFrog, an adversarial attack that injects uni-versal efﬁciency backdoors in DyNNs. To inject a backdoor trigger into DyNNs, EfficFrog poisons only a minimal percentage of the DyNNs’ training data. During the infer-ence phase, EfficFrog can slow down the backdoored
DyNNs and abuse the computational resources of systems running DyNNs by adding the trigger to any input. To eval-uate EfficFrog, we tested it on three DNN backbone ar-chitectures (based on VGG16, MobileNet, and ResNet56) using two popular datasets (CIFAR-10 and Tiny ImageNet).
Our results demonstrate that EfficFrog reduces the efﬁ-ciency of DyNNs on triggered input samples while keeping the efﬁciency of clean samples almost the same. 1.

Introduction
The requirement of higher accuracy in deploying deep neural networks(DNNs) leads to the trend of increasing lay-ers for the neural network, according to the “going deeper”
[41] strategy proposed in 2015: the higher number of layers within the neural network, the more complex representa-tions it can learn from the same input data. Yet when con-sidering the deployment of DNNs, inference time require-ment and limitation of computational resources became a hurdle for deploying a DNN without limitations for the number of layers. Such limitations can occur in applica-tions that have inherent limited computational resources, for example, edge computing [30]. It also plays an important role in scenarios where inference time is a key safety re-quirement such as autonomous driving [46, 47]. Therefore, the conﬂict between the computational resources available and inference time requirement for DNNs has raised the re-search interest in efﬁciency improvement while maintaining the same performance.
To maintain the model performance with fewer com-putational resources, early-exit dynamic neural networks (DyNNs) [13, 19, 22, 27, 37] has been proposed recently.
Early-exit dynamic neural networks achieve the balance be-tween performance and inference speed by terminating the computation process in neural networks early if the inter-mediate values satisfy a pre-deﬁned condition. For exam-ple, [20] proposes to add an intermediate classiﬁer to con-volution neural networks and terminate the computation if the conﬁdence scores from the intermediate classiﬁer are larger than a pre-set threshold.
These DyNNs bring in more efﬁcient inferences and make deploying DNNs on resource-constrained devices possible. However, it is unknown whether these DyNNs can maintain their designed “efﬁciency” under adversarial scenarios. Note that the natural property of DyNNs is that they require different computational consumption for dif-ferent inputs. This discloses a potential vulnerability of
DyNN models, i.e. the adversaries may inject a backdoor to a DyNN to give a false sense of efﬁciency to users of the DyNN. Such efﬁciency vulnerability is analogous to the denial-of-service attacks in cybersecurity ( [21,33]) and can lead to severe outcomes in real-world scenarios.
In this paper, we seek to understand such efﬁciency vul-nerability in DyNNs. Speciﬁcally, we aim to answer the following research question:
Can we inject efﬁciency backdoors into DyNNs that only affect DyNNs’ computational efﬁciency on trig-gered inputs, while keeping DyNNs’ behavior in terms of accuracy and efﬁciency unchanged on be-nign inputs?
Numerous studies [1, 10, 24, 28, 34] have demonstrated that it is possible to inject backdoors into deep neural net-works to manipulate the model’s prediction. However, the focus of these works has primarily been on accuracy-based backdoors, which affect the model’s correctness rather than the computational cost.
Injecting efﬁciency-based back-doors presents a more signiﬁcant challenge than accuracy-based ones because the injection process is unsupervised.
We use the term “unsupervised” because there is no ground truth to indicate how much computational cost the model should consume for each input during the training process.
Therefore, creating a backdoor that reduces the computa-tional efﬁciency of the model is a more complex task than one that alters the accuracy of its predictions.
To address the “unsupervised” challenge, our observa-tion is that DyNNs only stop computing when their inter-mediate predictions reach a conﬁdence threshold. Other-wise, DyNNs continue computing until they become con-ﬁdent enough. Motivated by such observation, we propose
EfficFrog, a backdoor injection approach that can inject efﬁciency backdoors into DyNNs to manipulate their efﬁ-ciency. In particular, we design a novel “unconﬁdent ob-jective” function (detailed in Sec. 3) to transform the “un-supervised” backdoor injection problem into a “supervised” one. Our approach utilizes triggered inputs to produce inter-mediate outputs with lower conﬁdence scores of prediction (i.e. evenly distributed conﬁdence scores). This causes a de-lay in the time when the prediction satisﬁes the pre-deﬁned conditions for early exiting, pushing the DyNNs to continue computing and exhaust their computational resources.
In this paper, we have implemented EfficFrog1 and evaluated its effectiveness and stealthiness in various set-tings. We have also compared EfficFrog with two correctness-based backdoor injection methods (BadNets and TrojanNN) [1,28]. Our evaluation results demonstrate that EfficFrog outperforms the comparison baselines by a signiﬁcant margin. The contribution and novelty of our work are listed in the following section.
• Empirical Novelty. We are the ﬁrst to study the efﬁ-ciency backdoor vulnerability of DyNNs. Speciﬁcally, we ﬁnd that the computational consumption of DyNNs can be manipulated by the adversary to provide a false sense of efﬁciency, and the adversary can produce trig-gered inputs to exhaust the computational resources of the victim DyNNs to harm the system’s availability.
• Technical Novelty. We propose a novel “unconﬁ-dent” training strategy to “supervisely” teach the vic-tim DyNNs to produce uniformly distributed conﬁ-dence scores. After injecting the backdoors to the
DyNNs, the DyNNs will produce uncertain predictions 1https://github.com/SeekingDream/EfﬁcFrog for triggered inputs, forcing the DyNNs to continue computing without early termination.
• Evaluation. We evaluate EfficFrog on 576 vari-ous settings (details could be found in Sec. 4). The evaluation results show that EfficFrog success-fully injects efﬁciency-based backdoors into DyNNs and results in more than 3 performance degradation,
⇥ suggesting the necessary to protect DyNNs against efﬁciency-based backdoor attacks. 2.