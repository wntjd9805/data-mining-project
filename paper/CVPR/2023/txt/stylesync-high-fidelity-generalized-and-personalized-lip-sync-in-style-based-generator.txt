Abstract
Despite recent advances in syncing lip movements with any audio waves, current methods still struggle to balance generation quality and the model’s generalization ability.
Previous studies either require long-term data for training or produce a similar movement pattern on all subjects with low quality. In this paper, we propose StyleSync, an effec-tive framework that enables high-fidelity lip synchroniza-tion. We identify that a style-based generator would suffi-ciently enable such a charming property on both one-shot and few-shot scenarios. Specifically, we design a mask-guided spatial information encoding module that preserves the details of the given face. The mouth shapes are accu-rately modified by audio through modulated convolutions.
Moreover, our design also enables personalized lip-sync by introducing style space and generator refinement on only limited frames. Thus the identity and talking style of a tar-get person could be accurately preserved. Extensive ex-periments demonstrate the effectiveness of our method in producing high-fidelity results on a variety of scenes. Re-sources can be found at https://hangz-nju-cuhk. github.io/projects/StyleSync.
*Equal contribution.
†Corresponding authors.
1.

Introduction
The problem of generating lip-synced videos accord-ing to conditional audio is of great importance to the field of digital human creation, audio dubbing, film-making, and entertainment. While the rapid development of this area has been witnessed within recent years, most meth-ods [6,8,9,16,19,24,27,28,38,39,46,50,54,57–60] focus on generating a whole dynamic talking head. Results created under such settings can hardly be blended into an existing scene. Under real-world scenarios like audio dubbing, one crucial need is to seamlessly alter the mouth or facial area while preserving other parts of the scene unchanged, mak-ing these methods non-feasible.
Previous methods take two different paths for achieving seamless mouth modification. A number of studies [38, 44] pursue realistic results on person-specific settings, which re-quire long-term clips for target modeling. Moreover, they rely on prior 3D facial structural information. The uncer-tainty and errors accumulated in the 3D fitting procedure would greatly influence their performances. On the other hand, it is desired to build models that break the data limi-tation on more generalized scenes. As a result, a few meth-ods [31, 32, 41] design person-agnostic models without re-lying on 3D or 2D structural priors. Nevertheless, such a setting is extremely challenging.
In order to produce high-fidelity lip-synced results on any-length videos, two essential challenges need to be ad-dressed. 1) How to efficiently design a powerful genera-tive backbone network that supports both accurate audio in-formation expression and seamless local alternation. Intu-itively, the lip-sync quality naturally contradicts the preser-vation of the original target frame information [32, 59]. 2)
How to effectively leverage as much provided information as possible and involve the personalized properties to a gen-eralized model. Though few-shot meta-learning has been proven effective in generating talking heads [5, 7, 56, 60], how to involve such ability into a lip-syncing pipeline has not been explored.
In this paper, we propose a highly concise and com-prehensive framework named StyleSync, which produces high-fidelity lip-sync results on both generalized and per-sonalized scenarios. The key is our simple but lip-sync-oriented modifications to style-based generator. Though style-based generators [22, 23] have been leveraged in var-ious talking head generation methods [5, 55, 59], their suc-cesses are only partially instructive. They aim at producing the whole head, which leads to unstable background and distortions which are non-acceptable in our scenarios.
By revisiting the details of style-based generators, we identify a few simple but essential modifications that make our framework suitable for lip-syncing. Different from the above methods, we adopt a masked mouth modeling protocol [32, 41] and delicately design a Mask-based Spa-tial Information Encoding strategy, where both the target and reference frames’ information is encoded into a noise space [52, 53] of the generator according to different mask-ing schemes. While the information on audio dynamics and high-level reference frame is injected into the style-modulated convolution in a similar manner as [25, 59]. In this way, our method can be benefited from the strong gen-erative power of style-based generators and also keeps the advantage of easy implementation and fast training.
Moreover, our network modification enables personal-ized information preserving (e.g., speaking styles and de-tails of the mouth and jaw). We take inspiration from the recent studies of inverting StyleGAN priors [1, 2, 35, 45] and propose a Personalized Optimization scheme. As au-dio dubbing is normally performed on speaking videos, our model can make use of only a few seconds of the person’s information and optimize additional person-specific param-eters including the W + and the generator. Extensive exper-iments show that our framework clearly outperforms previ-ous state of the arts on the one-shot setting by a large mar-gin, and the target-specific optimization further enhances the fidelity of our results.
Our contributions can be summarized as follows: 1)
We present the StyleSync framework, which adopts sim-ple but effective modifications including the Mask-based
Spatial Information Encoding to a style-based generator. 2) We propose the Personalized Optimization procedure which involves few-shot person-specific optimization into our framework. 3) Extensive experiments demonstrate that our framework can directly produce accurate and high-fidelity one-shot lip-sync results. Moreover, our proposed personalized optimization further improves the generation quality. Our method outperforms previous methods by a clear margin. 2.