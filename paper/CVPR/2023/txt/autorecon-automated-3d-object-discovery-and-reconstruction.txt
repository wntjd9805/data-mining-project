Abstract
A fully automated object reconstruction pipeline is cru-cial for digital content creation. While the area of 3D recon-struction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRe-con for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that fore-ground objects can be robustly located and segmented from
SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neu-ral scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate ob-ject reconstruction and segmentation. Experiments on the
DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon. The code and supplementary material are available on the project page: https://zju3dv.github.io/autorecon/.
Figure 1. Overview of our fully-automated pipeline and results.
Given an object-centric video, we achieve coarse decomposition by segmenting the salient foreground object from a semi-dense SfM point cloud, with pointwise-aggregated 2D DINO features [3]. Then we train a decomposed neural scene representation from multi-view images with the help of coarse decomposition results to reconstruct foreground objects and render multi-view consistent high-quality foreground masks. 1.

Introduction 3D object reconstruction has long been investigated in computer vision. In this work, we focus on the specific set-ting of reconstructing a salient foreground object from multi-view images and automatically segmenting the object from the background without any annotation, which enables scal-able 3D content creation for VR/AR and may open up the possibility to generate free 2D and 3D object annotations at a large scale for supervised-learning tasks.
Traditional multi-view stereo [8, 32] and recent neural scene reconstruction methods [40, 46] have attained impres-sive reconstruction quality. However, these methods cannot identify objects and the reconstructed object models are typ-ically coupled with the surrounding background. A straight-forward solution is utilizing the foreground object masks to
The authors are affiliated with the ZJU-SenseTime Joint Lab of 3D Vision.
†Corresponding author: Xiaowei Zhou. obtain clean foreground object models. However, accurate 2D object masks are expensive to annotate, and salient ob-ject segmentation techniques [21, 34, 41] generally produce masks with limited granularity, thus degrading the recon-struction quality, especially for objects with thin structures.
Recently, some methods [23,30,50] attempt to automatically decompose objects from 3D scenes given minimal human annotations, such as 3D object bounding boxes, scribbles or pixel labels. But the requirement of manual annotations limits the feasibility of more scalable 3D content creation.
In this paper, we propose a novel two-stage framework for the fully-automated 3D reconstruction of salient objects, as illustrated in Fig. 1. We first perform coarse decomposition to automatically segment the foreground SfM point cloud, and then reconstruct the foreground object geometry by learning an implicit neural scene representation under explicit super-vision from the coarse decomposition. The key idea of our coarse decomposition is to leverage the semantic features
provided by a self-supervised 2D Vision Transformer (ViT)
[3]. Specifically, we aggregate multi-view ViT features from input images to the SfM point cloud and then segment salient foreground points with a point cloud segmentation Trans-former. To train the Transformer on large-scale unlabeled data, we devise a pseudo-ground-truth generation pipeline based on Normalized Cut [33] and show its ability to pro-duce accurate segmentations and 3D bounding boxes upon training. For object reconstruction, we learn a neural scene representation within the estimated foreground bounding box from multi-view images. Our main idea is to reconstruct a decomposed scene representation with the help of explicit regularization provided by the previously decomposed point cloud. Finally, we can extract a clean object model and obtain high-quality object masks with foreground-only rendering.
We conduct experiments on the CO3D [29], Blended-MVS [45], and DTU [12] datasets to validate the effective-ness of the proposed pipeline. The experimental results show that our approach can automatically and robustly recover accurate 3D object models and high-quality segmentation masks from RGB videos, even with cluttered backgrounds.
In summary, we make the following contributions:
• We propose a fully-automated framework for recon-structing background-free object models from multi-view images without any annotation.
• We propose a coarse-to-fine pipeline for scene decom-position by first decomposing the scene in the form of an SfM point cloud, which then guides the decomposi-tion of a neural scene representation.
• We propose an SfM point cloud segmentation Trans-former and devise an unsupervised pseudo-ground-truth generation pipeline for its training.
• We demonstrate the possibility of automatically creat-ing object datasets with 3D models, 3D bounding boxes, and 2D segmentation masks. 2.