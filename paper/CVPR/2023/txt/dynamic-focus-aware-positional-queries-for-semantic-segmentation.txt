Abstract
The DETR-like segmentors have underpinned the most recent breakthroughs in semantic segmentation, which end-to-end train a set of queries representing the class proto-types or target segments. Recently, masked attention [8] is proposed to restrict each query to only attend to the fore-ground regions predicted by the preceding decoder block for easier optimization. Although promising, it relies on the learnable parameterized positional queries which tend to encode the dataset statistics, leading to inaccurate local-ization for distinct individual queries. In this paper, we pro-pose a simple yet effective query design for semantic seg-mentation termed Dynamic Focus-aware Positional Queries (DFPQ), which dynamically generates positional queries conditioned on the cross-attention scores from the preced-ing decoder block and the positional encodings for the cor-responding image features, simultaneously. Therefore, our
DFPQ preserves rich localization information for the tar-get segments and provides accurate and fine-grained posi-tional priors. In addition, we propose to efficiently deal with high-resolution cross-attention by only aggregating the con-textual tokens based on the low-resolution cross-attention scores to perform local relation aggregation. Extensive ex-periments on ADE20K and Cityscapes show that with the two modifications on Mask2former, our framework achieves
SOTA performance and outperforms Mask2former by clear margins of 1.1%, 1.9%, and 1.1% single-scale mIoU with
ResNet-50, Swin-T, and Swin-B backbones on the ADE20K validation set, respectively. Source code is available at https://github.com/ziplab/FASeg. 1.

Introduction
Semantic segmentation aims at assigning each pixel in an image with a semantic class label. As the end-to-end De-tection Transfomer (DETR) [3,42,49,58] is revolutionizing the paradigm of the object detection task, recent segmen-tors [2, 8, 9, 54] follow DETR to learn a set of queries repre-†Corresponding author. E-mail: bohan.zhuang@gmail.com senting the class prototypes or target segments and achieve state-of-the-art performance on semantic segmentation.
In DETR-like frameworks, providing the queries with meaningful positional priors and encourage each query to concentrate on specific regions is essential to learn repre-sentative queries [28, 43, 49, 58]. In this spirit, masked at-tention [8] is proposed, which restricts each query to only attend to a foreground region predicted by the previous de-coder block with binary masks. Although promising, the positional priors in masked attention may be inaccurate and deteriorate performance for two reasons. First, each query comprises a content query that contains semantic informa-tion and a positional query that provides positional infor-mation for the likely locations of the target segments. How-ever, masked attention still relies on positional queries that are randomly initialized learnable parameters [3, 40] (Fig-ure 1 (a)), which tend to encode the average statistics across the dataset and cannot reflect the segments with large lo-cation variances. Second, since each query only attends to the predicted foreground regions, inaccurate predictions lead to error accumulation across the decoder blocks, espe-cially during an early training stage.
To this end, recent detectors propose to dynamically en-code the anchor points into the positional queries to guide queries concentrating around the anchor positions [28, 30, 43] (Figure 1 (b)). The anchor-based query design miti-gates the mentioned issues as the positional queries are dy-namically generated for each target object, thus providing more accurate positional priors. In addition, it avoids re-stricting the queries to only attend to the foreground regions with binary masks to mitigate the error accumulation issue.
However, the anchor-based queries cannot describe the fine-grained positional priors for semantic segmentation, which has details, edges, and boundaries [5, 6].
Motivated by the observations that attention scores im-ply the salient regions for token pruning [24, 26], self-supervised learning [4], and semantic segmentation [34,56], in this paper, we propose a simple yet effective query design for semantic segmentation, dubbed Dynamic Focus-aware
Positional Queries (DFPQ), which dynamically generates
Figure 1. (a) The original randomly initialized positional queries [3] as learnable network parameters, where the positional queries are shared among the Transformer decoder blocks and tend to encode dataset statistics modelling the likely positions for the semantic regions, which leads to inaccurate localization. (b) The anchor-based positional queries [43] are conditional on the bounding box coordinates to give each query positional priors around the anchor. However, the anchor points cannot describe semantic regions, thus still sub-optimal for semantic segmentation. (c) Our dynamic focus-aware queries for semantic segmentation are dynamically generated from the cross-attention scores of the preceding decoder block to provide accurate and fine-grained positional priors, facilitating locating and refining the target segments progressively. the positional queries conditioned on the cross-attention scores of the preceding decoder block and the positional encodings for the corresponding image features, simulta-neously (Figure 1 (c)).
In this way, our DFPQ preserves the localization information of the target segments, thereby providing accurate and fine-grained positional priors and fa-cilitating progressively locating and refining the target seg-ments. When implementing the positional encodings with more powerful ones like [10], our DFPQ is further empow-ered with higher capacity to encode the neighbourhood in-formation for the target segments. Compared to the anchor-based positional queries [28, 43], our DFPQ can cover fine-grained locations for the segmentation details, edges, and boundaries which include rich segmentation cues.
In addition, we propose an efficient method named High-Resolution Cross-Attention (HRCA) to mine details for segmenting small regions from the high-resolution feature maps (1/4 × 1/4 of the original image size). Considering performing cross-attention on high-resolution feature maps requires a formidable amount of memory footprints and computational complexity, e.g., 11G extra floating-point operations with an input resolution of 512 × 512, we pro-pose to encode token affinity only on the informative areas of high-resolution feature maps that are indicated important in the low-resolution counterparts. In this way, fine-grained details are learned efficiently with affordable memory and computations.
Our main contributions can be summarized as follows:
• We make the pioneering attempt to present a simple yet effective query formulation for semantic segmentation, which provides accurate and fine-grained positional priors to localize the target segments, and mitigates the error accumulation problem while being lightweight with little extra computation.
• We propose an efficient high-resolution cross-attention layer to enrich the segmentation details, which dis-cards the semantically unimportant regions for any tar-get segments in the high-resolution feature maps with affordable memory footprint and computational cost.
• Extensive experiments on ADE20K and Cityscapes datasets demonstrate that simply incorporating our
DFPQ and HRCA into Mask2former [8] achieves sig-nificant performance gain and outperforms the SOTA methods. For instance, our FASeg outperforms SOTA methods by 1.1%, 1.3%, and 0.9% single-scale mIoU on the ADE20K [57] validation set with ResNet-50,
Swin-T, and Swin-B backbones, respectively. 2.