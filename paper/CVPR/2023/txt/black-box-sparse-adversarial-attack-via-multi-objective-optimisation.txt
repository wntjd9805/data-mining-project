Abstract
Deep neural networks (DNNs) are susceptible to adver-sarial images, raising concerns about their reliability in safety-critical tasks. Sparse adversarial attacks, which limit the number of modified pixels, have shown to be highly effective in causing DNNs to misclassify. However, exist-ing methods often struggle to simultaneously minimize the number of modified pixels and the size of the modifica-tions, often requiring a large number of queries and as-suming unrestricted access to the targeted DNN. In con-trast, other methods that limit the number of modified pixels often permit unbounded modifications, making them easily detectable.To address these limitations, we propose a novel multi-objective sparse attack algorithm that efficiently min-imizes the number of modified pixels and their size during the attack process. Our algorithm draws inspiration from evolutionary computation and incorporates a mechanism for prioritizing objectives that aligns with an attacker’s goals. Our approach outperforms existing sparse attacks on CIFAR-10 and ImageNet trained DNN classifiers while requiring only a small query budget, attaining competitive attack success rates while perturbing fewer pixels. Over-all, our proposed attack algorithm provides a solution to the limitations of current sparse attack methods by jointly minimizing the number of modified pixels and their size.
Our results demonstrate the effectiveness of our approach in restricted scenarios, highlighting its potential to enhance
DNN security. 1.

Introduction
Although deep neural networks (DNNs) have made im-pressive strides in computer vision tasks [17, 21, 22, 24, 28, 37, 38, 48], recent work has shown that small optimized perturbations to input images can cause DNNs to misclas-sify [1, 18, 26, 31, 34, 42]. As adversarial images have been found to exist in the physical world [26, 27, 43], particu-Figure 1. This illustration shows adversarial images and their cor-responding perturbations generated by two different algorithms, the proposed method and Sparse-RS [10], both attacking an adver-sarially trained CIFAR-10 [19] (top) and ImageNet [35] (bottom).
While both images are adversarial, the perturbation generated by the Sparse-RS algorithm visibly distorts the image, whereas the proposed method’s adversarial image remains more similar to the original. This similarity is demonstrated by calculating the struc-tural similarity (SSIM) between the adversarial images and the original. The effectiveness of the Sparse-RS algorithm is there-fore questionable due to the significant distortion it causes. lar concern has been expressed on their impact on security-critical applications [1]. To address this issue, previous works have emphasized the importance of generating strong adversarial images [3]. As a result, significant effort has been devoted to developing effective attack methods that
can construct perturbations capable of causing DNN classi-fiers to misclassify images while preserving their semantic content.
Most adversarial attack methods in the literature formu-late the attack as an optimization problem where a loss function is minimized to achieve the desired misclassifica-tion of an image. While many attack methods in the lit-erature constrain the adversarial perturbation by its l2 or l∞ norm [2, 4, 6–8, 18, 23, 26, 29, 36, 42, 45, 46] and allow all pixels of an image to be perturbed, there is also a need to develop sparse attack methods that constrain adversarial perturbations by their l0 norm [10, 11, 15, 16, 30, 41, 44, 47].
Such adversarial images have been found to also exist in the physical world and have shown to be as effective as the more traditional l2 or l∞-constrained adversarial images.
Numerous sparse attack methods have been proposed to address both the white-box [11, 15, 16, 44, 47] and black-box scenarios [10, 30, 41]. In the white-box scenario, the attacker has full access to a DNN’s information, while the black-box scenario assumes the attack only has access to the outputted class probabilities.
In this work, we focus on the black-box scenario. While existing attack meth-ods have shown success in generating adversarial images, they often struggle to handle the trade-off between optimiz-ing the loss function and minimizing the perturbations lp norms, where p = 0, 1, 2 or ∞. Several methods only con-strain the number of perturbed pixels [10, 11, 41], allow-ing the size of the perturbation to be unbounded. Despite their efficiency, the unbounded nature of the generated per-turbations result in obvious distortions of the original im-age, as shown in Fig. 1. On the other hand, other meth-ods allow the l0 norm to be minimized along with the loss function [15, 16, 47], while constraining the perturbation by its l2 or l∞ norm. These methods either generate an ad-versarial perturbation and then reduce its l0 norm [47] or add an l0 norm penalty term to the optimized loss func-tion [15, 16]. Despite their good performance, these meth-ods only address the white-box scenario and assume access to a large number of DNN queries, limiting their applicabil-ity to query-limited scenarios [23]. Therefore, by not prop-erly handling this trade-off, existing methods are limited in their applicability and effectiveness in real-world scenarios.
Within the evolutionary computation field a classic ap-proach to handling conflicting objectives is the use of a domination relation [13] which characterises the trade-off between objectives and is used to compare solutions within a population-based evolutionary algorithm. While the orig-inal approach assigns equal weight to each objective, the domination mechanism can be adapted to reflect the at-tacker’s preferences. In this work, our goal is to generate sparse adversarial perturbations with low l0 and l2 norms in an efficient manner. Our contributions can be summarised as follows:
• To address the challenge of generating sparse adver-sarial perturbations, we formulate the problem as a bi-objective optimization problem. By constraining the perturbation to a set of discrete values, we show that minimizing of the l2 norm also minimizes the l0 norm.
• We propose a new dominance relation to compare so-lutions that gives first priority to minimizing the loss function and then to minimizing its l2 norm.
• To generate adversarial perturbations, we propose a population-based heuristic attack method that utilizes two distributions to generate new solutions. These dis-tributions mimic the crossover and mutation operators commonly used in evolutionary computation. By sam-pling from these distributions, our approach explores the search space in an efficient and effective manner.
• To evaluate the effectiveness of our approach, we con-duct attacks on DNN classifiers trained on the CIFAR-10 and ImageNet datasets, using a low-query budget and considering both targeted and non-targeted attack scenarios. Our empirical results demonstrate that our proposed method outperforms state-of-the-art white-box sparse attacks, as well as the black-box Sparse-RS attack method [10], in terms of success rate and num-ber of perturbed pixels. 2.