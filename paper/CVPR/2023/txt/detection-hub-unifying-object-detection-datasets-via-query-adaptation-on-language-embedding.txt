Abstract
Combining multiple datasets enables performance boost on many computer vision tasks. But similar trend has not been witnessed in object detection when combining mul-tiple datasets due to two inconsistencies among detection datasets: taxonomy difference and domain gap. In this pa-per, we address these challenges by a new design (named
Detection Hub) that is dataset-aware and category-aligned.
It not only mitigates the dataset inconsistency but also pro-vides coherent guidance for the detector to learn across multiple datasets. In particular, the dataset-aware design is achieved by learning a dataset embedding that is used to adapt object queries as well as convolutional kernels in detection heads. The categories across datasets are seman-tically aligned into a unified space by replacing one-hot cat-egory representations with word embedding and leveraging the semantic coherence of language embedding. Detection
Hub fulfills the benefits of large data on object detection.
Experiments demonstrate that joint training on multiple datasets achieves significant performance gains over train-ing on each dataset alone. Detection Hub further achieves
SoTA performance on UODB benchmark with wide variety of datasets. 1.

Introduction
Recent computer vision development has demonstrated significant benefits of leveraging large-scale data for com-puter vision tasks, such as image retrieval [31], image recognition [47] and video recognition [39]. However, lim-ited effort has been explored for the object detection task due to the lack of a unified large-scale data. A naive attempt is to combine all annotated data from different sources.
† Corresponding author.
Figure 1. An illustration of the challenges of combing multiple datasets. (a) There are significant domain gaps between differ-ent datasets: the taxonomy are different and the annotations are inconsistent; (b) Even under a unified language embedding, the distribution of categories among datasets are significant different and requires special handling
However, due to the diversity of objects and the cost of an-notating bounding boxes in images, traditional object detec-tion datasets are collected in a domain-specific way, where a limited number of interested categories are regarded as fore-grounds and the rest of objects as backgrounds. This results
in a non-trivial domain shift between different datasets and limits detectors to be trained and tested on a single dataset in order to achieve the best performance on such dataset.
In this paper, we attempt to answer the question: “How can we unify multiple object detection datasets training with a single general object detector?”. Towards this goal, we observe two challenges: taxonomy difference and annota-tion inconsistency, both of which introduce the domain gap issue, shown in Figure 1 (a). More specifically, for the tax-onomy difference issue, the semantic names of similar con-cepts in different datasets may be very different. And for the annotation inconsistency issue, given similar images, fore-ground objects in one dataset may be labeled as background in another dataset. The existence of these two challenges may be the key underlying reason why most current detec-tors only focus on a specific training set, rather than deriving a universal object detector for multiple datasets.
Compared to traditional methods that regard semantic categories as class indices, converting them into language embedding can naturally unify the taxonomy and eases the challenge in recent works [19, 31]. However, it won’t solve the problem, as shown in Figure 1 (b), the distribution of categories among datasets are significant different, which contributes most to the domain gap.
Inspired by recent success of visual-language mod-els [19, 31], we propose a simple yet effective method
“Detection Hub” that can enjoy the synergy of multiple datasets. It builds upon current end-to-end detectors which use learnable object queries to produce final detection re-sults [37]. To overcome the aforementioned problems, we have two key ideas accordingly: 1) map the semantic cat-egory names of different datasets into a category-aligned embedding, and 2) more importantly, use the embedding to dynamically adapt object queries so that each dataset has its own specific query set. We further change the one-hot based classification branch into vision-language alignment, which can well align categories of different datasets. Accordingly, we adopt a region-to-word alignment loss instead of classi-cal cross entropy loss, which makes our “Detection Hub” not limited by a fixed category vocabulary.
Our method leverages the linguistic property of pre-trained language encoders (such that categories with sim-ilar semantic meanings across datasets will be automati-cally embedded together without using an expert-designed label mappers). In addition, categories specific to different datasets will be preserved by dedicated embedding. More-over, as each dataset has its own adapted object query set to generate detection results, the detector can learn how to adapt its behavior to each dataset based on its specific query set. Therefore, the potential competition or distur-bance caused by the annotation inconsistency among differ-ent datasets can be avoided.
To demonstrate the effectiveness of our method, we train our “Detection Hub” on three standard object de-tection datasets jointly: COCO [21], Object365 [35] and
Visual-Genome [14]. These large-scale datasets have dif-ferent properties of taxonomy, vocabulary size and anno-tation quality. Detecton Hub achieves 45.3, 23.2 and 5.7
AP on each dataset, with significant performance gain of
+2.3, +1.0, +0.9 compared with each independently model.
To further verify the effectiveness on datasets with larger variance, we conduct experiments on UODB [40], a com-bination of 11 extremely varied datasets. Detection Hub achieves an average score of 71 and outperforms the previ-ous SoTA UniversalDA [40] by a large margin of 6.8 point. 2.