Abstract
This paper focuses on federated semi-supervised learn-ing (FSSL), assuming that few clients have fully labeled data (labeled clients) and the training datasets in other clients are fully unlabeled (unlabeled clients). Existing methods attempt to deal with the challenges caused by not independent and identically distributed data (Non-IID) set-ting. Though methods such as sub-consensus models have been proposed, they usually adopt standard pseudo label-ing or consistency regularization on unlabeled clients which can be easily influenced by imbalanced class distribution.
Thus, problems in FSSL are still yet to be solved. To seek for a fundamental solution to this problem, we present Class
Balanced Adaptive Pseudo Labeling (CBAFed), to study
FSSL from the perspective of pseudo labeling. In CBAFed, the first key element is a fixed pseudo labeling strategy to handle the catastrophic forgetting problem, where we keep a fixed set by letting pass information of unlabeled data at the beginning of the unlabeled client training in each communication round. The second key element is that we design class balanced adaptive thresholds via consider-ing the empirical distribution of all training data in local clients, to encourage a balanced training process. To make the model reach a better optimum, we further propose a residual weight connection in local supervised training and global model aggregation. Extensive experiments on five datasets demonstrate the superiority of CBAFed. Code will be available at https://github.com/minglllli/
CBAFed. 1.

Introduction
Federated learning (FL) aims to train machine learning models on a decentralized manner while preserving data privacy, i.e., separate local models are trained on separate local training datasets independently. In recent years, FL has received much attention for privacy protection reasons
*Corresponding Author.
[32]. However, most of FL works focused on supervised learning with fully labeled data. But in practice, labeling process of large-scale training data is laborious and expen-sive. Due to the lack of funds or experts, large labeled train-ing dataset is difficult for many companies and institutions to obtain. These may hinder applicability of FL.
To handle this problem, federated semi-supervised learn-ing (FSSL) has been explored by many researchers recently
[8, 14, 15]. There are broadly three lines of FSSL methods by considering different places and status of labeled data.
The first two lines consider that there are only limited la-beled data in the central server [8] or each client has par-tially labeled data [15]. The third line assumes that few clients have fully labeled data and the training datasets in other clients are fully unlabeled [14, 18, 31]. Our paper mainly focuses on the third line of FSSL, and we call lo-cal clients with fully labeled data as labeled clients and the other clients as unlabeled clients.
The main difficulties to train a third line FSSL model lie in three folds: 1) There are no labeled data in unlabeled clients. Thus, the training can be easily biased without label guidance. 2) Due to the divergent class distribution of labeled and unlabeled clients, namely the not indepen-dent and identically distributed data (Non-IID) setting, in-accurate supervisory signals may be generated in unlabeled clients via employing the model trained in labeled clients by either pseudo labeling or consistency regularization frame-work. 3) Due to the catastrophic forgetting problems in
CNNs, with the training process of unlabeled clients going on, models may forget the knowledge learned on labeled clients and so decrease the prediction accuracy drastically.
RSCFed [14], the state-of-the-art method significantly boosts the FSSL performance by first distilling sub-consensus models, and then aggregating the sub-consensus models to the global model. The sub-consensus models can handle the Non-IID setting to some extent, but the mean-teacher based consistency regularization framework in un-labeled clients inevitably causes the accuracy degradation when the classes are imbalanced distributed. RSCFed at-tempts to address the catastrophic forgetting problem by adjusting aggregation weights for labeled and unlabeled clients. But, it seems just alleviates the negative impact on the unlabeled clients and the problem is still yet to be tackled. Besides, the sub-consensus models may occur un-stable training problems, and increase the communication cost. Other methods like FedConsist [31] and FedIRM
[18] achieve promising results by utilizing pseudo labeling methods to produce artificial labels, but they do not consider the Non-IID setting among local clients.
Pseudo labeling methods are shown to be effective in semi-supervised learning (SSL) [13, 24, 33, 37], which gen-erate pseudo-labels for unlabeled images and propose sev-eral strategies to ensure the quality of pseudo-labels. Ex-isting FSSL works only adopt standard pseudo labeling or consistency regularization on FSSL. Since the mentioned difficulties hamper the usage of these methods, other rem-edy is usually proposed to alleviate the difficulties, which is palliative. Thus, we propose to study FSSL from the perspective of pseudo labeling, seeking for a fundamental solution to this problem.
Concretely, we present Class Balanced Adaptive Pseudo
Labeling, namely CBAFed, by rethinking standard pseudo labeling methods in SSL. To handle the catastrophic forget-ting problem, we propose a fixed pseudo labeling strategy, which builds a fixed set by letting pass informative unla-beled data and their pseudo labels at the beginning of the unlabeled client training. Due to the Non-IID and heteroge-neous data partition problems in FL, training distribution of unlabeled data can be highly imbalanced, so existing thresh-olds are not suitable in FSSL. We design class balanced adaptive thresholds via considering the empirical distribu-tion of all training data in local clients at the previous com-munication round. Analysis proves that our method sets a reasonably high threshold for extremely scarce classes and encourages a balanced training process. To enhance the learning ability and discover unlabeled data from tail classes, we propose to leverage information from so-called
“not informative” unlabeled data. Besides, we also explore a novel training strategy for labeled clients and the central server, termed as residual weight connection, skip connect-ing weights from previous epochs (for labeled clients) or previous global models (for the central server). It can help the model reach better optimum, when the training distribu-tion is imbalanced and training amount is small. We con-duct extensive experiments on five datasets to show the ef-fectiveness of CBAFed. Overall, our main contributions can be summarized as follows:
• We present a CBAFed method to deal with the catas-trophic forgetting problems in federated learning . Un-like existing FSSL frameworks that directly adopts pseudo labeling or consistency regularization methods,
CBAFed explores a fundamental solution to FSSL via designing a novel pseudo labeling method.
• We introduce a residual weight connection method, to improve the robustness of the models in labeled clients and the central server, which skip connects weights from previous epoch or communication round to fi-nally reach better optimum.
• Experiments are conducted on five datasets: four nat-ural datasets CIFAR-10/100, SVHN, fashion MNIST and one medical dataset ISIC 2018 Skin. CBAFed outperforms state-of-the-art FSSL methods by a large margin, with 11.3% improvements on SVHN dataset. 2.