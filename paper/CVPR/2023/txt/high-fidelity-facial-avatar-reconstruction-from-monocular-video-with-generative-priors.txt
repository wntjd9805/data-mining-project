Abstract
High-fidelity facial avatar reconstruction from a monoc-ular video is a significant research problem in computer graphics and computer vision. Recently, Neural Radiance
Field (NeRF) has shown impressive novel view rendering results and has been considered for facial avatar recon-struction. However, the complex facial dynamics and miss-ing 3D information in monocular videos raise significant challenges for faithful facial reconstruction. In this work, we propose a new method for NeRF-based facial avatar re-construction that utilizes 3D-aware generative prior. Dif-*Work done during an internship at Tencent AI Lab.
†Corresponding Authors. ferent from existing works that depend on a conditional de-formation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. We propose an efficient method to construct the personalized generative prior based on a small set of fa-cial images of a given individual. After learning, it allows for photo-realistic rendering with novel views, and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM co-efficients, and audio. Compared with existing works, we obtain superior novel view synthesis results and faithfully face reenactment performance. The code is available here
https://github.com/bbaaii/HFA-GP. 1.

Introduction
Reconstructing high-fidelity controllable 3D faces from a monocular video is significant in computer graphics and computer vision and has great potential in digital human, video conferencing, and AR/VR applications. Yet it is very challenging due to the complex facial dynamics and missing 3D information in monocular videos.
Recently, Neural Radiance Field (NeRF) [30] has shown impressive quality for novel view synthesis. The key idea of NeRF is to encode color and density as a function of spa-tial location and viewing direction by a neural network and adopt volume rendering techniques for novel view synthe-sis. Its photo-realistic rendering ability has sparked great interest in facial avatar reconstruction. Deformable neural radiance fields have been proposed to handle the non-rigidly deforming faces captured in monocular videos. For exam-ple, the works of [34, 35] proposed to learn a conditional deformation field to capture the non-rigidly deformation of each frame. After training, they can provide novel view syn-thesis for the training frames. However, they don’t support facial editing and cannot be used for face reenactment.
The controllability of facial avatars is indispensable for many downstream applications, such as talking head syn-thesis. The core idea of existing works is to learn a dy-namic neural radiance field conditioned on specific driven signals. For example, 3D morphable face model (3DMM)
[3] is introduced as guidance in NeRF-based facial avatar reconstruction [2, 11, 13]. The work of [11] learns a dy-namic NeRF that is directly conditioned on the pose and ex-pression coefficients estimated by 3DMM. In RigNeRF [2], the deformation field is a combination of a pre-calculated 3DMM deformation field prior and a learned residual condi-tioned on the pose and expression coefficients. After mod-eling, one can use 3DMM coefficients for face reenactment.
In addition to the explicit 3DMM coefficients, audio-driven dynamic NeRF has also been studied [17, 41]. Recently,
AD-NeRF [17] has been proposed to optimize a dynamic neural radiance field by augmenting the input with audio features. DFRF [41] further considers the few-shot audio-driven talking head synthesis scenario. These works di-rectly learn a conditional deformation field and scene repre-sentation in the continuous 5D space. However, recovering 3D information from monocular videos is an ill-posed prob-lem. It is very challenging to obtain a high-fidelity facial avatar.
To alleviate the aforementioned challenges, we propose to adopt 3D generative prior. Recently, 3D-aware generative adversarial networks (3D-GAN) [5, 6, 16, 33, 43] are pro-posed for unsupervised generation of 3D scenes. By lever-aging the state-of-the-art 2D CNN generator [22] and neural volume rendering, the work of [5] can generate high-quality multi-view-consistent images. The latent space of 3D-GAN constitutes a rich 3D-aware generative prior, which moti-vates us to explore latent space inversion and navigation for 3D facial avatar reconstruction from monocular videos.
However, 3D-GAN is usually trained on the dataset with a large number of identities, such as FFHQ [21], resulting in a generic generative prior. It is inefficient for personalized fa-cial reconstruction and reenactment, which requires faithful maintenance of personalized characteristics.
In this work, we propose to learn a personalized 3D-aware generative prior to reconstruct multi-view-consistent facial images of that individual faithfully. Considering that facial variations share common characteristics, we learn a local and low-dimensional personalized subspace in the la-tent space of 3D-GAN. Specifically, we assign a group of learnable basis vectors for the individual. Each frame is sent to an encoder to regress a weight coefficient, which is used to form a linear combination of the basis. The resulting latent code is sent to a 3D-aware generator for multi-view-consistent rendering. We show that both the personalized basis and encoder can be well modeled given a small set of personalized facial images. After training, one can di-rectly project the testing frames with different facial expres-sions onto the learned personalized latent space to obtain a high-quality 3D consistent reconstruction. It is worth not-ing that the input modality is not limited to RGB frames.
We demonstrate with a simple modification. The encoder can be trained with different signals, such as 3DMM ex-pression coefficients or audio features, enabling 3DMM or audio-driven face reenactment. To verify its effectiveness, we conduct experiments with different input modalities, in-cluding monocular RGB videos, 3DMM coefficients, and audio. The comparison to state-of-the-art methods demon-strates our superior 3D consistent reconstruction and faith-fully face reenactment performance.
Our main contributions are four-fold: 1) we propose to utilize 3D-aware generative prior for facial avatar recon-struction; 2) we propose an efficient method to learn a lo-cal and low-dimensional subspace to maintain personalized characteristics faithfully; 3) we develop 3DMM and audio-driven face reenactment by latent space navigation; 4) we conduct complementary experimental studies and obtain su-perior facial reconstruction and reenactment performance. 2.