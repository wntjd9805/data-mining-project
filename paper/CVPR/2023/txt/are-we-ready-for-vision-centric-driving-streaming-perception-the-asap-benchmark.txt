Abstract
In recent years, vision-centric perception has flourished in various autonomous driving tasks, including 3D detec-tion, semantic map construction, motion forecasting, and depth estimation. Nevertheless, the latency of vision-centric approaches is too high for practical deployment (e.g., most camera-based 3D detectors have a runtime greater than 300ms). To bridge the gap between ideal researches and real-world applications, it is necessary to quantify the trade-off between performance and efficiency. Tradition-ally, autonomous-driving perception benchmarks perform the offline evaluation, neglecting the inference time de-lay. To mitigate the problem, we propose the Autonomous-driving StreAming Perception (ASAP) benchmark, which is the first benchmark to evaluate the online performance of vision-centric perception in autonomous driving. On the basis of the 2Hz annotated nuScenes dataset, we first propose an annotation-extending pipeline to generate high-frame-rate labels for the 12Hz raw images. Referring to the practical deployment, the Streaming Perception Under constRained-computation (SPUR) evaluation protocol is further constructed, where the 12Hz inputs are utilized for streaming evaluation under the constraints of differ-In the ASAP benchmark, ent computational resources. comprehensive experiment results reveal that the model rank alters under different constraints, suggesting that the model latency and computation budget should be consid-ered as design choices to optimize the practical deployment.
To facilitate further research, we establish baselines for camera-based streaming 3D detection, which consistently enhance the streaming performance across various hard-ware. ASAP project page: https://github.com/
JeffWang987/ASAP. 1.

Introduction
Vision-centric perception in autonomous driving has drawn extensive attention recently, as it can obtain richer 1Corresponding author, xingang.wang@ia.ac.cn
Figure 1. Comparison of streaming performances on the ASAP benchmark, where the model rank changes under different com-putational resources. Note that our baseline BEVDepth-Sv (built upon [30]) consistently improves the streaming performance on different platforms. semantic information from images with a desirable budget, compared to LiDAR-based perception. Notably, the past years have witnessed the blooming of vision-centric per-ception in various autonomous driving tasks (e.g., 3D de-tection [21, 22, 29–31, 33, 34, 41, 56, 61], semantic map con-struction [28, 40, 43, 45, 64, 72], motion forecasting [1, 20], and depth estimation [16, 17, 55, 57, 58, 60]).
Despite the growing research interest in vision-centric approaches, the high latency of these methods still prevents the practical deployment. Specifically, in the fundamen-tal task of autonomous-driving perception (e.g., 3D detec-tion), the inference time of most camera-based 3D detec-tors [21,29–31,34,61,70] is longer than 300ms (on the pow-erful NVIDIA RTX3090), which is ∼6× longer (see Tab. 1) than the LiDAR-based counterparts [25, 62, 66]. To enable practical vision-centric perception in autonomous driving, a quantitative metric is in an urgent need to balance the ac-curacy and latency. However, previous autonomous-driving benchmarks [3, 4, 7, 12, 13, 23, 38, 46, 49, 59, 67] mainly fo-Table 1. Comparison between autonomous-driving perception dataset, where L&C represents LiDAR and camera, #sensors de-notes number of sensors, Ann. frequency is the annotation fre-quency, and Model speed denotes the typical inference speed of the model on RTX3090. For 2D detectors [2, 11, 15, 44, 50], they achieve ∼45mAP@30FPS on COCO [32]. For LiDAR-based 3D detectors [25, 62, 66], they achieve ∼70mAP@20FPS on Waymo
[49]. For camera-based 3D detectors [21, 30, 31, 70], they achieve
∼40mAP@3FPS on nuScenes [4], which is 6×∼10× slower than the above two tasks.
Dataset
Stream. Modality
#sensors
Task
KITTI [12]
Argoverse [59]
Waymo [49] nuScenes [4]
Argoverse-HD [27]
Waymo [18] nuScenes-H (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33)
L&C
L&C
L&C
L&C
C
L
C
----1 1 6
Multi-task
Multi-task
Multi-task
Multi-task 2D det.
L-3D det.
C-3D det.
Ann. frequency
----30Hz 10Hz 12Hz
Model speed
----∼30FPS
∼20FPS
∼3FPS cus on the offline performance metrics (e.g., Average Pre-cision (AP), Truth Positive (TP)), and the model latency has not been particularly studied. Although [18, 27] lever-age the streaming perception paradigm [27] to measure the accuracy-latency trade-off, these benchmarks are designed for 2D detection or LiDAR-based 3D detection.
To address the aforementioned problem, this paper proposes the Autonomous-driving StreAming Perception (ASAP) benchmark. To the best of our knowledge, this is the first benchmark to evaluate the online performance of vision-centric perception in autonomous driving. The
ASAP benchmark is instantiated on the camera-based 3D detection, which is the core task of vision-centric percep-tion in autonomous driving. To enable the streaming evalua-tion of 3D detectors, an annotation-extending pipeline is de-vised to increase the annotation frame rate of the nuScenes dataset [4] from 2Hz to 12Hz. The extended dataset, termed nuScenes-H (High-frame-rate annotation), is uti-lized to evaluate 3D detectors with 12Hz streaming inputs.
Referring to the practical deployment, we delve into the problem of ASAP under different computational resources.
Specifically, the Streaming Perception Under constRained-computation (SPUR) evaluation protocol is constructed: (1)
To compare the model performance on varying platforms, multiple GPUs with different computation performances are assigned for the streaming evaluation. (2) To analyze the performance fluctuation caused by the sharing of com-putational resources [9, 61, 65, 70], the streaming evaluation is performed while the GPU is simultaneously processing other perception tasks. As depicted in Fig. 1, the streaming performances of different methods drop steadily as the com-putation power is increasingly constrained. Besides, the model rank alters under various hardware constraints, sug-gesting that the offline performance cannot serve as the de-terministic criterion for different approaches. Therefore, it is necessary to introduce our streaming paradigm to vision-centric driving perception. Based on the ASAP bench-mark, we further establish simple baselines for camera-based streaming 3D detection, and experiment results show that forecasting the future state of the object can compen-sate for the delay in inference time. Notably, the proposed
BEVDepth-Sv improves the streaming performance (mAP-S) by ∼2%, ∼3%, and ∼16% on three GPUs (RTX3090,
RTX2070S, GTX1060).
The main contributions are summarized as follows: (1)
We propose the ASAP benchmark to quantitatively eval-uate the accuracy-latency trade-off of camera-based per-ception methods, which takes a step towards the practical vision-centric perception in autonomous driving. (2) An annotation-extending pipeline is proposed to annotate the 12Hz raw images of the popular nuScenes dataset, which facilitates the streaming evaluation on camera-based 3D de-tection. (3) Simple baselines are established in the ASAP benchmark, which alleviates the influence of inference de-lay and consistently improves the streaming performances across different hardware. (4) The SPUR evaluation proto-col is constructed to facilitate the evaluation of practical de-ployment, where we investigate the streaming performance of the proposed baselines and seven modern camera-based 3D detectors under various computational constraints. 2.