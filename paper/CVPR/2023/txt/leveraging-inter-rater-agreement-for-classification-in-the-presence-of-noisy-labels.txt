Abstract
In practical settings, classiﬁcation datasets are obtained through a labelling process that is usually done by humans.
Labels can be noisy as they are obtained by aggregating the different individual labels assigned to the same sample by multiple, and possibly disagreeing, annotators. The inter-rater agreement on these datasets can be measured while the underlying noise distribution to which the labels are subject is assumed to be unknown.
In this work, we: (i) show how to leverage the inter-annotator statistics to esti-mate the noise distribution to which labels are subject; (ii) introduce methods that use the estimate of the noise distri-bution to learn from the noisy dataset; and (iii) establish generalization bounds in the empirical risk minimization framework that depend on the estimated quantities. We con-clude the paper by providing experiments that illustrate our
ﬁndings. 1.

Introduction
Supervised learning has seen enormous progress in the last decades, both theoretical and practical. Empirical risk minimization is used as a learning framework [23], which relies on the assumption that the model is trained with iid (independent and identically distributed) sampled data from the joint distribution between features and labels. As a con-sequence of generalization bounds, when this assumption is satisﬁed any desired performance can be achieved as long as enough training data is available. However in many real-world applications, due to ﬂaws during the data collection and labeling process, the assumption that the training data is sampled from the true feature-label joint distribution does not hold. Training data is often annotated by human raters who have some non-zero probability of making mistakes. It
*This work was done during Maria Soﬁa Bucarelli’s and Federico Si-ciliano’s internship at Amazon. has been reported in [21] that the ratio of corrupted labels in some real-world datasets is between 8.0% and, 38.5% .
As a consequence of the presence of incorrect labels in the training dataset, the aforementioned assumption is violated and hence performance guarantees based on generalization bounds no longer hold.
This gap between theory and practice raises the question whether it is possible to learn from datasets with noisy la-bels while still having performance guarantees. This ques-tion has received a lot of attention lately and has already been answered in the positive in some cases [15, 16]. In-deed multiple works have introduced learning algorithms that can cope with datasets with incorrect labels while guar-anteeing desirable performance through provable general-ization bounds. However, these solutions do not solve the entirety of the problem due to the fact that they rely on precise knowledge of the error rate to which the labels are subject, which is often unknown in practice. Several works [16, 26, 27] attempt to address this issue by introduc-ing techniques to estimate such error rate. Some of these methods have the drawback of relying on assumptions that do not always hold in practice, such as the existence of an-chor samples [16]. Ideally, it would be desirable to design learning algorithms that are both robust to noisy labels, and for which performance guarantees can be provided.
An approach, often used in industry to reduce the im-pact of errors made by human raters, is to label the same dataset multiple times by different annotators. Then the in-dividual labels are combined to reduce the probability of erroneous labels in the dataset, two popular approaches are majority vote or soft labeling. In these cases inter-annotator agreement (IAA) scores (like Cohen’s kappa [1] and Fleiss’ kappa [5]) provide measurable metrics that are directly re-lated to the probability of error present in the labels.
Since the IAA holds a direct relationship with the error rate associated with the human raters, one could potentially estimate the error rate and leverage this estimate to modify
the learning algorithms with the objective of making them robust to the resulting noise in the labels. This is the main direction we explore in this work.
Motivation and Contributions: This work is motivated by two main points: i- to the best of our knowledge there are no published results that indicate how to leverage the
IAA statistics to estimate the label noise distribution; and ii- the generalization bounds of existing noise tolerant train-ing methods often rely on unknown quantities (like the true noise distribution) instead of on quantities that can be mea-sured (like the IAA statistics).
Our contributions are the following: (i) we provide a methodology to estimate the label noise distribution based on the IAA statistics; (ii) we show how to leverage this estimate to learn from the noisy dataset; and (iii) we pro-vide generalization bounds for our methods that depend on known quantities. 2.