Abstract
Recently, open-vocabulary learning has emerged to ac-complish segmentation for arbitrary categories of text-based descriptions, which popularizes the segmentation system to more general-purpose application scenarios.
However, existing methods devote to designing specialized architectures or parameters for specific segmentation tasks.
These customized design paradigms lead to fragmentation between various segmentation tasks, thus hindering the uni-formity of segmentation models. Hence in this paper, we propose FreeSeg, a generic framework to accomplish Uni-fied, Universal and Open-Vocabulary Image Segmentation.
FreeSeg optimizes an all-in-one network via one-shot train-ing and employs the same architecture and parameters to handle diverse segmentation tasks seamlessly in the in-ference procedure. Additionally, adaptive prompt learn-ing facilitates the unified model to capture task-aware and category-sensitive concepts, improving model robustness in multi-task and varied scenarios. Extensive experimental re-sults demonstrate that FreeSeg establishes new state-of-the-art results in performance and generalization on three seg-⋆Equal contribution. †Corresponding author. This work was done while Jie Qin interned at ByteDance. mentation tasks, which outperforms the best task-specific architectures by a large margin: 5.5% mIoU on seman-tic segmentation, 17.6% mAP on instance segmentation, 20.1% PQ on panoptic segmentation for the unseen class on COCO. Project page: https://FreeSeg.github.io. 1.

Introduction
Image segmentation has been one of the most widely researched topics in computer vision, aiming to simulta-neously group and categorize object pixels in the image.
In the recent literature, the image segmentation commu-nity has witnessed tremendous success at cost of large-scale datasets [1, 3, 30], where objects are exhaustively annotated with pixel-level masks and category labels. However, due to the time-consuming and laborious annotations, the tem-plate categories sizes of existing segmentation tasks are still limited to an order of 10 or 102, which is in orders of mag-nitude much smaller than the vocabulary that humans use to describe the real world. Such learning objective binds the segmentors’ scalability into a limited cognitive space, and it becomes a critical bottleneck when this system is popular-ized to handle richer and more generalized semantics.
As a viable path to handle categories of custom specifi-cation beyond the training dataset, open-vocabulary learn-ing leverages large-scale visual-language pre-training mod-els (such as CLIP [26], ALIGN [14]) to calculate match-ing similarity between visual concept and text corpus.
Recently, a series of segmentation-based open-vocabulary studies [1, 37, 38] have emerged to design task-specific architectures and parameters for individual segmentation task. For example, ZSSeg [38] leverages the off-the-shelf pre-trained CLIP model and achieves competitive perfor-mance in open vocabulary semantic segmentation. How-ever, current works suffer from two obvious shortcomings when popularized to general segmentation scenes: i) task-insensitive: they can not capture task-aware characteris-tics and be effectively generalized to diverse segmentation tasks; ii) resource-unfriendly: the model needs to be trained from scratch when switching tasks, and diverse tasks re-quire deploying multiple customized models. Although
MaskFormer [6] succeeds in accomplishing multiple seg-mentation tasks into one compact system, it still needs to train a customized model for each task and it is not designed for open-vocabulary tasks. These observations motivate us to raise a question: how to design a unified open-vocabulary framework to accomplish universal segmentation tasks?
To address the above question, As shown in Fig.1, we propose FreeSeg, a novel framework to accomplish Uni-fied, Universal and Open-Vocabulary Image Segmenta-tion. In FreeSeg, our goals are mainly three-fold: i) Unified:
FreeSeg designs a unified (all-in-one) network that employs the same architecture and inference parameters to handle multiple segmentation tasks; ii) Universal: FreeSeg adapts to various tasks, namely semantic, instance and panoptic segmentation; iii) Open-Vocabulary: FreeSeg is capable of generalizing to arbitrary segmentation categories.
In general, FreeSeg advocates a two-stage segmentation framework, with the first stage extracting universal mask proposals and the second stage accomplishing zero-shot classification on these masks. Specifically, FreeSeg con-ducts a one-shot training procedure to optimize a unified segmentation model with multi-task labels, which helps to capture task-special characteristics for universal segmenta-tion. An adaptive prompt learning scheme is introduced to encode task-aware and category-sensitive concepts into the text abstraction.
It enables FreeSeg to flexibly ac-complish different segmentation tasks of arbitrary cate-gories, handling all tasks and categories in one model. To sum up, FreeSeg is a task-flexible, category-arbitrary and performance-excellent framework, the main contributions of our work are listed as follows:
• To the best of our knowledge, we offer the first at-tempt to tackle a novel computer vision task, namely, unified open-vocabulary segmentation. A universal framework FreeSeg is proposed to employ an all-in-one model with the same architecture and inference parameters to accomplish open-vocabulary semantic, instance, and panoptic segmentation.
• Adaptive prompt learning explicitly encodes multi-granularity concepts (task, category) into compact tex-tual abstraction and helps the unified model generalize to arbitrary text descriptions. FreeSeg further designs the semantic context interaction and test time prompt tuning mechanism to improve cross-model alignment and generalization for unseen classes.
• We evaluate FreeSeg on three image segmentation tasks (semantic, instance, and panoptic segmenta-tion) using COCO, ADE20K and VOC 2012. As shown in Fig.1 (c), extensive experiments demonstrate that FreeSeg establishes new state-of-the-art results in terms of performance and generalization. In addition to reducing the research effort by at least three times, it outperforms the best-specialized architectures and is more feasible for multi-task deployment. 2.