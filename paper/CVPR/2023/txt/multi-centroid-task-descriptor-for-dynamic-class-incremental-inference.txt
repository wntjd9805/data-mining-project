Abstract
Incremental learning could be roughly divided into two categories, i.e., class- and task-incremental learning. The main difference is whether the task ID is given during eval-uation. In this paper, we show this task information is in-deed a strong prior knowledge, which will bring signiﬁcant improvement over class-incremental learning baseline, e.g.,
DER [39]. Based on this observation, we propose a gate network to predict the task ID for class incremental infer-ence. This is challenging as there is no explicit semantic re-lationship between categories in the concept of task. There-fore, we propose a multi-centroid task descriptor by assum-ing the data within a task can form multiple clusters. The cluster centers are optimized by pulling relevant sample-centroid pairs while pushing others away, which ensures that there is at least one centroid close to a given sample.
To select relevant pairs, we use class prototypes as prox-ies and solve a bipartite matching problem, making the task descriptor representative yet not degenerate to uni-modal.
As a result, our dynamic inference network is trained inde-pendently of baseline and provides a ﬂexible, efﬁcient so-lution to distinguish between tasks. Extensive experiments show our approach achieves state-of-the-art results, e.g., we achieve 72.41% average accuracy on CIFAR100-B0S50, outperforming DER by 3.40%. 1.

Introduction
As a rapidly developing task in machine learning, incre-mental learning [7, 27] (IL) aims to continually learn new concepts (classes), where the training data comes as a se-quence of tasks with each including a couple of new classes at a time. Such training strategy allows the network to incre-mentally learn novel knowledge [28] and has become more prevalent in real-world applications.
†Corresponding authors.
Baseline
Prediction c
Dynamic 
Inference
Prediction (cid:2230)(cid:2176)
… (cid:2230)(cid:2779) (cid:2230)(cid:2778) (cid:2230)(cid:2176) … (cid:2230)(cid:2176)
… (cid:2230)(cid:2779) (cid:2230)(cid:2778) (cid:2230)(cid:2778) c
Concat
Gate
Similar
Dissimilar
Task distribution
Task ID: 2 (a) (b)
Figure 1. (a): Illustration of DER (Left) and our dynamic infer-(b): Step accuracy on CIFAR100-B0S10 ence method (Right). with DER under three different evaluation procedures: i) Given
Task ID: use t-th branch to infer the sample in t-th task ii) All branches: concatenate all branches like original DER; iii) Ran-dom Path: randomly select one branch for inference. incremental
Generally speaking, learning could be roughly divided into two categories, i.e., Task-IL vs. Class-IL [35]. It depends on whether the task ID is given during inference. For example, Task-IL could use this task ID in-formation to search prediction in a narrowed label space and is considered to be easier than Class-IL. Consequently, the accuracy of Task-IL is always outperforming Class-IL, and we can consider task ID as strong prior knowledge for in-cremental learning.
That motivates us to think about a critical problem, can we utilize task ID to improve the performance of Class-IL approaches? To answer this question, we select Class-IL
SOTA method DER [39] as our baseline shown in the left of Fig. 1(a). Once a new task arrived, DER expands the cur-rent network with a new task-speciﬁc branch Φ. This multi-branch architecture allows us to conveniently adapt DER to a Task-IL approach. That is, when a couple of categories are trained in a speciﬁc task, a test sample belonging to this task would be sent to the corresponding branch for infer-ence. So we compare the results of DER by giving task ID or directly using the original inference strategy over DER
and show the step accuracy in Fig. 1(b). It is amazing that even though the training procedure is the same, the accuracy gap between Task-IL and Class-IL is signiﬁcantly large.
Based on this observation, we propose to design a gate network by automatically predicting task ID for class in-cremental inference, which is named dynamic inference in this paper. A straightforward solution to this aim is to treat the task ID prediction problem as a classiﬁcation task. But it’s rather difﬁcult since there is no explicit semantic rela-tionship between categories in the concept of task. So the data in one task has high variance and also lacks criteria to discriminate them.
To address the aforementioned issues, we propose a new multi-centroid task descriptor, by assuming data within a task can form multiple clusters. Their centers, i.e., cen-troids, and then optimized to representatively describe a task. By pulling every relevant sample-centroid pair while pushing others away, it ensures that there is at least one cen-troid close to the given sample, which enables our network to distinguish between tasks. To select relevant pairs, we use class prototypes as proxy and solve a linear sum assign-ment problem [21], i.e., bipartite matching problem such that for a given sample, we are able to ﬁnd its matched cen-troid based on the prototype-centroid matching results.
During inference, we compare each instance feature and the task descriptor and then ﬁnd the most relevant branch for inference. The whole framework (dynamic inference network) is trained independently of the baseline, which al-lows our approach to be ﬂexibly integrated into trained DER or other multi-branch models. We validate our approach on three commonly used benchmarks, including CIFAR-100,
ImageNet-100, and ImageNet-1000. The results demon-strate the effectiveness of our approach, which obtains state-of-the-art results.
The main contributions of our work are: 1) Based on the idea of Task-IL, we propose a standalone gate network to automatically predict task ID for class incremental in-ference. The prediction is efﬁcient, has no restrictions on test data, and is able to distinguish between tasks for large-scale Class-IL at the ﬁrst time. 2) We propose a trainable multi-centroid task descriptor to describe the complicated task distribution. To make this descriptor representative, we solve a prototype-centroid bipartite matching problem to se-lect relevant sample-centroid pairs for optimization. 3) Ex-tensive experiments on large-scale benchmark CIFAR-100,
ImageNet-100 and ImageNet-1000 demonstrate the superi-ority of our approach compared with the state-of-the-art. 2.