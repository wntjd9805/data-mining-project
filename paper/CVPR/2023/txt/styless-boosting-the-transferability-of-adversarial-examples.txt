Abstract
Adversarial attacks can mislead deep neural networks (DNNs) by adding imperceptible perturbations to benign examples. The attack transferability enables adversarial examples to attack black-box DNNs with unknown archi-tectures or parameters, which poses threats to many real-world applications. We ﬁnd that existing transferable at-tacks do not distinguish between style and content features during optimization, limiting their attack transferability. To improve attack transferability, we propose a novel attack method called style-less perturbation (StyLess). Speciﬁ-cally, instead of using a vanilla network as the surrogate model, we advocate using stylized networks, which encode different style features by perturbing an adaptive instance normalization. Our method can prevent adversarial exam-ples from using non-robust style features and help gener-ate transferable perturbations. Comprehensive experiments show that our method can signiﬁcantly improve the transfer-ability of adversarial examples. Furthermore, our approach is generic and can outperform state-of-the-art transferable attacks when combined with other attack techniques. 1 1.

Introduction
Deep neural networks (DNNs) [14, 24] are currently ef-fective methods for solving various challenging tasks such as computer vision, and natural language processing. Al-though DNNs have amazing accuracy, especially for com-puter vision tasks such as image classiﬁcation, they are also known to be vulnerable to adversarial examples [12, 43].
Adversarial examples are malicious images obtained by adding imperceptible perturbations to benign images. No-tably, the transferability of adversarial examples is an in-triguing phenomenon, which refers to the property that the same adversarial example can successfully attack different black-box DNNs [5, 30, 34, 51].
It has been observed that image style can be decoupled from image content, and style transfer techniques allow us
*Corresponding author. 1Our code is available at https://github.com/uhiu/StyLess
Figure 1. An overview of our StyLess attack. We create stylized model ¯F by injecting synthesized style features into the surrogate model (F = F2 ◦ F1) using an adaptive IN layer. StyLess reduces the use of non-robust style features in the vanilla surrogate model
F , ultimately improving attack transferability. to generate stylized images based on arbitrary style im-Image style refers to the unique visual char-ages [18]. acteristics of an image, including its colors, textures, and lighting. For instance, two photos of the same object taken by different photographers can have very different styles.
Robust DNNs should rely more on content features of data than style features. This inspired us to improve attack trans-ferability from the perspective of avoiding non-robust fea-tures. We believe that style features of DNNs are non-robust for building transferable attacks. However, existing attacks do not distinguish between the surrogate models’ style and content features, which may reduce attack transferability.
We propose using stylized surrogate models to control style features, which can signiﬁcantly improve transferabil-ity. We refer to the original surrogate model as the “vanilla model.” The proposed stylized model is created by adding an adaptive instance normalization (IN) layer to the vanilla
model. By adjusting the parameters of the inserted IN layer, we can easily transform the style features of the surro-gate models. To compare the stylized and vanilla surrogate models, we analyzed their network losses during optimiza-tion. Surprisingly, we found that the adversarial loss of the vanilla model increased much faster than those of the styl-ized models, resulting in a widening loss gap. This phe-nomenon reveals that as the attack iteration progresses, cur-rent attack methods only focus on maximizing the loss of the vanilla model, leading to increased use of the style fea-tures of the vanilla surrogate model. However, we believe that style features are non-robust for transferable attacks, and relying too much on them may reduce attack transfer-ability. To enhance transferability, we aim to limit the use of non-robust style features and close the loss gap.
Based on the above ﬁndings, we propose a novel method called StyLess to improve the transferability of adversarial examples. Our method uses multiple synthesized style fea-tures to compete with the original style features during the iterative optimization of attack. The process is illustrated in
Figure 1. We encode various synthesized style features into a surrogate model via an IN layer to achieve stylized sur-rogate models. Instead of using only the vanilla surrogate model, we use the gradients of both the stylized surrogate models and the vanilla one to update adversarial examples.
The front part of the surrogate model works as a style en-coder, and the IN layer simulates synthesized style features.
Although we can use a decoder to explicitly generate the
ﬁnal stylized samples, it is unnecessary for the proposed at-tack method. Experimental results demonstrate that StyLess can enhance the transferability of state-of-the-art adversar-ial attacks on both unsecured and secured black-box DNNs.
Our main contributions are summarized as follows:
• We introduce a novel perspective for interpreting at-tack transferability: the original style features may hin-der transferability. We verify that current iterative at-tacks increasingly use the style features of the surro-gate model during the optimization process.
• We propose a novel attack called StyLess to enhance transferability by minimizing the use of original style features. To achieve this, we insert an IN layer to cre-ate stylized surrogate models and use gradients from both stylized and vanilla models.
• We conducted comprehensive experiments on various black-box DNNs to demonstrate that StyLess can sig-niﬁcantly improve attack transferability. Furthermore, we show that StyLess is a generic approach that can be combined with existing attack techniques. 2.