Abstract 1.

Introduction
We present a method for reconstructing articulated 3D models from videos in real-time, without test-time optimiza-tion or manual 3D supervision at training time. Prior work often relies on pre-built deformable models (e.g.
SMAL/SMPL), or slow per-scene optimization through dif-ferentiable rendering (e.g. dynamic NeRFs). Such methods fail to support arbitrary object categories, or are unsuit-able for real-time applications. To address the challenge of collecting large-scale 3D training data for arbitrary de-formable object categories, our key insight is to use off-the-shelf video-based dynamic NeRFs as 3D supervision to train a fast feed-forward network, turning 3D shape and motion prediction into a supervised distillation task. Our temporal-aware network uses articulated bones and blend skinning to represent arbitrary deformations, and is self-supervised on video datasets without requiring 3D shapes or viewpoints as input. Through distillation, our network learns to 3D-reconstruct unseen articulated objects at in-teractive frame rates. Our method yields higher-fidelity 3D reconstructions than prior real-time methods for animals, with the ability to render realistic images at novel view-points and poses.
We are interested in building high-quality animatable models of articulated 3D objects from videos in real time.
One promising application is virtual and augmented real-ity, where the goal is to create high-fidelity 3D experiences from images and videos captured live by users. For rigid scenes, structure from motion (SfM) and neural rendering can be used to build accurate 3D cities and landmarks from
Internet image collections [1, 20, 33]. For articulated ob-jects such as friends and pets, many works parameterize the range of motions using category-specific templates such as
SMPL [18] for humans and SMAL [4] for quadruped ani-mals. Although these methods can be trained on large-scale video datasets, they rely on parametric body template mod-els built from extensive real-world 3D scans: these body models are not easy to generate for diverse categories in the wild such as clothed humans or pets with distinct morpholo-gies, which are often the focus of user content.
Inspired by the breakthrough success of neural radiance fields [21], many works reconstruct arbitrary articulated ob-jects in an analysis-by-synthesis framework [16, 27, 28, 30, 36, 44] by defining time-dependent 3D warping fields and establishing long-range correspondences on top of canon-ical shape and appearance models. These methods out-put high-quality reconstructions of arbitrary objects without 3D data or pre-defined templates, but the output represen-tations are scene-specific and often require hours to com-pute from scratch on unseen videos - an unacceptable cost for real-time VR/AR tasks. We are thus interested in dy-namic 3D reconstruction algorithms that achieve the best of both worlds: the speed of template-based models and the quality and generalization ability of dynamic NeRFs.
To achieve this, our key insight is remarkably simple: we train category-specific feed-forward 3D predictors at scale by self-supervising them with dynamic NeRF “teachers” fit-ted to offline video data.
By leveraging scene-fitted dynamic NeRFs for 3D su-pervision at scale, our method learns a feed-forward pre-dictor for appearance, 3D shape, and articulations of non-rigid objects from videos. Our learned 3D models use lin-ear blend skinning to express articulations, allowing it to be animated by manipulating bone transformations. We ad-dress three key challenges in our work: (1) how to super-vise feed-forward models with internal representations of dynamic NeRFs, (2) how to produce temporally consistent predictions of pose, articulation, and appearance, and (3) how to build efficient systems for real-time reconstruction. 2.