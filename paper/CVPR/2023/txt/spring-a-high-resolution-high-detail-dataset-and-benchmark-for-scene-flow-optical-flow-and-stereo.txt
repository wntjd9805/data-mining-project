Abstract
Rendered Image
While recent methods for motion and stereo estimation recover an unprecedented amount of details, such highly detailed structures are neither adequately reflected in the data of existing benchmarks nor their evaluation methodol-ogy. Hence, we introduce Spring – a large, high-resolution, high-detail, computer-generated benchmark for scene flow, optical flow, and stereo. Based on rendered scenes from the open-source Blender movie “Spring”, it provides photo-realistic HD datasets with state-of-the-art visual effects and ground truth training data. Furthermore, we provide a web-site to upload, analyze and compare results. Using a novel evaluation methodology based on a super-resolved UHD ground truth, our Spring benchmark can assess the quality of fine structures and provides further detailed performance statistics on different image regions. Regarding the num-ber of ground truth frames, Spring is 60× larger than the only scene flow benchmark, KITTI 2015, and 15× larger than the well-established MPI Sintel optical flow bench-mark. Initial results for recent methods on our benchmark show that estimating fine details is indeed challenging, as their accuracy leaves significant room for improvement.
The Spring benchmark and the corresponding datasets are available at http://spring-benchmark.org.
Disparity 1920×1080
Zoom 3.6×
Optical Flow 3840×2160
Zoom 7.2× 3840×2160
Zoom 5.4×
Figure 1. Illustration of the high amount of details in the Spring dataset. The dataset consists of HD images with super-resolved
UHD ground truth for disparities and optical flow. 1.

Introduction
The estimation of dense correspondences in terms of scene flow, optical flow and disparity is the basis for nu-merous tasks in computer vision. Amongst others, such tasks include action recognition, driver assistance, robot navigation, visual odometry, medical image registration, video processing, stereo reconstruction and structure-from-motion. Given this multitude of applications and their fun-damental importance, datasets and benchmarks that allow quantitative evaluations have ever since driven the improve-ment of dense matching methods. The introduction of suit-able datasets and benchmarks did not only enable the com-parison and analysis of novel methods, but also triggered the transition from classical discrete [12, 21, 47] and con-tinuous [3, 4, 13, 15, 31] optimization frameworks to cur-rent learning based approaches relying on neural networks
[7, 8, 25, 42, 45, 46, 53]. The available benchmarks focus on distinct aspects like automotive scenarios [10, 22, 30, 34], differing complexity of motion [1, 2, 5] or (un)controlled il-lumination [35, 38]. However, none of these benchmarks provides a combination of high-quality data and a large number of frames, to assess a method’s quality in regions with fine details and to simultaneously satisfy the training needs of current neural networks. Furthermore, with KITTI 2015 [30], only a single benchmark that goes back to the
Table 1. Overview over recent datasets and benchmarks (BM). Where applicable, we report available image pairs and ground truth frames for motion estimation, i.e. scene flow (SF) or optical flow (OF), and for disparity estimation, i.e. stereo (ST), separately.
#image pairs
#gt frames
#pix scenes source ph.realism motion
Venue
Spring (ours)
CVPR ’23
CVPR ’15
KITTI 2015 [30]
FlyingThings3D [28] CVPR ’16 arXiv ’20
VKITTI 2 [6]
CVPR ’16
Monkaa [28]
CVPR ’16
Driving [28]
CVPR ’12
KITTI 2012 [10]
ECCV ’12
MPI Sintel [5]
CVPRW ’16
HD1K [22]
ICCV ’17
VIPER [34]
IJCV ’11
Middlebury-OF [2]
IJCV ’20
Human OF [33]
CVPR ’21
AutoFlow [41]
ICCV ’15
FlyingChairs [8]
VKITTI [9]
CVPR ’16
Middlebury-ST [35] GCPR ’14
CVPR ’17
ETH3D [38]
SF OF ST BM
✓ ✓ ✓ ✓
✓ ✓ ✓ ✓
✓ ✓ ✓ ✗
✓ ✓ ✓ ✗
✓ ✓ ✓ ✗
✓ ✓ ✓ ✗ 5953 400 6000 400 24084‡ 26760‡ 21210 21260 8640‡ 8664‡ 4392‡ 4400‡ 400 23812 12000 400 96336 53520 84840 42520 34560 17328 8800 17568 2.1M 47 0.5M n/a 0.5M 2676 0.5M 5 0.5M 8 0.5M 1
CGI real
CGI
CGI
CGI
CGI
✗ ✓ ✓ ✓ 389 389
✗ ✓ (✓)∗ ✓ 1593‡ 1064‡
✗ ✓ ✗ (✓)† n/a 1074
✗ ✓ ✗ ✓ 186285 n/a
✗ ✓ ✗ ✓ n/a 16
✗
✗ ✓ ✗ n/a 238900
✗
✗ ✓ ✗ n/a 40000
✗
✗ ✓ ✗ n/a 22872
✗ ✓ ✗
✗ n/a 21210
✗ ✗ ✓ ✓ 33 n/a
✗ ✗ ✓ ✓ 47 n/a 389 1593 1074 372570 16 238900 40000 22872 21210 n/a n/a 389 1064 n/a n/a n/a n/a n/a n/a n/a 66 47 real
CGI real
CGI 0.5M n/a 0.4M 35 2.8M 63 2.1M 184 0.2M 16 HT/CGI 0.4M 18432 0.3M n/a 0.2M n/a 0.5M 5 5.6M 33 0.4M 11
CGI
CGI
CGI
CGI real real high high low med. low med. high high high high med. med. low low low high high realistic automotive random automotive random automotive automotive realistic automotive automotive small rand./human random random automotive n/a n/a
HT: hidden texture, ‡: available in clean and final, ∗: not part of the benchmark, †: offline pre-deep-learning era is available for image-based scene flow, which currently prevents the development of well-generalizing methods due to lacking dataset variability.
Contributions. To tackle these challenges, we propose the
Spring dataset and benchmark, providing a large number of high-quality and high-resolution frames and ground truths to enable the development of even more accurate methods for scene flow, optical flow and stereo estimation. With
Spring, we complement existing benchmarks through a fo-cus on high-detail data, while we simultaneously broaden the number of available datasets for the development of well-generalizing methods across data with varying prop-erties. The latter aspect is particularly valuable for image-based scene flow methods. There, we provide the first benchmark with high-resolution, dense ground truth data in the literature. In summary, our contributions are fourfold: (i) New dataset: Based on the open-source Blender movie “Spring”, we rendered 6000 stereo image pairs from 47 sequences with state-of-the-art visual effects in HD resolution (1920×1080px). For those image pairs, we extracted ground truth from Blender in for-ward and backward direction, both in space and time, amounting to 12000 ground truth frames for stereo and 23812 ground truth frames for motion – 60× more than KITTI and 15× more than MPI Sintel. (ii) High-detail evaluation methodology: To adequately assess small details at a pixel level, we propose a novel evaluation methodology that relies on an even higher resolved ground truth. All ground truth frames are computed in UHD resolution (3840×2160px). (iii) Benchmark: We set up a public benchmark website to upload, analyze and compare novel methods.
It provides several widely used error measures and ad-ditionally analyzes the results in different types of regions, including high-detail, unmatched, non-rigid, sky and large-displacement areas. (iv) Baselines: We evaluated 15 state-of-the-art methods (8 optical flow, 4 stereo, 3 scene flow) as non-fine-tuned baselines. Results not only show that small de-tails still pose a problem to recent methods, but also hint at significant potential improvements in all tasks. 2.