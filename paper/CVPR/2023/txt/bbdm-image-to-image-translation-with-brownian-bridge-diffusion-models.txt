Abstract
Image-to-image translation is an important and chal-lenging problem in computer vision and image process-ing. Diffusion models (DM) have shown great potentials for high-quality image synthesis, and have gained competi-tive performance on the task of image-to-image translation.
However, most of the existing diffusion models treat image-to-image translation as conditional generation processes, and suffer heavily from the gap between distinct domains.
In this paper, a novel image-to-image translation method based on the Brownian Bridge Diffusion Model (BBDM) is proposed, which models image-to-image translation as a stochastic Brownian Bridge process, and learns the trans-lation between two domains directly through the bidirec-tional diffusion process rather than a conditional gener-ation process. To the best of our knowledge, it is the first work that proposes Brownian Bridge diffusion process for image-to-image translation. Experimental results on various benchmarks demonstrate that the proposed BBDM model achieves competitive performance through both vi-sual inspection and measurable metrics. 1.

Introduction
Image-to-image translation [14] refers to building a map-ping between two distinct image domains. Numerous prob-lems in computer vision and graphics can be formulated as image-to-image translation problems, such as style trans-fer [3,9,13,22], semantic image synthesis [21,24,34,36,37, 40] and sketch-to-photo synthesis [2, 14, 43].
A natural approach to image-to-image translation is to learn the conditional distribution of the target images given the samples from the input domain. Pix2Pix [14] is one of the most popular image-to-image translation methods.
It is a typical conditional Generative Adversarial Network (GAN) [26], and the domain translation is accomplished by learning a mapping from the input image to the output im-Figure 1. Comparison of directed graphical models of BBDM (Brownian Bridge Diffusion Model) and DDPM (Denoising Dif-fusion Probabilistic Model). age. In addition, a specific adversarial loss function is also trained to constrain the domain mapping. Despite the high fidelity translation performance, they are notoriously hard to train [1, 10] and often drop modes in the output distri-bution [23, 27].
In addition, most GAN-based image-to-image translation methods also suffer from the lack of di-verse translation results since they typically model the task as a one-to-one mapping. Although other generative models such as Autoregressive Models [25, 39], VAEs (Variational
Autoencoders) [16, 38], and Normalizing Flows [7, 15] suc-ceeded in some specific applications, they have not gained the same level of sample quality and general applicability as GANs.
Recently, diffusion models [12, 31] have shown compet-itive performance on producing high-quality images com-pared with GAN-based models [6]. Several conditional dif-fusion models [2, 4, 28–30] have been proposed for image-to-image translation tasks. These methods treat image-to-image translation as conditional image generation by in-tegrating the encoded feature of the reference image into the U-Net in the reverse process (the first row of Figure 1) to guide the diffusion towards the target domain. De-Figure 2. Architecture of our proposed Brownian Bridge Diffusion Model (BBDM). spite some practical success, the above condition mecha-nism does not have a clear theoretical guarantee that the final diffusion result yields the desired conditional distri-bution. Therefore, most of the conditional diffusion mod-els suffer from poor model generalization, and can only be adapted to some specific applications where the conditional input has high similarity with the output, such as inpaint-ing and super-resolution [2, 4, 30]. Although LDM (Latent
Diffusion Model) [28] improved the model generalization by conducting diffusion process in the latent space of cer-tain pre-trained models, it is still a conditional generation process and the multi-modal condition is projected and en-tangled via a complex attention mechanism which makes
LDM much more difficult to get such a theoretical guaran-tee. Meanwhile, the performance of LDM differs greatly across different levels of latent features showing instability.
In this paper, we propose a novel image-to-image trans-lation framework based on Brownian Bridge diffusion pro-cess. Compared with the existing diffusion methods, the proposed method directly builds the mapping between the input and the output domains through a Brownian Bridge stochastic process, rather than a conditional generation pro-cess. In order to speed up the training and inference pro-cess, we conduct the diffusion process in the same latent space as used in LDM [28]. However, the proposed method differs from LDM inherently in the way the mapping be-tween two image domains is modeled. The framework of
BBDM is shown in the second row of Figure 1. It is easy to find that the reference image y sampled from domain B is only set as the initial point xT = y of the reverse diffu-sion, and it will not be utilized as a conditional input in the prediction network µθ(xt, t) at each step as done in related works [2, 4, 28, 30]. The main contributions of this paper include: 1. A novel image-to-image translation method based on
Brownian Bridge diffusion process is proposed in this paper. As far as we know, it is the first work of Brow-nian Bridge diffusion process proposed for image-to-image translation. 2. The proposed method models image-to-image trans-lation as a stochastic Brownian Bridge process, and learns the translation between two domains directly
The through the bidirectional diffusion process. proposed method avoids the conditional information leverage existing in related work with conditional dif-fusion models. 3. Quantitative and qualitative experiments demonstrate the proposed BBDM method achieves competitive per-formance on various image-to-image translation tasks. 2.