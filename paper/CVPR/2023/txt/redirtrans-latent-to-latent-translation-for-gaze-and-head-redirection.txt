Abstract
Learning-based gaze estimation methods require large amounts of training data with accurate gaze annotations.
Facing such demanding requirements of gaze data collec-tion and annotation, several image synthesis methods were proposed, which successfully redirected gaze directions pre-cisely given the assigned conditions. However, these meth-ods focused on changing gaze directions of the images that only include eyes or restricted ranges of faces with low res-olution (less than 128 × 128) to largely reduce interference from other attributes such as hairs, which limits applica-tion scenarios. To cope with this limitation, we proposed a portable network, called ReDirTrans, achieving latent-to-latent translation for redirecting gaze directions and head orientations in an interpretable manner. ReDirTrans projects input latent vectors into aimed-attribute embed-dings only and redirects these embeddings with assigned pitch and yaw values. Then both the initial and edited embeddings are projected back (deprojected) to the initial latent space as residuals to modify the input latent vec-tors by subtraction and addition, representing old status re-moval and new status addition. The projection of aimed at-tributes only and subtraction-addition operations for status replacement essentially mitigate impacts on other attributes and the distribution of latent vectors. Thus, by combining
ReDirTrans with a pretrained ﬁxed e4e-StyleGAN pair, we created ReDirTrans-GAN, which enables accurately redi-recting gaze in full-face images with 1024×1024 resolution while preserving other attributes such as identity, expres-sion, and hairstyle. Furthermore, we presented improve-ments for the downstream learning-based gaze estimation task, using redirected samples as dataset augmentation. 1.

Introduction
Gaze is a crucial non-verbal cue that conveys attention and awareness in interactions. Its potential applications in-clude mental health assessment [5,18], social attitudes anal-ysis [19], human-computer interaction [12], automotive as-sistance [30], AR/VR [6,34]. However, developing a robust uniﬁed learning-based gaze estimation model requires large amounts of data from multiple subjects with precise gaze annotations [42, 44]. Collecting and annotating such an ap-propriate dataset is complex and expensive. To overcome this challenge, several methods have been proposed to redi-rect gaze directions [17, 39, 41, 42, 44] in real images with assigned directional values to obtain and augment train-ing data. Some works focused on generating eye images with new gaze directions by either 1) estimating warping maps [41,42] to interpolate pixel values or 2) using encoder-generator pairs to generate redirected eye images [17, 39].
ST-ED [44] was the ﬁrst work to extend high-accuracy gaze redirection from eye images to face images. By dis-entangling several attributes, including person-speciﬁc ap-pearance, it can explicitly control gaze directions and head orientations. However, due to the design of the encoder-decoder structure and limited ability to maintain appearance features by a 1 × 1024 projected appearance embedding,
ST-ED generates low-resolution (128 × 128) images with restricted face range (no hair area), which narrows the ap-plication ranges and scenarios of gaze redirection.
As for latent space manipulation for face editing tasks, large amounts of works [2–4, 14, 31, 35] were proposed to modify latent vectors in predeﬁned latent spaces (W [21],
W + [1] and S [40]). Latent vectors in these latent spaces can work with StyleGAN [21, 22] to generate high-quality and high-ﬁdelity face images with desired attribute editing.
Among these methods, Wu et.al [40] proposed the latent space S working with StyleGAN, which achieved only one degree-of-freedom gaze redirection by modifying a certain channel of latent vectors in S by an uninterpreted value in-stead of pitch and yaw values of gaze directions.
Considering these, we proposed a new method, called
ReDirTrans, to achieve latent-to-latent translation for redi-recting gaze directions and head orientations in high-resolution full-face images based on assigned directional values. Speciﬁcally, we designed a framework to project input latent vectors from a latent space into the aimed-attribute-only embedding space for an interpretable redirec-tion process. This embedding space consists of estimated pseudo conditions and embeddings of aimed attributes,
where conditions describe deviations from the canonical status and embeddings are the ‘carriers’ of the conditions.
In this embedding space, all transformations are imple-mented by rotation matrices multiplication built from pitch and yaw values, which can make the redirection process more interpretable and consistent. After the redirection pro-cess, the original embeddings and redirected ones are both decoded back to the initial latent space as the residuals to modify the input latent vectors by subtraction and addition operations. These operations represent removing the old state and adding a new one, respectively. ReDirTrans only focuses on transforming embeddings of aimed attributes and achieves status replacement by the residuals outputted from weight-sharing deprojectors. ReDirTrans does not project or deproject other attributes with information loss; and it does not affect the distribution of input latent vec-tors. Thus ReDirTrans can also work in a predeﬁned feature space with a ﬁxed pretrained encoder-generator pair for the redirection task in desired-resolution images.
In summary, our contributions are as follows:
• A latent-to-latent framework, ReDirTrans, which projects latent vectors to an embedding space for an interpretable redirection process on aimed attributes and maintains other attributes, including appearance, in initial latent space with no information loss caused by projection-deprojection processes.
• A portable framework that can seamlessly integrate into a pretrained GAN inversion pipeline for high-accuracy redirection of gaze directions and head ori-entations, without the need for any parameter tuning of the encoder-generator pairs.
• A layer-wise architecture with learnable parameters that works with the ﬁxed pretrained StyleGAN and achieves redirection tasks in high-resolution full-face images through ReDirTrans-GAN. 2.