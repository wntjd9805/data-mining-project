Abstract
Generating talking face videos from audio attracts lots of research interest. A few person-specific methods can gener-ate vivid videos but require the target speaker’s videos for training or fine-tuning. Existing person-generic methods have difficulty in generating realistic and lip-synced videos while preserving identity information. To tackle this prob-lem, we propose a two-stage framework consisting of audio-to-landmark generation and landmark-to-video rendering procedures. First, we devise a novel Transformer-based landmark generator to infer lip and jaw landmarks from the audio. Prior landmark characteristics of the speaker’s face are employed to make the generated landmarks coincide with the facial outline of the speaker. Then, a video ren-dering model is built to translate the generated landmarks into face images. During this stage, prior appearance in-formation is extracted from the lower-half occluded target face and static reference images, which helps generate real-istic and identity-preserving visual content. For effectively exploring the prior information of static reference images, we align static reference images with the target face’s pose and expression based on motion fields. Moreover, auditory features are reused to guarantee that the generated face im-ages are well synchronized with the audio. Extensive ex-periments demonstrate that our method can produce more realistic, lip-synced, and identity-preserving videos than ex-isting person-generic talking face generation methods. 1.

Introduction
Audio-driven talking face video generation is valuable in a wide range of applications, such as visual dubbing
[19, 26, 37], digital assistants [32], and animation movies
[46]. Based on the training paradigm and data requirement,
*Corresponding author is Guanbin Li.
Figure 1. This paper is targeted at generating a talking face video for a speaker which is coherent with input audio. We implement this task by completing the lower-half face of the speaker’s origi-nal video. The outline of the mouth and jaw is inferred from the input audio and then used to guide the video completion process.
Moreover, multiple static reference images are used to supply prior appearance information. the talking face generation methods can generally be cate-gorized as person-specific or person-generic types. Person-specific methods [11, 20, 21, 28, 32, 41] can generate photo-realistic talking face videos but need to be re-trained or fine-tuned with the target speaker’s videos, which might be in-accessible in some real-world scenarios. Hence, learning to generate person-generic talking face videos is a more sig-nificant and challenging problem in this field. This topic also attracts lots of research attention [14, 19, 24, 26, 37, 45].
In this paper, we focus on tackling the person-generic talk-ing face video generation by completing the lower-half face of the speaker’s original video under the guidance of audio data and multiple reference images, as shown in Figure 1.
The main challenges of the person-generic talking face video generation include two folds: 1) How can the model generate videos having facial motions, especially mouth and jaw motions, which are coherent with the input au-dio? 2) How can the model produce visually realistic frames while preserving the identity information? To address the first problem, many methods [3, 5, 6, 17, 37, 46] leverage fa-cial landmarks as intermediate representation when gener-ating person-generic talking face videos. However, trans-lation from audio to facial landmarks is an ambiguous task, considering the same pronunciation may correspond to mul-tiple facial shapes. A few landmark-based talking face gen-eration methods [3, 42] tend to produce results having the averaged lip shape of training samples, which may have remarkable differences with the lip shape of the speaker.
A line of methods [6, 37] incorporate the prior information from a reference image’s landmarks to generate landmarks consistent with the speaker’s shape. However, they directly fuse the features of audio and landmarks with simple con-catenation or addition operations without modeling the un-derlying correlation between them. For example, the rela-tions between the reference landmarks and the audio clips from different time intervals are different. Those meth-ods are not advantageous at capturing such kinds of dif-ferences. Moreover, the temporal dependencies are also valuable for predicting facial landmarks. Existing methods, such as [6, 17, 38, 46], depend on long short-term memory (LSTM) models to explore the temporal dependencies when transforming audio clips to landmark sequences. However, those models are limited in capturing long-range temporal relationships.
Since the input audio and intermediate landmarks do not contain visual content information intrinsically, it is very challenging to hallucinate realistic facial videos from audio and intermediate landmarks while preserving the identity information. A few existing methods, such as [37,46], adopt a single static reference image to supply visual appearance and identity information. However, one static reference im-age is insufficient to cover all facial details, e.g., the teeth and the side content of cheeks. This makes these algorithms struggle to synthesize unseen details, which is unreliable and easily leads to generation artifacts. [3, 7] use multiple reference images to provide more abundant details. How-ever, they simply concatenate the reference images without spatial alignment, which is limited in extracting meaningful features from reference images.
To cope with the above problems, we devise a novel two-stage framework composed of an audio-to-landmark gen-erator and a landmark-to-video rendering network. The goal of our framework is to complete the lower-half face of the video with content coherent to the phonetic motions of the audio. Specifically, we use pose prior landmarks of the upper-half face and reference landmarks extracted from static face images as extra inputs of the audio-to-landmark generator. The access to the two kinds of landmarks helps to prevent the generator from producing results that devi-ate from the face outline of the speaker. Then, we build up the network architecture of the generator based on the multi-head self-attention modules [33]. Our design is more advantageous at capturing relationships between phonetic units and landmarks compared to simple concatenation or addition operations [6, 37]. It is also more helpful for mod-eling temporal dependencies than LSTM used in previous methods [6, 17, 38, 46]. Additionally, multiple static face images are referred to extract prior appearance information for generating realistic and identity-preserving face frames.
Inspired by [10], we set up the landmark-to-video rendering network with a motion field based alignment module and a face image translation module. The alignment module is targeted at registering static reference images with the face pose and expression delivered by the results of the landmark generator. This target is achieved by inferring a motion field for each static reference image and then warping the image and its features. The alignment module can decrease the difficulty in translating meaningful features of static refer-ence images to the target image. The face image translation module produces the final face images by combining multi-source features from the inferred landmarks, the occluded original images, the registered reference images, and the audio. The inferred landmarks provide vital clues for con-straining the facial pose and expression. Those images are paramount for inferring the facial appearance. Besides, the audio features are reused to guarantee that the generated lip shapes are well synchronized with the audio. Extensive ex-periments demonstrate that our method produces more re-alistic and lip-synced talking face videos and preserves the identity information better than existing methods. Our main contributions are summarized as follows:
• We propose a two-stage framework composed of an audio-to-landmark generator and a landmark-to-video rendering model to address the person-generic talking face generation task under the guidance of prior land-mark and appearance information.
• We devise an audio-to-landmark generator that can ef-fectively fuse prior landmark information with the au-dio features. We also make an early effort to construct the generator with multi-head self-attention modules.
• We design a landmark-to-video rendering model which can make full use of multiple source signals, including prior visual appearance information, land-marks, and auditory features.
• Extensive experiments are conducted on LRS2 [1] and
LRS3 [2] dataset, demonstrating the superiority of our method over existing methods in terms of realism, identity preservation, and lip synchronization. 2.