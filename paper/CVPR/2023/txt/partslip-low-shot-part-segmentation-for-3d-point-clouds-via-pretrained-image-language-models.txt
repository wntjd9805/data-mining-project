Abstract
Generalizable 3D part segmentation is important but challenging in vision and robotics. Training deep models via conventional supervised methods requires large-scale 3D datasets with fine-grained part annotations, which are costly to collect. This paper explores an alternative way for low-shot part segmentation of 3D point clouds by lever-aging a pretrained image-language model, GLIP, which achieves superior performance on open-vocabulary 2D de-tection. We transfer the rich knowledge from 2D to 3D through GLIP-based part detection on point cloud ren-dering and a novel 2D-to-3D label lifting algorithm. We also utilize multi-view 3D priors and few-shot prompt tun-ing to boost performance significantly. Extensive evalua-tion on PartNet and PartNet-Mobility datasets shows that our method enables excellent zero-shot 3D part segmenta-tion. Our few-shot version not only outperforms existing few-shot approaches by a large margin but also achieves highly competitive results compared to the fully super-vised counterpart. Furthermore, we demonstrate that our method can be directly applied to iPhone-scanned point clouds without significant domain gaps. 1.

Introduction
Human visual perception can parse objects into parts and generalize to unseen objects, which is crucial for under-*Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc. standing their structure, semantics, mobility, and function-ality. 3D part segmentation plays a critical role in empower-ing machines with such ability and facilitates a wide range of applications, such as robotic manipulation, AR/VR, and shape analysis and synthesis [2, 31, 39, 69].
Recent part-annotated 3D shape datasets [40,67,72] have promoted advances in designing various data-driven ap-proaches for 3D part segmentation [34, 44, 65, 73]. While standard supervised training enables these methods to achieve remarkable results, they often struggle with out-of-distribution test shapes (e.g., unseen classes). How-ever, compared to image datasets, these 3D part-annotated datasets are still orders of magnitude smaller in scale, since building 3D models and annotating fine-grained 3D object parts are laborious and time-consuming. It is thus challeng-ing to provide sufficient training data covering all object
[40] categories. For example, the recent PartNet dataset contains only 24 object categories, far less than what an in-telligent agent would encounter in the real world.
To design a generalizable 3D part segmentation mod-ule, many recent works have focused on the few-shot set-ting, assuming only a few 3D shapes of each category dur-ing training. They design various strategies to learn better representations, and complement vanilla supervised learn-ing [33, 53, 54, 60, 80]. While they show improvements over the original pipeline, there is still a large gap between what these models can do and what downstream applica-tions need. The problem of generalizable 3D part segmen-tation is still far from being solved. Another parallel line of work focuses on learning the concept of universal object parts and decomposing a 3D shape into a set of (hierarchi-cal) fine-grained parts [37, 64, 74]. However, these works do not consider the semantic labeling of parts and may be limited in practical use.
In this paper, we seek to solve the low-shot (zero- and few-shot) 3D part segmentation problem by leveraging pre-trained image-language models, inspired by their recent striking performances in low-shot learning. By pretrain-ing on large-scale image-text pairs, image-language mod-els [1,22,29,45,46,50,76] learn a wide range of visual con-cepts and knowledge, which can be referenced by natural language. Thanks to their impressive zero-shot capabilities, they have already enabled a variety of 2D/3D vision and language tasks [10, 16, 20, 47, 49, 51, 77].
As shown in Figure 1, our method takes a 3D point cloud and a text prompt as input, and generates both 3D semantic and instance segmentations in a zero-shot or few-shot fash-ion. Specifically, we integrate the GLIP [29] model, which is pretrained on 2D visual grounding and detection tasks with over 27M image-text pairs and has a strong capabil-ity to recognize object parts. To connect our 3D input with the 2D GLIP model, we render multi-view 2D images for the point cloud, which are then fed into the GLIP model to-gether with a text prompt containing part names of interest.
The GLIP model then detects parts of interest for each 2D view and outputs detection results in the form of 2D bound-ing boxes. Since it is non-trivial to convert 2D boxes back to 3D, we propose a novel 3D voting and grouping module to fuse the multi-view 2D bounding boxes and generate 3D in-stance segmentation for the input point cloud. Also, the pre-trained GLIP model may not fully understand our definition of parts only through text prompts. We find that an effec-tive solution is prompt tuning with few-shot segmented 3D shapes. In prompt tuning, we learn an offset feature vector for the language embedding of each part name while fixing the parameters of the pretrained GLIP model. Moreover, we propose a multi-view visual feature aggregation module to fuse the information of multiple 2D views, so that the GLIP model can have a better global understanding of the input 3D shape instead of predicting bounding boxes from each isolated 2D view.
To better understand the generalizability of various ap-proaches and their performances in low-shot settings, we propose a benchmark PartNet-Ensembled (PartNetE) by in-corporating two existing datasets PartNet [40] and Part-NetMobility [67]. Through extensive evaluation on Part-NetE, we show that our method enables excellent zero-shot 3D part segmentation. With few-shot prompt tuning, our method not only outperforms existing few-shot approaches by a large margin but also achieves highly competitive per-formance compared to the fully supervised counterpart. We also demonstrate that our method can be directly applied to iPhone-scanned point clouds without significant domain gaps. In summary, our contributions mainly include:
• We introduce a novel 3D part segmentation method that leverages pretrained image-language models and achieves outstanding zero-shot and few-shot performance.
• We present a 3D voting and grouping module, which ef-fectively converts multi-view 2D bounding boxes into 3D semantic and instance segmentation.
• We utilize few-shot prompt tuning and multi-view feature aggregation to boost GLIP’s detection performance.
• We propose a benchmark PartNetE that benefits future work on low-shot and text-driven 3D part segmentation. 2.