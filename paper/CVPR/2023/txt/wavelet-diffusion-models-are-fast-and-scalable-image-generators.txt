Abstract
Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow train-ing and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent Diffu-sionGAN method significantly decreases the models’ run-ning time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag be-hind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffu-sion scheme. We extract low-and-high frequency compo-nents from both image and feature levels via wavelet decom-position and adaptively handle these components for faster processing while maintaining good generation quality. Fur-thermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Exper-imental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints are available at https:
//github.com/VinAIResearch/WaveDiff.git. 1.

Introduction
Despite being introduced recently, diffusion models have grown tremendously and drawn many research interests.
Such models revert the diffusion process to generate clean, high-quality outputs from random noise inputs. These tech-niques are applied in various data domains and applications but show the most remarkable success in image-generation tasks. Diffusion models can beat the state-of-the-art gen-erative adversarial networks (GANs) in generation quality on various datasets [4, 37]. More notably, diffusion mod-els provide a better mode coverage [14, 22, 40] and a flexi-ble way to handle different types of conditional inputs such as semantic maps, text, representations, and images [36].
†
Equal contribution.
Figure 1. Comparisons between our method and other GAN and diffusion techniques in terms of FID and sampling time the on
CIFAR-10 (32 × 32) dataset. Our method is 2.5× faster than
DDGAN [49], the fastest up-to-date diffusion method, and ap-proaches the real-time speed of StyleGAN methods [19, 20, 56] while still achieving comparable FID scores.
Thanks to this capability, they offer various applications such as text-to-image generation, image-to-image transla-tion, image inpainting, image restoration, and more. Recent diffusion-based text-to-image generative models [1, 34, 37] allow users to generate unbelievably realistic images just by text inputs, opening a new era of AI-based digital art and promising applications to various other domains.
While showing great potential, diffusion models have a very slow running speed, a critical weakness blocking them from being widely adopted like GANs. The foundation work Denoising Diffusion Probabilistic Models (DDPMs)
[13] requires a thousand sampling steps to produce the de-sired output quality, taking minutes to generate a single im-age. Many techniques have been proposed to reduce the inference time [25, 39], mainly via reducing the sampling steps. However, the fastest algorithm before Diffusion-GAN still takes seconds to produce a 32×32 image, which is about 100 times slower than GAN. DiffusionGAN [49] made a break-though in fastening inference speed by com-bining Diffusion and GANs in a single system, which ul-timately reduces the sampling steps to 4 and the inference time to generate a 32×32 image as a fraction of a second. It makes DiffusionGAN the fastest existing diffusion model.
Still, it is at least 4 times slower than the StyleGAN coun-terpart, and the speed gap consistently grows when increas-ing the output resolution. Moreover, DiffusionGAN still re-quires a long training time and a slow convergence, confirm-ing that diffusion models are not yet ready for large-scale or real-time applications.
This paper aims to bridge the speed gap by introducing a novel wavelet-based diffusion scheme. Our solution re-lies on discrete wavelet transform, which decomposes each input into four sub-bands for low- (LL) and high-frequency (LH, HL, HH) components. We apply that transform on both image and feature levels. This allows us to signifi-cantly reduce both training and inference times while keep-ing the output quality relatively unchanged. On the image level, we obtain a high speed boost by reducing the spatial resolution four times. On the feature level, we stress the im-portance of wavelet information on different blocks of the generator. With such a design, we can obtain considerable performance improvement while inducing only a marginal computing overhead.
Our proposed Wavelet Diffusion provides state-of-the-art training and inference speed while maintaining high generative quality, thoroughly confirmed via experiments on standard benchmarks including CIFAR-10, STL-10,
CelebA-HQ, and LSUN-Church. Our models significantly reduce the speed gap between diffusion models and GANs, targeting large-scale and real-time systems.
In summary, our contributions are as following:
• We propose a novel Wavelet Diffusion framework that takes advantage of the dimensional reduction of Wavelet subbands to accelerate Diffusion Models while maintaining good visual quality of generated re-sults through high-frequency components.
• We employ wavelet decomposition in both image and feature space to improve generative models’ robust-ness and execution speed.
• Our proposed Wavelet Diffusion provides state-of-the-art training and inference speed, which serves as a stepping-stone to facilitating real-time and high-fidelity diffusion models. 2.