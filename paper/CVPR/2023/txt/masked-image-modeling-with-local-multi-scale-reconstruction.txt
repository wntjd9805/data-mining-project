Abstract
Masked Image Modeling (MIM) achieves outstanding success in self-supervised representation learning. Unfortu-nately, MIM models typically have huge computational bur-den and slow learning process, which is an inevitable ob-stacle for their industrial applications. Although the lower layers play the key role in MIM, existing MIM models con-duct reconstruction task only at the top layer of encoder.
The lower layers are not explicitly guided and the interac-tion among their patches is only used for calculating new activations. Considering the reconstruction task requires non-trivial inter-patch interactions to reason target signals, we apply it to multiple local layers including lower and up-per layers. Further, since the multiple layers expect to learn the information of different scales, we design local multi-scale reconstruction, where the lower and upper layers re-construct fine-scale and coarse-scale supervision signals respectively. This design not only accelerates the represen-tation learning process by explicitly guiding multiple lay-ers, but also facilitates multi-scale semantical understand-ing to the input. Extensive experiments show that with sig-nificantly less pre-training burden, our model achieves com-parable or better performance on classification, detection and segmentation tasks than existing MIM models. Code is available with both MindSpore and PyTorch. 1.

Introduction
Recently, Masked Image Modeling (MIM) [2, 21, 50] achieves outstanding success in the field of self-supervised visual representation learning, which is inspired by the
Masked Language Modeling (MLM) [4, 29] in natural lan-guage processing and benefits from the development of vi-sion transformers [16, 33, 45]. MIM learns semantic repre-sentations by first masking some parts of the input and then predicting their signals based on the unmasked parts, e.g., normalized pixels [21,50], discrete tokens [2,15], HOG fea-ture [47], deep features [1, 54] or frequencies [32, 49].
*Corresponding author.
Despite superior performance on various downstream tasks, these models have huge computational burden and slow learning process [26]. They typically require thou-sands of GPU Hours for pre-training on ImageNet-1K to get generalizing representations. Since we expect to pre-train these models on more massive amount of unlabeled data (e.g., free Internet data) to obtain more generalizing representations in practice, the pre-training efficiency is an inevitable bottleneck limiting the industrial applications of MIM. How to accelerate the representation learning in
MIM is an important topic. To this end, MAE [21] pio-neered the asymmetric encoder-decoder strategy, where the costly encoder only operates few visible patches and the lightweight decoder takes all the patches as input for pre-diction. Further, GreenMIM [26] extends the asymmetric encoder-decoder strategy to hierarchical vision transform-ers (e.g., Swin [33]). Besides, [8, 19, 30] shrinks the input resolution to lessen the input patches, thereby reducing the computational burden. However, they all aim to accelerate the encoding process rather than the representation learning.
In MIM, the learning of upper layers depends on that of lower ones during pre-training, since the upper-layer fea-tures are calculated from the lower layers. Besides, during fine-tuning the upper layers are typically tuned quickly to adapt to the downstream task while the lower ones change more slowly and need to be well-learned [2, 25, 52]. Even fine-tuning only the several upper layers and freezing the others can obtain similar performance [21]. Therefore, the lower layers of encoder play the key role in MIM. However, all existing MIM models only conduct reconstruction task at the top layer of encoder and the lower ones are not explicitly guide, thus the interaction among their patches is only used for calculating the activations of the next layer. Considering the reconstruction task requires non-trivial inter-patch in-teractions to reason target signals, we apply it to both lower and upper layers to explicitly guide them and thus accelerate the overall learning process. Using tiny decoder is sufficient for each local reconstruction task and does not significantly increase the computational burden.
How to properly conduct reconstruction tasks at multi-ple local layers is a non-trivial problem. For example, ap-(a) ViT-B (b) Swin-B
Figure 1. Top-1 fine-tuning accuracy on ImageNet-1K vs. Pre-training duration. The duration is estimated on a machine with one Tesla
V100-32G GPU, CUDA 10.2 and PyTorch 1.8. ‘GPU Hours’ is the running time on single GPU. p × W plying the top-layer reconstruction task to carefully chosen local layers of ViT [16] can not achieve meaningful im-provement. In general, the lower layers exploit low-level information and the upper ones learn high-level informa-tion [18, 36], so it is not appropriate to use the supervision signals of same scale for multiple local reconstruction tasks.
Here ’scale’ is the spatial size of the supervision signals cal-culated from the divided input regions, e.g., the signals from the p × p regions in an input of H × W resolution has the scale of H p . The fine-scale and coarse-scale super-visions typically contain low-level and high-level informa-tion of the input respectively, and these multi-scale supervi-sions from input are widely ignored by existing MIM mod-els. To this end, we propose local multi-scale reconstruction where the lower and upper layers reconstruct fine-scale and coarse-scale supervisions respectively. This design not only accelerates the representation learning process, but also fa-cilitates multi-scale semantical understanding to the input.
When the decoded predictions have different scale with the supervisions (e.g., on ViT), we use the deconvolution/pool operations to rescale them. We also apply the asymmetric encoder-decoder strategy [21, 26] for quick encoding. Our model, dubbed as LocalMIM, are illustrated in Fig. 2 (a).
Overall, we summarize our contributions as follows.
• To the best of our knowledge, this is the first work in
MIM to conduct local reconstructions and use multi-scale supervisions from the input.
• Our model is architecture-agnostic and can be used in both columnar and pyramid architectures.
• From extensive experiments, we find that 1) Lo-calMIM is more efficient than existing MIM models, as shown in Fig. 1 and Table 1. For example, Lo-calMIM achieves the best MAE result with 3.1× ac-celeration on ViT-B and the best GreenMIM result with 6.4× acceleration on Swin-B. 2) In terms of top-1 fine-tuning accuracy on ImageNet-1K, LocalMIM achieves 84.0% using ViT-B and 84.1% using Swin-B with significantly less pre-training duration than ex-isting MIM models. The obtained representations also achieve better generalization on detection and segmen-tation downstream tasks, as shown in Table 2 and 3. 2.