Abstract
We propose a novel type of map for visual navigation, a renderable neural radiance map (RNR-Map), which is designed to contain the overall visual information of a 3D environment. The RNR-Map has a grid form and consists of latent codes at each pixel. These latent codes are embed-ded from image observations, and can be converted to the neural radiance field which enables image rendering given a camera pose. The recorded latent codes implicitly contain visual information about the environment, which makes the
RNR-Map visually descriptive. This visual information in
RNR-Map can be a useful guideline for visual localization and navigation. We develop localization and navigation frameworks that can effectively utilize the RNR-Map. We evaluate the proposed frameworks on camera tracking, vi-sual localization, and image-goal navigation. Experimental results show that the RNR-Map-based localization frame-work can find the target location based on a single query image with fast speed and competitive accuracy compared to other baselines. Also, this localization framework is robust to environmental changes, and even finds the most visually sim-ilar places when a query image from a different environment is given. The proposed navigation framework outperforms the existing image-goal navigation methods in difficult sce-narios, under odometry and actuation noises. The navigation framework shows 65.7% success rate in curved scenarios of the NRNS [21] dataset, which is an improvement of 18.6% over the current state-of-the-art. Project page: https:
//rllab-snu.github.io/projects/RNR-Map/ 1.

Introduction
In this paper, we address how to explicitly embed the vi-sual information from a 3D environment into a grid form and how to use it for visual navigation. We present renderable neural radiance map (RNR-Map), a novel type of a grid map for navigation.We point out three main properties of RNR-Map which make RNR-Map navigation-friendly. First, it is
*This work was supported by Institute of Information & Communica-tions Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-01190, [SW Star Lab] Robot Learning:
Efficient, Safe, and Socially-Acceptable Machine Learning). (Correspond-ing author: Songhwai Oh) visually descriptive. Commonly used grid-based maps such as occupancy maps [10,11,17] and semantic maps [9,19,36], record obstacle information or object information into grids.
In contrast, RNR-Map converts image observations to latent codes which are then embedded in grid cells. Each latent code in a grid cell can be converted to a neural radiance field, which can render the corresponding region. We can utilize the implicit visual information of these latent codes to understand and reason about the observed environment.
For example, we can locate places based on an image or determine which region is the most related to a given im-age. RNR-Map enables image-based localization only with a simple forward pass in a neural network, by directly uti-lizing the latent codes without rendering images. We build a navigation framework with RNR-Map, to navigate to the most plausible place given a query image. Through exten-sive experiments, we validate that the latent codes can serve as important visual clues for both image-based localization and image-goal navigation. More importantly, a user has an option to utilize the renderable property of RNR-Map for more fine-level of localization such as camera tracking.
RNR-Map is generalizable. There have been a number of studies that leverage neural radiance fields (NeRF) for various applications other than novel view synthesis. The robotic applications of NeRF are also now beginning to emerge [1, 15, 28, 35, 42]. However, many of the approaches require pretrained neural radiance fields about a specific scene and are not generalizable to various scenes. This can be a serious problem when it comes to visual navigation tasks, which typically assume that an agent performs the given task in an unseen environment [16]. In contrast, RNR-Map is applicable in arbitrary scenes without additional opti-mization. Even with the unseen environment, the RNR-Map can still embed the useful information from images to the map and render images. The neural radiance fields of RNR-Map are conditioned on the latent codes. A pair of encoder and decoder is trained to make these latent codes from im-ages of arbitrary scenes and reconstruct images using neural radiance fields. These pretrained encoder and decoder enable the generalization to unseen environments.
Third, RNR-Map is real-time capable. The majority of the present NeRF-based navigation methods require a sig-nificant time for inference because of required computation-heavy image rendering and rendering-based optimization
steps. The RNR-Map is designed to operate fast enough not to hinder the navigation system. By directly utilizing the latent codes, we can eliminate the rendering step in mapping and localization. The mapping and image-based localization frameworks operate at 91.9Hz and 56.8Hz, respectively. The only function which needs rendering-based optimization is camera tracking, which can localize under odometry noises, and it operates at 5Hz.
To the best of our knowledge, the RNR-Map is the first method having all three of the aforementioned characteris-tics as a navigation map. The RNR-Map and its localization and navigation frameworks are evaluated in various visual navigation tasks, including camera tracking, image-based lo-calization, and image-goal navigation. Experimental results show that the proposed RNR-Map serves as an informative map for visual navigation. Our localization framework ex-hibits competitive localization accuracy and inference speed when compared to existing approaches. On the image-goal navigation task, the navigation framework displays 65.7% success rate in curved scenarios of the NRNS [21] dataset, where the current state-of-the-art method [37] shows a suc-cess rate of 55.4%.
As RNR-Map finds a place based on the visual informa-tion of the map and the query image, we also consider a variant version of image-based localization. In real-world scenarios, there can be partial changes in the target place (changes in furniture placement, lighting conditions, ...).
Also, the user might only have images from similar but different environments. We test the proposed localization framework in both cases. We find that the RNR-Map is ro-bust to environmental changes and is able to find the most visually similar places even when a novel query image from a different environment is provided.
The contributions of this paper can be summarized as follows:
• We present RNR-Map, a novel type of renderable grid map for navigation, utilizing neural radiance fields for embedding the visual appearance of the environment.
• We demonstrate efficient and effective methods for uti-lizing the visual information in RNR-Map for searching an image goal by developing RNR-Map-based localiza-tion and navigation framework.
• Extensive experiments show that the proposed method shows the state-of-the-art performance in both localiza-tion and image-goal navigation. 2.