Abstract 1.

Introduction
There has been a recent explosion of impressive gen-erative models that can produce high quality images (or videos) conditioned on text descriptions. However, all such approaches rely on conditional sentences that contain un-ambiguous descriptions of scenes and main actors in them.
Therefore employing such models for more complex task of story visualization, where naturally references and co-references exist, and one requires to reason about when to maintain consistency of actors and backgrounds across frames/scenes, and when not to, based on story progression, remains a challenge. In this work, we address the afore-mentioned challenges and propose a novel autoregressive diffusion-based framework with a visual memory module that implicitly captures the actor and background context across the generated frames. Sentence-conditioned soft at-tention over the memories enables effective reference reso-lution and learns to maintain scene and actor consistency when needed. To validate the effectiveness of our approach, we extend the MUGEN dataset [19] and introduce addi-tional characters, backgrounds and referencing in multi-sentence storylines. Our experiments for story generation on the MUGEN, the PororoSV [30] and the FlintstonesSV
[16] dataset show that our method not only outperforms prior state-of-the-art in generating frames with high visual quality, which are consistent with the story, but also models appropriate correspondences between the characters and the background.
Multimodal deep learning approaches have pushed the quality and the breadth of conditional generation tasks such as image captioning [23, 33, 37, 54, 57] and text-to-image synthesis [25, 44, 59, 61–63]. Owing to the technical leaps made in generative models, such as generative adversar-ial networks (GANs) [14], variational autoencoders (VAEs)
[27] and the more recent diffusion models [21], approaches for text-to-image synthesis can now generate images with high visual fidelity representative of the textual descrip-tions. The captions, however, in such cases, are gener-ally short self-contained sentences representing the high-level semantics of a scene. This is rather restrictive in the real-world applications [8, 30, 31] where fine-grained un-derstanding of object interactions, motion and background information described by multiple sentences becomes nec-essary. One such task is that of story generation or visu-alization – the goal of which is to generate a sequence of illustrative image frames with coherent semantics given a sequence of sentences [30, 35, 36, 60].
Characteristic features of a good visual story is high vi-sual quality over multiple frames; this includes rendering of discernible objects, actors, poses and realistic interactions of those actors with objects and within the scene. More-over, for text-based story generation it is crucial to maintain consistency between the generated frames and the multi-sentence descriptions. Not only the actor context, but also the background of the generated story should be in-line with the description demonstrating effortless transition and adap-tation to the changing environments within the story [34].
Recent advances on the task of story generation have made significant advances along these lines, showing high visual fidelity and character consistency for story sentences that are self-contained and unambiguous (explicitly men-tioning characters and the setting each time). While impres-sive, this setup is fundamentally unrealistic. Realistic story text is considerably more complex and referential in nature; requiring ability to resolve ambiguity and references (or co-references) through reasoning. As shown in Fig. 1, while the description corresponding to the first frame has an ex-plicit reference to the character names, the (typical) subse-quent frame descriptions, provided by human, contain ref-erences such as “she, he, they”. Moreover, while maintain-ing character consistency, current approaches are limited in preserving, or transitioning through, the background infor-mation in agreement with the text (cf . Fig. 3) [30, 35, 36].
In natural language processing (NLP) co-reference res-olution in text is an important and core task [3, 26, 38].
While it maybe possible to apply such methods to story text to first resolve ambiguous references and then gener-ate corresponding images using existing story generation approaches, this is sub-optimal. The reason, is that co-reference resolution in the text domain, at best, would only allow to resolve references and maintain consistency across identity of the character. Appearance across frames would still lack consistency and require some form of visual rea-soning. As also noted in [47], reference resolution in the visual domain, or visio-lingual domain, is more powerful.
In this work, for the first time (to our knowledge), we study co-reference resolution in story generation. Prior work [35] offers limited performance when faced with text containing references (see Sec. 5). We address this by proposing a new autoregressive diffusion-based frame-work with a visual memory module that implicitly cap-tures the actor and background context across the generated frames. Sentence-conditioned soft attention over the mem-ories enables effective visio-lingual co-reference resolution and learns to maintain scene and actor consistency when needed. Further, given the lack of datasets that contain ref-erences and more complex sentence structure, we extend the
MUGEN dataset [19] and introduce additional characters, backgrounds and referencing in multi-sentence storylines.
Contributions. Our contributions are three-fold: (i) First, we introduce a novel autoregressive deep generative frame-work, Story-LDM, that adopts and extends latent diffusion models for the task of story generation. As part of Sto-ry-LDM, we propose a meticulously designed memory-at-tention mechanism capable of encoding and leveraging con-textual relevance between the part of the story-line that has already been generated, and the current frame being gen-erated based on learned semantic similarity of correspond-ing sentences. Equipped with this, our sequential diffu-sion model can generate consistent stories by resolving and then capturing temporal character and background context. (ii) Second, to validate our approach for co-reference res-olution, and character and background consistency in the visual domain, we extend existing datasets to include more complex scenarios and, importantly, referential text. Specif-ically, we extend the MUGEN dataset [19] to include mul-tiple characters and diverse backgrounds. We also modify
FlintstonesSV [16] and PororoSV [30] dataset to include character references. These enhancements allow us to in-crease the complexity of the aforementioned datasets by in-troducing co-references in the sentences of a story. (iii) Fi-nally, to evaluate different approaches for foreground (char-acter) as well as background consistency we propose novel evaluation metrics. Our results on the MUGEN [19], the
PororoSV [30] and the FlintstonesSV [16] datasets show that we outperform the prior state-of-the-art on consistency metrics by a large margin. 2.