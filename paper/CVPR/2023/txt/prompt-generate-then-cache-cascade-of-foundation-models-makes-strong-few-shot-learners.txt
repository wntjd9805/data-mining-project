Abstract language-contrastive
Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning.
In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pre-training paradigms for better few-shot learning. Our CaFo knowledge, incorporates CLIP’s
DINO’s vision-contrastive knowledge, DALL-E’s vision-generative knowledge, and GPT-3’s language-generative knowledge. Specifically, CaFo works by ‘Prompt, Generate, then Cache’. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower.
At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such col-laboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo. 1.

Introduction
Convolutional neural networks [44] and transform-ers [68] have attained great success on a wide range of vi-sion tasks with abundant datasets [15]. Instead, for some data-deficient and resource-finite scenarios, few-shot learn-ing [63, 70] also becomes a research hotspot, where the net-∗ Equal contribution. † Corresponding author
Figure 1. The Cascade Paradigm of CaFo. We adaptively incor-porate the knowledge from four types of pre-training methods and achieve a strong few-shot learner. works are constrained to learn from limited images with annotations. Many previous works have been proposed in this field to enhance model’s generalization capability by meta learning [20, 71], metric learning [74], and data augmentation [31, 73]. Recently, CLIP [57] pre-trained by large-scale language-image pairs shows favorable zero-shot transfer ability for open-vocabulary visual recogni-tion. The follow-up CoOp [83], CLIP-Adapter [24] and
Tip-Adapter [76] further extend it for few-shot classifica-tion and achieve superior performance on various down-stream datasets. This indicates that, even if the few-shot training data is insufficient, the large-scale pre-training has endowed the network with strong representation ability, which highly benefits the few-shot learning on downstream domains. Now that there exist various self-supervisory paradigms besides CLIP, could we adaptively integrate their pre-learned knowledge and collaborate them to be a better
few-shot learner?
To tackle this issue, we propose CaFo, a Cascade of
Fooundation models blending the knowledge from mul-tiple pre-training paradigms with a ‘Prompt, Generate, then Cache’ pipeline. As shown in Figure 1, we in-tegrate CLIP [57], DINO [7], DALL-E [58], and GPT-3 [4] to provide four types of prior knowledge for CaFo.
Therein, CLIP [57] is pre-trained to produce paired fea-tures in the embedding space for every image and its de-scriptive text. Guided by texts with different categor-ical semantics, CLIP [57] can well classify the images aided by language-contrastive knowledge. DINO fol-lows contrastive self-supervised learning [7] to match the representations between two transformations of one same image, which is expert at distinguishing different images with vision-contrastive knowledge. Similar to CLIP [57],
DALL-E [58] is also pre-trained by image-text pairs but learns to predict the encoded image tokens based on the given text tokens. Conditioned on the input text, DALL-E [58] could leverage the vision-generative knowledge to create high-quality synthetic images in a zero-shot manner.
Pre-trained by large-scale language corpus, GPT-3 [4] takes a few hand-written templates as input, and autoregressively generates human-like texts, which contain rich language-generative knowledge. Therefore, the four models have distinctive pre-training goals and can provide complemen-tary knowledge to assist the few-shot visual recognition.
In detail, we cascade them by three steps.: 1) Prompt.
We adopt GPT-3 [4] to produce textual prompts for CLIP based on a few hand-written templates. These prompts with richer language knowledge are fed into CLIP’s textual en-coder. 2) Generate. We adopt DALL-E [58] to gener-ate additional training images for different categories based on the domain-specific texts, which enlarges the few-shot training data, but costs no extra manpower for collection and annotation. 3) Cache. We utilize a cache model to adaptively incorporate the predictions from both CLIP [57] and DINO [7]. Referring to Tip-Adapter [76], we build the cache model with two kinds of keys respectively for the two pre-trained models. Regarding zero-shot CLIP as the distribution baseline, we adaptively ensemble the pre-dictions of two cached keys as the final output. By only fine-tuning the lightweight cache model via expanded train-ing data, CaFo can learn to fuse diverse prior knowledge and leverage their complementary characteristics for better few-shot visual recognition.
Our main contributions are summarized as follows:
• We propose CaFo to incorporate the prior knowledge learned from various pre-training paradigms for better few-shot learning.
• By collaborating CLIP, DINO, GPT-3 and DALL-E,
CaFo utilizes more semantic prompts, enriches the limited few-shot training data, and adaptively ensem-bles diverse predictions via the cache model.
• We conduct thorough experiments on 11 datasets for few-shot classification, where CaFo achieves state-of-the-art without using extra annotated data. 2.