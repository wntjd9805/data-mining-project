Abstract
Cross-modal retrieval across image and text modalities is a challenging task due to its inherent ambiguity: An im-age often exhibits various situations, and a caption can be coupled with diverse images. Set-based embedding has been studied as a solution to this problem. It seeks to en-code a sample into a set of different embedding vectors that capture different semantics of the sample. In this paper, we present a novel set-based embedding method, which is dis-tinct from previous work in two aspects. First, we present a new similarity function called smooth-Chamfer similar-ity, which is designed to alleviate the side effects of existing similarity functions for set-based embedding. Second, we propose a novel set prediction module to produce a set of embedding vectors that effectively captures diverse seman-tics of input by the slot attention mechanism. Our method is evaluated on the COCO and Flickr30K datasets across dif-ferent visual backbones, where it outperforms existing meth-ods including ones that demand substantially larger compu-tation at inference. 1.

Introduction
Cross-modal retrieval is the task of searching for data relevant to a query from a database when the query and database have different modalities. While it has been stud-ied for various pairs of modalities such as video-text [3, 7, 19] and audio-text [5, 16], the most representative setting for the task is the retrieval across image and text modali-ties [10, 29, 37, 45]. A na¨ıve solution to cross-modal re-trieval is a straightforward extension of the conventional unimodal retrieval framework [17, 18, 26], i.e., learning a joint embedding space of the different modalities with known ranking losses (e.g., contrastive loss [9] and triplet loss [43]). In this framework, each sample is represented as a single embedding vector and the task reduces to neighbor search on the joint embedding space.
However, this na¨ıve approach has trouble in handling the inherent ambiguity of the cross-modal retrieval across im-age and text modalities [10, 45, 46]. A cause of the am-Figure 1. An example of the ambiguity problem introduced in the cross-modal retrieval task; an image region and a word corre-sponding to each other are highlighted in the same color. This ex-ample demonstrates that a single image can be coupled with mul-tiple heterogeneous captions. biguity is the fact that even a single image often contains various situations and contexts. Consider an image in Fig-ure 1, which illustrates a group of children in a skate park.
One of the captions coupled with it could be about chil-dren carrying up a bike, while another may describe the child riding a skateboard. Indeed, different local features of the image are matched to different captions. Similarly, vi-sual manifestations of a caption could vary signiﬁcantly as text descriptions are highly abstract. This ambiguity issue suggests that a sample should be embedded while reﬂecting its varying semantics in cross-modal retrieval. Embedding models dedicated to the uni-modal retrieval do not meet this requirement since they represent a sample as a single em-bedding vector.
Various methods have been studied to mitigate the am-biguity issue of cross-modal retrieval. Most of them adopt cross-attention networks that directly predict the similarity of an input image-caption pair [12, 14, 25, 30, 37, 38, 49, 50, 53]. These models successfully address the ambiguity since they explicitly infer relations across the modalities by drawing attentions on both modalities at once. However, they inevitably impose a large computation burden on re-trieval systems since they demand both image and caption to be processed together for computing their similarity; all data in a database thus have to be reprocessed whenever a query arrives.
In contrast, methods using separate tex-tual and visual encoders [17, 21, 24, 29] seek to ﬁnd sam-ples relevant to query through nearest-neighbor search on pre-computed embedding vectors, which are more suitable to nowadays retrieval systems working on huge databases.
However, most of the previous arts in this direction cannot address the ambiguity issue since their encoders return a single embedding vector for a given input.
Recent studies [10, 45] have advanced embedding model architectures to tackle the ambiguity issue even with sepa-rate encoders for the two modalities. To be speciﬁc, their models compute a set of heterogeneous embedding vectors for a given sample using self-attention layers on top of vi-sual or textual features; such a set of embedding vectors is called embedding set in the remainder of this paper. Then the ambiguity issue is addressed through elements of the embedding set that encode diverse semantics of input.
Although the set-based embedding models enable a re-trieval system to be powerful yet efﬁcient, however, simi-larity functions used for their training do not consider the ambiguity of the data. Hence, training the models with the similarity functions often causes the following two side ef-fects: (1) Sparse supervision–An embedding set most of whose elements remain untrained, or (2) Set collapsing–An embedding set with a small variance where elements do not encode sufﬁcient ambiguity. Further, self-attention modules used for set prediction in the previous work do not explicitly consider disentanglement between set elements. These lim-itations lead to an embedding set whose elements encode redundant semantics of input, which also causes the set col-lapsing and degrades the capability of learned embedding models.
To address the aforementioned limitations of previous work, we propose a novel set-based embedding method for cross-modal retrieval. The proposed method is distinct from previous work in mainly two aspects. First, we design a novel similarity function for sets, called smooth-Chamfer similarity, that is employed for both training and evaluation of our model. In particular, our loss based on the smooth-Chamfer similarity addresses both limitations of the exist-ing similarity functions, i.e., sparse supervision and set col-lapsing. Second, we propose a model with a novel set pre-diction module motivated by slot attention [33]. In the pro-posed module, learnable embeddings called element slots compete with each other for aggregating input data while being transformed into an embedding set by progressive up-date. Therefore, our model captures the diverse semantic ambiguity of input successfully, with little redundancy be-tween elements of the embedding set.
The proposed method is evaluated and compared with previous work on two realistic cross-modal retrieval bench-marks, COCO [32] and Flickr30K [41], where it outper-forms the previous state of the art in most settings. In sum-mary, our contribution is three-fold as follows:
• We address issues on previous set-based embedding methods by proposing a novel similarity function for sets, named smooth-Chamfer similarity.
• We introduce a slot attention based set prediction mod-ule where elements of embedding set iteratively com-pete with each other for aggregating input data, which can capture semantic ambiguity of input without re-dundancy.
• Our model achieved state-of-the-art performance on two standard the COCO and Flickr30K datasets, benchmarks for cross-modal retrieval. 2.