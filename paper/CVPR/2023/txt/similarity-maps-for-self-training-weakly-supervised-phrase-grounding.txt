Abstract
A phrase grounding model receives an input image and a text phrase and outputs a suitable localization map. We present an effective way to refine a phrase ground model by considering self-similarity maps extracted from the la-tent representation of the model’s image encoder. Our main insights are that these maps resemble localization maps and that by combining such maps, one can obtain useful pseudo-labels for performing self-training. Our re-sults surpass, by a large margin, the state of the art in weakly supervised phrase grounding. A similar gap in per-formance is obtained for a recently proposed downstream task called WWbL, in which only the image is input, without any text. Our code is available at https://github. com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding. 1.

Introduction
The most important technological foundation of the on-going revolution in the field of jointly processing images with text is that of computing a similarity score between an image and a text phrase. Models such as CLIP [27] are able to learn powerful similarity functions based on large datasets that contain pairs of images and suitable captions. Build-ing upon this technology, endless possibilities opened up for zero-shot and few-shot learning applications, including image captioning [19, 35], image editing [6, 13, 23, 25, 41], and image recognition [24, 40].
Phrase grounding is a related image-text task, in which, given an image and a text phrase, the method identifies the image region described by the phrase. One can train a phrase grounding network in a weakly supervised manner, by employing the CLIP similarity. Such a model would learn to produce masks that create a foreground region that is similar to the phrase and a background region that is not.
To avoid trivial solutions, in which the mask includes the entire image, one should also include a regularization term.
Recently, Shaharabany et al. have obtained state-of-the-art results in phrase grounding using this scheme with the addition of a constraint that considers also the explainability map of Chefer et al. [8], when applied to the CLIP model.
This map is expected to focus on foreground regions and can, therefore, provide an additional training cue.
In this work, we focus on the output of the image encoder of the phrase grounding network of [32]. This encoder cre-ates a spatial image representation that is aligned with the text encoding of CLIP. In other words, each spatial location of the tensor that the image encoder outputs is associated with a vector that is correlated with a representation of the textual description of the corresponding image location.
As a result, spatial locations that are associated with the same image object have similar encoding and vice versa.
Building upon this insight, we compute for each image loca-tion, the cosine-similarity to the local encoding of all other locations. These similarity maps (one per location) are text-agnostic since they involve only the image input of the phrase grounding network.
As we demonstrate, these maps capture different objects in the image, i.e., the similarity map that is obtained by correlating the image embedding at the spatial location (x, y) provides a delineation of the object that can be found at this image location. This is a type of semantic “seed fill” effect, in which all image regions that are associated with the same object in the seed location are highlighted. These regions do not need to be connected, i.e., multiple persons in the image would be highlighted by a seed placed on one of them.
This localization-map effect is reminiscent of the emer-gence of segmentation maps in self-supervised transformer networks [5]. The two effects are, however, different. First, the effect in the transformer case relies on the attention maps, which do not exist in the networks we employ. Second, the attention maps rely on a single CLS token, and our effect is true to every spatial location. Third, the type of supervision and the task are both different.
The effect described is also different than the relevancy maps obtained by explainability methods such as Grad-CAM [31] or recent transformer explainability methods [8].
For example, our similarity maps do not require backtracking the relevancy scores all the way from the network’s output.
Also, the maps are per seed location and not per label.
By aggregating multiple similarity maps, we obtain fairly accurate foreground masks. Using these masks as pseudo labels, we refine the phrase grounding model using a super-vised loss term. On all of the known phrase grounding bench-marks, we demonstrate that our refinement method leads to a marked improvement across multiple scores. Similarly, a sizable gap over the current state-of-the-art is also shown in the recently proposed computer vision task of “What is
Where by Looking” (WWbL) [32], for which the phrase grounding network is coupled with a captioning model.
Our contributions are: (i) observing that spatial similar-ity maps of the latent space of phrase grounding networks capture the boundaries of objects, (ii) developing a method for aggregating multiple such similarity maps to obtain a comprehensive segmentation map, (iii) using the obtained maps to finetune the phrase grounding network, and (iv) sur-passing the current state-of-the-art by a sizable gap in both phrase grounding and WWbL. 2.