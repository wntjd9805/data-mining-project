Abstract
Weakly-supervised
Temporal Action
Localization (WTAL) attempts to localize the actions in untrimmed videos using only video-level supervision. Most recent works approach WTAL from a localization-by-classification perspective where these methods try to classify each video frame followed by a manually-designed post-processing pipeline to aggregate these per-frame action predictions into action snippets. Due to this perspective, the model lacks any explicit understanding of action boundaries and tends to focus only on the most discriminative parts of the video resulting in incomplete action localization. To address this, we present PivoTAL, Prior-driven Supervision for Weakly-supervised Temporal Action Localization, to approach WTAL from a localization-by-localization perspective by learning to localize the action snippets directly. To this end, PivoTAL leverages the underlying spatio-temporal regularities in videos in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to supervise the localization-based training. PivoTAL shows significant improvement (of at least 3% avg mAP) over all existing methods on the benchmark datasets, THUMOS-14 and ActivitNet-v1.3. 1.

Introduction
Temporal action localization (TAL) [5,24,43,46,47,56] refers to the task of predicting where and what category of action happens in an arbitrarily long untrimmed video.
While TAL is crucial in a wide variety of applications rang-ing from sports, robotics, and safety, it is challenging as it requires the model to develop a strong temporal and spatial understanding of the video scene and events for effective lo-calization. Furthermore, fully-supervised TAL relies on the
⋆Authors with equal contribution.
This work was done as Mamshad’s internship project at Microsoft.
Figure 1. Left: Green denotes ground truth action snippets, Purple denotes true positive action snippets, and Pink denotes false pos-itive action snippets. We observe that PivoTAL significantly out-performs Base WTAL by detecting all the ground-truth instances correctly. Right: Improvement of PivoTAL over the previous state-of-the-art on THUMOS’14 at different IoU thresholds. availability of expensive dense annotations in terms of the start and end of each action snippet in the training videos.
Weakly-supervised Temporal Action Localization (WTAL) serves to mitigate this dependency on dense anno-tations by operating only on video-level annotations (i.e., knowing which actions occur without knowing their precise locations in a video) during training while still being able to predict the start and end of the action snippet in test videos.
Several methods [20, 32, 33, 38, 45, 50, 53] have attempted to perform WTAL by employing different techniques which include Multiple Instance Learning (MIL) [20, 33] and attention mechanism [45, 50]. However, to the best of our knowledge, all of these previous works approach
WTAL from a localization-by-classification perspective where the underlying method tries to classify each video frame into zero or more action categories followed by a manually-designed post-processing pipeline to aggregate these per-frame action predictions into action snippets with explicit boundaries.
Fig. 1 shows the final action snippet predictions on a video with Long Jump action from a typical localization-by-classification method which we refer to as Base WTAL in the figure. We observe that Base WTAL suffers from some
challenges. First, the localization-by-classification training is performed only with the coarse video-level labels, which encourages the model to focus on the most discriminative parts of the video, resulting in incomplete and fragmented action snippets (140-160s for Base WTAL in Fig. 1). There is also a higher rate of false positives due to the misclassifi-cation of background that closely resembles foreground (at
∼60s for Base WTAL in Fig. 1). Second, since the model is trained to perform per-frame prediction, it lacks any explicit notion of action boundaries, thus resulting in a discrepancy between the classification-based training and localization-based test objectives. This is generally addressed by in-corporating carefully-designed post-processing algorithms.
Even though such post-hoc transformations can encode cru-cial prior knowledge of temporal structure of videos, they cannot influence the model training for improving the lo-calization performance directly.
To resolve this discrepancy, we propose PivoTAL,
Prior-driven Supervision for weakly-supervised Temporal
Action Localization. PivoTAL approaches WTAL from localization-by-localization perspective by learning to lo-calize the action snippets directly. To this end, PivoTAL in-troduces a novel algorithm that exploits the inherent spatio-temporal structure of the video data in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to derive pseudo-action snip-pets. These pseudo-action snippets act as an additional source of supervision in PivoTAL to complement the under-constrained video-level weak-supervision to perform the lo-calization task.
PivoTAL first employs a Base WTAL Head to per-form weakly-supervised temporal action localization using video-level supervision. While doing so, PivoTAL em-ploys a novel action-specific scene prior in the background
MIL loss to inject action-specific bias into the background frames to improve action boundaries (at 180s for PivoTAL in Fig. 1). PivoTAL also complements the per-frame action-ness scores learned by the model with learnable Gaussian prior-based actionness scores to incorporate context from nearby frames and to improve the smoothness of predicted action snippets (140-160s for PivoTAL in Fig. 1). Next,
PivoTAL creates pseudo-action snippets by employing ac-tion snippet generation prior and makes them confidence-aware using the confidence predictions of the Base WTAL
Head. Finally, these pseudo-action snippets are used to train the Prior-driven Localization Head of the model to predict the action snippets directly. We conduct extensive experi-ments on the standard WTAL datasets, THUMOS’14 and
ActivityNet-v1.3, achieving 3.2% and 3.0% absolute im-provement on the average mAP respectively over all pre-vious methods. This demonstrates PivoTAL’s advantage in effectively utilizing the priors, leading to a significant im-provement in the localization performance.
Our work makes the following major contributions, 1. We introduce PivoTAL, the first method to approach
WTAL from a localization-by-localization perspective by generating pseudo-action snippets as supervision to localize action snippets directly. 2. In the process, PivoTAL exploits the underlying spatio-temporal regularities in videos in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to complement the available weak video-level supervision. 3. PivoTAL significantly outperforms all previous meth-ods on WTAL benchmarks THUMOS’14 and Activi-tyNet, with 3% or higher absolute increase on average mAP metric. 2.