Abstract
Compositional zero-shot learning (CZSL) aims at learn-ing visual concepts (i.e., attributes and objects) from seen compositions and combining concept knowledge into un-seen compositions. The key to CZSL is learning the disen-tanglement of the attribute-object composition. To this end, we propose to exploit cross-attentions as compositional dis-entanglers to learn disentangled concept embeddings. For example, if we want to recognize an unseen composition
“yellow flower”, we can learn the attribute concept “yellow” and object concept “flower” from different yellow objects and different flowers respectively. To further constrain the disentanglers to learn the concept of interest, we employ a regularization at the attention level. Specifically, we adapt the earth mover’s distance (EMD) as a feature similarity met-ric in the cross-attention module. Moreover, benefiting from concept disentanglement, we improve the inference process and tune the prediction score by combining multiple concept probabilities. Comprehensive experiments on three CZSL benchmark datasets demonstrate that our method signifi-cantly outperforms previous works in both closed- and open-world settings, establishing a new state-of-the-art. Project page: https://haoosz.github.io/ade-czsl/ 1.

Introduction
Suppose we have never seen white bears (i.e., polar bears) before. Can we picture what it would look like? This is not difficult because we have seen many white animals in daily life (e.g., white dogs and white rabbits) and different bears with various visual attributes in the zoo (e.g., brown bears and black bears). Humans have no difficulty in disentangling
“white” and “bear” from seen instances and combining them into the unseen composition. Inspired by this property of human intelligence, researchers attempt to make machines learn compositions of concepts as well. Compositional zero-shot learning (CZSL) is a specific problem studying visual compositionality, aiming to learn visual concepts from seen
*Corresponding author
Figure 1. Motivation illustration. Given images from seen attribute-object compositions, human can disentangle the attribute “yellow” from “yellow bird” and “yellow pear”, and the object “flower” from
“purple flower” and “red flower”. After learning visual properties of the concepts “yellow” and “flower”, human can then recognize images from the unseen composition “yellow flower”. compositions of attributes and objects and generalize concept knowledge to unseen compositions.
Learning attribute-object compositions demands prior knowledge about attributes and objects. However, visual concepts of attributes and objects never appear alone in a nat-ural image. To learn exclusive concepts for compositionality learning, we need to disentangle the attribute concept and the object concept. As illustrated in Fig. 1, if we want to recog-nize the image of “yellow flower”, it is necessary to learn the
“yellow” concept and the “flower” concept, i.e., disentangle visual concepts, from images of seen compositions. Previous works [22, 24, 25, 28–30, 36, 46] tackle CZSL by composing attribute and object word embeddings, and projecting word and visual embeddings to a joint space. They fail to disentan-gle visual concepts. Recently, some works [21,40,41,50] con-sider visual disentanglement but still have limitations despite their good performance. SCEN [21] learns concept-constant samples contrastively without constructing concept embed-ding prototypes to avoid learning irrelevant concepts shared by positive samples. IVR [50] disentangles visual features into ideal concept-invariant domains. This ideal domain gen-eralization setting requires a small discrepancy of attribute and object sets and would degenerate on vaster and more
complex concepts. ProtoProp [40] and OADis [41] learn local attribute and object prototypes from spatial features on convolutional feature maps. However, spatial disentangle-ment is sometimes infeasible because attribute and object concepts are highly entangled in spatial features. Taking an image of “yellow flower” as an example, the spatial posi-tions related to the attribute “yellow” and the object “flower” completely overlap, which hinders effective attribute-object disentanglement.
To overcome the above limitations, we propose a sim-ple visual disentangling framework exploiting Attention as
DisEntangler (ADE) on top of vision transformers [6]. We notice that vision transformers (ViT) have access to more sub-space global information across multi-head attentions than CNNs [38]. Therefore, with the expressivity of different subspace representations, token attentions of ViT may pro-vide a more effective way for disentangling visual features, compared to using traditional spatial attentions across local positions on convolutional features [40, 41]. Specifically, it is difficult to disentangle the attribute-object composition
“yellow flower” by spatial positions, but it is possible for
ViT multi-head attentions to project attribute concept “yel-low” and object concept “flower” onto different subspaces.
Inspired by this property, we propose to learn cross-attention between two inputs that share the same concept, e.g., “yel-low bird” and “yellow pear” share the same attribute concept
“yellow”. In this way, we can derive attribute- and object-exclusive visual representations by cross-attention disentan-glement. To ensure that the concept disentangler is exclusive to the specific concept, we also need to constrain the disen-tanglers to learn the concept of interest instead of the other concept. For example, given attribute-sharing images, the attribute attention should output similar attribute-exclusive features while the object attention should not. To achieve this goal, we apply a regularization term adapted from the earth mover’s distance (EMD) [13] at the attention level. This reg-ularization term forces cross-attention to learn the concept of interest by leveraging the feature similarity captured from all tokens. Mancini et al. [24] propose an open-world evaluation setting, which is neglected by most previous works. We con-sider both closed-world and the open-world settings in our experiments, demonstrating that our method is coherently efficient in both settings. The contributions of this paper are summarized below:
• We propose a new CZSL approach, named ADE, us-ing cross-attentions to disentangle attribute- and object-exclusive features from paired concept-sharing inputs.
• We force attention disentanglers to learn the concept of interest with a regularization term adapted from EMD, ensuring valid attribute-object disentanglement.
• We comprehensively evaluate our method in both closed-world and open-world settings on three CZSL datasets, achieving consistent state-of-the-art. 2.