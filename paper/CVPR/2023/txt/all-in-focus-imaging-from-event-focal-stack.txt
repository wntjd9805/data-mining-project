Abstract 1.

Introduction
Traditional focal stack methods require multiple shots to capture images focused at different distances of the same scene, which cannot be applied to dynamic scenes well.
Generating a high-quality all-in-focus image from a single shot is challenging, due to the highly ill-posed nature of the single-image defocus and deblurring problem. In this pa-per, to restore an all-in-focus image, we propose the event focal stack which is defined as event streams captured dur-ing a continuous focal sweep. Given an RGB image focused at an arbitrary distance, we explore the high temporal reso-lution of event streams, from which we automatically select refocusing timestamps and reconstruct corresponding refo-cused images with events to form a focal stack. Guided by the neighbouring events around the selected timestamps, we can merge the focal stack with proper weights and restore a sharp all-in-focus image. Experimental results on both synthetic and real datasets show superior performance over state-of-the-art methods.
† Contributed equally to this work as first authors
∗ Corresponding author
Project page: https://hylz-2019.github.io/EFS
The lens aperture of a camera controls the amount of incoming luminous flux. A larger aperture maintains the signal-to-noise ratio with shorter exposure time, which is useful for shooting high-speed scenes or capturing images in low-light conditions with less noise. However, large aper-ture settings also make the depth of field (DoF) shallow, which results in defocus blur. This is preferable in certain scenarios, such as in portrait photography a shallow DoF can be used to emphasize the subject. Yet, all-in-focus im-ages preserve information from all distances and are desired in more situations, e.g., microscopy imaging [25]. Besides, all-in-focus imaging also benefits various high-level vision tasks, e.g., object detection [29] and semantic segmenta-tion [10].
An all-in-focus image could be obtained by deblurring a defocused image, but the defocus kernel, determined by the aperture shape and depth of the scene, is usually spatially-varying and difficult to be estimated accurately [48]. Con-ventional two-stage methods [9, 13, 36] first estimate the pixel-wise or patch-wise defocus kernels with image pri-ors and then apply non-blind image deconvolution to each
pixel or patch. Recently, benefiting from the data-driven strategy, end-to-end deep learning methods [18, 31, 32, 38] outperform conventional two-stage restoration methods, by observing defocused and all-in-focus image pairs during training. Although they have demonstrated high potential in removing defocus blur, the deblurred results still cannot avoid ringing artifacts or remain blurry in high-frequency regions due to inaccurate defocus kernel estimation espe-cially for weakly textured and defocused regions (an exam-ple is shown in Figure 1 right (c)).
To overcome the ill-posedness of estimating the defocus kernel from a single image, merging a focal stack, i.e., a sequence of images taken at different focus distances, can generate an all-in-focus image reliably [11, 40, 47]. How-ever, capturing a focal stack requires a static scene and mul-tiple exposures. Moreover, the selection of focus distances is a key factor in capturing the focal stack, which requires elaborate design.
Neuromorphic event cameras [5, 35] are novel sen-sors that can detect brightness changes and trigger an event whenever its log variation exceeds a preset thresh-old. Thanks to their high temporal resolution featured with microsecond-level sensitivity, they can capture ap-proximately continuous signals for intensity variations of a scene, and support applications like generating high-speed videos from event streams [28, 41–43]. These characteris-tics motivate us to think about: Can we use “focal stacks” composed of event streams for all-in-focus imaging?
In this paper, we propose event focal stack (EFS) for the first time. It is composed of event streams obtained from a continuous focal sweep with an event camera, which can be used to reconstruct an image focal stack (given an RGB im-age focused at an arbitrary distance) and predict the merging weights for all-in-focus image recovery, as shown in Fig-ure 1 left. EFS encodes scene texture information from con-tinuous different depths in temporal log-gradient domain, so we first select a refocusing timestamp for each patch of the scene, which corresponds to sharper edges and richer tex-ture information at that time. By fusing a defocused image and the EFS recorded between the defocused timestamp and refocusing timestamp, we generate a refocused image for each refocusing timestamp, forming an image focal stack.
Guided by neighbouring events around refocusing times-tamps, we can predict the merging weight for each image needed for composing a focal stack, and finally restore an all-in-focus image (an example is shown in Figure 1 right (d)). Contributions of this paper are demonstrated by ex-ploring the following benefits of the proposed EFS:
• reliable selection of refocusing timestamps by decod-ing continuous scene gradient changes from events;
• consistent link between defocused (given) and refo-cused images (estimated) composing an image focal stack; and
• robust guidance for merging weight prediction and all-in-focus reproduction with event triggered neighbour-ing the selected timestamps.
We quantitatively and qualitatively evaluate our method on both synthetic and real datasets and demonstrate its su-perior quality in recovering all-in-focus images over state-of-the-art methods. 2.