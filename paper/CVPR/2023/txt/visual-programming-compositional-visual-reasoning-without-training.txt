Abstract
We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given nat-ural language instructions. VISPROG avoids the need for any task-specific training. it uses the in-context learning ability of large language models to gener-ate python-like modular programs, which are then executed to get both the solution and a comprehensive and inter-pretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models,
Instead, image processing subroutines, or python functions to pro-duce intermediate outputs that may be consumed by subse-quent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual ques-tion answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image edit-ing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.
1.

Introduction
The pursuit of general purpose AI systems has lead to the development of capable end-to-end trainable models
[1, 5, 8, 13, 17, 22, 24], many of which aspire to provide a simple natural language interface for a user to interact with the model. The predominant approach to building these systems has been massive-scale unsupervised pretraining followed by supervised multitask training. However, this approach requires a well curated dataset for each task that makes it challenging to scale to the infinitely long tail of complex tasks we would eventually like these systems to perform. In this work, we explore the use of large language models to tackle the long tail of complex tasks by decom-posing these tasks described in natural language into sim-pler steps that may be handled by specialized end-to-end trained models or other programs.
Imagine instructing a vision system to “Tag the 7 main characters on the TV show Big Bang Theory in this image.”
To perform this task, the system first needs to understand the intent of the instruction and then perform a sequence of steps - detect the faces, retrieve list of main characters on Big Bang Theory from a knowledge base, classify faces using the list of characters, and tag the image with recog-nized character’s faces and names. While different vision and language systems exist to perform each of these steps, executing this task described in natural language is beyond the scope of end-to-end trained systems.
We introduce VISPROG which inputs visual data (a sin-gle image or a set of images) along with a natural language instruction, generates a sequence of steps, a visual pro-gram if you will, and then executes these steps to produce the desired output. Each line in a visual program invokes one among a wide range of modules currently supported by the system. Modules may be off-the-shelf computer vi-sion models, language models, image processing subrou-tines in OpenCV [4], or arithmetic and logical operators.
Modules consume inputs that are produced by executing previous lines of code and output intermediate results that can be consumed downstream. In the example above, the visual program generated by VISPROG invokes a face de-tector [16], GPT-3 [5] as a knowledge retrieval system, and
CLIP [20] as an open-vocabulary image classifier to pro-duce the desired output (see Fig. 1).
VISPROG improves upon previous methods for gener-ating and executing programs for vision applications. For the visual question answering (VQA) task, Neural Module
Networks (NMN) [2,9,10,12] compose a question-specific, end-to-end trainable network from specialized, differen-tiable neural modules. These approaches either use brittle, off-the-shelf semantic parsers to deterministically compute the layout of modules, or learn a layout generator through weak answer supervision via REINFORCE [30].
In con-trast, VISPROG uses a powerful language model (GPT-3)
Figure 2. Modules currently supported in VISPROG. Red modules use neural models (OWL-ViT [19], DSFD [16], Mask-Former [6], CLIP [20], ViLT [15], and Stable Diffusion [25]).
Blue modules use image processing and other python subroutines.
These modules are invoked in programs generated from natural language instructions. Adding new modules to extend VISPROG’s capabilities is straightforward (Code. 1). and a small number of in-context examples to create com-plex programs without requiring any training1. Programs created by VISPROG also use a higher-level of abstraction than NMNs and invoke trained state-of-the-art models and non-neural python subroutines (Fig. 2). These advantages make VISPROG an easy-to-use, performant, and modular neuro-symbolic system.
VISPROG is also highly interpretable. First, VISPROG produces easy-to-understand programs which a user can verify for logical correctness. Second, by breaking down the prediction into simple steps, VISPROG allows a user to inspect the outputs of intermediate steps to diagnose errors and if required, intervene in the reasoning process. Alto-gether, an executed program with intermediate step results (e.g. text, bounding boxes, segmentation masks, generated images, etc.) linked together to depict the flow of informa-tion serves as a visual rationale for the prediction.
To demonstrate its flexibility, we use VISPROG for 4 dif-ferent tasks that share some common skills (e.g. for im-age parsing) while also requiring some degree of special-ized reasoning and visual manipulation capabilities. These tasks are - (i) compositional visual question answering; (ii) zero-shot natural language visual reasoning (NLVR) on im-age pairs; (iii) factual knowledge object tagging from natu-ral language instructions; and (iv) language-guided image editing. We emphasize that neither the language model nor any of the modules are finetuned in any way. Adapt-ing VISPROG to any task is as simple as providing a few in-context examples consisting of natural language instruc-tions and the corresponding programs. While easy to use,
VISPROG shows an impressive gain of 2.7 points over a base VQA model on the compositional VQA task, strong zero-shot accuracy of 62.4% on NLVR without ever train-ing on image pairs, and delightful qualitative and quantita-tive results on knowledge tagging and image editing tasks. 1We use “training” to refer to gradient-based learning to differentiate it from in-context learning which only involves a feedforward pass.
Our key contributions include - (i) VISPROG - a sys-tem that uses the in-context learning ability of a language model to generate visual programs from natural language instructions for compositional visual tasks (Sec. 3); (ii) demonstrating the flexibility of VISPROG on complex vi-sual tasks such as factual knowledge object tagging and lan-guage guided image editing (Secs. 4.3 and 4.4) that have eluded or seen limited success with a single end-to-end model; and (iii) producing visual rationales for these tasks and showing their utility for error analysis and user-driven instruction tuning to improve VISPROG’s performance sig-nificantly (Sec. 5.3). 2.