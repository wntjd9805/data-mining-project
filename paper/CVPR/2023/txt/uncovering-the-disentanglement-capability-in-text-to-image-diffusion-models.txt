Abstract
Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated im-ages. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modiﬁcation towards a style without changing the semantic content, and the modiﬁcation parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or
ﬁne-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our ﬁnding is that for stable diffusion models, by partially changing the input text embedding from a neu-tral description (e.g., “a photo of person”) to one with style (e.g., “a photo of person with smile”) while ﬁxing all the
Gaussian random noises introduced during the denoising process, the generated images can be modiﬁed towards the target style without changing the semantic content. Based on this ﬁnding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and con-tent preservation. This entire process only involves optimiz-ing over around 50 parameters and does not ﬁne-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require ﬁne-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement. 1.

Introduction
Image generation has been a widely-studied research problem in computer vision, with many competitive gen-erative models proposed over the last decade, such as gen-erative adversarial networks (GANs) [5, 10, 18, 30, 32] and variational autoencoders (VAE) [39, 57, 59, 60]. Recently, diffusion models [23, 71–73], with their ability to gener-ate high-quality and high-resolution images in different do-Scenes
Person 4 Global
Local 8 Small edits
Styles (children drawing, cyberpunk, anime), Building appearance (wooden, red brick), Weather & time (sunset, night, snowy)
Styles (renaissance, Egyptian mural, sketch, Pixar)
Appearance (young, tanned, male)
Cherry blossom, rainbow, foothills
Expressions (smiling, crying, angry)
Cake toppings, remove people on the street
Hats, hair colors, earrings
Table 1. Summarization of explored attributes. 4 shows successfully disentangled attributes and 8 shows failure cases. Small edits on the image are harder to be disentangled when the target attribute correlates with other parts of the image. mains, have soon attracted wide research attention.
One important research direction regarding image gener-ative models is the ability to disentangle different aspects of the generated images, such as semantic contents and styles, which is crucial for image editing and style transfer. A generative model with a good disentanglement capability should satisfy the following two desirable properties. First, it should permit separate modiﬁcation of one aspect without changing other aspects. As an example shown in Fig. 2, in text-to-image generation, when the text input changes from
“a photo of person” to “a photo of person with smile”, the generative model should have the ability to modify just the expression of the person (i.e., from the top image to mid-dle image in Fig. 2) without changing the person’s iden-tity (the bottom image in Fig. 2). Second, the parameters learned from modifying one image should transfer well to other similar images. For example, the optimal parameters that can add smile to one person should also work for im-ages of different people with different genders and races.
Previous studies have discovered that GANs are inher-ently endowed with a strong disentanglement capability.
Speciﬁcally, it is found that there exist certain directions in the latent space separately controlling different attributes.
Therefore, by identifying these directions, e.g., via prin-cipal component analysis [19], GAN can achieve effective disentanglement without any re-training or ﬁne-tuning. On the other hand, such an inherent disentanglement capabil-ity has yet to be found in diffusion models. Hence come our research questions: Do diffusion models also possess a disentanglement capability with the aforementioned nice properties? If so, how can we uncover it?
In this paper, we seek to answer these research questions.
Our ﬁnding is that for stable diffusion model [61], one of the diffusion models that can generate images based on an in-put text description, disentangled image modiﬁcations can be achieved by partial modiﬁcations in the text embedding space. In particular, if we ﬁx the standard Gaussian noises introduced in the denoising process, and partially change the input text embedding from a neutral description (e.g., “a photo of person”) to one with style (e.g., “a photo of person with smile”), the generated image will also shift towards the target style without changing the semantic content. Based on this ﬁnding, we further propose a simple, light-weight al-gorithm, where we optimize the mixing weights of the two text embeddings under two objectives, a perceptual loss for content preservation and a CLIP-based style matching loss.
The entire process only involves optimizing over around 50 parameters and does not ﬁne-tune the diffusion model.
Our experiments show that the inherent disentanglement capability in stable diffusion model can already disentan-gle a wide range of concepts and attributes, ranging from global styles such as painting styles to local styles like fa-cial expressions, as shown in Table 1. As shown in Fig. 1, by learning the optimal mixing weights of the two de-scriptions, stable diffusion models can generate convinc-ing image pairs that only modify the target attribute, and the optimal weights can generalize well to different im-ages. The experiment results also show that our proposed image editing algorithm, without ﬁne-tuning the diffusion model, can match or outperform the more sophisticated diffusion-model-based image-editing baselines that require
ﬁne-tuning. The ﬁndings of this paper can shed some light on how diffusion models work and how they can be applied to image editing tasks. 2.