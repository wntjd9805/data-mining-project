Abstract
Language-based colorization produces plausible col-ors consistent with the language description provided by the user. Recent studies introduce additional annotation to prevent color-object coupling and mismatch issues, but they still have difﬁculty in distinguishing instances corre-sponding to the same object words. In this paper, we pro-pose a transformer-based framework to automatically ag-gregate similar image patches and achieve instance aware-ness without any additional knowledge. By applying our presented luminance augmentation and counter-color loss to break down the statistical correlation between luminance and color words, our model is driven to synthesize colors with better descriptive consistency. We further collect a dataset to provide distinctive visual characteristics and de-tailed language descriptions for multiple instances in the
# Equal contributions. * Corresponding author. same image. Extensive experiments demonstrate our ad-vantages of synthesizing visually pleasing and description-consistent results of instance-aware colorization. 1.

Introduction
Image colorization aims to predict missing chromatic channels from a given grayscale image, which has been widely used in black-and-white image restoration, artistic creation, and image compression. Since there are multiple reasonable choices for the colorization result, an increasing amount of effort has focused on introducing user-friendly interactions to determine a unique solution, e.g., user scrib-ble [33, 51], and reference example [2, 16, 47].
In con-trast to these visually-concrete conditions, the language de-scriptions have higher information density to ﬂexibly repre-sent high-level semantics, which empowers the colorization model to concrete visually-abstract user intention.
Language-based colorization aims to produce visually
pleasing and description-consistent results guided by the user-provided caption.
In such a task, the most crucial stage is to establish the correspondence between the col-ors in the language description and the regions in the im-age. Cross-modality feature fusion modules are designed in earlier methods [8, 29, 45, 56], but they are ineffective in generating satisfactory results on samples with fewer ob-served color-object correspondences and insufﬁcient color descriptions. By introducing additionally annotated cor-respondences between object words and color words, re-markable improvements are observed on recently reported results on a wide variety of images [6, 42], but these meth-ods still face challenges in distinguishing instances corre-sponding to the same object words (e.g., the “woman” in
Fig. 1 top/bottom right). While introducing additional ex-ternal priors (e.g., detection boxes [35]) is an alternative ap-proach to achieve instance-aware colorization, it may not perform well on “out-of-distribution” scenarios [41].
In this paper, we propose Language-based Colorization with Instance awareness (L-CoIns) to adaptively establish the correspondence between instance regions and color de-scriptions without additionally using external priors. L-CoIns considers an image as a composition of a number of groups with similar colors, hence adopting a group-ing mechanism to automatically aggregate similar image patches for correctly identifying corresponding regions to be colorized (Fig. 1 top left, regions of women are cor-rectly identiﬁed) and distinguishing instances correspond-ing to the same object words (Fig. 1 top right, correspond-ing colors are assigned to different instances) in an unsuper-vised manner. Our model is able to more ﬂexibly assign col-ors for instances, even when correspondences never occur during training, as opposed to learning manually annotated multiple color-object correspondences (Fig. 1 bottom left, the correspondence between violet and shirt is unobserved).
We propose the luminance augmentation and counter-color loss to break down the statistical correlation between lumi-nance and color words so that L-CoIns could produce col-orization results that are more consistent with the given lan-guage description (Fig. 1 bottom right, yellow and orange successfully colorize darker and brighter regions).
Our contribution could be summarized as follows:
• Without additionally annotating correspondences or external priors, we provide the grouping transformer to aggregate similar image patches and learn inter-group relations for instance-aware language colorization.
• We present the luminance augmentation and counter-color loss that stick the model to colorize according to the language description rather than the statistical correlation between luminance and color words.
• We collect a multi-instance dataset that offers mis-cellaneous cases with distinctive visual characteris-tics and detailed language descriptions for various in-stances within an image. 2.