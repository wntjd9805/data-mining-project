Abstract
Unsupervised domain adaptation (UDA) in semantic segmentation transfers the knowledge of the source domain to the target one to improve the adaptability of the seg-mentation model in the target domain. The need to access labeled source data makes UDA unable to handle adap-tation scenarios involving privacy, property rights protec-tion, and conﬁdentiality. In this paper, we focus on unsu-pervised model adaptation (UMA), also called source-free domain adaptation, which adapts a source-trained model to the target domain without accessing source data. We ﬁnd that the online self-training method has the potential to be deployed in UMA, but the lack of source domain loss will greatly weaken the stability and adaptability of the method.
We analyze two reasons for the degradation of online self-training, i.e. inopportune updates of the teacher model and biased knowledge from the source-trained model. Based on this, we propose a dynamic teacher update mechanism and a training-consistency based resampling strategy to im-prove the stability and adaptability of online self-training.
On multiple model adaptation benchmarks, our method ob-tains new state-of-the-art performance, which is compara-ble or even better than state-of-the-art UDA methods. The code is available at https://github.com/DZhaoXd/DT-ST. 1.

Introduction
Unsupervised Domain Adaptation (UDA) has received extensive attention on semantic segmentation tasks [49, 59, 60, 63], which transfers the knowledge in the source do-mains (e.g. synthetic scene) to the target ones (e.g. real scene). UDA in semantic segmentation aims to alleviate the dependence of deep neural network-based models on dense annotations [18, 46, 61] and improve their generaliza-This work is supported by the National Natural Science Foundation of
China(No.62271377, No.62201407), the Key Research and Development
Program of Shannxi (No.2021ZDLGY01-06, No.2022ZDLGY01-12), the
National Key R&D Program of China (No. 2021ZD0110404), the China
Postdoctoral Science Foundation (No. 2022M722496), the Foreign Schol-ars in University Research and Teaching Program’s 111 Project (B07048).
Figure 1. Under the unsupervised model adaptation (UMA) set-ting, the mIoU score (%) of different methods on the validation set throughout the training in GTA5 → Cityscapes adaptation task.
The dashed line represents the self-training UDA methods, and the solid line represents the MDA methods. tion ability to target domains [5, 12, 15]. However, in pro-prietary, privacy, or proﬁt-related concerns, source domain data is often unavailable, which presents new challenges for
UDA [9, 27, 55]. To this end, the setting of Unsupervised
Model Adaptation (UMA) is proposed [6,9,21,30,35], aim-ing to adapt the source-trained model to the unlabeled target domain without using source domain data.
In UMA, the knowledge in the source-trained model becomes the only available supervision signal, making self-training on pseudo-labels the mainstream in the ﬁeld.
Most existing UMA methods [26, 36, 57] adopt ofﬂine self-training methods, which iteratively updates the pseudo-labels and retrains the models. Although some improve-ments have been made, iterative self-training requires ex-pert intervention [1,59], as ill-suited rounds and termination often make it under-adapted.
The recently proposed online self-training (ONST) methods [1, 31, 59] in UDA avoid the iterative training by online co-evolving pseudo labels, showing great potential.
Then can ONST be applied to UMA scenarios without ac-cessing source data? We deploy the state-of-the-art ONST
methods ProDA [1], SAC [59] and CPST [31] to UMA and draw the mIoU score curve on the validation set dur-ing training, as shown in Fig. 1. These ONST methods (dashed line) achieve more competitive performance than existing UMA methods (solid line). Nevertheless, taking a closer look at the curves in Fig. 1, these ONST methods present different degrees of degradation and unstable adap-tation process. Besides, their best performance in UMA de-creased by 4% − 5% mIoU scores on average than in UDA (See Table 1 and 2 in detail). Consequently, we conclude that existing ONST methods suffer from impaired stability and adaptability when applied to UMA.
This paper is committed to improving the stability and adaptability of ONST methods in UMA. To begin with, we explore two reasons for the poor stability and adaptabil-ity of ONST in UMA. (1) The inopportune update of the teacher model causes the failure of co-evolution because the teacher model will continuously aggregate unevolved stu-dents. Concretely, as the teacher becomes the only supervi-sor in UMA, rapid updating will make the student lose the direction of evolution, and slow updating will make the stu-dent overﬁt the historical supervision, all of which leads to humble beneﬁts of teachers’ updating. (2) The bias towards minority categories in the source-trained model results in insufﬁcient adaptation to those minorities as the bias is eas-ily ampliﬁed in ONST, even with heuristic [1] or prototype thresholding [59] being set.
Next, we present the explored solutions. For (1), we ﬁnd that the student model’s performance on historical samples during evolution can feedback on whether the student has evolved. Consequently, we propose a Dynamic Teacher
Update (DTU) mechanism. DTU explores two feedback signals by information entropy [13] and soft neighborhood density [45], which can assess the evolutionary state of stu-dents. DTU then dynamically controls the update inter-val of the teacher model according to the students’ feed-back to aggregate more evolved students. For (2), we ﬁnd that resampling minority categories can effectively alleviate the bias towards minorities in UMA. However, most exist-ing resampling strategies [10, 11, 20, 50] rely on the source data and cannot apply in UMA. To this end, we propose a Training-Consistency based Resampling (TCR) strategy.
TCR adaptively estimates the biased categories from the being-adapted model and selects reliable samples in biased categories as resampling candidates. Through these efforts, our method greatly improves the stability and adaptability of ONST in UMA, as shown in Fig. 1 (red solid line). We refer our method to DT-ST, as DTU and TCR play critical parts in online Self-Training under UMA.
Sufﬁcient experiments show that DT-ST further exploits the potential of online self-training in UMA, towards bet-ter stability and adaptability. Moreover, DT-ST obtains new state-of-the-art performance on different UMA benchmarks and achieves comparable or even better performance than advanced UDA methods. 2.