Abstract
Interactive image segmentation aims at obtaining a seg-mentation mask for an image using simple user annotations.
During each round of interaction, the segmentation result from the previous round serves as feedback to guide the userâ€™s annotation and provides dense prior information for the segmentation model, effectively acting as a bridge be-tween interactions. Existing methods overlook the impor-tance of feedback or simply concatenate it with the orig-inal input, leading to underutilization of feedback and an increase in the number of required annotations. To address this, we propose an approach called Focused and Collabo-rative Feedback Integration (FCFI) to fully exploit the feed-back for click-based interactive image segmentation. FCFI first focuses on a local area around the new click and cor-rects the feedback based on the similarities of high-level features. It then alternately and collaboratively updates the feedback and deep features to integrate the feedback into the features. The efficacy and efficiency of FCFI were validated on four benchmarks, namely GrabCut, Berkeley, SBD, and
DAVIS. Experimental results show that FCFI achieved new state-of-the-art performance with less computational over-head than previous methods. The source code is available at https://github.com/veizgyauzgyauz/FCFI.
Figure 1. An overview of the interactive system and the com-parison among (a) independent segmentation [24], (b) feedback-as-input segmentation [35], and (c) deep feedback-integrated seg-mentation. See the description in Sec. 1. Throughout this paper, green/red clicks represent foreground/background annotations. 1.

Introduction
Interactive image segmentation aims to segment a target object in an image given simple annotations, such as bound-ing boxes [17, 32, 37, 38, 40], scribbles [1, 10, 18], extreme points [2,27,29,42], and clicks [6,15,23,24,34,39]. Due to its inherent characteristic, i.e., interactivity, it allows users to add annotations and receive refined segmentation results iteratively. Unlike semantic segmentation, interactive im-age segmentation can be applied to unseen categories (cate-gories that do not exist in the training dataset), demonstrat-ing its generalization ability. Additionally, compared with instance segmentation, interactive segmentation is specific since it only localizes the annotated instance. Owing to these advantages, interactive image segmentation is a pre-liminary step for many applications, including image edit-ing and image composition. This paper specifically focuses on interactive image segmentation using click annotations because clicks are less labor-intensive to obtain than other types of annotations.
The process of interaction is illustrated in Fig. 1. Given an image, users start by providing one or more clicks to label background or foreground pixels. The segmentation model then generates an initial segmentation mask for the
target object based on the image and the clicks. If the seg-mentation result is unsatisfactory, users can continue to in-teract with the segmentation system by marking new clicks to indicate wrongly segmented regions and obtain a new segmentation mask. From the second round of interaction, the segmentation result of the previous interaction - referred to as feedback in this paper - will be fed back into the cur-rent interaction. This feedback is instrumental in user label-ing and provides the segmentation model with prior infor-mation, which can facilitate convergence and improve the accuracy of segmentation [35].
Previous methods have made tremendous progress in interactive image segmentation. Some of them, such as
[24,26,35], focused on finding useful ways to encode anno-tated clicks, while others, like [5,15,19,21,34], explored ef-ficient neural network architectures to fully utilize user an-notations. However, few methods have investigated how to exploit informative segmentation feedback. Existing meth-ods typically treated each round of interaction independent
[5, 12, 15, 19, 24, 26] or simply concatenated feedback with the initial input [6,23,25,34,35]. The former approach (Fig. 1(a)) failed to leverage the prior information provided by feedback, resulting in a lack of consistency in the segmen-tation results generated by two adjacent interactions. In the latter case (Fig. 1(b)), feedback was only directly visible to the first layer of the network, and thus the specific spatial and semantic information it carried would be easily diluted or even lost through many convolutional layers, similar to the case of annotated clicks [12].
In this paper, we present Focused and Collaborative
Feedback Integration (FCFI) to exploit the segmentation feedback for click-based interactive image segmentation.
FCFI consists of two modules: a Focused Feedback Cor-rection Module (FFCM) for local feedback correction and a
Collaborative Feedback Fusion Module (CFFM) for global feedback integration into deep features. Specifically, the
FFCM focuses on a local region centered on the new an-notated click to correct feedback. It measures the feature similarities between each pixel in the region and the click.
The similarities are used as weights to blend the feedback and the annotated label. The CFFM adopts a collabora-tive calibration mechanism to integrate the feedback into deep layers (Fig. 1(c)). First, it employs deep features to globally update the corrected feedback for further improv-ing the quality of the feedback. Then, it fuses the feed-back with deep features via a gated residual connection.
Embedded with FCFI, the segmentation network leveraged the prior information provided by the feedback and outper-formed many previous methods. 2.