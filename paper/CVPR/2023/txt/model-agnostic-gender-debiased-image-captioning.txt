Abstract
Image captioning models are known to perpetuate and amplify harmful societal bias in the training set.
In this work, we aim to mitigate such gender bias in image caption-ing models. While prior work has addressed this problem by forcing models to focus on people to reduce gender mis-classification, it conversely generates gender-stereotypical words at the expense of predicting the correct gender. From this observation, we hypothesize that there are two types of gender bias affecting image captioning models: 1) bias that exploits context to predict gender, and 2) bias in the prob-ability of generating certain (often stereotypical) words be-cause of gender. To mitigate both types of gender biases, we propose a framework, called LIBRA, that learns from syn-thetically biased samples to decrease both types of biases, correcting gender misclassification and changing gender-stereotypical words to more neutral ones. 1.

Introduction
In computer vision, societal bias, for which a model makes adverse judgments about specific population sub-groups usually underrepresented in datasets, is increasingly concerning [4, 6, 7, 11, 17, 22, 41, 43, 52, 57]. A renowned example is the work by Buolamwini and Gebru [7], which demonstrated that commercial facial recognition models predict Black women with higher error rates than White men. The existence of societal bias in datasets and models is extremely problematic as it inevitably leads to discrimina-tion with potentially harmful consequences against people in already historically discriminated groups.
One of the computer vision tasks in which societal bias is prominent is image captioning [49, 58], which is the task of generating a sentence describing an image. Notably, image captioning models not only reproduce the societal bias in the training datasets, but also amplify it. This phenomenon is known as bias amplification [10,15,24,42,64] and makes models produce sentences more biased than the ones in the original training dataset. As a result, the generated sen-tences can contain stereotypical words about attributes such as gender that are sometimes irrelevant to the images.
Our study focuses on gender bias in image captioning models. First, based on the observations in previous work
[5,8,18,44,51], we hypothesize that there exist two different types of biases affecting captioning models:
Type 1. context → gender bias, which makes captioning models exploit the context of an image and precedently
Figure 2. Overview of LIBRA. For the original captions (i.e., ground-truth captions written by annotators), we synthesize biased captions with context → gender or/and gender → context bias (Biased Caption Synthesis). Then, given the biased captions and the original images, we train an encoder-decoder captioner, Debiasing Caption Generator, to debias the input biased captions (i.e., predict original captions). generated words, increasing the probability of predict-ing certain gender, as shown in Figure 1 (a).
Type 2. gender → context bias, which increases the prob-ability of generating certain words given the gender of people in an image, as shown in Figure 1 (b).
Both types of biases can result in captioning models gener-ating harmful gender-stereotypical sentences.
A seminal method to mitigate gender bias in image cap-tioning is Gender equalizer [8], which forces the model to focus on image regions with a person to predict their gen-der correctly. Training a captioning model using Gender equalizer successfully reduces gender misclassification (re-ducing context → gender bias). However, focusing only on decreasing such bias can conversely amplify the other type of bias [18,51]. For example, as shown in Figure 6, a model trained to correctly predict the gender of a person can pro-duce other words that are biased toward that gender (ampli-fying gender → context bias). This suggests that methods for mitigating bias in captioning models must consider both types of biases.
We propose a method called LIBRA (model-agnostic debiasing framework) to mitigate bias amplification in im-age captioning by considering both types of biases. Specif-ically, LIBRA consists of two main modules: 1) Bi-ased Caption Synthesis (BCS), which synthesizes gender-biased captions (Section 3), and 2) Debiasing Caption
Generator (DCG), which mitigates bias from synthesized captions (Section 4). Given captions written by anno-tators, BCS synthesizes biased captions with gender → context or/and context → gender biases.
DCG is the original caption given a then trained to recover
⟨synthetic biased caption, image⟩ pair. Once trained, DCG can be used on top of any image captioning models to miti-gate gender bias amplification by taking the image and gen-erated caption as input. Our framework is model-agnostic and does not require retraining image captioning models.
Extensive experiments and analysis, including quantita-tive and qualitative results, show that LIBRA reduces both types of gender biases in most image captioning models on various metrics [8, 18, 44, 66]. This means that DCG can correct gender misclassification caused by the context of the image/words that is biased toward a certain gender, mitigating context → gender bias (Figure 1 (a)). Also, it tends to change words skewed toward each gender to less biased ones, mitigating gender → context bias (Figure 1 (b)). Furthermore, we show that evaluation of the generated captions’ quality by a metric that requires human-written captions as ground-truth (e.g., BLEU [30] and SPICE [1]) likely values captions that imitate how annotators tend to describe the gender (e.g., women posing vs. men standing). 2.