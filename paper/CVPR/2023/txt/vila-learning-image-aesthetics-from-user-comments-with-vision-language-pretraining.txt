Abstract
Assessing the aesthetics of an image is challenging, as it is inﬂuenced by multiple factors including composition, color, style, and high-level semantics. Existing image aes-thetic assessment (IAA) methods primarily rely on human-labeled rating scores, which oversimplify the visual aes-thetic information that humans perceive. Conversely, user comments offer more comprehensive information and are a more natural way to express human opinions and prefer-ences regarding image aesthetics. In light of this, we pro-pose learning image aesthetics from user comments, and ex-ploring vision-language pretraining methods to learn mul-timodal aesthetic representations. Speciﬁcally, we pretrain an image-text encoder-decoder model with image-comment pairs, using contrastive and generative objectives to learn rich and generic aesthetic semantics without human labels.
To efﬁciently adapt the pretrained model for downstream
IAA tasks, we further propose a lightweight rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept. Our results show that our pretrained aes-thetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and it has powerful zero-shot capability for aesthetic tasks such as zero-shot style classiﬁcation and zero-shot IAA, sur-passing many supervised baselines. With only minimal ﬁne-tuning parameters using the proposed adapter module, our model achieves state-of-the-art IAA performance over the
AVA dataset. 1 1.

Introduction
Image Aesthetic Assessment (IAA) aims to quantify the hu-man perceived aesthetics of an image. It has many impor-tant applications, including photo recommendation, selec-tion, and editing. IAA is challenging because it is inherently subjective, and depends on various factors including image 1Our model is available at https://github.com/google-research/google-research/tree/master/vila
Figure 1. We present VILA, a vision-language aesthetics learning framework based on image and user comment pairs. By pretrain-ing on a contrastive and generative target, it shows superior perfor-mance on aesthetic captioning as well as zero-shot aesthetic tasks, e.g., IAA, and style classiﬁcation. With a lightweight rank-based adapter, we can efﬁciently adapt the pretrained model to IAA. composition, color usage, photographic style, and subject matter. In recent years, various learning-based IAA meth-ods have been proposed by leveraging deep models such as convolutional neural networks (CNN) [2, 12, 15, 42] and transformers [19]. These approaches learn from human-labeled IAA datasets where images are paired with aesthetic ratings, and models are trained to regress towards the mean opinion scores (MOS).
Directly learning IAA models on human-labeled aes-thetic ratings, such as MOS, can be suboptimal as it lacks context regarding why an image is aesthetically pleasing or not. To provide richer supervision, various methods have attempted to integrate external knowledge such as theme [12, 33], human eye ﬁxation [9], and aesthetic at-tributes [4, 24], to enhance IAA performance. These ap-proaches typically rely on multitask training or cascade
score prediction with a frozen attribute network. However, obtaining additional labeled data or off-the-shelf models for such methods can be costly.
Compared to the aforementioned methods that require additional annotations, our approach utilizes the abundance of image-comment pairs available on aesthetic websites and photographic forums. These pairs can be easily ob-tained from the Internet and contain extensive aesthetic in-formation (e.g. objects, themes, styles, and user emotions), since humans are better at expressing aesthetic preferences through natural language than through abstract scores. On image sharing platforms like Flickr and DPChallenge2, user comments offer valuable insights into how they evaluate an image’s aesthetics. For instance, as shown in Fig. 1 (top), comments such as “very cool patterns and curls” and “little bit on the blurry side” reﬂects users’ positive and negative aesthetic opinions respectively. We aim to learn the diverse aesthetic semantics present in these image-comment pairs to establish a solid foundation for downstream IAA tasks.
Using image-comment pairs for aesthetics learning re-mains largely unexplored. While previous works have leveraged user comments to improve IAA, their approaches differ signiﬁcantly from ours. For example, [14,57,58] pro-posed to aggregate visual and comment features, yet they require both the image and comment as inputs during infer-ence. This requirement makes it difﬁcult to use such meth-ods in real-world settings where images may not always be accompanied by comments. To mitigate this, Niu et al. [33] proposed to use the LDA topics [1] from the comments as pseudo labels to guide image representation learning. How-ever, the simpliﬁcation of comments into topics may result in a loss of valuable contextual information. Therefore, we are motivated to explore other strategies for utilizing raw comments to extract richer aesthetic textual information.
In this paper, we present a novel two-stage VIsion-Language Aesthetics (VILA) learning framework incor-porating image-text pretraining. Our goal is to develop a model that can effectively generalize to multiple down-In the ﬁrst Pretraining stream aesthetic tasks (Fig. 1). stage, we learn an image-text model (VILA-P) by employ-ing contrastive and text sequence generation objectives, en-baling us to fully leverage ﬁne-grained knowledge from aesthetic image-comment pairs. Our approach is moti-vated by recent advancements in vision-language models, such as CLIP [35], ALIGN [17], and CoCa [54], which exhibit impressive performance and generalization ability across multiple tasks. These models align vision and lan-guage feature spaces to capture the rich semantic infor-mation. However, these models are typically pretrained on general image-text pairs from the web, which can re-sult in under-representation of aesthetic-related informa-tion. Our experimental results indicate that such gener-2https://www.dpchallenge.com/ ally pretrained vision-language models underperform on aesthetic tasks (Sec. 5.3). As a solution, we propose the adoption of vision-language pretraining on aesthetic image-comment pairs from photograph sharing websites. To the best of our knowledge, our work is the ﬁrst to explore the use of image-comment pairs in vision-language pretraining for aesthetics learning.
After pretraining VILA-P on image-comment pairs, we
ﬁnetune it for downstream score-based IAA tasks using a lightweight Rank-based adapter (VILA-R). This adapter involves adding feature residuals to the frozen image em-beddings to move images with high aesthetic quality closer to the anchor text “good image,” and images with low aes-thetic quality away from it. This method can effectively rank images based on human rated preferences. With 0.1% tunable parameters, our model outperforms previous works on IAA correlation metrics over the AVA dataset [32].
Our proposed VILA is capable of tackling multiple aesthetic-related tasks beyond score-based IAA (Fig. 1).
Not only can it generate high-quality aesthetic comments, but it also exhibits impressive zero-shot learning (ZSL) ca-pabilities for aesthetic style classiﬁcation and quality anal-ysis. Using text queries such as “good image” and “bad image” to compare images, our ZSL model outperforms su-pervised learning models like NIMA [42] which requires labor-intensive ratings as ground truth. This highlights the potential of learning rich image aesthetic concepts without relying on human-labeled data, thereby signiﬁcantly reduc-ing data collection costs.
We summarize the contributions of our work as follows:
• We propose a vision-language aesthetic learning framework (VILA) for learning rich image aesthetic features using image-comment pairs. for aesthetic
• We design a novel rank-based module to adapt the model to downstream IAA tasks without perturbing the pretrained weights, effectively learning the aesthetic quality concepts with minimal additional parameters.
• Our pretrained aesthetic model outperforms prior captioning on the AVA-works
Captions [10] dataset. Even without any supervised labels, our zero-shot model achieves 69% mAP on the AVA-Style [32] dataset and 0.657 SRCC on the
AVA dataset [32], outperforming many supervised approaches. With the proposed adapter and a small number of tunable parameters, our method further achieves state-of-the-art performance on AVA. 2.