Abstract learner, named AttriCLIP,
Continual learning aims to enable a model to incre-mentally learn knowledge from sequentially arrived data.
Previous works adopt the conventional classification architecture, which consists of a feature extractor and a classifier. The feature extractor is shared across sequen-tially arrived tasks or classes, but one specific group of weights of the classifier corresponding to one new class the should be incrementally expanded. Consequently, parameters of a continual learner gradually increase.
Moreover, as the classifier contains all historical arrived classes, a certain size of the memory is usually required to store rehearsal data to mitigate classifier bias and catastrophic forgetting. In this paper, we propose a non-incremental to incrementally extract knowledge of new classes or tasks. Specifically,
AttriCLIP is built upon the pre-trained visual-language model CLIP. Its image encoder and text encoder are fixed to extract features from both images and text. Text consists of a category name and a fixed number of learnable parameters which are selected from our designed attribute word bank and serve as attributes. As we compute the visual and textual similarity for classification, AttriCLIP is a non-incremental learner. The attribute prompts, which encode the common knowledge useful for classification, can effectively mitigate the catastrophic forgetting and avoid constructing a replay memory. We evaluate our AttriCLIP and compare it with CLIP-based and previous state-of-the-art continual learning methods in realistic settings with domain-shift and long-sequence learning. The results show that our method performs favorably against previous state-of-the-arts. The implementation code will be available at https://gitee.com/mindspore/models/tree/master/research/ cv/AttriCLIP. 1.

Introduction
In recent years, deep neural networks have achieved re-markable progress in classification when all the classes (or
*Co-First Author.
†Corresponding Author. tasks) are jointly trained. However, in real scenarios, the tasks or classes usually sequentially arrive. Continual learn-ing [13, 21, 28] aims to train a model which incrementally expands its knowledge so as to deal with all the histori-cal tasks or classes, behaving as if those tasks or classes are jointly trained. The conventional continual learning methods learn sequentially arrived tasks or classes with a shared model, as shown in Fig. 1(a). Such processing that fine-tunes the same model in sequence inevitably results in subsequent values of the parameters overwriting previous ones [20], which leads to catastrophic forgetting. Besides, the classification ability on historical data can be easily de-stroyed by current-stage learning. In the conventional con-tinual learning methods, a classifier on top of the feature extractor is employed to perform recognition. As one group of weights in the classifier is responsible for the prediction of one specific class, the classifier needs to be expanded sequentially to make a continual learner able to recognize novel classes. Moreover, extra replay data is usually re-quired to reduce the classifier bias and the catastrophic for-getting of learned features. It is still challenging if we ex-pect a non-incremental learner, i.e., the trainable parame-ters of the model do not incrementally increase and no re-play data is needed to avoid the classifier bias and the catas-trophic forgetting.
To address the above issues, this paper proposes a con-tinual learning method named AttriCLIP , which adopts the frozen encoders of CLIP [19]. It is a typical visual-language model that conducts image classification by contrasting the features of images and their descriptive texts. In the face of increasing instances, we design a prompt tuning scheme for continual learning. As shown in Fig. 1(b), there are simi-lar attributes in images with different categories, such as “a brown-white dog lying on the grass” and “a brown-white cat lying on the grass”. They belong to different categories but both have “lying on the grass” and “brown-white” at-tributes, so the distance between these two images of differ-ent categories may be close in the feature space. Therefore, we selectively train different prompts based on the attributes of the images rather than the categories. In this way, there is
Figure 1. (a) Traditional framework for continual learning. The encoder and the classifier are trained by tasks in sequence, some of which even need extra memory data. In the framework, the model parameters of the current task are fine-tuned from the parameters trained by the previous last task and then are used for the classification of all seen tasks. The total number of categories the model can classify is fixed in the classifier. (b) Our proposed AttriCLIP for continual learning. AttriCLIP is based on CLIP, which classifies images by contrasting them with their descriptive texts. The trainable prompts are selected by the attributes of the current image from a prompt pool. The prompts are different if the attributes of the image are different. The trained prompts are concatenated with the class name of the image, which serve as a more accurate supervised signal for image classification than labels. no problem of knowledge overwriting caused by sequential training of the same model with increasing tasks.
Specifically, an attribute word bank is constructed as shown in Fig 1(b), which consists of a set of (key, prompt) pairs. The keys represent the local features (attributes) of images and the prompts represent the descriptive words cor-responding to the keys. Several prompts are selected ac-cording to the similarities between their keys and the in-put image. The selected prompts are trained to capture an accurate textual description of the attributes of the image.
If images with different labels have similar attributes, it is also possible to select the same prompts, i.e., the same prompts can be trained with images of different categories.
Similarly, different prompts can be trained by the images of the same category. The trained prompts are concate-nated with the class names and put into the text encoder to contrast with the image feature from the image encoder.
This process makes our AttriCLIP distinct from all previous classifier-based framework as it serves as a non-incremental learner without the need to store replay data. The goal of our method is to select the existing attributes of the current image as the text description in the inference process, to classify the image. In addition, the structure of AttriCLIP can also avoid the problem of the increasing classifier pa-rameters with the increase of the tasks and the problem of the inefficiency about memory data in the traditional con-tinual learning methods.
The experimental setup of existing continual learning methods is idealized which divides one dataset into sev-eral tasks for continual learning. The model can set the output dimensionality of the classifier according to the to-tal number of categories in the dataset. In practical appli-cations, with the continuous accumulation of data, the to-tal number of categories of samples usually cannot be ob-tained when the model is established. When the total num-ber of categories exceeds the preset output dimensionality of the classifier, the model has to add parameters to classi-fier and requires the previous samples for fine-tuning, which greatly increases the training burden. Therefore, the contin-ual learning approaches are required to have the ability to adapt to the categories of freely increasing data, i.e., the model capacity should not have a category upper limit. In order to measure such ability of continual learning models, we propose a Cross-Datasets Continual Learning (CDCL) setup, which verifies the classification performance of the model on long-sequence domain-shift tasks. The contribu-tions of this paper are summarized as follows:
• We establish AttriCLIP, which is a prompt tuning ap-proach for continual learning based on CLIP. We train different prompts according to the attributes of images to avoid knowledge overwriting caused by training the same model in sequence of classes.
• AttriCLIP contrasts the images and their descriptive texts based on the learned attributes. This approach avoids the memory data requirement for fine-tuning the classifier of increasing size.
• In order to evaluate the performance of the model on long-sequence domain-shift tasks, we propose a
Cross-Datasets Continual Learning (CDCL) experi-mental setup. AttriCLIP exhibits excellent perfor-mance and training efficiency on CDCL. 2.