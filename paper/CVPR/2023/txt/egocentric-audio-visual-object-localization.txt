Abstract
Humans naturally perceive surrounding scenes by uni-fying sound and sight from a first-person view. Likewise, machines are advanced to approach human intelligence by learning with multisensory inputs from an egocentric per-spective.
In this paper, we explore the challenging ego-centric audio-visual object localization task and observe that 1) egomotion commonly exists in first-person record-ings, even within a short duration; 2) The out-of-view sound components can be created when wearers shift their atten-tion. To address the first problem, we propose a geometry-aware temporal aggregation module that handles the ego-motion explicitly. The effect of egomotion is mitigated by estimating the temporal geometry transformation and ex-ploiting it to update visual representations. Moreover, we propose a cascaded feature enhancement module to over-come the second issue.
It improves cross-modal local-ization robustness by disentangling visually-indicated au-dio representation. During training, we take advantage of the naturally occurring audio-visual temporal synchro-nization as the “free” self-supervision to avoid costly la-beling. We also annotate and create the Epic Sounding
Object dataset for evaluation purposes. Extensive experi-ments show that our method achieves state-of-the-art local-ization performance in egocentric videos and can be gener-alized to diverse audio-visual scenes. Code is available at https://github.com/WikiChao/Ego-AV-Loc. 1.

Introduction
The emergence of wearable devices has drawn the at-tention of the research community to egocentric videos, the significance of which can be seen from egocentric research in a variety of applications such as robotics [32,34,48], aug-mented/virtual reality [31,61,75], and healthcare [53,66]. In recent years, the computer vision community has made sub-stantial efforts to build benchmarks [12, 13, 15, 40, 57, 69], establish new tasks [17, 36, 37, 39, 60], and develop frame-works [33, 41, 54, 82] for egocentric video understanding.
While existing works achieve promising results in the
Figure 1. Sounding object localization in egocentric videos.
Due to the wearer’s egomotion, the viewpoint changes continu-ously across time. Consequently, audio-visual relations are dy-namically changing in egocentric videos. Our approach tackles challenges in the egocentric audio-visual sounding object task and learns audio-visual associations from first-person videos. egocentric domain, it still remains an interesting but chal-lenging topic to perform fine-grained egocentric video un-derstanding. For instance, understanding which object is emitting sound in a first-person recording is difficult for ma-chines. As shown in Fig. 1, the wearer moves his/her head to put down the bottle. The frying pot which emits sound subsequently suffers deformation and occlusion due to the wearer’s egomotion. Human speech outside the wearer’s view also affects the machine’s understanding of the current scene. This example reveals two significant challenges for designing powerful and robust egocentric video understand-ing systems: First, people with wearable devices usually record videos in naturalistic surroundings, where a variety of illumination conditions, object appearance, and motion patterns are shown. The dynamic visual variations intro-duce difficulties in accurate visual perception. Second, ego-centric scenes are often perceived within a limited field of view (FoV). The common body and head movements cause frequent view changes (see Fig. 1), which brings object de-formation and creates dynamic out-of-view content.
Although a visual-only system may struggle to fully de-code the surrounding information and perceive scenes in egocentric videos, audio provides stable and persistent sig-nals associated with the depicted events. Instead of purely visual perception, numerous psychological and cognitive
studies [6, 29, 67, 73] show that integration of auditory and visual signals is significant in human perception. Audio, as an essential but less focused modality, often provides syn-chronized and complementary information with the video stream. In contrast to the variability of first-person visual footage, sound describes the underlying scenes consistently.
These natural characteristics make audio another indispens-able ingredient for egocentric video understanding.
To effectively leverage audio and visual information in egocentric videos, a pivotal problem is to analyze the fine-grained audio-visual association, specifically identifying which objects are emitting sounds in the scene. In this pa-per, we explore a novel egocentric audio-visual object lo-calization task, which aims to associate audio with dynamic visual scenes and localize sounding objects in egocentric videos. Given the dynamic nature of egocentric videos, it is exceedingly challenging to link visual content from var-ious viewpoints with audio captured from the entire space.
Hence, we develop a new framework to model the distinct characteristics of egocentric videos by integrating audio.
In the framework, we propose a geometry-aware tempo-ral module to handle egomotion explicitly. Our approach mitigates the impact of egomotion by performing geometric transformations in the embedding space and aligning visual features from different frames. We further use the aligned features to leverage temporal contexts across frames to learn discriminative cues for localization. Additionally, we intro-duce a cascaded feature enhancement module to handle out-of-view sounds. The module helps mitigate audio noises and improves cross-modal localization robustness.
Due to the dynamic nature of egocentric videos, it is hard and costly to label sounding objects for supervised train-ing. To avoid tedious labeling, we formulate this task in a self-supervised manner, and our framework is trained with audio-visual temporal synchronization. Since there are no publicly available egocentric sounding object localization datasets, we annotate an Epic Sounding dataset to facilitate research in this field. Experimental results demonstrate that modeling egomotion and mitigating out-of-view sound can improve egocentric audio-visual localization performance.
In summary, our contributions are: (1) the first system-atical study on egocentric audio-visual sounding object lo-calization; (2) an effective geometry-aware temporal aggre-gation approach to deal with unique egomotion; (3) a novel cascaded feature enhancement module to progressively in-ject localization cues; and (4) an Epic Sounding Object dataset with sounding object annotations to benchmark the localization performance in egocentric videos. 2.