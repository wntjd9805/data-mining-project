Abstract
Is she looking at me? 
When did I   knead dough? 
What am I doing? 
Different video understanding tasks are typically treated in isolation, and even with distinct types of curated data (e.g., classifying sports in one dataset, tracking animals in another). However, in wearable cameras, the immer-sive egocentric perspective of a person engaging with the world around them presents an interconnected web of video understanding tasks—hand-object manipulations, naviga-tion in the space, or human-human interactions—that un-fold continuously, driven by the person’s goals. We argue that this calls for a much more uniﬁed approach. We pro-pose EgoTask Translation (EgoT2), which takes a collec-tion of models optimized on separate tasks and learns to translate their outputs for improved performance on any or all of them at once. Unlike traditional transfer or multi-task learning, EgoT2’s “ﬂipped design” entails separate task-speciﬁc backbones and a task translator shared across all tasks, which captures synergies between even heteroge-neous tasks and mitigates task competition. Demonstrat-ing our model on a wide array of video tasks from Ego4D, we show its advantages over existing transfer paradigms and achieve top-ranked results on four of the Ego4D 2022 benchmark challenges.1 1.

Introduction the introduction of
In recent years, large-scale video datasets (e.g., Kinetics [6, 33] and Something-Something [22]) have enabled the application of powerful deep learning models to video understanding and have led to dramatic advances. These third-person datasets, however, have overwhelmingly focused on the single task of action recognition in trimmed clips [12, 36, 47, 64].
Unlike curated third-person videos, our daily life involves frequent and heterogeneous interactions with other hu-mans, objects, and environments in the wild. First-person videos from wearable cameras capture the observer’s perspective and attention as a continuous stream. As such,
*Work done during an internship at FAIR, Meta AI. 1Project webpage: https : / / vision . cs . utexas . edu / projects/egot2/. t a l k i n g   s h e   t o   m e ?   s
I
T a s k o f
I n t e r e s t
What will I   do next?
Who is speaking?
...
Did the object go through a state change?
EgoT2 l a r g e
T a s k
R e l a t e d n e s s s m a l l
Holistic 
Egocentric 
Perception
What will I do next?
[Task 1][Task 2][Task 3][Task 4]
Auxiliary Tasks
Figure 1. Given a set of diverse egocentric video tasks, the pro-posed EgoT2 leverages synergies among the tasks to improve each individual task performance. The attention maps produced by
EgoT2 offer good interpretability on inherent task relations. these multi-faceted, they are better equipped to reveal spontaneous interactions. Indeed egocentric datasets, such as EPIC-Kitchens [9] and Ego4D [23], provide suites of tasks associated with varied interactions. However, while these benchmarks have promoted a broader and more heterogeneous view of video understanding, they risk perpetuating the fragmented development of models specialized for each individual task.
In this work, we argue that the egocentric perspective offers an opportunity for holistic perception that can ben-eﬁcially leverage synergies among video tasks to solve all problems in a uniﬁed manner. See Figure 1.
Imagine a cooking scenario where the camera wearer ac-tively interacts with objects and other people in an environ-ment while preparing dinner. These interactions relate to each other: a hand grasping a knife suggests the upcoming action of cutting; the view of a tomato on a cutting board suggests that the object is likely to undergo a state transi-tion from whole to chopped; the conversation may further reveal the camera wearer’s ongoing and planned actions.              
Apart from the natural relation among these tasks, egocen-tric video’s partial observability (i.e., the camera wearer is largely out of the ﬁeld of view) further motivates us to seek synergistic, comprehensive video understanding to leverage complementary cues among multiple tasks.
Our goal presents several technical challenges for con-ventional transfer learning (TL) [65] and multi-task learn-ing (MTL) [63]. First, MTL requires training sets where each sample includes annotations for all tasks [15, 24, 48, 53, 55, 62], which is often impractical. Second, egocen-tric video tasks are heterogeneous in nature, requiring dif-ferent modalities (audio, visual, motion), diverse labels (e.g., temporal, spatial or semantic), and different tem-poral granularities (e.g., action anticipation requires long-term observations, but object state recognition operates at a few sparsely sampled frames)—all of which makes a uniﬁed model design problematic and fosters specializa-tion. Finally, while existing work advocates the use of a shared encoder across tasks to learn general representa-tions [3, 18, 26, 32, 39, 44, 45, 51], the diverse span of ego-centric tasks poses a hazard to parameter sharing which can lead to negative transfer [21, 24, 38, 53].
To address the above limitations, we propose EgoTask
Translation (EgoT2), a uniﬁed learning framework to ad-dress a diverse set of egocentric video tasks together. EgoT2 is ﬂexible and general in that it can handle separate datasets for the different tasks; it takes video heterogeneity into ac-count; and it mitigates negative transfer when tasks are not strongly related. To be speciﬁc, EgoT2 consists of special-ized models developed for individual tasks and a task trans-lator that explicitly models inter-task and inter-frame rela-tions. We propose two distinct designs: (1) task-speciﬁc
EgoT2 (EgoT2-s) optimizes a given primary task with the assistance of auxiliary tasks (Figure 2(c)) while (2) task-general EgoT2 (EgoT2-g) supports task translation for mul-tiple tasks at the same time (Figure 2(d)).
Compared with a uniﬁed backbone across tasks [62], adopting task-speciﬁc backbones preserves peculiarities of each task (e.g. different temporal granularities) and miti-gates negative transfer since each backbone is optimized on one task. Furthermore, unlike traditional parameter shar-ing [51], the proposed task translator learns to “translate” all task features into predictions for the target task by se-lectively activating useful features and discarding irrelevant ones. The task translator also facilitates interpretability by explicitly revealing which temporal segments and which subsets of tasks contribute to improving a given task.
We evaluate EgoT2 on a diverse set of 7 egocentric perception tasks from the world’s largest egocentric video benchmark, Ego4D [23].
Its heterogeneous tasks extend beyond mere action recognition to speaker/listener identi-ﬁcation, keyframe localization, object state change classiﬁ-cation, long-term action anticipation, and others, and pro-Task B
Transfer Layers
Backbone pretrained   on Task A
Task A
Task B
Task C
Head A
Head B
Head C
Shared  
Backbone (a) conventional TL (b) conventional MTL
Task B
Task A Task B Task C
Task Translator 
Shared Task
Translator  Backbone A   Backbone B   Backbone C   Backbone A   Backbone B   Backbone C  (c) EgoT2-s (d) EgoT2-g
Figure 2. (a) Conventional TL uses a backbone pretrained on the source task followed by a head transferring supervision to the tar-get task; (b) Traditional MTL consists of a shared backbone and several task-speciﬁc heads; (c) EgoT2-s adopts task-speciﬁc back-bones and optimizes the task translator for a given primary task; (d) EgoT2-g jointly optimizes the task translator for all tasks. vide a perfect ﬁt for our study. Our results reveal inher-ent task synergies, demonstrate consistent performance im-provement across tasks, and offer good interpretability in task translation. Among all four Ego4D challenges involved in our task setup, EgoT2 outperforms all submissions to three Ego4D-CVPR’22 challenges and achieves state-of-the-art performance in one Ego4D-ECCV’22 challenge. 2.