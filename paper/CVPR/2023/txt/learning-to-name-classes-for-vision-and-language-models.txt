Abstract
Both named “bow”
Both named “bat”
Large scale vision and language models can achieve im-pressive zero-shot recognition performance by mapping class specific text queries to image content. Two distinct challenges that remain however, are high sensitivity to the choice of hand-crafted class names that define queries, and the difficulty of adaptation to new, smaller datasets. Towards addressing these problems, we propose to leverage available data to learn, for each class, an optimal word embedding as a function of the visual content. By learning new word embeddings on an other-wise frozen model, we are able to retain zero-shot capabilities for new classes, easily adapt models to new datasets, and ad-just potentially erroneous, non-descriptive or ambiguous class names. We show that our solution can easily be integrated in image classification and object detection pipelines, yields sig-nificant performance gains in multiple scenarios and provides insights into model biases and labelling errors. 1.

Introduction
The introduction of large-scale vision and language pre-training techniques has led to significant breakthroughs in object recognition, notably for tasks where data efficiency forms a crucial component (e.g. zero-shot scenarios). The key idea is to learn a mapping between image and text spaces such that a latent image representation will be close to sentences that describe the image content, in representation space [33, 36]. Learning from image-specific descriptions, rather than a fixed and common class label, with limited semantic meaning, yields models with very strong representational power [36]. Such multi-modal approaches address one key shortcoming of visual-only recog-nition methods; they provide an ability to seamlessly expand the model’s output space without the requirement of additional training (i.e. open-set recognition).
In contrast, visual-only classification and object detection methods have historically relied on training a linear classification layer in order to identify objects of interest, resulting in the undesirable consequence that any introduction of new classes (with or without available data) is very challenging. A plethora of complex mechanisms have been proposed, towards addressing this problem [12,19]. Vision (a) Ambiguous class names
Class name:
“2007 Cadillac Escalade EXT Crew Cab”
Class name:
“A3400-200” (b) Technical class names
Figure 1. Vision and language recognition models are highly sensitive to class prompts. Ambiguous or highly technical class names can lead to poor classification performance. Rather than adjusting class names directly, current solutions rely on prompt context to reduce ambiguity, e.g.: ‘a photo of a bow, a type of weapon’. and language multi-modal approaches alternatively perform classification by computing a similarity between image features and a set of textual features, corresponding to text descriptions from the classes of interest. As the text component constitutes an input, such models naturally enable a route to identify new categories, simply by modifying the queried text, yielding very strong zero-shot and open-set performance [22, 36].
We draw attention to the fact that such performant multi-modal models suffer from two remaining key limitations.
Firstly, performance is highly sensitive to class-prompt textual content; both in terms of (i) the selected class name and (ii) the surrounding sentence context. Class name sensitivity can be illustrated by considering class homonyms; words that have the same spelling but different meanings and origins (Figure 1a). These lead to ambiguous queries that negatively effect per-formance and mitigation necessitates time consuming prompt
engineering. A further example of class name sensitivity can be demonstrated by classes exhibiting highly technical character-isations (Figure 1b). Poor performance is typically observed for datasets exhibiting such complex naming strategies [36]. We hypothesise that lack of clear semantic meaning leave it difficult for multi-modal models to obtain valuable embeddings.
Secondly; model adaptation to new data, that becomes avail-able, is not straightforward – especially when datasets are small.
A natural solution is to use the existing representation layers and learn a new classification layer (i.e. linear probing). However this strategy must sacrifice the flexibility of text input, for poten-tial subsequent adaptations [36]. Alternatives consider replacing text queries with image queries [33], or learning optimal prompt contexts from the new data [6, 50, 51]. The former can struggle to leverage larger amounts of available data successfully as im-age queries are averaged over all available class specific data.
The latter alternatively updates a fixed set of parameters, which can lead to forgetting issues under sequential adaptation. Fur-thermore, the approach fails to learn a new class representation or mapping from the new data, as the focus lies solely with the prompt context. Finally; while context learning is highly success-ful in simple classification tasks, it becomes more challenging for object detection purposes and requires more complex train-ing mechanisms [6]. We conjecture that this may be attributed to the fact that context then depends on the spatial region of the image, in addition to object type (i.e. foreground or background).
These considerations lead us to propose a complementary solution to prompt learning, which can be successfully leveraged for both classification and detection problems, without any task-specific tailoring or requirements. We build upon ideas from the field of text to image generation, and propose to leverage the concept of textual inversion [10] to adapt models to new data and thus address suboptimal, and potentially wrong, labels (e.g. incorrect class names, mislabelled data). The crux of our idea is to supersede manual word engineering, describing categories of interest, by leveraging available data to learn optimal word embeddings, as a function of the visual content.
This simple solution affords several advantages: 1) in contrast to linear probing, open-set capabilities are maintained; learned class representations remain in text space, 2) our approach learns class specific parameters, therefore preventing forgetting under sequential training, 3) direct applicability to any classification or object detection technique that utilises textual input, 4) interpretability properties, with regards to learned class names.
Experimentally, we realise our ideas in conjunction with two contemporary models; for image classification [36] and object detection [33]. Given a pre-trained model and (potentially pre-viously unseen, additional) training dataset, we introduce a new set of word embeddings in the model text branch, each corre-sponding to a class of interest. Following context tuning meth-ods [50, 51], embeddings are learned using standard losses on an otherwise frozen model. Comprehensive experiments across thirteen classification and detection datasets help to evidence that learning optimal class names can significantly enhance model performance, maintain open-vocabulary properties, successfully enable continual model adaptation, and improve performance for rare and longtail classes. Last but not least, we demonstrate the method’s potential for interpretability, notably with regards to mislabelling and inadequately chosen original class names.
Our main contributions can be summarised as:
• A simple proposition for data-efficient adaptation of large vision-language models: optimisation of class names from visual content that retains attractive open-vocabulary properties.
• Performance improvements complementary to those of prompt tuning; consistent experimental gains across 13 classification and detection datasets. Strong sequential adaptation, open-vocabulary and long tail performance.
• Model interpretability insights: we visualise class name changes and related semantic meanings. We highlight how model biases and labelling errors can be identified, and how vision-language models suffer from long-tail issues. 2.