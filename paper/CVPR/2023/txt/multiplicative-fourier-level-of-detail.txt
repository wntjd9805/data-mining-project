Abstract
We develop a simple yet surprisingly effective implicit representing scheme called Multiplicative Fourier Level of
Detail (MFLOD) motivated by the recent success of mul-tiplicative filter network. Built on multi-resolution feature grid/volume (e.g., the sparse voxel octree), each level’s fea-ture is first modulated by a sinusoidal function and then element-wisely multiplied by a linear transformation of pre-vious layer’s representation in a layer-to-layer recursive manner, yielding the scale-aggregated encodings for a sub-sequent simple linear forward to get final output. In con-trast to previous hybrid representations relying on inter-leaved multilevel fusion and nonlinear activation-based de-coding, MFLOD could be elegantly characterized as a lin-ear combination of sine basis functions with varying am-plitude, frequency, and phase upon the learned multilevel features, thus offering great feasibility in Fourier analysis.
Comprehensive experimental results on implicit neural rep-resentation learning tasks including image fitting, 3D shape representation, and neural radiance fields well demonstrate the superior quality and generalizability achieved by the proposed MFLOD scheme. 1.

Introduction
Classical geometric modeling techniques in computer graphics represent signals by storing discrete samples in array- or grid-based formats. It is nontrivial to adapt them to learning-based framework due to the lack of differentia-bility. Recently, neural implicit functions have emerged as an attractive alternative, which parameterize the continu-ous mapping between low dimensional coordinates and im-age/object domain signals using neural network, for exam-ple, as the representation of 3D shapes [5, 23, 29, 42] and radiance fields [16, 25].
Prior works commonly use a large multi-layer percep-tron (MLP) to parameterize the learning function. To cir-cumvent the well-known low frequency spectral bias of neu-*Corresponding author: Bingbing Ni.
Figure 1. Implicit Neural Representations from Spectral Per-spective. (a) Most implicit neural representations can be cate-gorized into global support-based method. A large network is required to enrich the basis coefficients for representing high-frequency details. (b) Our method can also be well-characterized like the global method, enabling explicit bandwidth control for each LOD. This allows for representing high-fidelity details at fine levels and smooth overall shape at coarse levels. ral networks [31], frequency encoding [15, 25, 33, 34, 40] is usually adopted to map input coordinates to a higher dimen-sional space. A useful property of these pure MLP methods is that the entire function can be theoretically characterized as a linear combination of Fourier bases [6, 49], facilitating the design of neural representation from the spectral point of view, such as explicitly manipulating the bandwidth [15], representing overall signal and details separately [46], bal-ancing representation generalization and spectrum cover-age [40]. However, solely relying on the Fourier bases to ex-press local high-frequency details is inefficient since these bases normally have (infinite) global support [49], resulting in the requirement of employing over-sized MLP to accom-modate a large set of basis coefficients.
Most recently, hybrid representations [27,38,39] emerge for their efficiency and high-fidelity. They employ a multi-level feature grid/volume to capture local details, and thus allow the use of a much smaller MLP as decoder. Yet, the interleaved multilevel fusion and nonlinear activation-based decoding make the entire function hard to characterize and not amenable to Fourier analysis like pure MLP methods.
Specifically, little is known of how multilevel features are combined to get the final output.
We address the above limitations with our proposed im-plicit feature representation framework named Multiplica-tive Fourier Level of Detail (MFLOD). Within a multi-resolution feature grid framework, MFLOD inherits the ef-ficiency merits of hybrid methods, and in the meantime, as multilevel local features are modulated with a rich set of frequency basis functions, the resulting representation is feasible in Fourier analysis. More concretely, each feature point within a multi-resolution feature grid/volume (e.g., the sparse voxel octree) is interpolated according to its spa-tial distances to the griding points, modulated by a sinu-soidal filter, and then element-wisely multiplied by a lin-ear transformation of previous layer’s representation in a layer-to-layer recursive manner. After that, a simple lin-ear forward is sufficient to decode these multilevel encod-ings to the final implicit function value. To enable mean-ingful levels of detail, we explicitly manipulate the spec-tral bandwidth of each level. This allows for representing high-fidelity details at fine levels and smooth overall shape at coarse levels, as shown in Fig. 1.
In addition to introducing MFLOD, we conduct in-depth theoretical study from the spectral and neural tangent ker-nel (NTK) [10] approximating perspectives, showing that the proposed method has better spectrum coverage and gen-eralization. Comprehensive experimental results on im-plicit neural representation learning tasks including image fitting, 3D shape representation, and neural radiance fields well demonstrate the superior quality and generalizability achieved by the proposed MFLOD scheme. 2.