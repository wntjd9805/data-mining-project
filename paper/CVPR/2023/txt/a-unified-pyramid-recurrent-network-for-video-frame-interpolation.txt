Abstract
Flow-guided synthesis provides a common framework for frame interpolation, where optical flow is estimated to guide the synthesis of intermediate frames between consec-In this paper, we present UPR-Net, a novel utive inputs.
Unified Pyramid Recurrent Network for frame interpola-tion. Cast in a flexible pyramid framework, UPR-Net ex-ploits lightweight recurrent modules for both bi-directional flow estimation and intermediate frame synthesis. At each pyramid level, it leverages estimated bi-directional flow to generate forward-warped representations for frame synthe-sis; across pyramid levels, it enables iterative refinement for both optical flow and intermediate frame. In particular, we show that our iterative synthesis strategy can significantly improve the robustness of frame interpolation on large mo-tion cases. Despite being extremely lightweight (1.7M pa-rameters), our base version of UPR-Net achieves excel-lent performance on a large range of benchmarks. Code and trained models of our UPR-Net series are available at: https://github.com/srcn-ivl/UPR-Net. 1.

Introduction
Video frame interpolation (VFI) is a classic low-level
It aims to increase the frame rate of videos, vision task. by synthesizing non-existent intermediate frames between consecutive frames. VFI technique supports many practi-cal applications including novel view synthesis [10], video compression [21], cartoon creation [32], etc.
Despite great potential in applications, video frame in-terpolation remains an unsolved problem, due to challenges like complex and large motions, occlusions, and illumina-tion changes in real-world videos. Depending on whether or not optical flow is incorporated to compensate for inter-frame motion, existing methods can be roughly classified into two categories: flow-agnostic methods [5,6,25,28], and flow-guided synthesis [2, 14, 19, 26, 27, 29, 30]. With recent advances in optical flow [12,13,34,35], flow-guided synthe-sis has developed into a popular framework with compelling performance for video frame interpolation.
Figure 1. Comparison of performance and model size on the hard subset of SNU-FILM benchmark [6]. Our UPR-Net series achieve state-of-the-art accuracy with extremely small parameters.
Most of existing flow-guided methods follow a sim-ilar procedure: estimating optical flow for desired time step, warping input frames and their context features based on optical flow, and synthesizing intermediate frame from warped representations. Where technical choices may di-verge in this procedure, is the warping operation and the optical flow it requires. Backward-warping is traditionally used for frame interpolation [2, 14, 19, 29, 30], but acquiring high-quality bilateral intermediate flow for it is often chal-lenging. Forward-warping can directly use linearly-scaled bi-directional flow between input frames (which is easier to obtain), and thus has recently emerged as a promising di-rection for frame interpolation [26, 27].
In common flow-guided synthesis pipeline [1,18,27,30], optical flow is typically estimated from coarse to fine by a pyramid network, but intermediate frame is synthesized just once by a synthesis network. Despite promising per-formance on low-resolution videos, this practice misses the opportunity of iteratively refining the interpolation for high-resolution inputs. Second, for large motion cases, an im-portant issue has been overlooked by previous works: even
when estimated motion is visually plausible, in many cases, the obvious artifacts in warped frames (e.g., large holes in forward-warped frames) may also degrade the interpolation performance. Last, existing methods typically rely on heavy model architectures to achieve good performance, block-ing them from being deployed on platforms with limited resources, e.g., mobile devices.
Aiming at these issues, we introduce UPR-Net, a novel
Unified Pyramid Recurrent Network for frame interpola-tion. Within a pyramid framework, UPR-Net exploits lightweight recurrent modules for both bi-directional flow estimation and forward-warping based frame synthesis. It enables iterative refinement of both optical flow and inter-mediate frame across pyramid levels, producing compelling results on complex and large motion cases.
Our work draws inspirations from many existing works, but is significantly distinguished from them in three as-pects. First, UPR-Net inherits the merit of recent pyramid recurrent bi-directional flow estimators [15,31], allowing to customize the number of pyramid levels in testing to esti-mate extremely large motions. But, it goes one step further, by exploiting pyramid recurrent network for coarse-to-fine frame synthesis, and unifying motion estimation and frame synthesis within a single pyramid recurrent network.
Second, we reveal that our coarse-to-fine iterative syn-thesis can significantly improve the robustness of frame in-terpolation on large motion cases. At high-resolution pyra-mid levels, forward-warped frames may suffer from obvious holes due to large motions, resulting in poor interpolation for many cases. We show that this issue can be remedied to a large extent, by feeding the frame synthesis module with the intermediate frame estimate upsampled from previous lower-resolution pyramid level.
Third, both of our optical flow and frame synthesis mod-ules are extremely lightweight. Yet, they are still carefully integrated with the key ingredients from modern researches on optical flow [34, 35] and frame synthesis [26]. Specifi-cally, at each pyramid level, UPR-Net firstly extracts CNN features for input frames, then constructs a correlation vol-ume for simultaneous bi-directional flow estimation. It pre-dicts refined intermediate frame from forward-warped input frames and their CNN features, along with upsampled inter-mediate frame estimate.
We conduct extensive experiments to verify the effec-tiveness of UPR-Net for frame interpolation. Our base ver-sion of UPR-Net only has 1.7M parameters. Yet, it achieves excellent performance on both low- and high-resolution benchmarks, when trained with low-resolution data. Fig-ure 1 gives a comparison of accuracy and model size on the hard subset of SNU-FILM [6], where our UPR-Net series achieve state-of-the-art accuracy with much fewer papram-eters.
In addition, we validate various design choices of
UPR-Net by ablation studies. 2.