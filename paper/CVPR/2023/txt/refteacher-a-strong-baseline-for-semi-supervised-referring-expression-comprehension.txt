Abstract
Referring expression comprehension (REC) often re-quires a large number of instance-level annotations for fully supervised learning, which are laborious and expensive. In this paper, we present the first attempt of semi-supervised learning for REC and propose a strong baseline method called RefTeacher. Inspired by the recent progress in com-puter vision, RefTeacher adopts a teacher-student learning paradigm, where the teacher REC network predicts pseudo-labels for optimizing the student one. This paradigm allows
REC models to exploit massive unlabeled data based on a small fraction of labeled.
In particular, we also identify two key challenges in semi-supervised REC, namely, sparse supervision signals and worse pseudo-label noise. To ad-dress these issues, we equip RefTeacher with two novel de-signs called Attention-based Imitation Learning (AIL) and
Adaptive Pseudo-label Weighting (APW). AIL can help the student network imitate the recognition behaviors of the teacher, thereby obtaining sufficient supervision signals.
APW can help the model adaptively adjust the contributions of pseudo-labels with varying qualities, thus avoiding con-firmation bias. To validate RefTeacher, we conduct exten-sive experiments on three REC benchmark datasets. Exper-imental results show that RefTeacher obtains obvious gains over the fully supervised methods. More importantly, using only 10% labeled data, our approach allows the model to achieve near 100% fully supervised performance, e.g., only
-2.78% on RefCOCO. Project: https://refteacher.github.io/. 1.

Introduction
Referring Expression Comprehension (REC) [33–35,45, 49, 53, 60, 62], also called Visual grounding [26, 51, 52] or Phrase localization [20, 39], aims to locate the target
*Corresponding Author
Statistics of the pseudo-label quality in semi-Figure 1. supervised REC with different percentages of labeled infor-mation. The REC model only predicts one pseudo-box for each image-text pair and cannot apply filtering, so the pseudo-labels are usually noisy and low-quality during training. objects in an image referred by a given natural language expression. Compared to conventional object detection tasks [4,9–11,23,24,28,40–42], REC is not limited to a fix set of categories and can be generalized to open-vocabulary recognition [31, 60]. However, as a detection task, REC also requires a large number of instance-level annotations for training, which poses a huge obstacle to its practical ap-plications.
To address this issue, one feasible solution is semi-supervised learning (SSL), which has been well studied on various computer vision tasks [1–3, 8, 16, 29, 43, 44, 47, 59] but not yet exploited in REC. In particular, recent advances in semi-supervised object detection (SSOD) [16, 29, 44, 47, 59] has yielded notable progress in practical applications.
These SSOD methods apply a training framework consisted of two detection networks with the same configurations, act-ing as teacher and student, respectively. The teacher net-work is in charge of generating pseudo-labels to optimize
the student during training, which can exploit massive unla-beled data based on a small amount of labeled information.
With the help of this effective training paradigm, the latest
SSOD method [37] can even achieve fully supervised per-formance with only 40% and 25% labeled information on
MSCOCO [25] and PASCAL VOC [7], respectively.
However, directly transferring this successful paradigm to REC still suffers from two main challenges due to task gaps. The first one is the extremely sparse supervision signals. In contrast to object detection, REC only ground one instance for each text-image pair. This prediction pat-tern makes the REC model receive much fewer pseudo-supervisions than SSOD during teacher-student learning, i.e., only one bounding box without class pseudo-labels.
For instance, Compared with SSOD that has 6-15 high-quality pseudo-boxes for each image [30, 37], semi-REC only has 0.5 box on average at the infant training stages as shown in Fig. 1. The sparse supervision signals also lead to worse pseudo-label quality, which is the second challenge.
SSOD methods [16, 29, 44, 47, 59] can apply NMS [10] and high-threshold filtering to discard the vast majority of noisy pseudo-labels, thereby avoiding the error accumulation is-sue [29, 37, 44] in SSL. But in REC, a strong filtering is not feasible due to the already sparse pseudo-label information.
This results in that most pseudo-labels of REC are of much lower-quality.
Based on this observations, we propose the first semi-supervised approach for REC called RefTeacher with two novel designs, namely Attention-based Imitation Learn-ing (AIL) and Adaptive Pseudo-label Weighting (APW). In principle, RefTeacher also adopts a teacher-student frame-work, where the teacher predicts the pseudo bounding boxes for the student according to the given expressions. Follow-ing the latest SSOD [29,30,37,47], we also use EMA to up-date the gradients of the teacher network from the student, and introduce data augmentation and burn-in strategies to improve SSL. To enrich the supervision signals, the pro-posed AIL helps the student imitate the attention behaviors of the teacher, thereby improving the knowledge transfer-ring. APW is further used to reduce the impact of noisy pseudo-labels, which is achieved via adaptively weighting label information and the corresponding gradient updates.
To validate RefTeacher, we apply RefTeacher to i.e. RealGIN [60] and two representative REC models,
TransVG [5], and conduct extensive experiments on three
REC benchmark datasets, namely RefCOCO [54], Ref-COCO+ [54] and RefCOCOg [38]. Experimental results show that RefTeacher can greatly exceed the supervised baselines, e.g. +18.8% gains on 10% RefCOCO. More im-portantly, using only 10% labeled data, RefTeacher can help
RealGIN achieve near 100% fully supervised performance.
Overall, the contributions of this paper are three-fold:
• We present the first attempt of semi-supervised learn-ing for REC with a strong baseline method called
RefTeacher.
• We identify two challenges of semi-supervised REC, i.e. sparse supervision signals and worse pseudo-label noise, and address them with two novel designs, namely Attention-based Imitation Learning (AIL) and
Adaptive Pseudo-label Weighting (APW).
• RefTeacher achieves significant performance gains on
RefCOCO, RefCOCO+, and RefCOCOg datasets over the fully supervised methods. 2.