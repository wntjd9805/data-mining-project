Abstract 1.

Introduction
Humans excel at acquiring knowledge through observa-tion. For example, we can learn to use new tools by watch-ing demonstrations. This skill is fundamental for intelligent systems to interact with the world. A key step to acquire this skill is to identify what part of the object affords each action, which is called affordance grounding. In this paper, we address this problem and propose a framework called
LOCATE that can identify matching object parts across im-ages, to transfer knowledge from images where an object is being used (exocentric images used for learning), to im-ages where the object is inactive (egocentric ones used to test). To this end, we first find interaction areas and extract their feature embeddings. Then we learn to aggregate the embeddings into compact prototypes (human, object part, and background), and select the one representing the object part. Finally, we use the selected prototype to guide affor-dance grounding. We do this in a weakly supervised manner, learning only from image-level affordance and object la-bels. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a large margin on both seen and unseen objects.1 1Project page: https://reagan1311.github.io/locate.
A fundamental skill of humans is learning to interact with objects just by observing someone else performing those interactions [5]. For instance, even if we have never played tennis, we can easily learn where to hold the racket just by looking at a single or few photographs of those inter-actions. Such learning capabilities are essential for intelli-gent agents to understand what actions can be performed on a given object. Current visual systems often focus primarily on recognizing what objects are in the scene (passive per-ception), rather than on how to use objects to achieve cer-tain functions (active interaction). To this end, a growing number of studies [3, 12, 26, 47] have begun to utilize affor-dance [17] as a medium to bridge the gap between passive perception and active interaction. In computer vision and robotics [2, 20], affordance typically refers to regions of an object that are available to perform a specific action, e.g., a knife handle affords holding, and its blade affords cutting.
In this paper, we focus on the task of affordance ground-ing, i.e, locating the object regions used for a given action.
Previous methods [10, 12, 14, 37, 39] have often treated af-fordance grounding as a fully supervised semantic segmen-tation task, which requires costly pixel-level annotations.
Instead, we follow the more realistic setting [32, 33, 38]
object parts involved in the interaction from exocentric im-ages and transferring this knowledge to inactive egocentric images. Refer to Fig. 2 for the illustration. Specifically, we first use the class activation mapping (CAM) [51] technique to find the regions of human-object-interaction in exocen-tric images. Despite being trained for the interaction recog-nition task, we observe that CAM can generate good local-ization maps for interaction regions. We then segment this region of interest further into regions corresponding to hu-man, object part, and background. We do this by extracting embeddings and performing k-means clustering to obtain several compact prototypes. Next, we automatically pre-dict which of these prototypes corresponds to the object part relevant to the affordance. To this end, we propose a mod-ule named PartSelect that leverages part-aware features and attention maps from a self-supervised vision transformer (DINO-ViT [6]) to obtain the desired prototype. Finally, we use the object-part prototype as a high-level pseudo su-pervision to guide egocentric affordance grounding.
Our contributions can be summarized as follows. (1)
We propose a framework called LOCATE that extracts affordance knowledge from weakly supervised exocentric human-object interactions, and transfers this knowledge to the egocentric image in a localized manner. (2) We intro-duce a novel module termed PartSelect to pick affordance-specific cues from human-object interactions. The extracted information is then used as explicit supervision to guide af-fordance grounding on egocentric images. (3) LOCATE achieves state-of-the-art results with far fewer parameters and faster inference speed than previous methods, and is able to locate accurate affordance region for unseen objects.
See Fig. 1 for examples of our results and comparison to state-of-the-art. 2.