Abstract
Audiovisual automatic speech recognition (AV-ASR) aims to improve the robustness of a speech recognition sys-tem by incorporating visual information. Training fully supervised multimodal models for this task from scratch, however is limited by the need for large labelled audiovi-sual datasets (in each downstream domain of interest). We present AVFormer, a simple method for augmenting audio-only models with visual information, at the same time per-forming lightweight domain adaptation. We do this by (i) injecting visual embeddings into a frozen ASR model using lightweight trainable adaptors. We show that these can be trained on a small amount of weakly labelled video data with minimum additional training time and parame-ters. (ii) We also introduce a simple curriculum scheme dur-ing training which we show is crucial to enable the model to jointly process audio and visual information effectively; and finally (iii) we show that our model achieves state of the art zero-shot results on three different AV-ASR bench-marks (How2, VisSpeech and Ego4D), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (LibriSpeech). Qualitative results show that our model effectively leverages visual in-formation for robust speech recognition. 1.

Introduction
Robustness or adaptation to new, unconstrained domains is a key challenge for automatic speech recognition (ASR)
In multimodal video (e.g., TV, online edited systems. videos), the visual stream can provide strong cues for im-proving the robustness of ASR systems, particularly in cases where the audio is noisy – this is called audiovisual
ASR (AV-ASR). Unlike works that simply focus on lip mo-tion [1, 7, 23, 24, 29, 33, 37, 41], we investigate the contri-bution of entire visual frames. This is particularly useful for videos ‘in the wild’, where the mouth is not necessarily visible (e.g., egocentric viewpoints, face coverings, and low resolution etc.) [11]. The task is illustrated in Figure 1.
Building audiovisual datasets for training AV-ASR mod-Figure 1. Unconstrained audiovisual speech recognition. We inject vision into a frozen speech model (BEST-RQ, in grey) for zero-shot audiovisual ASR via lightweight modules to create a pa-rameter and data efficient model called AVFormer (blue). The vi-sual context can provide helpful clues for robust speech recog-nition especially when the audio signal is noisy (the visual loaf of bread helps correct the audio-only mistake clove to loaf in the generated transcript). els, however, is challenging. Datasets such as How2 [36] and VisSpeech [11] have been created from instructional videos online, but they are small in size. Not only are datasets for this task small, but models are typically large and consist of both visual and audio encoders. For exam-ple the latest AV-ASR model AVATAR [11] shows impres-sive performance on both datasets, but requires the end-to-end training of visual and audio components in tandem, and consequently a large amount of compute. Like other AV-ASR works [4, 17, 25, 30, 38], it is also only trained and tested on instructional videos, and as we show in the exper-iments, generalizes poorly to new domains in the zero-shot setting.
On the other hand, there have been a number of recently released large-scale audio-only models [6, 8, 19] that are heavily optimised via self-supervised pretraining and large-scale supervised training on audio-only data obtained from audio books such as LibriLight [20] and LibriSpeech [31].
These models contain billions of parameters, are readily available, and show strong generalization across domains.
Our goal is to reuse the extensive expertise and train-ing time that has been invested in such models, by using their weights. We are inspired by recent works adapting frozen foundation models for multi-modal tasks. A popular example is [2] that injects visual information into large lan-guage models (LLMs) for vision-text tasks. The benefit of
building on strong frozen LLMs for these tasks is the hope that this will enable the visual-text model to retain powerful language-only abilities such as few-shot language adapta-tion or external knowledge retrieval. Our goal is simple -we wish to do the same for AV-ASR, using strong audio-only ASR models. We add visual inputs to these models in a lightweight manner to enable AV-ASR, but still maintain the benefits of audio-only pretraining for zero-shot general-ization.
Our framework is called AVFormer, and injects visual in-formation into a frozen ASR model using lightweight pro-jection layers and trainable adaptors. We show that these can be trained on a small amount of weakly labelled video data (only 5% of the data used by existing state of the art methods [11]) with minimum additional training time and parameters, minimizing the domain shift and catas-trophic forgetting that can accompany end-to-end finetun-ing. In order to further ensure stability during the finetun-ing of these adapters, we also introduce a simple curriculum scheme during training which we show is crucial to enable the model to jointly process audio and visual information effectively. Finally, we show that our model outperforms existing state of the art zero-shot methods on three differ-ent AV-ASR benchmarks (How2, VisSpeech and Ego4D) across different domains, while also crucially preserving decent performance on traditional audio-only speech recog-nition benchmarks (LibriSpeech). 2.