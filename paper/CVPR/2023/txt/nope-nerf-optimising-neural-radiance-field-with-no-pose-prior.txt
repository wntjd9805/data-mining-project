Abstract
Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly opti-mising a NeRF and camera poses in forward-facing scenes.
However, these methods still face difficulties during dra-matic camera movement. We tackle this challenging prob-lem by incorporating undistorted monocular depth priors.
These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames.
This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging cam-era trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation ac-curacy. Our project page is https://nope- nerf. active.vision. 1.

Introduction
The photo-realistic reconstruction of a scene from a stream of RGB images requires both accurate 3D geometry reconstruction and view-dependent appearance modelling.
Recently, Neural Radiance Fields (NeRF) [24] have demon-strated the ability to build high-quality results for generating photo-realistic images from novel viewpoints given a sparse set of images.
An important preparation step for NeRF training is the estimation of camera parameters for the input images. A current go-to option is the popular Structure-from-Motion (SfM) library COLMAP [35]. Whilst easy to use, this pre-processing step could be an obstacle to NeRF research and real-world deployments in the long term due to its long pro-cessing time and its lack of differentiability. Recent works such as NeRFmm [46], BARF [18] and SC-NeRF [12] pro-pose to simultaneously optimise camera poses and the neu-ral implicit representation to address these issues. Neverthe-less, these methods can only handle forward-facing scenes when no initial parameters are supplied, and fail in dramatic camera motions, e.g.a casual handheld captured video.
This limitation has two key causes. First, all these meth-ods estimate a camera pose for each input image individ-ually without considering relative poses between images.
Looking back to the literature of Simultaneous localisation and mapping (SLAM) and visual odometry, pose estimation can significantly benefit from estimating relative poses be-tween adjacent input frames. Second, the radiance field is known to suffer from shape-radiance ambiguity [55]. Esti-mating camera parameters jointly with NeRF adds another degree of ambiguity, resulting in slow convergence and un-stable optimisation.
To handle the limitation of large camera motion, we seek help from monocular depth estimation [22, 28, 29, 51]. Our motivation is threefold: First, monocular depth provides strong geometry cues that are beneficial to constraint shape-radiance ambiguity. Second, relative poses between ad-jacent depth maps can be easily injected into the training pipeline via Chamfer Distance. Third, monocular depth is lightweight to run and does not require camera parameters as input, in contrast to multi-view stereo depth estimation.
For simplicity, we use the term mono-depth from now on.
Utilising mono-depth effectively is not straightforward with the presence of scale and shift distortions.
In other words, mono-depth maps are not multi-view consistent.
Previous works
[9, 17, 47] simply take mono-depth into a depth-wise loss along with NeRF training. Instead, we propose a novel and effective way to thoroughly integrate mono-depth into our system. First, we explicitly optimise scale and shift parameters for each mono-depth map dur-ing NeRF training by penalising the difference between rendered depth and mono-depth. Since NeRF by itself is trained based on multiview consistency, this step transforms mono-depth maps to undistorted multiview consistent depth maps. We further leverage these multiview consistent depth maps in two loss terms: a) a Chamfer Distance loss between two depth maps of adjacent images, which injects relative pose to our system; and b) a depth-based surface rendering loss, which further improves relative pose estimation.
In summary, we propose a method to jointly optimise camera poses and a NeRF from a sequence of images with large camera motion. Our system is enabled by three contri-butions. First, we propose a novel way to integrate mono-depth into unposed-NeRF training by explicitly modelling scale and shift distortions. Second, we supply relative poses to the camera-NeRF joint optimisation via an inter-frame loss using undistorted mono-depth maps. Third, we further regularise our relative pose estimation with a depth-based surface rendering loss.
As a result, our method is able to handle large camera motion, and outperforms state-of-the-art methods by a sig-nificant margin in terms of novel view synthesis quality and camera trajectory accuracy. 2.