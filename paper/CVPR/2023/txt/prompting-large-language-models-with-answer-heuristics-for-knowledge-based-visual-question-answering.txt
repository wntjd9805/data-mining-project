Abstract
Knowledge-based visual question answering (VQA) re-quires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3 [3]) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet—a concep-tually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are encoded into the prompts to enable GPT-3 to better comprehend the task thus enhancing its capacity. Prophet significantly outperforms all existing state-of-the-art meth-ods on two challenging knowledge-based VQA datasets,
OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their testing sets, respectively. 1.

Introduction
Recent advances in deep learning have enabled substan-tial progress in visual question answering (VQA) which re-quires a machine to answer free-form questions by reason-ing about given images. Benefiting from large-scale vision-*Zhou Yu is the corresponding author
Figure 1. Conceptual comparisons of three knowledge-based
VQA frameworks using a frozen GPT-3 model [3]. While PICa
[43], KAT [11], and REVIVE [22] directly feed the caption (C) and question (Q) into GPT-3 as the prompt, we argue that the information they provide for GPT-3 is insufficient thus cannot fully activate GPT-3’s potential. In contrast, our Prophet learns a vanilla VQA model without external knowledge to produce answer heuristics, which endows GPT-3 with richer and more task-specific information for answer prediction. language pretraining, the state-of-the-art methods have even surpassed human level on several representative bench-marks [1,41,48]. Despite the success of these methods, their reasoning abilities are far from satisfactory, especially when external knowledge is required to answer the questions. the task of knowledge-based VQA is
In this situation, introduced to validate models’ abilities to leverage external knowledge.
Early knowledge-based VQA benchmarks additionally provide structured knowledge bases (KBs) and annotate required knowledge facts for all the questions
[38, 39]. More recently, benchmarks emphasizing on open-domain knowledge have been established [29, 32], which means KBs are no longer provided and any external knowl-edge resource can be used for answering. We focus on the task with open-domain knowledge in this paper.
A straightforward solution for knowledge-based VQA is to retrieve knowledge entries from explicit KBs, e.g.,
Wikipedia and ConceptNet [23]. Then a KB-augmented
VQA model performs joint reasoning over the retrieved knowledge, image, and question to predict the answer [7, 8, 28, 42, 51]. However, the performance of these retrieval-based approaches is limited for two reasons: (i) the required knowledge may not be successfully retrieved from the KBs; and (ii) even if the required knowledge is retrieved, plenty of irrelevant knowledge is inevitably introduced, which hampers the learning of VQA models.
Apart from those studies using explicit KBs, another line of research resorts to pretrained large language models, e.g.,
GPT-3 [3], as implicit knowledge engines for knowledge acquisition. A pioneering work by PICa employs the frozen
GPT-3 model to answer the question with formatted prompt as its input [43]. Given a testing image-question pair,
PICa first translates the image into a caption using an off-the-shelf captioning model. The question, caption, and a few in-context examples are then integrated into a textual prompt that can induce GPT-3 to predict the answer directly.
Thanks to the powerful knowledge reasoning ability of
GPT-3, PICa achieves significant performance improve-ments compared to those retrieval-based methods using explicit KBs.
Inspired by PICa, KAT [11] and REVIVE
[22] learn KB-augmented VQA models to exploit both the implicit knowledge from GPT-3 and explicit knowledge from KBs for answer prediction. The synergy of the two knowledge resources brings further improvements to their models. Despite the promising results achieved by these methods, they have not fully activated GPT-3 due to the following limitations: (i) The generated captions cannot cover all the necessary information in the image. Consider the example in
Figure 1, the caption “a group of people walk in a city square” contribute nothing to answering the question
“what fruit comes from these trees”. In this situation,
GPT-3 has to make an aimless and biased guess to answer the question. (ii) GPT-3 employs a few-shot learning paradigm that requires a few in-context examples to adapt to new tasks. Therefore, the choice of these examples is critical to model performance. As reported in [43], all its example selection strategies achieve far inferior performance to the oracle strategy that uses the simi-larity of ground-truth answers.
We ask: Is it possible to endow GPT-3 with some heuristics to enhance its capacity for knowledge-based VQA?
In this paper, we present Prophet—a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. By answer heuristics, we mean some promising answers that are presented in a proper manner in the prompt. Specifically, we introduce two types of answer heuristics, namely examples, candidates and answer-aware answer to overcome the limitations in (i) and (ii), respectively. Given a testing input consisting of an image and a question, the answer candidates refer to a list of promising answers to the testing input, where each answer is associated with a confidence score. The answer-aware examples refer to a list of in-context examples, where each example has a similar answer to the testing input. Interestingly, these two types of answer heuristics can be simultaneously obtained from any vanilla VQA model trained on a specific knowledge-based
VQA dataset. A schematic of Prophet is illustrated at the bottom of Figure 1.
Without bells and whistles, Prophet surpasses all previ-ous state-of-the-art single-model results on the challenging
OK-VQA and A-OKVQA datasets [29, 32], including the heavily-engineered Flamingo-80B model trained on 1.8B image-text pairs [1]. Moreover, Prophet is friendly to most researchers, as our results can be reproduced using a single
GPU and an affordable number of GPT-3 invocations. 2.