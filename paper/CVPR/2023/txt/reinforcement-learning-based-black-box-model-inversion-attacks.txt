Abstract
Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial
Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excel-lent attack performance. On the other hand, current black-box model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as white-box attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion at-tack. We formulate the latent space search as a Markov De-cision Process (MDP) problem and solve it with reinforce-ment learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Fi-nally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The exper-iment results on various datasets and models demonstrate that our attack successfully recovers the private informa-tion of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack. 1.

Introduction
With the rapid development of artificial intelligence, deep learning applications are emerging in various fields such as computer vision, healthcare, autonomous driving, and natural language processing. As the number of cases requiring private data to train the deep learning models in-creases, the concern of private data leakage including sensi-tive personal information is rising. In particular, studies on privacy attacks [21] show that personal information can be extracted from the trained models by malicious users. One of the most representative privacy attacks on machine learn-ing models is a model inversion attack, which reconstructs the training data of a target model with only access to the model. The model inversion attacks are divided into three categories, 1) white-box attacks, 2) black-box attacks, and 3) label-only attacks, depending on the amount of informa-tion of the target model. The white-box attacks can access all parameters of the model. The black-box attacks can ac-cess soft inference results consisting of confidence scores, and the label-only attacks only can access inference results in hard label forms.
The white-box model inversion attacks [5, 25, 27] have succeeded in restoring high-quality private data including personal information by using Generative Adversarial Net-works (GANs) [10]. First, they train the GANs on sepa-rate public data to learn the general prior of private data.
Then benefiting from the accessibility of the parameters of the trained white-box models, they search and find latent vectors that represent data of specific labels with gradient-based optimization methods. However, these methods can-not be applied to machine learning services such as Ama-zon Rekognition [1] where the parameters of the model are protected. To reconstruct private data from such ser-vices, studies on black-box and label-only model inversion attacks are required. Unlike the white-box attacks, these at-tacks require methods that can explore the latent space of the GANs in order to utilize them, as gradient-based op-timizations are not possible. The recently proposed Model
Inversion for Deep Learning Network (MIRROR) [2] uses a genetic algorithm to search the latent space with confidence scores obtained from a black-box target model.
In addi-tion, Boundary-Repelling Model Inversion attack (BREP-MI) [14] has achieved success in the label-only setting by using a decision-based zeroth-order optimization algorithm for latent space search.
Despite these attempts, each method has a significant is-sue. BREP-MI starts the process of latent space search from the first latent vector that generates an image classified as the target class. This does not guarantee how many query accesses will be required until the first latent vector is found by random sampling, and in the worst case, it may not be possible to start the search process for some target classes.
In the case of MIRROR, it performs worse than the label-only attack BREP-MI, despite its use of confidence scores for the attack. Therefore, we propose a new approach, Re-inforcement Learning-based Black-box Model Inversion at-tack (RLB-MI), as a solution that is free from the aforemen-tioned problems. We integrate reinforcement learning to ob-tain useful information for latent space exploration from the confidence scores. More specifically, we formulate the ex-ploration of the latent space in the GAN as a problem in
Markov Decision Processes (MDP). Then, we provide the agent with rewards based on the confidence scores of the generated images, and use update steps in the replay mem-ory to enable the agent to approximate the environment in-cluding latent space. Actions selected by the agent based on this information can navigate latent vectors more effectively than existing methods. Finally, we can reconstruct private data through the GAN from the latent vectors. We experi-ment with our attack on various datasets and models. The attack performance is compared with various model inver-sion attacks in three categories. The results demonstrate that the proposed attack can successfully recover meaningful in-formation about private data by outperforming all other at-tacks. 2.