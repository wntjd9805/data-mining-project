Abstract 1.

Introduction
N
We present Neighborhood Attention (NA), the ﬁrst ef-ﬁcient and scalable sliding window attention mechanism for vision. NA is a pixel-wise operation, localizing self at-tention (SA) to the nearest neighboring pixels, and there-fore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding window pattern allows NA’s receptive ﬁeld to grow without needing extra pixel shifts, and preserves translational equivariance, un-like Swin Transformer’s Window Self Attention (WSA). We
AT T EN (Neighborhood Attention Extension), develop a Python package with efﬁcient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin’s WSA while using up to 25% less memory. We further present
Neighborhood Attention Transformer (NAT), a new hier-archical transformer design based on NA that boosts image classiﬁcation and downstream vision performance. Exper-imental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% Im-ageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding window attention, we open source our project and release our checkpoints.
Convolutional neural networks (CNNs) [19] have been the de facto standard architecture for computer vision mod-els across different applications for years. AlexNet [18] showed their usefulness on ImageNet [10], and many oth-ers followed suit with architectures such as VGG [26],
ResNet [17], and EfﬁcientNet [27]. Transformers [31] on the other hand, were originally proposed as attention-based models for natural language processing (NLP), trying to ex-ploit the sequential structure of language. They were the ba-sis upon which BERT [11] and GPT [2, 23, 24] were built, and they continue to be the state of the art architecture in
NLP.
In late 2020, Vision Transformer (ViT) [12] was pro-posed as an image classiﬁer using only a Transformer En-coder operating on an embedded space of image patches, mostly for large-scale training. A number of other meth-ods followed, attempting to increase data efﬁciency [13, 15, 28], eventually making such Transformer-like models the state of the art in ImageNet-1K classiﬁcation (without pre-training on large-scale datasets such as JFT-300M).
These high-performing Transformer-like methods are all based on Self Attention (SA), the basic building block in the original Transformer [31]. SA has a linear complex-ity with respect to the embedding dimension (excluding lin-ear projections), but a quadratic complexity with respect to the number of tokens.
In the scope of vision, the num-ber of tokens is typically in linear correlation with image resolution. As a result, higher image resolution results in a quadratic increase in complexity and memory usage in models strictly using SA, such as ViT. The quadratic com-plexity has prevented such models from being easily appli-cable to downstream vision tasks, such as object detection and segmentation, in which image resolutions are usually much larger than classiﬁcation. Another problem is that convolutions beneﬁt from inductive biases such as locality, and the 2-dimensional spatial structure, while dot-product self attention is a global 1-dimensional operation by deﬁni-tion. This means that some of those inductive biases have to be learned with either large sums of data [12] or advanced training techniques and augmentations [15, 28].
Local attention modules were therefore proposed to alle-viate these issues. Stand-Alone Self-Attention (SASA) [25] was one of the earliest applications of local window-based attention to vision, where each pixel attends to a window around it. Its explicit sliding window pattern is identical to that of same convolutions, with zero paddings around and a simple 2-dimensional raster scan, therefore maintaining translational equivariance. SASA was aimed at replacing convolutions in a ResNet, and was shown to have a no-ticeable improvement over baselines. However, the authors noted SASA was limited in terms of speed due to the lack of an efﬁcient implementation similar to that of convolu-tions. Swin [21] on the other hand was one of the ﬁrst hierarchical vision transformers based on local self atten-tion.
Its design and the shifted-window self attention al-lowed it to be easily applicable to downstream tasks, as they made it computationally feasible, while also boosting performance through the additional biases injected. Swin’s localized attention, however, ﬁrst applies self attention to non-overlapping windows and then shifts the windows, the motivation of which was sliding window methods such as
SASA suffering throughput bottlenecks. HaloNet [30] used a haloing mechanism that localizes self attention for blocks of pixels at a time, as opposed to pixel-wise. One of their key motivations for this was also noted to be the lack of an efﬁcient sliding window attention.
In this work, we revisit explicit sliding window attention mechanisms, and propose Neighborhood Attention (NA).
NA localizes SA to each pixel’s nearest neighbors, which is not necessarily a ﬁxed window around the pixel. This change in deﬁnition allows all pixels to maintain an iden-tical attention span, which would otherwise be reduced for corner pixels in zero-padded alternatives (SASA). NA also approaches SA as its neighborhood size grows, and is equiv-alent to SA at maximum neighborhood. Additionally, NA has the added advantage of maintaining translational equiv-ariance [30], unlike blocked and window self attention. We
Self
Attention
Key
Value
Query
!
!
Query
Key
Value
Neighborhood
Attention
Positional
Bias
!
!
Figure 2.
Illustration of the query-key-value structure of
Neighborhood Attention (NA) vs Self Attention (SA) for a sin-SA allows each pixel to attend to every other pixel, gle pixel. whereas NA localizes attention for each pixel to a neighborhood around itself. Therefore, each pixel’s attention span is usually dif-ferent from the next.
N
AT T EN , a Python package with efﬁcient C++ develop and CUDA kernels that allow NA to run even faster than
Swin’s WSA in practice, while using less memory. We build Neighborhood Attention Transformer (NAT), which achieves competitive results across vision tasks.
To summarize, our main contributions are: 1. Proposing Neighborhood Attention (NA): A simple and ﬂexible explicit sliding window attention mech-anism that localizes each pixel’s attention span to its nearest neighborhood, approaches self attention as its span grows, and maintains translational equivariance.
We compare NA in terms of complexity and memory usage to self attention, window self attention, and con-volutions. 2. Developing efﬁcient C++ and CUDA kernels for NA, including the tiled NA algorithm, which allow NA to run up to 40% faster than Swin’s WSA while using up to 25% less memory. We release them under a new Python package for explicit sliding window at-ATTEN, to provide easy-to-tention mechanisms, use modules with autograd support that can be plugged into any existing PyTorch pipeline.
N 3. Introducing Neighborhood Attention Transformer (NAT), a new efﬁcient, accurate, and scalable hierar-chical transformer based on NA. We demonstrate its effectiveness on both classiﬁcation and downstream tasks. For instance, NAT-Tiny reaches 83.2% top-1 ac-curacy on ImageNet with only 4.3 GFLOPs and 28M parameters, and 51.4% box mAP on MS-COCO and 48.4% mIoU on ADE20K, signiﬁcantly outperforming both Swin Transformer and ConvNeXt [22].
Neighborhood Attention Transformer
ConvNeXt (CVPR 2022)
Swin Transformer (ICCV 2021)
NAT-B
NAT-S
NAT-T
ConvNeXt-S
Swin-S
NAT-M
ConvNeXt-T
Swin-T
Accuracy 84.5 84.0 83.5 83.0 82.5 82.0 81.5 81.0 80.5
ConvNeXt-B
Swin-B
Model parameters
Base
⇠ 90M
Small
Tiny
Mini
⇠ 50M
⇠ 30M
⇠ 20M 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
GFLOPs
Figure 3. ImageNet-1K classiﬁcation performance versus com-pute, with bubble size representing the number of parameters.
NAT outperfoms both Swin Transformer and ConvNeXt in classi-ﬁcation with fewer FLOPs, and a similar number of parameters. 2.