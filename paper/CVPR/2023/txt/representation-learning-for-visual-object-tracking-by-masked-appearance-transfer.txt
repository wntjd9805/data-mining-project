Abstract
Visual representation plays an important role in visual object tracking. However, few works study the tracking-specified representation learning method. Most trackers directly use ImageNet pre-trained representations. In this paper, we propose masked appearance transfer, a simple but effective representation learning method for tracking, based on an encoder-decoder architecture. First, we en-code the visual appearances of the template and search region jointly, and then we decode them separately. Dur-ing decoding, the original search region image is recon-structed. However, for the template, we make the decoder reconstruct the target appearance within the search region.
By this target appearance transfer, the tracking-specified representations are learned. We randomly mask out the inputs, thereby making the learned representations more dis-criminative. For sufficient evaluation, we design a simple and lightweight tracker that can evaluate the representa-tion for both target localization and box regression. Exten-sive experiments show that the proposed method is effec-tive, and the learned representations can enable the simple tracker to obtain state-of-the-art performance on six datasets. https://github.com/difhnp/MAT 1.

Introduction
Visual object tracking is a computer vision task that highly depends on the quality of visual representation [38].
On this basis, deep representations are adopted for track-ing and successfully boost the development of tracking al-gorithms in previous years. Unlike some primitive track-ers (e.g., [10, 28, 37]) that use ready-made deep features,
SiamFC [1] integrates a convolutional neural network (CNN) into the tracking model and learns task-specified representa-tions by end-to-end tuning. From here on, end-to-end model training becomes a common practice in siamese-based track-ing methods. The assumption of the siamese tracker is that
*Corresponding author: Huchuan Lu, lhchuan@dlut.edu.cn (a) Masked autoencoder. (b) Masked appearance transfer.
Figure 1. Comparison between the masked autoencoder [17] and our masked appearance transfer that uses a nontrivial learning objective. different visual appearances of the same target can be em-bedded to have similar representations, and tracking can be achieved by similarity matching. Based on this assumption, the learning objective aims to push the representations of the same target to be close to each other.
Although we have a clear learning objective, research on the effective representation learning methods for siamese tracking is still lacking. The common practice is to sim-ply fine-tune the ImageNet [33] pre-trained representations.
However, they are learned for classification rather than track-ing. High ImageNet accuracy does not indicate good per-formance in visual object tracking [43]. Without good rep-resentations, simple similarity matching cannot bring good tracking performance. Thus, most works focus on the design of tracker neck [7, 36, 47, 50] and tracker head [25, 40, 46].
Within these works, we can notice that the entire model is always trained end-to-end, and representation learning is cou-pled to the box regression task and target localization task.
It means that the representation learning is totally driven by other tracker components. To improve tracking performance, we have to make the neck or head increasingly complex.
On the aforementioned observation, we attempt to im-prove the tracking performance by learning good representa-tions and propose a simple but effective representation learn-ing method in this paper. This method aims to decouple the representation learning from tracker training, thereby mak-ing it a tracker-free method. It employs a transformer-based autoencoder to learn more discriminative visual features for object tracking. This is achieved by setting a nontrivial learning objective for the autoencoder, as illustrated by Fig-ure 1b. The embedded target appearance of the template can be pushed to be close to that in the search region. We further make the learned representations more discriminative by masking out the input images.
To evaluate the learned representations for target localiza-tion and box regression, we design a very simple tracker with a matching operator and a lightweight head. We evaluate the proposed method with different model architectures and initial weights. We also compare our simple tracker with many state-of-the-art trackers on the popular LaSOT [14],
TrackingNet [31], GOT10k [20], and many other tracking datasets [15, 30, 41]. The experiments demonstrate the effec-tiveness and generalization ability of our proposed represen-tation learning method. Comparison results show that our simple tracker can obtain state-of-the-art performance with the learned representations.
To summarize, our contributions are presented as follows:
• We propose masked appearance transfer (MAT), a novel representation learning method for visual object track-ing that jointly encodes the template and search region images, and learns tracking-specified representation with a simple encoder-decoder pipeline. A nontrivial training objective is proposed to make this method ef-fective.
• We design a simple and lightweight tracker for tracking-specified evaluation that has few parameters and no hyper-parameters. It can evaluate the representations not only for target localization but also for box regres-sion.
• Extensive experiments demonstrate the effectiveness and generalization ability of the proposed method. Ex-tensive comparisons show that the proposed simple tracker can obtain state-of-the-art performance by us-ing the learned tracking-specified representations. 2.