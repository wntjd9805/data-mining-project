Abstract
Recently, deep neural networks have shown great suc-cess on 3D point cloud classification tasks, which simulta-neously raises the concern of adversarial attacks that cause severe damage to real-world applications. Moreover, de-fending against adversarial examples in point cloud data is extremely difficult due to the emergence of various attack strategies. In this work, with the insight of the fact that the adversarial examples in this task still preserve the same se-mantic and structural information as the original input, we design a novel defense framework for improving the robust-ness of existing classification models, which consists of two main modules: the attention-based pooling and the dynamic contrastive learning. In addition, we also develop an algo-rithm to theoretically certify the robustness of the proposed framework. Extensive empirical results on two datasets and three classification models show the robustness of our ap-proach against various attacks, e.g., the averaged attack success rate of PointNet decreases from 70.2% to 2.7% on the ModelNet40 dataset under 9 common attacks. 1.

Introduction
With the rapid development of 3D sensors such as Li-DAR used in autonomous vehicles, point cloud data, which represents real-world objects by a set of 3D coordinates of points, has been widely applied in various 3D vision appli-cations [30]. Powered by the deep and non-linear structures, a number of deep learning models have proved to be ef-fective in modeling the geometric pattern underlying point cloud data, such as multi-layer perceptron (MLP) [36], con-volutional neural network (CNN) [24] and graph neural net-work (GNN) [47]. Despite the effectiveness, the extensive usage of DNN also raises the concern of adversarial ex-amples, where the input point clouds are slightly manipu-lated by an adversary to cause the misbehavior of a model
[22, 48, 52]. Considering its severe consequences and dam-age to real-world applications, the study of adversarial ex-*Corresponding authors: Mi Zhang and Min Yang
Figure 1. The demonstration of different adversarial attack strate-gies in point cloud classification. amples on point cloud data has been attracting more and more attention from both industry and academia.
Owing to the unique data format of point cloud, i.e., a set of 3D coordinates, the design of adversarial attacks varies in multiple aspects [30]. From the view of perturbations, the adversaries could shift existing points to create adver-sarial examples [52], which is similar to the adversarial at-tacks in images [13]. Besides, the adversaries could also delete [49, 63] or add points [28, 52] to conduct the attack.
Recent studies show that the generative model, i.e., trans-forming the original point cloud into a new one [15, 64], is also effective to find adversarial examples. From the view of restrictions, the constraints of the perturbations may differ in different approaches, e.g., limiting the number of altered points [18, 22], restricting the maximal/averaged distance of shifted points [40] and constraining the shape similarity between the adversarial examples and original ones [52].
Recently, many efforts have been made to mitigate po-tential adversarial examples in point cloud data [26, 65], which mainly fall into two categories,
• Adversarial Training-based (AT): this line of research takes inspiration from the work in the image domain [32], which proposes to pair adversarial examples with correct labels and put them into the training set [26, 40]. In the context of point cloud data, the main drawback of AT-based methods is that they are only robust to certain kinds of seen adversarial examples [61]. For instance, when the adversaries leverage an attack method that differs from the methods used in AT, the attack success rate could raise from 0.6% to 100.0% 1. Considering the diverse attack strategies in this task, it is difficult to find adversar-ial examples for AT-based methods that could generalize to various kinds of attacks.
• Recovery-based: this line of research reveals that adver-sarial examples in point cloud data often contain outlier points [52]. Based on this, recovery-based methods pro-pose to restore a clean sample from an adversarial exam-ple before feeding it to the classification model. For in-stance, SOR utilizes a rule-based strategy to filter outlier points [65], while DUP-Net [65] leverages a deep genera-tive network to recover the samples better [57]. Different from AT-based methods, the recovery-based methods do not focus on certain kinds of attack strategies, however, they could be evaded by shape-invariant attacks [42, 48] which take the geometric pattern of the perturbations into account, e.g., generating points that smoothly lie on the surface of the object [48], to make the recovery less ef-fective.
Our Work. Despite the variety of attack strategies, the semantic and structural information of different adversar-ial examples can hardly change [18, 22, 48, 52, 63]. For instance, an adversarial example of a car should still look like a car no matter how adversaries choose the attack strat-egy, e.g., adding, deleting, shifting or transforming. How-ever, previous works point out that existing classifiers often pay attention to limited segments or local features of the whole object to conduct the prediction, leading to the po-tential risk of different adversarial attacks [49, 50, 63]. This motivates us to improve the robustness of existing classifiers by enhancing the modeling of semantic and structural infor-mation. Based on this, we develop a novel defense frame-work called contrastive and attentional point cloud learning (CAP), which is mainly composed of two modules: (1) the attention-based feature pooling and (2) the dynamic con-trastive learning paradigm. The first module aims to cap-ture the global structural information of the object by rec-ognizing critical points among the point cloud data. To this end, we design a multi-head attention layer to assign critical points with higher weights, which will be used for obtaining the global representation of the input. We also introduce the temperature coefficient and random sampling techniques to prevent the module from focusing on a few fixed segments.
The second module aims to characterize the semantic infor-mation of different objects by disentangling the features of objects with different labels while gathering those with the same label. To this end, we design an interesting dynamic 1For detailed experimental setting and results please refer to Sec. 5 contrastive learning paradigm, which divides the learning goal into a coarse-to-fine process and helps the learning bet-ter converge.
With the aid of the proposed CAP, we can signifi-cantly improve the robustness against various adversarial examples for existing classification models such as Point-Net/PointNet++ [37], DGCNN [47] and PointCNN [24].
Furthermore, we show that the robustness of CAP is the-oretically certified. Specifically, given a certain constraint of the perturbations, we could evaluate whether the trained model is robust under arbitrary attack strategies, e.g., if an adversary would be able to add perturbations to a chair to obtain a prediction of a car. To this end, we first measure the changes in features after adding perturbations based on the manifold learning theory. Then we leverage the extreme value theory to estimate the upper bound of the potential changes, which indicates the optimal attack that aims to move the adversarial example across the decision bound-ary. After that, the robustness of the model can be mea-sured based on the estimated upper bound. With the pro-posed certified defense, a user could estimate the potential risk of adversarial attacks in real-world applications before deployment. We validate the proposed CAP on two bench-mark datasets and seven attack methods. In summary, the main contributions of this work are:
• We propose a novel and general solution for improving the robustness of existing point cloud classification mod-els by modeling the semantic and structural information, which is able to train robust models against various kinds of adversarial attacks.
• We present an algorithm to theoretically certify the ro-bustness of the proposed framework. With the aid of the manifold learning and the extreme value theory, the esti-mated robustness is highly consistent with the actual em-pirical results, i.e., attack success rate.
• Extensive experiments on two benchmark datasets show that our CAP can significantly improve the robustness of different classification models (PointNet/PointNet++,
DGCNN and PointCNN), e.g., the attack success rate of
PointNet decreases from 70.2% to 2.7% on average on the ModelNet40 dataset. 2.