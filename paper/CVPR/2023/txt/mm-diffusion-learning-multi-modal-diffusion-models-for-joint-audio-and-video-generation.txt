Abstract
We propose the first joint audio-video generation framework that brings engaging watching and listening experiences simultaneously, towards high-quality realistic videos. To generate joint audio-video pairs, we propose a novel
Multi-Modal Diffusion model (i.e., MM-Diffusion), with two-coupled denoising autoencoders. In contrast to existing single-modal diffusion models, MM-Diffusion consists of a sequential multi-modal U-Net for a joint denoising process by design. Two subnets for audio and video learn to grad-ually generate aligned audio-video pairs from Gaussian noises. To ensure semantic consistency across modalities, we propose a novel random-shift based attention block bridging over the two subnets, which enables efficient cross-modal alignment, and thus reinforces the audio-video fidelity for each other. Extensive experiments show superior
*This work was performed when Ludan Ruan was visiting Microsoft
Research Asia as research interns.
†Corressponding author. results in unconditional audio-video generation, and zero-shot conditional tasks (e.g., video-to-audio). In particular, we achieve the best FVD and FAD on Landscape and
AIST++ dancing datasets. Turing tests of 10k votes further demonstrate dominant preferences for our model. The code and pre-trained models can be downloaded at https:
//github.com/researchmm/MM-Diffusion. 1.

Introduction
AI-powered content generation in image, video, and audio domains has attracted extensive attention in recent years. For example, DALL·E 2 [33] and DiffWave [19] can create vivid art images and produce high-fidelity audio, re-spectively. However, such generated content can only pro-vide single-modality experiences either in vision or audi-tion. There are still large gaps with plentiful human-created contents on the Web which often involve multi-modal con-tents, and can provide engaging experiences for humans to perceive from both sight and hearing. In this paper, we take
one natural step forward to study a novel multi-modality generation task, in particular focusing on joint audio-video generation in the open domain.
Recent advances in generative models have been achieved by using diffusion models [14, 40]. From task-level perspectives, these models can be divided into two cat-egories: unconditional and conditional diffusion models. In particular, unconditional diffusion models generate images and videos by taking the noises sampled from Gaussian dis-tributions [14] as input. Conditional models usually import the sampled noises combined with embedding features from one modality, and generate the other modality as outputs, such as text-to-image [30, 33, 37], text-to-video [13, 39], audio-to-video [53], etc. However, most of the existing dif-fusion models can only generate single-modality content.
How to utilize diffusion models for multi-modality genera-tion still remains rarely explored.
The challenges of designing multimodal diffusion mod-els mainly lie in the following two aspects. First, video and audio are two distinct modalities with different data pat-terns. In particular, videos are usually represented by 3D signals indicating RGB values in both spatial (i.e., height × width) and temporal dimensions, while audio is in 1D wave-form digits across the temporal dimension. How to process them in parallel within one joint diffusion model remains a problem. Second, video and audio are synchronous in tem-poral dimension in real videos, which requires models to be able to capture the relevance between these two modalities and encourage their mutual influence on each other.
To solve the above challenges, we propose the first
Multi-Modal Diffusion model (i.e., MM-Diffusion) con-sisting of two-coupled denoising autoencoders for joint audio-video generation. Less-noisy samples from each modality (e.g., audio) at time step t − 1, are generated by implicitly denoising the outputs from both modalities (au-dio and video) at time step t. Such a design enables a joint distribution over both modalities to be learned. To further learn the semantic synchronousness, we propose a novel cross-modal attention block to ensure the generated video frames and audio segments can be correlated at each mo-ment. We design an efficient random-shift mechanism that conducts cross-attention between a given video frame and a randomly-sampled audio segment in a neighboring period, which greatly reduces temporal redundancies in video and audio and facilitates cross-modal interactions efficiently.
To verify the proposed MM-Diffusion model, we con-duct extensive experiments on Landscape dataset [21], and
AIST++ dancing dataset [22]. Evaluation results over
SOTA modality-specific (video or audio) unconditional generation models show the superiority of our model, with significant visual and audio gains of 25.0% and 32.9% by
FVD and FAD, respectively, on Landscape dataset. Supe-rior performances can be also observed in AIST++ dataset
[22], with large gains of 56.7% and 37.7% by FVD and
FAD, respectively, over previous SOTA models. We further demonstrate the capability of zero-shot conditional gener-ation for our model, without any task-driven fine-tuning.
Moreover, Turing tests of 10k votes further verify the high-fidelity performance of our results for common users. 2.