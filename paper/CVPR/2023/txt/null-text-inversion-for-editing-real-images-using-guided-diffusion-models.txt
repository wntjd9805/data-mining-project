Abstract prompt editing, showing high-fidelity editing of real images.
Recent large-scale text-guided diffusion models provide powerful image generation capabilities. Currently, a mas-sive effort is given to enable the modification of these im-ages using text only as means to offer intuitive and versatile editing tools. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaning-ful text prompt into the pretrained model’s domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the im-age. Our proposed inversion consists of two key novel com-ponents: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demon-strate that a direct DDIM inversion is inadequate on its own, but does provide a rather good anchor for our opti-mization. (ii) Null-text optimization, where we only mod-ify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embed-ding. This allows for keeping both the model weights and the conditional embedding intact and hence enables apply-ing prompt-based editing while avoiding the cumbersome tuning of the model’s weights. Our null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and various 1.

Introduction
The progress in image synthesis using text-guided diffu-sion models has attracted much attention due to their excep-tional realism and diversity. Large-scale models [29, 32, 34] have ignited the imagination of multitudes of users, en-abling image generation with unprecedented creative free-dom. Naturally, this has initiated ongoing research efforts, investigating how to harness these powerful models for im-age editing. Most recently, intuitive text-based editing was demonstrated over synthesized images, allowing the user to easily manipulate an image using text only [18].
However, text-guided editing of a real image with these state-of-the-art tools requires inverting the given image and textual prompt. That is, finding an initial noise vector that produces the input image when fed with the prompt into the diffusion process while preserving the editing capabili-ties of the model. The inversion process has recently drawn considerable attention for GANs [7,44], but has not yet been fully addressed for text-guided diffusion models. Although an effective DDIM inversion [15,37] scheme was suggested for unconditional diffusion models, it is found lacking for text-guided diffusion models when classifier-free guidance
[20], which is necessary for meaningful editing, is applied.
*Equal contribution. Performed this work while working at Google.
†Performed this work while working at Google.
In this paper, we introduce an effective inversion scheme, achieving near-perfect reconstruction, while retaining the rich text-guided editing capabilities of the original model (see Fig. 1). Our approach is built upon the analysis of two key aspects of guided diffusion models: classifier-free guid-ance and DDIM inversion.
In the widely used classifier-free guidance, in each dif-fusion step, the prediction is performed twice: once uncon-ditionally and once with the text condition. These predic-tions are then extrapolated to amplify the effect of the text guidance. While all works concentrate on the conditional prediction, we recognize the substantial effect induced by the unconditional part. Hence, we optimize the embedding used in the unconditional part in order to invert the input image and prompt. We refer to it as null-text optimization, as we replace the embedding of the empty text string with our optimized embedding.
DDIM Inversion consists of performing DDIM sampling in reverse order. Although a slight error is introduced in each step, this works well in the unconditional case. How-ever, in practice, it breaks for text-guided synthesis, since classifier-free guidance magnifies its accumulated error. We observe that it can still offer a promising starting point for the inversion. Inspired by GAN literature, we use the sequence of noised latent codes, obtained from an initial
DDIM inversion, as pivot [31]. We then perform our opti-mization around this pivot to yield an improved and more accurate inversion. We refer to this highly efficient op-timization as Diffusion Pivotal Inversion, which stands in contrast to existing works that aim to map all possible noise vectors to a single image.
To the best of our knowledge, our approach is the first to enable the text editing technique of Prompt-to-Prompt
[18] on real images. Moreover, unlike recent approaches
[21, 41], we do not tune the model weights, thus avoiding damaging the prior of the trained model and duplicating the entire model for each image. Throughout comprehen-sive ablation study and comparisons, we demonstrate the contribution of our key components to achieving a high-fidelity reconstruction of the given real image, while allow-ing meaningful and intuitive editing abilities. Our code, built upon the publicly available Stable Diffusion model, will be released. 2.