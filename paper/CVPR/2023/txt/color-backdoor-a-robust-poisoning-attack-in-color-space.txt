Abstract
Backdoor attacks against neural networks have been in-tensively investigated, where the adversary compromises the integrity of the victim model, causing it to make wrong predictions for inference samples containing a specific trig-ger. To make the trigger more imperceptible and human-unnoticeable, a variety of stealthy backdoor attacks have been proposed, some works employ imperceptible pertur-bations as the backdoor triggers, which restrict the pixel differences of the triggered image and clean image. Some works use special image styles (e.g., reflection, Instagram filter) as the backdoor triggers. However, these attacks sac-rifice the robustness, and can be easily defeated by common preprocessing-based defenses.
This paper presents a novel color backdoor attack, which can exhibit robustness and stealthiness at the same time. The key insight of our attack is to apply a uniform color space shift for all pixels as the trigger. This global feature is robust to image transformation operations and the triggered samples maintain natural-looking. To find the op-timal trigger, we first define naturalness restrictions through the metrics of PSNR, SSIM and LPIPS. Then we employ the
Particle Swarm Optimization (PSO) algorithm to search for the optimal trigger that can achieve high attack effective-ness and robustness while satisfying the restrictions. Exten-sive experiments demonstrate the superiority of PSO and the robustness of color backdoor against different main-stream backdoor defenses. 1.

Introduction
Neural networks have been applied in an increasing vari-ety of domains, including image classification [10], speech recognition [16] and natural language processing [1]. How-ever, recent studies show that neural networks are suscep-tible to backdoor attacks [9, 14]. The adversary can embed
*This work was done at NTU as a visiting student.
†Corresponding author a backdoor into the victim model by poisoning the train-ing dataset. Consequently, the backdoored victim model will perform normally on clean samples but behave wrongly on samples containing a specific trigger. Such threat can bring severe damages to many critical applications in the real world, such as face authentication [36], malware detec-tion [30], speech recognition [39], autonomous driving [13], etc.
Researchers advance the backdoor study by proposing a variety of sophisticated attack techniques. These attacks are improved from two perspectives. (1) Stealthiness. The backdoor in the infected model can bypass existing detec-tion approaches. Additionally, the triggers are designed to look natural and evade human inspection. (2) Robustness.
The backdoor and the triggers are expected to be robust and cannot be easily removed by the defender. A backdoor at-tack with these features will be very difficult to mitigate.
However, we observe that pursuing the visual stealthi-ness can sacrifice the attack robustness. Specifically, there can be two kinds of strategies for stealthy backdoor attacks.
The first one is invisible triggers, which restrict the pixel distances between the clean and triggered images [2,17,46].
Some attacks further enforce the consistency of the latent representation besides the pixels to achieve stealthiness in the feature space [5, 27, 44]. The second strategy is nat-ural triggers, which use special image styles (e.g., reflec-tion [22], Instagram filter [21], weather condition [3]) to activate the backdoor. The triggered images do not need to maintain the similarity from the clean images, but just look natural to human eyes. Unfortunately, these delicate backdoor triggers can be easily invalidated by common im-age transformation operations, and the corresponding back-door attacks are vulnerable to some preprocessing-based defenses, e.g., DeepSweep [25], image compression [37],
ShrinkPad [19] (see Section 4.4.1 for evaluation results).
Besides, some methods [3, 5, 27, 44] require the adversary to have full control over the victim’s training process, which can not be applied to the data poisoning threat model.
To overcome these limitations, we propose color back-door, a novel poisoning-based backdoor attack that can ex-(a) Original images (b) Triggered images of color backdoor
Figure 1. Visual comparisons of the original images and triggered images from ImageNet. hibit both stealthiness and robustness. Our color backdoor is inspired by the shape bias property of the human cogni-tive system [12] (i.e., humans prefer to categorize objects according to their shapes rather than colors).
It employs a uniform color space shift for all pixels as the backdoor trigger. As illustrated in Figure 1, the triggered image se-mantically represents the same object as the original image in a very natural way, and can evade the inspection of the defender. We also use Local Interpretable Model-Agnostic
Explanations (LIME) [28] to explain the effectiveness of our attack. As presented in Figure 2, LIME visualizes the areas contributed to the predictions of the backdoored model, the model focuses on the object itself when the test sample is clean and on the whole image when the test sam-ple is triggered. This is because the model can learn the structural information (i.e., the specific color space shift) of the image and recognize backdoor samples with this feature. an (color
Nevertheless,
LIME explanation.
Figure 2.
Left: clean image. Right: back-door image find-appropriate ing space trigger for color back-shift) door is non-trivial: a large shift makes the triggered samples less realistic (see Figure 4), while a small shift makes it difficult for the model to learn this feature, resulting in low effectiveness and robustness. To address this problem under the practical black-box setting1, we adopt Particle Swarm Optimization (PSO) [6], an effective gradient-free optimization algorithm, to systematically search for the optimal trigger. Specifically, we first use the backdoor loss of a semi-trained model (with surrogate model architecture) to efficiently estimate the effectiveness of a trigger. Then, we quantify the naturalness of a trigger through three popular similarity metrics, PSNR [42],
SSIM [35] and LPIPS [42], based on which we define a naturalness restriction. After that, we add a penalty function of the naturalness restriction during the searching process of PSO and find the optimal trigger. Finally, the color backdoor is embedded into the victim model when training with the poisoned dataset.
We perform extensive experiments to demonstrate the superiority of PSO over other optimization algorithms. We show our color backdoor is more resilient against state-of-the-art preprocessing-based defenses compared to existing attacks. Besides, it can also bypass other mainstream de-fenses including Neural Cleanse [34], Fine-Pruning [20],
STRIP [8], Grad-Cam [29] and Spectral Signature [31]. 2.