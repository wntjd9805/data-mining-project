Abstract
Finding an initial noise vector that produces an input image when fed into the diffusion process (known as inver-sion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The stan-dard approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs [29]) to deter-ministically noise the image to the intermediate state along the path that the denoising would follow given the original conditioning. However, DDIM inversion for real images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incor-rect image reconstruction and loss of content. To alleviate these problems, we propose Exact Diffusion Inversion via
Coupled Transformations (EDICT), an inversion method that draws inspiration from affine coupling layers. EDICT enables mathematically exact inversion of real and model-generated images by maintaining two coupled noise vectors which are used to invert each other in an alternating fash-ion. Using Stable Diffusion [25], a state-of-the-art latent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex im-age datasets like MS-COCO, EDICT reconstruction signif-icantly outperforms DDIM, improving the mean square er-ror of reconstruction by a factor of two. Using noise vectors inverted from real images, EDICT enables a wide range of image edits—from local and global semantic edits to image stylization—while maintaining fidelity to the original image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be combined with any pretrained DDM. 1.

Introduction
Using the iterative denoising diffusion principle, denois-ing diffusion models (DDMs) trained with web-scale data can generate highly realistic images conditioned on input text, layouts, and scene graphs [24, 25, 27]. After im-age generation, the next important application of DDMs being explored by the research community is that of im-age editing. Models such as DALL-E-2 [24] and Stable
Diffusion [25] can perform inpainting, allowing users to edit images through manual annotation. Methods such as
SDEdit [20] have demonstrated that both synthetic and real images can be edited using stroke or composite guidance via
DDMs. However, the goal of holistic image editing tools that can edit any real/artificial image using purely text is still a field of active research.
The generative process of DDMs starts with an initial noise vector (xT ) and performs iterative denoising (typ-ically with a guidance signal e.g. in the form of text-conditional denoising), ending with a realistic image sam-ple (x0). Reversing this generative process is key in solving this image editing problem for one family of approaches.
Formally, this problem is known as “inversion” i.e., finding the initial noise vector that produces the input image when passed through the diffusion process.
A na¨ıve approach for inversion is to add Gaussian noise to the input image and perform a predefined number of diffusion steps, which typically results in significant dis-tortions [7]. A more robust method is adapting Denois-ing Diffusion Implicit Models (DDIMs) [29]. Unlike the commonly used Denoising Diffusion Probabilistic Models (DDPMs) [9], the generative process in DDIMs is defined in a non-Markovian manner, which results in a determinis-tic denoising process. DDIM can also be used for inversion, deterministically noising an image to obtain the initial noise vector (x0 → xT ).
DDIM inversion has been used for editing real images through text methods such as DDIBs [30] and Prompt-to-Prompt (P2P) image editing [7]. After DDIM inversion,
P2P edits the original image by running the generative pro-cess from the noise vector and injecting conditioning infor-mation from a new text prompt through the cross-attention layers in the diffusion model, thus generating an edited im-age that maintains faithfulness to the original content while incorporating the edit. However, as noted in the original
P2P work [7], the DDIM inversion is unstable in many cases—encoding from x0 to xT and back often results in inexact reconstructions of the original image as in Fig. 2.
These distortions limit the ability to perform significant ma-Figure 1. EDICT enables complex real image edits, such as editing dog breeds. We highlight the fine-grain text preservation in the bottom row of examples, with the message remaining even as the dog undergoes dramatic transformations. More examples, including baseline comparisons for all image-breed pairs, are included in the Supplementary. All original images from the ImageNet 2012 validation set. nipulations through text as increase in the corruption is cor-related with the strength of the conditioning. not require any computationally-expensive model finetun-ing, prompt tuning, or multiple images.
To improve the inversion ability of DDMs and enable robust real image editing, we diagnose the problems in
DDIM inversion, and offer a solution: Exact Diffusion In-version via Coupled Transformations (EDICT). EDICT is a re-formulation of the DDIM process inspired by coupling layers in normalizing flow models [4, 5, 12] that allows for mathematically exact inversion. By maintaining two cou-pled noise vectors in the diffusion process, EDICT enables recovery of the original noise vector in the case of model-generated images; and for real imagery, initial noise vec-tors that are guaranteed to map to the original image when the EDICT generative process is run. While EDICT dou-bles the computation time of the diffusion process, it can be combined with any pretrained DDM model and does
For the standard generative process, EDICT approxi-mates DDIM well, resulting in nearly identical generations given equal initial conditions For real images, EDICT can recover a noise vector which yields an exact reconstruction when used as input to the generative process. Experiments with the COCO dataset [16] show that EDICT can recover complex image features such as detailed textures, thin ob-jects, subtle reflections, faces, and text, while DDIM fails to do so consistently. Finally, using the initial noise vectors de-rived from a real image with EDICT, we can sample from a
DDM and perform complex edits or transformations to real images using textual guidance. We show editing capabili-ties including local and global modifications of objects and background and object transformations (Fig. 1).
ing DDMs to image editing is SDEdit [20] where coarse layouts are used to guide the generative process by nois-ing the layout to resemble an intermediately noised image.
Prompt-to-Prompt [7] combines query-key pairs from one prompt with values from another in the attention layers of a DDM to enable prompt-guided image editing from in-termediate latents obtained by sampler inversion [29, 31].
DiffEdit [3] edits real/synthetic images using automatically generated masks for regions of an input image that should be edited given a text query. Kwon et al. [14] introduce style and structure losses to guide the sampling process to enable text-guided image translation. CycleDiffusion [32] uses a deterministic DPM encoder to enable zero-shot image-to-image translation. Concurrent work [21] also stabilizes text-conditioned inversion, optimizing the null embedding ∅ for consistency. While highly effective, the algorithmic exact-ness is not guaranteed and per-case optimization is required.
Another set of methods [6, 11, 26] finetune the model with the target image and/or learn a new conditioning prompt to enable indirect image editing via sampling. EDICT, our proposed approach, does not require any specialized train-ing/finetuning, and can be paired with any pretrained DDM. 3.