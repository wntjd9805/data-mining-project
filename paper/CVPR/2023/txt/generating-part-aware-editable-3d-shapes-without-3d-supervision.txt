Abstract
Impressive progress in generative models and implicit representations gave rise to methods that can generate 3D shapes of high quality. However, being able to locally con-trol and edit shapes is another essential property that can unlock several content creation applications. Local control can be achieved with part-aware models, but existing meth-ods require 3D supervision and cannot produce textures. In this work, we devise PartNeRF, a novel part-aware gener-ative model for editable 3D shape synthesis that does not require any explicit 3D supervision. Our model generates objects as a set of locally defined NeRFs, augmented with an affine transformation. This enables several editing op-erations such as applying transformations on parts, mixing
*Work done during internship at Stanford. parts from different objects etc. To ensure distinct, manip-ulable parts we enforce a hard assignment of rays to parts that makes sure that the color of each ray is only determined by a single NeRF. As a result, altering one part does not af-fect the appearance of the others. Evaluations on various
ShapeNet categories demonstrate the ability of our model to generate editable 3D objects of improved fidelity, compared to previous part-based generative approaches that require 3D supervision or models relying on NeRFs. 1.

Introduction
Generating realistic and editable 3D content is a long-standing problem in computer vision and graphics that has recently gained more attention due to the increased demand for 3D objects in AR/VR, robotics and gaming applications.
However, manual creation of 3D models is a laborious en-deavor that requires technical skills from highly experi-enced artists and product designers. On the other hand, edit-ing 3D shapes, typically involves re-purposing existing 3D models, by manually changing faces and vertices of a mesh and modifying its respective UV-map [95]. To accommo-date this process, several recent works introduced genera-tive models that go beyond generation and allow editing the generated instances [13,18,52,55,62,77,101,116,117,124].
Shape editing involves making local changes on the shape and the appearance of different parts of an object. There-fore, having a basic understanding of the decomposition of the object into parts facilitates controlling what to edit.
While Generative Adversarial Networks (GANs) [30] have emerged as a powerful tool for synthesizing photore-alistic images [7, 15, 16, 47â€“49], scaling them to 3D data is non-trivial as they ignore the physics of image forma-tion process.To address this, 3D-aware GANs incorporate 3D representations such as voxel grids [38, 72, 75] or com-bine them with differentiable renderers [57, 126]. While they faithfully recover the geometry and appearance, they do not allow changing specific parts of the object.
Inspired by the rapid evolution of neural implicit ren-dering techniques [68], recent works [8, 32, 77, 96, 114] proposed to combine them with GANs in order to allow for multi-view-consistent generations of high quality. De-spite their impressive performance on novel view synthe-sis, their editing capabilities are limited. To this end, editing operations in the latent space have been explored
[21, 42, 62, 115, 124] but these approaches lack intuitive control over the shape. By decomposing shapes into parts, other works facilitate structure-aware shape manipulations
[40,70,88,111]. However, they require 3D supervision dur-ing training and can only operate on textureless shapes.
To address these limitations, we devise PartNeRF, a novel part-aware generative model, implemented as an auto-decoder [5]. Our model enables part-level control, which fa-cilitates various editing operations on the shape and appear-ance of the generated instance. These operations include rigid and non-rigid transformations on the object parts, part mixing from different objects, removing/adding parts and editing the appearance of specific parts of the object.
Our key idea is to represent objects using a set of locally defined Neural Radiance Fields (NeRFs) that are arranged such that the object can be plausibly rendered from a novel view. To enable part-level control, we enforce a hard as-signment between parts and rays that ensures that altering one part does not affect the shape and appearance of the others. Our model does not require 3D supervision; we only assume supervision from images and object masks captured from known cameras. We evaluate PartNeRF on various
ShapeNet categories and demonstrate that it generates tex-tured shapes of higher fidelity than both part-based as well as NeRF-based generative models. Furthermore, we show-case several editing operations, not previously possible.
In summary, we make the following contributions:
We propose the first part-aware generative model that parametrizes parts as NeRFs and can generate editable 3D shapes. Unlike prior part-based approaches, our model does not require explicit 3D supervision and can gener-ate textured shapes. Compared to NeRF-based generative models, our work is the first that reasons about parts and hence enables operations both on the shape and the tex-ture of the generated object. Code and data is available at https://ktertikas.github.io/part nerf. 2.