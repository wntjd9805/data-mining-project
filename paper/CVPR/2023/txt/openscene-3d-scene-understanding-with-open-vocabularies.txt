Abstract
Traditional 3D scene understanding approaches rely on labeled 3D datasets to train a model for a single task with supervision. We propose OpenScene, an alternative ap-proach where a model predicts dense features for 3D scene points that are co-embedded with text and image pixels in
CLIP feature space. This zero-shot approach enables task-agnostic training and open-vocabulary queries. For exam-ple, to perform SOTA zero-shot 3D semantic segmentation it first infers CLIP features for every 3D point and later classifies them based on similarities to embeddings of ar-bitrary class labels. More interestingly, it enables a suite of open-vocabulary scene understanding applications that have never been done before. For example, it allows a user to enter an arbitrary text query and then see a heat map indicating which parts of a scene match. Our approach is effective at identifying objects, materials, affordances, ac-tivities, and room types in complex 3D scenes, all using a single model trained without any labeled 3D data. 1.

Introduction 3D scene understanding is a fundamental task in com-puter vision. Given a 3D mesh or point cloud with a set of posed RGB images, the goal is to infer the semantics, af-fordances, functions, and physical properties of every 3D point. For example, given the house shown in Figure 1, we would like to predict which surfaces are part of a fan (se-mantics), made of metal (materials), within a kitchen (room types), where a person can sit (affordances), where a person can work (functions), and which surfaces are soft (physical properties). Answers to these queries can help a robot inter-act intelligently with the scene or help a person understand it through interactive query and visualization.
Achieving this broad scene understanding goal is chal-lenging due to the diversity of possible queries. Tradi-tional 3D scene understanding systems are trained with su-pervision from benchmark datasets designed for specific tasks (e.g., 3D semantic segmentation for a closed set of 20 classes [5, 12]). They are each designed to answer one type of query (is this point on a chair, table, or bed?), but provide little assistance for related queries where training data is scarce (e.g., segmenting rare objects) or other queries with no 3D supervision (e.g., estimating material properties).
In this paper, we investigate how to use pre-trained text-image embedding models (e.g., CLIP [43]) to assist in 3D scene understanding. These models have been trained from
the couches, beds, and comfy chairs as the best matches.
Since our approach is zero-shot (i.e. no use of labeled data for the target task), it does not perform as well as fully-supervised approaches on the limited set of tasks for which there is sufficient training data in traditional benchmarks (e.g., 3D semantic segmentation with 20 classes). How-ever, it does achieve significantly stronger performance on other tasks. For example, it beats a fully-supervised ap-proach on indoor 3D semantic segmentation with 40, 80, or 160 classes. It also performs better than other zero-shot baselines, and can be used without any retraining on novel datasets even if they have different label sets. It works for indoor RGBD scans as well as outdoor driving captures.
Overall, our contributions are summarized as follows:
• We introduce open vocabulary 3D scene understanding tasks where arbitrary text queries are used for semantic segmentation, affordance estimation, room type classifi-cation, 3D object search, and 3D scene exploration.
• We propose OpenScene, a zero-shot method for extract-ing 3D dense features from an open vocabulary embed-ding space using multi-view fusion and 3D convolution.
• We demonstrate that the extracted features can be used for 3D semantic segmentation with performance better than fully supervised methods for rare classes. 2.