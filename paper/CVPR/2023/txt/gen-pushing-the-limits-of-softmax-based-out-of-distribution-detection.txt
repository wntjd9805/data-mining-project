Abstract
Out-of-distribution (OOD) detection has been exten-sively studied in order to successfully deploy neural net-works, in particular, for safety-critical applications. More-over, performing OOD detection on large-scale datasets is closer to reality, but is also more challenging. Sev-eral approaches need to either access the training data for score design or expose models to outliers during train-ing. Some post-hoc methods are able to avoid the afore-mentioned constraints, but are less competitive.
In this work, we propose Generalized ENtropy score (GEN), a simple but effective entropy-based score function, which can be applied to any pre-trained softmax-based classi-Its performance is demonstrated on the large-scale fier.
ImageNet-1k OOD detection benchmark.
It consistently improves the average AUROC across six commonly-used
CNN-based and visual transformer classifiers over a num-ber of state-of-the-art post-hoc methods. The average AU-ROC improvement is at least 3.5%. Furthermore, we used
GEN on top of feature-based enhancing methods as well as methods using training statistics to further improve the
OOD detection performance. The code is available at: https://github.com/XixiLiu95/GEN. 1.

Introduction
In order to make the usage of deep learning methods in real-word applications safer, it is crucial to distinguish whether an input at test time is a valid in-distribution (ID) sample or a previously unseen out-of-distribution (OOD) sample. Thus, a trained deep neural network (DNN) should ideally know what it does not know [30]. This ability is particularly important for high-stake applications in au-tonomous driving [8] and medical image analysis [35].
However, it is common for neural networks to make over-confident predictions even for OOD samples. A recent sur-vey on OOD detection [45] identifies several scenarios re-quiring the detection of OOD samples, with covariate shift
Figure 1. Performance of Post-hoc OOD Detection Methods Ap-plied to 6 Classifiers Trained on ImageNet-1K. Reported are AU-ROC values (%) averaged over the models. Methods marked with light squares use information from logits / probabilities. Meth-ods marked with dark crosses also use information from features.
ReActâˆ— corresponds to performing extra feature clipping before computing the score. (change in the input distribution) and semantic shift (change in the label distribution) being two important settings.
In this work, we focus on the semantic shift scenario, meaning that we aim to detect inputs with semantic labels not present in the training set. When solving the OOD de-tection problem, the idea is to design a scalar score function of a data sample as an argument that assigns higher values to true ID samples. The semantic shift scenario also allows us to mainly focus on the predictive distribution as provided by a DNN classifier to design such score function.
A number of existing works for OOD detection rely on the predictive distribution [14, 28], but often a better OOD detection performance can be achieved when also incor-porating feature statistics for ID data [13, 24, 37, 38, 43].
These high-performing methods have practical constraints that can be challenging to eliminate: some methods require access to at least a portion of training data [13, 24, 37, 43] while others need access to internal feature activations [38].
However, commercially deployed DNNs are often black-Figure 2. Score Distributions. The top row is GEN, and the bottom one is Energy [28]. The distributions are shown for the ID ImageNet-1K dataset (dark blue) and four OOD datasets (light blue). The classification model used here is Swin [29]. box classifiers, and the training data is likely to be confiden-tial. Hence, the goal of this work is to explore and push the limits of OOD detection when the output of a softmax layer is the only available source of information. Our method therefore falls under the post-hoc category of OOD detec-tion frameworks, where only a trained DNN is used without the need for training data. Fig. 1 highlights its performance compared to other methods in this category.
Contribution We propose GEN, a simple but effective entropy-based method for OOD detection. (i) GEN uses predictive distribution only. It does not require re-training and/or outlier exposure, it does not use any training data statistics. (ii) Yet it performs very well (see Figs. 1 and 2), meaning that it can potentially be used in more constrained model deployment scenarios. Compared to other post-hoc methods, score distributions produced by GEN lead to a bet-ter ID/OOD separation. We show that our method consis-tently achieves better results in terms of AUROC (and usu-ally in terms of FPR95) compared to other post-hoc meth-ods. In particular, GEN on average outperforms other post-hoc methods on the largest and carefully constructed OOD dataset OpenImage-O as well as on the very challenging
ImageNet-O dataset based on natural adversarial examples. 2.