Abstract
Graph is a highly generic and diverse representation, suitable for almost any data processing problem. Spec-tral graph theory has been shown to provide powerful algo-rithms, backed by solid linear algebra theory. It thus can be extremely instrumental to design deep network build-ing blocks with spectral graph characteristics. For in-stance, such a network allows the design of optimal graphs for certain tasks or obtaining a canonical orthogonal low-dimensional embedding of the data. Recent attempts to solve this problem were based on minimizing Rayleigh-quotient type losses. We propose a different approach of directly learning the graph’s eigensapce. A severe prob-lem of the direct approach, applied in batch-learning, is the inconsistent mapping of features to eigenspace coordinates in different batches. We analyze the degrees of freedom of learning this task using batches and propose a stable align-ment mechanism that can work both with batch changes and with graph-metric changes. We show that our learnt spec-tral embedding is better in terms of NMI, ACC, Grassman distnace, orthogonality and classification accuracy, com-pared to SOTA. In addition, the learning is more stable. 1.

Introduction
Representing information by using graphs and analyz-ing their spectral properties has been shown to be an effec-tive classical solution in a wide range of problems including clustering [8, 21, 32], classification [13], segmentation [26], dimensionality reduction [5, 10, 23] and more. In this set-ting, data is represented by nodes of a graph, which are em-bedded into the eigenspace of the graph-Laplacian, a canon-ical linear operator measuring local smoothness.
Incorporating analytic data structures and methods within a deep learning framework has many advantages.
It yields better transparency and understanding of the net-work, allows the use of classical ideas, which were thor-oughly investigated and can lead to the design of new ar-chitectures, grounded in solid theory. Spectral graph algo-rithms, however, are hard to incorporate directly in neural-networks since they require eigenvalue computations which cannot be integrated in back-propagation training algo-rithms. Another major drawback of spectral graph tools is their low scalability. It is not feasible to hold a large graph containing millions of nodes and to compute its graph-Laplacian eigenvectors. Moreover, updating the graph with additional nodes is combersome and one usually resorts to graph-interpolation techniques, referred to as Out Of Sam-ple Extension (OOSE) methods.
An approach to solve the above problems using deep neural networks (DNNs), firstly suggested in [24] and re-cently also in [9], is to train a network that approximates the eigenspace by minimizing Rayleigh quotient type losses.
The core idea is that the Rayleigh quotient of a sum of n vec-tors is minimized by the n eigenvectors with the correspond-ing n smallest eigenvalues. As a result, given the features of a data instance (node) as input, these networks generate the respective coordinate in the spectral embedding space.
This space should be equivalent in some sense to the ana-lytically calculated graph-Laplacian eigenvector space. A common way to measure the equivalence of these spaces is using the Grassman distance. Unfortunately, applying this indirect approach does not guarantee convergence to the de-sired eigenspace and therefore the captured might not be faithful.
An alternative approach, suggested in [18] for computing the diffusion map embedding, is a direct supervised method.
The idea is to compute the embedding analytically, use it as ground-truth and train the network to map features to eigenspace coordinates in a supervised manner. In order to compute the ground truth embedding, the authors used the entire training set. This operation is very demanding com-putationally in terms of both memory and time and is not scalable when the training set is very large.
Our proposed method is to learn directly the eigenspace in batches. We treat each batch as sampling of the full graph and learn the eigenvector values in a supervised manner.
A major problem of this kind of scheme is the inconsis-tency in the embedding coordinates. Thus, two instances
Figure 1. Toy examples. An Illustration of trained BASiS models over common spectral-clustering toy examples. Each figure describes the embedding values given by the network to each point in the space and the clustering results over selected points. BASiS performs successful clustering and is able to interpolate and extrapolate the training data smoothly. in different batches with the same features can be mapped to very different eigenspace coordinates. Our solution is to use affine registration techniques to align the batches. Fur-ther, we use this alignment strategy to also allow changes in the graph affinity metric. Our proposed method retains the following main qualities: 1) Scalability. Data is learnt in batches, allowing a training based on large and complex input sets; 2) OOSE. Out of sample extension is immediate. 3) High quality approximation of the eigenspace. Since our learning method is direct and fully supervised, an ex-cellent approximation of the graph eigenspace is obtained. 4) Robustness to features change. We can train the model also when features and affinities between nodes change; All the above properties yield a spectral building block which can be highly instrumental in various deep learning algo-rithms, containing an inherent orthogonal low dimensional embedding of the data, based on linear algebraic theory. 2. Settings and Notations
Let {xi}n i=1 be a set of data instances denoted as X which is a finite set in Rd. These samples are assumed to lie on a lower dimensional manifold M .
These instances are represented as nodes on an undi-rected weighted graph G = (V, E,W ), where V and E are sets of the vertices and edges, respectively, and W is the ad-jacency matrix. This matrix is symmetric and defined by a distance measure between the nodes. For example, a com-mon choice is a Gaussian kernel and Euclidean distance, (cid:32)
Wi j = exp
−
||xi − x j||2 2 2σ 2 (cid:33)
, (1) where σ is a soft-threshold parameter.
The degree matrix D is a diagonal matrix where Dii is the degree of the i-th vertex, i.e., Dii = ∑ j Wi j. The graph-Laplacian operator is defined by,
L := D −W. (2)
The graph-Laplacian is a symmetric, positive semi-definite matrix, its eigenvalues are real, and its eigenvectors form an orthogonal basis. The eigenvalues of L are sorted in as-cending order λ1 ≤ λ2 ≤ ... ≤ λn, where the corresponding eigenvectors are denoted by u1, u2..., un. The sample xi is represented in the spectral embedding space as the ith row
∈ Rn×K, denoted as ϕi.
· · · uK of the matrix U =
Thus, more formally, the dimensionality reduction process can be formulated as (cid:104) u1 (cid:105) xi (cid:55)−→ ϕi = [u1(i), u2(i), ..., uK(i)] ∈ RK, (3) where K ≪ d. This representation preserves well essential data information [10, 15, 17, 22]
Alternatively, one can replace the Laplacian definition (2) with
LN := D− 1 2 LD− 1 2 = I − D− 1 2 W D− 1 2 . (4)
This matrix may yield better performances for certain tasks and datasets [25–27]. 3.