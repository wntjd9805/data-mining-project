Abstract
The ability to distinguish between different movie scenes is critical for understanding the storyline of a movie. How-ever, accurately detecting movie scenes is often challeng-ing as it requires the ability to reason over very long movie segments. This contrasts with most existing video recogni-tion models, which are typically designed for short-range video analysis. This work proposes a State-Space Trans-former model that can efficiently capture dependencies in long movie videos for accurate movie scene detection. Our model, called TranS4mer, is built using a novel S4A building block, combining the strengths of structured state-space se-quence (S4) and self-attention (A) layers. Given a sequence of frames divided into movie shots (uninterrupted periods where the camera position does not change), the S4A block first applies self-attention to capture short-range intra-shot dependencies. Afterward, the state-space operation in the
S4A block aggregates long-range inter-shot cues. The fi-nal TranS4mer model, which can be trained end-to-end, is obtained by stacking the S4A blocks one after the other mul-tiple times. Our proposed TranS4mer outperforms all prior methods in three movie scene detection datasets, including
MovieNet, BBC, and OVSD, while being 2× faster and re-quiring 3× less GPU memory than standard Transformer models. We will release our code and models. 1.

Introduction
Imagine watching The Godfather movie as illustrated in
Figure 1. The first scene starts in the office room of mafia boss Don Vito Corleone, where he is discussing family busi-ness affairs with his eldest son Sonny Corleone and the fam-ily lawyer Tom Hagen. Then in the second scene, the family meets Virgil Sollozzo to hear about his business proposi-tion. Afterward, the movie transitions into a third scene, where Don Vito chides his son Sonny for interfering. All of these movie scenes are inherently connected and create a complex storyline. Thus, the ability to recognize such
*Research done while MI was an intern at Comcast Labs. movie scenes is essential for understanding the plot of a movie. Moreover, identifying movie scenes enables broader applications, such as content-driven video search, preview generation, and minimally disruptive ad insertion.
Unlike standard video recognition tasks (e.g., action recognition), which require a short-range analysis of video clips lasting only 5–10 seconds, movie scene detection re-quires short- and long-range understanding, where videos might last several minutes. For instance, to successfully recognize the scenes in Figure 1, we need to identify the vi-sual concepts in each frame, such as characters, objects, and backgrounds (short-range modeling), while also reasoning about complex temporal events and activities (long-range modeling). For example, if we only look at the bound-ary frames of the first and second scenes of Figure 1 and identify the local properties (e.g., characters, objects, and backgrounds), it is quite difficult to determine the transi-tion between the two scenes. However, considering longer temporal windows with frames from both scenes, we can recognize global events and identify their boundaries.
We also note that models that only use short-range tem-poral context typically produce many false positives due to abrupt low-level changes in the scene dynamics (e.g., cam-era switching between characters). For example, if we look at the middle two frames of scene 2 in Figure 1, we might mistake them for a scene boundary since the frames contain two characters. However, looking at all the frames in scene 2 makes it clear that they belong to the same scene. There-fore, a successful movie scene detection model should iden-tify the short-range visual elements and also reason about their long-range temporal dependencies in a movie segment.
However, most existing movie scene detection meth-ods [11, 35] are built using convolutional neural networks (CNNs), which are inherently designed using local opera-tors (i.e., convolution kernels) for short-range video anal-ysis. Thus, such models often fail to capture the long-range dependencies crucial for accurate movie scene de-tection. Recently, transformers have been shown to effec-tively model short-range and long-range dependencies in various Natural Language Processing (NLP) tasks [1, 7, 14,
Inspired by the success of transform-15, 32, 37, 47, 49].
Figure 1. Three scenes from the movie The Godfather. In the first scene, The Godfather (Don Vito Corleone) discusses family business affairs with his eldest son Sony and family lawyer Tom Hagen. Then the movie transitions into the second scene, where the mafia family meets Virgil Sollozzo to hear his business proposition. Afterward, in the third scene, Don Vito chides his son Sonny for interfering in the middle of his talk. Generally, movies are composed of well-defined scenes, and detecting those scenes is essential to high-level movie understanding. Therefore, this work aims to detect high-level scenes from a long-range movie video. ers in NLP, subsequent works have shown the remarkable success of using transformers in several short-range vision tasks [8,16]. However, applying Transformers to long video sequences remains challenging due to the quadratic cost of self-attention. To overcome these issues, recent work proposed a structured state-space sequence (S4) operator
[18, 19, 33] for efficient long-range sequence analysis.
Motivated by the developments in transformer and S4-based architectures, we present TranS4mer, an efficient end-to-end model for movie scene boundary detection, which combines the strengths of transformers for short-range modeling and S4 operators for long-range modeling.
Specifically, we propose a novel state-space self-attention (S4A) block and build our TranS4mer model by stacking multiple S4A blocks on top of each other. Our model takes a sequence of frames divided into movie shots as input, where a shot is defined as a series of frames captured by the same camera over an uninterrupted period [46]. The S4A block first captures the short-range intra-shot dependencies by ap-plying self-attention over the frames in each shot. Subse-quently, it aggregates the inter-shot interactions over long movie segments by employing the state-space operator over all shots in a given movie video.
One key property of our model is that it independently applies self-attention to each shot, significantly reducing the cost of standard self-attention. For instance, if we have a video of 25 shots and each shot contains 3 frames, apply-ing a standard vision transformer [16] with a patch size of 32×32 will result in 3,675 tokens and a single self-attention operation will require ∼13.5M pairwise comparisons.
In contrast, applying self-attention within each shot parallelly reduces the number of comparisons 25× times. Simul-taneously, TranS4mer attains long-range modeling ability through the efficient state-space layers that do not require costly pairwise comparisons and operate over the entire in-put video. As a result, the proposed TranS4mer model can be efficiently applied to long movie videos.
We experiment with three movie scene detection datasets: MovieNet [25], BBC [3], and OVSD [40], and report that our method outperforms prior approaches by large margins (+3.38% AP, +4.66% and +7.36% re-spectively). TranS4mer is also 2× faster and requires 3× less GPU memory than the pure Transformer-based mod-els. Moreover, to evaluate the generalizability of our model, we experiment with several long-range video understand-ing tasks, such as movie clip classification and procedu-ral activity recognition. TranS4mer performs the best in 5 out of 7 movie clip classification tasks on LVU [48], the best in procedural activity classification on Breakfast [30], and the second-best in procedural activity classification on
COIN [43] datasets. 2.