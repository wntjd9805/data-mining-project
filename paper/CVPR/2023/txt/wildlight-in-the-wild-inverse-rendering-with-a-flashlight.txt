Abstract trial renderers. Our source code and data are released to https://github.com/za-cheng/WildLight.
This paper proposes a practical photometric solution for the challenging problem of in-the-wild inverse render-ing under unknown ambient lighting. Our system recov-ers scene geometry and reﬂectance using only multi-view images captured by a smartphone. The key idea is to ex-ploit smartphone’s built-in ﬂashlight as a minimally con-trolled light source, and decompose image intensities into two photometric components – a static appearance corre-sponds to ambient ﬂux, plus a dynamic reﬂection induced by the moving ﬂashlight. Our method does not require
ﬂash/non-ﬂash images to be captured in pairs. Building on the success of neural light ﬁelds, we use an off-the-shelf method to capture the ambient reﬂections, while the
ﬂashlight component enables physically accurate photomet-ric constraints to decouple reﬂectance and illumination.
Compared to existing inverse rendering methods, our setup is applicable to non-darkroom environments yet sidesteps the inherent difﬁculties of explicit solving ambient reﬂec-tions. We demonstrate by extensive experiments that our method is easy to implement, casual to set up, and con-sistently outperforms existing in-the-wild inverse rendering techniques. Finally, our neural reconstruction can be eas-ily exported to PBR textured triangle mesh ready for indus-1.

Introduction
Rendering in computer graphics refers to computer gen-erating photo-realistic images from known properties of a scene including scene geometry, materials, lighting, as well as camera parameters. In contrast, inverse rendering is re-garded as a computer vision task, whose aim is to recover these unknown properties of a scene from images. Due to the ill-posed nature of inverse rendering, most existing methods concede the fullest solution and instead tackle only a simpliﬁed, partial problem; for instance, assuming sim-pliﬁed, known lighting conditions, initial geometry, or with diffuse or low specular materials.
Traditional photometric methods for solving inverse ren-dering (such as photometric stereo) are often restricted to laboratory settings: image intensities are measured by high dynamic range cameras, under controlled illumination con-ditions and without ambient light contamination. Moreover, many methods rely on the availability of a good initial esti-mation to start the optimization process (e.g. [19,26]) or as-sume purely diffuse (Lambertian) reﬂections (e.g. [21, 31]).
While recent neural-net methods are able to handle specu-lar highlights and complex geometries, e.g. [2, 10, 20, 39]), they are still restricted to a laboratory darkroom environ-ment, hindering their practicality.
Conversely, neural rendering and appearance learning techniques have the ability to work outside darkroom. This is achieved by bypassing the physical reﬂection model.
They instead learn the illumination-speciﬁc appearance (i.e. light ﬁeld) of the scene conditioned on a geometric repre-sentation (e.g. [24, 27, 38]). While these representations are empirically powerful in recovering complex scene geome-try and appearance, they cannot separate reﬂective proper-ties from illumination. Furthermore, the underlying geome-try is provably ill-constrained. Attempts have been made to physically decompose the appearance into reﬂectance and environment illumination to support in-the-wild inverse ren-dering [5, 25, 33, 40, 43]. However, this presents a much more complex and similarly ill-posed problem due to un-known ambient illumination. Consequently, these methods still trail behind traditional photometric methods in terms of accuracy and robustness.
This paper aims to ﬁll the gap between conventional darkroom methods and in-the-wild inverse rending, and of-fer a solution that combines the best of both worlds, i.e. being practical, well-posed and easy-to-solve at the same time. Instead of attempting to directly decouple reﬂectance from the unknown ambient illumination, we learn the ambi-ent reﬂection with a neural light ﬁeld, and exploit an addi-tional, minimally controlled light source, being the smart-phone’s ﬂashlight, for physical constraints on reﬂectance.
During the image capture stage, we take some images with the ﬂashlight turned on, and others with the ﬂashlight off, all at free viewpoints. Images without the ﬂashlight describe the ambient reﬂections only, while the images with ﬂash-light are the photometric summation of both ambient and
ﬂashlight reﬂections. We learn the ambient component with an off-the-shelf neural novel-view-synthesis technique [36], and delegate the ﬂashlight component to a physically-based reﬂection model for inferring reﬂectance. Both ambient and
ﬂashlight components are conditioned on a uniﬁed scene in-trinsic network that predicts scene geometry and reﬂectance distribution. Our method is easy to set up, easy to imple-ment, and consistently outperforms competing state-of-the-arts. The reconstructed objects can be directly plugged into game/rendering engines as high ﬁdelity virtual assets in the standard format of textured meshes. 2.