Abstract
GAN inversion and editing via StyleGAN maps an in-put image into the embedding spaces (W, W +, and F) to simultaneously maintain image fidelity and meaningful manipulation. From latent space W to extended latent space W + to feature space F in StyleGAN, the editability of GAN inversion decreases while its reconstruction quality increases. Recent GAN inversion methods typically explore
W + and F rather than W to improve reconstruction fidelity while maintaining editability. As W + and F are derived from W that is essentially the foundation latent space of
StyleGAN, these GAN inversion methods focusing on W + and F spaces could be improved by stepping back to W. In this work, we propose to first obtain the proper latent code in foundation latent space W. We introduce contrastive learning to align W and the image space for proper la-tent code discovery. Then, we leverage a cross-attention en-coder to transform the obtained latent code in W into W + and F, accordingly. Our experiments show that our explo-*Y. Song and Q. Chen are the joint corresponding authors. ration of the foundation latent space W improves the repre-sentation ability of latent codes in W + and features in F, which yields state-of-the-art reconstruction fidelity and ed-itability results on the standard benchmarks. Project page: https://kumapowerliu.github.io/CLCAE. 1.

Introduction
StyleGAN [29–31] achieves numerous successes in im-age generation. Its semantically disentangled latent space enables attribute-based image editing where image content is modified based on the semantic attributes. GAN in-version [62] projects an input image into the latent space, which benefits a series of real image editing methods [4,36, 49, 65, 72]. The crucial part of GAN inversion is to find the inversion space to avoid distortion while enabling ed-itability. Prevalent inversion spaces include the latent space
W + [1] and the feature space F [28]. W + is shown to balance distortion and editability [56, 71]. It attracts many editing methods [1, 2, 5, 20, 25, 53] to map real images into this latent space. On the other hand, F contains spatial im-age representation and receives extensive studies from the image embedding [28, 48, 59, 63] or StyleGAN’s parame-ters [6, 14] perspectives.
The latent space W + and feature space F receive wide investigations. In contrast, Karras et al. [31] put into explor-ing W and the results are unsatisfying. This may be because that manipulation in W will easily bring content distortions during reconstruction [56], even though W is effective for editability. Nevertheless, we observe that W + and F are indeed developed from W, which is the foundation latent space in StyleGAN. In order to improve image editability while maintaining reconstruction fidelity (i.e., W + and F), exploring W is necessary. Our motivation is similar to the following quotation:
“You can’t build a great building on a weak foundation.
You must have a solid foundation if you’re going to have a strong superstructure.”
—Gordon B. Hinckley
In this paper, we propose a two-step design to improve the representation ability of the latent code in W + and F.
First, we obtain the proper latent code in W. Then, we use the latent code in W to guide the latent code in W + and F.
In the first step, we propose a contrastive learning paradigm to align the W and image space. This paradigm is derived from CLIP [51] where we switch the text branch with W.
Specifically, we construct the paired data that consists of one image I and its latent code w ∈ W with pre-trained
StyleGAN. During contrastive learning, we train two en-coders to obtain two feature representations of I and w, re-spectively. These two features are aligned after the train-ing process. During GAN inversion, we fix this contrastive learning module and regard it as a loss function. This loss function is set to make the one real image and its latent code w sufficiently close. This design improves existing stud-ies [31] on W that their loss functions are set on the image space (i.e., similarity measurement between an input image and its reconstructed image) rather than the unified image and latent space. The supervision on the image space only does not enforce well alignment between the input image and its latent code in W.
After discovering the proper latent code in W, we lever-age a cross-attention encoder to transform w into w+ ∈
W + and f ∈ F. When computing w+, we set w as the query and w+ as the value and key. Then, we calculate the cross-attention map to reconstruct w+. This cross-attention map enforces the value w+ close to the query w, which en-ables the editability of w+ to be similar to that of w. Be-sides, w+ is effective in preserving the reconstruction abil-ity. When computing f , we set the w as the value and key, while setting f as the query. So w will guide f for fea-ture refinement. Finally, we use w+ and f in StyleGAN to generate the reconstruction result.
We named our method CLCAE (i.e., StyleGAN in-version with Contrastive Learning and Cross-Attention
Encoder). We show that our CLCAE can achieve state-of-the-art performance in both reconstruction quality and editing capacity on benchmark datasets containing human portraits and cars. Fig. 1 shows some results. This indi-cates the robustness of our CLCAE. Our contributions are summarized as follows:
• We propose a novel contrastive learning approach to align the image space and foundation latent space W of StyleGAN. This alignment ensures that we can ob-tain proper latent code w during GAN inversion.
• We propose a cross-attention encoder to transform la-tent codes in W into W + and F. The representation of latent code in W + and feature in F are improved to benefit reconstruction fidelity and editability.
• Experiments indicate that our CLCAE achieves state-of-the-art fidelity and editability results both qualita-tively and quantitatively. 2.