Abstract
Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational dif-ferences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the rea-son why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all lay-ers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the exper-iments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained SwinV2-L could achieve state-of-the-art perfor-mance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on
NYUv2 and 1.966 RMSE on KITTI), and video object track-ing (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understand-ing of MIM, we hope that our work can inspire new and solid research in this direction. Code will be available at https:
//github.com/zdaxie/MIM-DarkSecrets. 1.

Introduction
Pre-training of effective and general representations ap-plicable to a wide range of tasks in a domain is the key to the success of deep learning. In computer vision, supervised
*Equal Contribution. The work is done when Zhenda Xie, Zigang Geng, and Jingcheng Hu are interns at Microsoft Research Asia. † Contact person. classification on ImageNet [14] has long been the dominant pre-training task which is manifested to be effective on a wide range of vision tasks, especially on the semantic un-derstanding tasks, such as image classification [17, 18, 36, 38, 51], object detection [23, 29, 61, 63], semantic segmenta-tion [53, 69], video action recognition [7, 52, 65, 67] and so on. Over the past several years, “masked signal modeling”, which masks a portion of input signals and tries to predict these masked signals, serves as a universal and effective self-supervised pre-training task for various domains, including language, vision, and speech. After (masked) language mod-eling repainted the NLP field [15, 49], recently, such task has also been shown to be a competitive challenger to the su-pervised pre-training in computer vision [3, 8, 18, 27, 74, 80].
That is, masked image modeling (MIM) pre-trained models achieve very high fine-tuning accuracy on a wide range of vision tasks of different nature and complexity.
However, there still remain several questions: 1. What are the key mechanisms that contribute to the excellent performance of MIM? 2. How transferable are MIM and supervised models across different types of tasks, such as semantic un-derstanding, geometric and motion tasks?
To investigate these questions, we compare MIM with su-pervised models from two perspectives, the visualization perspective and the experimental perspective, trying to un-cover key representational differences between these two pre-training tasks and deeper understand the behaviors of
MIM pre-training.
We start with studying the attention maps of the pre-trained models. Firstly, we visualize the averaged attention distance in MIM models, and we find that masked image modeling brings locality inductive bias to the trained model, that the models tend to aggregate near pixels in part of the attention heads, and the locality strength is highly correlated with the masking ratio and masked patch size in the pre-training stage. But the supervised models tend to focus locally at lower layers but more globally at higher layers.
We next probe how differently the attention heads in MIM trained Transformer behave. We find that different atten-tion heads tend to aggregate different tokens on all layers in MIM models, according to the large KL-divergence on attention maps of different heads. But for supervised models, the diversity on attention heads diminishes as the layer goes deeper and almost disappears in the last three layers. We drop the last several layers for supervised pre-trained models during fine-tuning and find that it benefits the fine-tuning per-formance on downstream tasks, however this phenomenon is not observed for MIM models. That is, less diversity on attention heads would somewhat harm the performance on downstream tasks.
Then we examine the representation structures in the deep networks of MIM and supervised models via the simi-larity metric of Centered Kernel Alignment (CKA) [37]. We surprisingly find that in MIM models, the feature represen-tations of different layers are of high similarity, that their
CKA values are all very large (e.g., [0.9, 1.0]). But for supervised models, as in [59], different layers learn different representation structures, that their CKA similarities vary greatly (e.g., [0.5,1.0]). To further verify this, we load the pre-trained weights of randomly shuffled layers during fine-tuning and find that supervised pre-trained models suffer more than the MIM models.
From the experimental perspective, a fundamental pre-training task should be able to benefit a wide range of tasks, or at least it is important to know for which types of tasks
MIM models work better than the supervised counterparts.
To this end, we conduct a large-scale study by comparing the fine-tuning performance of MIM and supervised pre-trained models, on three types of tasks, semantic understanding tasks, geometric and motion tasks, and the combined tasks which simultaneously perform both.
For semantic understanding tasks, we select several rep-resentative and diverse image classification benchmarks, in-cluding Concept Generalization (CoG) benchmark [62], the widely-used 12-dataset benchmark [38], as well as a fine-grained classification dataset iNaturalist-18 [68]. For the classification datasets whose categories are sufficiently cov-ered by ImageNet categories (e.g. CIFAR-10/100), super-vised models can achieve better performance than MIM models. However, for other datasets, such as fine-grained classification datasets (e.g., Food, Birdsnap, iNaturalist), or datasets with different output categories (e.g., CoG), most of the representation power in supervised models is diffi-cult to transfer, thus MIM models remarkably outperform supervised counterparts.
For geometric and motion tasks that require weaker se-mantics and high-resolution object localization capabilities, such as pose estimation on COCO [48] and CrowdPose [44], depth estimation on NYUv2 [64] and KITTI [22], and video object tracking on GOT10k [32], TrackingNet [55], and La-SOT [20], MIM models outperform supervised counterparts by large margins. Note that, without bells and whistles,
Swin-L with MIM pre-training could achieve state-of-the-art performance on these benchmarks, e.g., 80.5 AP on COCO val, 78.9 AP on COCO test-dev, and 78.0 AP on Crowd-Pose of pose estimation, 0.287 RMSE on NYUv2 and 1.966
RMSE on KITTI of depth estimation, and 70.7 SUC on
LaSOT of video object tracking.
We select object detection on COCO as the combined task which simultaneously performs both semantic understanding and geometric learning. For object detection on COCO,
MIM models would outperform supervised counterparts. Via investigating the training losses of object classification and localization, we find that MIM models help localization task converge faster, and supervised models benefit more for object classification, that categories of COCO are fully covered by ImageNet.
In general, MIM models tend to exhibit improved perfor-mance on geometric/motion tasks with weak semantics or fine-grained classification tasks compared to their supervised counterparts. For tasks/datasets where supervised models excel in transfer, MIM models can still achieve competitive transfer performance. Masked image modeling appears to be a promising candidate for a general-purpose pre-trained model. We hope our paper contributes to this understanding within the community and stimulates further research in this direction. 2.