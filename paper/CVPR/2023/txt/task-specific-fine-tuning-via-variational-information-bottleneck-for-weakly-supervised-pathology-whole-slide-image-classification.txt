Abstract
While Multiple Instance Learning (MIL) has shown promising results in digital Pathology Whole Slide Image (WSI) analysis, such a paradigm still faces performance and generalization problems due to high computational costs and limited supervision of Gigapixel WSIs. To deal with the computation problem, previous methods utilize a frozen model pretrained from ImageNet to obtain repre-sentations, however, it may lose key information owing to the large domain gap and hinder the generalization ability without image-level training-time augmentation. Though
Self-supervised Learning (SSL) proposes viable represen-tation learning schemes, the downstream task-specific fea-tures via partial label tuning are not explored. To alleviate this problem, we propose an efficient WSI fine-tuning frame-work motivated by the Information Bottleneck theory. The theory enables the framework to find the minimal sufficient statistics of WSI, thus supporting us to fine-tune the back-bone into a task-specific representation only depending on
WSI-level weak labels. The WSI-MIL problem is further analyzed to theoretically deduce our fine-tuning method.
We evaluate the method on five pathological WSI datasets on various WSI heads. The experimental results show sig-nificant improvements in both accuracy and generalization compared with previous works. Source code will be avail-able at https://github.com/invoker-LL/WSI-finetuning. 1.

Introduction
Digital Pathology or microscopic images have been widely used for the diagnosis of cancers such as Breast
Cancer [13] and Prostate Cancer [6]. However, the reading of Whole Slide Images (WSIs) with gigapixel resolution is
*Corresponding author.
Figure 1. T-SNE visualization of different representations on patches. Our method converts chaotic ImageNet-1K and SSL fea-tures into a more task-specific and separable distribution. The cluster evaluation measurement, v-scores, show weakly super-vised fine-tuned features are more close to full supervision com-pared to others. a. ImageNet-1k pretraining. b. Full patch supervi-sion. c. Self-supervised Learning. d. Fine-tuning with WSI labels. time-consuming which poses an urgent need for automatic computer-assisted diagnosis. Though computers can boost the speed of the diagnosis process, the enormous size of res-olution, over 100M [45], makes it infeasible to acquire pre-cise and exhaustive annotations for model training, let alone the current hardware can hardly support the parallel train-ing on all patches of a WSI. Hence, an annotation-efficient learning scheme with light computation is increasingly de-sirable to cope with those problems. In pathology WSI anal-ysis, the heavy annotation cost is usually alleviated by Mul-tiple Instance Learning (MIL) with only WSI-level weak supervision, which makes a comprehensive decision on a series of instances as a bag sample [19, 31]. Intuitively, all
small patches in the WSI are regarded as instances to con-stitute a bag sample [7, 30, 38], where the WSIâ€™s category corresponds to the max lesion level of all patch instances.
However, most methods pay much effort to design WSI architectures while overlooking the instance-level represen-tation ability. Because of the computational limitation, the gradient at the WSI-level is impossible to parallelly back-propagate to instance encoders with more than 10k in-stances of a bag. Thus parameters of the pretrained back-bone from ImageNet-1k (IN-1K) are frozen to obtain in-variant embeddings. Due to the large domain gap between
IN-1K and pathological images, some essential informa-tion may be discarded by layers of frozen convolutional fil-ters, which constrains the accuracy of previous WSI meth-ods. To address the constraint, recent works [9, 25] make efforts to learn a good feature representation at the patch-level by leveraging Self-supervised Learning (SSL). How-ever, such task-agnostic features are dominated by the proxy objective of SSL, e.g. Contrastive Learning in [8, 11, 17] may push away the distance between two instances within the same category, thus only performs slightly better than
IN-1K pretraining in WSI classification. Nearly all SSL methods [8, 11, 16, 17] proposed on natural image recogni-tion utilize a small portion of annotations to get promising fine-tuning accuracy compared to full supervision, which is higher than Linear Probing [17] by a large margin.
These findings illuminate us to design a fine-tuning scheme for WSI analysis to convert IN-K or SSL task-agnostic representations into task-specifics. Motivated by the Information Bottleneck (IB) theory [1, 2], we argue that pretraining is limited to downstream tasks, therefore fine-tuning is necessary for WSI analysis.
In addition, we develop a solution based on Variational IB to tackle the dilemma of fine-tuning and computational limitation by its minimal sufficient statistics and attribution proper-ties [1, 23]. The differences among the above three feature representations are depicted in Figure 1, where the feature representation under full patch-level supervision is consid-ered as the upper bound.
Our main contributions are in 3 folds: 1) We propose a simple agent task of WSI-MIL by introducing an IB module that distills over 10k redundant instances within a bag into less than 1k of the most supported instances. Thus the paral-lel computation cost of gradient-based training on Gigapixel
Images is over ten times relieved. By learning and mak-ing classification on the simplified bag, we find that there are trivial information losses due to the low-rank property of pathological WSI, and the distilled bag makes it possi-ble to train a WSI-MIL model with the feature extractor on patches end-to-end, thus boosting the final performance. 2)
We argue that the performance can be further improved by combining with the SSL pretraining since we could convert the task-agnostic representation from SSL into task-specific one by well-designed fine-tuning. The proposed framework only relies on annotations at WSI levels, which is similar to recent SSL approaches [8, 11, 16, 17]. Note that our method only utilizes less than a 1% fraction of full patch annotation to achieve competitive accuracy compared to counterparts. 3) Versatile training-time augmentations can be incorpo-rated with our proposed fine-tuning scheme, thus resulting in better generalization in various real-world or simulated datasets with domain shift, which previous works ignore to validate. These empirical results show that our method advances accuracy and generalization simultaneously, and thus would be more practical for real-world applications. 2.