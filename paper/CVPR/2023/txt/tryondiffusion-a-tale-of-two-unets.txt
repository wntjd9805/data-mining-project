Abstract
Given two images depicting a person and a garment worn by another person, our goal is to generate a visu-alization of how the garment might look on the input per-son. A key challenge is to synthesize a photorealistic detail-preserving visualization of the garment, while warping the garment to accommodate a significant body pose and shape change across the subjects. Previous methods either fo-cus on garment detail preservation without effective pose 1Work done while author was an intern at Google. and shape variation, or allow try-on with the desired shape and pose but lack garment details. In this paper, we pro-pose a diffusion-based architecture that unifies two UNets (referred to as Parallel-UNet), which allows us to preserve garment details and warp the garment for significant pose and body change in a single network. The key ideas behind
Parallel-UNet include: 1) garment is warped implicitly via a cross attention mechanism, 2) garment warp and person blend happen as part of a unified process as opposed to a se-quence of two separate tasks. Experimental results indicate that TryOnDiffusion achieves state-of-the-art performance
both qualitatively and quantitatively. 1.

Introduction
Virtual apparel try-on aims to visualize how a garment might look on a person based on an image of the person and an image of the garment. Virtual try-on has the potential to enhance the online shopping experience, but most try-on methods only perform well when body pose and shape vari-ation is small. A key open problem is the non-rigid warping of a garment to fit a target body shape, while not introducing distortions in garment patterns and texture [5, 12, 41].
When pose or body shape vary significantly, garments need to warp in a way that wrinkles are created or flat-tened according to the new shape or occlusions.