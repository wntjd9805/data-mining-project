Abstract
Despite advances in global image representation, exist-ing image retrieval approaches rarely consider geometric structure during the global retrieval stage.
In this work, we revisit the conventional self-similarity descriptor from a convolutional perspective, to encode both the visual and structural cues of the image to global image representation.
Our proposed network, named Structural Embedding Net-work (SENet), captures the internal structure of the images and gradually compresses them into dense self-similarity descriptors while learning diverse structures from various images. These self-similarity descriptors and original im-age features are fused and then pooled into global embed-ding, so that global embedding can represent both geomet-ric and visual cues of the image. Along with this novel structural embedding, our proposed network sets new state-of-the-art performances on several image retrieval bench-marks, convincing its robustness to look-alike distractors.
The code and models are available: https://github. com/sungonce/SENet. 1.

Introduction
Content-based image retrieval is the task of searching for images with the same content present in the query im-age in the large-scale database. What across images rep-resents the same content are two things: the visual prop-erties and the geometrical structure, so comparing them well is the key to the image retrieval task. To achieve this goal, two image representation types have been exten-sively explored in many image retrieval solutions. The first one is local features [1–3, 5, 19–22, 24, 47] that comprise visual descriptors and spatial information about local re-gions of the image, and the other one is a global descriptor
[3,11–13,18,24–26,28,33,43], also known as global embed-ding, that summarizes the local features of the entire image.
In a general sense, the global descriptor loses spatial infor-mation of local features during the summarization process.
Thus, many image retrieval solutions [3, 18, 24, 35, 36, 42] first retrieve coarse candidates with similar visual proper-*Corresponding author.
Figure 1. Images of the same content share both similar image properties and internal self-similarities. Our proposed networks leverage both visual features and self-similarity features and en-code them to global embedding in an end-to-end manner. ties for the query using global embeddings (typically re-ferred to as global retrieval) and further verify that coarse candidates have geometrically similar shapes to the query using local features (typically referred to as local feature re-ranking). This separation of tasks may sound reasonable at first glance. However, in fact, they miss the opportunities to perform robust retrieval by comparing structural informa-tion also in the global retrieval stage.
In computer vision, a self-similarity descriptor [31] has long been used as a regional descriptor for matching images based on the aggregation of local internal structures. This work has shown its effectiveness in challenging matching problems even in situations where the visual properties of images are not shared at all (e.g. matching between draw-ing and photo domains). However, their self-similarity en-coding process is neither learnable nor differentiable. And it also completely ignores visual properties, making it dif-ficult to use directly for image retrieval tasks where visual properties are also valuable cues.
In this paper, we revisit the self-similarity descriptor in a convolutional manner and propose a novel global em-bedding network named Structural Embedding Network (SENet). The proposed network captures the internal struc-tures of the images and encodes them to self-similarity de-scriptors while learning diverse structures from various im-ages. These self-similarity descriptors and original image features are fused and then pooled into global embedding, so that global embedding can represent both valuable geo-metric and visual cues of the image. All proposed modules of our networks are comprised of point-wise operations, en-abling efficient descriptor encoding. Our proposed network sets state-of-the-art performance on several image retrieval benchmarks, convincing its robustness to look-alike distrac-tors. 2.