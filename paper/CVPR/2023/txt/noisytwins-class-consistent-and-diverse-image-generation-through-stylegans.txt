Abstract
StyleGANs are at the forefront of controllable image generation as they produce a latent space that is seman-tically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the W latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the W space. This decorrelation miti-gates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation.
We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNatu-ralist 2019, where our method outperforms other methods by ∼ 19% on FID, establishing a new state-of-the-art. 1.

Introduction
StyleGANs [21, 22] have shown unprecedented success in image generation, particularly on well-curated and artic-ulated datasets (eg. FFHQ for face images, etc.). In addition to generating high fidelity and diverse images, StyleGANs also produce a disentangled latent space, which is exten-sively used for image editing and manipulation tasks [49].
As a result, StyleGANs are being extensively used in var-ious applications like face-editing [11, 41], video genera-tion [46, 52], face reenactment [3], etc., which are a tes-tament to their usability and generality. However, de-spite being successful on well-curated datasets, training
StyleGANs on in-the-wild and multi-category datasets is still challenging. A large-scale conditional StyleGAN (i.e.
StyleGAN-XL) on ImageNet was recently trained success-fully by Sauer et al. [39] using the ImageNet pre-trained model through the idea of a projection discriminator [38].
While the StyleGAN-XL uses additional pre-trained mod-*Equal Contribution. Link: rangwani-harsh.github.io/NoisyTwins
Figure 1. Qualitative Comparison on tail classes (T1-T4) for iNaturalist 2019. We provide sample(s) from real class (with class frequency), generated by StyleGAN2-ADA and after adding proposed NoisyTwins. NoisyTwins achieves remarkable diversity, class-consistency and quality by just using 38 samples on average. els, obtaining such models for distinctive image domains like medical, forensics, and fine-grained data may not be feasible, which limits its generalization across domains.
In this work, we aim to train vanilla class-conditional
StyleGAN without any pre-trained models on challenging real-world long-tailed data distributions. As training Style-GAN with augmentations [20, 54] leads to low recall [24] (which measures diversity in the generated images) and mode collapse, particularly for minority (i.e. tail) classes.
For investigating this phenomenon further, we take a closer look at the latent W space of StyleGAN that is produced by a fully-connected mapping network that takes the condi-tioning variables z (i.e. random noise) and class embedding c as inputs. The vectors w in W space are used for con-ditioning various layers of the generator (Fig. 2). We find that output vectors w from the mapping network hinge on the conditioning variable c and become invariant to random conditioning vector z. This collapse of latents leads to un-Figure 2. Schematic illustration of W space for different GANs. Existing conditioning methods either suffer from mode collapse [20] or lead to class confusion [35] in W space. With proposed NoisyTwins, we achieve intra class diversity while avoiding class confusion. stable training and is one of the causes of poor recall (a.k.a. mode collapse) for minority classes. Further, on augment-ing StyleGAN with recent conditioning and regularization techniques [17, 35], we find that they either lead to a poor recall for minority classes or lead to class confusion (Fig. 2) instead of mitigating the collapse.
To mitigate the collapse of w in W space, we need to ensure that the change in conditioning variable z leads to the corresponding change in w. Recently in self-supervised learning, several techniques [2, 53] have been introduced to prevent the collapse of learned representations by maxi-mizing the information content in the feature dimensions.
Inspired by them we propose NoisyTwins, in which we first generate inexpensive twin augmentations for class em-beddings and then use them to decorrelate the w variables through self-supervision. The decorrelation ensures that w vectors are diverse for each class and the GAN is able to produce intra-class diversity among the generated images.
We evaluate NoisyTwins on challenging benchmarks of large-scale long-tailed datasets of ImageNet-LT [30] and iNaturalist 2019 [48]. These benchmarks are particularly challenging due to a large number of classes present, which makes GANs prone to class confusion. On the other hand, as these datasets are long-tailed with only a few images per class in tail classes, generating diverse images for those classes is challenging. We observe that existing metrics used in GAN evaluations are not able to capture both class confusion and mode collapse. As a remedy, we propose to use intra-class Frechet Inception Distance (FID) [12] based on features obtained from pre-trained CLIP [34] embed-dings as an effective metric to measure the performance of class-conditional GANs in long-tailed data setups. Us-ing NoisyTwins enables StyleGAN to generate diverse and class-consistent images across classes, mitigating the mode collapse and class confusion issues in existing state-of-the-art (SotA) (Fig. 1). Further, with NoisyTwins, we obtain diverse generations for tail classes even with ≤ 30 images, which can be attributed to the transfer of knowledge from head classes through shared parameters (Fig. 1 and 6). In summary, we make the following contributions: 1. We evaluate various recent SotA GAN conditioning and regularization techniques on the challenging task of long-tailed image generation. We find that all exist-ing methods either suffer from mode collapse or lead to class confusion in generations. 2. To mitigate mode collapse and class confusion, we in-troduce NoisyTwins, an effective and inexpensive aug-mentation strategy for class embeddings that decorre-lates latents in the W latent space (Sec. 4). 3. We evaluate NoisyTwins on large-scale long-tailed datasets of ImageNet-LT and iNaturalist-2019, where it consistently improves the StyleGAN2 performance (∼ 19%), achieving a new SotA. Further, our approach can also prevent mode collapse and enhance the perfor-mance of few-shot GANs (Sec. 5.3). 2.