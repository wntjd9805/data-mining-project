Abstract
Deep learning models are challenged by the distribu-tion shift between the training data and test data. Re-cently, the large models pre-trained on diverse data have demonstrated unprecedented robustness to various distri-bution shifts. However, fine-tuning these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD ro-bustness problem. In this paper, based on causal analysis of the aforementioned problems, we propose a novel fine-tuning method, which uses masked images as counterfactual samples that help improve the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the pre-trained model. Extensive experi-ments verify that regularizing the fine-tuning with the pro-posed masked images can achieve a better trade-off between
ID and OOD performance, surpassing previous methods on the OOD performance. Our code is available at https:
//github.com/Coxy7/robust-finetuning. 1.

Introduction
Deep learning has made impressive advances in various tasks on computer vision. Despite the remarkable perfor-mance achieved on benchmark datasets, deep models are often challenged by the distribution shift between the train-ing data and test data [5,15,16,33]. It is commonly assumed that the training and test samples follow the same distribu-tion, which may not hold in real-world applications due to the unpredictable change of lighting conditions, viewpoints, backgrounds, etc. Although there are attempts to improve the robustness of deep models to the distribution shift (or
OOD robustness), it is still rather under-explored [29, 38].
*Corresponding author.
Figure 1. Illustration of our work. Vanilla fine-tuned models tend to learn spurious correlations that degrade the OOD robustness. To tackle this issue, our model learns from the pre-trained model on the counterfactual CAM-based masked images.
Recently, large-scale models pre-trained on diverse data have demonstrated unprecedented robustness to various dis-tribution shifts [8, 19, 32]. Fine-tuning these pre-trained models on downstream tasks can be a promising approach to building robust models for different applications. How-ever, it is found that while fine-tuning improves the per-formance on in-distribution (ID) data, it may reduce that on out-of-distribution (OOD) data [23, 43]. To tackle this trade-off, several methods [23, 42, 43] have been proposed to improve both ID and OOD performance in fine-tuning.
However, they do not explicitly address the OOD robust-ness problem; instead, they implicitly preserve the robust-ness of the pre-trained model by constraining the distortion of pre-trained weights or using model ensembles.
In this paper, we revisit the issue of robustness degrada-tion in fine-tuning from a causal perspective. A large-scale pre-trained model somewhat shows properties in causality and stays robust to OOD samples [41]. However, when fine-tuning on downstream tasks, a majority of the model parameters tend to be adjusted for the downstream task in fine-tuning due to the highly entangled representation of
images, arguably destructive to the generalizable knowl-edge [20, 35].
In contrast, distribution shifts are usually sparse in the underlying causal factorization of the data gen-eration process [7, 35]. In this low-dimensional case, if we know which variables vary with different data distributions in this factorization (i.e., the non-stationary factors), we can achieve the OOD robustness by simply excluding their in-fluence on the final predictions of the model.
Specifically, we consider a Structural Causal Model (SCM) [31] for the object-centric image generation process, as depicted in Fig. 2. In this SCM, images are generated according to a non-stationary domain-relevant factor and a stationary semantic factor. Between them is a spurious correlation caused by a hidden non-stationary confounder that influences how the domain-relevant factor changes with the semantic one. To retain the OOD robustness, a fine-tuning model should avoid mapping non-stationary domain-relevant features to the predicted semantics.
To this end, we propose to fine-tune the models with masked images, which serve as counterfactual samples breaking the spurious correlation. Training on these sam-ples helps preserve the stationary and generalizable knowl-edge of the pre-trained model. Concretely, we either mask the patches that contribute most to the label (i.e., the main object) or mask those with the least contribution (e.g., the context), which can be implemented based on class activa-tion map (CAM) [9,46]. Such image masking forms an ma-nipulation of a factual image and produces a counterfactual sample. Since the pre-trained model can better disentan-gle invariant features across domains, we require the fine-tuning model to learn from the pre-trained model on these counterfactual samples, as illustrated in Fig. 1. Further-more, we argue that simply dropping the masked patches may be insufficient to alleviate the risk of fitting spurious correlations, and we propose to refill the masked patches with those from other images.
We study different combinations of masking strategies (e.g., masking the object or the context) and refilling strate-gies (e.g., filling with patches from single or multiple im-ages). Experimental results suggest that most of the strate-gies are applicable to the construction of counterfactual samples that help improve the robustness in fine-tuning, while masking the object generally achieves the best ro-bustness. Compared with existing methods [23, 42, 43] on fine-tuning CLIP [32] models, our approach achieves bet-ter average accuracy on various OOD datasets without re-lying on model ensembles or weight constraints. We also find that, taking the weight-space ensemble of the zero-shot model and our fine-tuned model following WiSE-FT [43], hardly improves the trade-off between ID and OOD accu-racy, which contradicts previous observations and implies that our approach may produce essentially different models in comparison with conventional fine-tuning. 2.