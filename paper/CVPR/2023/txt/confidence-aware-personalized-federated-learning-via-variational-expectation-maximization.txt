Abstract
Federated Learning (FL) is a distributed learning scheme to train a shared model across clients. One com-mon and fundamental challenge in FL is that the sets of data across clients could be non-identically distributed and have different sizes. Personalized Federated Learning (PFL) at-tempts to solve this challenge via locally adapted models.
In this work, we present a novel framework for PFL based on hierarchical Bayesian modeling and variational infer-ence. A global model is introduced as a latent variable to augment the joint distribution of clients’ parameters and capture the common trends of different clients, optimiza-tion is derived based on the principle of maximizing the marginal likelihood and conducted using variational expec-tation maximization. Our algorithm gives rise to a closed-form estimation of a confidence value which comprises the uncertainty of clients’ parameters and local model devia-tions from the global model. The confidence value is used to weigh clients’ parameters in the aggregation stage and ad-just the regularization effect of the global model. We evalu-ate our method through extensive empirical studies on mul-tiple datasets. Experimental results show that our approach obtains competitive results under mild heterogeneous cir-cumstances while significantly outperforming state-of-the-art PFL frameworks in highly heterogeneous settings. 1.

Introduction
Federated learning (FL) is a distributed learning frame-work, in which clients optimize a shared model with their local data and send back parameters after training, and a central server aggregates locally updated models to obtain a global model that it re-distributes to clients [24]. FL is expected to address privacy concerns and to exploit the computational resources of a large number of edge devices.
Despite these strengths, there are several challenges in the
*Authors with equal contribution.
†Work was done at KU Leuven prior to joining Amazon. application of FL. One of them is the statistical hetero-geneity of client data sets since in practice clients’ data correlate with local environments and deviate from each other [13,18,19]. The most common types of heterogeneity are defined as:
Label distribution skew. Let J be the number of clients and the data distribution of client j be Pj(x, y) and rewrite it as Pj(x|y)Pj(y), two kinds of non-identical scenarios can be identified. One of them is label distribution skew, that is, the label distributions {Pj(y)}J j=1 are varying in different clients but the conditional generating distributions
{Pj(x|y)}J j=1 are assumed to be the same. This could hap-pen when certain types of data are underrepresented in the local environment.
Label concept drift. Another common type of non-IID scenario is label concept drift, in which the label distribu-tions {Pj(y)}J j=1 are the same but the conditional generat-ing distributions {Pj(x|y)}J j=1 are different across different clients. This could happen when features of the same type of data differ across clients and correlates with their envi-ronments, e.g. the Labrador Retriever (most popular dog in the United States) and the Border Collie (most popular dog in Europe) look different, thus the dog pictures taken by the clients in these two areas contain label concept drift.
Data quantity disparity. Additionally, clients may pos-sess different amounts of data. Such data quantity disparity can lead to inconsistent uncertainties of the locally updated models and heterogeneity in the number of local updates. In practice, the amount of data could span a large range across clients, for example large hospitals usually have many more medical records than clinics. In particular, data quantity dis-tributions often exhibit that large datasets are concentrated in a few locations, whereas a large amount of data is scat-tered across many locations with small dataset sizes [11,32].
It has been proven that if federated averaging (FedAvg
[24]) is applied, the aforementioned heterogeneity will slow down the convergence of the global model and in some cases leads to arbitrary deviation from the optimum [19,33].
Several works have been proposed to alleviate this prob-lem [4, 18, 33]. Another stream of work is personalized fed-tional data distribution from the same label in label distri-bution skew is the same across different clients.
In label concept drift there can be high discrepancy between local data distributions, indicating higher heterogeneity.
Paper organization. Necessary background and notation that will be used in this paper are given in the next section.
Section 3 presents the theoretical analysis of the proposed framework and the implementation of our algorithm is pro-vided in Section 4. Experimental results are presented in
Section 5.