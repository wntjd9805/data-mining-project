Abstract
Structural pruning enables model acceleration by re-moving structurally-grouped parameters from neural net-works. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures.
In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimpor-tant, thereby avoiding structural issues and significant per-formance degradation after pruning. To address this prob-lem, we propose a general and fully automatic method, De-pendency Graph (DepGraph), to explicitly model the depen-dency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evalu-ate our method on several architectures and tasks, including
ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances. 1.

Introduction
The recent emergence of edge computing applications calls for the necessity for deep neural compression [16, 22, 25,33,34,61,65–67,69,75]. Among the many network com-pression paradigms, pruning has proven itself to be highly effective and practical [7, 11, 30, 31, 44, 58, 59, 74]. The goal of network pruning is to remove redundant parame-ters from a given network to lighten its size and potentially speed up the inference. Mainstream pruning approaches can
*Corresponding author
Figure 1. Parameters from different layers are inherently depen-dent on each other across network architectures, which forces sev-eral layers to be pruned simultaneously. For instance, to prune the Conv2 in (a), all other layers {Conv1, BN1, BN2} within the block must be pruned as well. In this work, we introduce a generic scheme, termed as Dependency Graph, to explicitly account for such dependencies and execute the pruning of arbitrary architec-ture in a fully automatic manner. be roughly categorized into two schemes, structurual prun-ing [4, 29, 71] and unstructurual pruning [8, 13, 44]. The core difference between the two lies in that, structural prun-ing changes the structure of neural networks by physically removing grouped parameters, while unstructural pruning conducts zeroing on partial weights without modification to the network structure. Compared to unstructural ones, structural pruning does not rely on specific AI accelerators or software to reduce memory consumption and computa-tional costs, thereby finding a wider domain of applications in practice [38, 68].
Nevertheless, the nature of structural pruning per se makes itself a challenging task, especially for modern deep neural networks with coupled and complex internal struc-tures. The rationale lies in that, deep neural networks are built upon a large number of basic modules like convolu-tion, normalization, or activation, yet these modules, either parameterized or not, are intrinsically coupled through the intricate connections [17, 23]. As a result, even when we seek to remove only one channel from a CNN illustrated in
Figure 1(a), we have to take care of its inter-dependencies to all layers simultaneously, otherwise we will eventually get a broken network. To be exact, the residual connection requires the output of two convolutional layers to share the same number of channels and thus forces them to be pruned together [20, 40, 71]. The same goes for structural pruning on other architectures like Transformers, RNNs and GNNs as illustrated in Figs. 1(b-d).
Unfortunately, dependency does not only emerge in residual structures, which can be infinitely complex in mod-ern models [23, 46]. Existing structural approaches have largely relied on case-by-case analyses to handle depen-dencies in networks [29, 40]. Despite the promising re-sults achieved, such a network-specific pruning approach is effort-consuming. Moreover, these methods are not directly generalizable, meaning that the manually-designed group-ing scheme is not transferable to other network families or even the network architectures in the same family, which in turn, greatly limit their industrial applications.
In this paper, we strive for a generic scheme towards any structural pruning, where structural pruning over arbitrary network architectures is executed in an automatic fashion,
At the heart of our approach is to estimate the Dependency
Graph (DepGraph), which explicitly models the interdepen-dency between paired layers in neural networks. Our moti-vation to introduce DepGraph for structural pruning stems from the observation that, structural pruning at one layer ef-fectively “triggers” pruning at adjacent layers, which further leads to a chain effect like {BN2←Conv2→BN1→Conv1} as shown in Figure 1(a). As such, to trace the dependen-cies across different layers, we may decompose and model the dependency chain as a recursive process, which natu-rally boils down to the problem of finding the maximum connected components in the graph, and can be solved in
O(N ) complexity via graph traversal.
It is also worth noting that in structural pruning, grouped layers are pruned simultaneously, which expects all re-moved parameters in the same group to be consistently unimportant. This brings certain difficulties to existing im-portance criteria designed for a single layer [20, 27, 29, 42].
To be exact, the parameter importance in a single layer no longer reveals correct importance due to the entangle-ment with other parameterized layers. To address this prob-lem, we fully leverage the comprehensive ability of depen-dency modeling powered by DepGraph to design a “group-level” importance criterion, which learns consistent sparsity within groups, so that those zeroized groups can be safely removed without too much performance degradation.
To validate the effectiveness of DepGraph, we apply the proposed method to several popular architectures includ-ing CNNs [23, 40], Transformers [10], RNNs [12, 53], and
GNNs [55], where competitive performance is achieved compared to state-of-the-art methods [7, 32, 58, 71]. For
CNN pruning, our method obtains a 2.57× accelerated
ResNet-56 model with 93.64% accuracy on CIFAR, which is even superior to the unpruned model with 93.53% ac-curacy. And on ImageNet-1k, our algorithm achieves more than 2× speed-up on ResNet-50, with only 0.32% performance lost. More importantly, our method can be readily transferred to various popular networks, includ-ing ResNe(X)t [40, 63], DenseNet [23], VGG [51], Mo-bileNet [48], GoogleNet [54] and Vision Transformer [10], and demonstrate gratifying results. Besides, we also con-duct further experiments on non-image neural networks, in-cluding LSTM [12] for text classification, DGCNN [60] for 3D point cloud, and GAT [55] for graph data, where our method achieves from 8× to 16× acceleration without a significant performance drop.
In sum, our contribution is a generic pruning scheme to-wards any structural pruning, termed as Dependency Graph (DepGraph), which allows for automatic parameter group-ing and effectively improves the generalizability of struc-tural pruning over various network architectures, including
CNNs, RNNs, GNNs and Vision Transformers. 2.