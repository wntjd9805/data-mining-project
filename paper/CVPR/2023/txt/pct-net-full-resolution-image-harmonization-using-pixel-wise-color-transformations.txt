Abstract
In this paper, we present PCT-Net, a simple and general image harmonization method that can be easily applied to images at full-resolution. The key idea is to learn a param-eter network that uses downsampled input images to pre-dict the parameters for pixel-wise color transforms (PCTs) which are applied to each pixel in the full-resolution image.
We show that affine color transforms are both efficient and effective, resulting in state-of-the-art harmonization results.
Moreover, we explore both CNNs and Transformers as the parameter network, and show that Transformers lead to bet-ter results. We evaluate the proposed method on the public full-resolution iHarmony4 dataset, which is comprised of four datasets, and show a reduction of the foreground MSE (fMSE) and MSE values by more than 20% and an increase of the PSNR value by 1.4dB, while keeping the architecture light-weight. In a user study with 20 people, we show that the method achieves a higher B-T score than two other re-cent methods. 1.

Introduction
Cutting and pasting parts of an image into another im-age is an important editing task, also referred to as image compositing. However, creating a composite image by sim-ply adding a foreground region to a different image will typically produce unrealistic results due to different condi-tions at the time the images were taken. In order to reduce this discrepancy between foreground and background, im-age harmonization aims to align the colors by modifying the foreground region.
A variety of approaches have been proposed to solve this task using traditional statistical techniques as well as deep learning methods. Nonetheless, most of the research
[5,12–15,17,23,28,30] has solely focused on low-resolution images (256 × 256 pixels), whereas high resolution images
*work conducted during an internship at Rakuten Group, Inc. have become in fact the standard for most real use cases.
Since most approaches are built on convolutional neural networks (CNN), they would theoretically be able to pro-cess images of any size. However, due to poor scaling, the computational cost required for high resolution images ren-der them effectively impractical.
More recently, some methods have started exploring high-resolution image harmonization [6,18,22,34] by lever-aging a network that takes a low-resolution image as its in-put, but instead of predicting the final image, further pro-cesses the image according to the output of the network.
While this allows us to apply high resolution image harmo-nization based on a low-resolution input, current models are either simplifying the problem for the sake of efficiency or employ a series of complex operations to improve perfor-mance. Following the general dual branch approach, we propose a light-weight model capable of harmonizing im-ages at high resolutions. As shown in Fig. 1, our method achieves significant improvements in terms of foreground-normalized MSE (fMSE), but only requires roughly the same or less parameters, depending on the backbone.
We are able to outperform state-of-the-art methods on full resolution image harmonization by interpolating the network output in parameter space instead of introducing interpolation errors in the image space. Following the rea-soning by Xue et al. [34], we argue that the parameter space contains less high frequency components which cause higher interpolation errors during upsampling. In contrast to [34], we greatly reduce the complexity by introducing pixel-wise color transformations (PCT) and find that a sim-ple affine transformation is sufficient to achieve significant improvements. We show that this idea can easily be applied to both, CNN-based and Transformer-based models, while outperforming current state-of-the-art models in terms of re-construction error.
In our approach, the backbone network predicts a set of parameters for each input pixel. Since this is done in low resolution, we interpolate the parameter map to match the full resolution of the original composite image. We then
view [16, 25, 31, 32] in order to seamlessly blend a fore-ground object with a background image, or from a statisti-cal perspective by considering the different foreground and background color distributions [4, 26, 27, 35]. However, these approaches are limited by the information contained within a single image. To take advantage of the informa-tion in multiple images, prior work has used statistics from image sets in their approaches [21, 39].
Current methods are mostly based on neural networks, making use of an encoder-decoder CNN architecture based on U-Net [29], optionally enhanced by attention blocks [8], such as iSSAM [30]. While Cong et al. [5, 7] framed the problem as a domain gap task between foreground and background, Ling et al. [23] considered image harmoniza-tion as reducing the style discrepancy between inharmo-nious regions. Guo et al. [12–14] proposed a lighting-based approach that separately harmonizes an image after decomposing it into its reflection and illumination compo-nents. One of the methods employed a Transformer-based architecture, which improves the performance over archi-tectures using CNNs. Based on the revision of architec-tures presented in [8, 33], Sofiiuk et al. [30] introduced se-mantic information by adding pre-trained HR-Net [36] fea-tures that are further processed by an encoder-decoder net-work. Unlike previous methods, Jiang et al. [17] proposed a framework that does not require any labeled training data at the expense of overall performance compared to supervised training. Hang et al. [15] introduced contrastive learning for image harmonization by combining two different con-trastive regularization losses that can be used on top of any existing network. The above methods exclusively focus on low-resolution images, such as 256×256 pixels. While con-volutional neural networks can be applied to larger images regardless of the training image size, the steep increase in computational cost renders existing approaches unsuitable for high resolution tasks.
To handle higher resolution images, most image har-monization approaches downscale the input image, e.g., to 256 × 256 pixels. Instead of predicting new pixel values, a different output is regressed by a network and is subse-quently applied to the full resolution image. For example,
Liang et al. [22] trained a network to produce a color curve based on a low-resolution image that is then applied to the full-resolution foreground image. Similarly, Ke et al. [18] proposed a model named Harmonizer, which uses an en-coder network on a low-resolution image to regress coeffi-cients controlling various filter operations, such as bright-ness and contrast. While these two light-weight approaches are easily adaptable to high resolutions, they are not able to account for local differences across the foreground re-gions compared to pixel-to-pixel transformations. Another recent approach combined pixel-to-pixel as well as RGB-to-RGB transformations using two separate branches [6].
Figure 1. Model size vs. performance (fMSE score) compari-son. Even though our model size is smaller than others, our pro-posed models achieve better performance on full resolution im-ages than prior work. The performance is calculated using the iHarmony4 dataset [7]. apply the same, pre-determined PCT function to each pixel according to the predicted parameters. In order to find suit-able parameters that represent an appropriate color trans-formation, the backbone network needs to consider spatial and semantic information within the image. When applying the PCT function, we change each pixel solely based on its value and the parameters predicted for that pixel position.
Our contributions can be summarized as follows:
• We introduce PCT-Net, an architecture based on a dual branch approach that is able to handle high-resolution images. It processes images at full resolution through a pixel-wise color transformation (PCT).
• To the best of our knowledge, we are the first to use a
Transformer-based architecture for training and testing at full resolution. We further propose a novel training strategy for image harmonization where we do not re-size the images, but instead evaluate the loss function on the full-resolution images.
• We demonstrate significant improvements in quantita-tive and qualitative performance compared to existing approaches, while retaining a light-weight and simple network architecture. 2.