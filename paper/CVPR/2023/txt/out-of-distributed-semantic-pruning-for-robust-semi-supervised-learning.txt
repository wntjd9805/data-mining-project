Abstract (a)
Predict: Beetles
Recent advances in robust semi-supervised learning (SSL) typically ﬁlter out-of-distribution (OOD) information at the sample level. We argue that an overlooked problem of robust SSL is its corrupted information on semantic level, practically limiting the development of the ﬁeld. In this pa-per, we take an initial step to explore and propose a uniﬁed framework termed OOD Semantic Pruning (OSP), which aims at pruning OOD semantics out from in-distribution (ID) features. Speciﬁcally, (i) we propose an aliasing OOD matching module to pair each ID sample with an OOD sam-ple with semantic overlap. (ii) We design a soft orthogonal-ity regularization, which ﬁrst transforms each ID feature by suppressing its semantic component that is collinear with paired OOD sample. It then forces the predictions before and after soft orthogonality decomposition to be consistent.
Being practically simple, our method shows a strong per-formance in OOD detection and ID classiﬁcation on chal-lenging benchmarks. In particular, OSP surpasses the pre-vious state-of-the-art by 13.7% on accuracy for ID classiﬁ-cation and 5.9% on AUROC for OOD detection on TinyIm-ageNet dataset. The source codes are publicly available at https://github.com/rain305f/OSP. 1.

Introduction
Deep neural networks have obtained impressive per-formance on various tasks [30, 44, 46]. Their success is partially dependent on a large amount of labeled train-ing data, of which the acquisition is expensive and time-consuming [20, 25, 40, 45]. A prevailing way to reduce the dependency on human annotation is semi-supervised learn-ing (SSL). It learns informative semantics using annotation-*Equal contribution.
†Corresponding author.
OOD Semantic Pruning
Pruned ID 
Feature s r e a t u r e u t a e e o r I D   F
I D   F h c n
A
Predict: Butterfly
OOD Semantic
OOD Samples
ID Samples
OOD Semantic
OOD Semantic Pruning (b) (c)
Figure 1. (a) Intuitive diagram of OOD Semantic Pruning (OSP), that pruning OOD semantics out from ID features. (b) t-SNE vi-sualization [52] from the baseline [23]. (c) t-SNE visualization from our OSP model. The colorful dots donate ID features, while the black dots mark OOD features. The dots with the same color represent the features of the same class. Here, our OSP and the baseline are trained on CIFAR100 with 100 labeled data per class and 60% OOD in unlabeled data. free and acquisition-easy unlabeled data to extend the la-bel information from limited labeled data and has achieved promising results in various tasks [38, 50, 51, 54].
Unfortunately, classical SSL relies on a basic assump-tion that the labeled and unlabeled data are collected from the same distribution, which is difﬁcult to hold in real-world applications. In most practical cases, unlabeled data usually contains classes that are not seen in the labeled data. Exist-ing works [12, 17, 21, 40] have shown that training the SSL model with these OOD samples in unlabeled data leads to a large degradation in performance. To solve this problem, robust semi-supervised learning (Robust SSL) has been in-vestigated to train a classiﬁcation model that performs sta-bly when the unlabeled set is corrupted by OOD samples.
Typical methods focus on discarding OOD information at the sample level, that is, detecting and ﬁltering OOD sam-ples to purify the unlabeled set [12, 17, 21, 57]. However, these methods ignore the semantic-level pollution caused by the classiﬁcation-useless semantics from OOD samples, which improperly disturbs the feature distribution learned from ID samples, eventually resulting in weak ID and OOD discrimination and low classiﬁcation performance. We pro-vide an example to explain such a problem in Fig. 1. As we can see, due to the semantics of Orchid in OOD examples, the model pays too much attention to the background and misclassiﬁes the Butterﬂy as Beetle.
In this paper, we propose Out-of-distributed Semantic
Pruning (OSP) method to solve the problem mentioned above and achieve effective robust SSL.
Concretely, our OSP approach consists of two main modules. We ﬁrst develop an aliasing OOD matching mod-ule to pair each ID sample with an OOD sample with which it has feature aliasing. Secondly, we propose a soft orthogo-nality regularization, which constrains the predictions of ID samples to keep consistent before and after soft-orthogonal decomposition according to their matching OOD samples.
We evaluate the effectiveness of our OSP in extensive robust semi-supervised image recognition benchmarks in-cluding MNIST [53], CIFAR10 [29], CIFAR100 [29] and
TinyImageNet [15]. We show that our OSP obtains signiﬁ-cant improvements compared to state-of-the-art alternatives (e.g., 13.7% and 15.0% on TinyImagetNet with an OOD ratio of 0.3 and 0.6 respectively). Besides, we also empiri-cally demonstrate that OSP indeed increases the feature dis-crimination between ID and OOD samples. To summarize, the contributions of this work are as follows:
• To the best of our knowledge, we are the ﬁrst to exploit the OOD effects at the semantic level by regularization
ID features to be orthogonal to OOD features.
• We develop an aliasing OOD matching module that adaptively pairs each ID sample with an OOD sample.
In addition, we propose a soft orthogonality regulariza-tion to restrict ID and OOD features to be orthogonal.
• We conduct extensive experiments on four datasets, i.e., MNIST, CIFAR10, CIFAR100, and TIN200, and achieve new SOTA performance. Moreover, we ana-lyze that the superiority of OSP lies in the enhanced discrimination between ID and OOD features. in SSL classiﬁcation [2,4,5,8,10,11,14,19,24,26,35,43,48].
Powerful methods based on entropy minimization enforce their networks to make low-entropy predictions on unla-beled data [3, 27, 32, 32, 34, 42]. Another spectrum of pop-ular approaches is consistency regularization, whose core idea is to obtain consistent prediction under various pertur-bations [31, 38, 50, 51, 54]. VAT [38] enforces prediction invariance under adversarial noises on unlabeled images.
UDA [54] and FixMatch [50] employ weak and strong aug-mentation to compute the consistency loss.
The effectiveness of these SSL methods relies on an as-sumption that the labeled and unlabeled data are drawn from the same distribution. However, in practice, such an as-sumption is difﬁcult to satisfy, resulting in severe perfor-mance degeneration of close-set SSL [18, 40, 57]. Thus, there is an urgent need to develop SSL algorithms that could work robustly with an unlabeled dataset that contains OOD samples. 2.2. Robust Semi-Supervised Learning
Robust SSL aims to train a classiﬁcation model that per-forms stably when the unlabeled set is corrupted by OOD samples [1, 6, 18, 22, 24, 28, 33, 47]. This paper considers a common case: unlabeled data contains classes not seen in the labeled data [55]. Current typical approaches focus on removing the effects of OOD information at the sample-level [12, 17, 21, 57]. UASD [12] utilizes self-distillation to detect OOD samples and ﬁlter them out later from un-labeled data. MTC [55] proposes a multi-task curriculum learning framework, which detects the OOD samples in un-labeled data and simultaneously estimates the probability of the sample belonging to OOD. DS3L [17] trains a soft weighting function to assign small weights to OOD unla-beled samples and large weights to ID unlabeled samples.
More recently, some works have proposed utilizing OOD samples to improve the feature representation capacity of their models [23, 37]. Simultaneously, they also inherited the idea of previous work to ﬁlter out OOD samples in clas-siﬁcation supervision.
[37] extracts style features of ID samples and transfers OOD samples to ID style. T2T [23] employs an agent self-supervised task on both ID and OOD samples to enhance representation learning. Different from existing methods, we propose to prune the harmful OOD se-mantics out from ID features by regularizing ID and OOD features to be orthogonal, resulting in accurate ID classiﬁ-cation and OOD detection. 2.