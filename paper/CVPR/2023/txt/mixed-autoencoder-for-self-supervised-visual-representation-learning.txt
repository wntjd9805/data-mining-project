Abstract
Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open ques-tions, different from those in contrastive learning that serve as the most important part. This paper studies the prevail-ing mixing augmentation for MAE. We first demonstrate that naÈıve mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed
Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) aug-mentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by
+0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K,
ADE20K and COCO respectively with a standard ViT-Base.
Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2×. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available. 1.

Introduction
Self-supervised learning (SSL) has become one of the most popular pre-training paradigm due to its independence of human annotation. Previous literature mainly focuses on the handcrafted pretext task design [13, 19, 36] and instance discrimination [6,10], while with the development of Vision
Transformer [15], masked image modeling (MIM), deeply motivated by masked language modeling [12], has started to demonstrate more superior effectiveness by firstly masking some patches of the input images and then reconstructing the masked patches from visible ones by predicting certain
Figure 1. Fine-tuning accuracy on ImageNet-1K. Our MixedAE achieves the best trade-off between pre-training overhead and transfer performance. Specifically, MixedAE surpasses MAE [22] consistently with only 3% extra overhead, while outperforms the strong iBOT [49] with only 53.4% of its computation overhead.
See more detailed comparisons in Tab. 1. ID stands for instance discrimination, while MIM represents masked image modeling. targets generated by masked patches. In order to complete reconstruction, the encoder is expected to generate highly semantic representation which can be better transferred to downstream tasks [21, 29, 30, 48] for superior performance.
Existing MIM works mainly concentrate on the design of the reconstruction targets (e.g., visual tokenizers [3, 14], pixels [22,44], graphical features [41] and instance discrim-ination [2, 16, 49]) and masking strategies (e.g., random [3, 22], attention-guide [25] and sample-dependent [39]). See more detailed discussions in Sec. 2. Despite the superior performance, we observe that the input augmentations for
MIM have been seldom explored. Specifically, adding color jittering, an essential augmentation technique of contrastive learning [8], with MAE [22] even degrades the transfer re-sults, suggesting MIM might have a different preference for data augmentations, and the effective data augmentation strategies for MIM are still an open question.
In this paper, we explore the usage of image mixing, a commonly used technique in both supervised [46, 47] and contrastive learning [38, 45], with MAE [22]. We start by
constructing a simple baseline to adopt mixing with MAE directly, which, different from in supervised and contrastive learning, would instead ease the reconstruction pretext by increasing the mutual information between the model input and reconstruction target due to the usage of image mixing with global self-attention as proved in Sec. 3.1. To address this issue, we propose homologous recognition, an auxiliary pretext task to enforce each patch to recognize homologous patches explicitly according to attention distributions before reconstruction, and build our Mixed Autoencoder network (MixedAE) in Sec. 3.3. Moreover, we demonstrate that our simple yet effective method can not only achieve significant performance improvement, but also conduct object-aware
SSL pre-training without any specifically designed modules for better downstream dense perception results in Sec. 3.4.
Concurrently, MixMIM [31] also considers mixing with
MAE, but different from ours, 1) Purpose: MixMIM uses mixing to recover the 2D structure after random masking for an efficient implementation to conduct MAE-style pre-training on hierarchy Vision Transformers [34], while ours utilizes mixing to conduct object-aware SSL pre-training for better representation learning. 2) Method: MixMIM uses masked self-attention to only perform attention within patches from the same images given the mixing masks as in-put, sharing the exactly same pretext task with MAE, while ours requires explicit homologous recognition given mixing masks as target, actively emerging mixing into the pretext design. 3) Formulation: The mixing ratio r is limited to 0.5 in MixMIM, which instead can be flexibly selected from (0, 0.5] in our formulation. See more details in Sec. 3.
The main contributions of this work contain three parts: 1. We propose the Mixed Autoencoder (MixedAE), a sim-ple yet effective approach to conduct object-aware pre-training without introducing any specifically designed modules. With extensive experiments, we demonstrate that MixedAE can achieve the state-of-the-art transfer performance on various downstream tasks including image classification, semantic segmentation and object detection, while maintaining significant efficiency. 2. We theoretically demonstrate the underlying design differences between MIM and previous supervision with mixing (e.g., supervised and contrastive learning). 3. To our best knowledge, this is the first work to consider mixing as an effective data augmentation strategy for
MIM from the perspective of pretext design with a pure autoencoder-based architecture. 2.