Abstract
Visible-modal object tracking gives rise to a series of downstream multi-modal tracking tributaries. To inherit the powerful representations of the foundation model, a natural modus operandi for multi-modal tracking is full ﬁne-tuning on the RGB-based parameters. Albeit effective, this man-ner is not optimal due to the scarcity of downstream data and poor transferability, etc. In this paper, inspired by the recent success of the prompt learning in language models, we develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multi-modal tracking tasks. ViPT ﬁnds a better way to stimulate the knowledge of the RGB-based model that is pre-trained at scale, meanwhile only introducing a few trainable pa-rameters (less than 1% of model parameters). ViPT outper-forms the full ﬁne-tuning paradigm on multiple downstream tracking tasks including RGB+Depth, RGB+Thermal, and
RGB+Event tracking. Extensive experiments show the po-tential of visual prompt learning for multi-modal tracking, and ViPT can achieve state-of-the-art performance while satisfying parameter efﬁciency. Code and models are avail-able at https://github.com/jiawen-zhu/ViPT. 1.

Introduction
RGB-based tracking, a foundation task of visual object tracking, gains from large-scale benchmarks [9, 13, 27, 36, 39, 45] provided by the community, and many excellent works [2, 3, 5, 7, 22] have spurted out over the past decades.
Despite the promising results, object tracking based on pure RGB sequences is still prone to failure in some com-plex and corner scenarios, e.g., extreme illumination, back-ground clutter, and motion blur. Therefore, multi-modal tracking is drawing increasing attention due to the ability to achieve more robust tracking by utilizing inter-modal com-plementarity, among which RGB+Depth (RGB-D) [47, 60],
†Equal contribution. (cid:66)Corresponding author: Dr. Dong Wang.
Figure 1. Existing multi-modal tracking paradigm vs. ViPT. (a) Foundation model training on large-scale RGB sequences. (a)→(b) Existing multi-modal methods extend the off-the-shelf foundation model and conduct full ﬁne-tuning on downstream tracking tasks. (a)→(c) We investigate the design of a multi-modal tracking method in prompt-learning paradigm. Compared to (b), the proposed ViPT has a more concise network structure, bene-ﬁts from parameter-friendly prompt-tuning, and narrows the gap between the foundation and downstream models.
RGB+Thermal (RGB-T) [44, 56], and RGB+Event (RGB-E) [51, 52] are represented.
However, as the downstream task of RGB-based track-ing, the main issue encountered by multi-modal tracking is the lack of large-scale datasets. For example, the widely used RGB-based tracking datasets, GOT-10k [13], Track-ingNet [36], and LaSOT [9], contain 9.3K, 30.1K, and 1.1K sequences, corresponding to 1.4M, 14M, and 2.8M frames for training. Whereas the largest training datasets in multi-modal tracking, DepthTrack [47], LasHeR [25],
VisEvent [43], contain 150, 979, 500 training sequences, corresponding to 0.22M, 0.51M, 0.21M annotated frame pairs, which is at least an order of magnitude less than the former. Accounting for the above limitation, multi-modal tracking methods [43, 47, 61] usually utilize pre-trained
RGB-based trackers and perform ﬁne-tuning on their task-oriented training sets (as shown in Figure 1 (a)→(b)).
DeT [47] adds a depth feature extraction branch to the orig-inal ATOM [7] or DiMP [3] tracker and ﬁne-tunes on RGB-D training data. Zhang et al. [57] extend SiamRPN++ [21] with dual-modal inputs for RGB-T tracking. They ﬁrst con-struct a unimodal tracking network trained on RGB data, then tune the whole extended multi-modal network with
RGB-T image pairs. Similarly, Wang et al. [43] develop dual-modal trackers by extending single-modal ones with various fusion strategies for visible and event ﬂows and per-form extra model training on the RGB-E sequences. Al-though effective, the task-oriented full-tuning approach has some drawbacks. (i) Full ﬁne-tuning the model is time ex-pensive and inefﬁcient, and the burden of parameters stor-ing is large, which is unfriendly to numerous applications and cumbersome to transfer deploying. (ii) Full ﬁne-tuning is unable to obtain generalized representation due to the limited annotated samples, the inability to utilize the pre-trained knowledge of the foundation model trained on large-scale datasets. Thus, a natural question is thrown up: Is there a more effective manner to adapt the RGB-based foun-dation model to downstream multi-modal tracking?
More recently, in Natural Language Processing (NLP)
ﬁeld, researchers have injected textual prompts into down-stream language models to effectively exploit the repre-sentational potential of foundation models, this method, is known as prompt-tuning. After that, a few researchers have tried to freeze the entire upstream model and add only some learnable parameters to the input side to learn valid visual prompts. Existing researches
[1, 14, 38, 59] show its great potential and visual prompt learning is ex-pected to be an alternative to full ﬁne-tuning. Intuitively, there is a large inheritance between multi-modal and sin-gle RGB-modal tracking, which should share most of prior
In knowledge on feature extraction or attention patterns. this spirit, we present ViPT, a uniﬁed visual prompt-tuning paradigm for downstream multi-modal tracking.
Instead of fully ﬁne-tuning an RGB-based tracker combined with an auxiliary-modal branch, ViPT freezes the whole foun-dation model and only learns a few modal-speciﬁc visual prompts, which inherits the RGB-based model parame-ters trained at scale to the maximum extent (see Figure 1 (c)). Different from the prompt learning of other single-modal vision tasks, ViPT introduces additional auxiliary-modal inputs into the prompt-tuning process, adapting the foundation model to downstream tasks while simultane-ously learning the association between different modalities.
Speciﬁcally, ViPT inserts several simple and lightweight modality-complementary prompter (MCP) blocks into the frozen foundation model to effectively learn the inter-modal complementarities. Notably, ViPT is a general framework for various downstream multi-modal tracking tasks, includ-ing RGB-D, RGB-T, and RGB-E tracking. We summarize the contribution of our work as follows:
• A visual prompt tracking framework is proposed to achieve task-oriented multi-modal tracking. Facili-tated by learned prompts, the off-the-shelf foundation model can be effectively adapted from RGB domain to downstream multi-modal tracking tasks. Besides,
ViPT is a general method that can be applied to vari-ous tasks, i.e., RGB-D, RGB-T, and RGB-E tracking.
• A modality-complementary prompter is designed to generate valid visual prompts for the task-oriented multi-modal tracking. The auxiliary-modal inputs are streamlined to a small number of prompts instead of designing an extra network branch.
• Extensive experiments show that our method achieves
SOTA performance on multiple downstream multi-modal tracking tasks while maintaining parameter-efﬁcient (<1% trainable parameters). 2.