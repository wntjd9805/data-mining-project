Abstract
This paper proposes a regularizer called Implicit Neural
Representation Regularizer (INRR) to improve the general-ization ability of the Implicit Neural Representation (INR).
The INR is a fully connected network that can represent sig-nals with details not restricted by grid resolution. However, its generalization ability could be improved, especially with non-uniformly sampled data. The proposed INRR is based on learned Dirichlet Energy (DE) that measures similarities between rows/columns of the matrix. The smoothness of the
Laplacian matrix is further integrated by parameterizing
DE with a tiny INR. INRR improves the generalization of
INR in signal representation by perfectly integrating the sig-nal’s self-similarity with the smoothness of the Laplacian matrix. Through well-designed numerical experiments, the paper also reveals a series of properties derived from INRR, including momentum methods like convergence trajectory and multi-scale similarity. Moreover, the proposed method could improve the performance of other signal representa-tion methods. 1.

Introduction
INR uses a fully connected network (FCN) ϕθ(x) :
Rd (cid:55)→ Ro to approximate the explicit solution of an implicit xϕθ, . . .(cid:1) = 0. For an example, function F (cid:0)x, ϕθ, ∇xϕθ, ∇2 we can represent a gray-scale image X ∈ Rm×n with an
INR ϕθ(x) : R2 (cid:55)→ R which satisfied ϕθ( i n ) = Xij, i ∈
{1, . . . , m} , j ∈ {1, . . . , n}. Compared with traditional grid representation X, INR’s representation ability to details is not restricted by grid resolution m, n as INR can predict the pixel value at any location (x, y) ∈ R2 even not equals to m , j ( i n ).
Besides the representation ability of INR, generalization ability is critical for a neural network. We explore the em-m , j
*This work was supported by the National Key Research ,Development
Program (2020YFA0713504), the National Natural Science Foundation of
China (61977065) and the Macao Science and Technology Development
Fund (061/2020/A2). pirical generalization ability via a 256 × 256 gray-scale non-uniformly sampled image inpainting task as Figure 2(a) shows. Although INR fits training data perfectly in Fig-ure 2(b), its prediction outside training data is unreasonable.
Theoretical analysis of INR illustrates that a hyper-parameter controls the smoothness degree of ϕθ(x). Moreover, the ex-periments show that the best hyper-parameter varies with the missing rate (the percentage of unsampled pixels) as Fig-ure 3 shows. Adjusting this hyper-parameter cannot make the non-uniformly missing case perform best, as different locations might have different missing rates.
A carefully designed regularizer is proposed to improve the generalization ability of INR. It is based on Adaptive and
Implicit Regularization (AIR) which is a learned Dirichlet
Energy (DE) [12] that measures similarities or correlations between rows/columns of X. The smoothness of the Lapla-cian matrix is further integrated by parameterizing DE with a tiny INR. The structure of the proposed implicit neural representation regularizer (INRR) is shown in Figure 1(b).
Because a smooth Laplacian matrix represents non-local prior and large-scale local prior in vision data, INRR can improve the generalization of INR in image representation.
Numerous numerical experiments show that INRR outper-forms various classical regularizers, including total variation (TV), L2 energy, and so on. As a regularizer both in a new form and with new meaning, INRR can be combined with other signal representation methods, such as deep matrix factorization (DMF) [1].
To summarize, the contributions of our work include the following:
• Neural Tangent Kernel (NTK) [1] theoretically analyzes the generalization ability of INR and why INR performs poorly with nonuniform sampling is given.
• A tiny INR parameterized regularizer named INRR is proposed based on DE, which perfectly integrates the image’s self-similarity with the smoothness of the
Laplacian matrix.
• A series of properties derived from INRR, including momentum methods, multi-scale similarity, and gener-Figure 1. Overview of proposed improve scheme for INR. (a) INR is a fully connected neural network which maps from coordinate to pixel value. (b) INRR is a regularization term represented by an INR which can capture the self-similarity. (c) INR-Z improve the performance of
INR by combining the neighbor pixels with coordinate together as the input of another INR. (a) Sampling (b) INR (18.1 dB) (c) INRR (23.3 dB)
Figure 2. Image fitting results. All the methods are based on the
SIREN to fit an 256 × 256 Baboon with the sampling data in (a). (b) trained with a vanilla SIREN while (c) trained with proposed
INRR. alization ability, are revealed by well-designed numeri-cal experiments. 2.