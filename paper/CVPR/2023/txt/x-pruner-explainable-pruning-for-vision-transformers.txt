Abstract
Recently vision transformer models have become promi-nent models for a range of tasks. These models, how-ever, usually suffer from intensive computational costs and heavy memory requirements, making them impractical for deployment on edge platforms. Recent studies have pro-posed to prune transformers in an unexplainable manner, which overlook the relationship between internal units of the model and the target class, thereby leading to infe-rior performance. To alleviate this problem, we propose a novel explainable pruning framework dubbed X-Pruner, which is designed by considering the explainability of the pruning criterion. Speciﬁcally, to measure each prunable unit’s contribution to predicting each target class, a novel explainability-aware mask is proposed and learned in an end-to-end manner. Then, to preserve the most informative units and learn the layer-wise pruning rate, we adaptively search the layer-wise threshold that differentiates between unpruned and pruned units based on their explainability-aware mask values. To verify and evaluate our method, we apply the X-Pruner on representative transformer mod-els including the DeiT and Swin Transformer. Comprehen-sive simulation results demonstrate that the proposed X-Pruner outperforms the state-of-the-art black-box methods with signiﬁcantly reduced computational costs and slight performance degradation. Code is available at https:
//github.com/vickyyu90/XPruner. 1.

Introduction
Over the last few years, transformers have attracted in-creasing attention in various challenging domains, such as natural language processing, vision, or graphs [3, 9]. It is composed of two key modules, namely the Multi-Head At-tention (MHA) and Multi-Layer Perceptron (MLP). How-ever, similar to CNNs, the major limitations of transform-ers include the gigantic model sizes with intensive compu-tational costs. Which severely restricts their deployment
*Corresponding author. in resource-constrained devices like edge platforms. To compress and accelerate transformer models, a variety of techniques naturally emerge. Popular approaches include weight quantization [30], knowledge distillation [28], ﬁlter compression [24], and model pruning [21]. Among them, model pruning especially structured pruning has gained considerable interest that removes the least important pa-rameters in pre-trained models in a hardware-friendly man-ner, which is thus the focus of our paper.
Due to the signiﬁcant structural differences between
CNNs and transformers, although there is prevailing suc-cess in CNN pruning methods, the research on pruning transformers is still in the early stage. Existing studies (1) could empirically be classiﬁed into three categories.
Criterion-based pruning resorts to preserving the most im-portant weights/attentions by employing pre-deﬁned crite-ria, e.g., the L1/L2 norm [20], or activation values [6]. (2)
Training-based pruning retrains models with hand-crafted sparse regularizations [31] or resource constraints [28, 29]. (3) Architecture-search pruning methods directly search for an optimal sub-architecture based on pre-deﬁned policies
[3, 10]. Although these studies have made considerable progress, two fundamental issues have not been fully ad-dressed, i.e., the optimal layer-wise pruning ratio and the weight importance measurement.
For the ﬁrst issue, the ﬁnal performance is notably af-fected by the selection of pruning rates for different layers.
To this end, some relevant works have proposed a series of methods for determining the optimal per-layer rate [7, 11].
For instance, Michel et al. [18] investigate the effectiveness of attention heads in transformers for NLP tasks and pro-pose to prune attention heads with a greedy algorithm. Yu et al. [28] develop a pruning algorithm that removes attention scores below a learned per-layer threshold while preserving the overall structure of the attention mechanism. However, the proposed methods do not take into account the inter-dependencies between weight. Recently, Zhu et al. [31] in-troduce the method VTP with a sparsity regularization to identify and remove unimportant patches and heads from the vision transformers. However, VTP needs to try the thresholds manually for all layers. 1
For the second issue, previous studies resort to identi-fying unimportant weights by various importance metrics, including magnitude-based, gradient-based [12, 19], and mask-based [27]. Among them, the magnitude-based ap-proaches usually lead to suboptimal results as it does not take into account the potential correlation between weights
[24].
In addition, gradient-based methods often tend to prune weights with small values, as they have small gra-dients and may not be identiﬁed as important by the back-ward propagation. Finally, the limitation of current mask-based pruning lies in two folds: (1) Most mask-based prun-ing techniques manually assign a binary mask w.r.t. a unit according to a per-layer pruning ratio, which is inefﬁcient and sub-optimal. (2) Most works use a non-differentiable mask, which results in an unstable training process and poor convergence.
In this paper, we propose a novel explainable structured pruning framework for vision transformer models, termed
X-Pruner, by considering the explainability of the prun-ing criterion to solve the above two problems. As stated in the eXplainable AI (XAI) ﬁeld [2], important weights in a model typically capture semantic class-speciﬁc infor-mation. Inspired by this theory, we propose to effectively quantitate the importance of each weight in a class-wise manner. Firstly, we design an explainability-aware mask for each prunable unit (e.g., an attention head or matrix in linear layers), which measures the unit’s contribution to pre-dicting every class and is fully differentiable. Secondly, we use each input’s ground-truth label as prior knowledge to guide the mask learning, thus the class-level information w.r.t. each input will be fully utilized. Our intuition is that if one unit generates feature representations that make a pos-itive contribution to a target class, its mask value w.r.t. this class would be positively activated, and deactivated other-wise. Thirdly, we propose a differentiable pruning oper-ation along with a threshold regularizer. This enables the search of thresholds through gradient-based optimization, and is superior to most previous studies that prune units with hand-crafted criteria. Meanwhile, the proposed prun-ing process can be done automatically, i.e., discriminative units that are above the learned threshold are retained. In this way, we implement our layer-wise pruning algorithm in an explainable manner automatically and efﬁciently. In summary, the major contributions of this paper are:
• We propose a novel explainable structured pruning framework dubbed X-Pruner, which prunes units that make less contributions to identifying all the classes in terms of explainability. To the best knowledge of the authors, this is the ﬁrst work to develop an explainable pruning framework for vision transformers;
• We propose to assign each prunable unit an explainability-aware mask, with the goal of quantify-ing its contribution to predicting each class. Speciﬁ-cally, the proposed mask is fully differentiable and can be learned in an end-to-end manner;
• Based on the obtained explainability-aware masks, we propose to learn the layer-wise pruning thresholds that differentiate the important and less-important units via a differentiable pruning operation. Therefore, this pro-cess is done in an explainable manner;
• Comprehensive simulation results are presented to demonstrate that the proposed X-Pruner outperforms a number of state-of-the-art approaches, and shows its superiority in gaining the explainability for the pruned model. 2.