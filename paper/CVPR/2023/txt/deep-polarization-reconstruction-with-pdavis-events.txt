Abstract
The polarization event camera PDAVIS is a novel bio-inspired neuromorphic vision sensor that reports both con-ventional polarization frames and asynchronous, continu-ously per-pixel polarization brightness changes (polariza-tion events) with fast temporal resolution and large dy-namic range. A deep neural network method (Polariza-tion FireNet) was previously developed to reconstruct the polarization angle and degree from polarization events for bridging the gap between the polarization event camera and mainstream computer vision. However, Polarization
FireNet applies a network pre-trained for normal event-based frame reconstruction independently on each of four channels of polarization events from four linear polariza-tion angles, which ignores the correlations between chan-nels and inevitably introduces content inconsistency be-tween the four reconstructed frames, resulting in unsatisfac-tory polarization reconstruction performance. In this work, we strive to train an effective, yet efficient, DNN model that directly outputs polarization from the input raw polariza-tion events. To this end, we constructed the first large-scale event-to-polarization dataset, which we subsequently employed to train our events-to-polarization network E2P.
E2P extracts rich polarization patterns from input polariza-tion events and enhances features through cross-modality context integration. We demonstrate that E2P outperforms
Polarization FireNet by a significant margin with no addi-tional computing cost. Experimental results also show that
E2P produces more accurate measurement of polarization than the PDAVIS frames in challenging fast and high dy-namic range scenes. Code and data are publicly available at: https://github.com/SensorsINI/e2p. 1.

Introduction
Visual information is encoded in light by intensity, color, and polarization [12]. Polarization is a property of trans-verse light waves that specifies the geometric orientation of
the oscillations (which can be described by the Angle of
Linear Polarization (AoLP) and the Degree of Linear Polar-ization (DoLP)), providing strong vision cues and enabling solutions to challenging problems in medical [27], under-water [34], and remote sensing [53] applications. Existing polarization digital cameras capture synchronous polariza-tion frames with a linear photo response [14], while biologi-cal eyes tend to perceive asynchronous and sparse data with a compressed non-linear response [12].
Inspired by the mantis shrimp visual system [29], the novel neuromorphic vision sensor called Polarization Dy-namic and Active pixel VIsion Sensor (PDAVIS) illustrated in Figure 1 was developed to concurrently record a high-frequency stream of asynchronous polarization brightness change events under four polarization angles (i.e., 0◦, 45◦, 90◦, and 135◦) over a wide range of illumination. PDAVIS also outputs low-frequency synchronous frames like con-ventional polarization cameras [15].
Even though the stream of polarization events has ad-vantages of low latency and HDR, it is not friendly to hu-man observation and traditional computer vision due to the sparse, irregular, and unstructured properties. To better ex-ploit the advantages of PDAVIS, an intuitive solution is to reconstruct polarization from polarization events, which can bridge off-the-shelf frame-based algorithms and PDAVIS.
Gruev et al. [15] proposed the Polarization FireNet, which first runs the FireNet [41] pre-trained for normal event-based intensity frame reconstruction on each of four types of polarization events under four different polarization an-gles, and then computes the polarization from four recon-structed intensity frames via mathematical formulas. Since this method treats four polarization angle channels indepen-dently, the correlation between channels is ignored and in-consistency between the four reconstructed frames hinders accurate measurement of polarization.
In this work, we make the first attempt to train an accurate yet efficient DNN model tailored for event-to-polarization reconstruction. We approach this twofold.
First, we construct the first large-scale event-to-polarization synthetic-real mixed dataset, dubbed Events to Polariza-tion Dataset (E2PD), which contains 5 billion polarization events and corresponding 133 thousand polarization video frames. The diversity and practicality of E2PD are ensured by including diverse real-world road scenes under differ-ent weather conditions (rainy and sunny) in different cities.
Second, we design an E2P network that consists of three branches to reconstruct intensity, AoLP, and DoLP, respec-tively, from the raw polarization events directly. E2P is built on two key modules: (i) a Rich Polarization Pattern Per-ception (RPPP) module that effectively harvests features from raw polarization events and (ii) a Cross-Modality At-tention Enhancement (CMAE) module that explores cross-modality contextual cues for feature enhancement.
We perform extensive validation experiments to demon-strate the efficacy of our method and show that the network trained on our E2PD is more accurate than all previously reported PDAVIS methods, and produces more accurate po-larization compared with polarization computed from the
PDAVIS frames in challenging scenes (e.g., Figure 1). In summary, our contributions are: 1. the first attempt to solve the event-to-polarization problem using an end-to-end trained deep neural net-work with polarization events as input, intensity, AoLP and DoLP as outputs; 2. a new and unique large-scale event-to-polarization dataset containing both synthetic and real data; and 3. a novel network that perceives rich polarization pat-terns from raw polarization events and enhances fea-tures via a cross-modality attention mechanism. 2.