Abstract
FlowFormer [24] introduces a transformer architecture into optical flow estimation and achieves state-of-the-art performance. The core component of FlowFormer is the transformer-based cost-volume encoder. Inspired by the re-cent success of masked autoencoding (MAE) pretraining in unleashing transformers’ capacity of encoding visual rep-resentation, we propose Masked Cost Volume Autoencod-ing (MCVA) to enhance FlowFormer by pretraining the cost-volume encoder with a novel MAE scheme. Firstly, we introduce a block-sharing masking strategy to prevent masked information leakage, as the cost maps of neigh-boring source pixels are highly correlated. Secondly, we propose a novel pre-text reconstruction task, which encour-ages the cost-volume encoder to aggregate long-range in-formation and ensures pretraining-finetuning consistency.
We also show how to modify the FlowFormer architecture to accommodate masks during pretraining. Pretrained with
MCVA, FlowFormer++ ranks 1st among published meth-ods on both Sintel and KITTI-2015 benchmarks. Specifi-cally, FlowFormer++ achieves 1.07 and 1.94 average end-point error (AEPE) on the clean and final pass of Sintel benchmark, leading to 7.76% and 7.18% error reductions from FlowFormer. FlowFormer++ obtains 4.52 F1-all on the KITTI-2015 test set, improving FlowFormer by 0.16. 1.

Introduction
Optical flow is a long-standing vision task, targeting at estimating per-pixel displacement between consecutive video frames.
It can provide motion and correspondence information in many downstream video problems, includ-ing video object detection [65, 81, 82], action recogni-*Xiaoyu Shi and Zhaoyang Huang assert equal contributions.
Figure 1. Overview of FlowFormer++. The core component of
FlowFormer architecture is the transformer-based cost-volume en-coder. We propose Masked Cost Volume Autoencoding to pretrain the cost-volume encoder. During pretraining, a portion of cost val-ues are masked and the cost-volume encoder is required to recon-struct masked cost patches. tion [48, 61, 80], and video restoration [5, 14, 32, 34, 51, 70].
Recently, FlowFormer [24] introduces a transformer ar-chitecture for optical flow estimation and achieves state-of-the-art performance. The core of its success lies on two aspects: the ImageNet-pretrained transformer-based image encoder and the transformer-based cost-volume encoder.
Notably, adopting an ImageNet-pretrained visual backbone leads to considerable performance gain over the train-from-scratch counterpart, indicating that random weight initial-ization hinders the learning of correspondence estimation.
This naturally begs the question: can we also pretrain the transformer-based cost-volume encoder and thus further un-leash its power to achieve more accurate optical flow?
In this paper, we propose masked cost-volume autoen-coding (MCVA), a self-supervised pretraining scheme to enhance the cost-volume encoding on top of the Flow-Former framework. We are inspired by the recent success of masked autoencoding, such as BERT [11] in NLP and
MAE [20] in computer vision. The key idea of masked au-toencoding is masking a portion of input data, and requir-ing networks to learn high-level representation for masked
contents reconstruction. However, it is non-trivial to adapt the masked autoencoding strategy to learn a better cost vol-ume encoder for optical flow estimation, because of the two following reasons. Firstly, the cost volume might con-tain redundancy and the cost maps (cost values between a source-image pixel to all target-image pixels) of neighbor-ing source-image pixels are highly correlated. Randomly masking cost values, as done in other single-image pre-training methods [20], leads to information leakage and makes the model biased towards aggregating local infor-mation. Secondly, existing masked autoencoding methods target at reconstructing masked content randomly selected from fixed locations. This suffices to pretrain general-purpose single-image encoder in other fields. However, the cost-volume encoder of FlowFormer is deeply coupled with the follow-up recurrent decoder, which demands cost infor-mation of long range at flexible locations.
To tackle the aforementioned issues, we introduce two task-specific designs. Firstly, instead of randomly masking the cost volume, we partition source pixels into large varied-size blocks and let source pixels within the same block share a common mask pattern on their cost maps. This strategy, termed block-sharing masking, prevents the cost-volume encoder from reconstructing masked cost values by simply copying from neighboring source pixels’ cost maps
Such design enforces the cost-volume encoder to abstract useful cues from cost maps belonging to far-away source pixels, which encourages long-range information aggrega-tion. Secondly, to mimic the decoding process in finetuning and thus avoid pretraining-finetuning discrepancy, we pro-pose a novel pre-text reconstruction task as shown in Fig. 3: small cost patches are randomly cropped from the cost maps to retrieve features from the cost-volume encoder, aiming to reconstruct larger cost map patches centered at the same locations. This is in line with the decoding process of Flow-Former in the finetuning stage. This pre-text task explicitly encourages the cost-volume encoder to capture long-range information for cost-volume encoding, which is critical for optical flow estimation.
In essence, the proposed masked cost-volume autoen-coding (MCVA) has unique designs compared with conven-tional MAE methods, which encourages the cost-volume encoder 1) to construct high-level holistic representation of the cost volume, more effectively encoding long-range in-formation, 2) to reason about occluded (i.e., masked) infor-mation by aggregating faithful unmasked costs, and 3) to decode task-specific feature (i.e., larger cost patches at re-quired locations) to better align the pretraining process with that of the finetuning. These designs contribute to better handling of hard cases, such as noises, large-displacement motion and occlusion, for more accurate flow estimation.
To conclude, the contributions of this work are three-fold: 1) We propose the masked cost-volume autoencod-ing scheme to better pretrain the cost-volume encoder of
FlowFormer. 2) We propose task-specific masking strat-egy and reconstruction pre-text task to mitigate pretraining-finetuning discrepancy, the learned representations from pretraining. 3) With the pro-posed pretraining technique, our proposed FlowFormer++ obtains all-sided improvements over FlowFormer, setting new state-of-the-art performance on public benchmarks. fully taking advantage of 2.