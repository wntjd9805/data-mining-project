Abstract
Deep neural networks obtained by standard training have been constantly plagued by adversarial examples. Although adversarial training demonstrates its capability to defend against adversarial examples, unfortunately, it leads to an inevitable drop in the natural generalization. To address the issue, we decouple the natural generalization and the robust generalization from joint training and formulate different training strategies for each one. Specifically, instead of min-imizing a global loss on the expectation over these two gen-eralization errors, we propose a bi-expert framework called
Generalist where we simultaneously train base learners with task-aware strategies so that they can specialize in their own fields. The parameters of base learners are collected and combined to form a global learner at intervals during the training process. The global learner is then distributed to the base learners as initialized parameters for continued train-ing. Theoretically, we prove that the risks of Generalist will get lower once the base learners are well trained. Extensive experiments verify the applicability of Generalist to achieve high accuracy on natural examples while maintaining con-siderable robustness to adversarial ones. Code is available at https://github.com/PKU-ML/Generalist.
Figure 1. Comparison with other advanced adversarial training methods. Both clean accuracy and robust accuracy (against Au-toAttack [9]) are given for a handy reference. It is noted that current adversarial training methods achieve high clean accuracy by greatly sacrificing robustness. That means it is hard to obtain sufficient robustness but maintain high clean accuracy in the joint training framework. Our Generalist attains excellent clean accuracy while staying competitively robust. The improvement of Generalist is notable since we only use the naive cross-entropy loss with negligi-ble computational overhead and even without increasing the model size. 1.

Introduction
Modern deep learning techniques have achieved remark-able success in many fields, including computer vision
[14, 16], natural language processing [10, 31], and speech recognition [28, 36]. Yet, deep neural networks (DNNs) suffer a catastrophic performance degradation by human imperceptible adversarial perturbations where wrong predic-tions are made with extremely high confidence [13, 29, 34].
The vulnerability of DNNs has led to the proposal of var-ious defense approaches [3, 24, 25, 33, 40] for protecting
DNNs from adversarial attacks. One of those representa-*Work was done as an internship at Peking University. Now, he is a
Ph.D. student at the University of Hong Kong.
†Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn) tive techniques is adversarial training (AT) [20, 21, 37, 38], which dynamically injects perturbed examples that deceive the current model but preserve the right label into the train-ing set. Adversarial training has been demonstrated to be the most effective method to improve adversarially robust generalization [2, 39].
Despite these successes, such attempts of adversarial training have found a tradeoff between natural and robust accuracy, i.e., there exists an undesirable increase in the er-ror on unperturbed images when the error on the worst-case perturbed images decreases, as illustrated in Figure 1. Prior works [30, 43] even argue that natural and robust accuracy are fundamentally at odds, which indicates that a robust clas-sifier can be achieved only when compromising the natural generalization. However, the following works found that
the tradeoff may be settled in a roundabout way, such as incorporating additional labeled/unlabeled data [1, 8, 22, 26] or relaxing the magnitude of perturbations to generate suit-able adversarial examples for better optimization [18, 44].
These works all focus on the data used for training while we propose to tackle the tradeoff problem from the perspective of the training paradigm in this paper.
Inspired by the spirit of the divide-and-conquer method, we decouple the objective function of adversarial training into two sub-tasks: one is used for natural example classifi-cation while the other one is used for adversarial example classification. Specifically, for each sub-task, we train a base learner on natural/adversarial datasets with the task-specific configuration while sharing the same model architecture.
The parameters of base learners are collected and combined to form a global learner at intervals during the training pro-cess, which is then distributed to base learners as initialized parameters for continued training. We name the framework as Generalist whose proof-of-concept pipeline is shown in
Figure 2. Different from the traditional joint training frame-work for natural and robust generalization, our proposed
Generalist fully leverages task-specific information to indi-vidually train the base learners, which makes each sub-task to be solved better. Theoretically, we show that if the base learners are well trained, the final global learner is guaran-teed to have a lower risk. Our proposed Generalist is the first to effectively address the tradeoff between natural and ro-bust generalization by utilizing task-aware training strategies to achieve high clean accuracy in the natural setting, while also maintaining considerable robustness to the adversarial setting (as shown in Figure 1).
In summary, the main contributions are as follows:
• For the tradeoff between natural and robust generaliza-tion, previous methods have struggled to find a sweet point to meet both goals in the joint training framework.
Here, we propose a novel Generalist paradigm, which constructs multiple task-aware base learners to respec-tively achieve the generalization goal on natural and adversarial counterparts separately.
• For each task, rather than being constricted in a stiff manner, every detail of the training strategies (e.g., opti-mization scheme) can be totally customized, thus each base learner can better explore the optimal trajectory in its field while the global learner can fully leverage the merits of all base learners.
• We conduct extensive experiments in common settings against a wide range of adversarial attacks to demon-strate the effectiveness of our approach. Results show that our Generalist paradigm greatly improves both clean and robust accuracy on benchmark datasets com-pared to relevant techniques.
Figure 2. A pipeline for the proposed Generalist. It consists of two base learners separately trained within their respective fields and a global learner aggregates the parameters of base learners through the training process. The global learner assigns its accumulated knowledge to each base learner with a fixed frequency, based on which the base learner continues learning. 2. Preliminaries and