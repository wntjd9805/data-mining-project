Abstract
Most recent 6D object pose methods use 2D optical flow to refine their results. However, the general optical flow methods typically do not consider the target’s 3D shape in-formation during matching, making them less effective in
In this work, we propose a 6D object pose estimation. shape-constraint recurrent matching framework for 6D ob-ject pose estimation. We first compute a pose-induced flow based on the displacement of 2D reprojection between the initial pose and the currently estimated pose, which em-beds the target’s 3D shape implicitly. Then we use this pose-induced flow to construct the correlation map for the following matching iterations, which reduces the matching space significantly and is much easier to learn. Further-more, we use networks to learn the object pose based on the current estimated flow, which facilitates the computation of the pose-induced flow for the next iteration and yields an end-to-end system for object pose. Finally, we optimize the optical flow and object pose simultaneously in a recurrent manner. We evaluate our method on three challenging 6D object pose datasets and show that it outperforms the state of the art significantly in both accuracy and efficiency. 1.

Introduction 6D object pose estimation, i.e., estimating the 3D rota-tion and 3D translation of a target object with respect to the camera, is a fundamental problem in 3D computer vision and also a crucial component in many applications, includ-ing robotic manipulation [8] and augmented reality [34].
Most recent methods rely on pose refinement to obtain ac-curate pose results [16, 31, 52]. Typically, they first syn-thesize an image based on the rendering techniques [9, 38] according to the initial pose, then estimate dense 2D-to-2D correspondence between the rendered image and the input based on optical flow networks [46]. After lifting the esti-mated 2D optical flow to 3D-to-2D correspondence based on the target’s 3D shape, they can obtain a new refined pose using Perspective-n-Points (PnP) solvers [27].
Although this paradigm works well in general, it suffers from several weaknesses. First, the general optical flow
Input
Initialization
Flow
Flow warp
Figure 1. The problem of optical flow in 6D pose estimation.
Given an initial pose, one can estimate the dense 2D-to-2D cor-respondence (optical flow) between the input and the synthetic image rendered from the initial pose, and then lift the dense 2D matching to 3D-to-2D correspondence to obtain a new refined pose by PnP solvers (PFA-Pose [16]). However, the flow estimation does not take the target’s 3D shape into account, as illustrated by the warped image based on the estimated flow in the last figure, which introduces significant matching noise to pose solvers and is suboptimal for 6D object pose estimation. networks they use are mainly built on top of two assump-tions, i.e., the brightness consistency between two poten-tial matches and the smoothness of matches within a local neighbor [1]. These assumptions, however, are too general and do not have any clue about the target’s 3D shape in the context of 6D object pose estimation, making the potential matching space of every pixel unnecessarily large in the tar-get image. Second, the missing shape information during matching often results in flow results that do not respect the target shape, which introduces significant matching noise, as shown in Fig. 1. Third, this multi-stage paradigm trains a network that relies on a surrogate matching loss that does not directly reflect the final 6D pose estimation task [17], which is not end-to-end trainable and suboptimal.
To address these problems, we propose a shape-constraint recurrent matching framework for 6D object pose estimation. It is built on top of the intuition that, in addition to the brightness consistency and smoothness constraint in classical optical flow solutions [2, 35], the dense 2D match-ing should comply with the 3D shape of the target. We first build a 4D correlation volume between every pixel of the source image and every pixel of the target image, similar to
RAFT [46]. While, instead of indexing from the correla-tion volume according to the current flow during the itera-tion, we propose indexing the correlation volume based on
(a) The standard strategy (b) Our strategy
Figure 2. Different pose refinement paradigms. (a) Most pose refinement methods [16] rely on a recurrent architecture to estimate dense 2D flow between the rendered image I1 and the real input image I2, based on a dynamically-constructed correlation map according to the flow results of the previous iteration. After the convergence of the flow network and lifting the 2D flow to a 3D-to-2D correspondence field, they use PnP solvers to compute a new refined pose ˆP . This strategy, however, has a large matching space for every pixel in constructing correlation maps, and optimizes a surrogate matching loss that does not directly reflect the final 6D pose estimation task. (b) By contrast, we propose optimizing the pose and flow simultaneously in an end-to-end recurrent framework with the guidance of the target’s 3D shape.
We impose a shape constraint on the correlation map construction by forcing the construction to comply with the target’s 3D shape, which reduces the matching space significantly. Furthermore, we propose learning the object pose based on the current flow prediction, which, in turn, helps the flow prediction and yields an end-to-end system for object pose. a pose-induced flow, which is forced to contain only all the 2D reprojections of the target’s 3D shape and reduces the matching space of the correlation map construction signif-icantly. Furthermore, we propose to use networks to learn the object pose based on the current flow prediction, which facilitates the computation of the pose-induced flow for the next iteration and also removes the necessity of explicit PnP solvers, making our system end-to-end trainable and more efficient, as shown in Fig. 2(b).
We evaluate our method on the challenging 6D object pose benchmarks, including LINEMOD [14], LINEMOD-Occluded [25], and YCB-V [50], and show that our method outperforms the state of the art significantly, and converges much more quickly. 2.