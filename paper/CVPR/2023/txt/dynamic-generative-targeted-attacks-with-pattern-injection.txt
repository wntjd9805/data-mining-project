Abstract
Adversarial attacks can evaluate model robustness and have been of great concern in recent years. Among various attacks, targeted attacks aim at misleading victim models to output adversary-desired predictions, which are more chal-lenging and threatening than untargeted ones. Existing tar-geted attacks can be roughly divided into instance-specific and instance-agnostic attacks.
Instance-specific attacks craft adversarial examples via iterative gradient updating on the specific instance. In contrast, instance-agnostic at-tacks learn a universal perturbation or a generative model on the global dataset to perform attacks. However, they rely too much on the classification boundary of substitute mod-els, ignoring the realistic distribution of the target class, which may result in limited targeted attack performance.
And there is no attempt to simultaneously combine the in-formation of the specific instance and the global dataset. To deal with these limitations, we first conduct an analysis via a causal graph and propose to craft transferable targeted adversarial examples by injecting target patterns. Based on this analysis, we introduce a generative attack model com-posed of a cross-attention guided convolution module and a pattern injection module. Concretely, the former adopts a dynamic convolution kernel and a static convolution ker-nel for the specific instance and the global dataset, respec-tively, which can inherit the advantages of both instance-specific and instance-agnostic attacks. And the pattern in-jection module utilizes a pattern prototype to encode target patterns, which can guide the generation of targeted adver-sarial examples. Besides, we also provide rigorous theoret-ical analysis to guarantee the effectiveness of our method.
Extensive experiments demonstrate that our method shows superior performance than 10 existing adversarial attacks against 13 models. 1.

Introduction
With the encouraging progress of deep neural networks (DNNs) in various fields [6, 19, 16, 44, 42], recent studies
∗ Equal Contribution
† Corresponding Author
Figure 1. Visualization comparison between adversarial exam-ples generated by our method (a) and the instance-specific method
MIM [9] (b). Our perturbations (a) not only show an underlying dependency with the input instance, but also have strong semantic patterns or styles of the target class (“Hippopotamus”). In contrast, the perturbations generated by MIM perform like random noises.
The adversarial examples are generated against ResNet-152, and labels are predicted by another unknown model (VGG-16).
[36, 29, 28, 38] have corroborated that adversarial examples generated with small-magnitude perturbations can mislead the DNNs to make incorrect predictions. Due to the vulner-ability of DNNs, adversarial attacks expose a security threat to real application systems based on deep neural networks, especially in some sensitive fields such as autonomous driv-ing [27], face verification and financial systems [43], to name a few. Therefore, adversarial attacks have become a research hotspot over the past decade [2, 48, 35, 3, 33], which are significant in demonstrating the adversarial ro-bustness and stability of deep learning models.
To further understand adversarial examples, there are tremendous works [1, 48, 33, 4, 36, 38] focusing on ad-versarial attacks. Recently, it has been found that adver-sarial examples possess an intriguing property of transfer-ability, which indicates that adversarial examples generated for a white-box model can also mislead another black-box model. Due to this inherent property, black-box attacks be-come workable in real-world scenarios where attackers can-not access the attacked model. Extensive methods, such as
MIM [9], DIM [53] and CD-AP [40], have made an impres-sive performance of boosting the transferability for untar-geted attacks, which mislead the model to make an incor-rect classification without specifying a target class. How-ever, targeted attacks are more challenging compared with untargeted attacks, making the model output the adversary-desired target class. It is claimed in recent works [14, 55] that transferable targeted attacks are more worthy of study because attackers can directly control the unknown model to output the desired prediction, which can expose huge threats to data privacy and security. Therefore, it is of great signif-icance to develop transferable targeted attacks.
Existing methods of transferable targeted attacks can be roughly categorized into instance-specific [9, 14, 13, 54, 50, 34, 31] and instance-agnostic [52, 35, 27, 40, 39] attacks. Specifically, almost all instance-specific attacks craft adversarial examples via iterative gradient updating, where attackers can only take advantage of the specific in-put instance, the white-box model and the target class la-bel. Instance-specific attacks rely on optimizing the classi-fication score of the adversary-desired class label to perturb the specific instance, which ignore the global data distri-bution. As a result, they inevitably lead to adversarial ex-amples over-fitting the white-box model and result in mod-est transferability of targeted attacks. On the other hand, via learning a universal perturbation [37] or a generative at-tack model [52, 55], instance-agnostic attacks optimize ad-versarial perturbations on the global data distribution rather than the specific instance. To a certain extent, they can al-leviate the problem of data-specific over-fitting and lead to more transferable targeted adversarial examples. However, taking the generative attack methods as an example, they (1) Most generative attacks suffer from two limitations.
[52, 40, 35] still rely on the target label and the classification boundary information of white-box models rather than the realistic data distribution of the target class. Consequently, it is claimed that most generative attacks still have the pos-sibility of over-fitting the white-box model, which may re-sult in limited performance of transferable targeted attacks. (2) Another limitation is that existing generative attacks
[39, 55, 27] apply the same network weights to every input instance in the test dataset. Nevertheless, it is considered that the shared network weights cannot stimulate the best attack performance of generative models [39, 55, 55]. Thus these aforementioned limitations have become the bottle-neck of developing transferable targeted attacks, to a certain extent.
To address the aforementioned limitations, in this pa-per we construct a causal graph to formulate the prediction process of classifiers, and analyze the origin of adversar-ial examples. Based on this analysis, we propose to gener-ate targeted adversarial examples via injecting the specific pattern or style of the target class. To this end, we intro-duce a generative attack model, which can not only inject pattern or style information of the target class to improve transferable targeted attacks, but also learn specialized con-volutional kernels for each input instance. Specifically, we designed a cross-attention guided convolution module and a pattern injection module in the proposed generative attack (1) The cross-attention guided convolution mod-model. ule consists of a static convolutional kernel and a dynamic convolutional kernel that is computed according to the in-put instance. Consequently, this static and dynamic mixup module can not only encode the global information of the dataset, but also learn specialized convolutional kernels for each input instance. This paradigm makes our generative model inherit the advantages of both instance-specific and instance-agnostic attacks. (2) The pattern injection module is designed to model the pattern or style information of the target class and guide the generation of targeted adversar-ial examples. Concretely, we propose a pattern prototype to learn a global pattern representation over images from the target class, and use the prototype to guide the generation of more transferable targeted adversarial examples. And the generated adversarial images of our method are presented in Figure 1. It is observed that our generated perturbations (as shown in Figure 1(a)) pose strong semantic patterns or styles of the target class and show an underlying depen-dency on the input instance. In contrast, the perturbations (as presented in Figure 1(b)) generated by MIM [9] per-form like random noises. Finally, to further understand our method, we provide rigorous theoretical analysis to guaran-tee the effectiveness of our method, as shown in Section 3.4, where we derive a concise conclusion based on the problem of Gaussian binary classification.
In summary, the main contributions of this paper are three-fold: (1) We propose a dynamic generative model to craft transferable targeted adversarial examples, which can not only inject pattern or style information of the target class to improve transferable targeted attacks, but also learn spe-cialized convolutional kernels for each input instance. Be-sides, our method inherits the advantages of both instance-specific and instance-agnostic attacks, and to the best of our (2) We state knowledge, we are the first to bridge them. that injecting the specific pattern or style of the target class can improve the transferability of targeted adversarial ex-amples, and we provide a comprehensive theoretical analy-sis to verify the rationality of this statement. (3) Extensive experimental results demonstrate that our method signifi-cantly boosts the transferability of targeted adversarial ex-amples over 10 existing methods against 13 models. 2.