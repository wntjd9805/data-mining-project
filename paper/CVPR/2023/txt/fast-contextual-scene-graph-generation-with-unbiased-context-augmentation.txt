Abstract
Scene graph generation (SGG) methods have histori-cally suffered from long-tail bias and slow inference speed.
In this paper, we notice that humans can analyze relation-ships between objects relying solely on context descriptions, and this abstract cognitive process may be guided by expe-rience. For example, given descriptions of cup and table with their spatial locations, humans can speculate possible relationships < cup, on, table > or < table, near, cup >.
Even without visual appearance information, some impossi-ble predicates like f lying in and looking at can be empiri-cally excluded. Accordingly, we propose a contextual scene graph generation (C-SGG) method without using visual in-formation and introduce a context augmentation method.
We propose that slight perturbations in the position and size of objects do not essentially affect the relationship between objects. Therefore, at the context level, we can produce di-verse context descriptions by using a context augmentation method based on the original dataset. These diverse context descriptions can be used for unbiased training of C-SGG to alleviate long-tail bias. In addition, we also introduce a context guided visual scene graph generation (CV-SGG) method, which leverages the C-SGG experience to guide vision to focus on possible predicates. Through extensive experiments on the publicly available dataset, C-SGG al-leviates long-tail bias and omits the huge computation of visual feature extraction to realize real-time SGG. CV-SGG achieves a great trade-off between common predicates and tail predicates. 1.

Introduction
SGG is a challenging technology that identifies triplet relationships < subject, predicate, object > between ob-jects from images. With the development of artificial intel-ligence, SGG has gradually become a bridge from image recognition to image understanding. Scene graphs are an
*Corresponding Author indispensable part of complex visual understanding tasks, such as visual question answering [24], visual grounding
[21] and visual-language navigation [40]. However, the re-searches [3, 19, 36]on SGG suffer from two insurmount-able obstacles. The first is the long-tail bias derived from datasets. More common predicates such as on and near have more samples than tail predicates such as from and above, causing the model to prefer to classify the common predicates. The second is the low-speed inference in prac-tical applications. Analyzing the predicate between each objects-pair to generate a scene graph is a quadratic time complexity problem, making real-time inference difficult.
On the one hand, some SGG methods [3, 26, 31, 34] are dedicated to solving the long-tail bias. Tang [26] and Chiou
[2] introduce the causal graph and the label frequency to reason tail predicates and attempt unbiased SGG inference based on biased training. Li [11] and Desai [3] propose to optimize label distribution and rebalance category sam-pling, which realize unbiased SGG training. However, these methods increase the recall of tail predicates, but inevitably reduce the recall of common predicates. The current SGG methods are difficult to consider and balance the recall of common predicates and tail predicates simultaneously. We think the internal reason is that there are not enough data samples for each predicate.
On the other hand, some SGG methods [18, 33] focus on improving the inference speed of the SGG task. In de-tail, Yang [33] designs all objects in a fully connected graph structure and prunes the connections between objects.
It can reduce the time complexity of SGG inference. Liu [18] transforms the predicate inference into an integral on rela-tionship affinity fields. Although the time complexity is not reduced, the computation amount of integral operation is much less than that of deep learning calculation of visual features. These methods improve the inference speed, but sacrifice the recall performance.
We reflect on the human cognitive process of predicate analysis between objects and discover two overlooked phe-nomena. First, humans can roughly infer the predicate be-tween objects based on the context descriptions only includ-Figure 1. A. The example of human speculate predicates based on the context description. B. (a) Use software to change objects in the image to produce fake images; (b) Project object pairs in the fake image to the context level. C. Examples of C-SGG outputs in different context descriptions. D. Examples of CV-SGG to further analyze high-confidence relationships and possible predicates. ing categories and positions. In other words, humans can speculate and analyze possible relationships through con-text descriptions even without seeing objects. As shown in
Fig.1 A, when humans know the subject man and the ob-ject bike, they can speculate and analyze which predicates are possible (’riding’, ’sitting on’ , ’near’) and which predi-cates are impossible (’wears’, ’flying in’, ’eating’) based on past experience. Second, when humans analyze the predi-cate between the objects-pair, the apparent features of the objects themself are not important. As shown in Fig.1 B (a), we use adobe photoshop software to move the human body and replace the style of the glasses, but the relation-ship < man, wears, glass > remains the same. Therefore, we argue that context features may be more important than visual features in rough predicates judgment.
Based on this thought, we weaken the role of vision in the SGG task and propose a contextual SGG (C-SGG) method with context augmentation. As we did in Fig.1 B (a), software such as photoshop can be used to modify the image by moving the position and replacing the like objects without changing the predicate. Different from traditional image augmentation, HSV variation and size scale chang-ing the entire image, this kind of image modification will change the shape and position of a certain object. However, using software to modify images is an extremely complex task. We project the modified image to the context level, as shown in Fig.1 B (b), which is the slight translation and scale of the object position with a cheap cost, and we named it context augmentation. In our C-SGG method, we only use context descriptions to predict predicates, and context augmentation can increase context description samples of any tail predicate for unbiased training. To some extent, through the context augmentation, during our training for
C-SGG, there are no two identical context descriptions. In addition, since there are no visual image features, we do not need complex computational models. Although it is still a quadratic time complexity task, the computation amount per object pair is extremely cheap.
Certainly, the C-SGG lacks the analysis of the visual in-teraction information between objects. We also propose a context guided visual SGG method (CV-SGG) to confirm truth predicates between object pairs further. As shown in Fig.1 C, C-SGG can roughly analyze the confidence in the existence of relationships between objects-pairs and the possible types of predicates. Our CV-SGG focuses on those high-confidence relationships and the high possible predi-cates. We use a simple visual model to extract visual fea-tures and fuse them with contextual features. During the training, we apply a ReLuL1 function and only calculate the loss on high possible predicates. In this way, CV-SGG only pays attention to possible predicates from C-SGG and ignores impossible predicates. As shown in Fig.1 D, con-text guided visual SGG is used to boost the truth predicate and suppress other possible but false predicates.
We validate our methods on the most common SGG dataset VG [10] and the latest SGG dataset PSG [32]. Our methods achieve the best balance between common pred-icates and tail predicates, and accomplish real-time SGG.
The contributions of this paper can be summarized as: 1) Inspired by the human cognitive process, we propose context augmentation to produce diverse context de-scriptions at the context level for unbiased training, which weakens the role of vision. 2) We propose two methods for SGG: C-SGG which only uses context descriptions and CV-SGG which guides vi-sual attention based on C-SGG results. 3) Based on extensive experiments on two SGG datasets
VG and PSG, our methods have obvious advantages in dealing with long-tail bias and inference speed. 2.