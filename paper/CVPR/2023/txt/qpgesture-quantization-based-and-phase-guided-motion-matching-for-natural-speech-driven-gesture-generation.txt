Abstract
Speech-driven gesture generation is highly challenging due to the random jitters of human motion.
In addition, there is an inherent asynchronous relationship between hu-man speech and gestures. To tackle these challenges, we in-troduce a novel quantization-based and phase-guided mo-tion matching framework. Specifically, we first present a gesture VQ-VAE module to learn a codebook to summa-rize meaningful gesture units. With each code representing a unique gesture, random jittering problems are alleviated effectively. We then use Levenshtein distance to align di-verse gestures with different speech. Levenshtein distance based on audio quantization as a similarity metric of cor-responding speech of gestures helps match more appropri-ate gestures with speech, and solves the alignment prob-lem of speech and gestures well. Moreover, we introduce phase to guide the optimal gesture matching based on the semantics of context or rhythm of audio. Phase guides when text-based or speech-based gestures should be performed to make the generated gestures more natural. Extensive experiments show that our method outperforms recent ap-proaches on speech-driven gesture generation. Our code, database, pre-trained models and demos are available at https://github.com/YoungSeng/QPGesture. 1.

Introduction
Nonverbal behavior plays a key role in conveying mes-sages in human communication [26], including facial ex-pressions, hand gestures and body gestures. Co-speech ges-ture helps better self-expression [45]. However, producing human-like and speech-appropriate gestures is still very dif-ficult due to two main challenges: 1) Random jittering:
People make many small jitters and movements when they speak, which can lead to a decrease in the quality of the gen-Figure 1. Gesture examples generated by our proposed method on various types of speech. The character is from Mixamo [2]. erated gestures. 2) Inherent asynchronicity with speech:
Unlike speech with face or lips, there is an inherent asyn-chronous relationship between human speech and gestures.
Most existing gesture generation studies intend to solve the two challenges in a single ingeniously designed neural network that directly maps speech to 3D joint sequence in high-dimensional continuous space [18, 24, 27, 31] using a sliding window with a fixed step size [17, 46, 47]. How-ever, such methods are limited by the representation power of proposed neural networks, like the GENEA gesture-generation challenge results. No system in GENEA chal-lenge 2020 [26] rated above a bottom line that paired the in-put speech audio with mismatched excerpts of training data motion. In GENEA challenge 2022 [48], a motion match-ing based method [50] ranked first in the human-likeness evaluation and upper-body appropriateness evaluation, and outperformed all other neural network-based models. These results indicate that motion matching based models, if de-signed properly, are more effective than neural network based models.
Inspired by this observation, in this work, we propose a novel quantization-based motion matching framework for audio-driven gesture generation. Our framework includes two main components aiming at solving the two above chal-lenges, respectively. First, to address the random jittering
Figure 2. Gesture generation pipeline of our proposed framework. is short for
‘candidate’. Given a piece of audio, text and seed pose, the audio and gesture are quantized. The candidate for the speech is calculated based on the Levenshtein distance, and the candidate for the text is calculated based on the cosine similarity. The optimal gesture is selected based on phase-guidance corresponding to the seed code and the phase corresponding to the two candidates. is short for ‘quantization’ and ‘Cand.’
‘Quan.’ challenge, we compress human gestures into a space that is lower dimensional and discrete, to reduce input redundancy.
Instead of manually indicating the gesture units [23], we use a vector quantized variational autoencoder (VQ-VAE) [42] to encode and quantize joint sequences to a codebook in an unsupervised manner, using a quantization bottleneck.
Each learned code is shown to represent a unique gesture pose. By reconstructing the discrete gestures, some ran-dom jittering problems such as grabbing hands and push-ing glasses will be solved. Second, to address the inherent asynchronicity of speech and gestures, Levenshtein distance
[28] is used based on audio quantization. Levenshtein dis-tance helps match more appropriate gestures with speech, and solves the alignment problem of speech and gestures well. Moreover, unlike the recent gesture matching mod-els [17, 50], we also consider the semantic information of the context. Third, since the body motion is composed of multiple periodic motions spatially, meanwhile the phase values are able to describe the nonlinear periodicity of the high-dimensional motion curves well [39], we use phase to guide how the gestures should be matched to speech and text.
The inference procedure of our framework is shown in
Figure 2. Given a piece of audio, text and seed pose, the audio and gesture are first quantized. The best candidate for the speech is calculated based on the Levenshtein dis-tance, and the best candidate for the text is calculated based on the cosine similarity. Then the most optimal gesture is selected based on the phase corresponding to the seed code and the phase corresponding to the two candidates. Our code, database, pre-trained models and demos will be pub-licly available soon.
The main contributions of our work are:
• We present a novel quantization-based motion match-ing framework for speech-driven gesture generation.
• We propose to align diverse gestures with different speech using Levenshtein distance, based on audio quantization.
• We design a phase guidance strategy to select optimal audio and text candidates for motion matching.
• Extensive experiments show that jittering and asyn-chronicity issues can be effectively alleviated by our framework. 2.