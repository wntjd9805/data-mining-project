Abstract
Despite neural implicit representations demonstrating impressive high-quality view synthesis capacity, decom-posing such representations into objects for instance-level editing is still challenging. Recent works learn object-compositional representations supervised by ground truth instance annotations and produce promising scene editing results. However, ground truth annotations are manually labeled and expensive in practice, which limits their usage in real-world scenes. In this work, we attempt to learn an object-compositional neural implicit representation for ed-itable scene rendering by leveraging labels inferred from the off-the-shelf 2D panoptic segmentation networks in-stead of the ground truth annotations. We propose a novel framework named Panoptic Compositional Feature Field (PCFF), which introduces an instance quadruplet metric learning to build a discriminating panoptic feature space for reliable scene editing. In addition, we propose semantic-related strategies to further exploit the correlations between semantic and appearance attributes for achieving better rendering results. Experiments on multiple scene datasets including ScanNet, Replica, and ToyDesk demonstrate that our proposed method achieves superior performance for novel view synthesis and produces convincing real-world scene editing results. 1.

Introduction
Virtually editing real-world scenes (e.g., moving a chair in the room) in mixed reality applications on various de-vices is desired by users. Such expectation requires an effective 3D scene representation with the capacity of photo-realistic view rendering and promising scene decom-position. Recently, emerging neural implicit representa-tions with volumetric rendering, especially neural radiance field (NeRF) [29] and its variants, show impressive re-sults in novel view synthesis [1, 2] and scene reconstruc-This work was supported in part by the Shenzhen General Research
Project JCYJ20220531093215035. (Corresponding author: Jian Zhang.) tion [13, 31, 43] tasks. However, decomposing a neural im-plicit representation into objects for scene editing is chal-lenging due to the holistic scene is implicitly encoded as weights of connectionist networks like multi-layer percep-tions (MLPs). To build object-compositional neural im-plicit representations for instance-level scene editing, sev-eral works [46, 47, 52] jointly encode the appearance and instance attributes with extra instance annotations.
Though existing object-compositional methods can ex-tract convincing object representations from the scene rep-resentation for further editing, their successes rely heav-ily on ground truth instance annotations, which are la-beled manually and expensive to obtain in real-world prac-tice. An intuitive alternative solution is training object-compositional representations with labels inferred by 2D panoptic segmentation networks [5, 6, 19] instead of ground truth instance annotations. However, their methods are tough to leverage the network-inferred labels due to the sig-nificant 3D index inconsistency, and a detailed discussion is shown in Fig. 1. We note that 3D index consistency is the instance indices of a specific object are same across multi-view labels. Due to network-inferred labels are individually predicted on each view image and objects order is uncertain in each prediction, the instance indices of a specific object in different view labels are usually index inconsistent from the perspective of 3D, e.g., the index of the target chair is purple in the label of view #1 and is red in the label of view
#2. Therefore, how to learn object-compositional neural im-plicit representations by leveraging network-inferred labels is critical for real-world application.
In this work, we propose a novel panoptic compositional feature field (PCFF), which integrates the deep metric learn-ing [18, 27] into the learning of object-compositional rep-resentations to overcome the challenge of using 2D net-work predictions. Concretely, we employ metric learning to constrain the distances among projected panoptic fea-tures of pixels in each view separately, which circumvents the requirement of 3D index consistent labels and builds a discriminating panoptic feature space. Combined with the
Figure 1. Core challenge. Existing methods (e.g., ObjSDF [46]) require (a) manually-labeled ground truth annotations to train object-compositional representations, and the trained representation can extract the target object correctly. We note that (a) manually-labeled annotations are 3D index consistency, i.e., the instance indices of the target object are same across multi-view labels. However, when (b) network-inferred labels predicted by 2D panoptic segmentation networks are utilized for training their representations, the corresponding object extraction result is obviously incorrect. Due to (b) network-inferred labels are inferred by networks on each view image individually, these labels are usually index inconsistent from the perspective of 3D and tough to be used by existing methods. feature spaces, we provide an easily query-based manner for scene decomposition, i.e., given a user-specified pixel query in an arbitrary view, our trained PCFF extracts the target object by measuring the similarity between the pro-jected feature of query pixel and corresponding features of each 3D point. Furthermore, two semantic-related learn-ing strategies are proposed based on our observation of the correlations between semantic and appearance attributes for improving our rendering performance. The semantic-appearance hierarchical learning enforces our framework to encode appearances and semantics with MLPs of different depths, and the semantic-guided regional refinement impels the framework to focus on inaccurate regions with the guid-ance of semantic information entropy maps.
We evaluate our method on multiple scene datasets in-cluding ScanNet [8], Replica [40], and ToyDesk [47]. Ex-periments demonstrate our method outperforms state-of-the-art methods in novel view synthesis. More impor-tantly, PCFF successfully leverages 2D network-inferred la-bels to build object-compositional representations for object extraction and real-world scene editing, whereas existing methods fail to utilize labels without 3D index consistency.
The main contributions of our work are summarized as:
• We propose a novel Panoptic Compositional Feature
Field (PCFF) that learns object-compositional repre-sentations for editable scene rendering with network-inferred panoptic labels by building a discriminating feature space with the assistance of introduced instance quadruplet metric learning.
• We propose strategies including semantic-appearance hierarchical learning and semantic-guided regional re-finement to properly exploit the correlations between semantic and appearance attributes and improve the rendering capacity of our method.
• Our method achieves superior novel view synthe-sis performance than state-of-the-art methods and produces convincing scene edits by using network-inferred panoptic labels on real-world scenes. 2.