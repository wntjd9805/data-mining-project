Abstract
The field of text-to-image generation has made remark-able strides in creating high-fidelity and photorealistic im-ages. As this technology gains popularity, there is a grow-ing concern about its potential security risks. However, there has been limited exploration into the robustness of these models from an adversarial perspective. Existing re-search has primarily focused on untargeted settings, and lacks holistic consideration for reliability (attack success rate) and stealthiness (imperceptibility).
In this paper, we propose RIATIG, a reliable and im-perceptible adversarial attack against text-to-image mod-els via inconspicuous examples. By formulating the exam-ple crafting as an optimization process and solving it using a genetic-based method, our proposed attack can generate imperceptible prompts for text-to-image generation models in a reliable way. Evaluation of six popular text-to-image generation models demonstrates the efficiency and stealthi-ness of our attack in both white-box and black-box settings.
To allow the community to build on top of our findings, we’ve made the artifacts available1. 1.

Introduction
The text-to-image generation has captured widespread attention from the research community with its creative and realistic image generation capability [52, 55, 56]. The abil-ity to generate text-consistent images from natural language descriptions could potentially bring tremendous benefits to many areas of life, such as multimedia editing, computer-aided design, and art creation [23, 27, 37, 43].
Driven by recent advances in models trained with large datasets [42, 57] and multimodal learning [45] (e.g., diffu-sion models [17]), text-to-image generation has made sig-nificant progress in synthesizing high-fidelity and photore-alistic images, such as DALL·E [43], DALL·E 2 [42] and
Imagen [45]. At the same time, there are a growing num-1Code is available at: https://github.com/WUSTL-CSPL/RIATIG
Figure 1. Examples of RIATIG attacks. The top texts are the adversarial prompts, and the bottom texts are the target models.
The first row represents the target images, while the second row represents the adversarial image generated by the prompt. ber of ethical concerns about the potential misuse of this technology [36,45,53]. Generative models could be used to generate synthetic video/audio/images of individuals (e.g.,
Deepfakes [14]), or synthetic contents with harmful stereo-types, violence, or obscenities [10, 45, 53]. To prevent the generation of such harmful content, content moderation fil-ters are deployed in public APIs (e.g., DALL·E 2) to filter unsafe text prompts that may lead to harmful content. How-ever, despite their best intentions, existing model-based text filters remain susceptible to adaptive adversarial attacks.
Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples [9, 31, 32]. By applying these techniques, it is possible to craft an adversarial text that looks natural to bypass the content filters, yet gener-ates a completely different category of potentially malicious images. However, the adversarial attacks on text-to-image generators are less explored. To the best of our knowledge, there are two closely related studies [15, 36]. Nevertheless, two challenges remain:
1) Reliability. One significant limitation is that the existing works [15, 36] do not offer a reliable method to find ad-versarial examples. [15] discovers that DALL·E 2 has cer-tain hidden vocabularies that can be used to generate images with some absurd (non-natural) prompts; however, this vo-cabulary is often limited and not stealthy (natural). Built on evocative prompting, [36] crafts adversarial examples via the morphological similarity between existing words. How-ever, it is very difficult to find texts with such linguistic sim-ilarity, and as a result, it can be challenging for this method to be adopted and generalized in different scenarios. 2) Stealthiness. The existing approaches can only craft ad-versarial examples that appear to be non-natural compared to normal texts or retain similar meanings, making them easily filtered and recognized by human examiners. For ex-ample, [15] crafts Apoploe vesrreaitais to represent bugs, and [36] crafts falaiscoglieklippantilado to represent cliff.
Also, [36] combines creepy and spooky into creepooky to generate an image that looks creepy and scary, yet the in-ferred meaning of this new word is highly related to the generated image, limiting its stealthiness.
To address these challenges, we propose RIATIG, a re-liable and imperceptible adversarial attack against text-to-image models using natural examples. To achieve this, we first formulate the generation of the adversarial examples as an optimization problem and apply genetic-based optimiza-tion methods to solve it, thus making our methods much more reliable in finding working adversarial examples. Fur-thermore, in order to improve the stealthiness, we propose a new text mutation technique to generate adversarial text that is visually and semantically similar to its normal ver-sion (some example results are shown in Figure 1).
RIATIG is evaluated on six popular text-to-image mod-els with both white-box and black-box attack settings. Ex-perimental results show that compared with the state-of-the-art text-to-image-oriented adversarial attacks, RIATIG demonstrates significantly better performance in terms of attack effectiveness and sample quality. Overall, the contri-butions of this work are summarized as follows:
• We are the first to systematically analyze the adver-sarial robustness of text-to-image generation models in both the white-box and black-box settings.
• We propose genetic-based optimization methods to find natural adversarial examples reliably.
• We evaluate our attacks on six popular text-to-image generation models and compare our attacks with five baselines. The evaluation results show that our meth-ods achieve a much higher success rate and sample quality, raising awareness of improving and securing the robustness of text-to-image models. 2.