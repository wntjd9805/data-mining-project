Abstract
We develop a novel method to remove injected backdoors in deep learning models. It works by cloning the benign behaviors of a trojaned model to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of im-portant neurons across the two models. The set of important neurons varies for each input, depending on their magni-tude of activations and their impact on the classification result. We theoretically show our method can better recover benign functions of the backdoor model. Meanwhile, we prove our method can be more effective in removing back-doors compared with fine-tuning. Our experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outper-forming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning.1 1.

Introduction
Backdoor attack is a prominent threat to applications of Deep Learning models. Misbehaviors, e.g., model mis-classification, can be induced by an input stamped with a backdoor trigger, such as a patch with a solid color or a pattern [14, 29]. A large number of various backdoor attacks have been proposed, targeting different modalities and fea-turing different attack methods [2, 22, 37, 44, 49, 55]. An important defense method is hence to remove backdoors from pre-trained models. For instance, Fine-prune proposed to remove backdoor by fine-tuning a pre-trained model on clean inputs [27]. Distillation [23] removes neurons that are not critical for benign functionalities. Model connectiv-ity repair (MCR) [56] interpolates weight parameters of a poisoned model and its finetuned version. ANP [50] adver-sarially perturbs neuron weights of a poisoned model and prunes those neurons that are sensitive to perturbations (and hence considered compromised). While these techniques are very effective in their targeted scenarios, they may fall short in some other attacks. For example, if a trojaned model is substantially robust, fine-tuning may not be able to remove the backdoor without lengthy retraining. Distillation may be-come less effective if a neuron is important for both normal functionalities and backdoor behaviors. And pruning neu-rons may degrade model benign accuracy. More discussions of related work can be found in Section 2.
In this paper, we propose a novel backdoor removal tech-nique MEDIC 2 as illustrated in Figure 1 (A). It works by cloning the benign functionalities of a pre-trained model to a new sanitized model of the same structure. Given a trojaned model and a small set of clean samples , MEDIC trains the clone model from scratch. The training is not only driven by the cross-entropy loss as in normal model training, but also by forcing the clone model to generate the same internal activation values as the original model at the correspond-ing internal neurons, i.e., steps 1⃝, 2⃝, and 4⃝ in Figure 1 (A). Intuitively, one can consider it essentially derives the weight parameters in the clone model by resolving the acti-vation equivalence constraints. There are a large number of such constraints even with just a small set of clean samples.
However, such faithful cloning likely copies the backdoor behaviors as well as it tends to generate the same set of weight values. Therefore, our cloning is further guided by importance (step 3⃝). Specifically, for each sample x, we only force the important neurons to have the same activation values. A neuron is considered important if (1) it tends to be substantially activated by the sample, when compared to its activation statistics over the entire population (the activation criterion in red in Figure 1 (A)), and (2) the large activation value has substantial impact on the classification result (the impact criterion) . The latter can be determined by analyz-ing the output gradient regarding the neuron activation. By constraining only the important neurons and relaxing the 1Code is available at https : / / github . com / qiulingxu / 2MEDIC stands for “Remove ModEl Backdoors via Importance DrIven
MEDIC
Cloning”.
others, the model focuses on cloning the behaviors related to the clean inputs and precludes the backdoor. The set of weight values derived by such cloning are largely different from those in the original model. Intuitively, it is like solving the same set of variables with only a subset of equivalence constraints. The solution tends to differ substantially from that by solving the full set of constraints.
Example. Figure 2 shows an example by visualizing the in-ternal activations and importance values for a trojaned model on a public benchmark [35]. Image (a) shows a right-turn traffic sign stamped with a polygon trigger in yellow, which induces the classification result of stop sign. Image (c) visu-alizes the activation values for the trojaned image whereas (d) shows those for its clean version. The differences be-tween the two, visualized in (e), show that the neurons fall into the red box are critical to the backdoor behaviors. A faithful cloning method would copy the behaviors for these neurons (and hence the backdoor). In contrast, our method modulates the cloning process using importance values and hence precludes the backdoor. The last image shows the importance values with the bright points denoting important neurons and the dark ones unimportant. Observe that it pre-vents copying the behaviors of the compromised neurons for this example as they are unimportant. One may notice that the unimportant compromised neurons for this example may become important for another example. Our method naturally handles this using per-sample importance values.
□
Our technique is different from fine-tuning as it trains from scratch. It is also different from knowledge distilla-tion [25] shown in Figure 1 (B), which aims to copy be-haviors across different model structures and constrains log-its equivalence (and sometimes internal activation equiva-lence [53] as well). In contrast, by using the same model structure, we have a one-to-one mapping for individual neu-rons in the two models such that we can enforce strong constraints on internal equivalence. This allows us to copy behaviors with only a small set of clean samples.
We evaluate our technique on nine different kinds of backdoor attacks. We compare with five latest backdoor removal methods (details in related work). The results show our method can reduce the attack success rate to 8.5% on average with only 2% benign accuracy degradation. It con-sistently outperforms the other baselines by 25%. It usually takes 15 mins to sanitize a model. We have also conducted an adaptive attack in which the trigger is composed of ex-isting benign features in clean samples. It denotes the most adversary context for MEDIC . Our ablation study shows all the design choices are critical. For example, without using importance values, it can only reduce ASR to 36% on average.
In summary, our main contributions include the follow-ing.
• We propose a novel importance driven cloning method to remove backdoors. It only requires a small set of clean samples.
• We theoretically analyze the advantages of the cloning method.
• We empirically show that MEDIC outperforms the state-of-the-art methods. 2.