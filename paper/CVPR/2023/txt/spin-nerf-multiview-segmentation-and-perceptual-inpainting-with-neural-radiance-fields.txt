Abstract
Neural Radiance Fields (NeRFs) have emerged as a pop-ular approach for novel view synthesis. While NeRFs are quickly being adapted for a wider set of applications, intu-itively editing NeRF scenes is still an open challenge. One important editing task is the removal of unwanted objects from a 3D scene, such that the replaced region is visually plausible and consistent with its context. We refer to this task as 3D inpainting. In 3D, solutions must be both con-sistent across multiple views and geometrically valid.
In this paper, we propose a novel 3D inpainting method that addresses these challenges. Given a small set of posed im-ages and sparse annotations in a single input image, our framework ﬁrst rapidly obtains a 3D segmentation mask for a target object. Using the mask, a perceptual optimization-based approach is then introduced that leverages learned 2D image inpainters, distilling their information into 3D space, while ensuring view consistency. We also address the lack of a diverse benchmark for evaluating 3D scene inpainting methods by introducing a dataset comprised of
In particular, our dataset challenging real-world scenes. contains views of the same scene with and without a target object, enabling more principled benchmarking of the 3D inpainting task. We ﬁrst demonstrate the superiority of our approach on multiview segmentation, comparing to NeRF-based methods and 2D segmentation approaches. We then evaluate on the task of 3D inpainting, establishing state-of-the-art performance against other NeRF manipulation al-gorithms, as well as a strong 2D image inpainter baseline. 1.

Introduction
Neural rendering methods, especially Neural Radiance
Fields (NeRFs) [35], have recently emerged as a new modality for representing and reconstructing scenes [50], achieving impressive results for novel view synthesis. Sub-stantial research effort continues to focus on formulating more efﬁcient NeRFs (e.g., [6, 20, 43]), to make NeRFs more accessible in use-cases with more limited computa-tional resources. As NeRFs become more widely accessi-ble, the need for editing and manipulating the scenes rep-resented by NeRFs will continue to grow. One notable editing application is removing objects and inpainting the 3D scene, analogous to the well-studied 2D image inpaint-ing task [23]. However, several obstacles impede progress on this task, not only for the 3D inpainting process itself, but also in obtaining the input segmentation masks. First,
NeRF scenes are implicitly encoded within the neural map-ping weights, resulting in an entangled and uninterpretable representation that is non-trivial to manipulate (compared to, say, the explicit discretized form of 2D image arrays or meshes in 3D). Moreover, any attempt to inpaint a 3D scene must not only generate a perceptually realistic appearance in a single given view, but also preserve fundamental 3D properties, such as appearance consistency across views and geometric plausibility. Finally, to obtain masks for the tar-get object, it is more intuitive for most end users to interact with 2D images, rather than 3D interfaces; however, requir-ing annotations of multiple images (and maintaining view-consistent segments) is burdensome to users. An appealing alternative is to expect only a minimal set of annotations for a single view. This motivates a method capable of obtaining a view-consistent 3D segmentation mask of the object (for use in inpainting) from single-view sparse annotations.
In this paper, we address these challenges with an in-tegrated method that takes in multiview images of a scene, extracts a 3D mask with minimal user input, and ﬁts a NeRF to the masked images, such that the target object is replaced with plausible 3D appearance and geometry. Existing inter-active 2D segmentation methods do not consider the 3D as-pects of the problem (e.g., [42]), while current NeRF-based approaches are unable to use sparse annotations [76] to per-form well, or do not attain sufﬁcient accuracy [44]. Sim-ilarly, while some current NeRF manipulation algorithms allow object removal, they do not attempt to provide percep-tually realistic inpaintings of newly unveiled parts of space (e.g., [64]). To our knowledge, this is the ﬁrst approach that handles both interactive multiview segmentation and full 3D inpainting in a single framework.
Our technique leverages off-the-shelf, 3D-unaware mod-els for segmentation and inpainting, and transfers their out-puts to 3D space in a view-consistent manner. Building on the (2D) interactive segmentation [8, 15, 33] literature, our framework starts from a small number of user-deﬁned image points on a target object (and a few negative sam-ples outside it). From these, our algorithm initializes masks with a video-based model [4], and lifts them into a coherent 3D segmentation via ﬁtting a semantic NeRF [36, 76, 77].
Then, after applying a pretrained 2D inpainter [48] to the multiview image set, a customized NeRF ﬁtting process is used to reconstruct the 3D inpainted scene, utilizing per-ceptual losses [72] to account for inconsistencies in the 2D inpainted images, as well as inpainted depth images to reg-ularize the geometry of the masked region. Overall, we pro-vide a complete method, from object selection to novel view synthesis of the inpainted scenes, in a uniﬁed framework with minimal burden on the user, illustrated in Figure 1.
We demonstrate the effectiveness of our approach through extensive qualitative and quantitative evaluations.
In addition, we address the lack of a benchmark for compar-ing scene inpainting methods, and introduce a new dataset where the “ground-truth inpaintings” (i.e., real images of the scene without the object) are available as well.
In summary, our contributions are as follows: (i) a com-plete process for 3D scene manipulation, starting from ob-ject selection with minimal user interaction and ending with a 3D inpainted NeRF scene; (ii) to perform such selec-tion, an extension of 2D segmentation models to the multi-view case, capable of recovering 3D-consistent masks from sparse annotations; (iii) to ensure view-consistency and per-ceptual plausibility, a novel optimization-based formulation of 3D inpainting in NeRFs, which leverages 2D inpainters; and (iv) a new dataset for 3D object removal evaluation that includes corresponding object-free ground-truth. 2.