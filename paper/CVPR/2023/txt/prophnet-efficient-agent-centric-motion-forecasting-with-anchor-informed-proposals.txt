Abstract
Motion forecasting is a key module in an autonomous driving system. Due to the heterogeneous nature of multi-sourced input, multimodality in agent behavior, and low la-tency required by onboard deployment, this task is notori-ously challenging. To cope with these difficulties, this paper proposes a novel agent-centric model with anchor-informed proposals for efficient multimodal motion prediction. We design a modality-agnostic strategy to concisely encode the complex input in a unified manner. We generate diverse proposals, fused with anchors bearing goal-oriented scene context, to induce multimodal prediction that covers a wide range of future trajectories. Our network architecture is highly uniform and succinct, leading to an efficient model amenable for real-world driving deployment. Experiments reveal that our agent-centric network compares favorably with the state-of-the-art methods in prediction accuracy, while achieving scene-centric level inference latency. 1.

Introduction
Predicting the future behaviors of various road partici-pants is an essential task for autonomous driving systems to be able to safely and comfortably operate in dynamic driving environments. However, motion forecasting is ex-tremely challenging in the sense that (i) the input consists of interrelated modalities from multiple sources; (ii) the output is inherently stochastic and multimodal; and (iii) the whole prediction pipeline must fulfill tight run time requirements with a limited computation budget.
A motion forecasting model typically collects the com-prehensive information from perception signals and high-definition (HD) maps, such as traffic light states, motion history of agents, and the road graph [11, 13, 14, 23]. Such a collection of information is a heterogeneous mix of static and dynamic, as well as discrete and continuous elements.
Moreover, there exist complex semantic relations between these components, including agent-to-agent, agent-to-road,
*Corresponding author xiaodong@qcraft.ai
Figure 1. Overview of accuracy-andâ€“latency trade-off for the task of motion forecasting on Argoverse-1. ProphNet outperforms the state-of-the-art methods in prediction accuracy and considerably speeds up the agent-centric inference latency, leading to the best balance between accuracy and latency. and road-to-road interactions. Previous methods in the field usually model this diverse set of input via an equally intri-cate system with modality-specific designs. LaneGCN [9] adopts four sub-networks to separately process the inter-actions of agent-to-agent, agent-to-road, road-to-agent and road-to-road. MultiPath++ is developed in [21] to employ multi-context gating to first capture the interactions between a target agent and other agents, and then fuse them with the map in a hierarchical manner.
Due to the unknown intent of an agent, motion predic-tion output is highly multimodal by nature. It is often im-possible to assert with full certainty whether a vehicle will go straight or turn right as it approaches an intersection.
The motion prediction model is therefore required to accu-rately model the underlying probability distribution of fu-ture behaviors. Recently, there have been various efforts on improving output multimodality. TNT [30] introduces three stages including goal (endpoint of a predicted trajec-tory) prediction, goal-conditioned motion estimation, and trajectory scoring. DenseTNT is proposed in [7] to improve upon the former by predicting from dense goal candidates
and conducting an iterative offline optimization algorithm to generate multi-future pseudo labels. A region based train-ing strategy is developed in mmTransformer [12] to manu-ally partition a scene into subregions to enforce predictions to fall into different regions to promote multimodality.
In a practical autonomous driving system, the whole pre-diction pipeline typically operates at a frequency of 10Hz, including (i) feature extraction from upstream modules, (ii) network inference, and (iii) post-processing. Therefore, a motion forecasting network is required to meet the strict inference latency constraint to be useful in the real-world setting. While the recent agent-centric paradigm [15, 21] is trending and has made remarkable progress on improving accuracy, its latency is dramatically increased compared to its scene-centric counterpart [5, 12], as shown in Figure 1.
This is because agent-centric models compute scene repre-sentations with respect to each agent separately, meaning the amount of features to process is dozens of times larger than that with scene-centric models. This substantially in-creases the latency of the whole prediction pipeline and is computationally prohibitive for onboard deployment.
In light of these observations, we present ProphNet: an efficient agent-centric motion forecasting model that fuses heterogeneous input in a unified manner and enhances mul-timodal output via a design of anchor-informed proposals.
In contrast to the existing complex modality-specific pro-cessing [9, 21], we develop a modality-agnostic architec-ture that combines the features of agent history, agent rela-tion and road graph as the agent-centric scene representa-tion (AcSR), which directly feeds to a unified self-attention encoder [22].
In this way, we motivate the self-attention network to learn the complex interactions with minimum in-ductive bias. Additionally, we propose the anchor-informed proposals (AiP) in an end-to-end learning fashion to induce output multimodality. In our approach, proposals are the future trajectory embeddings conjectured exclusively from agent history, and anchors refer to the goal embeddings learned from AcSR. In such a manner, the network learns to first generate proposals to maximize diversity without environmental restrictions, and then select and refine after absorbing anchors that carry rich goal-oriented contextual information. Based on random combinations of AiP, we further introduce the hydra prediction heads to encourage the network to learn complementary information and mean-while perform ensembling readily.
As illustrated in Figure 2, this design formulates a suc-cinct network with high efficiency in terms of both archi-tecture and inference. Instead of employing a self-attention encoder for each input modality [9,12], the self-attention on
AcSR unifies the learning of intra- and inter-modality inter-actions in a single compact space, which allows the network to assign associations within and across input modalities with maximum flexibility. Our model also considerably re-duces inference latency and performs prediction with a peak latency as low as 28ms. In addition, rather than heuristically predefining or selecting goals to strengthen output multi-modality [1, 7, 30], AiP is learned end-to-end to infuse goal based anchors to proposals that are deliberately produced with diverse multimodality. In the end, together with the hydra prediction heads, our network is capable of generat-ing future trajectories with rich variety.
Our main contributions are summarized as follows. First, we develop an input-source-agnostic strategy based on
AcSR to model heterogeneous input and simplify network architecture, making the model amenable for real-world driving deployment. Second, we propose a novel frame-work that couples proposal and anchor learning end-to-end through AiP to promote output multimodality. Third, we in-troduce hydra prediction heads to learn complementary pre-diction and explore ensembling. Fourth, we show that our network, as an agent-centric model, achieves state-of-the-art accuracy on the popular benchmarks while maintaining scene-centric level low inference latency. 2.