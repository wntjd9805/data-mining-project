Abstract
Deep learning with noisy labels is challenging and in-evitable in many circumstances. Existing methods reduce the impact of mislabeled samples by reducing loss weights or screening, which highly rely on the model’s superior discriminative power for identifying mislabeled samples.
However, in the training stage, the trainee model is im-perfect and will wrongly predict some mislabeled samples, which cause continuous damage to the model training. Con-sequently, there is a large performance gap between exist-ing anti-noise models trained with noisy samples and mod-els trained with clean samples. In this paper, we put forward a Gradient Switching Strategy (GSS) to prevent the contin-uous damage of mislabeled samples to the classifier. Theo-retical analysis shows that the damage comes from the mis-leading gradient direction computed from the mislabeled samples. The trainee model will deviate from the correct optimization direction under the influence of the accumu-lated misleading gradient of mislabeled samples. To ad-dress this problem, the proposed GSS alleviates the damage by switching the gradient direction of each sample based on the gradient direction pool, which contains all-class gradi-ent directions with different probabilities. During training, each gradient direction pool is updated iteratively, which assigns higher probabilities to potential principal direc-tions for high-confidence samples. Conversely, uncertain samples are forced to explore in different directions rather than mislead model in a fixed direction. Extensive experi-ments show that GSS can achieve comparable performance with a model trained with clean data. Moreover, the pro-posed GSS is pluggable for existing frameworks. This idea of switching gradient directions provides a new perspective for future noisy-label learning. 1.

Introduction
Recently, Deep Neural Networks (DNNs) have achieved breakthrough results across various computer vision tasks [9, 14–16, 20, 34, 36, 49, 50]. The high performance
*Li Sun is the corresponding author.
Figure 1. Performance comparison of existing methods and the proposed GSS on CIFAR-10 with 40% noisy labels. The red dashed line denotes the upper limit, which is the accuracy of mod-els trained with completely clean labels. of DNNs requires a large amount of labeled data, but it is hard to guarantee label quality in many circumstances. As a matter of fact, many benchmark datasets inevitably contain noisy labels according to investigation results in [35].
Various types of researches are proposed to address the noisy-label problem. The mainstream types are robust loss function [18, 27, 44, 54] and sample screening [30, 37, 40].
These methods deal with mislabeled samples in essen-tially similar ways, that is, by decreasing the weights of low-confidence samples, which highly rely on the trainee model’s discriminative power of identifying mislabeled samples. However, during training the trainee model is imperfect and will miss many mislabeled samples, which will continuously damage the model. That is why there is a large performance gap between existing anti-noise models trained with noisy samples and models trained with clean samples. As shown in Fig. 1, existing anti-noise meth-ods (denoted by solid lines) have 1.61% ∼ 9.81% accu-racy gaps compared with the reference upper limit (red dash line), which denotes the performance of models trained with
It raises an important question: how to clean samples. prevent the continuous damage of noises to model train-ing? The theoretical analysis in Section 4 shows that the noise damage comes from the misleading gradient direc-tions caused by noises. Therefore, it is a viable solution to handle the continuous damage of noises to model training by eliminating the impact of misleading gradient directions.
In this paper, we put forward a Gradient Switching Strat-egy (GSS) to prevent the continuous damage of mislabeled samples to the model training. The core idea is assigning a random gradient direction to cancel out the negative im-pact of mislabeled samples, especially for uncertain sam-ples which could continuously generate a misleading gra-dient in a single direction. For high-confidence samples, the model will be optimized using their potential principal directions with a larger probability. As the model’s dis-criminative power grows over training time, parts of uncer-tain samples will become high-confidence samples, which in turn optimizes the model with their potential principal di-rections. Finally, the model will be well-trained with almost all samples in the dataset step by step.
Specifically, we devise a gradient direction pool for each sample, which contains all-class gradient directions with different probabilities. The probabilities of different gra-dient directions are determined based on the original noisy label, predictions, and partial randomness. In the training stage, for uncertain samples, the probabilities of different gradient directions are dominated by randomness. The mul-tiple random gradient directions prevent a fixed misdirec-tion from continuously damaging the training.
The high-confidence samples consist of two groups: the predictions are consistent with original labels (consistent sample), and the predictions are not consistent with original labels (non-consistent sample). For consistent samples, the gradient direction of the original label (potential principal direction) has a higher probability than those for the remain-ing gradient directions. For non-consistent samples, two highest probabilities correspond to the gradient directions of the original label and model prediction. The model ex-plores two gradient directions and determines the potential principal direction during training. In summary, the poten-tial principal directions of high-confidence samples guide the optimization of the model.
Experiment results demonstrate that the proposed GSS can effectively prevent the damage of mislabeled samples to the model training. The proposed GSS is pluggable for existing frameworks for noisy-label learning, which can achieve 1.23% ∼ 9.22% accuracy improvement than SOTA for high noise rates. Additionally, the model with GSS trained on noisy samples can achieve comparable perfor-mance with models trained with clean samples.
Overall, our contributions are summarized as follows:
• This paper is the first to clarify the continuous damage of the mislabeled samples to model training. Theoreti-cal analysis shows the continuous damage comes from the misleading gradient direction derived from misla-beled samples, which provides a new perspective for future noisy-label learning research.
• We propose the Gradient Switching Strategy (GSS) to prevent the continuous gradient damage of mislabeled samples to the model training. A gradient direction pool containing gradient directions of all classes with dynamic probabilities for each sample is devised to al-leviate the impact of uncertain samples and optimize the model with the potential principal direction.
• Detailed theoretical analysis and extensive experimen-tal results show that the proposed GSS can effectively prevent damage of mislabeled samples. Through com-bining GSS with existing anti-noise learning methods, the final classification performance can achieve up to 1.23% ∼ 9.22% accuracy improvement over SOTA on datasets with severe noise, some of which are even comparable to the model trained with clean samples. 2.