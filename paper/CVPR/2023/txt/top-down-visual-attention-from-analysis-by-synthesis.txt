Abstract
Current attention algorithms (e.g., self-attention) are stimulus-driven and highlight all the salient objects in an image. However, intelligent agents like humans often guide their attention based on the high-level task at hand, fo-cusing only on task-related objects. This ability of task-guided top-down attention provides task-adaptive represen-tation and helps the model generalize to various tasks. In this paper, we consider top-down attention from a classic
Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a functional equivalence between visual at-tention and sparse reconstruction; we show that an AbS visual system that optimizes a similar sparse reconstruc-tion objective modulated by a goal-directed top-down sig-nal naturally simulates top-down attention. We further pro-pose Analysis-by-Synthesis Vision Transformer (AbSViT), which is a top-down modulated ViT model that variationally approximates AbS, and achieves controllable top-down at-tention. For real-world applications, AbSViT consistently improves over baselines on Vision-Language tasks such as VQA and zero-shot retrieval where language guides the top-down attention. AbSViT can also serve as a gen-eral backbone, improving performance on classification, se-mantic segmentation, and model robustness. Project page: https://sites.google.com/view/absvit. 1.

Introduction
Human visual attention is often task-guided, i.e., we tend to focus on different objects when processing different tasks [7, 69]. For example, when we answer different ques-tions about one image, we only attend to the objects that are relevant to the question (Fig. 1 (b-c)). This stands in contrast with the widely-used self-attention [17], which is completely stimulus-driven, i.e., it highlights all the salient objects in the image without task-guided selection (Fig. 1 (a)). While the stimulus-driven bottom-up attention has shown promising results in visual representation learning [6], current vision transformers still lack the ability of task-guided top-down attention, which provides task-adaptive representation and
Figure 1. Top-down vs. bottom-up attention. (a) Bottom-up attention is stimulus-driven, i.e., any salient objects (dog and cat) in the image may attract attention. (b-c) Top-down attention is task-guided. For example, when the task is to answer a question about a specific object, the attention will only center on that object and ignore the others. In this way, a more focused representation can be extracted for the current goal. potentially improves task-specific performances [1, 64, 65].
Although some algorithms of top-down attention are pro-posed in the literature [1, 9, 46, 64, 65], they are incompat-ible with self-attention-based transformers and principled and unified designs are still missing.
Previous work [5, 10, 34, 35, 50] has studied the mecha-nism of top-down attention in human vision systems, hypoth-esizing top-down attention is a result of the human visual sys-tem performing Analysis by Synthesis (AbS). AbS [32, 68] is a classic idea that suggests the human visual perception depends on both the input image and a high-level prior about the latent cause of the image, and different priors can lead to different ways to perceive the same image (e.g., visual illusion [33] and bistable perception [55]). This is formu-lated as Bayesian inference maxz p(h|z)p(z), where h is the input image, and z is the latent representation. It is hy-pothesized that the high-level goal can be formulated as a prior to direct the low-level recognition of different objects through AbS, achieving top-down attention. Still, existing works [10, 44, 67] are conceptual and hardly guide model designs in practice.
In this work, we present a novel perspective on how AbS entails top-down attention, followed by a new Analysis-by-Synthesis Vision Transformer (AbSViT) based on the find-ings. We start from previous work [56], which shows that visual attention (e.g., self-attention) is functionally equiva-lent to sparse reconstruction which reconstructs the input using a dictionary containing templates of separate objects in the input. We show that AbS optimizes a similar sparse re-construction objective modulated by a top-down signal. The top-down signal depends on the prior and acts as a preference on which object templates to choose to reconstruct the input.
Therefore, only the objects consistent with the high-level prior are selected, equivalent to top-down attention.
Inspired by the connection, we propose AbSViT, a
ViT [17] model with prior-conditioned top-down modulation trained to approximate AbS in a variational way. AbSViT contains a feedforward (encoding) and a feedback (decod-ing) pathway. The feedforward path is a regular ViT, and the feedback path contains linear decoders for each layer. Each inference starts with an initial feedforward run. The output tokens are manipulated by the prior and fed back through the decoders to each self-attention module as top-down input for the final feedforward pass (Fig. 3).
When only pretrained on ImageNet [15], which contains mostly single-object images, AbSViT can attend to different objects in multi-object scenes controllably. For real-world applications, we observe consistent improvements from Ab-SViT on Vision-Language tasks such as VQA [3] and zero-shot image retrieval, where language is used as a prior to guide attention. For tasks without a strong prior, such as
ImageNet classification and semantic segmentation, AbSViT can also serve as a general backbone and achieve substantial improvements. Additionally, the object-centric representa-tion resulting from the top-down attention design enables better generalization to corrupted, adversarial, and out-of-distribution images. We hope this work can encourage future exploration of task-guided attention designs and visual rep-resentation learning. 2.