Abstract
Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only
In this work, we investigate whether vision-pretraining. and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being under-performed by them on the NLU tasks, lending new context to previously mixed results regarding the NLU capabilities of multimodal models. We conclude that exposure to images during pretraining affords inherent visual reasoning knowl-edge that is reflected in language-only tasks that require im-plicit visual reasoning. Our findings bear importance in the broader context of multimodal learning, providing princi-pled guidelines for the choice of text encoders used in such contexts1. 1.

Introduction
Humans are multimodal learners. We communicate with each other about things that we have experienced and knowledge we have gained using our senses—most com-monly including sight as well as hearing, touch, smell, and taste. Our communication channel is limited to a single modality—spoken language, signed language, or text—but a reader or listener is expected to use his or her imagination to visualize and reason about the content being described.
In general, language is used to describe scenes, events, and images; the words used to describe these are used to con-jure up a visual impression in the listener. Therefore, it is natural to consider the types of visual reasoning used in un-derstanding language, and to ask how well we can currently 1Our code will be made available at https://isbertblind.
*These authors contributed equally to this work github.io/
model them with computational methods.
Consider, for instance, the questions in Figure 1. Con-creteness is typically correlated with how well a concept can be visually imagined. For example, a concrete word such as present often has a unique visual representation.
In addition, common associations such as ocean→blue (color) and corn chip→triangle (shape) reflect properties of an imagined visual representation of the item in ques-tion. These properties may be difficult to infer from text alone without prior knowledge gained from visual input; for instance, a number of studies have investigated the partial ability of blind English speakers to predict color associa-tions and how it differs from the intuition of sighted speak-ers2 [40, 50, 51, 54, 65].
There has been a wealth of recent research vision-and-language (V&L) tasks involving both text and image data, and the use of vision-language pretraining (VLP) to cre-ate models that are able to reason jointly about both of these modalities together [11, 12, 29, 35]. Notable in this regard is CLIP [46], consisting of paired text and image en-coders jointly trained on a contrastive objective, that learns to align text and image embeddings in a shared semantic space. On the other hand, text encoder models such as
BERT [14] learn to reason about text in a unimodal vac-uum, with knowledge derived from pretraining tasks that only involve textual data.
Prior work has investigated the performance of multi-modally trained text encoders on various natural language understanding (NLU) tasks with mixed results, sometimes finding that they are outperformed by unimodal models [22] and at other times suggesting improved performance [69].
However, these works fine-tune the models under consider-ation on NLU tasks before evaluation, making it difficult to disentangle the effects of multimodal pretraining and fine-tuning configuration on the observed performance. Addi-tionally, these works do not address the distinction between
NLU tasks requiring implicit visual reasoning and ones that are purely non-visual. We refer to natural language infer-ence involving implicit visual reasoning as visual language understanding (VLU) and propose a suite of VLU tasks that may be used to evaluate visual reasoning capabilities of pre-trained text encoders, focusing primarily on zero-shot meth-ods.
We compare multimodally trained text encoders such as that of CLIP to BERT and other unimodally trained text en-coders, evaluating their performance on our suite of VLU tasks. We evaluate these models in without modifying their internal weights in order to probe their knowledge obtained during pretraining. A key design aspect of these tests is the probing method used to evaluate knowledge. Previ-2This phenomenon is illustrated in this interview with Tommy Edison, a congenitally blind man, in which he describes his understanding and fre-quent confusion regarding color associations. ous work has probed the knowledge of BERT and sim-ilar models using a masked language modelling (MLM) paradigm [43, 48], but this cannot be directly applied to
CLIP since it was not pretrained with MLM. We there-fore propose a new zero-shot probing method that we term
Stroop probing. This is based on the psychological Stroop effect [39] (described in Section 3.2), which suggests that salient items should have a stronger interference effect on the representation of their context.
Strikingly, we find that the multimodally trained text en-coders under consideration outperform unimodally trained text encoders on VLU tasks, both when comparing to much larger encoders as well as ones of comparable size. We also compare these models on baseline NLU tasks that do not involve visual reasoning and find that models such as
CLIP underperform on these tasks, demonstrating that they do not have a global advantage on NLU tasks. We conclude that exposure to images during pretraining improves per-formance on text-only tasks that require visual reasoning.
Furthermore, our findings isolate the effect of the text com-ponent of multimodal models for tasks such as text to image generation, providing principled guidelines for understand-ing the knowledge that such models inject into downstream vision tasks. 2.