Abstract
Conditional image-to-video (cI2V) generation aims to synthesize a new plausible video starting from an image (e.g., a person’s face) and a condition (e.g., an action class label like smile). The key challenge of the cI2V task lies in the simultaneous generation of realistic spatial appear-ance and temporal dynamics corresponding to the given image and condition.
In this paper, we propose an ap-proach for cI2V using novel latent flow diffusion models (LFDM) that synthesize an optical flow sequence in the la-tent space based on the given condition to warp the given image. Compared to previous direct-synthesis-based works, our proposed LFDM can better synthesize spatial details and temporal motion by fully utilizing the spatial content of the given image and warping it in the latent space accord-ing to the generated temporally-coherent flow. The training of LFDM consists of two separate stages: (1) an unsuper-vised learning stage to train a latent flow auto-encoder for spatial content generation, including a flow predictor to es-timate latent flow between pairs of video frames, and (2) a conditional learning stage to train a 3D-UNet-based diffu-sion model (DM) for temporal latent flow generation. Un-like previous DMs operating in pixel space or latent feature space that couples spatial and temporal information, the
DM in our LFDM only needs to learn a low-dimensional latent flow space for motion generation, thus being more computationally efficient. We conduct comprehensive ex-periments on multiple datasets, where LFDM consistently outperforms prior arts. Furthermore, we show that LFDM can be easily adapted to new domains by simply finetun-ing the image decoder. Our code is available at https:
//github.com/nihaomiao/CVPR23_LFDM . 1.

Introduction
Image-to-video (I2V) generation is an appealing topic and has many potential applications, such as artistic cre-*Work done during the internship at NEC Laboratories America.
Figure 1. Examples of generated video frames and latent flow se-quences using our proposed LFDM. The first column shows the given images x0 and conditions y. The latent flow maps are back-ward optical flow to x0 in the latent space. We use the color coding scheme in [4] to visualize flow, where the color indicates the di-rection and magnitude of the flow. ation, entertainment and data augmentation for machine learning [30]. Given a single image x0 and a condition y, conditional image-to-video (cI2V) generation aims to synthesize a realistic video with frames 0 to k, ˆxK 0 =
{x0, ˆx1, . . . , ˆxK}, starting from the given frame x0 and sat-isfying the condition y. Similar to conditional image syn-thesis works [43, 82], most existing cI2V generation meth-ods [16, 19, 21, 30, 36, 77] directly synthesize each frame in the whole video based on the given image x0 and condition y. However, they often struggle with simultaneously pre-serving spatial details and keeping temporal coherence in the generated frames. In this paper, we propose novel latent
cial images from a new domain and generate better spatial details if the image decoder is finetuned.
During inference, as Fig 2 shows, we first adopt the DM trained in stage two to generate a latent flow sequence ˆf K 1 conditioned on y and given image x0. To generate the oc-cluded regions in new frames, the DM also produces an oc-clusion map sequence ˆmK 1 . Then image x0 is warped with
ˆf K 1 and ˆmK 1 in the latent space frame-by-frame to generate the video ˆxK 1 . By keeping warping the given image x0 in-stead of previous synthesized frames, we can avoid artifact accumulation. More details will be introduced in Sec. 3.3.
Our contributions are summarized as follows:
• We propose novel latent flow diffusion models (LFDM) to achieve image-to-video generation by syn-thesizing a temporally-coherent flow sequence in the latent space based on the given condition to warp the given image. To the best of our knowledge, we are the first to apply diffusion models to generate latent flow for conditional image-to-video tasks.
• A novel two-stage training strategy is proposed for
LFDM to decouple the generation of spatial content and temporal dynamics, which includes training a la-tent flow auto-encoder in stage one and a conditional 3D U-Net based diffusion model in stage two. This disentangled training process also enables LFDM to be easily adapted to new domains.
• We conduct extensive experiments on multiple datasets, including videos of facial expression, human action and gesture, where our proposed LFDM consis-tently outperforms previous state-of-the-art methods. 2.