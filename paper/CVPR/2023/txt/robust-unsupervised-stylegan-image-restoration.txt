Abstract
GAN-based image restoration inverts the generative process to repair images corrupted by known degrada-tions. Existing unsupervised methods must be carefully tuned for each task and degradation level. In this work, we make StyleGAN image restoration robust: a single set of hyperparameters works across a wide range of degradation levels. This makes it possible to handle combinations of several degradations, without the need to retune. Our proposed approach relies on a 3-phase progressive latent space extension and a conservative optimizer, which avoids the need for any additional reg-ularization terms. Extensive experiments demonstrate robustness on inpainting, upsampling, denoising, and deartifacting at varying degradations levels, outperform-ing other StyleGAN-based inversion techniques. Our approach also favorably compares to diffusion-based restoration by yielding much more realistic inversion results. Code is available at the above URL. 1.

Introduction
Image restoration, the task of recovering a high quality image from a degraded input, is a long-standing problem in image processing and computer vision. Since differ-ent restoration tasks—such as denoising, upsampling, deartifacting, etc.—can be quite distinct, many recent approaches [9, 10, 37, 54, 61, 62] propose to solve them in a supervised learning paradigm by leveraging curated datasets speciﬁcally designed for the task at hand. Un-fortunately, designing task-speciﬁc approaches requires retraining large networks on each task separately.
In parallel, the advent of powerful generative models has enabled the emergence of unsupervised restoration methods [42], which do not require task-speciﬁc training.
The idea is to invert the generative process to recover a clean image. Assuming a known (or approximate) degradation model, the optimization procedure there-fore attempts to recover an image that both: 1) closely matches the target degraded image after undergoing a similar degradation model (ﬁdelity); and 2) lies in the space of realistic images learned by the GAN (realism).
In the recent literature, StyleGAN [29–31] has been found to be particularly effective for unsupervised image restoration [13–15, 39, 40] because of the elegant design of its latent space. Indeed, these approaches leverage style inversion techniques [3, 4] to solve for a latent vec-tor that, when given to the generator, creates an image close to the degraded target. Unfortunately, this only works when such a match actually exists in the model distribution, which is rarely the case in practice. Hence, effective methods extend the learned latent space to cre-1
ate additional degrees of freedom to admit more images; this creates the need for additional regularization losses.
Hyperparameters must therefore carefully be tuned for each speciﬁc task and degradation level.
In this work, we make unsupervised StyleGAN image restoration-by-inversion robust to the type and intensity of degradations. Our proposed method employs the same hyperparameters across all tasks and levels and does not rely on any regularization loss. Our approach leans on two key ideas. First, we rely on a 3-phase progressive latent space extension: we begin by optimizing over the learned (global) latent space, then expand it across in-dividual layers of the generator, and ﬁnally expand it further across individual ﬁlters—where optimization at each phase is initialized with the result of the previous one. Second, we rely on a conservative, normalized gra-dient descent (NGD) [58] optimizer which is naturally constrained to stay close to its initial point compared to more sophisticated approaches such as Adam [33]. This combination of prudent optimization over a progressively richer latent space avoids additional regularization terms altogether and keep the overall procedure simple and constant across all tasks. We evaluate our method on upsampling, inpainting, denoising and deartifacting on a wide range of degradation levels, achieving state-of-the-art results on most scenarios even when baselines are optimized on each independently. We also show that our approach outperforms existing techniques on composi-tions of these tasks without changing hyperparameters.
• We propose a robust 3-phase StyleGAN image restora-tion framework. Our optimization technique maintains: 1) strong realism when degradations level are high; and 2) high ﬁdelity when they are low. Our method is fully unsupervised, requires no per-task training, and can handle different tasks at different levels without having to adjust hyperparameters.
• We demonstrate the effectiveness of the proposed method under diverse and composed degradations.
We develop a benchmark of synthetic image restora-tion tasks—making their degradation levels easy to control—with care taken to avoid unrealistic assump-tions. Our method outperforms existing unsuper-vised [13, 40] and diffusion-based [32] approaches. 2.