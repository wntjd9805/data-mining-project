Abstract
Prior works on improving speech quality with visual in-put typically study each type of auditory distortion sepa-rately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regenera-tion, where the goal is not to reconstruct the exact refer-ence clean signal, but to focus on improving certain as-pects of speech while not necessarily preserving the rest
In particular, this paper concerns intelli-such as voice. gibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is com-posed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly sup-presses noise and improves quality. Project page: https:
//wnhsu.github.io/ReVISE/. 1.

Introduction
Unlike anechoic studio recordings, speech in-the-wild is rarely clean: outdoor recordings are corrupted with all sorts of natural and non-natural sounds like wind and traf-fic noise [6]. Speech recorded indoor often contains rever-beration, mechanical noise, and overlapping speech from non-target speakers [54]. On top of those, recording de-vices and network may also introduce other types of dis-Figure 1. Illustration of AVSE with various distortion. tortion, such as amplitude clipping, band-pass filtering, and package loss [1]. Distortion makes it hard for both human and machines to comprehend speech [10, 31]. Improving the quality and the intelligibility of corrupted speech is es-sential for assistive listening and robust speech processing.
Generating clean speech signal based on its corrupted ver-sion is herein referred to as speech enhancement.
In speech enhancement, one line of research uses vi-sual speech to provide auxiliary information [14, 17, 19, 57], which is known as audio-visual speech enhancement.
Audio-visual speech (e.g., talking-head videos) can be seen as a multimodal view of the speech. Since visual modality is immune to acoustic noise, combining both views enables more robust estimation of shared generating factors such as textual content. Meanwhile, despite sharing the same goal of recovering corrupted speech, prior work often treats en-hancement from each type of distortion as a separate prob-lem: speech denoising and dereverberation addresses addi-tive and convolutive non-speech noises [17], speech separa-tion focuses on speech noises that exhibit similar character-istics to the target speech [19], speech inpainting aims to re-cover dropped audio frames [38], and video-to-speech syn-thesis is the extreme case of inpainting where all the frames are dropped [15, 36]. As a result, algorithms designed for one type of distortion may not be effective for another.
In this paper, we advocate a more holistic approach to audio-visual speech enhancement, where an algorithm 1
should be evaluated on all types of distortion, and a sin-gle model should also be effective on all types of corrupted data, shifting from building distortion-specific models to a universal model. Following [48], we coin the concept uni-versal speech enhancement. In turn, we also argue that exact reconstruction of the reference clean speech is not an appropriate objective especially when the level of distortion is high. To address the issue, we propose to relax the objec-tive and solve the generalized speech regeneration (GSR) problem: instead of focusing on exact reconstruction and measuring metrics like signal-to-noise ratios (SNRs), the goal of GSR is to enhance a predefined set of attributes, such as content intelligibility that can be measured by word error rates (WERs). In contrast, the model does not need to preserve other attributes.
This paper focuses on recovering intelligibility, syn-chronicity and quality. The task of improving those could be broken down into two steps: predicting the frame-level content and synthesizing high quality audio from it.
In-spired by the resemblance to audio-visual speech recogni-tion and speech synthesis, we propose ReVISE, short for
Resynthesis with Visual Input for Speech REgeneration.
ReVISE is composed of a pseudo audio-visual speech recognition model (P-AVSR) and a pseudo text-to-speech synthesis model (P-TTS); instead of using text as the out-put/input of the two models, self-supervised speech units that encode speech content [23, 44] are adopted to bridge them, making the system free of text supervision. Fur-thermore, observing the gain on speech recognition brought by self-supervised learning, we also initialize the P-AVSR with a self-supervised audio-visual speech model, AV-HuBERT [49], which significantly improves the perfor-mance, especially on low-resource setups.
To demonstrate the universality and compare with the literature, we construct four types of corrupted speech us-ing Lip-reading Sentences 3 (LRS3) [2] and AudioSet [20], including audio-visual denoising, separation, inpainting, and video-to-speech. Results suggest that ReVISE is the first model capable of high-quality in-the-wild video-to-speech synthesis, while prior models fail to produce intel-ligible content [21] or generate low-quality audio for in-the-wild videos [36]. Compared to a strong masking-based method [19] on denoising and separation, ReVISE achieves comparable performance on mid-/high-SNR conditions (0-20dB), and are significantly stronger on lower SNR con-ditions, reducing WERs by up to 37.5% absolute and im-proving MOS by up to 1.09. Finally, we also show that a single ReVISE model can tackle all four types of distor-tion with similar performance to distortion-specific mod-els. To further show the data efficiency and effectiveness of ReVISE on real data, we evaluate it on EasyCom [13], an audio-visual speech dataset addressing the cocktail party problem which contains clean close-talking recordings and noisy distant recordings with background noise, loud in-terfering speech, and room reverberation. Results show that ReVISE still shines in this challenging setup, reducing
WER by up to 32% while other methods fail. 2.