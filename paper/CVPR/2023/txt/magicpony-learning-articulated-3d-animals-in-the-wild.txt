Abstract
We consider the problem of predicting the 3D shape, ar-ticulation, viewpoint, texture, and lighting of an articulated animal like a horse given a single test image as input. We present a new method, dubbed MagicPony, that learns this predictor purely from in-the-wild single-view images of the object category, with minimal assumptions about the topol-ogy of deformation. At its core is an implicit-explicit repre-sentation of articulated shape and appearance, combining the strengths of neural fields and meshes. In order to help the model understand an object’s shape and pose, we distil the knowledge captured by an off-the-shelf self-supervised vision transformer and fuse it into the 3D model. To over-come local optima in viewpoint estimation, we further in-troduce a new viewpoint sampling scheme that comes at no additional training cost. MagicPony outperforms prior work on this challenging task and demonstrates excellent generalisation in reconstructing art, despite the fact that it is only trained on real images. The code can be found on the project page at https://3dmagicpony.github.io/. 1.

Introduction
Reconstructing the 3D shape of an object from a sin-gle image of it requires knowing a priori what are the possible shapes and appearances of the object. Learning
*Equal contribution. such a prior usually requires ad-hoc data acquisition se-tups [2, 20, 35, 36], involving at least multiple cameras, and often laser scanners, domes and other hardware, not to men-tion significant manual effort. This is viable for certain types of objects such as humans that are of particular in-terest in applications, but it is unlikely to scale to the long tail of objects that can appear in natural images. The alter-native is to learn a 3D prior from 2D images only, which are available in abundance. However, this prior is highly com-plex and must be learned while using it to reconstruct in 3D the 2D training data, which is a major challenge.
In this paper, we propose MagicPony, a novel approach to learning 3D models of articulated object categories such as horses and birds with only single-view input images for training. We leverage recent progress in unsupervised rep-resentation learning, unsupervised image matching, effi-cient implicit-explicit shape representations and neural ren-dering, and devise a new auto-encoder architecture that re-constructs the 3D shape, articulation and texture of each ob-ject instance from a single image. For training, we only require a 2D segmenter for the object category and a de-scription of the topology and symmetry of its 3D skeleton (i.e., the number and connectivity of bones). We do not require apriori knowledge of the objects’ 3D shapes, key-points, viewpoints, or of any other 2D or 3D cue which are often used in prior work [14, 21, 22, 31]. From this, we learn a function that, at test time, can estimate the shape and texture of a new object from a single image, in a feed-forward manner. The function exhibits remarkable gener-alisation properties, including reconstructing objects in ab-stract drawings, despite being trained on real images only.
In order to learn such disentangled 3D representations simply from raw images, MagicPony addresses a few key challenges. The first challenge is viewpoint estimation. Be-fore the 3D model is available, it is very difficult to assign a viewpoint to the objects in the training images. To tackle this problem, prior works have often assumed additional cues such as 2D keypoint correspondences [21, 22, 31]. In-stead, we avoid requiring additional keypoint supervision and implicitly infer noisy correspondences by fusing into the 3D model knowledge distilled from DINO-ViT [5], a self-supervised visual transformer network (ViT) [28]. We also develop a new efficient disambiguation scheme that explores multiple viewpoint assignment hypotheses at es-sentially no cost, avoiding local optima that are caused by greedily matching the noisy 2D correspondences.
The second challenge is how to represent the 3D shape, appearance and deformations of the object. Most prior works have used textured meshes [13, 14, 21, 31, 32, 67, 70], but these are difficult to optimise from scratch, leading to problems that often require ad-hoc heuristics such as re-meshing [13, 70]. The other and increasingly popular ap-proach is to use a volumetric representation such as a neu-ral radiance field [3, 40, 52, 63], which can model complex shapes, including manipulating their topology during train-ing. However, this modelling freedom comes at the cost of over-parametrisation, which is particularly problematic in monocular reconstruction and often leads to meaning-less short-cut solutions [63]. Furthermore, modelling ar-ticulation with a volumetric representation is difficult. A posing transformation is only defined for the object’s sur-face and interior and is more easily expressed from the canonical/pose-free space to the posed space. However, rendering a radiance field requires transforming 3D points off the object surface and in the direction opposite to the posing transformation, which is hard [8].
We address these issues by using a hybrid volumetric-mesh representation based on DMTet [42, 56]. Shape and appearance are defined volumetrically in canonical space, but a mesh is extracted on the fly for posing and rendering.
This sidesteps the challenges of using neural rendering di-rectly while retaining most of its advantages and enabling the use of powerful shape regularisers.
To summarise, we make the following contributions: (1) A new 3D object learning framework that combines re-cent advances in unsupervised learning, 3D representations and neural rendering, achieving better reconstruction results with less supervision; (2) An effective mechanism for fus-ing self-supervised features from DINO-ViT into the 3D model as a form of self-supervision; and (3) an efficient multi-hypothesis viewpoint prediction scheme that avoids local optima in reconstruction with no additional cost.
Table 1.