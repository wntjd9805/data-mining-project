Abstract
We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details un-der full disentangled control over camera poses, facial ex-pressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first ex-plicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, fol-lowed by a volume rendering step guided by the volumetric correspondence map to output into the observation space.
To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic de-tails compared to the state-of-the-art methods both qualita-tively and quantitatively. We also provide an ablation study to justify many of our system design choices. 1.

Introduction
Photo-realistic face image synthesis, editing and ani-mation attract significant interests in computer vision and graphics, with a wide range of important downstream appli-cations in visual effects, digital avatars, telepresence and many others. With the advent of Generative Adversar-ial Networks (GANs) [15], remarkable progress has been achieved in face image synthesis by StyleGAN [23–25]
as well as in semantic and style editing for face im-ages [46, 54]. To manipulate and animate the expressions and poses in face images, many methods attempted to lever-age 3D parametric face models, such as 3D Morphable
Models (3DMMs) [6, 40], with StyleGAN-based synthesis models [10, 41, 53]. However, all these methods operate on 2D convolutional networks (CNNs) without explicitly en-forcing the underlying 3D face structure. Therefore they cannot strictly maintain the 3D consistency when synthe-sizing faces under different poses and expressions.
Recently, a line of work has explored neural 3D rep-resentations by unsupervised training of 3D-aware GANs from in-the-wild unstructured images [7, 8, 11, 17, 37, 44, 45, 48, 61, 62, 67]. Among them, methods with generative
Neural Radiance Fields (NeRFs) [33] have demonstrated striking quality and multi-view-consistent image synthesis
[7, 11, 17, 37, 48]. The progress is largely due to the integra-tion of the power of StyleGAN in photo-realistic image syn-thesis and NeRF representation in 3D scene modeling with view-consistent volumetric rendering. Nevertheless, these methods lack precise 3D control over the generated faces beyond camera pose, as well as the quality and consistency in control over other attributes, such as shape, expression, neck and jaw pose, leave much to be desired.
In this work, we present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images. Our model can synthesize a wide range of 3D human heads with full control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled con-trol for 3D human head synthesis, we devise our model learning in two stages. We first define a novel semantic signed distance function (SDF) around a head geometry (i.e.
FLAME [29]) conditioned on its control parameters. This semantic SDF fully distills rich 3D geometric prior knowl-edge from the statistical FLAME model and allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. In the second training stage, we then leverage the state-of-the-art 3D GAN framework (EG3D [7]) to synthesize realistic shape and appearance of 3D human heads in the canonical space, including the mod-eling of hair and apparels. Following that, a volume render-ing step is guided by the volumetric correspondence map to output the geometry and image in the observation space.
To ensure the consistency of synthesized 3D head shape with controlling head geometry, we introduce a geometry prior loss to minimize the difference between the synthe-sized neural density field and the FLAME head SDF in ob-servation space. Furthermore, to improve the control ac-curacy, we pre-train an image encoder of the control pa-rameters and formulate a control loss to ensure synthesized images matching the input control code upon encoding. An-other key aspect of synthesis realism is dynamic details such as wrinkles and varying shading as subjects change expres-sions and poses. To synthesize dynamic details, we propose to condition EG3D’s triplane feature decoding with noised controlling expression.
Compare to state-of-the-art methods, our method achieves superior synthesized image quality in terms of
Frechet Inception Distance (FID) and Kernel Inception Dis-tance (KID). Our method can consistently preserve the iden-tity of synthesized subjects with compelling dynamic de-tails while changing expressions and poses, outperforming prior methods both quantitatively and qualitatively.
The contributions of our work can be summarized as:
• A novel geometry-guided 3D GAN framework for high-quality 3D head synthesis with full control on camera poses, facial expressions, head shapes, artic-ulated neck and jaw poses.
• A novel semantic SDF formulation that defines the vol-umetric correspondence map from observation space to canonical space and allows full disentanglement of control parameters in 3D GAN training.
• A geometric prior loss and a control loss to ensure the head shape and expression synthesis accuracy.
• A robust noised expression conditioning scheme to en-able dynamic detail synthesis. 2.