Abstract
In this paper, we study the application of Test-time domain adaptation in semantic segmentation (TTDA-Seg) where both efficiency and effectiveness are crucial. Exist-ing methods either have low efficiency (e.g., backward op-timization) or ignore semantic adaptation (e.g., distribution alignment). Besides, they would suffer from the accumu-lated errors caused by unstable optimization and abnormal distributions. To solve these problems, we propose a novel backward-free approach for TTDA-Seg, called Dynamically
Instance-Guided Adaptation (DIGA). Our principle is uti-lizing each instance to dynamically guide its own adapta-tion in a non-parametric way, which avoids the error ac-cumulation issue and expensive optimizing cost. Specifi-cally, DIGA is composed of a distribution adaptation mod-ule (DAM) and a semantic adaptation module (SAM), en-abling us to jointly adapt the model in two indispensable aspects. DAM mixes the instance and source BN statistics to
*Corresponding author encourage the model to capture robust representation. SAM combines the historical prototypes with instance-level pro-totypes to adjust semantic predictions, which can be asso-ciated with the parametric classifier to mutually benefit the final results. Extensive experiments evaluated on five target domains demonstrate the effectiveness and efficiency of the proposed method. Our DIGA establishes new state-of-the-art performance in TTDA-Seg. Source code is available at: https://github.com/Waybaba/DIGA. 1.

Introduction
Semantic segmentation (Seg) [3, 40, 45, 46, 49] is a fun-damental task in computer vision, which is an important step in the visual-based robot, autonomous driving and etc.
Modern deep-learning techniques have achieved impressive success in segmentation. However, one serious drawback of them is that the segmentation models trained on one dataset (source domain) may undergo catastrophic perfor-mance degradation when applied to another dataset sam-pled from a different distribution. This phenomenon will be even more serious under complex and ever-changing con-texts, e.g., autonomous driving.
To solve this well-known problem caused by domain shifts, researchers have devoted great effort to domain gen-eralization (DG) [6,11,18,19,21,30] and domain adaptation (DA) [23,47,47,50]. Specifically, DG aims to learn general-ized models with only labeled source data. Traditional DA attempts to adapt the model on the target domain by using both labeled source data and unlabeled target data. How-ever, both learning paradigms have their own disadvantages.
The performance of DG is limited especially when evalu-ated on a domain with a large gap from the source since it does not leverage target data [11]. DA assumes that the unlabeled target data are available in advance and can be chronically exploited to improve target performance. This assumption, however, can not always be satisfied in real-world applications. For example, when driving in a new city, the data are incoming sequentially and we expect the system to dynamically adapt to the ever-changing scenario.
To meet the real-world applications, [41] introduces the test-time domain adaptation (TTDA), which aims at adapt-ing the model during the testing phase in an online fashion (see Fig. 1 Top). Generally, existing methods can be divided into two categories: backward-based methods [1, 22, 27, 37, 41] and backward-free methods [15, 25, 28, 33]. The former category (see Fig. 1 (a)) focuses on optimizing the parame-ters of models with self-supervision losses, such as entropy loss [27, 41]. In this way, both distribution adaptation and semantic adaptation can be achieved, which however has the following drawbacks. (1) Low-Efficiency : Due to the requirement of back-propagation, the computation cost will be multiplied, leading to low efficiency. (2) Unstable Op-timization & Error Accumulation: Since the gradient is calculated with single sample by weak supervision, the ran-domness could be high thus leading to unstable optimiza-tion. Although this problem can be mitigated in some cer-tain by increasing the testing batch size, it still cannot be solved well. In such cases, the accumulated errors may lead the model to forget the original well-learned knowledge and thus cause performance degradation.
The second category aims to adapt the model in the dis-tribution level by updating statistics in batch normalization (BN) [25] layers, which is very efficient as it is directly im-plemented in forward propagation with a light computation cost. Instance normalization [28] (see Fig. 1 (b)) directly replaces the source statistics with those from each instance, which is sensitive to the target variations due to discard-ing the basic source knowledge and thus is unstable. Mirza et al [25] (see Fig. 1 (c)) study the impacts of updating the historical statistics by instance statistics with fixed mo-mentum or dynamically fluctuating momentum. However, these methods also suffer from the error accumulation is-Figure 2. Illustration of the implementation of our DIGA. Given a source model, our DIGA can be readily equipped with only access to the BN layers, classifier head and feature head. sue caused by abnormal target distributions as well as the neglect of semantic adaptation, both of which will result in inferior adaptation performance.
To this end, we propose a holistic approach (see
Fig. 1 (d)), called Dynamically Instance-Guided Adaptation (DIGA), for TTDA-Seg, which takes into account both ef-fectiveness and efficiency. The main idea of DIGA is lever-aging each instance to dynamically its own adaptation in a non-parametric manner, which is efficient and can largely avoid the error accumulation issue. In addition, our DIGA is implemented in a considerate manner by injecting with distribution adaptation module (DAM) and semantic adap-tation module (SAM). Specifically, in DAM, we compute the weighed sum of the source and current statistics in BN layers to adapt target distribution, which enables the model to obtain a more robust representation. In SAM, we build a dynamic non-parametric classifier by mixing the histori-cal prototypes with instance-level prototypes, enabling us
In addition, the non-to adjust the semantic prediction. parametric classifier can be associated with the parametric one, which can further benefit the adaptation results. Our contributions can be summarized as follows:
• Efficiency. We propose a backward-free approach for
TTDA-Seg, which can be implemented within one for-ward propagation with a light computation cost.
• Effectiveness. We introduce a considerate approach to adapt the model in both distribution and semantic aspects. In addition, our method takes the mutual ad-vantage of two types of classifiers to achieve further improvements.
• Usability. Our method is easy to implement and is model-agnostic, which can be readily injected into ex-isting models (see Fig.2).
• Promising Results. We conduct experiments on three source domains and five target domains based on driv-ing benchmarks and show that our method produces new state-of-the-art performance for TTDA-Seg. We also study the continual TTDA-Seg and verify the su-periority of our method in this challenging task.
2.