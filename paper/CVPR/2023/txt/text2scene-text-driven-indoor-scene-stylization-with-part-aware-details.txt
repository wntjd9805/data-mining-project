Abstract create detailed and realistic textures of the desired style that maintain structural context for scenes with multiple objects.
We propose Text2Scene, a method to automatically cre-ate realistic textures for virtual scenes composed of multiple objects. Guided by a reference image and text descriptions, our pipeline adds detailed texture on labeled 3D geome-tries in the room such that the generated colors respect the hierarchical structure or semantic parts that are often com-posed of similar materials. Instead of applying flat styliza-tion on the entire scene at a single step, we obtain weak semantic cues from geometric segmentation, which are fur-ther clarified by assigning initial colors to segmented parts.
Then we add texture details for individual objects such that their projections on image space exhibit feature embedding aligned with the embedding of the input. The decomposition makes the entire pipeline tractable to a moderate amount of computation resources and memory. As our framework uti-lizes the existing resources of image and text embedding, it does not require dedicated datasets with high-quality tex-tures designed by skillful artists. To the best of our knowl-edge, it is the first practical and scalable approach that can 1.

Introduction
Virtual spaces provide an immersive experience for metaverse, films, or games. With increasing demands for virtual environments, various applications seek practical methods to create realistic 3D scenes with high-quality tex-tures. Currently, skillful artists need to manually create 3D assets and accompanying textures with careful parameteri-zation, which is not scalable enough to account for the di-verse content the industry is heading for. Scenes can also be populated with existing 3D database models or created with recent shape-generation approaches using data-driven methods [33, 54]. However, most of them lack texture in-formation or are limited to simple coloring.
To build realistic content, we need fine details containing the artistic nuances of styles that obey the implicit correla-tions with geometric shapes and semantic structure. Recent works provide methods to color a single object with the help
Figure 2. Our scene stylization results. Given a target image It and the style text ts, Text2Scene can produce the stylized results for the entire scene. of differentiable rendering [5], but often they are limited to single texture or blurred boundaries [6, 21, 27, 29, 52]. More importantly, the 3D objects are often textured in isolation, and only limited attempts exist to add visual appearances for large-scale scenes with multiple objects [14, 15, 19]. The biggest challenge is adding consistent style for an entire scene, but still accounting for the boundaries of different materials due to the functional and semantic relationship between parts, as observed within real-world scenes.
Our proposed Text2Scene adds plausible texture details on 3D scenes without explicit part labels or large-scale data with complex texturing. We take inspiration from abundant 3D shape and image datasets and decompose the problem into sub-parts such that the entire scene can be processed with a commodity memory and computation. Given scenes of multiple objects of 3D mesh geometry, we separately handle walls and individual objects. Specifically, the styl-ization of walls is formulated as texture retrieval, and the objects are initialized with base colors. From the base color assignment, we can deduce the part-level relationship for stylization and further refine them in later stages, such that their rendered images are close to the input text within the joint embedding space of foundational models.
Our coarse-to-fine strategy keeps the problem tractable yet generates high-quality texture with clean part bound-aries. We first create segments of input mesh such that the segment boundaries align with low-level geometric cues.
Then we start with the simplified problem of assigning a
Interestingly, the prior obtained from color per segment. large-scale image datasets assign similar colors for the parts with similar textures, reflecting the semantic context or symmetry as shown in Figure 1. We add the detailed texture on individual objects as an additional perturbation on the assigned base colors by enforcing constraints on the image features of their projections. The additional perturbations are high-frequency neural fields added to the base color.
In summary, Text2Scene is a new method that
• can easily generate realistic texture colors of the scene with the desired style provided by text or an image;
• can add detailed texture that respects the semantic part boundaries of individual objects; and
• can process the entire scene without a large amount of textured 3D scenes or an extensive memory footprint.
We expect the proposed approach to enable everyday users to quickly populate virtual scenes of their choices, and en-joy the possibility of next-generation technology with high-quality visual renderings. 2.