Abstract 1.

Introduction
The artistic style within a painting is the means of ex-pression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes, including semantic elements and object shapes. Previous arbitrary example-guided artistic image generation meth-ods often fail to control shape changes or convey elements.
Pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality but often require extensive textual descriptions to accurately portray the at-tributes of a particular painting. The uniqueness of an art-work lies in the fact that it cannot be adequately explained with normal language. Our key idea is to learn the artis-tic style directly from a single painting and then guide the synthesis without providing complex textual descriptions.
Specifically, we perceive style as a learnable textual de-scription of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paint-ings of various artists and styles. Codes are available at https://github.com/zyxElsa/InST.
*Corresponding author: weiming.dong@ia.ac.cn
If a photo speaks 1000 words, then every painting tells a story. A painting contains the engagement of an artist’s own creation. The artistic style of a painting can be the per-sonalized textures and brushstrokes, the portrayed beautiful moment, or some particular semantic elements. All these artistic factors are difficult to describe with words. There-fore, when we wish to utilize a favorite painting to create new digital artworks that can imitate the original idea of the artist, the task turns into example-guided artistic image gen-eration.
Generating artistic image(s) from given example(s) has attracted great interest in recent years. A typical task is style transfer [1, 6, 8, 17, 34, 55, 59], which can create a new artistic image from an arbitrary input pair of natural images and painting image, by combining the content of the nat-ural image and the style of the painting image. However, particular artistic factors such as object shape and semantic elements are difficult to be transferred (see Figures 2(b) and 2(e)). Text guided stylization [14, 16, 29, 38] produces an artistic image from a natural image and a text prompt, but the text prompt for the target style can usually be a rough description only of the material (e.g., “oil”, “watercolor” or “sketch”), art movement(e.g. “Impressionism” or “Cu-bism”), artist (e.g., “Vincent van Gogh” or “Claude Monet”) or a famous artwork (e.g., “Starry Night” or “The Scream”).
Figure 2. Concept differences between text-to-image synthesis [50], classical style transfer [7, 59] and our InST.
Diffusion-based methods [9,22,23,26,36,54] generate high-quality and diverse artistic images on the basis of a text prompt, with or without image examples. In addition to the input image, a detailed auxiliary textual input is required to guide the generation process if we want to reproduce some vivid contents and styles. However, the creative idea of a specific painting may still be difficult to reproduce in the result.
In this paper, we propose a novel example-guided artis-tic image generation framework (i.e., inversion-based style transfer, InST) which related to style transfer and text-to-image synthesis, to mitigate all the above problems. Given only a single input painting image, our method can learn and transfer its style to a natural image with a very simple text prompt (see Figures 1 and 2(f)). The resulting image exhibits very similar artistic attributes to those of the orig-inal painting, including material, brushstrokes, colors, ob-ject shapes and semantic elements, without losing diversity.
Furthermore, the content of the resulting image can also be controlled by providing a text description (see Figure 2(c)).
To achieve this goal, we need to obtain the representa-tion of the image style, which refers to the set of attributes that appear in the high-level textual description of the im-age. We define the textual descriptions as “new words” that do not exist in the normal language and obtain the embed-dings via inversion method. We exploit the recent success of diffusion models [42, 50] and inversion [2, 15]. We adapt diffusion models in our work as the backbone to be inverted and as a generator in image-to-image and text-to-image syn-thesis. Specifically, we propose an efficient and accurate textual inversion based on the attention mechanism, which can quickly learn key features from an image, and a stochas-tic inversion to maintain the semantic of the content image.
We use CLIP [39] image encoder and learn key information of the image through multi-layer cross-attention. Taking an artistic image as a reference, the attention-based inversion module is fed with its image embedding and then gives its textual embedding. The diffusion models conditioning on the textual embedding can produce new images with the learned style of the reference.
To demonstrate the effectiveness of InST, we conduct comprehensive experiments on numerous images of various artists and styles. The experiments show that InST produces outstanding results, generating artistic images that imitate the style attributes to a high degree and with a content that is consistent with that of the input natural images or text de-scriptions. InST demonstrates much improved visual qual-ity and artistic consistency compared with state-of-the-art approaches. These outcomes indicate the generality, preci-sion, and adaptability of the proposed method. 2.