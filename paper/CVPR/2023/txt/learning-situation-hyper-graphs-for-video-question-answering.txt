Abstract
Answering questions about complex situations in videos requires not only capturing the presence of actors, objects, and their relations but also the evolution of these relation-ships over time. A situation hyper-graph is a representa-tion that describes situations as scene sub-graphs for video frames and hyper-edges for connected sub-graphs and has been proposed to capture all such information in a compact structured form. In this work, we propose an architecture for
Video Question Answering (VQA) that enables answering questions related to video content by predicting situation hyper-graphs, coined Situation Hyper-Graph based Video
Question Answering (SHG-VQA). To this end, we train a situation hyper-graph decoder to implicitly identify graph representations with actions and object/human-object rela-tionships from the input video clip. and to use cross-attention between the predicted situation hyper-graphs and the ques-tion embedding to predict the correct answer. The proposed method is trained in an end-to-end manner and optimized by a VQA loss with the cross-entropy function and a Hungarian matching loss for the situation graph prediction. The effec-tiveness of the proposed architecture is extensively evaluated on two challenging benchmarks: AGQA and STAR. Our results show that learning the underlying situation hyper-graphs helps the system to significantly improve its perfor-mance for novel challenges of video question-answering tasks1. 1.

Introduction
Video question answering in real-world scenarios is a challenging task as it requires focusing on several factors including the perception of the current scene, language un-derstanding, situated reasoning, and future prediction. Visual perception in the reasoning task requires capturing various aspects of visual understanding, e.g., detecting a diverse set 1Code will be available at https://github.com/aurooj/SHG-VQA
Figure 1. The situation hyper-graph for a video is composed of situations with entities and their relationships (shown as subgraphs in the pink box). These situations may evolve over time. Temporal actions act as hyper-edges connecting these situations into one situ-ation hyper-graph. Learning situation graphs, as well as temporal actions, is vital for reasoning-based video question answering. of entities, recognizing their interactions, as well as under-standing the changing dynamics between these entities over time. Similarly, linguistic understanding has its challenges as some question or answer concepts may not be present in the input text or video.
Visual question answering, as well as its extension over time, video question answering, have both benefited from representing knowledge in graph structures, e.g., scene graphs [20, 35], spatio-temporal graphs [4, 55], and knowl-edge graphs [36, 45]. Another approach in this direction is the re-introduction of the concept of “situation cognition” embodied in “situation hyper-graphs” [47]. This adds the computation of actions to the graphs that capture the interac-tion between entities. In this case, situations are represented by hyper-graphs that join atomic entities and relations (e.g., agents, objects, and relationships) with their actions (Fig. 1). This is an ambitious task for existing systems as it is impractical to encapsulate all possible interactions in the real-world context.
Recent work [23] shows that transformers are capable
of learning graphs without adapting graph-specific details in the architectures achieving competitive or even better performance than sophisticated graph-specific models. Our work supports this idea by implicitly learning the underlying hyper-graphs of a video. Thus, it requires no graph compu-tation for inference and uses decoder’s output directly for cross attention module. More precisely, we propose to learn situation hyper-graphs, namely framewise actor-object and object-object relations as well as their respective actions, from the input video directly without the need for explicit object detection or other required prior knowledge. While the actions capture events across transitions over multiple frames, such as Drinking from a bottle, the relationship en-coding actually considers all possible combinations of static, single frame actor-object, and object-object relationships as unique classes, e.g., in the form of person – hold – bottle or bottle – stands on – table, thus serving as an object and relation classifier. Leveraging this setup allows us to stream-line the spatio-temporal graph learning as a set prediction task for predicting relationship predicates and actions in a
Situation hyper-graph Decoder block. To train the Situation
Graph Decoder, we use a bipartite matching loss between the predicted set and ground truth hyper-graph tokens. The output of the situation graph decoder is a set of action and relationship tokens, which are then combined with the em-bedding of the associated question to derive the final answer.
An overview of the proposed architecture is given in Fig. 2.
Note that, compared to other works targeting video scene graph generation, e.g., those listed in [63], we are less fo-cused on learning the best possible scene graph, but rather on learning the representation of the scene which best sup-ports the question answering task. Thus, while capturing the essence of a scene, as well as the transition from one scene to the other, we are not only optimizing the scene graph accuracy but also considering the VQA loss.
We evaluate the proposed method on two challenging video question answering benchmarks: a) STAR [47], fea-turing four different question types, interaction, sequence, prediction, and feasibility based on a subset of the real-world Charades dataset [44]; and b) Action Genome QA (AGQA) [11] dataset which tests vision focused reasoning skills based on novel compositions, novel reasoning steps, and indirect references. Compared to other VQA datasets, these datasets provide dense ground truth hyper-graph infor-mation for each video, which allows us to learn the respective embedding. Our results show that the proposed hyper-graph encoding significantly improves VQA performance as it has the ability to infer correct answers from spatio-temporal graphs from the input video. Our ablations further reveal that achieving high-quality graphs can be critical for VQA performance.
Our contributions to this paper are as follows:
• We introduce a novel architecture that enables the com-putation of situation hyper-graphs from video data to solve the complex reasoning task of video question-answering;
• We propose a situation hyper-graph decoder module to decode the atomic actions and object/actor-object relationships and model the hyper-graph learning as a transformer-based set prediction task and use a set pre-diction loss function to predict actions and relationships between entities in the input video;
• We use the resulting high-level embedding information as sole visual information for the reasoning and show that this is sufficient for an effective VQA system. 2.