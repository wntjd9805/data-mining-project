Abstract
Post-training quantization (PTQ) is an effective com-pression method to reduce the model size and computa-tional cost. However, quantizing a model into a low-bit one, e.g., lower than 4, is difﬁcult and often results in non-negligible performance degradation. To address this, we investigate the loss landscapes of quantized networks with various bit-widths. We show that the network with more ragged loss surface, is more easily trapped into bad local minima, which mostly appears in low-bit quantization. A deeper analysis indicates, the ragged surface is caused by the injection of excessive quantization noise. To this end, we detach a sharpness term from the loss which reﬂects the impact of quantization noise. To smooth the rugged loss surface, we propose to limit the sharpness term small and stable during optimization. Instead of directly optimizing the target bit network, we design a self-adapted shrinking scheduler for the bit-width in continuous domain from high bit-width to the target by limiting the increasing sharpness term within a proper range. It can be viewed as iteratively adding small “instant” quantization noise and adjusting the network to eliminate its impact. Widely experiments includ-ing classiﬁcation and detection tasks demonstrate the ef-fectiveness of the Bit-shrinking strategy in PTQ. On the Vi-sion Transformer models, our INT8 and INT6 models drop within 0.5% and 1.5% Top-1 accuracy, respectively. On the traditional CNN networks, our INT4 quantized models drop within 1.3% and 3.5% Top-1 accuracy on ResNet18 and MobileNetV2 without ﬁne-tuning, which achieves the state-of-the-art performance. 1.

Introduction
In recent years, network compression, such as Knowl-edge distillation [16], Pruning [9, 17, 38], and Quantiza-tion [30, 34, 35, 45] are rapidly developing to achieve efﬁ-cient inference for Deep Neural Networks (DNNs) both at
Figure 1. The loss landscapes of the full-precision and various bits ResNet-20 on CIFAR-100. We plot the loss landscapes using the visualization methods in [18]. The landscape of the lower-bit quantized network is more ragged, and is more easily trapped into bad local minima. edge devices and in the clouds. Among them, quantization is a promising as well as hardware-friendly approach. It re-duces the computation complexity and memory footprint by representing weights and activations with low-bit integers.
Traditional approaches often perform a Quantization-aware Training (QAT) [35–37, 45] process to achieve guar-anteed accuracy. However, the long time re-training and the requirements of full training data consume unaccept-able computation and storage resources, making it imprac-tical in industry. On the contrary, Post Training Quantiza-[2, 12, 25, 26, 43] is fast and light. PTQ efﬁ-tion (PTQ) ciently turns a pre-trained full precision network to quan-tized one with only a small calibration set. Although more
user-friendly, PTQ suffers performance degradation when pursuing ultra low compression ratio [28, 39] (e.g. lower than 4 bits). This is because only the magnitude informa-tion of weights and activations is utilized by simply apply-ing rounding-to-nearest in predominant approaches. The ir-reparable quantization noise will accumulate layer by layer throughout the network. or
To this end, some layer-wise block-wise reconstruction-based algorithms (e.g. AdaRound [26],
Bit-Split [28], AdaQuant [12], QDrop [39], BRECQ [19],
UWC [20]) are proposed, which greatly improve the accuracy when the quantized weights go down to 4-bit. An analysis [26] on the second order Taylor series expansion of the task loss indicates reconstruction-based methods could reduce the error introduced by rounding because they leverage the interaction between weights. QDrop [39] proposes to take activation quantization into consideration to increase the ﬂatness of loss landscape. Nevertheless, when the compression rate goes higher , e.g., the activations quantized to 4 bits or the weights quantized to 2 bits, or compressing more complex models, e.g., the recent prevail-ing Vision Transformer models [6, 23, 33], it still remains a non-negligible accuracy gap with original model [24, 42].
Fig. 1 shows that, as the bit-width goes lower, the loss land-scapes have more ragged surface. Therefore, in lower bits’ optimization process, the networks are easily trapped into bad local minima and result in performance degradation.
As mentioned above, optimizing a low-bit model in
PTQ is very challenging. In forward process, full-precision weights and activations are quantized to a small set of ﬁxed-point values, which introduces large quantization noise to the network. The quantization noise causes the loss dis-torted, i.e. a rugged loss landscape shown in Fig. 1. As a consequence, the distorted loss makes the optimization un-stable, misleading the network to poor local minima.
In
QAT, some progressive approaches [14, 47] are proposed.
For instance, CTMQ [14] trains multiple precision quan-tized models from high-bit to low-bit, and use the weight of trained higher bit model to initialize the next low-bit model.
The progressive process is experimentally veriﬁed to be helpful for the quantized network to reduce the quantiza-tion noise, resulting in better local minima. In QAT, to fully optimize the low-bit quantization network, the progressive approaches preset a complex bit dropping schedule, includ-ing long precision sequence and tedious training iterations.
With multiplied training cost, traditional progressive meth-ods are not practical in PTQ.
In this paper, a sharpness term, detaching the quantiza-tion noise’s impact on loss, is deﬁned to precisely estimate the degree of the loss distortion. Based on the sharpness term, a self-adapted progressive quantization scheduler for
PTQ, named Bit-shrinking, is designed to help the low-bit model ﬁnd better local minima. Instead of directly quan-tizing the network to the target low bit-width, Bit-shrinking relaxes bit-width to continuous value and gradually shrink it to the target bit-width during optimization to limit the sharpness. Each time the bit-width is shrunk, the “instant” sharpness introduced is limited within a preset threshold.
Shrinking the bit-width and adjusting the weights are itera-tively performed until the target bit arrives. Consequently, with the “instant” sharpness term limited, the loss surface is smoothed which helps to ﬁnd good minima. Different from traditional progressive approaches which evenly drops the integer bit-width, Bit-shrinking has more proper dropping scheduler on continuous bit-width. It tackles the additional training cost issue by alleviating over-optimizing some triv-ial bit-width.
This paper proposes a novel and practical Post-Training quantization framework to improve the accuracy of low-bit quantized network. The motivation is to reduce the impact of quantization noise during optimization by a Bit-shrinking strategy. We show that Bit-shrinking can help the low-bit network to ﬁnd better local minima comparing with the direct quantization approach. Our main contributions are summarized as follows:
• Based on the observation of the loss landscape in low-bit quantization, we ﬁnd that excessive sharpness of the loss landscape misleads the optimization direction.
To precisely estimate and further limit the impact of quantization noise during optimization, we detach a sharpness term from the loss.
• To calibrate the optimization direction, we propose
Bit-shrinking, a self-adapted progressive quantization scheduler for PTQ, to limit sharpness term small and stable during optimization. The landscape is smoothed as the progressive scheduler iteratively adds small “in-stant” sharpness and adjusts the network. As a re-sult, Bit-shrinking helps quantized low-bit network
ﬁnd better local minima comparing with direct quanti-zation approach, and results in better performance.
• Widely experiments, including Vision Transformer and CNN classiﬁcation models on ImageNet dataset and Faster RCNN, RetinaNet detection models on
COCO dataset, demonstrate the superiority of the Bit-shrinking without end-to-end ﬁne-tuning. On the Vi-sion Transformer models, our INT8 and INT6 mod-els drop within 0.5% and 1.5% Top-1 accuracy, re-spectively. Our INT4 quantized model drops within 1.3% and 3.5% Top-1 accuracy on ResNet18 and Mo-bileNetV2, which achieves the SOTA performance. 2.