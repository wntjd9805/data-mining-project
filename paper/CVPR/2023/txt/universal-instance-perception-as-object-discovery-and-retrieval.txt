Abstract
All instance perception tasks aim at finding certain ob-jects specified by some queries such as category names, lan-guage expressions, and target annotations, but this com-plete field has been split into multiple independent sub-tasks. In this work, we present a universal instance per-ception model of the next generation, termed UNINEXT.
UNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings (1) enormous data from different the following benefits: tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is es-pecially beneficial for tasks lacking in training data. (2) the unified model is parameter-efficient and can save re-dundant computation when handling multiple tasks simul-taneously. UNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks in-cluding classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (refer-ring expression comprehension and segmentation), and six video-level object tracking tasks. Code is available at https://github.com/MasterBin-IIAU/UNINEXT. 1.

Introduction
Object-centric understanding is one of the most essen-tial and challenging problems in computer vision. Over the years, the diversity of this field increases substantially. In this work, we mainly discuss 10 sub-tasks, distributed on the vertices of the cube shown in Figure 1. As the most fundamental tasks, object detection [8, 9, 30, 59, 82, 84, 91] and instance segmentation [6, 37, 64, 90, 97] require finding all objects of specific categories by boxes and masks re-spectively. Extending inputs from static images to dynamic
*This work was performed while Bin Yan worked as an intern at
ByteDance. Email: yan bin@mail.dlut.edu.cn. † Corresponding authors: jiangyi.enjoy@bytedance.com, lhchuan@dlut.edu.cn.
Figure 1. Task distribution on the Format-Time-Reference space.
Better view on screen with zoom-in. videos, Multiple Object Tracking (MOT) [3, 74, 126, 128],
Multi-Object Tracking and Segmentation (MOTS) [47, 93, 108], and Video Instance Segmentation (VIS) [43, 100, 103, 112] require finding all object trajectories of specific cate-gories in videos. Except for category names, some tasks provide other reference information. For example, Refer-ring Expression Comprehension (REC) [115,121,130], Re-ferring Expression Segmentation (RES) [116,119,121], and
Referring Video Object Segmentation (R-VOS) [7, 86, 104] aim at finding objects matched with the given language ex-pressions like “The fourth person from the left”. Besides,
Single Object Tracking (SOT) [5, 51, 106] and Video Ob-ject Segmentation (VOS) [18, 78, 107] take the target an-notations (boxes or masks) given in the first frame as the reference, requiring to predict the trajectories of the tracked objects in the subsequent frames. Since all the above tasks aim to perceive instances of certain properties, we refer to them collectively as instance perception.
Although bringing convenience to specific applications, such diverse task definitions split the whole field into frag-mented pieces. As the result, most current instance percep-tion methods are developed for only a single or a part of sub-tasks and trained on data from specific domains. Such
fragmented design philosophy brings the following draw-backs: (1) Independent designs hinder models from learn-ing and sharing generic knowledge between different tasks and domains, causing redundant parameters. (2) The pos-sibility of mutual collaboration between different tasks is overlooked. For example, object detection data enables models to recognize common objects, which can naturally improve the performance of REC and RES. (3) Restricted by fixed-size classifiers, traditional object detectors are hard to jointly train on multiple datasets with different label vo-cabularies [36,61,88] and to dynamically change object cat-egories to detect during inference [22, 61, 74, 81, 112, 120].
Since essentially all instance perception tasks aim at find-ing certain objects according to some queries, it leads to a natural question: could we design a unified model to solve all mainstream instance perception tasks once and for all?
To answer this question, we propose UNINEXT, a uni-versal instance perception model of the next generation.
We first reorganize 10 instance perception tasks into three types according to the different input prompts: (1) cate-gory names as prompts (Object Detection, Instance Seg-mentation, VIS, MOT, MOTS). (2) language expressions as prompts (REC, RES, R-VOS). (3) reference annota-tions as prompts (SOT, VOS). Then we propose a uni-fied prompt-guided object discovery and retrieval formula-tion to solve all the above tasks. Specifically, UNINEXT first discovers N object proposals under the guidance of the prompts, then retrieves the final instances from the proposals according to the instance-prompt match-ing scores. Based on this new formulation, UNINEXT can flexibly perceive different instances by simply changing the input prompts. To deal with different prompt modalities, we adopt a prompt generation module, which consists of a reference text encoder and a reference visual encoder. Then an early fusion module is used to enhance the raw visual features of the current image and the prompt embeddings.
This operation enables deep information exchange and pro-vides highly discriminative representations for the later in-stance prediction step. Considering the flexible query-to-instance fashion, we choose a Transformer-based object de-tector [131] as the instance decoder. Specifically, the de-coder first generates N instance proposals, then the prompt is used to retrieve matched objects from these proposals.
This flexible retrieval mechanism overcomes the disadvan-tages of traditional fixed-size classifiers and enables joint training on data from different tasks and domains.
With the unified model architecture, UNINEXT can learn strong generic representations on massive data from various tasks and solve 10 instance-level perception tasks using a single model with the same model parameters. Ex-tensive experiments demonstrate that UNINEXT achieves superior performance on 20 challenging benchmarks. The contributions of our work can be summarized as follows.
• We propose a unified prompt-guided formulation for reuniting previously universal fragmented instance-level sub-tasks into a whole. instance perception,
• Benefiting from the flexible object discovery and re-trieval paradigm, UNINEXT can train on different tasks and domains, in no need of task-specific heads.
• UNINEXT achieves superior performance on 20 chal-lenging benchmarks from 10 instance perception tasks using a single model with the same model parameters. 2.