Abstract
This work addresses cross-view camera pose estimation, i.e., determining the 3-Degrees-of-Freedom camera pose of a given ground-level image w.r.t. an aerial image of the lo-cal area. We propose SliceMatch, which consists of ground and aerial feature extractors, feature aggregators, and a pose predictor. The feature extractors extract dense features from the ground and aerial images. Given a set of can-didate camera poses, the feature aggregators construct a single ground descriptor and a set of pose-dependent aerial descriptors. Notably, our novel aerial feature aggregator has a cross-view attention module for ground-view guided aerial feature selection and utilizes the geometric projec-tion of the ground camera’s viewing frustum on the aerial image to pool features. The efficient construction of aerial descriptors is achieved using precomputed masks. Slice-Match is trained using contrastive learning and pose es-timation is formulated as a similarity comparison between the ground descriptor and the aerial descriptors. Compared to the state-of-the-art, SliceMatch achieves a 19% lower median localization error on the VIGOR benchmark using the same VGG16 backbone at 150 frames per second, and a 50% lower error when using a ResNet50 backbone. 1.

Introduction
Cross-view camera pose estimation aims to estimate the 3-Degrees-of-Freedom (3-DoF) ground camera pose, i.e., planar location and orientation, by comparing the captured ground-level image to a geo-referenced overhead aerial im-age containing the camera’s local surroundings.
In prac-tice, the local aerial image can be obtained from a reference database using any rough localization prior, e.g., Global
Navigation Satellite Systems (GNSS), image retrieval [19], or dead reckoning [13]. However, this prior is not nec-essarily accurate, for example, GNSS can contain errors up to tens of meters in urban canyons [2, 48, 49]. The cross-view formulation provides a promising alternative to
* indicates equal contribution.
Figure 1. SliceMatch identifies for a ground-level image (a) its camera’s 3-DoF pose within a corresponding aerial image (b).
It divides the camera’s Horizontal Field-of-View (HFoV) into
‘slices’, i.e., vertical regions in (a). After self-attention, our novel aggregation step (c) applies cross-view attention to create ground slice-specific aerial feature maps. To efficiently test many candi-date poses, the slice features are aggregated using pose-dependent aerial slice masks that represent the camera’s sliced HFoV at that pose. The slice masks for each pose are precomputed. All aerial pose descriptors are compared to the ground descriptor, resulting in a dense scoring map (d). Our output is the best-scoring pose. ground-level camera pose estimation techniques that require detailed 3D point cloud maps [31] or semantic maps [3,43], since the aerial imagery provides continuous coverage of the Earth’s surface including the area where accurate point clouds are difficult to collect. Moreover, acquiring up-to-date aerial imagery is less costly than maintaining and up-dating large-scale 3D point clouds or semantics maps.
Recently, several works have addressed cross-view cam-era localization [55] or 3-DoF pose estimation [33, 36, 44, 50]. Roughly, those methods can be categorized into global image descriptor-based [50, 55] and dense pixel-level feature-based [33, 36, 44] methods. Global descriptor-based methods take advantage of the compactness of the image representation and often have relatively fast infer-ence time [50, 55]. Dense pixel-level feature-based meth-ods [33, 36, 44] are potentially more accurate as they pre-serve more details in the image representation. They use the geometric relationship between the ground and aerial view to project features across views and estimate the cam-era pose via computationally expensive iterations. Aiming for both accurate and efficient camera pose estimation, in this work, we improve the global descriptor-based approach and enforce feature locality in the descriptor.
We observe several limitations in existing global descriptor-based cross-view camera pose estimation meth-ods [50, 55]. First, they rely on the aerial encoder to encode all spatial context and the aerial encoder has to learn how to aggregate local information, e.g., via the SAFA mod-ule [34], into the global descriptor, without accessing the information in the ground view or exploiting geometric con-straints between the ground-camera viewing frustum and the aerial image. Second, existing global descriptor-based methods for cross-view localization [50, 55] do not explic-itly consider the orientation of the ground camera in their descriptor construction. As a result, they either do not esti-mate the orientation [55] or require multiple forward passes on different rotated samples to infer the orientation [50].
Third, existing global descriptors-based methods [50, 55] are not trained discriminatively against different orienta-tions. Therefore, the learned features may be less discrimi-native for orientation prediction.
To address the observed gaps, we devise a novel, accu-rate, and efficient method for cross-view camera pose es-timation called SliceMatch (see Figure 1). Its novel aerial feature aggregation explicitly encodes directional informa-tion and pools features using known camera geometry to ag-gregate the extracted aerial features into an aerial global de-scriptor. The proposed aggregation step ‘slices’ the ground
Horizontal Field-of-View (HFoV) into orientation-specific descriptors. For each pose in a set of candidates, it aggre-gates the extracted aerial features into corresponding aerial slice descriptors. The aggregation uses cross-view attention to weigh aerial features w.r.t. to the ground descriptor, and exploits the geometric constraint that every vertical slice in the ground image corresponds to an azimuth range extrud-ing from the projected ground camera position in the aerial image. The feature extraction is done only once for con-structing the descriptors for all pose candidates, resulting in fast training and inference speed. We contrastively train the model by pairing the ground image descriptor with aerial descriptors at different locations and orientations. Hence, the model learns to extract discriminative features for both localization and orientation estimation.
Contributions: i) A novel aerial feature aggregation step that uses a cross-view attention module for ground-view guided aerial feature selection, and the geometric rela-tionship between the ground camera’s viewing frustum and the aerial image to construct pose-dependent aerial descrip-tors. ii) SliceMatch’s design allows for efficient implemen-tation, which runs significantly faster than previous state-of-the-art methods. Namely, for an input ground-aerial image pair, SliceMatch extracts dense features only once, aggre-gates aerial descriptors at a set of poses without extra com-putation, and compares the aerial descriptor of each pose iii) Compared to the previous with the ground descriptor. state-of-the-art global descriptor-based cross-view camera pose estimation method, SliceMatch constructs orientation-aware descriptors and adopts contrastive learning for both locations and orientations. Powered by the above designs,
SliceMatch sets the new state-of-the-art for cross-view pose estimation on two commonly used benchmarks. 2.