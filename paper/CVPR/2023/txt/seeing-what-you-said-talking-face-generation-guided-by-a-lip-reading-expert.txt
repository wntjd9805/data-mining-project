Abstract
Talking face generation, also known as speech-to-lip generation, reconstructs facial motions concerning lips given coherent speech input. The previous studies revealed the importance of lip-speech synchronization and visual quality. Despite much progress, they hardly focus on the content of lip movements i.e., the visual intelligibility of the spoken words, which is an important aspect of generation quality. To address the problem, we propose using a lip-reading expert to improve the intelligibility of the gener-ated lip regions by penalizing the incorrect generation re-sults. Moreover, to compensate for data scarcity, we train the lip-reading expert in an audio-visual self-supervised manner. With a lip-reading expert, we propose a novel contrastive learning to enhance lip-speech synchronization, and a transformer to encode audio synchronically with video, while considering global temporal dependency of au-dio. For evaluation, we propose a new strategy with two different lip-reading experts to measure intelligibility of the generated videos. Rigorous experiments show that our pro-posal is superior to other State-of-the-art (SOTA) methods, such as Wav2Lip, in reading intelligibility i.e., over 38%
Word Error Rate (WER) on LRS2 dataset and 27.8% ac-curacy on LRW dataset. We also achieve the SOTA perfor-mance in lip-speech synchronization and comparable per-formances in visual quality. 1.

Introduction
Talking Face Generation (TFG) aims at generating high-fidelity talking heads which are temporally synchronized with the input speech. It plays a significant role in many
Human Robot Interaction (HRI) applications, such as film dubbing [1], video editing, face animation [2, 3], and com-munication with people who has hearing loss but master in lip-reading. Thanks to its various practical usage, TFG has
∗ jiadong.wang@u.nus.edu
† corresponding author (qianxy@ustb.edu.cn) also received an increasing attention in both industrial and research community over the past decades [4, 5].
In TFG, there are two major aspects of concerna: lip-speech synchronization and visual quality. While hu-mans are sensitive to subtle abnormalities in asynchronized speech and facial motions [6], the mechanism of speech production highly relies on lip movements [7]. As a result, the main challenge of TFG exists in temporal alignment between the input speech and synthesized video streams.
One solution to this problem is to place an auxiliary em-bedding network at the end of the generator to analyze the audio-visual coherence. For example, [8] uses a pre-trained embedding network [9] as the lip-sync discriminator, while
[10] investigates an asymmetric mutual information estima-tor. Another solution is to compute a sync loss between visual lip features of ground truth and generated video se-quences [11]. Other attempts use the encoder-decoder struc-ture to facilitate TFG by improving audio and visual repre-sentations, such as [12, 13], which disentangle the visual features to enhance audio representations into a shared la-tent space; or, [14], which disentangles audio factors, i.e., emotional and phonetic content, to remove sync-irrelevant features.
Apart from lip-speech synchronization, blurry or unreal-istic visual quality also penalizes generation performance.
To preserve defining facial features of target persons, skip connections [15] are applied [5]. Others [12, 16] employ
Generative Adversarial Nets (GAN) to distinguish real and synthesized results by modelling the temporal dynamics of visual outputs. In this way, the resulting models can gen-erate more plausible talking heads that can be qualitatively measured by subjective evaluations.
In addition, reading intelligibility should be indispens-able, but it has not been emphasized. Reading intelligibil-ity indicates how much text content can be interpreted from face videos by humans’ lip reading ability, which is espe-cially significant for hearing-impaired users. However, im-age quality and lip-speech synchronization do not explicitly reflect reading intelligibility. Specifically, a well-qualified
image may contain fine-grained lip-sync errors [8] while precise synchronization may convey incorrect text contents.
According to the McGurk effect [17], when people listen and see an unpaired but synchronized sequence of speech and lip movements, they may recognize a phoneme from audio or video, or a fused artifact.
In this paper, we propose a TalkLip net to synthesize talking faces by focusing on reading intelligibility. Specifi-cally, we employ a lip-reading expert which transcribes im-age sequences to text to penalize the generator for incor-rectly generated face images. We replace some images of a face sequence with the generated ones, and feed it to the lip-reading expert during training to supervise the face gen-erator.
However, lip reading is hard even for humans. In [18], four people with equal gender distribution are invited to read lip movements. However, the average error rate is as high as 47%. Therefore, a reliable lip-reading model re-lies on a great amount of data. We employ AV-Hubert [19], a self-supervised method, which has yielded SOTA perfor-mance in lip-reading, speech recognition, and audio-visual speech recognition. The encoders of lip-reading and speech recognition systems are highly synchronized since they are supervised by the same pseudo label during pre-training.
Leveraging the lip-reading expert from the AV-Hubert, we propose a new method to enhance lip-speech synchro-nization. Particularly, we conduct contrastive learning be-tween audio embeddings for face generation and visual con-text features from the lip-reading encoder. Besides, the
AV-Hubert also provides a synchronized speech recognition system whose encoder considers long-term temporal depen-dency, we adopt this encoder to encode audio inputs, instead of encoders in [8, 11, 13] only rely on short-term temporal dependency (0.2s audio), or the single-modality (audio) pre-trained encoder in [20]. Our contributions are summarized as follows:
• We tackle the reading intelligibility problem of speech-driven talking face generation by leveraging a lip-reading expert.
• To enhance lip-speech synchronization, we propose a novel cross-modal contrastive learning strategy, as-sisted by a lip-reading expert.
• We employ a transformer encoder trained synchron-ically with the lip-reading expert to consider global temporal dependency across the entire audio utterance.
• We propose a new strategy to evaluate reading intelli-gibility for TFG and make the benchmark code pub-licly available*.
• Extensive experiments demonstrate the feasibility of our proposal and its superiority over other prevail-ing methods in reading intelligibility (over 38% WER on LRS and 27.8% accuracy on LRW). Additionally, our approach performs comparably to or better than other SOTA methods in terms of visual quality and lip-speech synchronization. 2.