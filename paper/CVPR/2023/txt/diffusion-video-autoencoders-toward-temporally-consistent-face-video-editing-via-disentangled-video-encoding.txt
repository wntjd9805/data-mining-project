Abstract
Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consis-tency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the ﬁrst time as a face video edit-ing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulat-ing the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.1 1.

Introduction
As one of the standard tasks in computer vision to change various face attributes such as hair color, gender, or glasses 1Project page: https://diff-video-ae.github.io of a given face image, face editing has been continuously gaining attention due to its various applications and en-tertainment. In particular, with the improvement of analy-sis and manipulation techniques for recent Generative Ad-versarial Network (GAN) models [8, 11, 22, 29, 30], we simply can do this task by manipulating a given image’s latent feature. In addition, very recently, many methods for face image editing also have been proposed based on
Diffusion Probabilistic Model (DPM)-based methods that show high-quality and ﬂexible manipulation performance
[3, 12, 17, 19, 23, 25].
Naturally, further studies [2, 35, 41] have been proposed to extend image editing methods to incorporate the tempo-ral axis for videos. Now, given real videos with a human face, these studies try to manipulate some target facial at-tributes with the other remaining features and motion in-tact. They all basically edit each frame of a video inde-pendently via off-the-shelf StyleGAN-based image editing techniques [22, 29, 41].
Despite the advantages of StyleGAN in this task such as high-resolution image generation capability and highly dis-entangled semantic representation space, one harmful draw-back of GAN-based editing methods is that the encoded real
images cannot perfectly be recovered by the pretrained gen-erator [1, 26, 34]. Especially, if a face in a given image is unusually decorated or occluded by some objects, the ﬁxed generator cannot synthesize it. For perfect reconstruction, several methods [6, 27] are suggested to further tune the generator for GAN-inversion [26, 34] on one or a few tar-get images, which is computationally expensive. Moreover, after the ﬁne-tuning, the original editability of GANs can-not be guaranteed. This risk could be worse in video domain since we have to ﬁnetune the model on the multiple frames.
Aside from the reconstruction issue of existing GAN-based methods, it is critical in video editing tasks to con-sider the temporal consistency among edited frames to pro-duce realistic results. To address this, some prior works rely on the smoothness of the latent trajectory of the original frames [35] or smoothen the latent features directly [2] by simply taking the same editing step for all frames. However, smoothness does not ensure temporal consistency. Rather, the same editing step can make different results for differ-ent frames because it can be unintentionally entangled with irrelevant motion features. For example, in the middle row of Fig. 1, eyeglasses vary across time and sometimes dimin-ish when the man closes his eyes.
In this paper, we propose a novel video editing frame-work for human face video, termed Diffusion Video Au-toencoder, that resolves the limitations of the prior works.
First, instead of GAN-based editing methods suffering from imperfect reconstruction quality, we newly introduce diffu-sion based model for face video editing tasks. As the re-cently proposed diffusion autoencoder (DiffAE) [23] does, thanks to the expressive latent spaces of the same size as input space, our model learns a semantically meaningful la-tent space that can perfectly recover the original image back and are directly editable. Not only that, for the ﬁrst time as a video editing model, encode the decomposed features of the video: 1) identity feature shared by all frames, 2) fea-ture of motion or facial expression in each frame, and 3) background feature that could not have high-level represen-tation due to large variances. Then, for consistent editing, we simply manipulate a single invariant feature for the de-sired attribute (single editing operation per video), which is also computationally beneﬁcial compared to the prior works that require editing the latent features of all frames.
We experimentally demonstrate that our model appropri-ately decomposes videos into time-invariant and per-frame variant features and can provide temporally consistent ma-nipulation. Speciﬁcally, we explore two ways of manipula-tion. The ﬁrst one is to edit features in the predeﬁned set of attributes by moving semantic features to the target di-rection found by learning linear classiﬁer in semantic repre-sentation space on annotated CelebA-HQ dataset [10]. Ad-ditionally, we explore the text-based editing method that op-timizes a time-invariant latent feature with CLIP loss [7]. It is worth noting that since we cannot fully generate edited images for CLIP loss due to the computational cost, we pro-pose the novel strategy that rather uses latent state of inter-mediate time step for the efﬁciency.
To summarize, our contribution is four-fold:
• We devise diffusion video autoencoders based on dif-fusion autoencoders [23] that decompose the video into a single time-invariant and per-frame time-variant features for temporally consistent editing.
• Based on the decomposed representation of diffusion video autoencoder, face video editing can be con-ducted by editing only the single time-invariant iden-tity feature and decoding it together with the remaining original features.
• Owing to the nearly-perfect reconstruction ability of diffusion models, our framework can be utilized to edit exceptional cases such that a face is partially occluded by some objects as well as usual cases.
• In addition to the existing predeﬁned attributes edit-ing method, we propose a text-based identity editing method based on the local directional CLIP loss [7,24] for the intermediately generated product of diffusion video autoencoders. 2.