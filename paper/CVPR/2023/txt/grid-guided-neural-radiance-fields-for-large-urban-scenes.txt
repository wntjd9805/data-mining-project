Abstract
Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred ren-derings on large-scale scenes due to limited model capac-ity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alterna-tive solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches subop-timal solutions, producing noisy artifacts in renderings, es-pecially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being com-putationally efficient. We propose to use a compact multi-resolution ground feature plane representation to coarsely capture the scene, and complement it with positional encod-ing inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground fea-ture planes, can meanwhile gain further refinements, form-ing a more accurate and compact feature space and output much more natural rendering results. 1.

Introduction
Large urban scene modeling has been drawing lots of research attention with the recent emergence of neural radi-ance fields (NeRF) due to its photorealistic rendering and model compactness [3, 34, 56, 59, 62, 64]. Such model-ing can enable a variety of practical applications, including autonomous vehicle simulation [23, 39, 67], aerial survey-ing [6,15], and embodied AI [35,61]. NeRF-based methods have shown impressive results on object-level scenes with their continuity prior benefited from the MLP architecture and high-frequency details with the globally shared posi-tional encodings. However, they often fail to model large and complex scenes. These methods suffer from underfit-ting due to limited model capacity and only produce blurred renderings without fine details [56,62,64]. BlockNeRF [58] and MegaNeRF [62] propose to geographically divide ur-ban scenes and assign each region a different sub-NeRF to learn in parallel. Subsequently, when the scale and com-plexity of the target scene increases, they inevitably suffer from a trade-off between the number of sub-NeRFs and the capacity required by each sub-NeRF to fully capture all the fine details in each region. Another stream of grid-based representations represents the target scene using a grid of features [27, 27, 30, 36, 55, 68, 69]. These methods are gen-erally much faster during rendering and more efficient when the scene scales up. However, as each cell of the feature grid is individually optimized in a locally encoded manner, the resulting feature grids tend to be less continuous across the scene compared to NeRF-based methods. Although more flexibility is intuitively beneficial for capturing fine details, the lack of inherent continuity makes this representation vulnerable to suboptimal solutions with noisy artifacts, as demonstrated in Fig. 1.
To effectively reconstruct large urban scenes with im-plicit neural representations, in this work, we propose a two-branch model architecture that takes a unified scene repre-sentation that integrates both grid-based and NeRF-based approaches under a joint learning scheme. Our key insight is that these two types of representations can be used com-plementary to each other: while feature grids can easily fit local scene content with explicit and independently learned features, NeRF introduces an inherent global continuity on the learned scene content with its shareable MLP weights across all 3D coordinate inputs. NeRF can also encourage capturing high-frequency scene details by matching the po-sitional encodings as Fourier features with the bandwidth of details. However, unlike feature grid representation, NeRF is less effective in compacting large scene contents into its globally shared latent coordinate space.
Concretely, we firstly model the target scene with a fea-ture grid in a pre-train stage, which coarsely captures scene geometry and appearance. The coarse feature grid is then used to 1) guide NeRF’s point sampling to let it concentrate around the scene surface; and 2) supply NeRF’s positional encodings with extra features about the scene geometry and appearance at sampled positions. Under such guidance,
NeRF can effectively and efficiently pick up finer details in a drastically compressed sampling space. Moreover, as coarse-level geometry and appearance information are ex-plicitly provided to NeRF, a light-weight MLP is sufficient to learn a mapping from global coordinates to volume den-sities and color values. The coarse feature grids get further optimized with gradients from the NeRF branch in the sec-ond joint-learning stage, which regularizes them to produce more accurate and natural rendering results when applied in isolation. To further reduce memory footprint and learn a reliable feature grid for large urban scenes, we adopt a com-pact factorization of the 3D feature grid to approximate it without losing representation capacity. Based on the obser-vation that essential semantics such as the urban layouts are mainly distributed on the ground (i.e., xy-plane), we pro-pose to factorize the 3D feature grid into 2D ground feature planes spanning the scene and a vertically shared feature vector along the z-axis. The benefits are manifold: 1) The memory is reduced from O(N 3) to O(N 2). 2) The learned feature grid is enforced to be disentangled into highly com-pact ground feature plans, offering explicit and informative scene layouts. Extensive experiments show the effective-ness of our unified model and scene representation. When rendering novel views in practice, users are allowed to use either the grid branch at a faster rendering speed, or the
NeRF branch, with more high-frequency details and spatial smoothness, yet at the cost of a relatively slower rendering. 2.