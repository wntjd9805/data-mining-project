Abstract
Visual representation based on covariance matrix has demonstrates its efficacy for image classification by char-acterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correla-tion will become misleading once there is another channel correlating with both channels of interest, resulting in the
“confounding” effect. For this case, “partial correlation” which removes the confounding effect shall be estimated in-stead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix op-timisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainabil-ity, we develop an iterative method to solve the above ma-trix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep vi-sual representation and mitigates the small sample problem often encountered by covariance matrix estimation in CNN.
Computationally, our model can be effectively trained with
GPU and works well with a large number of channels of advanced CNNs. Experiments show the efficacy and supe-rior classification performance of our deep visual represen-tation compared to covariance matrix based counterparts. 1.

Introduction
Learning effective visual representation is a central issue in computer vision. In the past two decades, describing im-ages with local features and pooling them to a global rep-resentation has shown promising performance. As one of the pooling methods, covariance matrix based pooling has attracted much attention due to its exploitation of second-order correlation information of features. A variety of tasks
*Corresponding author. Code: https://github.com/csiro-robotics/iSICE
Figure 1. Understanding the partial correlation (a 3D toy case).
Unlike the ordinary covariance (pairwise correlation of say x and y corresponding to channels), partial correlation between variables x and y removes the influence of the confounding variable z. Let the number of samples n = 3 and channels d = 3. For the 3D case, x and y are projected onto a plane perpendicular to z. Then ρxy = cos φxy (and ρxz and ρyz can be computed by analogy). Projected x =
“residuals” rx and ry are computed as indicated in the plot, w′ arg minw y is computed by analogy). The green box: for d > 3, the computation of partial correlation requires covariance inversion [7]. x zi) where zi = [zi, 1]⊤ (and w′ i=1(xi − w⊤ (cid:80)3 such as fine-grained image classification [27], image seg-mentation [16], generic image classification [24, 26, 34], image set classification [44], action recognition [18], few-shot classification [50] and few-shot detection [52–54] have benefited from the covariance matrix based representation.
A few pioneering works have integrated covariance ma-trix as a pooling method within convolutional neural net-works (CNN) and investigated associated issues such as matrix function backpropagation [16], matrix normalisa-tion [23,28,38], compact matrix estimation [11,49] and ker-nel based extension [9]. The above works further improved visual representations based on covariance matrix.
Despite the above progress, covariance matrix merely measures the pairwise correlation (more accurately, covari-ance) of two variables without taking any other variables into account. This can be easily verified because its (i, j)-th entry solely depends on the i-th and j-th variables on a
Figure 2. Proposed iterative sparse inverse covariance estimation (iSICE) method in a CNN pipeline. sample set. In Statistics, it is known that such a pairwise correlation will give misleading results once a third vari-able is correlated with both variables of interest due to the
“confounding” effect. For this situation, the partial correla-tion is the right measure to use. It regresses out the effects of other variables from the two variables and then calculates the correlation of their residuals instead. Partial correlation can be conveniently obtained by computing inverse covari-ance matrix, also known as the precision matrix [7] in the statistical community. Figure 1 illustrates the geometrical interpretation of partial correlation.
The above observation motivates us to investigate a vi-sual representation for image classification based on the in-verse covariance matrix. After all, it has better theoreti-cal support on characterising the essential relationship of variables (e.g., the channels in a convolutional feature map) when other variables are present. Note that inverse covari-ance matrix can be used for many vision tasks but in this pa-per, we investigate it from the perspective of image classifi-cation. Nevertheless, reliably estimating inverse covariance matrix from the local descriptors of a CNN feature map is a challenging task. This is primarily due to the small spa-tial size of the feature map, i.e., sample size, and a higher number of channels, i.e., feature dimensions, and this issue becomes more pronounced for advanced CNN models. An unreliable estimate of inverse covariance matrix will criti-cally affect its effectiveness as a visual representation. One might argue that by increasing the size of input images or using a dimension reduction layer to reduce the number of feature channels, such an issue could be resolved. In this paper, we investigate this issue from the perspective of ro-bust precision matrix estimation.
To achieve our goal, we explore the use of sparsity prior for inverse covariance matrix estimation in the literature.
Specifically, the general principle of “bet on sparsity” [12] is adopted in estimating the structure of high-dimensional data, and this leads to an established technique called sparse inverse covariance estimation (SICE) [10]. It solves an op-timisation in the space of symmetric and positive definite (SPD) matrix to estimate the inverse covariance matrix by imposing the sparsity prior on its entries. SICE is designed to handle small sample problem and it is known for its ex-cellent effectiveness to that end [10]. An initial attempt to apply SICE for visual representation is based on hand-crafted or pre-extracted features of small size and an off-the-shelf SICE solver, and it does not have the ability to back-propagate through SICE due to optimisation of the SPD ma-trix with the imposed non-smooth sparsity term [51].
Our work is the first one that truly integrates SICE into
CNN for end-to-end training. Clearly, such an integration will fully take advantage of the feature learning capability of CNN and the partial correlation offered by inverse co-variance matrix. On the other hand, realising such an inte-gration is not trivial. Unlike covariance matrix, which is ob-tained by simple arithmetic operations, SICE is obtained by solving an SPD matrix based optimisation. How to incorpo-rate this optimisation process into CNN as a layer is an is-sue. Furthermore, this SICE optimisation needs to be solved for each training image during both forward and backward phases to generate a visual representation. Directly solving this optimisation within CNN will not be practical even for a medium-sized SICE problem.
To efficiently integrate SICE into CNN, we propose a fast end-to-end training method for SICE by taking inspiration from Newton-Schulz iteration [14]. Our method solves the
SICE optimisation with a smooth convex cost function by re-parameterising the non-smooth term in the original SICE cost function (see Eq. (1)), and it can therefore be optimised with standard optimisation techniques such as gradient de-scend. Furthermore, we effectively enforce the SPD con-straint during optimisation so that the obtained SICE solu-tion remains SPD as desired. Figure 2 shows our “Iterative
Sparse Inverse Covariance Estimation (iSICE)”. In contrast to SICE, iSICE works with end-to-end trainable deep learn-ing models. Our iSICE involves simple matrix arithmetic operations fully compatible with GPU. It can approximately solve large SICE problems within CNN efficiently.
Our main contributions are summarised as follows. 1. To more precisely characterise the relationship of fea-tures for visual representation, this paper proposes to integrate sparse inverse covariance estimation (SICE) process into CNNs as a novel layer. To achieve this, we develop a method based on Newton-Schulz iteration and box constraints for ℓ1 penalty to solve the SICE optimisation with CNN and maintain the end-to-end
training efficiency. To the best of our knowledge, our iSICE is the first end-to-end SICE solution for CNN. 2. Our iSICE method requires a minimal change of net-work architecture. Therefore, it can readily be inte-grated with existing works to replace those using deep network models to learn covariance matrix based vi-sual representation. The iSICE is fully compatible with GPU and can be easily implemented with mod-ern deep learning libraries. 3. As the objective of SICE is a combination of log det term (may change rapidly) and sparsity (changes slowly), achieving the balance between both terms dur-ing optimisation by the gradient descent is hard. To this end, we propose a minor contribution: a simple modulating network whose goal is to adapt on-the-fly learning rate and sparsity penalty.
Experiments on multiple image classification datasets show the effectiveness of our proposed iSICE method.
Structure sparsity cannot be readily applied to covari-ance representation as it requires the access to partial cor-relation between feature components. A covariance matrix captures pairwise correlation of feature components with-out taking into account the confounding effect of remaining components. Therefore, it is unlikely that the covariance matrix will be sparse by nature. To obtain partial corre-lation, SICE moves from covariance matrix to its inverse.
An inverse covariance matrix captures partial correlation between feature components by regressing out the effects of other variables [15]. Once other variables are factored out, structure sparsity can be effectively enforced in SICE.
In a recent work [51], SICE-based visual representation has been applied to image classification with handcrafted and pre-extracted features of small size. However, SICE has never been integrated into CNN for end-to-end training with the goal of adapting to such a representation. The existing solvers for computing SICE also have limited GPU sup-port [8, 10]. Thus, we propose an end-to-end trainable it-erative method for solving SICE optimisation with CNNs. 2.