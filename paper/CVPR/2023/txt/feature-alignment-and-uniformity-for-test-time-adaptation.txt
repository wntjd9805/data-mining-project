Abstract
Test time adaptation (TTA) aims to adapt deep neural networks when receiving out of distribution test domain samples. In this setting, the model can only access online unlabeled test samples and pre-trained models on the train-ing domains. We first address TTA as a feature revision problem due to the domain gap between source domains and target domains. After that, we follow the two measure-ments alignment and uniformity to discuss the test time fea-ture revision. For test time feature uniformity, we propose a test time self-distillation strategy to guarantee the consis-tency of uniformity between representations of the current batch and all the previous batches. For test time feature alignment, we propose a memorized spatial local cluster-ing strategy to align the representations among the neigh-borhood samples for the upcoming batch. To deal with the common noisy label problem, we propound the entropy and consistency filters to select and drop the possible noisy la-bels. To prove the scalability and efficacy of our method, we conduct experiments on four domain generalization bench-marks and four medical image segmentation tasks with var-ious backbones. Experiment results show that our method not only improves baseline stably but also outperforms ex-isting state-of-the-art test time adaptation methods. 1.

Introduction
Deep learning has achieved great success in computer vision tasks when training and test data are sampled from the same distribution [17, 26, 37]. However, in real-world applications, performance degradation usually occurs when training (source) data and test (target) data are collected from different distributions, i.e. domain shift. In practice, test samples may encounter different types of variations or corruptions. Deep learning models will be sensitive to these variations or corruptions, which can cause perfor-mance degradation.
To tackle this challenging but practical problem, various works have been proposed to adapt the model at test time [5, 8,21,36,43,55,64,65,74]. Test time training (TTT) [36,55] adapts model using self-supervised tasks, such as rotation classification [15] during both training and test phases. This paradigm relies on additional model modifications in both training and test phases, which is not feasible and scalable in the real world.
Similar to TTT, test time adaptation [64] (TTA) also adapts the model i.e., updates the parameters in the test phase. But TTA does not require any specific modifications in training and requires only the pre-trained source model and unlabeled target data during the test phase, which is more practical and generalizable. In TTA, the model could be adapted with only online unlabeled data. Hence, the model trained on source data is incompatible with the target data due to the possible domain gap between source data and target data.
To deal with the above-mentioned problem, we address
TTA as a representation revision problem in this paper.
In the test phase of TTA, the accessed model has already learned the feature representations specialized to source do-mains and may generate inaccurate representations for the target domain due to the large domain gap. It is necessary to rectify the feature representations for the target domain.
To achieve better representations for the target domain, we utilize the commonly used measurements for representation quality that can be summarized to feature alignment and uniformity [66, 71]. Alignment refers that the similar im-ages should have the similar representations, while unifor-mity means that images of different classes should be dis-tributed as uniform as possible in the latent space. Hence, we propose to address the TTA problem from the above-mentioned properties.
Most of the previous works about TTA can be in-ducted from the proposed representation revision perspec-tive. Some methods adapt the source model by conduct-ing a feature alignment process, such as feature matching
[25, 36] and predictions adjustment [5]. One of the repre-sentative methods is LAME [5], which encourages neigh-borhood samples in the feature space to have similar pre-dictions using Laplacian adjusted maximum-likelihood es-timation. Moreover, other methods aim to make target feature more uniform in feature space, including entropy minimization [43, 64, 74], prototype adjustment [21], infor-mation maximization [34] and batch normalization statis-tics alignment [33, 42, 51]. One representative method is
T3A [21], which adjusts prototypes (class-wise centroids) to have a more uniform representation by building a sup-port set. However, none of the method address the TTA problem from the representation alignment and uniformity simultaneously. In this paper, we identify this limitation and propose a novel method that rectifies feature representation from both two properties. We formulate the two properties in TTA as test time feature uniformity and test time feature alignment.
Test Time Feature Uniformity. Following the feature uniformity perspective, we hope that representations of test images from different classes should be distributed as uni-form as possible. However, only limited test samples can be accessed in an online manner in the TTA setting.
To better deal with all of the samples in the target do-main, we propose to introduce historical temporal informa-tion for every arriving test sample. A memory bank is built to store feature representations and logits for all arriving samples to maintain the useful information from the previ-ous data. We then calculate the pseudo-prototypes for every class by using the logits and features in the memory bank.
After that, to guarantee the uniformity for the current batch of samples, the prediction distribution of prototype-based classification and model prediction (outputs of linear clas-sifier) should be similar, i.e., the feature distribution of the current images of one class should be consistent with the feature distribution of all the previous images of the same class. This can reduce the bias of misclassified outlier sam-ples to form a more uniform latent space.
Motivated by this, we minimize the distance between the outputs of linear and prototype-based classifiers. This pattern is similar to self-distillation [23, 73] that transfers knowledge between different layers of the same network architecture. However, unlike typical self-distillation, our method does not require any ground truth supervision. We refer to this method as Test Time Self-Distillation (TSD).
Test Time Feature Alignment. Feature alignment en-courages the images from the same class to have similar fea-ture representations in the latent space. As for TTA, pseudo labels generated by the source model may be noisy due to the domain gap. Thus, instead of aligning all the positive pairs, we propose a K-nearest feature alignment to encour-age features from the same class to be closed or features from different classes to be far away from each other. This can reduce the negative impact imposed by the noisy la-bels and maintain the alignment of images with the same semantics. Specifically, we retrieve K-nearest features in the memory bank for the upcoming images and add consis-tency regularization between the representations and logits of the images. We refer to this as Memorized Spatial Local
Clustering (MSLC). The ablation of hyperparameter K is shown in Table 5 and Fig. 3.
Entropy Filter and Consistency Filter. During the adaption, we use stored pseudo features and logits to com-pute the pseudo-prototypes. However, the noisy label prob-lem cannot be completely alleviated despite the proposed efforts. To further reduce the impact, we adopt both entropy and consistency filters to filter noisy labels to boost perfor-mance. As for the entropy filter, we filter noisy features with high entropy when we compute prototypes because unreli-able samples usually produce high entropy.
In addition, the predictions of prototype-based and lin-ear classifiers of the network for reliable samples should be consistent ideally. We use this property to filter unreliable samples and back-propagate the gradient using only reliable samples. We refer to this filter as the consistency filter. The ablation study on the two proposed filters is presented in
Table 5.
Finally, we demonstrate the effectiveness of our pro-posed approach on commonly used domain generalization benchmarks, including PACS [30], VLCS [58], OfficeHome
[62] and DomainNet [47]. Furthermore, to prove the effi-cacy of our method, we conduct more experiments on four cross-domain medical image segmentation benchmarks, in-cluding prostate segmentation [1, 35], cardiac structure segmentation [4, 76, 77] and optic cup/disc segmentation
[13, 45, 53]. Our method achieves the state-of-the-art per-formance on the above benchmarks.
We summarize our contributions as follows:
• We propose a new perspective for test time adapta-tion from the view of feature alignment and unifor-mity. The proposed test time feature uniformity en-courages the representations of the current batch of samples along with the uniformity of all the previous samples. The test time feature alignment manipulates the representation of the test sample according to its neighbors in the latent space to align the representa-tions based on the pseudo label.
• Specifically, to meet the online setting and noisy la-bel problem in TTA, we propose two complementary strategies: unsupervised self-distillation for test time feature uniformity and memorized spatial local cluster-ing for test time feature alignment. We also propound the entropy filter and consistency filter to further miti-gate the effect of the noisy labels.
• The experiments demonstrate that our proposed method outperforms existing test time adaptation ap-proaches on both the domain generalization bench-marks and medical image segmentation benchmarks. 2.