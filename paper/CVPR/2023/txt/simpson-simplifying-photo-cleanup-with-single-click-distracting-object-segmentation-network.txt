Abstract 1.

Introduction
In photo editing, it is common practice to remove vi-sual distractions to improve the overall image quality and highlight the primary subject. However, manually select-ing and removing these small and dense distracting regions can be a laborious and time-consuming task.
In this pa-per, we propose an interactive distractor selection method that is optimized to achieve the task with just a single click.
Our method surpasses the precision and recall achieved by the traditional method of running panoptic segmenta-tion and then selecting the segments containing the clicks.
We also showcase how a transformer-based module can be used to identify more distracting regions similar to the user’s click position. Our experiments demonstrate that the model can effectively and accurately segment unknown distracting objects interactively and in groups. By signif-icantly simplifying the photo cleaning and retouching pro-cess, our proposed model provides inspiration for explor-ing rare object segmentation and group selection with a single click. More information can be found at https:
//github.com/hmchuong/SimpSON .
Both professional photographers and casual users often require efficient photo retouching to enhance the quality of their images. One essential aspect of this task is the removal of visual distractions from photos [7]. These distractions can take various forms, such as unexpected pedestrians, ob-jects that are cropped out of the photo’s edge, dirty spots on the ground, repeated outlets on a wall, or even colorful and blurry lens flare. These distractions can be challenging to categorize due to their diverse appearance. As a result, users tend to select and mask them entirely and use photo editing software such as Photoshop to remove them.
Segmentation is necessary for photo cleaning tasks be-cause rough masks may not be suitable for all scenarios.
Accurate masks are required in situations where distractors are touching the main foreground subjects or where distrac-tors are small but dense in the image. User-drawn rough masks can result in the deletion of too much background texture when connected. In other cases, users may have a
*This work was done during Chuong Huynh’s internship at Adobe
mask that covers the entire object but does not change the background too much. In all scenarios, our findings suggest that for inpainting, a tiny dilation from a highly accurate mask produces better background preservation and fewer leftover pixels of distractors. This finding is consistent with most of the existing inpainting models.
The process of manually masking distracting elements in a photo can be a tedious and time-consuming task. Users often seek an automated tool that can efficiently select and segment all distractors. One approach is to train an instance segmentation model like Mask-RCNN [11] to detect and segment distractors in a supervised manner. However, iden-tifying distractors can be subjective, and collecting datasets requires scientific validation of the distractor annotations to ensure that most users agree. For instance, Fried et al. [7] invited 35 users to mark distractors on a single image and received varying feedback. Even with a model that detects distractors, it may not always satisfy users’ preferences.
Therefore, tasks like these should rely heavily on user in-teraction, such as allowing users to click and decide where to retouch photos based on their own preferences.
Our goal is to propose a single-click distractor seg-mentation model. With the rapid development of panop-tic segmentation technologies like PanopticFCN [20] and
Mask2Former [5], can we utilize state-of-the-art models to retrieve distractor masks by clicking on the panoptic seg-mentation results? Unfortunately, most distractors belong to unknown categories, and some are tiny, making them dif-ficult to segment using models [2, 13] trained on datasets such as COCO [21], ADE20K [31], or Cityscapes [6] with a closed-set of categories. Qi et al. proposed entity segmen-tation [24] to train panoptic segmentation in a class-agnostic manner to address the long-tail problem, but it still may not be guaranteed to separate all regions in the photos.
What if we use clicks as the input guidance for seg-mentation? Interactive segmentation models are closely re-lated to our task, and recent works like FocalClick [4] and
RiTM [26] have achieved practical and high-precision seg-mentation performance. However, interactive segmentation aims to use multiple clicks, including positive and negative ones, to segment larger foreground objects accurately, espe-cially the boundary regions. In our task, we focus more on medium to small distracting objects and only require a sin-gle positive click to select semi-precise masks for inpainting purposes. The difference in our goal makes it challenging to follow the problem definition of interactive segmenta-tion. Additionally, previous interactive segmentation mod-els cannot select objects in groups, whereas most of our dis-tractors are repeated, dense, and evenly distributed across photos.
This paper addresses the two challenges of accurate one-click universal class-agnostic segmentation and efficient similarity selection. Our proposed method can significantly reduce the photo retouching process from hours (e.g., 100+ clicks) to minutes (e.g., 1-2 clicks) when removing dense and tiny distractors. Firstly, we optimize the click-based segmentation model to accurately segment distractor-like objects with a single click. This is achieved by utilizing the entity segmentation [24] method to discard category labels and using single-click embedding to guide the segmenta-tion of a single object. Secondly, we design a transformer-based Click Proposal Network (CPN) that mines similar distractor-like objects within the same image and regress click positions for them. Lastly, we rerun the single-click segmentation module using the proposed clicks to generate the mask and verify the similarity among the selected ob-jects via the Proposal Verification Module (PVM). We also run the process iteratively to ensure that more similar ob-jects are fully selected. In summary, our contributions con-sist of three main aspects:
• We introduce a novel one-click Distractor Segmenta-tion Network (1C-DSN) that utilizes a single-click-based approach to segment medium to small distract-ing objects with high accuracy. Unlike other interac-tive segmentation methods, our model targets the seg-mentation of distracting objects with just one positive click. Our model is capable of generalizing well to ob-jects of any rare categories present in the photos.
• We propose a Click Proposal Network (CPN) that mines all similar objects to the user’s single click. The proposed clicks are then reused in the segmentation model, and their similarity is verified using the Pro-posal Verification Module (PVM). This allows for the group selection of distracting objects with one click.
• We further explore running the selection process iter-atively to fully select similar distractors with slightly diverse appearances. Our proposed distractor selection pipeline, which we call ’SimpSON,’ significantly sim-plifies the photo retouching process. By using Simp-SON, users can remove distracting objects in their pho-tos quickly and easily with just a few clicks. 2.