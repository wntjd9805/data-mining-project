Abstract
Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud
Today, representations, achieve state-of-the-art results. projection-based methods leverage 2D CNNs but recent ad-vances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer posi-tively but only after combining them with three key ingre-dients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations.
By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to ac-quire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b)
We compensate ViTs’ lack of inductive bias by substituting a tailored convolutional stem for the classical linear em-bedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the con-volutional stem to combine low-level but fine-grained fea-tures of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingre-dients, we show that our method, called RangeViT, out-performs existing projection-based methods on nuScenes and SemanticKITTI. The code is available at https:// github.com/valeoai/rangevit. 1.

Introduction
Semantic segmentation of LiDAR point clouds permits vehicles to perceive their surrounding 3D environment in-*This project was done during an internship at Valeo.ai.
Figure 1. Exploiting vision transformer (ViT) architectures and weights for LiDAR point cloud semantic segmentation.
We leverage the flexibility of transformer-based architectures to re-purpose them with minimal changes for processing sparse point clouds in autonomous driving tasks. The common ViT backbone across modalities allows to effectively transfer weights pre-trained on large image repositories towards improving point cloud seg-mentation performance with fine-tuning. dependently of the lighting condition, providing useful in-formation to build safe and reliable vehicles. A common approach to segment large scale LiDAR point clouds is to project the points on a 2D surface and then to use regular
CNNs, originally designed for images, to process the pro-jected point clouds [1, 11, 26, 36, 60, 66]. Recently, Vision
Transformers (ViTs) were introduced as an alternative to convolutional neural networks for processing images [14]: images are divided into patches which are linearly embed-ded into a high-dimensional space to create a sequence of visual tokens; these tokens are then consumed by a pure transformer architecture [51] to output deep visual repre-sentations of each token. Despite the absence of almost any domain-specific inductive bias apart from the image to-kenization process, ViTs have a strong representation learn-ing capacity [14] and achieve excellent results on various
image perception tasks, such as image classification [14], object detection [8] or semantic segmentation [45].
Inspired by this success of ViTs for image understand-ing, we propose to implement projection-based LiDAR se-mantic segmentation with a pure vision transformer archi-tecture at its core. Our goals are threefold in doing so: (1)
Exploit the strong representation learning capacity of vision transformer for LiDAR semantic segmentation; (2) Work towards unifying network architectures used for process-ing LiDAR point clouds or images so that any advance in one domain benefits to both; (3) Show that one can lever-age ViTs pre-trained on large-size natural image datasets for LiDAR point cloud segmentation. The last goal is cru-cial because the downside of having few inductive biases in
ViTs is that they underperform when trained from scratch on small or medium-size datasets and that, for now, the only well-performing pre-trained ViTs [9, 14, 45] publicly avail-able are trained on large collections of images that can be acquired, annotated and stored easier than point clouds.
In this context, our main contribution is a ViT-based Li-DAR segmentation approach that compensates ViTs’ lack of inductive biases on our data and that achieves state-of-the-art results among projection-based methods. To the best of our knowledge, although works using ViT architectures on dense indoor point clouds already exists [63, 67], this is the first solution using ViTs for the LiDAR point clouds of autonomous driving datasets, which are significantly sparser and noisier than the dense depth-map-based points clouds found in indoor datasets. Our solution, RangeViT, starts with a classical range projection to obtain a 2D rep-resentation of the point cloud [11, 26, 36, 60]. Then, we extract patch-based visual tokens from this 2D map and feed them to a plain ViT encoder [14] to get deep patch representations. These representations are decoded using a lightweight network to obtain pixel-wise label predictions, which are projected back to the 3D point cloud.
Our finding is that this ViT architecture needs three key ingredients to reach its peak performance. First, we lever-age ViT models pre-trained on large natural image datasets for LiDAR segmentation and demonstrate that our method benefits from them despite the fact that natural images dis-play little resemblance with range-projection images. Sec-ond, we further compensate for ViTs’ lack of inductive bias by substituting the classical linear embedding layer with a multi-layer convolutional stem. Finally, we refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grain features of the convolutional stem with the high-level but coarse predictions of the ViT encoder. (1)
In summary, our contributions are the following:
To the best of our knowledge, we are the first to exploit the strong representation learning capacity of vision trans-formers architectures for 3D semantic segmentation from
LiDAR point clouds. By revisiting, in the context of our problem, the tokenization process of the ViT’s encoder and adding a light-weight convolutional decoder for refining the coarse patch-wise ViT representations, we derive a sim-ple but effective projection-based LiDAR segmentation ap-proach, which we call RangeViT. (2) Furthermore, as shown in Fig. 1, the proposed approach allows one to harness
ViT models pre-trained on the RGB image domain for the
LiDAR segmentation problem.
Indeed, despite the large gap between the two domains, we empirically demonstrate that using such pre-training strategies improves segmenta-tion performance. (3) Finally, our RangeViT approach, de-spite its simplicity, achieves state-of-the-art results among project-based segmentation methods. 2.