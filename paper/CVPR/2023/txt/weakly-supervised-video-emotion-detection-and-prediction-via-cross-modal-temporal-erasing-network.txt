Abstract
Automatically predicting the emotions of user-generated videos (UGVs) receives increasing interest recently. How-ever, existing methods mainly focus on a few key visual frames, which may limit their capacity to encode the con-text that depicts the intended emotions. To tackle that, in this paper, we propose a cross-modal temporal eras-ing network that locates not only keyframes but also con-text and audio-related information in a weakly-supervised manner. leverage the intra- and inter-modal relationship among different segments to ac-Then, we iteratively erase curately select keyframes. keyframes to encourage the model to concentrate on the contexts that include complementary information. Exten-sive experiments on three challenging video emotion bench-marks demonstrate that our method performs favorably against state-of-the-art approaches. The code is released on https://github.com/nku-zhichengzhang/WECL.
In specific, we first
Figure 1. Illustration of the keyframes with larger boxes that are detected by off-the-shelf method [65] on the Ekman-6 dataset [54].
Note that the deeper color represents the higher impact on overall video emotion, and “GT” represents the category label from the ground truth. The texts with orange color are the categorical pre-dicting results. 1.

Introduction
Emotion analysis in user-generated videos (UGVs) has attracted much attention since a growing number of people tend to express their views on social networks [20, 32, 36].
Automatic predictions of video emotions [52, 54] can po-tentially be applied in various areas like online content fil-tering [1], attitude recognition [32], and customer behavior analysis [39]. Emotions evoked in UGVs usually depend on multiple perspectives, such as actions, events, and objects, where different frames in a UGV may contribute unequally to conveying emotions.
Existing methods in this field mainly focus on extracting keyframes from visual content, assuming that these frames hold the dominant information for the intended emotions in videos. For example, Tu et al. [12] introduce an attribu-tion network to locate keyframes with temporal annotations, which are more precise than video-level labeling and lead
† Corresponding author. to better performance for video emotion recognition. Yet, annotating the emotional labels frame-by-frame is labor-sensitive and time-consuming [63]. Zhao et al. [65] fur-ther present the visual-audio network (VAANet) conduct-ing three types of attention to automatically discover the discriminative keyframes, which makes it state-of-the-art.
However, the selected “keyframes” may fail to represent the intended emotions exactly due to the inherent charac-teristics of human emotions, i.e., subjectivity and ambigu-ity [49, 57, 66]. As illustrated in Figure 1 (a), a woman receives a gift and moves to cry. The video-level emo-tion category is labeled as ‘surprise’. We could observe that VAANet [65] gives the most attention to the keyframes (i.e., “crying” frames in the larger boxes) while ignoring the context and leading to the wrong prediction. Further-more, in Figure 1 (b), a man sees his beloved girl talking with another man happily, which makes him feel sad. How-ever, VAANet only focuses on frames about chatting and categorizes the emotion of this video as ‘joy’. Therefore,
keyframes may lead to limited prediction results. Although the detected keyframes directly convey emotions in most videos, some other information that contains the necessary context should not be ignored. This is because the contex-tual frames could not only provide complementary infor-mation for understanding emotions in UGVs (especially for the cases where keyframes are hard to be distinguished), but also make the model more robust since more cues are considered to recognize emotions rather than the dominant information.
To address these problems, we propose a novel cross-modal temporal erasing network, which enables the model to be aware of context for recognizing emotions. Our pro-posed method mainly contains two crucial modules: tempo-ral correlation learning module, and temporal erasing mod-ule. First, we extract the visual feature together with the audio one from each equal-length segment derived from the video. Second, the temporal correlation learning module is introduced to fully discover comprehensive implicit cor-respondences among different segments across audio and visual modal. Then, keyframes are selected by consider-ing the correspondences with other frames in a weakly-supervised manner, where only the video-level class label is used. Finally, the temporal erasing module iteratively erases the most dominant visual and audio information to train with difficult samples online, that encouraging the model to detect more complementary information from context in-stead of the most dominant ones.
Our contributions can be summarized as follows: 1)
We introduce a weakly-supervised network to exploit keyframes and the necessary context in a unified CNN framework, which encourages model to extract features from multiple discriminative parts and learn better represen-tation for video emotion analysis. 2) We exploit intra- and inter-modal relationships to provide frame-level localized information only with video-level annotation, with which the model consolidates both holistic and local representa-tions for affective computing. We demonstrate the advan-tages of the above contributions with extensive experiments.
Our method achieves state-of-the-art on three video emo-tion datasets. 2.