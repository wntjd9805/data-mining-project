Abstract
Existing 3D-aware image synthesis approaches mainly focus on generating a single canonical object and show limited capacity in composing a complex scene containing a variety of objects. This work presents DisCoScene: a 3D-aware generative model for high-quality and controllable scene synthesis. The key ingredient of our method is a very abstract object-level representation (i.e., 3D bounding boxes without semantic annotation) as the scene layout prior, which is simple to obtain, general to describe various scene contents, and yet informative to disentangle objects and background. Moreover, it serves as an intuitive user control for scene editing. Based on such a prior, the proposed model spatially disentangles the whole scene into object-centric generative radiance fields by learning on only 2D images with the global-local discrimination. Our model obtains the generation fidelity and editing flexibility of individual objects while being able to efficiently compose objects and the background into a complete scene. We demonstrate state-of-the-art performance on many scene datasets, including the challenging Waymo outdoor dataset.
Project page can be found here. 1.

Introduction 3D-consistent image synthesis from single-view 2D data has become a trendy topic in generative modeling. Recent approaches like GRAF [40] and Pi-GAN [5] introduce 3D inductive bias by taking neural radiance fields [1, 28, 29, 36, 38] as the underlying representation, gaining the capability of geometry modeling and explicit camera control. Despite their success in synthesizing individual objects (e.g., faces, cats, cars), they struggle on scene images that contain multiple objects with non-trivial layouts and complex backgrounds. The varying quantity and large
*Work partially done during internships at Snap Inc.
†Corresponding author. diversity of objects, along with the intricate spatial arrange-ment and mutual occlusions, bring enormous challenges, which exceed the capacity of the object-level generative models [4, 13, 15, 33, 45, 46, 60].
Recent efforts have been made towards 3D-aware scene synthesis. Despite the encouraging progress, there are still fundamental drawbacks. For example, Generative Scene
Networks (GSN) [8] achieve large-scale scene synthesis by representing the scene as a grid of local radiance fields and training on 2D observations from continuous camera paths.
However, object-level editing is not feasible due to spatial entanglement and the lack of explicit object definition. On the contrary, GIRAFFE [32] explicitly composites object-centric radiance fields [16, 34, 56, 63] to support object-level control. Yet, it works poorly on challenging datasets containing multiple objects and complex backgrounds due to the absence of proper spatial priors.
To achieve high-quality and controllable scene synthesis, the scene representation stands out as one critical design focus. A well-structured scene representation can scale up the generation capability and tackle the aforementioned challenges.
Imagine, given an empty apartment and a furniture catalog, what does it take for a person to arrange the space? Would people prefer to walk around and throw things here and there, or instead figure out an overall layout and then attend to each location for the detailed selection? Obviously, a layout describing the arrangement of each furniture in the space substantially eases the scene composition process [17, 26, 58]. From this vantage point, here comes our primary motivation — an abstract object-oriented scene representation, namely a layout prior, could facilitate learning from challenging 2D data as a lightweight supervision signal during training and allow user interaction during inference. More specifically, to make such a prior easy to obtain and generalizable across different scenes, we define it as a set of object bounding boxes without semantic annotation, which describes the spatial composition of ob-jects in the scene and supports intuitive object-level editing.
In this work, we present DisCoScene, a novel 3D-aware generative model for complex scenes. Our method allows for high-quality scene synthesis on challenging datasets and flexible user control of both the camera and scene objects. Driven by the aforementioned layout prior, our model spatially disentangles the scene into compositable radiance fields which are shared in the same object-centric generative model. To make the best use of the prior as a lightweight supervision during training, we propose global-local discrimination which attends to both the whole scene and individual objects to enforce spatial disentanglement between objects and against the background. Once the model is trained, users can generate and edit a scene by explicitly controlling the camera and the layout of objects’ bounding boxes. In addition, we develop an efficient render-ing pipeline tailored for the spatially-disentangled radiance fields, which significantly accelerates object rendering and scene composition for both training and inference stages.
Our method is evaluated on diverse datasets, including both indoor and outdoor scenes. Qualitative and quanti-tative results demonstrate that, compared to existing base-lines, our method achieves state-of-the-art performance in terms of both generation quality and editing capability.
Tab. 1 compares DisCoScene with relevant works. it is worth noting that, to the best of our knowledge, DisCoScene stands as the first method that achieves high-quality 3D-aware generation on challenging datasets like WAYMO [48], while enabling interactive object manipulation. 2.