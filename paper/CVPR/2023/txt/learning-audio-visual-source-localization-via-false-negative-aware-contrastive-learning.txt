Abstract
Self-supervised audio-visual source localization aims to locate sound-source objects in video frames without extra annotations. Recent methods often approach this goal with the help of contrastive learning, which assumes only the au-dio and visual contents from the same video are positive samples for each other. However, this assumption would suffer from false negative samples in real-world training.
For example, for an audio sample, treating the frames from the same audio class as negative samples may mislead the model and therefore harm the learned representations (e.g., the audio of a siren wailing may reasonably correspond to the ambulances in multiple images). Based on this obser-vation, we propose a new learning strategy named False
Negative Aware Contrastive (FNAC) to mitigate the prob-lem of misleading the training with such false negative sam-ples. Specifically, we utilize the intra-modal similarities to identify potentially similar samples and construct corre-sponding adjacency matrices to guide contrastive learning.
Further, we propose to strengthen the role of true negative samples by explicitly leveraging the visual features of sound sources to facilitate the differentiation of authentic sound-ing source regions. FNAC achieves state-of-the-art perfor-mances on Flickr-SoundNet, VGG-Sound, and AVSBench, which demonstrates the effectiveness of our method in mit-igating the false negative issue. The code is available at https://github.com/OpenNLPLab/FNAC_AVL. 1.

Introduction
When hearing a sound, humans can naturally imagine the visual appearance of the source objects and locate them in the scene. This demonstrates that audio-visual corre-spondence is an important ability for scene understand-ing. Given that unlimited paired audio-visual data ex-ists in nature, there is an emerging interest in developing multi-modal systems with audio-visual understanding abil-*Indicates equal contribution
Figure 1. False negative in audio-visual contrastive learning.
Audio-visual pairs with similar contents are falsely considered as negative samples to each other and pushed apart in the shared la-tent space, which we find would affect the model performance. ity. Various audio-visual tasks have been studied, including sound source localization [8, 19–21, 26–28], audio-visual event localization [32, 33, 35, 39], audio-visual video pars-ing [11, 18, 31] and audio-visual segmentation [37, 38]. In this work, we focus on unsupervised visual sound source lo-calization, with the aim of localizing the sound-source ob-jects in an image using its paired audio clip, but without relying on any manual annotations.
The essence of unsupervised visual sound source local-ization is to leverage the co-occurrences between an audio clip and its corresponding image to extract representations.
A major part of existing methods [8, 19–21, 28] formulates this task as contrastive learning. For each image sample, its paired audio clip is viewed as the positive sample, while all other audio clips are considered as negative. Likewise, each audio clip considers its paired image as positive and all others as negative. As such, the Noise Contrastive Es-timation (NCE) loss [24, 30] is used to perform instance discrimination by pushing closer the distance between a positive audio-image pair, while pulling away any nega-tive pairs. However, the contrastive learning scheme above suffers from the issue of false negatives during training, i.e., audio/image samples that belong to the semantically-matched class but are not regarded as a positive pair (due to the lack of manual labeling). A typical example is shown
in Fig. 1. Research shows [4, 16, 29, 36] that these false negatives will lead to contradictory objectives and harm the representation learning.
Motivated by this observation, we assess the impact of false negatives in real-world training. We discover that with a batch size of 128, around 40% of the samples in VGG-Sound [9] will encounter at least one false negative sample during training. We then validate that false negatives indeed harm performance by artificially increasing the proportion of false negatives during training, and observing a notice-able performance drop. To make matters worse, larger batch sizes are often preferred in contrastive learning [24], but it may inadvertently increase the number of false negative samples during training and affect representation quality.
To this end, we propose a false-negative aware audio-visual contrastive learning framework (FNAC), where we employ the intra-modal similarities as weak supervision.
Specifically, we compute pair-wise similarities between all audio clips in a mini-batch without considering the visual to form an audio intra-modal adjacency matrix. Likewise, in the visual modality, we obtain an image adjacency matrix.
We found that the adjacency matrices effectively identify potential samples of the same class within each modality (Fig. 4). The information can then be used to mitigate the false negatives and enhance the effect of true pairings.
Specifically, we propose two complementary strategies: 1) FNS for False Negatives Suppression, and 2) TNE for
True Negatives Enhancement. First, when optimizing the
NCE loss, FNS regularizes the inter-modal and intra-modal
Intrinsically, intra-modal adjacency explores similarities. potential false negatives by the similarity intensities and the pulling forces applied to these false negatives are canceled accordingly. Furthermore, we introduce TNE to empha-size the true negative influences in a region-wise manner, which in turn reduces the effect of false negative samples as well. We adopt the audio adjacency matrix to identify dissimilar samples, i.e., true negatives. Intuitively, dissimi-lar (true negative) sounds correspond to distinct regions, so the localized regions across the identified true negatives are regularized to be different. Such a mechanism encourages the model to discriminate genuine sound-source regions and suppress the co-occurring quiet objects. we conduct exten-sive analysis to demonstrate the effectiveness of our pro-posed method and report competitive performances across different settings and datasets. In summary, our main con-tributions are:
• We investigate the false negative issue in audio-visual contrastive learning. We quantitatively validate that this issue occurs and harms the representation quality.
• We exploit intra-modal similarities to identify poten-tial false negatives and introduce FNS to suppress their impact.
• We propose TNE, which emphasizes true negatives us-ing different localization results between the identified true negatives, thus encouraging more discriminative sound source localizations. 2.