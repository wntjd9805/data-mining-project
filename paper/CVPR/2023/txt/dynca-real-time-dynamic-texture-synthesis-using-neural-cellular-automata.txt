Abstract 1.

Introduction
Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow it-erative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural
Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by 2 4 orders of magni-tude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an on-line interactive demo that runs on local hardware and is accessible on personal computers and smartphones.
∼
Textures are everywhere. We perceive them as spa-tially repetitive patterns. Dynamic Textures are textures that change over time and induce a sense of motion. Flames, sea waves, and fluttering branches are everyday examples.
Understanding and computationally modeling dynamic tex-tures is an intriguing problem, as these patterns are observed in most natural scenes.
The goal of Dynamic Texture Synthesis (DyTS) [5– 8, 10, 12, 23, 24, 27, 29, 32] is to generate perceptually-equivalent samples of an exemplar video texture3. Ap-plications of DyTS include the creation of special effects for backdrops and video games [21], dynamic style trans-fer [24], and creating cinemagraphs [12].
The state-of-the-art (SOTA) dynamic texture synthesis 1We use the same flow visualization as Baker et al. [2]. 2Link to the demo: https://dynca.github.io 3We use ”video texture” and ”dynamic texture” interchangeably.
motion either by a motion vector field or an exemplar dy-namic texture video. Moreover, by using a different target for the appearance and the motion, our model can perform dynamic style transfer, as shown in Figure 1. Our contribu-tions summarized are:
• Our DyNCA model, once trained, can synthesize dy-namic texture videos in real time.
• Our synthesized videos are on-par with or even better than the existing results in terms of realism and quality.
• After training, our DyNCA model can synthesize infinitely-long videos with arbitrary frame sizes.
• Our DyNCA framework enables several real-time in-teractive video editing controls including speed, direc-tion, a brush tool, and local coordinate transformation.
• We can perform real-time dynamic style transfer by learning appearance and motion from distinct sources. 2.