Abstract
Class incremental semantic segmentation (CISS) fo-cuses on alleviating catastrophic forgetting to improve dis-crimination. Previous work mainly exploits regularization (e.g., knowledge distillation) to maintain previous knowl-edge in the current model. However, distillation alone of-ten yields limited gain to the model since only the repre-sentations of old and new models are restricted to be con-sistent.
In this paper, we propose a simple yet effective method to obtain a model with a strong memory of old knowledge, named Endpoints Weight Fusion (EWF). In our method, the model containing old knowledge is fused with the model retaining new knowledge in a dynamic fusion manner, strengthening the memory of old classes in ever-changing distributions.
In addition, we analyze the rela-tionship between our fusion strategy and a popular mov-ing average technique EMA, which reveals why our method is more suitable for class-incremental learning. To facili-tate parameter fusion with closer distance in the parameter space, we use distillation to enhance the optimization pro-cess. Furthermore, we conduct experiments on two widely used datasets, achieving state-of-the-art performance. 1.

Introduction
As a fundamental task, semantic segmentation plays a key role in visual applications [10, 25]. Previous fully-supervised works aim to segment fixed classes defined in the training set. However, the trained segmentation model is expected to recognize more classes in realistic applica-tions. One straightforward solution is to re-train the model on the entire dataset by mixing old and new data. Never-theless, this strategy will bring huge labeling and training costs. From the transfer learning perspective [22, 30], an-other plain solution is to adjust the previously learned model on the newly added data. But the model will overfit to new
*The first two authors contribute equally.
†Corresponding author (xialei@nankai.edu.cn)
Figure 1. Illustration of different fusion strategies for incremen-tal learning. Ensemble methods utilize multiple models to accu-mulate more knowledge. Compression methods reduce the model size and distill the knowledge into a small network. While Re-parameterization methods use equivalent operations for model fu-sion. Our Endpoints Weight Fusion (EWF) proposes model addi-tion with a dynamic factor (αt) with no further training. classes quickly, while forgetting previous old classes. This phenomenon is also known as catastrophic forgetting [35].
To alleviate the problem of catastrophic forgetting with-out extra labeling or training cost, class incremental se-mantic segmentation (CISS) [3, 16, 50] aims at optimiz-ing the trade-off between maintaining discrimination for old classes and learning knowledge of new classes. Most works [3, 16, 17, 37] designed regularization methods to maintain a balance between memorizing old knowledge and learning new one. We observe that existing works can still suffer from catastrophic forgetting, resulting in a significant
performance drop in old classes. In the scenario of CISS, not only the previous data is not accessible due to privacy issues or data storage limitations, but regions of old classes in the newly added dataset are labeled as background, which further exacerbates the model over-fitting.
Besides, training a new model from the old one and fus-ing them to obtain the final model is a common strategy in continual learning. As shown in Fig. 1, we roughly di-vide them into four categories with two stages of model ex-pansion and fusion. Some methods [27, 33, 42, 47] propose to expand the model in incremental steps and ensemble the old and new outputs, which have large memory and infer-ence costs. While some works apply compression [46, 47] to compress the old and new model to a unified model with fewer parameters. Nevertheless, these require further train-ing on only new data, which can lead to a bias toward new data. Subsequently, some works [50,53] explore knowledge decoupling and perform linear parameter fusion with re-parameterization. However, this is an intra-module fusion strategy, which is restricted to certain operations. As the last category, we propose Endpoints Weight Fusion (EWF) in the form of parameter addition between the old and new model with a dynamic factor, which requires no further training and re-parameterization, and maintains a constant model size as more tasks are encountered.
In this work, we adapt weight fusion to CISS and pro-pose the EWF strategy, which aims at utilizing weight fu-sion to find a new balance between old and new knowledge.
During incremental training, we choose a starting point and an ending point model of the current task training trajectory.
The starting point represents the old knowledge, while the ending point represents the new knowledge. After learn-ing the current task, a dynamic weight fusion is proposed for efficient knowledge integration. We aggregate them by taking the weighted average of the corresponding parame-ters of the two models. Nevertheless, the training procedure without restraints on the model would increase the param-eter distance between the start and end points, limiting the performance improvement brought by the EWF strategy. To overcome this shortcoming, we further enhance the EWF strategy with a knowledge distillation scheme [16, 17, 50], which can largely increase the similarity of the models at the two points and boost the efficiency of EWF.
To summarize, the main contributions of this paper are:
• We propose an Endpoints Weight Fusion strategy, which has no cost of further training and keeps the model size the same.
It can effectively find a new balance between old and new categories and alleviate catastrophic forgetting.
• Our method can be easily integrated with several state-of-the-art methods. In several CISS scenarios of long sequences, it can boost the baseline performance by more than 20%.
• We conduct experiments on various CISS scenarios, which demonstrate that our method achieves the state-of-the-art performance on both PASCAL VOC and
ADE20K. 2.