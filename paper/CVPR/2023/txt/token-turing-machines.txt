Abstract
We propose Token Turing Machines (TTM), a sequen-tial, autoregressive Transformer model with memory for real-world sequential visual understanding. Our model is inspired by the seminal Neural Turing Machine, and has an external memory consisting of a set of tokens which sum-marise the previous history (i.e., frames). This memory is efﬁciently addressed, read and written using a Transformer as the processing unit/controller at each step. The model’s memory module ensures that a new observation will only be processed with the contents of the memory (and not the entire history), meaning that it can efﬁciently process long sequences with a bounded computational cost at each step.
We show that TTM outperforms other alternatives, such as other Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential vi-sual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning.
Code is publicly available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token turing. 1.

Introduction
Processing long, sequential visual inputs in a causal man-ner is a problem central to numerous applications in robotics and vision. For instance, human activity recognition mod-els for monitoring patients and elders are required to make real-time inference on ongoing activities from streaming videos. As the observations grow continuously, these models require an efﬁcient way of summarizing and maintaining information in their past image frames with limited com-pute. Similarly, robots learning their action policies from training videos, need to abstract history of past observations and leverage it when making its action decisions in real-time.
This is even more important if the robot is required to learn complicated tasks with longer temporal horizons.
A traditional way of handling online observations of variable sequence lengths is to use recurrent neural net-works (RNNs), which are sequential, auto-regressive mod-els [13, 22, 35]. As Transformers [64] have become the de facto model architecture for a range of perception tasks, sev-eral works have proposed variants which can handle longer sequence lengths [19, 61, 67]. However, in streaming, or sequential inference problems, efﬁcient attention operations for handling longer sequence lengths themselves are often not sufﬁcient since we do not want to run our entire trans-former model for each time step when a new observation (e.g., a new frame) is provided. This necessitates developing models with explicit memories, enabling a model to fuse relevant past history with current observation to make a pre-diction at current time step. Another desideratum for such models, to scale to long sequence lengths, is that the compu-tational cost at each time step should be constant, regardless of the length of the previous history.
In this paper, we propose Token Turing Machines (TTMs), a sequential, auto-regressive model with external memory and constant computational time complexity at each step.
Our model is inspired by Neural Turing Machines [30] (NTM), an inﬂuential paper that was among the ﬁrst to propose an explicit memory and differentiable addressing mechanism. The original NTM was notorious for being a complex architecture that was difﬁcult to train, and it has therefore been largely forgotten as other modelling advances have been made in the ﬁeld. However, we show how we can formulate an external memory as well as a processing unit that reads and writes to this memory using Transformers (plus other operations which are common in modern Trans-former architectures). Another key component of TTMs is a token summarization module, which provides an inductive bias that intuitively encourages the memory to specialise to different parts of its history during the reading and writing operations. Moreover, this design choice ensures that the computational cost of our network is constant irrespective of the sequence length, enabling scalable, real-time, online inference.
In contrast to the original NTM, our Transformer-based modernisation is simple to implement and train. We demon-strate its capabilities by achieving substantial improvements over strong baselines in two diverse and challenging tasks: (1) online temporal action detection (i.e., localisation) from videos and (2) vision-based robot action policy learning.
2.