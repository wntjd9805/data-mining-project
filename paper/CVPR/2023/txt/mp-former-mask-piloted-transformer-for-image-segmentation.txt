Abstract
We present a mask-piloted Transformer which improves masked-attention in Mask2Former for image segmenta-tion. The improvement is based on our observation that
Mask2Former suffers from inconsistent mask predictions between consecutive decoder layers, which leads to incon-sistent optimization goals and low utilization of decoder queries. To address this problem, we propose a mask-piloted training approach, which additionally feeds noised ground-truth masks in masked-attention and trains the model to reconstruct the original ones. Compared with the predicted masks used in mask-attention, the ground-truth masks serve as a pilot and effectively alleviate the negative impact of inaccurate mask predictions in Mask2Former. Based on this technique, our MP-Former achieves a remarkable perfor-mance improvement on all three image segmentation tasks (instance, panoptic, and semantic), yielding +2.3AP and
+1.6mIoU on the Cityscapes instance and semantic seg-mentation tasks with a ResNet-50 backbone. Our method also significantly speeds up the training, outperforming
Mask2Former with half of the number of training epochs on ADE20K with both a ResNet-50 and a Swin-L backbones.
Moreover, our method only introduces little computation dur-ing training and no extra computation during inference. Our code will be released at https://github.com/IDEA-Research/MP-Former. 1.

Introduction
Image segmentation is a fundamental problem in com-puter vision which includes semantic, instance, and panoptic
*Equal contribution.
†This work was done when Hao Zhang and Feng Li were interns at
IDEA.
‡Corresponding author. segmentation. Early works design specialized architectures for different tasks, such as mask prediction models [2, 15] for instance segmentation and per-pixel prediction mod-els [25, 28] for semantic segmentation. Panoptic segmen-tation [18] is a unified task of instance and semantic seg-mentation. But universal models for panoptic segmentation such as Panoptic FPN [18] and K-net [34] are usually sub-optimal on instance and semantic segmentation compared with specialized models.
Recently, Vision Transformers [29] has achieved tremen-dous success in many computer vision tasks, such as image classification [11, 24] and object detection [1]. Inspired by
DETR’s set prediction mechanism, Mask2Former [7] pro-poses a unified framework and achieves a remarkable per-formance on all three segmentation tasks. Similar to DETR, it uses learnable queries in Transformer decoders to probe features and adopts bipartite matching to assign predictions to ground truth (GT) masks. Mask2Former also uses pre-dicted masks in a decoder layer as attention masks for the next decoder layer. The attention masks serve as a guidance to help next layer’s prediction and greatly ease training.
Despite its great success, Mask2Former is still an initial attempt which is mainly based on a vanilla DETR model that has a sub-optimal performance compared to its later variants. For example, in each decoder layer, the predicted masks are built from scratch by dot-producting queries and feature map without performing layer-wise refinement as proposed in deformable DETR [37]. Because each layer build masks from scratch, the masks predicted by a query in different layers may change dramatically. To show this inconsistent predictions among layers, we build two metrics mIoU-Li and Utili to quantify this problem and analyze its consequences in Section 3. As a result, this problem leads to a low utilization rate of decoder queries, which is especially severe in the first few decoder layers. As the attention masks predicted from an early layer are usually inaccurate, when serving as a guidance for its next layer, they will lead to
sub-optimal predictions in the next layer.
Since detection is a similar task to segmentation, both aiming to locate instances in images, we seek inspirations from detection to improve segmentation. For example, the recently proposed denoising training [19] for detection has shown very effective to stabilize bipartite matching between training epochs. DINO [33] achieves a new SOTA on COCO detection built upon an improved denoising training method.
The key idea in denoising training is to feed noised GT boxes in parallel with learnable queries into Transformer decoders and train the model to denoise and recover the GT boxes.
Inspired by DN-DETR [19], we propose a mask-piloted (MP) training approach. We divide the decoder queries into two parts, an MP part and a matching part. The matching part is the same as in Mask2Former, feeding learnable queries and adopting bipartite matching to assign predictions to GT instances. In the MP part, we feed class embeddings of GT categories as queries and GT masks as attention masks into a Transformer decoder layer and let the model to reconstruct
GT masks and labels. To reinforce the effectiveness of our method, we propose to apply MP training for all decoder layers. Moreover, we also add point noises to GT masks and flipping noises to GT class embeddings to force the model to recover GT masks and labels from inaccurate ones.
We summarize our contributions as follows. 1. We propose a mask-piloted training approach to im-proving masked-attention in Mask2Former, which suf-fers from inconsistent and inaccurate mask predictions among layers. We also develop techniques including multi-layer mask-piloted training with point noises and label-guided training to further improve the segmenta-tion performance. Our method is computationally ef-ficient, introducing no extra computation during infer-ence and little computation during training. 2. Through analyzing the predictions of Mask2Former, we discover a failure mode of Mask2Former—inconsistent predictions between consecutive layers. We build two novel metrics to measure this failure, which are layer-wise query utilization and mean IoU. We also show that our method improves the two metrics by a large margin, which validates its effectiveness on alleviating inconsistent predictions. 3. When evaluating on three datasets, including ADE20k,
Cityscapes, and MS COCO, our MP-Former outper-forms Mask2Former by a large margin. Besides im-proved performance, our method also speeds up train-ing. Our model exceeds Mask2Former on all three seg-mentation tasks with only half of the number of train-ing steps on ADE20k. Also, our model trained for 36 epochs is comparable with Mask2Former trained for 50 epochs on instance and panoptic segmentation on MS
COCO. 2.