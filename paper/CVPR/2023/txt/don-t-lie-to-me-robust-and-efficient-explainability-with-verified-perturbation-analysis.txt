Abstract
A plethora of attribution methods have recently been de-veloped to explain deep neural networks. These methods use different classes of perturbations (e.g, occlusion, blur-ring, masking, etc) to estimate the importance of individ-ual image pixels to drive a model’s decision. Neverthe-less, the space of possible perturbations is vast and cur-rent attribution methods typically require signiﬁcant com-putation time to accurately sample the space in order to achieve high-quality explanations. In this work, we intro-duce EVA (Explaining using Veriﬁed Perturbation Analysis) – the ﬁrst explainability method which comes with guaran-tees that an entire set of possible perturbations has been exhaustively searched. We leverage recent progress in ver-iﬁed perturbation analysis methods to directly propagate bounds through a neural network to exhaustively probe a – potentially inﬁnite-size – set of perturbations in a single forward pass. Our approach takes advantage of the bene-ﬁcial properties of veriﬁed perturbation analysis, i.e., time efﬁciency and guaranteed complete – sampling agnostic – coverage of the perturbation space – to identify image pixels that drive a model’s decision. We evaluate EVA systemat-ically and demonstrate state-of-the-art results on multiple benchmarks. Our code is freely available: github.com/ deel-ai/formal-explainability 1.

Introduction
Deep neural networks are now being widely deployed in many applications from medicine, transportation, and secu-rity to ﬁnance, with broad societal implications [40]. They are routinely used to making safety-critical decisions – of-ten without an explanation as their decisions are notoriously
Figure 1. Manifold exploration of current attribution meth-ods. Current methods assign an importance score to individ-ual pixels using perturbations around a given input image x.
Saliency [56] uses inﬁnitesimal perturbations around x, Occlu-sion [71] switches individual pixel intensities on/off. More recent approaches [17,43,46,48,49] use (Quasi-) random sampling meth-ods in speciﬁc perturbation spaces (occlusion of segments of pix-els, blurring, ...). However, the choice of the perturbation space un-doubtedly biases the results – potentially even introducing serious artifacts [26, 29, 38, 64]. We propose to use veriﬁed perturbation analysis to efﬁciently perform a complete coverage of a perturba-tion space around x to produce reliable and faithful explanations. hard to interpret.
Many explainability methods have been proposed to gain insight into how network models arrive at a particular deci-sion [17,24,43,46,48,49,53,55,61,65,71]. The applications of these methods are multiple – from helping to improve or debug their decisions to helping instill conﬁdence in the reliability of their decisions [14]. Unfortunately, a severe limitation of these approaches is that they are subject to a conﬁrmation bias: while they appear to offer useful expla-nations to a human experimenter, they may produce incor-rect explanations [2, 23, 59]. In other words, just because the explanations make sense to humans does not mean that they actually convey what is actually happening within the model. Therefore, the community is actively seeking for better benchmarks involving humans [12, 29, 37, 45].
In the meantime, it has been shown that some of our current and commonly used benchmarks are biased and that explainability methods reﬂect these biases – ultimately providing the wrong explanation for the behavior of the model [25, 29, 64]. For example, some of the current ﬁ-delity metrics [7, 18, 27, 34, 48] mask one or a few of the input variables (with a ﬁxed value such as a gray mask) in order to assess how much they contribute to the output of the system. Trivially, if these variables are already set to the mask value in a given image (e.g., gray), masking these variables will not yield any effect on the model’s output and the importance of these variables is poised to be underesti-mated. Finally, these methods rely on sampling a space of perturbations that is far too vast to be fully explored – e.g.,
LIME on a image divided in 64 segments image would need more than 1019 samples to test all possible perturbations.
As a result, current attribution methods may be subject to bias and are potentially not entirely reliable.
To address the baseline issue, a growing body of work is starting to leverage adversarial methods [8, 29, 31, 42, 50] to derive explanations that reﬂect the robustness of the model to local adversarial perturbations. Speciﬁcally, a pixel or an image region is considered important if it allows the easy generation of an adversarial example. That is if a small perturbation of that pixel or image region yields a large change in the model’s output. This idea has led to the design of several novel robustness metrics to evaluate the quality of explanations, such as Robustness-Sr [29]. For a better ranking on those robustness metrics, several meth-ods have been proposed that make intensive use of adversar-ial attacks [29, 70], such as Greedy-AS for Robustness-Sr.
However, these methods are computationally very costly – in some cases, requiring over 50 000 adversarial attacks per explanation – severely limiting the widespread adoption of these methods in real-world scenarios.
In this work, we propose to address this limitation by introducing EVA (Explaining using Veriﬁed perturbation
Analysis), a new explainability method based on robustness analysis. Veriﬁed perturbation analysis is a rapidly growing toolkit of methods to derive bounds on the outputs of neural networks in the presence of input perturbations. In contrast to current attributions methods based on gradient estimation or sampling, veriﬁed perturbation analysis allows the full exploration of the perturbation space, see Fig. 1. We use a tractable certiﬁed upper bound of robustness conﬁdence to derive a new estimator to help quantify the importance of input variables (i.e., those that matter the most). That is, the variables most likely to change the predictor’s decision.
We can summarize our main contributions as follows:
• We introduce EVA, the ﬁrst explainability method guar-anteed to explore its entire set of perturbations using Ver-iﬁed Perturbation Analysis.
• We propose a method to scale EVA to large vision models and show that the exhaustive exploration of all possible perturbations can be done efﬁciently.
• We systematically evaluate our approach using several image datasets and show that it yields convincing results on a large range of explainability metrics
• Finally, we demonstrate that we can use the proposed method to generate class-speciﬁc explanations, and we study the effects of several veriﬁed perturbation analysis methods as a hyperparameter of the generated explana-tions. 2.