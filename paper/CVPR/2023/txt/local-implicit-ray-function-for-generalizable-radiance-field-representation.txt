Abstract
We propose LIRF (Local Implicit Ray Function), a gen-eralizable neural rendering approach for novel view render-ing. Current generalizable neural radiance fields (NeRF) methods sample a scene with a single ray per pixel and may therefore render blurred or aliased views when the input views and rendered views capture scene content with dif-ferent resolutions. To solve this problem, we propose LIRF to aggregate the information from conical frustums to con-struct a ray. Given 3D positions within conical frustums,
LIRF takes 3D coordinates and the features of conical frus-tums as inputs and predicts a local volumetric radiance field. Since the coordinates are continuous, LIRF renders high-quality novel views at a continuously-valued scale via volume rendering. Besides, we predict the visible weights for each input view via transformer-based feature matching
*Work was done during an internship at Tencent AI Lab.
†Corresponding authors. to improve the performance in occluded areas. Experimen-tal results on real-world scenes validate that our method outperforms state-of-the-art methods on novel view render-ing of unseen scenes at arbitrary scales. 1.

Introduction
Novel view synthesis has garnered recent attention with compelling applications of neural rendering in virtual and augmented reality. Different from image-based rendering
[6, 19, 32, 42, 74], Neural Radiance Fields (NeRF) [43] im-plicitly represents the 3D scenes within multilayer percep-trons (MLPs) by mapping coordinates to color and geom-etry of scenes. To render a pixel, the ray projected to that pixel is traced and the color of each sampled point along the ray is accumulated based on volume rendering.
Despite NeRF and its variants having demonstrated re-markable performance in providing immersive experiences in various view synthesis tasks, their practical applications
are constrained by the requirement of training from scratch on each new scene, which is time-consuming. To overcome this problem, many researches [10, 12, 29, 33, 37, 58, 64, 70] introduce image-based rendering techniques to NeRF, which achieves generalization on unseen scenes. They take into consideration the image features (from nearby views) of a 3D point. The common motivation is to predict the density and color of this point by matching the multi-view features, which is similar to the stereo matching methods
[20, 54, 68] that find a surface point by checking the consis-tency of multi-view features.
While these methods generalize well on new scenes when the distance of input and testing views are roughly constant from the scene (as in NeRF), they cannot prop-erly deal with the less constrained settings such as differ-ent resolutions or varying focal length and produce results with blurring or aliasing artifacts. Since a single ray is cast through each pixel whose size and shape are ignored, it’s challenging to query the accurate feature of the target ray from input images as shown in Fig. 2(a), and the model learns an ambiguous result as shown in Fig. 2(b). Mip-NeRF [3], a NeRF variant of per-scene optimization, pro-poses an anti-aliasing design that models the ray through a pixel as a cone and uses a 3D Gaussian to approximate the sampled conical frustum (a cone cut perpendicular to its axis) for volumetric representation. However, directly ex-tending Mip-NeRF to a generalizable method is also chal-lenging to extract the accurate features of the ray from input images due to the subpixel precision. Consequently, an ef-ficient solution is to supersample each pixel by marching multiple rays according to its footprint, similar to the strat-egy used in offline raytracing.
Our key insight is the local implicit ray function (LIRF) that represents the feature aggregation of samples within ray conical frustum in a continuous manner, as shown in Fig. 1.
Specifically, given any 3D sampled position within a con-ical frustum, our LIRF outputs the aggregated feature by taking the 3D coordinate of this position and the features of vertices within the conical frustum as inputs (the vertices of a conical frustum are defined with eight points (red points) as shown in Fig. 1). The continuous sampled position al-lows our method to arbitrarily upsample the rendered rays and thus synthesize novel views of the same unseen scene at multiple levels of detail (anti-blurring and anti-aliasing).
Furthermore, recent generalizable NeRF methods [29, 37] introduce multi-view depth estimation to reduce the arti-facts caused by occlusions, but it is computationally expen-sive to construct the cost volume for each view. We instead match local multi-view feature patches to estimate the vis-ibility weights of each sample for anti-occlusion. Overall, our main contributions are: 1. A new generalizable approach that renders pixels by casting cones and outperforms existing methods on
Figure 2. Most generalizable variants of NeRF represent a ray as a set of infinitesimal samples (shown here as dots) along that ray and map these samples into input views to query image features for volumetric representation prediction. However, this results in two drawbacks when training on multi-scale images with less con-strained settings: (a) Inaccurate features. The sampling strategy which ignores the shape and size of each ray is difficult to query accurate image features. (b) Ambiguous supervisions. The same 3D position captured by cameras under different scales results in different colors because these pixels are the integral of regions with different shapes and sizes (shown here as trapezoids). During the training, the network learns to map the same image features (from the source view) to these different colors, which causes am-biguous results. novel view synthesis at multiple scales. 2. A local implicit ray function that simplifies the repre-sentation of conical frustums and enables continuous supersampling of rays. 3. A transformer-based visibility weight estimation mod-ule that alleviates the occlusion problem.
To evaluate our method, we construct extensive series of ex-periments on real forward-facing scenes. Our experiments show that trained on large amounts of multi-view data,
LIRF outperforms state-of-the-art generalizable NeRF-like methods on novel views synthesis for unseen scenes. 2.