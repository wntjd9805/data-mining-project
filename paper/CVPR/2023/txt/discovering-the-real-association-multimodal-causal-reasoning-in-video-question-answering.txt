Abstract
Video Question Answering (VideoQA) is challenging as it requires capturing accurate correlations between modal-ities from redundant information. Recent methods focus on the explicit challenges of the task, e.g. multimodal feature extraction, video-text alignment and fusion. Their frameworks reason the answer relying on statistical evi-dence causes, which ignores potential bias in the multi-modal data. In our work, we investigate relational structure from a causal representation perspective on multimodal data and propose a novel inference framework. For vi-sual data, question-irrelevant objects may establish simple matching associations with the answer. For textual data, the model prefers the local phrase semantics which may de-viate from the global semantics in long sentences. There-fore, to enhance the generalization of the model, we dis-cover the real association by explicitly capturing visual features that are causally related to the question seman-tics and weakening the impact of local language seman-tics on question answering. The experimental results on two large causal VideoQA datasets verify that our pro-posed framework 1) improves the accuracy of the existing
VideoQA backbone, 2) demonstrates robustness on com-plex scenes and questions. The code will be released at https://github.com/Chuanqi-Zang/Discovering-the-Real-Association. 1.

Introduction
Video Question Answering (VideoQA) aims to un-derstand visual information and describe it in language question-answer format, which is a natural cognitive capa-bility for humans. Computer Vision (CV) and Natural Lan-guage Processing (NLP) as the base models for VideoQA have shown significant progress due to the successful appli-cation of deep learning, such as action classification [25], object detection [10], instance segmentation [31], and large-scale pre-trained language model [6,19]. These tremendous
*Wei Liang is the corresponding author.
Figure 1. Two samples in VideoQA dataset. They exhibit spurious reasoning processes of B2A [26] that rely on statistical patterns, including visually spurious object-relationship associations (top) and textually unilateral semantic representations (bottom). advances in basic applications fuel the confidence in fine-grained multimodal analysis and reasoning, not only fea-ture extraction, but also fine-grained general causality esti-mation, which is critical for a robust cognitive system.
Recent VideoQA methods usually explore multimodal relational knowledge by sophisticated structured architec-ture, such as memory-augmented model [9], hierarchical model [20], topological model [18], and transformer-based model [37]. Although experiments validate their feature fusion capabilities, we find that these methods concentrate on statistical association based on multimodal data, ignor-ing the real stable association. They usually use a gener-ally constrained approach with Empirical Risk Minimiza-tion (ERM), which tends to over-rely on the co-occurrence bias of repeated objects and words in the collected observa-tional data, and bypasses the impact of complete semantics at the sentence level. This mechanism reduces the robust-ness of the model on new data, even in the test set which has similar distributions to the training set.
For example, as shown in Fig. 1 (top), two people are playing ”cricket” indoors, and there are other objects in the room, including a TV that is on. If relying on the statisti-cal relationship, the model may be confused by the two im-portant visual factors of ”indoor” and ”television”, and mis-judge the concerned event, that is, ”sing karaoke”. Based on the clues provided by the question, the concerned event is related to the action that is taking place, e.g. ”two men play-ing cricket”. This requires the model to accurately judge the objects involved in the event and infer the answer. In
Fig. 1 (bottom), we show the predicted deviation caused by statistical relationships in the text. When inferring the rea-sons for answer selection, existing models intensively rely on correlations between local words and video content, e.g.
”someone; help”, ignoring unreasonable inferences from other parts of the sentence, e.g. ”a lot”.
From these observations, we summarize two causal chal-lenges for the VideoQA task. 1) Irrelevant objects con-founder. The visual information related to the question is usually causally related to finite objects in the video. When other objects or background are considered, they are sub-ject to data bias and become confounders, misleading the model to select the negative candidate. 2) Keywords con-founder. The semantics expressed by textual information is represented by the overall sentence. Some long sentences contain partially sensible keywords. When just focusing on local keywords semantics, the model falls into spurious causal inferences, reducing the robustness of the model.
To address the above causal challenges and improve the robustness of the model, we propose a Multimodal Causal
Reasoning (MCR) framework for video-text data. In this framework, causal features and confounding features are decoupled separately in visual and textual modes through two training strategies. To explicitly decouple the influence of different objects and scenes, MCR extracts fine-gained object-level appearance features and motion dynamics by spatial Region of Interest (ROI) Align [13] on global vi-sual features. Among them, causally related objects are se-lected based on the correlation between object features and question semantics.
In addition to the visual feature, we also model object coordinate information, category infor-mation, and global object interaction information to pro-vide spatio-temporal relation representations for accurate causal attribute classification. For textual confounders, we adopt a strategy to reduce the impact of keywords on causal-ity. MCR relies on the correlation between word encod-ing and question-visual co-embedding to select keywords which have a crucial impact on the prediction results. These keywords provide negative representations for successive deductive answers. Therefore, we combine these keywords with other candidate answers to generate difficult negative samples to improve the recognition ability of the model.
During training, visual intervention and textual interven-tion are iteratively optimized. Multimodal causal relation-ships are gradually established which improves the robust-ness and reusability of the model.
We summarize our contributions as: (1) We discover two new types of causal challenges for both visual data and tex-tual data. (2) We propose an object-level causal relationship extraction strategy to establish the real association between objects and language semantics, and a keyword broadcast-ing strategy to cut off the spurious influence of local textual information. (3) We achieve state-of-the-art performance on two latest large causal VideoQA datasets. 2.