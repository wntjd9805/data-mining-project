Abstract
Open-vocabulary scene understanding aims to localize and recognize unseen categories beyond the annotated la-bel space. The recent breakthrough of 2D open-vocabulary perception is largely driven by Internet-scale paired image-text data with rich vocabulary concepts. However, this success cannot be directly transferred to 3D scenarios due to the inaccessibility of large-scale 3D-text pairs. To this end, we propose to distill knowledge encoded in pre-trained vision-language (VL) foundation models through captioning multi-view images from 3D, which allows ex-plicitly associating 3D and semantic-rich captions. Fur-ther, to foster coarse-to-fine visual-semantic representa-tion learning from captions, we design hierarchical 3D-caption pairs, leveraging geometric constraints between 3D scenes and multi-view images. Finally, by employ-ing contrastive learning, the model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method not only remarkably outperforms base-line methods by 25.8% ∼ 44.7% hIoU and 14.5% ∼ 50.4% hAP50 in open-vocabulary semantic and instance segmen-tation, but also shows robust transferability on challenging zero-shot domain transfer tasks. See the project website at https://dingry.github.io/projects/PLA. 1.

Introduction 3D scene understanding is a fundamental perception component in real-world applications such as robot manipu-lation, virtual reality and human-machine interaction. Deep learning has attained remarkable success in this area [13, 38, 28]. However, deep models trained on a human-annotated dataset are only capable of understanding semantic cate-gories in that dataset, i.e. closet-set prediction. As a result, they fail to recognize unseen categories in the open world (see Fig. 1). This largely restricts their applicability in real-world scenarios with unbounded categories. Besides, heavy annotation costs on 3D datasets (e.g. 22.3 minutes for one scene with 20 classes [7]) further make it infeasible to rely
*Equal contribution: {ryding, jhyang}@eee.hku.hk
†Part of the work is done during an internship at ByteDance AI Lab.
‡Corresponding authors: song.site@gmail.com, xjqi@eee.hku.hk
Figure 1. An example of 3D open-vocabulary scene understanding with “bookshelf” as unseen class for ScanNet [7]. The close-set model mistakes “bookshelf” as “cabinet” or simply misses “book-shelf” in (a) and (c). Our open-vocabulary model correctly local-izes and recognizes “bookshelf” in (b) and (d). on human labor to cover all real-world categories.
This motivates us to study open-vocabulary 3D scene un-derstanding, which equips a model with the ability to local-ize and recognize open-set classes beyond the label space of an annotated dataset (see Fig. 1). Recently, vision-language (VL) foundation models [33, 22, 47] trained on billions of web-crawled image data with semantic-rich captions [36] are capable of learning adequate vision-language embed-dings to connect text and image, which are further leveraged to solve many 2D open-vocabulary tasks including object detection [15, 35], semantic segmentation [43, 26, 51], vi-sual question answering [31] and etc. Albeit significantly advancing open-vocabulary image understanding tasks, this pre-training paradigm is not directly viable in the 3D do-main due to the absence of large-scale 3D-text pairs.
To this end, initial efforts [50, 20] have attempted to project 3D data into 2D modalities, such as RGB images and depth maps, enabling pre-trained VL foundation mod-els to process the 2D data and achieve object-level open-vocabulary recognition. Nevertheless, this line of methods suffers from several major issues, making it suboptimal to handle scene-level understanding tasks (e.g., instance seg-mentation). First, multiple RGB images and depth maps are required to represent a 3D sample, which incurs heavy com-putation and memory costs during training and inference.
Second, the projection from 3D to 2D induces information
loss and prohibits direct learning from rich 3D data, lead-ing to subpar performance. Our preliminary study shows the cutting-edge 2D open-vocabulary semantic segmenta-tion method MaskCLIP [51] attains a mere 17.8% mIoU with a 20-fold increase in latency when applied to analyze projected 2D images from 3D ScanNet dataset.
Thus, considering the success of VL foundation mod-els for a variety of vision-language tasks [15, 35, 43, 26, 51, 50, 20], we ask: is it possible to elicit knowledge encoded in powerful VL foundation models to build an explicit associa-tion between 3D and language for open-vocabulary under-standing? To this end, our core idea is to exploit pre-trained
VL foundation models [1, 39] to caption easily-obtained im-age data aligned with 3D data (i.e. the point set in the corre-sponding frustum to produce the image). Note that these images can be acquired through neural rendering [9, 46] or from the 3D data collection pipeline [7]. By doing so, we can distill semantic-rich textual descriptions to the 3D domain, which allows explicit association between 3D and vocabulary-rich text for zero-shot 3D scene understanding.
Given 3D-language association, the next question is en-abling a 3D network to learn language-aware embeddings from (pseudo) captions. The key challenge stems from intri-cate object compositions in 3D scene-level data (see Fig. 3), making it difficult to connect objects with corresponding words in the caption. This differs from object-centric image data containing a single centered object [33]. Fortunately, the captioned multi-view images from a 3D scene are re-lated by 3D geometry, which can be leveraged to build hi-erarchical point-caption pairs, including scene-, view- and entity-level captions. These multi-level point-caption pairs offer coarse-to-fine supervision signals, facilitating learning adequate visual-semantic representations from rich vocabu-lary by contrastive learning. Without task-specific design, our Point-Language Association paradigm, namely PLA, is generic for various open-vocabulary 3D scene understand-ing tasks, such as semantic and instance segmentation.
Experimental results for ScanNet [7] and S3IDS [2] datasets show the effectiveness of our method in in-domain open-vocabulary tasks with only category shifts, i.e. train-ing and evaluation are conducted on the same dataset, sur-passing baselines by 25.8% ∼ 44.7% hIoU on semantic seg-mentation and 14.5% ∼ 50.4% hAP50 on instance segmen-tation. Besides, our model, trained on a dataset (i.e. Scan-Net), can generalize to another dataset (i.e. S3IDS) with both data distribution and category shifts, manifesting its transferability. Finally, our model can benefit from more ad-vanced foundation models that provide higher-quality cap-tion supervision, showing its scalability and extensibility. 2.