Abstract
In this work we study the benefits of using tracking and 3D poses for action recognition. To achieve this, we take the
Lagrangian view on analysing actions over a trajectory of human motion rather than at a fixed point in space. Taking this stand allows us to use the tracklets of people to pre-dict their actions. In this spirit, first we show the benefits of using 3D pose to infer actions, and study person-person in-teractions. Subsequently, we propose a Lagrangian Action
Recognition model by fusing 3D pose and contextualized appearance over tracklets. To this end, our method achieves state-of-the-art performance on the AVA v2.2 dataset on both pose only settings and on standard benchmark settings.
When reasoning about the action using only pose cues, our pose model achieves +10.0 mAP gain over the correspond-ing state-of-the-art while our fused model has a gain of +2.8 mAP over the best state-of-the-art model. Code and results are available at: https://brjathu.github.io/LART 1.

Introduction
In fluid mechanics, it is traditional to distinguish between the Lagrangian and Eulerian specifications of the flow field.
Quoting the Wikipedia entry, “Lagrangian specification of the flow field is a way of looking at fluid motion where the observer follows an individual fluid parcel as it moves through space and time. Plotting the position of an indi-vidual parcel through time gives the pathline of the parcel.
This can be visualized as sitting in a boat and drifting down a river. The Eulerian specification of the flow field is a way of looking at fluid motion that focuses on specific locations in the space through which the fluid flows as time passes.
This can be visualized by sitting on the bank of a river and watching the water pass the fixed location.”
These concepts are very relevant to how we analyze videos of human activity.
In the Eulerian viewpoint, we would focus on feature vectors at particular locations, either (x, y) or (x, y, z), and consider evolution over time while
In the Lagrangian staying fixed in space at the location. viewpoint, we would track, say a person over space-time and track the associated feature vector across space-time.
While the older literature for activity recognition e.g.,
[11, 18, 53] typically adopted the Lagrangian viewpoint, ever since the advent of neural networks based on 3D space-time convolution, e.g., [50], the Eulerian viewpoint became standard in state-of-the-art approaches such as SlowFast
Networks [16]. Even after the switch to transformer archi-tectures [12, 52] the Eulerian viewpoint has persisted. This is noteworthy because the tokenization step for transform-ers gives us an opportunity to freshly examine the question,
“What should be the counterparts of words in video analy-sis?”. Dosovitskiy et al. [10] suggested that image patches were a good choice, and the continuation of that idea to video suggests that spatiotemporal cuboids would work for video as well.
On the contrary, in this work we take the Lagrangian viewpoint for analysing human actions. This specifies that we reason about the trajectory of an entity over time. Here, the entity can be low-level, e.g., a pixel or a patch, or high-level, e.g., a person. Since, we are interested in understand-ing human actions, we choose to operate on the level of
“humans-as-entities”. To this end, we develop a method that processes trajectories of people in video and uses them to recognize their action. We recover these trajectories by capitalizing on a recently introduced 3D tracking method
PHALP [43] and HMR 2.0 [19]. As shown in Figure 1
PHALP recovers person tracklets from video by lifting peo-ple to 3D, which means that we can both link people over a series of frames and get access to their 3D representa-tion. Given these 3D representations of people (i.e., 3D pose and 3D location), we use them as the basic content of each token. This allows us to build a flexible system where the model, here a transformer, takes as input tokens corre-sponding to the different people with access to their identity, 3D pose and 3D location. Having 3D location of the peo-ple in the scene allow us to learn interaction among people.
Our model relying on this tokenization can benefit from 3D tracking and pose, and outperforms previous baseline that only have access to pose information [8, 45].
While the change in human pose over time is a strong signal, some actions require more contextual information about the appearance and the scene. Therefore, it is im-portant to also fuse pose with appearance information from
Figure 1. Overview of our method: Given a video, first, we track every person using a tracking algorithm (e.g. PHALP [43]). Then every detection in the track is tokenized to represent a human-centric vector (e.g. pose, appearance). To represent 3D pose we use SMPL [35] parameters and estimated 3D location of the person, for contextualized appearance we use MViT [12] (pre-trained on MaskFeat [59]) features. Then we train a transformer network to predict actions using the tracks. Note that, at the second frame we do not have detection for the blue person , at these places we pass a mask token to in-fill the missing detections. humans and the scene, coming directly from pixels. To achieve this, we also use the state-of-the-art models for ac-tion recognition [12, 34] to provide complementary infor-mation from the contextualized appearance of the humans and the scene in a Lagrangian framework. Specifically, we densely run such models over the trajectory of each tracklet and record the contextualized appearance features localized around the tracklet. As a result, our tokens include explicit information about the 3D pose of the people and densely sampled appearance information from the pixels, processed by action recognition backbones [12]. Our complete system outperforms the previous state of the art by a large margin of 2.8 mAP, on the challenging AVA v2.2 dataset.
Overall, our main contribution is introducing an ap-proach that highlights the effects of tracking and 3D poses for human action understanding. To this end, in this work, we propose a Lagrangian Action Recognition with
Tracking (LART) approach, which utilizes the tracklets of people to predict their action. Our baseline version lever-ages tracklet trajectories and 3D pose representations of the people in the video to outperform previous baselines utiliz-ing pose information. Moreover, we demonstrate that the proposed Lagrangian viewpoint of action recognition can be easily combined with traditional baselines that rely only on appearance and context from the video, achieving signif-icant gains compared to the dominant paradigm. 2.