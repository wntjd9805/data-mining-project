Abstract
Recent works have shown that unstructured text (doc-uments) from online sources can serve as useful auxiliary information for zero-shot image classification. However, these methods require access to a high-quality source like
Wikipedia and are limited to a single source of information.
Large Language Models (LLM) trained on web-scale text show impressive abilities to repurpose their learned knowl-edge for a multitude of tasks. In this work, we provide a novel perspective on using an LLM to provide text supervi-sion for a zero-shot image classification model. The LLM is provided with a few text descriptions from different annota-tors as examples. The LLM is conditioned on these exam-ples to generate multiple text descriptions for each class (re-ferred to as views). Our proposed model, I2MVFormer, learns multi-view semantic embeddings for zero-shot image classification with these class views. We show that each text view of a class provides complementary information allow-ing a model to learn a highly discriminative class embed-ding. Moreover, we show that I2MVFormer is better at con-suming the multi-view text supervision from LLM compared to baseline models. I2MVFormer establishes a new state-of-the-art on three public benchmark datasets for zero-shot im-age classification with unsupervised semantic embeddings.
Code available at https://github.com/ferjad/I2DFormer 1.

Introduction
In Zero-Shot Learning (ZSL), we task an image classifi-cation model trained on a set of seen classes to generalize to a disjoint set of unseen classes using shared auxiliary in-formation. While there has been great progress made in the field, most works treat the auxiliary information to be fixed to a set of human-labeled attributes [16, 37, 48, 53]. While powerful, these attributes are hard to annotate and expen-sive to scale [46,58]. Unsupervised alternatives to attributes rely on pretrained word embeddings which provide limited 1First and second author contributed equally.
Figure 1. Different annotators focus on different attributes when describing a class. Large Language Models prompted with each of these annotations as k-shot examples can reveal complementary information about a class for zero-shot image classification. We refer to multiple LLM-generated descriptions as views of a class. information about a class. Recent works [7,20,35,39] show that text documents from internet sources like Wikipedia can provide great auxiliary information for ZSL. Since these web documents describe a queried class in detail, they pro-vide more information for the ZSL model compared to word embeddings. However, these methods only rely on a single source of text documents like Wikipedia, which might not sufficiently represent all classes a model is faced with. Mul-tiple sources of text documents of a class can provide com-plementary information for the ZSL model. For example, in the case of birds, one source might focus more on the patterns of the feather, while another source might better describe the belly and the face of the bird. However, find-ing multiple good sources of text documents for each class requires additional annotation effort.
Large Language Models (LLM) [5, 12, 63] trained on web-scale text have shown impressive abilities of using their learned information to solve a multitude of tasks.
These models can be conditioned with a k-shot prompt to generalize to a wide set of applications [5, 31, 60] using knowledge from multiple sources they were trained on. In
this work, we aim to generate multiple text descriptions of a class, that we recall as “views” hereinafter, with an LLM us-ing a k-shot prompting strategy. We show that the LLM can act as a mixture of annotators conditioned on different anno-tation styles to generate complementary information about a class. Moreover, we propose a novel model, I2MVFormer, which utilizes our memory-efficient summary modules to extract discriminative information from each view of a class with the aim of learning a multi-view class embedding.
Our contributions in this work are as follows. 1) We pro-vide the first study into using an LLM to generate auxil-iary information for zero-shot image classification. More-over, we propose a prompting strategy to extract multiple descriptions from an LLM that reveal complementary in-formation about a class. 2) We propose I2MVFormer, a novel transformer-based model for zero-shot image classi-fication which exploits multiple complementary sources of text supervision to learn a class embedding. I2MVFormer utilizes our Single-View Summary (SVSummary) module to extract rich discriminative information from each class view. This information is utilized by our Multi-View
Summary (MVSummary) module to represent a class-level set of tokens from multiple views. The multi-view to-kens are aligned with the image to maximize global and local compatibility between the images and the multiple views. 3) Our I2MVFormer achieves significant perfor-mance gains to establish a new state-of-the-art (SOTA) in unsupervised class embeddings in ZSL on three public benchmarks AWA2 [22], CUB [48] and FLO [36]. 2.