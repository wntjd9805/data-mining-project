Abstract 1.

Introduction
Modern methods for vision-centric autonomous driving perception widely adopt the bird’s-eye-view (BEV) repre-sentation to describe a 3D scene. Despite its better effi-ciency than voxel representation, it has difficulty describing the fine-grained 3D structure of a scene with a single plane.
To address this, we propose a tri-perspective view (TPV) representation which accompanies BEV with two additional perpendicular planes. We model each point in the 3D space by summing its projected features on the three planes. To lift image features to the 3D TPV space, we further pro-pose a transformer-based TPV encoder (TPVFormer) to ob-tain the TPV features effectively. We employ the attention mechanism to aggregate the image features corresponding to each query in each TPV plane. Experiments show that our model trained with sparse supervision effectively pre-dicts the semantic occupancy for all voxels. We demon-strate for the first time that using only camera inputs can achieve comparable performance with LiDAR-based meth-ods on the LiDAR segmentation task on nuScenes. Code: https://github.com/wzzheng/TPVFormer.
Perceiving the 3D surroundings accurately and compre-hensively plays an important role in the autonomous driving system. Vision-based 3D perception recently emerges as a promising alternative to LiDAR-based one to effectively extract 3D information from 2D images. Though lacking direct sensing of depth information, vision-based models empowered by surrounding cameras demonstrate promising performance on various 3D perception tasks such as depth estimation [17, 42], semantic map reconstruction [1, 19, 48], and 3D object detection [27, 30, 46].
The core of 3D surrounding perceiving lies in how to ef-fectively represent a 3D scene. Conventional methods split the 3D space into voxels and assign each voxel a vector to represent its status. Despite its accuracy, the vast number of voxels poses a great challenge to computation and re-quires specialized techniques like sparse convolution [13].
As the information in outdoor scenes is not isotropically distributed, modern methods collapse the height dimension and mainly focus on the ground plane (bird’s-eye-view) where information varies the most [20,26,28,31,35,46,48].
*Equal contribution. †Corresponding author.
occupancy prediction, where only sparse lidar semantic la-bels are provided for training and predictions for all voxels are required for testing, as shown in Figure 2. However, as no benchmark is provided on this challenging setting, we only perform qualitative analysis but provide a quanti-tative evaluation on two proxy tasks: LiDAR segmentation (sparse training, sparse testing) on nuScenes [4] and 3D se-mantic scene completion (dense training, dense testing) on
SemanticKITTI [2]. For both tasks, we only use RGB im-ages as inputs. For LiDAR segmentation, our model use the LiDAR data only for point query to compute evalua-tion metrics. Visualization results show that TPVFormer produces consistent semantic voxel occupancy prediction with only sparse point supervision during training, as shown in Figure 1. We also demonstrate for the first time that our vision-based method achieves comparable performance with LiDAR-based methods on LiDAR segmentation. 2.