Abstract
Barlow Twins and VICReg are self-supervised represen-tation learning models that use regularizers to decorrelate features. Although these models are as effective as conven-tional representation learning models, their training can be computationally demanding if the dimension 𝑑 of the pro-jected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for 𝑛 samples takes
𝑂 (𝑛𝑑2) time. In this paper, we propose a relaxed decorre-lating regularizer that can be computed in 𝑂 (𝑛𝑑 log 𝑑) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits ac-curacy comparable to that of existing regularizers in down-stream tasks, whereas their training requires less memory and is faster for large 𝑑. The source code is available.1 1.

Introduction
Self-supervised learning (SSL) of representations [1, 3– 5, 10, 13, 15, 27–30] has become an integral part of deep learning applications in computer vision. Most SSL mod-els for visual representations employ a multi-view, Siamese network architecture. First, an input image is altered by different transformations that preserve its original seman-tics. The two augmented examples are then fed to a neu-ral network (or in some cases, two different networks) that consists of a backbone network cascaded with a small pro-jection network (usually a multi-layer perceptron) to pro-duce “twin” projected embeddings, or views, of the orig-inal. Finally, the network weights are trained so that the twin embeddings (referred to as a “positive pair”) are sim-ilar, reflecting the fact that they represent the same orig-inal image. After training, the projection network is dis-carded, while the backbone network is reused for down-*Equal contribution. 1https://github.com/yutaro-s/scalable-decorrelation-ssl.git stream tasks; the idea is that the pretrained backbone should produce generic representations of images that also benefit downstream tasks.
One major issue in SSL is to sidestep collapsed embed-dings, or the presence of trivial solutions such that all exam-ples are projected to a single embedding vector. Contrastive approaches [3, 4, 15, 27, 28] eliminate such solutions by a loss term that repels embeddings of different original im-ages (known as “negative pairs”) from each other. Consid-ering all possible negative pairs is infeasible, and negative sampling is usually performed.
Recent studies have explored non-contrastive SSL mod-els. Among these models, Barlow Twins [29] and VICReg
[1] use loss functions to penalize features (components of embedding vectors) with a small variance, which are char-acteristic of collapsed embeddings. Also notable in these loss functions are regularization terms for feature decorre-lation. They reduce redundancy in the learned features and make Barlow Twins and VICReg perform as well as con-trastive models. However, since the regularizers are defined in terms of elements in a sample cross-correlation or covari-ance matrix, they require 𝑂 (𝑛𝑑2) time to compute, where 𝑛 is the number of examples in a batch, and 𝑑 is the dimen-sionality of the projected embeddings. This is problematic, as an increased 𝑑 has been reported to improve the perfor-mance of both Barlow Twins and VICReg [1, 29].
Contributions. We propose a relaxed decorrelating regu-larizer to address the inefficiency of Barlow Twins and VI-CReg. The proposed regularizer does not require the ex-plicit calculation of cross-correlation or covariance matrices and can be computed in 𝑂 (𝑛𝑑 log 𝑑) time by means of Fast
Fourier Transform (FFT) [7]. Although undesirable local minima develop as a result of relaxation, they can be mit-igated by feature permutation; we give an account of why this simple technique works. The SSL models using the proposed regularizer achieve competitive performance with
Barlow Twins and VICReg in downstream tasks, with sub-stantially less computation time for large 𝑑. With 𝑑 = 8192, training is 1.2 (with a ResNet-50 backbone) or 2.2 (with a
lightweight ResNet-18 backbone) times as fast as Barlow
Twins. The proposed method also reduces memory con-sumption, which allows for larger batch size.
Notation. We use zero-based indexing for vector and ma-trix components unless stated otherwise; thus, for a vector x ∈ R𝑑, the component index ranges from 0 to 𝑑 −1. We use
[x]𝑖 to denote the 𝑖th component of vector x, and [M]𝑖 𝑗 to denote the (𝑖, 𝑗)th element of matrix M. For a complex vec-tor c, c denotes its componentwise complex conjugate. For vectors x and y, x ◦ y denotes their componentwise product. 2.