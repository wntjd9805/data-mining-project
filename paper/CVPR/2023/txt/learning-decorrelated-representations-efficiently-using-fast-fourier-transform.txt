Abstract
Barlow Twins and VICReg are self-supervised represen-tation learning models that use regularizers to decorrelate features. Although these models are as effective as conven-tional representation learning models, their training can be computationally demanding if the dimension ğ‘‘ of the pro-jected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for ğ‘› samples takes
ğ‘‚ (ğ‘›ğ‘‘2) time. In this paper, we propose a relaxed decorre-lating regularizer that can be computed in ğ‘‚ (ğ‘›ğ‘‘ log ğ‘‘) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits ac-curacy comparable to that of existing regularizers in down-stream tasks, whereas their training requires less memory and is faster for large ğ‘‘. The source code is available.1 1.

Introduction
Self-supervised learning (SSL) of representations [1, 3â€“ 5, 10, 13, 15, 27â€“30] has become an integral part of deep learning applications in computer vision. Most SSL mod-els for visual representations employ a multi-view, Siamese network architecture. First, an input image is altered by different transformations that preserve its original seman-tics. The two augmented examples are then fed to a neu-ral network (or in some cases, two different networks) that consists of a backbone network cascaded with a small pro-jection network (usually a multi-layer perceptron) to pro-duce â€œtwinâ€ projected embeddings, or views, of the orig-inal. Finally, the network weights are trained so that the twin embeddings (referred to as a â€œpositive pairâ€) are sim-ilar, reflecting the fact that they represent the same orig-inal image. After training, the projection network is dis-carded, while the backbone network is reused for down-*Equal contribution. 1https://github.com/yutaro-s/scalable-decorrelation-ssl.git stream tasks; the idea is that the pretrained backbone should produce generic representations of images that also benefit downstream tasks.
One major issue in SSL is to sidestep collapsed embed-dings, or the presence of trivial solutions such that all exam-ples are projected to a single embedding vector. Contrastive approaches [3, 4, 15, 27, 28] eliminate such solutions by a loss term that repels embeddings of different original im-ages (known as â€œnegative pairsâ€) from each other. Consid-ering all possible negative pairs is infeasible, and negative sampling is usually performed.
Recent studies have explored non-contrastive SSL mod-els. Among these models, Barlow Twins [29] and VICReg
[1] use loss functions to penalize features (components of embedding vectors) with a small variance, which are char-acteristic of collapsed embeddings. Also notable in these loss functions are regularization terms for feature decorre-lation. They reduce redundancy in the learned features and make Barlow Twins and VICReg perform as well as con-trastive models. However, since the regularizers are defined in terms of elements in a sample cross-correlation or covari-ance matrix, they require ğ‘‚ (ğ‘›ğ‘‘2) time to compute, where ğ‘› is the number of examples in a batch, and ğ‘‘ is the dimen-sionality of the projected embeddings. This is problematic, as an increased ğ‘‘ has been reported to improve the perfor-mance of both Barlow Twins and VICReg [1, 29].
Contributions. We propose a relaxed decorrelating regu-larizer to address the inefficiency of Barlow Twins and VI-CReg. The proposed regularizer does not require the ex-plicit calculation of cross-correlation or covariance matrices and can be computed in ğ‘‚ (ğ‘›ğ‘‘ log ğ‘‘) time by means of Fast
Fourier Transform (FFT) [7]. Although undesirable local minima develop as a result of relaxation, they can be mit-igated by feature permutation; we give an account of why this simple technique works. The SSL models using the proposed regularizer achieve competitive performance with
Barlow Twins and VICReg in downstream tasks, with sub-stantially less computation time for large ğ‘‘. With ğ‘‘ = 8192, training is 1.2 (with a ResNet-50 backbone) or 2.2 (with a
lightweight ResNet-18 backbone) times as fast as Barlow
Twins. The proposed method also reduces memory con-sumption, which allows for larger batch size.
Notation. We use zero-based indexing for vector and ma-trix components unless stated otherwise; thus, for a vector x âˆˆ Rğ‘‘, the component index ranges from 0 to ğ‘‘ âˆ’1. We use
[x]ğ‘– to denote the ğ‘–th component of vector x, and [M]ğ‘– ğ‘— to denote the (ğ‘–, ğ‘—)th element of matrix M. For a complex vec-tor c, c denotes its componentwise complex conjugate. For vectors x and y, x â—¦ y denotes their componentwise product. 2.