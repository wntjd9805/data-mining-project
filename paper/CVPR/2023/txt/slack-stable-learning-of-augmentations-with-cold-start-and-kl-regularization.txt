Abstract
Data augmentation is known to improve the generaliza-tion capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation al-gorithm.
In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowl-edge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to competitive results on standard bench-marks despite a more challenging setting, and generalizes beyond natural images.1 1.

Introduction
Data augmentation, which encourages predictions to be stable with respect to particular image transformations, has become an essential component in visual recognition sys-tems. While the data augmentation process is conceptually simple, choosing the optimal set of image transformations for a given task or dataset is challenging. For instance, designing a good set for ImageNet [4] or even CIFAR-10/100 [13] has been the result of a long-standing research effort. Whereas data augmentation strategies that have been chosen by hand for ImageNet have been used successfully for many recognition tasks involving natural images, they may fail to generalize to other domains such as medical imaging, remote sensing or hyperspectral imaging. 1Project page: https://europe.naverlabs.com/slack
This has motivated automating the design of data aug-mentation strategies [10,12,14–16,19,27,31]. Those are of-ten represented as a stochastic policy that randomly draws a combination of transformations along with their magnitudes from a large predefined set, each time an image is sampled.
The goal becomes to learn strategies that effectively com-pose multiple transformations, which is a challenging task given the large search space of augmentations.
A natural framework for learning the parameters of this policy is that of bilevel optimization. Intuitively, one looks for the best possible policy such that a neural network trained with this policy on a training set (inner problem) generalizes well on a distinct validation set (outer problem).
Optimizing the resulting formulation is challenging as the outer problem depends on the solution of the inner problem.
Classical techniques for solving this bilevel problem, such as unrolled optimization, can become highly unstable as the network weights become progressively suboptimal for the current policy during the learning process.
Moreover, augmentations are often non-differentiable in the parameters of the policy, thus requiring techniques other than direct differentiation, such as Bayesian optimiza-tion [15], gradient approximations (e.g. RELAX [7]), or the score method / REINFORCE [28] algorithm. While these techniques bypass the differentiability issues, they can suf-fer from large bias or variance. As a result, learning aug-mentation policies is a difficult problem whose challenges are exacerbated by the inherent instability of the optimiza-tion techniques developed to solve bilevel problems, such as unrolled optimization [1].
A standard way to improve stability and make the auto-matic data augmentation problem simpler is to reduce the search space. This is often achieved by learning the pol-icy on top of “default” transformations such as Cutout [5], random cropping and resizing, or color jittering, all known to be well-suited to natural images which compose stan-dard benchmarks such as CIFAR or ImageNet, or by dis-carding transformations known to be harmful such as In-vert. Fixing some of the transformations and removing oth-ers mitigate the challenges inherent to learning a compo-Figure 1. For different domains of the DomainNet dataset [21] (one per line), we show an image from that domain (left) and that image transformed using the three most likely (middle) and the three least likely (right) augmentations for that domain, as estimated by SLACK. sition of transformations. TrivialAugment [19] also shows that state-of-the-art results can be achieved on these pre-vious benchmarks simply by directly applying the policy classically used for initializing auto-augmentation models, up to minor modifications. Moreover, all methods rely on carefully chosen ranges that constraint the transformation’s magnitudes. Despite its effectiveness, manually selecting default transformations and magnitude ranges restricts the applicability of such policies to natural images and prevents generalisation to other domains.
In this paper, our goal is to choose augmentation strate-gies without relying on default transformations nor on hand-selected magnitude ranges known to suit common bench-marks. To achieve this objective, we first introduce a simple interpretable model for the augmentation policies which al-lows learning both the frequency by which a given augmen-tation is selected and the magnitude by which it is applied.
Then, we propose a method for learning these augmenta-tion policies by solving a bilevel optimization problem. Our method relies on the REINFORCE technique for computing the gradient of the policy and on unrolled optimization for learning the policy, both of which can result in instabilities and yield high variance estimates.
To address these issues, we introduce an efficient multi-stage algorithm with a cold-start strategy and a Kullback-Leibler (KL) regularization that are designed to improve the stability of the process for learning the data augmentation policy. More precisely, the algorithm first pre-trains a net-work with a data augmentation policy uniformly sampling over all transformations. Then, each stage uses a “cold-start” strategy by restarting from the pre-trained network and performs incremental updates of the current policy.
This multi-stage approach with cold start prevents the network from becoming progressively suboptimal as the policy is updated using unrolled optimization. The KL reg-ularization defines a trust region for the policy to compen-sate for the possibly high variance of gradient estimates ob-tained using the REINFORCE technique and encourages exploration during training, preventing collapse to trivial solutions. This regularization is inspired by proximal point algorithms in convex optimization [22], which have also been successful in reinforcement learning tasks [23].
By combining the regularized multi-stage approach with our interpretable model of the augmentation policies, we obtained the proposed SLACK method, which stands for
Stable Learning of Augmentations with Cold-start and
Kullback-Leibler regularization. SLACK is an efficient data augmentation learning method that is able to ad-dress the challenging bilevel optimization problem of learn-ing a stochastic data augmentation policy without relying strongly on prior knowledge. Figure 1 illustrates the trans-formations found by SLACK to be most important / detri-mental on a dataset of different domains including non-natural images.
To summarize, our contribution is threefold. (i) We pro-pose a simple and interpretable model of the policies which allows learning both frequency and magnitudes of the aug-mentations. (ii) We propose a regularized multi-stage strat-egy to improve the stability of the bilevel optimization algo-rithm used for solving the data augmentation learning prob-lem. (iii) We evaluate our method on challenging experi-mental settings, and show that it finds competitive augmen-tation strategies on natural images without resorting to prior information and generalizes to other domains.
2.