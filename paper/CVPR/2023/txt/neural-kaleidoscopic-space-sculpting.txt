Abstract
We introduce a method that recovers full-surround 3D reconstructions from a single kaleidoscopic image using a neural surface representation. Full-surround 3D recon-struction is critical for many applications, such as aug-mented and virtual reality. A kaleidoscope, which uses a single camera and multiple mirrors, is a convenient way of achieving full-surround coverage, as it redistributes light directions and thus captures multiple viewpoints in a single image. This enables single-shot and dynamic full-surround 3D reconstruction. However, using a kaleidoscopic im-age for multi-view stereo is challenging, as we need to de-compose the image into multi-view images by identifying which pixel corresponds to which virtual camera, a process we call labeling. To address this challenge, pur approach avoids the need to explicitly estimate labels, but instead
“sculpts” a neural surface representation through the care-ful use of silhouette, background, foreground, and texture information present in the kaleidoscopic image. We demon-strate the advantages of our method in a range of simulated and real experiments, on both static and dynamic scenes. 1.

Introduction
Generating digital replicas of real-world objects from image measurements is a hard problem. Multi-view recon-struction approaches require a diverse set of viewpoints that provide full-surround coverage. A single camera, even if it is moving, can be insufficient for this problem, for exam-ple when the object under consideration undergoes dynamic motion and has complex shape. To capture the shape of a dynamic object, we would need simultaneous captures from multiple viewpoints. While a multi-camera system can pro-vide such information, its cost and complexity can be pro-hibitive when we need to acquire objects with very complex appearance, geometry, and self-occlusions, and thus requir-ing very large numbers of viewpoints.
We use a kaleidoscope [6] to achieve single-shot full-surround 3D reconstruction for general dynamic objects.
A kaleidoscope is a configuration of multiple interreflect-ing mirrors imaged by a camera, and dramatically increases
Figure 1. 3D printing of shape reconstructions. The proposed neural kaleidoscopic space sculpting can generate replicas of real objects with a range of shapes and reflectances. Reconstructed meshes are available on the project webpage [2]. the number of viewpoints, thereby enabling a virtual time-synchronized multi-view system. However, 3D reconstruc-tion with a kaleidoscope requires identifying the specific se-quence of mirrors encountered by light reaching each cam-era pixel; this is equivalent to identifying the specific vir-tual view corresponding to the pixel, commonly referred to as the labeling problem. This labeling problem can be solved using time-of-flight cameras [36] or structured light systems [3], but such active techniques require long scan times that make them unsuitable for dynamic objects. On the other hand, prior art with passive illumination first con-structs the visual hull of the object, then uses it to estimate its label [29]. This two-stage process often produces erro-neous results, especially when the visual hull differs signif-icantly from the true shape.
Contributions. We propose a technique for full-surround 3D reconstruction with a single kaleidoscopic image. Our key insight is that a single pixel in a kaleidoscopic image is equivalent to multiple such pixels in its multi-camera counterpart. For example, the pre-image of the background pixel, which is the collection of 3D points that map to that pixel, in a kaleidoscope does not intersect with the object; this implies that is also a background pixel in all virtual views associated with it. Similarly, even a foreground pixel that intersects with the object can be used to carve out space since all of the light path prior to a ray’s intersection with
the object can be considered as background.
Armed with these insights on the nature of information encoded in a kaleidoscopic image, we propose a technique that we call kaleidoscopic space sculpting; sculpting sets up an optimization problem that updates a neural implicit surface [37] using a collection of cost functions that en-code background information (to remove regions) and fore-ground regions (to add regions), as well as the texture of the object. Interestingly, our technique does not explicitly calculate the label information. Despite this, it provides ro-bust single-shot full-surround 3D reconstructions. For dy-namic objects, we apply our technique separately on each frame of a kaleidoscopic video, to obtain full-surround 3D videos. Figure 1 shows a gallery of objects placed beside their 3D printed counterparts, obtained using neural kalei-doscopic space sculpting.
Limitations. Our technique has a number of limitations, some of which are inherent in the use of a kaleidoscope.
First, the size of objects we can scan is restricted by the kaleidoscope; for our lab setup, this constrains our tech-nique to objects that fit in a sphere of diameter 4 inches.
Second, the total number of pixels that we have at our dis-posal is limited to that of a single image sensor; divvying this pixel budget across the many (virtual) views results in lower resolution imagery, especially when we consider multi-view alternatives where the total pixel count grows linearly with the number of cameras. Third, our proposed technique is sensitive to foreground-background masking.
We observed that automatic masking techniques produce erroneous masks that significantly reduce the quality of the final result. For this reason, we manually correct such mis-takes prior to shape estimation. 2.