Abstract
Temporal video segmentation is the get-to-go automatic video analysis, which decomposes a long-form video into smaller components for the following-up understanding tasks. Recent works have studied several levels of granu-larity to segment a video, such as shot, event, and scene.
Those segmentations can help compare the semantics in the corresponding scales, but lack a wider view of larger temporal spans, especially when the video is complex and structured. Therefore, we present two abstractive levels of temporal segmentations and study their hierarchy to the ex-isting fine-grained levels. Accordingly, we collect NewsNet, the largest news video dataset consisting of 1,000 videos in over 900 hours, associated with several tasks for hierar-chical temporal video segmentation. Each news video is a collection of stories on different topics, represented as aligned audio, visual, and textual data, along with exten-sive frame-wise annotations in four granularities. We assert that the study on NewsNet can advance the understanding of complex structured video and benefit more areas such as short-video creation, personalized advertisement, digital instruction, and education. Our dataset and code is pub-licly available at https://github.com/NewsNet-Benchmark/NewsNet. 1.

Introduction
† Equal Contribution
Corresponding Authors: Bing Li (bing.li@kaust.edu.sa) and Ruizhi
Qiao (ruizhiqiao@tencent.com).
Temporal video segmentation is a critical problem in video understanding, which is essential for many video ap-plications such as video classification [10,15,37,58,59,62],
Table 1. Data comparison between the NewsNet and other related datasets. The NewsNet provides various multimodal data and hierarchical temporal segmentation annotations. Doc: documentary, Ads: advertisement. (Please refer to our project page for more details.)
Dataset
# Video
Duration (hours)
Modality
# Annotation(s) per Video
Topic
Story
Scene Event
Source
AVS [75]
BBC [6]
OVSD [48]
Kinetics-GEBD [51]
MovieNet [23] †
RAI [7]
TI-News [35]
NewsNet (Ours) 197 11 21 54,691 1,100 10 477 1,000
-9 10 152 2174
-244 946
Visual
Visual
Visual
Audio + Visual
Text + Audio + Visual
----------Visual
Audio + Visual
Text + Audio + Visual
--8.5
-55.6 51.6
-49.7 28.9
-66.0
--87.9
Ads 14.2
Doc
-Generic
-4.9
Action 849.1 Movie 98.7 530.4 654.4
News
News
News
† The number of annotations for Scene and Shot is counted from MovieScene [47], which is a subset of MovieNet. captioning [3, 34, 63, 71] and retrieval [5, 16, 31, 52]. Tem-poral video segmentation aims to group successive video frames into short segments along the temporal dimension.
With the explosive growth of long-form videos, it is desir-able that temporal video segmentation can convert a video into more meaningful segments for more efficient access to the video. However, it is challenging to develop effective temporal segmentation tools for long-form videos, since this requires a comprehensive understanding of video struc-ture, while long-form videos contain complex content.
Towards temporal video segmentation, existing works explore shot, event, and scene segmentation tasks, respec-tively. Shot segmentation [38, 55, 57] divides a video into shots, where a shot consists of consecutive and visually continuous frames captured by a camera without interrup-tion [23]. Yet, shot segmentation only considers low-level visual cues (i.e. visual similarity), lacking semantic under-standing. Instead, event segmentation [25, 51, 56] divides a video by detecting the moments of changes such as ac-tion/subject changes. To better capture the underlying struc-ture of a video, recent works [12, 47, 65] introduce video scene segmentation which segments a video into scene seg-ments, each comprising successive shots semantically re-lated to the same scene. Scene segmentation enables a coarser and higher-level representation than shot segmen-tation. However, compared to the rich content of mas-sive long-form videos, scene/event is fine-grained and often lacks a high-level summarization of video content, which is insufficient for capturing the complex semantic structure of many videos and briefly representing video content.
In this work, we first explore how to comprehensively represent the complex structure of a long-form video for temporal video segmentation. Humans can hierarchically divide a video into segments of different granularities ac-cording to multi-level semantic information (e.g. scene and topic), from the perspective of cognitive science. Natu-ral language processing researchers have widely explored topic-level understanding [29, 41] for summarizing docu-ments [8, 42], while little effort has been devoted to long-form videos. Inspired by these observations, besides scene and event, we propose to introduce two higher-level seman-tics (i.e. story and topic) into temporal video segmentation, to provide a brief and semantic structure representation.
As a result, such hierarchical and multi-level understand-ing brings about scalable video structure representation for temporal video segmentation on long-form videos. That is, a long-form video can be split into finer segments with lower-level semantics (e.g. scene), but also can be summa-rized into coarser ones yet with higher-level semantics (e.g. topic) by recursively grouping finer segments, which com-prehensively represents video structures from coarse to fine.
However, the community lacks high-quality datasets to conduct this research. In particular, as shown in Table 1, most datasets only provide temporal structure annotations with regard to events or scenes. TI-News [35] and Movi-eScene [47] provide two levels of annotations, but these datasets lack topic-level ones.
To effectively break this limitation, we build a novel large-scale dataset for hierarchical temporal segmentation, named NewsNet. The unique properties of our NewsNet introduce many advantages. First, it is among the largest datasets in the news domain. We collect over 900 hours of videos from 20 mainstream news platforms. It has a highly diverse distribution of data. Second, we carefully anno-tated it frame-by-frame with 4 hierarchical levels to ensure its quality can meet our needs. Third, it is multimodal, in-cluding textual, visual, and audio information. Due to the nature of the news, the alignment across modalities is ac-curate, which makes multimodal joint training of models feasible. Finally, the videos in NewsNet provide a complete understanding of public events. Compared with other video datasets [4, 23, 26], it introduces more objective open-world knowledge (e.g., news introduction) while including sub-jective factual commentary (e.g., host comments on news
events), making it more amenable to real-life application.
Based on NewsNet, we empirically highlight two promising directions for long-span temporal segmentation: 1) Infusing Multi-Modality knowledge can significantly im-prove the performance of long-form temporal segmentation; 2) Although story- and topic-level segmentation is challeng-ing, it can be benefited from hierarchical modeling with the event- and scene-level segmentation tasks.
The main contributions of this paper are as follows:
• We propose a novel large-scale dataset NewsNet for long-form video structure understanding. This dataset is derived from 900+ hours of video and annotated with 4 hierarchical levels of semantics.
• NewsNet provides dense annotations and multi-modal information, promoting diverse benchmarks: sep-temporal video segmentation in arate/hierarchical scene/story/topic levels, as well as other common tasks like classification, video localization/grounding, and highlight detection.
• We formulate a new benchmark, i.e., hierarchical mod-eling in the temporal segmentation task, which needs a single model to predict segments of multiple hierarchi-cal levels. Based on the empirical study, we bring in-sights into how hierarchical modeling potentially ben-efits the temporal video segmentation task, which was almost never discussed. 2.