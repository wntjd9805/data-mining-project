Abstract
Humans constantly interact with objects in daily life tasks. Capturing such processes and subsequently conduct-ing visual inferences from a fixed viewpoint suffers from occlusions, shape and texture ambiguities, motions, etc.
To mitigate the problem, it is essential to build a train-ing dataset that captures free-viewpoint interactions. We construct a dense multi-view dome to acquire a complex human object interaction dataset, named HODome, that consists of ∼71M frames on 10 subjects interacting with 23 objects. To process the HODome dataset, we develop
NeuralDome, a layer-wise neural processing pipeline tai-* These authors contributed equally.
†Corresponding author. lored for multi-view video inputs to conduct accurate track-ing, geometry reconstruction and free-view rendering, for both human subjects and objects. Extensive experiments on the HODome dataset demonstrate the effectiveness of
NeuralDome on a variety of inference, modeling, and ren-dering tasks. Both the dataset and the NeuralDome tools will be disseminated to the community for further devel-opment, which can be found at https://juzezhang. github.io/NeuralDome 1.

Introduction
A key task of computer vision is to understand how hu-mans interact with the surrounding world, by faithfully cap-turing and subsequently reproducing the process via mod-eling and rendering. Successful solutions benefit broad ap-plications ranging from sports training to vocational educa-tion, digital entertainment to tele-medicine.
Early solutions [11, 12, 51] that reconstruct dynamic meshes with per-frame texture maps are time-consuming and vulnerable to occlusions or lack of textures. Recent advances in neural rendering [37, 62, 71] bring huge po-tential for human-centric modeling. Most notably, the variants of Neural Radiance Field (NeRF) [37] achieve compelling novel view synthesis, which can enable real-time rendering performance [38, 59, 67] even for dynamic scenes [45, 63, 77], and can be extended to the generative setting without per-scene training [23, 69, 80]. However, less attention is paid to the rich and diverse interactions be-tween humans and objects, mainly due to the severe lack of dense-view human-object datasets. Actually, existing datasets of human-object interactions are mostly based on optical markers [61] or sparse RGB/RGBD sensors [6, 26], without sufficient appearance supervision for neural render-ing tasks. As a result, the literature on neural human-object rendering [21,57] is surprisingly sparse, let alone further ex-ploring the real-time or generative directions. Besides, ex-isting neural techniques [52,77] suffer from tedious training procedures due to the human-object occlusion, and hence infeasible for building a large-scale dataset. In a nutshell, despite the recent tremendous thriving of neural rendering, the lack of both a rich dataset and an efficient reconstruction scheme constitute barriers in human-object modeling.
In this paper, we present NeuralDome, a neural pipeline that takes multi-view dome capture as inputs and conducts accurate 3D modeling and photo-realistic rendering of com-plex human-object interaction. As shown in Fig. 1, Neural-Dome exploits layer-wise neural modeling to produce rich and multi-modality outputs including the geometry of dy-namic human, object shapes and tracked poses, as well as a free-view rendering of the sequence.
Specifically, we first capture a novel human-object dome (HODome) dataset that consists of 274 human-object inter-acting sequences, covering 23 diverse 3D objects and 10 human subjects (5 males and 5 females) in various appar-els. We record multi-view video sequences of natural inter-actions between the human subjects and the objects where each sequence is about 60s in length using a dome with 76
RGB cameras, resulting in 71 million video frames. We also provide an accurate pre-scanned 3D template for each object and utilize sparse optical markers to track individual objects throughout the sequences.
To process the HODome dataset, we adopt an extended
Neural Radiance Field (NeRF) pipeline. The brute-force adoption of off-the-shelf neural techniques such as Instant-NSR [79] and Instant-NGP [38], although effective, do not separate objects from human subjects and therefore lack sufficient fidelity to model their interactions. We instead introduce a layer-wise neural processing pipeline. Specif-ically, we first perform a joint optimization based on the dense inputs for accurately tracking human motions using the parametric SMPL-X model [43] as well as localizing objects using template meshes. We then propose an ef-ficient layer-wise neural rendering scheme where the hu-mans and objects are formulated as a pose-embedded dy-namic NeRF and a static NeRF with tracked 6-DoF rigid poses, respectively. Such a layer-wise representation ef-fectively exploits temporal information and robustly tack-les the occluded regions under interactions. We further in-troduce an object-aware ray sampling strategy to mitigate artifacts during layer-wise training, as well as template-aware geometry regularizers to enforce contact-aware de-formations. Through weak segmentation supervision, we obtain the decoupled and occlusion-free appearances for both the humans and the objects at a high fidelity amenable for training the input multi-view inputs for a variety of tasks from monocular motion capture to free-view rendering from sparse multi-view inputs.
To summarize, our main contributions include:
• We introduce NeuralDome, a neural pipeline, to ac-curately track humans and objects, conduct layer-wise geometry reconstruction, and enable novel-view syn-thesis, from multi-view HOI video inputs.
• We collect a comprehensive dataset that we call
HODome that will be disseminated to the community, with both raw data and the output modalities including separated geometry and rendering of individual objects and human subjects, their tracking results, free-view rendering results, etc.
• We demonstrate using the dataset to train networks for a variety of visual inference tasks with complex human object interactions. 2.