Abstract 1.

Introduction
We present a new multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from sensors of different resolutions and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are se-lected to emphasize a diverse set of material properties challenging for existing algorithms. We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions. We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks. The dataset is available at skoltech3d.appliedai.tech.
*Work partially performed while at Skolkovo Institute of Science and
Technology.
Sensor data used in 3D reconstruction range from highly specialized and expensive CT, laser, and structured-light scanners to video from commodity cameras and depth sen-sors; computational 3D reconstruction methods are typically tailored to a speciﬁc type of sensors. Yet, even commod-ity hardware increasingly provides multi-sensor data: for example, many recent phones have multiple RGB cameras as well as lower resolution depth sensors. Using data from different sensors, RGB-D data in particular, has the potential to considerably improve the quality of 3D reconstruction.
For example, multi-view stereo algorithms (e.g., [55, 82]) produce high-quality 3D geometry from RGB data, but may miss featureless surfaces; supplementing RGB images with depth sensor data makes it possible to have more complete reconstructions. Conversely, commodity depth sensors often
lack resolution provided by RGB cameras.
Learning-based techniques substantially simplify the chal-lenging task of combining data fom multiple sensors. How-ever, learning methods require suitable data for training. Our dataset aims to complement existing ones (e.g., [13, 28, 31, 58, 63, 83]), as discussed in Section 2, most importantly, by providing multi-sensor data and high-accuracy ground truth for objects with challenging reﬂection properties.
The structure of our dataset is expected to beneﬁt research on 3D reconstruction in several ways.
• Multi-sensor data. We provide data from seven differ-ent devices with high-quality alignment, including low-resolution depth data from commodity sensors (phones,
Kinect, RealSense), high-resolution geometry data from a structured-light scanner, and RGB data at different resolu-tions and from different cameras. This enables supervised learning for reconstruction methods relying on different combinations of sensor data, in particular, increasingly common combination of high-resolution RGB with low-resolution depth data. In addition, multi-sensor data sim-pliﬁes comparison of methods relying on different sensor types (RGB, depth, and RGB-D).
• Lighting and pose variability. We chose to focus on a setup with controlled (but variable) lighting and a ﬁxed set of camera positions for all scenes, to enable high-quality alignment of data from multiple sensors, and systematic comparisons of algorithm sensitivity to various factors. We aimed to make the dataset large enough (1.39 M images of different modalities in total) to support training machine learning algorithms, and provide systematic variability in camera poses (100 per object), lighting (14 lighting setups, including “hard” and diffuse lighting, ﬂash, and backlighting, as illustrated in Figure 1b), and reﬂection properties these algorithms need.
• Object selection. Among 107 objects in our dataset, we include primarily objects that may present challenges to existing algorithms mentioned above (see examples in
Figure 1c); we made special effort to improve quality of 3D high-resolution structured-light data for these objects.
Our focus is on RGB and depth data for individual objects in laboratory setting, similar to the DTU dataset [28], rather than on complex scenes with natural lighting, as in Tanks and Temples [31] or ETH3D [58]. This provides means for systematic exploration and isolation of different factors con-tributing to strengths and weaknesses of different algorithms, and complements more holistic evaluation and training data provided by datasets with complex scenes. 2.