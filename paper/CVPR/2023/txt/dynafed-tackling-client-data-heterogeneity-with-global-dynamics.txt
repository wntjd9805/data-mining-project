Abstract
The Federated Learning (FL) paradigm is known to face challenges under heterogeneous client data. Local training on non-iid distributed data results in deflected local optimum, which causes the client models drift further away from each other and degrades the aggregated global model’s perfor-mance. A natural solution is to gather all client data onto the server, such that the server has a global view of the en-tire data distribution. Unfortunately, this reduces to regular training, which compromises clients’ privacy and conflicts with the purpose of FL. In this paper, we put forth an idea to collect and leverage global knowledge on the server without hindering data privacy. We unearth such knowledge from the dynamics of the global model’s trajectory. Specifically, we first reserve a short trajectory of global model snapshots on the server. Then, we synthesize a small pseudo dataset such that the model trained on it mimics the dynamics of the reserved global model trajectory. Afterward, the synthe-sized data is used to help aggregate the deflected clients into the global model. We name our method DYNAFED, which enjoys the following advantages: 1) we do not rely on any external on-server dataset, which requires no additional cost for data collection; 2) the pseudo data can be synthesized in early communication rounds, which enables DYNAFED to take effect early for boosting the convergence and stabilizing training; 3) the pseudo data only needs to be synthesized once and can be directly utilized on the server to help aggre-gation in subsequent rounds. Experiments across extensive benchmarks are conducted to showcase the effectiveness of
DYNAFED. We also provide insights and understanding of the underlying mechanism of our method. 1.

Introduction
Federated learning (FL) has become a popular distributed training paradigm to alleviate the server’s computational bur-*Joint first authors. Code is available at this link.
†Correspondence to Weizhong Zhang (weizhongzhang@fudan.edu.cn) and Qifeng Chen (chenqifeng22@gmail.com)
Figure 1. Illustration of DYNAFED. Firstly, we run the standard
FedAvg for L communication rounds and save the checkpoints to form a trajectory of the global model at the server. Then, we synthesize a pseudo dataset Dsyn, with which the network can be trained to mimic the dynamics of the global trajectory. In this way, the knowledge that captures the essential information about the global data distribution is transferred from the global dynamics to
Dsyn. Afterward, Dsyn is adopted to help aggregate the deflected clients into the global model at the server. den and preserve clients’ data privacy [3, 23, 31, 45]. In the
FL paradigm, the clients only have access to their private datasets, while the server is responsible for aggregating the clients’ updates into a global model. The most prevalent approaches in FL are based on local-SGD [34] (also referred to as FedAvg), where the client model is updated locally for multiple steps before being sent and merged on the server.
Such approaches save communication costs and perform well given the client data are iid-distributed. However, in real-world applications such as healthcare [22,23,36,37] and bio-metrics [2], the client data usually demonstrates hetero-geneity (highly non-iid), which deflects the local optimum from the global optimum [24, 32, 52] and makes the locally
trained clients biased. Therefore, naively averaging the client models results in slow convergence and performance drop.
To alleviate the difficulty of training under heterogeneity, a few lines of work have been frequently discussed. The first line attempts to modify the local training process, including imposing regularization on the client models [1, 24, 29, 31] and data sharing or augmentation [35, 38, 48, 52]. How-ever, these solutions have a high requirement for the server’s control of local clients. An orthogonal line focuses on refining the global model in the server aggregation pro-cess [6, 33, 40, 44, 46]. The majority of these methods typi-cally require a large external dataset on the server, then use it to align the outputs of the global model with that of the client ensemble [6, 15, 33, 44]. Unfortunately, such a large-scale task-related dataset is often hard to acquire in reality. To circumvent this limitation, a few data-free knowledge distil-lation (KD) approaches are recently proposed [50,53]. These methods attempt to transfer the knowledge contained in the global model to a generator, which is subsequently leveraged to produce pseudo data to either help local training at the clients [53] or finetune the global model at the server [50].
It is clear that these methods require a global model with reasonable performance to ensure the generation of helpful pseudo data. However, such requirement is hard to achieve in practice since the global model often performs poorly under heterogeneity, especially in the early rounds of train-ing. Other solutions include personalized FL [16, 27, 30, 39], simlarity clustering [4, 10, 13], meta learning [21, 28], etc.
Even though the above-mentioned works propose tech-niques to alleviate the challenges posed by heterogeneity to some extent, they do not tackle the issue from its root cause: the data is unevenly scattered at different clients and is kept locally due to privacy concerns, which is also the main obstacle for training an accurate global model. Ideally, imagine if we can collect all the client data to the server, then training can be directly conducted at the server, and the heterogeneity issue no longer exists. However, this reduces to regular training and conflicts with the original purpose of
FL to protect client privacy. We then raise a natural ques-tion: is it possible to derive the essential information about the global data distribution on the server to help training without compromising client privacy?
Despite the global model typically performing poorly due to heterogeneity, the changes in its parameters are steered jointly by the data scattered at different clients. Therefore, the update dynamics of the global model contain knowledge about global data distribution. Driven by this intuition, we propose DYNAFED to explicitly unearth such knowledge hid-den in the global dynamics and transfer it to a pseudo dataset
Dsyn at the server. Dsyn can then approximate the global data distribution on the server to aid aggregation. More specifically, inspired by recent works in dataset condensa-tion [5,41,51], we formulate the data synthesis process into a learning problem, which minimizes the distance between the trajectory trained with Dsyn and the global model trajectory derived with D. Fine-tuning the aggregated global model with Dsyn effectively alleviates the performance degradation caused by deflected clients. An appealing feature of our
DYNAFED is that the data can be synthesized using just the global model’s trajectory of the first few rounds, which enables Dsyn to take effect and help aggregation from early rounds. In addition, the synthesizing process only needs to be conducted once in practice, after which the derived
Dsyn can be directly applied in subsequent rounds to help aggregate the deviated client models.
Notably, our framework can be readily applied to the majority of FL approaches, since we rely on only the his-tory of the global model’s parameters for synthesizing Dsyn, which is available in the conventional setting of FedAvg-based methods. Furthermore, because we extract global knowledge using the global dynamics, rather than any client-specific information as in [12,17,18,20,47], the derived Dsyn comprises of information mixed with the entire global data distribution, thus prevents leakage of client privacy.
Our DYNAFED possesses the following advantages com-pared with previous approaches: 1) It leverages the knowl-edge of the global data distribution to alleviate the aggre-gation bias of the global model without depending on any external datasets; 2) DYNAFED is able to generate informa-tive data in the early rounds of federated learning, which significantly helps convergence and stabilizes training in sub-sequent rounds; 3) Compared with [50, 53], which need to keep updating the generator throughout all communication rounds, the data synthesis process in our method only needs to be done once, which reduces the computational overhead.
In summary, we make the following contributions:
• We propose a practical approach named DYNAFED for tackling the heterogeneity problem, which extracts and exploits the hidden information from the global model’s trajectory. In this way, the server can access the essential knowledge of the global data distribution to reinforce aggregation;
• We experimentally show the synthesized dataset helps stabilize training, boost convergence and achieve signif-icant performance improvement under heterogeneity;
• We provide insights and detailed analysis into the work-ing mechanisms of the proposed DYNAFED both exper-imentally and theoretically. 2.