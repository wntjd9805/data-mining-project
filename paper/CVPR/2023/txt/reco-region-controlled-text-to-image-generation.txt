Abstract
Recently, large-scale text-to-image (T2I) models have shown impressive performance in generating high-fidelity images, but with limited controllability, e.g., precisely spec-ifying the content in a specific region with a free-form text description. In this paper, we propose an effective technique for such regional control in T2I generation. We augment T2I models’ inputs with an extra set of position tokens, which represent the quantized spatial coordinates. Each region is specified by four position tokens to represent the top-left and bottom-right corners, followed by an open-ended nat-ural language regional description. Then, we fine-tune a pre-trained T2I model with such new input interface. Our model, dubbed as ReCo (Region-Controlled T2I), enables the region control for arbitrary objects described by open-ended regional texts rather than by object labels from a constrained category set. Empirically, ReCo achieves bet-ter image quality than the T2I model strengthened by posi-tional words (FID: 8.82 → 7.36, SceneFID: 15.54 → 6.51 on COCO), together with objects being more accurately placed, amounting to a 20.40% region classification accu-racy improvement on COCO. Furthermore, we demonstrate that ReCo can better control the object count, spatial re-lationship, and region attributes such as color/size, with the free-form regional description. Human evaluation on
PaintSkill shows that ReCo is +19.28% and +17.21% more accurate in generating images with correct object count and spatial relationship than the T2I model. Code is available at https://github.com/microsoft/ReCo. 1.

Introduction
Text-to-image (T2I) generation aims to generate faith-ful images based on an input text query that describes the image content. By scaling up the training data and model size, large T2I models [31, 34, 36, 45] have recently shown 1
remarkable capabilities in generating high-fidelity images.
However, the text-only query allows limited controllability, e.g., precisely specifying the content in a specific region.
The naive way of using position-related text words, such as “top left” and “bottom right,” often results in ambigu-ous and verbose input queries, as shown in Figure 2 (a).
Even worse, when the text query becomes long and com-plicated, or describes an unusual scene, T2I models [31, 45] might overlook certain details and rather follow the visual or linguistic training prior. These two factors together make region control difficult. To get the desired image, users usu-ally need to try a large number of paraphrased queries and pick an image that best fits the desired scene. The process known as “prompt engineering” is time-consuming and of-ten fails to produce the desired image.
The desired region-controlled T2I generation is closely related to the layout-to-image generation [7,9,22,23,34,38, 44, 49]. As shown in Figure 2 (b), layout-to-image mod-els take all object bounding boxes with labels from a close set of object vocabulary [24] as inputs. Despite showing promise in region control, they can hardly understand free-form text inputs, nor the region-level combination of open-ended text descriptions and spatial positions. The two in-put conditions of text and box provide complementary re-ferring capabilities. Instead of separately modeling them as in text-to-image and layout-to-image generations, we study
“region-controlled T2I generation” that seamlessly com-bines these two input conditions. As shown in Figure 2 (c), the new input interface allows users to provide open-ended descriptions for arbitrary image regions, such as precisely placing a “brown glazed chocolate donut” in a specific area.
To this end, we propose ReCo (Region-Controlled T2I) that extends pre-trained T2I models to understand spatial coordinate inputs. The core idea is to introduce an ex-tra set of input position tokens to indicate the spatial po-sitions. The image width/height is quantized uniformly into
Nbins bins. Then, any float-valued coordinate can be ap-proximated and tokenized by the nearest bin. With an extra embedding matrix (EP ), the position token can be mapped onto the same space as the text token. Instead of designing a text-only query with positional words “in the top red donut” as in Figure 2 (a), ReCo takes region-controlled text inputs
“<x1>, <y1>, <x2>, <y2> red donut,” where <x>,<y> are the position tokens followed by the corresponding free-form text description. We then fine-tune a pre-trained T2I model with EP to generate the image from the extended input query. To best preserve the pre-trained T2I capabil-ity, ReCo training is designed to be similar to the T2I pre-training, i.e., introducing minimal extra model parameters (EP ), jointly encoding position and text tokens with the text encoder, and prefixing the image description before the ex-tended regional descriptions in the input query.
Figure 2. (a) With positional words (e.g., bottom/top/left/right and large/small/tall/long), the T2I model (Stable Diffusion [34]) does not manage to create objects with desired properties. (b) Layout-to-image generation [22, 34, 38, 49] takes all object boxes and la-bels as the input condition, but only works well with constrained object labels. (c) Our ReCo model synergetically combines the text and box referring, allowing users to specify an open-ended re-gional text description precisely at any image region. shown in Figure 1 (a), ReCo could reliably follow the input spatial constraints and generate the most plausible images by automatically adjusting object statues, such as the view (front/side) and type (single-/double-deck) of the “bus.” Po-sition tokens also allow the user to provide free-form re-gional descriptions, such as “an orange cat wearing a red hat” at a specific location. Furthermore, we empirically ob-serve that position tokens are less likely to get overlooked or misunderstood than text words. As shown in Figure 1 (b),
ReCo has better control over object count, spatial relation-ship, and size properties, especially when the query is long and complicated, or describes a scene that is less common in real life. In contrast, T2I models [34] may struggle with generating scenes with correct object counts (“ten”), rela-tionships (“boat below traffic light”), relative sizes (“chair larger than airplane”), and camera views (“zoomed out”).
To evaluate the region control, we design a comprehen-sive experiment benchmark based on a pre-trained regional object classifier and an object detector. The object classi-fier is applied on the generated image regions, while the detector is applied on the whole image. A higher accuracy means a better alignment between the generated object lay-out and the region positions in user queries. On the COCO dataset [24], ReCo shows a better object classification ac-curacy (42.02% → 62.42%) and detector averaged preci-sion (2.3 → 32.0), compared with the T2I model with care-fully designed positional words. For image generation qual-ity, ReCo improves the FID from 8.82 to 7.36, and Scene-FID from 15.54 to 6.51. Furthermore, human evaluations on PaintSkill [5] show +19.28% and +17.21% accuracy gain in more correctly generating the query-described ob-ject count and spatial relationship, indicating ReCo’s capa-bility in helping T2I models to generate challenging scenes.
Figure 1 visualizes ReCo’s use cases and capabilities. As
Our contributions are summarized as follows. 2
• We propose ReCo that extends pre-trained T2I mod-els to understand coordinate inputs. Thanks to the in-troduced position tokens in the region-controlled input query, users can easily specify free-form regional de-scriptions in arbitrary image regions.
• We instantiate ReCo based on Stable Diffusion. Ex-tensive experiments show that ReCo strictly follows the regional instructions from the input query, and also generates higher-fidelity images.
• We design a comprehensive evaluation benchmark to validate ReCo’s region-controlled T2I generation ca-pability. ReCo significantly improves both the region control accuracy and the image generation quality over a wide range of datasets and designed prompts. 2.