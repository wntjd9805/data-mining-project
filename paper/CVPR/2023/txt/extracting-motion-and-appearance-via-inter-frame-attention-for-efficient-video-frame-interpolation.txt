Abstract
Effectively extracting inter-frame motion and appear-ance information is important for video frame interpolation (VFI). Previous works either extract both types of informa-tion in a mixed way or devise separate modules for each type of information, which lead to representation ambiguity and low efficiency. In this paper, we propose a new mod-ule to explicitly extract motion and appearance information via a unified operation. Specifically, we rethink the infor-mation process in inter-frame attention and reuse its at-tention map for both appearance feature enhancement and motion information extraction. Furthermore, for efficient
VFI, our proposed module could be seamlessly integrated into a hybrid CNN and Transformer architecture. This hy-brid pipeline can alleviate the computational complexity of inter-frame attention as well as preserve detailed low-level structure information. Experimental results demon-strate that, for both fixed- and arbitrary-timestep interpo-lation, our method achieves state-of-the-art performance on various datasets. Meanwhile, our approach enjoys a lighter computation overhead over models with close per-formance. The source code and models are available at https://github.com/MCG-NJU/EMA-VFI. 1.

Introduction
As a fundamental low-level vision task, the goal of video frame interpolation (VFI) is to generate intermediate frames given a pair of consecutive frames [17, 33]. It has a wide range of real-life applications, such as video com-pression [53], novel-view rending [13,47], and slow-motion video creation [19]. In general, VFI can be seen as the pro-cess of capturing the motion between consecutive frames and then blending the corresponding appearance to synthe-size the intermediate frames. From this perspective, the mo-tion and appearance information between input frames is essential for achieving excellent performance in VFI tasks.
*: Corresponding author (lmwang@nju.edu.cn).
Figure 1. Illustration of various approaches in video frame inter-polation for acquiring motion and appearance information.
Concerning the extraction paradigm of motion and ap-pearance information, the current VFI approaches can be divided into two categories. The first is to handle both ap-pearance and motion information in a mixed way [2, 11, 14, 17, 20, 21, 30, 33, 37, 38, 44], as shown in Fig. 1(a). The two neighboring frames are directly concatenated and fed into a backbone composed of stacked similar modules to generate features with mixed motion and appearance infor-mation. Though simple, this approach requires an elabo-rate design and high capacity in the extractor module, as it needs to deal with both motion and appearance information jointly. The absence of explicit motion information also re-sults in limitations for arbitrary-timestep interpolation. The second category, as shown in Fig. 1(b), is to design sep-arate modules for motion and appearance information ex-traction [9, 18, 35, 40–42, 45, 56]. This approach requires additional modules, such as cost volume [18, 40, 41], to ex-tract motion information, which often imposes a high com-putational overhead. Also, only extracting appearance fea-tures from a single frame fails to capture the correspondence of appearance information of the same regions between frames, which is an effective cue for the VFI task [18].
To address the issues of the above two extraction paradigms, in this paper, we propose to explicitly extract both motion and appearance information via a unified op-eration of inter-frame attention. With a single inter-frame attention, as shown in Fig. 1(c), we are able to enhance the appearance features between consecutive frames and ac-quire motion features at the same time by reusing the atten-tion maps. This basic processing unit could be stacked to obtain the hierarchical motion and appearance information.
Specifically, for any patch in the current frame, we take it as the query and its temporal neighbors as keys and values to derive an attention map representing their temporal cor-relation. After that, the attention map is leveraged to aggre-gate the appearance features of neighbors to contextualize the current region representation. In addition, the attention map is also used to weight the displacement of neighbors to get an approximate motion vector of the patch from the current frame to the neighbor frame. Finally, the obtained features are utilized with light networks for motion estima-tion and appearance refinement to synthesize intermediate frames. Compared with previous works, our design enjoys three advantages. (1) The appearance features of each frame can be enhanced with each other yet not be mixed with mo-tion features to preserve the detailed static structure infor-mation. (2) The obtained motion features can be scaled by time and then used as cues to guide the generation of frames at any moment between input frames. (3) We only need to control the complexity and the number of modules to bal-ance the overall performance and the inference speed.
Directly using inter-frame attention on original reso-lution results in huge memory usage and computational
Inspired by some recent works [8, 12, 26, 49, overhead. 54, 55, 58], which combines Convolutional Neural Net-work (CNN) [23] with Transformer [48] to improve the model learning ability and robustness, we adopt a sim-ple but effective architecture: first utilize CNN to extract high-resolution low-level features and then use Transformer blocks equipped with inter-frame attention to extracting low-resolution motion features and inter-frame appearance features. Our proposed module could be seamlessly inte-grated into this hybrid pipeline to extract motion and ap-pearance features efficiently without losing fine-grained in-formation. Our contributions are summarized as follows:
• We propose to utilize inter-frame attention to extract both motion and appearance information simultane-ously for video frame interpolation.
• An hybrid CNN and Transformer design is adopted to overcome the overhead bottleneck of the inter-frame attention at high-resolution input while preserv-ing fine-grained information.
• Our model achieves state-of-the-art performance on various datasets while being efficient compared to models with similar performance. 2.