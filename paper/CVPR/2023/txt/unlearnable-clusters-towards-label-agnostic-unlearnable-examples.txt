Abstract
There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet.
UEs are training samples added with invisible but unlearn-able noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are gen-erated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original sam-ples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called label-consistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are ren-dered ineffective in this challenging setting. To tackle this challenge, we present a novel technique called Unlearn-able Clusters (UCs) to generate label-agnostic unlearn-able examples with cluster-wise perturbations. Furthermore, we propose to leverage Vision-and-Language Pre-trained
Models (VLPMs) like CLIP as the surrogate model to im-prove the transferability of the crafted UCs to diverse do-mains. We empirically verify the effectiveness of our pro-posed approach under a variety of settings with different datasets, target models, and even commercial platforms Mi-crosoft Azure and Baidu PaddlePaddle. Code is avail-able at https://github.com/jiamingzhang94/
Unlearnable-Clusters. 1.

Introduction
While the huge amount of “free” data available on the
Internet has been key to the success of deep learning and computer vision, this has also raised public concerns on the unauthorized exploitation of personal data uploaded to the
*This work is done when the author interned at Peng Cheng Lab.
†Corresponding authors.
Figure 1. An illustration of two different data protection assump-tions: label-consistency vs. label-agnostic, where the hacker ex-ploits the protected data in different manners.
Internet to train commercial or even malicious models [16].
For example, a company named Clearview AI has been found to have scraped billions of personal images from Face-book, YouTube, Venmo and millions of other websites to construct a commercial facial recognition application [44].
This has motivated the proposal of Unlearnable Examples (UEs) [17] to make data unlearnable (or unusable) to ma-chine learning models/services. Similar techniques are also known as availability attacks [2, 41] or indiscriminate poi-soning attacks [14] in the literature. These techniques allow users to actively adding protective noise into their private data to avoid unauthorized exploitation, rather than putting our trust into the hands of large corporations.
The original UE generation method generates error-minimizing noise via a bilevel min-min optimization frame-work with a surrogate model [17]. The noise can then be added to samples in a training set in either a sample-wise or class-wise manner to make the entire dataset unlearnable to different DNNs. It has been found that this method can-not survive adversarial training, which has been addressed by a recent method [11].
In this work, we identify one common assumption made by existing UE methods: label-consistency, where the hackers will exploit the protected dataset in the same way as the protector including the labels.
This means that, for the same image, the hacker and protec-tor hold the same label. We argue that this assumption is too ideal, and it is possible that the hackers will collect the protected (unlearnable) samples into a dataset for a different task and label the dataset into different number of classes.
As illustrated in Figure 1, an image can be labelled with different annotated labels (cat or animal), showing that a m-class (e.g., 10-class) unlearnable dataset may be exploited by the hacker as a n-class (e.g., 5-class or 20-class) dataset depending on its actual needs. We term this more generic assumption as label-agnostic and propose a novel method
Unlearnable Clusters (UCs) to generate more effective and transferable unlearnable examples under this harsh setting.
In Figure 2 (a), we show that this more generic label-agnostic setting poses a unique transferability challenge for the noise generated by existing methods like Error-Minimizing Noise (EMinN) [17], Adversarial Poisoning (AdvPoison) [10], Synthetic Perturbations (SynPer) [41] and
DeepConfuse [9]. This indicates that the protective noise generated by these methods are label-dependent and are ren-dered ineffective when presented with different number of classes. As such, we need more fundamental approaches to make a dataset unlearnable regardless of the annotations.
To this end, we start by analyzing the working mechanism of UEs generated by EMinN, AdvPoison as they are very representative under the label-consistency setting. Through a set of visual analyses, we find that the main reason why they could break supervised learners is that the generated noise tends to disrupts the distributional uniformity and dis-crepancy in the deep representation space. Uniformity refers to the property that the manifold of UEs in the deep rep-resentation space does not deviate much from that of the clean examples, while discrepancy refers to the property that examples belonging to the same class are richly diverse in the representation space. Inspired by the above obser-vation, we propose a novel approach called Unlearnable
Clusters (UCs) to generate label-agnostic UEs using cluster-wise (rather than class-wise) perturbations. This allows us to achieve a simultaneous disruption of the uniformity and discrepancy without knowing the label information.
Arguably, the choose of a proper surrogate model also plays an important role in generating effective UEs. Previ-ous methods generate UEs by directly attacking a surrogate model and then transfer the generated UEs to fight against a diverse set of target models [10, 17]. This may be easily achievable under the label-consistency setting, but may fail badly under the label-agnostic setting. However, even un-der the label-consistency setting, few works have studied the impact of the surrogate model to the final unlearnable performance. To generate effective, and more importantly, transferable UEs under the label-agnostic setting, we need to explore more generic surrogate model selection strategies, especially those that can be tailored to a wider range of un-known target models. Intuitively, the surrogate model should be a classification DNN that contains as many classes as possible so as to facilitate the recognition and protection of billions of images on the Internet. In this paper, we propose to leverage the large-scale Vision-and-Language Pre-trained
Models (VLPMs) [22, 23, 30] like CLIP [30] as the surrogate model. Pre-trained on over 400 million text-to-image pairs,
CLIP has the power to extract the representation of extremely diverse semantics. Meanwhile, VLPMs are pre-trained with a textual description rather than a one-hot label to align with the image, making them less overfit to the actual class “la-bels”. In this work, we leverage the image encoder of CLIP to extract the embeddings of the input images and then use the embeddings to generate more transferable UCs.
We evaluate our UC approach with different backbones and datasets, all in a black-box setting (the protector does not know the attacker’s network architecture or the class labels). Cluster-wise unlearnable noise can also prevent un-supervised exploitation against contrastive learning to certain extent, proving its superiority to existing UEs. We also com-pare UC with existing UE methods against two commercial machine learning platforms: Microsoft Azure1 and Baidu
PaddlePaddle2. To the best of our knowledge, this is the first physical-world attack to commercial APIs in this line of work. Our main contributions are summarized as follows:
• We promote a more generic data protection assumption called label-agnostic, which allows the hackers to ex-ploit the protected dataset differently (in terms of the annotated class labels) as the protector. This opens up a more practical and challenging setting against unau-thorized training of machine learning models.
• We reveal the working mechanism of existing UE gener-ation methods: they all disrupt the distributional unifor-mity and discrepancy in the deep representation space.
• We propose a novel approach called Unlearnable Clus-ters (UCs) to generate label-agnostic UEs with cluster-wise perturbations without knowing the label informa-tion. We also leverage VLPMs like CLIP as the surro-gate model to craft more transferable UCs.
• We empirically verify the effectiveness of our proposed approach with different backbones on different datasets.
We also show its effectiveness in protecting private data against commercial machine learning platforms Azure and PaddlePaddle. 2.