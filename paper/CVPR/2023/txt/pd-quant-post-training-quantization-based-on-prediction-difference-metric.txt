Abstract
Post-training quantization (PTQ) is a neural network compression technique that converts a full-precision model into a quantized model using lower-precision data types.
Although it can help reduce the size and computational cost of deep neural networks, it can also introduce quantiza-tion noise and reduce prediction accuracy, especially in ex-tremely low-bit settings. How to determine the appropriate quantization parameters (e.g., scaling factors and round-ing of weights) is the main problem facing now. Existing methods attempt to determine these parameters by minimize the distance between features before and after quantization, but such an approach only considers local information and may not result in the most optimal quantization parameters.
We analyze this issue and propose PD-Quant, a method that addresses this limitation by considering global infor-mation. It determines the quantization parameters by us-ing the information of differences between network predic-tion before and after quantization. In addition, PD-Quant can alleviate the overfitting problem in PTQ caused by the small number of calibration sets by adjusting the distribu-tion of activations. Experiments show that PD-Quant leads to better quantization parameters and improves the predic-tion accuracy of quantized models, especially in low-bit settings. For example, PD-Quant pushes the accuracy of
ResNet-18 up to 53.14% and RegNetX-600MF up to 40.67% in weight 2-bit activation 2-bit. The code is released at https://github.com/hustvl/PD-Quant. 1.

Introduction
Various neural networks have been used in many real-world applications with high prediction accuracy. When de-ployed on resource-limited devices, networks’ vast memory and computation costs become significant challenges. Re-⋆ Equal contribution. ⋄ This work was done when Jiawei Liu and Lin
Niu were interns at Houmo AI. † Corresponding authors. ducing overhead while maintaining the model accuracy has received considerable attention. Network quantization is an effective technique that can compress the neural networks by converting the format of values from floating-point to low-bit [10, 12, 27]. There are two types of quantization: post-training quantization (PTQ) [32] and quantization-aware training (QAT) [18]. QAT requires retraining a model on the labeled training dataset, which is time-consuming and computationally expensive. While PTQ only requires a small number of unlabeled calibration samples to quantize the pre-trained models without retraining, which is suitable for quick deployment. Existing PTQ methods can achieve good prediction accuracy with 8-bit or 4-bit quantization by selecting appropriate quantization parameters. [22, 23, 30].
Local metrics (such as MSE [7] or cosine distance [45] of the activation before and after quantization in layers) are commonly used to search for quantization scaling factors.
These factors are chosen layer by layer by minimizing the local metric with a small number of calibration samples. In this paper, we observe that there is a gap between the se-lected scaling factors and the optimal scaling factors .
Since the noise from quantization will be more severe at low-bit, the prediction accuracy of the quantized model significantly decreases at 2-bit. Recently, some meth-ods [24, 25, 44] have added a new class of quantization pa-rameters, weight rounding value, to adjust the rounding of weights. They optimize both quantization scaling factors and rounding values by reconstructing features layer-wisely or block-wisely. Besides, the quantized model by PTQ re-construction is more likely to be overfitting to the calibra-tion samples because adjusting the rounding of weights will significantly increase the PTQ’s degree of freedom.
We propose an effective PTQ method, PD-Quant, to ad-dress the above-mentioned issues.
In this paper, we fo-cus on improving the performance of PTQ on extremely low bit-width. PD-Quant uses the metric that considers the
We define the optimal quantization scaling factors as the factors that make the quantized model have the lowest task loss (cross-entropy loss calculated by real label) on the validation set.
global information from the prediction difference between the quantized model and the full-precision (FP) model. We show that the quantization parameters optimized by predic-tion difference are more accurate in modeling the quan-tization noise. Besides, PD-Quant adjusts the activations for calibration in PTQ to mitigate the overfitting problem.
The distribution of the activations is adjusted to meet the mean and variance saved in batch normalization layers. Ex-periments show that PD-Quant leads to better quantization parameters and improves the prediction accuracy of quan-tized models, especially in low-bit settings. Our PD-Quant achieves state-of-the-art performance in PTQ. For example,
PD-Quant pushes the accuracy of weight 2-bit activation 2-bit ResNet-18 up to 53.14% and RegNetX-600MF up to 40.67%. Our contributions are summarized as follows: 1. We analyze the influence of different metrics and indi-cate that the widely used local metric can be improved further. 2. We propose to use the information of the prediction difference in PTQ, which improves the performance of the quantized model. 3. We propose Distribution Correction (DC) to adjust the activation distribution to approximate the mean and variance stored in the batch normalization layer, which mitigates the overfitting problem. 2.