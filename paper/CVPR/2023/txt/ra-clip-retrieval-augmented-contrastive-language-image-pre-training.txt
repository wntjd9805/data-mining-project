Abstract
Contrastive Language-Image Pre-training (CLIP) is at-tracting increasing attention for its impressive zero-shot recognition performance on different down-stream tasks.
However, training CLIP is data-hungry and requires lots of image-text pairs to memorize various semantic concepts.
In this paper, we propose a novel and efﬁcient frame-work: Retrieval Augmented Contrastive Language-Image
Pre-training (RA-CLIP) to augment embeddings by online retrieval. Speciﬁcally, we sample part of image-text data as a hold-out reference set. Given an input image, relevant image-text pairs are retrieved from the reference set to enrich the representation of input image. This process can be considered as an open-book exam: with the reference set as a cheat sheet, the proposed method doesn’t need
It to memorize all visual concepts in the training data. explores how to recognize visual concepts by exploiting correspondence between images and texts in the cheat sheet. The proposed RA-CLIP implements this idea and comprehensive experiments are conducted to show how
RA-CLIP works. Performances on 10 image classiﬁcation datasets and 2 object detection datasets show that RA-CLIP outperforms vanilla CLIP baseline by a large margin on zero-shot image classiﬁcation task (+12.7%), linear probe image classiﬁcation task (+6.9%) and zero-shot ROI classiﬁcation task (+2.8%). 1.

Introduction
Traditional visual representation learning systems are trained to predict a ﬁxed set of predetermined image cate-gories [12, 16, 22, 34]. This limits their transferability since additional labeled training data are required to recognize new visual concepts. Recently, vision-language pre-training approaches such as CLIP [29] emerge as a promising alternative which introduces text description as supervision.
CLIP aligns image modality and text modality by learning a
*indicates equal contribution.
Figure 1. Transferring the CLIP and RA-CLIP to 12 down-stream visual recognition datasets for zero-shot evaluation. Our RA-CLIP achieves better results in 10 out of 12 datasets, and brings +12.7% averaged improvements on the 10 image classiﬁcation datasets and 2.8% averaged improvements on the 2 object detection datasets.. modality-shared representation. During pre-training, CLIP learns to pull matched image-text pairs together and push non-matched pairs apart. After pre-training, CLIP can be transferred to zero-shot image classiﬁcation task: categories can be referred by textual descriptions, and the image clas-siﬁcation task can be converted to image-to-text retrieval task. Experimental results show that CLIP performs well on zero-shot image classiﬁcation task, e.g., for ImageNet zero-shot classiﬁcation task, CLIP can match the accuracy of ImageNet pre-trained ResNet50, even that CLIP doesn’t use any of the 1.28 million training examples of ImageNet for training.
Despite the impressive zero-shot performance, CLIP requires lots of image-text pairs to train encoders and memorize various semantic concepts, which limits its ap-plications since it is not affordable for most laboratories and companies. Recent works [24, 25] try to alleviate this limitation by taking full advantage of existing data and
train encoders to memorize concepts as many as possible, e.g., DeCLIP [24] explores widespread supervision from given image-text pairs, and SLIP [25] introduces self-supervised learning which helps encoders learn better visual representation.
In this paper, we propose a novel and efﬁcient way to make use of image-text pairs. We sample part of image-text data as a hold-out reference set. Given an input image, our model ﬁrst retrieves similar images from the reference set with an unsupervised pre-trained image retrieval model, then we use the relationship between retrieved images and texts to augment the representation of input image. A heuristic explanation of the idea is that, it can be considered as an open-book exam: our model doesn’t have to memorize all visual concepts in the training data, but learns to recognize visual concepts with the help of a cheat sheet (i.e., the reference set). We propose a framework called Retrieval
Augmented Contrastive Language-Image Pre-training (RA-CLIP) to implement this idea. Although using the same amount of image-text data with the vanilla CLIP, RA-CLIP achieves better zero-shot classiﬁcation performance and linear probe classiﬁcation performance.
Our contributions are three-fold:
• For contrastive language-image pre-training (CLIP), we present a novel and efﬁcient utilization of image-text pairs. Concretely, we construct a hold-out ref-erence set composed by image-text pairs. Given an input image, we ﬁnd relevant image-text pairs and use them to help us build better representation for the input image.
• We propose Retrieval Augmented Contrastive
Language-Image a
Pre-training framework to implement the idea described above.
We conduct comprehensive experiments to validate the effectiveness of each block. Visualization results are also provided to explain how RA-CLIP works. (RA-CLIP),
• We compare the proposed RA-CLIP with previous methods on a dozen of commonly used visual recog-nition benchmarks. Experimental results show that our proposed method signiﬁcantly outperforms vanilla
CLIP baseline and other recently proposed methods. 2.