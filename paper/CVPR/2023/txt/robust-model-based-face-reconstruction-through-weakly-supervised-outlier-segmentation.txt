Abstract
In this work, we aim to enhance model-based face re-construction by avoiding fitting the model to outliers, i.e. regions that cannot be well-expressed by the model such as occluders or make-up. The core challenge for localizing outliers is that they are highly variable and difficult to anno-tate. To overcome this challenging problem, we introduce a joint Face-autoencoder and outlier segmentation approach (FOCUS). In particular, we exploit the fact that the outliers cannot be fitted well by the face model and hence can be localized well given a high-quality model fitting. The main challenge is that the model fitting and the outlier segmenta-tion are mutually dependent on each other, and need to be inferred jointly. We resolve this chicken-and-egg problem with an EM-type training strategy, where a face autoen-coder is trained jointly with an outlier segmentation net-work. This leads to a synergistic effect, in which the seg-mentation network prevents the face encoder from fitting to the outliers, enhancing the reconstruction quality. The im-proved 3D face reconstruction, in turn, enables the segmen-tation network to better predict the outliers. To resolve the ambiguity between outliers and regions that are difficult to fit, such as eyebrows, we build a statistical prior from syn-thetic data that measures the systematic bias in model fit-ting. Experiments on the NoW testset demonstrate that FO-CUS achieves SOTA 3D face reconstruction performance among all baselines trained without 3D annotation. More-over, our results on CelebA-HQ and AR database show that the segmentation network can localize occluders accurately
âˆ— Denotes same contribution.
Codes available at: github.com/unibas-gravis/Occlusion-Robust-MoFA
C.Li is funded by the China Scholarship Council (CSC) from the Min-istry of Education of P.R. China. B.Egger was supported by a Post-Doc Mobility Grant, Swiss National Science Foundation P400P2 191110.
A.Kortylewski acknowledges support via his Emmy Noether Research
Group funded by the German Science Foundation (DFG) under Grant No. 468670075. Sincere gratitude to Tatsuro Koizumi and William A. P. Smith who offered the MoFA re-implementation.
Figure 1. FOCUS conducts face reconstruction and outlier seg-mentation jointly under weak supervision. Top to bottom: target images, our reconstruction images, and estimated outlier masks. despite being trained without any segmentation annotation. 1.

Introduction
Monocular 3D face reconstruction aims at estimating the pose, shape, and albedo of a face, as well as the illumination conditions and camera parameters of the scene. Solving for all these factors from a single image is an ill-posed problem.
Model-based face autoencoders [31] overcome this problem through fitting a 3D Morphable Model (3DMM) [1, 9] to a target image. The 3DMM provides prior knowledge about the face albedo and geometry such that 3D face reconstruc-tion from a single image becomes feasible, enabling face autoencoders to set the current state-of-the-art in 3D face reconstruction [5]. The network architectures in the face au-toencoders are devised to enable end-to-end reconstruction and to enhance the speed compared to optimization-based alternatives [19, 41], and sophisticated losses are designed to stabilize the training and to get better performance [5].
A major remaining challenge for face autoencoders is that their performance in in-the-wild environments is still limited by nuisance factors such as model outliers, extreme illumination, and poses. Among those nuisances, model outliers are ubiquitous and inherently difficult to handle be-cause of their wide variety in shape, appearance, and loca-tion. The outliers are a combination of the occlusions that do not belong to the face and the mismatches which are the facial parts but cannot be depicted well by the face model, such as pigmentation and makeup on the texture and wrin-kles on the shape. Fitting to the outliers often distorts the prediction (see Fig. 3) and fitting to the mismatches can-not improve the fitting further due to the limitation of the model. Therefore we propose to only fit the inliers, i.e. the target with the outliers excluded.
To prevent distortion caused by model outliers, existing methods often follow a bottom-up approach. For exam-ple, a multi-view shape consistency loss is used as prior to regularize the shape variation of the same face in dif-ferent images [5, 10, 33], or the face symmetry is used to detect occluders [34]. Training the face encoder with dense landmark supervision also imposes strong regulariza-tion [37, 42], while pairs of realistic images and meshes are costly to acquire. Most existing methods apply face [27] or skin [5] segmentation models and subsequently exclude the non-facial regions during reconstruction. These segmenta-tion methods operate in a supervised manner, which suffers from the high cost and efforts for acquiring a great variety of occlusion annotations from in-the-wild images.
In this work, we introduce an approach to handle outliers for model-based face reconstruction that is highly robust, without requiring any annotations for skin, occlusions, or dense landmarks. In particular, we propose to train a Face-autOenCoder and oUtlier Segmentation network, abbrevi-ated as FOCUS, in a cooperative manner. To train the seg-mentation network in an unsupervised manner, we exploit the fact that the outliers cannot be well-expressed by the face model to guide the decision-making process of an out-lier segmentation network. Specifically, the discrepancy be-tween the target image and the rendered face image (Fig. 1 1st and 2nd rows) are evaluated by several losses that can serve as a supervision signal by preserving the similarities among the target image, the reconstructed image, and the reconstructed image under the estimated outlier mask.
The training process follows the core idea of the
Expectation-Maximization (EM) algorithm, by alternating between training the face autoencoder given the currently estimated segmentation mask, and subsequently training the segmentation network based on the current face reconstruc-tion. The EM-like training strategy resolves the chicken-and-egg problem that the outlier segmentation and model fitting are dependent on each other. This leads to a syner-gistic effect, in which the outlier segmentation first guides the face autoencoder to fit image regions that are easy to classify as face regions. The improved face fitting, in turn, enables the segmentation model to refine its prediction.
We define in-domain misfits as errors in regions, where a fixed model can explain but constantly not fit well, which are observed in the eyebrows and the lip region. We assume that such misfits result from the deficiencies of the fitting pipeline. Model-based face autoencoders use image-level losses only, which are highly non-convex and suffer from local optima. Consequently, it is difficult to converge to the globally optimal solution. In this work, we propose to mea-sure and adjust the in-domain misfits with a statistical prior.
Our misfit prior learns from synthetic data at which regions these systematic errors occur on average. Subsequently, the learnt prior can be used to counteract these errors for pre-dictions on real data, especially when our FOCUS structure excludes the outliers. Building the prior requires only data generated by a linear face model without any enhancement and no further improvement in landmark detection.
We demonstrate the effectiveness of our proposed pipeline by conducting experiments on the NoW testset
[29], where we achieve state-of-the-art performance among model-based 3D face methods without 3D supervision. Re-markably, experiments on the CelebA-HQ dataset [20] and the AR database [22] validate that our method is able to predict accurate occlusion masks without requiring any su-pervision during training.
In summary, we make the following contributions: 1. We introduce an approach for model-based 3D face reconstruction that is highly robust, without requiring any human skin or occlusion annotation. 2. We propose to compensate for the misfits with an in-domain statistical misfit prior, which is easy to imple-ment and benefits the reconstruction. 3. Our model achieves SOTA performance at self-supervised 3D face reconstruction and provides accu-rate estimates of the facial occlusion masks on in-the-wild images. 2.