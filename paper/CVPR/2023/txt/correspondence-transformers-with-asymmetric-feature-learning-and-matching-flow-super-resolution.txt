Abstract
This paper solves the problem of learning dense visual correspondences between different object instances of the same category with only sparse annotations. We decompose this pixel-level semantic matching problem into two easier ones: (i) First, local feature descriptors of source and tar-get images need to be mapped into shared semantic spaces to get coarse matching flows. (ii) Second, matching flows in low resolution should be refined to generate accurate point-to-point matching results. We propose asymmetric feature learning and matching flow super-resolution based
†: Corresponding Authors on vision transformers to solve the above problems. The asymmetric feature learning module exploits a biased cross-attention mechanism to encode token features of source im-ages with their target counterparts. Then matching flow in low resolutions is enhanced by a super-resolution network to get accurate correspondences. Our pipeline is built upon vision transformers and can be trained in an end-to-end manner. Extensive experimental results on several popular benchmarks, such as PF-PASCAL, PF-WILLOW, and SPair-71K, demonstrate that the proposed method can catch sub-tle semantic differences in pixels efficiently. Code is avail-able on https://github.com/YXSUNMADMAX/ACTR.
1.

Introduction
Robust semantic matching methods aim to build dense visual correspondences between different objects or scenes from the same category regardless of the large variations in appearances and layouts. These algorithms have been widely exploited in various computer vision tasks, such as object recognition [12,48], cosegmentation [1,40], few-shot learning [20, 29], image editing [15, 18] and etc. Different from classical dense matching tasks, such as stereo match-ing [6,41] and image registration [2,19], semantic matching aims to find the visual consistency in image pairs with large intra-class appearance and layout variations.
State-of-the-art methods, such as Proposal Flow [16],
NCNet [37], Hyperpixel Flow [34], CATs [8] and etc, typ-ically extract features from backbones to measure point-to-point similarity in 4-D correlation tensors, and then refine these 4-D tensors to enforce neighborhood matching con-sistency. Despite that these algorithms have achieved im-pressive results, there are still two key issues that haven’t been discussed thoroughly. First, how to learn feature rep-resentations appropriate for semantic correspondence? Sec-ond, how to enforce neighborhood consensus when the 4-D correlation tensors that are in high resolution? For the first problem, Hyperpixel Flow [34] selects feature maps in convolutional neural networks with Beam search [32], and
MMNet [49] aggregates feature maps at different resolu-tions in a top-down manner to get feature maps in high res-olution. For the second problem, VAT [20] firstly reduces the resolutions of 4-D correlation tensors with a 4-D con-volution operation and then conducts shifted window atten-tion [31] to further reduce the computational cost. Although these exploratory works brought a lot of new ideas, ques-tions listed above still deserve to be discussed seriously.
In this paper, we aim to answer the above questions and propose a novel pipeline for semantic correspondence. Our pipeline consists of feature extraction based on pre-trained feature backbone [5, 17, 50], asymmetric feature learning, and matching flow super-resolution. Different from state-of-the-art methods [8, 34, 49] which directly calculate 4-D matching scores with refined generic backbone features, our core idea focuses on finding a shared semantic space where local feature descriptors of images can be aligned with their to-match counterpart. The asymmetric feature learn-ing module reconstructs source image features with target image features to reduce domain discrepancy between the two thus avoiding reconstructing source and target image features synchronously as in [9, 28]. Meanwhile, to high-light important image regions in target images, we also use source images to identify discriminative parts of foreground objects. In this way, a specific feature space is found for ev-ery image pair to conduct semantic matching.
To avoid huge computational cost during neighborhood consensus enhancement, we map the matching information hidden in 4-D correlation matrices to 2-D matching flow maps through the soft argmax [25]. By reducing the di-mension of the optimization goal from 4-D to 2-D, com-putational cost is alleviated drastically. To achieve pixel-level correspondence, we conduct matching flow super-resolution to enhance neighborhood consensus and improve matching accuracy at the same time. We find that the pro-posed method works quite well in conjunction with trans-former feature backbones, such as MAE [17], DINO [5], and iBOT [50], so we call the proposed method asym-metric correspondence transformer, written as ACTRans-former, and train it end-to-end. We summarize our contri-butions as follows.
• We introduce a novel pipeline for semantic correspon-dence which contains generic feature extraction, asymmet-ric feature learning, and matching flow super-resolution. By conducting asymmetric feature learning, we extract specific features for every image pair and thus get more accurate correspondences. Besides, we replace the 4-D correlation refinement with the 2-D matching flow super-resolution, which saves computational cost greatly.
• We propose asymmetric feature learning that can project features of image pairs into a shared feature space easily and reduce the feature ambiguity at the same time. For match-ing flow super-resolution, we conduct multi-path super-resolution to benefit from different matching tensors and acquire significant improvements.
• Experiments on several popular benchmarks indicate that the proposed ACTRansformer outperforms previous state-of-the-art methods by clear margins. Qualitative results are presenred in Figure 1. 2.