Abstract
Video-language embeddings are a promising avenue for injecting semantics into visual representations, but exist-ing methods capture only short-term associations between seconds-long video clips and their accompanying text. We propose HierVL, a novel hierarchical video-language em-bedding that simultaneously accounts for both long-term and short-term associations. As training data, we take videos accompanied by timestamped text descriptions of human actions, together with a high-level text summary of the activity throughout the long video (as are available in Ego4D). We introduce a hierarchical contrastive train-ing objective that encourages text-visual alignment at both the clip level and video level. While the clip-level con-straints use the step-by-step descriptions to capture what is happening in that instant, the video-level constraints use the summary text to capture why it is happening, i.e., the broader context for the activity and the intent of the ac-tor. Our hierarchical scheme yields a clip representation that outperforms its single-level counterpart as well as a long-term video representation that achieves SotA results on tasks requiring long-term video modeling. HierVL success-fully transfers to multiple challenging downstream tasks (in EPIC-KITCHENS-100, Charades-Ego, HowTo100M) in both zero-shot and fine-tuned settings. 1.

Introduction
Understanding human activity in video is a fundamental vision problem with abundant applications in augmented re-ality, robotics, and information retrieval. The field has made exciting advances, from new models for recognition [24, 53, 86] and self-supervised representations [55, 58, 61, 90] to major datasets [16, 34, 63, 74, 106]. Nonetheless, activity understanding in video lags noticeably behind object under-standing in images, where today’s AI models compete well with people.
One key reason for this discrepancy is the fact that whereas objects present themselves directly in the pixels— no subtext required—activity naturally has broad temporal
Website: https://vision.cs.utexas.edu/projects/hiervl/
Figure 1. Conventional video-language embeddings are trained to match short-term clips with their corresponding descriptions, e.g., open tap (in orange boxes), thus capturing what is happen-ing. Our hierarchical video-language embedding (in dotted blue box) learns both short-term and long-term visual-text relations, thereby capturing why is it happening (e.g., making salad dress-ing). Long-term intent is conveyed by textual summaries (blue) that give an abstractive summary of the whole video, and comple-ment the more literal step-by-step narrations (green). context rooted in the human actor’s (latent) intentions. Not only does an activity stretch across video frames, but also its interpretation relies on the larger context of what the person is trying to accomplish. Thus, there is a natural hierarchy of information in video, starting with the short-term “what the person is literally doing right now” (e.g., reaching for the stove) and going all the way to the long-term “what the person aims to do” (e.g., cook dinner).
As a step towards capturing this hierarchy, we explore video-language representation learning. Video often has ac-companying timestamped text, whether from spoken nar-rations in a how-to video [63, 75, 106], closed caption text and scripts [9,76], or deliberate text annotations [16,34,91].
Existing video-language models learn a correspondence be-tween the two modalities by matching short video segments with their text counterpart, typically with a learned embed-ding [3, 55, 61, 90] that produces a language-enriched video clip encoder. However, this standard approach risks captur-ing only the short-term actions. Granular comments such as “now I pour milk in the pan” or “he picked up a wa-ter hose” fail to capture the overall goal of the activity, like making a coffee or cleaning a car. As a result, at inference time their encodings for unseen videos can be myopic and miss sequential dependencies between observed events.
To tackle this problem, we introduce HierVL: a novel hierarchical video-language model that captures both short-term actions and long-term intents in video. Unlike standard video-language embeddings, our method aims to simulta-neously capture the immediate observed actions as well as their contribution to the longer-term goal. To that end, given training video accompanied by timestamped clip-level text descriptions as well as global (video-level) text summaries,
HierVL learns a video-text embedding for hierarchical tem-poral understanding using two layers of contrastive learn-ing. The top (parent) layer encourages the aggregated video clips to be close to the overarching textual summary (e.g., he makes spaghetti dinner), while the bottom (child) layer trains individual clips to be similar to their respective de-scriptions (e.g., he turns on the cooker). See Fig. 1.
To our knowledge, ours is the first work to create a hier-archical video-language embedding. Our idea to blend ab-stract textual summaries with literal text descriptions is new.
Furthermore, our model design addresses constituent tech-nical challenges—namely, we circumvent the typical ex-pense of long-term feature learning [4, 43, 86] by using ag-gregation of short-term features, and we show how to jointly train with two levels of annotation in a way that staves off catastrophic forgetting of either layer.
This hierarchical training yields not only global video-level representations that capture long-term information (e.g., intent and temporal dependencies), but also clip-level video features that are more expressive than those tradi-tionally learned via single-level schemes. This happens by means of our parent-child learning framework, which re-quires the aggregation of clip features within a video to match the long-term context captured by the summary.
We demonstrate our model by training with the narra-tions and summaries in the 3,670-hour egocentric video dataset Ego4D [13, 34]. We show that HierVL outperforms strong baselines and state-of-the-art methods for multiple video benchmarks, successfully transferring its pretrained representation for inference on Charades-Ego [74], EPIC-KITCHENS [16], and HowTo100M [63].1 We evaluate our representations on both hierarchy levels.
In particu-lar, at the time of submission, HierVL achieves state-of-the-art performance on Ego4D Long Term Anticipation (LTA),
Charades-Ego Action Recognition, EPIC-KITCHENS-100 1Note that we do not need any text or summary annotations for these downstream datasets and tasks.
Multi-Instance Retrieval (zero-shot and fine-tuned settings), and HowTo100M Long Video Classification. 2.