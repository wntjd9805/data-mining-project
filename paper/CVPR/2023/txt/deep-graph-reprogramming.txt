Abstract
In this paper, we explore a novel model reusing task tai-lored for graph neural networks (GNNs), termed as “deep graph reprogramming”. We strive to reprogram a pre-trained GNN, without amending raw node features nor model parameters, to handle a bunch of cross-level down-stream tasks in various domains. To this end, we propose an innovative Data Reprogramming paradigm alongside a
Model Reprogramming paradigm. The former one aims to address the challenge of diversiﬁed graph feature dimen-sions for various tasks on the input side, while the latter alleviates the dilemma of ﬁxed per-task-per-model behav-ior on the model side. For data reprogramming, we specif-ically devise an elaborated Meta-FeatPadding method to deal with heterogeneous input dimensions, and also de-velop a transductive Edge-Slimming as well as an induc-tive Meta-GraPadding approach for diverse homogenous samples. Meanwhile, for model reprogramming, we pro-pose a novel task-adaptive Reprogrammable-Aggregator, to endow the frozen model with larger expressive capaci-ties in handling cross-domain tasks. Experiments on four-teen datasets across node/graph classiﬁcation/regression, 3D object recognition, and distributed action recognition, demonstrate that the proposed methods yield gratifying re-sults, on par with those by re-training from scratch. 1.

Introduction
With the explosive growth of graph data, graph neural networks (GNNs) have been deployed across increasingly wider areas [18, 20, 55, 57, 58], such as recommendation system [48] and autonomous driving [32, 45, 47]. However, the favorable performance for such applications generally comes at the expense of tremendous training efforts and high memory loads, precluding the deployment of GNNs on the edge side. As such, reusing pre-trained GNNs to alleviate training costs has recently emerged as a trending research topic [7, 11, 19, 21, 39, 53, 54, 56, 69].
Pioneered by the work of [56] that generalize knowl-edge distillation [14,31,34,59–61] to the non-Euclidean do-Figure 1. Illustrations of the proposed task of deep graph repro-gramming (GARE) that aims to reuse pre-trained GNNs to handle plenty of cross-level tasks with heterogeneous graph feature di-mensions, without changing model architectures nor parameters. main, almost all existing approaches on reusing GNNs are achieved by following the distillation pipeline in [56]. De-spite the encouraging results, the distilling-based scheme is limited to the per-task-per-distillation setting, where a dis-tilled model can only tackle the same task as the teacher can, leading to considerable storage and computation bur-dens, especially for the deployment of multiple tasks.
Meanwhile, the distillation mechanism rests upon the hypothesis that abundant pre-trained models are available in the target domains, which indeed holds for image-based areas that always take data in the regular RGB form, thereby readily allowing for per-model-multiple-dataset reusing.
However, such an assumption is typically not satisﬁed in the non-Euclidean domain: on the input side, irregular graph samples have heterogeneous feature dimensions, as shown with the color bars in Fig. 1; on the task side, graph analysis takes various task levels and settings, such as graph-, node-, and edge-level learning, as well as transductive and induc-tive scenarios. Such nature of topological diversities leads to inadequate pre-trained GNNs that ﬁt the target down-stream tasks.
In this paper, we strive to take one step towards gener-alized and resource-efﬁcient GNN reusing, by studying a novel deep graph reprogramming (GARE) task. Our goal is
to reuse a single pre-trained GNN across multiple task lev-els and domains, for example the pre-trained one working on node classiﬁcation and the downstream ones on graph classiﬁcation and regression, as shown in Fig. 1. We fur-ther impose two constraints to both data and model, where raw features and parameters are frozen in handling down-stream tasks. As such, unlike distillation that essentially leverages a pre-trained teacher to guide the re-training of a student, the proposed task of GARE, without re-training nor ﬁne-tuning, can thereby be considered to reprogram a pre-trained GNN to perform formerly unseen tasks.
Nonetheless, such an ambitious goal is accomplished with challenges: diversiﬁed graph feature dimensions and limited model capacities with a single frozen GNN. Driven by this observation, we accordingly reformulate GARE into two dedicated paradigms on data and model sides, respec-tively, termed as data reprogramming (DARE) and model reprogramming (MERE). The goal of DARE is to handle downstream graph samples with both the heterogeneous and homogenous dimensions, without amending pre-trained ar-chitectures. Meanwhile, MERE aims to strengthen the ex-pressive power of frozen GNNs by dynamically changing model behaviors depending on various tasks.
Towards this end, we propose a universal Meta-FeatPadding (MetaFP) approach for heterogeneous-DARE that allows the pre-trained GNN to manipulate heterogeneous-dimension graphs, by accommodating pre-trained feature dimensions via adaptive feature padding in a task-aware manner. The rationale behind the proposed
MetaFP, paradoxically, is derived from adversarial repro-gramming examples [8] that are conventionally treated as attacks to learning systems, where attackers secretly repur-pose the use of a target model without informing model providers, by inserting perturbations to input images. Here we turn the role of the adversarial reprogramming example on its head, by padding around graph perturbations for generalized cross-task model reusing.
Complementary to the dedicated MetaFP that is tai-lored for heterogeneous-DARE, we also devise a trans-ductive Edge-Slimming (EdgSlim) and an inductive Meta-GraPadding (MetaGP) methods for homogenous-DARE, that handle the downstream graphs with homogenous di-mensions under transductive and inductive task settings, re-spectively, by adaptively eliminating node connections or inserting a tiny task-speciﬁc graph, with only, for exam-ple, ten vertices, to the raw input sample. Furthermore, we perform a pilot study on MERE, exploring the pre-trained model capacity for various downstream tasks, by only re-programming the pre-trained aggregation behavior (ReAgg) upon the well-established Gumbel-Max trick.
In sum, our contribution is a novel GNN-based model reusing paradigm that allows for the adaption of a pre-trained GNN to multiple cross-level downstream tasks, and meanwhile requires no re-training nor ﬁne-tuning.
This is typically achieved through a series of complemen-tary approaches entitled MetaFP, EdgSlim, and MetaGP, that tackle the heterogeneous- and homogenous-dimension graphs within the transductive and inductive scenarios, re-spectively, together with an elaborated ReAgg method to enhance the model capacity. Experimental results on four-teen benchmarks demonstrate that a pre-trained GNN with
GARE is competent to handle all sorts of downstream tasks. 2.