Abstract
Visual grounding (VG) aims to establish fine-grained alignment between vision and language. Ideally, it can be a testbed for vision-and-language models to evaluate their understanding of the images and texts and their reason-ing abilities over their joint space. However, most existing
VG datasets are constructed using simple description texts, which do not require sufficient reasoning over the images and texts. This has been demonstrated in a recent study [27], where a simple LSTM-based text encoder without pretrain-ing can achieve state-of-the-art performance on mainstream
VG datasets. Therefore, in this paper, we propose a novel benchmark of Scene Knowledge-guided Visual Grounding (SK-VG), where the image content and referring expressions are not sufficient to ground the target objects, forcing the models to have a reasoning ability on the long-form scene knowledge. To perform this task, we propose two approaches to accept the triple-type input, where the former embeds knowledge into the image features before the image-query interaction; the latter leverages linguistic structure to assist in computing the image-text matching. We conduct exten-sive experiments to analyze the above methods and show that the proposed approaches achieve promising results but still leave room for improvement, including performance and interpretability. The dataset and code are available at https://github.com/zhjohnchan/SK-VG. 1.

Introduction
Visual grounding (VG), aiming to locate an object re-ferred to by a description phrase/text in an image, has emerged as a prominent attractive research direction. It can be applied to various tasks (e.g., visual question answering
[4, 13, 38, 51] and vision-and-language navigation [1, 11, 35]) and also be treated as a proxy to evaluate machines for
*Equal contribution
†Corresponding author
Figure 1. An example from the proposed SK-VG dataset for scene knowledge-guided visual grounding. The task requires a model to reason over the (image, scene knowledge, query) triple to locate the target object referred to by the query. open-ended scene recognition. Typically, VG requires mod-els to reason over vision and language and build connec-tions through single-modal understanding and cross-modal matching. Yet, current VG benchmarks (e.g., RefCOCO [47],
RefCOCO+ [47], RefCOCOg [29], ReferItGame [17], and
CLEVR-Ref+ [23]) can not serve as a good test bed to eval-uate the reasoning ability since they only focus on simple vision-language alignment. In addition to the simple nature of constructed referring expressions, this can be reflected in the recent state-of-the-art study [27], where they showed that
VG models are less affected by language modeling through extensive empirical analyses.
In this paper, we believe that the intrinsic difficulty of VG lies in the difference between perceptual representations of images and cognitive representations of texts. Specifically, vi-sual features are obtained through perceptual learning, which
Figure 2. Illustrations of four categories of grounding tasks, including categories, phrases, linguistic expressions, and linguistic expres-sion+scene knowledge. The height of the input green and blue rectangles denotes its relative information. only maps visual appearances in images to semantic con-cepts. However, open-ended queries might require VG mod-els to understand the whole scene knowledge before perform-ing reasoning to locate the target object. As shown in Figure 1, the perceptual features can encode the information about
“a wine glass”, but it would struggle to locate “Jake’s wine glass” without the scene knowledge about “who is Jake?”.
This is a challenging task owing to two facts: (i) From the dataset perspective, there are no relevant benchmarks for the VG researchers to evaluate their models; (ii) From the model/algorithm perspective, it is not easy to design models to perform reasoning among images, scene knowledge, and open-ended querying texts.
Therefore, we propose to break this limitation of cur-rent VG research and construct a new benchmark requir-ing VG to perform reasoning over Scene Knowledge (i.e., text-based stories). The benchmark named SK-VG contains
∼40,000 referring expressions and 8,000 scene stories from 4,000 images, where each image contains 2 scene stories with 5 referring expressions for each story. Moreover, to evaluate the difficulty levels of queries, we curate the test set by splitting the samples into easy/medium/hard cate-gories to provide a detailed evaluation of the vision-language models. Under this new setting, we develop a one-stage ap-proach (i.e., Knowledge-embedded Vision-Language Inter-action (KeViLI)) and a two-stage approach (i.e., Linguistic-enhanced Vision-Language Matching (LeViLM)). In KeViLI, the scene knowledge is firstly embedded into the image fea-tures, and then the interaction between the image and the query is performed; In LeViLM, the image features and the text features are first extracted, and then the match-ing between the (image) regions and the (text) entities are computed, assisted by the structured linguistic information.
Through extensive experiments, we show that the proposed approaches can achieve the best performance but still leave room for improvement, especially in the hard split. It chal-lenges the models from three perspectives: First, it is an open-ended grounding task; Second, the scene stories are long narratives consisting of multiple sentences; Third, it might require the multi-hop reasoning ability of the models.
In summary, the contributions of this paper are three-fold:
• We introduce a challenging task that requires VG mod-els to reason over (image, scene knowledge, query) triples and build a new dataset named SK-VG on top of real images through manual annotations.
• We propose two approaches to enhance the reasoning in
SK-VG, i.e., one one-stage approach KeViLI and one two-stage approach LeViLM.
• Extensive experiments demonstrate the effectiveness of the proposed approaches. Further analyses and discus-sions could be a good starting point for future study in the vision-and-language field. 2.