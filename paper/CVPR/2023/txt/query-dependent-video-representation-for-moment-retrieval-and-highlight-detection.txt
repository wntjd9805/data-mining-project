Abstract
Recently, video moment retrieval and highlight detec-tion (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query.
Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insigniﬁcant role of a given query in transformer architec-tures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video repre-sentation. Then, to enhance the model’s capability of exploit-ing the query information, we manipulate the video-query pairs to produce irrelevant pairs. Such negative (irrelevant) video-query pairs are trained to yield low saliency scores, which in turn, encourages the model to estimate precise ac-cordance between query-video pairs. Lastly, we present an input-adaptive saliency predictor which adaptively deﬁnes the criterion of saliency scores for the given video-query pairs. Our extensive studies verify the importance of build-ing the query-dependent representation for MR/HD. Specif-ically, QD-DETR outperforms state-of-the-art methods on
QVHighlights, TVSum, and Charades-STA datasets. Codes are available at github.com/wjun0830/QD-DETR. 1.

Introduction
Along with the advance of digital devices and platforms, video is now one of the most desired data types for con-sumers [2, 48]. Although the large information capacity of videos might be beneﬁcial in many aspects, e.g., informative
∗ Equal contribution (cid:2) Corresponding author (cid:57) (cid:56)
Positive Query: Man with curly hair speaks directly to camera
Negative Query: Kids exercise in front of parked cars. (cid:57) (cid:57)
Lack of query-relevance
GT
GT  t DETR)
Pred. (Moment-DETR)
P d (M
GT
GT o e d i
V t
Moment
DETR
Ours (cid:1872) = 0
Pred.
Pred. (cid:1872) = T
Large gap between pos. and neg. in relevant moment
Saliency of Positive Query
Saliency of Negative Query
Figure 1. Comparison of highlight-ness (saliency score) when relevant and non-relevant queries are given. We found that the existing work only uses queries to play an insigniﬁcant role, thereby may not be capable of detecting negative queries and video-query relevance; saliency scores for clips in ground-truth (GT) moments are low and equivalent for positive and negative queries. On the other hand, query-dependent representations of QD-DETR result in corresponding saliency scores to the video-query relevance and precisely localized moments. and entertaining, inspecting the videos is time-consuming, so that it is hard to capture the desired moments [1, 2].
Indeed, the need to retrieve user-requested or highlight moments within videos is greatly raised. Numerous research efforts were put into the search for the requested moments in the video [1, 12, 13, 30] and summarizing the video high-lights [4, 33, 47, 57]. Recently, Moment-DETR [23] further spotlighted the topic by proposing a QVHighlights dataset that enables the model to perform both tasks, retrieving the moments with their highlight-ness, simultaneously.
When describing the moment, one of the most favored types of query is the natural language sentence (text) [1].
While early methods utilized convolution networks [14, 44, 59], recent approaches have shown that deploying the atten-tion mechanism of transformer architecture is more effective to fuse the text query into the video representation. For example, Moment-DETR [23] introduced the transformer
architecture which processes both text and video tokens as input by modifying the detection transformer (DETR), and
UMT [31] proposed transformer architectures to take multi-modal sources, e.g., video and audio. Also, they utilized the text queries in the transformer decoder. Although they brought breakthroughs in the ﬁeld of MR/HD with seminal architectures, they overlooked the role of the text query. To validate our claim, we investigate the Moment-DETR [23] in terms of the impact of text query in MR/HD (Fig.1). Given the video clips with a relevant positive query and an irrel-evant negative query, we observe that the baseline often neglects the given text query when estimating the query-relevance scores, i.e., saliency scores, for each video clip.
To this end, we propose Query-Dependent DETR (QD-DETR) that produces query-dependent video representation.
Our key focus is to ensure that the model’s prediction for each clip is highly dependent on the query. First, to fully utilize the contextual information in the query, we revise the transformer encoder to be equipped with cross-attention lay-ers at the very ﬁrst layers. By inserting a video as the query and a text as the key and value of the cross-attention layers, our encoder enforces the engagement of the text query in extracting video representation. Then, in order to not only in-ject a lot of textual information into the video feature but also make it fully exploited, we leverage the negative video-query pairs generated by mixing the original pairs. Speciﬁcally, the model is learned to suppress the saliency scores of such negative (irrelevant) pairs. Our expectation is the increased contribution of the text query in prediction since the videos will be sometimes required to yield high saliency scores and sometimes low ones depending on whether the text query is relevant or not. Lastly, to apply the dynamic criterion to mark highlights for each instance, we deploy a saliency token to represent the entire video and utilize it as an input-adaptive saliency criterion. With all components combined, our QD-DETR produces query-dependent video representa-tion by integrating source and query modalities. This further allows the use of positional queries [29] in the transformer decoder. Overall, our superior performances over the exist-ing approaches validate the signiﬁcance of the role of text query for MR/HD. 2.