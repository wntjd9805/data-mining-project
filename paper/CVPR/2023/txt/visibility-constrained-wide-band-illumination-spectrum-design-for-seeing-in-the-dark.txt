Abstract
Seeing-in-the-dark is one of the most important and challenging computer vision tasks due to its wide appli-cations and extreme complexities of in-the-wild scenar-ios. Existing arts can be mainly divided into two threads: 1) RGB-dependent methods restore information using de-graded RGB inputs only (e.g., low-light enhancement), 2)
RGB-independent methods translate images captured under auxiliary near-infrared (NIR) illuminants into RGB domain (e.g., NIR2RGB translation). The latter is very attractive since it works in complete darkness and the illuminants are visually friendly to naked eyes, but tends to be unstable due to its intrinsic ambiguities. In this paper, we try to robustify
NIR2RGB translation by designing the optimal spectrum of auxiliary illumination in the wide-band VIS-NIR range, while keeping visual friendliness. Our core idea is to quan-tify the visibility constraint implied by the human vision sys-tem and incorporate it into the design pipeline. By model-ing the formation process of images in the VIS-NIR range, the optimal multiplexing of a wide range of LEDs is auto-matically designed in a fully differentiable manner, within the feasible region defined by the visibility constraint. We also collect a substantially expanded VIS-NIR hyperspec-tral image dataset for experiments by using a customized 50-band filter wheel. Experimental results show that the task can be significantly improved by using the optimized wide-band illumination than using NIR only. Codes Avail-able: https://github.com/MyNiuuu/VCSD. 1.

Introduction
Seeing-in-the-dark is critical for modern industries, be-cause of its promising applications in nighttime photogra-phy and visual surveillance. However, it remains challeng-ing due to complex degradation mechanisms and dynamics of in-the-wild environments.
To achieve this task, a number of methods have been pro-*Corresponding author posed, which can be roughly divided into two threads. The first thread features RGB-dependent methods [3, 4, 29, 42, 44,45,52] that aim to fully exploit the RGB input, even with severe degradations. These methods have gained great suc-cess through directly learning the mapping from low-light input to normal-light output, in the presence of complex noises and color discrepancies. However, even state-of-the-art methods along this thread may struggle with in-the-wild data captured under nearly complete darkness.
In contrast, the second thread features RGB-independent methods [24, 28, 33, 38, 40] for non-interfering surveillance that try to recover RGB information from images of invis-ible ranges, without requiring any RGB input. The most attractive characteristics lie in its applicability to complete darkness and the visual friendliness of auxiliary illumina-tion to naked eyes. NIR2RGB is one of the representative tasks of this thread, which aims to translate near-infrared images to RGB images.
As for auxiliary illumination in the NIR range, the in-dustry practice is to use NIR LEDs, usually centered at 850 nm or 940 nm. However, the captured images are almost monochromatic and lack visual color and texture, which makes NIR2RGB translation ambiguous. The funda-mental reasons for the ambiguities are two folds: 1) The spectral sensitivities of commodity RGB cameras almost overlap around both 850 nm and 940 nm, making it hard to recover three-channel color from a single intensity ob-servation. 2) Reflectance spectra of many materials become almost indistinguishable beyond 850 nm, which leads to ob-vious structure gaps from RGB images. As a result, exist-ing studies that tried to directly convert such NIR images to VIS images, even with the most advanced deep learn-ing techniques, can hardly provide satisfying results due to these fundamental restrictions. In [24], Liu et al. pro-posed to properly multiplex different NIR LEDs, ranging from 700 nm to 1000 nm, to robustify the NIR2RGB task, and achieves apparently better results than using traditional 850 nm or 940 nm LEDs. However, structure gaps still ex-ist due to the restriction of wavelengths in the NIR range, making the results far from satisfying.
The basic motivation of these methods arises from the in-visibility of human naked eyes to NIR lights, so as to reduce visual interference and light pollution. However, up to now, none of these works have explicitly formulated the visibility of certain illumination. Liu et al. [24] empirically picked up the NIR range beyond 700 nm, and there is a clear tendency that LEDs closer to this prescribed boundary are preferred according to their results. A natural question is: Is there an exact boundary between visible and invisible? This is important since it determines how much information in the
VIS range can be utilized to help RGB recovery.
Inspired by the aforementioned methods, we propose to quantify and incorporate the human vision system into our model, which enables us to significantly robustify this task via illumination spectrum design in the wide-band spectral range from 420 nm to 890 nm. Similar to [24], we directly optimize the spectral curve by training an image enhance-ment model on hyperspectral datasets. Specifically, based on the human vision system, we establish a Visibility Con-strained Spectrum Design (VCSD) model to quantify the visibility of certain spectra, and to assure the prescribed vis-ibility level will not be violated. To achieve this, a visibility threshold ˆΨ is introduced, which serves as the visibility up-per bound during the spectrum design process. In practice, this threshold can be changed according to the desired level of visibility, without destroying the validity of our method.
According to the upper bounded visibility level, the model scales down the designed LED spectrum (if necessary) to assure that the new spectrum is friendly to naked eyes. Af-ter that, we design a physic-based Imaging Process Simu-lation (IPS) model which synthesizes images using the cor-responding LED spectrum, camera spectral sensitivity, and the reflectance spectrum of the scene. The IPS model also contains a noise model to consider the noise effect during the realistic imaging process. Since we consider the spec-trum from 420 nm to 890 nm, we synthesize one VIS image with lights shorter than 700 nm and one VIS-NIR image with the full spectrum. Through deep learning, we directly minimize the reconstruction loss and finally get the optimal
LED spectral curve that can be physically realized by driv-ing LEDs with appropriate voltage and current.
We evaluate the effectiveness of our model and designed curve on hyperspectral datasets including our proposed and previous [32] datasets. Compared to existing methods, our model clearly achieves superior results, demonstrating the powerfulness of wide-band illumination spectrum design under visibility constraints.
The main highlights of this work are:
• For the first time, we propose a paradigm that quan-tifies and incorporates the human vision system for seeing-in-the-dark, which enables us to significantly improve the task via illumination spectrum design in a wide-band coverage from 420 nm to 890 nm.
• A novel Visibility Constrained Spectrum Design (VCSD) model is proposed to formulate and assure the visibility level of certain spectra to human naked eyes during the optimization process. The visibility thresh-old can be changed according to the desired level of visibility, without destroying the validity of the model.
• We design a physic-based Imaging Process Simula-tion (IPS) module which synthesizes the input images based on the imaging process and the noise model.
• We contribute a VIS-NIR wide-band hyperspectral im-age dataset to supplement existing ones in terms of quality and quantity. 2.