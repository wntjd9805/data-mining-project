Abstract
Low-light image enhancement (LLIE) investigates how to improve illumination and produce normal-light images.
The majority of existing methods improve low-light images via a global and uniform manner, without taking into ac-count the semantic information of different regions. With-out semantic priors, a network may easily deviate from a region’s original color. To address this issue, we propose a novel semantic-aware knowledge-guided framework (SKF) that can assist a low-light enhancement model in learning rich and diverse priors encapsulated in a semantic segmen-tation model. We concentrate on incorporating semantic knowledge from three key aspects: a semantic-aware em-bedding module that wisely integrates semantic priors in feature representation space, a semantic-guided color his-togram loss that preserves color consistency of various in-stances, and a semantic-guided adversarial loss that pro-duces more natural textures by semantic priors. Our SKF is appealing in acting as a general framework in LLIE task. Extensive experiments show that models equipped with the SKF significantly outperform the baselines on mul-tiple datasets and our SKF generalizes to different models and scenes well. The code is available at Semantic-Aware-Low-Light-Image-Enhancement 1.

Introduction
In real world, low-light imaging is fairly common due to unavoidable environmental or technical constraints such as insufficient illumination and limited exposure time. Low-light images not only have poor visibility for human per-ception, but also are unsuitable for subsequent multime-dia computing and downstream vision tasks designed for high-quality images [4, 9, 36]. Thus, low-light image en-hancement (LLIE) is proposed to reveal buried details in low-light images and avoid degraded performance in sub-sequent vision tasks. Mainstream traditional methods for
LLIE include Histogram Equalization-based methods [2] and Retinex model-based methods [18].
Recently, many deep learning-based LLIE methods have proposed, such as end-to-end frameworks [5,7,34,45,46,48] and Retinex-based frameworks [29, 41, 43, 44, 49, 53, 54].
Benefiting from their ability in modeling the mapping be-tween the low-light and high-quality image, deep LLIE methods commonly achieve better results than traditional approaches. However, existing methods typically improve low-light images globally and uniformly, without taking into account the semantic information of different regions, which is crucial for enhancement. As shown in Fig. 1(a), a network that lacks the utilization of semantic priors can easily deviate from a region’s original hue [22]. Further-more, studies have demonstrated the significance of incor-porating semantic priors into low-light enhancement. Fan et al. [8] utilize semantic map as prior and incorporated it into the feature representation space, thereby enhancing image quality. Rather than relying on optimizing intermediate fea-tures, Zheng et al. [58] adopt a novel loss to guarantee the semantic consistency of the enhanced images. These meth-ods successfully combine the semantic priors with LLIE task, demonstrating the superiority of semantic constraints and guidance. However, their methods fail to fully ex-ploit the knowledge that semantic segmentation networks can provide, limiting the performance gain by semantic pri-ors. Furthermore, the interaction between segmentation and enhancement is designed for specific methods, limiting the possibility of incorporating semantic guidance into LLIE task. Hence, we wonder two questions: 1. How can we obtain various and available semantic knowledge? 2. How does semantic knowledge contribute to image quality im-provement in LLIE task?
We attempt to answer the first question. First, a semantic segmentation network pre-trained on large-scale datasets is introduced as a semantic knowledge bank (SKB). The SKB can provide richer and more diverse semantic priors to im-prove the capability of enhancement networks. Second, ac-cording to previous works [8, 19, 58], the available priors provided by the SKB primarily consist of intermediate fea-tures and semantic maps. Once training a LLIE model, the
SKB yields above semantic priors and guides the enhance-ment process. The priors can not only refine image fea-tures by employing techniques like affinity matrices, spatial feature transformations [40], and attention mechanisms, but also guide the design of objective functions by explicitly incorporating regional information into LLIE task [26].
Then we try to answer the second question. We design a series of novel methods to integrate semantic knowledge into LLIE task based on the above answers, formulating in a novel semantic-aware knowledge-guided framework (SKF). First, we use the High-Resolution Network [38] (HRNet) pre-trained on the PASCAL-Context dataset [35] as the previously mentioned SKB. In order to make use of intermediate features, we develop a semantic-aware embed-ding (SE) module. It computes the similarity between the reference and target features and employs cross-modal in-teractions between heterogeneous representations. As a re-sult, we quantify the semantic awareness of image features as a form of attention and embed semantic consistency in enhancement network.
Second, some methods [20, 55] propose to optimize im-age enhancement using color histogram in order to preserve the color consistency of the image rather than simply en-hancing the brightness globally. The color histogram, on the other hand, is still a global statistical feature that cannot guarantee local consistency. Hence, we propose a semantic-guided color histogram (SCH) loss to refine color consis-tency. Here, we intend to make use of local geometric information derived from the scene semantics and global color information derived from the content. In addition to guarantee original color of the enhanced image, it can also add spatial information to the color histogram, performing a more nuanced color recovery.
Third, existing loss functions are not well aligned with human perception and fail to capture an image’s intrinsic signal structure, resulting in unpleasing visual results. To improve visual quality, EnlightenGAN [16] employs global and local image-content consistency and randomly chooses the local patch. However, the discriminator do not know where the regions are likely to be ‘fake’. Thus, we propose a semantic-guided adversarial (SA) loss. Specifically, the ability of the discriminator is improved by using segmen-tation map to determine the fake areas, which can improve the image quality further.
The main contributions of our work are as follows:
• We propose a semantic-aware knowledge-guided framework (SKF) to boost the performance of exist-ing methods by jointly maintaining color consistency and improving image quality.
• We propose three key techniques to take full advantage of semantic priors provided by semantic knowledge bank (SKB): semantic-aware embedding (SE) mod-ule, semantic-guided color histogram (SCH) loss, and semantic-guided adversarial (SA) loss.
• We conduct experiments on LOL/LOL-v2 datasets and unpaired datasets. The experimental results demon-strate large performance improvements by our SKF, verifying its effectiveness in resolving the LLIE task. 2.