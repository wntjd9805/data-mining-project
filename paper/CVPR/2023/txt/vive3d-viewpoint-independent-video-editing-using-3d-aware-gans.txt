Abstract 1.

Introduction
We introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. We propose two new build-ing blocks. First, we introduce a novel GAN inversion tech-nique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters.
Second, besides traditional semantic face edits (e.g. for age and expression), we are the first to demonstrate edits that show novel views of the head enabled by the inherent prop-erties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video.
Our experiments demonstrate that VIVE3D generates high-fidelity face edits at consistent quality from a range of cam-era viewpoints which are composited with the original video in a temporally and spatially consistent manner.
*This work was conducted during an internship at Meta RL Research.
Semantic image editing has been an active research topic for the past few years. Previous work [21] uses Generative
Adversarial Networks (GANs) to produce high-fidelity re-sults in the image space. The most popular backbone is
StyleGAN [26–29] as it generates high-resolution domain-specific images while providing a disentangled latent space that can be utilized for editing operations. To edit real pho-tographs, there are typically two steps: The first step maps the input image to the latent space of a pre-trained gener-ator. This is usually accomplished either through encoder-based embedding or through optimization, such that gen-erator can accurately reconstruct the image from the latent code [53]. The second step is semantic image manipula-tion, where one latent input representation is mapped to an-other to obtain a certain attribute edit, (e.g. changing age, facial expression, glasses, or hairstyle). While existing ap-proaches produce impressive results on single images, ex-tending them to videos is far from straightforward. Among
the challenges that arise are: (1) people tend to move their heads freely in videos (instead of assuming frontal image inputs), (2) the inversion of multiple frames should be coor-dinated, (3) the inverted face and edits need to be temporally consistent and (4) the compositing of the edited face with the original frame must maintain boundary consistency.
A recent set of approaches has focused on 3D-aware
GANs where a 2D face generator is combined with a neural renderer. Given a latent code, a 2D image and the under-lying 3D geometry are generated, thus allowing for some camera movement while rendering the head of the person.
In this paper, we tackle the problem of viewpoint-independent face editing in videos. The edited face is ren-dered from novel views in a temporally-consistent man-ner. Specifically, we use a 3D-aware GAN in the tempo-ral domain and apply facial image editing techniques per frame that are temporally smooth regardless of the ren-dered view. Compared with other GAN-based video edit-ing approaches [5, 48], our method is the first to perform viewpoint-independent video editing while showing the full upper body of the person in the video with high fidelity.
VIVE3D takes a video of a person captured from a monocular camera as input. The captured person can move freely across time, talk, and make facial expressions while their body can be visible. Unlike all prior work that learns a generator and performs edits on the exact same video, we disentangle these steps. Hence the output of our approach can be a different video of the same person or the same video. In both cases, the face has undergone one or more attribute edits and is rendered from a novel view. To ac-complish this challenging task, we introduce several novel components, each addressing one challenge of the problem at hand. Specifically, we first propose a simple yet effec-tive technique to create a personalized generator by invert-ing multiple frames at the same time. The simultaneous inversion of N frames exposes the generator to a variety of facial poses and expressions, which results in a larger ca-pacity that we can then utilize. Our generator can generalize to new unseen videos of the same identity where the per-son might be wearing a different shirt, a result not demon-strated in the literature so far. In addition, we propose to optimize the camera pose of the 3D-aware GAN during in-version to obtain an accurate estimate which angle the face was captured from. Finally, we introduce an optical flow-based compositing method to properly place the novel view of the edited face back into the original frame while ensur-ing that the end result is temporally and spatially consistent.
Our experimental work provides a wide range of qualita-tive and quantitative results to demonstrate that VIVE3D accomplishes semantic video editing with changing camera poses in a faithful way. In summary, our contributions are:
• A new 3D GAN inversion technique that jointly embeds multiple images while optimizing for their camera poses.
• A complete attribute editing framework and an optical flow-based compositing technique to replace the edited face in the original video.
• VIVE3D is the first 3D GAN-based video editing method and the first that can change the camera pose of the face. 2.