Abstract 1.

Introduction
Deep neural networks are proven to be vulnerable to backdoor attacks. Detecting the trigger samples during the inference stage, i.e., the test-time trigger sample detection, can prevent the backdoor from being triggered. However, existing detection methods often require the defenders to have high accessibility to victim models, extra clean data, or knowledge about the appearance of backdoor triggers, limiting their practicality.
In this paper, we propose the test-time corruption ro-bustness consistency evaluation (TeCo)1, a novel test-time trigger sample detection method that only needs the hard-label outputs of the victim models without any extra infor-mation. Our journey begins with the intriguing observa-tion that the backdoor-infected models have similar per-formance across different image corruptions for the clean images, but perform discrepantly for the trigger samples.
Based on this phenomenon, we design TeCo to evaluate test-time robustness consistency by calculating the deviation of severity that leads to predictions’ transition across different corruptions. Extensive experiments demonstrate that com-pared with state-of-the-art defenses, which even require ei-ther certain information about the trigger types or acces-sibility of clean data, TeCo outperforms them on different backdoor attacks, datasets, and model architectures, enjoy-ing a higher AUROC by 10% and 5 times of stability.
Backdoor attacks have been shown to be a threat to deep neural networks (DNNs) [14, 26, 32, 38]. A backdoor-infected DNN will perform normally on clean input data, but output the adversarially desirable target label when the input data are tampered with a special pattern (i.e., the back-door trigger), which may cause serious safety issues.
A critical dependency of a successful backdoor attack is that the attacker must provide the samples with back-door triggers (we call them trigger samples for short here-after) to the infected models on the inference stage, other-wise, the backdoor will not be triggered. Thus, one way to counter the backdoor attacks is to judge whether the test data have triggers on it, i.e., the test-time trigger sample de-tection (TTSD) defense2 [5, 12, 42]. This kind of defense can work corporately with other backdoor defenses such as model diagnosis defense [9, 15, 46] or trigger reverse engi-neering [40, 43], and also provide prior knowledge of the trigger samples in a comprehensive defense pipeline, which can help the down-steam defenses to statistically analyze the backdoor samples and mitigate the backdoor more ef-fectively.
On the other hand, the TTSD method, especially the black-box TTSD method can also serve as the last line of defense when someone adopts models with unknown cred-ibility and has no authority to get access to the training data or model parameters, this scenario exists widely in the pre-1https://github.com/CGCL-codes/TeCo 2Some paper also call it online backdoor defense [30, 39].
vailing machine-learning-as-a-service (MLaaS) [19, 35].
However, with the development of backdoor attacks, the TTSD defense is facing great challenges. One of the major problems is that different types of triggers have been presented. Unlike the early backdoor attacks whose triggers are universal [3, 14] for all the images and usu-ally conspicuous to human observers, recent works intro-duced sample-specific triggers [32] and even invisible trig-gers [8, 21, 26, 33, 49], making it harder to apply pattern statistics or identify out-liners in the image space. An-other main problem is the hardship of accomplishing the
TTSD defense without extra knowledge such as supplemen-tal data or model accessibility. On the other hand, exist-ing TTSD methods require certain knowledge and assump-tion. Such assumptions include that the trigger is a specific type [12, 42], the defenders have white-box accessibility to victim models, the predicted soft confidence score of each class [5, 12] or extra clean data for statistical analysis [48], limiting the practicality for real-world applications.
In this paper, we aim to design a TTSD defense free from these limitations. Specifically, we concentrate on a more practicable black-box hard-label backdoor setting [15] where defenders can only get the final decision from the black-box victim models. In addition, no extra data is acces-sible and no assumption on trigger appearance is allowed.
This setting assumes the defenders’ ability as weak as pos-sible and makes TTSD hard to achieve. To the best of our knowledge, we are the first to focus on the effectiveness of
TTSD in this strict setting, and we believe it is desirable to develop TTSD methods working on such a scenario be-cause it is very relevant to the wide deployment of cloud AI service [4, 11] and embedded AI devices [1].
Since the setting we mentioned above has restricted the accessibility of victim models and the use of extra data, we cannot analyze the information in feature space [30, 39] or train a trigger sample detector [10, 48] like existing works.
Fortunately, we find that the backdoor-infected models will present clearly different corruption robustness for trigger samples influenced by different image corruptions, but have relatively similar robustness throughout different image cor-ruptions for clean samples, leaving the clue for trigger sam-ple detection. We call these findings the anomalous cor-ruption robustness consistency of backdoor-infected mod-els and describe them at length in Sec. 3. It is not the first time that image corruptions are discussed in backdoor at-tacks and defenses [27, 28, 34]. However, previous works fail to explore the correlations between robustness against different corruptions, as discussed in Sec. 3.3.
Based on our findings above, we propose test-time corruption robustness consistency evaluation (TeCo), a novel test-time trigger sample detection method. At the in-ference stage of backdoor-infected models, TeCo modifies the input images by commonly used image corruptions [18]
Method
SentiNet [5]
SCan [39]
Beatrix [30]
NEO3 [42]
STRIP [12]
FreqDetector [48]
TeCo (Ours)
Black-box Access
No Need of
Trigger Aussmptions
Logits-based Decision-based Clean Data Universal
Sample-specific
Invisible (cid:35) (cid:35) (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:32) (cid:32) (cid:35) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:32) (cid:32) (cid:32) (cid:32) (cid:35) (cid:32) (cid:32) (cid:32) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:32) (cid:32) (cid:35) (cid:35) (cid:32) (cid:35) (cid:35) (cid:32) (cid:32)
Table 1. The model’s accessibility, the use of clean data, and the assumptions on backdoor triggers required by various TTSD meth-ods. We detail on some most related defenses in Sec. 2. ”
” rep-resents the TTSD method supports this condition. (cid:32) with growing severity and estimates the robustness against different types of corruptions from the hard-label outputs of the models. Then, a deviation measurement method is ap-plied to calculate how spread out the results of robustness are. And TeCo makes the final judgment of whether the in-put images are with triggers based on this metric. Extensive experiments show that compared with the existing advanced
TTSD method, TeCo improves AUROC about 10%, has a higher F1-score of 14%, and achieves 5 times of stability against different types of trigger.
Finally, we take a deep investigation into our observa-tions by constructing adaptive attacks against TeCo. From the results of feature space visualization and quantification of adaptive attacks, we speculate that the anomalous behav-ior of corruption robustness consistency derives from the widely-used dual-target training in backdoor attacks and it is hard to be avoided by existing trigger types. We hope these findings can shed light on a new perspective of back-door attacks and defenses for the community. In summary, we make the following contributions:
• We propose TeCo, a novel test-time trigger sample de-tection method that only requires the hard-label out-puts of the victim models and without extra data or assumptions about trigger types.
• We discover the fact of anomalous corruption robust-ness consistency, i.e., the backdoor-infected models have similar performance across different image cor-ruptions for clean images, but not for the trigger sam-ples.
• We evaluate TeCo on five datasets, four model archi-tectures (including CNNs and ViTs), and seven back-door attacks with diverse trigger types. All experimen-tal results support that TeCo outperforms state-of-the-art methods.
• We further analyze our observations by constructing adaptive attacks against TeCo. Experiments show that the widely-used dual-target training in backdoor at-tacks leads to anomalous corruption robustness consis-tency and it is hard to be avoided by existing backdoor triggers. 3NEO assumes the backdoor trigger is localized [14] thus will be in-valid on distributed or global triggers [3, 8, 32, 48], including universal, sample-sepcific, and invisible ones.
2.