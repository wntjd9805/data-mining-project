Abstract
In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant perfor-mance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL ap-proaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Con-solidation (BMC) to support more realistic CL under condi-tions where multiple agents are exposed to a range of tasks.
During a regularization phase, BMC trains multiple expert models in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a base model through a sta-bility loss, and constructs a buffer from a fraction of the task’s data. During the consolidation phase, we combine the learned knowledge on ‘batches’ of expert models us-ing a batched consolidation loss in memory data that ag-gregates all buffers. We thoroughly evaluate each compo-nent of our method in an ablation study and demonstrate the effectiveness on standardized benchmark datasets Split-CIFAR-100, Tiny-ImageNet, and the Stream dataset com-posed of 71 image classification tasks from diverse domains and difficulties. Our method outperforms the next best CL approach by 70% and is the only approach that can main-tain performance at the end of 71 tasks. 1.

Introduction
Continual Learning (CL) has allowed deep learning models to learn in a real world that is constantly evolv-ing, in which data distributions change, goals are updated, and critically, much of the information that any model will encounter is not immediately available [2]. Current ap-proaches in CL provide a trade-off to the stability-plasticity dilemma [3] where improving performance for a novel task leads to catastrophic forgetting.
Continual Learning benchmarks are composed of a lim-ited number of tasks and with tasks of non-distinct domains,
Figure 1. The loss contours by sequential training compared with batch task training [1] (shaded areas as low-error zones for each task). Intuition: Similar to mini-batch training batched task train-ing can reduce the local minima and improve the convexity of the loss landscape. such as Split-CIFAR100 [4] and Split-Tiny-ImageNet [5].
Previous approaches in Continual Learning suffer signifi-cant performance degradation when faced with a large num-ber of tasks, or tasks from diverse domains [1]. Addition-ally, the cost of many methods increases with the number of tasks [6,7] and becomes ultimately unacceptable for certain applications, while other methods [8–11] are tightly cou-pled to training on a single device and therefore cannot ben-efit from scaling in distributed settings. As such, current ap-proaches are impractical for many real-world applications, where multiple devices are trained on a diverse and disjoint set of tasks with the goal of maintaining a single model.
Motivated by the performance, memory cost, training time and flexibility issues of current approaches, we pro-pose Batch Model Consolidation (BMC), a Continual
Learning framework that supports distributed training on multiple streams of diverse tasks, but also improves per-formance when applied on a single long task stream. Our method trains and consolidates multiple workers that each become an expert in a task that is disjoint from all other tasks. In contrast, for Federated Learning the training set is composed of a single task of heterogeneous data [12].
Our method is composed of two phases. First, during the regularization phase, a set of expert models is trained in new tasks in parallel with their weights regularized to a
Figure 2. A single incremental step of BMC. On the right figure, the updating of a base model with Multi-Expert Training: after receiving the data of the new tasks Di, . . . , Di+k, a batch of experts θi, . . . , θi+k are trained separately on their corresponding tasks with stability loss applied from the base model. The newly trained experts then sample a subset of their training data and combine them with the memory to perform batched distillation on the base model. On the left figure, the regularization helps the batched distillation to update the model closer to the regularization boundary and towards the jointly low-error zone of old tasks and two new tasks. base model. Second, during the consolidation phase the expert models are combined into the base model in a way that better retains the performance on the current tasks of all experts and all previously learned tasks. The main ad-vantage of our method is that it provides a better approxi-mation to the multi-task gradient of all tasks from all expert models, Fig. 2. Lastly, BMC better retains performance for significantly more tasks than current baselines, while reduc-ing the total time of training when compared to training on the same task-stream in a sequential manner. The primary contributions of our paper are as follows. 1. We propose Batch Model Consolidation (BMC) to support CL for training multiple expert models on a single task stream composed of tasks from diverse do-mains. 2. We extend BMC for a distributed learning framework where we train multiple expert models on disjoint task streams. 3. We propose a stability loss to reduce forgetting that is applied between expert models and a base model.
Lastly, a batched distillation loss combines multiple expert models to update a single base model in a single incremental step. 4. We verify our approach on popular benchmarks and show that BMC is robust against large domain-shifts and for a large number of tasks. 2.