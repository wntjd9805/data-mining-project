Abstract
Model selection is essential for reducing the search cost of the best pre-trained model over a large-scale model zoo for a downstream task. After analyzing recent hand-designed model selection criteria with 400+ ImageNet pre-trained models and 40 downstream tasks, we ﬁnd that they can fail due to invalid assumptions and intrinsic limitations. The prior knowledge on model capacity and dataset also can not be easily integrated into the existing criteria. To address these issues, we propose to convert model selection as a recommendation problem and to learn from the past training history. Speciﬁcally, we characterize the meta information of datasets and models as features, and use their transfer learning performance as the guided score. With thousands of historical training jobs, a recommendation system can be learned to predict the model selection score given the features of the dataset and the model as input. Our approach enables integrating existing model selection scores as ad-ditional features and scales with more historical data. We evaluate the prediction accuracy with 22 pre-trained mod-els over 40 downstream tasks. With extensive evaluations, we show that the learned approach can outperform prior hand-designed model selection methods signiﬁcantly when relevant training history is available. 1.

Introduction
Much of the success of deep learning can be ascribed to its
ﬂexibility: One can train a neural network on a task, and then use it on a different one, typically after ﬁne-tuning. There are currently two trends for scaling this practice: One is to pre-train a large number of specialized models (a “Model
Zoo” [10]) and then select one to ﬁne-tune once the down-stream task of interest becomes manifest, typically with a smaller ﬁne-tuning dataset. Another is to pre-train a single
“Foundation Model” which is then used to support any and all downstream tasks [47, 57].
Without additional speciﬁcations, the second case is a subset of the ﬁrst, for one can take the Model Zoo and
Model Selection (MS) mechanism and call it a single model.
For this reason, Foundation Models are characterized as homogeneous and task-agnostic, where homogeneity refers to a single neural network architecture, in contrast with the heterogeneous collection of models in a zoo. Even with this restriction, the model zoo is more general, for nothing prevents a Foundation Model to be part of a zoo. In addition, selecting a smaller dedicated model pretrained for a task can be much more efﬁcient than using a giant monolithic model
For these reasons, we focus on model selection over a large heterogeneous model zoo for ﬁne-tuning as the key solution for scaling inference to a wide variety of downstream tasks.
Brute-force model selection [1, 12] requires ﬁne-tuning each pre-trained model on the task of interest, and then rank-ing them using the test error on a held-out dataset as a model selection score. This is not feasible for large model zoos.
Current model-selection methods therefore aim to predict the model selection score without actually ﬁne-tuning.
However, current model selection methods do not take into explicit account even basic characteristics of the ﬁne-tuning dataset, such as the number of classes or the number of images, nor of the pre-trained model, such as the model family, the size of the input, the number of parameters and the dataset on which it is pre-trained. While coarse, these fea-tures can affect the best model to ﬁne-tune, since a mismatch between ﬁne-tuning dataset size and pre-trained model, or input dimensions, or number of classes, can inﬂuence the success of downstream performance.
Instead of proposing yet another model selection score, we propose re-framing model selection as a recommender system, and directly predict the selection score and corre-sponding ranking, from whatever existing model selection scores are readily available, in addition to whatever coarse features a user deems informative – which may be context dependent, as some users may wish to penalize large models, or models that require high-resolution input. Such features help guide the model selection using criteria beyond raw downstream validation error. For this reason, we refer to our recommendation approach as guided, in addition to trained.
We ﬁnd that incorporating model size, dataset size, cardi-nality of the hypothesis set and other simple features already improves the prediction of the expected model selection
score compared to current model selection methods. Coarse features, such as the index of the model class family (con-volutional, fully-connected, residual, attention-based, etc.) can help associate certain architectural inductive biases such as translation vs. permutation invariance, to the best-ﬁtting downstream tasks, for instance object detection vs. image inpainting or segmentation.
Our contribution can be summarized as:
• We conduct comprehensive analysis of existing model selection approaches with a large heterogeneous model zoo and conﬁrmed their limitations. We ﬁnd feature-based model selection becomes inaccurate when the target dataset is different from the source task and the effect of model initialization diminishes as the number of images grows. The useful meta information and prior knowledge in the training history are often neglected and cannot be easily integrated into existing model selection criteria.
• We convert the model selection problem as model rec-ommendation by learning from past training history.
The meta information of both dataset and model are em-bedded as features and a recommender system can be learned to predict the performance. The existing model selection can be used as additional features and makes the framework comply with existing approaches. We show signiﬁcant performance improvement over tradi-tional model selection methods when historical training data is available and relevant.
In the next section we formalize the MS problem, and discuss the issues with existing approaches. In section 3 we describe our approach to casting it as a recommendation system and evaluate it in the following section. 2.