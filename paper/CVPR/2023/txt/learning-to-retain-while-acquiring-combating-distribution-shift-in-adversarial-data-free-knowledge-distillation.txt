Abstract
Data-free Knowledge Distillation (DFKD) has gained popularity recently, with the fundamental idea of carrying out knowledge transfer from a Teacher neural network to a Student neural network in the absence of training data.
However, in the Adversarial DFKD framework, the student network’s accuracy, suffers due to the non-stationary distri-bution of the pseudo-samples under multiple generator up-dates. To this end, at every generator update, we aim to main-tain the student’s performance on previously encountered examples while acquiring knowledge from samples of the cur-rent distribution. Thus, we propose a meta-learning inspired framework by treating the task of Knowledge-Acquisition (learning from newly generated samples) and Knowledge-Retention (retaining knowledge on previously met samples) as meta-train and meta-test, respectively. Hence, we dub our method as Learning to Retain while Acquiring. Moreover, we identify an implicit aligning factor between the Knowledge-Retention and Knowledge-Acquisition tasks indicating that the proposed student update strategy enforces a common gradient direction for both tasks, alleviating interference be-tween the two objectives. Finally, we support our hypothesis by exhibiting extensive evaluation and comparison of our method with prior arts on multiple datasets. 1.

Introduction
The primary goal of Data-Free Knowledge Distillation (DFKD) is to acquire a trustworthy alternative dataset for knowledge distillation. The quality of knowledge transfer de-pends on the caliber of these alternative samples. Noise opti-mization [11,22,30] and generative reconstruction [4,10,20] are the two primary ways to replace the original training data used in the distillation process with synthetic or pseudo sam-ples. Adversarial DFKD methods [10, 19, 20] investigate an adversarial exploration framework to seek pseudo-samples.
In such a paradigm, the Generator (G) is used to create pseudo-samples as surrogates to perform knowledge distilla-tion/transfer, and the Teacher-Student (T -S) setup acts as a joint discriminator to penalize and update generator parame-ters (θG) (Figure 1). In the adversarial framework, the gener-ator explores the input space to find suitable pseudo-samples as the distillation progresses. Consequently, the distribution
Figure 1. The top-section represents the learning evolution of a generic Adversarial Data-Free Knowledge Distillation framework; the color-intensity variation signifies the change in the distribution of the pseudo-samples, the student network, and the generator net-work over the learning epochs. Under the variation in the distribu-tion of the pseudo-samples, the bottom-section shows the learning curves for cases when the student accuracy degrades (shown in
Red), which is undesirable, and when the student accuracy is main-tained, if not improved, as proposed (shown in Green). of the generated samples consistently keeps changing dur-ing the process due to the generator updates [25]. From the student network’s perspective, at each iteration, the pseudo samples seem to be generated from different generator pa-rameters (θG). Hence, the convergence of the student network gets hampered due to successive distributional alterations over time [29], as depicted by the red curve in Figure 1.
This observation hints that updating the student network, solely using the samples generated from the current gener-ator parameters is not adequate to generalize the student.
Moreover, the student forgets the knowledge acquired previ-ously and decelerates the knowledge distillation. Therefore, the generator, apart from exploring the input space, seeks to compensate for the loss of knowledge in future iterations.
Additionally, in a practical setting, during the distillation process, high variation in the student network’s classification accuracy is undesirable, especially when the validation data is not available, since that prevents the user from tracking
(a) (b)
Figure 2. Student update strategies: (a) Typical student update by optimizing the Knowledge-Acquisition loss (LAcq) with the batch pseudo samples (ˆx), produced by the generator (G) [5, 10, 20]. (b) Student update with simultaneous optimization of the Knowledge-Acquisition loss (LAcq) and Knowledge-Retention loss (LRet) on the batch of pseduo samples (ˆx) and memory samples (ˆxm) obtained from the generator (G) and the memory (M), respectively [2, 3]. (c) The proposed student update strategy, which treats LAcq as meta-train and LRet as meta-test, and implicitly imposes the alignment between them. (c) the student’s accuracy over time, and selecting the distilled model parameters with the highest accuracy.
To circumvent the above-discussed problems, existing methods maintain a memory buffer to rehearse the exam-ples from previously encountered distributions while learn-ing with current examples. Binci et al. [3] introduce Re-play based methods to explicitly retrain/replay on a limited subset of previously encountered samples while training on the current examples. Then, carrying forward, they use
Generative-Replay [27] to transfer the learned examples to an auxiliary generative model (VAE), and sample from the it (VAE decoder) in subsequent iterations [2]. Nonetheless, the performance of these methods is upper bounded by joint training on previous and current examples [7]. Although, recent works have focused on modeling the memory, we seek to work towards effectively utilizing the samples from memory.
In this paper, we aim to update the student network pa-rameters (θS ) such that its performance does not degrade on the samples previously produced by the generator net-work (G), aspiring towards Learning to Retain while Ac-quiring. Thus, we propose a meta-learning inspired strat-egy to achieve this goal. We treat the task of Knowledge-Acquisition (learning from newly generated samples) and
Knowledge-Retention (learning from previously encountered samples from memory) as meta-train and meta-test, respec-tively. Hence, in the proposed approach, the student network acquires new information while maintaining performance on previously encountered samples. By doing so, the proposed strategy (Figure 2c) implicitly aligns Knowledge-Acquisition and Knowledge-Retention, as opposed to simply combining them [2, 3] without any coordination or alignment (Figure 2b), which leaves them to potentially interfere with one an-other.
Additionally, analyzing the proposed meta-objective, we discover that (in Section 3.4) the latent alignment factor as the dot product between the gradients of the Knowledge-Acquisition and Knowledge-Retention objectives, suggesting that the meta-objective enforces a common gradient direc-tion for both tasks, encouraging the alignment between the task-specific gradients. Thus, the proposed method simulta-neously minimizes the loss and matches the gradients cor-responding to the individual tasks (Knowledge-Acquisition and Knowledge-Retention), enforcing the optimization paths to be same for both tasks.
Moreover, the proposed student update strategy is scal-able to different deep architectures as the gradient align-ment is implicit, and memory-agnostic, making no assump-tions about the replay scheme employed. Nonetheless, recent works on gradient alignment, have shown great empirical ad-vantages in Zero-Shot Learning [24], Distributed/Federated
Learning [6] and Domain Generalization [26]. Our method extends the advantages of gradient alignment to memory-based Adversarial DFKD, thus strengthening the empirical findings in these works.
Finally, to demonstrate the advantages of the proposed student update strategy, we evaluate and compare against cur-rent non-memory [4, 5, 10] and memory-based [2, 3] Adver-sarial DFKD methods, and observe substantial improvement in the student learning evolution.
In summary, the key points of this work are as follows:
• We propose a novel meta-learning inspired student up-date strategy in the Adversarial DFKD setting, that aims to maintain the student’s performance on previously encountered examples (Knowledge-Retention) while acquiring knowledge from samples of the current distri-bution (Knowledge-Acquisition).
• We theoretically identify (in Section 3.4) that the pro-posed student update strategy enforces an implicit gra-dient alignment between the Knowledge-Acquisition and Knowledge-Retention tasks.
• A major practical advantage of the proposed method is its applicability in the absence of a validation set, discussed in Section 4.2.
• Finally, we evaluate our method and compare against various Adversarial DFKD methods, on multiple stu-dent architectures and replay schemes (Memory Buffer and Generative Replay). 2.