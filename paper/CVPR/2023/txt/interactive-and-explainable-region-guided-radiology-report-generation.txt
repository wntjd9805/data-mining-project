Abstract
The automatic generation of radiology reports has the potential to assist radiologists in the time-consuming task of report writing. Existing methods generate the full re-port from image-level features, failing to explicitly focus on anatomical regions in the image. We propose a sim-ple yet effective region-guided report generation model that detects anatomical regions and then describes indi-vidual, salient regions to form the final report. While previous methods generate reports without the possibility of human intervention and with limited explainability, our method opens up novel clinical use cases through addi-tional interactive capabilities and introduces a high de-gree of transparency and explainability. Comprehensive ex-periments demonstrate our method’s effectiveness in report generation, outperforming previous state-of-the-art models, and highlight its interactive capabilities. The code and checkpoints are available at https://github.com/ ttanida/rgrg . 1.

Introduction
Chest radiography (chest X-ray) is the most common type of medical image examination in the world and is criti-cal for identifying common thoracic diseases such as pneu-monia and lung cancer [19, 37]. Given a chest X-ray, radi-ologists examine each depicted anatomical region and de-scribe findings of both normal and abnormal salient regions in a textual report [12]. Given the large volume of chest X-rays to be examined in daily clinical practice, this often be-comes a time-consuming and difficult task, which is further exacerbated by a shortage of trained radiologists in many healthcare systems [3, 40, 41]. As a result, automatic radi-ology report generation has emerged as an active research area with the potential to alleviate radiologists’ workload.
Generating radiology reports is a difficult task since re-ports consist of multiple sentences, each describing a spe-cific medical observation of a specific anatomical region.
∗Equal contribution
Figure 1. Our approach at a glance. Unique anatomical regions of the chest are detected, the most salient regions are selected for the report and individual sentences are generated for each region.
Consequently, each sentence in the generated report is explicitly grounded on an anatomical region.
As such, current radiology report generation methods tend to generate reports that are factually incomplete (i.e., miss-ing key observations in the image) and inconsistent (i.e., containing factually wrong information) [28]. This is fur-ther exacerbated by current methods utilizing image-level visual features to generate reports, failing to explicitly fo-cus on salient anatomical regions in the image. Another is-sue regarding existing methods is the lack of explainability.
A highly accurate yet opaque report generation system may not achieve adoption in the safety-critical medical domain if the rationale behind a generated report is not transparent and explainable [11, 14, 27]. Lastly, current methods lack inter-activity and adaptability to radiologists’ preferences. E.g., a radiologist may want a model to focus exclusively on spe-cific anatomical regions within an image.
Inspired by radiologists’ working patterns, we propose a simple yet effective Region-Guided Radiology Report
Generation (RGRG) method to address the challenges high-lighted before.
Instead of relying on image-level visual features, our work is the first in using object detection to directly extract localized visual features of anatomical regions, which are used to generate individual, anatomy-specific sentences describing any pathologies to form the final report. Conceptually, we divide and conquer the dif-ficult task of generating a complete and consistent report from the whole image into a series of simple tasks of gen-erating short, consistent sentences for a range of isolated anatomical regions, thereby achieving both completeness and consistency for the final, composed report.
While existing models [7, 17, 26] can produce heatmaps to illustrate which parts of an image were attended dur-ing report generation, our model visually grounds each sen-tence in the generated report on a predicted bounding box around an anatomical region (see Fig. 1), thereby introduc-ing a high degree of explainability to the report generation process. The visual grounding allows radiologists to eas-ily verify the correctness of each generated sentence, which may increase trust in the model’s predictions [11, 27, 39].
Moreover, our approach enables interactive use, allowing radiologists to select anatomical structures or draw bound-ing boxes around regions of interest for targeted description generation, enhancing flexibility in the clinical workflow.
Our contributions are as follows:
• We introduce a simple yet effective Region-Guided
Radiology Report Generation (RGRG) method that detects anatomical regions and generates individual descriptions for each. Empirical evidence demon-strates that our model produces relevant sentences per-taining to the anatomy. To the best of our knowledge, it is the first report generation model to visually ground sentences to anatomical structures. region independently.
• We condition a pre-trained language model on each anatomical
This enables anatomy-based sentence generation, where radiolo-gists interactively select individual, detected anatomi-cal regions for which descriptions are generated.
• Additionally, our approach enables radiologists to manually define regions of interest using bounding boxes, generating corresponding descriptions. We assess the impact of these manually drawn boxes, demonstrating the robustness of the selection-based sentence generation task.
• For full radiology report generation, a module is in-troduced to select salient anatomical regions, whose generated descriptions are concatenated to form factu-ally complete and consistent reports. We empirically demonstrate this on the MIMIC-CXR dataset [18, 19], where we outperform competitive baselines in both language generation and clinically relevant metrics. 2.