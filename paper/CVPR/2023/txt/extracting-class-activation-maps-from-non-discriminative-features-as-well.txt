Abstract
Extracting class activation maps (CAM) from a classifi-cation model often results in poor coverage on foreground objects, i.e., only the discriminative region (e.g., the “head” of “sheep”) is recognized and the rest (e.g., the “leg” of
“sheep”) mistakenly as background. The crux behind is that the weight of the classifier (used to compute CAM) cap-tures only the discriminative features of objects. We tackle this by introducing a new computation method for CAM that explicitly captures non-discriminative features as well, thereby expanding CAM to cover whole objects. Specifi-cally, we omit the last pooling layer of the classification model, and perform clustering on all local features of an object class, where “local” means “at a spatial pixel posi-tion”. We call the resultant K cluster centers local proto-types — represent local semantics like the “head”, “leg”, and “body” of “sheep”. Given a new image of the class, we compare its unpooled features to every prototype, derive K similarity matrices, and then aggregate them into a heatmap (i.e., our CAM). Our CAM thus captures all local features of the class without discrimination. We evaluate it in the chal-lenging tasks of weakly-supervised semantic segmentation (WSSS), and plug it in multiple state-of-the-art WSSS meth-ods, such as MCTformer [45] and AMN [26], by simply re-placing their original CAM with ours. Our extensive exper-iments on standard WSSS benchmarks (PASCAL VOC and
MS COCO) show the superiority of our method: consistent improvements with little computational overhead. Our code is provided at https://github.com/zhaozhengChen/LPCAM. 1.

Introduction
Extracting CAM [50] from classification models is the essential step for training semantic segmentation models when only image-level labels are available, in the
WSSS tasks [8, 22, 24, 26, 46]. More specifically, the gen-eral pipeline of WSSS consists of three steps: 1) training a multi-label classification model with the image-level labels; 2) extracting CAM of each class to generate a 0-1 mask i.e., (usually called seed mask), often with a further step of re-finement to generate pseudo mask [1, 20]; and 3) taking all-class pseudo masks as pseudo labels to train a semantic seg-mentation model in a fully-supervised fashion [5, 6, 41]. It is clear that the CAM in the first step determines the perfor-mance of the final semantic segmentation model. However, the conventional CAM and its variants often suffer from the poor coverage of foreground objects in the image, i.e., a large amount of object pixels are mistakenly recognized as background, as demonstrated in Figure 1(a) where only few pixels are activated in warm colors.
We point out that this locality is due to the fact that
CAM is extracted from a discriminative model. The train-ing of such model naturally discards the non-discriminative regions which confuse the model between similar as well as highly co-occurring object classes. This is a general prob-lem of discriminative models, and is particularly obvious when the number of training classes is small [37,46,48]. To visualize the evidence, we use the classifier weights of con-fusing classes, e.g., “car”, “train” and “person”, to compute
CAMs for the “bus” image, and show them in Figure 1(b).
We find from (a) and (b) that the heating regions in ground truth class and confusing classes are complementary. E.g., the upper and frontal regions (on the “bus” image) respec-tively heated in the CAMs of “car” and “train” (confusing classes) are missing in the CAM of “bus” (ground truth), which means the classifier de-activates those regions for
“bus” as it is likely to recognize them as “car” or “train”.
Technically, for each class, we use two factors to com-pute CAM: 1) the feature map block after the last conv layer, and 2) the weight of the classifier for that class. As afore-mentioned, the second factor is often biased to discrimina-tive features. Our intuition is to replace it with a non-biased one. The question becomes how to derive a non-biased clas-sifier from a biased classification model, where non-biased means representing all local semantics of the class. We find the biased classifier is due to incomplete features, i.e., only discriminative local features fed into the classifier, and the non-discriminative ones are kicked out by the global aver-age pooling (GAP) after the last conv layer. To let the model pay attention to non-discriminative features as well, we pro-Figure 1. The CAM of image x is computed by f (x) · wc, where f (x) is the feature map block (before the last pooling layer of the multi-label classification model) and wc denotes the classifier weights of class c. (a) Input images. (b) CAMs generated from the classifier weights of ground truth class. (c) CAMs generated from the classifier weights of confusing classes. (d) LPCAM (our method). pose to omit GAP, derive a prototype-based classifier by clustering all local features (collected across all spatial lo-cations on the feature map blocks of all training samples in the class) into K local prototypes each representing a local semantic of the class. In Section 3, we give a detailed justi-fication that this prototype-based classifier is able to capture both discriminative and non-discriminative features.
Then, the question is how to use local prototypes on the feature map block (i.e., the 1st factor) to generate CAM.
We propose to apply them one-by-one on the feature map block to generate K similarity maps (e.g., by using cosine distance), each aiming to capture the local regions that con-tain similar semantics to one of the prototypes. We high-light that this “one-by-one” is important to preserve non-discriminative regions as the normalization on each similar-ity map is independent. We provide a detailed justification from the perspective of normalization in Section 3.2. Then, we average across all normalized similarity maps to get a single map—we call Local Prototype CAM (LPCAM). In addition, we extend LPCAM by using the local prototypes of contexts as well. We subtract the context similarity maps (computed between the feature map block and the clustered context prototypes) from LPCAM. The idea is to remove the false positive pixels (e.g., the “rail” of “train” [25]). There-fore, our LPCAM not only captures the missing local fea-tures of the object but also mitigates the spurious features caused by confusing contexts.
LPCAM is a new operation to compute class activa-tion maps based on clustered local prototypes. In princi-ple, it can be taken as a generic substitute of the conven-tional CAM in CAM-based WSSS methods. To evaluate
LPCAM on different WSSS methods (as they use different backbones, pre-training strategies or extra data), we conduct extensive experiments by plugging it in multiple methods: the popular refinement method IRN [1], the top-performing
AMN [26], the saliency-map-based EDAM [39], and the transformer-arch-based MCTformer [45], on two popu-lar benchmarks of semantic segmentation, PASCAL VOC 2012 [11] and MS COCO 2014 [30].
Our Contributions in this paper are thus two-fold. 1) A novel method LPCAM that leverages non-discriminative lo-cal features and context features (in addition to discrimina-tive ones) to generate class activation maps with better cov-erage on the complete object. 2) Extensive evaluations of
LPCAM by plugging it in multiple WSSS methods, on two popular WSSS benchmarks. 2.