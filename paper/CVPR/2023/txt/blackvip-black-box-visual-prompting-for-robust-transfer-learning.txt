Abstract
With the surge of large-scale pre-trained models (PTMs), fine-tuning these models to numerous downstream tasks be-comes a crucial problem. Consequently, parameter effi-cient transfer learning (PETL) of large models has grasped huge attention. While recent PETL methods showcase im-pressive performance, they rely on optimistic assumptions: 1) the entire parameter set of a PTM is available, and 2) a sufficiently large memory capacity for the fine-tuning is equipped. However, in most real-world applications, PTMs are served as a black-box API or proprietary software with-out explicit parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs.
In this work, we propose black-box visual prompting (Black-VIP), which efficiently adapts the PTMs without knowl-edge about model architectures and parameters. Black-VIP has two components; 1) Coordinator and 2) simul-taneous perturbation stochastic approximation with gradi-ent correction (SPSA-GC). The Coordinator designs input-dependent image-shaped visual prompts, which improves few-shot adaptation and robustness on distribution/location shift. SPSA-GC efficiently estimates the gradient of a tar-get model to update Coordinator. Extensive experiments on 16 datasets demonstrate that BlackVIP enables robust adaptation to diverse domains without accessing PTMs’ parameters, with minimal memory requirements. Code: https://github.com/changdaeoh/BlackVIP 1.

Introduction
Based on their excellent transferability, large-scale pre-trained models (PTMs) [7, 17, 54] have shown remarkable success on tasks from diverse domains and absorbed in-creasing attention in machine learning communities. By witnessing PTMs’ success, Parameter-Efficient Transfer
Learning (PETL) methods that efficiently utilize the PTMs
†Work done at University of Seoul
‡Corresponding author; Work partly done at University of Seoul
Figure 1. While FT updates the entire model, VP has a small num-ber of parameters in the input pixel space. However, VP still re-quires a large memory capacity to optimize the parameters through backpropagation. Moreover, FT and VP are only feasible if the
PTM’s parameters are accessible. Meanwhile, BlackVIP does not assume the parameter-accessibility by adopting a black-box opti-mization (SPSA-GC) algorithm rather than relying on backpropa-gation. Besides, BlackVIP reparameterizes the visual prompt with a neural network and optimizes tiny parameters with SPSA-GC.
Based on the above properties, BlackVIP can be widely adopted in realistic and resource-limited transfer learning scenarios. are recently emerging. While the standard fine-tuning (FT) and its advanced variants [38, 73] update the entire or large portion of a PTM [16], PETL methods aim to achieve com-parable performance to FT by optimizing a small number of learnable parameters.
Among them, prompt-based approaches [3, 5, 33, 40, 41] have been widely investigated from diverse research areas.
For vision PTMs, Visual Prompt Tuning [33] injects a few additional learnable prompt tokens inside of ViT’s [17] lay-ers or embedding layer and only optimizes them. Bahng et al. [3] investigate visual prompting (VP), which adopts the learnable parameters on input pixel space as a visual prompt, while no additional modules are inserted into the pre-trained visual model. Besides, prompt learning meth-ods for VLM are also actively studied [35, 78, 81, 83].
While existing PETL methods show impressive perfor-mance with few learnable parameters, they rely on two
the previous PETL as-First, optimistic assumptions. sumes that the full parameters of the PTM are accessible.
However, many real-world AI applications are served as
API and proprietary software, and they do not reveal the implementation-level information or full parameters due to commercial issues, e.g., violating model ownership. As a result, exploiting high-performing PTMs to specific down-stream tasks not only in the white-box setting but also black-box setting (limited accessibility to the model’s de-tail) is a crucial but unexplored problem. Second, exist-ing methods require a large memory capacity. While PETL approaches have few learnable parameters, they require a large amount of memory for backpropagating the gradient throughout the large-scale PTM parameters to learnable pa-rameters. Therefore, users who want to adopt a large-scale
PTM should satisfy large memory requirements despite the small learnable parameters. Besides, if the users entrust
PTM fine-tuning to the model owner with their specific data, data-privacy concerns will inevitably arise [74].
To alleviate the above unrealistic assumptions, we are pioneering black-box visual prompting (BlackVIP) approach, which enables the parameter-efficient transfer learning of pre-trained black-box vision models from the low-resource user perspective (illustrated in Figure 1).
BlackVIP works based on the following two core compo-nents: 1) pixel space input-dependent visual prompting and 2) a stable zeroth-order optimization algorithm.
Firstly, we augment an input image by attaching an vi-sual prompt per pixel. It is noted that input space prompt-ing does not require the accessibility on parts of architec-ture [37, 78] or the first embedding layer [35, 81, 83] of
PTM. While the previous works only introduce a pixel-level prompt to a small fraction of the fixed area, such as out-side of the image [3], BlackVIP designs the prompt with the same shape as the original given image to cover the entire image view. Therefore, our prompt has a higher capability and can flexibly change the semantics of the original image.
In addition, we reparameterize the prompt with a neural net-work. Specifically, we propose the Coordinator, an asym-metric autoencoder-style network that receives the original image and produces a corresponding visual prompt for each individual image. As a result, Coordinator automatically designs each prompt conditioned on the input rather than the shared manual design of a previous work [3]. By opti-mizing the reparameterized model instead of the prompt it-self, we greatly reduce the number of parameters (from 69K of VP [3] to 9K) so that suitable for black-box optimization.
Next, unlike other PETL approaches, BlackVIP adopts a zeroth-order optimization (ZOO) that estimates the zeroth-order gradient for the coordinator update to relax the as-sumption that requires access to the huge PTM parame-ters to optimize the prompt via backpropagation. There-fore, BlackVIP significantly reduces the required mem-ory for fine-tuning. Besides, we present a new ZOO al-gorithm, Simultaneous Perturbation Stochastic Approx-imation with Gradient Correction (SPSA-GC) based on (SPSA) [58]. SPSA-GC first estimates the gradient of the target black-box model based on the output difference of perturbed parameters and then corrects the initial estimates in a momentum-based look-ahead manner. By integrating the Coordinator and SPSA-GC, BlackVIP achieves signifi-cant performance improvement over baselines.
Our main contributions are summarized as follows:
• To our best knowledge, this is the first paper that ex-plores the input-dependent visual prompting on black-box settings. For this, we devise Coordinator, which reparameterizes the prompt as an autoencoder to han-dle the input-dependent prompt with tiny parameters.
• We propose a new ZOO algorithm, SPSA-GC, that gives look-ahead corrections to the SPSA’s estimated gradient resulting in boosted performance.
• Based on Coordinator and SPSA-GC, BlackVIP adapts the PTM to downstream tasks without parame-ter access and large memory capacity. We extensively validate BlackVIP on 16 datasets and demonstrate its effectiveness regarding few-shot adaptability and ro-bustness on distribution/object-location shift. 2.