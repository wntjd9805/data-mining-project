Abstract
Recently, a number of iterative learning methods have been introduced to improve generalization. These typically rely on training for longer periods of time in exchange for improved generalization. LLF (later-layer-forgetting) is a state-of-the-art method in this category. It strengthens learn-ing in early layers by periodically re-initializing the last few layers of the network. Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers.
Essentially, later layers go through the normal gradient de-scent process, while the early layers go through short stints of gradient ascent followed by gradient descent. Extensive experiments on the popular Tiny-ImageNet dataset bench-mark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin.
We further show that, compared to normal training, LLF features, although improving on the target task, degrade the transfer learning performance across all datasets we ex-plored. In comparison, our method outperforms LLF across the same target datasets by a large margin. We also show that the prediction depth of our method is significantly lower than that of LLF and normal training, indicating on average better prediction performance.1 1.

Introduction
Overfitting is a crucial challenge in supervised deep learning, which prevents a neural network from performing well on unseen data. This problem is particularly perva-sive in smaller dataset regimes. Classical machine learn-ing approaches to address this problem typically rely on explicit regularization added as part of the optimization ob-This research was partially funded by NSERC Discovery
Grant RGPIN2021-04104 ac-knowledge the resources provided by Compute Canada and Cal-a.m.sarfi@gmail.com, cul Quebec. eugene.belilovsky@concordia.ca, sudhir.mudur@concordia.ca and RGPIN-2019-05729.
Correspondence to:
We 1The code to reproduce our results is publicly available at: https://github.com/amiiir-sarfi/SEAL jective [37]. A number of implicit approaches are often applied in training deep networks. These implicit methods are usually easier to design than explicit terms added to the objective function. For example, early stopping [42] and dropout [32] are classical examples of implicit regulariza-tion methods. It can be shown that these techniques can be shown to be linked directly to explicit regularization terms.
Consider the case of linear regression, early stopping can be directly seen as Tikhonov regularization [42]. Indeed the use of modified optimization strategies to improve generalization is becoming pervasive in deep learning [7].
Recently, iterative training methods have been introduced to improve generalization in deep networks. These allow neural networks to be trained for many epochs while pro-gressively improving generalization [9, 10, 35, 41, 46]. These works typically rely on a notion of a generation, in which a model is optimized towards a local minimum in a single generation. Subsequently, in a new generation, the objective function is modified, or the network is perturbed towards a high loss, requiring a new generation of optimization to-wards a minimum. The earliest work on this topic focused on self-distillation [10, 41], where, in each successive gen-eration, a student network with the same architecture was initialized and trained to mimic the softmax output distribu-tion of the previous model. This procedure was theoretically studied by [22] and provided the formal link to its explicit regularization effects.
A number of other techniques were subsequently pro-posed. These have been characterized by Zhou et al. [46] as instances of a forget-and-relearn scheme, where they define forgetting as any process that worsens the training accuracy of a model. In forget-and-relearn, the forgetting stage hap-pens at the beginning of each generation where some part of the weights are perturbed (e.g., by removing [8, 9] or by re-initialization [35,46]), and the network is normally trained for the rest of the generation. They further introduce a simple forgetting method called later-layer-forgetting (LLF), where they re-initialize the last few layers of the network. The intu-ition behind LLF comes from the concept of prediction depth introduced by Baldock et al. [1]. The prediction depth of a sample refers to the earliest layer, after which the layer-wise k-nearest neighbor (k-NN) probe of all layers is the same as
Figure 1. Our iterative training method (SEAL) compared to LLF. The model is shown with the input on the right and output on the left.
E denotes the epochs in a generation. LLF re-initializes the top layers right before a new generation begin. In our approach, we do not re-initialize but perform gradient ascent in the first k epochs of a generation, only on the early layers of the network. the modelâ€™s prediction. Zhou et al. [46] empirically show that LLF improves the prediction depth.
We hypothesize that by constantly re-initializing the later layers, LLF pushes high-level information to the early layers, which explains its stronger generalization as compared to normal training. However, when it comes to transfer learning,
Zhao et al. [44] have stated that it is mainly the low- and mid-level representations that are transferred. Following this insight, we analyzed features learned by LLF in a transfer learning setting. We observed that LLF performs weaker than normal training, which affirms our hypothesis. This is discussed further in the Experimental Results section 4.2.
Simulated annealing is a method for solving uncon-strained and bound-constrained optimization problems. It models the physical process of heating a material and then slowly lowering the temperature to decrease defects, thus minimizing the system energy. During training, gradient ascent mimics increase in defects (temperature increase), while gradient descent emulates the cooling process to reach a better minimum [17]. Cai [2] adapted the classical sim-ulated annealing for gradient descent (SA-GD) to improve the optimization by escaping local minimums and saddle points. In SA-GD, this is done by probabilistically doing gradient ascent during training. Their method, however, is not used for iterative learning and unlike our work is not adapted to take advantage of the specific architecture biases of deep networks. Based on the hypothesis that LLF pri-marily benefits from pushing high-level information into the early layers (and thereby the prediction depth), in this work, we adapt the SA idea as an alternative to LLF, and we do not reset the later layers. Instead, we perform an intermittent gradient ascent procedure just on the early layers, coaxing them to find better solutions through multiple generations of training.
Our main contribution is a new iterative training method that performs gradient ascent on the initial layers of the net-work, for a few epochs, at the beginning of every generation to induce forgetting. By performing the ascent on only a sub-set of layers, we ensure that the model retains information from the previous generations, rendering it suitable for long training. The intuition behind gradient ascent on the early layers is that it prohibits them from being overly specific to the task, avoiding the encoding of high-level semantics, unlike what happens in LLF. Furthermore, even though we perform simulated annealing on the early layers and LLF performs re-initialization on the later layers, the goal behind both approaches is the same; both methods try to enhance the early layers.
We have carried out extensive experiments on the Tiny-ImageNet [19] dataset and transfer learning to Flower [25],
CUB [39], Aircraft [21], Dogs [16], and MIT [28] datasets, and Cross-Domain Few-Shot Learning (CD-FSL) [11]. Our experiments show that our method outperforms LLF in both in-distribution and transfer learning settings (Tables 1, 2, 4).
Our method also provides better prediction depth (Fig. 2). By analyzing the statistics of Hessian eigenvalues, we observe that our method has a lower max eigenvalue and no negative eigenvalues, suggesting SEAL reaches a flatter local minima and avoids saddle points (Table 6). 2.