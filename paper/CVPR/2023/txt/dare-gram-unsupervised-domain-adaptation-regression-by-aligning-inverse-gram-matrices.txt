Abstract
Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap between a labeled source dataset and an unlabelled target dataset for regression problems. Recent works mostly focus on learning a deep feature encoder by minimizing the discrepancy between source and target features. In this work, we present a dif-ferent perspective for the DAR problem by analyzing the closed-form ordinary least square (OLS) solution to the linear regressor in the deep domain adaptation context.
Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the fea-tures, which is motivated by its presence in the OLS solu-tion and the Gram matrix’s ability to capture the feature correlations. Specifically, we propose a simple yet effec-tive DAR method which leverages the pseudo-inverse low-rank property to align the scale and angle in a selected sub-space generated by the pseudo-inverse Gram matrix of the two domains. We evaluate our method on three domain adaptation regression benchmarks. Experimental results demonstrate that our method achieves state-of-the-art per-formance. Our code is available at https://github. com/ismailnejjar/DARE-GRAM . 1.

Introduction
Regression problems, in which models learn to pre-dict continuous variables, are one fundamental paradigm in machine learning. Regression problems are omnipresent in many different applications, including computer vision tasks, such as head-pose estimation [76], facial landmark detection [37], human pose estimation [80], depth estima-tion [20] and eye-tracking problems [58], and also widely in industrial applications, such as product quality prediction and condition monitoring [69]. Nevertheless, real-world ap-plications are often subject to the environmental conditions under which the data are collected and other influencing fac-tors, hence domain gaps between datasets are inevitable.
Figure 1. Illustration of the UDA for regression setup and our main motivation. (a) Deep domain adaptation networks commonly use a shared deep feature encoder and a shared linear regression layer.
We propose to pay close attention to the linear regressor, where the ordinary least square (OLS) solution is well-known. (b) Given a trained source linear regressor βs, the target features Zt may not be calibrated to βs. (c) Unlike previous adaptation methods which align in the original feature embedding space, we propose to align the inverse Gram matrix of the features (Z T Z)−1, which is motivated by its presence in the OLS solution and the Gram matrix’s ability to capture the feature correlations.
Unsupervised Domain adaptation (UDA) aims to over-come the distributional shift between a labeled source do-main and an unlabelled target domain. Many UDA meth-ods have been proposed to alleviate the domain shift prob-lem. One common UDA direction is feature alignment by adversarial learning [45] or explicit losses such as maxi-mum mean discrepancy [44] to learn domain-invariant rep-resentations.
Input alignment [77] and self-training us-ing pseudo-label refinement [41] are also popular UDA di-rections. While many DA methods have been developed and evaluated for classification and segmentation prob-lems, some are not directly transferable to DA regres-sion [10]. Pioneer works in Domain Adaptation Regres-sion (DAR) [12, 46] introduced theoretical analysis for the problem. A few algorithms were proposed to tackle DAR.
For example, importance weighting [14, 74] and feature alignment [7, 51] have shown improved results over learn-ing only from the source. Most recent unsupervised DAR methods [10,59] use the deep learning framework and focus on learning a shared deep feature extractor by directly min-imizing the discrepancy between source and target features.
By doing so, it is implicitly assumed that if the feature dis-crepancy is small, a shared linear regressor can be easily learned from the source supervision. This formulation used by existing works focuses solely on the feature extractor.
In this work, we propose to look at the DAR problem from a different perspective. In particular, we pay close at-tention to the linear regressor, which is attached directly af-ter the feature extractor. Motivated by the closed-form ordi-nary least squares (OLS) regression solution, we analyze the potential optimal regressor for each domain. We reveal in
Section 3.2 that even when the discrepancy between source and target features is small, the learning of a shared linear regressor could still be difficult because of the inverse Gram matrix term in the OLS solution.
In light of this, we propose an ordinary least squares inspired deep domain adaptation method for regression called Domain Adaptation Regression by aligning the in-verse GRAM matrices (DARE-GRAM). As shown in Fig-ure 1, unlike previous methods, which directly align the fea-tures, we align the inverse Gram matrix of the features. This is motivated by its presence in the closed-form solution of the ordinary least squares. More specifically, we leverage the low-rank property of the pseudo-inverse to align a se-lected subspace in scale and angle engendered by the Gram
Matrix, which represents the intensity and pairwise inter-actions between different features for the source and target domains. The scale and angle alignment based on the Gram matrix can lead to a better-calibrated regressor with regard to both source and target data. The contributions of this work are as follows:
• We offer a new perspective to understand the UDA for regression problems by leveraging the well-known closed-form solutions to the linear regression problem.
• Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features.
• Empirical results on three benchmarks validate the su-periority of the DARE-GRAM over baseline methods. 2.