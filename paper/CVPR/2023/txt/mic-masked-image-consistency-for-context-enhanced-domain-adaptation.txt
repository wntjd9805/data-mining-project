Abstract
In unsupervised domain adaptation (UDA), a model trained on source data (e.g. synthetic) is adapted to tar-get data (e.g. real-world) without access to target anno-tation. Most previous UDA methods struggle with classes that have a similar visual appearance on the target domain as no ground truth is available to learn the slight appear-ance differences. To address this problem, we propose a
Masked Image Consistency (MIC) module to enhance UDA by learning spatial context relations of the target domain as additional clues for robust visual recognition. MIC en-forces the consistency between predictions of masked target images, where random patches are withheld, and pseudo-labels that are generated based on the complete image by an exponential moving average teacher. To minimize the consistency loss, the network has to learn to infer the pre-dictions of the masked regions from their context. Due to its simple and universal concept, MIC can be integrated into various UDA methods across different visual recogni-tion tasks such as image classification, semantic segmenta-tion, and object detection. MIC significantly improves the state-of-the-art performance across the different recognition tasks for synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For instance, MIC achieves an un-precedented UDA performance of 75.9 mIoU and 92.8% on GTA→Cityscapes and VisDA-2017, respectively, which corresponds to an improvement of +2.1 and +3.0 percent points over the previous state of the art. The implementation is available at https://github.com/lhoyer/MIC. 1.

Introduction
In order to train state-of-the-art neural networks for visual recognition tasks, large-scale annotated datasets are neces-sary. However, the collection and annotation process can be very time-consuming and tedious. For instance, the annota-tion of a single image for semantic segmentation can take more than one hour [10,66]. Therefore, it would be beneficial to resort to existing or simulated datasets, which are easier
Figure 1. (a) Previous UDA methods such as HRDA [31] strug-gle with similarly looking classes on the unlabeled target domain.
Here, the interior of the sidewalk is wrongly segmented as road, probably, due to the ambiguous local appearance. (b) The proposed
Masked Image Consistency (MIC) enhances the learning of context relations to consider additional context clues such as the curb in the foreground. With MIC, the adapted network is able to correctly segment the sidewalk. (c) MIC can be plugged into most existing
UDA methods. It enforces the consistency of the predictions of a masked target image with the pseudo-label of the original image.
So, the network is trained to better utilize context clues on the target domain. Further details are shown in Fig. 3. to annotate. However, a network trained on such a source dataset usually performs worse when applied to the actual target dataset as neural networks are sensitive to domain gaps. To mitigate this issue, unsupervised domain adapta-tion (UDA) methods adapt the network to the target domain using unlabeled target images, for instance, with adversarial training [20, 27, 57, 73] or self-training [30, 31, 72, 79, 97].
UDA methods have remarkably progressed in the last few years. However, there is still a noticeable performance gap compared to supervised training. A common problem is the confusion of classes with a similar visual appearance on the target domain such as road/sidewalk or pedestrian/rider as
there is no ground truth supervision available to learn the slight appearance differences. For example, the interior of the sidewalk in Fig. 1 is segmented as road, probably, due to a similar local appearance. To address this problem, we propose to enhance UDA with spatial context relations as additional clues for robust visual recognition. For instance, the curb in the foreground of Fig. 1 a) could be a crucial context clue to correctly recognize the sidewalk despite the ambiguous texture. Although the used network architectures already have the capability to model context relations, previ-ous UDA methods are still not able to reach the full potential of using context dependencies on the target domain as the used unsupervised target losses are not powerful enough to enable effective learning of such information.
Therefore, we design a method to explicitly encourage the network to learn comprehensive context relations of the target domain during UDA. In particular, we propose a novel
Masked Image Consistency (MIC) plug-in for UDA (see
Fig. 1 c), which can be applied to various visual recognition tasks. Considering semantic segmentation for illustration,
MIC masks out a random selection of target image patches and trains the network to predict the semantic segmentation result of the entire image including the masked-out parts. In that way, the network has to utilize the context to infer the se-mantics of the masked regions. As there are no ground truth labels for the target domain, we resort to pseudo-labels, gen-erated by an EMA teacher that uses the original, unmasked target images as input. Therefore, the teacher can utilize both context and local clues to generate robust pseudo-labels.
Over the course of the training, different parts of objects are masked out so that the network learns to utilize different context clues, which further increases the robustness. After
UDA with MIC, the network is able to better exploit context clues and succeeds in correctly segmenting difficult areas that rely on context clues such as the sidewalk in Fig. 1 b).
To the best of our knowledge, MIC is the first UDA ap-proach to exploit masked images to facilitate learning con-text relations on the target domain. Due to its universality and simplicity, MIC can be straightforwardly integrated into various UDA methods across different visual recognition tasks, making it highly valuable in practice. MIC achieves significant and consistent performance improvements for dif-ferent UDA methods (including adversarial training, entropy-minimization, and self-training) on multiple visual recog-nition tasks (image classification, semantic segmentation, and object detection) with different domain gaps (synthetic-to-real, clear-to-adverse-weather, and day-to-night) and dif-ferent network architectures (CNNs and Transformer). It sets a new state-of-the-art performance on all tested bench-marks with significant improvements over previous methods as shown in Fig. 2. For instance, MIC respectively improves the state-of-the-art performance by +2.1, +4.3, and +3.0 per-cent points on GTA→Cityscapes(CS), CS→DarkZurich, and
Figure 2. MIC significantly improves state-of-the-art UDA methods across different UDA benchmarks and recognition tasks such as image classification (Cls.), semantic segmentation (Segm.), and object detection (Det.). Detailed results can be found in Sec. 4.
VisDA-2017 and achieves an unprecedented UDA perfor-mance of 75.9 mIoU, 60.2 mIoU, and 92.8%, respectively. 2.