Abstract
We introduce PointConvFormer, a novel building block for point cloud based deep network architectures. Inspired by generalization theory, PointConvFormer combines ideas from point convolution, where filter weights are only based on relative position, and Transformers which utilize feature-based attention. In PointConvFormer, attention computed from feature difference between points in the neighborhood is used to modify the convolutional weights at each point.
Hence, we preserved the invariances from point convolu-tion, whereas attention helps to select relevant points in the neighborhood for convolution. PointConvFormer is suitable for multiple tasks that require details at the point level, such as segmentation and scene flow estimation tasks. We exper-iment on both tasks with multiple datasets including Scan-Net, SemanticKitti, FlyingThings3D and KITTI. Our re-sults show that PointConvFormer offers a better accuracy-speed tradeoff than classic convolutions, regular transform-ers, and voxelized sparse convolution approaches. Visual-izations show that PointConvFormer performs similarly to convolution on flat areas, whereas the neighborhood selec-tion effect is stronger on object boundaries, showing that it has got the best of both worlds. The code will be available. 1.

Introduction
Depth sensors for indoor and outdoor 3D scanning have significantly improved in terms of both performance and affordability. Hence, their common data format, the 3D point cloud, has drawn significant attention from academia and industry. Understanding the 3D real world from point clouds can be applied to many application domains, e.g. robotics, autonomous driving, CAD, and AR/VR. However, unlike image pixels arranged in regular grids, 3D points are unstructured, which makes applying grid based Convolu-tional Neural Networks (CNNs) difficult.
Various approaches have been proposed in response to this challenge.
[3, 5, 31, 35, 37, 65] introduced interesting ways to project 3D point clouds back to 2D image space
*this work was done entirely at Apple Inc., Wenxuan Wu was an intern at Apple Inc. when he participated in the work
Figure 1. Performance vs. running time on ScanNet. PointCon-vFormer achieves a state-of-the-art 74.5% mIoU while being effi-cient with faster speed and way less learnable parameters. Larger dot indicates more learnable parameters. All results are reported on a single TITAN RTX GPU and apply 2D convolution. Another line of research di-rectly voxelizes the 3D space and apply 3D discrete con-volution, but it induces massive computation and memory overhead [47, 62]. Sparse 3D convolution operations [9, 20] save a significant amount of computation by computing convolution only on occupied voxels.
Some approaches directly operate on point clouds [41, 57,58,64,69,81]. [57,58] are pioneers which aggregate in-formation on point clouds using max-pooling layers. Others proposed to reorder the input points with a learned transfor-mation [42], a flexible point kernel [69], and a convolutional operation that directly work on point clouds [75, 81] which utilizes a multi-layer perceptron (MLP) to learn convolu-tion weights implicitly as a nonlinear transformation from the relative positions of the local neighbourhood.
The approach to directly work on points is appealing to us because it allows direct manipulation of the point co-ordinates, thus being able to encode rotation/scale invari-ance/equivariance directly into the convolution weights [41, 42, 58, 94]. These invariances serve as priors to make the models more generalizable. Besides, point-based ap-proaches require less parameters than voxel-based ones, which need to keep e.g. 3 × 3 × 3 convolution kernels on all input and output channels. Finally, point-based ap-proaches can utilize k-nearest neighbors (kNN) to find the local neighborhood, thus can adapt to variable sampling densities in different 3D locations.
However, so far the methods with the best accuracy-speed tradeoff have still been the sparse voxel-based ap-proaches or a fusion between sparse voxel and point-based models. Note that no matter the voxel-based or point-based representation, the information from the input is exactly the same, so it is unclear why fusion is needed. Besides, fu-sion adds significantly to model complexity and memory usage. This leads us to question the component that is in-deed different between these representations: the general-ization w.r.t. the irregular local neighbourhood. The shape of the kNN neighbourhood in point-based approaches varies in different parts of the point cloud. Irrelevant points from other objects, noise and the background might be included in the neighborhood, especially around object boundaries, which can be detrimental to the performance and the ro-bustness of point-based models.
To improve the robustness of models with kNN neigh-borhoods, we refer back to the generalization theory of
CNNs, which indicates that points with significant fea-ture correlation should be included in the same neighbor-hood [40]. A key idea in this paper is that feature corre-lation can be a way to filter out irrelevant neighbors in a kNN neighborhood, which makes the subsequent convolu-tion more generalizable. We introduce PointConvFormer, which computes attention weights based on feature differ-ences and uses that to reweight the points in the neighbor-hood in a point-based convolutional model, which indirectly
“improves” the neighborhood for generalization.
The idea of using feature-based attention is not new, but there are important differences between PointConvFormer and other vision transformers [16,54,96]. PointConvFormer combines features in the neighborhood with point-wise con-volution, whereas Transformer attention models usually adopt softmax attention in this step.
In our formulation, the positional information is outside the attention, hence viewpoint-invariance can be introduced into the convoulu-tional weights. We believe that invariance helps generaliz-ing across neighborhood (size/rotation) differences between training/testing sets, especially with a kNN neighborhood.
We evaluate PointConvFormer on two point cloud tasks, semantic segmentation and scene flow estimation. For semantic segmentation, experiment results on the indoor
ScanNet [11] and the outdoor SemanticKitti [2] demon-strate superior performances over classic convolution and transformers with a much more compact network. The per-formance gap is the most significant at low resolutions, e.g. on ScanNet with a 10cm resolution we achieved more than 10% improvement over MinkowskiNet with only 15% of its parameters (Fig. 1). We also apply PointConvFormer as the backbone of PointPWC-Net [82] for scene flow esti-mation, and observe significant improvements on FlyingTh-ings3D [48] and KITTI scene flow 2015 [52] datasets as well. These results show that PointConvFormer could po-tentially compete with sparse convolution as the backbone choice for dense prediction tasks on 3D point clouds. 2.