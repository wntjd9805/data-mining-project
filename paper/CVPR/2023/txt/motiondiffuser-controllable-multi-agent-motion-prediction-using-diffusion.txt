Abstract 1.

Introduction
We present MotionDiffuser, a diffusion based represen-tation for the joint distribution of future trajectories over multiple agents. Such representation has several key ad-vantages: ﬁrst, our model learns a highly multimodal dis-tribution that captures diverse future outcomes. Second, the simple predictor design requires only a single L2 loss train-ing objective, and does not depend on trajectory anchors.
Third, our model is capable of learning the joint distribu-tion for the motion of multiple agents in a permutation-invariant manner. Furthermore, we utilize a compressed trajectory representation via PCA, which improves model performance and allows for efﬁcient computation of the exact sample log probability. Subsequently, we propose a general constrained sampling framework that enables controlled trajectory sampling based on differentiable cost functions. This strategy enables a host of applications such as enforcing rules and physical priors, or creating tai-lored simulation scenarios. MotionDiffuser can be com-bined with existing backbone architectures to achieve top motion forecasting results. We obtain state-of-the-art re-sults for multi-agent motion prediction on the Waymo Open
Motion Dataset.
Motion prediction is a central yet challenging problem for autonomous vehicles to safely navigate under uncertain-ties. Motion prediction, in the autonomous driving setting, refers to the prediction of the future trajectories of modeled agents, conditioned on the histories of the modeled agents, context agents, road graph and trafﬁc light signals.
Several key challenges arise in the motion prediction problem. First, motion prediction is probabilistic and multi-modal in nature where it is important to faithfully predict an unbiased distribution of possible futures. Second, mo-tion prediction requires jointly reasoning about the future distribution for a set of agents that may interact with each other in each such futures. Naively predicting and sampling from the marginal distribution of trajectories for each agent independently leads to unrealistic and often conﬂicting out-comes. Last but not least, while it is challenging to constrain or bias the predictions of conventional regression-based tra-jectory models, guided sampling of the trajectories is often required. For example, it may be useful to enforce rules or physical priors for creating tailored simulation scenarios.
This requires the ability to enforce constraints over the fu-ture time steps, or enforce a speciﬁed behavior for one or
Figure 2. Overview for multi-agent motion prediction using diffusion models. The input scene containing agent history, trafﬁc lights and road graphs is encoded via a transformer encoder into a set of condition tokens C. During training, a random set of noises are sampled i.i.d. from a normal distribution and added to the ground truth (GT) trajectory. The denoiser, while attending to the condition tokens, predicts the denoised trajectories corresponding to each agent. The entire model can be trained end-to-end using a simple L2 loss between the predicted denoised trajectory and the GT trajectory. During inference, a population of trajectories for each agent can ﬁrst be sampled from pure noise at the highest noise level σmax, and iteratively denoised by the denoiser to produce a plausible distribution of future trajectories.
An optional constraint in the form of an arbitrary differentiable loss function can be injected in the denoising process to enforce constraints. more agents among a set of agents.
In light of these challenges, we present MotionDiffuser, a denoising diffusion model-based representation for the joint distribution of future trajectories for a set of agents (see Fig. 2). MotionDiffuser leverages a conditional denoising diffusion model. Denoising diffusion models
[16, 23, 33, 43, 44] (henceforth, diffusion models) are a class of generative models that learns a denoising function based on noisy data and samples from a learned data distri-bution via iteratively reﬁning a noisy sample starting from pure Gaussian noise (see Fig. 1). Diffusion models have recently gained immense popularity due to their simplicity, strong capacity to represent complex, high dimensional and multimodal distributions, ability to solve inverse problems
[4, 6, 24, 44], and effectiveness across multiple problem do-mains, including image generation [36, 37, 39], video gen-eration [15, 18, 49] and 3D shape generation [35].
Building on top of conditional diffusion models as a basis for trajectory generation, we propose several unique design improvements for the multi-agent motion predic-tion problem. First, we propose a cross-attention-based permutation-invariant denoiser architecture for learning the motion distribution for a set of agents regardless of their ordering. Second, we propose a general and ﬂexible frame-work for performing controlled and guided trajectory sam-pling based on arbitrary differentiable cost functions of the trajectories, which enables several interesting applications such as rules and controls on the trajectories, trajectory in-painting and creating tailored simulation scenarios. Fi-nally, we propose several enhancements to the representa-tion, including PCA-based latent trajectory diffusion and improved trajectory sample clustering to further boost the performance of our model.
In summary, the main contributions of this work are:
• A novel permutation-invariant, multi-agent joint mo-tion distribution representation using conditional dif-fusion models.
• A general and ﬂexible framework for performing con-trolled and guided trajectory sampling based on ar-bitrary differentiable cost functions of the trajectories with a range of novel applications.
• Several signiﬁcant enhancements to the representation, including PCA-based latent trajectory diffusion formu-lation and improved trajectory sample clustering algo-rithm to further boost the model performance. 2.