Abstract
In this paper, we study a novel problem in egocentric action recognition, which we term as “Multimodal Gener-alization” (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new ac-tion categories. MMG consists of two novel scenarios, de-signed to support security, and efficiency considerations in real-world applications: (1) missing modality generaliza-tion where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modali-ties present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modali-ties. Our dataset is derived from Ego4D [27] dataset, but processed and thoroughly re-annotated by human experts to facilitate research in the MMG problem. We evaluate a diverse array of models on MMG-Ego4D and propose new methods with improved generalization ability. In par-ticular, we introduce a new fusion module with modality dropout training, contrastive-based alignment training, and a novel cross-modal prototypical loss for better few-shot performance. We hope this study will serve as a bench-mark and guide future research in multimodal generaliza-tion problems. The benchmark and code are available at https://github.com/facebookresearch/MMG Ego4D 1.

Introduction
Action recognition systems are typically trained on data captured from a third-person or spectator perspective [37, 56]. However, in areas such as robotics and augmented reality, we capture data through the eyes of agents, i.e., in a first-person or egocentric perspective. With head-*Equal contribution
†Work done during an internship at Meta Reality Labs.
Figure 1. Overview of MMG-Ego4D challenge. In a typical eval-uation setting (a) networks are trained for the supervised setting or the few-shot setting using training/support sets with data from all modalities and evaluated on data points with all modalities. How-ever, there can often be a mismatch between training and testing modalities. Our proposed challenge contains two tasks to mimic these settings. In (b) missing modality evaluation, the model can only use a subset of training modalities for inference. In (c) Cross-modal zero-shot evaluation, the models are on modalities unseen during training. mounted devices such Ray-Ban Stories becoming popu-lar, action recognition from egocentric videos is critical to enable downstream applications, such as contextual rec-ommendations or reminders. However, egocentric action recognition is fundamentally different and more challeng-ing [6, 7, 43, 55]. While third-person video clips are of-ten curated, egocentric video clips are uncurated and have low-level corruptions, such as large motion blur due to head motion. Moreover, egocentric perception requires a careful understanding of the camera wearer’s physical surround-ings, and must interpret the objects and interactions from the wearer’s perspective.
Recognizing egocentric activity exclusively from one
Modality video audio
IMU
Memory per second of data (KB)
Typical model FLOPs (G) 593.92 70.50 62.76 42.08 9.44 1.65
Table 1. Compute and memory cost for different modalities.
Memory used per second for each modality is computed by aver-aging the memory used by 1000 data points drawn randomly from
Ego4D [27]. The provided compute number corresponds to the forward pass cost of MViT [15] for video, AST [26] for audio, and a ViT [11] based transformer model for IMU data. strictions during training. For example, if a user has to train a system, often in a few-shot setting, computationally ex-pensive modalities like video are best trained on the cloud.
However, the user might prefer that their data stays on the device. However, the video will consume 60× more stor-age, and 43× more compute compared to cheaper modali-ties like IMU (see Tab. 1), significantly increasing the dif-ficulty of training on devices with limited compute and storage. In this situation, we may want to enable training with computationally less demanding modalities like audio while maintaining the flexibility of performing inference on more informative modalities like video. Multimodal sys-tems should robustly generalize across modalities.
In this work, we propose MMG-Ego4D: a challenge de-signed to measure the generalization ability of egocentric activity recognition models. Our challenge consists of two novel tasks: (1) missing modality generalization aimed at measuring the generalization ability of models when eval-uated on an incomplete set of modalities (shown in Fig. 1 (b)), and (2) cross-modal zero-shot generalization aimed at measuring the generalization ability of models in general-izing to unseen modalities during test time (shown in Fig. 1 (c)). We evaluate several widely-used architectures us-ing this benchmark and introduce a novel approach that en-hances generalization capability in the MMG-Ego4D chal-lenge, while also improving performance in standard full-modalities settings. Our primary contributions are:
• MMG Problem. We present MMG, a novel and practical problem with two tasks, missing modality generalization and cross-modal zero-shot generalization, for evaluating the generalization ability of multimodal action recogni-tion models. These tasks are designed to support real-world security and efficiency considerations, and we de-fine them in both supervised and more challenging few-shot settings.
• MMG-Ego4D Dataset. To facilitate the study of MMG problem in ego-centric action recognition task, we intro-duce a new dataset, MMG-Ego4d, which is derived from
Ego4D [27] dataset by preprocessing the data points and thoroughly re-annotating by human experts to suit the task. To the best of our knowledge, this is the first work
Figure 2. Multimodal data is crucial for egocentric perception.
Input data consists of three modalities: video, audio, and IMU. (top) Video action recognition identifies the clip with a tool and much grass in the background as the class trim grass with other tools. Audio action recognition system classifies the periodic rub-bing sound as put trash in a trash can. The IMU model clas-sifies the head movement action into the class wipe a table. (bot-tom) Multimodal action recognition system correctly combines the video feed and audio feed and identifies the activity as collect dry leaves on the ground. modality can often be ambiguous. This is because we want to perceive what the device’s wearer is performing instead of what the camera feed is capturing. To this end, Mul-timodal information can be crucial for understanding and disambiguating the user’s intent or action. We demonstrate it through an example in Fig. 2. In the example, the video feed shows a tool in the background of the grassland. An activity recognition model exclusively based on video rec-ognizes it as the class trim grass with other tools. Similarly, a model exclusively trained in audio identifies the rubbing sounds in the clip as the class put trash in a trash can, and an IMU model mistakes the head motion as wipe a table.
However, a multimodal system correctly identifies the class as collect dry leaves on the ground by combining video, au-dio, and IMU signals. information is essential
While using multimodal to achieve state-of-the-performance, it also presents a unique challenge - we may not be able to use all modalities in the real world due to security or efficiency considerations.
For example, a user might be located in a sensitive envi-ronment and decide to turn off the camera due to security concerns. Similarly, users may turn off microphones so that their voices are not heard. In these situations, multimodal systems must be able generalize to missing modalities (Fig. 1 (b)), i.e., work with an incomplete set of modalities at in-ference, and make a robust prediction. These challenges are not just limited to inference time but could manifest in re-to introduce these novel evaluation tasks and a benchmark challenge of its kind.
• Strong Baselines. We present a new method that achieves strong performance on the generalization abil-ity benchmark and also improves the performance un-der the normal full-modalities setting. Our method em-ploys a Transformer-based fusion module, which allows for flexible input of different modalities. We employ a cross-modal contrastive alignment loss to project features of different modalities into a unified space. Finally, a novel loss function is introduced, which is called cross-modal prototypical loss, achieving state-of-the-art results in multimodal few-shot settings. Extensive ablation stud-ies are performed to identify each proposed component’s contribution. 2.