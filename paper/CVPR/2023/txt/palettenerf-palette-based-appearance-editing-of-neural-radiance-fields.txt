Abstract
Recent advances in neural radiance fields have enabled the high-fidelity 3D reconstruction of complex scenes for novel view synthesis. However, it remains underexplored how the appearance of such representations can be effi-ciently edited while maintaining photorealism. In this work, we present PaletteNeRF, a novel method for photorealis-tic appearance editing of neural radiance fields (NeRF) based on 3D color decomposition. Our method decom-poses the appearance of each 3D point into a linear com-bination of palette-based bases (i.e., 3D segmentations de-fined by a group of NeRF-type functions) that are shared across the scene. While our palette-based bases are view-independent, we also predict a view-dependent function to capture the color residual (e.g., specular shading). Dur-ing training, we jointly optimize the basis functions and the color palettes, and we also introduce novel regulariz-ers to encourage the spatial coherence of the decomposi-*Parts of this work were done when Zhengfei Kuang was an intern at
Adobe Research. tion. Our method allows users to efficiently edit the appear-ance of the 3D scene by modifying the color palettes. We also extend our framework with compressed semantic fea-tures for semantic-aware appearance editing. We demon-strate that our technique is superior to baseline methods both quantitatively and qualitatively for appearance edit-ing of complex real-world scenes. Our project page is https://palettenerf.github.io. 1.

Introduction
Neural Radiance Fields (NeRF) [23] and its variants
[8, 25, 27, 39] have received increasing attention in recent years for their ability to robustly reconstruct real-world 3D scenes from 2D images and enable high-quality, photoreal-istic novel view synthesis. However, such volumetric repre-sentations are challenging to edit due to the fact that scene appearance is implicitly encoded in neural features and net-work weights that do not support local manipulation or in-tuitive modification.
Multiple approaches have been proposed to support edit-ing of NeRF. One category of methods [4, 18, 41, 45] re-cover the material properties of the scene so that they can re-render them under novel lighting conditions or ad-just the material properties such as surface roughness.
Such methods rely on accurate estimation of the scene reflectance, which is typically challenging for real-world complex scenes captured under unconstrained environment.
Another category of methods [21,35] learns a latent code on which NeRF can be conditioned to produce the desired ap-pearance. However, these methods often suffer from limited capacity and flexibility and do not support fine-grained edit-ing. In addition, some other methods [40] learn to transfer the appearance of NeRF to match a given style image, but sometimes fail to maintain the same level of photorealism in the original scene.
In this paper, we propose PaletteNeRF, a novel method to support flexible and intuitive editing of NeRF. Our method is inspired by previous image-editing methods based on color palettes [7, 31], where a small set of col-ors are used to represent the full range of colors in the im-age. We model the radiance of each point using a combi-nation of specular and diffuse components, and we further decompose the diffuse component into a linear combina-tion of view-independent color bases that are shared across the scene. During training, we jointly optimize the per-point specular component, the global color bases and the per-point linear weights to minimize the difference between the rendered images and the ground truth images. We also introduce novel regularizers on the weights to encourage the sparseness and spatially coherence of the decomposition and achieve more meaningful grouping. With the proposed framework, we can intuitively edit the appearance of NeRF by freely modifying the learned color bases (Fig. 1). We further show that our framework can be combined with se-mantic features to support semantic-aware editing. Unlike previous palette-based image [1, 31] or video [10] editing methods, our method produces more globally coherent and 3D consistent recoloring results of the scene across arbi-trary views. We demonstrate that our method can enable more fine-grained local color editing while faithfully main-taining the photorealism of the 3D scene, and achieves bet-ter performance than baseline methods both quantitatively and qualitatively. In summary, our contributions include:
• We propose a novel framework to facilitate the edit-ing of NeRF by decomposing the radiance field into a weighted combination of learned color bases.
• We introduced a robust optimization scheme with novel regularizers to achieve intuitive decompositions.
• Our approach enables practical palette-based appear-ance editing, making it possible for novice users to in-teractively edit NeRF in an intuitive and controllable manner on commodity hardware. 2.