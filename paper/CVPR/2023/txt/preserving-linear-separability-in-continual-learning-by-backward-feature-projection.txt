Abstract
Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowl-edge distillation in feature space have been proposed and shown to reduce forgetting [16, 17, 25]. However, most fea-ture distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity.
To achieve a better stability-plasticity trade-off, we propose
Backward Feature Projection (BFP), a method for contin-ual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while al-lowing the emergence of new feature directions to accom-modate new classes. BFP can be integrated with existing experience replay methods and boost performance by a sig-nificant margin. We also demonstrate that BFP helps learn a better representation space, in which linear separability is well preserved during continual learning and linear prob-ing achieves high classification accuracy. 1.

Introduction
Despite their many successes, deep neural networks remain prone to catastrophic forgetting [37], whereby a model’s performance on old tasks degrades significantly while it is learning to solve new tasks. Catastrophic forget-ting has become a major challenge for continual learning (CL) scenarios, where the model is trained on a sequence of tasks, with limited or no access to old training data.
The ability to learn continually without forgetting is cru-cial to many real-world applications, such as computer vi-sion [36, 46], intelligent robotics [30], and natural language processing [6, 23]. In these settings, an agent learns from a stream of new data or tasks, but training on the old data is restricted due to limitations in storage, scaling of training time, or even concerns about privacy.
The continual learning problem has received significant
Figure 1. Feature distribution before and after training on a task in a class incremental learning experiment on MNIST, visualized by t-SNE. Left: before training on task 2, seen classes (1,2) are learned to be separable along the horizontal axis for classification, while unseen classes (3, 4) are not separable. Right: after training on task 2, the new vertical axis is learned to separate new classes (3,4). Based on this observation, we propose the Backward Fea-ture Projection loss LBF P , which allows new feature dimensions to emerge to separate new classes in feature space and also pre-serves the linear separability of old classes to reduce catastrophic forgetting. attention and multiple solution themes have emerged. Ex-perience replay methods [8, 33], for example, store a lim-ited number of (or generate) old training examples and use them together with new data in continual learning. Parame-ter regularization methods [29,50] restrict the change of im-portant network parameters. Knowledge distillation meth-ods [16, 17, 31] regularize the intermediate output of the
CL model to preserve the knowledge from old tasks. Ar-chitectural methods [34, 42, 48] adopt expansion and isola-tion techniques with neural networks to prevent forgetting.
All these methods strive to balance learning new knowledge (plasticity) and retaining old knowledge (stability).
We present a continual learning algorithm, focusing on knowledge distillation (KD) in feature space. In the con-tinual learning context, KD treats the continual learning model as the student and its old checkpoint as the teacher and regularizes the network intermediate outputs to reduce forgetting [4,8,11,16,17,25,31]. Although recent CL meth-ods based on KD have been effective at reducing forgetting, they typically adopt the L2 distance for distillation, forc-ing the learned features to be close to their exact old values.
This is too restrictive and results in CL models that are more rigid in retaining old knowledge (stronger stability), but less flexible in adapting to new tasks (weaker plasticity). Our method has a better tradeoff of stability and plasticity.
In this paper, we pay attention to the feature space in CL and study its evolution. We show that a small number of principal directions explain most of the variance in feature space and only these directions are important for classifi-cation. A large number of directions in the feature space have little variance and remain unused. When the model is trained on new tasks, new features need to be learned along those unused directions to accommodate new classes, as illustrated in Figure 1. Without handling forgetting, the old principal directions, along which the old classes are lin-early separable, will be forgotten. Our results indicate that such forgetting of learned principal directions in the feature space is an important reason for catastrophic forgetting.
Based on this insight, as shown in Figure 1, we propose a Backward Feature Projection (BFP) loss, an effective fea-ture distillation loss that enforces feature consistency up to a learnable linear transformation, not imposing exact equality of features. This transformation aims to preserve the linear separability of features backward in time. We show that this linear projection is important because it can rotate, reflect, and scale features, while maintaining the linear separability of the previously learned classes in the new feature space.
Projecting backward allows the features to change and new decision boundaries to be learned along the unused feature directions to classify new classes. BFP can be integrated into existing CL methods in a straightforward way and ex-periments show that this simple change boosts the perfor-mance over baselines by a large margin.
Our experiments show that the proposed BFP regular-ization loss can improve the baseline methods by up to 6%-8% on the challenging Split-CIFAR10 and Split-CIFAR100 datasets, achieving state-of-the-art class-incremental learn-ing accuracy. More importantly, the linear probing experi-ments show that BFP results in a better feature space where different classes are more separable. See Figure 1 for an illustrative example. Our contributions are as follows:
• We provide an analysis of feature space evolution dur-ing continual learning, distinguishing the important feature components from unimportant ones.
• We propose the Backward Feature Projection (BFP) loss, which preserves the linear separability of old classes while allowing plasticity during continual learning, i.e. features are allowed to change.
• When combined with simple experience replay base-lines, BFP helps learn better feature space and achieves state-of-the-art performance on challenging datasets. 2.