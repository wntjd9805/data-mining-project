Abstract 1.

Introduction
Recognizing scenes and objects in 3D from a single image is a longstanding goal of computer vision with applications in robotics and AR/VR. For 2D recognition, large datasets and scalable solutions have led to unprecedented advances.
In 3D, existing benchmarks are small in size and approaches specialize in few object categories and specific domains, e.g. urban driving scenes. Motivated by the success of 2D recognition, we revisit the task of 3D object detection by introducing a large benchmark, called OMNI3D. OMNI3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 98 categories. 3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types. We propose a model, called Cube
R-CNN, designed to generalize across camera and scene types with a unified approach. We show that Cube R-CNN outperforms prior works on the larger OMNI3D and existing benchmarks. Finally, we prove that OMNI3D is a powerful dataset for 3D object recognition and show that it improves single-dataset performance and can accelerate learning on new smaller datasets via pre-training.1 1We release the OMNI3D benchmark and Cube R-CNN models at https://github.com/facebookresearch/omni3d.
Understanding objects and their properties from single images is a longstanding problem in computer vision with applications in robotics and AR/VR. In the last decade, 2D object recognition [26, 40, 63, 64, 72] has made tremendous advances toward predicting objects on the image plane with the help of large datasets [24,45]. However, the world and its objects are three dimensional laid out in 3D space. Perceiv-ing objects in 3D from 2D visual inputs poses new challenges framed by the task of 3D object detection. Here, the goal is to estimate a 3D location and 3D extent of each object in an image in the form of a tight oriented 3D bounding box.
Today 3D object detection is studied under two different lenses: for urban domains in the context of autonomous vehi-cles [7,13,50,52,57] or indoor scenes [31,36,58,73]. Despite the problem formulation being shared, methods share little insights between domains. Often approaches are tailored to work only for the domain in question. For instance, ur-ban methods make assumptions about objects resting on a ground plane and model only yaw angles for 3D rotation.
Indoor techniques may use a confined depth range (e.g. up to 6m in [58]). These assumptions are generally not true in the real world. Moreover, the most popular benchmarks for image-based 3D object detection are small. Indoor SUN
RGB-D [70] has 10k images, urban KITTI [21] has 7k im-ages; 2D benchmarks like COCO [45] are 20× larger.
We address the absence of a general large-scale dataset for 3D object detection by introducing a large and diverse 3D benchmark called OMNI3D. OMNI3D is curated from pub-licly released datasets, SUN RBG-D [70], ARKitScenes [6],
Hypersim [65], Objectron [2], KITTI [21] and nuScenes [9], and comprises 234k images with 3 million objects anno-tated with 3D boxes across 98 categories including chair, sofa, laptop, table, cup, shoes, pillow, books, car, person, etc. Sec. 3 describes the curation process which involves re-purposing the raw data and annotations from the aforemen-tioned datasets, which originally target different applications.
As shown in Fig. 1, OMNI3D is 20× larger than existing popular benchmarks used for 3D detection, SUN RGB-D and KITTI. For efficient evaluation on the large OMNI3D, we introduce a new algorithm for intersection-over-union of 3D boxes which is 450× faster than previous solutions [2].
We empirically prove the impact of OMNI3D as a large-scale dataset and show that it improves single-dataset performance by up to 5.3% AP on urban and 3.8% on indoor benchmarks.
On the large and diverse OMNI3D, we design a gen-eral and simple 3D object detector, called Cube R-CNN, inspired by advances in 2D and 3D recognition of recent years [22, 52, 64, 68]. Cube R-CNN detects all objects and their 3D location, size and rotation end-to-end from a sin-gle image of any domain and for many object categories.
Attributed to OMNI3D’s diversity, our model shows strong generalization and outperforms prior works for indoor and urban domains with one unified model, as shown in Fig. 1.
Learning from such diverse data comes with challenges as
OMNI3D contains images of highly varying focal lengths which exaggerate scale-depth ambiguity (Fig. 4). We rem-edy this by operating on virtual depth which transforms object depth with the same virtual camera intrinsics across the dataset. An added benefit of virtual depth is that it allows the use of data augmentations (e.g. image rescaling) during training, which is a critical feature for 2D detection [12, 80], and as we show, also for 3D. Our approach with one unified design outperforms prior best approaches in AP3D, ImVoxel-Net [66] by 4.1% on indoor SUN RGB-D, GUPNet [52] by 9.5% on urban KITTI, and PGD [77] by 7.9% on OMNI3D.
We summarize our contributions:
• We introduce OMNI3D, a benchmark for image-based 3D object detection sourced from existing 3D datasets, which is 20× larger than existing 3D benchmarks.
• We implement a new algorithm for IoU of 3D boxes, which is 450× faster than prior solutions.
• We design a general-purpose baseline method, Cube
R-CNN, which tackles 3D object detection for many categories and across domains with a unified approach.
We propose virtual depth to eliminate the ambiguity from varying camera focal lengths in OMNI3D. 2.