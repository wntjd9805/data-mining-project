Abstract
Novel Class Discovery (NCD) aims to discover unknown classes without any annotation, by exploiting the transfer-able knowledge already learned from a base set of known classes. Existing works hold an impractical assumption that the novel class distribution prior is uniform, yet neglect
In this paper, the imbalanced nature of real-world data. we relax this assumption by proposing a new challenging task: distribution-agnostic NCD, which allows data drawn from arbitrary unknown class distributions and thus ren-ders existing methods useless or even harmful. We tackle this challenge by proposing a new method, dubbed “Boot-strapping Your Own Prior (BYOP)”, which iteratively es-timates the class prior based on the model prediction it-self. At each iteration, we devise a dynamic temperature technique that better estimates the class prior by encour-aging sharper predictions for less-confident samples. Thus,
BYOP obtains more accurate pseudo-labels for the novel samples, which are beneficial for the next training itera-tion. Extensive experiments show that existing methods suf-fer from imbalanced class distributions, while BYOP1 out-performs them by clear margins, demonstrating its effec-tiveness across various distribution scenarios. 1.

Introduction
With the ever-increasing growth of massive unlabeled data, our community is interested in mining and leveraging the “dark” knowledge therein [2, 7, 28]. To this end, Novel
Class Discovery (NCD) [14] is considered as a pivotal step, which aims to automatically recognize novel classes by par-titioning the unlabeled data into different clusters with the knowledge learned from a labeled base class set. Note that the base knowledge is indispensable because clustering without a prior is known as an ill-posed problem [20]—data
*Corresponding author. 1Code: https://github.com/muliyangm/BYOP.
Figure 1. Novel Class Discovery (NCD) in different scenarios. (a)
NCD with no prior on balanced unlabeled data. (b) NCD with the uniform prior on balanced unlabeled data. (c) NCD with the uniform prior on imbalanced unlabeled data. can always be clustered w.r.t. any feature dimension, e.g., color and background. Hence, the base set provides a pre-liminary prior for defining class vs. non-class features, e.g., the object background feature is removed for discovering new classes.
Yet, clustering is still ambiguous to other features not re-moved by the base knowledge. As shown in Fig. 1(a), if we do not specify the class distribution prior, i.e., #sample per class, the two clusters may be considered as red vs. other color, but not the desired moose vs. cow. Therefore, clustering with such a specified prior is a common practice in existing NCD methods [11,34,47]. However, they hold a na¨ıve assumption that the class distribution in the unlabeled data is balanced, i.e., the prior is uniform. This is imprac-tical because the nature of data distribution—especially for
The cluster assignments are used as pseudo-labels to train a classifier to discover novel classes. However, due to the imperfections in pseudo-labels, the predicted class distribu-tions are inevitably ambiguous, especially for those minor-ity classes. To this end, we propose a dynamic tempera-ture technique that can be integrated into the classifier to output more confident distribution predictions (Fig. 2(b)).
The main idea is to encourage sharper predicted distribu-tions for less-confident data by a per-sample temperature adjustment. In particular, we call it “adaptive” because it won’t hurt the prediction for the samples which are already confident, while significantly disambiguating those who are less confident, as later discussed in Fig. 3.
To estimate the class prior, we gather the predicted novel class distributions as the class assignments for the training samples, and calculate the proportion of each class assign-ment (Fig. 2(c)), so that we can derive a new class prior that is beneficial for the next training iteration. Note that the higher prediction accuracy for majority classes guar-antees to estimate a preliminary prior that helps generate more accurate pseudo-labels, which in turn promotes the reliability of the prior estimation for other classes via more accurate model predictions. We benchmark our proposed
BYOP and the current state-of-the-art methods in the chal-lenging distribution-agnostic NCD task on several standard datasets. While current methods suffer from imbalanced class distributions, BYOP outperforms them by large mar-gins, demonstrating its effectiveness across different class distributions, including the conventionally balanced one.
To sum up, our contributions are three-fold:
• A new challenging distribution-agnostic NCD task that relaxes the impractical uniform class distribution as-sumption in current NCD works.
• A novel training paradigm dubbed BYOP to handle ar-bitrary unknown class distributions in NCD by itera-tively estimating and utilizing the class prior.
• Extensive experiments that benchmark the current state-of-the-art methods as well as the superiority of the proposed BYOP in distribution-agnostic NCD. 2.