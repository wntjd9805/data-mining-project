Abstract 1.

Introduction
An increasing amount of applications rely on data-driven models that are deployed for perception tasks across a sequence of scenes. Due to the mismatch between training and deployment data, adapting the model on the new scenes is often crucial to obtain good performance. In this work, we study continual multi-scene adaptation for the task of semantic segmentation, assuming that no ground-truth la-bels are available during deployment and that performance on the previous scenes should be maintained. We propose training a Semantic-NeRF network for each scene by fusing the predictions of a segmentation model and then using the view-consistent rendered semantic labels as pseudo-labels to adapt the model. Through joint training with the seg-mentation model, the Semantic-NeRF model effectively en-ables 2D-3D knowledge transfer. Furthermore, due to its compact size, it can be stored in a long-term memory and subsequently used to render data from arbitrary viewpoints to reduce forgetting. We evaluate our approach on Scan-Net, where we outperform both a voxel-based baseline and a state-of-the-art unsupervised domain adaptation method.
*Authors share first authorship.
† Authors share senior authorship.
Data-driven models trained for perception tasks play an increasing role in applications that rely on scene under-standing, including, e.g., mixed reality and robotics. When deploying these models on real-world systems, however, mismatches between the data used for training and those en-countered during deployment can lead to poor performance, prompting the need for an adaptation of the models to the new environment. Oftentimes, the supervision data required for this adaptation can only be obtained through a labori-ous labeling process. Furthermore, even when such data are available, a na¨ıve adaptation to the new environment results in decreased performance on the original training data, a phenomenon known as catastrophic forgetting [19, 23].
In this work, we focus on the task of adapting a seman-tic segmentation network across multiple indoor scenes, un-der the assumption that no labeled data from the new envi-ronment are available. Similar settings are explored in the literature in the areas of unsupervised domain adaptation (UDA) [23, 38] and continual learning (CL) [19]. How-ever, works in the UDA literature usually focus on a sin-gle source-to-target transfer where the underlying assump-tion is that the data from both the source and the target domain are available all at once in the respective training stage, and often study the setting in which the knowledge transfer happens between a synthetic and a real environ-ment [6, 31, 32, 38]. On the other hand, the CL commu-nity, which generally explores the adaptation of networks across different tasks, has established the class-incremental setting as the standard for semantic segmentation, in which new classes are introduced across different scenes from the same domain and ground-truth supervision is provided [23].
In contrast, we propose to study network adaptation in a set-ting that more closely resembles the deployment of seman-tic networks on real-world systems. In particular, instead of assuming that data from a specific domain are available all at once, we focus on the scenario in which the network is sequentially deployed in multiple scenes from a real-world indoor environment (we use the ScanNet dataset [7]), and therefore has to perform multiple stages of adaptation from one scene to another. Our setting further includes the possi-bility that previously seen scenes may be revisited. Hence, we are interested in achieving high prediction accuracy on each new scene, while at the same time preserving perfor-mance on the previous ones. Note that unlike the better explored class-incremental CL, in this setting we assume a closed set of semantic categories, but tackle the covariate shift across scenes without the need for ground-truth labels.
We refer to this setting as continual semantic adaptation.
In this work, we propose to address this adaptation problem by leveraging advances in neural rendering [27].
Specifically, in a similar spirit to [11], when deploying a pre-trained network in a new scene, we aggregate the se-mantic predictions from the multiple viewpoints traversed by the agent into a 3D representation, from which we then render pseudo-labels that we use to adapt the network on the current scene. However, instead of relying on a voxel-based representation, we propose to aggregate the predic-tions through a semantics-aware NeRF [27, 48]. This for-mulation has several advantages. First, we show that us-ing NeRFs to aggregate the semantic predictions results in higher-quality pseudo-labels compared to the voxel-based method of [11]. Moreover, we demonstrate that using these pseudo-labels to adapt the segmentation network results in superior performance compared both to [11] and to the state-of-the-art UDA method CoTTA [43]. An even more interesting insight, however, is that due the differentiabil-ity of NeRF, we can jointly train the frame-level semantic network and the scene-level NeRF to enforce similarity be-tween the predictions of the former and the renderings of the latter. Remarkably, this joint procedure induces better performance of both labels, showing the benefit of mutual 2D-3D knowledge transfer.
A further benefit of our method is that after adapting to a new scene, the NeRF encoding the appearance, geometry and semantic content for that scene can be compactly saved in long-term storage, which effectively forms a “memory bank” of the previous experiences and can be useful in re-ducing catastrophic forgetting. Specifically, by mixing pairs of semantic and color NeRF renderings from a small num-ber of views in the previous scenes and from views in the current scene, we show that our method is able to outper-form both the baseline of [11] and CoTTA [43] on the adap-tation to the new scene and in terms of knowledge reten-tion on the previous scenes. Crucially, the collective size of the NeRF models is lower than that of the explicit replay buffer required by [11] and of the teacher network used in
CoTTA [43] up to several dozens of scenes. Additionally, each of the NeRF models stores a potentially infinite num-ber of views that can be used for adaptation, not limited to the training set as in [11], and removes the need to explicitly keep color images and pseudo-labels in memory.
In summary, the main contributions of our work are the following: (i) We propose using NeRFs to adapt a semantic segmentation network to new scenes. We find that enforc-ing 2D-3D knowledge transfer by jointly adapting NeRF and the segmentation network on a given scene results in a consistent performance improvement; (ii) We address the problem of continually adapting the segmentation network across a sequence of scenes by compactly storing the NeRF models in a long-term memory and mixing rendered images and pseudo-labels from previous scenes with those from the current one. Our approach allows generating a potentially infinite number of views to use for adaptation at constant memory size for each scene; (iii) Through extensive exper-iments, we show that our method achieves better adapta-tion and performance on the previous scenes compared both to a recent voxel-based method that explored a similar set-ting [11] and to a state-of-the-art UDA method [43]. 2.