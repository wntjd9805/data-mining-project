Abstract
Editing 3D geometry is a challenging task requiring spe-cialized skills. In this work, we aim to facilitate the task of editing the geometry of 3D models through the use of natural language. For example, we may want to modify a 3D chair model to “make its legs thinner” or to “open a hole in its back”. To tackle this problem in a man-ner that promotes open-ended language use and enables fine-grained shape edits, we introduce the most extensive existing corpus of natural language utterances describing shape differences: ShapeTalk. ShapeTalk contains over half a million discriminative utterances produced by con-trasting the shapes of common 3D objects for a variety of object classes and degrees of similarity. We also intro-duce a generic framework, ChangeIt3D, which builds on
ShapeTalk and can use an arbitrary 3D generative model of shapes to produce edits that align the output better with the edit or deformation description. Finally, we introduce metrics for the quantitative evaluation of language-assisted shape editing methods that reflect key desiderata within this editing setup. We note that ShapeTalk allows methods to be trained with explicit 3D-to-language data, bypassing the necessity of “lifting” 2D to 3D using methods like neural rendering, as required by extant 2D image-language foun-dation models. Our code and data are publicly available at https://changeit3d.github.io/. 1.

Introduction
Visual content creation and adaptation, whether in 2D or 3D scenes, has traditionally been a time-consuming ef-fort, requiring specialized skills, software, and multiple it-erations. The use of natural language promises to democ-ratize this process and let ordinary users perform semanti-cally plausible content synthesis, as well as addition, dele-tion, and modification by describing their intent in words – and then letting AI-powered tools translate that into edits of their content. There has been very strong recent interest in and impressive results from large visual language models able to transform text into 2D images, such as DALL-E 2 from OpenAI, or Imagen from Google. The same need ex-ists for 3D asset creation for video games, movies, as well as mixed-reality experiences – though fully automated tools in the 3D area are only now starting to appear [26, 33, 35].
The task of editing 2D or 3D content via language is even more challenging, as references to extant scene components have to be resolved, while unreferenced parts of the scene should be kept unchanged as much as possible.
This work focuses on the task of modifying the shape of a 3D object in a fine-grained manner according to the semantics of free-form natural language. Operating directly in a 3D representation has many advantages for downstream tasks that need to be 3D-aware, such as scene composition and manipulation, interaction, etc. Even if only 2D views are needed, 3D provides superior attribute disentanglement and guarantees view consistency. Furthermore, note that modifying the 3D geometry of an object in ways that are faithful to its class semantics is itself a highly non-trivial undertaking (e.g., stretching a sedan should keep the wheels circular) and has been the focus of recent work [44, 45].
Our language-driven shape deformation task is applica-ble to many real-world situations: e.g., in assisting visually-impaired users, graphic designers, or artists to interact with objects of interest and change them to better fit their design needs. We build a framework, ChangeIt3D, to address this task, consisting of three major components: the ShapeTalk large-scale dataset with an order of magnitude more utter-ances than in previous work (Section 2), a modular architec-ture for implementing edits on top of a variety of 3D shape representations, and a set of evaluation metrics to quantify the quality of the performed transformations.
The ShapeTalk dataset, linking 3D shapes and free-form language, contains over half a million discriminative ut-terances produced by contrasting pairs of common 3D ob-jects for a variety of object classes and degrees of similar-ity. Shape differentiation helps focus the language on fine-Figure 1. Samples of contrastive utterances in ShapeTalk. For each paired distractor-target object, ultra-fine-grained shape differences are enumerated by an annotator in decreasing order of importance in the annotator’s judgment. Interestingly, both continuous and discrete geometric features that objects share across categories naturally emerge in the language of ShapeTalk; e.g., humans describe the “thinness” of a chair leg or a vase lip (top row) or the presence of an “arm” that a lamp or a clock might have (bottom row). grained but important differences that might not rise to the surface when we describe object geometry individually, as in PartIt [19], where clearly different geometries can end up with very similar descriptions because they share a com-mon underlying structure. Furthermore, unlike the dataset used by ShapeGlot [5], our goal is to obtain as complete de-scriptions of the geometry differences between two objects as possible, with the goal of enabling reconstruction of the differing object from the reference object and the language – going well beyond simple discrimination. Examples of utterances in ShapeTalk are provided in Figure 1.
We approach the task of language-based shape editing by enabling shape edits and deformations on top of a variety of 3D generative models of shapes, including Point-Cloud
Auto-Encoders (PC-AE) [4], implicit neural methods (Im-Net) [11] and Shape Gradient Fields (SGF) [8]. To this end, we train a network on ShapeTalk for a discriminative task of identifying the target within a distractor-target pair (exam-ples in Figure 1) and show that the same network can guide edits done directly inside the latent spaces of these genera-tive models. We note that a great deal of ShapeTalk refers to shape parts. Even though the underlying shape represen-tations we deploy do not have explicit knowledge of parts, we demonstrate that our framework can apply a variety of part-based edits and deformations. This confirms a remark-able finding – already described in [24] and [20] – that the notion of parts can be learned from language alone, without any geometric part supervision.
As already mentioned, making edits to an existing shape is more demanding than ab initio shape generation as (a) it requires understanding of the input shape and its relation to the modification language, and (b) changes to parts not referenced in the modification utterance should be avoided.
Hence, a further contribution of our work is a set of eval-uation metrics for the modification success and quality, re-flecting realism of the resulting shape, faithfulness to the language instructions, and stability or avoidance of unnec-essary changes. Such metrics are essential for encouraging further progress in the field.
In summary, this work introduces 1 a new large-scale multimodal dataset, ShapeTalk, with referential language that differentiates shapes of common objects with rich lev-els of detail, enabling a new setup for doing language-driven shape deformations directly in 3D. We approach the task of language-based shape editing with 2 a modular frame-work supporting diverse 3D shape representations and im-plementing fine-grained edits guided by a 3D-aware neural-listening network. To set the stage for future developments on the task, we introduce 3 a set of intuitive evaluation metrics for the shape edits and deformations performed. 2.