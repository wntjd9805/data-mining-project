Abstract
In the context of incremental class learning, deep neu-ral networks are prone to catastrophic forgetting, where the accuracy of old classes declines substantially as new knowl-edge is learned. While recent studies have sought to address this issue, most approaches suffer from either the stability-plasticity dilemma or excessive computational and param-eter requirements. To tackle these challenges, we propose a novel framework, the Diverse Knowledge Transfer Trans-former (DKT), which incorporates two knowledge trans-fer mechanisms that use attention mechanisms to transfer both task-specific and task-general knowledge to the current task, along with a duplex classifier to address the stability-plasticity dilemma. Additionally, we design a loss func-tion that clusters similar categories and discriminates be-tween old and new tasks in the feature space. The pro-posed method requires only a small number of extra param-eters, which are negligible in comparison to the increas-ing number of tasks. We perform extensive experiments on CIFAR100, ImageNet100, and ImageNet1000 datasets, which demonstrate that our method outperforms other com-petitive methods and achieves state-of-the-art performance.
Our source code is available at https://github.com/MIV-XJTU/DKT. 1.

Introduction
Deep neural networks have demonstrated notable suc-cess in the domain of class-fixed classification problems, wherein the object classes are predetermined and constant throughout the training and testing phases [12, 13]. Never-theless, in practical scenarios, these models are frequently employed in an ever-changing and dynamic environment, necessitating the incorporation of capabilities to enable them to learn and identify new classes that arise contin-* Corresponding authors
Figure 1. The accuracy and parameters number comparisons of different methods on CIFAR100 10-step. We report the final parameter number and accuracy. The height of the green pillars represents the number of final parameters, the polyline represents the average accuracy of different methods. We can see that DKT surpasses state-of-the-art methods with much fewer parameters. uously. This challenge, commonly known as the Class-Incremental Learning (CIL) problem [4, 27], is of utmost significance.
Numerous recent studies [9, 10, 15, 27, 36, 41, 44, 45] have endeavored to address the challenges associated with
Class-Incremental Learning (CIL). Among these methods, some [15,27,41,45] have adopted the concept of knowledge distillation [14], which entails transferring prior knowledge from a teacher model to a student model, to retain the previ-ous knowledge encoded in the output logits of the network.
Meanwhile, other methods [10, 36, 44] have employed dy-namic expandable networks to overcome the CIL issues.
These techniques involve dynamically augmenting the net-work architectures, such as feature extractors, by utilizing supplementary parameters and memory. Despite recent ad-vancements in addressing the CIL problem, several chal-lenges persist. Firstly, knowledge distillation techniques, as demonstrated in the literature [14], exhibit significant feature degradation [44], leading to reduced performance when transferring knowledge from prior to new tasks. Sec-ondly, networks must be stable enough to preserve existing knowledge [23,26] while simultaneously exhibiting plastic-ity to acquire new information, creating a stability-plasticity dilemma [3]. Lastly, dynamic expandable networks de-mand significant additional parameters, memory storage, and computational resources, hindering their practical ap-plication in real-world scenarios.
We introduce a new approach, named the Diverse
Knowledge Transfer Transformer (DKT), to address the aforementioned challenges. DKT comprises two innovative attention blocks and a duplex classifier. Firstly, to mitigate feature degradation and catastrophic forgetting, we propose two novel attention blocks: the General Knowledge Trans-fer Attention Block (GKAB) and the Specific Knowledge
Transfer Attention Block (SKAB). These attention blocks can transfer previous knowledge by utilizing a unified task-general token and a set of task-specific tokens from a to-ken pool. For each task, the token pool initializes a new task-specific token to accumulate task-specific knowledge and update the unified task-general token storing the gen-eral knowledge of previous tasks. Unlike other dynamic expandable networks that use a feature extractor network, we initialize a task-specific token for each task, which is a 1 × 384 trainable vector. This design reduces the number of extra parameters and computational requirements signif-icantly. We demonstrate the relationship between parame-ters and performance in Figure 1. Notably, DKT achieves state-of-the-art performance with only 1/10 of the param-eters of the competitive DER w/o P method [44] and sig-nificantly outperforms other representative methods. Sec-ondly, to address the stability-plasticity dilemma, we pro-pose a duplex classifier comprising a stability classifier to maintain the model’s stability on old categories and a plas-ticity classifier to learn the knowledge of new categories.
Additionally, we propose a cluster-separation loss to pull features belonging to the same categories together and push features between old and new tasks apart. This encourages the model to learn diverse task-specific knowledge in differ-ent tasks.
We conduct extensive experiments on three widely-used image classification benchmarks, namely CIFAR100, Im-ageNet100, and ImageNet1000, to showcase the effective-ness of our proposed method. We compare our DKT with other state-of-the-art methods, including Dytox [10] and
DER [44]. Dytox is the first attempt to utilize the trans-former architecture for CIL, while DER uses extra parame-ters to achieve state-of-the-art performance. Our proposed approach outperforms Dytox [10] and sets a new state-of-the-art performance surpassing DER [44]. Our ablation study confirms the efficacy of our proposed method.
In summary, our key contributions include:
• We propose a novel framework, DKT, that comprises the GKAB and SKAB attention blocks to facilitate di-verse knowledge transfer and mitigate catastrophic for-getting in continual learning scenarios.
• We introduce a duplex classifier that enables a model to maintain stability in recognizing old categories while retaining plasticity in learning new categories.
• We develop a cluster-separation loss that clusters fea-tures belonging to the same categories and discrimi-nates features between old and new tasks to encourage the model to learn diverse task-specific knowledge.
• We conduct extensive experiments on three bench-marks, and our approach achieves a new state-of-the-art performance with fewer parameters on the CIL benchmarks. 2.