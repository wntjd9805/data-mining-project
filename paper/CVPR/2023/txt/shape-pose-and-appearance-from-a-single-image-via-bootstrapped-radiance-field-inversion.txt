Abstract 1.

Introduction
Neural Radiance Fields (NeRF) coupled with GANs rep-resent a promising direction in the area of 3D reconstruc-tion from a single view, owing to their ability to efﬁciently model arbitrary topologies. Recent work in this area, how-ever, has mostly focused on synthetic datasets where ex-act ground-truth poses are known, and has overlooked pose estimation, which is important for certain down-stream applications such as augmented reality (AR) and robotics. We introduce a principled end-to-end reconstruc-tion framework for natural images, where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an object, without exploiting multiple views during training. More speciﬁcally, we leverage an uncondi-tional 3D-aware generator, to which we apply a hybrid in-version scheme where a model produces a ﬁrst guess of the solution which is then reﬁned via optimization. Our frame-work can de-render an image in as few as 10 steps, enabling its use in practical scenarios. We demonstrate state-of-the-art results on a variety of real and synthetic benchmarks.
∗Work done during an internship at Google.
We focus on single-view 3D reconstruction, where the goal is to reconstruct shape, appearance, and camera pose from a single image of an object (Fig. 1). Such a task has applications in content creation, augmented & virtual reality (AR/VR), robotics, and is also interesting from a scientiﬁc perspective, as most neural architectures cannot reason about 3D scenes. As humans, we learn object pri-ors, abstract representations that allow us to imagine what a partially-observed object would look like from other view-points. Incorporating such knowledge into a model would enable higher forms of 3D reasoning. While early work on 3D reconstruction has focused on exploiting annotated data [16, 20, 57, 63, 72], e.g. ground-truth 3D shapes or mul-tiple 2D views, more recent work has relaxed the assump-tions required by the task. In particular, there has been effort in learning this task from single-view collections of images depicting a speciﬁc category [17, 27, 33] (e.g. a dataset of cars), and we also follow this line of work.
Most established 3D representations in the single-view reconstruction literature are based on deformable trian-gle meshes [17, 27, 33], although Neural Radiance Fields (NeRF) [1, 39] have recently become more prominent in the broader 3D vision community owing to their ability to efﬁciently model arbitrary topologies. These have been
combined with GANs [18] for unconditional 3D generation tasks [5,6,40,62], as they produce more perceptually pleas-ing results. There has also been work on combining the two in the single-view reconstruction task, e.g. Pix2NeRF [4], which is however demonstrated on simple settings of faces or synthetic datasets where perfect ground-truth poses are available. Furthermore, there has been less focus overall on producing an end-to-end reconstruction system that addi-tionally tackles pose estimation (beyond simple settings),
In which is particularly important for AR applications. our work, we bridge this gap by proposing a more general
NeRF-based end-to-end reconstruction pipeline that tackles both reconstruction and pose estimation, and demonstrate its broader applicability to natural images where poses can-not be accurately estimated. We further characterize the problem by comparing encoder-based approaches (the ma-jority of methods in the single-view reconstruction litera-ture) to inversion-based approaches (which invert a gener-ator via optimization), and show that the latter are more suited to real datasets without accurate ground-truth poses.
Motivated by this, we propose a hybrid GAN inversion technique for NeRFs that can be regarded as a compro-mise between the two: an encoder produces a ﬁrst guess of the solution (bootstrapping), which is then reﬁned via optimization. We further propose a series of technical con-tributions, including: (i) the adoption of an SDF represen-tation [65] to improve the reconstructed surfaces and facil-itate their conversion to triangle meshes, (ii) regularizers to accelerate inversion, and (iii) the addition of certain equiv-ariances in the model architecture to improve generaliza-tion. We show that we can invert an image in as few as 10 optimization steps, making our approach usable even in constrained scenarios. Furthermore, we incorporate a prin-cipled pose estimation framework [53] that frames the prob-lem as a regression of a canonical representation followed by Perspective-n-Point (PnP), and show that it boosts pose estimation accuracy without additional data assumptions.
We summarize our main contributions as follows:
• We introduce an end-to-end single-view 3D reconstruc-tion pipeline based on NeRFs. In this setting, we success-fully demonstrate 360◦ object reconstruction from natu-ral images under the CMR [17] benchmark.
• We propose a hybrid inversion scheme for NeRFs to ac-celerate the reversal of pre-trained 3D-aware generators.
• Inspired by the literature on pose estimation, we propose a principled PnP-based pose estimator that leverages our framework and does not require extra data assumptions.
To validate our contributions, we obtain state-of-the-art results on both real/synthetic benchmarks. Furthermore, to our knowledge, we are the ﬁrst to demonstrate NeRF-based reconstruction on in-the-wild datasets such as ImageNet.
We release our code and pretrained models at https:// github.com/google-research/nerf-from-image. 2.