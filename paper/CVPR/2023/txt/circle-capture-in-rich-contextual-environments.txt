Abstract
To download the data please visit our website.
Synthesizing 3D human motion in a contextual, ecologi-cal environment is important for simulating realistic activ-ities people perform in the real world. However, conven-tional optics-based motion capture systems are not suited for simultaneously capturing human movements and com-plex scenes. The lack of rich contextual 3D human motion datasets presents a roadblock to creating high-quality gen-erative human motion models. We propose a novel motion acquisition system in which the actor perceives and oper-ates in a highly contextual virtual world while being mo-tion captured in the real world. Our system enables rapid collection of high-quality human motion in highly diverse scenes, without the concern of occlusion or the need for physical scene construction in the real world. We present
CIRCLE, a dataset containing 10 hours of full-body reach-ing motion from 5 subjects across nine scenes, paired with ego-centric information of the environment represented in various forms, such as RGBD videos. We use this dataset to train a model that generates human motion conditioned on scene information. Leveraging our dataset, the model learns to use ego-centric scene information to achieve non-trivial reaching tasks in the context of complex 3D scenes. 1.

Introduction
Humans excel at interacting with complex environments, effortlessly engaging in everyday tasks such as getting out of a car while carrying a backpack or plugging a power cord into an outlet behind a cabinet. The remarkably ﬂexible and compliant human body enables access to narrow or clut-tered spaces where clear paths are not available. Synthesiz-ing 3D human motion that reﬂects this ability to navigate in highly contextual, ecological environments, such as our homes, grocery stores, or hospital operating rooms, will sig-niﬁcantly impact applications in Embodied AI, Computer
Animation, Robotics, and AR/VR.
Machine learning models have signiﬁcantly advanced the creation of 3D human motion and behaviors in recent years. However, the success of the ML-approach hinges on one condition—the human motion data for training models must be of high quality, volume, and diversity. Traditional motion capture (mocap) techniques focus on the “human movement” itself, rather than the state of the environment in which the motion takes place. While mocap can faithfully record human kinematics, capturing humans in a contextual
scene requires physical construction of a production set and speciﬁc props in the capture studio. This steep requirement limits the capability of today’s mocap technologies to holis-tically capture realistic human activities in the real world.
We propose to eliminate the costly requirement of phys-ical staging by capturing human motion during interactions with a virtual reality simulation. This allows us to capture motion like the ones shown in Figure 1, where a person reaches into cluttered spaces in a furnished apartment. Ad-ditionally, we are able to simultaneously record paired ﬁrst-person perspectives of the virtual environment through VR, as illustrated in Figure 3. With paired ego-centric observa-tion of the world, we can now train motion models to not only comprehend the how of certain tasks, but also the why behind an individual’s movements.
By creating the complex scene in the virtual world and keeping the capture space in the real world empty, our method provides four crucial advantages over state-of-the-art solutions. First, creating a highly contextual environ-ment in VR is much simpler and less costly than in actual reality. Second, capturing the state of the real world requires complex sensor instrumentation, while the state of the vir-tual world is readily available from the simulator. Third, because the capture space in reality is always empty, our system is not subject to occlusions that degrade the motion quality, regardless of any clutter in the perceived environ-ment. Fourth, the data acquired by such a system provide 3D human motions and corresponding videos of the envi-ronment rendered in any camera view of choice, such as the egocentric view.
We use a Meta Quest 2 headset and the AI Habitat sim-ulator in our experiments. However, our system is agnostic to the choice of hardware, simulator, and virtual environ-ment. To illustrate the possibilities enabled by the availabil-ity of contextual motion capture data, we collect a dataset,
CIRCLE, containing ten hours of full-body reaching mo-tion within nine indoor household scenes. CIRCLE con-tains challenging reaching scenarios, including reaching for an object behind the toilet, between tightly placed furni-ture, and underneath the table. Finally, we use CIRCLE to train a model that generates reaching motions conditioned on scene information. Our model takes as input the starting pose of the person in the scene as well as the target location of the right hand, and automatically generates a scene-aware sequence of human motion that reaches the target location.
We propose two different methods to encode the scene in-formation and compare them against baselines.
In summary, the contributions of this work include:
• A novel motion acquisition system to collect 3D hu-man motion with synchronized scene information,
• A novel dataset, CIRCLE, with 10 hours of human motion data from 5 subjects in 9 realistic apartment scenes,
• A data-driven model, trained on CIRCLE, for generat-ing full-body reaching motion within an environment. 2.