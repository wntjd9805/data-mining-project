Abstract
Emotion understanding is an essential but highly chal-lenging component of artificial general intelligence. The absence of extensive annotated datasets has significantly impeded advancements in this field. We present Emotion-CLIP, the first pre-training paradigm to extract visual emo-tion representations from verbal and nonverbal communi-cation using only uncurated data. Compared to numerical labels or descriptions used in previous methods, commu-nication naturally contains emotion information. Further-more, acquiring emotion representations from communica-tion is more congruent with the human learning process.
We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emo-tion cues using sentiment-guided contrastive learning. Ex-tensive experiments validates the effectiveness and transfer-ability of EmotionCLIP. Using merely linear-probe evalua-tion protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and ri-vals many multimodal approaches across various bench-marks. We anticipate that the advent of EmotionCLIP will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related do-mains. The code and pre-trained models are available at https://github.com/Xeaver/EmotionCLIP. 1.

Introduction
If artificial intelligence (AI) can be equipped with emo-tional intelligence (EQ), it will be a significant step toward developing the next generation of artificial general intelli-gence [46, 92]. The combination of emotion and intelli-gence distinguishes humans from other animals. The ability to understand, use, and express emotions will significantly facilitate the interaction of AI with humans and the environ-ment [20, 48–50], making it the foundation for a wide vari-ety of HCI [3], robotics [11], and autonomous driving [31] applications.
*equal contribution
Figure 1. Emotions emerge naturally in human communica-tion through verbal and nonverbal cues. The rich semantic de-tails within the expression can hardly be represented by human-annotated categorical labels and descriptions in current datasets.
Artificial emotional intelligence (AEI) research is still in its nascency [30, 73]. The recent emergence of pre-trained models in CV [10, 24, 62] and NLP [7, 16, 33, 68] domains has ushered in a new era of research in related subjects. By training on large-scale unlabeled data in a self-supervised manner, the model learns nontrivial representa-tions that generalize to downstream tasks [42, 62]. Unfortu-nately, such a technique remains absent from AEI research.
The conventional approaches in visual emotion understand-ing have no choice but to train models from scratch, or leverage models from less-relevant domains [27, 66], suf-fering from data scarcity [29, 45]. The lack of pre-trained models greatly limits the development of AEI research.
Research in neuroscience and psychology offers insights for addressing this problem. Extending from the capabil-ities that have been coded genetically, humans learn emo-tional expressions through daily interaction and communi-cation as early as when they are infants. It has been shown that both vision [58] and language [41] play crucial roles in this learning process. By absorbing and imitating expres-sions from others, humans eventually master the necessary
feelings to comprehend emotional states by observing and analyzing facial expressions, body movements, contextual environments, etc.
Inspired by how humans comprehend emotions, we pro-pose a new paradigm for emotion understanding that learn directly from human communication. The core of our idea is to explore the consistency between verbal and nonverbal affective cues in daily communication. Fig. 1 shows how communication reveals emotion. Our method that learns from communication is not only aligned with the human learning process but also has several advantages: 1) Our method bypasses the problems in emotion data collection by leveraging uncurated data from daily com-munication. Existing emotion understanding datasets are mainly annotated using crowdsourcing [29, 45, 77]. For image classification tasks, it is straightforward for annota-tors to agree on an image’s label due to the fact that the label is determined by certain low-level visual character-istics. However, crowdsourcing participants usually have lower consensus on producing emotion annotations due to the subjectivity and subtlety of affective labels [90]. This phenomenon makes it extremely difficult to collect accurate emotion annotations on a large scale. Our approach does not rely on human annotations, allowing us to benefit from nearly unlimited web data. 2) Our use of verbal expressions preserves fine-grained semantics to the greatest extent possible. Limited by the data collection strategy, existing datasets usually only con-tain annotations for a limited number of emotion categories, which is far from covering the space of human emotional expression [86]. Moreover, the categorical labels com-monly used in existing datasets fail to precisely represent the magnitude or intensity of a certain emotion. 3) Our approach provides a way to directly model ex-pressed emotion. Ideally, AEI should identify the individ-ual’s emotional state, i.e., the emotion the person desires to express. Unfortunately, it is nearly impossible to col-lect data on this type of “expressed emotion” on a large scale. Instead, the current practice is to collect data on “per-ceived emotion” to approximate the person’s actual emo-tional state, which inevitably introduces noise and bias to labels.
In general, learning directly from how humans express themselves is a promising alternative that gives a far broader source of supervision and a more comprehensive represen-tation. This strategy is closely analogous to the human learning process and provides an efficient solution for ex-tracting emotion representations from uncurated data.
We summarize our main contributions as follows:
• We introduce EmotionCLIP, the first vision-language pre-training paradigm using uncurated data to the vi-sual emotion understanding domain.
• We propose two techniques to guide the model to cap-ture salient emotional expressions from human verbal and nonverbal communication.
• Extensive experiments and analysis demonstrate the superiority and transferability of our method on vari-ous downstream datasets in emotion understanding. 2.