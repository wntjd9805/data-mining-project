Abstract
Modeling and re-rendering dynamic 3D scenes is a chal-lenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it re-quires many MLP evaluations, constraining real-world ap-plications. We show that dynamic 3D scenes can be ex-plicitly represented by six planes of learned features, lead-ing to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vec-tors extracted from each plane, which is highly efficient.
Pairing a HexPlane with a tiny MLP to regress output col-ors and training via volume rendering gives impressive re-sults for novel view synthesis on dynamic scenes, match-ing the image quality of prior work but reducing training time by more than 100×. Extensive ablations confirm our
HexPlane design and show that it is robust to different fea-ture fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlane is a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.1 1.

Introduction
Reconstructing and re-rendering 3D scenes from a set of 2D images is a core vision problem which can enable many AR/VR applications. The last few years have seen tremendous progress in reconstructing static scenes, but this assumption is restrictive: the real world is dynamic, and in complex scenes motion is the norm, not the exception.
Many current approaches for representing dynamic 3D scenes rely on implicit representations, building on
NeRF [42]. They train a large multi-layer perceptron (MLP) that inputs the position of a point in space and time, and out-puts either the color of the point [28, 29] or a deformation to a canonical static scene [16, 49, 50, 54]. In either case, rendering images from novel views is expensive since each generated pixel requires many MLP evaluations. Training is 1Project page: https://caoang327.github.io/HexPlane.
Figure 1. HexPlane for Dynamic 3D Scenes. Instead of regress-ing colors and opacities from a deep MLP, we explicitly compute features for points in spacetime via HexPlane. Pairing with a tiny
MLP, it allows above 100× speedups with matching quality. similarly slow, requiring up to days of GPU time to model a single dynamic scene; this computational bottleneck pre-vents these methods from being widely applied.
Several recent methods for modeling static scenes have demonstrated tremendous speedups over NeRF through the use of explicit and hybrid methods [7, 43, 66, 81]. These methods use an explicit spatial data structure that stores ex-plicit scene data [14, 81] or features that are decoded by a tiny MLP [7, 43, 66]. This decouples a model’s capacity from its speed, and allows high-quality images to be ren-dered in realtime [43]. While effective, these methods have thus far been applied only to static scenes.
In this paper, we aim to design an explicit representa-tion of dynamic 3D scenes, building on similar advances for static scenes. To this end, we design a spatial-temporal data structure that stores scene data. It must overcome two key technical challenges. First is memory usage. We must model all points in both space and time; na¨ıvely storing data in a dense 4D grid would scale with the fourth power of grid resolution which is infeasible for large scenes or long durations. Second is sparse observations. Moving a single camera through a static scene can give views that densely cover the scene; in contrast, moving a camera through a dynamic scene gives just one view per timestep. Treating timesteps independently may give insufficient scene cov-erage for high-quality reconstruction, so we must instead share information across timesteps.
We overcome these challenges with our novel HexPlane architecture. Inspired by factored representations for static scenes [5, 7, 51], a HexPlane decomposes a 4D spacetime grid into six feature planes spanning each pair of coordinate axes (e.g. XY , ZT ). A HexPlane computes a feature vector for a 4D point in spacetime by projecting the point onto each feature plane, then aggregating the six resulting feature vectors. The fused feature vector is then passed to a tiny
MLP which predicts the color of the point; novel views can then be rendered via volume rendering [42].
Despite its simplicity, a HexPlane provides an elegant solution to the challenges identified above. Due to its fac-tored representation, a HexPlane’s memory footprint only scales quadratically with scene resolution. Furthermore, each plane’s resolution can be tuned independently to ac-count for scenes requiring variable capacity in space and time. Since some planes rely only on spatial coordinates (e.g. XY ), by construction a HexPlane encourages sharing information across disjoint timesteps.
Our experiments demonstrate that HexPlane is an effec-tive and highly efficient method for novel view synthesis in dynamic scenes. On the challenging Plenoptic Video dataset [28] we match the image quality of prior work but improve training time by >100×; we also outperform prior approaches on a monocular video dataset [54]. Extensive ablations validate our HexPlane design and demonstrate that it is robust to different feature fusion mechanisms, co-ordinate systems (rectangular vs. spherical), and decoding mechanisms (spherical harmonics vs. MLP).
HexPlane is a simple, explicit, and general representa-tion for dynamic scenes.
It makes minimal assumptions about the underlying scene, and does not rely on deforma-tion fields or category-specific priors. Besides improving and accelerating view synthesis, we hope HexPlane will be useful for a broad range of research in dynamic scenes [61]. 2.