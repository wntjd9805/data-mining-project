Abstract
Foundation models have shown outstanding perfor-mance and generalization capabilities across domains.
Since most studies on foundation models mainly focus on the pretraining phase, a naive strategy to minimize a sin-gle task-specific loss is adopted for fine-tuning. However, such fine-tuning methods do not fully leverage other losses that are potentially beneficial for the target task. There-fore, we propose MEta Loss TRansformer (MELTR), a plug-in module that automatically and non-linearly com-bines various loss functions to aid learning the target task via auxiliary learning. We formulate the auxiliary learn-ing as a bi-level optimization problem and present an ef-ficient optimization algorithm based on Approximate Im-plicit Differentiation (AID). For evaluation, we apply our framework to various video foundation models (UniVL,
Violet and All-in-one), and show significant performance gain on all four downstream tasks: text-to-video retrieval, video question answering, video captioning, and multi-modal sentiment analysis. Our qualitative analyses demon-strate that MELTR adequately ‘transforms’ individual loss functions and ‘melts’ them into an effective unified loss.
Code is available at https://github.com/mlvlab/
MELTR. 1.

Introduction
Large-scale models trained on a huge amount of data have gained attention due to their adaptability to a wide range of downstream tasks. As introduced in [1], deep learning models with the generalizability are referred to as foundation models. In recent years, several foundation models for various domains have been proposed (e.g., [2, 3]
*Equal contribution.
†Corresponding author. for natural language processing, [4, 5] for images and lan-guage, and [6–8] for videos) and they mainly focus on pre-train the model often with various multiple pretext tasks.
On the other hand, strategies for fine-tuning on downstream tasks are less explored. For instance, a recently proposed video foundation model UniVL [7] is pretrained with a lin-ear combination of several pretext tasks such as text-video alignment, masked language/frame modeling, and caption generation. However, like other domains, fine-tuning is simply performed by minimizing a single target loss. Other potentially beneficial pretext tasks have remained largely unexplored for fine-tuning.
Auxiliary learning is a natural way to utilize multiple pretext task losses for learning. Contrary to multi-task learning that aims for generalization across tasks, auxiliary learning focuses only on the primary task by taking ad-vantage of several auxiliary tasks. Most auxiliary learning frameworks [9,10] manually selected auxiliary tasks, which require domain knowledge and may not always be benefi-cial for the primary task. To automate task selection, meta learning was integrated into auxiliary learning [11–13].
Here, the model learns to adaptively leverage multiple aux-iliary tasks to assist learning of the primary task. Likewise, the pretext task losses can be unified into a single auxiliary loss to be optimized in a way that helps the target down-stream task.
To this end, we propose Meta Loss Transformer (MELTR), a plug-in module that automatically and non-linearly transforms various auxiliary losses into a unified loss. MELTR built on Transformers [14] takes the tar-get task loss as well as pretext task losses as input and learns their relationship via self-attention. In other words,
MELTR learns to fine-tune a foundation model by combin-ing the primary task with multiple auxiliary tasks, and this can be viewed as a meta-learning (or ‘learning-to-learn’) problem. Similar to meta-learning-based auxiliary learning frameworks [13,15], this can be formulated as a bi-level op-timization problem, which generally involves a heavy com-putational cost due to the second-order derivative and its in-verse, e.g., the inverse Hessian matrix. To circumvent this, we present an efficient training scheme that approximates the inverse Hessian matrix. We further provide empirical analyses on the time-performance trade-off of various opti-mization algorithms.
To verify the generality of our proposed method, we ap-ply it to three video foundation models: UniVL [7], Vi-olet [16], and All-in-one [17]. These foundation models are originally pretrained with a linear combination of sev-eral pretext tasks such as text-video alignment, masked lan-guage/frame modeling, and caption generation. We exper-iment by fine-tuning on the text-to-video retrieval, video question answering, video captioning, and multi-modal sentiment analysis task with five datasets: YouCook2,
MSRVTT, TGIF, MSVD, and CMU-MOSI. For each task and dataset, our MELTR improves both previous foun-dation models and task-specific models by large margins.
Furthermore, our extensive qualitative analyses and abla-tion studies demonstrate that MELTR effectively learns to non-linearly combine pretext task losses, and adaptively re-weights them for the target downstream task.
To sum up, our contributions are threefold:
• We propose MEta Loss TRansformer (MELTR), a novel fine-tuning framework for video foundation models. We also present an efficient optimization al-gorithm to alleviate the heavy computational cost of bi-level optimization.
• We apply our framework to three video foundation models in four downstream tasks on five benchmark video datasets, where MELTR significantly outper-forms the baselines fine-tuned with single-task and multi-task learning schemes.
• We provide in-depth qualitative analyses on how
MELTR non-linearly transforms individual loss func-tions and combines them into an effective unified loss for the target downstream task. 2.