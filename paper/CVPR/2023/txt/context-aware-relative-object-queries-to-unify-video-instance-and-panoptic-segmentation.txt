Abstract
Object queries have emerged as a powerful abstraction to generically represent object proposals. However, their use for temporal tasks like video segmentation poses two questions: 1) How to process frames sequentially and prop-agate object queries seamlessly across frames. Using inde-pendent object queries per frame doesn’t permit tracking, and requires post-processing. 2) How to produce tempo-rally consistent, yet expressive object queries that model both appearance and position changes. Using the entire video at once doesn’t capture position changes and doesn’t scale to long videos. As one answer to both questions we propose ‘context-aware relative object queries’, which are continuously propagated frame-by-frame. They seamlessly track objects and deal with occlusion and re-appearance of objects, without post-processing. Further, we find context-aware relative object queries better capture position changes of objects in motion. We evaluate the proposed approach across three challenging tasks: video instance segmentation, multi-object tracking and segmentation, and video panoptic segmentation. Using the same approach and architecture, we match or surpass state-of-the art results on the diverse and challenging OVIS, Youtube-VIS, Cityscapes-VPS, MOTS 2020 and KITTI-MOTS data. 1.

Introduction
Video instance segmentation (VIS) [56] and Multi-Object
Tracking and Segmentation (MOTS) combines segmentation and tracking of object instances across frames of a given video, whereas video panoptic segmentation (VPS) requires to also pixel-wise categorize the entire video semantically.
These are challenging tasks because objects are occasion-ally partly or entirely occluded, because the appearance and position of objects change over time, and because objects may leave the camera’s field of view only to re-appear at a later time. Addressing these challenges to obtain an accurate method for the aforementioned tasks that works online is important in fields like video editing, autonomous systems, and augmented as well as virtual reality, among others.
Classically, VIS or MOTS treat every frame or clip in a video independently and associate the predictions temporally via a post-processing step [1, 3, 4, 6, 12, 19, 35, 41, 50, 56, 57].
Many of these approaches are based on object proposal gen-eration, that are used in classical detection methods [16, 43].
For image detection and segmentation, recently, query-vectors have been shown to encode accurate object propos-als [7, 9, 10]. These query-vector-based object proposals are more flexible than classical object proposals because they are not axis-aligned but rather feature-vector based. Using these accurate query vectors for images, recent methods on
VIS [22, 52] adopt the classical method of operating frame-by-frame independently, followed by a post-processing step for associating the query vectors temporally based on their similarity. It remains unclear how the query-vector-based object proposals can be seamlessly extended to the temporal domain.
Some recent transformer-based works [8, 25, 49, 51] use global object queries to process entire videos at once offline, but these methods fail to scale to long videos. However, intuitively, offline approaches should be more accurate than online methods since they operate with a much larger tem-poral context. Surprisingly, this is not the case. The best methods on VIS [18,22,52] produce query vectors frame-by-frame independently, raising the question why global query vectors fail to accurately represent objects spatio-temporally.
We study this carefully and observe that the query vectors are often too reliant on the static spatial positions of objects in a few frames. They hence fail to encode the position changes well. This over-reliance of query vectors on spatial positions has not been observed before in the context of video seg-mentation. How to address this remains an open question.
It also remains unclear how the query-vector-based object proposals can be extended to the temporal domain, while keeping the processing of frames sequential.
In a first attempt to sequentially propagate object queries, the problem of multi-object tracking was studied [38, 44, 58]. These works use separate, distinct queries to represent existing object tracks and new objects. New object queries
Figure 1. An example from the KITTI-MOTS dataset showing the need for context-aware relative object queries. Object queries from
Mask2Former-VIS [8] (top row) heavily rely on the spatial positions of objects, hence can’t reason about the position-changes of the cars in the scene. The green car in the first frame is mistaken as the red car when the original red car leaves the scene and the green car takes its spatial position. Similarly, the yellow car is first mistaken as the green car and later as the red car. Cyan boxes in the top row indicate the identity switches. Our method (bottom row) is able to retain identities of the cars despite their significant motion. are initialized each frame. However, it remains unclear how to seamlessly unify 1) the new object queries, and 2) track queries, while avoiding heuristic post-processing. the Youtube-VIS, Cityscapes VPS, MOTS 2020, and KITTI-MOTS data, demonstrating generalizability of the approach to video segmentation tasks.
Different from prior work, we develop a simple approach which propagates object queries frame-by-frame while si-multaneously refining queries via a transformer decoder.
Intuitively, the query-vectors in the proposed approach rep-resent all objects of interest in a video without the need to introduce new object queries every frame. Instead, queries are activated if the objects they represent appear in a frame.
A continuous refinement of the query-vectors permits to ad-just to gradual appearance changes. Their propagation across frames helps them carry long-term temporal information, so that they can seamlessly handle long-term occlusions or ab-sence from the camera field-of-view. While studying why global object queries are sub-optimal at encoding position changes of objects, we observed that the use of absolute po-sition encodings during self- and cross-attention causes the object queries to heavily rely on the object positions in a few frames, as illustrated in the top row of Fig. 1. To address this, we use relative positional encodings (inspired from [13]) instead of absolute encodings. The ‘relative object queries’ (queries with relative positional encodings) better encode the position changes of objects (bottom row of Fig. 1). Moreover, we use spatio-temporal context (image features from previ-ous frames and the current frame) to modulate the object queries in the transformer decoder, making them ‘context-aware.’ This permits to more holistically reason about the current frame without losing spatio-temporal details.
We evaluate the proposed approach on the challenging
VIS, VPS and MOTS tasks. We outperform methods that reason about an entire video at once by 5% and 11% on the challenging OVIS data using the Resnet-50 and Swin-L backbones. We perform similar to image or clip-based online methods which rely heavily on post-processing. We also outperform or perform close to the state-of-the-art on 2.