Abstract
This paper deals with the problem of localizing objects in image and video datasets from visual exemplars. In par-ticular, we focus on the challenging problem of egocen-tric visual query localization. We first identify grave im-plicit biases in current query-conditioned model design and visual query datasets. Then, we directly tackle such bi-ases at both frame and object set levels. Concretely, our method solves these issues by expanding limited annota-tions and dynamically dropping object proposals during training. Additionally, we propose a novel transformer-based module that allows for object-proposal set context to be considered while incorporating query information. We name our module Conditioned Contextual Transformer or
CocoFormer. Our experiments show the proposed adapta-tions improve egocentric query detection, leading to a better visual query localization system in both 2D and 3D configu-rations. Thus, we can improve frame-level detection perfor-mance from 26.28% to 31.26% in AP, which correspond-ingly improves the VQ2D and VQ3D localization scores by significant margins. Our improved context-aware query object detector ranked first and second respectively in the
VQ2D and VQ3D tasks in the 2nd Ego4D challenge. In ad-dition to this, we showcase the relevance of our proposed model in the Few-Shot Detection (FSD) task, where we also achieve SOTA results. Our code is available at https:
//github.com/facebookresearch/vq2d_cvpr. 1.

Introduction
The task of Visual Queries Localization can be described as the question, ‘when was the last time that I saw X’, where
X is an object query represented by a visual crop. In the
Ego4D [24] setting, this task aims to retrieve objects from an ‘episodic memory’, supported by the recordings from an
*Work done during an internship at Meta AI.
Figure 1. Existing approaches to visual query localization are biased. Top: only positive frames containing query object are used to train and validate the query object detector, while the model is naturally tested on whole videos, where the query object is mostly absent. Training with background frames alleviates confu-sion caused by hard negatives during inference and helps predict fewer false positives in negative frames. Bottom: visual query, target object, and distractors of three query objects from easy to hard. The confidence scores in white from a biased model get sup-pressed in our model in cyan. Our method improves the visual query localization system with fewer false positives, even for the very challenging scenario shown in the last row.
egocentric device, such as VR headsets or AR glasses. A real-world application of such functionality is localizing a user’s items via a pre-registered object-centered image of them. A functional visual query localization system will allow users to find their belongings by a short re-play, or via a 3D arrow pointing to their real-world localization.
The current solutions to this problem [24, 64] rely on a so-called ‘Siam-detector’ that is trained on annotated response tracks but tested on whole video sequences, as shown in Fig. 1 (top, gray arrows). The Siam-detector model design allows the incorporation of query exemplars by independently comparing the query to all object propos-als. During inference on a given video, the visual query is fixed, and the detector runs over all the frames in the ego-centric video recording.
Although existing methods offer promising results in query object detection performance, it still suffers from do-main and task biases. The domain bias appears due to only training with frames with well-posed objects in clear view, while daily egocentric recordings are naturally out of fo-cus, blurry, and undergoing uncommon view angles. On the other hand, task bias refers to the issue of the query object always being available during training, while in re-ality, it is absent during most of the test time. Therefore, baseline models predict false positives on distractors, espe-cially when the query object is out of view. These issues are exacerbated by the current models’ independent scoring of each object proposal, as the baseline models learn to give high scores to superficially similar objects while disregard-ing other proposals to reassess those scores.
In Fig. 1 (bottom), we show the visual query, target object, and distractors for three data samples from easy to hard. The confidence scores in white of the distract-ing objects are reported from a publicly-available ‘Siam-RCNN’ model [24]. Due to random viewpoints and the large number of possible object classes that are exhibited in egocentric recordings, the target object is hard to dis-cover and confused with high-confidence false positives. In this work, we show that these biases can be largely tack-led by training a conditioned contextual detector with aug-mented object proposal sets. Our detector is based on a hypernetwork architecture that allows incorporating open-world visual queries, and a transformer-based design that permits context from other proposals to take part in the de-tection reasoning. We call this model Conditioned Contex-tual Transformer or CocoFormer. CocoFormer has a condi-tional projection layer that generates a transformation ma-trix from the query. This transformation is then applied to the proposal features to create query-conditioned proposal embeddings. These query-aware proposal embeddings are then fed to a set-transformer [33], which effectively allows our model to utilize the global context of the correspond-ing frame. We show our proposed CocoFormer surpasses
Siam-RCNN and is more flexible in different applications as a hyper-network, such as multimodal query object detec-tion and few-shot object detection (FSD).
Our CocoFormer has the increased capability to model objects for visual query localization because it tackles the domain bias by incorporating conditional context, but it still suffers from task bias induced by the current training strat-egy. To alleviate this, we propose to sample proposal sets from both labeled and unlabeled video frames, which col-lects data closer to the real distribution. Concretely, we en-large the positive query-frame training pairs by the natural view-point change of the same object in the view, while we create negative query-frame training pairs that incorporate objects from background frames. Essentially, we sample the proposal set from those frames in a balanced manner.
We collect all the objects in a frame as a set and decouple the task by two set-level questions: (1) does the query ob-ject exist? (2) what is the most similar object to the query?
Since the training bias issue only exists in the first prob-lem, we sample positive/negative proposal sets to reduce it.
Note that this is an independent process of frame sampling, as the domain bias impairs the understanding of objects in the egocentric view, while task bias implicitly hinders the precision of visual query detection.
Overall, our experiments show diversifying the object query and proposal sets, and leveraging global scene infor-mation can evidently improve query object detection. For example, training with unlabeled frames can improve the detection AP from 27.55% to 28.74%, while optimizing the conditional detector’s architecture can further push AP to 30.25%. Moreover, the visual query system can be evalu-ated in both 2D and 3D configurations. In VQ2D, we ob-serve a significant improvement from baseline 0.13 to 0.18 on the leaderboard. In VQ3D, we show consistent improve-ment across all metrics. In the end, our improved context-aware query object detector ranked first and second respec-tively in the VQ2D and VQ3D tasks in the 2nd Ego4D chal-lenge, and achieve SOTA in Few-Shot Detection (FSD) on the COCO dataset. 2.