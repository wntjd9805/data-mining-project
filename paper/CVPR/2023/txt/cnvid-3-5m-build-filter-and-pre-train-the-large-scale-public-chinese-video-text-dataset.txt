Abstract
Owing to well-designed large-scale video-text datasets, recent years have witnessed tremendous progress in video-text pre-training. However, existing large-scale video-text datasets are mostly English-only. Though there are certain methods studying the Chinese video-text pre-training, they pre-train their models on private datasets whose videos and text are unavailable. This lack of large-scale public datasets and benchmarks in Chinese hampers the research and downstream applications of Chinese video-text pre-training. Towards this end, we release and benchmark
CNVid-3.5M, a large-scale public cross-modal dataset con-taining over 3.5M Chinese video-text pairs. We summarize our contributions by three verbs, i.e., “Build”, “Filter”, and “Pre-train”: 1) To build a public Chinese video-text dataset, we collect over 4.5M videos from the Chinese websites. 2) To improve the data quality, we propose a novel method to filter out 1M weakly-paired videos, resulting in the CNVid-3.5M dataset. And 3) we benchmark CNVid-3.5M with three mainstream pixel-level pre-training archi-tectures. At last, we propose the Hard Sample Curriculum
Learning strategy to promote the pre-training performance.
To the best of our knowledge, CNVid-3.5M is the largest public video-text dataset in Chinese, and we provide the first pixel-level benchmarks for Chinese video-text pre-training. The dataset, codebase, and pre-trained models are available at https://github.com/CNVid/CNVid-3.5M. 1.

Introduction
Owing to well-designed large-scale datasets, video-text pre-training [15, 17, 19] has achieved superior performance in various downstream tasks, such as video-text retrieval
[4, 10, 36], video question answering [27, 34, 42], and video captioning [1, 22, 30]. However, recent large-scale video-*Equal contribution.
†Corresponding author.
Figure 1. Here presents the motivations of this paper, based on which we highly summarize our contributions with three verbs:
“Build”, “Filter”, and “Pre-train”. text datasets are mostly English-only (e.g., Howto100M
[25] and WebVid-2.5M [4]). Though some methods [14,26, 45] turn to study the Chinese video-text pre-training, they pre-train their models on private datasets whose videos and text are unavailable. Therefore, the research towards
Chinese video-text pre-training is still in its infancy due to the lack of large-scale public datasets.
Towards this problem, directly translating English text into Chinese is a simple solution. However, it may result in unacceptable performance degradation for two reasons: 1) Translation errors are inevitable. Moreover, since most of the large-scale video-text datasets employ the Automatic
Speech Recognition (ASR) system to generate text, the language translator would amplify the error from the incom-plete and noisy ASR text. And 2) there remains an intrinsic linguistic gap between English and Chinese. Many widely-used English idioms and slang can hardly find their Chinese counterparts, leading some translated text incomprehensible and even contrary to the original meaning.
In this paper, we aim to release and benchmark a large-scale public Chinese video-text dataset to facilitate future researchers and the community. As illustrated in Figure 1, three verbs could highly summarize our contributions, i.e.,
“Build”, “Filter”, and “Pre-train”.
To build a large-scale Chinese video-text dataset, we collect over 4.5M videos from Chinese websites. All videos are associated with user-uploaded titles and ASR text.
We filter out the weakly-paired data by a novel method to improve the data quality. As some work [25, 26] pointed out, the pre-training performance would suffer from the noisy ASR text that fails to accurately describe the video content. Unfortunately, the problem is raised with few practical solutions. Therefore, we employ a well-trained image-text model to evaluate the video-text consistency for three reasons: 1) The text information in existing image-text datasets [11] are usually manually-written titles or captions, whose consistency is guaranteed. 2) Some video-text pre-training architectures [23,35] are based upon image-text ones. And 3) it is cheap and efficient to “hire” a well-trained model to check millions of videos. In this way, we filter out about 1M weakly-paired videos based on the balance between the pre-training performance and efficiency, deriving the proposed CNVid-3.5M dataset.
We pre-train various models to benchmark our CNVid-3.5M dataset. Current video-text pre-training methods could be roughly divided into two categories: 1) feature-level pre-training methods [24, 33, 40] that employ offline video and textual feature extractors, and 2) pixel-level ones
[4,15,36] that learn cross-modal representations end-to-end from raw videos and text. Since there remain domain gaps between pre-training datasets and frozen feature extrac-tors, pixel-level pre-training methods usually achieve better performance and have been widely employed in recent years. However, existing Chinese video-text pre-training methods [14, 26, 45] are all feature-level ones pre-trained on private datasets, limiting their contributions on the development of Chinese video-text pre-training techniques.
Hence, we adopt three mainstream pixel-level pre-training frameworks, which are the first pixel-level benchmarks for
Chinese video-text pre-training.
Moreover, we propose the novel Hard Sample Curricu-lum Learning strategy to promote the pre-training perfor-mance. Since contrastive learning is a significant compo-nent in video-text pre-training, some methods [16, 18, 43] employ the hard sample mining [12,29] strategy to promote the cross-modal alignment. However, hard sample mining would bring side effects to pre-training when the model is far from convergence. Suppose that a model is incapable of discriminating the ground-truth video-text pairs, recklessly introducing hard negatives would lead to the sub-optimal performance. Inspired by the curriculum learning [32, 37] strategy that “starts small” and gradually “learns hard”, we combine these two strategies and propose the novel Hard
Sample Curriculum Learning (HSCL). By gradually and smoothly emphasizing those hard samples, HSCL could effectively improve the pre-training performance.
Our contributions are summarized in four folds:
• To fill in the blank of large-scale public Chinese video-text datasets, we collect over 4.5M videos associated with titles and ASR text from the websites.
• To improve the data quality, we propose a novel method to filter out 1M weakly-paired videos, result-ing in the CNVid-3.5M dataset.
• To promote the pre-training performance, we propose the novel Hard Sample Curriculum Learning strategy for better cross-modal contrastive learning.
• To the best of our knowledge, the constructed
CNVid-3.5M is the largest public Chinese video-text dataset. Moreover, we provide the first Chinese pixel-level benchmarks based on CNVid-3.5M. The dataset, codebase, and benchmarks are available at https://github.com/CNVid/CNVid-3.5M. 2.