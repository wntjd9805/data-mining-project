Abstract
Federated learning enables the privacy-preserving train-ing of neural network models using real-world data across distributed clients. FedAvg has become the preferred opti-mizer for federated learning because of its simplicity and effectiveness. FedAvg uses naïve aggregation to update the server model, interpolating client models based on the num-ber of instances used in their training. However, naïve ag-gregation suffers from client drift when the data is heteroge-nous (non-IID), leading to unstable and slow convergence.
In this work, we propose a novel aggregation approach, elastic aggregation, to overcome these issues. Elastic ag-gregation interpolates client models adaptively according to parameter sensitivity, which is measured by computing how much the overall prediction function output changes when each parameter is changed. This measurement is performed in an unsupervised and online manner. Elastic aggregation reduces the magnitudes of updates to the more sensitive pa-rameters so as to prevent the server model from drifting to any one client distribution, and conversely boosts updates to the less sensitive parameters to better explore different client distributions. Empirical results on real and synthetic data as well as analytical results show that elastic aggregation leads to efficient training in both convex and non-convex settings while being fully agnostic to client heterogeneity and robust to large numbers of clients, partial participation, and imbalanced data. Finally, elastic aggregation works well with other federated optimizers and achieves significant improvements across the board. 1.

Introduction
Unlike traditional centralized learning in which models are trained using large datasets stored in a central server [15], federated learning - first proposed in [40] - leverages data spread across many clients to learn classification tasks dis-*Equal contribution.
†Corresponding author. This work is supported in part by NSFC Grants (62072449). tributively without explicitly sharing data [22, 26, 27, 42], thereby ensuring a basic level of privacy. Federated learning is characterized by four key features:
• Unreliable links: The links connecting the server and clients can be unreliable, and only a small subset of clients may be active at any given time.
• Massive distribution: The number of clients is typically high, but the amount of data per client is relatively small.
• Substantial heterogeneity: Client data is heterogeneous and non-IID [26], meaning that data across different clients can be sampled from varying regions of the sampling space.
• Imbalanced data: There can be significant imbalances in the amount of data available per client.
The most popular algorithm for federated learning is Fe-dAvg [40], which tackles the communication bottleneck by performing multiple local updates on the available clients before communicating the overall change to the server. Fe-dAvg uses naïve aggregation to interpolate client models and has shown success in certain applications. However, its performance on heterogeneous data is still an active area of research [16, 25, 33]. According to [25], training models on local data that minimize local empirical loss appears to be meaningful, but yet, doing so is fundamentally inconsistent with minimizing the global empirical loss. Client updates drive the server model away from the ideal distribution, a phenomenon known as ’client drift’. Naïve aggregation [40] is efficient in aggregating client models but does not account for distribution inconsistencies across client data or the con-sequent objective inconsistency. In other words, with naïve aggregation, the server model risks converging to a station-ary point of a mismatched objective function which can be arbitrarily different from the true objective [53].
Prior works [24, 40, 48] attempt to overcome this issue by running fewer epochs or iterations of SGD on the devices or by stabilizing server-side updates so that the resulting models correspond to inexact minimizations and keep globally desir-able properties. In this work, we propose a novel aggregation
Figure 1. Illustration of naïve aggregation and elastic aggregation. The local updates of client A and client B drive the server model θ towards their individual minima (black dots in plot). Naïve aggregation simply averages the received model from clients A and B, yielding
θ′ as the new server model. Although θ′ minimizes the local empirical loss of clients A and B, θ′ drifts from ideal distribution for the server model. Elastic aggregation adjusts gradient with respect to parameter sensitivity. Parameter θx is more sensitive (has a larger gradient norm), and is restricted with ζx < 1 to reduce the magnitude of its update. Parameter θy is less sensitive (has a smaller gradient norm), and is correspondingly boosted with ζy > 1 to better explore the parameter space. This minimizes the loss for clients A and B, while not causing the server model to drift from its ideal distribution. Hence, elastic aggregation results in a better update θ′′. approach, elastic aggregation, to overcome client drift. We measure parameter sensitivity using unlabeled samples of client data, by computing the changes to the overall function output for a given change to the parameter in question, with-out relying on the loss. This allows our method to not only avoid requiring labeled data but importantly also pre-empt complications that could otherwise arise from the loss being at a local minimum with gradients close to zero. During the aggregation of client models, updates to the more sensitive parameters can then be reduced in magnitude, preventing the server model from drifting to any client distribution.
Conversely, updates to the less sensitive parameters can be boosted to better explore different client distributions.
Contributions. Elastic aggregation tackles distribution in-consistency across client data using the concept of parameter sensitivity and is simple to implement, requiring little hy-perparameter tuning. Furthermore, parameter sensitivity is computed in an online and unsupervised manner, and thus better utilizes the unlabeled data generated by the client during runtime. Elastic aggregation is easily integrated into different federated optimizers, achieving substantial improvements over naïve aggregation. The empirical results on real and synthetic data and analytical results show that elastic aggregation leads to efficient training in both convex and non-convex settings, across all four federated learning scenarios (unreliable links, massive distribution, substantial heterogeneity, and imbalanced data). 2.