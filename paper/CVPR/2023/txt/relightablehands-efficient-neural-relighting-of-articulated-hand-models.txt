Abstract at real-time speeds. https://sh8.io/#/relightable hands
We present the ﬁrst neural relighting approach for ren-dering high-ﬁdelity personalized hands that can be ani-mated in real-time under novel illumination. Our approach adopts a teacher-student framework, where the teacher learns appearance under a single point light from images captured in a light-stage, allowing us to synthesize hands in arbitrary illuminations but with heavy compute. Using images rendered by the teacher model as training data, an efﬁcient student model directly predicts appearance under natural illuminations in real-time. To achieve generaliza-tion, we condition the student model with physics-inspired illumination features such as visibility, diffuse shading, and specular reﬂections computed on a coarse proxy geometry, maintaining a small computational overhead. Our key in-sight is that these features have strong correlation with sub-sequent global light transport effects, which proves sufﬁ-cient as conditioning data for the neural relighting network.
Moreover, in contrast to bottleneck illumination condition-ing, these features are spatially aligned based on underlying geometry, leading to better generalization to unseen illumi-nations and poses. In our experiments, we demonstrate the efﬁcacy of our illumination feature representations, outper-forming baseline approaches. We also show that our ap-proach can photorealistically relight two interacting hands
⇤This work was done during an internship at Meta 1.

Introduction
Neural rendering approaches have signiﬁcantly ad-vanced photorealistic face rendering [42, 55, 66] in recent years. These methods use deep neural networks to model the light transport on human skin [11, 14, 31, 63], directly reproducing physical effects such as subsurface scattering by reconstructing real images. However, despite the suc-cess of neural relighting, extending this approach to animat-able hand models poses a unique challenge: generalization across articulations.
Unlike faces, hands have many joints, and the state of a single joint affects all child joints. This leads to ex-tremely diverse shape variations even within a single sub-ject. Changes in pose drastically affect the appearance of hands, creating wrinkles, casting shadows, and inter-reﬂecting across topologically distant regions. Rendering these effects is challenging because sufﬁciently accurate ge-ometry and material properties required for photorealism are difﬁcult to obtain, and even then, path tracing to suf-ﬁcient accuracy is computationally expensive. The use of simpliﬁed geometric and appearance models (such as linear blend skinning and reduced material models) allow faster computation but come at a noticeable degradation in render-ing ﬁdelity. So far, photorealistic rendering of animatable
hands with global illumination effects in real-time remains an open problem.
In this work, we aim to enable photorealistic rendering of a personalized hand model that can be animated with novel poses, in novel lighting environments, and supports rendering two-hand interactions. To this end, we present the ﬁrst neural relighting framework of a parameteric 3D hand model for real-time rendering. Speciﬁcally, we build a relightable hand model to reproduce light-stage captures of dynamic hand motions.
Inspired by [4], we capture performances under spatiotemporal-multiplexed illumination patterns, where fully-on illumination is interleaved to enable tracking of the current state of hand geometry and poses. We use a two-stage teacher-student approach to learn a model that gener-alizes to natural illuminations outside of the capture system.
We ﬁrst train a teacher model that infers radiance given a point-light position, a viewing direction, and light visibil-ity. As this model directly learns the mapping between an input light position and output radiance, it can accurately model complex reﬂectance and scattering on the hand with-out the need for path tracing. To render hands in arbitrary illuminations, we treat natural illuminations as a combina-tion of distant point-light sources by using the linearity of light transport [9]. We then take renderings from the teacher model as pseudo ground-truth to train an efﬁcient student model that is conditioned on the target environment maps.
However, we found that the student model architecture used in [4] for faces leads to severe overﬁtting when applied to relightable hands. This is caused by the architecture de-sign of holistically conditioning a bottleneck representation with the target lighting environment. This representation makes it difﬁcult to reproduce geometric interactions be-tween lights and hand pose, such as those required to cast shadows from the ﬁngers onto the palm across all possible
ﬁnger conﬁgurations.
Therefore, motivated by recent neural portrait relight-ing works [42, 61], we instead propose to compute spatially aligned lighting information using physics-inspired illumi-nation features, including visibility, diffuse shading, and specular reﬂections. Because these features are based on geometry and approximate the ﬁrst bounce of light trans-port, they show strong correlation with the full appearance and provide sufﬁcient conditioning information to infer ac-curate radiance under natural illuminations. In particular, visibility plays a key role in disentangling lights and pose, reducing the learning of spurious correlations that can be present in limited training data. However, computing vis-ibility at full geometric resolution for every single light is too computationally expensive for real-time rendering. To address this, we propose using a coarse proxy mesh that shares the same UV parameterization as our hand model for computing the lighting features. We compute the features at vertices of the coarse geometry, and use barycentric inter-polation to create texel-aligned lighting features. Our fully convolutional architecture learns to compensate for the ap-proximate nature of the input features and infers both local and global light transport effects. This way, our model can render appearance under natural illuminations at real-time framerates as shown in Figure 1.
Our study shows that both integrating visibility informa-tion and spatially aligned illumination features are impor-tant for generalization to novel illuminations and poses. We also demonstrate that our approach supports rendering of two hands in real-time, with realistic shadows cast across hands.
Our contributions can be summarized as follows:
• The ﬁrst method to learn a relightable personalized hand model from multi-view light-stage data that sup-ports high-ﬁdelity relighting under novel lighting en-vironments.
• An illumination representation for parametric model relighting that is spatially aligned, leading to signiﬁ-cant improvements in generalization and accuracy of shadows under articulation.
• An efﬁcient algorithm to compute spatially-aligned lighting features with visibility and shading informa-tion incorporated using a coarse proxy mesh, enabling real-time synthesis. 2.