Abstract
Pose
Bobcut
Smile
Color
We present a novel image inversion framework and a training pipeline to achieve high-fidelity image inversion
Inverting real images with high-quality attribute editing. into StyleGAN’s latent space is an extensively studied prob-lem, yet the trade-off between the image reconstruction fi-delity and image editing quality remains an open challenge.
The low-rate latent spaces are limited in their expressive-ness power for high-fidelity reconstruction. On the other hand, high-rate latent spaces result in degradation in edit-ing quality. In this work, to achieve high-fidelity inversion, we learn residual features in higher latent codes that lower latent codes were not able to encode. This enables preserv-ing image details in reconstruction. To achieve high-quality editing, we learn how to transform the residual features for adapting to manipulations in latent codes. We train the framework to extract residual features and transform them via a novel architecture pipeline and cycle consistency losses. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improve-ments. Code: https://github.com/hamzapehlivan/StyleRes t u p n
I
] 2 3
[ e 4 e
] 4 3
[
I
G
F
H
] 5
[ e l y t
S r e p y
H
) s r u
O ( s e
R e l y t
S 1.

Introduction
Generative Adversarial Networks (GANs) achieve high quality synthesis of various objects that are hard to distin-guish from real images [14, 21, 22, 41, 43]. These networks also have an important property that they organize their la-tent space in a semantically meaningful way; as such, via latent editing, one can manipulate an attribute of a gener-ated image. This property makes GANs a promising tech-nology for image attribute editing and not only for gener-ated images but also for real images. However, for real im-ages, one also needs to find the corresponding latent code that will generate the particular real image. For this pur-pose, different GAN inversion methods are proposed, aim-ing to project real images to pretrained GAN latent space
[15, 30, 31, 33, 37]. Even though this is an extensively stud-Figure 1. Comparison of our method with e4e, HFGI, and Hy-perStyle for the pose, bob cut hairstyle, smile removal, and color change edits. Our method achieves high fidelity to the input and high quality edits. ied problem with significant progress, the trade-off between image reconstruction fidelity and image editing quality re-mains an open challenge.
The trade-off between image reconstruction fidelity and image editing quality is referred to as the distortion-editability trade-off [32]. Both are essential for real im-age editing. However, it is shown that the low-rate latent spaces are limited in their expressiveness power, and not every image can be inverted with high fidelity reconstruc-tion [1, 27, 32, 34]. For that reason, higher bit encodings
and more expressive style spaces are explored for image in-version [1, 2]. Although with these techniques, images can be reconstructed with better fidelity, the editing quality de-creases since there is no guarantee that projected codes will naturally lie in the generator’s latent manifold.
In this work, we propose a framework that achieves high fidelity input reconstruction and significantly improved ed-itability compared to the state-of-the-art. We learn residual features in higher-rate latent codes that are missing in the reconstruction of encoded features. This enables us to re-construct image details and background information which are difficult to reconstruct via low rate latent encodings. Our architecture is single stage and learns the residuals based on the encoded features from the encoder and generated fea-tures from the pretrained GANs. We also learn a module to transform the higher-latent codes if needed based on the generated features (e.g. when the low-rate latent codes are manipulated). This way, when low-rate latent codes are edited for attribute manipulation, the decoded features can adapt to the edits to reconstruct details. While the attributes are not edited, the encoder can be trained with image recon-struction and adversarial losses. On the other hand, when the image is edited, we cannot use image reconstruction loss to regularize the network to preserve the details. To guide the network to learn correct transformations based on the generated features, we train the model with adversarial loss and cycle consistency constraint; that is, after we edit the latent code and generate an image, we reverse the edit and aim at reconstructing the original image. Since we do not want our method to be limited to predefined edits, during training, we simulate edits by randomly interpolating them with sampled latent codes.
The closest to our approach is HFGI [34], which also learns higher-rate encodings. Our framework is different as we learn a single-stage architecture designed to learn fea-tures that are missing from low-rate encodings and we learn how to transform them based on edits. As shown in Fig. 1, our framework achieves significantly better results than
HFGI and other methods in editing quality. In summary, our main contributions are:
• We propose a single-stage framework that achieves high-fidelity input embedding and editing. Our framework achieves that with a novel encoder architecture.
• We propose to guide the image projection with cycle con-sistency and adversarial losses. We edit encoded images by taking a step toward a randomly sampled latent code.
We expect to reconstruct the original image when the edit is reversed. This way, edited images preserve the details of the input image, and edits become high quality.
• We conduct extensive experiments to show the effective-ness of our framework and achieve significant improve-ments over state-of-the-art for both reconstruction and real image attribute manipulations. 2.