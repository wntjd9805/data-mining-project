Abstract
Continual learning requires a model to incrementally learn a sequence of tasks and aims to predict well on all the learned tasks so far, which notoriously suffers from the catastrophic forgetting problem. In this paper, we find a new type of bias appearing in continual learning, coined as task-induced bias. We place continual learning into a causal framework, based on which we find the task-induced bias is reduced naturally by two underlying mechanisms in task and domain incremental learning. However, these mecha-nisms do not exist in class incremental learning (CIL), in which each task contains a unique subset of classes. To eliminate the task-induced bias in CIL, we devise a causal intervention operation so as to cut off the causal path that causes the task-induced bias, and then implement it as a causal debias module that transforms biased features into unbiased ones. In addition, we propose a training pipeline to incorporate the novel module into existing methods and jointly optimize the entire architecture. Our overall ap-proach does not rely on data replay, and is simple and convenient to plug into existing methods. Extensive em-pirical study on CIFAR-100 and ImageNet shows that our approach can improve accuracy and reduce forgetting of well-established methods by a large margin. 1.

Introduction
Deep learning has been widely applied in many fields like computer vision, social media analytics, natural lan-guage processing, etc. The standard paradigm of deep learning is to train models on a prepared dataset, of which all data are accessible in the whole training stage. However, in most real-world applications, a sequence of tasks arrives incrementally, which requires models to learn continuously from a new task, namely continual learning [27]. Despite fine-tuning achieves this goal somewhat, such na¨ıve method forgets obtained knowledge of previous tasks after learning
*Corresponding authors.
Figure 1. Illustration of CIL, task-induced bias and their detrimen-tal effects. (a) Illustration of CIL. (b) Task-induced bias between class features. Two classes in a task are correlated by a task iden-tifier. (c) Task-induced bias between images and labels. Images and labels in a task are correlated by a task identifier. In (b)(c), the first column describes perspectives of task-induced bias, the sec-ond shows the underlying causal effect causing task-induced bias, and the last illustrates adverse influence of task-induced bias. a new task, dubbed as catastrophic forgetting [18].
Much effort has been devoted as a remedy for alleviating this problem in literature [2, 3, 11–14, 16, 33]. One general direction is to preserve knowledge representations of pre-vious tasks by knowledge distillation [10] and by storing a few observed exemplars for replay [22]. [11] discovers the class imbalance between old classes and new ones is a key challenge of large-scale applications in class incremental learning (CIL), in which classes of a new task never appear in the previous tasks, as illustrated in Fig. 1 (a). [2,7,31,35] all follow this direction and propose effective methods to overcome the class imbalance. However, these methods ig-nore bias problems in continual learning. In this work, we
consider the problem of CIL in a novel perspective of bias.
We think that catastrophic forgetting is the reflection of a new-type bias, coined as task-induced bias.
Deep neural networks are easily misled by language bias [1] and vision bias [6, 9, 29], which are both derived from spurious correlations of two or more items in differ-ent positions of a sentence or an image. In contrast to these two bias, the task-induced bias occurs across different im-In CIL, data of specific classes ages in a common task. are accessible only in a certain task if replay buffer is not applied. A deep neural network is capable of learning cor-relations among images of different categories or between images and labels as possible as it can, which includes a few spurious correlations, namely biases, leading to a limited generalization. On the one hand, a portion of these image correlations can be shared beneficially in this task, but these benefits cannot share among different tasks. For example, in
Fig. 1 (b), color is sufficient to discriminate elephants and leopards in a specific task since elephants are grey while leopards are orange, and thus deep models have no moti-vation to capture more complex characteristic, like stripe.
This is detrimental to the feature extractor because it cap-tures a few details that cannot generalize among all tasks.
On the other hand, as shown in Fig. 1 (c), a classifier will be biased to new classes in the current task [11, 31, 35], be-cause the classes in the training data are all the new classes and to predict new classes is more possible to be correct.
To better understand the task-induced bias in continual learning, we model three continual learning scenarios [27] into a common causal inference framework, which merely contains three necessary variables: image, label, and task identifier. Thanks to the framework, we discover that the task-induced bias is minor in task incremental and domain incremental learning due to innate mechanisms that cut off an inappropriate causal path, which partially explain why the two scenarios are easier to tackle than CIL [27]. For the existence of the causal path in CIL, we seek an ac-tive causal intervention to close the path. To implement the intervention, we devise a simple but effective causal debias module, which is established on attention mecha-nisms [28]. Subsequently, we incorporate this module into existing CIL architectures and propose a training pipeline to optimize it. Our overall approach transforms causally bi-ased features into unbiased ones, and hence we refer to it as
Casual Feature Boost (CafeBoost). Finally, we systemat-ically compare accuracy and forgetting of well-established methods with ours in various settings. Experimental results show that CafeBoost yields significant and consistent per-formance boost for existing methods in CIFAR-100 and Im-ageNet by a large margin (0.67%∼ 12.08% on accuracy). 2.