Abstract
In this work, we show that it is feasible to perform multi-ple tasks concurrently on point cloud with a straightforward yet effective multi-task network. Our framework, Poly-PC, tackles the inherent obstacles (e.g., different model architec-tures caused by task bias and conflicting gradients caused by multiple dataset domains, etc.) of multi-task learning on point cloud. Specifically, we propose a residual set abstrac-tion (Res-SA) layer for efficient and effective scaling in both width and depth of the network, hence accommodating the needs of various tasks. We develop a weight-entanglement-based one-shot NAS technique to find optimal architec-tures for all tasks. Moreover, such technique entangles the weights of multiple tasks in each layer to offer task-shared parameters for efficient storage deployment while providing ancillary task-specific parameters for learning task-related features. Finally, to facilitate the training of Poly-PC, we introduce a task-prioritization-based gradient balance al-gorithm that leverages task prioritization to reconcile con-flicting gradients, ensuring high performance for all tasks.
Benefiting from the suggested techniques, models optimized by Poly-PC collectively for all tasks keep fewer total FLOPs and parameters and outperform previous methods. We also demonstrate that Poly-PC allows incremental learning and evades catastrophic forgetting when tuned to a new task. 1.

Introduction
With the advances in deep learning, modern architec-tures offer tremendous improvements in 3D understand-ing [29, 36, 39, 54], e.g., point classification, segmentation, and detection, etc. Nevertheless, these networks are inef-ficient when handling numerous tasks since they are often intended to accomplish a single task. Even if parallel com-puting can address this issue, the memory footprints and storage costs grow linearly with the number of networks,
*Corresponding author. rendering them unaffordable with constrained resources.
Multitask learning (MTL) [3, 12, 13] offers a solution to this difficulty. In vision tasks, MTL models have been pre-dominantly proposed to simultaneously perform depth esti-mation, surface normal estimation, and semantic segmenta-tion on an input image [14, 19, 52]. Besides, the joint part-of-speech tagging, chunking, and named-entity recognition for Natural Language Processing (NLP) have also been in-vestigated [8, 9]. Since a substantial piece of the network (i.e., the backbone) is shared among tasks, an MTL model offers benefits in terms of complexity, inference time, and learning efficiency. However, training multiple tasks for point cloud poses two key challenges: 1) In contrast to common vision tasks, where a backbone that performs well on image classification can be directly ported to other tasks, the backbone for point cloud tasks must be carefully developed. Consequently, it is not feasible for all point cloud tasks to directly share a single backbone. 2) Instead of using a multi-task dataset as input, we seek to jointly perform multiple tasks on point cloud with mul-tiple dataset domains as input. Thus, in such a circum-stance, multi-task learning would result in considerable dis-parities in directions and magnitude of different task gradi-ents, a phenomenon known as task interference or negative transfer [34]. Meanwhile, task difficulty induced by multi-ple dataset domains is also different, so task prioritization should be considered to prevent placing undue emphasis on easier tasks when optimizing the multi-task network.
To address the first challenge, we introduce residual set abstraction (Res-SA) layer, a scalable point feature learning module that can adapt to requirements for a variety of tasks in terms of width and depth of the network. Simultane-ously, when multiple tasks are presented to us, to reduce the manpower loss caused by manually designing the network, we seek to find the optimal architecture for each task us-ing neural network search (NAS). Thus, we construct differ-ent search spaces (neighbour points number, group radius, width, and depth, etc.) for multiple tasks on Res-SA. Then, inspired by AutoFormer [4], BigNAS [49] and slimmable
networks [48], we propose weight-entanglement-based one-shot NAS technique that entangles the weights of different tasks in the same layer, enabling different tasks to share parameters in their common parts for efficient storage de-ployment and offering task-specific parameters for learn-ing task-related features. Moreover, unlike previous works that share or monopolize all parameters in one layer for all tasks, such strategy allows different tasks to share a certain proportion of parameters in a single layer, achieving fine-grained parameter sharing.
For negative transfer, previous methods narrow down the problem to two types of differences (i.e., gradient magni-tudes and directions) among task gradients, and propose several algorithms [5, 6, 12, 18, 35, 50] to homogenize the differences. However, these methods exclusively focus on one aspect of task gradient differences and also disregard the difficulty of various tasks.
Intuitively, it is conceiv-able for easy tasks to dominate learning while harder ones stagnate during multi-task training. In response, we intro-duce a task-prioritization-based gradient balance algorithm that resolves negative transfer as a whole by homogeniz-ing both gradient magnitudes and directions across tasks via task prioritization. Specifically, we evaluate task prior-itization using the previous loss record. Then, we use task prioritization to homogenize task gradient magnitudes and directions in the current epoch, endowing difficult task with larger grad scale and enabling direction of final merged gra-dient vector closer to that of the difficult task. Since the task prioritization is dynamically adjusted, our algorithm avidly concentrates on the difficult task learning at each epoch and ensures that each task converges to the optimal solution.
Over the well-trained Poly-PC, we undertake an evolu-tionary search with model size constraint to identify promis-ing architectures for different tasks. Experiments show that the searched models with weights inherited from the super-net outperform several baselines and are comparable with the state-of-the-arts trained individually for specific tasks.
We also demonstrate that Poly-PC allows incremental learn-ing and evades catastrophic forgetting when generalizing to a new task. Thus, Poly-PC is parameter-efficient and can scale up more gracefully as the number of tasks increases.
The key contributions can be summarized as follows: 1) We propose Poly-PC to perform multi-task learning on point cloud. To the best of our knowledge, Poly-PC is the first framework that takes multiple dataset domains as in-put for multi-task learning on point cloud. 2) We intro-duce Res-SA layer that meets the needs of different tasks in both the width and depth of the network. 3) We de-velop weight-entanglement-based one-shot NAS technique to find optimal architectures for different tasks as well as shared parameters inside each layer for efficient storage. 4)
We propose task-prioritization-based gradient balance algo-rithm that resolves negative transfer as a whole to promote the training of Poly-PC. 5) We demonstrate that Poly-PC allows incremental learning with fewer parameters. 2.