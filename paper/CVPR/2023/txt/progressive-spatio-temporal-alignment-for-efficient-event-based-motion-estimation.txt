Abstract
In this paper, we propose an efficient event-based motion estimation framework for various motion models. Differ-ent from previous works, we design a progressive event-to-map alignment scheme and utilize the spatio-temporal cor-relations to align events. In detail, we progressively align sampled events in an event batch to the time-surface map and obtain the updated motion model by minimizing a novel time-surface loss. In addition, a dynamic batch size strat-egy is applied to adaptively adjust the batch size so that all events in the batch are consistent with the current mo-tion model. Our framework has three advantages: a) the progressive scheme refines motion parameters iteratively, achieving accurate motion estimation; b) within one iter-ation, only a small portion of events are involved in opti-mization, which greatly reduces the total runtime; c) the dynamic batch size strategy ensures that the constant ve-locity assumption always holds. We conduct comprehen-sive experiments to evaluate our framework on challeng-ing high-speed scenes with three motion models: rotational, homography, and 6-DOF models. Experimental results demonstrate that our framework achieves state-of-the-art estimation accuracy and efficiency. The code is available at https://github.com/huangxueyan/PEME. 1.

Introduction
Event cameras [25, 30, 33], also known as bio-inspired silicon retinas, are novel vision sensors that asynchronously respond to pixel-wise brightness changes. Event cameras have the properties of high temporal resolution and high dy-namic range, which make event cameras appealing to tackle many computer vision tasks under challenging conditions, such as high-speed pose estimation [5, 18, 19], HDR video generation [27, 31, 32, 34], 3D reconstruction [9, 11, 26] and low-latency motion estimation [7, 10, 24].
Event-based motion estimation aims to find the ego-*Corresponding author (a): A segment
Figure 1. Event-based motion estimation. of the event cloud generated by rotational motion from the shapes rotation dataset [20]. (b): An event frame generated from unaligned events. (c): An event frame generated from aligned events. motion of the event camera. Since events can be triggered by the motion of the camera, the alignment of events is highly correlated with the camera’s motion. The motion es-timation problem is normally cast to an optimization prob-lem based on the alignment of events [4]. With correct mo-tion parameters, events triggered by the same world point can be aligned to the same pixel, forming an event frame with sharp edges. As for unaligned events, they generate blurred edges in the event frame. Fig. 1 shows a segment of the event cloud as well as two event frames with unaligned and aligned events.
Many approaches have been proposed for event-based motion estimation, such as contrast maximization, entropy minimization, and Poisson point process [6, 8, 22]. These methods follow the same procedure: slice an event cloud into batches with a fixed size or a fixed time interval, and then optimize a loss function with all the events in the batch.
In practice, an event batch usually contains tens of thou-sands of events. It is very time-consuming and computa-tionally redundant to involve such an amount of event data to optimize a motion transformation with 3 or 6 degrees of freedom (DOF). We observe that events approximately fol-low the same motion transformation in a short period; thus, it is probably not necessary to take all the events into ac-count for motion estimation. From this point of view, we attempt to utilize sampled events to reduce the computa-tional burden. To achieve this, we propose a distinct event-to-map alignment scheme. In specific, we construct a time-surface (TS) map that maintains the timestamps of the for-mer events at each pixel, and warp the later events to align the former events in the TS map. We measure the degree of alignment by a novel TS loss and update the motion param-eters by minimizing the TS loss. We observe that in a short time interval, the events triggered by the same world point differ slightly in the timestamps and the coordinates, yield-ing almost identical residuals and gradient directions when evaluating the TS loss. Therefore, we can greatly reduce the computational burden by evaluating the TS loss with a small fraction of events.
In the alignment procedure, we attempt to warp the later events backward to the start timestamp tstart and align them with the former events. But few of the former events are triggered at tstart due to the spatial sparsity of the event camera. Moreover, these former events are normally trig-gered with a slight time shift from tstart, resulting in a slight drift in the coordinates. To fix this issue, we propose an iter-ative scheme to update the coordinates of the former events in the TS map with the latest motion parameters and pro-gressively evaluate the TS loss based on the latest TS map.
In addition, the choice of event batch size has a signifi-cant impact on the accuracy. In practice, the batch size or the batch time interval is set manually, which is mainly de-termined by two constraints: one is that events in the batch should share the same motion parameters, i.e., the time in-terval of the batch must be short enough to hold the con-stant velocity assumption, and the other is that the batch must contain sufficient events for the algorithm to execute normally. Essentially, these two constraints are mutually exclusive, making it difficult to determine the global batch size or time interval. To address this problem, we propose a dynamic batch strategy that can dynamically adjust the batch size to ensure that the constant velocity assumption always holds in this batch. We slice unprocessed events into event bundles with a small size and append these event bundles into the event batch if they meet the requirement that their overlap ratio reaches a threshold; otherwise, we stop merging and output an event batch with a certain num-ber of bundles. With this strategy, our algorithm can adapt to the scenes under different conditions, such as different scene texture richness, camera motion speeds, and camera spatial resolutions, while for the fixed-size methods, they need to re-adjust the batch size to accommodate these scene changes.
We summarize the contributions of this work in the fol-lowing.
• We present a unified event-based motion estimation framework that progressively aligns events using a novel event-to-map scheme with spatio-temporal in-formation of sampled events.
• We also propose a dynamic batch size strategy to ensure that the constant velocity assumption always holds, which is more generalizable to different scene textures, camera speeds, and camera resolutions com-pared to the fixed batch strategy.
• Comprehensive experimental results demonstrate that our framework achieves state-of-the-art performance both in terms of accuracy and efficiency on publicly available datasets with three motion models.
• By utilizing a small number of sampled events in each iteration, our framework is able to achieve real-time implementation for the rotational model and the 6-DOF model with standard CPUs. 2.