Abstract
ConV
ConV
ConV
Target
Estimation
Most current RGB-T trackers adopt a two-stream struc-ture to extract unimodal RGB and thermal features and complex fusion strategies to achieve multi-modal feature fusion, which require a huge number of parameters, thus hindering their real-life applications. On the other hand, a compact RGB-T tracker may be computationally efﬁ-cient but encounter non-negligible performance degrada-tion, due to the weakening of feature representation abil-ity. To remedy this situation, a cross-modality distillation framework is presented to bridge the performance gap be-tween a compact tracker and a powerful tracker. Speciﬁ-cally, a speciﬁc-common feature distillation module is pro-posed to transform the modality-common information as well as the modality-speciﬁc information from a deeper two-stream network to a shallower single-stream network. In addition, a multi-path selection distillation module is pro-posed to instruct a simple fusion module to learn more ac-curate multi-modal information from a well-designed fusion mechanism by using multiple paths. We validate the effec-tiveness of our method with extensive experiments on three
RGB-T benchmarks, which achieves state-of-the-art perfor-mance but consumes much less computational resources. 1.

Introduction
RGB-T tracking is the task of estimating the state of an arbitrary target in each frame of an RGB-T video se-quence [35]. Due to the affordability of thermal infrared (TIR) sensors, RGB-T tracking draws more and more re-search interest.
As shown in Fig. 1 (a), most existing RGB-T tracking models ﬁrst adopt a two-stream structure to extract multi-level unimodal RGB and TIR features, respectively, and then employ elaborate-designed multi-modal feature fusion modules to exploit complementary information within the
*Corresponding author.
Complex
Fusion
Complex
Fusion
Complex
Fusion
ConV
ConV
ConV (a) Two-stream Structure
Teacher: 
Two-stream Feature Extractor
& Complex Fusion
Cross-Modality Distillation
Student: 
Single-stream Feature Extractor
& Simple Fusion (c) Our proposed method
ConV
Simple
Fusion
Shared
ConV
Simple
Fusion
ConV
Simple
Fusion
Target
Estimation
ConV
ConV
Shared (b) Single-stream Structure 
ConV
ConV
Conv block in RGB feature extractor
ConV
Conv block in TIR feature extractor
Target
Estimation
ConV Conv block in single-stream feature extractor
Fusion
Fusion Module
RGB data flow
TIR data flow
TIR & RGB data flow
Figure 1. Architectures of different RGB-T tracking models. (a)
Two-stream structure. (b) Single-stream structure. (c) Our pro-posed method. multi-modal data. Finally, they deduce the target state, of-ten represented by a bounding box, from the fused features.
Although great progress has been made, these powerful
RGB-T tracking models usually require high computational costs and large model sizes to handle the information of two modalities in the stages of unimodal feature extraction and multi-modal feature fusion.
There are two straightforward solutions to tackle the complexity and efﬁciency issues. One is to adopt a single-stream feature extractor with fewer convolutional layers, and the other is to employ simpler multi-modal feature fu-sion modules, as shown in Fig. 1 (b). Although such com-pact models can reduce computational complexity, they in-evitably bring non-negligible performance degradation due to the weakening of unimodal feature representation ability and multi-modal complementary information exploration ability. For instance, a powerful RGB-T tracker [35] with a two-stream structure and complicated multi-modal feature fusion modules suffers from severe performance degrada-tion after the above model simpliﬁcation operations (84.4% precision rate vs 78.1% precision rate on RGBT234 dataset
[11]), as shown in Fig. 2.
Now, the research question becomes: can we shrink the RGB-T tracker without sacriﬁcing performance? This paper answers this question using knowledge distillation, which allows a compact model to obtain a similar abil-ity of a complex model at little cost. We call this com-plex but powerful model the teacher model, and call this
85.5 84.5 83.5 82.5 81.5 80.5 79.5 78.5 77.5
)
% ( e t a
R n o i s i c e r
P
Teacher 10M 20M 40M
Student3
Student2
Student1
Student1: Student + SCFD
Student2: Student + SCFD + MPSD
Student3: Student + SCFD + MPSD+HFRD
Student 15 17 19 21 23 25 27 29 31 33
Model Speed (FPS)
Figure 2. Experimental results of different RGB-T tracking struc-tures on RGBT234 dataset [11]. Teacher denotes the two-stream structure with complex fusion modules. Student denotes a single-stream structure with simple fusion operations. The teacher model employs ResNet50 [7] for feature extraction and fusion modules in [35] for multi-modal feature fusion, respectively. The student model employs ResNet18 [7] for feature extraction and concate-nation for multi-modal feature fusion, respectively. compact model the student model. Although some works
[15, 19, 20, 29] have made considerable progress on knowl-edge distillation in multi-modal tasks, they fail to conduct a deep investigation on the huge feature differences between teacher and student in the unimodal feature extraction stage as well as in the multi-modal feature fusion stage, thereby resulting in suboptimal efﬁciency of the knowledge trans-formation. For that, a novel teacher-student knowledge dis-tillation training framework, named Cross-Modality Dis-tillation (CMD), is proposed to elaborately guide efﬁcient imitation from three stages: unimodal feature extraction, multi-modal feature fusion and target estimate estimation, as shown in Fig. 1 (c).
Speciﬁcally, in the stage of unimodal feature extraction, as pointed out by many previous works [9, 17], the shal-lower layers of unimodal features usually contain abun-dant low-level spatial details, which are usually modality-dependent. Differently, the deeper layers of unimodal fea-tures often contain many high-level semantic cues, which tend to be strongly modality-consistent. The student model uses a compact single-stream network to extract both RGB features and TIR features, which not only lacks the ability to extract modality-speciﬁc information in the shallower lay-ers, but also insufﬁciently explores the modality-common information in the deeper layers. These interesting observa-tions inspire us to design a Speciﬁc-common Feature Dis-tillation (SCFD) module, which transforms the modality-speciﬁc information as well as the modality-common in-formation from a two-stream deeper network to a single-stream shallower network.
Second, in the stage of multi-modal feature fusion, the complex multi-modal feature fusion modules in the teacher model show great advantages in various scenarios, while the simple fusion strategies in the student model are usu-ally effective in some speciﬁc scenarios. It is difﬁcult for a student model with a single simple fusion strategy to learn more effective complementary information mining capabil-ities from a complex teacher model due to the large fea-ture differences. Therefore, we design a fusion module with multiple simple fusion strategies in the student model, de-noted as Multi-path Selection Distillation (MPSD) module.
In the process of learning from the teacher model, the stu-dent model can adaptively combine different types of fusion features to make up for the lack of complementary informa-tion mining capabilities of a single simple fusion strategy.
Finally, in the stage of target state estimation, with the weakening of the feature representation ability of the stu-dent model, the discriminative ability of the tracker for distractors is also reduced. For that, we further present a
Hard-focused Response Distillation (HFRD) module to im-prove the student model’s discriminative ability by alleviat-ing the problem of data imbalance between the targets and the backgrounds, which employs the response maps gener-ated by the teacher model to instruct the student to focus on distinguishing targets from hard negative samples.
As shown in Fig. 2, each of our proposed modules con-tinuously reduces the performance gap between the student model and the teacher model without increasing the number of parameters obviously. To sum up, our work improves an
RGB-T tracker dramatically because of the following two contributions:
• A Cross-Modality Distillation (CMD) framework is presented to bridge the performance gap between a compact student model and a powerful teacher model through three stages, i.e, unimodal feature extraction, multi-modal feature fusion and target state estimation.
To the best of our knowledge, we are the ﬁrst to intro-duce knowledge distillation for multi-modal tracking.
• Experimental results show that our proposed approach helps a student model achieves the state-of-the-art per-formance on the challenging GTOT [10], RGBT234
[11] and LasHer [14], while reducing the number of parameters and computational complexity. 2.