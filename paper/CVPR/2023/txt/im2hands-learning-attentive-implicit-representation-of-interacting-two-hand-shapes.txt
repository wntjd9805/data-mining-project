Abstract 1.

Introduction
We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Un-like existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes,
Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context be-tween two hands, Im2Hands models the occupancy volume of two hands – conditioned on an RGB image and coarse 3D keypoints – by two novel attention-based modules respon-sible for (1) initial occupancy estimation and (2) context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canon-ical space designed for each hand using query-image at-It then refines the initial two-hand occupancy in tention. the posed space to enhance the coherency between the two hand shapes using query-anchor attention.
In addi-tion, we introduce an optional keypoint refinement mod-ule to enable robust two-hand shape estimation from pre-dicted hand keypoints in a single-image reconstruction sce-nario. We experimentally demonstrate the effectiveness of
Im2Hands on two-hand reconstruction in comparison to re-lated methods, where ours achieves state-of-the-art results.
Our code is publicly available at https://github. com/jyunlee/Im2Hands.
Humans use hand-to-hand interaction in everyday activ-ities, which makes modeling 3D shapes of two interact-ing hands important for various applications (e.g., human-computer interaction, robotics, and augmented or virtual re-ality). However, the domain of two-hand shape reconstruc-tion remains relatively under-explored, while many exist-ing studies have put efforts into single-hand reconstruction from RGB [1, 2, 4, 13, 21, 24, 44], depth [38], or sparse key-points [18, 44]. These single hand-based methods are not effective when directly applied for interacting two-hand re-construction, since it introduces additional challenges in-cluding inter-hand collisions and mutual occlusions.
Recently, reconstruct few learning-based methods on two-hand shape reconstruction [23, 32, 42] have been proposed since the release of the large-scale interacting hand dataset (i.e., InterHand2.6M [27]). Two-Hand-Shape-Pose [42] and IHMR [32] two-hands by estimating
MANO [31] parameters, which are later mapped to triangu-lar hand meshes using a pre-defined statistical model (i.e.,
MANO). IntagHand [23] directly regresses a fixed number of mesh vertex coordinates using a graph convolutional net-work (GCN). These methods mainly model the shape of two interacting hands based on a low-resolution mesh represen-tation with a fixed topology of MANO (please refer to Fig-ure 1).
In this paper, we present
Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike the existing mesh-based two-hand reconstruction methods, Im2Hands can capture the fine-grained geometry of two interacting hands by learn-ing a continuous 3D occupancy field. Im2Hands (1) pro-duces two-hand meshes with an arbitrary resolution, (2) does not require dense vertex correspondences or statisti-cal model parameter annotations for training, and (3) learns output shapes with precise hand-to-hand and hand-to-image alignment. As two interacting hands are highly articulated objects, we take inspiration from recent neural articulated implicit functions [7, 8, 18, 25, 29, 34] that learn an implicit geometry leveraging the object canonical space computed from an input pose observation. Our two-hand articulated implicit function is also driven by input pose and shape ob-servations, which are represented as sparse 3D keypoints and an RGB image, respectively.
To effectively handle the shape complexity and interac-tion context between two hands, Im2Hands consists of two novel attention-based modules responsible for initial hand occupancy estimation and context-aware two-hand occu-pancy refinement, respectively. The initial occupancy es-timation network first predicts the articulated occupancy volume of each hand in the canonical space. Given a 3D query point, it (1) performs query canonicalization using the keypoint encoder of HALO [18] to effectively capture pose-dependent hand deformations and (2) extracts a hand shape feature using our novel query-image attention module to model shape-dependent hand deformations. As it is non-trivial to model two-hand interaction while learning in the canonical space defined for each hand, our context-aware occupancy refinement network modifies the initial two-hand occupancy in the original posed space to enhance hand-to-hand coherency. Given the initial two-hand shape repre-sented as anchored point clouds, it uses query-anchor at-tention to learn a refined two-hand occupancy in a context-aware manner. Furthermore, we consider a practical sce-nario of two-hand reconstruction using Im2Hands from sin-gle images, where no ground truth keypoints are observed as inputs to our method. To this end, we introduce an op-tional input keypoint refinement network to enable more robust two-hand shape reconstruction by alleviating errors in the input 3D keypoints predicted from an off-the-shelf image-based two-hand keypoint estimation method (e.g.,
[11, 20, 23, 27, 42]).
Overall, our main contributions are summarized as fol-lows:
• We introduce Im2Hands, the first neural implicit rep-resentation of two interacting hands. Im2Hands recon-structs resolution-independent geometry of two-hands with high hand-to-hand and hand-to-image coherency.
• To effectively learn an occupancy field of the complex two-hand geometries, we propose two novel attention-based modules that perform (1) initial occupancy es-timation in the canonical space and (2) context-aware occupancy refinement in the original posed space, re-spectively. We additionally introduce an optional key-point refinement module to enable more robust two-hand shape estimation using a single image input.
• We demonstrate the effectiveness of Im2Hands in comparison to the existing (1) two-hand mesh-based and (2) single-hand implicit function-based recon-struction methods, where Im2Hands achieves state-of-the-art results in interacting two-hand reconstruction. 2.