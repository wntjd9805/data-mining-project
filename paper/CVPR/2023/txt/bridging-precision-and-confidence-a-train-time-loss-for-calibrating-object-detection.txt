Abstract
Deep neural networks (DNNs) have enabled astound-ing progress in several vision-based problems. Despite showing high predictive accuracy, recently, several works have revealed that they tend to provide overconfident pre-dictions and thus are poorly calibrated. The majority of the works addressing the miscalibration of DNNs fall un-der the scope of classification and consider only in-domain predictions. However, there is little to no progress in study-ing the calibration of DNN-based object detection models, which are central to many vision-based safety-critical ap-plications. In this paper, inspired by the train-time calibra-tion methods, we propose a novel auxiliary loss formulation that explicitly aims to align the class confidence of bound-ing boxes with the accurateness of predictions (i.e. preci-sion). Since the original formulation of our loss depends on the counts of true positives and false positives in a mini-batch, we develop a differentiable proxy of our loss that can be used during training with other application-specific loss functions. We perform extensive experiments on challeng-ing in-domain and out-domain scenarios with six bench-mark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. Our results reveal that our train-time loss surpasses strong calibration baselines in reducing calibra-tion error for both in and out-domain scenarios. Our source code and pre-trained models are available at https:// github.com/akhtarvision/bpc_calibration 1.

Introduction
Deep neural networks (DNNs) have shown remarkable results in various mainstream computer vision tasks, in-cluding image classification [5, 10, 33], object detection
[29, 36, 39], and semantic segmentation [2, 34]. However, some recent works [9,26] show that these deep models have the tendency to provide overconfident predictions. This greatly limits the overall trust in their predictions, especially when they are part of the decision-making system in safety-Figure 1. Comparison in terms of Detection Expected Calibra-tion Error (D-ECE) on in-domain Cityscapes (CS) and out-domain
FoggyCS datasets. (a) Detection model trained with our proposed
BPC loss provides the lowest D-ECE (lower is better). (b) Post-hoc and train-time calibration methods for classification: MDCA
[11] and MbLS [25] are sub-optimal for object detection. In con-trast, BPC loss better calibrates in-domain and out-domain detec-tions. critical applications [6, 8, 32]. For instance, a decision sys-tem in an AI-powered healthcare diagnostic application can safely reject predictions with low confidence, however, if it mistakenly skips reviewing an incorrect prediction with high confidence, it can lead to serious consequences.
An important underlying reason behind the miscalibra-tion of DNNs is training with zero-entropy supervision sig-nal which makes them overconfident, and thus inadvertently miscalibrated. There have been few attempts towards im-proving the model calibration. A prominent technique is based on a post-processing step that transforms the out-puts of a trained model with parameter(s) learned on a held-out validation set [9, 14, 15, 19, 28]. Although sim-ple to implement, these methods are architecture and data-dependent [25], and further requires a separate held-out val-idation set which is not readily available in many real-world applications. An alternative approach is a train-time cal-ibration method which tends to involve all model param-eters during training. Existing train-time calibration meth-ods [11,18,22,25] propose an auxiliary loss term that can be used in conjunction with an application-specific loss func-tion (e.g., Cross Entropy or Focal loss [26]). Recently, [11] propose a differentiable auxiliary loss formulation to cali-brate the class confidence of both the predicted label along with non-predicted labels.
Almost all work towards improving model calibration target the task of classification [9, 11, 20, 22, 25]. However, the calibration of object detection models has not been ac-tively explored. Similar to classification models, object de-tection models also occupy an important position in many safety-critical applications. For instance, they form an in-tegral part of the perception component of self-driving ve-hicles. Furthermore, the majority of efforts tackling model calibration focus on calibrating in-domain predictions. A deployed deep learning-based model can encounter sam-ples from a distribution that is radically different from the training distribution. Therefore, a real-world model should be well-calibrated for both in-domain and out-domain pre-dictions. So, in essence, well-calibrated object detectors, particularly under distribution shifts, not only contribute to algorithmic advancements but are of great importance to many vision-based safety-critical applications.
In this paper, we study the calibration of object detection models for both in-domain and out-domain predictions. We observe that the recent state-of-the-art object detectors are rather miscalibrated when compared to their predictive ac-curacy (Fig. 1). To this end, inspired by the train-time cal-ibration approaches [11, 18, 25], we propose a novel train-time auxiliary loss formulation (Fig. 2), which explicitly at-tempts to bridge the modelâ€™s precision with the predicted class confidence (BPC). It leverages the count of true posi-tives and false positives in a minibatch, which are then em-ployed to construct a penalty for miscalibrated predictions.
We develop a differentiable proxy to the actual loss formu-lation that is based on counts. Our loss function is designed to be used with other application-specific loss functions.
We perform extensive experiments on both in-domain and out-domain scenarios, including the large-scale MS-COCO benchmark. Results reveal that our train-time auxiliary loss is capable of significantly improving the calibration of a state-of-the-art vision-transformer based object detector un-der both in-domain and out-domain scenarios. 2.