Abstract
Recent object detection approaches rely on pretrained vision-language models for image-text alignment. However, they fail to detect the Mobile User Interface (MUI) ele-ment since it contains additional OCR information, which describes its content and function but is often ignored. In this paper, we develop a new MUI element detection dataset named MUI-zh and propose an Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR in-formation. APT is a lightweight and effective module to jointly optimize category prompts across different modal-ities. For every element, APT uniformly encodes its vi-sual features and OCR descriptions to dynamically adjust the representation of frozen category prompts. We evalu-ate the effectiveness of our plug-and-play APT upon several existing CLIP-based detectors for both standard and open-vocabulary MUI element detection. Extensive experiments show that our method achieves considerable improvements on two datasets. The datasets is available at github. com/antmachineintelligence/MUI-zh. 1.

Introduction
While significant progress has been made in object de-tection [2,17,23,24,28], with the development of deep neu-ral networks, less attention has been paid to its challeng-ing variant in the Mobile User Interface (MUI) domain [1].
Instead of personal computers and books, people nowa-days spend more time on mobile phones due to the con-venience of various apps for daily life. However, there may exist some risks, including illegal gambling [10, 19], mal-ware [31,32], security [4,8], privacy [14,15], copy/fake [27] and fraudulent behaviors [6, 13] in apps, which need to be detected and alarmed as required by government authorities and app markets. In apps, these risks may occur in one ele-ment or even hide in the subpage after clicking one element.
As a result, it is in great need of an accurate, robust, and even open-vocabulary MUI element detection approach in practice. Such technology can benefit a great variety of sce-Figure 1. Two MUI samples from VINS and MUI-zh dataset.
Compared to VINS, we additionally obtain the OCR descriptions as supplemental information in MUI-zh. Moreover, we further link
OCR descriptions and element annotations with the same color. narios as mentioned above, towards building a better mobile ecosystem [13, 30].
This paper proposes MUI element detection as a variant object detection task and develops a corresponding dataset named MUI-zh. In general, object detection aims to clas-sify and locate each object, such as an animal or a tool, in one raw image. While in MUI data, our primary concern is detecting elements, e.g., products and clickable buttons in the screenshots. The main difference between the two tasks is that MUI data often have discriminative OCR de-scriptions as supplemental information for every element, significantly influencing detection results. To better explain it, we put two MUI data examples from VINS [1] and our
MUI-zh in Figure 1. VINS only provides the category anno-tation and bounding box for every element, as the object de-tection dataset does. At the same time, MUI-zh additionally obtains the OCR descriptions and links them with elements for further usage. Since the OCR descriptions are texts and will be an additional input modality, it is natural to lever-age recent Open-Vocabulary object Detection (OVD) mod-els [3, 11, 20, 22, 36, 37, 40] as the MUI element detection baseline because of their rich vision-language knowledge learned from pretrained CLIP [21].
OVD detectors usually detect and classify objects by cal-we demonstrate that the APT can achieve noticeable per-formance gains based on previous OVD detectors, which will benefit many mobile layout analyses [34, 35] and risk hunters [4,10]. We summarize our contributions as follows.
• We develop a high-quality MUI dataset (called MUI-zh) containing 18 common categories with OCR de-scriptions as the supplemental information. Besides
MUI-zh, we will also provide the OCR descriptions of the existing dataset VINS to facilitate future research.
• Inspired by the MUI data characteristics, we further proposed a novel Adaptive Prompt Tuning (APT) mod-ule to finetune category prompts for standard and open-vocabulary MUI element detection.
• Experiments on two datasets demonstrate that our
APT, as a plug-and-play module, achieves competitive improvements upon four recent CLIP-based detectors. 2.