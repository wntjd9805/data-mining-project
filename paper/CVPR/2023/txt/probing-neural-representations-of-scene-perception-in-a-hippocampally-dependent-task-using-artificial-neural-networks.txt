Abstract
Deep artificial neural networks (DNNs) trained through the mam-backpropagation provide effective models of malian visual system, accurately capturing the hierarchy of neural responses through primary visual cortex to inferior temporal cortex (IT) [41, 43]. However, the ability of these networks to explain representations in higher cortical areas is relatively lacking and considerably less well researched.
For example, DNNs have been less successful as a model of the egocentric to allocentric transformation embodied by circuits in retrosplenial and posterior parietal cortex. We describe a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of DNNs to transform scenes viewed from different egocen-tric perspectives. Using a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, we demonstrate that DNNs trained using a triplet loss can learn this task. Moreover, by enforcing a factorized latent space, we can split information propaga-tion into ”what” and ”where” pathways, which we use to reconstruct the input. This allows us to beat the state-of-the-art for unsupervised object segmentation on the CATER and MOVi-A,B,C benchmarks. 1.

Introduction
Recently, it has been shown that neural networks trained with large datasets can produce coherent scene understand-ing and are capable of synthesizing novel views [12, 21].
These models are trained on egocentric (self-centred) sen-sory input and can construct allocentric (world-centred) re-In animals, this transformation is governed by sponses. structures along the hierarchy from the visual cortex to the hippocampal formation, an important model system related to navigation and memory [20, 30]. Notably, the hippocam-pus is a necessary component of the network supporting memory and perception of places and events and is one of the first brain regions compromised during the progression of Alzheimer’s disease (AD) [3,34]. However, experimental knowledge regarding the interplay across multiple interact-ing brain regions is limited and new computational models are needed to better explain the single-cell responses across the whole transformation circuit.
Here, we developed a scene recognition model to better understand the intrinsic computations governing the trans-formation from egocentric to allocentric reference frames, which controls successful view synthesis in humans and other animals.
For this, we developed a novel hip-pocampally dependent task, inspired by the 4-Mountains-Test [18], which is used in clinics to predict early-onset
Alzheimer’s disease [40]. We tested this task by creating a biologically realistic model inspired by recent work in scene perception, in which scenes need to be re-imagined from several different viewpoints.
The main contributions of our paper are the following:
• We introduce and open-source the allocentric scene perception (ASP) benchmark for training view synthe-sis models based on a hippocampally dependent task which is frequently used to predict AD.
• We show that a biologically realistic neural network model trained using a triplet loss can accurately distin-guish between hundreds of scenes across many differ-ent viewpoints and that it can disentangle object infor-mation from location information when using a factor-ized latent space.
• Lastly, we show that by using a reconstruction loss combined with a pixel-wise decoder we can perform unsupervised object segmentation, outperforming the state-of-the-art models on the CATER, MOVi-A,B,C benchmarks. 2.