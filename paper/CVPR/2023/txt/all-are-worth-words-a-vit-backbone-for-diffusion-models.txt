Abstract
Vision transformers (ViT) have shown promise in vari-ous vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion mod-els. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion mod-els. U-ViT is characterized by treating all inputs includ-ing the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image gen-eration tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on Im-ageNet 256×256, and 5.48 in text-to-image generation on
MS-COCO, among methods without accessing large exter-nal datasets during the training of generative models.
Our results suggest that, for diffusion-based image mod-eling, the long skip connection is crucial while the down-sampling and up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can pro-vide insights for future research on backbones in diffu-sion models and benefit generative modeling on large scale cross-modality datasets. 1.

Introduction
Figure 1. The U-ViT architecture for diffusion models, which is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing (#Blocks-1)/2 long skip connections between shallow and deep layers.
Diffusion models [24, 56, 61] are powerful deep gener-ative models that emerge recently for high quality image generation [12, 25, 49]. They grow rapidly and find ap-plications in text-to-image generation [47, 49, 51], image-to-image generation [10, 42, 74], video generation [23, 27],
*Corresponding to C. Li and J. Zhu. speech synthesis [6, 33], and 3D synthesis [46].
Along with the development of algorithms [2, 3, 14, 24, 32, 40, 41, 45, 57, 58, 61, 65], the revolution of backbones plays a central role in diffusion models. A representative example is U-Net based on a convolutional neural network (CNN) employed in prior work [24,59]. The CNN-based U-Net is characterized by a group of down-sampling blocks, a group of up-sampling blocks, and long skip connections between the two groups, which dominates diffusion mod-els for image generation tasks [12, 47, 49, 51]. On the other hand, vision transformers (ViT) [15] have shown promise in various vision tasks, where ViT is comparable or even supe-rior to CNN based approaches [9, 20, 35, 62, 75]. Therefore, a very natural question arises: whether the reliance of the
CNN-based U-Net is necessary in diffusion models?
In this paper, we design a simple and general ViT-based architecture called U-ViT (Figure 1). Following the de-sign methodology of transformers, U-ViT treats all inputs including the time, condition and noisy image patches as tokens. Crucially, U-ViT employs long skip connections between shallow and deep layers inspired by U-Net.
In-tuitively, low-level features are important to the pixel-level prediction objective in diffusion models and such connec-tions can ease the training of the corresponding prediction network. Besides, U-ViT optionally adds an extra 3×3 con-volutional block before output for better visual quality. See a systematical ablation study for all elements in Figure 2.
We evaluate U-ViT in three popular tasks: unconditional image generation, class-conditional image generation and text-to-image generation. In all settings, U-ViT is compara-ble if not superior to a CNN-based U-Net of a similar size.
In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional im-age generation on ImageNet 256×256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of gen-erative models.
Our results suggest that the long skip connection is cru-cial while the down/up-sampling operators in CNN-based
U-Net are not always necessary for image diffusion mod-els. We believe that U-ViT can provide insights for future research on diffusion model backbones and benefit genera-tive modeling on large scale cross-modality datasets. 2.