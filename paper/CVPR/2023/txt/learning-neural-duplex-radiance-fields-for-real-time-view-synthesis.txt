Abstract
Neural radiance fields (NeRFs) enable novel-view synthe-sis with unprecedented visual quality. However, to render photorealistic images, NeRFs require hundreds of deep mul-tilayer perceptron (MLP) evaluations – for each pixel. This is prohibitively expensive and makes real-time rendering infeasible, even on powerful modern GPUs. In this paper, we propose a novel approach to distill and bake NeRFs into highly efficient mesh-based neural representations that are fully compatible with the massively parallel graphics render-ing pipeline. We represent scenes as neural radiance features encoded on a two-layer duplex mesh, which effectively over-comes the inherent inaccuracies in 3D surface reconstruc-tion by learning the aggregated radiance information from a reliable interval of ray-surface intersections. To exploit local geometric relationships of nearby pixels, we leverage screen-space convolutions instead of the MLPs used in NeRFs to achieve high-quality appearance. Finally, the performance of the whole framework is further boosted by a novel multi-view distillation optimization strategy. We demonstrate the effectiveness and superiority of our approach via extensive experiments on a range of standard datasets. 1.

Introduction
Reconstructing 3D scenes by a representation that can be ren-dered from unobserved viewpoints using only a few posed
* Corresponding author. images has been a long-standing goal in the computer graph-ics and computer vision communities. Significant progress has recently been achieved by neural radiance fields (NeRFs)
[26], which are capable of generating photorealistic novel views and modeling view-dependent effects such as specu-lar reflections. In particular, a radiance field is a volumetric function parameterized by MLPs that estimates density and emitted radiance at sampled 3D locations in a given direction.
Differentiable volume rendering then allows the optimization of this function by minimizing the photometric discrepancy between the real observed color and the rendered color.
Despite the unprecedented success and enormous practi-cal potential of NeRF and its various extensions [3, 6, 57], an inescapable problem is the high computational cost of rendering novel views. For instance, even using a powerful modern GPU, NeRF requires about 30 seconds to render a single image with 800×800 pixels, which prevents its use for interactive applications in virtual and augmented real-ity. On the other hand, the rapid development of NeRF has spawned abundant follow-up works that focus on optimiza-tion acceleration [18, 28, 54], generalization [5, 47, 53], and enabled different downstream tasks, including 3D styliza-tion [11, 16, 30], editing [24, 50, 55, 56], or even perception
[10, 17]. Thus, a generalized method that can learn and ex-tract a real-time renderable representation given an arbitrary pretrained NeRF-based model, while maintaining high ren-dering quality, is highly desirable.
The huge computational cost of rendering a NeRF rep-resentation mainly comes from two aspects: (1) For each individual pixel, NeRF requires sampling hundreds of loca-tions along the corresponding ray, querying density and then accumulating radiance using volume rendering; and (2) A large model size is required to represent the geometric details of complex scenes well, so the MLPs used in NeRF architec-ture are relatively deep and wide, which incurs significant computation for the evaluation of each point sample.
In this paper, we present an ap-proach that achieves high-fidelity real-time view synthesis by ad-dressing these issues. To avoid the dense sampling along each ray, one solution is to generate a geome-try proxy from a pretrained NeRF model, e.g., via marching cubes
[25]. By exploiting the highly-optimized graphics pipeline, we could almost instantly obtain the sample location for each ray. How-ever, due to inaccuracies in the den-sity field around surfaces, the extracted mesh may not faith-fully represent the true underlying geometry and can contain artifacts, as shown in Figure 2. Dense local sampling could alleviate these errors to some degree, but cannot handle miss-ing geometries or occlusions. An alternative approach is to use rasterization for fast neural rendering [1, 37, 38, 43] by directly baking neural features onto the surface of explicit the geometry or point cloud. This is usually accompanied by a deep CNN to translate rasterized features to colors and learn to resolve the existing errors, which is expensive to evaluate and can prevent real-time rendering.
Figure 2. NeRF surface.
To significantly reduce the sampled numbers while effi-ciently handling the geometric errors, our first key technical innovation is to infer the final RGB color according to the radiance of duplex points along the ray. As illustrated in
Figure 3, NeRFs represent a continuous density distribution along each ray. While it will generally be difficult to deter-mine the exact location of a surface along the ray, it will be easier to extract a reliable interval that contributes the most to the final prediction by using an under- and an overesti-mation of the geometry. Motivated by this idea, instead of selecting a specific location for appearance calculation, or performing expensive dense volume rendering in this inter-val, we represent the scene using learnable features at the two intersection points of a ray with the duplex geometry and use a neural network to learn the color from this ag-gregated duplex radiance information. Although only two sampled locations are considered, we found this proposed neural duplex radiance field to be robust in compensating for the errors of the geometry proxy even without a deep neural network, while effectively preserving the efficiency of rasterization-based approaches. The NeRF MLP has become the most standard architecture for most neural implicit repre-sentations [7, 8, 26, 27, 31, 39]. Yet, with only a few points
Figure 3. Motivation. The continuous density distribution of NeRF (curve above the ray) makes it difficult to identify the accurate location of a surface ((cid:72)) for appearance calculation. We thus seek to extract a reliable interval from the density field (between dashed lines), and learn the duplex radiance combinations to tolerate errors. considered along the ray, the MLP struggles to constrain the proposed neural duplex radiance field. Instead, we use a shal-low convolutional network, which can effectively capture the local geometric information of neighboring pixels, and leads to a considerably better rendering quality. Finally, we found that directly training the neural duplex radiance field from scratch will lead to noticeable artifacts. We therefore propose a multi-view distillation optimization strategy that enables us to effectively approximate the rendering quality of the original NeRF models. Remarkably, our method im-proves run-time performance by 10,000 times compared to the original NeRF while maintaining high-quality rendering. 2.