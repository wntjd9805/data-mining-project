Abstract
We introduce submodel co-training, a regularization method related to co-training, self-distillation and stochas-tic depth. Given a neural network to be trained, for each sample we implicitly instantiate two altered networks, “sub-models”, with stochastic depth: we activate only a subset of the layers. Each network serves as a soft teacher to the other, by providing a loss that complements the regular loss provided by the one-hot label. Our approach, dubbed “co-sub”, uses a single set of weights, and does not involve a pre-trained external model or temporal averaging.
Experimentally, we show that submodel co-training is effective to train backbones for recognition tasks such as image classiﬁcation and semantic segmentation. Our ap-proach is compatible with multiple architectures, including
RegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training strategy improves their results in comparable settings. For instance, a ViT-B pretrained with cosub on ImageNet-21k obtains 87.4% top-1 acc. @448 on ImageNet-val. 1.

Introduction
Although the fundamental ideas of deep trainable neural networks have been around for decades, only recently have barriers been removed to allow breakthroughs in success-fully training deep neural architectures in practice. Many of these barriers are related to non-convex optimization in one way or another, which is central to the success of modern neural networks. The optimization challenges have been addressed from multiple angles in the literature. First, mod-ern architectures are designed to facilitate the optimization of very deep networks. An exceptionally successful design principle is using residual connections [24, 25]. Although this does not change the expressiveness of the functions that the network can implement, the improved gradient ﬂow al-leviates, to some extent, the difﬁculties of optimizing very deep networks. Another key element to the optimization is the importance of data, revealed by the step-change in vi-sual recognition performance resulting from the ImageNet dataset [11], and the popularization of transfer learning with pre-training on large datasets [39, 58]. label “tree”
λ target2 1 − λ target1 1 l e d o m b u s 2 l e d o m b u s
θ1
<latexit sha1_base64="vDhJRxCARDrEDJh8J1+qCQooVZs=">AAACSnicdVBdaxNBFJ2NVWv8aKr4JMJgKvRp2d02Jn0r+OJjBdMWsku4OztphszHMnNXDcP+GF/1r/gH/Bu+iS/OphGs6IVhzj3nXu7hlLUUDpPkW9S7tXP7zt3de/37Dx4+2hvsPz53prGMT5mRxl6W4LgUmk9RoOSXteWgSskvytXrTr94z60TRr/Ddc0LBVdaLAQDDNR88DQvjazcWoXP57jkCPO0nQ+GSZwlJ9nkmCbx0SiAUQDjdHIyzmgaJ5sakm2dzfej53llWKO4RibBuVma1Fh4sCiY5G0/bxyvga3gis8C1KC4K/zGf0tfBqaiC2PD00g37J8bHpTrLIZJBbh0f2sd+S9t1uBiUnih6wa5ZteHFo2kaGgXBq2E5QzlOgBgVgSvlC3BAsMQ2Y0rpTErhNK1/X6u+QdmlAJd+dxhpYRuXDtLi9AxkLw0H30Svxq1/iCvFR2mB20X6O/U6P/BeRanR3H29nh4eriNdpc8Iy/IIUnJmJySN+SMTAkjnnwin8mX6Gv0PfoR/bwe7UXbnSfkRvV2fgHPwrJ7</latexit>
θ
<latexit sha1_base64="UakMuYN+OmatTz2zliypy+NMzBA=">AAACSHicdVDLahtBEJxVXo7yku1jCAyRAz4tu4oT+WjIJUcHItugXUTvbMsaNI9lpteOWPZbfHV+JX+Qv8gt5JZZWYE4j4Zhqqu66aKKSklPSfI16t25e+/+g62H/UePnzx9NtjeOfG2dgInwirrzgrwqKTBCUlSeFY5BF0oPC2W7zr99AKdl9Z8pFWFuYZzI+dSAAVqNtjNCqtKv9LhazJaIEE7GwyT+CBNxqMx/xukcbKuIdvU8Ww7epGVVtQaDQkF3k/TpKK8AUdSKGz7We2xArGEc5wGaECjz5u1+5a/CkzJ59aFZ4iv2d83GtC+MxgmNdDC/6l15L+0aU3zw7yRpqoJjbg5NK8VJ8u7KHgpHQpSqwBAOBm8crEAB4JCYLeuFNYuCQrf9vuZwUthtQZTNpmnUktT+3aa5qEToLCwn5okfvumbfaySvNhutd2gf5Kjf8fnIzi9HU8+nAwPNrfRLvFnrOXbJ+lbMyO2Ht2zCZMsBW7Ytfsc/Ql+hZ9j37cjPaizc4uu1W93k8/QrG7</latexit>T
T
<latexit sha1_base64="eKHKbIM1GmwN+GJF4oitKCUY1GQ=">AAACPXicdVBNTxRBEO1B0XVFBTkak44LCafJzIosRxIvHteEBeLOhNT01EJn+2PSXYNuJvMvuMJf8Xf4A7gZr17tXdZERF/SyatXVanXr6iU9JQk36KVBw9XHz3uPOk+XXv2/MX6xssjb2sncCSssu6kAI9KGhyRJIUnlUPQhcLjYvp+3j++QOelNYc0qzDXcGbkRAqgIH3KNNC5AMUPT9d7SbybJoP+gN8naZws0GNLDE83otdZaUWt0ZBQ4P04TSrKG3AkhcK2m9UeKxBTOMNxoAY0+rxZWG75dlBKPrEuPEN8of650YD2fqaLMDm36P/uzcV/9cY1TfbzRpqqJjTi9tCkVpwsn/+fl9KhIDULBISTwSsX5+BAUEjpzpXC2ilB4dtuNzP4WVitwZRN5qnU0tS+Had5qEJ6WNgvTRLvvWubrazSvJdutW0I9Hdq/P/kqB+nb+P+x93ewc4y2g57xd6wHZayATtgH9iQjZhghl2yK3YdfY1uou/Rj9vRlWi5s8nuIPr5C2MJrdw=</latexit>
<latexit sha1_base64="eKHKbIM1GmwN+GJF4oitKCUY1GQ=">AAACPXicdVBNTxRBEO1B0XVFBTkak44LCafJzIosRxIvHteEBeLOhNT01EJn+2PSXYNuJvMvuMJf8Xf4A7gZr17tXdZERF/SyatXVanXr6iU9JQk36KVBw9XHz3uPOk+XXv2/MX6xssjb2sncCSssu6kAI9KGhyRJIUnlUPQhcLjYvp+3j++QOelNYc0qzDXcGbkRAqgIH3KNNC5AMUPT9d7SbybJoP+gN8naZws0GNLDE83otdZaUWt0ZBQ4P04TSrKG3AkhcK2m9UeKxBTOMNxoAY0+rxZWG75dlBKPrEuPEN8of650YD2fqaLMDm36P/uzcV/9cY1TfbzRpqqJjTi9tCkVpwsn/+fl9KhIDULBISTwSsX5+BAUEjpzpXC2ilB4dtuNzP4WVitwZRN5qnU0tS+Had5qEJ6WNgvTRLvvWubrazSvJdutW0I9Hdq/P/kqB+nb+P+x93ewc4y2g57xd6wHZayATtgH9iQjZhghl2yK3YdfY1uou/Rj9vRlWi5s8nuIPr5C2MJrdw=</latexit>
θ2
<latexit sha1_base64="pBZz6yBzS2GzIG2wxfqacBFCUHs=">AAACSnicdVBdaxNBFJ2NVWv8aKr4JMJgKvRp2d02Jn0r+OJjBdMWskuYnb1phszHMnNXDcP+GF/1r/gH/Bu+iS/OphGs6IVhzj3nXu7hlLUUDpPkW9S7tXP7zt3de/37Dx4+2hvsPz53prEcptxIYy9L5kAKDVMUKOGytsBUKeGiXL3u9Iv3YJ0w+h2uaygUu9JiITjDQM0HT/PSyMqtVfh8jktANs/a+WCYxFlykk2OaRIfjQIYBTBOJyfjjKZxsqkh2dbZfD96nleGNwo0csmcm6VJjYVnFgWX0PbzxkHN+IpdwSxAzRS4wm/8t/RlYCq6MDY8jXTD/rnhmXKdxTCpGC7d31pH/kubNbiYFF7oukHQ/PrQopEUDe3CoJWwwFGuA2DciuCV8iWzjGOI7MaV0pgVstK1/X6u4QM3SjFd+dxhpYRuXDtLi9BxJqE0H30Svxq1/iCvFR2mB20X6O/U6P/BeRanR3H29nh4eriNdpc8Iy/IIUnJmJySN+SMTAknnnwin8mX6Gv0PfoR/bwe7UXbnSfkRvV2fgHRk7J8</latexit> augmentation
Figure 1. Co-training of submodels (cosub): for each image, two sub-models are sampled by randomly dropping layers of the full model. The training signal for each submodel mixes the cross-entropy loss from the image label with a self-distillation loss obtained from the other submodel.
However, even when (pre-)trained with millions of im-ages, recent deep networks with millions if not billions of parameters, are still heavily overparameterized. Tradi-tional regularization like weight decay, dropout [46], or la-bel smoothing [47] are limited in their ability to address this issue. Data-augmentation strategies, including those mixing different images like Mixup [61] and CutMix [60], have proven to provide a complementary data-driven form of regularization. More recently, multiple works propose to resort to self-supervised pre-training. These approaches rely on a proxy objective that generally provides more su-pervision signal than the one available from labels, like in recent (masked) auto-encoders [5,16,22], which were popu-lar in the early deep learning literature [7,19,27]. Similarly, contrastive approaches [23] or self-distillation [9] provide a richer supervision less prone to supervision collapse [12].
Overall, self-supervised learning makes it possible to learn larger models with less data, possibly reducing the need of a pre-training stage [15].
Distillation is a complementary approach to improve op-timization. Distillation techniques were originally devel-oped to transfer knowledge from a teacher model to a stu-dent model [4, 28], allowing the student to improve over
In contrast to traditional learning from the data directly. distillation, co-distillation does not require pre-training a (strong) teacher. Instead, a pool of models supervise each other. Practically, it faces several limitations, including the difﬁculty of jointly training more than two students for com-plexity reasons, as it involves duplicating the weights.
In this paper, we propose a practical way to enable co-training for a very large number of students. We consider a single target model to be trained, and we instantiate two submodels on-the-ﬂy, simply by layerwise dropout [20, 31].
This gives us two neural networks through which we can backpropagate to the shared parameters of the target model.
In addition to the regular training loss, each submodel serves as a teacher to the other, which provides an addi-tional supervision signal ensuring the consistency across the submodels. Our approach is illustrated in Figure 1: the pa-rameter λ controls the importance of the co-training loss compared to the label loss, and our experiments show that it signiﬁcantly increases the ﬁnal model accuracy.
This co-training across different submodels, which we refer to as cosub, can be regarded as a massive co-training between 2L models that share a common set of parameters, where L is the number of layers in the target architecture.
The target model can be interpreted as the expectation of all models. With a layer drop-rate set to 0.5, for instance for a ViT-H model, all submodels are equiprobable, and then it amounts to averaging the weights of 22×32 models.
Our contributions can be summarized as follows:
• We introduce a novel training approach for deep neu-ral networks: we co-train submodels. This signiﬁ-cantly improves the training of most models, establish-ing the new state of the art in multiple cases. For in-stance, after pre-training ViT-B on Imagenet-21k and
ﬁne-tuning it at resolution 448, we obtain 87.4% top-1 accuracy on Imagenet-val.
• We provide an efﬁcient implementation to subsample models on the ﬂy. It is a simple yet effective variation of stochastic depth [31] to drop residual blocks.
• We provide multiple analyses and ablations. Notice-ably, we show that our submodels are effective models by themselves even with signiﬁcant trimming, similar to LayerDrop [20] in natural language processing.
• We validate our approach on multiple architectures (like ViT, ResNet, RegNet, PiT, XCiT, Swin, Con-vNext), both for image classiﬁcation –trained from scratch or with transfer–, and semantic segmentation.
• We will share models/code for reproducibility in the
DeiT repository. 2.