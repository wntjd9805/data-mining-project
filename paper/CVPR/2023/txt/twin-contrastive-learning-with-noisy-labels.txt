Abstract
Learning from noisy data is a challenging task that sig-niﬁcantly degenerates the model performance. In this paper, we present TCL, a novel twin contrastive learning model to learn robust representations and handle noisy labels for classiﬁcation. Speciﬁcally, we construct a Gaussian mix-ture model (GMM) over the representations by injecting the supervised model predictions into GMM to link label-free latent variables in GMM with label-noisy annotations.
Then, TCL detects the examples with wrong labels as the out-of-distribution examples by another two-component GMM, taking into account the data distribution. We further propose a cross-supervision with an entropy regularization loss that bootstraps the true targets from model predictions to handle the noisy labels. As a result, TCL can learn discriminative representations aligned with estimated labels through mixup and contrastive learning. Extensive experimental results on several standard benchmarks and real-world datasets demonstrate the superior performance of TCL. In particular,
TCL achieves 7.5% improvements on CIFAR-10 with 90% noisy label—an extremely noisy scenario. The source code is available at https://github.com/Hzzone/TCL. 1.

Introduction
Deep neural networks have shown exciting performance for classiﬁcation tasks [13]. Their success largely results from the large-scale curated datasets with clean human anno-tations, such as CIFAR-10 [19] and ImageNet [6], in which the annotation process, however, is tedious and cumbersome.
In contrast, one can easily obtain datasets with some noisy annotations—from online shopping websites [40], crowd-sourcing [42, 45], or Wikipedia [32]—for training a clas-∗Corresponding author siﬁcation neural network. Unfortunately, the mislabelled data are prone to signiﬁcantly degrade the performance of deep neural networks. Therefore, there is considerable inter-est in training noise-robust classiﬁcation networks in recent years [20, 21, 25, 29, 31, 48].
To mitigate the inﬂuence of noisy labels, most of the meth-ods in literature propose the robust loss functions [37, 47], reduce the weights of noisy labels [35, 39], or correct the noisy labels [20, 29, 31]. In particular, label correction meth-ods have shown great potential for better performance on the dataset with a high noise ratio. Typically, they correct the labels by using the combination of noisy labels and model predictions [31], which usually require an essential itera-tive sample selection process [1, 20, 21, 29]. For example,
Arazo et al. [1] uses the small-loss trick to carry out sample selection and correct labels via the weighted combination.
In recent years, contrastive learning has shown promising results in handling noisy labels [21, 21, 29]. They usually leverage contrastive learning to learn discriminative repre-sentations, and then clean the labels [21, 29] or construct the positive pairs by introducing the information of nearest neighbors in the embedding space. However, using the near-est neighbors only considers the label noise within a small neighborhood, which is sub-optimal and cannot handle ex-treme label noise scenarios, as the neighboring examples may also be mislabeled at the same time.
To address this issue, this paper presents TCL, a novel twin contrastive learning model that explores the label-free unsupervised representations and label-noisy annotations for learning from noisy labels. Speciﬁcally, we leverage contrastive learning to learn discriminative image represen-tations in an unsupervised manner and construct a Gaus-sian mixture model (GMM) over its representations. Un-like unsupervised GMM, TCL links the label-free GMM and label-noisy annotations by replacing the latent variable of GMM with the model predictions for updating the pa-rameters of GMM. Then, beneﬁtting from the learned data
distribution, we propose to formulate label noise detection as an out-of-distribution (OOD) problem, utilizing another two-component GMM to model the samples with clean and wrong labels. The merit of the proposed OOD label noise detection is to take the full data distribution into account, which is robust to the neighborhood with strong label noise.
Furthermore, we propose a bootstrap cross-supervision with an entropy regulation loss to reduce the impact of wrong labels, in which the true labels of the samples with wrong la-bels are estimated from another data augmentation. Last, to further learn robust representations, we leverage contrastive learning and Mixup techniques to inject the structural knowl-edge of classes into the embedding space, which helps align the representations with estimated labels.
The contributions are summarized as follows:
• We present TCL, a novel twin contrastive learning model that explores the label-free GMM for unsuper-vised representations and label-noisy annotations for learning from noisy labels.
• We propose a novel OOD label noise detection method by modeling the data distribution, which excels at han-dling extremely noisy scenarios.
• We propose an effective cross-supervision, which can bootstrap the true targets with an entropy loss to regu-larize the model.
• Experimental results on several benchmark datasets and real-world datasets demonstrate that our method outperforms the existing state-of-the-art methods by a signiﬁcant margin. In particular, we achieve 7.5% improvements in extremely noisy scenarios. 2.