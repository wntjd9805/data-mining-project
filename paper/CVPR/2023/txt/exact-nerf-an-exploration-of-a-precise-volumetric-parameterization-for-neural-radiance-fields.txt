Abstract
Neural Radiance Fields (NeRF) have attracted significant attention due to their ability to synthesize novel scene views with great accuracy. However, inherent to their underly-ing formulation, the sampling of points along a ray with zero width may result in ambiguous representations that lead to further rendering artifacts such as aliasing in the final scene. To address this issue, the recent variant mip-NeRF proposes an Integrated Positional Encoding (IPE) based on a conical view frustum. Although this is expressed with an integral formulation, mip-NeRF instead approxi-mates this integral as the expected value of a multivariate
Gaussian distribution. This approximation is reliable for short frustums but degrades with highly elongated regions, which arises when dealing with distant scene objects un-In this paper, we explore the der a larger depth of field. use of an exact approach for calculating the IPE by using a pyramid-based integral formulation instead of an approx-imated conical-based one. We denote this formulation as
Exact-NeRF and contribute the first approach to offer a pre-cise analytical solution to the IPE within the NeRF domain.
Our exploratory work illustrates that such an exact formula-tion (Exact-NeRF) matches the accuracy of mip-NeRF and furthermore provides a natural extension to more challeng-ing scenarios without further modification, such as in the case of unbounded scenes. Our contribution aims to both address the hitherto unexplored issues of frustum approx-imation in earlier NeRF work and additionally provide in-sight into the potential future consideration of analytical so-lutions in future NeRF extensions. 1.

Introduction
Novel view synthesis is a classical and long-standing task in computer vision that has been thoroughly re-investigated via recent work on Neural Radiance Fields (NeRF) [20].
NeRF learns an implicit representation of a 3D scene from a set of 2D images via a Multi-Layer Perceptron (MLP) that
Figure 1. Comparison of Exact-NeRF (ours) with mip-NeRF 360
[2]. Our method is able to both match the performance and obtain superior depth estimation over a larger depth of field. predicts the visual properties of 3D points uniformly sam-pled along the viewing ray given its coordinates and view-ing direction. This parameterization gives NeRF the dual ability to both represent 3D scenes and synthesize unseen views. In its original formulation, NeRF illustrates strong reconstruction performance for synthetic datasets compris-ing object-centric scenes and no background (bounded) and forward-facing real-world scenes. Among its appli-cations, NeRF has been used for urban scene representa-tion [25, 27, 29], human body reconstruction [3, 16], image processing [12, 17, 19] and physics [9, 14].
Nonetheless, the underlying sparse representation of 3D points learnt by the MLP may cause ambiguities that can lead to aliasing and blurring. To overcome these issues,
Barron et al. proposed mip-NeRF [1], an architecture that uses cone tracing instead of rays. This architecture en-codes conical frustums as the inputs of the MLP by ap-proximating the integral of a sine/cosine function over a region in the space with a multivariate Gaussian. This re-parameterization notably increases the reconstruction qual-ity of multi-scale datasets. However, this approximation is only really valid for bounded scenes, where the conic frus-tums do not suffer from large elongations attributable to a large depth of field within the scene.
The NeRF concept has been extended to represent in-creasingly difficult scenes. For instance, mip-NeRF 360 [2] learns a representation of unbounded scenes with a cen-tral object by giving more capacity to points that are near the camera, modifying the network architecture and intro-ducing a regularizer that penalizes ‘floaters’ (unconnected depth regions in free space) and other small unconnected regions.
In order to model distant regions, mip-NeRF 360 transforms the multivariate Gaussians with a contrac-tion function. This modification allows a better representa-tion and outperforms standard mip-NeRF for an unbounded scenes dataset. However, the modification of the Gaussians requires attentive analysis to encode the correct information in the contracted space, which includes the linearization of the contraction function to accommodate the Gaussian ap-proximations. This leads to a degraded performance of mip-NeRF 360 when the camera is far from the object. Addition-ally, mip-NeRF 360 struggles to render thin structures such as tree branches or bicycle rays.
Motivated by this, we present Exact-NeRF as an explo-ration of an alternative exact parameterization of underly-ing volumetric regions that are used in the context of mip-NeRF (Fig. 1). We propose a closed-form volumetric po-sitional encoding formulation (Sec. 3) based on pyrami-dal frustums instead of the multivariate Gaussian approx-imation used by mip-NeRF and mip-NeRF 360. Exact-NeRF matches the performance of mip-NeRF on a synthetic dataset, but gets a sharper reconstruction around edges. Our approach can be applied without further modification to the contracted space of mip-NeRF 360. Our naive implementa-tion of Exact-NeRF for the unbounded scenes of mip-NeRF 360 has a small decrease in performance, but it is able to get cleaner reconstructions of the background. Addition-ally, the depth map estimations obtained by Exact-NeRF are less noisy than mip-NeRF 360. Our key contribution is the formulation of a general integrated positional encod-ing framework that can be applied to any shape that can be broken into triangles (i.e., a polyhedron). We intend that our work serves as a motivation to investigate differ-ent shapes and analytical solutions of volumetric positional encoding. The code is available at https://github. com/KostadinovShalon/exact-nerf. 2.