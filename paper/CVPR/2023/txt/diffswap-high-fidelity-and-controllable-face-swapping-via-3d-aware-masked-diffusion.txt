Abstract
In this paper, we propose DiffSwap, a diffusion model based framework for high-fidelity and controllable face swapping. Unlike previous work that relies on carefully de-signed network architectures and loss functions to fuse the information from the source and target faces, we reformu-late the face swapping as a conditional inpainting task, per-formed by a powerful diffusion model guided by the desired face attributes (e.g., identity and landmarks). An impor-tant issue that makes it nontrivial to apply diffusion mod-els to face swapping is that we cannot perform the time-consuming multi-step sampling to obtain the generated im-age during training. To overcome this, we propose a mid-point estimation method to efficiently recover a reasonable diffusion result of the swapped face with only 2 steps, which enables us to introduce identity constraints to improve the face swapping quality. Our framework enjoys several fa-vorable properties more appealing than prior arts: 1) Con-trollable. Our method is based on conditional masked diffu-sion on the latent space, where the mask and the conditions can be fully controlled and customized. 2) High-fidelity.
The formulation of conditional inpainting can fully exploit the generative ability of diffusion models and can preserve the background of target images with minimal artifacts. 3)
†Corresponding author
Shape-preserving. The controllability of our method en-ables us to use 3D-aware landmarks as the condition during generation to preserve the shape of the source face. Ex-tensive experiments on both FF++ and FFHQ demonstrate that our method can achieve state-of-the-art face swapping results both qualitatively and quantitatively. 1.

Introduction
There has been growing interest in face swapping tech-nology from both vision and graphics communities [1, 2, 6, 21,36,39] because of its broad applications in creating digi-tal twins, making films, protecting privacy, etc. The goal of face swapping is to transfer the identity of the source face to a target image or a video frame while keeping the attributes (e.g., pose, expression, background) unchanged.
There are two essential steps in realizing high-quality face swapping: encoding the identity information of the source face effectively and blending identity and attributes from different images seamlessly. Early work [4,24] on face swapping adopts 3D models [5] to represent the source face and directly replace the reconstructed faces in the target im-age based on 3D structural priors, leading to recognizable artifacts. The development of generative adversarial net-works (GAN) [12] provides a strong tool to generate photo-realistic face images. Many recent methods [2, 6, 21, 39] perform face swapping by extracting the identity feature
from the source image and then injecting it into the gen-erative models powered by adversarial training. However, these methods tend to make minor modifications to the tar-get image, which may fail in totally transferring the identity information when the face shapes of the source and the tar-get face largely differ.
Very recently, diffusion-based models (DM) [22, 25, 26] have exhibited high customizability for various conditions and impressive power in generating images with high res-olution and complex scenes. It is natural to ask: whether the strong generation ability of diffusion models can benefit face swapping? However, we find it is nontrivial to apply diffusion models to the task of face swapping. Since there is no ground-truth data pair for face swapping, face swapping models are usually trained in a weakly-supervised manner, where several losses about image fidelity, identity, and fa-cial attributes are imposed to guide the training. These su-pervisory signals can be easily added to GAN-based models but it is difficult for DMs. Different from previous genera-tive models like GANs [12, 16] and VAEs [14, 19], DMs learn a denoising autoencoder to gradually recover the data density step-by-step. Although the autoencoder can be ef-ficiently learned by performing score matching [15] at an arbitrary step during training, image generation using an al-ready trained DM requires executing the autoencoder se-quentially for a large number of steps (typically, 200 steps), which is computationally expensive.
To tackle these challenges, we propose the first diffusion model based face swapping framework, which can produce high-fidelity results faces with high controllability. Figure 2 shows the overview of our method. Different from existing methods [1,6,21,36] that modify the target face to match the identity of the source face, we reformulate face swapping as a conditional inpainting task guided by the identity fea-ture and facial landmarks. Our diffusion model is learned to generate a face that shares the same identity as the source face and is spatially aligned with the target face. In order to introduce identity constraints during training, we propose a midpoint estimation method that can efficiently generate swapped faces with only 2 steps. Our framework is by de-sign highly controllable, where both the conditioned land-mark and the inpainting mask can be customized during in-ference. Thanks to this property, we propose the 3D-aware masked diffusion where we perform the inpainting inside the 3D-aware mask conditioned on the 3D-aware landmark that explicitly enforces the shape consistency between the source face and the swapped face.
We conducted extensive experiments on FaceForen-sics++ [27] and FFHQ [16] to verify the effectiveness of our model both and quantitatively. On FF++ dataset, our method outperforms previous methods in both ID retrieval (98.54%) and FID (2.16), while achieving comparable re-sults on pose error and expression error. Qualitative results show that our method can generate high-fidelity swapped faces that can better preserve the source face shape than the previous method. Besides, we also demonstrate the scala-bility and controllability of our method. Our model can be easily extended to higher-resolution such as 512 × 512 with affordable extra computational costs and allows region-customizable face swapping by controlling the inpainting mask. Our results demonstrate that DiffSwap is a very promising face swapping framework that is distinct from the existing methods and enjoys high fidelity, controllabil-ity, and scalability. 2.