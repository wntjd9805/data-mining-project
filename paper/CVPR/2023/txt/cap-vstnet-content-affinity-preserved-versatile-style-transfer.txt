Abstract
Content affinity loss including feature and pixel affinity is a main problem which leads to artifacts in photorealis-tic and video style transfer. This paper proposes a new framework named CAP-VSTNet, which consists of a new re-versible residual network and an unbiased linear transform module, for versatile style transfer. This reversible resid-ual network can not only preserve content affinity but not introduce redundant information as traditional reversible networks, and hence facilitate better stylization. Empow-ered by Matting Laplacian training loss which can address the pixel affinity loss problem led by the linear transform, the proposed framework is applicable and effective on ver-satile style transfer. Extensive experiments show that CAP-VSTNet can produce better qualitative and quantitative re-sults in comparison with the state-of-the-art methods. 1.

Introduction
Photorealistic style transfer aims to reproduce content image with the style from a reference image in a photore-*Corresponding Author alistic way. To be photorealism, the stylized image should preserve clear content detail and consistent stylization of the same semantic regions. Content affinity preservation, including feature and pixel affinity preservation [23,25,28], is the key to achieve both clear content detail and consistent stylization in the transfer.
The framework of a deep learning based photorealis-tic style transfer generally uses such an architecture: an encoder module extracting content and style features, fol-lowed by a transformation module to adjust features statis-tics, and finally a decoder module to invert stylized feature back to stylized image. Existing photorealistic methods typ-ically employ pre-trained VGG [30] as encoder. Since the encoder is specifically designed to capture object-level in-formation for the classification task, it inevitably results in content affinity loss. To reduce the artifacts, existing meth-ods either use skip connection modules [2, 14, 40] or build a shallower network [8, 23, 39]. However, these strategies, limited by the image recovery bias, cannot achieve a perfect content affinity preservation on unseen images.
In this work, rather than use the traditional encoder-to a re-transformation-decoder architecture, we resort versible framework [1] based solution called CAP-VSTNet, which consists of a specifically designed reversible residual
network followed by an unbiased linear transform module based on Cholesky decomposition [19] that performs style transfer in the feature space. The reversible network takes the advantages of the bijective transformation and can avoid content affinity information loss during forward and back-ward inference. However, directly using the reversible net-work cannot work well on our problem. This is because re-dundant information will accumulate greatly when the net-work channel increases. It will further lead to content affin-ity loss and noticeable artifacts as the transform module is sensitive to the redundant information. Inspired by knowl-edge distillation methods [8, 35], we improve the reversible network and employ a channel refinement module to avoid the redundant information accumulation. We achieve this by spreading the channel information into a patch of the spatial dimension. In addition, we also introduce cycle con-sistency loss in CAP-VSTNet to make the reversible net-work robust to small perturbations caused by numerical er-ror.
Although the unbiased linear transform based on
Cholesky decomposition [19] can preserve feature affinity, it cannot guarantee pixel affinity. Inspired by [25, 28], we introduce Matting Laplacian [22] loss to train the network and preserve pixel affinity. Matting Laplacian [22] may result in blurry images when it is used with another net-work like one with an encoder-decoder architecture. But it does not have this issue in CAP-VSTNet, since the bijective transformation of reversible network theoretically requires all information to be preserved.
CAP-VSTNet can be flexibly applied to versatile style transfer, including photorealistic and artistic image/video style transfer. We conduct extensive experiments to eval-uate its performance. The results show it can produce better qualitative and quantitative results in comparison with the state-of-the-art image style transfer methods. We show that with minor loss function modifications, CAP-VSTNet can perform stable video style transfer and outperforms existing methods. 2.