Abstract
Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined
In this work, we present CLIP-S4 that leverages classes. self-supervised pixel representation learning and vision-language models to enable various semantic segmenta-tion tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different aug-mented views of images. To further improve the pixel em-beddings and enable language-driven semantic segmenta-tion, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP [34]; and 2) semantic con-sistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CLIP-S4 en-ables a new task of class-free semantic segmentation where no unknown class information is needed during training. As a result, our approach shows consistent and substantial per-formance improvement over four popular benchmarks com-pared with the state-of-the-art unsupervised and language-driven semantic segmentation methods. More importantly, our method outperforms these methods on unknown class recognition by a large margin. 1.

Introduction
Semantic segmentation aims to partition an input image into semantically meaningful regions and assign each re-gion a semantic class label. Recent advances in seman-tic segmentation [6, 27, 48] heavily rely on pixel-wise hu-man annotations, which have two limitations. First, ac-quiring pixel-wise annotations is extremely labor intensive and costly, which can take up to 1.5 hours to label one im-age [31]. Second, human annotations are often limited to a set of predefined semantic classes, with which the learned models lack the ability to recognize unknown classes [25].
Figure 1. (a) Pixel embeddings from different CLIP-based un-supervised methods: Our method, CLIP-S4, generates sharper and more coherent pixel embeddings than MaskCLIP [49] and
MaskCLIP+’s [49]; (b) Language-driven semantic segmentation by different methods: CLIP-S4 can recognize challenging un-known classes (e.g., moon); (c) The key idea behind CLIP-S4: aligning the pixel embeddings and their semantics with CLIP fea-ture space.
Various approaches have been proposed to tackle these limitations, among which we are inspired by two lines of recent research in particular. First, for unsupervised seman-tic segmentation (i.e., without human annotations), self-supervised pixel representation learning approaches [14,18, 19, 23, 40] have shown promising results on popular un-supervised benchmarks. The main idea is to extend self-supervised contrastive learning [7, 16] from images to pix-els by attracting each pixel’s embedding to its positive pairs and repelling it from negative pairs. The prior of pairs can be contours [18, 19], hierarchical groups [23], salience maps [40], and pre-trained models [14]. Although these approaches can group pixels into semantically meaningful clusters, human annotations are still needed to assign class labels to the clusters for semantic segmentation [37].
Second, for unknown classes in semantic segmenta-tion, large-scale vision-language models such as CLIP [34]
have shown great potential. This line of research, called language-driven semantic segmentation, aims to segment images with arbitrary classes defined by texts during test-ing time [25, 37, 46, 49]. Among these methods, most still need training time annotations, such as pixel annota-tions
[25] and captions [46]. Only a few recent work,
MaskCLIP & MaskCLIP+ [49] attempts to address this without using additional supervision: MaskCLIP directly extracts pixel embeddings correlated with texts from CLIP, but these pixel embeddings are coarse and noisy (Fig. 1a).
To address this issue, MaskCLIP+ [49] trains a segmenta-tion model on the pseudo-labels generated by MaskCLIP for a set of predefined classes. However, the pixel embed-dings of MaskCLIP+ are distorted by the predefined classes (Fig. 1a), which limits its ability to recognize unknowns (Fig. 1b). Also, it needs unknown class information during training, which hinders its real-world applications.
We propose a language-guided self-supervised semantic segmentation approach, CLIP-S4, which takes advantage of the strengths from both lines of research and addresses their limitations accordingly. The key idea is to learn consistent pixel embeddings with respect to visual and conceptual se-mantics using self-supervised learning and the guidance of a vision-language model, CLIP.
Specifically, we first train pixel embeddings with pixel-segment contrastive learning from different augmented im-age views [18, 19, 23] such that images can be partitioned into visually meaningful regions. To further improve pixel embedding quality and enable language-driven semantic segmentation, we introduce vision-language model guided consistency to regularize our model (Fig. 1c). The con-sistency is enforced from two aspects: embedding consis-tency and semantic consistency. First, embedding consis-tency aims to align the pixel embeddings generated by our model with the joint feature space of texts and images of
CLIP by minimizing the distance between the pixel embed-dings generated by our model and CLIP. Second, semantic consistency forces our model to make the same prediction as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Note that unlike the previous methods [25,49] that use a predefined set of known classes, CLIP-S4 also learns the representation of unknown classes from images during training.
In the end, CLIP-S4 also enables a new task, namely class-free semantic segmentation, as shown in Tab. 1. This new task does not need any human annotations and even as-sumes NO class names are given during training. This is a more challenging task than the recent work [49] that re-quires class names of both known and unknown.
In summary, the contributions of this paper are threefold:
• We propose a self-supervised semantic segmenta-tion approach that combines pixel-segment contrastive learning with the guidance of pre-trained vision lan-Known Unknown e m a
N
Cls e m a
N
Cls ot. n n
A ot. n n
A
Info. d. d
A
Un/Self-supervised ( [19] etc.)
Supervised ( [27] etc.)
Zero-shot ( [3] etc.)
✓ ✓ N/A N/A
✓ ✓
Language- MaskCLIP+ [49]
Driven
CLIP-S4
✓
✓
Fine-Tuning
N/A
✓ Word2Vec, etc.
✓
CLIP
CLIP
Table 1. Comparison of information required for training over dif-ferent tasks. CLIP-S4 enables a new task called class-free seman-tic segmentation. Compared with MaskCLIP+ [49], the new task assumes unknown class names are NOT given during training. guage models. Our method can generate high-quality pixel embeddings without any human annotations and be applied to a variety of semantic segmentation tasks.
• We open up new research potentials for language-driven semantic segmentation without any human an-notations by introducing and addressing a new task of class-free semantic segmentation (Tab. 1). Un-like previous work that assumes all the class names are known during training, our method can discover unknown classes from unlabelled image data without even knowing unknown class names.
• Consistent and substantial gains are observed with our approach over the state-of-the-art unsupervised and language-driven semantic segmentation methods on four popular datasets. More importantly, our method significantly outperforms the state-of-the-art on the segmentation of unknown classes. 2.