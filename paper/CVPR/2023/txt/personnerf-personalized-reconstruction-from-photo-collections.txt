Abstract
We present PersonNeRF, a method that takes a collec-tion of photos of a subject (e.g. Roger Federer) captured across multiple years with arbitrary body poses and ap-pearances, and enables rendering the subject with arbitrary novel combinations of viewpoint, body pose, and appear-ance. PersonNeRF builds a customized neural volumetric 3D model of the subject that is able to render an entire space spanned by camera viewpoint, body pose, and appearance.
A central challenge in this task is dealing with sparse ob-servations; a given body pose is likely only observed by a single viewpoint with a single appearance, and a given appearance is only observed under a handful of different body poses. We address this issue by recovering a canon-ical T-pose neural volumetric representation of the subject that allows for changing appearance across different ob-servations, but uses a shared pose-dependent motion field across all observations. We demonstrate that this approach, along with regularization of the recovered volumetric ge-ometry to encourage smoothness, is able to recover a model that renders compelling images from novel combinations of viewpoint, pose, and appearance from these challenging un-structured photo collections, outperforming prior work for free-viewpoint human rendering.
1.

Introduction
We present a method for transforming an unstructured personal photo collection, containing images spanning mul-tiple years with different outfits, appearances, and body poses, into a 3D representation of the subject. Our system, which we call PersonNeRF, enables us to render the subject under novel unobserved combinations of camera viewpoint, body pose, and appearance.
Free-viewpoint rendering from unstructured photos is a particularly challenging task because a photo collection can contain images at different times where the subject has dif-ferent clothing and appearance. Furthermore, we only have access to a handful of images for each appearance, so it is unlikely that all regions of the body would be well-observed for any given appearance. In addition, any given body pose is likely observed from just a single or very few camera viewpoints.
We address this challenging scenario of sparse viewpoint and pose observations with changing appearance by mod-eling a single canonical-pose neural volumetric represen-tation that uses a shared motion weight field to describe how the canonical volume deforms with changes in body pose, all conditioned on appearance-dependent latent vec-tors. Our key insight is that although the observed body poses have different appearances across the photo collec-tion, they should all be explained by a common motion model since they all come from the same person. Further-more, although the appearances of a subject can vary across the photo collection, they all share common properties such as symmetry so embedding appearance in a shared latent space can help the model learn useful priors.
To this end, we build our work on top of Human-NeRF [46], which is a state-of-the-art free-viewpoint hu-man rendering approach that requires hundreds of images of a subject without clothing or appearance changes. Along with regularization, we extend HumanNeRF to account for sparse observations as well as enable modeling diverse ap-pearances. Finally, we build an entire personalized space spanned by camera view, body pose, and appearance that allows intuitive exploration of arbitrary novel combinations of these attributes (as shown in Fig. 1). 2.