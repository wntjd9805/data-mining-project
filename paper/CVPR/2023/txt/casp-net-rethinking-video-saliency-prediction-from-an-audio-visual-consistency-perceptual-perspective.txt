Abstract
Incorporating the audio stream enables Video Saliency
Prediction (VSP) to imitate the selective attention mech-anism of human brain. By focusing on the benefits of joint auditory and visual information, most VSP methods are capable of exploiting semantic correlation between vi-sion and audio modalities but ignoring the negative effects due to the temporal inconsistency of audio-visual intrinsics.
Inspired by the biological inconsistency-correction within multi-sensory information, in this study, a consistency-aware audio-visual saliency prediction network (CASP-Net) is proposed, which takes a comprehensive considera-tion of the audio-visual semantic interaction and consistent perception. In addition a two-stream encoder for elegant association between video frames and corresponding sound source, a novel consistency-aware predictive coding is also designed to improve the consistency within audio and vi-sual representations iteratively. To further aggregate the multi-scale audio-visual information, a saliency decoder is introduced for the final saliency map generation. Substan-tial experiments demonstrate that the proposed CASP-Net outperforms the other state-of-the-art methods on six chal-lenging audio-visual eye-tracking datasets. For a demo of our system please see our project webpage. 1.

Introduction
The task of saliency prediction is to automatically esti-mate the most prominent area in a scenario by simulating human selective attention. It has been extended to an al-ternative way to extract the most valuable information from a massive of data, which serves wide applications such as robotic camera control [7], video captioning [35], motion tracking [30], image quality evaluation [50] and video com-pression [51], etc.
In recent years, a lot of saliency prediction works have been developed by their increasing attention [4, 15, 41–43,
*Indicates equal contributions.
†Corresponding author: zh0036ng@nwpu.edu.cn.
Figure 1. The example figure shows the saliency results of our model compared to STAViS [39] in audio and video temporal se-quences. In the last time segment, the audio information that oc-curs in the event is inconsistent with the visual information. Our method can cope with such challenge by automatically learning to align the audio-visual features. The results of STAViS, however, show that it is incapable to address the problem of audio-visual inconsistency. GT denotes ground truth. 49]. According to different data types, these studies can be categorized into Image Saliency Prediction (ISP) and Video
Saliency Prediction (VSP). The ISP investigates how to combine the low-level heuristic characteristics (e.g., colour, texture and luminance) with high-level semantic image at-tributes to predict prominent areas in the scene [15, 41, 42].
Differently, VSP exploits how to apply the spatio-temporal structure information in videos, and benefits the perception and identification of dynamic scenes [4, 49].
From the view of data modalities, the vision and au-dio present the video content from different sensing, which complement each other to enhance the perception. Based on multi-modal data, more recent studies have that audio information can significantly improve the understanding of the video semantics [33, 37, 39]. Min et al. [33] conduct a cross-modal kernel canonical correlation analysis (CCA)
by exploring audio-visual correspondence clues, and ef-fectively enhance the video-level saliency prediction accu-racy. Tsiami et al. [39] propose a deep model by combin-ing spatio-temporal visual and auditory information to ad-dress the video saliency estimation efficiently. Neverthe-less, these works heavily depend on temporal consistency of visual and audio information, and thus may suffer an unexpected degradation in practical scenarios, where such consistency cannot be satisfied as shown in Figure 1.
Temporal inconsistency commonly exists in real-life videos because realistic visual scenarios usually contain multiple sound sources, which may come from on-screen (e.g., dialogue in a talk show), or from off-screen (e.g., nar-ration in a movie). Without understanding the complex sce-nario components, simply performing audio-visual consis-tency learning would result in an irrelevant semantic match-ing. A promising solution to this challenge is motivated by the study of neuroscience [18, 36], which explains how our brain minimizes the matching errors within multisen-sory data using both iterative inference and learning, and also inspired the Consistency-aware Audio-visual Saliency
Prediction network CASP-Net of this study.
By substantially exploring the latent semantic correla-tions of cross-modal signals, in CASP-Net, the potential temporal inconsistency between different modalities can be
In addition, a two-stream network is corrected as well. also introduced to elegantly associate video frames with the corresponding sound source, which is able to achieve semantic similarities between audio and visual features by cross-modal interaction. To further reason the coherent vi-sual and audio content in an iterative feedback manner, a consistency-aware predictive coding (CPC) module is de-signed. Subsequently, a saliency decoder (SalDecoder) is proposed to aggregate the multi-scale audio-visual informa-tion from all previous decoder’s blocks and to generate the final saliency map. The main contributions in this work can be summarized as follows: (1) A novel audio-visual saliency prediction model is proposed by comprehensively considering the functionali-ties of audio-visual semantic interaction and consistent per-ception. (2) A consistency-aware predictive coding module is designed to improve the consistency within audio and vi-sual representations iteratively. (3) Solid experiments have been conducted on six audio-visual eye-tracking datasets, which demonstrate a superior performance of the proposed method in comparison to the other state-of-the-art works. 2.