Abstract
We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cam-eras. Event cameras provide visual information with sub-millisecond latency at a high-dynamic range and with strong robustness against motion blur. These unique prop-erties offer great potential for low-latency object detec-tion and tracking in time-critical scenarios. Prior work in event-based vision has achieved outstanding detection per-formance but at the cost of substantial inference time, typ-ically beyond 40 milliseconds. By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance.
To achieve this, we explore a multi-stage design that uti-lizes three key concepts in each stage: ﬁrst, a convolutional prior that can be regarded as a conditional positional em-bedding. Second, local and dilated global self-attention for spatial feature interaction. Third, recurrent temporal fea-ture aggregation to minimize latency while retaining tempo-ral information. RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detec-tion - achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference (< 12 ms on a T4 GPU) and favorable parameter efﬁciency (5× fewer than prior art). Our study brings new insights into effective design choices that can be fruitful for research be-yond event-based vision.
Code: https://github.com/uzh-rpg/RVT 1.

Introduction
Time matters for object detection. In 30 milliseconds, a human can run 0.3 meters, a car on public roads covers up to 1 meter, and a train can travel over 2 meters. Yet, during this time, an ordinary camera captures only a single frame.
Frame-based sensors must strike a balance between la-tency and bandwidth. Given a ﬁxed bandwidth, a frame-based camera must trade-off camera resolution and frame rate. However, in highly dynamic scenes, reducing the res-olution or the frame rate may come at the cost of missing essential scene details, and, in safety-critical scenarios like
Figure 1. Detection performance vs inference time of our RVT models on the 1 Mpx detection dataset using a T4 GPU. The circle areas are proportional to the model size. automotive, this may even cause fatalities.
In recent years, event cameras have emerged as alter-native sensor that offers a different trade-off.
Instead of counterbalancing bandwidth requirements and perceptual latency, they provide visual information at sub-millisecond latency but sacriﬁce absolute intensity information.
In-stead of capturing intensity images, event cameras measure changes in intensity at the time they occur. This results in a stream of events, which encode time, location, and po-larity of brightness changes [14]. The main advantages of event cameras are their sub-millisecond latency, very high dynamic range (> 120 dB), strong robustness to motion blur, and ability to provide events asynchronously in a con-tinuous manner.
In this work, we aim to utilize these outstanding proper-ties of event cameras for object detection in time-critical scenarios. Therefore, our objective is to design an ap-proach that reduces the processing latency as much as possi-ble while maintaining high performance. This is challeng-ing because event cameras asynchronously trigger binary events that are spread of pixel space and time. Hence, we need to develop detection algorithms that can continuously associate features in the spatio-temporal domain while si-multaneously satisfying strict latency requirements.
Recent work has shown that dynamic graph neural net-works (GNNs) [28, 43] and sparse neural networks [10, 34, 55, 57] can theoretically achieve low latency inference for
event-based object detection. Yet, to achieve this in prac-tical scenarios they either require specialized hardware or their detection performance needs to be improved.
An alternative thread of research approaches the problem from the view of conventional, dense neural network de-signs [7,19,20,26,38]. These methods show impressive per-formance on event-based object detection, especially when using temporal recurrence in their architectures [26, 38].
Still, the processing latency of these approaches remains beyond 40 milliseconds such that the low-latency aspect of event cameras cannot be fully leveraged. This raises the question: How can we achieve both high accuracy and efﬁ-ciency without requiring specialized hardware?
We notice that common design choices yield a subop-timal trade-off between performance and compute. For example, prior work uses expensive convolutional LSTM (Conv-LSTM) cells [44] extensively in their feature extrac-tion stage [26, 38] or relies on heavy backbones such as the VGG architecture [26]. Sparse neural networks instead struggle to model global mixing of features which is crucial to correctly locate and classify large objects in the scene.
To achieve our main objective, we fundamentally revisit the design of vision backbones for event-based object de-tection. In particular, we take inspiration from neural net-work design for conventional frame-based object detection and combine them with ideas that have proven successful in the event-based vision literature. Our study deliberately focuses on macro design of the object detection backbone to identify key components for both high performance and fast inference on GPUs. The resulting neural network is based on a single block that is repeated four times to form a multi-stage hierarchical backbone that can be used with off-the-shelf detection frameworks.
We identify three key components that enable an excel-lent trade-off between detection performance and inference time. First, we ﬁnd that interleaved local- and global self-attention [50] is ideally suited to mix both local and global features while offering linear complexity in the input reso-lution. Second, this attention mechanism is most effective when preceded by a simple convolution that also downsam-ples the spatial resolution from the previous stage. This convolution effectively provides a strong prior about the grid-structure of the pixel array and also acts as a condi-tional positional embedding for the transformer layers [9].
Third, temporal recurrence is paramount to achieve strong detection performance with events. Differently from prior work, we ﬁnd that Conv-LSTM cells can be replaced by plain LSTM cells [18] that operate on each feature sepa-rately1. By doing so, we dramatically reduce the number of parameters and latency but also slightly improve the over-all performance. Our full framework achieves competitive performance and higher efﬁciency compared to state-of-the-1equivalent to 1 × 1 kernel in a Conv-LSTM cell art methods. Speciﬁcally, we reduce parameter count (from 100M to 18.5 M) and inference time (from 72 ms to 12 ms) up to a factor of 6 compared to prior art [26]. At the same time, we train our networks from scratch, showing that these beneﬁts do not originate from large-scale pretraining.
Our paper can be summarized as follows: (1) We re-examine predominant design choices in event-based object detection pipelines and reveal a set of key enablers for high performance in event-based object detection. (2) We pro-pose a simple, composable stage design that uniﬁes the cru-cial building blocks in a compact way. We build a 4-stage hierarchical backbone that is fast, lightweight and still of-fers performance comparable to the best reported so far. (3)
We achieve state-of-the-art object detection performance of 47.2% mAP on the Gen1 detection dataset [11] and a highly competitive 47.4% mAP on the 1 Mpx detection dataset
[38] while training the proposed architecture from scratch.
In addition, we also provide insights into effective data aug-mentation techniques that contribute to these results. 2.