Abstract
This work focuses on a practical knowledge transfer task defined as Source-Free Unsupervised Domain Adaptation (SFUDA), where only a well-trained source model and un-labeled target data are available. To fully utilize source knowledge, we propose to transfer the class relationship, which is domain-invariant but still under-explored in previ-ous works. To this end, we first regard the classifier weights of the source model as class prototypes to compute class re-lationship, and then propose a novel probability-based sim-ilarity between target-domain samples by embedding the source-domain class relationship, resulting in Class Rela-tionship embedded Similarity (CRS). Here the inter-class term is particularly considered in order to more accurately represent the similarity between two samples, in which the source prior of class relationship is utilized by weighting.
Finally, we propose to embed CRS into contrastive learn-ing in a unified form. Here both class-aware and instance discrimination contrastive losses are employed, which are complementary to each other. We combine the proposed method with existing representative methods to evaluate its efficacy in multiple SFUDA settings. Extensive experimen-tal results reveal that our method can achieve state-of-the-art performance due to the transfer of domain-invariant class relationship. 1 1.

Introduction
Benefiting from the large amount of labeled training data, deep neural networks have achieved promising results in many computer vision tasks [7, 15, 19, 95]. To reduce the annotation cost, Unsupervised Domain Adaptation (UDA) has been devised by transferring knowledge from a label-rich source domain to a label-scarce target domain. Cur-rently, many UDA methods have been proposed that jointly
*Corresponding author 1Code is available at https://github.com/zhyx12/CRCo
Figure 1. Illustration of our proposed method. The upper left rep-resents the target-domain featrue distribution by source pre-trained model. The upper right is the feature distribution obtained by our method. The bottom is the process of class relationship embedded contrastive learning. Best viewed in color. learn on the source and target data. But they would be unapplicable for some real-world scenarios involving pri-vacy (e.g., medical images, surveillance videos) because the source-domain data cannot be accessed. Thus, more recent methods [36, 42, 71, 79, 87, 88] focus on Source-Free Un-supervised Domain Adaptation (SFUDA). Under this set-ting, the labeled source data are not accessible any more when training the target model, but the pre-trained model in the source domain is provided. Then a natural question arises, i.e., what knowledge should we transfer to facilitate the learning of unlabeled target-domain data?
Some methods [21, 36, 42] assume that the source hy-pothesis (i.e., classifier [42] or whole model [21, 36]) con-tains sufficient knowledge for target domain. Then they transfer source knowledge by directly aligning features with the fixed source classifier [42], resorting to historical mod-els [21], or weight regularization [36]. Another line of works [87, 89, 90] assume the source model already forms some semantic structure, and then utilize the local infor-mation (i.e., neighborhood) of feature space to enforce the similarity in the output space. Despite these progress, what knowledge to be transferred remains an open question.
In this work, we propose to transfer the class relation-ship represented by the similarity of classifier weights. Ac-tually, the class relationship is domain-invariant [14], since the same class in the source and target domains essentially represents the same semantic in spite of domain discrep-ancy. For example, the computers are always more simi-lar with the TV than the scissors. Thus, it is reasonable to guide the target domain learning using class relationship prior. Unlike previous methods that learn class distribu-tion by pseudo labeling [21, 42] and local aggregation of neighborhood predictions [79, 87, 89, 90], here we explic-itly model the source-domain class relationship. To be spe-cific, we regard each weight of classifier as the class proto-type [90], and then compute a class relationship matrix As by cosine similarity, as shown in Figure 1.
Before explaining how to use this matrix, we illustrate the purpose of representation learning in the target domain using Figure 1, where three classes are adopted for clarity.
The top left shows the target-domain feature distribution to-gether with the class weights learned from the source do-main. It can be seen that such a situation makes it difficult to perform correct classification. In fact, it is expected that the learned features are discriminative and compact around the corresponding class weight, as shown on the top right.
To this end, an intuitive way is to make the relationship of learned class prototypes in the target domain consistent with that in the source domain. However, it has a very lim-ited effect on training the target-domain model as such a prototype-level constraint is too weak for optimization. i̸=j pip′
In this work, we propose to embed the source-domain class relationship in contrastive learning, which has been shown to be outstanding in representation learning [9, 18].
Here we design a novel sample similarity by taking into ac-count As. Specifically, we compute the similarity between two target domain samples in the output space, represented by the prediction probabilities (i.e., p and p′), as shown in
Figure 1. In particular, we consider the inter-class term (i.e. (cid:80) j) in addition to the traditional intra-class term (i.e. (cid:80) i). Note that the sum of these two terms equals one, in which the intra-class term measures the similarity that two samples come from the same class, while the inter-class term measures that from different classes. Consider-ing the relationship between classes, we weight the inter-class term by the non-diagonal elements in As. In partic-ular, if the class i is closer to j than k (i.e., As ik), pip′ j would be more important in calculating the similarity of p and p′. By the above design, our proposed similarity ij > As i pip′ can more accurately express the relationship of two samples based on output space. For example, among three classes 1, 2, 3 in Figure 1, the classes 1 and 2 are closer. Given three samples x1, x2, x3 belonging to them with the probabilities
[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], and [0.05, 0.05, 0.9].
If we only use the intra-class term, all three samples have the same similarity. But for our designed similarity, x1 is closer to x2 than x3, which is more reasonable.
On the basis of the proposed similarity, we further pro-pose to perform contrastive learning in a unified form. In-spired by recent success in semi-supervised learning [67, 85, 91] and unsupervised learning [9, 18], we propose two types of contrastive losses. As shown in Figure 1, the first one is Class Relationship embedded Class-Aware
Contrastive (CR-CACo) loss where the high-confident sam-ples are enforced to be close to the corresponding proto-type and away from other prototypes. Due to embedding the prior class relationship, our CR-CACo loss is more ro-bust to label noise caused by domain shift. The second one is Class Relationship embedded Instance Discrimination
Contrastive (CR-IDCo) loss where two views of the same sample are encouraged to be close and away from all other samples. Benefited from our designed accurate similarity, the CR-IDCo loss would more effectively learn discrimina-tive features [9, 18, 75]. Actually, these two losses are com-plementary to each other, and their combination can achieve better performance.
Our contributions are summarized as follows:
• We propose to explicitly transfer class relationship for
SFUDA which is more domain-invariant. And we pro-pose to embed the class relationship into contrastive learning in order to effectively perform representation learning in the target domain.
• We propose a novel class relationship embedded sim-ilarity which can more accurately express the sample relationship in the output space. Furthermore, we pro-pose two contrastive losses (i.e., CR-CACo and CR-IDCo) exploiting our designed similarity.
• We conduct extensive experiments to evaluate the pro-posed method, and the results validate the effective-ness of our method, which achieves the state-of-the-art performance on multiple SFUDA benchmarks. 2.