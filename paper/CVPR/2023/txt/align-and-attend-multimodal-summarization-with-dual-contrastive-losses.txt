Abstract
The goal of multimodal summarization is to extract the most important information from different modalities to form summaries. Unlike unimodal summarization, the multimodal summarization task explicitly leverages cross-modal information to help generate more reliable and high-quality summaries. However, existing methods fail to lever-age the temporal correspondence between different modal-ities and ignore the intrinsic correlation between differ-ent samples. To address this issue, we introduce Align and Attend Multimodal Summarization (A2Summ), a uni-fied multimodal transformer-based model which can ef-In ad-fectively align and attend the multimodal input. dition, we propose two novel contrastive losses to model both inter-sample and intra-sample correlations. Exten-sive experiments on two standard video summarization datasets (TVSum and SumMe) and two multimodal sum-marization datasets (Daily Mail and CNN) demonstrate the superiority of A2Summ, achieving state-of-the-art perfor-mances on all datasets. Moreover, we collected a large-scale multimodal summarization dataset BLiSS, which contains livestream videos and transcribed texts with annotated sum-maries. Our code and dataset are publicly available at https://boheumd.github.io/A2Summ/. 1.

Introduction
With the development in multimodal learning, multi-modal summarization has drawn increasing attention [1–9].
Different from traditional unimodal summarization tasks, such as video summarization [10–17] and text summariza-tion [18–22], multimodal summarization aims at generating summaries by utilizing the information from different modal-ities. With the explosive growing amount of online content (e.g., news, livestreams, vlogs, etc.), multimodal summa-rization can be applied in many real-world applications. It provides summarized information to the users, which is es-pecially useful for redundant long videos such as livestream
*Part of this work was done when Bo was an intern at Adobe Research.
Figure 1. A2Summ is a unified multimodal summarization frame-work, which aligns and attends multimodality inputs while lever-aging time correspondence (e.g., video and transcript) and outputs the selected important frames and sentences as summaries. and product review videos.
Previous multimodal summarization methods [2,4,23,24] leverage the additional modality information but can only generate the main modality summary, i.e., either a video summary or a text summary, severely limiting the use of complementary benefits in the additional modality. Recently, multimodal summarization with multimodal output (MSMO) has been explored in several studies [1, 6, 25, 26], which aim at generating both video and text summaries using a joint model. Compared to previous methods, which only produce a unimodal summary, MSMO provides a better user experience with an easier and faster way to get useful information. However, we find that the existing MSMO methods still have the following limitations. First, even if both modalities are learned together, the correspondence between different modalities is not exploited. For example, given a video and its transcripts, which are automatically matched along the time axis, no existing method utilizes the mutual temporal alignment information and treats the two modalities separately. Second, previous works adopt simple strategies to model the cross-modal correlation by sequence modeling and attention operation [1, 4, 25, 25, 26, 26], which requires a large number of annotated multimodal data which is hard to obtain.
Motivated by the above observations, we propose a novel architecture for multimodal summarization based on a uni-fied transformer model, as shown in Figure 1. First, to leverage the alignment information between different modal-ities, we propose alignment-guided self-attention module to align the temporal correspondence between video and text modalities and fuse cross-modal information in a unified manner. Second, inspired by the success of self-supervised training [27–29], which utilizes the intrinsic cross-modality correlation within the same video and between different videos, we propose dual contrastive losses with the combi-nation of an inter-sample and an intra-sample contrastive loss, to model the cross-modal correlation at different gran-ularities. Specifically, the inter-sample contrastive loss is applied across different sample pairs within a batch, which leverages the intrinsic correlation between each video-text pair and contrasts them against remaining unmatched sam-ples to provide more training supervision. Meanwhile, the intra-sample contrastive loss operates within each sample pair, which exploits the mutual similarities between ground-truth video and text summaries and contrasts the positive features against hard-negative features.
To facilitate the research of long video summarization with multimodal information, we also collected a large-scale livestream video dataset from the web. Livestream broadcast-ing is growing rapidly, and the summarization of livestream videos is still an unexplored area with great potential. Pre-vious video summarization datasets consist of short videos with great variations in scene transitions. On the contrary, livestream videos are significantly longer (in hours as op-posed to minutes) and the video content changes much more slowly over time, which makes it even harder for the sum-marization task. Besides, there has been a lack of annotated datasets with focus on transcript summarization, which can be a great complement to the livestream video summariza-tion. Therefore, we collect a large-scale multimodal sum-marization dataset with livestream videos and transcripts, which are both annotated with ground-truth summaries by selecting important frames and sentences.
To summarize, our contributions include:
• We propose A2Summ, a unified transformer-based ar-chitecture for multimodal summarization. It can handle multimodal input with time correspondences which pre-vious work neglects.
• We present dual contrastive losses that account for mod-eling cross-modal information at different levels. Ex-tensive experiments on multiple datasets demonstrate the effectiveness and superiority of our design.
• A large-scale Behance LiveStream Summarization (BLiSS) dataset is collected containing livestream videos and transcripts with multimodal summaries. 2.