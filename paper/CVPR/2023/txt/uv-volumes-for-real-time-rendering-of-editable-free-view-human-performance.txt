Abstract
Neural volume rendering enables photo-realistic render-ings of a human performer in free-view, a critical task in immersive VR/AR applications. But the practice is severely limited by high computational costs in the rendering pro-cess. To solve this problem, we propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in real-time. It separates the high-frequency (i.e., non-smooth) human appearance from the 3D volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV volumes allow much smaller and shallower neural networks to obtain densities and texture
*Authors contributed equally to this work.
†Corresponding Author. This work is partly supported by the Na-tional Key Research and Development Program of China under Grant 2022YFB3303800 and National Key Projects of China, 2021XJTU0040. coordinates in 3D while capturing detailed appearance in 2D NTS. For editability, the mapping between the parame-terized human model and the smooth texture coordinates al-lows us a better generalization on novel poses and shapes.
Furthermore, the use of NTS enables interesting applica-tions, e.g., retexturing. Extensive experiments on CMU
Panoptic, ZJU Mocap, and H36M datasets show that our model can render 960 × 540 images in 30FPS on average with comparable photo-realism to state-of-the-art methods.
The project and supplementary materials are available at https://fanegg.github.io/UV-Volumes. 1.

Introduction
Synthesizing a free-view video of a human performer in motion is a long-standing problem in computer vision.
Early approaches [4] rely on obtaining an accurate 3D mesh sequence through multi-view stereo. However, the com-puted 3D mesh often fails to depict the complex geome-try structure, resulting in limited photorealism.
In recent years, methods (e.g., NeRF [33]) that make use of volumet-ric representation and differentiable ray casting have shown promising results for novel view synthesis. These tech-niques have been further extended to tackle dynamic scenes.
Nonetheless, NeRF and its variants require a large number of queries against a deep Multi-Layer Perceptron (MLP). Such time-consuming computation prevents them from being applied to applications that require high ren-dering efficiency. In the case of static NeRF, a few meth-ods [10, 42, 58] have already achieved real-time perfor-mance. However, for dynamic NeRF, solutions for real-time rendering of volumetric free-view video are still lacking.
In this work, we present UV Volumes, a novel frame-work that can produce an editable free-view video of a hu-man performer in motion and render it in real-time. Specif-ically, we take advantage of a pre-defined UV-unwrapping (e.g., SMPL or dense pose) of the human body to tackle the geometry (with texture coordinates) and textures in two branches. We employ a sparse 3D Convolutional Neu-ral Networks (CNN) to transform the voxelized and struc-tured latent codes anchored with a posed SMPL model to a 3D feature volume, in which only smooth and view-independent densities and UV coordinates are encoded. For rendering efficiency, we use a shallow MLP to decode the density and integrate the feature into the image plane by volume rendering. Each feature in the image plane is then individually converted to the UV coordinates. Accordingly, we utilize the yielded UV coordinates to query the RGB value from a pose-dependent neural texture stack (NTS).
This process greatly reduces the number of queries against
MLPs and enables real-time rendering.
It is worth noting that the 3D Volumes in the proposed framework only need to approximate relatively “smooth” signals. As shown in Figure 2, the magnitude spectrum of the RGB image and the corresponding UV image indi-cates that UV is much smoother than RGB. That is, we only model the low-frequency density and UV coordinate in the 3D volumes, and then detail the appearance in the 2D NTS, which is also spatially aligned across different poses. The disentanglement also enhances the generalization ability of such modules and supports various editing operations.
We perform extensive experiments on three widely-used datasets: CMU Panoptic, ZJU Mocap, and H36M datasets.
The results show that the proposed approach can effec-tively generate an editable free-view video from both dense and sparse views. The produced free-view video can be rendered in real-time with comparable photorealism to the state-of-the-art methods that have much higher computa-tional costs. In summary, our major contributions are:
Figure 2. Discrete Fourier Transform (DFT) for RGB and UV image. In the magnitude spectrum, the distance from each point to the midpoint describes the frequency, the direction from each point to the midpoint describes the direction of the plane wave, and the value of the point describes its amplitude. The distribution of the UV magnitude spectrum is more concentrated in the center, which indicates that the frequency of the UV image is lower.
• A novel system for rendering editable human perfor-mance video in free-view and real-time.
• UV Volumes, a method that can accelerate the render-ing process while preserving high-frequency details.
• Extended editing applications enabled by this frame-work, such as reposing, retexturing, and reshaping. 2.