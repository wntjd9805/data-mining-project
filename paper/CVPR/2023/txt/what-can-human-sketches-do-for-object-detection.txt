Abstract
Sketches are highly expressive, inherently capturing sub-jective and fine-grained visual cues. The exploration of such innate properties of human sketches has, however, been lim-ited to that of image retrieval. In this paper, for the first time, we cultivate the expressiveness of sketches but for the fundamental vision task of object detection. The end result is a sketch-enabled object detection framework that detects based on what you sketch – that “zebra” (e.g., one that is eating the grass) in a herd of zebras (instance-aware de-tection), and only the part (e.g., “head” of a “zebra”) that you desire (part-aware detection). We further dictate that our model works without (i) knowing which category to ex-pect at testing (zero-shot) and (ii) not requiring additional bounding boxes (as per fully supervised) and class labels (as per weakly supervised).
Instead of devising a model from the ground up, we show an intuitive synergy between foundation models (e.g., CLIP) and existing sketch models build for sketch-based image retrieval (SBIR), which can al-ready elegantly solve the task – CLIP to provide model gen-eralisation, and SBIR to bridge the (sketch→photo) gap.
In particular, we first perform independent prompting on both sketch and photo branches of an SBIR model to build highly generalisable sketch and photo encoders on the back of the generalisation ability of CLIP. We then devise a train-ing paradigm to adapt the learned encoders for object de-tection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings from
SBIR. Evaluating our framework on standard object de-tection datasets like PASCAL-VOC and MS-COCO outper-forms both supervised (SOD) and weakly-supervised ob-ject detectors (WSOD) on zero-shot setups. Project Page: https:// pinakinathc.github.io/ sketch-detect 1.

Introduction
Sketches have been used from prehistoric times for hu-mans to express and record ideas [35, 76]. The level of ex-pressiveness [28, 41] they carry remains unparalleled today even in the face of language [14, 82] – recall that moment that you want to resort to pen and paper (or Zoom White-Figure 1. We train an object detector using SBIR models. (a)
First, we train an FG-SBIR model using existing sketch–photo pairs that generalise to unseen categories. (b) To train the object detector module, we tile multiple object-level photos from SBIR datasets [75] and use its paired sketch encoding via a pre-trained sketch encoder to align the region embedding of detected boxes. (c) Inclusion of sketches for object detection opens several avenues like detecting a specific object for query sketch (e.g., detect a “ze-bra” eating grass) or part of an object (e.g., “head” of “zebra”). board) to sketch down an idea?
Sketch research has also flourished over the past decade
[16, 70, 90, 99], with a whole spectrum of works on tradi-tional tasks such as classification [30] and synthesis [15, 31, 54], and those more sketch-specific such as modelling visual abstraction [1, 59], style transfer [74] and continu-ous stroke fitting [17], to cute applications such as turning a sketch into a photo classifier [5, 37].
The expressiveness of sketches, however, has been only explored in the form of sketch-based image retrieval (SBIR)
[19, 70, 94], especially the fine-grained [2, 6, 9] variant (FG-SBIR). Great strides have been made, with recent systems already reaching maturity for commercial adaptation [6] – a great testimony to how cultivating sketch expressiveness can make a real impact.
In this paper, we ask the question – what can human sketches do for the fundamental vision tasks of object detec-tion? The envisaged outcome is, therefore, a sketch-enabled object detection framework that detects based on what you sketch, i.e., how you want to express yourself. Sketching a “zebra eating the grass” (in Fig. 1) should detect “that”
zebra from a herd of zebras (instance-aware detection), and it will also give you the freedom to be specific with parts (part-aware detection), so if the “head” of a “zebra” is what you would rather desire, then just sketch the very head.
Instead of devising a sketch-enabled object detection model from the ground up, we show that an intuitive syn-ergy between foundation models (e.g., CLIP [64]) and off-the-shelf SBIR models [8,100] can already, rather elegantly, solve the problem – CLIP to provide model generalization, and SBIR to bridge the (sketch→photo) gap.
In partic-ular, we adapt CLIP to build sketch and photo encoders (branches in a common SBIR model) by learning indepen-dent prompt vectors [42, 71] separately for both modalities.
More specifically, during training, the learnable prompt vectors are prepended into the input sequence of the first transformer layer of CLIP’s ViT backbone [22] while keep-ing the rest frozen. As such, we inject model generalization into the learned sketch and photo distributions. Next, we devise a training paradigm to adapt the learned encoders for object detection, such that the region embeddings of detected boxes are aligned with the sketch and photo em-beddings from SBIR. This allows our object detector to train without requiring additional training photos (bounding boxes and class labels) from auxiliary datasets.
To make our sketch-based detector more interesting (general-purpose [13, 57]), we further dictate it also works in a zero-shot manner. For that, following [10], we ex-tend object detection from a pre-defined fixed-set setup to an open-vocab setup. Specifically, we replace the clas-sification heads in object detectors with prototype learn-ing [49], where the encoded query sketch features act as the support set (or prototypes). Next, the model is trained under the weakly supervised object detection (WSOD) set-ting [10, 78], using a multi-category cross-entropy loss over the prototypes of all possible categories or instances. How-ever, while SBIR is trained using object-level (single object) sketch/photo pairs, object detection works on image-level (multiple categories). Hence, to train object detectors using
SBIR, we also need to bridge the gap between object and image-level features. Towards this, we use a data augmenta-tion trick that is embarrassingly simple yet highly effective for robustness towards corruption and generalisation to out-of-vocab [101, 102] – we randomly select n = {1, . . . , 7} photos from SBIR datasets [30, 75] and arbitrarily tile them on a blank canvas (similar to CutMix [101]).
In summary, our contributions are (i) for the first time cultivating the expressiveness of human sketches for object detection, (ii) sketch-based object detector that detects what you intend to express in your sketch, (iii) an object detec-tor that is both instance-aware and part-aware, in addition to performing conventional category-level detection. (iv) a novel prompt learning setup to marry CLIP and SBIR to build the sketch-aware detector that works without needing bounding box annotations (as supervised [67]), class labels (as weakly supervised [10]), and in a zero-shot manner. (v) results outperform both supervised (SOD) and weakly su-pervised object detectors (WSOD) on zero-shot setup. 2.