Abstract
Online class-incremental continual learning is a specific task of continual learning. It aims to continuously learn new classes from data stream and the samples of data stream are seen only once, which suffers from the catastrophic for-getting issue, i.e., forgetting historical knowledge of old classes. Existing replay-based methods effectively allevi-ate this issue by saving and replaying part of old data in a proxy-based or contrastive-based replay manner. Although these two replay manners are effective, the former would incline to new classes due to class imbalance issues, and the latter is unstable and hard to converge because of the limited number of samples.
In this paper, we conduct a comprehensive analysis of these two replay manners and find that they can be complementary. Inspired by this find-ing, we propose a novel replay-based method called proxy-based contrastive replay (PCR). The key operation is to re-place the contrastive samples of anchors with correspond-ing proxies in the contrastive-based way. It alleviates the phenomenon of catastrophic forgetting by effectively ad-dressing the imbalance issue, as well as keeps a faster con-vergence of the model. We conduct extensive experiments on three real-world benchmark datasets, and empirical re-sults consistently demonstrate the superiority of PCR over various state-of-the-art methods 1. 1.

Introduction
Online class-incremental continual learning (online
CICL) is a special scenario of continual learning [12]. Its goal is to learn a deep model that can achieve knowledge accumulation of new classes and not forget information learned from old classes. In the meantime, the samples of a continuously non-stationary data stream are accessed only once during the learning process. At present, catastrophic forgetting (CF) is the main problem of online CICL. It is as-*Corresponding author 1https://github.com/FelixHuiweiLin/PCR
Figure 1. Illustration of our work. (a) The example of proxy-based replay manner. For each anchor sample, it calculates similarities of all anchor-to-proxy pairs. (b) The example of contrastive-based replay manner. For each anchor sample, it calculates similarities of all anchor-to-sample pairs in the same batch. (c) The example of our method. It calculates similarities of anchor-to-proxy pairs, which is similar to the proxy-based method. However, the anchor-to-proxy pairs are selected by the anchor-to-sample pairs in the same batch, which performs in the contrastive-based manner. sociated with the phenomenon that the model has a signif-icant performance drop for old classes when learning new classes. The main reason is historical knowledge of old data would be overwritten by novel information of new data.
Among all types of methods proposed in continual learn-ing, the replay-based methods have shown superior perfor-In this family of methods, mance for online CICL [25]. part of previous samples are saved in an episodic memory buffer and then used to learn together with current samples.
In general, there are two ways to replay. The first is the proxy-based replay manner, which is to replay by using the proxy-based loss and softmax classifier. As shown in Fig-ure 1(a), it calculates similarities between each anchor with
all proxies belonging to C classes. A proxy can be regarded as the representative of a sub-dataset [38], and the anchor is one of the samples in the training batch. The second is the contrastive-based replay manner that replays by using the contrastive-based loss and nearest class mean (NCM) clas-sifier [27]. Shown as Figure 1(b), it computes similarities between each anchor with all N samples in the same train-ing batch. Although these two manners are effective, they have their corresponding limitations. The former is sub-jected to the “bias” issue caused by class imbalance, tending to classify most samples of old classes into new categories.
The latter is unstable and hard to converge in the training process due to the small number of samples.
In this work, we comprehensively analyze their charac-teristics and find that the coupling of them can achieve com-plementary advantages. On the one hand, the proxy-based manner enables fast and reliable convergence with the help of proxies. On the other hand, although the contrastive-based manner is not very robust, it has advantages in the se-lection of anchor-to-sample pairs. Only the classes associ-ated with samples in anchor-to-sample pairs can be selected to learn. Previous studies [1, 6] have proved that suitably selecting of anchor-to-proxy pairs is effective to address the
“bias” issue. Therefore, it is necessary to develop a cou-pling manner to jointly keep these advantages at the same time. In other words, it not only takes proxies to improve the robustness of the model as proxy-based manner, but also overcomes the “bias” problem by selecting anchor-to-proxy pairs as the pairs selection of contrastive-based manner.
With these inspirations, we propose a novel replay-based method called proxy-based contrastive replay (PCR) to al-leviate the phenomenon of CF for online CICL. The core motivation is the coupling of proxy-based and contrastive-based loss, and the key operation is to replace anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss. As shown in Figure 1(c), our method calcu-lates similarities between each anchor and other proxies, which is similar to the proxy-based loss. However, it does not straightly make full use of proxies from all classes. It only takes the proxies whose associated classes of sam-ples appear in the same batch, which is analogous to the contrastive-based loss. For one thing, it keeps fast conver-gence and stable performance with the help of proxies. For another thing, it addresses the “bias” issue by only choosing part of anchor-to-proxy pairs to calculate categorical proba-bility. And the selected anchor-to-proxy pairs are generally better than the ones selected by existing solutions [1, 6].
Our main contributions can be summarized as follows: 1) We theoretically analyze the characteristics of proxy-based and contrastive-based replay manner, discover-ing the coupling manner of them is beneficial. To the best of our knowledge, this work is the first one to com-bine these two manners for the online CICL problem. 2) We develop a novel online CICL framework called
PCR to mitigate the forgetting problem. By replac-ing the samples for anchor with proxies in contrastive-based loss, we achieve the complementary advantages of two existing approaches. 3) We conduct extensive experiments on three real-world datasets, and the empirical results consistently demon-strate the superiority of our PCR over various state-of-the-art methods. We also investigate and analyze the benefits of each component by ablation studies. 2.