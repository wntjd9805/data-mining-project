Abstract
Diffusion models arise as a powerful generative tool re-cently. Despite the great progress, existing diffusion models mainly focus on uni-modal control, i.e., the diffusion process is driven by only one modality of condition. To further un-leash the users’ creativity, it is desirable for the model to be controllable by multiple modalities simultaneously, e.g. gen-erating and editing faces by describing the age (text-driven) while drawing the face shape (mask-driven).
In this work, we present Collaborative Diffusion, where pre-trained uni-modal diffusion models collaborate to achieve multi-modal face generation and editing without re-training. Our key insight is that diffusion models driven by different modalities are inherently complementary regard-ing the latent denoising steps, where bilateral connections can be established upon. Specifically, we propose dynamic diffuser, a meta-network that adaptively hallucinates multi-modal denoising steps by predicting the spatial-temporal (cid:66)
Corresponding author.
Project page: https://ziqihuangg.github.io/projects/collaborative-diffusion.html
Code: https://github.com/ziqihuangg/Collaborative-Diffusion influence functions for each pre-trained uni-modal model.
Collaborative Diffusion not only collaborates generation capabilities from uni-modal diffusion models, but also inte-grates multiple uni-modal manipulations to perform multi-modal editing. Extensive qualitative and quantitative experi-ments demonstrate the superiority of our framework in both image quality and condition consistency. 1.

Introduction
Recent years have witnessed substantial progress in image synthesis and editing with the surge of diffusion models [9, 20, 56, 58]. In addition to the remarkable synthesis quality, one appealing property of diffusion models is the flexibility of conditioning on various modalities, such as texts [3, 16, 29, 30, 39, 48, 51], segmentation masks [48, 65, 66], and sketches [7, 65]. However, existing explorations are largely confined to the use of a single modality at a time. The exploitation of multiple conditions remains under-explored.
As a generative tool, its controllability is still limited.
To unleash users’ creativity, it is desirable that the model
is simultaneously controllable by multiple modalities. While it is trivial to extend the current supervised framework with multiple modalities, training a large-scale model from scratch is computationally expensive, especially when exten-sive hyper-parameter tuning and delicate architecture designs are needed. More importantly, each trained model could only accept a fixed combination of modalities, and hence re-training is necessary when a subset of modalities are ab-sent, or when additional modalities become available. The above demonstrates the necessity of a unified framework that effectively exploits pre-trained models and integrate them for multi-modal synthesis and editing.
In this paper, we propose Collaborative Diffusion, a framework that synergizes pre-trained uni-modal diffusion models for multi-modal face generation and editing without the need of re-training. Motivated by the fact that different modalities are complementary to each other (e.g., text for age and mask for hair shape), we explore the possibility of establishing lateral connections between models driven by different modalities. We propose dynamic diffuser to adaptively predict the spatial-temporal influence function for each pre-trained model. The dynamic diffuser dynamically determines spatial-varying and temporal-varying influences of each model, suppressing contributions from irrelevant modalities while enhancing contributions from admissible modalities. In addition to multi-modal synthesis, the sim-plicity and flexibility of our framework enable extension to multi-modal face editing with minimal modifications. In particular, the dynamic diffuser is first trained for collabora-tive synthesis. It is then fixed and combined with existing face editing approaches [29, 50] for multi-modal editing. It is worth-mentioning that users can select the best editing approaches based on their needs without the need of altering the dynamic diffusers.
We demonstrate both qualitatively and quantitatively that our method achieves superior image quality and condition consistency in both synthesis and editing tasks. Our contri-butions can be summarized as follows:
• We introduce Collaborative Diffusion, which exploits pre-trained uni-modal diffusion models for multi-modal controls without re-training. Our approach is the first attempt towards flexible integration of uni-modal diffu-sion models into a single collaborative framework.
• Tailored for the iterative property of diffusion models, we propose dynamic diffuser, which predicts the spatial-varying and temporal-varying influence functions to selectively enhance or suppress the contributions of the given modalities at each iterative step.
• We demonstrate the flexibility of our framework by ex-tending it to face editing driven by multiple modalities.
Both quantitative and qualitative results demonstrate the superiority of Collaborative Diffusion in multi-modal face generation and editing. 2.