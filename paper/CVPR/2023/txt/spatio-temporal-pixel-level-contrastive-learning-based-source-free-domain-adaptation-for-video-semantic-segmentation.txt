Abstract
Unsupervised Domain Adaptation (UDA) of semantic segmentation transfers labeled source knowledge to an un-labeled target domain by relying on accessing both the source and target data. However, the access to source data is often restricted or infeasible in real-world scenar-ios. Under the source data restrictive circumstances, UDA is less practical. To address this, recent works have ex-plored solutions under the Source-Free Domain Adaptation (SFDA) setup, which aims to adapt a source-trained model to the target domain without accessing source data. Still, existing SFDA approaches use only image-level informa-tion for adaptation, making them sub-optimal in video ap-plications. This paper studies SFDA for Video Semantic
Segmentation (VSS), where temporal information is lever-aged to address video adaptation. Speciﬁcally, we pro-pose Spatio-Temporal Pixel-Level (STPL) contrastive learn-ing, a novel method that takes full advantage of spatio-temporal information to tackle the absence of source data better. STPL explicitly learns semantic correlations among pixels in the spatio-temporal space, providing strong self-supervision for adaptation to the unlabeled target domain.
Extensive experiments show that STPL achieves state-of-the-art performance on VSS benchmarks compared to cur-rent UDA and SFDA approaches. Code is available at: https://github.com/shaoyuanlo/STPL 1.

Introduction
The availability of large amounts of labeled data has made it possible for various deep networks to achieve re-markable performance on Image Semantic Segmentation (ISS) [2, 4, 30]. However, these deep networks often general-ize poorly on target data from a new unlabeled domain that is visually distinct from the source training data. Unsupervised
Domain Adaptation (UDA) attempts to mitigate this domain shift problem by using both the labeled source data and un-*This work was mostly done when S.-Y. Lo was an intern at Amazon.
Figure 1. Comparison of VSS accuracy. Video-based UDA meth-ods [12, 38, 49] outperform image-based UDA methods [33, 51], showing the importance of video-based strategies for the VSS task.
Image-based SFDA methods [16, 39] perform lower than the UDA methods, which shows the difﬁculty of the more restricted SFDA setting. The proposed STPL, even with SFDA, achieves the best accuracy and locates at the top-right corner of the chart (i.e., more restriction, but higher accuracy). labeled target data to train a model transferring the source knowledge to the target domain [11, 12, 31, 32, 38, 41]. UDA is effective but relies on the assumption that both source and target data are available during adaptation. In real-world scenarios, the access to source data is often restricted (e.g., data privacy, commercial proprietary) or infeasible (e.g., data transmission efﬁciency, portability). Hence, under these source data restrictive circumstances, UDA approaches are less practical.
To deal with these issues, the Source-Free Domain Adap-tation (SFDA) setup, also referred to as Unsupervised Model
Adaptation (UMA), has been recently introduced in the lit-erature [6, 26, 27, 52]. SFDA aims to use a source-trained
model (i.e., a model trained on labeled source data) and adapt it to an unlabeled target domain without requiring access to the source data. More precisely, under the SFDA formula-tion, given a source-trained model and an unlabeled target dataset, the goal is to transfer the learned source knowledge to the target domain. In addition to alleviating data privacy or proprietary concerns, SFDA makes data transmission much more efﬁcient. For example, a source-trained model (∼ 0.1
- 1.0 GB) is usually much smaller than a source dataset (∼ 10 - 100 GB). If one is adapting a model from a large-scale cloud center to a new edge device that has data with different domains, the source-trained model is far more portable and transmission-efﬁcient than the source dataset.
Under SFDA, label supervision is not available. Most
SFDA studies adopt pseudo-supervision or self-supervision techniques to adapt the source-trained model to the target domain [16, 39]. However, they consider only image-level information for model adaptation. In many real-world seman-tic segmentation applications (autonomous driving, safety surveillance, etc.), we have to deal with temporal data such as streams of images or videos. Supervised approaches that use temporal information have been successful for Video Seman-tic Segmentation (VSS), which predicts pixel-level semantics for each video frame [19, 22, 28, 46]. Recently, video-based
UDA strategies have also been developed and yielded better performance than image-based UDA on VSS [12, 38, 49].
This motivates us to propose a novel SFDA method for
VSS, leveraging temporal information to tackle the absence of source data better.
In particular, we ﬁnd that current image-based SFDA approaches suffer from sub-optimal per-formance when applied to VSS (see Figure 1). To the best of our knowledge, this is the ﬁrst work to explore video-based
SFDA solutions.
In this paper, we propose a novel spatio-temporal SFDA method namely Spatio-Temporal Pixel-Level (STPL) Con-trastive Learning (CL), which takes full advantage of both spatial and temporal information for adapting VSS mod-els. STPL consists of two main stages. (1) Spatio-temporal feature extraction: First, given a target video sequence in-put, STPL fuses the RGB and optical ﬂow modalities to extract spatio-temporal features from the video. Meanwhile, it performs cross-frame augmentation via randomized spatial transformations to generate an augmented video sequence, then extracts augmented spatio-temporal features. (2) Pixel-level contrastive learning: Next, STPL optimizes a pixel-level contrastive loss between the original and augmented spatio-temporal feature representations. This objective en-forces representations to be compact for same-class pixels across both the spatial and temporal dimensions.
With these designs, STPL explicitly learns semantic corre-lations among pixels in the spatio-temporal space, providing strong self-supervision for adaptation to an unlabeled tar-get domain. Furthermore, we demonstrate that STPL is a non-trivial uniﬁed spatio-temporal framework. Speciﬁcally,
Spatial-only CL and Temporal-only CL are special cases of
STPL, and STPL is better than a na¨ıve combination of them.
Extensive experiments demonstrate the superiority of STPL over various baselines, including the image-based SFDA as well as image- and video-based UDA approaches that rely on source data (see Figure 1). The key contributions of this work are summarized as follows:
• We propose a novel SFDA method for VSS. To the best of our knowledge, this is the ﬁrst work to explore video-based
SFDA solutions.
• We propose a novel CL method, namely STPL, which explicitly learns semantic correlations among pixels in the spatio-temporal space, providing strong self-supervision for adaptation to an unlabeled target domain.
• We conduct extensive experiments and show that STPL provides a better solution compared to the existing image-based SFDA methods as well as image- and video-based
UDA methods for the given problem formulation. 2.