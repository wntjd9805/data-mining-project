Abstract 3D visual language reasoning plays an important role in effective human-computer interaction. The current ap-proaches for 3D visual reasoning are task-speciﬁc, and lack pre-training methods to learn generic representations that can transfer across various tasks. Despite the encourag-ing progress in vision-language pre-training for image-text data, 3D-language pre-training is still an open issue due to limited 3D-language paired data, highly sparse and ir-regular structure of point clouds and ambiguities in spa-tial relations of 3D objects with viewpoint changes. In this paper, we present a generic 3D-language pre-training ap-proach, that tackles multiple facets of 3D-language rea-soning by learning universal representations. Our learn-ing objective constitutes two main parts. 1) Context aware spatial-semantic alignment to establish ﬁne-grained corre-spondence between point clouds and texts. It reduces rela-tional ambiguities by aligning 3D spatial relationships with textual semantic context. 2) Mutual 3D-Language Masked modeling to enable cross-modality information exchange.
Instead of reconstructing sparse 3D points for which lan-guage can hardly provide cues, we propose masked pro-posal reasoning to learn semantic class and mask-invariant
Corresponding Author: Yinjie Lei (yinjie@scu.edu.cn) representations. Our proposed 3D-language pre-training method achieves promising results once adapted to vari-ous downstream tasks, including 3D visual grounding, 3D dense captioning and 3D question answering. Our codes are available at https://github.com/leolyj/3D-VLP 1.

Introduction 3D Vision and Language (3D V+L) reasoning aims to jointly understand 3D point clouds and their textual descrip-tions.
It lies at the intersection of 3D visual understand-ing and natural language processing, and plays an impor-tant role in applications e.g., Metaverse, AR/VR and au-tonomous robots. 3D V+L reasoning has recently gained signiﬁcant research interest, with multiple works tackling 3D visual grounding [1, 8, 33, 59], 3D dense captioning
[11, 23, 58] and 3D question answering [3, 51, 54].
Despite promising progress made towards solving 3D vi-sual reasoning tasks, the existing approaches are highly spe-cialized and task speciﬁc. This is in contrast to multi-modal reasoning from RGB images, where the dominant approach is to pre-train a generic model on large scale image-text paired data, and then adapt this model for multiple down-stream tasks. The pre-training step enables learning highly transferable and generic cross-modality representations via
techniques such as image-text feature alignment [21,41] and masked signal reconstruction [10,28]. For RGB images, the transfer learning from pre-trained Vision-Language models achieves impressive results on numerous downstream tasks (e.g., image-text retrieval [28, 49], visual question answer-ing [46,47] and image captioning [17,48]). However, due to unique challenges posed by irregular and unstructured point cloud data, 3D-language pre-training to learn a uniﬁed rep-resentation space, that can be transferred across tasks, has not yet been investigated in the existing literature.
As point clouds have different characteristics from 2D images, 3D-Language Pre-training (3D-LP) poses multiple unique challenges: 1) Available 3D-language samples are limited. Compared to image-text samples that can be web crawled, the existing pairwise point cloud and language samples are much scarce. 2) Point clouds are naturally unstructured. Unlike 2D images having pixels densely ar-ranged in regular grids, point clouds are highly sparse and irregularly distributed. 3) The spatial relations between 3D objects are complex, as they are not restricted to a 2D plane, and introduce ambiguities with viewpoint changes.
In this paper, we propose a 3D-language pre-training approach that aims to establish ﬁne-grained interactions between point clouds and their textual descriptions, thus learning universal multi-modal features for various 3D V+L tasks, as illustrated in Fig 1. First, to bridge the distribution discrepancy between 3D geometric features and their text semantics, we propose a Context aware Spatial-semantic
Alignment (CSA) strategy (Sec. 3.2). Different from the global contrastive learning in image-text, we align point cloud and language features from semantic and contextual perspectives separately, so that the spatial context between 3D objects and the semantic context in the language are simultaneously considered to overcome relational ambigu-ity. We further introduce Mutual 3D-Language Masked modeling (M3LM) (Sec. 3.3) that reconstructs the masked parts and enable meaningful cross-modal information ex-change to enhance the feature of both modality. Due to the irregular structure and variable (unﬁxed) number of 3D points, existing masking methods that reconstruct raw in-put signal are not suitable to learn effective representation for point clouds. We propose to reconstruct the semantic class and high-level features of masked 3D objects by tak-ing complementary information from language, which gives the model more meaningful objective than merely recon-structing the xyz of points.
In our approach, we predict the semantic class of masked 3D objects and reconstruct momentum-distilled encoded features for the unmasked in-put. We jointly train the 3D-language model with our proposed multi-task learning objectives to learn and se-mantically align multi-modal features that generalize well across tasks. Through experiments on various downstream 3D V+L tasks, we demonstrate the versatility of our pro-posed 3D-language pre-training for three different tasks on
ScanRefer [8], Scan2Cap [11] and ScanQA [3] benchmark datasets. Our main contributions are:
• We propose a pre-training method to learn transfer-able 3D-language representations to solve 3D visual grounding, 3D dense captioning and 3D question an-swering from a uniﬁed perspective.
• In order to jointly train point cloud and language encoders, we propose context-aware 3D-language alignment and mutual masked modeling strategies, which ensure that the learned multi-modal features are semantically-aligned and complement each other.
• We consistently surpass existing task-speciﬁc meth-ods on ScanRefer [8] (+2.1 Acc@0.5), Scan2Cap [11] (+5.5 CIDEr@0.5) and ScanQA [3] (+1.3 EM@1) benchmark datasets, achieving new state-of-the-arts. 2.