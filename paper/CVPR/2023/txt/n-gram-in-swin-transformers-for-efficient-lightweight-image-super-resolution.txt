Abstract
While some studies have proven that Swin Transformer (Swin) with window self-attention (WSA) is suitable for sin-gle image super-resolution (SR), the plain WSA ignores the broad regions when reconstructing high-resolution im-In addition, many ages due to a limited receptive field. deep learning SR methods suffer from intensive computa-tions. To address these problems, we introduce the N-Gram context to the low-level vision with Transformers for the first time. We define N-Gram as neighboring local win-dows in Swin, which differs from text analysis that views
N-Gram as consecutive characters or words. N-Grams in-teract with each other by sliding-WSA, expanding the re-gions seen to restore degraded pixels. Using the N-Gram context, we propose NGswin, an efficient SR network with
SCDP bottleneck taking multi-scale outputs of the hierar-chical encoder. Experimental results show that NGswin achieves competitive performance while maintaining an efficient structure when compared with previous leading methods. Moreover, we also improve other Swin-based SR methods with the N-Gram context, thereby building an en-hanced model: SwinIR-NG. Our improved SwinIR-NG out-performs the current best lightweight SR approaches and establishes state-of-the-art results. Codes are available at https://github.com/rami0205/NGramSwin. 1.

Introduction
The goal of single image super-resolution (SR) is to re-construct high-resolution (HR) images from low-resolution (LR) images. Many deep learning-based methods have worked in this field.
In particular, several image restora-tion studies [16, 27, 55, 61, 63, 66] have adapted the win-dow self-attention (WSA) proposed by Swin Transformer (Swin) [32] as it integrates long-range dependency of Vi-sion Transformer [14] and locality of conventional convolu-tion. However, two critical problems remain in these works.
First, the receptive field of the plain WSA is limited within a small local window [52,56,58]. It prevents the models from utilizing the texture or pattern of neighbor windows to re-*Corresponding author.
Figure 1. Two tracks of this paper using the N-Gram context. (Left) NGswin outperforms previous leading SR methods with an efficient structure. (Right) Our proposed N-Gram context im-proves different Swin Transformer-based SR models. cover degraded pixels, producing the distorted images. Sec-ond, recent state-of-the-art SR [9,27,61,66] and lightweight
SR [6, 15, 35, 63] networks require intensive computations.
Reducing operations is essential for real-world applications if the parameters are kept around a certain level (e.g., 1M, 4MB sizes), because the primary consumption of semicon-ductor energy (concerning time) for neural networks is re-lated to Mult-Adds operations [17, 47].
To overcome these problems, we define the N-Gram con-text as the interaction of neighbor local windows. Neighbor uni-Gram embeddings interact with each other by sliding-WSA to produce the N-Gram context features before win-dow partitioning. The uni-Gram embeddings result from a channel-reducing group convolution [10] to decrease the complexity of N-Gram interaction (see Fig. 3c). Our N-Gram context efficiently expands the receptive field of WSA for recovery tasks. This work introduces N-Gram to low-level vision with Transformers for the first time, inspired by the following facts: N-Gram language models treat the extended context beyond each separate word to understand text statistically [8]. Since images have heavy spatial redun-dancy, some degraded pixels can be recovered from contex-tual information of neighbor pixels [18].
As shown in Fig. 1, our work progresses in two tracks.
Mainly, to solve the problem of the intensive opera-tions in SR, we propose an efficient N-Gram Swin Trans-former (NGswin). As illustrated in Fig. 3a, NGswin con-sists of five components: a shallow module, three hier-archical encoder stages (with patch-merging) that contain
NSTBs (N-Gram Swin Transformer Blocks), SCDP Bot-tleneck (pixel-Shuffle, Concatenation, Depth-wise convo-lution, Point-wise projection), a small decoder stage with
NSTBs, and a reconstruction module. NSTBs employ our
N-Gram context and the scaled-cosine attention proposed by Swin V2 [31]. SCDP bottleneck, which takes multi-scale outputs of the encoder, is a variant of bottleneck from U-Net [46]. Experimental results demonstrate that the components above contribute to the efficient and com-petitive performance of NGswin. Secondly, focusing on improved performances, we apply the N-Gram context to other Swin-based SR models, such as SwinIR-light [27] and
HNCT [16]. Notably, SwinIR-NG (improved SwinIR-light with N-Gram) establishes state-of-the-art lightweight SR.
The main contributions of this paper are summarized as: (1) We introduce the N-Gram context to the low-level vi-sion with Transformer for the first time. It enables the
SR networks to expand the receptive field to recover each degraded pixel by sliding-WSA. For efficient cal-culation of N-Gram WSA, we produce uni-Gram em-beddings by a channel-reducing group convolution. (2) We propose an efficient SR network, NGswin. It ex-ploits the hierarchical encoder (with patch-merging), an asymmetrically small decoder, and SCDP bottle-neck. These elements are critical for competitive per-formance in the efficient SR on ×2, ×3, and ×4 tasks. (3) The N-Gram context improves other Swin Trans-former methods. The improved SwinIR-NG achieves state-of-the-art results on lightweight SR. 2.