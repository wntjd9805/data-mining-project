Abstract
This paper studies the potential of distilling knowledge from pre-trained models, especially Masked Autoencoders.
Our approach is simple: in addition to optimizing the pixel reconstruction loss on masked inputs, we minimize the dis-tance between the intermediate feature map of the teacher model and that of the student model. This design leads to a computationally efficient knowledge distillation framework, given 1) only a small visible subset of patches is used, and 2) the (cumbersome) teacher model only needs to be par-tially executed, i.e., forward propagate inputs through the first few layers, for obtaining intermediate feature maps.
Compared to directly distilling fine-tuned models, distill-ing pre-trained models substantially improves downstream performance. For example, by distilling the knowledge from an MAE pre-trained ViT-L into a ViT-B, our method achieves 84.0% ImageNet top-1 accuracy, outperforming the baseline of directly distilling a fine-tuned ViT-L by 1.2%.
More intriguingly, our method can robustly distill knowl-edge from teacher models even with extremely high mask-ing ratios: e.g., with 95% masking ratio where merely TEN patches are visible during distillation, our ViT-B competi-tively attains a top-1 ImageNet accuracy of 83.6%; surpris-ingly, it can still secure 82.4% top-1 ImageNet accuracy by aggressively training with just FOUR visible patches (98% masking ratio). The code and models are publicly available at https://github.com/UCSC-VLAA/DMAE. 1.

Introduction
Following the success in the natural language processing
[10, 27], the Transformer architecture is showing tremen-dous potentials in computer vision [2, 11, 25, 26, 33], es-pecially when they are pre-trained with a huge amount of unlabelled data [3] with self-supervised learning techniques
[1, 14]. Masked image modeling, which trains models to predict the masked signals (either as raw pixels or as se-mantic tokens) of the input image, stands as one of the most powerful ways for feature pre-training. With the most re-cent representative work in this direction, masked autoen-coder (MAE) [13], we are now able to efficiently and effec-tively pre-train high-capacity Vision Transformers (ViTs) with strong feature representations, leading to state-of-the-art solutions for a wide range of downstream visual tasks.
In this paper, we are interested in applying knowledge distillation [17], which is one of the most popular model compression techniques, to transfer the knowledge from
In these strong but cumbersome ViTs into smaller ones. contrast to prior knowledge distillation works [17, 21, 34], the teacher considered here is a pre-trained model whose predictions do not necessarily reveal the fine-grained re-lationship between categories; therefore, typical solutions like aligning the soft/hard logits between the teacher model and the student model may no longer remain effective.
Moreover, after distilling the pre-trained teacher model, these student models need an extra round of fine-tuning to adapt to downstream tasks. These factors turn distilling pre-trained models seemingly a less favorable design choice in terms of both performance and computational cost.
Nonetheless, surprisingly, we find by building upon
MAE, the whole distillation framework can efficiently yield high-performance student models. There are two key de-signs. Firstly, we follow MAE to let the encoder exclusively operate on a small visible subset of patches and to employ a lightweight decoder for pixel reconstruction. Whereas rather than using the “luxury” setups in MAE, we show ag-gressively simplifying pre-training from 1600 epochs to 100 epochs and pushing masking ratio from 75% to 95% suffice to distill strong student models. Secondly, instead of align-ing logits, we alternatively seek to match the intermediate feature representation; this enables the cumbersome teacher model to only forward propagate inputs through the first few layers, therefore, reducing computations. We note applying
L1 norm for distance measure is essential recipe for ensur-ing a successful intermediate feature alignment.
We name this distilling MAE framework as DMAE.
Compared to the traditional knowledge distillation frame-work where the teacher is a fine-tuned model, DMAE is more efficient and can train much stronger student models
Figure 1. Illustration of the distillation process in DMAE. There are two key designs. Firstly, following MAE, we hereby only take visible patches as inputs and aims to reconstruct the masked ones. Secondly, knowledge distillation is achieved by aligning the intermediate features between the teacher model and the student model. Note the gray blocks denote the dropped high-level layers of the teacher model during distillation. at different capacities. For example, by setting ViT-B as the student model, while the baseline of distilling a fine-tuned
ViT-L achieves 82.8% top-1 ImageNet accuracy, DMAE substantially boosts the performance to 84.0% (+1.2%) top-1 ImageNet accuracy, at a even lower training cost (i.e., 195
GPU hours vs. 208 GPU hours, see Table 9). More intrigu-ingly, we found that DMAE allows for robust training with extremely highly masked images—even with TEN visible patches (i.e., 95% masking ratio), ViT-B can competitively attain a top-1 ImageNet accuracy of 83.6%; this masking ra-tio can further be aggressively pushed to 98% (FOUR vis-ible patches) where DMAE still help ViT-B secure 82.4% top-1 ImageNet accuracy. We hope this work can benefit future research on efficiently unleashing the power of pre-train models. 2.