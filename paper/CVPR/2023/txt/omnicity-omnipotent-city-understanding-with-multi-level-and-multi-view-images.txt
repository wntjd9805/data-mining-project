Abstract
This paper presents OmniCity, a new dataset for omnipo-tent city understanding from multi-level and multi-view im-ages. More precisely, OmniCity contains multi-view satel-lite images as well as street-level panorama and mono-view images, constituting over 100K pixel-wise annotated images that are well-aligned and collected from 25K geo-locations in New York City. To alleviate the substantial
*Corresponding authors. pixel-wise annotation efforts, we propose an efficient street-view image annotation pipeline that leverages the exist-ing label maps of satellite view and the transformation re-lations between different views (satellite, panorama, and mono-view). With the new OmniCity dataset, we pro-vide benchmarks for a variety of tasks including build-ing footprint extraction, height estimation, and building plane/instance/fine-grained segmentation. Compared with existing multi-level and multi-view benchmarks, OmniCity contains a larger number of images with richer annota-tion types and more views, provides more benchmark results
of state-of-the-art models, and introduces a new task for fine-grained building instance segmentation on street-level panorama images. Moreover, OmniCity provides new prob-lem settings for existing tasks, such as cross-view image matching, synthesis, segmentation, detection, etc., and fa-cilitates the developing of new methods for large-scale city understanding, reconstruction, and simulation. The Om-niCity dataset as well as the benchmarks will be released at https://city-super.github.io/omnicity/. 1.

Introduction
Owning over a half global population and contribut-ing the most economic growth, the city areas have been recorded and characterized by various data sources includ-ing satellite and aerial imagery, street-level imagery, LiDAR data, public maps, crowd-sourced data, etc. A great num-ber of benchmarks have been proposed towards facilitating different vision tasks in city scene, of which the street-level imagery has been broadly used in multiple driving-related benchmarks [4, 10, 14, 18, 26, 31, 43]. The rich visual in-formation of street-level imagery also enables complicated visual recognition tasks on specific categories, such as per-son detection [3], vehicle tracking and re-identification [32], and fine-grained land use classification [49].
Nevertheless, constructing pixel-wise annotations for street-level imagery requires substantial human efforts, re-sulting in the small image quantity and limited view types of existing datasets, especially for the street-view panorama datasets with only hundreds of annotated images [36, 41, 42]. Regarding the annotation categories and levels, exist-ing datasets mostly provide instance-level annotations for dynamic object categories in driving scenes. As a vital component for city understanding, the static objects such as buildings and roads take up a larger proportion of cities and remain a high consistency across the satellite and ground-level images. However, existing street-level datasets ei-ther provide pixel-wise building annotations without fine-grained semantic labels [4, 10, 14, 47] or provide fine-grained annotations at only bbox or image level [46, 49].
Compared with street-level images, remote sensing im-ages usually contain less visual information for conduct-ing complicated tasks such as fine-grained land use seg-mentation and building function recognition. On the other hand, unlike the sparsely-distributed street-level images, re-mote sensing images have a dense spatial distribution and a worldwide coverage, which are well aligned with the open maps and government datasets at pixel level [22]. These ex-isting maps and datasets contain a variety of satellite-level annotations for buildings (such as the footprint, land use, height, year built), roads (category and line coordinates), and other geographical objects, providing new perspectives for promoting novel city understanding datasets and tasks.
In this work, as illustrated in Figure 1, we construct an omnipotent city dataset unifying data sources from both satellite and street views, linked by geo-locations and ur-ban planning data. Unlike existing city datasets that only support a limited number of tasks, OmniCity dataset incor-porate rich geometric annotations and semantic meta data for each image, where multiple tasks can be conducted on. To leverage the existing map labels and the rich visual context from the street-level imagery, we propose an effi-cient pipeline for producing diverse street-level annotations.
Based on this annotation pipeline, we built OmniCity, a dataset that contains over 100K annotated images collected from 25K geo-locations in New York City. We provide benchmark results on OmniCity for a variety of tasks, in-cluding building footprint extraction and height estimation on satellite images, as well as fine-grained/instance/plane segmentation of buildings on street-level panorama and mono-view images. To the best of our knowledge, this is the first work that involves fine-grained building instance segmentation on street-level panorama images. We also an-alyze the potential of OmniCity for promoting new tasks and methods with multi-level imagery.
Our main contributions are summarized as follows:
• We propose a novel pipeline for efficiently pro-ducing diverse pixel-wise annotations on street-level panorama and mono-view images.
• We build the OmniCity dataset, which contains well-aligned satellite and street-level images with a larger quantity, richer annotations and more views compared with existing datasets.
• We provide a series of benchmark experimental results for multiple tasks and data sources, and analyze the limitations of the current benchmarks on OmniCity.
• We discuss the potential of OmniCity for facilitating new methods and tasks for large-scale city understand-ing, reconstruction, and simulation. 2.