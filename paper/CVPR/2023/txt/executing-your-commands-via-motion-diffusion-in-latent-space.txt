Abstract
We study a challenging task, conditional human motion generation, which produces plausible human motion se-quences according to various conditional inputs, such as action classes or textual descriptors. Since human mo-tions are highly diverse and have a property of quite dif-ferent distribution from conditional modalities, such as tex-tual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modal-ity to the human motion sequences. Besides, the raw mo-tion data from the motion capture system might be redun-dant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and condi-tional modalities would need a heavy computational over-head and might result in artifacts introduced by the cap-tured noises. To learn a better representation of the var-ious human motion sequences, we ﬁrst design a powerful
Variational AutoEncoder (VAE) and arrive at a representa-tive and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to es-tablish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves signiﬁcant improvements over the state-of-the-art methods among extensive human motion genera-tion tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences. 1.

Introduction
Human motion synthesis has recently rapidly developed in a multi-modal generative fashion. Various condition in-*These authors contributed equally to this work.
†Corresponding author.
Figure 1. Our Motion Latent-based Diffusion (MLD) model can achieve high-quality and diverse motion generation given a text prompt. The darker colors indicate the later in time, and the col-ored words refer to the motions with same colored trajectory. puts, such as music [34, 33, 32], control signals [45, 66, 65], action categories [46, 19], and natural language descrip-tions [16, 47, 69, 18, 2, 28], provide a more convenient and human-friendly way to animate virtual characters or even control humanoid robots.
It will beneﬁt numerous appli-cations in the game industry, ﬁlm production, VR/AR, and robotic assistance.
Among all conditional modalities, text-based conditional human motion synthesis has been driving and dominating research frontiers because the language descriptors provide a convenient and natural user interface for people to inter-act with computers [47, 2, 75, 69, 28]. However, since the distributions between the natural language descriptors and
motion sequences are quite different, it is not easy to learn a probabilistic mapping function from the textural descrip-tors to the motion sequences, which is also mentioned in the previous work, MotionCLIP [68]. Two typical meth-ods address this problem: 1) the cross-modal compatible latent space between motion and language [47, 2] and 2) the conditional diffusion model [75, 69, 28]. The form-ers, such as TEMOS [47], usually learn a motion Varia-tional AutoEncoder (VAE) and a text Variational Encod-ing (without decoder) and then constrain the text encoder and the motion encoder into a compatible latent space via the Kullback-Leibler (KL) divergences loss, which pushes a foundational step forward on creating human motion se-quences by natural language inputs. However, since the dis-tributions of natural languages and motion sequences are highly different, forcibly aligning these two simple gaus-sian distributions, in terms of variational text encoding and variational motion encoding, into a compatible distribution might result in misalignments and thereby reduce the gen-erative diversity inevitably. In light of the tremendous suc-cess of the diffusion-based generative models on other do-mains [53, 61, 56, 22, 79, 73], the latter category meth-ods [75, 69, 28] propose a conditional diffusion model for human motion synthesis to learn a more powerful proba-bilistic mapping from the textual descriptors to human mo-tion sequences and improve the synthesized quality and di-versity. Nevertheless, the raw motion sequences are some-what time-axis redundant, and diffusion models in raw se-quential data [54, 22, 35] usually require exhausting compu-tational overhead in both the training and inference phase, which is inefﬁcient. Besides, since the raw motion data from the motion capture system might contain noises, the powerful diffusion models might learn the clues of a prob-abilistic mapping from the conditional inputs to the noise motion sequences and produce artifacts.
To efﬁciently synthesize plausible and diverse human motion sequences according to the conditional inputs, in-spired by the success of the diffusion model on latent space in text-to-image synthesis [56], we combine the advantages of the latent space-based and the conditional diffusion-based methods and propose a motion latent-based diffusion model (MLD) for human motion generation. Speciﬁcally, we ﬁrst design a transformer-based autoencoder [46] with the UNet-like long skip connections [59] to learn a rep-resentative and low-dimensional latent distribution of hu-man motion sequences. Then, instead of using a diffusion model to establish the connections between the raw motion sequences and the conditional inputs, we propose a motion latent-based diffusion model (MLD) to learn a better prob-abilistic mapping from the conditions to the representative motion latent codes, which could not only produce the vivid motion sequences conforming to the given conditional in-puts but also substantially reduce the computational over-head in both training and inference stage. In addition, high-quality human motion sequences with well-annotated ac-tion labels or textual descriptions are expensive and limited.
In contrast, the large-scale non-annotated or weakly anno-tated motion sequences are publicly available, such as the
AMASS dataset [41]. Our proposed MLD could individu-ally train a motion latent autoencoder on these large-scale datasets, arriving at a representative and low-dimensional latent space for diverse human motion sequences. This low-dimensional latent space with higher information den-sity could accelerate the model’s convergence and signif-icantly reduce computational consumption for the down-stream conditional human motion generation tasks.
We summarize the contributions as follows: 1) we de-sign and explore a more representative motion variational autoencoder (VAE), which provides state-of-the-art motion reconstruction and diverse generation, beneﬁting the train-ing of the latent diffusion models; 2) we further demon-strate that motion generation tasks on latent spaces, such as text-to-motion and action-to-motion, are more efﬁcient than the diffusion models on raw motion sequences; 3) our pro-posed MLD achieves competitive performance on multiple tasks (unconditional motion generation, action-to-motion, and text-to-motion), and codes are available. 2.