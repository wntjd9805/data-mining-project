Abstract
Inspired by properties of semantic segmentation, in this paper we investigate how to leverage robust image segmen-tation in the context of privacy-preserving visual localiza-tion. We propose a new localization framework, SegLoc, that leverages image segmentation to create robust, com-pact, and privacy-preserving scene representations, i.e., 3D maps. We build upon the correspondence-supervised, fine-grained segmentation approach from [42], making it more robust by learning a set of cluster labels with discriminative clustering, additional consistency regularization terms and we jointly learn a global image representation along with a dense local representation. In our localization pipeline, the former will be used for retrieving the most similar im-ages, the latter to refine the retrieved poses by minimizing the label inconsistency between the 3D points of the map and their projection onto the query image. In various ex-periments, we show that our proposed representation al-lows to achieve (close-to) state-of-the-art pose estimation results while only using a compact 3D map that does not contain enough information about the original images for an attacker to reconstruct personal information. 1.

Introduction
Visual localization is the problem of estimating the pre-cise camera pose – position and orientation – from which the image was taken in a known scene. It is a core compo-nent of systems such as self-driving cars [31], autonomous robots [49], and mixed-reality applications [4, 53].
Traditionally, visual localization algorithms rely on a 3D scene representation of the target area, which can be a 3D point cloud map [29, 34, 35, 45, 46, 66, 68, 69, 73, 79], e.g. , from Structure-from-Motion (SfM), or a learned 3D repre-sentation [9,10,14,37,38,71,76]. The representation is typ-ically derived from reference images with known camera poses. Depending on the application scenario, these maps
Figure 1. The SegLoc localization pipeline: Our model jointly creates a robust global descriptor used to retrieve an initial pose (R0, T0) and dense local representations used to obtain the re-fined pose (R, T) by maximizing the label consistency between the reprojected 3D points and the query image. need to be stored in the cloud, which raises important ques-tions about memory consumption and privacy preserva-tion.
It is possible to reconstruct images from maps that contain local image features [62], amongst the most widely used for scene representation.
To tackle the above challenges that feature-based ap-proaches may face, inspired by semantic-based [48, 82] and segmentation-based [42] approaches, we propose a visual localization pipeline where robust segmentations are used as the sole cue for localization, yielding reduced storage requirements (compared to using local features) while in-creasing privacy-preservation. Our proposed localization pipeline, called SegLoc, follows standard structure based-localization pipelines [34, 66] that represent the scene via a 3D model: first, image retrieval based on a compact im-age representation is used to coarsely localize a query im-age. Given such an initial pose estimate, the camera pose is refined by aligning the query image to the 3D map. Con-trary to prior work that is based on extracting features di-rectly from images, we derive a more abstract representa-tion in the form of a robust dense segmentation based on a
set of clusters learned in a self-supervised manner. As illus-trated in Figure 1, we use this segmentation to both extract a global descriptor for image retrieval and for pose refine-ment. The pose is refined by maximizing the label consis-tency between the predictions in the query image and a set of labeled 3D points in the scene.
Such an approach has multiple advantages. First, our model is able to learn representations which are robust to seasonal or appearance changes. Similar to semantic seg-mentations, which are invariant to viewing conditions as the semantic meaning of regions do not change, our represen-tation is trained such that the same 3D point is mapped to the same label regardless of viewing conditions. Second, it results in low storage requirements, as instead of storing high-dimensional feature descriptors, for each 3D point we only keep its label. Finally, it allows privacy-preserving vi-sual localization [15,22,28,78], as it creates a non-injective mapping from multiple images showing similar objects with different appearances to similar labels. While, ensuring user privacy comes at the cost of reduced pose accuracy [19,98], our method comes close to state-of-the-art results with a better accuracy vs. memory vs. privacy trade-off.
To summarize, our first contribution is a new localiza-tion framework, called SegLoc, that extends the idea [41, 42] of learning robust fine-grained image segmentations in a self-supervised manner. To that end, we leverage dis-criminative clustering while putting more emphasis on rep-resentation learning. Furthermore, we derive a full local-ization pipeline, where our model jointly learns global im-age representation to retrieve images for pose initialization, and dense local representations for building a compact 3D map – an order of magnitude smaller compared to feature-based approaches – and to perform privacy-preserving pose refinement. As a second contribution, we draw a con-nection between segmentation-based representations and privacy-preserving localization, opening up viable alter-natives to keypoint-based methods within the accuracy-privacy-memory trade-off. We evaluate our approach in multiple indoor and outdoor environments while quantita-tively measuring privacy through detailed experiments. 2.