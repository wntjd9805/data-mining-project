Abstract 1.

Introduction
Diffusion models have emerged as the best approach for generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not bil-lions of images with a stable learning objective. However, extending these models to 3D remains difficult for two rea-sons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in mem-ory and compute complexity makes this infeasible. We ad-dress the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. We evaluate our method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. We show that our dif-fusion models are scalable, train robustly, and are compet-itive in terms of sample quality and fidelity to existing ap-proaches for 3D generative modeling.
Diffusion models have rapidly emerged as formidable generative models for images, replacing others (e.g., VAEs,
GANs) for a range of applications, including image col-orization [47], image editing [35], and image synthesis [9, 21]. These models explicitly optimize the likelihood of the training samples, can be trained on millions if not billions of images, and have been shown to capture the underlying model distribution better [9] than previous alternatives.
A natural next step is to bring diffusion models to 3D data. Compared to 2D images, 3D models facilitate di-rect manipulation of the generated content, result in perfect view consistency across different cameras, and allow object placement using direct handles. However, learning 3D dif-fusion models is hindered by the lack of a sufficient volume of 3D data for training. A further question is the choice of representation for the 3D data itself (e.g., voxels, point clouds, meshes, occupancy grids, etc.). Researchers have proposed 3D-aware diffusion models for point clouds [33],
⋆ Indicates equal contribution.
† part of this work was done during an internship at MetaAI.
volumetric shape data using wavelet features [22] and novel view synthesis [61]. They have also proposed to distill a pretrained 2D diffusion model to generate neural radiance fields of 3D objects [30, 45]. However, a diffusion-based 3D generator model trained using only 2D image for super-vision is not available yet.
In this paper, we contribute HOLODIFFUSION, the first unconditional 3D diffusion model that can be trained with only real posed 2D images. By posed, we mean different views of the same object with known cameras, for example, obtained by means of structure from motion [49].
We make two main technical contributions: (i) We pro-pose a new 3D model that uses a hybrid explicit-implicit feature grid. The grid can be rendered to produce images from any desired viewpoint and, since the features are de-fined in 3D space, the rendered images are consistent across different viewpoints. Compared to utilizing an explicit den-sity grid, the feature representation allows for a lower res-olution grid. The latter leads to an easier estimation of the probability density due to a smaller number of variables.
Furthermore, the resolution of the grid can be decoupled from the resolution of the rendered images. (ii) We design a new diffusion method that can learn a distribution over such 3D feature grids while only using 2D images for su-pervision. Specifically, we first generate intermediate 3D-aware features conditioned only on the input posed images.
Then, following the standard diffusion model learning, we add noise to this intermediate representation and train a de-noising 3D UNet to remove the noise. We apply the denois-ing loss as photometric error between the rendered images and the Ground-Truth training images. The key advantage of this approach is that it enables training of the 3D diffu-sion model from 2D images, which are abundant, sidestep-ping the difficult problem of procuring a huge dataset of 3D models for training.
We train and evaluate our method on the Co3Dv2 [46] dataset where HOLODIFFUSION outperforms existing alter-natives both qualitatively and quantitatively. 2.