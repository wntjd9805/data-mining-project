Abstract https://chenyanglei.github.io/deflicker.
Many videos contain flickering artifacts; common causes of flicker include video processing algorithms, video gener-ation algorithms, and capturing videos under specific situ-ations. Prior work usually requires specific guidance such as the flickering frequency, manual annotations, or extra
In this work, we consistent videos to remove the flicker. propose a general flicker removal framework that only re-ceives a single flickering video as input without additional guidance. Since it is blind to a specific flickering type or guidance, we name this “blind deflickering.” The core of our approach is utilizing the neural atlas in cooperation with a neural filtering strategy. The neural atlas is a uni-fied representation for all frames in a video that provides temporal consistency guidance but is flawed in many cases.
To this end, a neural network is trained to mimic a filter to learn the consistent features (e.g., color, brightness) and avoid introducing the artifacts in the atlas. To validate our method, we construct a dataset that contains diverse real-world flickering videos. Extensive experiments show that our method achieves satisfying deflickering performance and even outperforms baselines that use extra guidance on a public benchmark. The source code is publicly available at
* Equal contribution.
† Corresponding authors. 1.

Introduction
A high-quality video is usually temporally consistent, but many videos suffer from flickering artifacts for var-ious reasons, as shown in Figure 2. For example, the brightness of old movies can be very unstable since some old cameras with low-quality hardware cannot set the ex-posure time of each frame to be the same [16]. Be-sides, high-speed cameras with very short exposure time can capture the high-frequency (e.g., 60 Hz) changes of in-door lighting [25]. Effective processing algorithms such as enhancement [38, 45], colorization [29, 60], and style transfer [33] might bring flicker when applied to tempo-rally consistent videos. Videos from video generations ap-proaches [46, 50, 61] also might contain flickering artifacts.
Since temporally consistent videos are generally more vi-sually pleasing, removing the flicker from videos is highly desirable in video processing [9, 13, 14, 54, 59] and compu-tational photography.
In this work, we are interested in a general approach for deflickering: (1) it is agnostic to the patterns or levels of flickering (e.g., old movies, high-speed cameras, process-ing artifacts), (2) it only takes a single flickering video and does not require other guidance (e.g., flickering types, ex-tra consistent videos). That is to say, this model is blind to flickering types and guidance, and we name this task as
1 e m a r
F 2 e m a r
F
Old movies Old cartoons
Time-lapse
Slow-motion Colorization White balance
Intrinsic
Text-to-video
Figure 2. Videos with flickering artifacts. Flickering artifacts exist in unprocessed videos, including old movies, old cartoons, time-lapse videos, and slow-motion videos. Besides, some processing algorithms [6, 18, 60] can introduce the flicker to temporally consistent unpro-cessed videos. Synthesized videos from video generations approaches [46, 50, 61] also might be temporal inconsistent. Blind deflickering aims to remove various types of flickering with only an unprocessed video as input. blind deflickering. Thanks to the blind property, blind de-flickering has very wide applications.
Blind deflickering is very challenging since it is hard to enforce temporal consistency across the whole video with-out any extra guidance. Existing techniques usually de-sign specific strategies for each flickering type with specific knowledge. For example, for slow-motion videos captured by high-speed cameras, prior work [25] can analyze the lighting frequency. For videos processed by image process-ing algorithms, blind video temporal consistency [31, 32] obtains long-term consistency by training on a temporally consistent unprocessed video. However, the flickering types or unprocessed videos are not always available, and existing flickering-specific algorithms cannot be applied in this case.
One intuitive solution is to use the optical flow to track the correspondences. However, the optical flow from the flick-ering videos is not accurate, and the accumulated errors of optical flow are also increasing with the number of frames due to inaccurate estimation [7].
With two key observations and designs, we successfully propose the first approach for blind deflickering that can re-move various flickering artifacts without extra guidance or knowledge of flicker. First, we utilize a unified video rep-resentation named neural atlas [26] to solve the major chal-lenge of solving long-term inconsistency. This neural atlas tracks all pixels in the video, and correspondences in differ-ent frames share the same pixel in this atlas. Hence, a se-quence of temporally consistent frames can be obtained by sampling from the shared atlas. Secondly, while the frames from the shared atlas are consistent, the structures of images are flawed: the neural atlas cannot easily model dynamic objects with large motion; the optical flow used to construct the atlas is imperfect. Hence, we propose a neural filtering strategy to take the treasure and throw the trash from the flawed atlas. A neural network is trained to learn the invari-ant under two types of distortion, which mimics the artifacts in the atlas and the flicker in the video, respectively. At test time, this network works as a filter to preserve the consis-tency property and block the artifacts from the flawed atlas.
We construct the first dataset containing various types of flickering videos to evaluate the performance of blind deflickering methods faithfully. Extensive experimental re-sults show the effectiveness of our approach in handling dif-ferent flicker. Our approach also outperforms blind video temporal consistency methods that use an extra input video as guidance on a public benchmark.
Our contributions can be summarized as follows:
• We formulate the problem of blind deflickering and construct a deflickering dataset containing diverse flickering videos for further study.
• We propose the first blind deflickering approach to re-move diverse flicker. We introduce the neural atlas to the deflickering problem and design a dedicated strat-egy to filter the flawed atlas for satisfying deflickering performance.
• Our method outperforms baselines on our dataset and even outperforms methods that use extra input videos on a public benchmark. 2.