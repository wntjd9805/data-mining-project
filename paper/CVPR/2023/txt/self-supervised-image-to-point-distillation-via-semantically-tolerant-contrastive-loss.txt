Abstract
An effective framework for learning 3D representations for perception tasks is distilling rich self-supervised image features via contrastive learning. However, image-to-point representation learning for autonomous driving datasets faces two main challenges: 1) the abundance of self-similarity, which results in the contrastive losses pushing away semantically similar point and image regions and thus disturbing the local semantic structure of the learned rep-resentations, and 2) severe class imbalance as pretraining gets dominated by over-represented classes. We propose to alleviate the self-similarity problem through a novel seman-tically tolerant image-to-point contrastive loss that takes into consideration the semantic distance between positive and negative image regions to minimize contrasting seman-tically similar point and image regions. Additionally, we address class imbalance by designing a class-agnostic bal-anced loss that approximates the degree of class imbalance through an aggregate sample-to-samples semantic similar-ity measure. We demonstrate that our semantically-tolerant contrastive loss with class balancing improves state-of-the-art 2D-to-3D representation learning in all evaluation set-tings on 3D semantic segmentation. Our method con-sistently outperforms state-of-the-art 2D-to-3D representa-tion learning frameworks across a wide range of 2D self-supervised pretrained models. 1.

Introduction
Self-supervised learning (SSL) has shown significant success in learning useful representations from unlabeled images [7, 9, 11, 22], mainly due to large, diverse, and bal-anced 2D image datasets. These successes promise to alle-viate the requirement for large labeled datasets, which can be expensive, not attainable, or task-specific. These issues are exacerbated when generating labels for 3D point clouds, which are usually much more difficult to annotate [27] than 2D images. Additionally, the sparse nature of point clouds generated using a LiDAR sensor, as is common in outdoor autonomous driving data, substantially increases the diffi-Figure 1. Bottom row: Superpixel-to-superpixel cosine similarity with respect to, bottom left: a road anchor, and bottom right: a vehicle anchor (both marked in red). Superpixel-driven contrastive loss [21] treats all superpixels excluding the anchor as negative samples. As such, loss will be dominated by gradients from se-mantically similar negative samples, disturbing the local seman-tic structure of the learned 3D representations. Our loss uses su-perpixel similarity to 1) Reduce the contribution of false negative samples, and 2) Balance the contribution of well-represented (i.e., road) and under-represented (i.e., vehicle) anchors culty of manually generating per-point labels, particularly at large distances.
A common approach to learn 3D representations is through multimodal invariance [16], where 3D represen-tations are learned to be invariant to features extracted from image encoders trained with self-supervised learn-ing [17, 21]. The current state-of-the-art, SLidR [21], en-courages learning representations of 3D point regions (su-perpoints) to match pre-trained representations of 2D image regions (superpixels) through a novel contrastive loss. By contrasting 2D and 3D regions, SLidR [21] enables learning representations from point clouds with varying point densi-ties, as is common in autonomous driving datasets.
Unfortunately, SlidR’s region-based sampling does not address self-similarity, which results when fewer unique semantic classes exist in the data relative to the number of chosen contrastive pairs during training. Under self-similarity, many negative samples will belong to the same semantic class as the positive sample used to compute the
contrastive loss, pushing apart their embeddings and break-ing the local semantic structure of learned 3D representa-tion [24](see Figure 1). This issue is further exacerbated by the implicit hardness-aware property of contrastive loss, where the largest gradient contributions come from the most semantically similar negative samples [24](see 3.1.2).
In addition, autonomous driving datasets are highly im-balanced, for example, in the nuScenes dataset [6], the
’Pedestrian’ class covers 0.25% of the data, while ’vege-tation’ class covers 22.19% of the data. Since the class of the positive sample is unknown during pretraining, SLidR’s loss gives an equal weight to all samples in the batch.
Hence, the 3D pretraining is predominately driven by gra-dients from a few over-represented samples, leading to poor performance on under-represented samples.
In this work, we simultaneously address the challenge of contrasting semantically similar point and image re-gions and the challenge of learning 3D representations from highly imbalanced autonomous driving datasets. Figure 1 shows that image regions semantically similar to the an-chor exhibit high cosine similarity in the 2D self-supervised feature space. Our first key idea is to exploit the seman-tic distance between positive and negative pooled image features to guide negative sample selection. Reducing the contribution of false negative samples, which are abundant in autonomous driving datasets due to the self-similarity, prevents the disturbance of the local semantic structure of the pre-trained 3D representations [24]. Figure 2 shows that most anchors come from over-represented classes (i.e., road, vegetation) resulting in a 3D point encoder that is less discriminative with respect to under-represented classes. To address this challenge, we propose using aggregate seman-tic similarity between samples as a proxy for class imbal-ance. By balancing the contribution of over and under-representated anchors, we improve the learned 3D represen-tations of under-represented semantic classes (i.e., pedestri-ans and vehicles). We summarize our approach with two main contributions, which we present below.
Semantically-Tolerant Loss. To address the similarity of samples in 2D-to-3D representation learning frameworks, we propose a novel contrastive loss that relies on 2D self-supervised image features to infer the semantic distance between positive and negative pooled image features. We propose to either directly reduce the gradient contribution of semantically-similar negative samples or exclude the K-nearest samples based on the semantic distance to the posi-tive sample.
Class Agnostic Balanced Loss. To address pre-training us-ing highly imbalanced 3D data, we propose a novel class ag-nostic balancing for contrastive losses that weights the con-tribution of each 3D region in a point cloud based on the ag-gregate semantic similarity of its corresponding 2D region with all negative samples. We reason that samples with high
MoCoV2
SwAV
DenseCL
DINO
Figure 2. t-SNE [23] visualization of superpixel-level features for a given batch of nuScenes [6] images. Each superpixel fea-ture is colorized based on its semantic class derived from LiDAR ground truth point-wise labels. Here, we show that MoCoV2 [10],
SwAV [7], DenseCL [25] and DINO [8] weights generate mean-ingful semantic clusters on the superpixel level. aggregate semantic similarity to other samples come from over-represented classes, while under-represented samples are semantically similar to very few other samples. Hence, we reduce the contribution of over-represented samples, while increasing that of under-represented samples.
By extending the state-of-the-art 2D-to-3D representa-tion learning frameworks using our proposed semantically-tolerant contrastive loss with class balancing, we show that we can improve their performance on in-distribution lin-ear probing and finetuning semantic segmentation, as well as on out-of-distribution few-shot semantic segmentation.
We also show that our proposed semantically-tolerant loss improves 3D semantic segmentation performance across a wide range of 2D self-supervised pretrained image features, consistently outperforming state-of-the-art 2D-to-3D repre-sentation learning frameworks. 2.