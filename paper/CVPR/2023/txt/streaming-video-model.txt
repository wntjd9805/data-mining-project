Abstract
Video understanding tasks have traditionally been mod-eled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as ac-tion recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vi-sion Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal de-*This work was done during the internship of Yucheng at MSRA.
†Corresponding author. coder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based ac-tion recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at https://github.com/yuzhms/
Streaming-Video-Model. 1.

Introduction
As a fundamental research topic in computer vision, video understanding mainly deals with two types of tasks.
The sequence-based [9, 56] tasks aim to understand what is happening in a period of time. For example, the action
to gather additional temporal information. We believe that a video frame should be treated differently from a single im-age and that temporal-aware spatial features are more pow-erful for solving frame-based video understanding tasks.
In this paper, we propose a unified architecture to han-dle both types of video tasks. The proposed streaming video model, as shown in Fig.1, circumvents the draw-backs of the conventional treatment by a two-stage de-it is composed of a temporal-aware sign. Specifically, spatial encoder, which extracts temporal-aware spatial fea-ture for each video frame, and a task-related temporal de-coder, which transfers frame-level features to task-specific outputs for sequence-based tasks. When compared with frame-based architecture, the temporal-aware spatial en-coder in streaming video model leverages additional infor-mation from past frames, so that it has potential to obtain more powerful and robust features. When compared with clip-based architecture, our model disentangles the frame-level feature extraction and clip-level feature fusion, so as to alleviate the computation pressure while enabling more flexible use scenarios, such as long-term video inference or online video inference.
We instantiate such a streaming video model by building the streaming video Transformer (S-ViT) based on the vi-sion Transformer [14]. S-ViT is featured by self-attention within a frame to extract spatial information and cross-attention across frames to make the fused feature temporal-aware. Specifically, for the first frame of a video, S-ViT extracts exactly the same spatial feature as a standard im-age ViT, but it stores keys and values of every Trans-former layer in a memory. For subsequent frames in a video, both intra-frame self-attention and inter-frame cross-attention [54] with the stored memory is calculated. S-ViT borrows ideas from triple 2D (T2D) decomposition [74] and limits the cross-attention region within patches with the same horizontal or vertical positions. This decomposition reduces the computational cost and allows S-ViT to handle long histories. The output of this stage can directly be used by the frame-based video tasks. For sequence-based tasks, an additional temporal decoder, implemented by a temporal
Transformer, is used to gather information from multiple frames.
We evaluate out S-ViT model on two downstream tasks.
The first task is the sequence-based action recognition. We get 84.7% top-1 accuracy on Kinetics-400 [23] dataset and 69.3% top-1 accuracy on Something-Something v2 [20] dataset, which is on par with the state-of-the-art, but at a reduced computation expenditure. The second task is
MOT, which operates on video frames in a widely adopted tracking-by-detection framework. We show that introduc-ing temporal-aware spatial encoder creates comparative ad-vantage over a frame-based architecture under a fair setting on MOT17 [40] benchmark.
Figure 2. Comparison on video modeling paradigm on both the sequence-based action recognition task and frame-based multi-ple object tracking task. The proposed streaming model achieves higher performance than the frame-based model on both tasks while has no loss compared to clip-based model on the sequence-based task. The clip-based model can not be directly used in frame-based tasks. recognition task classifies the object action in a video se-quence into a set of predefined categories. The frame-based tasks [11, 31, 72], on the other hand, aim to look for key information in a certain point of time in a video. For ex-ample, the multiple object tracking (MOT) task predicts the bounding boxes of objects in each video frame. Although both types of tasks take a video as input, they are handled very differently in computer vision research.
The different treatment of these two types of tasks is mainly reflected in the type of backbone network used.
The action recognition task is usually handled by a clip-based architecture, where a video model [1], which takes a video clip as input and outputs spatiotemporal features, is used. In the video object segmentation (VOS), video ob-ject detection (VOD), and multiple object tracking (MOT) tasks, however, a frame-based architecture [14, 21] is of-ten adopted. The frame-based architecture employs image backbone to generate independent spatial features for each frame. In most tracking-by-detection MOT solutions, these features are directly used as the input to the object detector.
Both types of treatment have their respective drawbacks.
On the one hand, the clip-based architecture processes a group of video frames at one time, which puts great pressure on the processor’s memory space and processing power. As a result, it is difficult to handle long videos or long actions effectively. In addition, the summarized spatiotemporal fea-tures extracted by a video backbone usually lack sufficient spatial resolution to be used for dense prediction tasks. On the other hand, the frame-based architecture does not con-sider surrounding frames in the process of spatial feature extraction. As a result, the features do not contain any tem-poral information or an out-of-band mechanism is in need
We summarize the contributions as follows. First, we propose a unified architecture, named streaming video model, for both frame-based and sequence-based video un-derstanding tasks. Second, we implement a T2D-based streaming video Transformer and demonstrate how it can be used to serve different types of video tasks. Third, ex-periments on action recognition and MOT tasks show that our unified model could achieve state-of-the-art results on both types of tasks. We believe that the work presented in this paper is a solid step towards a universal video process-ing architecture. 2.