Abstract
Prior work has shown that Visual Recognition datasets frequently underrepresent bias groups B (e.g. Female) within class labels Y (e.g. Programmers). This dataset bias can lead to models that learn spurious correlations between class labels and bias groups such as age, gender, or race.
Most recent methods that address this problem require sig-nificant architectural changes or additional loss functions requiring more hyper-parameter tuning. Alternatively, data sampling baselines from the class imbalance literature (e.g.
Undersampling, Upweighting), which can often be imple-mented in a single line of code and often have no hyper-parameters, offer a cheaper and more efficient solution.
However, these methods suffer from significant shortcom-ings. For example, Undersampling drops a significant part of the input distribution per epoch while Oversampling re-peats samples, causing overfitting. To address these short-comings, we introduce a new class-conditioned sampling method: Bias Mimicking. The method is based on the obser-vation that if a class c bias distribution, i.e. PD(B|Y = c) is mimicked across every c′ ̸= c, then Y and B are statisti-cally independent. Using this notion, BM, through a novel training procedure, ensures that the model is exposed to the entire distribution per epoch without repeating samples.
Consequently, Bias Mimicking improves underrepresented groups’ accuracy of sampling methods by 3% over four benchmarks while maintaining and sometimes improving performance over nonsampling methods. Code: https:
//github.com/mqraitem/Bias-Mimicking 1.

Introduction
Figure 1. Comparison of sampling approaches for mitigating bias of class labels Y (Hair Color) toward sensitive group labels B (a) illustrates Undersampling/Oversampling methods (Gender). that drop/repeat samples respectively from a dataset D per epoch and thus ensure that PD(Y |B) = PD(Y ). However, dropping samples hurt the model’s predictive performance, and repeating samples can cause overfitting with over-parameterized models like neural nets [34]. (b) shows our Bias Mimicking approach which subsamples D and produces three variants. Each variant, denoted as dc ⊂ D, preserves class c samples (i.e. mimicked class) and mimics the bias of class c in each c′
̸= c. This mimicking pro-cess, as we show in our work, ensures that Pdc (Y |B) = Pdc (Y ).
Moreover, by using each dc separately to train the model, we ex-pose it to all the samples in D per epoch, and since we do not repeat samples in each dc, our method is less prone to overfitting.
Spurious predictive correlations have been frequently documented within the Deep Learning literature [33, 37].
These correlations can arise when most samples in class a c (e.g. blonde hair) belong to a bias group s (e.g. fe-male). Thus, the model might learn to predict classes by us-ing their membership to their bias groups (e.g. more likely to predict blonde hair if a sample is female). Mitigating such spurious correlations (Bias) involves decorrelating the model’s predictions of input samples from their member-ship to bias groups. Previous research efforts have primar-ily focused on model-based solutions. These efforts can be mainly categorized into two directions 1) ensemble-based methods [34], which introduce separate prediction heads for samples from different bias groups 2) methods that intro-duce additional bias regularizing loss functions and require additional hyper-parameter tuning [12, 15, 25, 26, 32].
Dataset resampling methods, popular within the class imbalance literature [3, 8, 13, 28], present a simpler and cheaper alternative. They do not require hyperparameter tuning or extra model parameters. Therefore, they are faster to train. Moreover, as illustrated in Figure 1(a), they can be extended to Bias Mitigation by considering the imbalance within the dataset subgroups rather than classes. Most com-mon of these methods are Undersampling [3, 13, 28] and
Oversampling [34]. They mitigate class imbalance by alter-ing the dataset distribution through dropping/repeating sam-ples, respectively. Another similar solution is Upweight-ing [4,29], which levels each sample contribution to the loss function by appropriately weighting its loss value. How-ever, these methods suffer from significant shortcomings.
For example, Undersampling drops a significant portion of the dataset per epoch, which could harm the models’ predic-tive capacity. Moreover, Upweighting can be unstable when used with stochastic gradient descent [2]. Finally, models trained with Oversampling, as shown by [34], are likely to overfit due to being exposed to repetitive sample copies.
To address these problems, we propose Bias Mimick-ing (BM): a class-conditioned sampling method that mit-igates the shortcomings of prior work. As shown in Fig-ure 1(b), given a dataset D with a set of three classes C,
BM subsamples D and produces three different variants.
Each variant, dc ⊂ D retains every sample from class c while subsampling each c′
̸= c such that c′ bias distribu-tion, i.e. Pdc(B|Y = c′), mimics that of c. For example, observe dBlonde Hair in Figure 1(b) bottom left. Note how the bias distribution of class ”Blonde Hair” remains the same while the bias distributions of ”Black Hair” and ”Red Hair” are subsampled such that they mimic the bias distribution of ”Blonde Hair”. This mimicking process decorrelates Y from B since Y and B are now statistically independent as we prove in Section 3.1.
The strength of our method lies in the fact that dc re-tains class c samples while at the same time ensuring
Pdc (Y |B) = Pdc(Y ) in each dc. Using this result, we intro-duce a novel training procedure that uses each distribution separately to train the model. Consequently, the model is exposed to the entirety of D since each dc retains class c samples. Refer to Section 3.1 for further details. Note how our method is fundamentally different from Undersampling.
While Undersampling also ensures statistical independence on the dataset level, it subsamples every subgroup. There-fore, the training distribution per epoch is a smaller portion of the total dataset D. Moreover, our method is also differ-ent from Oversampling since each dc does not repeat sam-ples. Thus we reduce the risk of overfitting.
In addition to proposing Bias Mimicking, another con-tribution of our work is providing an extensive analysis of sampling methods for bias mitigation. We found many sampling-based methods were notably missing in the com-parisons used in prior work [12, 32, 34]. Despite their short-comings, we show that Undersampling and Upweighting are surprisingly competitive on many bias mitigation bench-marks. Therefore, this emphasizes these methods’ impor-tance as an inexpensive first choice for mitigating bias.
However, in cases where these methods are ineffective, Bias
Mimicking bridges the performance gap and achieves com-parable performance to nonsampling methods. Finally, we thoroughly analyze our approach’s behavior through two experiments. First, we verify the importance of each dc to the model’s predictive performance in Section 4.2. Second, we investigate our method’s sensitivity to the mimicking condition in Section 4.3. Both experiments showcase the importance of our design in mitigating bias.
Our contributions can be summarized as:
• We show that simple sampling methods can be compet-itive on some benchmarks when compared to non sam-pling state-of-the-art approaches.
• We introduce a novel resampling method: Bias Mimick-ing that bridges the performance gap between sampling and nonsampling methods; it improves the average under-represented subgroups accuracy by > 3% compared to other sampling methods.
• We conduct an extensive empirical analysis of Bias Mim-icking that details the method’s sensitivity to the Mimick-ing condition and uncovers insights about its behavior. 2.