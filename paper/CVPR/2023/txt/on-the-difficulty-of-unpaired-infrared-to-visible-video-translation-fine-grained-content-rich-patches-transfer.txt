Abstract
Explicit visible videos can provide sufficient visual in-formation and facilitate vision applications. Unfortunately, the image sensors of visible cameras are sensitive to light conditions like darkness or overexposure. To make up for this, recently, infrared sensors capable of stable imaging have received increasing attention in autonomous driving and monitoring. However, most prosperous vision mod-els are still trained on massive clear visible data, facing huge visual gaps when deploying to infrared imaging sce-narios. In such cases, transferring the infrared video to a distinct visible one with fine-grained semantic patterns is a worthwhile endeavor. Previous works improve the out-puts by equally optimizing each patch on the translated vis-ible results, which is unfair for enhancing the details on content-rich patches due to the long-tail effect of pixel dis-tribution. Here we propose a novel CPTrans framework to tackle the challenge via balancing gradients of different patches, achieving the fine-grained Content-rich Patches
Transferring. Specifically, the content-aware optimization module encourages model optimization along gradients of target patches, ensuring the improvement of visual details.
Additionally, the content-aware temporal normalization module enforces the generator to be robust to the motions of target patches. Moreover, we extend the existing dataset In-fraredCity to more challenging adverse weather conditions (rain and snow), dubbed as InfraredCity-Adverse1. Exten-sive experiments show that the proposed CPTrans achieves state-of-the-art performance under diverse scenes while re-quiring less training time than competitive methods. 1.

Introduction
Visible light cameras have broad applicability in com-puter vision algorithms for the sufficient visual informa-(cid:12)
Corresponding author 1The code and dataset are available at https://github.com/BIT-DA/I2V-Processing
Figure 1. (a) Visualization of pixel category distribution on dataset
IRVI [25] and semantic examples in random selected frames. We conduct semantic segmentation via a pre-trained SegFormer [47] on all visible video frames of IRVI and predict all pixels accord-ing to the predefined categories in ADE20K [52]. (b) Outputs and
GradCAM++ results of different methods. ROMA pays equal at-tention on the whole output, and the long-tail effect of training data leads to the generation optimization along prejudiced gradi-ents caused by the large proportion of pixels (e.g., sky and road).
We can generate more vivid details for content-rich patches (e.g., cars and road signs) than other methods. tion (e.g., structure, texture, and color) of their captured results. Most state-of-the-art vision algorithms have been observed to show admirable performance under clear visi-bility conditions [10, 16, 50]. Unfortunately, in most cases, the real-world weather is unpredictable and diverse, leading to complex and variable light conditions like overexposure on snowing days. While image sensors of visible cameras are sensitive to light conditions, their imaging results are ambiguous in adverse weather. Under such circumstances, people take infrared sensors to make up for the deficien-cies of visible cameras. These infrared sensors can capture stable structural information in diverse environments due to the thermal imaging principle. In emergency avoidance or hazard detection, they could be applied in autonomous driv-ing and monitoring scenarios [28, 30]. However, most com-puter vision models are trained under visible data. Although infrared videos outline surrounding objects all the time, the existing huge gaps and semantic lacking problems hinder
the applications in infrared imaging scenarios. Therefore, it is worth translating stable and accessible infrared videos into clear visible ones. The translated visible results may provide visual information for supporting visual applica-tions like object detection and semantic segmentation.
To tackle the unpaired infrared-to-visible translation challenge, previous methods [12, 13, 33, 37] mainly fo-cus on learning color mapping functions with complex manual coloring. The high costs and inevitable human bias limit the application of such approaches.
Inspired by GANs [11], unpaired image translation methods have emerged. For instance, cycle-based methods [17, 21, 22, 48] preserve content during the translation via the cycle con-sistency [19]. Furthermore, one-sided methods [20, 31, 51] maintain the content through hand-designed feature-level restraints. However, substantial visual gaps between in-frared and visible data lead to difficulties in generating fine-grained visible results. Additionally, continuous in-frared video signals are more challenging to transfer be-cause of the need to ensure temporal consistency. Thus, taking long-term information into account,
[3, 7, 25] pro-pose their temporal consistency losses to refine frameworks based on unpaired image translation methods. Besides,
I2V-GAN [25] and ROMA [49] are tailored approaches for unpaired infrared-to-visible video translation. Espe-cially, ROMA has achieved state-of-art performance, il-lustrating the importance of retaining structural informa-tion and proposing cross-similarity consistency for struc-ture. Despite its success, experiments indicate that cross-similarity still faces challenges in accurately transferring fine-grained (i.e., realistic and delicate) details, especially for the content-rich patches.
In fact, most GAN-based methods utilize the PatchGAN discriminator [19] for style optimization. Similar to the classification task, the discriminator outputs w × h predic-tions (True or False) for corresponding patches. To ana-lyze the optimizing behavior of discriminators in the train-ing process, we visualize the gradients via GradCAM++ [6] and pixel category distribution as shown in Fig. 1. Grad-CAM++ utilizes the gradients of the classification score to identify the parts of interest. The left part (a) shows that a few majority categories occupy most of the pixels while most minority categories contain a limited number of pix-els. Additionally, content-rich patches (including rich vi-sual details like patches of cars) are mostly the minority categories, while those content-lacking patches (including lacking visual details like patches of the sky) are mostly the majority. Upon exposure to new data, gradient-based optimization methods, without any constraint, change the learned encoding to minimize the objective function with global respect [36]. Thus, equal optimization for each patch (GradCAM++ of ROMA on Fig. 1 (b)) faces prejudiced gra-dients to content-lacking patches (i.e., major pixels) when applied to the generation. Moreover, it will lead to the in-ability of discriminators to improve the qualities of content-rich patches. An approach is needed to break the prejudice on optimization caused by the usually exhibiting long-tail distribution in real-world training data [24, 45, 54].
In this paper, we start with the analysis of difficulty for fine-grained Content-rich Patches Transfer on unpaired infrared-to-visible video translation and propose the CP-Trans framework To improve the results of content-rich patches, we introduce two novel modules: Content-aware
Optimization (CO), balancing the gradients of patches for improving generated content-rich patches, and Content-aware Temporal Normalization (CTN), which enforces the generator to be robust to the motion of content. Be-sides, we extend the InfraredCity dataset to adverse weather conditions (i.e., raining and snowing scenes), noted as
InfraredCity-Adverse, for promoting infrared-related re-search. Our extensive evaluations of diverse datasets show that our approach improves upon the previous ROMA method, setting new state-of-the-art performances on un-paired infrared-to-visible video translation. Remarkably, further applications validate our task’s value and confirm our approach’s admirable performance. Contributions are:
• We focus on the difficulty of fine-grained unpaired infrared-to-visible video translation and point out the ex-isting problem that models are optimized along preju-diced gradients due to the long-tail effect.
• We propose a novel CPTrans framework consisting of content-aware optimization and temporal normalization, which benefits the generation of content-rich patches.
• We extend the InfraredCity to more challenging ad-verse weather conditions (rain and snow), noted as
InfraredCity-Adverse for infrared-related study and val-idate the remarkable success of CPTrans through suffi-cient experiments (including further applications, i.e., ob-ject detection, video fusion, and semantic segmentation). 2.