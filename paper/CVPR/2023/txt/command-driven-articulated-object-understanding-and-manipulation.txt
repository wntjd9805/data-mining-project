Abstract
We present Cart, a new approach towards articulated-object manipulations by human commands. Beyond the existing work that focuses on inferring articulation struc-tures, we further support manipulating articulated shapes to align them subject to simple command templates. The key of Cart is to utilize the prediction of object structures to connect visual observations with user commands for effective manipulations. It is achieved by encoding com-mand messages for motion prediction and a test-time adap-tation to adjust the amount of movement from only com-mand supervision. For a rich variety of object categories,
Cart can accurately manipulate object shapes and outper-form the state-of-the-art approaches in understanding the inherent articulation structures. Also, it can well general-ize to unseen object categories and real-world objects. We hope Cart could open new directions for instructing ma-chines to operate articulated objects. Code is available at https:// github.com/ dvlab-research/ Cart. 1.

Introduction
Articulated objects such as doors, cabinets, and laptops are ubiquitous in our daily life. Enabling machines to manip-ulate them under human instructions will pave the way for many applications, such as robotic work, human-computer interaction, and augmented reality. Recent research majorly focuses on understanding the structural properties of articu-lated objects, e.g., discovering the movable parts [51,61] and estimating the articulations between connected parts [19, 28].
Despite their success, the way from understanding objects to user-controllable manipulation remains challenging.
First, the model should be able to interpret the user in-struction and determine the corresponding object part for performing the manipulation action. To do so requires un-derstanding not only the object’s articulated structure but
If a part is also the operational properties (see Fig. 1). movable, the action type and current state should be rec-*Corresponding Author
Figure 1. Given a single point cloud of an articulated object and a user command, Cart accordingly manipulates the object shape, based on understanding object structure and connecting the com-mand to predict the motion parameters. The command gives the desired state and we use it to check if the manipulation succeeds. ognized ahead. Second, to achieve successful manipulation, the model should know the target configuration or state of the object so as to determine the associated manipulation parameters, such as the rotation orientation and angle, for achieving the user instruction (see Fig. 1).
In this work, we study the above challenges by formu-lating a new task called command-driven articulated object manipulation. Given a visual observation of an articulated object and a simple user command that specifies how to manipulate the object, our objective is to manipulate the associated object part to reach the target articulation state given by the command. Specifically, this task combines the needs of understanding the object structure (which part to move), recognizing the associated articulation property (how it moves), and predicting the amount of motion subject to the user command (how much to move). To this end, we propose a novel learning-based framework called Command-driven articulation modeling, or Cart for short; see Fig. 1.
In particular, Cart is powerful in both structure under-standing and shape manipulation for articulated objects. The former acts as a critical building block, where we jointly
segment individual object parts, predict their motion joints, and model the current articulation state, given only a single visual observation. Based on the structural prediction, we further connect the user commands with the inherent object geometry for effective object manipulation.
In our approach, we choose structured templates to create user-friendly commands. This design allows users to select a single point on object’s point cloud to identify the part to operate and specify its desired target state. To associate the user-selected point to an intact object part, we lever-age model segmentation prediction to locate the matched one. Subsequently, we transform the target states to a learn-able embedding and fuse it with the visual features in a part-aware manner. Since the fused features encapsulate twofold information, it can be further applied to estimate the command-related motion parameters.
After the manipulation, to ensure the target object state satisfies the user command, we formulate the novel Test-time State Adaptation (TTSA) algorithm for state regular-ization. It automatically generates the state label from the command input and uses it to optimize the movement param-eters, thereby adjusting the final object status. Essentially, it is self-supervised and works without requiring ground-truth supervision. With little additional computation cost,
TTSA is able to robustly improve the quality of the object manipulation, especially for unseen object categories.
We extensively evaluate Cart on two widely-used datasets [51, 57] of articulated objects. Compared to the recent work [19, 28, 61], Cart yields superior predictive per-formance on both static object structure and dynamic motion parameters used for manipulation. We specifically show that each previous method can only partially achieve our task, and their simple augmentations to fit our setting underper-form as well. Then, we apply our method to real-world scenarios. We provide examples of manipulating real articu-lated objects’ shape subject to the user command. Further, we spawn our predictions in a virtual environment to sim-ulate robotic manipulations. Our overall contributions are listed as follows.
• We propose a command-driven shape manipulation task for articulated objects. It enables a simple interaction way for users to control the manipulation procedure.
• We present an integrated framework Cart towards this systematic task. It succeeds by understanding the ar-ticulated object structure and further connecting user commands to predict movement to the desired state.
• Cart works on both articulation understanding and ob-ject manipulation on synthetic and real-world data. 2.