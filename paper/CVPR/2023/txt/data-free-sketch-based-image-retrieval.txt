Abstract
Data-Dependent (Conventional)
Rising concerns about privacy and anonymity preser-vation of deep learning models have facilitated research in data-free learning (DFL). For the ﬁrst time, we iden-tify that for data-scarce tasks like Sketch-Based Image Re-trieval (SBIR), where the difﬁculty in acquiring paired pho-tos and hand-drawn sketches limits data-dependent cross-modal learning algorithms, DFL can prove to be a much more practical paradigm. We thus propose Data-Free (DF)-SBIR, where, unlike existing DFL problems, pre-trained, single-modality classiﬁcation models have to be leveraged to learn a cross-modal metric-space for retrieval without access to any training data. The widespread availability of pre-trained classiﬁcation models, along with the difﬁculty in acquiring paired photo-sketch datasets for SBIR justify the practicality of this setting. We present a methodology for DF-SBIR, which can leverage knowledge from models independently trained to perform classiﬁcation on photos and sketches. We evaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks, designing a variety of baselines based on state-of-the-art DFL literature, and observe that our method surpasses all of them by signiﬁ-cant margins. Our method also achieves mAPs competitive with data-dependent approaches, all the while requiring no training data. Implementation is available at https:
//github.com/abhrac/data-free-sbir. 1.

Introduction
Motivated by the high degree of expressiveness and ﬂex-ibility provided by sketches, sketch-based image retrieval (SBIR) has emerged as a popular area of computer vision research [3,6,7,13,15]. SBIR is generally achieved by train-ing photo and sketch encoders to respectively map photo and sketch inputs to a class or instance aligned common space. Training deep neural photo-sketch encoders for this task, however, requires datasets with matching photo-sketch pairs [45, 58]. Unlike photos, sketches are fundamentally difﬁcult to acquire as drawing them involve long time peri-ods of laborious human participation. Driven by this practi-Real Data
Data-Free (Proposed)
Class-Aligned 
Estimated Distributions
Inverted 
Photo 
Classifier
Independently 
Trained
Inverted 
Sketch 
Classifier
Photo 
Encoder
Sketch 
Encoder
Photo 
Encoder
Sketch 
Encoder
Cross-Modal 
Representation Space
Figure 1. Our proposed Data-Free setting for SBIR does not need a real-world dataset of paired sketches and photos. Using only independently trained, modality-speciﬁc classiﬁers, it can estimate their train set distributions, as well as pair them at class-level for training the sketch and photo encoders. cal constraint, the problem has been studied under a variety of data-scarce settings like semi-supervised [3], few-shot class incremental [5], zero-shot [14], and any-shot [15].
However, all such settings assume the availability of some amount of instance/class-aligned data for training the en-coders. With the tremendous effort involved in acquir-ing such labelled photo-sketch pairs [3, 13, 14], as well as the rising concerns about privacy, security and anonymity preservation abilities of deep learning models [8,31,46,51], such assumptions may no longer be practical.
With this view, we propose Data-Free Sketch-Based Im-age Retrieval (DF-SBIR), a novel setting that requires train-ing photo and sketch encoders for retrieval, but with no training data. Speciﬁcally, we only assume access to pre-trained photo and sketch classiﬁcation models. In contrast to unsupervised cross-domain image retrieval [22] which only requires access to training data, but with no in-domain or cross-domain labels, our setting goes a step further and assumes access to no training data at all. Since classiﬁcation does not require cross-modal pairings as in SBIR, and ow-ing to the recent advances in domain generalization [29,52],
such pre-trained classiﬁers are widely available [40, 52], making our setting quite practical. Under this scenario, the problem of training the encoders can be posed in the light of data-free knowledge distillation (DFKD) [8, 30]. Classi-cal knowledge distillation [21] aims to transfer knowledge from a pre-trained model (teacher) to a different model (stu-dent), by aligning the predictions of the latter with the for-mer on train set inputs. Differently, DFKD aims to achieve this knowledge transfer without access to any form of train-ing data. The conventional approach to DFKD involves the following two steps – (1) Reconstructing the train set dis-tribution of the teacher; (2) Training the student network to match its predictions with that of the teacher, on sam-ples from the reconstructed distribution. However, existing
DFKD approaches so far have only been able to operate in a single modality, performing the same kind of task as that of the teacher. SBIR, being a cross-modal retrieval problem, cannot be tackled in the data-free setting by directly adapt-ing the machineries developed for DFKD, the reasons for which we detail below.
First, the teachers (being classiﬁers) and the students (being encoders) operate in metric spaces of different na-ture, i.e., probabilistic and Euclidean respectively. This re-strains us from measuring the agreement between teach-ers and students in a straightforward way, thus preventing the direct application of state-of-the-art approaches from the DFKD literature like data-free adversarial distillation
[10, 35]. We address this by designing a uniﬁed, class-proxy based interface via which the teachers and students can interact. Second, the sketch and the image classiﬁers that act as teachers are independently trained on modality-speciﬁc data. Their intermediate representations are thus modality sensitive. However, the representations learned by the encoders to be used for DF-SBIR need to be modal-ity invariant. To this end, we introduce the concept of a modality guidance network, which constrains the recon-structions to belong to speciﬁc (photo/sketch) modalities.
Training the encoders with such samples will ensure that they learn to eliminate unnecessary, modality-speciﬁc in-formation. Third, the independent training of the classiﬁers also mean that their train set distributions may not have direct class-level correspondence. To address this, we de-sign our distribution estimation process to reconstruct class-aligned samples, i.e., ones that have class-level correspon-dence across the two modalities. This will guarantee the availability of matching photo-sketch pairs for the metric learning of the encoders. Our approach is hence able to perform Data-Free Learning Across Modalities and Metric-Spaces, which motivates us to abbreviate it as CrossX-DFL.
We make the following contributions – (1) Propose Data-Free SBIR, a novel setting keeping in view the data-scarcity constraints arising from the collection of paired photos and sketches for SBIR, as well as concerns around pri-vacy preservation; (2) A class-proxy based approach to per-form data-free adversarial distillation from teachers with probabilistic outputs to students with outputs in the Eu-clidean space; (3) A novel technique to reconstruct class-aligned samples across independent modalities for cross-modal data-free knowledge distillation; (4) Introduce the concept of a modality guidance network to constrain the re-constructed sample distributions to speciﬁc modalities; (5)
Extensive experiments on benchmark datasets and ablation studies that demonstrate the usefulness of our novel compo-nents in providing competitive performance relative to the data-dependent setting. 2.