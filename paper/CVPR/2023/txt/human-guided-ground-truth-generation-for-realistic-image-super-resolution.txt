Abstract
How to generate the ground-truth (GT) image is a criti-cal issue for training realistic image super-resolution (Real-ISR) models. Existing methods mostly take a set of high-resolution (HR) images as GTs and apply various degra-dations to simulate their low-resolution (LR) counterparts.
Though great progress has been achieved, such an LR-HR pair generation scheme has several limitations. First, the perceptual quality of HR images may not be high enough, limiting the quality of Real-ISR outputs. Second, existing schemes do not consider much human perception in GT generation, and the trained models tend to produce over-smoothed results or unpleasant artifacts. With the above considerations, we propose a human guided GT generation scheme. We first elaborately train multiple image enhance-ment models to improve the perceptual quality of HR im-ages, and enable one LR image having multiple HR coun-terparts. Human subjects are then involved to annotate the high quality regions among the enhanced HR images as GTs, and label the regions with unpleasant artifacts as negative samples. A human guided GT image dataset with both positive and negative samples is then constructed, and a loss function is proposed to train the Real-ISR mod-els. Experiments show that the Real-ISR models trained on our dataset can produce perceptually more realistic re-sults with less artifacts. Dataset and codes can be found at https://github.com/ChrisDud0257/HGGT. 1.

Introduction
Owing to the rapid development of deep learning tech-niques [14, 18, 19, 22, 44], the recent years have witnessed the great progress in image super-resolution (ISR) [2, 8–10, 12, 13, 23, 26–29, 31–33, 35, 45, 46, 48, 51, 52, 54, 56], which aims at generating a high-resolution (HR) version of the low-resolution (LR) input. Most of the ISR models (e.g.,
*Equal contribution.
†Corresponding author. This work is supported by the Hong Kong RGC
RIF grant (R5001-18) and the PolyU-OPPO Joint Innovation Lab.
Figure 1. From left to right and top to bottom: one original HR image (Ori) in the DIV2K [1] dataset, two of its enhanced positive versions (Pos-1 and Pos-2) and one negative version (Neg). The positive versions generally have clearer details and better percep-tual quality, while the negative version has some unpleasant visual artifacts. Please zoom in for better observation.
CNN [37, 38] or transformer [5, 29] based ones) are trained on a large amount of LR-HR image pairs, while the gen-eration of LR-HR image pairs is critical to the real-world performance of ISR models.
Most of the existing ISR methods take the HR images (or after some sharpening operations [46]) as ground-truths (GTs), and use them to synthesize the LR images to build the LR-HR training pairs. In the early stage, bicubic down-sampling is commonly used to synthesize the LR images from their HR counterparts [8,9,23,33,42,56]. However, the
ISR models trained on such HR-LR pairs can hardly gen-eralize to real-world images whose degradation process is much more complex. Therefore, some researchers proposed to collect HR-LR image pairs by using long-short camera focal lengths [3, 4]. While such a degradation process is more reasonable than bicubic downsampling, it only covers a small subspace of possible image degradations. Recently, researchers [12, 20, 30, 32, 34, 46, 50, 51, 59] have proposed
to shuffle or combine different degradation factors, such as
Gaussian/Poisson noise, (an-)isotropic blur kernel, down-sampling/upsampling, JPEG compression and so on, to syn-thesize LR-HR image pairs, largely improving the general-ization capability of ISR models to real-world images.
Though great progress has been achieved, existing LR-HR training pair generation schemes have several limita-tions. First, the original HR images are used as the GTs to supervise the ISR model training. However, the perceptual quality of HR images may not be high enough (Fig. 1 shows an example), limiting the performance of the trained ISR models. Second, existing schemes do not consider much human perception in GT generation, and the trained ISR models tend to produce over-smoothed results. When the adversarial losses [27, 40, 48] are used to improve the ISR details, many unpleasant artifacts can be introduced.
In order to tackle the aforementioned challenges, we pro-pose a human guided GT data generation strategy to train perceptually more realistic ISR (Real-ISR) models. First, we elaborately train multiple image enhancement models to improve the perceptual quality of HR images. Meanwhile, one LR image can have multiple enhanced HR counter-parts instead of only one. Second, to discriminate the visual quality between the original and enhanced images, human subjects are introduced to annotate the regions in enhanced
HR images as “Positive”, “Similar” or “Negative” samples, which represent better, similar or worse perceptual qual-ity compared with the original HR image. Consequently, a human guided multiple-GT image dataset is constructed, which has both positive and negative samples.
With the help of human annotation information in our dataset, positive and negative LR-GT training pairs can be generated (examples of the positive and negative GTs can be seen in Fig. 1), and a new loss function is proposed to train the Real-ISR models. Extensive experiments are conducted to validate the effectiveness and advantages of the proposed
GT image generation strategy. With the same backbone, the
Real-ISR models trained on our dataset can produce more perceptually realistic details with less artifacts than models trained on the current datasets. 2.