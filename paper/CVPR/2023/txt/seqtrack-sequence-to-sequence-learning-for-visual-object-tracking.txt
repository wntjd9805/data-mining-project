Abstract
In this paper, we present a new sequence-to-sequence learning framework for visual tracking, dubbed SeqTrack.
It casts visual tracking as a sequence generation problem, which predicts object bounding boxes in an autoregres-sive fashion. This is different from prior Siamese track-ers and transformer trackers, which rely on designing com-plicated head networks, such as classification and regres-sion heads. SeqTrack only adopts a simple encoder-decoder transformer architecture. The encoder extracts visual fea-tures with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregres-sively with a causal transformer. The loss function is a plain cross-entropy. Such a sequence learning paradigm not only simplifies tracking framework, but also achieves competitive performance on benchmarks. For instance, Se-qTrack gets 72.5% AUC on LaSOT, establishing a new state-of-the-art performance. Code and models are available at https://github.com/microsoft/VideoX. 1.

Introduction
Visual object tracking is a fundamental task in computer vision. It aims to estimate the position of an arbitrary tar-get in a video sequence, given only its location in the ini-tial frame. Existing tracking approaches commonly adopt a divide-and-conquer strategy, which decomposes the track-ing problem into multiple subtasks, such as object scale es-timation and center point localization. Each subtask is ad-dressed by a specific head network. For example, SiamRPN
[27] and its follow-up works [3, 7, 48, 55, 58] adopt classi-fication heads for object localization and regression heads for scale estimation, as sketched in Fig. 1(a). STARK [53] and transformer-based trackers [4, 10, 17, 44] design corner head networks to predict the bounding box corners of target objects, as visualized in Fig. 1(b).
Such a divide-and-conquer strategy has demonstrated su-perior performance on tracking benchmarks and thereby be-come the mainstream design in existing models. However,
† Corresponding authors: Houwen Peng (houwen.peng@microsoft.com),
Dong Wang (wdice@dlut.edu.cn).
Figure 1. Comparison of tracking frameworks. (a) The framework with object classification head and bounding box regression head. (b) The framework with corner prediction heads. (c) Sequence-to-sequence tracking framework without complicated head networks. two deficiencies still exist. First, each subtask requires a customized head network, leading to a complicated track-ing framework. Second, each head network requires one or more learning loss functions, e.g., cross-entropy loss [7,27],
ℓ1 loss [7,27,53,55], generalized IoU loss [7,53,55], which make the training difficult due to extra hyperparameters.
To address these issues, in this paper, we propose a new
Sequence-to-sequence Tracking (SeqTrack) framework, as shown in Fig. 1(c). By modeling tracking as a sequence generation task, SeqTrack gets rid of complicated head net-works and redundant loss functions. It is based upon the intuition that if the model knows where the target object is, we could simply teach it how to read the bounding box out, rather than explicitly performing additional classifica-tion and regression using a divide-and-conquer strategy.
To this end, we convert the four values of a bounding box into a sequence of discrete tokens and make the model to learn generating this sequence token-by-token. We adopt a simple encoder-decoder transformer to model the gener-ation. The encoder is to extract visual features of video frames, while the decoder is to generate the sequence of bounding box values using the extracted features. The generation is executed in an autoregressive fashion, which means the model generates a token depending on previously observed ones. At each step, a new generated token value is fed back into the model to produce the next one. We impose a causal mask on the self-attention modules in the decoder to prevent tokens from attending to subsequent to-kens. Such a causal masking mechanism ensures that the generation of the token at position i only depends on its proceeding tokens at positions less than i. The visual fea-tures are integrated into the decoder through cross-attention layers [46]. The generation ends when it outputs four token values of the bounding box. The output sequence is directly used as the result.
Experiments demonstrate our SeqTrack method is effec-tive, achieving new state-of-the-art performance on several tracking benchmarks. For instance, SeqTrack-B256 obtains 74.7% AO score on GOT-10k [20], outperforming the re-cent OSTrack-256 tracker [55] by 3.7% under aligned set-tings, i.e., using the same encoder architecture and input resolution. Moreover, compared to the recent state-of-the-art tracker MixFormer [10], SeqTrack-B256 runs 1.4 times faster (40 v.s. 29 fps) while getting 0.7% superior AUC score on LaSOT [16]. It is worth noting that all these prior methods heavily rely on well-designed head networks and the corresponding complicated loss functions [30, 41]. In contrast, our SeqTrack only adopts a plain encoder-decoder transformer architecture with a simple cross-entropy loss.
In summary, the contributions of this work are two-fold:
• We propose a sequence-to-sequence learning method for
It casts tracking as a generation task, visual tracking. which offers a new perspective on tracking modeling.
• We present a new family of sequence tracking models, which strike a good trade-off between speed and accu-racy. Experiments verify the efficacy of the new models. 2.