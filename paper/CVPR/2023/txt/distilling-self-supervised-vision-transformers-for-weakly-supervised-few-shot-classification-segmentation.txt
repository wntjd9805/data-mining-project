Abstract
We address the task of weakly-supervised few-shot im-age classification and segmentation, by leveraging a Vision
Transformer (ViT) pretrained with self-supervision. Our proposed method takes token representations from the self-supervised ViT and leverages their correlations, via self-attention, to produce classification and segmentation pre-dictions through separate task heads. Our model is able to effectively learn to perform classification and segmen-tation in the absence of pixel-level labels during train-ing, using only image-level labels. To do this it uses at-tention maps, created from tokens generated by the self-supervised ViT backbone, as pixel-level pseudo-labels. We also explore a practical setup with “mixed” supervision, where a small number of training images contains ground-truth pixel-level labels and the remaining images have only image-level labels. For this mixed setup, we propose to im-prove the pseudo-labels using a pseudo-label enhancer that was trained using the available ground-truth pixel-level la-bels. Experiments on Pascal-5i and COCO-20i demonstrate significant performance gains in a variety of supervision settings, and in particular when little-to-no pixel-level la-bels are available. 1.

Introduction
Few-shot learning [22, 23, 77] is the problem of learn-ing to perform a prediction task using only a small number of examples (i.e. supports) of the completed task, typically in the range of 1-10 examples. This problem setting is ap-pealing for applications where large amounts of annotated data are expensive or impractical to collect. In computer vi-sion, few-shot learning has been actively studied for image classification [24, 36, 46, 66, 70, 85] and semantic segmen-tation [17, 58, 61, 88, 89].
In this work, we focus on the combined task of few-shot classification and segmentation (FS-CS) [33], which aims to jointly predict (i) the presence of each support class, i.e., multi-label classification, and (ii)
*Work done during an internship at FAIR.
Figure 1. Weakly-supervised few-shot classification and seg-mentation.
In this paper, we explore training few-shot models using little-to-no ground-truth segmentation masks. its pixel-level semantic segmentation.
Few-shot learners are typically trained either by meta-learning [24, 56, 63, 71] or by transfer learning with fine-tuning [11, 12, 16]. Both these paradigms commonly as-sume the availability of a large-scale and fully-annotated training dataset. For FS-CS, we need Ground-Truth (GT) segmentation masks for query and support images during training. We also need these GT masks for support im-ages during testing. A natural alternative to collecting such expensive segmentation masks would be to instead em-ploy a weaker supervision, e.g. image-level [52, 59] or box-level labels [13, 30], for learning, i.e., to adopt a weakly-supervised learning approach [5, 41, 43, 51]. Compared to conventional weakly-supervised learning however, weakly-supervised few-shot learning has rarely been explored in the literature and is significantly more challenging. This is be-cause in few-shot learning, object classes are completely disjoint between training and testing, resulting in models which are susceptible to severe over-fitting to the classes seen during training.
In this work we tackle this challenging scenario, address-ing weakly-supervised FS-CS where only image-level la-bels, i.e. class labels, are available (cf. Fig. 1). Inspired by
recent work by Caron et al. [9], which showed that pixel-level semantic attention emerges from self-supervised vi-sion transformers (ViTs) [18], we leverage attention maps from a frozen self-supervised ViT to generate pseudo-GT segmentation masks. We train an FS-CS learner on top of the frozen ViT using the generated pseudo-GT masks as pixel-level supervision. The FS-CS learner is a small trans-former network that takes features from the ViT as input and is trained to predict the pseudo-GT masks. Our com-plete model thus learns to segment using its own intermedi-ate byproduct, with no segmentation labeling cost, in a form of distillation [2, 28, 91, 94] between layers within a model.
We also explore a practical training setting where a lim-ited number of training images have both image-level and pixel-level annotations, while the remaining images have only image-level labels. For this mixed-supervised setting, we propose to train an auxiliary mask enhancer that refines attention maps into pseudo-GT masks. The enhancer is su-pervised using the small number of available GT masks and is used to generate the final pseudo-GT masks for the train-ing images that do not have GT masks.
Lastly, to effectively address this weakly-supervised FS-CS task, we propose a Classification-Segmentation Trans-former (CST) architecture. CST takes as input ViT tokens for query and support images, computes correlations be-tween them, and then predicts classification and segmen-tation outputs through separate task heads; the classifica-tion head is trained with class labels while the segmenta-tion head is trained with either GT masks or pseudo-GT masks, depending on availability. Unlike prior work us-ing ViTs that generate prediction for a single task using ei-ther the class token [18, 74] or image tokens [50, 65], CST uses each of them for each task prediction, which proves ad-vantageous for both tasks. CST achieves moderate-to-large performance gains for FS-CS on all three supervision lev-els (image-level-, mixed-level, and pixel-level), when com-pared to prior state-of-the-art methods.
Our contributions are the following: i. We introduce a powerful baseline for few-shot classi-fication and segmentation (FS-CS) using only image-level supervision, which leverages localized semantic information encoded in self-supervised ViTs [9]. ii. We propose a learning setup for FS-CS with mixed su-pervision, and present an auxiliary mask enhancer to improve performance compared to image-level super-vision only. iii. We design Classification-Segmentation Transformer, and a multi-task learning objective of classification and segmentation, which is beneficial for both tasks, and al-lows for flexibly tuning the degree of supervision. 2.