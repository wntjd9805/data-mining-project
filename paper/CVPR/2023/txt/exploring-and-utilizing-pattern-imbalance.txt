Abstract
In this paper, we identify pattern imbalance from sev-eral aspects, and further develop a new training scheme to avert pattern preference as well as spurious correlation. In contrast to prior methods which are mostly concerned with category or domain granularity, ignoring the potential finer structure that existed in datasets, we give a new definition of seed category as an appropriate optimization unit to dis-tinguish different patterns in the same category or domain.
Extensive experiments on domain generalization datasets of diverse scales demonstrate the effectiveness of the proposed method. 1.

Introduction
Over the past decade, the rise of deep neural networks (DNNs) has promoted the rapid development of various ar-tificial intelligence communities [13, 20, 22]. Despite the remarkable success, DNNs tend to take shortcuts to learn spurious features [24, 27]. The causal correlation between these spurious features and ground truth only exists in the training set, which hinders the generalization of DNN mod-els. This phenomenon is also known as domain shift. More-over, due to the incomplete distribution of training data, the learned model may have a preference for gender, race, and skin color, which will lead to serious ethical problems.
To tackle these problems, various methods have been proposed to discuss the failure modes of out-of-distribution (OOD) generalization [18, 30, 32, 43]. Some researchers fo-cus on encouraging the model to learn domain invariant fea-tures. Ganin et al. [9] simultaneously optimize a standard classifier and a domain classifier through adversarial train-ing, where the features extracted by DNN can be used for original classification but failed on domain recognition to inhibit domain characteristics learning. Arjovsky et al. [1] restrict the learned representations to be classified by sim-ilar optimal linear classifiers in different domains. Other researchers start by avoiding spurious features. Zhang et
*Corresponding author. al. [42] argue that there exist sub-networks with preferable domain generalization ability in the model and represent the sub-network through a learnable mask. Nam et al. [28] as-sume that the spurious features are generally embodied in the texture or style of the image. They design SagNet to decouple the content and style of the image, impelling the feature extractor to pay more attention to the content infor-mation. Most of the above methods manually design spe-cific model structures to handle domain generalization.
Instead of designing specific networks, we are more concerned about solving domain generalization by ex-ploring the character of the dataset. In particular, sup-pose a simple handwritten digit recognition scenario, where a large amount of digit 0 possesses the red background and digit 1 possesses the green background. The dataset with only the above two patterns cannot be effectively learned, since the model has no idea whether the task is to classify the digits or the background color. Therefore, in a given learnable data set, there must exist a minority of digit 0 with green background and digit 1 with red background.
These samples play a significant role in establishing the true causal relationship between images and labels but have not been paid enough attention. We call pattern imbalance the phenomenon that different patterns in the same class ap-pear imbalanced, thus leading to model learning preference.
Based on the above observations, we attribute the domain generalization problem to the mining of hard or minority patterns under imbalanced patterns. First of all, we iden-tify the pattern imbalance in the dataset from several per-spectives. We note that even though a model has achieved favorable performance on average, Achilles’heel still exists on some weak patterns. To alleviate the influence caused by imbalance patterns, we pay more attention to these samples of minority patterns and propose a training scheme based on dynamic distribution. To this end, we define a new concept, seed category, that is, the inherent pattern to distinguish, to promote model training by paying full attention to various patterns in the data set. Specifically, for samples of the same class, the seed category is divided based on the distance of the samples in the embedding space as a more fine-grained weight allocation unit than previous methods [19, 32, 39].
In this paper, this dynamic and fine-grained training scheme enables our method to obtain excellent domain generaliza-tion performance.
We argue that it is effective to apply more detailed weight allocation on out-of-distribution generalization tasks, that is, the patterns that are crucial but laborious to be learned by the model deserve special treatments, which is the most significant difference between our method and the previ-ous methods. Prior methods, e.g., GroupDRO [32], mini-mize the worst-case loss over domains to treat different do-mains differently, and the performance will be limited by the coarse granularity of grouped distribution. On the con-trary, the flexibility of our method is revealed in two as-pects, that is, the weight allocation unit is more detailed and the seed category can be dynamically adjusted during the training process. Our contributions can be summarized as follows:
• We identify pattern imbalance generally existed in classification tasks and give a new definition of seed category, that is, the inherent pattern to recognize.
• We further develop a dynamic weight distribution training strategy based on seed category to facilitate out-of-distribution performance.
• Extensive experiments on several domain generaliza-tion datasets well demonstrate the effectiveness of the proposed method. 2.