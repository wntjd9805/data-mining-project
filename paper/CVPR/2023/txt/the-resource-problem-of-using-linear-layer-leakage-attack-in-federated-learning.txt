Abstract
Secure aggregation promises a heightened level of pri-vacy in federated learning, maintaining that a server only has access to a decrypted aggregate update. Within this setting, linear layer leakage methods are the only data re-construction attacks able to scale and achieve a high leak-age rate regardless of the number of clients or batch size.
This is done through increasing the size of an injected fully-connected (FC) layer. However, this results in a resource overhead which grows larger with an increasing number of clients. We show that this resource overhead is caused by an incorrect perspective in all prior work that treats an at-tack on an aggregate update in the same way as an individ-ual update with a larger batch size. Instead, by attacking the update from the perspective that aggregation is combin-ing multiple individual updates, this allows the application of sparsity to alleviate resource overhead. We show that the use of sparsity can decrease the model size overhead by over 327× and the computation time by 3.34× compared to
SOTA while maintaining equivalent total leakage rate, 77% even with 1000 clients in aggregation. 1.

Introduction
Federated learning (FL) [17] has been hailed as a privacy-preserving method of training. FL involves mul-tiple clients which train their model on their private data before sending the update back to a server. The promise is that FL will keep the client data private from all (server as well as other clients) as the update cannot be used to infer information about client training data.
However, many recent works have shown that client gra-dients are not truly privacy preserving. Specifically, data reconstruction attacks [3, 8, 9, 12, 18, 26, 28, 34] use a model update to directly recover the private training data. These methods typically consist of gradient inversion [9, 28, 34] and analytic attacks [3, 8, 12, 14, 18, 26]. Gradient inver-sion attacks observe an honest client gradient and iteratively optimizes randomly initialized dummy data such that the resulting gradient becomes closer to the honest gradient.
The goal is that dummy data that creates a similar gradi-ent will be close to the ground truth data. These methods have shown success on smaller batch sizes, but fail when batch sizes become too large. Prior work has shown that reconstruction on ImageNet is possible up to a batch size of 48, although the reconstruction quality is low [28]. An-alytic attacks cover a wide range of methods. Primarily, they use a malicious modification of model architecture and parameters [18, 26], linear layer leakage methods [3, 8], ob-serve updates over multiple training rounds [14], or treat images as a blind-source separation problem [12]. How-ever, most of these approaches fail when secure aggrega-tion is applied [4, 6, 7, 23, 24]. Particularly, when a server can only access the updates aggregated across hundreds or thousands of training images, the reconstruction process be-comes very challenging. Gradient inversion attacks are im-possible without additional model modifications or training rounds. This is where linear layer leakage attacks [3,8] have shown their superiority.
This sub-class of analytic data reconstruction attacks is based on the server crafting maliciously modified models that it sends to the clients. In particular, the server uses a fully-connected (FC) layer to leak the input images. Com-pared to any other attack, linear layer leakage attacks are the only methods able to scale to an increasing number of clients or batch size, maintaining a high total leakage rate.
This is done by continually increasing the size of an FC layer used to leak the images. For example, with 100 clients and a batch size of 64 on CIFAR-100, an attacker can leak 77.2% of all images in a single training round using an in-serted FC layer of size 25,600.
In this case, the number of units in the layer is 4× the number of total images, and maintaining this ratio when the number of clients or batch size increases allows the attack to still achieve roughly the same leakage rate. Despite the potential of linear layer leak-age, however, an analysis of the limits of its scalability in FL has been missing till date.
In this work, we dive into this question and explore the potential of scaling linear layer leakage attacks to secure aggregation. We particularly highlight the challenges in resource overhead corresponding to memory, communica-tion, and computation, which are the primary restrictions of cross-device FL. We discover that while SOTA attacks can maintain a high leakage rate regardless of aggregation size, the overhead is massive. With 1,000 clients and a batch size of 64, maintaining the same leakage rate as before would re-sult in the added layers increasing the model size by 6GB.
There would also be an added computation time of 21.85s for computing the update for a single batch (size 64), a 10× overhead compared to a baseline ResNet-50. This is a massive problem for resource-constrained FL where clients have limited communication or computation budgets.
However, this problem arises from an incorrect perspec-tive from prior work where they treat the attack on an aggre-gate update the same as an individual client update. Specifi-cally, we argue that it is critical to treat an aggregation attack not as an attack on a single large update, but as individual client updates combined together. In the context of linear layer leakage, this is the difference between separating the scaling of the attack between batch size and the number of clients or scaling to all images together.
Following this, we use the attack MANDRAKE [32] with sparsity in the added parameters between the convolutional output and the FC layer to highlight the difference in model size compared to prior SOTA. The addition can decrease the added model size by over 327× and decrease computation time by 3.34× compared to SOTA attacks while achiev-ing the same total leakage rate. For a batch size of 64 and 1000 clients participating in training, the sparse MAN-DRAKE module adds only a little over 18MB to the model while leaking 77.8% of the total data in a single training round (comparable to other SOTA attacks).
We discuss other fundamental challenges for linear layer leakage including the resource overhead of leaking larger input data sizes. We also discuss that sparsity in the client update fundamentally cannot be maintained through secure aggregation and the client still accrues a communication overhead when sending the update back to the server. All other aspects of resource overhead such as communication cost when the server sends the model to the client, computa-tion time, and memory size, are decreased through sparsity.
Our contributions are as follows:
• We show the importance of sparsity in maintaining a small model size overhead when scaling to a large number of clients and the incorrect perspective prior work has had when treating the aggregate update as a single large update. By using sparsity with MAN-DRAKE and attacking 1000 clients with a batch size of 64, the added model size is only 18.33 MB. Compared to SOTA attacks, this is a decrease in over 327× in size and also results in a decreased computation time by 3.3× while maintaining the same leakage rate.
• We show the fundamental challenge of linear layer leakage attacks for scaling attacks towards leaking larger input image sizes and the resulting resource overhead added.
• We show the problem of maintaining sparsity in se-cure aggregation when the encryption mask is ap-plied, which adds to the communication overhead when clients send updates back to the server. 2.