Abstract
Current semantic segmentation models have achieved great success under the independent and identically dis-tributed (i.i.d.) condition. However, in real-world appli-cations, test data might come from a different domain than training data. Therefore, it is important to improve model robustness against domain differences. This work stud-ies semantic segmentation under the domain generalization setting, where a model is trained only on the source domain and tested on the unseen target domain. Existing works show that Vision Transformers are more robust than CNNs and show that this is related to the visual grouping property of self-attention. In this work, we propose a novel hierarchi-cal grouping transformer (HGFormer) to explicitly group pixels to form part-level masks and then whole-level masks.
The masks at different scales aim to segment out both parts and a whole of classes. HGFormer combines mask clas-siﬁcation results at both scales for class label prediction.
We assemble multiple interesting cross-domain settings by using seven public semantic segmentation datasets. Exper-iments show that HGFormer yields more robust semantic segmentation results than per-pixel classiﬁcation methods and ﬂat-grouping transformers, and outperforms previous methods signiﬁcantly. Code will be available at https:
//github.com/dingjiansw101/HGFormer.
Figure 1. Semantic segmentation can be considered as partition-ing an image into classiﬁcation units (regions), then classifying the units. The units can range from pixels to large masks. Intuitively, mask classiﬁcation is more robust than per-pixel classiﬁcation, as masks allow to aggregate features over large image regions of the same class to predict a ‘global’ label. Despite this promise, the process of grouping pixels into whole-level masks directly from pixels is very challenging under the distribution shift (e.g., Gaus-sian Noise). In order to tackle this problem, we present a hierar-chical grouping paradigm to group pixels to part-level masks ﬁrst and then to group part-level masks to whole-level masks to get re-liable masks. Then we combine both part-level and whole-level mask classiﬁcation for robust semantic segmentation, given that the masks at the two levels capture complementary information. 1.

Introduction
Research in semantic image segmentation has leaped for-ward in the past years due to the development of deep neural network. However, most of these models assume that the training and testing data follow the same distribution. In the real-world, we frequently encounter testing data that is out of distribution. The generalization ability of models under distribution shift is crucial for applications related to safety,
*Corresponding author such as self-driving. In domain generalization setting, mod-els are trained only on source domains and tested on tar-get domains, where the distributions of source domains and target domains are different. Unlike the domain adapta-tion [25, 56], target data is not accessible / needed during training, making the task challenging but practically useful.
Recently, Vision Transformers have been shown to be signiﬁcantly more robust than traditional CNNs in the out-of-distribution generalization [21, 25, 42, 58, 60, 70]. Some works interpret self-attention as a kind of visual group-1
ing [7, 38], and believe that it is related to robustness [70].
However, these works mainly focus on classiﬁcation. Al-though FAN [70] and Segformer [60] have been evaluated on segmentation, they do not explicitly introduce visual grouping in their networks. Since grouping is naturally aligned with the task of semantic segmentation, we would like to ask the question: can we improve the robustness of semantic segmentation by introducing an explicit grouping mechanism into semantic segmentation networks?
Most deep learning based segmentation models directly conduct per-pixel classiﬁcation without the process of grouping. Some recent segmentation models introduced ﬂat grouping [15,67] into the segmentation decoder, where pix-els are grouped into a set of binary masks directly and clas-siﬁcation on masks is used to make label prediction. By us-ing a one-to-one matching similar to DETR [5], the loss be-tween predicted masks and ground truth masks is computed.
Therefore the network is trained to directly predict whole-level masks, as shown in Fig. 1. Intuitively, if whole-level masks are accurate, mask classiﬁcation will be more robust than per-pixel classiﬁcation due to its information aggrega-tion over regions of the same class. But we ﬁnd that using the ﬂat grouping to generate whole-level masks is suscepti-ble to errors, especially under cross-domain settings. This is shown by the example in Fig. 1 - bottom.
Different from the ﬂat grouping works [14, 67], we pro-pose a hierarchical grouping in the segmentation decoder, where the pixels are ﬁrst grouped into part-level masks, and then grouped into whole-level masks. Actually, the hierar-chical grouping is inspired by the pioneer works of image segmentation [2, 13, 49] and is further supported by strong psychological evidence that humans parse scenes into part-whole hierarchies [24]. We ﬁnd that grouping pixels to part-level masks and then to whole-level masks is more robust than grouping pixels directly to whole-level masks. Part-level masks and whole-level masks segment images at dif-ferent scales such as parts and a whole of classes. There-fore, part-level and whole-level masks are complementary, and combining mask classiﬁcation results at those different scales improves the overall robustness.
To instantiate a hierarchical grouping idea, we propose a hierarchical grouping transformer (HGFormer) in the de-coder of a segmentation model. The diagram is shown in
Fig. 2. We ﬁrst send the feature maps to the part-level grouping module.
In the part-level grouping module, the initialization of cluster centers is down sampled from fea-ture maps. Then we compute the pixel-center similarities and assign pixels to cluster centers according to the simi-larities. To get the part-level masks, we only compute the similarities between each pixel feature and its nearby cen-ter features. We then aggregate information of the part-level masks and generate whole-level masks by using cross-attention, similar to how previous methods aggregate pixels information to generate whole-level masks [14,67]. Finally, we classify masks at different levels, and average the se-mantic segmentation results of all the scales.
We evaluate the method under multiple settings, which are assembled by using seven challenging semantic seg-mentation datasets.
In each of the setting, we train the methods on one domain and test them on other domains.
Extensive experiments show that our model is signiﬁcantly better than previous per-pixel classiﬁcation based, and whole-level mask based segmentation models for out-of-distribution generalization.
To summarize, our contributions are: 1) We present a hi-erarchical grouping paradigm for robust semantic segmen-tation; 2) based on the hierarchical grouping paradigm, we propose a hierarchical grouping transformer (HGFormer), where the pixels are ﬁrst grouped into part-level masks, and then grouped into whole-level masks. Final semantic segmentation results are obtained by making classiﬁcations on all masks; 3) HGFormer outperforms previous seman-tic segmentation models on domain generalized semantic segmentation across various experimental settings. We also give detailed analyses of the robustness of grouping-based methods under distribution shift. 2.