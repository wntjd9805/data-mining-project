Abstract
How to learn discriminative video representation from unlabeled videos is challenging but crucial for video anal-ysis. The latest attempts seek to learn a representation model by predicting the appearance contents in the masked regions. However, simply masking and recovering ap-pearance contents may not be sufficient to model tempo-ral clues as the appearance contents can be easily recon-structed from a single frame. To overcome this limitation, we present Masked Motion Encoding (MME), a new pre-training paradigm that reconstructs both appearance and motion information to explore temporal clues.
In MME, we focus on addressing two critical challenges to improve the representation performance: 1) how to well represent the possible long-term motion across multiple frames; and 2) how to obtain fine-grained temporal clues from sparsely sampled videos. Motivated by the fact that human is able to recognize an action by tracking objects’ position changes and shape changes, we propose to reconstruct a motion trajectory that represents these two kinds of change in the masked regions. Besides, given the sparse video input, we enforce the model to reconstruct dense motion trajectories in both spatial and temporal dimensions. Pre-trained with our MME paradigm, the model is able to anticipate long-term and fine-grained motion details. Code is available at https://github.com/XinyuSun/MME. 1.

Introduction
Video representation learning plays a critical role in video analysis like action recognition [15,32,79], action lo-calization [12,81], video retrieval [4,82], videoQA [40], etc.
Learning video representation is very difficult for two rea-sons. Firstly, it is extremely difficult and labor-intensive to annotate videos, and thus relying on annotated data to learn
*Equal contribution. Email: {csxinyusun, phchencs}@gmail.com
†Corresponding author. Email: mingkuitan@scut.edu.cn video representations is not scalable. Also, the complex spatial-temporal contents with a large data volume are diffi-cult to be represented simultaneously. How to perform self-supervised videos representation learning only using unla-beled videos has been a prominent research topic [7,13,49].
Taking advantage of spatial-temporal modeling using a flexible attention mechanism, vision transformers [3, 8, 25, 26, 53] have shown their superiority in representing video.
Prior works [5, 37, 84] have successfully introduced the mask-and-predict scheme in NLP [9, 23] to pre-train an im-age transformer. These methods vary in different recon-struction objectives, including raw RGB pixels [37], hand-crafted local patterns [75], and VQ-VAE embedding [5], all above are static appearance information in images. Based on previous successes, some researchers [64,72,75] attempt to extend this scheme to the video domain, where they mask 3D video regions and reconstruct appearance information.
However, these methods suffers from two limitations. First, as the appearance information can be well reconstructed in a single image with an extremely high masking ratio (85% in MAE [37]), it is also feasible to be reconstructed in the tube-masked video frame-by-frame and neglect to learn im-portant temporal clues. This can be proved by our ablation study (cf. Section 4.2.1). Second, existing works [64, 75] often sample frames sparsely with a fixed stride, and then mask some regions in these sampled frames. The recon-struction objectives only contain information in the sparsely sampled frames, and thus are hard to provide supervision signals for learning fine-grained motion details, which is critical to distinguish different actions [3, 8].
In this paper, we aim to design a new mask-and-predict paradigm to tackle these two issues. Fig. 1(a) shows two key factors to model an action, i.e., position change and shape change. By observing the position change of the person, we realize he is jumping in the air, and by observing the shape change that his head falls back and then tucks to his chest, we are aware that he is adjusting his posture to cross the bar.
We believe that anticipating these changes helps the model better understand an action.
Figure 1. Illustration of motion trajectory reconstruction for Masked Motion Encoding. (a) Position change and shape change over time are two key factors to recognize an action, we leverage them to represent the motion trajectory. (b) Compared with the current appearance reconstruction task, our motion trajectory reconstruction takes into account both appearance and motion information.
Based on this observation, instead of predicting the ap-pearance contents, we propose to predict motion trajectory, which represents impending position and shape changes, for the mask-and-predict task. Specifically, we use a dense grid to sample points as different object parts, and then track these points using optical flow in adjacent frames to gener-ate trajectories, as shown in Fig. 1(b). The motion trajectory contains information in two aspects: the position features that describe relative movement; and the shape features that describe shape changes of the tracked object along the tra-jectory. To predict this motion trajectory, the model has to learn to reason the semantics of masked objects based on the visible patches, and then learn the correlation of objects among different frames and try to estimate their accurate motions. We name the proposed mask-and-predict task as
Masked Motion Encoding (MME).
Moreover, to help the model learn fine-grained motion details, we further propose to interpolate the motion trajec-tory. Taking sparsely sampled video as input, the model is asked to reconstruct spatially and temporally dense motion trajectories. This is inspired by the video frame interpo-lation task [77] where a deep model can reconstruct dense video at the pixel level from sparse video input. Differ-ent from it, we aim to reconstruct the fine-grained motion details of moving objects, which has higher-level motion information and is helpful for understanding actions. Our main contributions are as follows:
• Existing mask-and-predict task based on appearance reconstruction is hard to learn important temporal clues, which is critical for representing video content.
Our Masked Motion Encoding (MME) paradigm over-comes this limitation by asking the model to recon-struct motion trajectory.
• Our motion interpolation scheme takes a sparsely sam-pled video as input and then predicts dense motion tra-jectory in both spatial and temporal dimensions. This scheme endows the model to capture long-term and fine-grained motion clues from sparse video input.
Extensive experimental results on multiple standard video recognition benchmarks prove that the representations learned from the proposed mask-and-predict task achieve state-of-the-art performance on downstream action recog-nition tasks. Specifically, pre-trained on Kinetics-400 [10], our MME brings the gain of 2.3% on Something-Something
V2 [34], 0.9% on Kinetics-400, 0.4% on UCF101 [59], and 4.7% on HMDB51 [44]. 2.