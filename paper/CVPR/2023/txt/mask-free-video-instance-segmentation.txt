Abstract 1.

Introduction
The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and in-creasingly data-hungry transformer-based models. How-ever, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation require-ment. We propose MaskFreeVIS, achieving highly compet-itive VIS performance, while only using bounding box an-notations for the object state. We leverage the rich tempo-ral mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor se-lection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. We val-idate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our method by drastically nar-rowing the gap between fully and weakly-supervised VIS performance. Our code and trained models are available at http://vis.xyz/pub/maskfreevis.
Video Instance Segmentation (VIS) requires jointly de-tecting, tracking and segmenting all objects in a video from a given set of categories. To perform this challenging task, state-of-the-art VIS models are trained with complete video annotations from VIS datasets [39, 61, 64]. However, video annotation is costly, in particular regarding object mask labels. Even coarse polygon-based mask annotation is multiple times slower than annotating video bounding boxes [7]. Expensive mask annotation makes existing VIS benchmarks difficult to scale, limiting the number of object categories covered. This is particularly a problem for the re-cent transformer-based VIS models [6,17,57], which tend to be exceptionally data-hungry. We therefore revisit the need for complete mask annotation by studying the problem of weakly supervised VIS under the mask-free setting.
While there exist box-supervised instance segmenta-tion models [13, 23, 27, 50], they are designed for images.
These weakly-supervised single-image methods do not uti-lize temporal cues when learning mask prediction, leading to lower accuracy when directly applied to videos. As a source for weakly supervised learning, videos contain much richer information about the scene. In particular, videos ad-here to the temporal mask consistency constraint, where the regions corresponding to the same underlying object across different frames should have the same mask label. In this work, we set out to leverage this important constraint for mask-free learning of VIS.
Table 1. Mask annotation requirement for state-of-the-art VIS methods. Results are reported using ResNet-50 as backbone on the
YTVIS 2019 [61] benchmark. Video Mask: using YTVIS video mask labels. Image Mask: using COCO [30] image mask labels for image-based pretraining. Pseudo Video: using Pseudo Videos from COCO images for joint training [57]. MaskFreeVIS achieves 91.5% (42.5 vs. 46.4) of its fully-supervised baseline performance (Mask2Former) without using any masks during training.
Method
SeqFormer [57]
VMT [17]
Mask2Former [6]
MaskFreeVIS (ours)
Mask2Former [6]
MaskFreeVIS (ours)
Video
Mask
Image
Mask
Pseudo
Video
✓
✓
✓
✗
✓
✗
✓
✓
✓
✓
✓
✗
✓
✓
✓
✓
✗
✗
AP 47.4 47.9 47.8 46.6 46.4 42.5 supervised video instance segmentation. It further demon-strates that expensive video masks, or even image masks, is not necessary for training high-performing VIS models.
Our contributions are summarized as follows: (i) To uti-lize temporal information, we develop a new parameter-free Temporal KNN-patch Loss, which leverages temporal masks consistency using unsupervised one-to-k patch cor-respondence. We extensively analyze the TK-Loss through (ii) Based on the TK-Loss, we de-ablative experiments. velop the MaskFreeVIS method, enabling training existing state-of-the-art VIS models without any mask annotation. (iii) To the best of our knowledge, MaskFreeVIS is the first mask-free VIS method attaining high-performing segmen-tation results. We provide qualitative results in Figure 1.
As in Table 1, when integrated into the Mask2Former [6] baseline with ResNet-50, our MaskFreeVIS achieves 42.5% mask AP on the challenging YTVIS 2019 benchmark while using no video or image mask annotations. Our approach further scales to larger backbones, achieving 55.3% mask
AP on Swin-L backbone with no video mask annotations.
We hope our approach will facilitate achieving label-efficient video instance segmentation, enabling building even larger-scale VIS benchmarks with diverse categories by lifting the mask annotation restriction. 2.