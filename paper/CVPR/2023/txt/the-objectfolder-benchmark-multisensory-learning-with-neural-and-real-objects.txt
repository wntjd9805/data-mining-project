Abstract
We introduce the OBJECTFOLDER BENCHMARK, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruc-tion, and manipulation with sight, sound, and touch. We also introduce the OBJECTFOLDER REAL dataset, in-cluding the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. We conduct system-atic benchmarking on both the 1,000 multisensory neural objects from OBJECTFOLDER, and the real multisensory data from OBJECTFOLDER REAL. Our results demon-strate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for differ-ent object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and en-able new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu 1.

Introduction
Computer vision systems today excel at recognizing ob-jects in 2D images thanks to many image datasets [3,17,35, 40]. There is also a growing interest in modeling an object’s shape and appearance in 3D, with various benchmarks and tasks introduced [8, 28, 44, 45, 54, 61]. Despite the exciting progress, these studies primarily focus on the visual recog-nition of objects. At the same time, our everyday activities often involve multiple sensory modalities. Objects exist not just as visual entities, but they also make sounds and can be touched during interactions. The different sensory modes of an object all share the same underlying object intrinsics— its 3D shape, material property, and texture. Modeling the
*indicates equal contribution.
†Yiming is affiliated with Shanghai Jiao Tong University. The work was done when he was visiting Stanford University as a summer intern. complete multisensory profile of objects is of great impor-tance for many applications beyond computer vision, such as robotics, graphics, and virtual and augmented reality.
Some recent attempts have been made to combine mul-tiple sensory modalities to complement vision for various tasks [2,6,39,58,59,63,70,73]. These tasks are often studied in tailored settings and evaluated on different datasets. As an attempt to develop assets generally applicable to diverse tasks, the OBJECTFOLDER dataset [23, 26] has been intro-duced and includes 1,000 neural objects with their visual, acoustic, and tactile properties. OBJECTFOLDER however has two fundamental limitations. First, no real objects are included; all multisensory data are obtained through simula-tion with no simulation-to-real (sim2real) calibration. Sec-ond, only a few tasks were presented to demonstrate the usefulness of the dataset and to establish the possibility of conducting sim2real transfer with the neural objects.
Consequently, we need a multisensory dataset of real ob-jects and a robust benchmark suite for multisensory object-centric learning. To this end, we present the OBJECT-FOLDER REAL dataset and the OBJECTFOLDER BENCH-MARK suite, as shown in Fig. 1.
The OBJECTFOLDER REAL dataset contains multisen-sory data collected from 100 real-world household objects.
We design a data collection pipeline for each modality: for vision, we scan the 3D meshes of objects in a dark room and record HD videos of each object rotating in a lightbox; for audio, we build a professional anechoic chamber with a tailored object platform and then collect impact sounds by striking the objects at different surface locations with an impact hammer; for touch, we equip a Franka Emika Panda robot arm with a GelSight robotic finger [18,71] and collect tactile readings at the exact surface locations where impact sounds are collected.
The OBJECTFOLDER BENCHMARK suite consists of 10 benchmark tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and ma-nipulation. The three recognition tasks are cross-sensory retrieval, contact localization, and material classification; the three reconstruction tasks are 3D shape reconstruc-1
Figure 1. The OBJECTFOLDER BENCHMARK suite consists of 10 benchmark tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation. Complementing the 1,000 multisensory neural objects from OBJECT-FOLDER [26], we also introduce OBJECTFOLDER REAL, which contains real multisensory data collected from 100 real-world objects, including their 3D meshes, video recordings, impact sounds, and tactile readings. tion, sound generation of dynamic objects, and visuo-tactile cross-generation; and the four manipulation tasks are grasp stability prediction, contact refinement, surface traversal, and dynamic pushing. We standardize the task setting for each task and present baseline approaches and results.
Experiments on both neural and real objects demonstrate the distinct value of sight, sound, and touch in different tasks. For recognition, vision and audio tend to be more re-liable compared to touch, where the contained information is too local to recognize. For reconstruction, we observe that fusing multiple sensory modalities achieve the best re-sults, and it is possible to hallucinate one modality from the other. This agrees with the notion of degeneracy in cog-nitive studies [60], which creates redundancy such that our sensory system functions even with the loss of one compo-nent. For manipulation, vision usually provides global po-sitional information of the objects and the robot, but often suffers from occlusion. Touch, often as a good complement to vision, is especially useful to capture the accurate local geometry of the contact point.
We will open-source all code and data for OBJECT-FOLDER REAL and OBJECTFOLDER BENCHMARK to fa-cilitate research in multisensory object-centric learning. 2.