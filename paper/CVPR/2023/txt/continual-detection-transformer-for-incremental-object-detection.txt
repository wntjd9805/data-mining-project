Abstract
Incremental object detection (IOD) aims to train an ob-ject detector in phases, each with annotations for new ob-ject categories. As other incremental settings, IOD is sub-ject to catastrophic forgetting, which is often addressed by techniques such as knowledge distillation (KD) and exem-plar replay (ER). However, KD and ER do not work well if applied directly to state-of-the-art transformer-based ob-ject detectors such as Deformable DETR [59] and UP-DETR [9]. In this paper, we solve these issues by proposing a ContinuaL DEtection TRansformer (CL-DETR), a new method for transformer-based IOD which enables effective usage of KD and ER in this context. First, we introduce a
Detector Knowledge Distillation (DKD) loss, focusing on the most informative and reliable predictions from old ver-sions of the model, ignoring redundant background predic-tions, and ensuring compatibility with the available ground-truth labels. We also improve ER by proposing a calibration strategy to preserve the label distribution of the training set, therefore better matching training and testing statistics. We conduct extensive experiments on COCO 2017 and demon-strate that CL-DETR achieves state-of-the-art results in the
IOD setting.1 1.

Introduction
Humans inherently learn in an incremental manner, ac-quiring new concepts over time without forgetting previ-ous ones. In contrast, machine learning suffers from catas-trophic forgetting [21, 35, 36], where learning from non-i.i.d. data can override knowledge acquired previously. Un-surprisingly, forgetting also affects object detection [2, 12, 20, 37, 44, 50, 54]. In this context, the problem was formal-ized by Shmelkov et al. [44], who defined an incremental object detection (IOD) protocol, where the training samples for different object categories are observed in phases, re-Figure 1. The final Average Precision (AP, %) of two-phase incre-mental object detection on COCO 2017. We observe 70 and 10 categories in the first and second phases, respectively. The base-line is Deformable DETR [59]. “Upper bound” shows the results of joint training with all previous data accessible in each phase. stricting the ability of the trainer to access past data.
Popular methods to address forgetting in tasks other than detection include Knowledge Distillation (KD) and Exem-plar Replay (ER). KD [11,16,17,26,57] uses regularization in an attempt to preserve previous knowledge when train-ing the model on new data. The key idea is to encourage the new model’s logits or feature maps to be close to those of the old model. ER methods [5, 29, 32, 33, 41, 52] work instead by memorising some of the past training data (the exemplars), replaying them in the following phases to “re-member” the old object categories.
Recent state-of-the-art results in object detection have been achieved by a family of transformer-based architec-tures that include DETR [4], Deformable DETR [59] and
In this paper, we show that KD and ER
UP-DETR [9]. do not work well if applied directly to these models. For instance, in Fig. 1 we show that applying KD and ER to
Deformable DETR leads to much worse results compared to training with all data accessible in each phase (i.e., the standard non-incremental setting). 1Code: https://lyy.mpi-inf.mpg.de/CL-DETR/
We identify two main issues that cause this drop in per-formance. First, transformer-based detectors work by test-ing a large number of object hypotheses in parallel. Because the number of hypotheses is much larger than the typical number of objects in an image, most of them are negative, resulting in an unbalanced KD loss. Furthermore, because both old and new object categories can co-exist in any given training image, the KD loss and regular training objective can provide contradictory evidence. Second, ER methods for image classification try to sample the same number of exemplars for each category. In IOD, this is not a good strat-egy because the true object category distribution is typically highly skewed. Balanced sampling causes a mismatch be-tween the training and testing data statistics.
In this paper, we solve these issues by proposing Con-tinuaL DEtection TRansformer (CL-DETR), a new method for transformer-based IOD which enables effective usage of
KD and ER in this context. CL-DETR introduces the con-cept of Detector Knowledge Distillation (DKD), selecting the most confident object predictions from the old model, merging them with the ground-truth labels for the new cate-gories while resolving conflicts, and applying standard joint bipartite matching between the merged labels and the cur-rent model’s predictions for training. This approach sub-sumes the KD loss, applying it only for foreground predic-tions correctly matched to the appropriate model’s hypothe-ses. CL-DETR also improves ER by introducing a new cal-ibration strategy to preserve the distribution of object cat-egories observed in the training data. This is obtained by carefully engineering the set of exemplars remembered to match the desired distribution. Furthermore, each phase consists of a main training step followed by a smaller one focusing on better calibrating the model.
We also propose a more realistic variant of the IOD
In previous works [12, 44], in each benchmark protocol. phase, the incremental detector is allowed to observe all im-ages that contain a certain type of object. Because images often contain a mix of object classes, both old and new, this means that the same images can be observed in different training phases. This is incompatible with the standard def-inition of incremental learning [16, 33, 41] where, with the exception of the examples deliberately stored in the exem-plar memory, the images observed in different phases do not repeat. We redefine the IOD protocol to avoid this issue.
We demonstrate CL-DETR by applying it to dif-ferent transformer-based detectors including Deformable
DETR [59] and UP-DETR [9]. As shown in Fig. 1, our results on COCO 2017 show that CL-DETR leads to signif-icant improvements compared to the baseline, boosting AP by 4.2 percentage points compared to a direct application of KD and ER to the underlying detector model. We further study and justify our modelling choices via ablations.
To summarise, we make four contributions: (1) The
DKD loss that improves KD for knowledge distillation by resolving conflicts between distilled knowledge and new ev-idence and by ignoring redundant background detections; (2) A calibration strategy for ER to match the stored ex-emplars to the training set distribution; (3) A revised IOD benchmark protocol that avoids observing the same images in different training phases; (4) Extensive experiments on
COCO 2017, including state-of-the-art results, an in-depth ablation study, and further visualizations. 2.