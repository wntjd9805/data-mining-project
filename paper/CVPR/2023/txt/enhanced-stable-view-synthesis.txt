Abstract
We introduce an approach to enhance the novel view syn-thesis from images taken from a freely moving camera. The introduced approach focuses on outdoor scenes where re-covering accurate geometric scaffold and camera pose is challenging, leading to inferior results using the state-of-the-art stable view synthesis (SVS) method. SVS and related methods fail for outdoor scenes primarily due to (i) over-relying on the multiview stereo (MVS) for geometric scaf-fold recovery and (ii) assuming COLMAP computed camera poses as the best possible estimates, despite it being well-studied that MVS 3D reconstruction accuracy is limited to scene disparity and camera-pose accuracy is sensitive to key-point correspondence selection. This work proposes a principled way to enhance novel view synthesis solutions drawing inspiration from the basics of multiple view geome-try. By leveraging the complementary behavior of MVS and monocular depth, we arrive at a better scene depth per view for nearby and far points, respectively. Moreover, our ap-proach jointly refines camera poses with image-based ren-dering via multiple rotation averaging graph optimization.
The recovered scene depth and the camera-pose help better view-dependent on-surface feature aggregation of the entire scene. Extensive evaluation of our approach on the popu-lar benchmark dataset, such as Tanks and Temples, shows substantial improvement in view synthesis results compared to the prior art. For instance, our method shows 1.5 dB of
PSNR improvement on the Tank and Temples. Similar statis-tics are observed when tested on other benchmark datasets such as FVS, Mip-NeRF 360, and DTU. 1.

Introduction
Image-based rendering, popularly re-branded as view synthesis, is a long-standing problem in computer vision and graphics [42, 44]. This problem aims to develop a
*Equal Contribution
†Corresponding Author (k.sur46@gmail.com) method that allows the user to seamlessly explore the scene via rendering of the scene from a sparse set of captured im-ages [2, 20, 42]. Furthermore, the rendered images must be as realistic as possible for a better user experience
[34–36]. Currently, among the existing approaches, Riegler and Koltun stable view synthesis (SVS) approach [36] has shown excellent results and demonstrated photorealism in novel view synthesis, without using synthetic gaming en-gine 3D data, unlike [34]. SVS is indeed stable in render-ing photorealistic images from novel viewpoints for large-scale scenes. Yet, it assumes MVS [39, 40] based dense 3D scene reconstruction and camera poses from COLMAP [39] are correct. The off-the-shelf algorithms used for 3D data acquisition and camera poses from images are, of course, popular, and to assume these algorithms could provide fa-vorable 3D reconstruction and camera poses is not an out-landish assumption. Nonetheless, taking a step forward, in this paper, we argue that although choices made by SVS for obtaining geometric scaffold and camera poses in the pursuit of improving view synthesis is commendable, we can do better by making mindful use of fundamentals from multiple-view geometry [11,12] and recent developments in deep-learning techniques for 3D computer vision problems.
To start with, we would like to emphasize that it is clearly unreasonable, especially in an outdoor setting, to assume that multi-view stereo (MVS) can provide accurate depth for all image pixels. It is natural that pixels with low dis-parity will not be reconstructed well using state-of-the-art
MVS approaches [8, 39, 40, 46]. Even a precise selection of multiple view images with reasonable distance between them (assume good baseline for stereo) may not be helpful due to loss of common scene points visibility, foreshorten-ing issue, etc. [43]. Such issues compel the practitioner to resort to post-processing steps for refining the MVS-based 3D geometry so that it can be helpful for rendering pipeline or neural-rendering network at train time.
Another critical component to view synthesis, which is often brushed aside in the literature is the accurate recovery of the camera poses. In neural view synthesis approaches such as [36], if the camera pose is wrong, the feature ag-Figure 1. Qualitative comparison. Our result compared to the popular SVS method [36] on the M60 scene of the tanks and temples dataset [18]. It is easy to observe that our approach can better render fine details in the scene. For this scene, the PSNR values for SVS [36] and our method are 19.1 and 20.8, respectively, demonstrating improved PSNR result. gregation corresponding surface points could be mislead-ing, providing inferior results. Therefore, we should have camera poses as accurate as possible. Unfortunately, de-spite the camera pose importance to this problem, discus-sion on improving camera pose is often ignored under the assumption that COLMAP [39] provide the best possible camera pose estimates for any possible scenarios. Practi-cally speaking, this is generally not the case for outdoor scenes [5, 11, 15]. What is more surprising is that some re-cent benchmark datasets put COLMAP recovered poses as the ground-truth poses [33]. Hence, we want to get this out way upfront that a robust and better camera-pose estimates are vital for better modeling view synthesis problem.
From the above predication, it is apparent that a more mindful approach is required to make view synthesis ap-proaches practically useful, automatic, and valuable for real-world application. To this end, we propose a principled and systematic approach that provides a better geometric scaffold and camera poses for reliable feature aggregation of the scene’s surface points, leading to improved novel-view synthesis results enabling superior photorealism.
In practice, we can have suitable initial camera poses from images using COLMAP. Yet, it must be refined fur-ther for improved image-feature aggregation corresponding to 3D surface points for neural rendering. It is well-studied in multiple-view geometry literature that we can improve and refine camera poses just from image key-point corre-spondences [10, 11]. Accordingly, we introduce a learning-based multiple motion averaging via graph neural network for camera pose recovery, where the pose graph is initial-ized using COLMAP poses for refinement.
Meanwhile, it is challenging to accurately recover the 3D geometry of scene points with low or nearly-zero disparity using MVS methods [12, 43]. Another bad news from the theoretical side is that a precise estimation of scene depth from a single image is unlikely1, which is a correct state-ment and hard to argue. The good news is that advance-ments in deep-learning-based monocular depth prediction have led to some outstanding results in several practical 1As several 3D scene points can have same image projection. applications [24, 31]. Thus, at least practically, it seems possible to infer reliable monocular depth estimates up to scale. Using single image depth prediction, we can reason about the depth of scene points with low disparities. So, our proposed strategy is to use confidence based multiple-view stereo 3D that favours pixels with near-to-mid disparity and allows monocular depth estimates for the rest of the pixels.
Overall depth is recovered after scaling all the scene depth appropriately using MVS reconstructed metric.
By encoding the image features via convolutional neural networks, we map the deep features to our estimated 3D ge-ometric scaffold of the scene. Since we have better camera poses and scene reconstruction, we obtain and aggregate ac-curate feature vectors corresponding to each imaging view-rays—both from the camera to the surface point and from the surface point to viewing image pixels, giving us a fea-ture tensor. We render the new image from the features ten-sor via a convolutional network and simultaneously refine the camera pose. In summary, our contributions are
• A systematic and principled approach for improved stable view synthesis enabling enhanced photorealism.
• The introduced approach exploits the complementary na-ture of MVS and monocular depth estimation to recover better 3D geometric scaffold of the scene. Meanwhile, the robust camera poses are recovered using graph neural network based multiple motion averaging.
• Our approach proposes an improved loss function to jointly optimize and refine for poses, neural image ren-dering, and scene representation showing superior results.
Our approach when tested on benchmark datasets such as
Tank and Temples [18], FVS [35], Mip-NeRF 360 [1], and
DTU [16] gives better image based rendering results with generally more than 1 dB PSNR gain (see Fig.1). 2.