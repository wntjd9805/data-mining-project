Abstract introduce
`A-la-carte Prompt Tuning (APT), a
We transformer-based scheme to tune prompts on distinct data so that they can be arbitrarily composed at inference time. The individual prompts can be trained in isolation, possibly on different devices, at different times, and on dif-ferent distributions or domains. Furthermore each prompt only contains information about the subset of data it was exposed to during training. During inference, models can be assembled based on arbitrary selections of data sources,
`A-la-carte learning which we call `a-la-carte learning. enables constructing bespoke models speciﬁc to each user’s individual access rights and preferences. We can add or remove information from the model by simply adding or removing the corresponding prompts without retraining from scratch. We demonstrate that `a-la-carte built models achieve accuracy within 5% of models trained on the union of the respective sources, with comparable cost in terms of training and inference time. For the continual learning benchmarks Split CIFAR-100 and CORe50, we achieve state-of-the-art performance. 1.

Introduction
As large neural network models make their way into com-mercial applications, the basic paradigm of training them on a monolithic dataset leads to a number of challenges. First, as new data become available, updating the whole model can be prohibitively expensive. Even when training time is not an issue, some users may still require access and main-tenance of previous versions of the model to avoid disrup-tions of their downstream workﬂows. Second, owners of the training data may modify their sharing preferences at any time, leading to datasets that shrink over time (machine unlearning) or to different subsets of the training data being
*Work done during an internship at AWS AI Labs. usable by different users (compartmentalization). Finally, the users themselves may want to use custom subsets of the data to better tailor their model to their use cases (model customization).
These challenges are well known and addressed separately in different ﬁelds such as continual learning, forgetting, and model adaption. However, in order for a commercial sys-tem to be viable at scale, these issues have to be tackled concurrently.
Ideally, one would have a large model that each user can run, trained using only data the speciﬁc user wants and has rights to, that can evolve without the need for
ﬁne-tuning as new data becomes available, or as individ-ual data owners exercise their right to have their data erased (“the right to be forgotten”).
We refer to the problem of building such a model as `a-la-carte learning since, depending on the data availability and the user, the service may need to select and use different data chunks from a menu of available training data. More
D1, . . . , Dn} speciﬁcally, let be a variable collection
{ of data sources (a data pool). In `a-la-carte learning a user at inference time can specify a subset S of training data together with an input sample x to receive a personalized
`a-la-carte output f (x, S) from the model f . Critically, the
S. output f (x, S) must not depend on any data source Di / 2
⇢D
=
D
`A-la-carte learning can be na¨ıvely tackled in two ways. The service could pre-train one model for each possible sub-set of the data pool, and serve each user the most power-ful model they have rights to. While optimal from the user view-point, this requires a prohibitive exponential complex-ity O(2|D|) in both training time and storage. On the other extreme, the service could train a separate model on each data source individually and, at inference time, ensemble all models obtained from the sources in S. This requires
) training time complexity to pre-train only linear O( each model, but still has a signiﬁcant storage cost. Fur-thermore due to the ensembling inference time is signiﬁ-|D|
`A-la-carte Learning and APT. Given a pool of multiple data sources, the goal of `A-la-carte Learning is to allow the user to
Figure 1. select – at inference time – an arbitrary subset S of sources to use. The performance of the `a-la-carte model should be comparable to the performance of a model trained on S. (A) APT enables efﬁcient `A-la-carte Learning by converting each source into a prompt, and composing together the relevant prompts at inference time. (B) To perform inference, APT uses a modiﬁed attention mechanism that prevents the prompts from interfering with each other and ensembles the individual outputs to construct the ﬁnal prediction.
⇢D cantly increased while also potentially suffering from lower performance than the ideal “paragon” model trained on the union of sources in S. The goal of `a-la-carte learning is to achieve performance as close as possible to the paragon without signiﬁcantly increasing inference or training time.
To address these key issues, we propose `A-la-carte Prompt
Tuning (APT). APT leverages vision transformers and prompt tuning to solve the `a-la-carte learning problem.
First, APT converts each dataset Di into a learned prompt pi, thus transforming the data pool into a prompt pool. Then at inference time, given a subset of sources S to use, APT retrieves all corresponding prompts and concatenates them together with the input. Surprisingly, we show that in most cases APT has performance comparable to the paragon of joint learning with all data in S. Moreover, since each prompt is trained on an individual dataset, information is naturally compartmentalized. Thanks to the small size of prompts and an efﬁcient forwarding method, APT is sig-niﬁcantly cheaper (in both storage and inference time) than ensembling models.
Importantly however, we note that simply concatenating different prompts that were trained separately leads to de-structive interference in the attention block which corrupts the representations (see Table 2). To address this problem, we introduce a modiﬁed attention mechanism that elimi-nates such interference, while also signiﬁcantly reducing the inference time when multiple prompts are concatenated.
A priori, this change comes with a small reduction in ex-pressive power and in the ability to capture synergistic in-formation between data sources. However, one of our main contributions is to show that the resulting drop in accuracy is generally modest, while providing far more valuable ben-eﬁts to scalability, maintainability, and privacy.
We empirically demonstrate the advantage of APT-based `a-la-carte learning for forgetting and continual learning (both domain-incremental and class-incremental). We observe that in most cases the performance of APT is within 5% of the performance of the paragon at a fraction of the cost.
We also show that APT outperforms all comparable base-lines with the advantage of computational scalability from the structured attention mechanism.
Summary of our contributions. 1. We introduce the `A-la-carte Learning problem to ad-learning, machine unlearning, and dress continual model customization concurrently. 2. We propose APT, an efﬁcient method to address `A-la-carte Learning based on visual prompt tuning and a modiﬁed attention mechanism. 3. We demonstrate that for most tasks APT achieves ac-curacy within 5% of paragon performance even when each individual prompt has access to an order of mag-nitude less data 4. We show that APT with a simple prompt weighting mechanism achieves state-of-the-art performance on continual learning benchmarks Split CIFAR-100 and
CORe50. 2.