Abstract
Personalized federated learning (pFL) collaboratively trains personalized models, which provides a customized model solution for individual clients in the presence of het-erogeneous distributed local data. Although many recent studies have applied various algorithms to enhance per-sonalization in pFL, they mainly focus on improving the performance from averaging or top perspective. How-ever, part of the clients may fall into poor performance and are not clearly discussed. Therefore, how to prevent these poor clients should be considered critically.
Intu-itively, these poor clients may come from biased univer-sal information shared with others. To address this issue, we propose a novel pFL strategy, called Personalize Lo-cally, Generalize Universally (PLGU). PLGU generalizes the fine-grained universal information and moderates its bi-ased performance by designing a Layer-Wised Sharpness
Aware Minimization (LWSAM) algorithm while keeping the personalization local. Specifically, we embed our proposed
PLGU strategy into two pFL schemes concluded in this pa-per: with/without a global model, and present the training procedures in detail. Through in-depth study, we show that the proposed PLGU strategy achieves competitive general-ization bounds on both considered pFL schemes. Our exten-sive experimental results show that all the proposed PLGU based-algorithms achieve state-of-the-art performance. 1.

Introduction
Federated Learning (FL) is a popular collaborative re-search paradigm that trains an aggregated global learning model with distributed private datasets on multiple clients
[16, 29]. This setting has achieved great accomplishments when the local data cannot be shared due to privacy and communication constraints [36]. However, because of the
*Corresponding author.
ExE
Figure 1. Toy example in a heterogeneous pFL on CIFAR10, which includes 100 clients and each client obtains 3 labels. non-IID/heterogeneous datasets, learning a single global model to fit the “averaged distribution” may be difficult to propose a well-generalized solution to the individual client and slow the convergence results [24]. To address this problem, personalized federated learning (pFL) is devel-oped to provide a customized local model solution for each client based on its statistical features in the private train-ing dataset [5, 9, 11, 34]. Generally, we can divide existing pFL algorithms into two schemes: (I) with a global model
[5, 23, 25, 40] or (II) without a global model [27, 28, 37].
Though many pFL algorithms make accomplishments by modifying the universal learning process [41, 47] or en-hancing the personalization [3, 27, 37], they may lead part of clients to fall into poor learning performance, where the personalization of local clients performs a large statistical deviation from the “averaged distribution”. To the best of our knowledge, none of the existing studies explore how to prevent clients from falling into poor personalized per-formance on these two schemes. For example, the poor medical learning models of some clients may incur seri-ous medical malpractice. To better present our concerned problem, we introduce a toy example in Figure 1, which is learned by two pFL algorithms representing these two schemes: pFedMe [40] and FedRep [3]. Though both al-gorithms achieve high averaged local model performance of 66.43% and 71.35%, there also 15% of clients are less than 64% and 14% clients are less than 69%, respectively.
This motivates us to exploit an effective strategy to prevent clients from falling into poor performance while without de-grading others, e.g., the green curve.
Intuitively, we consider this phenomenon oftentimes comes from the biased universal information towards the clients with better learning performance. For scheme I, a simple-averaged aggregation may not perfectly handle data heterogeneity, as it generates serious bias between the global and local clients. For scheme II, abandoning the universal contribution may dismiss some information from other clients.
Instead of designing a new pFL algorithm, we propose a novel pFL strategy on existing pFL stud-ies: generalizing the universal learning for unbiased local adaptation as well as keeping the local personalized fea-tures, called Personalize Locally, Generalize Universally (PLGU). The main challenge of PLGU is to generalize uni-versal information without local feature perturbation, as the
In this pa-statistical information is only stored locally. per, we tackle this challenge by developing a fine-grained perturbation method called Layer-Wised Sharpness-Aware-Minimization (LWSAM) based on the SAM optimizer [7, 33], which develops a generalized training paradigm by leveraging linear approximation. Furthermore, we present how to embed this PLGU strategy with the perturbed uni-versal generalization on both the two pFL schemes.
For scheme I (with the global model), we propose the
PLGU-Layer Freezing (LF) algorithm. As illustrated in
[21, 31, 48], each layer in a personalized model shares a different contribution: the shallow layers focus more on lo-cal feature extraction (personalization), and the deeper lay-ers are for extracting global features (universal). Specifi-cally, the PLGU-LF first explores the personalization score of each layer. Then, PLGU-LF freezes the important layer locally for personalization and uses the LWSAM optimizer with the consideration of obtained layer importance score for universal generalization. For scheme II (without the global model), we mainly focus on our proposed PLGU strategy FedRep algorithm [3], named PLGU-GRep. It gen-eralizes the universal information by smoothing personal-ization in the representation part. To show the extensibil-ity, we present that we can successfully extend our PLGU strategy to pFedHN [37], called PLGU-GHN, to improve learning performance, especially for poor clients. Further-more, we analyze the generalization bound on PLGU-LF,
PLGU-GRep, and PLGU-GHN algorithms in-depth. Exten-sive experimental results also show that all three algorithms successfully prevent poor clients and outperform the aver-age learning performance while incrementally reducing the top-performance clients. 2.