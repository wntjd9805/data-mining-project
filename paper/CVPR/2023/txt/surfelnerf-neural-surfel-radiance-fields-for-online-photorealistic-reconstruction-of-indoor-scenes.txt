Abstract
Online reconstructing and rendering of large-scale in-door scenes is a long-standing challenge. SLAM-based methods can reconstruct 3D scene geometry progressively in real time but can not render photorealistic results. While
NeRF-based methods produce promising novel view syn-thesis results, their long offline optimization time and lack of geometric constraints pose challenges to efficiently han-dling online input. Inspired by the complementary advan-tages of classical 3D reconstruction and NeRF, we thus in-vestigate marrying explicit geometric representation with
NeRF rendering to achieve efficient online reconstruction and high-quality rendering. We introduce SurfelNeRF, a variant of neural radiance field which employs a flexible and scalable neural surfel representation to store geomet-ric attributes and extracted appearance features from input images. We further extend the conventional surfel-based fusion scheme to progressively integrate incoming input frames into the reconstructed global neural scene represen-tation. In addition, we propose a highly-efficient differen-tiable rasterization scheme for rendering neural surfel radi-ance fields, which helps SurfelNeRF achieve 10× speedups in training and inference time, respectively. Experimental results show that our method achieves the state-of-the-art 23.82 PSNR and 29.58 PSNR on ScanNet in feedforward inference and per-scene optimization settings, respectively.1 1.

Introduction
Large-scale scene reconstruction and rendering is a cru-cial but challenging task in computer vision and graphics with many applications. Classical visual simultaneous lo-calization and mapping (SLAM) systems [6, 12, 16, 24, 39, 41] can perform real-time 3D scene reconstruction. How-ever, they usually represent the scene geometry as solid sur-faces and appearance as vertex color or texture maps; thus, 1Project website: https://gymat.github.io/SurfelNeRF-web
Figure 1. Examples to illustrate the task of online photorealistic reconstruction of an indoor scene. The online photorealistic re-construction of large-scale indoor scenes: given an online input image stream of a previously unseen scene, the goal is to progres-sively build and update a scene representation that allows for high-quality rendering from novel views. the reconstructed results fail to fully capture the scene con-tent and cannot be used for photorealistic rendering. Re-cently, neural radiance fields (NeRF) and its variants [2, 3, 22,27,35,42] have achieved unprecedented novel view syn-thesis quality on both object-centric and large-scale scenes.
However, NeRFs suffer from long per-scene optimization time and slow rendering speed, especially for large-scale scenes. Although recent advances [7, 14, 19, 23, 33] achieve faster optimization and rendering via incorporating explicit representations, they still require gathering all input images in an offline fashion before optimizing each scene.
In this paper, we target the challenging task of online photorealistic reconstruction of large-scale indoor scenes: given an online input image stream of a previously unseen scene, the goal is to progressively build and update a scene representation that allows for high-quality rendering from novel views. The online setting can unlock a variety of real-time interactive applications, providing crucial immediate feedback to users during 3D capture. However, this task brings multiple extra requirements, including the scalabil-Methods
DeepSurfel [21]
Instant-NGP [23]
PointNeRF [43]
VBA [10]
NeRFusion [50]
Ours
Representation Generalization Real-time Rendering Online Fusion Scalability
Surfels
Hash Grids
Point Clouds
B+ Trees
Voxel Grids
Surfels
✗
✗
✓
✗
✓
✓
✓
✓
✗
✗
✗
✓
✗
✗
✗
✓
✓
✓
✗
✗
✓
✓
✗
✓
Table 1. Comparison of representation and features with existing methods. ity of the underlying scene representation, the ability to per-form on-the-fly updates to the scene representation, and op-timizing and rendering at interactive framerates. Recently,
NeRFusion [50] followed NeuralRecon [34] to unproject in-put images into local sparse feature volumes, fusing them to a global volume via Gated Recurrent Units (GRUs), and then generating photorealistic results from the global fea-ture volume via volume rendering. However, updating the sparse volumetric feature involves computationally heavy operations; the volume rendering is also very slow since it requires hundreds of MLP evaluations to render a pixel.
Thus, although NeRFusion achieves efficient online scene reconstruction, it still needs dozens of seconds to render a frame. VBA [10] is another recent approach to online pho-torealistic scene capture, but it only applies to object-centric scenes. We compare with representation and key features used in online photorealistic rendering with existing meth-ods, which is shown in Tab. 1.
We propose surfel-based neural radiance fields, Sur-felNeRF, for online photorealistic reconstruction and ren-Surfels (surface dering of large-scale indoor scenes. elements) [25] are point primitives containing geometric at-tributes, e.g., position, color, normal, and radius. We ex-tend this representation to neural surfels, storing extra neu-ral features that encode the neural radiance field of the target scene. Compared with volumetric representations, neural surfels are more compact and flexible and can easily scale to large scenes. Besides, we further employ a fast and dif-ferentiable rasterization process to render neural surfel ra-diance fields, which produces a pixel with only a few MLP evaluations based on the rasterized neural surfels.
Inspired by classical real-time surfel-based geometric re-construction methods [6, 15, 41], we propose an efficient neural radiance field fusion method to progressively build the scene representation by integrating neighboring neural surfels. Unlike point-based representations [1, 26, 30, 43] that are computationally heavy when finding neighboring points, it is easier to locate overlapping surfels and then merge neural features from multiview observations. By coupling the SurfelNeRF representation, the efficient neural surfel fusion approach, and the fast neural surfel rasteriza-tion algorithm, we achieve high-quality, photorealistic 3D scene reconstruction in an online manner.
We conduct experiments on the large-scale indoor scene dataset ScanNet [11], which contains complex scene struc-tures and a large variety of scene appearances. We train the
SurfelNeRF end-to-end across the scenes on the ScanNet, obtaining a generalizable model that enables both feedfor-ward inference on unseen data and per-scene fine-tuning.
We demonstrate in experiments that the proposed Surfel-NeRF achieves favorably better rendering quality than the state-of-the-art approaches in both feedforward and fine-tuning settings while maintaining high training and render-ing efficiency. We believe the proposed online photorealis-tic reconstruction framework has great potential in practical applications. 2.