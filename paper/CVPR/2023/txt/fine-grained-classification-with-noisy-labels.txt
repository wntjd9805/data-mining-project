Abstract
Learning with noisy labels (LNL) aims to ensure model generalization given a label-corrupted training set. In this work, we investigate a rarely studied scenario of LNL on fine-grained datasets (LNL-FG), which is more practical and challenging as large inter-class ambiguities among fine-grained classes cause more noisy labels. We empiri-cally show that existing methods that work well for LNL fail to achieve satisfying performance for LNL-FG, aris-ing the practical need of effective solutions for LNL-FG.
To this end, we propose a novel framework called stochas-tic noise-tolerated supervised contrastive learning (SNSCL) that confronts label noise by encouraging distinguishable representation. Specifically, we design a noise-tolerated supervised contrastive learning loss that incorporates a weight-aware mechanism for noisy label correction and se-lectively updating momentum queue lists. By this mecha-nism, we mitigate the effects of noisy anchors and avoid inserting noisy labels into the momentum-updated queue.
Besides, to avoid manually-defined augmentation strategies in contrastive learning, we propose an efficient stochas-tic module that samples feature embeddings from a gener-ated distribution, which can also enhance the representa-tion ability of deep models. SNSCL is general and compati-ble with prevailing robust LNL strategies to improve their performance for LNL-FG. Extensive experiments demon-strate the effectiveness of SNSCL. 1.

Introduction
Learning from noisy labels [12, 13, 18, 21, 26, 40, 55, 58] poses great challenges for training deep models, whose per-formance heavily relies on large-scaled labeled datasets [28, 47–49]. Annotating data with high confidence would be resource-intensive, especially for some domains, such as medical and remote sensing images [29, 36, 37, 41, 46].
∗ denotes corresponding author
Figure 1. LNL-FG is more challenging than LNL on generic clas-sification. denote mislabeled samples.
Thus, label noise would inevitably arise and then greatly degrade the generalization performance of deep models.
Previous methods [1, 6, 7, 9, 18, 23, 38, 53, 54] in LNL al-ways focus on generic classification (e.g. CIFAR-10 & 100) and artificially construct random label noise [21, 23, 42, 43] and dependent label noise [9, 18, 38, 53, 55] to evaluate the performance of their algorithms. In this work, we extend
LNL to fine-grained classification, which is a rarely studied task. Firstly, this scenario is more realistic since annota-tors are easier to be misguided by indistinguishable charac-teristics among fine-grained images and give an uncertain target. Fig. 1 illustrates comparison between two types of noise simulated on generic and fine-grained sets. Fur-ther, we extensively investigate the performance of prevail-ing LNL methods on our proposed LNL-FG task. The de-tailed results are shown in Fig. 2. Although these robust al-gorithms lead to statistically significant improvements over vanilla softmax cross-entropy on LNL, these gains do not transfer to LNL-FG task. Instead, some methods degrade the generalization performance of deep models compared to cross-entropy. Intuitively, due to large inter-class ambi-guity among those classes in LNL-FG, the margin between noisy samples and the decision boundary in the fine-grained dataset is smaller than that in the generic dataset, leading to severe overfitting of deep models to noisy labels. De-spite this fact, the typical method for better representation,
Figure 2. Comparison results of previous methods on four fine-grained benchmarks with 20% random label noise. Methods with same color and shape belong to the same strategy. The X-axis denotes their performance on typical LNL tasks while the Y-axis denotes that on LNL-FG tasks. It is obvious that not all robust methods outperform the performance of vanilla cross-entropy on LNL-FG task..
More analysis and results can be found in Appx. A. i.e., DivideMix, consistently achieves better performance on both LNL and LNL-FG tasks (See Fig. 2). From this perspective, we consider that encouraging discrimitive fea-ture not only confronts overfitting to label noise but also facilitates the learning of fine-grained task.
For this, contrastive learning (CL), as a powerful unsu-pervised learning approach for generating discrimitive fea-ture [4,8,11,14,31], has attracted our attention. CL methods usually design objective functions as supervised learning to perform pretext similarity measurement tasks derived from an unlabeled dataset, which can learn effective visual repre-sentations in downstream tasks, especially for fine-grained classification [3]. The following work, supervised con-trastive learning (SCL) [15], leverages label information to further enhance representation learning, which can avoid a vast training batch and reduce the memory cost. However,
SCL cannot be directly applied to the noisy scenario as it is lack of noise-tolerated mechanism.
To resolve the noise-sensitivity of SCL, we propose a novel framework named stochastic noise-tolerated su-pervised contrastive learning (SNSCL), which contains a noise-tolerated contrastive loss and a stochastic module.
For the noise-tolerated contrastive loss, we roughly cate-gorize the noise-sensitive property of SCL into two parts of noisy anchors and noisy query keys in the momentum queue. To mitigate the negative effect introduced by noisy anchors or query keys, we design a weight mechanism for measuring the reliability score of each sample and give cor-responding weight. Based on these weights, we modify the label of noisy anchors in current training batch and selec-tively update the momentum queue for decreasing the prob-ability of noisy query keys. These operations are adaptive and can achieve a progressive learning process. Besides, to avoid manual adjustment of strong augmentation strategies for SCL, we propose a stochastic module for more com-plex feature transformation. In practice, this module gener-ates the probabilistic distribution of feature embedding. By sampling operation, SNSCL achieves better generalization performance for LNL-FG.
Our contributions can be summarized as
• We consider a hardly studied LNL task, dubbed LNL-FG and conduct empirical investigation to show that some existing methods in LNL cannot achieve satisfy-ing performance for LNL-FG.
• We design a novel framework dubbed stochastic noise-tolerated supervised contrastive learning (SNSCL), which alters the noisy labels for anchor samples and selectively updates the momentum queue, avoiding the effects of noisy labels on SCL.
• We design a stochastic module to avoid manually-defined augmentation, improving the performance of
SNSCL on representation learning.
• Our proposed SNSCL is generally applicable to pre-vailing LNL methods and significantly improves their performance on LNL-FG.
Extensive experiments on four fine-grained datasets and two real-world datasets consistently demonstrate the state-of-the-art performance of SNSCL, and further analysis ver-ify its effectiveness. 2.