Abstract
Audio-driven facial reenactment is a crucial technique that has a range of applications in film-making, virtual avatars and video conferences. Existing works either em-ploy explicit intermediate face representations (e.g., 2D fa-cial landmarks or 3D face models) or implicit ones (e.g.,
Neural Radiance Fields), thus suffering from the trade-offs between interpretability and expressive power, hence be-tween controllability and quality of the results. In this work, we break these trade-offs with our novel parametric implicit face representation and propose a novel audio-driven fa-cial reenactment framework that is both controllable and can generate high-quality talking heads. Specifically, our parametric implicit representation parameterizes the im-plicit representation with interpretable parameters of 3D face models, thereby taking the best of both explicit and im-plicit methods. In addition, we propose several new tech-niques to improve the three components of our framework, including i) incorporating contextual information into the audio-to-expression parameters encoding; ii) using condi-tional image synthesis to parameterize the implicit repre-sentation and implementing it with an innovative tri-plane structure for efficient learning; iii) formulating facial reen-actment as a conditional image inpainting problem and proposing a novel data augmentation technique to improve model generalizability. Extensive experiments demonstrate that our method can generate more realistic results than previous methods with greater fidelity to the identities and talking styles of speakers. 1.

Introduction
Audio-driven facial reenactment, also known as audio-driven talking head generation or synthesis, plays an im-portant role in various applications, such as digital human, film-making and virtual video conference. It is a challeng-ing cross-modal task from audio to visual face, which re-quires the generated talking heads to be photo-realistic and
*Corresponding author is Guanbin Li.
Figure 1. Comparison between previous explicit, implicit repre-sentations and our parametric implicit representation (PIR). (a)
Explicit representations (e.g., 3D face models) have interpretable parameters but lack expressive power. (b) Implicit representa-tions (e.g., NeRF) have strong expressive power but are not in-terpretable. (c) Our PIR takes the best of both approaches and is both interpretable and expressive, thus paving the way for control-lable and high-quality audio-driven facial reenactment. have lip movements synchronized with the input audio.
According to the intermediate face representations, ex-isting facial reenactment methods can be roughly classi-fied into two categories: explicit and implicit methods.
Between them, explicit methods [5, 18, 27, 29, 30, 34, 37, 40, 44] exploit relatively sophisticated 2D (e.g., 2D facial landmarks [5, 18, 29, 34, 44]) or 3D (e.g., 3D Morphable
Model [27, 30, 37, 40]) parametric face models to recon-struct 2D or 3D faces, and map them to photo-realistic faces with a rendering network such as the Generative Adversar-ial Networks (GANs) [32, 39]. Their distinct advantage is
the controllability (e.g., expressions) resulting from their in-terpretable facial parameters. However, despite this advan-tage, the parametric face models used in explicit methods are often sparse and have very limited expressive power, which inevitably sacrifices the quality of synthesized faces (e.g., the inaccurate lip movements and blurry mouth caused by the missing teeth area in 3D face models). In contrast, implicit methods [12, 16, 17, 24, 25, 28, 42, 43] use implicit 2D or 3D representations that are more expressive and can generate more realistic faces. For example, Neural Radi-ance Fields (NeRF) based methods [12, 17, 25] are one of the more representative implicit methods that use NeRF to represent the 3D scenes of talking heads. Although being more expressive and producing higher-quality results, im-plicit methods are not interpretable and lose the control-lability of the synthesis process, thus requiring model re-training to change its target person. As a result, the explicit and implicit methods mentioned above form a trade-off be-tween the interpretability and expressive power of interme-diate face representations, while a representation that is both interpretable and expressive remains an open problem.
In this work, we break the above trade-off by propos-ing a parametric implicit representation that is both inter-pretable and expressive, paving the way for controllable and high-quality audio-driven facial reenactment. Specif-ically, we propose to parameterize implicit face represen-tations with the interpretable parameters of the 3D Mor-phable Model (3DMM) [10] using a conditional image syn-thesis paradigm. In our parametric implicit representation, the 3DMM parameters offer interpretability and the implicit representation offers strong expressive power, which take the best of both explicit and implicit methods (Fig. 1). To implement our idea, we propose a novel framework consist-ing of three components: i) contextual audio to expression (parameters) encoding; ii) implicit representation parame-terization; iii) rendering with parametric implicit represen-tation. Among them, our contextual audio to expression encoding component employs a transformer-based encoder architecture to capture the long-term context of an input audio, making the resulting talking heads more consistent and natural-looking; our implicit representation parameter-ization component uses a novel conditional image synthe-sis approach for the parameterization, and innovatively em-ploys a tri-plane based generator offered by EG3D [3] to learn the implicit representation in a computationally effi-cient way; our rendering with parametric implicit represen-tation component formulates face reenactment as an image inpainting problem conditioned on the parametric implicit representation to achieve a consistent and natural-looking
“blending” of the head and torso of a target person. In addi-tion, we observe that the model slightly overfits to the train-ing data consisting of paired audio and video, causing jitters in the resulting talking heads whose lip movements are re-quired to be synchronized with unseen input audio. To help our model generalize better and produce more stable results, we further propose a simple yet effective data augmentation strategy for our rendering component.
In summary, our main contributions include:
• We propose an innovative audio-driven facial reen-actment framework based on our novel paramet-ric implicit representation, which breaks the previ-ous trade-off between interpretability and expressive power, paving the way for controllable and high-quality audio-driven facial reenactment.
• We propose several new techniques to improve the three components of our innovative framework, in-cluding: i) employing a transformer-based encoder ar-chitecture to incorporate contextual information into the audio to expression (parameters) encoding; ii) us-ing a novel conditional image synthesis approach for the parameterization of implicit representation, which is implemented with an innovative tri-plane based gen-erator [3] for efficient learning; iii) formulating facial reenactment as a conditional image inpainting problem for natural “blending” of head and torso, and propos-ing a simple yet effective data augmentation technique to improve model generalizability.
• Extensive experiments show that our method can gen-erate high-fidelity talking head videos and outperforms state-of-the-art methods in both objective evaluations and user studies. 2.