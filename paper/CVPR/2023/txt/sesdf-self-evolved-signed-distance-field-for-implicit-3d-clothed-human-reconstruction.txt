Abstract
We address the problem of clothed human reconstruction from a single image or uncalibrated multi-view images. Ex-isting methods struggle with reconstructing detailed geome-try of a clothed human and often require a calibrated setting for multi-view reconstruction. We propose a flexible frame-work which, by leveraging the parametric SMPL-X model, can take an arbitrary number of input images to reconstruct a clothed human model under an uncalibrated setting. At the core of our framework is our novel self-evolved signed distance field (SeSDF) module which allows the framework to learn to deform the signed distance field (SDF) derived from the fitted SMPL-X model, such that detailed geometry reflecting the actual clothed human can be encoded for bet-ter reconstruction. Besides, we propose a simple method for self-calibration of multi-view images via the fitted SMPL-X parameters. This lifts the requirement of tedious man-ual calibration and largely increases the flexibility of our method. Further, we introduce an effective occlusion-aware feature fusion strategy to account for the most useful fea-tures to reconstruct the human model. We thoroughly eval-uate our framework on public benchmarks, demonstrating significant superiority over the state-of-the-arts both quali-tatively and quantitatively. 1.

Introduction
Clothed human reconstruction is a hot topic with increas-ing demand in real-world applications such as 3D telep-resence, game modeling, metaverse [40], etc. Early works show promise under equipment-assisted settings, requiring expensive dense camera rigs [12] and tedious calibration procedures [15]. Parametric models, such as SMPL [36] and SMPL-X [47], have been introduced to model a naked human body with constrained parameters. With the wit-nessed success of deep learning in many vision tasks, many deep learning methods have been proposed to regress the parameters of such parametric human models from a sin-gle [16, 66, 67] or multiple images [6, 13, 68]. However, these methods can only reconstruct a minimally-clothed hu-man model without many details (e.g., hairs and clothes).
Recently, methods based on implicit shape representation have reported encouraging performance, showing promis-ing reconstruction with increased details in both single-view [23, 26, 52, 63, 71] and multi-view [24, 25, 54, 55] settings.
Despite the stunning results reported by the above meth-ods, their reconstructions remain far from perfect, restrict-ing their practical applications. For instance, state-of-the-art (SOTA) single-view methods, such as PIFuHD [53],
PaMIR [71], and ARCH++ [23], struggle with many self-occluding non-frontal human poses that widely occur in the real world. ICON [63] can handle these cases but the reconstructions contain serious artifacts (see Fig. 3). On the other hand, many multi-view methods depend on cali-brated cameras (e.g., [24, 54, 55]) which are tedious to ob-tain in practice. Hence, how to carry out multi-view recon-struction with uncalibrated cameras is an important topic to study. Meanwhile, effective multi-view feature fusion is another key factor for robust mutli-view reconstruction.
Prior techniques for multi-view feature fusion include aver-age pooling [52], SMPL-visibility [63], and attention-based mechanism [70]. However, the reconstruction results based on these fusion techniques still contain notable artifacts in many cases. This indicates that more efforts are needed to derive a better multi-view feature fusion for more robust re-construction.
In this paper, to extract more clothed human details flex-ibly and robustly from a single RGB image or uncalibrated multi-view RGB images, we present a novel framework, named SeSDF, that employs the parametric model SMPL-X [47] as a 3D prior and combines the merits of both im-plicit and explicit representations. SeSDF takes a single image or multi-view images as input to predict the occu-pancy for each 3D location in the space representing the human model. To reconstruct high-frequency details such as hairs and clothes, we introduce a self-evolved signed dis-tance field module that learns to deform the signed distance field (SDF) derived from the fitted SMPL-X model using the input images. The resulting SDF can reflect more accu-rate geometry details than SMPL-X, which inherently devi-ates from the actual clothed human model. The SDF refined by our SeSDF module is further encoded to allow for 3D re-construction with better geometry details.
Besides reconstructing a faithful 3D clothed human model from a single image, our SeSDF framework can also work with uncalibrated multi-view images to gener-ate clothed human avatar with enhanced appearance (see
Fig. 1). To this end, we first propose a self-calibration method by fitting a shared SMPL-X model across multi-view images and projecting the shared model to different images based on the optimized rigid body motion for each input image. Further, we propose an occlusion-aware fea-ture fusing strategy by probing the visibility of each 3D point under different views through ray-tracing, leveraging the SMPL-X model, such that features from visible views will contribute more to the fused feature while those from invisible views will be suppressed.
The contributions are summarized as follows:
• We propose a flexible framework that, by leveraging the
SMPL-X model as a shape prior, can take an arbitrary number of uncalibrated images to perform high-fidelity clothed human reconstruction. At the core of our frame-work is a self-evolved signed distance field (SeSDF) mod-ule for recovering faithful geometry details.
• For uncalibrated multi-view reconstruction, we propose a simple self-calibration method through leveraging the
SMPL-X model, as well as an occlusion-aware feature fusion strategy which takes into account the visibility of a space point under different views via ray-tracing.
• We thoroughly evaluate our framework on public bench-marks. SeSDF exhibits superior performances over cur-rent state-of-the-arts both qualitatively and quantitatively. 2.