Abstract 1.

Introduction
We propose a novel approach for unsupervised 3D animation of non-rigid deformable objects. Our method learns the 3D structure and dynamics of objects solely from single-view RGB videos, and can decompose them into semantically meaningful parts that can be tracked and animated. Using a 3D autodecoder framework, paired with a keypoint estimator via a differentiable PnP algorithm, our model learns the underlying object geometry and parts decomposition in an entirely unsupervised manner. This al-lows it to perform 3D segmentation, 3D keypoint estimation, novel view synthesis, and animation. We primarily evaluate the framework on two video datasets: VoxCeleb 2562 and
TEDXPeople 2562.
In addition, on the Cats 2562 image dataset, we show it even learns compelling 3D geometry from still images. Finally, we show our model can obtain animatable 3D objects from a single or few images 1. 1Code and visual results available on our project website: https://snap-research.github.io/unsupervised-volumetric-animation.
⇤ Work done while interning at Snap.
The ability to realistically animate a dynamic object seen in a single image enables compelling creative tasks.
Such applications range from tractable and cost-effective approaches to visual effects for cinema and television, to more lightweight consumer applications (e.g., enabling ar-bitrary users to create “performances” by famous modern or historical ﬁgures). However, this requires understanding the object’s structure and motion patterns from a single static depiction. Efforts in this ﬁeld are primarily divided into two approaches: those that outsource this understanding to existing, off-the-shelf models speciﬁc to an object category that capture its particular factors of variation; and those that learn the object structure from the raw training data itself.
The former group employs supervision, and thus requires knowledge about the animated object (e.g., the plausible range of shapes and motions of human faces or bodies).
The latter group is unsupervised, providing the ﬂexibility needed for a wider range of arbitrary object categories.
Signiﬁcant progress has been made recently in the do-main of unsupervised image animation. Methods in this category typically learn a motion model based on object
parts and the corresponding transformations applied to them. Initially, such transformations were modeled using a simple set of sparse keypoints. Further works improved the motion representation [52, 55], learned latent motion dictionaries [64], kinematic chains [59] or used thin-plate spline transformations [81]. However, broadly speaking, all such works propose 2D motion representations, warping the pixels or features of the input image such that they correspond to the pose of a given driving image. As such, prior unsupervised animation methods offer means to perform 2D animation only, and are inherently limited in modeling complex, 3D effects, such as occlusions, viewpoint changes, and extreme rotations, which can only be explained and addressed appropriately when considering the 3D nature of the observed objects.
Our work fundamentally differs from prior 2D works in that it is the ﬁrst to explore unsupervised image animation in 3D. This setting is substantially more challenging compared to classical 2D animation for several reasons. First, as the predicted regions or parts now exist in a 3D space, it is quite challenging to identify and plausibly control them from only 2D videos without extra supervision. Second, this challenge is further compounded by the need to properly model the distribution of the camera in 3D, which is a problem in its own right [40], with multiple 3D generators resorting to existing pose predictors to facilitate the learning of the underlying 3D geometry [5,58]. Finally, in 3D space, there exists no obvious and tractable counterpart for the bias of 2D CNNs, which are essential for unsupervised keypoint detection frameworks for 2D images [53].
We offer a solution to these challenges. Our framework maps an embedding of each object to a canonical volumetric representation, parameterized with a voxel grid, containing volumetric density and appearance. To allow for non-rigid deformations of the canonical object representation, we assume the object consists of a certain number of rigid parts which are softly assigned to each of the points in the canon-ical volume. A procedure based on linear blend skinning is employed to produce the deformed volume according to the pose of each part. Rather than directly estimating the poses, we introduce a set of learnable 3D canonical keypoints for each part, and leverage the 2D inductive bias of 2D CNNs to predict a set of corresponding 2D keypoints in the current frame. We propose the use of a differentiable Perspective-n-Point (PnP) algorithm to estimate the corresponding pose, explicitly linking 2D observations to our 3D representation.
This framework allows us to propagate the knowledge from 2D images to our 3D representation, thereby learning rich and detailed geometry for diverse object categories using a photometric reconstruction loss as our driving objective.
The parts are learned in an unsupervised manner, yet they converge to meaningful volumetric object constituents. For example, for faces, they correspond to the jaw, hair, neck, and the left and right eyes and cheeks. For bodies, the same approach learns parts to represent the torso, head, and each hand. Examples of these parts are given in Fig. 1.
To simplify the optimization, we introduce a two-stage strategy, in which we start by learning a single part such that the overall geometry is learned, and proceed by allowing the model to discover the remaining parts so that animation is possible. When the object is represented with a single part, the model can perform 3D reconstruction and novel view synthesis. When more parts are used, our method allows us to not only identify meaningful object parts, but to perform non-rigid animation and novel view synthesis at the same time. Examples of images animated using our Unsupervised
Volumetric Animation (UVA) are given in Fig. 1.
We train our framework on three datasets containing images or videos of various objects. We ﬁrst show that our method learns meaningful 3D geometry when trained on still images of cat faces [79]. We then train our method on the VoxCeleb [38] and TEDXPeople [17] video datasets to evaluate 3D animation. Since our method is the ﬁrst to consider unsupervised 3D animation, we further introduce evaluation metrics assessing novel view synthesis and ani-mation quality when only single-view data is available. 2.