Abstract
Enormous hand images with reliable annotations are collected through marker-based MoCap . Unfortunately, degradations caused by markers limit their application in hand appearance reconstruction. A clear appearance re-covery insight is an image-to-image translation trained with unpaired data. However, most frameworks fail because there exists structure inconsistency from a degraded hand to a bare one. The core of our approach is to first disentan-gle the bare hand structure from those degraded images and then wrap the appearance to this structure with a dual ad-versarial discrimination (DAD) scheme. Both modules take full advantage of the semi-supervised learning paradigm:
The structure disentanglement benefits from the modeling ability of ViT, and the translator is enhanced by the dual dis-crimination on both translation processes and translation results. Comprehensive evaluations have been conducted to prove that our framework can robustly recover photo-realistic hand appearance from diverse marker-contained and even object-occluded datasets. It provides a novel av-enue to acquire bare hand appearance data for other down-stream learning problems. 1.

Introduction
Both bare hand appearance and vivid hand motion are of great significance for virtual human creation. A dilemma hinders the synchronous acquisition of these two: accu-rate motion capture [20, 27, 68] relies on markers that de-grade hand appearance, whereas detailed appearance cap-ture [50, 59, 75] in a markerless setting makes hand motion hard to track. Is there a win-win solution that guarantees high fidelity for both?
Existing ones include markerless MoCap [26,83,88] and graphic rendering [16, 29, 80]. However, the former re-*Corresponding author. E-mail: yangangwang@seu.edu.cn. This work was supported in part by the National Natural Science Foundation of China (No. 62076061), in part by the Natural Science Foundation of Jiangsu
Province (No. BK20220127).
Figure 1. Hand appearance recovery from diverse degrada-tions. Compared with CycleGAN-based frameworks, we recover more bare hand appearance while preserving more semantics. quires a pose estimator [13, 47, 90] trained with laborious annotations. And the latter often produces artifacts be-cause it is hard to simulate photo-realistic lighting. An-other insight is to “translate” the degraded appearances as bare ones end-to-end. Nevertheless, it is tough to collect paired data for its training. Moreover, most unsupervised frameworks [56, 57, 91] are only feasible when the translat-ing target and source are consistent in structure, while our task needs to change those marker-related structures in the source. To this end, our key idea is to first disentangle the bare hand structure represented by a pixel-aligned map, and then wrap the appearance on this bare one trained with a dual adversarial discrimination (DAD) scheme.
There are two strategies to wrap the appearance from one image to another. (i) Template-based strategies learn [6, 63,
Figure 2. Structure disentanglement from monocular RGBs. (Row-1) Input images. (Row-2) Mesh recovery by a template-based strategy [90]. (Row-4) Structure prediction by our sketcher w/o/ bare structure prior. (Row-5) Structure disentanglement by our full sketcher. Red circles indicate the artifacts in the results. (Row-3) Structure prediction by a template-free strategy [76]. 72] or optimize [2, 55] sophisticated wrappings based on parametric instance templates [59, 62]. However, the accu-rate estimation of those parameters is heavily influenced by the degraded appearance in the images (See Fig. 2 Row-2). (ii) Template-free ones [40,73] excel at visible feature wrap-pings between structure-consistent images but are unable to selectively exclude marker-related features (See Fig. 2
Row-3 and Row-4). To address the problem, we first embed the bare hand structure prior into pixel-aligned maps. Then this prior is encoded as the token form [9], and a ViT [15] sketcher is trained to disentangle the corresponding struc-ture tokens from partial image patches [30]. Interestingly, this ViT sketcher satisfies S[S(X)] = S(X) [1], which means that when feeding its output as the input again, the two outputs should be consistent. We further utilize this elegant property to intensively train our sketcher in a semi-supervised paradigm.
Disappointingly, the recovered appearances remain un-satisfactory when a structure-assisted translator trained with existing adversarial paradigms: (i) In popular supervised paradigms [34, 76], the discriminator focuses on the qual-ity of the translation process. (ii) In most unsupervised paradigms [5, 52, 56], the discriminator can only evaluate the translation result since there is no reliable reference for the translation process. Based on these two, we innovate the DAD scheme under a semi-supervised paradigm, which enables dual discrimination (both on the process and re-sult) in our unpaired translation task. Initially, a partner do-main is synthesized by degrading hand regions of the bare one. It possesses pairwise mapping relationships with the bare target domain, as well as similarity to the degraded source domain. During the translator training, data from the source and the partner domain are fed to the transla-tor simultaneously. The two discriminators evaluate those translation processes and results with a clear division of la-bor. This scheme is more efficient than most unsupervised schemes [57, 91] because of those trustworthy pairs. It is more generalizable than a supervised scheme trained only with synthetic degradation [42,43,77] because of those mul-timodal inputs.
Our main contributions are summarized as follows.
• A semi-supervised framework that makes degraded im-ages in marker-based MoCap regain bare appearance;
• A powerful ViT sketcher that disentangles bare hand structure without parametric model dependencies;
• An adversarial scheme that promotes the degraded-to-bare appearance wrapping effectively.
The codes will be publicly available at https://www. yangangwang.com. 2.