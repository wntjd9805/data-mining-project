Abstract
Video captioning aims to describe the content of videos using natural language. Although significant progress has been made, there is still much room to improve the per-formance for real-world applications, mainly due to the long-tail words challenge. In this paper, we propose a text with knowledge graph augmented transformer (TextKG) for video captioning. Notably, TextKG is a two-stream trans-former, formed by the external stream and internal stream.
The external stream is designed to absorb additional knowl-edge, which models the interactions between the additional knowledge, e.g., pre-built knowledge graph, and the built-in information of videos, e.g., the salient object regions, speech transcripts, and video captions, to mitigate the long-tail words challenge. Meanwhile, the internal stream is de-signed to exploit the multi-modality information in videos (e.g., the appearance of video frames, speech transcripts, and video captions) to ensure the quality of caption results.
In addition, the cross attention mechanism is also used in between the two streams for sharing information. In this way, the two streams can help each other for more accurate results. Extensive experiments conducted on four challeng-ing video captioning datasets, i.e., YouCookII, ActivityNet
Captions, MSR-VTT, and MSVD, demonstrate that the pro-posed method performs favorably against the state-of-the-art methods. Specifically, the proposed TextKG method out-performs the best published results by improving 18.7% ab-solute CIDEr scores on the YouCookII dataset. 1.

Introduction
Video captioning aims to generate a complete and natu-ral sentence to describe video content, which attracts much
*Corresponding author:libo@iscas.ac.cn, Libo Zhang was supported by Youth Innovation Promotion Association, CAS (2020111).
This work was done during internship at ByteDance Inc. attention in recent years. Generally, most existing meth-ods [21, 38, 41, 58] require a large amount of paired video and description data for model training. Several datasets, such as YouCookII [69], and ActivityNet Captions [19] are constructed to promote the development of video caption-ing field. Meanwhile, some methods [29, 40, 48, 72] also use the large-scale narrated video dataset HowTo100M [30] to pretrain the captioning model to further improve the ac-curacy.
Although significant progress has been witnessed, it is still a challenge for video captioning methods to be applied in real applications, mainly due to the long-tail issues of words. Most existing methods [29, 40, 48, 72] attempt to design powerful neural networks, trained on the large-scale video-text datasets to make the network learn the relations between video appearances and descriptions. However, it is pretty tough for the networks to accurately predict the ob-jects, properties, or behaviors that are infrequently or never appearing in training data. Some methods [14, 71] attempt to use knowledge graph to exploit the relations between ob-jects for long-tail challenge in image or video captioning, which produces promising results.
In this paper, we present a text with knowledge graph augmented transformer (TextKG), which integrates addi-tional knowledge in knowledge graph and exploits the multi-modality information in videos to mitigate the long-tail words challenge. TextKG is a two-stream transformer, formed by the external stream and internal stream. The ex-ternal stream is used to absorb additional knowledge to help mitigate long-tail words challenge by modeling the interac-tions between the additional knowledge in pre-built knowl-edge graph, and the built-in information of videos, such as the salient object regions in each frame, speech transcripts, and video captions. Specifically, the information is first re-trieved from the pre-built knowledge graphs based on the detected salient objects. After that, we combine the features of the retrieved information, the appearance features of de-tected salient objects, the features of speech transcripts and
captions, then feed them into the external stream of Tex-tKG to model the interactions. The internal stream is de-signed to exploit the multi-modality information in videos, such as the appearance of video frames, speech transcripts and video captions, which can ensure the quality of cap-tion results. To share information between two streams, the cross attention mechanism is introduced. In this way, the two streams can obtain the required modal information from each other for generating more accurate results. The architecture of the proposed method is shown in Figure 1.
Several experiments conducted on four challenging datasets, i.e., YouCookII [69], ActivityNet Captions [19],
MSR-VTT [56], and MSVD [3] demonstrate that the pro-posed method performs favorably against the state-of-the-art methods. Notably, our TextKG method outperforms the best published results by improving 18.7% and 3.2% abso-lute CIDEr scores in the paragraph-level evalution mode on the YouCookII and Activity-Net Captions datasets. 2.