Abstract
Object discovery – separating objects from the back-ground without manual labels – is a fundamental open chal-lenge in computer vision. Previous methods struggle to go be-yond clustering of low-level cues, whether handcrafted (e.g., color, texture) or learned (e.g., from auto-encoders). In this work, we augment the auto-encoder representation learning framework with two key components: motion-guidance and mid-level feature tokenization. Although both have been separately investigated, we introduce a new transformer de-coder showing that their benefits can compound thanks to motion-guided vector quantization. We show that our ar-chitecture effectively leverages the synergy between motion and tokenization, improving upon the state of the art on both synthetic and real datasets. Our approach enables the emergence of interpretable object-specific mid-level features, demonstrating the benefits of motion-guidance (no labeling) and quantization (interpretability, memory efficiency). 1.

Introduction
Objects are central in human and computer vision. In the former, they are a fundamental primitive used to decompose the complexity of the visual world into an actionable repre-sentation. This abstraction in turn enables higher-level cogni-tive abilities, such as casual reasoning and planning [32, 54].
In computer vision, object detection has achieved remarkable progress [7, 45] and is now an essential component in many applications (e.g., driving, robotics). However, these models require a large amount of manual labels from a fixed vocab-ulary of categories. Consequently, learning unsupervised, object-centric representations is an important step towards scaling up computer vision to the real world.
This topic has received renewed attention recently thanks to structured generative networks with iterative inference over a fixed set of variables [6,16,22,38,39]. These methods cluster pixels in the feature space of an auto-encoder, exhibit-ing behavior similar to grouping based on low-level cues,
*Work done during an internship at TRI
Figure 1. TRI-PD dataset results: (left) top-10 foreground slot segments produced by our approach; (right) corresponding token representations. Compared to raw images, tokens in our framework present a more structured and compact space for reconstruction. such as color or texture. Hence, they are restricted to toy im-ages with colored geometric shapes on a plain background, and fail on more complex realistic scenes [4].
Two main types of works attempt to address this short-coming. The first family of methods sets out to simplify the grouping problem by introducing more structure into the out-put space, e.g., reconstructing optical flow [36] or depth [15].
They, however, require supervision, either in the form of known poses [36] or ground truth bounding boxes [15], veer-ing away from the unsupervised goal of object discovery.
In contrast, Bao et al. [4] resolve the object-background ambiguity by explicitly integrating an unsupervised motion segmentation algorithm [11] into the pipeline, showing sub-stantial progress on realistic scenes. The second main direc-tion to improve object discovery focuses on improving the decoder part of auto-encoding architectures [52, 53], replac-ing convolutional decoders with transformers [43, 60] com-bined with discrete variational auto-encoders (DVAE) [47] to reduce memory footprint. These more sophisticated architec-tures improve performance without additional supervision, including on real-world sequences. However, these methods are evaluated with different protocols (metrics, datasets) and therefore no clear architectural principles have emerged yet for unsupervised object discovery.
In this work, we introduce a novel architecture, Motion-guided Tokens (MoTok), based on the combination of two fundamental structural principles: motion and discretization.
We define objects as discrete entities that might have an in-dependent motion. As prior works have shown encouraging results thanks to unsupervised motion guidance and better
transformer-based decoders, we propose to leverage motion to guide tokenization, the vector quantization process at the heart of attention mechanisms in transformer architectures (See Figure 1). In addition, to comprehensively evaluate the contributions of prior works, we ablate key design choices proposed in the past, such as the decoder architecture and reconstruction space, in a unified framework.
Our key contributions are as follows. (1) We intro-duce a novel auto-encoder architecture, MoTok, for unsu-pervised video object discovery with a new transformer decoder leveraging unsupervised motion-guided tokeniza-tion. (2) Our results on real and synthetic datasets show that with sufficient capacity in the decoder, motion guid-ance alleviates the need for labels, optical flow, or depth decoding thanks to tokenization, improving upon the state of the art. (3) We show that our motion-guided tokens map to interpretable mid-level features, going beyond typ-ical clusters of low-level features, thus explaining why our method scales to challenging realistic videos. Our code, models, and synthetic data are made available at https://github.com/zpbao/MoTok/. 2.