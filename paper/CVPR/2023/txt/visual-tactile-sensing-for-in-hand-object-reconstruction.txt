Abstract
Tactile sensing is one of the modalities humans rely on heavily to perceive the world. Working with vision, this modality refines local geometry structure, measures defor-mation at the contact area, and indicates the hand-object contact state. With the availability of open-source tactile sensors such as DIGIT, research on visual-tactile learning is becoming more accessible and reproducible. Leverag-ing this tactile sensor, we propose a novel visual-tactile in-hand object reconstruction framework VTacO, and ex-tend it to VTacOH for hand-object reconstruction. Since our method can support both rigid and deformable ob-ject reconstruction, no existing benchmarks are proper for the goal. We propose a simulation environment, VT-Sim, which supports generating hand-object interaction for both rigid and deformable objects. With VT-Sim, we gener-ate a large-scale training dataset and evaluate our method on it. Extensive experiments demonstrate that our pro-posed method can outperform the previous baseline meth-ods qualitatively and quantitatively. Finally, we directly ap-ply our model trained in simulation to various real-world test cases, which display qualitative results. Codes, mod-els, simulation environment, and datasets are available at https://sites.google.com/view/vtaco/. 1.

Introduction
Human beings have a sense of object geometry by seeing and touching, especially when the object is in manipulation and undergoes a large portion of occlusion, where visual information is not enough for the details of object geom-etry. In such cases, vision-based tactile sensing is a good supplement as a way of proximal perception. In the past, few vision-based tactile sensors were commercially avail-able or open-source, so the visual-tactile sensing techniques could not be widely studied. Previous works [27, 34] on in-hand object reconstruction either studied rigid objects or were limited to simple objects with simple deformation.
* indicates equal contributions.
§ Cewu Lu is the corresponding author, the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, China and Shanghai Qi Zhi institute.
Vision-based tactile sensors [6, 15, 30, 33] can produce colorful tactile images indicating local geometry and defor-mation in the contact areas. In this work, we mainly work with DIGIT [15] as it is open-source for manufacture and is easier to reproduce sensing modality. With tactile images, we propose a novel Visual-Tactile in-hand Object recon-struction framework named VTacO. VTacO reconstructs the object geometry with the input of a partial point cloud observation and several tactile images. The tactile and ob-ject features are extracted by neural networks and fused in the Winding Number Field (WNF) [13], and the object shape is extracted by Marching Cubes algorithm [17]. WNF can represent the object shape with open and thin structures.
The poses of tactile sensors can be determined either by markers attached or by hand kinematics. By default, VTacO assumes the tactile sensor poses can be obtained indepen-dently, but we also discuss how to obtain the tactile sensor poses alongside the object with hand pose estimation. The corresponding method is named VTacOH.
With tactile information, we can enhance pure visual in-formation from three aspects: (1) Local geometry refine-ment. We use tactile sensing as proximal perception to complement details of local geometry. (2) Deformation at contact area. Objects, even those we consider rigid, can undergo considerable deformation given external forces ex-erted by hand. (3) Hand-object contact state. Tactile sen-sors indicate whether the hand is in contact with the object’s surface. To demonstrate such merits, we conduct the object reconstruction tasks in both rigid and non-rigid settings.
Since obtaining the ground truth of object deformation in the real world is hard, we first synthesize the training data from a simulator. DIGIT has an official simulation im-plementation, TACTO [29]. However, it is based on py-bullet [5], which has limited ability to simulate deformable objects. Thus, we implement a tactile simulation environ-ment VT-Sim in Unity. In VT-Sim, we generate hand poses with GraspIt! [18], and simulate the deformation around the contact area with an XPBD-based method. In the sim-ulation, we can easily obtain depth image, tactile image,
DIGIT pose, and object WNF as training samples for both rigid and non-rigid objects.
To evaluate the method, we compare the proposed visual-tactile models with its visual-only setting, and the previous baseline 3D Shape Reconstruction from Vision and Touch (3DVT) [23]. Extensive experiments show that our method can achieve both quantitative and qualitative improvements on baseline methods. Besides, since we make the tactile features fused with winding number predic-tion, we can procedurally gain finer geometry reconstruc-tion results by incrementally contacting different areas of objects. It can be useful for robotics applications [8, 22].
Then, we directly apply the model trained with synthesis data to the real world. It shows great generalization ability.
We summarize our contributions as follows:
• A visual-tactile learning framework to reconstruct an object when it is being manipulated. We provide the object-only version VTacO, and the hand-object ver-sion VTacOH.
• A simulation environment, VT-Sim, which can gener-ate training samples. We also validate the generaliza-tion ability of the models trained on the simulated data to the real-world data. 2.