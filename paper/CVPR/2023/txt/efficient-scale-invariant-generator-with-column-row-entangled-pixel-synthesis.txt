Abstract
Any-scale image synthesis offers an efficient and scal-able solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, exist-ing GAN-based solutions depend excessively on convolu-tions and a hierarchical architecture, which introduce in-consistency and the “texture sticking” issue when scal-ing the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these net-works from being adopted in large-scale or real-time sys-tems.
In this work, we propose Column-Row Entangled
Pixel Synthesis (CREPS), a new generative model that is both efficient and scale-equivariant without using any spa-tial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate “thick” column and row encodings. Ex-periments on various datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery, confirm CREPS’ ability to synthesize scale-consistent and alias-free images at any arbitrary resolution with proper training and infer-ence speed. Code is available at https://github. com/VinAIResearch/CREPS. 1.

Introduction
Generative Adversarial Networks (GANs) [8] are one of the most widely used structures for image generation and manipulation [2, 28]. Previously, a GAN model could only generate images with a fixed scale and layout as de-fined in the training dataset. However, natural images come with varying resolutions and contain unstructured objects at diverse poses. Therefore, designing a generative model that can handle more flexible geometric configurations is gaining more attention in the machine-learning community.
StyleGAN3 [13] already supports out-of-the-box transla-tion and rotation with consistent and artifact-free outputs.
*Equal contribution.
Figure 1. Previous any-scale image synthesis networks, including
AnyresGAN [4] and ScaleParty [18], produce inconsistent image details when changing the output scale (see zoomed-in patches).
In contrast, our proposed network can produce the same details but sharper when increasing the scale. Check the supplemental video for a clearer comparison.
Any-scale synthesis, however, remains under-explored.
In this paper, we are interested in the task of arbitrary-scale image synthesis where a single generator can effort-lessly synthesize images at many different scales while strongly preserving detail consistency. Such a model can be a promising research direction and bring many benefits. It enables synthesizing a high-resolution image from a lower-resolution training dataset. Hence, it eliminates the need for collecting and training models on high-resolution images, which is costly in storage, time, and computation resources.
The output resolution can be ultra-high, e.g., 2048 × 2048, which is impossible for standard GAN models due to the limit of GPU memory. Any-scale image synthesis also al-lows geometric interactions like zooming in and out. De-spite promising results, previous works on this topic, such as AnyresGAN [4] and ScaleParty [18], show strong incon-sistency when scaling the output resolution (see Fig. 1).
We investigate the GAN structures to find the poten-tial cause of the inconsistency at image scaling. Tradi-tional GAN models are based on convolutional generators
[3,15,20], which introduce an implicit spatial bias that helps the model to produce high-quality images. Recently, Xu et al. [29] and Karras et al. [13] discovered that these posi-tional priors can hamper the model’s consistency when ap-plying translations, rotations, or scalings. In order to com-bat this issue, many works introduce non-trivial changes such as sophisticated architecture re-design [13] or opt for a better training strategy and input positional encoding [4,18].
However, these are only partial remedies as the output pix-els still depend on their surroundings, making it impossible for these models to produce consistent attributes of an ob-ject regardless of positions and scales.
In contrast to the traditional GANs, some recent meth-ods are based on Implicit Neural Representation (INR)
[1, 23]. By predicting the color of each pixel separately,
INR-based GANs can, in theory, synthesize objects in a spatial-arbitrary manner and still achieve comparable qual-ity at small to medium resolution compared to convolution-based approach. However, these models’ memory usage grows quadratically with the input resolution since all pixels have to be queried. Thus, there has been no existing work that can efficiently scale INR-GANs to resolutions higher than 1024. To reduce training complexity, Anokhin et al. [1] employs a simple patch-based strategy where only a portion of pixels is generated and passed through the discriminator at a time. However, this approach unsurprisingly leads to poor results and inconsistency between patches.
Inspired by the latter approach, we aim to tackle the task of scale-consistent image generation with essential changes to StyleGAN2 [15]. Similar to Anokhin et al. [1], we change the 3×3 convolutions to 1×1 ones and add a Fourier feature embedding [25] at the input layer. Although these two changes alone already achieve our goal, it is still expen-sive to train in high-resolution settings. Thus, instead of us-ing dense 2D features, our model relies on a novel thick bi-line representation, which largely reduces the training and inference complexity by using two low-rank features for row and column. Our network first regresses these row and column embeddings, then composes layer-wise intermedi-ate 2D features, and finally fuses these maps to produce the final output. We name this novel structure Column-Row
Entangled Pixel Synthesis, or CREPS for short.
We run a series of experiments on four datasets, includ-ing FFHQ, MetFaces, LSUN-Church, and Flickr-Scenery, to confirm the effectiveness of our proposed CREPS struc-ture. Our model can synthesize images with quality compa-rable to the previous generative models like CIPS or Style-GAN2. While CIPS has trouble in training on images of res-olutions more than 256 × 256, CREPS can sufficiently han-dle training data at resolutions 512 × 512 and 1024 × 1024.
CREPS produces scale-equivariant images and keeps the object details unchanged when scaling the output resolu-tion, unlike previous any-scale GANs such as AnyresGAN and ScaleParty. Using a CREPS model trained on 512×512 images, we still can generate near-realistic images at higher resolution. Finally, we demonstrate CREPS’s ability to syn-thesize images with complex geometric transformations and distortions while preserving attribute consistency.
To summarize our contributions:
• We propose a simple and elegant network equipped with only modulated linear layers and no upsampling layers in-between. It supports scale-consistent outputs for any-scale image synthesis.
• To further improve efficiency, we introduce a thick bi-line representation, which decomposes 2D network features into two light-weight row and column embed-dings. It significantly saves memory and computation costs compared with the full 2D-feature counterparts.
• We demonstrate competitive results for unconditional image synthesis on the FFHQ, LSUN-Church, Met-Faces, and Flickr-Scenery datasets, along with the abil-ity to generate each image at arbitrary scales with con-sistent details.
• Our CREPS models support complex geometric trans-formations and distortions. 2.