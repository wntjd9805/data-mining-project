Abstract
Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present
SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and differ-ent types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces.
As for the head pose, we design PoseVAE via a conditional
VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the un-supervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experi-ments to demonstrate the superiority of our method in terms of motion and video quality. 1
* Equal Contribution
† Corresponding Author 1The code and demo videos are available at https://sadtalker. github.io.
1.

Introduction
Animating a static portrait image with speech audio is a challenging task and has many important applications in the fields of digital human creation, video conferences, etc. Previous works mainly focus on generating lip mo-tion [2, 3, 28, 29, 49] since it has a strong connection with speech. Recent works also aim to generate a realistic talking face video containing other related motions, e.g., head pose.
Their methods mainly introduce 2D motion fields by land-marks [50] and latent warping [37, 38]. However, the quality of the generated videos is still unnatural and restricted by the preference pose [16, 49], month blur [28], identity modi-fication [37, 38], and distorted face [37, 38, 47].
Generating a natural-looking talking head video contains many challenges since the connections between audio and different motions are different. i.e., the lip movement has the strongest connection with audio, but audio can be talked via different head poses and eye blink. Thus, previous facial landmark-based methods [2, 50] and 2D flow-based audio to expression networks [37, 38] may generate the distorted face since the head motion and expression are not fully disentan-gled in their representation. Another popular type of method is the latent-based face animation [3, 16, 28, 49]. Their meth-ods mainly focus on the specific kind of motions in talking face animation and struggle to synthesize high-quality video.
Our observation is that the 3D facial model contains a highly decoupled representation and can be used to learn each type of motion individually. Although a similar observation has been discussed in [47], their methods also generate inaccu-rate expressions and unnatural motion sequences.
From the above observation, we propose SadTalker, a
Stylized Audio-Driven Talking-head video generation sys-tem through implicit 3D coefficient modulation. To achieve this goal, we consider the motion coefficients of the 3DMM as the intermediate representation and divide our task into two major components. On the one hand, we aim to generate the realistic motion coefficients (e.g., head pose, lip motion, and eye blink) from audio and learn each motion individu-ally to reduce the uncertainty. For expression, we design a novel audio to expression coefficient network by distilling the coefficients from the lip motion only coefficients from
[28] and the perceptual losses (lip-reading loss [1], facial landmark loss) on the reconstructed rendered 3D face [5].
For the stylized head pose, a conditional VAE [6] is used to model the diversity and life-like head motion by learning the residual of the given pose. After generating the realistic 3DMM coefficients, we drive the source image through a novel 3D-aware face render. Inspired by face-vid2vid [40], we learn a mapping between the explicit 3DMM coefficients and the domain of the unsupervised 3D keypoint. Then, the warping fields are generated through the unsupervised 3D keypoints of source and driving and it warps the reference im-age to generate the final videos. We train each sub-network of expression generation, head poses generation and face renderer individually and our system can be inferred in an end-to-end style. As for the experiments, several metrics show the advantage of our method in terms of video and motion methods.
The main contribution of this paper can be summarized as:
• We present SadTalker, a novel system for a stylized audio-driven single image talking face animation using the generated realistic 3D motion coefficients.
• To learn the realistic 3D motion coefficient of the 3DMM model from audio, ExpNet and PoseVAE are presented individually.
• A novel semantic-disentangled and 3D-aware face ren-der is proposed to produce a realistic talking head video.
• Experiments show that our method achieves state-of-the-art performance in terms of motion synchronization and video quality. 2.