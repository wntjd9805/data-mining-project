Abstract
Generating synthetic datasets for training face recogni-tion models is challenging because dataset generation en-tails more than creating high fidelity images.
It involves generating multiple images of same subjects under different factors (e.g., variations in pose, illumination, expression, aging and occlusion) which follows the real image condi-tional distribution. Previous works have studied the gener-ation of synthetic datasets using GAN or 3D models. In this work, we approach the problem from the aspect of combin-ing subject appearance (ID) and external factor (style) con-ditions. These two conditions provide a direct way to con-trol the inter-class and intra-class variations. To this end, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our novel Patch-wise style ex-tractor and Time-step dependent ID loss enables DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition mod-els trained on synthetic images from the proposed DCFace provide higher verification accuracies compared to previ-ous works by 6.11% on average in 4 out of 5 test datasets,
LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code Link 1.

Introduction
What does it take to create a good training dataset for visual recognition? An ideal training dataset for recogni-tion tasks would have 1) large inter-class variation, 2) large intra-class variation and 3) small label noise. In the context of face recognition (FR), it means, the dataset has a large number of unique subjects, large intra-subject variations, and reliable subject labels. For instance, large-scale face datasets such as WebFace4M [73] contain over 1M subjects and large number of images/subject. Both the number of subjects and the number of images per subject are impor-tant for training FR models [11,31]. Also, datasets amassed by crawling the web are not free from label noise [7, 73].
In various domains, synthetic datasets are traditionally
Figure 1. Illustration of three factors that characterize a labeled face dataset. It contains large subject variation, style variation and label consistency. Synthetic face datasets should be created with all three factors in mind. Face images in this figure are samples generated by our proposed method which combines arbitrary ID condition with style condition while preserving subject identity. used to help generalize deep models when only limited real datasets could be collected [13, 21, 63, 74] or when bias ex-ists in the real dataset [34, 64]. Lately, more attention has been drawn to training with only synthetic datasets in the face domain, as synthetic data can avoid leaking the privacy of real individuals. This is important as real face datasets have been under scrutiny for their lack of informed consent, as web-crawling is the primary means of large-scale data collection [17, 22, 73]. Also, synthetic training datasets can remedy some long-standing issues in real datasets, e.g. the long tail distribution, demographic bias, etc.
When it comes to generating synthetic training datasets, (i) How many the following questions should be raised. novel subjects can be synthesized (ii) How well can we mimic the distribution of real images in the target domain and (iii) How well can we consistently generate multiple images of the same subjects? We start with the hypothesis that face dataset generation can be formulated as a problem that maximizes these criteria together.
Previous efforts in generating synthetic face datasets touch on one of the three aspects but do not consider all of them together [3, 49]. SynFace [49] generates high-fidelity face images based on DiscoFaceGAN [12], coming close to real images in terms of FID metric [18]. However, we were
Figure 2. Two stage dataset generation paradigm. In the sampling stage, 1) Gid generates a high-quality face image Xid that defines how a person looks and 2) the style bank selects a style image Xsty that defines the overall style of the final image. The mixing stage generates image with identity from Xid and style from Xsty. Repeating this process multiple times, one can generate a labeled synthetic face dataset. surprised to find that the actual number of unique subjects that can be generated by DiscoFaceGAN is less than 500, a finding that will be discussed in Sec. 3.1. The recent state of the art (SoTA), DigiFace [3], can generate 1M large-scale synthetic face images with many unique subjects based on 3D parametric model rendering. However, it falls short in matching the quality and style of real face images.
We propose a new data generation scheme that addresses all three criteria, i.e. the large number of novel subjects (uniqueness), real dataset style matching (diversity) and la-bel consistency (consistency). In Fig. 1, we illustrate the high-level idea by showcasing some of our generated face samples. The key motivation of our paper is that the syn-thetic dataset generator needs to control the number of unique subjects, match the training dataset’s style distribu-tion and be consistent in the subject label.
In light of this, we formulate the face image generation as a dual condition inverse problem, retrieving the unknown image Y from the observable Identity condition Xid and
Style condition Xsty. Specifically, Xid specifies how a per-son looks and Xsty specifies how Xid should be portrayed in an image. Xsty contains identity-independent informa-tion such as pose, expression, and image quality.
Our choice of dual conditions (identity and style) is im-portant in how we generate a synthetic dataset as ID and style conditions are controllable factors that govern the dataset’s characteristics. To achieve this, we propose a two-stage generation paradigm. First, we generate a high-quality face image Xid using a face image generator and sample a style image Xsty from a style bank. Secondly, we mix these two conditions using a dual condition generator which predicts an image that has the ID of Xid and a style of Xsty. An illustration is given in Fig. 2. id, XB
Training the mixing generator in stage 2 is not trivial as it would require a triplet of (XA sty) where XA sty is a hypothetical combination of the ID of subject A and the style of subject B. To solve this problem, we propose a new dual condition generator that can learn from (XA sty), a tuple of same subject images that can always be obtained in a labeled dataset. The novelty lies in our style condi-sty, XA id, XA tion extractor and ID loss which prevents the training from falling into a degenerate solution. We modify the diffusion model [19, 55] to take in dual conditions and apply an aux-iliary time-dependent ID loss that can control the balance between sample diversity and label consistency.
We show that our Dual Condition Face Dataset Gener-ator (DCFace) is capable of surpassing the previous meth-ods in terms of FR performance, establishing a new bench-mark in face recognition with synthetic face datasets. We also show the roles dataset subject uniqueness, diversity and consistency play in face recognition performance.
The followings are the contributions of the paper.
• We propose a two-stage face dataset generator that controls subject uniqueness, diversity and consistency.
• For this, we propose a dual condition generator that mixes the two independent conditions Xid and Xsty.
• We propose uniqueness, consistency and diversity met-rics that quantify the respective properties of a given dataset, useful measures that allow one to compare datasets apart from the recognition performance.
• We achieve SoTA in FR with 0.5M image synthetic training dataset by surpassing the previous methods by 6.11% on average in 5 popular test datasets. 2.