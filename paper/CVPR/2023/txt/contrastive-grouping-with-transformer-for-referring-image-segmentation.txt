Abstract
Referring image segmentation aims to segment the tar-get referent in an image conditioning on a natural language expression. Existing one-stage methods employ per-pixel classification frameworks, which attempt straightforwardly to align vision and language at the pixel level, thus failing to capture critical object-level information. In this paper, we propose a mask classification framework, Contrastive
Grouping with Transformer network (CGFormer), which explicitly captures object-level information via token-based querying and grouping strategy. Specifically, CGFormer first introduces learnable query tokens to represent objects and then alternately queries linguistic features and groups visual features into the query tokens for object-aware cross-modal reasoning. In addition, CGFormer achieves cross-level interaction by jointly updating the query tokens and decoding masks in every two consecutive layers. Finally,
CGFormer cooperates contrastive learning to the grouping strategy to identify the token and its mask corresponding to the referent. Experimental results demonstrate that CG-Former outperforms state-of-the-art methods in both seg-mentation and generalization settings consistently and sig-nificantly. Code is available at https://github.com/
Toneyaya/CGFormer. 1.

Introduction
Referring Image Segmentation (RIS) aims to segment the target referent in an image given a natural language ex-pression [12, 17, 64]. It attracts increasing attention in the research community and is expected to show its potential in real applications, such as human-robot interaction via natural language [50] and image editing [3]. Compared to classical image segmentation that classifies pixels or masks into a closed set of fixed categories, RIS requires locat-ing referent at pixel level according to the free-form nat-ural languages with open-world vocabularies. It faces chal-†Sibei Yang is the corresponding author.
Figure 1. Comparison of Transformer-based RIS methods and our
CGFormer. (a) CRIS [51] and ReSTR [25] fuse relevant linguistic features into visual features, while (b) VLT [12] generates query vectors to query visual features for segmentation. In contrast, (c) our CGFormer introduces learnable query tokens and explicitly groups visual features into tokens conditioning on language. Co-operating the grouping strategy with contrastive learning, it iden-tifies the token and its mask corresponding to the referent. lenges of comprehensively understanding vision and lan-guage modalities and aligning them at pixel level.
Existing works mainly follow the segmentation frame-work of per-pixel classification [17, 34] integrated with multi-modal fusion to address the challenges. They intro-duce various fusion methods [19,20,26,32,46,63] to obtain vision-language feature maps and predict the segmentation results based on the feature maps. Recently, the improve-ment of vision-language fusion for RIS mainly lies in utiliz-ing the Transformer [12, 25, 51, 62]. LAVT [62] integrates fusion module into the Transformer-based visual encoder.
CRIS [51] and ReSTR [25] fuse linguistic features into each feature of the visual feature maps, as shown in Figure 1a.
In contrast, VLT [12] integrates the relevant visual features into language-conditional query vectors via transformer de-coder, as shown in Figure 1b.
Although these methods have improved the segmenta-tion accuracy, they still face several intrinsic limitations.
First, the works [25,51,62] based on pixel-level fusion only model the pixel-level dependencies for each visual feature, which fails to capture the crucial object/region-level infor-mation. Therefore, they cannot accurately ground expres-sions that require efficient cross-modal reasoning on ob-jects. Second, although VLT [12]’s query vectors contain object-level information after querying, it directly weights and reshapes different tokens into one multi-modal feature map for decoding the final segmentation mask. Therefore, it loses the image’s crucial spatial priors (relative spatial ar-rangement among pixels) in the reshaping process. More importantly, it does not model the inherent differences be-tween query vectors, resulting in that even though differ-ent query vectors comprehend expressions in their own way, they still focus on similar regions, but fail to focus on dif-ferent regions and model their relations.
In this paper, we aim to propose a simple and effective framework to address these limitations.
Instead of using per-pixel classification framework, we adopt an end-to-end mask classification framework [1, 16] (see Figure 1c) to explicitly capture object-level information and decode seg-mentation masks for both the referent and other disturbing objects/stuffs. Therefore, we can simplify RIS task by find-ing the corresponding mask for the expression. Note that our framework differs from two-stage RIS methods [53, 64] which require explicitly detecting the objects first and then predicting the mask in the detected bounding boxes.
Specifically, we propose a Contrastive Grouping with
Transformer (CGFormer) network consisting of the Group
Transformer and Consecutive Decoder modules.
The
Group Transformer aims to capture object-level informa-tion and achieve object-aware cross-modal reasoning. The success of applying query tokens in object detection [1, 69] and instance segmentation [8, 16, 54] could be a potential solution. However, it is non-trivial to apply them to RIS.
Without the annotation supervision of other mentioned ob-jects other than the referent, it is hard to make tokens pay attention to different objects and distinguish the token cor-responding to the referent from other tokens. Therefore, although we also specify query tokens as object-level in-formation representations, we explicitly group the visual feature map’s visual features into query tokens to ensure that different tokens focus on different visual regions with-out overlaps. Besides, we can further cooperate contrastive learning with the grouping strategy to make the referent to-ken attend to the referent-relevant information while forcing other tokens to focus on different objects and background regions, as shown in Figure 1c. In addition, we alternately query the linguistic features and group the visual features into the query tokens for cross-modal reasoning.
Furthermore, integrating and utilizing multi-level fea-ture maps are crucial for accurate segmentation. Previous works [32, 40, 63] fuse visual and linguistic features at mul-tiple levels in parallel and late integrate them via ConvL-STM [47] or FPNs [30]. However, their fusion modules are solely responsible for cross-modal alignment at each level, which fails to perform joint reasoning for multiple levels.
Therefore, we propose a Consecutive Decoder that jointly updates query tokens and decodes masks in every two con-secutive layers to achieve cross-level reasoning.
To evaluate the effectiveness of CGFormer, we conduct experiments on three standard benchmarks, i.e., RefCOCO series datasets [39,41,65]. In addition, unlike semantic seg-mentation, RIS is not limited by the close-set classification but to open-vocabulary alignment. It is necessary to evalu-ate the generalization ability of RIS models. Therefore, we introduce new subsets of training sets on the three datasets to ensure the categories of referents in the test set are not seen in the training stage, inspired by the zero-shot visual grounding [44] and open-set object detection [67].
In summary, our main contributions are as follows,
• We propose a Group Transformer cooperated with con-trastive learning to achieve object-aware cross-modal reasoning by explicitly grouping visual features into different regions and modeling their dependencies con-ditioning on linguistic features.
• We propose a Consecutive Decoder to achieve cross-level reasoning and segmentation by jointly perform-ing the cross-modal inference and mask decoding in every two consecutive layers in the decoder.
• We are the first to introduce an end-to-end mask clas-sification framework, the Contrastive Grouping with
Transformer (CGFormer), for referring image segmen-tation. Experimental results demonstrate that our CG-Former outperforms all state-of-the-art methods on all three benchmarks consistently.
• We introduce new splits on datasets for evaluating generalization for referring image segmentation mod-els. CGFormer shows stronger generalizability com-pared to state-of-the-art methods thanks to object-aware cross-modal reasoning via contrastive learning. 2.