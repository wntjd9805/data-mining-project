Abstract
Masked Modeling (MM) has demonstrated widespread success in various vision challenges, by reconstructing masked visual patches. Yet, applying MM for large-scale 3D scenes remains an open problem due to the data spar-sity and scene complexity. The conventional random mask-ing paradigm used in 2D images often causes a high risk of ambiguity when recovering the masked region of 3D scenes.
To this end, we propose a novel informative-preserved re-construction, which explores local statistics to discover and preserve the representative structured points, effectively en-hancing the pretext masking task for 3D scene understand-ing.
Integrated with a progressive reconstruction man-ner, our method can concentrate on modeling regional ge-ometry and enjoy less ambiguity for masked reconstruc-tion. Besides, such scenes with progressive masking ra-* Equal contribution.
† Corresponding authors. tios can also serve to self-distill their intrinsic spatial con-sistency, requiring to learn the consistent representations from unmasked areas. By elegantly combining informative-preserved reconstruction on masked areas and consistency self-distillation from unmasked areas, a unified framework called MM-3DScene is yielded. We conduct comprehensive experiments on a host of downstream tasks. The consistent improvement (e.g., +6.1% mAP@0.5 on object detection and +2.2% mIoU on semantic segmentation) demonstrates the superiority of our approach. 1.

Introduction 3D scene understanding plays an essential role in vari-ous visual applications, such as virtual reality, robot naviga-tion, and autonomous driving. Over the past few years, deep learning has dominated 3D scene parsing tasks [26, 40, 73].
However, traditional supervised learning methods require massive annotation of 3D scene data that are extremely la-borious to obtain [9], where millions of points or mesh ver-tices per scene need to be labeled.
Datasets
S3DIS
Complexity
Task
Gain (from scratch)
Entire floor, office segmentation (+1.5%) mIoU
To solve this, self-supervised learning (SSL) becomes a favorable choice since it can extract rich representa-tions without any annotation [10, 16, 17]. Masked Mod-eling (MM) [16, 57], as one of the representative meth-ods in SSL, recently draws significant attention in the vi-sion community. Recently, It has been explored in 3D vi-sion [31,39,51,67,69,70], where these 3D MM approaches randomly mask local regions of point clouds, and pre-train neural networks to reconstruct the masked areas. Neverthe-less, such random masking paradigms are not feasible for large-scale 3D scenes, which often causes a high risk of re-construction ambiguity. As illustrated in Fig. 1 (a), a chair and a TV are totally masked, which are extremely difficult to be recovered without any context guidance. Such ambi-guity often makes MM difficult to learn informative repre-sentation for 3D scenes. Hence, we ask a natural question: can we customize a better way of masked modeling for 3D scene understanding?
To tackle this question, we propose a novel informative-preserved masked reconstruction scheme in this paper.
Specifically, we leverage local statistics of each point (i.e., the difference between each point and its neighboring points in terms of color and shape) as guidance to discover the rep-resentative structured points which are usually located at the boundary regions in the 3D scene. We denote these points as ‘Informative Points’ since they provide highly useful information hints and rich semantic context for assisting masked reconstruction. To this end, our mask strategy is definite: to preserve Informative Points in a scene and mask other points. In this way, the basic geometric information of a scene is explicitly retained, which effectively simpli-fies the pretext task and reduces ambiguity.
Based on our mask strategy, a progressive masked recon-struction manner is integrated, to better model the masked areas. As illustrated in Fig. 1 (b), during each iteration, our method concentrates on reconstructing the local regional geometric patterns rather than rebuilding the original intact scene. In doing so, it enjoys less ambiguity and is able to restore accurate geometric information.
Moreover, we realize the information of unmasked ar-eas (i.e., Informative Points) is underexplored. We find that there exists point correspondence in the unmasked ar-eas under progressive masking ratios. Accordingly, we in-troduce a dual-branch encoding scheme for learning such intrinsic consistency, with the ultimate goal of unearthing the consistent (i.e., masking-invariant) representations from unmasked areas. This leads to a more powerful SSL frame-work on 3D scenes, called MM-3DScene, which elegantly combines the masked modeling on the masked and un-masked areas in 3D scenes together, while complements each other. It achieves superior performance in Table 7 (v).
ScanNet
Large rooms
SUN-RGBD Cluttered rooms segmentation (+2.2%) mIoU detection detection (+4.4) mAP@0.25 (+6.1) mAP@0.5 detection detection (+2.9) mAP@0.25 (+4.4) mAP@0.5
Table 1. Summary of fine-tuning MM-3DScene on various downstream tasks and datasets for 3D understanding. Our MM-3DScene conspicuously boosts the performance of the baseline model trained from scratch.
Our contributions are motivated and comprehensive:
• We raise the concept of Informative Points – the points providing significant information hints, and indicate that preserving them is critical for assisting masked modeling on 3D scenes (Table 8).
• For masked areas, we propose an informative-preserved reconstruction scheme to focus on restoring the regional geometry in a novel progressive manner, which explicitly simplifies the pretext task.
• For unmasked areas, we introduce a self-distillation branch, which is encouraged to learn spatial-consistent representations under progressive masking ratios.
• A unified self-supervised framework, called MM-3DScene, delivers performance improvements across on various downstream tasks and datasets (Table 1). 2.