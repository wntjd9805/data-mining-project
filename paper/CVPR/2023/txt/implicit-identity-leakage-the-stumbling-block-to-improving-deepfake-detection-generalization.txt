Abstract
In this paper, we analyse the generalization ability of bi-nary classifiers for the task of deepfake detection. We find that the stumbling block to their generalization is caused by the unexpected learned identity representation on images.
Termed as the Implicit Identity Leakage, this phenomenon has been qualitatively and quantitatively verified among various DNNs. Furthermore, based on such understand-ing, we propose a simple yet effective method named the ID-unaware Deepfake Detection Model to reduce the influence of this phenomenon. Extensive experimental results demon-strate that our method outperforms the state-of-the-art in both in-dataset and cross-dataset evaluation. The code is available at https://github.com/megvii-research/CADDM. 1.

Introduction
Recently, face-swap abusers use different face manip-ulation methods [17, 29, 29, 31, 65] to generate fake im-ages/videos. Those images/videos are then used to spread fake news, make malicious hoaxes, and forge judicial ev-idence, which have caused severe consequences. In order to alleviate such situations, an increasing number of deep-fake detection methods [12,14,42,59,60,63] have been pro-posed to filter out manipulated images/videos from massive online media resources, ensuring the filtered images/videos are genuine and reliable.
Previous methods usually dealt with the task of deep-fake detection with binary classifiers [1,2,10,46,51]. These methods have achieved great detection accuracy in detect-ing the seen attacks learned in the training datasets (i.e. the in-dataset evaluations). However, when confronted with media generated from newly-proposed deepfake methods (i.e. the cross-dataset evaluations), these methods often suf-fered from significant performance drops. Though plenty of researchers have designed effective methods [32, 74, 75] to
*Equal contribution
†Corresponding author
Figure 1. The Implicit Identity Leakage phenomenon. Since the fake image retains some features of its source image, its identity should not be completely regarded as its target image. As a conse-quence, there exists an implicit gap between genuine identities and fake identities in the training set, which is unintentionally captured by binary classifiers. When confronted with images manipulated by unseen face-swap methods, the classifier tends to misuse iden-tity information and make false predictions. improve the generalization of deepfake detection models, it still lacks a thorough analysis of why binary classifiers fail to perform well on the cross-dataset evaluation.
In this paper, given well-trained binary classifiers of deepfake detection, we find that the stumbling block for their generalization ability is caused by the mistakenly learned identity representation on images. As shown in Fig. 1 (a), a deepfake image is usually generated by replacing the face of the source image with the face of the target im-age. However, we notice that the procedure of synthesizing the fake image [5, 17, 29] may cause the information loss of ID representations. The identity of the fake image can not be considered as the same as either its target image or its source image. In particular, when the face of the target image is swapped back with the face of the fake image, it is noticeable that the identity of the target image is altered.
In this way, as shown in Fig. 1 (b), when learning a
deepfake detection model, there exists an implicit decision boundary between fake images and genuine images based on identities. During the training phase, binary classifiers may accidentally consider certain groups of identities as genuine identities and other groups of identities as fake identities. When tested on the cross-dataset evaluation, such biased representations may be mistakenly used by binary classifiers, causing false judgments based on the facial ap-pearance of images.
In this paper, we have qualitatively and quantitatively verified this phenomenon (termed as the
Implicit Identity Leakage) in binary classifiers of various backbones. Please see Sec. 3 and Sec. 5.2 for analyses.
Furthermore, based on such understanding, we propose a simple yet effective method named the ID-unaware Deep-fake Detection Model to reduce the influence of Implicit
Identity Leakage. Intuitively, by forcing models to only fo-cus on local areas of images, less attention will be paid to the global identity information. Therefore, we design an anchor-based detector module termed as the Artifact Detec-tion Module to guide our model to focus on the local artifact areas. Such a module is expected to detect artifact areas on images with multi-scale anchors, each of which is assigned a binary label to indicate whether the artifact exists. By lo-calizing artifact areas and classifying multi-scale anchors, our model learns to distinguish the differences between lo-cal artifact areas and local genuine areas at a finer level, thus reducing the misusage of the global identity information.
Extensive experimental results show that our model accurately predicted the position of artifact areas and learned generalized artifact features in face manipulation algorithms, successfully outperforming the state-of-the-art.
Contributions of the paper are summarized as follows:
• We discover that deepfake detection models super-vised only by binary labels are very sensitive to the identity information of the images, which is termed as the Implicit Identity Leakage in this paper.
• We propose a simple yet effective method termed as the ID-unaware Deepfake Detection Model to reduce the influence of the ID representation, successfully outperforming other state-of-the-art methods.
• We conduct extensive experiments to verify the Im-plicit Identity Leakage phenomenon and demonstrate the effectiveness of our method. 2.