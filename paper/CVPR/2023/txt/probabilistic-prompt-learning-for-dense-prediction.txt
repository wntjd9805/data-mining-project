Abstract
Recent progress in deterministic prompt learning has be-come a promising alternative to various downstream vision tasks, enabling models to learn powerful visual representa-tions with the help of pre-trained vision-language models.
However, this approach results in limited performance for dense prediction tasks that require handling more complex and diverse objects, since a single and deterministic description cannot sufficiently represent the entire image.
In this paper, we present a novel probabilistic prompt learning to fully exploit the vision-language knowledge in dense prediction tasks. First, we introduce learnable class-agnostic attribute prompts to describe universal attributes across the object class.
The attributes are combined with class information and visual-context knowl-edge to define the class-specific textual distribution. Text representations are sampled and used to guide the dense prediction task using the probabilistic pixel-text matching loss, enhancing the stability and generalization capability of the proposed method. Extensive experiments on different dense prediction tasks and ablation studies demonstrate the effectiveness of our proposed method. 1.

Introduction
Dense predictions, e.g., semantic segmentation [3, 31], instance segmentation [30], and object detection [11, 42], are fundamental computer vision problems, which aim to produce pixel-level predictions to thoroughly understand the scene. Due to the expensive cost of collecting dense an-notations, most approaches [10, 38] employ a “pre-training
+ fine-tuning” paradigm. Based on existing pre-trained net-works [8, 15] trained on large-scale datasets such as Ima-geNet [6], semi- [36,60] or self-supervised learning [29,53] has been extensively researched to fine-tune additional
*Corresponding author.
This research was supported by the National Research Founda-tion of Korea (NRF) grant funded by the Korea government (MSIP) (NRF2021R1A2C2006703).
Figure 1. Probabilistic Prompt Learning. The proposed PPL exploits N multiple prompts sampled from probabilistic text em-beddings, which can leverage granular textual representations, en-abling a better understanding of the details of the image. modules for dense prediction. However, due to the biased visual representations, they suffer from a lack of scalability when there exists a large semantic gap between pre-trained and target tasks w.r.t. dataset and objective, such as transfer-ring ImageNet classification network to COCO object de-tection [13, 61].
Recently, applying vision-language pre-trained models (VLM) to the downstream tasks has demonstrated remark-able success, including zero-shot classification [58, 59], referring expression segmentation [45], and object detec-tion [41]. VLM, such as CLIP [40] and ALIGN [21], is trained on large-scale web noisy image-text pair datasets via contrastive learning to align representations between text and image in a joint embedding space.
In this way,
VLM learns robust visual representations by exploiting the semantic relationship between text and image representa-tions, which is beneficial to transfer knowledge to various
vision tasks. To efficiently leverage the language knowl-edge, there have been many pioneering attempts [16,22,46] to deploy VLM to the downstream tasks via prompting.
For example, based on a prompt template "a photo of a {class}", it measures the confidence score by calcu-lating image-text similarity and classifies the image into
{class}. In practice, however, this hand-crafted prompt may not be the optimal description for a particular task, and furthermore, manually designing a task-specific prompt is laborious.
To tackle this issue, several methods [41, 58, 59] have in-troduced learning-based prompting techniques inspired by early works in NLP [18, 28]. The goal is to automatically construct the prompts according to the task by optimiz-ing continuous prompt variables based on VLM. This sim-ple and intuitive setup, referred to as deterministic prompt learning, is the most popular approach in the current liter-It has shown performance improvement on ature [9, 46]. classification tasks [40, 52], where a single deterministic embedding is sufficient for representing a class. However, this approach is not fully compositional in dense prediction tasks due to a semantic ambiguity problem. Firstly, while the dense prediction tasks require granular information to generate precise pixel-wise results, not only complex and multiple objects within an input image but also their various attributes (e.g., color, location, etc.) cannot be comprehen-sively represented in a single textual representation [12,49].
Thus, a single prompt fails to comprehend the object of all classes in detail. Second, the visual representation has high randomness [5, 7] due to various contexts with external ob-jects and object representations, and it results in high uncer-tainty in representing in language. For example, as shown in Fig. 1, the image can be described as "A photo of the dog on the sandy beach", but it can also be expressed differently such as "A photo of the dog near the ocean". Therefore, it is not appropriate to exploit a deterministically visual representation when trans-ferring VLM in dense prediction tasks.
In this paper, we propose a Probabilistic Prompt Learn-ing (PPL) that explores learning appropriate textual de-scriptions using visual cues in a probabilistic embedding space. We present a set of prompts that express class-agnostic attributes such as position, size, and color to rep-resent objects without semantic ambiguity. To effectively learn the probabilistic distribution to describe diverse and informative attributes for the entire class, we model each attribute distribution as a distinct normal distribution. To this end, we set its variance as contextual relations between text and visual features to explicitly utilize visual-text in-formation. With these attribute distributions, class-specific attribute distribution is approximated by a Mixture of Gaus-sian (MoG). Furthermore, we introduce a novel probabilis-tic pixel-text matching loss to attenuate the instability of prediction probability caused by high uncertainty.
In summary, our contributions are as follows: (1) We propose a novel PPL to effectively represent class-agnostic attributes of objects in probabilistic text embedding space.
To the best of our knowledge, this is the first attempt to leverage context-aware probabilistic prompt learning. (2)
We introduce a novel probabilistic pixel-text matching loss to alleviate the adverse impact of uncertainty. (3) We demonstrate the effectiveness of the proposed probabilistic approach through extensive experiments on dense predic-tion tasks, including semantic segmentation, instance seg-mentation, and object detection. 2.