Abstract
Although recent studies empirically show that injecting
Convolutional Neural Networks (CNNs) into Vision Trans-formers (ViTs) can improve the performance of person re-identification, the rationale behind it remains elusive. From a frequency perspective, we reveal that ViTs perform worse than CNNs in preserving key high-frequency components (e.g, clothes texture details) since high-frequency compo-nents are inevitably diluted by low-frequency ones due to the intrinsic Self-Attention within ViTs. To remedy such inadequacy of the ViT, we propose a Patch-wise High-frequency Augmentation (PHA) method with two core de-signs. First, to enhance the feature representation ability of high-frequency components, we split patches with high-frequency components by the Discrete Haar Wavelet Trans-form, then empower the ViT to take the split patches as aux-iliary input. Second, to prevent high-frequency components from being diluted by low-frequency ones when taking the entire sequence as input during network optimization, we propose a novel patch-wise contrastive loss. From the view of gradient optimization, it acts as an implicit augmentation to improve the representation ability of key high-frequency components. This benefits the ViT to capture key high-frequency components to extract discriminative person rep-resentations. PHA is necessary during training and can be removed during inference, without bringing extra complex-ity. Extensive experiments on widely-used ReID datasets validate the effectiveness of our method. 1.

Introduction
*Corresponding author https://github.com/zhangguiwei610/PHA
This work was partially supported by the National Natural Science
Foundation of China (No. 62072022) and the Fundamental Research
Funds for the Central Universities.
Figure 1. Performance comparisons between ResNet101 (44.7M
#Param) and TransReID ( 100M #Param) on (a) original person images, (b) low-frequency components, and (c) high-frequency components of Market1501 and MSMT17 datasets, respectively.
Note that “#Param” refers to the number of parameters.
Person re-identification (ReID) aims to retrieve a specific person, given in a query image, from the search over a large set of images captured by various cameras [33, 36, 37, 45].
Most research to date has focused on extracting discrimi-native person representations from single images, either by
[24, 25, 39, 42],
Convolutional Neural Networks (CNNs)
Vision Transformers (ViTs) [2, 8, 23, 27, 41, 43] or Hybrid-based approaches [10, 11, 14, 16, 35]. These studies empir-ically show that injecting CNNs into ViTs can improve the discriminative of person representations [8, 35].
Despite a couple of empirical solutions, the rationale for why ViTs can be improved by CNNs remains elusive. To this end, we explore possible reasons from a frequency per-spective, which is of great significance in digital image pro-cessing [4, 12, 32]. As shown in Fig. 1, we first employ
Discrete Haar Wavelet Transform (DHWT) [19] to trans-form original person images into low-frequency compo-nents and high-frequency components, then conduct perfor-mance comparisons between the ResNet101 [6] and Tran-sReID [8] on original images, the low-frequency compo-nents and high-frequency components of Market1501 and
MSMT17 datasets, respectively. Our comparisons reveal: (1) Certain texture details of person images, which are more related to the high-frequency components, are crucial for ReID tasks. Specifically, from Fig. 1 (a) and (b), with the same model, the performance on the origi-nal images is consistently better than that on low-frequency components. Taking the TransReID as an example, the
Rank-1/mAP on original images of MSMT17 dataset is 6.5%/10.0% higher than that on the low-frequency compo-nents. The root reason might be that low-frequency compo-nents only reflect coarse-grained visual patterns of images, and lose texture details (e.g., bags and edges). In contrast, the lost details are more related to the high-frequency com-ponents, as shown in Fig. 1 (c). The degradation from Fig. 1 (a) to (b) indicates that certain details are key components to improve the performance of ReID. (2) The ViT performs worse than CNNs in preserv-ing key high-frequency components (e.g., texture de-tails of clothes and bags) of person images. As shown in Fig. 1 (c), although the TransReID outperforms the
ResNet101 consistently on the original images and low-frequency components, the ResNet101 exceeds the Tran-sReID by 4.6%/2.4% and 5.9%/1.9% Rank-1/mAP on high-frequency components of Market1501 and MSMT17 datasets. The poor performance of the TransReID on high-frequency components shows its inadequacy in capturing key high-frequency details of person images.
In view of the above, we analyze the possible reason for such inadequacy of the ViT by revisiting Self-Attention from a frequency perspective (Sec. 3.1). We reveal that high-frequency components of person images are inevitably diluted by low-frequency ones due to the Self-Attention mechanism within ViTs. To remedy such inadequacy of the ViT without modifying its architecture, we propose a
Patch-wise High-frequency Augmentation (PHA) method with two core designs (Sec. 3.2). First, unlike previous works that directly take frequency subbands as network in-put [1, 4, 5, 17], we split patches with high-frequency com-ponents by the Discrete Haar Wavelet Transform (DHWT) and drop certain low-frequency components correspond-ingly, then empower the ViT to take the split patches as auxiliary input. This benefits the ViT to enhance the fea-ture representation ability of high-frequency components.
Note that the dropped components are imperceptible to hu-man eyes but essential for the model, thereby preventing the model from overfitting to low-frequency components.
Second, to prevent high-frequency components from be-ing diluted by low-frequency ones when taking the entire sequence as input during network optimization, we pro-pose a novel patch-wise contrastive loss. From the view of gradient optimization, it acts as an implicit augmentation to enhance the feature representation ability of key high-frequency components to extract discriminative person rep-resentations (Sec. 3.3). With it, our PHA is necessary dur-ing training and can be discarded during inference, without bringing extra complexity. Our contributions include:
• We reveal that due to the intrinsic Self-Attention mech-anism, the ViT performs worse than CNNs in capturing high-frequency components of person images, which are key ingredients for ReID. Hence, we develop a
Patch-wise High-frequency Augmentation (PHA) to extract discriminative person representations by en-hancing high-frequency components.
• We propose a novel patch-wise contrastive loss, en-abling the ViT to preserve key high-frequency com-ponents of person images. From the view of gradi-ent optimization, it acts as an implicit augmentation to enhance the feature representation ability of key high-frequency components. By virtue of it, our PHA is necessary during training and can be removed during inference, without bringing extra complexity.
• Extensive experimental results perform favorably the mainstream methods on CUHK03-NP, against
Market-1501, and MSMT17 datasets. 2.