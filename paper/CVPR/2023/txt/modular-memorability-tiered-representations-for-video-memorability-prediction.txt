Abstract
The question of how to best estimate the memorability of visual content is currently a source of debate in the mem-orability community. In this paper, we propose to explore how different key properties of images and videos affect their consolidation into memory. We analyze the impact of several features and develop a model that emulates the most important parts of a proposed “pathway to memory”: a simple but effective way of representing the different hur-dles that new visual content needs to surpass to stay in mem-ory. This framework leads to the construction of our M3-S model, a novel memorability network that processes input videos in a modular fashion. Each module of the network emulates one of the four key steps of the pathway to mem-ory: raw encoding, scene understanding, event understand-ing and memory consolidation. We find that the different representations learned by our modules are non-trivial and substantially different from each other. Additionally, we ob-serve that certain representations tend to perform better at the task of memorability prediction than others, and we in-troduce an in-depth ablation study to support our results.
Our proposed approach surpasses the state of the art on the two largest video memorability datasets and opens the door to new applications in the field. Our code is available at https://github.com/tekal- ai/modular-memorability. 1.

Introduction
The human brain is optimized to remember important content and forget irrelevant information. Research has shown that in the world of visual imagery, the brain’s recall ability is influenced by the content itself: certain images and videos tend to stay in memory for longer, no matter the au-dience it is shown to or the context it appears in [3, 9]. The property of visual content that makes it more or less mem-*Corresponding author.
Figure 1. Our proposed modular framework. Our framework predicts memorability by extracting low-level, mid-level and high-level memorability-aware representations. These representations are compared to a predefined visual context to extract features measuring similarity with this given context. Our M3-S model utilizes four modules to obtain these representations: a low-level understanding module composed of traditional feature extractors, a mid-level understanding module focused on scene and object properties, a high-level understanding module that extracts tem-poral patterns and actions, and a contextual similarity module that computes features through clustering. The feature vectors pro-duced by the modules are fused and fed to a regression module to produce memorability scores. orable is referred to as memorability, and current research studies this phenomenon as an intrinsic property. Memo-rability has been shown to be highly consistent across ob-servers [10, 13, 28, 40], uncorrelated with aesthetics [28, 29] and highly unintuitive [29]. Some studies are proposing that it might be a proxy to the utility of the information carried by visual content, as measured by the human brain [9].
Given its consistency, many previous works have tried to develop systems to predict memorability scores from vi-sual media directly. Some developments attempt to use low-level image features and specific semantic information
[29, 30], while more recent work has focused on deep neu-ral networks, leveraging their ability to learn rich represen-tations through regressing ground-truth scores directly from the pixel-level visual input [31]. Here, we argue that current
DNN approaches are trying to solve the problem with black-box predictors that do not leverage the underlying structure governing memorability. Indeed, previous work [42] shows that our brain processes visual stimuli by first aggregating low level patterns (early visual areas V1 and V2), then un-derstanding the contents of the scene (higher visual areas
V3A, V4v, V7), and finally integrating the meaning of the event being witnessed and linking with previous knowledge (prefrontal cortex). Although some of the existing systems work on different dimensions of the input (optical flow, raw pixels, text descriptions), they tend to overlook predictive patterns that can be acquired through a specific modeling of low-level, mid-level and high-level representations. Specif-ically, it has been shown that memorability is sensitive to a set of specific properties [29] (that we define and expand on in this work), such as clutter, camera movement, distinc-tiveness of objects, and other semantic and cognitive dimen-sions. Some of these properties are considered low-level: they correspond to simple transformations of the raw pixel input, photometric properties, clarity of image, or proper-ties of the capturing process (blurriness, camera movement, etc). Other properties can be considered mid-level, such as the composition of the scene, the type of objects in it, the general setting, etc. Finally, high-level properties are usu-ally related to the action depicted, emotion transmitted by the content, or general goals of the actors.
We propose a new memorability framework that explic-itly models these three categories by instantiating modules that are specifically designed to extract representations that are relevant for memorability, and representative of each category. We call these representations tiered, as each rep-resentation captures information from a different tier (low, mid and high) of memorability properties. Our modular memorability model additionally introduces a fourth mod-ule that computes representations capturing the similarity of a given input with its most likely visual context: mod-eling this final property is key to understand contextual ef-fects on memorability. To define these modules, we per-form an in-depth analysis over the factors that influence memorability. We show that each of these modules con-tribute to memorability in their own way, that the repre-sentations they yield are more interpretable than black box counterparts, and that combining the information from these representations yields competitive models on the two main datasets for video memorability: VideoMem [13] and Me-mento10k [40].
To summarize, our key contributions are: 1. We introduce a comprehensive analysis of the factors that influence memorability, leading to a categoriza-tion in tiers that we leverage to propose a new modular framework to learn representations which capture the essence of each tier; 2. We propose a novel memorability model based on these modules, M3-S, that combines information from different tiers, contrasts it with contextual data, and uses it to perform competitively on VideoMem and
Memento10k; 3. We perform an in-depth ablation study of the model to obtain key insights about each tier of representations, such as their potential for interpretability, their impact on model performance and the feature representations they learn. 2.