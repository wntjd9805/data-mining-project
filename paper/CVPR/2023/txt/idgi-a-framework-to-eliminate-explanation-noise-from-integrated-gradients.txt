Abstract
Integrated Gradients (IG) as well as its variants are well-known techniques for interpreting the decisions of deep neu-ral networks. While IG-based approaches attain state-of-the-art performance, they often integrate noise into their ex-planation saliency maps, which reduce their interpretabil-ity. To minimize the noise, we examine the source of the noise analytically and propose a new approach to re-duce the explanation noise based on our analytical find-ings. We propose the Important Direction Gradient Integra-tion (IDGI) framework, which can be easily incorporated into any IG-based method that uses the Reimann Integra-tion for integrated gradient computation. Extensive exper-iments with three IG-based methods show that IDGI im-proves them drastically on numerous interpretability met-rics. The source code for IDGI is available at https:
//github.com/yangruo1226/IDGI. 1.

Introduction
With the deployment of deep neural network (DNN) models for safety-critical applications such as autonomous driving [5–7] and medical diagnostics [10, 24], explaining the decisions of DNNs has become a critical concern. For humans to trust the decision of DNNs, not only the model must perform well on the specified task, it also must gen-erate explanations that are easy to interpret. A series of ex-planation methods (e.g., gradient-based saliency/attribution map approaches [21, 22, 29, 33, 36, 38, 43, 46] as well as many that are not based on gradients [4, 11, 13, 19, 25, 27, 30, 32, 35, 39, 40, 42, 47, 48]) have been developed to con-nect a DNN’s prediction to its input. Among them, In-tegrated Gradients (IG) [43], a well-known gradient-based explanation method, and its variants [22, 46] have attracted significant interest due to their state-of-the-art explanation performance and desirable theoretical properties. However, we observe that explanation noise exists in the attribution generated by these IG methods (please see Fig. 1). In this research, we investigate IG-based methods, study the expla-nation noise introduced by these methods, propose a frame-Figure 1. Saliency/attribution map of the existing IG-based meth-ods and those with our method on explaining the prediction from
InceptionV3 model. Our method significantly reduces the noise in the saliency map created by these IG-based methods. work to remove the explanation noise, and empirically val-idate the effectiveness of our approach.
A few recent IG-based methods (e.g., [38] [22], [46],
[41]) have been proposed to address the noise issue.
Kapishnikov et al. [22] provide the following main reasons1 that could generate the noise: 1) DNN model’s shape often has a high curvature; and 2) The choice of the reference point impacts explanation. They propose Guided Integrated
Gradients (GIG) [22], which tackles point #1 by iteratively finding the integration path that tries to avoid the high cur-vature points in the space. Blur Integral Gradients [46], on the other hand, shows that the noise could be reduced by finding the integration path through the frequency domain instead of the original image domain. Formally, it finds the path by successively blurring the input via a Gaussian blur filter. Sturmfels et al. [41] tackle point #2 by performing the integration from multiple reference points, while Smilkov et al. [38] aggregate the attribution with respect to multiple
Gaussian noisy inputs to reduce the noise. Nevertheless, all IG-based methods share a common point in that they compute the integration of gradients via the Riemann inte-gral. We highlight that, the integration calculation by the existing methods fundamentally introduces the explanation noise. To this end, we offer a general solution that elimi-1 [22] mentions the accuracy of integration is also a reason to generate the noise, but this is not the focus of existing IG methods and this paper.
nates the noise by examining the integration directions from the explanation perspective.
Specifically, we investigate each computation step in the
Riemann Integration and then theorize about the noises’ ori-gin. Each Riemann integration calculation integrates the gradient in the original direction—it first computes the gra-dient with respect to the starting point of the current path segment and then multiplies the gradient by the path seg-ment. We show that the original direction can be divided into an important direction and a noise direction. We theo-retically demonstrate that the true gradient is orthogonal to the noise direction, resulting in the gradient’s multiplication along the noise direction having no effect on the attribution.
Based on this observation, we design a framework, termed
Important Direction Gradient Integration (IDGI), that can eliminate the explanation noise in each step of the compu-tation in any existing IG method. Extensive investigations reveal that IDGI reduces noise significantly when evaluated using state-of-the-art IG-based methods.
In summary, our main contributions are as follows:
• We propose the Important Direction Gradient Integra-tion (IDGI), a general framework to eliminate the ex-planation noise in IG-based methods, and investigate its theoretical properties.
• We propose a novel measurement for assessing the at-tribution techniques’ quality, i.e., AIC and SIC using
MS-SSIM. We show that this metric offers a more pre-cise measurement than the original AIC and SIC.
• Our extensive evaluations on 11 image classifiers with 3 existing and 1 proposed attribution assessment tech-niques indicate that IDGI significantly improves the at-tribution quality over the existing IG-based methods. 2.