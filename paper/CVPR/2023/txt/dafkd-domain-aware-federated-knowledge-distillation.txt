Abstract
Federated Distillation (FD) has recently attracted in-creasing attention for its efficiency in aggregating multi-ple diverse local models trained from statistically hetero-geneous data of distributed clients. Existing FD methods generally treat these models equally by merely computing the average of their output soft predictions for some given input distillation sample, which does not take the diversity across all local models into account, thus leading to de-graded performance of the aggregated model, especially when some local models learn little knowledge about the sample. In this paper, we propose a new perspective that treats the local data in each client as a specific domain and design a novel domain knowledge aware federated distilla-tion method, dubbed DaFKD, that can discern the impor-tance of each model to the distillation sample, and thus is able to optimize the ensemble of soft predictions from di-verse models. Specifically, we employ a domain discrimi-nator for each client, which is trained to identify the corre-lation factor between the sample and the corresponding do-main. Then, to facilitate the training of the domain discrim-inator while saving communication costs, we propose shar-ing its partial parameters with the classification model. Ex-tensive experiments on various datasets and settings show that the proposed method can improve the model accuracy by up to 6.02% compared to state-of-the-art baselines. 1.

Introduction
Federated learning (FL) has emerged as a prominent dis-tributed machine learning framework to train a global model via the collaboration among users without sharing their original dataset [17, 22, 27]. Due to the benefits of preserv-ing privacy and economic communication efficiency, FL has been widely adopted in various applications such as medical
*Ruixuan Li is the corresponding author. image processing [8, 19, 32] and recommendation [2, 26].
The classic FL paradigm, FedAvg [22], iteratively opti-mize the global model by aggregating the parameters of lo-cal models trained from data resides on a number of remote devices or servers. However, these methods usually suffer from serious model performance degradation when the data is not independently and identically distributed (Non-IID) across clients, which is a common issue in FL scenarios.
This is mainly because the model parameters on different clients are optimized towards diverse directions [14], lead-ing to the overlarge variance of the aggregated model.
To tackle this challenge, federated distillation (FD) [13] proposes to distill the knowledge of multiple local models into the global model by aggregating only the output soft predictions, which recently attracts increasing attention.
For instance, Tao et al. [18] leveraged the public dataset as the distillation data samples to obtain the soft predictions from multiple local models and then updated the global model with the average of these soft predictions. Based on [18], Zhu et al. [40] and Zhang et al. [37] improved the distillation by replacing the public dataset with data gen-erated by the generative model. Although these methods achieve significant improvement over existing parameter-averaging methods, they still do not take the model diver-sity into account and may still limit the model performance.
More specifically, only computing the average of soft pre-dictions will inevitably bring errors when some local mod-els make wrong predictions for the distillation sample.
To break the limitations of existing federated distillation methods, we in this paper propose a novel federated distil-lation method dubbed DaFKD that can discern the impor-tance of each model to the given distillation sample, and thus is able to reduce the impact of wrong soft predictions.
More specifically, we consider that the local data in each client constitutes as a specific domain and employ a domain discriminator for each client to identify the correlation fac-tor between the sample and the domain. For a given dis-tillation sample, we endow the local model with high im-portance when its correlation factor is significant and vice versa. The principle behind this is the fact that a model tends to make the correct prediction when the sample is con-tained in the domain for training the model. Furthermore, to facilitate the training of domain discriminator, we pro-pose sharing its partial parameters with the target classifica-tion model. Through extensive experiments over various datasets (MNIST, EMNIST, FASHION MNIST, SVHN) and different settings (various models and data distribu-tions), we show that the proposed method significantly im-proves the model accuracy as compared to state-of-the-art algorithms. The contributions of this paper are:
• We propose a new domain aware federated distillation method named DaFKD which endows the model with different importance according to the correlation be-tween the distillation sample and the training domain.
• To adaptively discern the importance of multiple local models, we propose employing the domain discrimi-nator for each client which identifies the correlation factors. To facilitate the training of the discriminator, we further propose sharing partial parameters between the discriminator and the target classification model.
• We establish the theories for the generalization bound of the proposed method. Different from existing meth-ods, we theoretically show that DaFKD efficiently solves the Non-IID problem where the generalization bound of DaFKD does not increases with the growth of data heterogeneity.
• We conduct extensive experiments over various datasets and settings. The results demonstrate the ef-fectiveness of the proposed method which improves the model accuracy by up to 6.02% compared to state-of-the-art methods. 2.