Abstract
We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultane-ously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Moti-vated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a gen-eralization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential out-liers are introduced alongside the usual parametric model.
Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfi-dent predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation Open-Set Likeli-hood Optimization (OSLO). OSLO is interpretable and fully modular; it can be applied on top of any pre-trained model seamlessly. Through extensive experiments, we show that our method surpasses existing inductive and transduc-tive methods on both aspects of open-set recognition, namely inlier classification and outlier detection. Code is avail-able at https://github.com/ebennequin/few-shot-open-set. 1.

Introduction
Few-shot classification consists in recognizing concepts for which we have only a handful of labeled examples. These form the support set, which, together with a batch of unla-beled instances (the query set), constitute a few-shot task.
*Equal contribution.
Corresponding authors:
{ma-lik.boudiaf.1@etsmtl.net, etienneb@sicara.com}
†MICS, CentraleSupélec, Université Paris-Saclay
Most few-shot methods classify the unlabeled query samples of a given task based on their similarity to the support in-stances in some feature space [36]. This implicitly assumes a closed-set setting for each task, i.e. query instances are supposed to be constrained to the set of classes explicitly defined by the support set. However, the real world is open and this closed-set assumption may not hold in practice, especially for limited support sets. Whether they are unex-pected items circulating on an assembly line, a new dress not yet included in a marketplace’s catalog, or a previously undiscovered species of fungi, open-set instances occur ev-erywhere. When they do, a closed-set classifier will falsely label them as the closest known class.
This drove the research community toward open-set recognition i.e. recognizing instances with the awareness that they may belong to unknown classes. In large-scale settings, the literature abounds of methods designed specif-ically to detect open-set instances while maintaining good accuracy on closed-set instances [1, 32, 51]. Very recently, the authors of [21] introduced a Few-Shot Open-Set Recog-nition (FSOSR) setting, in which query instances may not belong to any known class. The study in [21], together with other recent follow-up works [15, 16], exposed FSOSR to be a difficult task.
To help alleviate the scarcity of labeled data, transduction
[38] was recently explored for few-shot classification [24], and has since become a prominent research direction, fuel-ing a large body of works, e.g. [3, 4, 9, 14, 23, 26, 40, 43, 52], among many others. By leveraging the statistics of the query set, transductive methods yield performances that are sub-stantially better than their inductive counterparts [4, 40] in the standard closed-set setting.
In this work, we seek to explore transduction for the
FSOSR setting. We argue that theoretically, transduction has the potential to enable both classification and outlier detection (OD) modules to act symbiotically. Indeed, the classification module can reveal valuable structure of the
inlier’s marginal distribution that the OD module seeks to estimate, such as the number of modes or conditional dis-tributions, while the OD part indicates the “usability” of each unlabelled sample. However, transductive principles currently adopted for few-shot learning heavily rely on the closed-set assumption in the unlabelled data, leading them to match the classification confidence for open-set instances with that of closed-set instances. In the presence of out-liers, this not only harms their predictive performance on closed-set instances, but also makes prediction-based outlier detection substantially harder than with simple inductive baselines.
Contributions.
In this work, we aim at designing a princi-pled framework that reconciles transduction with the open nature of the FSOSR problem. Our idea is simple but pow-erful: instead of finding heuristics to assess the outlierness of each unlabelled query sample, we treat this score as a latent variable of the problem. Based on this idea, we pro-pose a generalization of the maximum likelihood principle, in which the introduced latent scores weigh potential out-liers down, thereby preventing the parametric model from fitting those samples. Our generalization embeds additional supervision constraints from the support set and penalties discouraging overconfident predictions. We proceed with a block-coordinate descent optimization of our objective, with the closed-set soft assignments, outlierness scores, and para-metric models co-optimized alternately, thereby benefiting from each other. We call our resulting formulation Open-Set
Likelihood Optimization (OSLO). OSLO provides highly interpretable and closed-form solutions within each iteration for both the soft assignments, outlierness variables, and the parametric model. Additionally, OSLO is fully modular; it can be applied on top of any pre-trained model seamlessly.
Empirically, we show that OSLO significantly surpasses its inductive and transductive competitors alike for both out-lier detection and closed-set prediction. Applied on a wide variety of architectures and training strategies and without any re-optimization of its parameters, OSLO’s improvement over a strong baseline remains large and consistent. This modularity allows our method to fully benefit from the latest advances in standard image recognition. Before diving into the core content, let us summarize our contributions: 1. To the best of our knowledge, we realize the first study and benchmarking of transductive methods for the Few-Shot Open-Set Recognition setting. We reproduce and benchmark five state-of-the-art transductive methods. 2. We introduce Open-Set Likelihood Optimization (OSLO), a principled extension of the Maximum Like-lihood framework that explicitly models and handles the presence of outliers. OSLO is interpretable and modular i.e. can be applied on top of any pre-trained model seamlessly. 3. Through extensive experiments spanning five datasets and a dozen of pre-trained models, we show that OSLO consistently surpasses both inductive and existing trans-ductive methods in detecting open-set instances while competing with the strongest transductive methods in classifying closed-set instances. 2.