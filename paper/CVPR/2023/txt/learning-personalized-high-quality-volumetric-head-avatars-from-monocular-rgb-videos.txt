Abstract state-of-the-art approaches.
We propose a method to learn a high-quality implicit 3D head avatar from a monocular RGB video captured in the wild. The learnt avatar is driven by a paramet-ric face model to achieve user-controlled facial expressions and head poses. Our hybrid pipeline combines the geom-etry prior and dynamic tracking of a 3DMM with a neural radiance ﬁeld to achieve ﬁne-grained control and photore-alism. To reduce over-smoothing and improve out-of-model expressions synthesis, we propose to predict local features anchored on the 3DMM geometry. These learnt features are driven by 3DMM deformation and interpolated in 3D space to yield the volumetric radiance at a designated query point. We further show that using a Convolutional Neural
Network in the UV space is critical in incorporating spatial context and producing representative local features. Exten-sive experiments show that we are able to reconstruct high-quality avatars, with more accurate expression-dependent details, good generalization to out-of-training expressions, and quantitatively superior renderings compared to other 1.

Introduction
Creating a controllable human avatar is a fundamen-tal piece of technology for many downstream applications, such as AR/VR communication [20,31], virtual try-on [37], virtual tourism [13], games [42], and visual effects for movies [12, 18]. Prior art in high-quality avatar generation typically requires extensive hardware conﬁgurations (i.e., camera arrays [6, 12, 31], light stages [18, 29], dedicated depth sensors [8]), or laborious manual intervention [1]. Al-ternatively, reconstructing avatars from monocular RGB videos signiﬁcantly relaxes the dependency on equipment setup and broadens the application scenarios. However, monocular head avatar creation is highly ill-posed due to the dual problems of reconstructing and tracking highly articulated and deformable facial geometry, while model-∗Work was conducted while Ziqian Bai was an intern at Google.
ing sophisticated facial appearance. Traditionally, 3D Mor-phable Models (3DMM) [7, 24] have been used to model facial geometry and appearance for various applications in-cluding avatar generation [9, 16, 22]. However, 3DMMs do not fully capture subject-speciﬁc static details and dynamic variations, such as hair, glasses, and expression-dependent high frequency details such as wrinkles, due to the limited capacity of the underlying linear model.
Recent works [3, 15] have incorporated neural radiance
ﬁelds in combination with 3DMMs for head avatar gener-ation to achieve photorealistic renderings, especially im-proving challenging areas, such as hair, and adding view-dependent effects, such as reﬂections on glasses. The pio-neering work of NerFACE [15] uses a neural radiance ﬁeld parameterized by an MLP that is conditioned on 3DMM expression parameters and learnt per-frame latent codes.
While they achieve photorealistic renderings, the reliance on an MLP to directly decode from 3DMM parameter space leads to the loss of ﬁne-grain control over geometry and ar-ticulation. Alternatively, RigNeRF [3] learns the radiance
ﬁeld in a canonical space by warping the target head ge-ometry using a 3DMM ﬁt, which is further corrected by a learnt dense deformation ﬁeld parameterized by another
MLP. While they demonstrate in-the-wild head pose and expression control, the use of two global MLPs to model canonical appearance and deformations for the full spatial-temporal space leads to a loss of high frequency details, and an overall uncanny appearance of the avatar. Both of these works introduce new capabilities but suffer from lack of de-tail in both appearance and motion because they attempt to model the avatar’s global appearance and deformation with an MLP network.
In this paper, we propose a method to learn a neural head avatar from a monocular RGB video. The avatar can be controlled by an underlying 3DMM model and deliver high-quality rendering of arbitrary facial expressions, head poses, and viewpoints, which retain ﬁne-grained details and accu-rate articulations. We achieve this by learning to predict expression-dependent spatially local features on the surface of the 3DMM mesh. A radiance ﬁeld for any given 3D point in the volume is then obtained by interpolating the features from K-nearest neighbor vertices on the deformed 3DMM mesh in target expression, and passing them through a lo-cal MLP to infer density and color. The local features and local MLP are trained jointly by supervising the radiance
ﬁeld through standard volumetric rendering on the train-ing sequence [30]. Note that our networks rely on the local features to model appearance and deformation details, and leverages the 3DMM to model only the global geometry.
Learning local features is critical in achieving a high-quality head avatar. To this end, we train an image-to-image translation U-Net that transforms the 3DMM defor-mations in the UV space to such local features. These UV-space features are then attached to the corresponding ver-tices of the 3DMM mesh geometry. We show that learn-ing features from such explicit per-vertex local displace-ment of the 3DMM geometry makes the model retain high-frequency expression-dependent details and also general-izes better to out-of-training expressions, presumably be-cause of the spatial context between nearby vertices incor-porated by the convolutional neural network (CNN). An al-ternative approach is to feed the 3DMM parameters directly into a CNN decoder running on the UV space. However, we found this produces severe artifacts on out-of-training expressions, particularly given a limited amount of training data, e.g. for a lightweight, 1-minute data collection proce-dure during the avatar generation process.
In summary, our contributions are as follows: we pro-pose a neural head avatar representation based on a 3DMM-anchored neural radiance ﬁeld, which can model complex expression-dependent variations, but requires only monoc-ular RGB videos for training. We show that a convolutional neural network running on per-vertex displacement in UV space is effective in learning local expression-dependent features, and delivers favorable training stability and gen-eralization to out-of-training expressions. Experiments on real-world datasets show that our model provides compet-itive controllability and generates sharper and detail en-riched rendering compared to state-of-the-art approaches. 2.