Abstract
Concept-based interpretability methods aim to explain a deep neural network model’s components and predictions using a pre-defined set of semantic concepts. These meth-ods evaluate a trained model on a new, “probe” dataset and correlate the model’s outputs with concepts labeled in that dataset. Despite their popularity, they suffer from lim-itations that are not well-understood and articulated in the literature. In this work, we identify and analyze three com-monly overlooked factors in concept-based explanations.
First, we find that the choice of the probe dataset has a pro-found impact on the generated explanations. Our analysis reveals that different probe datasets lead to very different explanations, suggesting that the generated explanations are not generalizable outside the probe dataset. Second, we find that concepts in the probe dataset are often harder to learn than the target classes they are used to explain, call-ing into question the correctness of the explanations. We argue that only easily learnable concepts should be used in concept-based explanations. Finally, while existing meth-ods use hundreds or even thousands of concepts, our human studies reveal a much stricter upper bound of 32 concepts or less, beyond which the explanations are much less prac-tically useful. We discuss the implications of our findings and provide suggestions for future development of concept-based interpretability methods. Code for our analysis and user interface can be found at https://github.com/ princetonvisualai/OverlookedFactors 1.

Introduction
Performance and opacity are often correlated in deep neural networks: the highly parameterized nature of these models that enable them to achieve high task accuracy also reduces their interpretability. However, in order to respon-sibly use and deploy them, especially in high-risk settings such as medical diagnoses, we need these models to be in-terpretable, i.e., understandable by people. With the grow-ing recognition of the importance of interpretability, many methods have been proposed in recent years to explain some aspects of neural networks and render them more inter-pretable (see [4, 14, 18, 42, 44, 53] for surveys).
In this work, we dive into concept-based interpretabil-ity methods for image classification models, which explain model components and/or predictions using a pre-defined set of semantic concepts [5, 16, 25, 29, 56]. Given access to a trained model and a set of images labelled with seman-tic concepts (i.e., a “probe” dataset), these methods produce explanations with the provided concepts. See Fig. 1 for an example explanation.
Concept-based methods are a particularly promising approach for bridging the interpretability gap between complex models and human understanding, as they ex-plain model components and predictions with human-interpretable units, i.e., semantic concepts. Recent work finds that people prefer concept-based explanations over other forms (e.g., heatmap and example-based) because they resemble human reasoning and explanations [27]. Fur-ther, concept-based methods uniquely provide a global, high-level understanding of a model, e.g., how it predicts a certain class [39, 56] and what the model (or some part of it) has learned [5,16,25]. These insights are difficult to gain from local explanation methods that only provide an expla-nation for a single model prediction, such as saliency maps that highlight relevant regions within an image.
However, existing research on concept-based inter-pretability methods focuses heavily on new method devel-opment, ignoring important factors such as the probe dataset used to generate explanations or the concepts composing the explanations. Outside the scope of concept-based meth-ods, there have been several recent works that study the ef-fect of different factors on explanations. These works, how-ever, are either limited to saliency maps [1, 28, 31, 41] or a general call for transparency, e.g., include more information when releasing an interpretability method [47].
In this work, we conduct an in-depth study of commonly overlooked factors in concept-based interpretability meth-ods. Concretely, we analyze four representative methods:
Figure 1. Concept-based interpretability methods explain model components and/or predictions using a pre-defined set of semantic concepts. In this example, a scene classification model’s prediction bedroom is explained as a complex linear combination of 37 visual concepts, with the final explanation score calculated based on the presence or absence of these concepts. The coefficients are learned by evaluating the model on a new, “probe” dataset, and correlating its predictions with visual concepts labeled in that dataset. However, concept-based explanations can (1) be noisy and heavily dependent on the probe dataset, (2) use concepts that are hard to learn (all concepts in red are harder to learn than the class bedroom) and (3) be overwhelming to people due to the complexity of the explanation.
NetDissect [5], TCAV [25], Concept Bottleneck [29] and
IBD [56]. These are a representative and comprehensive set of existing concept-based interpretability methods for computer vision models. Using multiple probe datasets (ADE20k [57, 58] and Pascal [13] for NetDissect, TCAV and IBD; CUB-200-2011 [48] for Concept Bottleneck), we examine the effects of (1) the choice of probe dataset, (2) the concepts used within the explanation, and (3) the com-plexity of the explanation. Through our analyses, we learn a number of key insights, which we summarize below:
• The choice of the probe dataset has a profound impact on explanations. We repeatedly find that different probe datasets give rise to different explanations, when explain-ing the same model with the same interpretability method. the prediction of the arena/hockey
For instance, class is explained with concepts {grandstand, goal, ice-rink, skate-board} with one probe dataset, and {plaything, road} with another probe dataset.
We highlight that concept-based explanations are not solely determined by the model or the interpretability method. Hence, probe datasets should be chosen with caution. Specifically, we suggest using probe datasets whose data distribution is similar to that of the dataset the model-being-explained was trained on.
• Concepts used in explanations are frequently harder to learn than the classes they aim to explain. The choice of concepts used in explanations is dependent on the available concepts in the probe dataset. Surprisingly, we find that learning some of these concepts is harder than learning the target classes. For example, in one experi-ment we find that the target class bathroom is explained using concepts {toilet, shower, countertop, bathtub, screen-door}, all of which are harder to learn than bathroom. Moreover, these concepts can be hard for people to identify, limiting the usefulness of these explanations. We argue that learnability is a necessary (albeit not sufficient) condition for the correctness of the explanations, and advocate for future explanations to only use concepts that are easily learnable.1
• Current explanations use hundreds or even thousands of concepts, but human studies reveal a much stricter upper bound. We conduct human studies with 125 par-ticipants recruited from Amazon Mechanical Turk to un-derstand how well people reason with concept-based ex-planations with varying number of concepts. We find that participants struggle to identify relevant concepts in im-ages as the number of concepts increases (the percent-age of concepts recognized per image decreases from 71.7% ± 27.7% with 8 concepts to 56.8% ± 24.9% for 32 concepts). Moreover, the majority of the participants pre-fer that the number of concepts be limited to 32. We also find that concept-based explanations offer little to no ad-vantage in predicting model output compared to example-based explanations (the participants’ mean accuracy at predicting the model output when given access to expla-nations with 8 concepts is 64.8% ± 23.9% whereas the accuracy when given access to example-based explana-tions is 60.0% ± 30.2%).
These findings highlight the importance of vetting in-tuitions when developing and using interpretability meth-ods. We have open-sourced our analysis code and human study user interface to aid with this process in the future: https : / / github . com / princetonvisualai /
OverlookedFactors. 2.