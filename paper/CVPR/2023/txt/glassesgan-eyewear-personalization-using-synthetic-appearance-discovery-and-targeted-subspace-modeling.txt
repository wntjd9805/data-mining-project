Abstract
We present GlassesGAN, a novel image editing frame-work for custom design of glasses, that sets a new stan-dard in terms of output-image quality, edit realism, and continuous multi-style edit capability. To facilitate the editing process with GlassesGAN, we propose a Targeted
Subspace Modelling (TSM) procedure that, based on a novel mechanism for (synthetic) appearance discovery in the latent space of a pre-trained GAN generator, constructs an eyeglasses-specific (latent) subspace that the editing framework can utilize. Additionally, we also introduce an appearance-constrained subspace initialization (SI) tech-nique that centers the latent representation of the given in-put image in the well-defined part of the constructed sub-space to improve the reliability of the learned edits. We test GlassesGAN on two (diverse) high-resolution datasets (CelebA-HQ and SiblingsDB-HQf) and compare it to three state-of-the-art baselines, i.e., InterfaceGAN, GANSpace, and MaskGAN. The reported results show that GlassesGAN convincingly outperforms all competing techniques, while
*Supported by ARRS J2-2501(A), the Fulbright Scholarship Fund, the
Center for Identification Technology Research, and the National Science
Foundation under Grant No. 1650503. offering functionality (e.g., fine-grained multi-style editing) not available with any of the competitors. The source code for GlassesGAN is made publicly available. 1.

Introduction
Consumers are increasingly choosing the convenience of online shopping over traditional brick-and-mortar stores [3]. For the apparel industry, which traditionally re-lied on individuals being able to try on items to suit their taste and body shape before purchasing, the shift to digital commerce has created an unsustainable cycle of purchasing, shipping, and returns. Now, an estimated 85% of manufac-tured fashion items end up in landfills each year, largely due to consumer returns and unsatisfied online customers [35].
In response to these challenges, the computer vision community has become increasingly interested in virtual-try-on (VTON) techniques [3, 6, 9, 11] that allow for the de-velopment of virtual fitting rooms, where consumers can try on clothing in a virtual setting. Furthermore, such tec-hniques also give users the flexibility to explore custom designs and personalize fashion items by rendering them photo-realistically in the provided input image.
While considerable progress has been made recently in
image-based virtual try-on techniques for clothing and ap-parel that do not require (costly) dedicated hardware and difficult-to-acquire 3D annotated data [5,6,11,18,48], most deployed solutions for virtual eyewear try-on still largely rely on traditional computer graphics pipelines and 3D modeling [1, 29, 30,49, 51]. Such 3D solutions provide con-vincing results, but save for a few exceptions, e.g., [14], are only able to handle predefined glasses and do not sup-port custom designs and eyewear personalization. Although work has also been done with 2D (image) data only, rele-vant research for virtual eyewear try-on has mostly focused on editing technology (facilitated by Generative Adversarial
Network - GANs [10]) capable of inserting glasses into an image [15, 27, 34, 37, 44]. The images these methods gen-erate are often impressive, but adding eyewear with finely tunable appearance control still remains challenging.
In this work, we address this gap by introducing Glasses-GAN, a flexible image editing framework that allows users to add glasses to a diverse range of input face images (at a high-resolution) and control their appearance. Distinct from existing virtual try-on work in the vision literature, the goal of GlassesGAN is not to try on existing glasses, but rather to allow users to explore custom eyewear designs. Glass-esGAN is designed as a GAN inversion method [45], that uses a novel Targeted Subspace Modeling (TSM) technique to identify relevant directions within the latent space of a pre-trained GAN model that can be utilized to manipulate the appearance of eyeglasses in the edited images. A key component of GlassesGAN is a new Synthetic Appearance
Discovery (SAD) mechanism that samples the GAN latent space for eyeglasses appearances, without requiring real-world facial images with eyewear. Additionally, we propose an appearance-constrained subspace initialization proce-dure for the (inference-time) editing stage, which helps to produce consistent editing results across a diverse range of input images. We evaluate GlassesGAN in comprehensive experiments over two test datasets and in comparison to state-of-the-art solutions from the literature, with highly en-couraging results.
In summary, our main contributions in this paper are:
• We present GlassesGAN, an image editing framework for custom design of eyeglasses in a virtual try-on set-ting that sets a new standard in terms of output image quality, edit realism, and continuous multi-style edit capability, as illustrated in Figure 1.
• We introduce a Synthetic Appearance Discovery (SAD) mechanism and a Targeted Subspace Model-ing (TSM) procedure, capable of capturing eyeglasses-appearance variations in the latent space of GAN mod-els using glasses-free facial images only.
• We introduce a novel initialization procedure for the editing process that improves the reliability of the fa-cial manipulations across different input images. 2.