Abstract
Unsupervised domain adaptation (UDA) for semantic segmentation aims at improving the model performance on the unlabeled target domain by leveraging a labeled source domain. Existing approaches have achieved im-pressive progress by utilizing pseudo-labels on the unla-beled target-domain images. Yet the low-quality pseudo-labels, arising from the domain discrepancy, inevitably hin-der the adaptation. This calls for effective and accurate ap-proaches to estimating the reliability of the pseudo-labels, in order to rectify them. In this paper, we propose to esti-mate the rectiﬁcation values of the predicted pseudo-labels with implicit neural representations. We view the rectiﬁ-cation value as a signal deﬁned over the continuous spa-tial domain. Taking an image coordinate and the nearby deep features as inputs, the rectiﬁcation value at a given coordinate is predicted as an output. This allows us to achieve high-resolution and detailed rectiﬁcation values es-timation, important for accurate pseudo-label generation at mask boundaries in particular. The rectiﬁed pseudo-labels are then leveraged in our rectiﬁcation-aware mixture model (RMM) to be learned end-to-end and help the adap-tation. We demonstrate the effectiveness of our approach on different UDA benchmarks, including synthetic-to-real and day-to-night. Our approach achieves superior results com-pared to state-of-the-art. The implementation is available at https://github.com/ETHRuiGong/IR2F. 1.

Introduction
Semantic segmentation, aiming at assigning the seman-tic label to each pixel in an image, is a fundamental prob-lem in computer vision. Driven by the availability of large-scale datasets and the advancements in deep neural networks (DNNs), the state-of-the-art boundary has been pushed rapidly in the last decade [9, 35, 38, 51, 59, 70, 78].
However, the DNNs trained on a source domain, e.g. day images, generalize poorly to a different target domain, e.g. night images, due to the distribution shift between the do-mains. One straightforward idea to circumvent the issue is to annotate the images from the target domain, and then retrain the model. However, annotations for semantic seg-mentation are particularly costly and labor-intensive to pro-duce, since each pixel has to be labeled. To this end, some recent works [18,21,61,63,77] resort to unsupervised domain adaptation (UDA), where the model is trained on the labeled source domain and an unlabeled target domain dataset, reducing the annotation burden.
Different from the predominant UDA methods that ex-plicitly align the source and target distributions on the image-level [18, 21, 33, 73] or the feature-level [61–63], pseudo-labeling or self-training [23,24,60,76,82,83] has re-cently emerged as a simple yet effective approach for UDA.
Pseudo-labeling approaches typically ﬁrst generate pseudo-labels on the unlabeled target domain using the current model. The model is then ﬁne-tuned with target pseudo-labels in an iterative manner. However, some pseudo-labels are inevitably incorrect because of the domain shift. There-fore, pseudo-label correction, or rectiﬁcation, is critical for the adaptation process. This is typically implemented in the literature by removing [82, 83] or assigning a smaller weight [24, 67, 76, 79] to pixels with low-quality and poten-tially incorrect pseudo-labels. The key problem is thus to formulate a rectiﬁcation function that estimates the pseudo-label quality. We identify two important issues with current approaches.
First, most existing methods use hard-coded heuristics as the rectiﬁcation function, e.g. hard thresholding of the softmax conﬁdence [82, 83], prediction variances of differ-ent learners [79], or distance to prototypes [67, 76]. These heuristic rectiﬁcation functions assume on strong correla-tions between the function and the pseudo-label quality, which may not be the case. For example, the rectiﬁcation function that uses the variance of multiple learners [79] to suppress disagreement on the pseudo-labels can be sensitive to small objects in the adaptation [24].
The second issue is that the existing works [24] typically model the rectiﬁcation function in a discrete spatial grid
s e u l a
V n o i t a c i f i t c e
R s e u l a
V n o i t a c i f i t c e
R
Rectification Values Map
Coordinate Space
Conv-Upsampling
/Interpolation
P i x g d i n o c e e l- W is e   D
Feature Space
Down-Sampling
Inputs s e u l a
V n o i t a c i f i t c e
R s e u l a
V n o i t a c i f i t c e
R
Coordinate Space
Arbitrary 
Resolutions
I m
R p li c it  N e s e r p e
Rectification Values Map a l  r u e n n t a ti o s
Feature Space
Down-Sampling
Inputs
Coordinate Space (a) Discrete Modeling
Coordinate Space (b) Continuous Modeling
Figure 1. Discrete vs. Continuous Rectiﬁcation Function Modeling. Discrete modeling suffers from the convolutional pixel-wise decoding in the ﬁxed-grid, where some coordinates are missing (see dashed circle in (a)). Thus, the rectiﬁcation values corresponding to these coordinates can only be obtained by upsampling/interpolation, which is constrained by the blurring effect and induces the inaccurate rectiﬁcation values estimation in some areas, e.g. mask boundaries. In contrast, our continuous modeling decodes the features – in the continuous coordinate space – into rectiﬁcation values, which can be generalized to arbitrary resolution and preserve ﬁner details. (The coordinate space and rectiﬁcation values are shown in 1-D axis just for better viewing.) (see Fig.1a). Rectiﬁcation values are predicted by the pixel-wise decoding from the ﬁxed-grid feature space, which is constrained by the limited resolution. This is especially harmful when the objects in the test images are of a differ-ent scale than in the training, since the rectiﬁcation function cannot generalize well on these unseen scales (see Fig.1a).
Existing approaches also lose vital high-frequency informa-tion through down-/up-sampling operations [24, 25, 40, 56], which may lead to poorer pseudo-labels, in particular close to mask boundaries.
To address these two issues, we propose a novel contin-uous rectiﬁcation-aware mixture model (RMM). First, in-stead of formulating the rectiﬁcation function with heuris-tics and priors, we propose a principled mixture model rep-resentation, i.e. rectiﬁcation-aware mixture model (RMM), ensuring a probabilistic end-to-end learnable formulation.
Second, the rectiﬁcation function in RMM is represented by our proposed implicit rectiﬁcation-representative func-tion (IR2F), to model the pixel-wise rectiﬁcation of pseudo-labels in continuous spatial coordinates, i.e. continuous
RMM. The primary idea of IR2F is to learn pixel-wise rec-tiﬁcation values as latent codes, which are decoded at arbi-trary continuous spatial coordinates. Given a queried coor-dinate, our IR2F inputs latent codes around the given coor-dinate from the different learners (e.g. high-/low-resolution decoder in [24] and primary/auxiliary classiﬁer in [79]) along with their spatial coordinates. IR2F then predicts the rectiﬁcation value at the queried coordinate. Our principled formulation is a general plug-in module, compatible with different rectiﬁcation-aware UDA architectures.
We thoroughly analyze our continuous RMM on differ-ent UDA benchmarks, including synthetic-to-real and day-to-night settings. Extensive experimental results demon-strate the effectiveness of continuous RMM, outperform-ing the previous state-of-the-art (SOTA) methods by a large margin, including on SYNTHIA
Cityscapes (+1.9% mIoU), Cityscapes
Dark Zurich (+3.0% mIoU) and
ACDC-Night (+3.4% mIoU). Overall, continuous RMM reveals the signiﬁcant potential of modeling pseudo-labels rectiﬁcation for UDA in the learnable and continuous man-ner, inspiring further research in this ﬁeld.
!
! 2.