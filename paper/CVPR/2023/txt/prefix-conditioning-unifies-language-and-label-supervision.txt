Abstract
Pretraining visual models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pretraining on image classiﬁcation data.
Image-caption datasets are more “open-domain”, contain-ing broader scene types and vocabulary words, and re-sult in models that have strong performance in few- and zero-shot recognition tasks. However large-scale classi-ﬁcation datasets can provide ﬁne-grained categories with a balanced label distribution.
In this work, we study a pretraining strategy that uses both classiﬁcation and cap-tion datasets to unite their complementary beneﬁts. First, we show that naively unifying the datasets results in sub-optimal performance in downstream zero-shot recognition tasks, as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem with novel Preﬁx Condi-tioning, a simple yet effective method that helps disentangle dataset biases from visual concepts. This is done by intro-ducing preﬁx tokens that inform the language encoder of the input data type (e.g., classiﬁcation vs caption) at training time. Our approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Preﬁx conditioning is generic and can be eas-ily integrated into existing VL pretraining objectives, such as CLIP or UniCL. In experiments, we show that it improves zero-shot image recognition and robustness to image-level distribution shift. 1.

Introduction
Supervised classiﬁcation datasets (e.g., ImageNet
[7]) have traditionally been used to pretrain image representa-tions for use in downstream tasks. However, web-scale image-caption datasets have recently emerged as a pow-erful pretraining alternative [13, 20, 31]. Such datasets
Figure 1. We propose Preﬁx Conditioning to unify image-caption (e.g., CC12M [5]) and image classiﬁcation datasets (e.g., Ima-geNet21K (IN21K) [7]) for training better zero-shot models. Pre-ﬁx conditioning improves zero-shot recognition performance by more than 6% on average when training on ImageNet21K and
CC12M. are more “open-domain”, containing a wider variety of scene types and vocabularies than traditional classiﬁcation datasets, which are biased towards speciﬁc categories in their ﬁxed label sets. Consequently, models trained on web-scale image-caption datasets have shown stronger gener-alization in novel tasks [4, 31] and demonstrated remark-able performance on few and zero-shot image classiﬁcation tasks [31]. Nevertheless, classiﬁcation datasets are still use-ful for pre-training as they have a more balanced coverage of categories, including rare and ﬁne-grained categories, and a better focus on the labeled objects in each image.
Recent works [43,45] therefore propose to combine cap-tion and classiﬁcation datasets for pre-training. [43] convert classiﬁcation labels to “label-prompts” by inserting the la-bel into a template sentence, e.g., “a photo of a <label>.”1 1We use the term prompt to indicate a template sentence ﬁlled with a
∗Work done during internship at Google Cloud AI Research. class name.
Although training on the caption and label-prompt data achieves promising results, it does not fully resolve distribu-tion differences between the open-domain caption data and the classiﬁcation data. In particular, it produces a language embedding entangled with the classiﬁcation dataset “bias”.
We note that classiﬁcation datasets tend to be biased in at least two ways: 1) the images mostly contain single ob-jects from restricted domains, and 2) the vocabulary is lim-ited and lacks the linguistic ﬂexibility required for zero-shot learning. Therefore, the class embedding of “a photo of a dog” optimized for ImageNet may really mean a photo of a dog from ImageNet instead, which is biased to ImageNet and does not generalize well to other datasets. We empiri-cally show that such dataset biases negatively affect uniﬁed pretraining by reducing the generalization of learned repre-sentations and thus jeopardizing zero-shot performance.
To recognize diverse concepts in the open domain, the language model needs to disentangle the dataset bias from the visual concepts and extract language embeddings gen-eralizable to the open domain, e.g., the language embed-ding representing a photo of a dog from an open-domain dataset, such as image-caption dataset, instead of a photo of a dog from ImageNet. Given this intuition, we propose to learn dataset-speciﬁc language embeddings, while sharing knowledge from both datasets during training. We achieve this by a simple yet effective approach we call Preﬁx Con-ditioning. The idea is to learn a dataset-speciﬁc text token (preﬁx) for each dataset so that the bias of the dataset can be absorbed into this token, and in return the remaining text tokens can focus on learning visual concepts. Speciﬁcally, we prepend a different token for each dataset (e.g., image classiﬁcation or caption dataset) to the text input token se-quence during pre-training.
The idea is in part inspired by preﬁx or prompt tun-ing [18, 21, 46], which showed that learnable tokens prepended to the input token sequences of the pre-trained language models are able to learn task-speciﬁc knowledge and thus can be used to solve downstream tasks by com-bining the knowledge of pre-trained large language models and task-speciﬁc preﬁx tokens. Our approach differs from prompt tuning in two ways: 1) the proposed preﬁx condi-tioning is designed to unify image-caption and classiﬁcation datasets by disentangling the dataset bias, which is a unique distinction to prompt-tuning works, 2) our approach is ap-plied for VL pre-training while the standard prompt tuning is used in ﬁne-tuning.
In experiments, the proposed simple technique achieves superior performance on zero-shot evaluation if we use the preﬁx of the caption dataset to get the language embedding at test time as shown in Fig. 1. Meanwhile, inserting the pre-ﬁx of the classiﬁcation dataset leads to better performance on classiﬁcation data. We also observe a drastic perfor-mance improvement when combining our preﬁx condition-ing with the UniCL [43] objective because of their comple-mentarity. Our contributions are summarized as follows:
• We propose novel Preﬁx Conditioning at pre-training time to unify image-label and image-caption supervision.
It is the ﬁrst mechanism to use preﬁxes to condition the source of the dataset during vision language contrastive pre-training, rather than post pre-training.
• This simple approach improves zero-shot recognition performance by more than 6% on average in experiments on ImageNet21K [7] and CC12M [5].
• Our comprehensive ablation study shows that preﬁx con-ditioning enables the model to switch its approach to ex-tracting language features, e.g., attend to different words. 2.