Abstract
We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we propose the first purely unsupervised method, called GrowSP, to successfully identify complex se-mantic classes for every point in 3D scenes, without need-ing any type of human labels or pretrained models. The key to our approach is to discover 3D semantic elements via progressive growing of superpoints. Our method consists of three major components, 1) the feature extractor to learn per-point features from input point clouds, 2) the superpoint constructor to progressively grow the sizes of superpoints, and 3) the semantic primitive clustering module to group superpoints into semantic elements for the final semantic segmentation. We extensively evaluate our method on mul-tiple datasets, demonstrating superior performance over all unsupervised baselines and approaching the classic fully-supervised PointNet. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning. 1.

Introduction
Giving machines the ability to automatically discover se-mantic compositions of complex 3D scenes is crucial for many cutting-edge applications. In the past few years, there has been tremendous progress in fully-supervised semantic segmentation for 3D point clouds [14]. From the seminar works PointNet [40] and SparseConv [12] to a plethora of
*Corresponding author recent neural models [21, 27, 41, 55, 60], both the accuracy and efficiency of per-point semantic estimation have been greatly improved. Unarguably, the success of these meth-ods primarily relies on large-scale human annotations for training deep neural networks. However, manually annotat-ing real-world 3D point clouds is extremely costly due to the unstructured data format [3, 20]. To alleviate this prob-lem, a number of recent methods start to use fewer 3D point labels [19, 69], cheaper 2D image labels [59, 77], or active annotations [22,63] in training. Although achieving promis-ing results, they still need tedious human efforts to annotate or align 3D points across images for particular datasets, thus being inapplicable to novel scenes without training labels.
In this paper, we make the first step towards unsuper-vised 3D semantic segmentation of real-world point clouds.
To tackle this problem, there could be two strategies: 1) to na¨ıvely adapt existing unsupervised 2D semantic segmen-tation techniques [4, 7, 24] to 3D domain, and 2) to apply existing self-supervised 3D pretraining techniques [17, 66] to learn discriminative per-point features followed by clas-sic clustering methods to obtain semantic categories. For unsupervised 2D semantic methods, although achieving en-couraging results on color images, they can be hardly ex-tended to 3D point clouds primarily because: a) there is no general pretrained backbone to extract high-quality fea-tures for point clouds due to the lack of representative 3D datasets akin to ImageNet [46] or COCO [29], b) they are usually designed to group pixels with similar low-level fea-tures, e.g. colors or edges, as a semantic class, whereas such a heuristic is normally not satisfied in 3D point clouds due to point sparsity and spatial occlusions. For self-supervised 3D pretraining methods, although the pretrained per-point
features could be discriminative, they are lack of seman-tic meanings fundamentally because the commonly adopted data augmentation techniques do not explicitly capture cat-egorical information. Section 4 clearly demonstrates that all these methods fail catastrophically on 3D point clouds.
Given a sparse point cloud composed of multiple seman-tic categories, we can easily observe that a relative small local point set barely contains distinctive semantic infor-mation. Nevertheless, when the size of a local point set is gradually growing, that surface patch naturally emerges as a basic element or primitive for a particular semantic class, and then it becomes much easier for us to identify the cat-egories just by combining those basic primitives. For ex-ample, two individual 3D points sampled from a spacious room are virtually meaningless, whereas two patches might be easily identified as the back and/or arm of chairs.
Inspired by this, we introduce a simple yet effective pipeline to automatically discover per-point semantics, sim-ply by progressively growing the size of per-point neigh-borhood, without needing any human labels or pretrained backbone. In particular, our architecture consists of three major components: 1) a per-point feature extractor which is flexible to adopt an existing (untrained) neural network such as the powerful SparseConv [12]; 2) a superpoint con-structor which progressively creates larger and larger su-perpoints during training to guide semantic learning; 3) a semantic primitive clustering module which aims to group basic elements of semantic classes via an existing clustering algorithm such as K-means. The key to our pipeline is the superpoint constructor together with a progressive growing strategy in training. Basically, this component drives the feature extractor to progressively learn similar features for 3D points within a particular yet growing superpoint, while the features of different superpoints tend to be pushed as distinct elements of semantic classes. Our method is called
GrowSP and Figure 1 shows qualitative results of an indoor 3D scene. Our contributions are:
• We introduce the first purely unsupervised 3D semantic segmentation pipeline for real-world point clouds, with-out needing any pretrained models or human labels.
• We propose a simple strategy to progressively grow su-perpoints during network training, allowing meaningful semantic elements to be learned gradually.
• We demonstrate promising semantic segmentation results on multiple large-scale datasets, being clearly better than baselines adapted from unsupervised 2D methods and self-supervised 3D pretraining methods. Our code is at: https://github.com/vLAR-group/GrowSP 2.