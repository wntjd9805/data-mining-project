Abstract
Automatic Perceptual Image Quality Assessment is a challenging problem that impacts billions of internet, and social media users daily. To advance research in this field, we propose a Mixture of Experts approach to train two sep-arate encoders to learn high-level content and low-level im-age quality features in an unsupervised setting. The unique novelty of our approach is its ability to generate low-level representations of image quality that are complementary to high-level features representing image content. We refer to the framework used to train the two encoders as Re-IQA.
For Image Quality Assessment in the Wild, we deploy the complementary low and high-level image representations obtained from the Re-IQA framework to train a linear re-gression model, which is used to map the image representa-tions to the ground truth quality scores, refer Figure 1. Our method achieves state-of-the-art performance on multiple large-scale image quality assessment databases containing both real and synthetic distortions, demonstrating how deep neural networks can be trained in an unsupervised setting to produce perceptually relevant representations. We conclude from our experiments that the low and high-level features obtained are indeed complementary and positively impact the performance of the linear regressor. A public release of all the codes associated with this work will be made avail-able on GitHub. 1.

Introduction
Millions of digital images are shared daily on social media platforms such as Instagram, Snapchat, Flickr, etc.
Making robust and accurate Image Quality Assessments (IQA) that correlate well with human perceptual judgments is essential to ensuring acceptable levels of visual experi-ence. Social media platforms also use IQA metrics to de-cide parameter settings for post-upload processing of the images, such as resizing, compression, enhancement, etc.
*Equal
Contribution,
Saha (avinab.saha@utexas.edu) & Sandeep Mishra (sandy.mishra@utexas.edu).
This work was supported by the National Science Foundation AI Institute for Foundations of Machine Learning (IFML) under Grant 2019844.
Correspondence
Avinab to
Figure 1. IQA score prediction uses two encoders trained for com-plementary tasks of learning content and quality aware image rep-resentations. The encoders are frozen while the regressor learns to map image representations to quality predictions.
In addition, predictions generated by IQA algorithms are of-ten used as input to recommendation engines on social me-dia platforms to generate user feeds and responses to search queries. Thus, accurately predicting the perceptual qual-ity of digital images is a high-stakes endeavor affecting the way billions of images are stored, processed, and displayed to the public at large.
IQA metrics can be simply categorized into Full-Reference (FR) and No-Reference (NR) algorithms. FR-IQA algorithms like SSIM [35], FSIM [43], and LPIPS [44] require both reference (undistorted) and distorted version of an image to quantify the human-perceivable quality. This requirement limits their applicability for the “Images in the
Wild” scenario, where the reference image is unavailable.
On the contrary, NR-IQA algorithms like BRISQUE [19],
PaQ-2-PiQ [40], and CONTRIQUE [16] do not require a reference image nor any knowledge about the kind of present distortions to quantify human-perceivable quality in a test image, paving the way for their use in “Images in the
Wild” scenarios.
No-Reference IQA for “Images in the Wild” presents ex-citing challenges due to the complex interplay among the various kinds of distortions. Furthermore, due to the intri-cate nature of the human visual system, image content af-fects quality perception. In this work, we aim to learn low-level quality-aware image representations that are comple-mentary to high-level features representative of image con-tent. Figure 2 illustrates some of the challenges encountered
(a) JPEG Compressed : 1 (b) JPEG Compressed : 2 (c) Motion Blur - Camera Shake (d) Overlaid Film Grain/ Noise
Figure 2. Exemplar Synthetically and “In the Wild” distorted pictures. (a), (b) are two images captured on iPhone 13 Pro and then JPEG compressed using the same encoding parameters. (c), (d), were taken from KonIQ and AVA datasets respectively, and exhibit typical
“Images in the Wild” distortions. Best viewed when zoomed in. in the development of NR-IQA algorithms. Figures 2 (a-b) show two images captured by the authors on an iPhone 13
Pro and compressed using the same encoding parameters.
While any distortions are almost negligible in Figure 2 (a), there are artifacts that are clearly visible in Figure 2 (b). As in these examples, it is well known that picture distortion perception is content dependent, and is heavily affected by content related perceptual processes like masking [1]. Fig-ures 2 (c-d) illustrates a few distorted pictures “In the Wild”.
Figures 2 (c-d) show two exemplar distorted pictures, one impaired by motion blur (Figure 2 (c)) and the other by film grain noise (Figure 2 (d)). It is also well established that perceived quality does not correlate well with image meta-data like resolution, file size, color profile, or compression ratio [36]. Because of all these factors and the essentially infinite diversity of picture distortions, accurate prediction of the perception of image quality remains a challenging task, despite its apparent simplicity, and hence research on this topic remains quite active [16, 17, 19, 28, 35, 39, 42–44].
Our work is inspired by the success of momentum con-trastive learning methods [2, 7] in learning unsupervised representations for image classification. In this work, we engineer our models to learn content and quality-aware im-age representations for NR-IQA on real, authentically dis-torted pictures in an unsupervised setting. We adopt a Mix-ture of Experts approach to independently train two en-coders, each of which accurately learns high-level content and low-level image quality features. We refer to the new framework as Re-IQA. The key contributions we make are as follows:
• We propose an unsupervised low-level image quality representation learning framework that generates fea-tures complementary to high-level representations of image content. We demonstrate how the “Mixture” of the two enables Re-IQA to produce image qual-ity predictions that are highly competitive with exist-ing state-of-the-art traditional, CNN, and Transformer based NR-IQA models, developed in both supervised and unsupervised settings across several databases.
• We demonstrate the superiority of high-level represen-tations of image content for the NR-IQA task, obtained from the unsupervised pre-training of the ResNet-50
[8] encoder over the features obtained from super-vised pre-trained ResNet-50 on the ImageNet database
[3]. We learn these high-level representations of im-age content using the unsupervised training framework proposed in MoCo-v2 [2]
• Inspired by the principles of visual distortion percep-tion we propose a novel Image Augmentation and
Intra-Pair Image Swapping scheme to enable learning of low-level image quality representations. The dy-namic nature of the image augmentation scheme pre-vents the learning of discrete distortion classes, since it is applied to both pristine and authentically dis-torted images, enforcing learning of perceptually rel-evant image-quality features. 2.