Abstract
Recent advances in self-supervised visual representa-tion learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance seg-mentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired ob-jects, when to separate them into parts, how many are there, and of what classes? The answers to these questions de-pend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single conv1 × 1 initialized with coarse back-ground masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on un-supervised saliency detection and object discovery bench-marks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task. The code to reproduce our results is available at https://github.com/valeoai/FOUND. 1.

Introduction
The task of object localization — either performed by detecting [4, 39] or segmenting [7] objects — is required in many safety-critical systems such as self-driving cars. To-day’s best methods train large deep models [4, 7] on large sets of labeled data [13, 30]. To mitigate such needs in annotation, it is possible to use strategies such as semi-supervised [10, 31], weakly-supervised [10, 61] and active learning [37, 50, 65].
In this work, we consider the unsupervised object local-ization task, which consists in discovering objects in an im-age with no human-made annotation. This task has recently received a lot of attention [40,41,43] as it is a solution to de-tect objects in a scene with no prior about what they should look like or which category they should belong to. Early works exploit hand-crafted features [46,62,71,74] and inter-image information [49,51] but hardly scale to large datasets.
Recent works leverage strong self-supervised [6,16,19] fea-tures learned using pretext tasks: [33, 44, 58] localize a sin-gle object per image just by exploiting a similarity graph at the level of an image; [43] proposes to combine differ-ent self-supervised representations, in an ensemble-fashion, and trains a model to learn the concept of object, the same as what [56] does. However, most of these methods make assumptions about what an object is. For example, [44, 58] assume that an image contains more background pixels than object pixels, while [43] discards masks that fill in the width of an image. Such hypotheses restrict objects one can find.
In this work, we propose to tackle the problem the other way around: we make no assumptions about objects but fo-cus instead on the concept of background. Then, we use the idea that a pixel not belonging to the background is likely to belong to an object. Doing so, we do not need to make hypotheses about the number or the size of objects in order to find them. Our method, named FOUND, is cheap both at training and inference time.
We start by computing a rough estimate of the back-ground mask; this step works by mining a first patch that likely belongs to the background. To do this, we leverage attention maps in a self-supervised transformer and select one of the patches that received the least attention. Then the background mask incorporates patches similar to this mined one. One of our contributions is a reweighting scheme to reduce the effect of noisy attention maps based on the spar-sity concept. In the second step, we use the fact that the complement of this background mask provides an approxi-mate estimation of the localization of the objects. This es-timate is refined by training a single conv1 × 1 layer on top of the frozen self-supervised transformer, using only the masks computed in the first step, an edge-preserving fil-ter, and a self-labelling procedure. We show that this cheap method allows us to reach state-of-the-art results in the tasks of saliency detection, unsupervised object discovery and se-mantic segmentation retrieval.
Our main contributions are as follows:
• We propose to think about the object discovery problem upside-down, and to look for what is not background instead of directly looking for objects.
• We propose a new way to exploit already self-trained features and show that they allow us to discover the con-cept of background.
• We show that the use of attention heads can be improved by integrating a weighting scheme based on attention sparsity.
• We propose a lightweight model composed only of a single conv1 × 1 layer and show that there is no need to train a large segmenter for the task.
• We demonstrate that our model performs well on unsu-pervised saliency detection, unsupervised object discov-ery and unsupervised semantic segmentation retrieval tasks. We reach state-of-the-art results in all tasks with a method much faster and lighter than competing ones. 2.