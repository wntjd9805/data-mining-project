Abstract
This paper studies the fundamental problem of learn-ing multi-layer generator models. The multi-layer gener-ator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learn-ing complex data distribution and hierarchical represen-tations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distribu-tions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we pro-pose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer gen-erator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estima-tion (MLE), which involves Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior distribu-tions of the latent variables from different layers. To ensure efficient inference and learning, we further propose a varia-tional training scheme where an inference model is used to amortize the costly posterior MCMC sampling. Our experi-ments demonstrate that the learned model can be expressive in generating high-quality images and capturing hierarchi-cal features for better outlier detection. 1.

Introduction
Deep generative models (a.k.a, generator models) have made promising progress in learning complex data distri-butions and achieved great successes in image and video synthesis [21, 34, 37, 39] as well as representation learn-ing [5,48]. Such models usually consist of low-dimensional latent variables together with a top-down generation model that maps such latent factors to the observed data. The latent factors can serve as an abstract data representation, but it is often modelled via a single latent vector with non-informative prior distribution which leads to limited model expressivity and fails to capture different levels of abstrac-tions. Learning an informative prior model for hierarchical representations is needed, yet research in this direction is still under-developed.
A principled way to learn such a prior model is by learn-ing the generator models with multiple layers of latent vari-ables. However, the learning of multi-layer generator model can be challenging as the inter-layer structural relation (i.e., latent variables across different layers) and the intra-layer contextual relation (i.e., latent units within the same layer) have to be effectively modelled and efficiently learned. Var-ious methods have been proposed [5,28,32,35,40], but they only focused on inter-layer modeling by assuming the con-ditional Gaussian distribution across different layers while ignoring the intra-layer contextual modeling as the latent units are conditional independent within each layer.
The energy-based models (EBMs), on the other hand, are shown to be expressive and proved to be powerful in cap-turing contextual and non-structural data regularities. No-tably, [33] considers the EBM in the latent space for the non-hierarchical generator model, where the energy func-tion is considered as a correction of the non-informative
Gaussian prior. The low dimensionality of the latent space makes EBM effective in capturing regularities in the data.
However, a single latent vector in [33] is infeasible for cap-turing the patterns at multiple layers of abstractions, which limits its model capacity.
In this paper, we propose to combine the strengths of the latent space EBM and the generator with multiple lay-ers of latent variables for better hierarchical representations and a more expressive prior model. Specifically, we in-troduce layer-wise energy terms to exponentially tilt the non-informative Gaussian conditional at each layer, and la-tent variables across different layers are modelled jointly through EBM with the multi-layer generator model as its backbone. Such a joint EBM prior model seamlessly in-tegrates the intra-layer contextual modeling via layer-wise energy terms and inter-layer structural modeling with multi-layer latent variables.
The joint EBM prior model can be learned by maxi-mum likelihood estimation (MLE). Each learning iteration involves Markov chain Monte Carlo (MCMC) sampling of latent variables in each layer from both the prior and pos-terior distributions. The prior sampling can be efficiently done due to the low dimensionality of the latent variables and, more importantly, the lightweight networks for energy functions, while the posterior sampling can be less effi-cient. Therefore, we further develop the variational train-ing scheme where an additional inference model is used for posterior approximation and is jointly trained with the joint
EBM prior model.
Contributions: 1) We propose a joint latent space EBM prior model for the generator model with multiple layers of latent variables; 2) We develop the maximum likelihood learning algorithm that learns the joint EBM prior model based on MCMC prior and posterior sampling across differ-ent layers. We further propose the variational joint training scheme for efficient learning and inference; 3) We provide strong empirical results through extensive experiments. 2.