Abstract
Vision and Language (VL) models have demonstrated re-markable zero-shot performance in a variety of tasks. How-ever, some aspects of complex language understanding still remain a challenge. We introduce the collective notion of Structured Vision & Language Concepts (SVLC) which includes object attributes, relations, and states which are present in the text and visible in the image. Recent stud-ies have shown that even the best VL models struggle with
SVLC. A possible way of fixing this issue is by collecting dedicated datasets for teaching each SVLC type, yet this might be expensive and time-consuming. Instead, we pro-pose a more elegant data-driven approach for enhancing
VL models’ understanding of SVLCs that makes more ef-fective use of existing VL pre-training datasets and does not require any additional data. While automatic under-standing of image structure still remains largely unsolved, language structure is much better modeled and understood, allowing for its effective utilization in teaching VL models.
In this paper, we propose various techniques based on lan-guage structure understanding that can be used to manipu-late the textual part of off-the-shelf paired VL datasets. VL models trained with the updated data exhibit a significant improvement of up to 15% in their SVLC understanding with only a mild degradation in their zero-shot capabilities both when training from scratch or fine-tuning a pre-trained model. Our code and pretrained models are available at: https://github.com/SivanDoveh/TSVLC 1.

Introduction
Recent Vision & Language (VL) models [19, 31, 43, 44, 47, 57] achieve excellent zero-shot performance with re-spect to various computer-vision tasks such as detection, classification, segmentation, etc. However, recent stud-ies [68, 82] have demonstrated that even the strongest VL models struggle with the compositional understanding of some basic Structured VL Concepts (SVLC) such as ob-*Equal contribution
Figure 1. Teaching language structure to VL models. (a) Stan-dard contrastive text-to-image loss (e.g. CLIP [57]) tends to under-emphasize SVLC content of the text, likely due to the random na-ture of the training batches; (b) We generate modified versions of corresponding texts and use them to add losses to explicitly teach language structure (SVLC) to VL models. ject attributes, inter-object relations, transitive actions, ob-ject states and more. Collecting specialized large scale data to teach VL models these missing ‘skills’ is impractical, as finding specialized text-image pairs for each kind and pos-sible value of the different attributes, relations, or states, is both difficult and expensive.
Another important challenge in training VL models with new concepts is catastrophic forgetting, which is a com-mon property to all neural models [7, 34, 37, 49, 58] and has been explored for VL models in a recent concurrent work [14]. Large VL models such as CLIP [57] and Cy-CLIP [19] have exhibited excellent zero-shot learning abil-ities in many tasks. Therefore, even given a large dataset with new concepts, it is important not to lose these abilities when performing the adaptation to the new data.
Figure 2. Teaching structured image understanding to VL models via structured textual data manipulation harnessing the power of language modeling. (1) Generating Rule-Based negative texts (Sec. 3.1.1); (2) Generating negatives using Large Language Model (LLM) unmasking (Sec. 3.1.2); (3) Generating analogies (positives) via LLM prompting (Sec. 3.1.3).
In this paper, we propose a way to leverage existing (off-the-shelf) VL pre-training data sources in order to improve the SVLC understanding skills of a given model, while at the same time maintaining its zero-shot object recognition accuracy. Naturally, succeeding in this goal would lead to potential improvement w.r.t. SVLC understanding in a wide variety of downstream tasks building upon pre-trained VL models, such as zeros-shot detection, segmentation, image generation, and many more.
Recent research [68, 82] has shown that VL models ex-hibit an ‘object bias’ partially due to the contrastive text-to-image loss used in their pre-training. For example, the popular CLIP-loss [57] is computed over a random batch of text-image pairs sampled from a large-scale and diverse
VL dataset with the chance of two images in the same batch containing the same set of objects being very low. For such a loss, representing just a ’bag of objects’ in each image or text is sufficient for matching the corresponding pairs. In-tuitively, this leads to the ‘object bias’ where SVLCs like attributes, states, and relations are being underrepresented (e.g. having a much smaller amplitude in the resulting feature superposition), consequently causing the aforemen-tioned issues with SVLC understanding.
Based on this intuition, we propose a simple data-driven technique that harnesses existing language parsing and modeling capabilities to enhance the importance of SVLCs in the VL model training losses. For each text in the train-ing batch, we automatically generate alternative negative or positive text by manipulating its content to be opposite or equivalent to the original text. Using the newly gener-ated texts, we explicitly teach SVLC to the model via ad-ditional losses (see Fig. 1) that enforce differentiating be-tween different (original and generated) SVLC texts and are no longer satisfiable by the ’bag of objects’ representation.
Towards this end, we propose several techniques for im-plementing this approach, including (i) rule-based priors based on classical NLP parsing and word substitution vo-cabulary according to attribute/relation type; (ii) prompting a Large Language Model (LLM) (e.g. [53]) for analogous text; (iii) generating different meaning (negative) alterna-tives by LLM-based unmasking of parsed text entities of different kinds; (iv) combinations of these methods.
We demonstrate that all these techniques can lead to sig-nificant improvements of up to 15% percent when measur-ing the VL models’ SVLC understanding. We verify this on 5 datasets: VG [39], HAKE [48], VAW [54], SWIG [55], and all combined, using the protocol recently proposed in
VL-Checklist [82]. In addition, we show that the resulting
VL models largely preserve their zero-shot object recogni-tion performance. For the latter, we also propose a variant of efficient LLM fine-tuning using low-rank residual adapters (LoRA) [27] adjusted to VL models. Finally, we show that our framework allows better harnessing of the standard available VL data, e.g. CC3M [62] and LAION [61]. This is exhibited by the aforementioned gains, both in new VL models trained from scratch, as well as in models fine-tuned using strong available VL models such as CLIP [57] and
CyCLIP [19].
To summarize, we offer the following contributions: (i) We propose a data-driven approach for better harness-ing the standard available VL data to improve VL models’
SVLC understanding skills, such as understanding object attributes, inter-object relations, transitive actions, object states, and more, without sacrificing zero-shot object recog-Dataset Model
Pre
Arch
Object
VL-Checklist
Attribute
CLIP [57]
CLIP + LoRA
CLIP + Ours RB Neg
CLIP + Ours LLM Neg
CLIP + Ours RB+LLM Negs
CLIP + Ours Combined
CLIP [57]
CC3M
CLIP + Ours RB+LLM Negs
CLIP + Ours Combined
CLIP
CLIP + Ours Combined
CLIP [57]
CLIP + Ours RB+LLM Negs
CLIP + Ours Combined
CyCLIP [19]
CyCLIP + LoRA
CyCLIP + Ours Combined
CyCLIP
CyCLIP + Ours Combined
CLIP [57]
CLIP + LoRA
CLIP + Ours Combined
LAION
✓
✓
✓
✓
✓
✓
✓
✓
✓
✗
✗
✗
✗
✗
✓
✓
✓
✗
✗
✓
✓
✓
Relation 63.05% 21 Zero-Shot
Tasks Average 56.37% (a)
Vit-B/32 81.58% 67.60%
Vit-B/32
Vit-B/32
Vit-B/32
Vit-B/32
Vit-B/32 80.93% (-0.66%) 83.89% (+2.30%) 84.44% (+2.85%) 85.09% (+3.50%) 85.00% (+3.42%) 66.28% (-1.32%) 73.35% (+5.75%) 71.63% (+4.03%) 73.90% (+6.30%) 71.97% (+4.37%) 55.52% (-7.53%) 75.33% (+12.28%) 74.82% (+11.77%) 78.72% (+15.67%) 68.95% (+5.90%) 56.41% (+0.04%) 54.32% (-2.05%) 55.60% (-0.77%) 54.66% (-1.71%) 54.77% (-1.60%)
Vit-B/16 82.91% 67.32% 61.80% 60.00% (b)
Vit-B/16
Vit-B/16
Vit-B/32
Vit-B/32 85.82% (+2.91%) 84.75% (+1.84%) 73.92% (+6.6%) 71.18% (+3.86%) 77.40% (+15.6%) 69.68% (+7.88%) 59.37% (-0.63%) 59.87% (-0.13%) 71.17% 71.79% (+0.62%) 57.86% 63.29% (+5.43%) 45.20% 58.13% (+12.93%) 21.96% 20.96% (-1.00%)
Vit-B/16 64.01% 54.27% 41.57% 15.49%
Vit-B/16
Vit-B/16 73.11% (+9.1%) 72.99% (+8.98%) 65.32% (+11.05) 63.01% (+8.74%) 71.93% (+30.36) 62.95% (+21.38) 20.78% (+5.29%) 20.61% (+5.12%) (c) (d) 73.49% 59.33% 53.83% 26.00% (e) 73.30% (-0.19%) 74.20% (+0.71%) 58.89% (-0.44%) 63.52% (+4.20%) 53.03% (-0.80%) 59.47% (+5.63%) 26.30% (+0.30%) 26.31% (+0.31%) 69.41% 71.50% (+2.09%) 57.59% 65.69% (+8.10%) 53.70% 70.20% (+16.50%) 21.02% 20.44% (-0.42%)
Vit-B/32 81.58% 67.6% 63.05% 56.37%
Vit-B/32
Vit-B/32 82.18% (+0.60%) 82.54% (+0.96%) 68.48% (+0.88%) 69.64% (+2.04%) 62.72% (-0.33%) 66.05% (+3.00%) 57.15% (+0.78%) 56.71% (+0.34%) (f) (g)
R50
R50
R50
R50
R50
Table 1. VL-Checklist and Zero-Shot classification evaluation (ImageNet + 20 datasets). Finetuned models (for 5 epochs as detailed in Sec. 3.3, starting from officially released CLIP [57] and CyCLIP [19] weights) are marked with ✓in the Pre-trained (Pre) column, while models trained from scratch (for 10 epochs) are marked with ✗. The gains and losses of our approach (+Ours) are in color and are computed w.r.t. to corresponding baselines in each section. CLIP/CyCLIP + LoRA indicate finetuning in the same way and on the same data, but without our approach. Finetuning without LoRA on the same data yields significantly worse performance in all metrics. Sections are separated by bold horizontal lines. (a) CC3M fine-tuning - CLIP - Vit-B/32: we significantly improve the SVLC understanding, observing only small ZS performance drops, 0.77%; (b) CC3M fine-tuning - CLIP - Vit-B/16: we can observe similar improvements as in (a), with an even smaller impact on ZS performance; (c) CC3M from scratch - CLIP - Vit-B/32: we significantly improve SVLC understanding with only a small (1%) decrease in ZS performance; (d) CC3M from scratch - CLIP - Vit-B/16: compared to (c), even greater SVLC understanding improvement is observed (up to 30.36%), at no cost in ZS performance, and even improvement of over 5%; (e) CC3M fine-tuning -CyCLIP: we use CyCLIP original code (with LoRA integration) and losses, as can be seen - adding our techniques improves CyCLIP
SVLC performance considerably without sacrificing ZS performance; (f) CC3M from scratch - CyCLIP: observing even largergains in
SVLC understanding compared to (e) (up to 16.5%), with small reduction in ZS performance of 0.42%; (g) LAION fine-tuning - CLIP -Vit-B/32: we improve SVLC understanding without any decrease in ZS performance. nition performance; (ii) More specifically, we propose to leverage the well-understood and well-modeled structure of language, through classical NLP parsing and/or use of the modern pre-trained LLMs, for manipulating the text part of the standard VL paired datasets to regularize VL train-ing and teach SVLC understanding to VL models. (iii) We further propose an adaptation of efficient LLM fine-tuning technique of [27] for fine-tuning VL models, allowing for only minimal reduction in zero-shot object recognition per-formance after fine-tuning, while still obtaining the afore-mentioned SVLC understanding gains. (iv) Empirically, for the popular CLIP [57] and its most recent extension Cy-CLIP [19], we demonstrate SVLC understanding average improvements of up to 13% when training from scratch, and 15% when fine-tuning from a pretrained model. 2.