Abstract
Approximating radiance fields with discretized volu-metric grids is one of promising directions for improv-ing NeRFs, represented by methods like DVGO, Plenox-els and TensoRF, which achieve super-fast training con-vergence and real-time rendering. However, these meth-ods typically require a tremendous storage overhead, cost-ing up to hundreds of megabytes of disk space and run-time memory for a single scene. We address this issue in this paper by introducing a simple yet effective framework, called vector quantized radiance fields (VQRF), for com-pressing these volume-grid-based radiance fields. We first present a robust and adaptive metric for estimating redun-dancy in grid models and performing voxel pruning by bet-ter exploring intermediate outputs of volumetric rendering.
A trainable vector quantization is further proposed to im-prove the compactness of grid models. In combination with an efficient joint tuning strategy and post-processing, our method can achieve a compression ratio of 100× by reduc-ing the overall model size to 1 MB with negligible loss on visual quality. Extensive experiments demonstrate that the proposed framework is capable of achieving unrivaled per-formance and well generalization across multiple methods with distinct volumetric structures, facilitating the wide use of volumetric radiance fields methods in real-world appli-cations. Code is available at https://github.com/
AlgoHunt/VQRF. 1.

Introduction
Novel view synthesis aims to realize photo-realistic ren-dering for a 3D scene at unobserved viewpoints, given a set of images recorded from multiple views with known cam-era poses. The topic has growing importance because of its potential use in a wide range of Virtual Reality and Aug-mented Reality applications. Neural radiance fields (NeRF)
[29] have demonstrated compelling ability on this topic by modelling and rendering 3D scenes effectively through the
∗denote equal contribution
Figure 1. The pipeline can realize 100× compression rate on vol-umetric models while highly preserving the rendering quality. use of deep neural networks, which are learned to map each 3D location given a viewing direction to its correspond-ing view-dependent color and volume density according to volumetric rendering techniques [27]. The rendering pro-cess relies on sampling a huge number of points and feed-ing them through a cumbersome network, incurring con-siderable computational overhead during training and in-ference. Recent progress following radiance fields recon-struction shows that integrating voxel-based structures [23] into the learning of representations can significantly boost training and inference efficiency. These volumetric radi-ance fields methods typically store features on voxels and retrieve sampling points (including color features and vol-ume densities) by performing efficient trilinear interpola-tion without neural network [44] or only equipped with a lightweight neural network [37] instead of cumbersome net-works. However, the use of volumetric representations in-evitably introduces considerable storage cost, e.g., costing over one hundred megabytes to represent a scene, which is prohibitive in real-world applications.
In this paper, we aim to counteract the storage issue of representations induced by using voxel grids meanwhile
In order to better understand retaining rendering quality. the characteristic of grid models, we estimated the distri-bution of voxel importance scores (shown in Fig. 4) and
Figure 2. (a) NeRF learns a mapping from 3D coordinate (x, y, z) and viewing direction (θ, ϕ) to color and density (r, g, b, σ). (b)
Volumetric NeRF optimizes volumetric grids, estimates color features Fx,y,z and density for sampling points via tri-linear interpolation to get the final color through (or without) tiny MLPs. (c) VQRF compresses voxel features to codebook and stores per-voxel k-bits mapping index, pointing to the codebook consisting of 2k codes. observed that only 10% voxels contribute over 99% im-portance scores of a grid model, indicating that large re-dundancy exists in the model. Inspired by traditional tech-niques of deep network compression [14], we present an ef-fective and efficient framework for compressing volumetric radiance fields, allowing about 100× storage reduction over original grid models, with competitive rendering quality.
The illustration of the idea and the framework are shown in Fig. 2 and Fig. 3. The proposed framework is quite gen-eral rather than restricted to certain architecture. It is com-prised of three steps, i.e., voxel pruning, vector quantization and post processing. Voxel pruning is used to omit the least important voxels which dominate model size while con-tributing little to the final rendering. We introduce an adap-tive strategy for pruning threshold selection with the aid of a cumulative score rate metric, enabling the pruning strategy general across different scenes or base models. In order to further reduce model size, we propose to encode important voxel features into a compact codebook by developing an importance-aware vector quantization with an efficient op-timization strategy, and using a joint tuning mechanism to enable the compressed models approaching to the rendering quality of original ones. We finally perform a simple post-processing step to obtain a model with quite small storage cost. For example, as shown in Fig. 1, the volumetric model with the storage cost of 104 MB and the rendering quality of PSNR 32.66 can be compressed into a tiny model costing 1.05 MB with a negligible visual quality loss (PSNR 32.65).
We conduct extensive experiments and empirical studies to validate the method, showing the effectiveness and gener-alization of the proposed compression pipeline on a wide range of volumetric methods and varying scenarios. 2.