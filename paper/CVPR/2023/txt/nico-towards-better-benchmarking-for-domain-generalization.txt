Abstract
Despite the remarkable performance that modern deep neural networks have achieved on independent and iden-tically distributed (I.I.D.) data, they can crash under dis-tribution shifts. Most current evaluation methods for do-main generalization (DG) adopt the leave-one-out strat-egy as a compromise on the limited number of domains.
We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational eval-uation methods for comprehensively evaluating DG algo-rithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively.
Two novel generalization bounds from the perspective of data construction are proposed to prove that limited con-cept shift and significant covariate shift favor the evalua-tion capability for generalization. Through extensive ex-periments, NICO++ shows its superior evaluation capabil-ity compared with current DG datasets and its contribu-tion in alleviating unfairness caused by the leak of oracle knowledge in model selection. The data and code for the benchmark based on NICO++ are available at https:
//github.com/xxgege/NICO-plus. 1.

Introduction
Machine learning has illustrated its excellent capability in a wide range of areas [37, 65, 82]. Most current algo-rithms minimize the empirical risk in training data relying on the assumption that training and test data are indepen-dent and identically distributed (I.I.D.). However, this ideal hypothesis is hardly satisfied in real applications, especially those high-stake applications such as healthcare [10, 49], autonomous driving [1, 13, 39] and security systems [6], owing to the limitation of data collection and intricacy of the scenarios. Distribution shifts between training and test data may lead to the unreliable performance of current approaches in practice. Hence, instead of generalization
â€ Equal contribution
*Corresponding Author
Figure 1. Covariate shift (Mcov in Equation (1)) and concept shift (Mmax in Equation (2)) of NICO++ and current DG datasets. cpt
NICO++ has the lowest concept shift and highest covariate shift, showing the superiority in evaluation capability. within the training distribution, the ability to generalize un-der distribution shift, domain generalization (DG) [75, 94], is of more critical significance in realistic scenarios.
In the field of computer vision, benchmarks that pro-vide the common ground for competing approaches often play a role of catalyzer promoting the advance of research
[14]. An advanced DG benchmark should provide sufficient diversity in distributions for both training and evaluating
DG algorithms [74, 78] while ensuring essential common knowledge of categories for inductive inference across do-mains [33, 34, 93]. The first property drives generalization challenging, and the second ensures the solvability [81].
This requires adequate distinct domains and instructive fea-tures for each category shared among all domains.
Current DG benchmarks, however, either lack sufficient domains (e.g., 4 domains in PACS [40], VLCS [18] and
Office-Home [73] and 6 in DomainNet [53]) or too simple or limited to simulating significant distribution shifts in real scenarios [2, 21, 30]. To enrich the diversity and perplexing distribution shifts in training data as much as possible, most of the current evaluation methods for DG adopt the leave-one-out strategy, where one domain is considered as the test domain and the others for training. This is not an ideal eval-uation for generalization but a compromise due to the lim-ited number of domains in current datasets, which impairs
the evaluation capability. To address this issue, we suggest testing DG methods on multiple test domains instead of one specific domain in each evaluation after training.
To benchmark DG methods comprehensively and sim-ulate real scenarios where a trained model may encounter any possible test data while providing sufficient diversity in the training data, we construct a large-scale DG dataset named NICO++ with extensive domains and two proto-cols supported by aligned and flexible domains across cate-gories, respectively, for better evaluation. Our dataset con-sists of 80 categories, 10 aligned common domains for all categories, 10 unique domains specifically for each cat-egory, and more than 230,000 images. Abundant diver-sity in both domain and category supports flexible assign-ments for training and test, controllable degree of distribu-tion shifts, and extensive evaluation on multiple target do-mains. Images collected from real-world photos and consis-tency within category concepts provide sufficient common knowledge for recognition across domains on NICO++.
To evaluate DG datasets in-depth, we investigate dis-tribution shifts on images (covariate shift) and common knowledge for category discrimination across domains (concept agreement) within them. Formally, we present quantification for covariate shift and the opposite of concept agreement, namely concept shift, via two novel metrics. We propose two novel generalization bounds and analyze them from the perspective of data construction instead of models.
Through these bounds, we prove that limited concept shift and significant covariate shift favor the evaluation capabil-ity for generalization.
Moreover, a critical yet common problem in DG is the model selection and the potential unfairness in the compar-ison caused by leveraging the knowledge of target data to choose hyperparameters that favors test performance [3,27].
This issue is exacerbated by the notable variance of test per-formance with various algorithm irrelevant hyperparame-ters on current DG datasets. Intuitively, strong and unsta-ble concept shift such as confusing mapping relations from images to labels across domains embarrasses training con-vergence and enlarges the variance.
We conduct extensive experiments on three levels. First, we evaluate NICO++ and current DG datasets with the pro-posed metrics and show the superiority of NICO++ in eval-uation capability, as shown in Figure 1. Second, we con-duct copious experiments on NICO++ to benchmark cur-rent representative methods with the proposed protocols.
Results show that the room for improvement of generaliza-tion methods on NICO++ is spacious. Third, we show that
NICO++ helps alleviate the issue by squeezing the possible improvement space of oracle leaking and contributes as a fairer benchmark to the evaluation of DG methods, which meets the proposed metrics. 2.