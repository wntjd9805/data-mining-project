Abstract
Federated Learning (FL) has emerged as a de facto ma-chine learning area and received rapid increasing research interests from the community. However, catastrophic forget-ting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspira-tion from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server’s rich computing and mem-ory resources. Furthermore, we elaborate a memory reduc-tion strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of
GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers.
At last, our extensive experiments on various image classi-fication tasks show that GradMA achieves significant per-formance gains in accuracy and communication efficiency compared to SOTA baselines. We provide our code here: https://github.com/lkyddd/GradMA. 1.

Introduction
Federated Learning (FL) [18, 26] is a privacy-preserving distributed machine learning scheme in which workers jointly participate in the collaborative training of a central-ized model by sharing model information (parameters or updates) rather than their private datasets. In recent years,
FL has shown its potential to facilitate real-world appli-cations, which falls broadly into two categories [10]: the cross-silo FL and the cross-device FL. The cross-silo FL corresponds to a relatively small number of reliable work-ers, usually organizations, such as healthcare facilities [9] and financial institutions [41], etc. In contrast, for the cross-*Corresponding author device FL, the number of workers can be very huge and unreliable, such as mobile devices [26], IoT [27] and au-tonomous driving cars [22], among others. In this paper, we focus on cross-device FL.
The privacy-preserving and communication-efficient properties of the cross-device FL make it promising, but it also confronts practical challenges arising from data hetero-geneity (i.e., non-iid data distribution across workers) and partial participation [5, 12, 20, 39]. Specifically, the datasets held by real-world workers are generated locally accord-ing to their individual circumstances, resulting in the dis-tribution of data on different workers being not identical.
Moreover, owing to the flexibility of worker participation in many scenarios (e.g., IoT and mobile devices), workers can join or leave the FL system at will, thus making the set of active workers random and time-varying across commu-nication rounds. Note that we consider a worker participates or is active at round t (i.e., the index of the communication round) if it is able to complete the computation task and send back model information at the end of round t.
The above-mentioned challenges mainly bring catas-trophic forgetting (CF) [25, 30, 37] to FL. In a typical FL process, represented by FedAvg [26], a server updates the centralized model by iteratively aggregating the model in-formation from workers that generally is trained over sev-eral steps locally before being sent to the server. On the one hand, due to data heterogeneity, the model is updated on private data in local training, which is prone to overfit the current knowledge and forget the previous experience, thus leading to CF [8]. In other words, the updates of the lo-cal models are prone to drift and diverge increasingly from the update of the centralized model [12]. This can seriously deteriorate the performance of the centralized model. To ameliorate this issue, a variety of existing efforts regular-ize the objectives of the local models to align the central-ized optimization objective [1, 12, 13, 17, 19]. On the other hand, the server can only aggregate model information from active workers per communication round caused by partial participation. In this case, many existing works directly dis-card [1, 11, 12, 18, 26, 39] or implicitly utilize [7, 28], by means of momentum, the information provided by work-ers who have participated in the training but dropped out in the current communication round (i.e., stragglers). This results the centralized model, which tends to forget the ex-perience of the stragglers, thus inducing CF. In doing so, the convergence of popular FL approaches (e.g., FedAvg) can be seriously slowed down by stragglers. Moreover, all above approaches solely aggregate the collected informa-tion by averaging in the server, ignoring the server’s rich computing and memory resources that could be potentially harnessed to boost the performance of FL [45].
In this paper, to alleviate CF caused by data heterogene-ity and stragglers, we bring forward a new FL approach, dubbed as GradMA (Gradient-Memory-based Accelerated
Federated Learning), which takes inspiration from contin-ual learning (CL) [4,14,24,29,44] to simultaneously correct the server-side and worker-side update directions and fully utilize the rich computing and memory resources of the server. Concretely, motivated by the success of GEM [24] and OGD [4], two memory-based CL methods, we invoke quadratic programming (QP) and memorize updates to cor-rect the update directions. On the worker side, GradMA harnesses the gradients of the local model in the previous step and the centralized model, and the parameters differ-ence between the local model in the current step and the centralized model as constraints of QP to adaptively correct the gradient of the local model. Furthermore, we maintain a memory state to memorize accumulated update of each worker on the server side. GradMA then explicitly takes the memory state to constrain QP to augment the momen-tum (i.e., the update direction) of the centralized model.
Here, we need the server to allocate memory space to store memory state. However, it may be not feasible in FL scenar-ios with a large size of workers, which can increase the stor-age cost and the burden of computing QP largely. There-fore, we carefully craft a memory reduction strategy to alle-viate the said limitations. In addition, we theoretically ana-lyze the convergence of GradMA in the smooth non-convex setting.
To sum up, we highlight our contributions as follows:
• We formulate a novel FL approach GradMA, which aims to simultaneously correct the server-side and worker-side update directions and fully harness the server’s rich computing and memory resources. Mean-while, we tailor a memory reduction strategy for
GradMA to reduce the scale of QP and memory cost.
• For completeness, we analyze the convergence of
GradMA theoretically in the smooth non-convex set-ting. As a result, the convergence result of GradMA achieves the linear speed up as the number of selected active workers increases.
• We conduct extensive experiments on four com-monly used image classification datasets (i.e., MNIST,
CIFAR-10, CIFAR-100 and Tiny-Imagenet) to show that GradMA is highly competitive compared with other state-of-the-art baselines. Meanwhile, ablation studies demonstrate efficacy and indispensability for core modules and key parameters. 2.