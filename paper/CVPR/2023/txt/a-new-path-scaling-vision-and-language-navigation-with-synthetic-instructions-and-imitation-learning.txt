Abstract
Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navi-gation instructions in photorealistic environments, as a step towards robots that can follow human instructions. How-ever, given the scarcity of human instruction data and lim-ited diversity in the training environments, these agents still struggle with complex language grounding and spa-tial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investi-gate large-scale augmentation with synthetic instructions.
We take 500+ indoor environments captured in densely-sampled 360◦ panoramas, construct navigation trajecto-ries through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky [63], a high-quality multilingual navigation instruction genera-tor. We also synthesize image observations from novel view-points using an image-to-image GAN [27]. The resulting dataset of 4.2M instruction-trajectory pairs is two orders of magnitude larger than existing human-annotated datasets, and contains a wider variety of environments and view-points. To efﬁciently leverage data at this scale, we train a simple transformer agent with imitation learning. On the challenging RxR dataset, our approach outperforms all ex-isting RL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen environments, and from 64.6 to 66.8 in unseen test environments. Our work points to a new path to improving instruction-following agents, emphasizing large-scale training on near-human quality synthetic instructions. 1.

Introduction
Developing intelligent agents that follow human instruc-tions is a long-term, formidable challenge in AI [66]. A recent focus addressing this problem space is Vision-and-Language Navigation (VLN) [3, 9]. Navigation is an ideal
∗Equal Contribution. †Work done while at Google Research.
Figure 1. Simpler agents with more data: We investigate large-scale augmentation using 500+ environments annotated with syn-thetic instructions that approach human quality. test bed for studying instruction-following, since the task can be simulated photo-realistically at scale and evaluation is straightforward. However, datasets that capture the lin-guistic diversity and idiosyncrasies of real human instruc-tors are small and expensive to collect.
Shortages of human-annotated training data for other vision-and-language tasks have been partially addressed by pretraining transformers on up to billions of image-text pairs. This has underpinned dramatic improvements in image captioning [65, 70], visual question answering
[59], phrase grounding [26, 35], text-to-video retrieval, video question answering [32] and text-to-image synthesis
[49, 69]. However, these are all static image or video tasks, whereas VLN agents interact with 3D environments.
In
VLN, pretraining on large image-text and text-only datasets has been thoroughly explored [21, 22, 40, 45], but improve-ments are more limited. Arguably, progress in VLN has plateaued while still leaving a large gap between machine and human performance [73]. We hypothesize that static image-text and text-only datasets – despite their size – lack the spatially grounded and action-oriented language needed for effective VLN pretraining. Consider instructions from the Room-across-Room (RxR) dataset [30], which illus-trate that wayﬁnding requires an understanding of allocen-tric and egocentric spatial expressions (near a grey console table behind you), verbs (climb the stairs), imperatives and negations (do not enter the room in front) and temporal conditions (walk until you see an entrance on your left).
Such expressions are rarely found in image-text datasets.
Though similar expressions are found in text-only corpora, their meaning as it relates to the physical world is hard to infer from text alone (without sensorimotor context) [6].
To address this problem, we investigate large-scale augmentation with synthetic in-domain data, i.e., model-generated navigation instructions for trajectories in realistic 3D environments using previously developed components
[27, 63]. We construct a large dataset using Marky [63], which generates VLN instructions that approach the qual-ity of human instructors. [63] released the 1M Marky instruction-trajectory pairs situated in 61 Matterport3D [7] environments. To increase the diversity of the environments (and thus the scenes and objects available in them), we au-tomatically annotate an additional 491 environments from the Gibson dataset [67]. Gibson environments have been underutilized in prior VLN work due to the lack of navi-gation graphs indicating navigable trajectories through its densely-sampled 360◦ panoramas. We train a model that classiﬁes navigable directions for Matterport3D and use it to construct the missing navigation graphs. We sample 3.2M trajectories from these graphs and annotate them with
Marky. To further increase the variability of trajectories, we synthesize image observations from novel viewpoints using an image-to-image GAN [27]. The resulting dataset is two orders of magnitude larger than existing human-annotated ones, and contains a wider variety of scenes and viewpoints.
We have released our Gibson navigation graphs and the
Marky-Gibson dataset.2
With orders of magnitude more training examples and environments, we explore VLN agent performance with imitation learning (IL), i.e., behavioral cloning and DAG-GER [53] IL can take advantage of high-throughput trans-former frameworks such as T5 [48] and thus efﬁciently train on 4.2M instructions (accumulating over 700M steps of ex-perience). This is a departure from most prior VLN work in low-data settings, e.g. [10] report that pure IL under-performs by 8.5% success rate compared to agents trained with both IL and online reinforcement learning (RL) algo-rithms such as A3C [44]. However, IL outperforms RL in related tasks with sufﬁcient training data [50]. Online
RL also requires interacting with the environment at each this precludes efﬁcient data prefetching and paral-step; lel preprocessing and thus imposes unavoidable overhead compared to IL. Empirically, we conﬁrm that training ex-isting models such as HAMT [10] on 4.2M instructions is infeasible without ground-up re-engineering, though we do
ﬁnd incorporating 10K additional synthetic instructions into 2//github.com/google-research-datasets/RxR/tree/main/marky-mT5
HAMT training modestly improves performance. Training with IL aligns with the trend towards large-scale multi-task vision-and-language models trained with supervised learn-ing; these have uniﬁed tasks as diverse as visual question answering, image captioning, object detection, image clas-siﬁcation, OCR and text reasoning [12] – and could include
VLN in future.
Experimentally, in detailed ablation studies we show that adding Gibson environments, synthesizing additional image observations from novel viewpoints, increasing the capacity of the transformer, and ﬁnetuning with DAGGER all im-prove agent performance. On the challenging RxR dataset – which contains multilingual instructions with a median trajectory length of 15m – our best agent using only imita-tion learning outperforms all prior RL agents. Evaluating on novel instruction-trajectories in seen environments (Val-Seen), we improve over the state-of-the-art by 8%, reaching 79.1 NDTW. In new, unseen environments (Test), we im-prove by 2%, achieving 66.8 NDTW. We also show that that self-training with synthetic instructions in new envi-ronments (still without human annotations) improves per-formance by an additional 2% to 68.6 NDTW. Overall, our
RxR results point to a new path to improving instruction-following agents, emphasizing large-scale training on near-human quality synthetic instructions. Perhaps surprisingly, on the English-only R2R dataset [3], our IL agent achieves strong but not state-of-the-art results. Marky was trained on RxR, so we attribute this to domain differences between
R2R and RxR, underscoring the domain dependence of syn-thetic instructions. 2.