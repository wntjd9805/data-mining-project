Abstract
The most recent efforts in video matting have focused on eliminating trimap dependency since trimap annotations are expensive and trimap-based methods are less adapt-able for real-time applications. Despite the latest tripmap-free methods showing promising results, their performance often degrades when dealing with highly diverse and un-structured videos. We address this limitation by introduc-ing Adaptive Matting for Dynamic Videos, termed AdaM, which is a framework designed for simultaneously differen-tiating foregrounds from backgrounds and capturing alpha matte details of human subjects in the foreground. Two in-terconnected network designs are employed to achieve this goal: (1) an encoder-decoder network that produces alpha mattes and intermediate masks which are used to guide the transformer in adaptively decoding foregrounds and back-grounds, and (2) a transformer network in which long- and short-term attention combine to retain spatial and tempo-ral contexts, facilitating the decoding of foreground de-tails. We benchmark and study our methods on recently introduced datasets, showing that our model notably im-proves matting realism and temporal coherence in complex real-world videos and achieves new best-in-class general-izability. Further details and examples are available at https://github.com/microsoft/AdaM . 1.

Introduction
Video human matting aims to estimate a precise alpha matte to extract the human foreground from each frame of an input video. In comparison with image matting [6,10,14, 21, 30, 39, 44, 47], video matting [2, 5, 11, 18, 19, 33, 36, 38] presents additional challenges, such as preserving spatial and temporal coherence.
Many different solutions have been put forward for the video matting problem. A straightforward approach is to build on top of image matting models [44], which is to im-plement an image matting approach frame by frame. It may, however, result in inconsistencies in alpha matte predictions across frames, which will inevitably lead to flickering ar-tifacts [42]. On the other hand, top performers leverage dense trimaps to predict alpha mattes, which is expensive and difficult to generalize across large video datasets. To alleviate the substantial trimap limitation, OTVM [35] pro-posed a one-trimap solution recently. BGM [22, 33] pro-poses a trimap-free solution, which needs to take an addi-tional background picture without the subject at the time of capture. While the setup is less time-consuming than cre-ating trimaps, it may not work well if used in a dynamic background environment. The manual prior required by these methods limits their use in some real-time applica-tions, such as video conferencing. Lately, more general so-lutions, e.g., MODNet [16] and RVM [23], have been pro-posed which involve manual-free matting without auxiliary inputs. However, in challenging real-world videos, back-grounds are inherently non-differentiable at some points, causing these solutions to produce blurry alpha mattes.
It is quite challenging to bring together the benefits of both worlds, i.e., a manual-free model that produces accu-rate alpha mattes in realistic videos. In our observation, the
fied handling of complex unconstrained videos without requiring manual efforts. The proposed method pro-vides a data-driven estimation of the foreground masks to guide the network to distinguish foregrounds and backgrounds adaptively.
â€¢ Our network architecture and training scheme have been carefully designed to take advantage of both long-and short-range spatial and motion cues.
It reaches top-tier performance on the VM [23] and CRGNN [42] benchmarks.
Figure 2. Qualitative sample results of MODNet [16], RVM [23] and the proposed AdaM on real video scenes. 2.