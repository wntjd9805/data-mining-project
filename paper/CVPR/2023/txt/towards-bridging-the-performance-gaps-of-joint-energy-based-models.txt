Abstract
Can we train a hybrid discriminative-generative model with a single network? This question has recently been answered in the affirmative, introducing the field of Joint
Energy-based Model (JEM) [17, 48], which achieves high classification accuracy and image generation quality si-multaneously. Despite recent advances, there remain two performance gaps: the accuracy gap to the standard soft-max classifier, and the generation quality gap to state-of-the-art generative models.
In this paper, we introduce a variety of training techniques to bridge the accuracy gap and the generation quality gap of JEM. 1) We incorporate a recently proposed sharpness-aware minimization (SAM) framework to train JEM, which promotes the energy land-scape smoothness and the generalization of JEM. 2) We exclude data augmentation from the maximum likelihood estimate pipeline of JEM, and mitigate the negative im-pact of data augmentation to image generation quality. Ex-tensive experiments on multiple datasets demonstrate our
SADA-JEM achieves state-of-the-art performances and out-performs JEM in image classification, image generation, calibration, out-of-distribution detection and adversarial robustness by a notable margin. Our code is available at https://github.com/sndnyang/SADAJEM . 1.

Introduction
Deep neural networks (DNNs) have achieved state-of-the-art performances in a wide range of learning tasks, in-cluding image classification, image generation, object de-tection, and language understanding [21, 30]. Among them, energy-based models (EBMs) have seen a flurry of inter-est recently, partially inspired by the impressive results of
IGEBM [10] and JEM [17], which exhibit the capability of training generative models within a discriminative frame-work. Specifically, JEM [17] reinterprets the standard soft-max classifier as an EBM and achieves impressive perfor-mances in image classification and generation simultane-ously. Furthermore, these EBMs enjoy improved perfor-mance on out-of-distribution detection, calibration, and ad-versarial robustness. The follow-up works (e.g., [18, 48]) further improve the training in terms of speed, stability and accuracy.
Despite the recent advances and the appealing property of training a single network for hybrid modeling, training
JEM is still challenging on complex high-dimensional data since it requires an expensive MCMC sampling. Further-more, models produced by JEM still have an accuracy gap to the standard softmax classifier and a generation quality gap to the GAN-based approaches.
In this paper, we introduce a few simple yet effective training techniques to bridge the accuracy gap and gener-ation quality gap of JEM. Our hypothesis is that both per-formance gaps are the symptoms of lack of generalization of JEM trained models. We therefore analyze the trained models under the lens of loss geometry. Figure 1 visu-alizes the energy landscapes of different models by the technique introduced in [34]. Since different models are trained with different loss functions, visualizing their loss functions is meaningless for the purpose of comparison.
Therefore, the LSE energy functions (i.e., Eq. 4) of dif-ferent models are visualized. Comparing Figure 1(a) and (b), we find that JEM converges to extremely sharp lo-cal maxima of the energy landscape as manifested by the significantly large y-axis scale. By incorporating the re-cently proposed sharpness-aware minimization (SAM) [12] to JEM, the energy landscape of trained model (JEM+SAM) becomes much smoother as shown in Figure 1(c). This also substantially improves the image classification accu-racy and generation quality. To further improve the en-ergy landscape smoothness, we exclude data augmentation from the maximum likelihood estimate pipeline of JEM, and visualize the energy landscape of SADA-JEM in Fig-ure 1(d), which achieves the smoothest landscape among all the models considered. This further improves image gen-eration quality dramatically while retaining or sometimes improving classification accuracy. Since our method im-proves the performance of JEM primarily in the framework of sharpness-aware optimization, we refer it as SADA-JEM, a Sharpness-Aware Joint Energy-based Model with single branched Data Augmentation.
(a) Softmax Classifier (b) JEM (c) JEM+SAM (d) SADA-JEM
Figure 1. Visualizing the energy landscapes [34] of different models trained on CIFAR10. Note the dramatic scale differences of the y-axes, indicating SADA-JEM identifies the smoothest local optimum among all the methods considered.
Our main contributions are summarized as follows: 1. We investigate the energy landscapes of different mod-els and find that JEM leads to the sharpest one, which potentially undermines the generalization of trained models. 2. We incorporate the sharpness-aware minimization (SAM) framework to JEM to promote the energy land-scape smoothness, and thus model generalization. 3. We recognize the negative impact of data augmenta-tion in the training pipeline of JEM, and introduce two data loaders for image classification and image gen-eration separately, which improves image generation quality significantly. 4. Extensive experiments on multiple datasets show that
SADA-JEM achieves the state-of-the-art discrimina-tive and generative performances, while outperforming
JEM in calibration, out-of-distribution detection and adversarial robustness by a notable margin. 2.