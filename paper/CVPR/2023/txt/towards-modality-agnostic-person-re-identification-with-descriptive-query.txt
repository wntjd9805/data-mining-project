Abstract
Person re-identiﬁcation (ReID) with descriptive query (text or sketch) provides an important supplement for gen-eral image-image paradigms, which is usually studied in a single cross-modality matching manner, e.g., text-to-image or sketch-to-photo. However, without a camera-captured photo query, it is uncertain whether the text or sketch is available or not in practical scenarios. This motivates us to study a new and challenging modality-agnostic person re-identiﬁcation problem. Towards this goal, we propose a uniﬁed person re-identiﬁcation (UNIReID) architecture that can effectively adapt to cross-modality and multi-modality tasks. Speciﬁcally, UNIReID incorporates a simple dual-encoder with task-speciﬁc modality learning to mine and fuse visual and textual modality information.
To deal with the imbalanced training problem of different tasks in
UNIReID, we propose a task-aware dynamic training strat-egy in terms of task difﬁculty, adaptively adjusting the train-ing focus. Besides, we construct three multi-modal ReID datasets by collecting the corresponding sketches from pho-tos to support this challenging study. The experimental results on three multi-modal ReID datasets show that our
UNIReID greatly improves the retrieval accuracy and gen-eralization ability on different tasks and unseen scenarios. 1.

Introduction
Person re-identiﬁcation [42] involves using computer vi-sion techniques to identify pedestrians in video and still images. Given a monitored pedestrian image/video or a text description, ReID aims to retrieve all images/videos of that pedestrian across devices. ReID is widely used in in-telligent video surveillance, intelligent security, and other
ﬁelds. The existing ReID researches include single-modal
*Corresponding Author: Mang Ye (yemang@whu.edu.cn)
Uncertain 
U
D
Descriptive 
Query
Query 1:
Text girl was
“The wearing white short sleeves, pink shorts and …”
Query 2:
Sketch girl was
“The wearing white short sleeves, pink shorts and …”
Query 3: Text +Sketch
Model 1
Model 2
Model 3
Paradigm 1: Text-to-RGB
Paradigm 2: Sketch-to-RGB
Paradigm 3: Text+Sketch-to-RGB (a) Existing Methods (b) Our Method
Query 1: Text
Query 2: Sketch
Query 3: Text+Sketch
Unified 
ReID
Model
Paradigm 4: Uncertain Query-to-RGB
Figure 1.
Illustration of our idea. Existing ReID methods
[17, 24, 43] yield different paradigm models for different descrip-tive queries. However, it is uncertain whether the text or sketch is available or not in practical scenarios. Our uniﬁed ReID model enables target pedestrian search under uncertain query inputs. The green boxes match the query.
ReID [4, 13, 21, 47, 48] and cross-modal ReID [2, 17, 41].
The former is restricted to retrieval between RGB images, whereas the latter allows retrieval of RGB images based on different query modalities (e.g., IR, text, or sketch). Partic-ularly, the appearance of a suspect may only be described verbally in many criminal cases. Person re-identiﬁcation with descriptive query (text or sketch) is well suited for these scenarios and has greater research value for grounded applications of ReID models.
Most existing text-based or sketch-based person re-identiﬁcation methods rely on only one of the modalities as a query set to achieve pedestrian retrieval. Although the text modality is relatively easy to access, it fails to accurately de-pict visual appearance details, i.e., coarse-grained represen-tations [29]. As the saying goes, a picture is worth a thou-sand words, and the sketch image of a pedestrian is closer to the visual modality, showing speciﬁc information about
the target pedestrian, such as structural information. Since each task is trained independently, as shown in Fig 1(a), it is impossible to generalize to unseen modalities. For ex-ample, a ReID model trained on text-based datasets is basi-cally invalid on sketch-based scenes, and vice versa. This greatly limits the applicability of existing methods for prac-tical model deployment.
Meanwhile, multi-modal fusion has been proven to be an important technique to improve model accuracy in com-puter vision, e.g., multi-modal face anti-spooﬁng [11], multi-modal generation and understanding [18]. Recently,
Zhai et al. [43] ﬁrst propose to implement multi-modal
ReID using both sketch and text modalities as the query.
Their experimental results indicate that the combination of text and sketch modalities enhances the performance of the
ReID model. However, this method adopts independent text and image pre-training parameters for multi-modal rep-resentation learning, which has poor generalizability and yields low accuracy. More importantly, it is often uncer-tain whether text or sketch is available in a real scenario, i.e., modality deﬁcit problems often arise when a speciﬁc modality is not available. Due to the independent training of tasks, existing cross-modal or multi-modal ReID methods are difﬁcult to handle this problem. A smarter surveillance system should be capable of handling various modalities of information efﬁciently. Therefore, in this paper, we propose the concept of modality-agnostic person re-identiﬁcation to handle the modality uncertainty issue.
Speciﬁcally, we design a uniﬁed person re-identiﬁcation architecture (UNIReID) in Fig 1(b), which is effective in both cross-modal and multi-modal applications. The great-est challenge in unifying learning across modalities is to mine a shared semantic representation space. At ﬁrst, we propose a task-speciﬁc modality learning scheme to support individual task learning. Essentially, this scheme considers uniﬁed person re-identiﬁcation as a set of retrieval tasks in-volving Text-to-RGB, Sketch-to-RGB, and Text+Sketch-to-RGB. Inspired by CLIP [25] which is a pre-trained model for matching image and text modalities, UNIReID uses a simple dual-encoder for visual modality and textual modal-ity feature extraction and fusion. All visual modalities share a single image encoder. For multi-modal ReID, we fuse the sketch and text modalities into a single query by a sim-ple feature-level summation. Task-speciﬁc metric learn-ing explicitly minimizes the feature distances between var-ious types of query samples and gallery samples to learn modality-shared feature representations.
In addition, considering that tasks have varying difﬁcul-ties, uniﬁed ReID faces an additional challenge in balanc-ing learning among tasks, which may result in the overﬁt-ting of individual tasks. To handle this problem, we design a task-aware dynamic training strategy that adaptively ad-justs for training imbalances between tasks. The rationale for dynamic training is to modulate the loss contribution of different retrieval tasks according to the training difﬁculty of tasks (i.e., prediction conﬁdence). Our dynamic train-ing strategy improves generalization capability by tending to train difﬁcult tasks. Finally, a cross-modality interaction is designed to align sketch and text feature representations.
In view of the differences between sketch and text modal features, we minimize the similarity distribution distances between sketch-RGB and text-RGB pairs to align modal-ity information for modality fusion. With the help of rich multi-modal data, our model achieves mutual enhancement of tasks and improves the robustness against diverse query modality variations.
To facilitate the modality-agnostic ReID study, we construct three multi-modal ReID datasets. Concretely, based on the text-based ReID datasets, namely CUHK-PEDES [17], ICFG-PEDES [7], and RSTPReid [50], we collect the sketch modality for each identity. Consider-ing that sketching by hand is time-consuming and labor-intensive, we propose to generate the sketch modality for each identity from the photo modality. The detailed collec-tion information for the datasets can be found in Section 3.
The main contributions of this paper are listed below:
• We start the ﬁrst attempt to investigate the modality-agnostic person re-identiﬁcation with the descriptive query, which provides a ﬂexible solution to deal with query uncertainty in real-world scenarios.
• We introduce a novel uniﬁed person re-identiﬁcation (UNIReID) architecture based on a dual-encoder to jointly integrate cross-modal and multi-modal task learning. With task-speciﬁc modality learning and task-aware dynamic training, UNIReID enhances gen-eralization ability across tasks and domains.
• We contribute three multi-modal ReID datasets to support uniﬁed ReID evaluation. Extensive experi-ments on both multi-modal matching and generalized cross-modality matching have veriﬁed the advantage of UNIReID, achieving much higher accuracy than ex-isting counterparts. 2.