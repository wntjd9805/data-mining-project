Abstract
In practical scenarios where training data is limited, many predictive signals in the data can be rather from some biases in data acquisition (i.e., less generalizable), so that one cannot prevent a model from co-adapting on such (so-called) “shortcut” signals: this makes the model fragile in various distribution shifts. To bypass such failure modes, we consider an adversarial threat model under a mutual information constraint to cover a wider class of perturba-tions in training. This motivates us to extend the standard information bottleneck to additionally model the nuisance information. We propose an autoencoder-based training to implement the objective, as well as practical encoder designs to facilitate the proposed hybrid discriminative-generative training concerning both convolutional- and Transformer-based architectures. Our experimental results show that the proposed scheme improves robustness of learned repre-sentations (remarkably without using any domain-speciﬁc knowledge), with respect to multiple challenging reliability measures. For example, our model could advance the state-of-the-art on a recent challenging OBJECTS benchmark in novelty detection by 78.4% → 87.2% in AUROC, while simultaneously enjoying improved corruption, background and (certiﬁed) adversarial robustness. Code is available at https://github.com/jh-jeong/nuisance_ib. 1.

Introduction
Despite the recent breakthroughs in computer vision in aid of deep learning, e.g., in image/video recogni-tion [9, 20, 109], synthesis [42, 55, 96, 125], and 3D scene rendering [85, 86, 104], deploying deep learning models to the real-world still places a burden on contents providers as it affects the reliability of their services. In many cases, deep neural networks make substantially fragile predictions for out-of-distribution inputs, i.e., samples that are not likely
*Work done at KAIST.
Figure 1. A high-level illustration of our method, nuisance-extended information bottleneck (NIB). In this paper, we focus on scenarios when the input x can be corrupted x → ˆx in test-time while preserving its semantics. Unlike the standard cross-entropy training (CE), NIB aims to encode every target-correlated signal in x, some of which can be more reliable under distribution shifts. from the training distribution, even when the inputs are se-mantically close enough to in-distribution samples for hu-mans [35, 102]. Such a vulnerability can be a signiﬁcant threat in risk-sensitive systems, such as autonomous driving, medical imaging, and health-care applications, to name a few [2]. Overall, the phenomena highlight that deep neural networks tend to extract “shortcut” signals [26] from given limited (or potentially biased) training data in practice.
To address such concerns, multiple literatures have been independently developed based on different aspects of model reliability. Namely, their methods use different threat mod-els and benchmarks, depending on how a shift in input dis-tribution happens in test-time, and how to evaluate model performance against the shift. For example, in the con-text of adversarial robustness [11, 17, 82, 128], a typical threat model is to consider the worst-case noise inside a
ﬁxed ℓp-ball around given test samples. Another example of corruption robustness [34, 35, 39, 116] instead assumes pre-deﬁned types of common corruptions (e.g., Gaussian noise, fog, etc.) that applies to the test samples. Lastly, novelty detection [36, 72, 74, 76] usually tests whether a model can detect a speciﬁc benchmark dataset as out-of-distribution from the (in-distribution) test samples.
Due to discrepancy between each of “ideal” objectives and its practical threat models, however, the literatures have commonly found that optimizing under a certain threat model often hardly generalizes to other threat ones: e.g., (a) several works [16, 62, 121] have observed that standard adversarial training [82] often harms other reliability mea-sures such as corruption robustness or uncertainty estimation, as well as its classiﬁcation performance; (b) Hendrycks et al. [34] criticize that none of the previous claims on cor-ruption robustness could consistently generalize on a more comprehensive benchmark. This also happens even for threat models targeting the same objective: e.g., (c) Yang et al. [122] show that state-of-the-arts in novelty detection are often too sensitive, so that they tend to also detect “near-in-distribution” samples as out-of-distribution and perform poorly on a benchmark regarding this. Overall, these obser-vations suggest that one should avoid optimizing reliability measures assuming a speciﬁc threat model or benchmark, and motivate to ﬁnd a new threat model that is generally applicable for diverse scenarios of reliability concerns.
Contribution. In this paper, we propose nuisance-extended information bottleneck (NIB), a new training objective tar-geting model reliability without assuming a prior on domain-speciﬁc tasks. Our method is motivated by rethinking the information bottleneck (IB) principle [107, 108] under pres-ence of distribution shifts. Speciﬁcally, we argue that a
“robust” representation z := f (x) should always encode ev-ery signal in the input x that is correlated with the target y, rather than extracting only a few shortcuts (e.g., Figure 1).
This motivates us to consider an adversarial form of threat model on distribution shifts in x, under a constraint on the mutual information I(x, y). To implement this idea, we propose a practical design by incorporating a nuisance rep-resentation zn alongside z of the standard IB so that (z, zn) can reconstruct x. This results in a novel synthesis of adver-sarial autoencoder [83] and variational IB [1] into a single framework. For the architectural side, we propose (a) to utilize the internal feature statistics for convolutional net-work based encoders, and (b) to incorporate vector-quantized patch representations for Transformer-based [24] encoders to model zn, mainly to efﬁciently encode the nuisance zn (as well as z) in a scalable manner.
We perform an extensive evaluation on the representations learned by our scheme, showing comprehensive improve-ments in modern reliability metrics: including (a) novelty detection, (b) corruption (or natural) robustness, (c) back-ground robustness and (d) certiﬁed adversarial robustness.
The results are particularly remarkable as the gains are not from assuming a prior on each of speciﬁc threat models. For example, we obtain a signiﬁcant reduction in CIFAR-10-C error rates of the highest severity, i.e., by 26.5% → 19.5%, without extra domain-speciﬁc prior as assumed in recent methods [39, 40]. Here, we also show that the effective-ness of our method is scalable to larger-scale (ImageNet) datasets. For novelty detection, we could advance AUROCs in recent OBJECTS [122] benchmarks by a large margin of 78.4% → 87.2% in average upon the state-of-the-art, show-ing that our representations can provide a more semantic information to better discriminate out-of-distribution sam-ples. Finally, we also demonstrate how the representations can further offer enhanced robustness against adversarial examples, by applying randomized smoothing [17] on them. 2.