Abstract
Diffusion models have achieved great success in a range of tasks, such as image synthesis and molecule design. As such successes hinge on large-scale training data collected from diverse sources, the trustworthiness of these collected data is hard to control or audit. In this work, we aim to explore the vulnerabilities of diffusion models under poten-tial training data manipulations and try to answer: How hard is it to perform Trojan attacks on well-trained diffu-sion models? What are the adversarial targets that such
Trojan attacks can achieve? To answer these questions, we propose an effective Trojan attack against diffusion mod-els, TrojDiff, which optimizes the Trojan diffusion and gen-erative processes during training.
In particular, we de-sign novel transitions during the Trojan diffusion process to diffuse adversarial targets into a biased Gaussian dis-tribution and propose a new parameterization of the Tro-jan generative process that leads to an effective training objective for the attack.
In addition, we consider three types of adversarial targets: the Trojaned diffusion models will always output instances belonging to a certain class from the in-domain distribution (In-D2D attack), out-of-domain distribution (Out-D2D-attack), and one specific in-stance (D2I attack). We evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM dif-fusion models. We show that TrojDiff always achieves high attack performance under different adversarial targets us-ing different types of triggers, while the performance in be-nign environments is preserved. The code is available at https://github.com/chenweixin107/TrojDiff. 1.

Introduction
Recently, diffusion models [1–4] have emerged as the new competitive deep generative models, demonstrating their impressive capacities in generating diverse, high-quality samples in various data modalities [5–7]. Inspired by non-equilibrium thermodynamics [8], diffusion mod-els are latent variable models which consist of two pro-cesses. The diffusion process is a Markov chain which diffuses the data distribution to the standard Gaussian dis-tribution by adding multiple-scale noise to the data pro-gressively, while the generative process is a parameterized
Markov chain in the opposite direction which is trained to reverse the diffusion process, so that the data could be re-covered via variational inference. Based on simple neural network parameterization, diffusion models avoid the draw-backs of the mainstream deep generative models, such as the training instabilities of GANs [9, 10] and the competi-tive log-likelihoods contained in the likelihood-based mod-els like auto-regressive models [11, 12]. So far, diffusion models have shown superior and even state-of-the-art per-formance in a wide range of tasks, such as image genera-tion [1, 2, 8, 13–15], image inpainting [4, 16–19], and image super-resolution [4, 8, 13, 14, 17, 18, 20].
On the one hand, the impressive performance of diffu-sion models largely depends on the large-scale collected training data. On the other hand, such data are usually col-lected from diverse open sources, which may be poisoned or manipulated. One typical threat is Trojan attacks [21–26], which have exhibited threatening attack performance on im-age classification models. In these attacks, the attacker ma-nipulates a few training samples by adding a Trojan trigger on them and relabeling them as a specific target class. Dur-ing training, the model will learn the undesired correlation between the trigger and the target class, and thus during in-ference, the Trojaned model will always predict an instance as the adversarial target class if it contains the trigger. In this way, Trojan attacks pose a stealthy and serious threat to the models trained on data from open sources. Thus, a natural question arises: Can diffusion models be Trojaned?
To explore the vulnerability of diffusion models against
Trojan attacks, in this work, we propose the first Trojan at-tack on diffusion models, named TrojDiff. Particularly, we study two generic diffusion models, i.e., DDPM [1] and
DDIM [2]. The pipeline of TrojDiff is illustrated in the second row of Figure 1. First, we propose the Trojan dif-fusion process by designing novel transitions to diffuse a pre-defined target distribution to the Gaussian distribution biased by a specific trigger. Then, we apply a new parame-terization of the generative process which learns to reverse
Figure 1. Framework of TrojDiff. First row: Benign procedures of DDPM [1]. Second row: Trojan procedures proposed in TrojDiff.
Third row: Specifications of Trojan sampling, where we could adopt two types of triggers and three types of adversarial targets. Note that by replacing q (p, ˜q, ˜p) with qI (pI, ˜qI, ˜pI), the attack procedures are generalized to DDIM [2]. the Trojan diffusion process via an effective training objec-tive. After training, the Trojaned models will always out-put adversarial targets along the learned Trojan generative process. In particular, as shown in the third row of 1, we consider both the blend-based trigger and the patch-based trigger to generate different adversarial shifts on the stan-dard Gaussian distribution. We consider three types of ad-versarial targets based on different attack goals, and the Tro-janed diffusion model can output 1) instances belonging to the adversarial class (target) from the in-domain distribu-tion in In-D2D attack, 2) an out-of-domain distribution in
Out-D2D attack, and 3) a specific instance in D2I attack.
Empirically, TrojDiff achieves high attack performance against DDPM and DDIM on CIFAR-10 and CelebA datasets based on three adversarial targets and two types of triggers. For instance, on CelebA dataset, TrojDiff could reach the attack precision and attack success rate of up to 84.70% and 96.90% in In-D2D attack. Moreover, the attack success rate is always higher than 98% in Out-D2D attack and the mean square error is as low as 1×10−4 level in D2I attack. Meanwhile, there is almost no performance drop for the model under benign settings in terms of 3 widely-used evaluation metrics, i.e., FID, precision, and recall.
Our main contributions are threefold. (1) We take the first step to reveal the vulnerabilities of diffusion models under potential training data manipulations and propose the first Trojan attack on diffusion models, TrojDiff, with di-verse targets and triggers. (2) We propose the Trojan diffu-sion process with novel transitions to diffuse adversarial tar-gets into a biased Gaussian distribution and the Trojan gen-erative process based on a new parameterization that leads to a simple training objective for the Trojan attack. (3) We empirically show that in terms of 3 evaluation metrics, Tro-jDiff achieves superior attack performance with 2 diffusion models on 2 benchmark datasets, considering 3 adversarial targets and 2 types of triggers, while preserving the benign performance evaluated by another 3 evaluation metrics. 2.