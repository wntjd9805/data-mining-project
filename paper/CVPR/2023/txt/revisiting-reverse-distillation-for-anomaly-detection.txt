Abstract
Anomaly detection is an important application in large-scale industrial manufacturing. Recent methods for this task have demonstrated excellent accuracy but come with a latency trade-off. Memory based approaches with domi-nant performances like PatchCore or Coupled-hypersphere-based Feature Adaptation (CFA) require an external mem-ory bank, which significantly lengthens the execution time.
Another approach that employs Reversed Distillation (RD) can perform well while maintaining low latency.
In this paper, we revisit this idea to improve its performance, es-tablishing a new state-of-the-art benchmark on the chal-lenging MVTec dataset for both anomaly detection and localization. The proposed method, called RD++, runs six times faster than PatchCore, and two times faster than CFA but introduces a negligible latency compared to RD. We also experiment on the BTAD and Retinal
OCT datasets to demonstrate our method’s generalizabil-ity and conduct important ablation experiments to provide insights into its configurations. Source code will be avail-able at https : / / github . com / tientrandinh /
Revisiting-Reverse-Distillation. 1.

Introduction
Detecting anomalies is a crucial aspect of computer vi-sion with numerous applications, such as product quality control [4], and healthcare monitor system [21]. Unsuper-vised anomaly detection can help to reduce the cost of col-lecting abnormal samples. This task identifies and local-izes anomalous regions in images without defect annota-tions during training. Instead, a set of abnormal-free sam-ples is utilized. Early approaches rely on generative adver-sarial models to extract meaningful latent representations on normal samples [25, 29, 31]. However, these approaches are computationally expensive, resulting in higher latency and potential performance limitations on unseen data. Other approaches leverage the pre-trained Convolutional Neural
Networks (CNNs) [19] backbones to extract comprehensive visual features for anomaly detection systems [3, 23].
Figure 1. Comparisons of different anomaly detection methods in terms of AUROC sample (vertical axis), inference time (horizontal axis), and memory footprint (circle radius). Our RD++ achieves the highest AUROC sample metric for anomaly detection while being 6× faster than PatchCore, 4× faster than CSFlow, and 2× faster than CFA. Additionally, RD++ requires only 4GB of mem-ory for inference, making it one of the least memory-use methods.
The test environment was conducted on a computer with Intel(R)
Xeon(R) 2.00GHz (4 cores) and Tesla T4 GPU (15GB VRAM).
Alternative approach employs knowledge distillation (KD) [14] based frameworks. For example, Salehi et al.
[30] set up a teacher-student network pair, and knowledge is transferred from teacher to student. During training, the student is learned from only normal samples. Thus, it is expected to learn the distribution of normal samples, sub-sequently generating the out-of-distribution representations when inference with anomalous query [30]. However, Deng and Li [8] point out that the statement is not always accu-rate due to limitations in the similarity of network architec-tures and the same data flows in the teacher-student model.
To overcome these limitations, they propose a reverse flow, called Reverse Distillation (RD), in which the teacher out-put is fed to the student through a One-class Bottleneck module (OCBE). The reverse distillation approach achieves competitive performance and also maintains low latency.
Recent research in anomaly detection, such as Patch-Core [27], and CFA [20], have achieved state-of-the-art per-formance in detecting and localizing anomalies. However,
these methods are based on the memory bank framework, which leads to significant latency and makes them challeng-ing to apply in practical scenarios. Our key question: How can we develop a method that achieves high accuracy and fast inference for real-world applications?
This paper answers the question from the perspective of reverse distillation. We identify the limitations of the RD approach by examining feature compactness and anoma-lous signal suppression. We argue that relying solely on the distillation task and an OCBE module is insufficient for providing a compact representation to the student. Further-more, we do not observe an explicit mechanism to discard anomalous patterns using the OCBE block as the authors claim. To address these concerns, we incorporate RD with multi-task learning to propose RD++, which demonstrates a favorable gain in performance.
The contributions of the proposed RD++ are highlighted as follows:
• We propose RD++ to tackle two tasks. First, fea-ture compactness task: by presenting a self-supervised optimal transport method. Second, anomalous sig-nal suppression task: by simulating pseudo-abnormal samples with simplex noise and minimizing the recon-struction loss.
• We conduct extensive experiments on several pub-lic datasets in different domains, including MVTec,
BTAD, and Retinal OCT. Results show that our ap-proach achieves state-of-the-art performance on detec-tion and localization, demonstrating strong general-ization capabilities across domains. Furthermore, our method’s real-time capability is at least twice as fast as its latest counterparts (see Fig. 1), making it a promis-ing method for practical applications. 2.