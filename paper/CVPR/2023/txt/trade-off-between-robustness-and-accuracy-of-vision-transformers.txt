Abstract
Although deep neural networks (DNNs) have shown great successes in computer vision tasks, they are vulner-able to perturbations on inputs, and there exists a trade-off between the natural accuracy and robustness to such per-turbations, which is mainly caused by the existence of robust non-predictive features and non-robust predictive features.
Recent empirical analyses find Vision Transformers (ViTs) are inherently robust to various kinds of perturbations, but
In this the aforementioned trade-off still exists for them. work, we propose Trade-off between Robustness and Accu-racy of Vision Transformers (TORA-ViTs), which aims to efficiently transfer ViT models pretrained on natural tasks for both accuracy and robustness. TORA-ViTs consist of two major components, including a pair of accuracy and robustness adapters to extract predictive and robust fea-tures, respectively, and a gated fusion module to adjust the trade-off. The gated fusion module takes outputs of a pre-trained ViT block as queries and outputs of our adapters as keys and values, and tokens from different adapters at different spatial locations are compared with each other to generate attention scores for a balanced mixing of predic-tive and robust features. Experiments on ImageNet with various robust benchmarks show that our TORA-ViTs can efficiently improve the robustness of naturally pretrained
ViTs while maintaining competitive natural accuracy. Our most balanced setting (TORA-ViTs with λ = 0.5) can main-tain 83.7% accuracy on clean ImageNet and reach 54.7% and 38.0% accuracy under FGSM and PGD white-box at-tacks, respectively. In terms of various ImageNet variants, it can reach 39.2% and 56.3% accuracy on ImageNet-A and
ImageNet-R and reach 34.4% mCE on ImageNet-C. 1.

Introduction
In the past few decades, deep neural networks (DNNs) have been well developed to achieve or even surpass the per-formance of humans on computer vision tasks [13,23,24,54, 55]. However, a fatal drawback of them is that they are vul-nerable to perturbations on inputs [8, 14, 15, 17, 32], which will cause dramatically drop in their accuracy. Furthermore, recent studies demonstrate that there exists a trade-off be-tween natural accuracy and adversarial robustness [48, 57], which means improving the robustness of a network typi-cally leads to a decrease in accuracy on natural samples.
A popular theory explains this trade-off by the existence of two kinds of different features [22, 48, 53]. The first kind of feature is moderately correlated to the task and robust to attacks, while the second kind of feature is weakly cor-related to the task and therefore non-robust. It is unfortu-nate that those moderately correlated and robust features only have limited contributions to accurate predictions (ro-bust and non-predictive), and further improving the accu-racy heavily relies on those weakly related and non-robust features (predictive and non-robust) [48]. Therefore, this trade-off is usually considered an inherent characteristic of
DNNs. Although there are many efforts that aim to control or improve this trade-off [26,40,41,50,57], it is still hard to efficiently and effectively improve it on large-scale datasets such as ImageNet [3].
Recently, a new family of vision models, namely Vision
Transformers (ViTs) [6,44,47], has outperformed CNNs on various kinds of tasks. There are many subsequent works that discuss diverse variants of ViTs to improve their per-formance. TNT [11] divides patches in ViTs into smaller sub-patches and applies a transformer-in-transformer archi-tecture with an additional inner transformer. T2T-ViT [56] introduces local feature aggregation to boost local informa-tion. Swin [29] performs local attention within various win-dows, and a shifted window partitioning approach is intro-duced for cross-window connections.
However, the aforementioned works mainly focus on the natural accuracy on clean data. Although empirical anal-yses have demonstrated that ViTs demonstrates robustness against various kinds of perturbations [1, 33, 37], there are only a limited number of works [9, 19, 34, 60] focus on im-proving the robustness. Besides, how to boost a naturally pretrained ViT for robustness has been ignored by exist-ing methods. A pretrained ViT has high utility because it can extract predictive features to ensure high accuracy on downstream tasks. Despite the high utility, it also has low reliability, because its non-robust features are vulnerable to perturbations. Therefore, it is worth studying how to obtain a useful and reliable ViT.
In the first phase, the accuracy and robustness adapters are optimized alternately along with the gated fusion module.
When each of them reaches a proper performance, they are frozen, and the gated fusion module is optimized with a joint objective of accuracy and robustness with a trade-off ratio λ. Experiments on ImageNet with various ro-bust benchmarks, including white-box adversarial attacks (FGSM and PGD), natural adversarial example (ImageNet-A), out-of-distribution data (ImageNet-R), and common corruptions (ImageNet-C), show that our TORA-ViTs can efficiently improve the robustness of naturally pretrained
ViTs. Meanwhile, the natural accuracy is still competi-tive with or even better than the models pursuing accu-racy. Our most balanced setting (TORA-ViTs with λ = 0.5) can maintain 83.7% accuracy on clean ImageNet and reach 54.7% and 38.0% accuracy under FGSM and PGD white-box attacks, respectively.
In terms of various Im-ageNet variants, it can reach 39.2% and 56.3% accuracy on ImageNet-A and ImageNet-R and reach 34.4% mCE on
ImageNet-C. 2.