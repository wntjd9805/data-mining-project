Abstract
Correct predictions
Correct predictions
As scientiﬁc and technological advancements result from human intellectual labor and computational costs, protect-ing model intellectual property (IP) has become increas-ingly important to encourage model creators and owners.
Model IP protection involves preventing the use of well-trained models on unauthorized domains. To address this issue, we propose a novel approach called Compact Un-Transferable Isolation Domain (CUTI-domain), which acts as a barrier to block illegal transfers from authorized to unauthorized domains. Speciﬁcally, CUTI-domain blocks cross-domain transfers by highlighting the private style fea-tures of the authorized domain, leading to recognition fail-ure on unauthorized domains with irrelevant private style features. Moreover, we provide two solutions for using
CUTI-domain depending on whether the unauthorized do-main is known or not: target-speciﬁed CUTI-domain and target-free CUTI-domain. Our comprehensive experimen-tal results on four digit datasets, CIFAR10 & STL10, and
VisDA-2017 dataset demonstrate that CUTI-domain can be easily implemented as a plug-and-play module with differ-ent backbones, providing an efﬁcient solution for model IP protection. 1.

Introduction
The recent success of deep learning models heavily re-lies on massive amounts of high-quality data, specialized training resources, and elaborate manual ﬁne-tuning [4, 10, 26, 35]. Obtaining a well-trained deep model is both time-consuming and labor-intensive [22]. Therefore, it should be protected as a kind of scientiﬁc and technological achieve-ment intellectual property (IP) [5, 50], thereby stimulating innovation enthusiasm in the community and further pro-moting the development of deep learning. As shown in
*L. Wang and M. Wang contributed equally to this work.
†Corresponding author: D. Zhang (dqzhang@nuaa.edu.cn) and H. Fu (hzfu@ieee.org).
Authorization
Authorization
Authorized domain
Transferable
Model
Model
Authorized domain
Un-transferable
CUTI-domain
Stealer
Unauthorized domain
Model owner
Model owner
Unauthorized domain
Wrong predictions
Wrong predictions
Figure 1. Model IP protection with our proposed CUTI-domain.
Left: In standard supervised learning (SL), the model owner trains a high-performance model on the authorized domain (pink square) and then authorizes a speciﬁc user. Authorized users have the right to use the model on the authorized domain to get the correct pre-diction. However, a stealer can easily access the model on the unauthorized domain (blue square), which violates the legitimate rights and interests of the model owner. Right: Our method con-structs a CUTI-domain between the authorized and unauthorized domains, which could block the illegal transferring and lead to a wrong prediction for unauthorized domains.
Fig. 1, in supervised learning (SL), the model owner uses the overall features of the authorized domain (pink square) for training, obtains a high-performance model, and grants the right to use it to a speciﬁc user. Authorized users can use the model on the authorized domain to obtain correct predictions. However, since the model is trained with over-all features, its potential feature generalization region is large and may cover some unauthorized domains. There-fore, there is a natural pathway between the authorized do-main and the unauthorized domain, and the released high-performance model obtained by SL can be illegally trans-ferred to the unauthorized domain (blue squares) through methods such as domain adaptation [30, 51], and domain generalization [44, 52], to obtain correct prediction results.
This presents a challenge in protecting well-trained models.
One of the most concerning threats raised is “Will releasing
the model make it easy for the main competitor to copy this new feature and hurt owner differentiation in the market?”
Thus, the model IP protection has been proposed to defend against model stealing or unauthorized use.
A comprehensive intellectual property (IP) protection strategy for deep learning models should consider both ownership veriﬁcation and applicability authorization [45, 47]. Ownership veriﬁcation involves verifying who has permission to use the deep model by embedding water-marks [37, 48], model ﬁngerprint [31], and predeﬁned trig-gers [14]. The model owner can grant usage permission to a speciﬁc user, and any other users will be infringing on the owner’s IP rights. However, an authorized user can easily transfer the model to an unauthorized user, so the model owner must add special marks during training to identify and verify ownership. Moreover, these methods are vulner-able to ﬁne-tuning, classiﬁer retraining, elastic weight con-solidation algorithms, and watermark overwriting, which can weaken the model’s protection. On the other hand, ap-plicability authorization involves verifying the model’s us-age scenarios. Users with permission can apply the deep model for the tasks speciﬁed by the model owner, and it is an infringement to use it for unauthorized tasks [45]. How-ever, users can easily transfer high-performance models to other similar tasks to save costs, which is a common and hidden infringement. Therefore, if the performance of the model can be limited to the tasks speciﬁed by the owner and reduced on other similar tasks, unauthorized users will lose conﬁdence in stealing and re-authoring the model. To achieve this, a non-transferable learning (NTL) method is proposed [45], which uses an estimator with a characteristic kernel from Reproducing Kernel Hilbert Spaces to approx-imate and increase the maximum mean difference between two distributions on ﬁnite samples. However, the authors only considered using limited samples to increase the mean distribution difference of features between domains and ig-nored outliers. The convergence region of NTL is not tight enough. Moreover, the calculation of the maximum mean difference is class-independent, which reduces the model’s feature recognition ability in the authorized domain to a cer-tain extent.
To address the challenges outlined above, we ﬁrst pro-pose a novel approach called the Compact Un-Transferable
Isolation (CUTI) domain to prevent illegal transferring of deep models from authorized to unauthorized domains. Our approach considers the overall feature of each domain, con-sisting of two components: shared features and private fea-tures. Shared features refer to semantic features, while pri-vate features include stylistic cues such as perspective, tex-ture, saturation, brightness, background environment, and so on. We emphasize the private features of the autho-rized domain and construct a CUTI-domain as a model bar-rier with similar private style features. This approach pre-vents illegal transfers to unauthorized domains with new private style features, thereby leading to wrong predictions.
Furthermore, we also provide two CUTI-domain solutions for different scenarios. When the unauthorized domain is known, we propose the target-speciﬁed CUTI-domain, where the model is trained with a combination of autho-rized, CUTI, and unauthorized domains. When the unau-thorized domain is unknown, we use the target-free CUTI-domain, which employs a generator to synthesize unau-thorized samples that replace the unauthorized domain in model training. At last, our comprehensive experimen-tal results on four digit datasets, CIFAR10 & STL10, and
VisDA-2017 demonstrate that our proposed CUTI-domain effectively reduces the recognition ability on unauthorized domains while maintaining strong recognition on autho-rized domains. Moreover, as a plug-and-play module, our
CUTI-domain can be easily implemented within different backbones and provide efﬁcient solutions.* 2.