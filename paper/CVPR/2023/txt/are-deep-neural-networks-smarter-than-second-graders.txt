Abstract
Recent times have witnessed an increasing number of ap-plications of deep neural networks towards solving tasks that require superior cognitive abilities, e.g., playing Go, generating art, question answering (e.g., ChatGPT), etc.
Such a dramatic progress raises the question: how general-izable are neural networks in solving problems that demand broad skills? To answer this question, we propose SMART: a Simple Multimodal Algorithmic Reasoning Task and the associated SMART-101 dataset1, for evaluating the abstrac-tion, deduction, and generalization abilities of neural net-works in solving visuo-linguistic puzzles designed specifi-cally for children in the 6–8 age group. Our dataset con-sists of 101 unique puzzles; each puzzle comprises a picture and a question, and their solution needs a mix of several elementary skills, including arithmetic, algebra, and spa-tial reasoning, among others. To scale our dataset towards training deep neural networks, we programmatically gen-erate entirely new instances for each puzzle while retaining their solution algorithm. To benchmark the performance on the SMART-101 dataset, we propose a vision-and-language meta-learning model that can incorporate varied state-of-the-art neural backbones. Our experiments reveal that while powerful deep models offer reasonable performances on puzzles in a supervised setting, they are not better than random accuracy when analyzed for generalization – filling this gap may demand new multimodal learning approaches. 1.

Introduction
“An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.”
The Dartmouth Summer Project on AI, 1956
Deep learning powered AI systems have been increas-ing in their data modeling abilities at an ever more vigor 1The SMART-101 dataset is publicly available at: https://doi.org/10.5281/zenodo.7761800
Question: Bird Bobbie jumps on a fence from the post on the left end to the other end. Each jump takes him 4 seconds. He makes 4 jumps ahead and then 1 jump back. Then he again makes 4 jumps
In how many seconds can ahead and 1 jump back, and so on.
Bobbie get from one end to the other end?
Answer Options: A: 64 B: 48 C: 56 D: 68 E: 72
Figure 1. An example puzzle instance from our SMART-101 dataset generated using our programmatic augmentation method.
Solving this puzzle needs various skills such as counting the num-ber of posts, spatially locating Bobbie, and using the details in the question to derive an algorithm for the solution. At a foundational level, a reasoning agent needs to recognize abstracted objects such as posts and identify the bird. The answer is shown below2. in the recent times, with compelling applications emerg-ing frequently, many of which may even seem to chal-lenge human abilities. A few notable such feats in-limited to game playing (e.g., Al-clude but are not phaGo [60]), language-guided image generation (e.g., the recent DALLE-2 [54] and ImageGen [56]), creative story writing (e.g., using GPT-3 [10]), solving university level math problems [17], algorithmic inference [20], and general question-answering/dialog (e.g., ChatGPT [48] and vari-ants). Such impressive performances have prompted an in-trospection into the foundation of what constitutes artificial intelligence and deriving novel tasks that could challenge deep models further [13, 37, 45, 55].
While deep neural networks offer compelling perfor-mances on specialized tasks on which they are trained on, (i) how well do they model abstract data, attend on key entities, and transfer knowledge to solve new problems? (ii) how fluid are they in acquiring new skills? and (iii) how effec-tive are they in the use of language for visual reasoning? We task ourselves to understand and seek a way to answer these 2The answer to the puzzle in Figure 1 is: C.
questions for state-of-the-art (SOTA) vision and language deep learning models. An approach that has been taken several times in the past is to design specialized datasets that can measure the cognitive abilities of well-trained neu-ral networks. For example, in CLEVR [34], a diagnostic dataset is proposed that comprises visuo-linguistic spatial reasoning problems. The abstraction abilities of neural net-works have been explored towards solving types of Bon-gard problems [33, 47] and human IQ puzzles (e.g., Ravens progressive matrices) have been extended to evaluate neu-ral reasoning abilities [7, 8, 31, 49, 64, 66, 72, 75]. However, while the puzzles in these prior works are often seemingly diverse, they are often confined to a common setting and may need only specialized skill sets, bringing in inductive biases that could be exploited by well-crafted deep learn-ing models, thereby solving such puzzles with near perfect accuracy [59, 64].
In this paper, we take a look back at the foundations of intelligence, by asking the question: Are state-of-the-art deep neural networks capable of emulating the thinking process of even young children? To gain insights into an-swering this question, we introduce the Simple Multimodal
Algorithmic Reasoning Task (SMART) – a visuo-linguistic task and the associated SMART-101 dataset built from 101 distinct children’s puzzles. As this is the first step in this di-rection, we keep the puzzles simple – to ensure this, we took inspiration from the puzzles in the Math Kangaroo USA
Olympiad [3] which has puzzle sets professionally designed for children in the age group of 6–8. Each puzzle in our dataset has a picture describing the problem setup and an associated natural language question. To solve the puzzle, one needs to use the question to gather details from the pic-ture and infer a simple mathematical algorithm that leads to a solution to be matched against multiple answer options.
In Figure 1, we illustrate our task with an example puzzle from our dataset. Unlike prior datasets with similar goals, each of the 101 puzzles in our dataset is distinct and needs a broad range of elementary mathematical skills for their so-lutions, including skills in algebra, basic arithmetic, geom-etry, ordering, as well as foundational skills to interpret ab-stract images, and execute counting, spatial reasoning, pat-tern matching, and occlusion reasoning. To the best of our knowledge, this is the first dataset that offers such a richly diverse set of visuo-linguistic puzzles in an open-world set-ting, with a psychometric control on their difficulty levels against human performance.
To benchmark performances on the SMART-101 dataset, we propose an end-to-end meta-learning based neural net-work [21], where we use a SOTA pre-trained image encoder backbone (e.g., Transformers/ResNets) to embed the pic-ture part of the puzzles, and a strong large language model (e.g., GPT/BERT) to model the questions. As each puzzle may have a different range for their answers (e.g., selection from a few choices, sequential answers, etc.), we propose to treat each puzzle as a separate task, with task-specific neu-ral heads and training objectives, while a common vision-language backbone is used on all the puzzles.
We provide experiments using our learning framework under various evaluation settings, analyzing the ability of SOTA vision and language backbones for: (i) in-distribution generalization, when training and test data are from the same distributions of puzzle instances, and out-of-distribution generalization, when training and test data are from: (ii) distinct answer distributions, or (iii) different puz-zles. We find the backbones performing poorly in our model on (i) and (ii), while failing entirely on (iii), suggesting that solving our dataset would demand novel research directions into algorithmic reasoning.
We experiment on various settings, evaluating the ability of our model to (i) solve puzzles when trained and tested on the same distribution of instances, (ii) out of distribution generalization when training and testing data are disjoint at the answer level, and (iii) out of distribution generalization when the training and testing sets are disjoint at the puzzle levels. We find that our model performs poorly on the tasks (i) and (ii), while failing entirely on (iii), suggesting that solving our dataset would demand novel research directions into neural abstractions, and algorithmic reasoning abilities.
We summarize below the key contributions of this paper. 1. With the goal of making progress towards improving the visuo-linguistic algorithmic reasoning abilities of neural networks, we introduce a novel task: SMART, and the associated large-scale SMART-101 dataset. 2. We propose a programmatic augmentation strategy for replicating abstract puzzles. 3. We design a baseline meta-solver neural architecture for solving the puzzles in our task. 4. We present experiments using our approach in various algorithmic generalization settings, bringing out key insights on the performance of SOTA neural networks on this task. We also compare performances against humans and using large language models. 2.