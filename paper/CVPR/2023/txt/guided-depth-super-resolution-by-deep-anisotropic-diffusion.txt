Abstract
Performing super-resolution of a depth image using the guidance from an RGB image is a problem that con-cerns several fields, such as robotics, medical imaging, and remote sensing. While deep learning methods have achieved good results in this problem, recent work high-lighted the value of combining modern methods with more formal frameworks. In this work, we propose a novel ap-proach which combines guided anisotropic diffusion with a deep convolutional network and advances the state of the art for guided depth super-resolution. The edge transfer-ring/enhancing properties of the diffusion are boosted by the contextual reasoning capabilities of modern networks, and a strict adjustment step guarantees perfect adherence to the source image. We achieve unprecedented results in three commonly used benchmarks for guided depth super-resolution. The performance gain compared to other meth-ods is the largest at larger scales, such as ×32 scaling.
Code1 for the proposed method is available to promote re-producibility of our results. 1.

Introduction
It is a primordial need for visual data analysis to in-crease the resolution of images after they have been cap-tured.
In many fields one is faced with images that, for technical reasons, have too low resolutions for the intended purposes, e.g., MRI scans in medical imaging [48], multi-spectral satellite images in Earth observation [22], thermal surveillance images [1] and depth images in robotics [9]. In some cases, an image of much higher resolution is available in a different imaging modality, which can act as a guide for super-resolving the low-resolution source image, by in-jecting the missing high-frequency content. For instance, in Earth observation, the guide is often a panchromatic im-age (hence the term ”pan-sharpening”), whereas in robotics a conventional RGB image is often attached to the same
*Equal contribution. 1 https://github.com/prs- eth/Diffusion- Super-Resolution
Figure 1. We super-resolve a low-resolution depth image by find-ing the equilibrium state of a constrained anisotropic diffusion pro-cess. Learned diffusion coefficients favor smooth depth within ob-jects and suppress diffusion across discontinuities. They are de-rived from the guide with a neural feature extractor that is trained by back-propagating through the diffusion process. platform as a TOF camera or laser scanner. In this paper, we focus on super-resolving depth images guided by RGB images, but the proposed framework is generic and can be adapted to other sensor combinations, too.
Research into guided super-resolution has a long his-tory [16, 29]. The proposed solutions range from classical, entirely hand-crafted schemes [10] to fully learning-based methods [15], while some recent works have combined the two schools of thought, with promising results [5, 32].
Many classical methods boil down to an image-specific op-timization problem that must be solved at inference time, which often makes them slow and memory-hungry. More-over, they are limited to low-level image properties of the guide, such as color and contrast, and lack the high-level image understanding and contextual reasoning of modern neural networks. On the positive side, by design, they can not overfit the peculiarities of a training set and tend to gen-eralize better. Recent work on guided super-resolution has focused on deep neural networks. Their superior ability to capture latent image structure has greatly advanced the state of the art over traditional, learning-free approaches. Still, these learning-based methods tend to struggle with sharp
discontinuities and often produce blurry edges in the super-resolved depth maps. Moreover, like many deep learning systems, they degrade – often substantially – when applied to images with even slightly different characteristics. Note also that standard feed-forward architectures cannot guar-antee a consistent solution: feeding the source and guiding images through an encoder-decoder structure to obtain the super-resolved target will, by itself, not ensure that down-sampling the target will reproduce the source.
We propose a novel approach for guided depth super-resolution which combines the strengths of optimization-based and deep learning-based super-resolution. In short, our method is a combination of anisotropic diffusion (based on the discretized version of the heat equation) with deep feature learning (based on a convolutional backbone). The diffusion part resembles classical optimization approaches, solved via an iterative diffusion-adjustment loop. Every it-eration consists of (1) an anisotropic diffusion step [2, 4, 23, 30], with diffusion weights driven by the guide in such a way that diffusion (i.e., smoothing) is low across high-contrast boundaries and high within homogeneous regions; and (2) an adjustment step that rescales the depth values such that they exactly match the low-resolution source when downsampled. To harness the unmatched ability of deep learning to extract informative image features, the diffusion weights are not computed from raw brightness values but are set by passing the guide through a (fully) convolutional feature extractor. An overview of the method is depicted in Fig. 1. The technical core of our method is the insight that such a feature extractor can be trained end-to-end to optimally fulfill the requirements of the subsequent opti-mization, by back-propagating gradients through the iter-ations of the diffusion loop. Despite its apparent simplicity, this hybrid approach delivers excellent super-resolution re-sults. In our experiments, it consistently outperforms prior art on three different datasets, across a range of upsampling factors from ×4 to ×32. In our experiments, we compare it to six recent learning methods as well as five different learning-free methods. For completeness, we also include a learning-free version of our diffusion-adjustment scheme and show that it outperforms all other learning-free meth-ods. Beyond the empirical performance, our method in-herits several attractive properties from its ingredients: the diffusion-based optimization scheme ensures strict adher-ence to the depth values of the source, crisp edges, and a degree of interpretability; whereas deep learning equips the very local diffusion weights with large-scale context infor-mation, and offers a tractable, constant memory footprint at inference time. In summary, our contributions are: 1. We develop a hybrid framework for guided super-resolution that combines deep feature learning and anisotropic diffusion in an integrated, end-to-end train-able pipeline; 2. We provide an implementation of that scheme with constant memory demands, and with inference time that is constant for a given upsampling factor and scales linearly with the number of iterations; 3. We set a new state of the art for the Middlebury [38],
NYUv2 [39] and DIML [20] datasets, for upsampling factors from 4× to 32×, and provide empirical evi-dence that our method indeed guarantees exact consis-tency with the source image. 2.