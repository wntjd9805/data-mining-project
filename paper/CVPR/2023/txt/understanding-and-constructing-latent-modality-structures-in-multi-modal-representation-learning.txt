Abstract
Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to ex-actly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance.
In this paper, based on an information-theoretic argument, we ﬁrst prove that ex-act modality alignment is sub-optimal in general for down-stream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality struc-tures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Speciﬁcally, we design 1) a deep fea-ture separation loss for intra-modality regularization; 2) a
Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are con-ducted on two popular multi-modal representation learn-ing frameworks: the CLIP-based two-tower model and the
ALBEF-based fusion model. We test our model on a va-riety of tasks including zero/few-shot image classiﬁcation, image-text retrieval, visual question answering, visual rea-soning, and visual entailment. Our method achieves consis-tent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization. 1.

Introduction
Vision-language representation learning aims to learn generic representations from images and texts that could beneﬁt multimodal downstream applications. As the two modalities are essentially from different data sources and distributions, how to effectively fuse the two modalities has
*Work done while at Amazon.
In-Modality 
Regularization 1.Deep Feature Separation
Cross-Modality 
Regularization 2.Brownian Bridges
In-Modality 
Regularization 1.Deep Feature Separation
Augmented image space
Image 
Encoder 3.Geometric Consistency
Text 
Encoder
The trail climbs  steadily uphill  most of the way.
Figure 1. Constructing latent modality structures to improve multi-modal representation learning. become an important question. Some work aims to unify the representations of two modalities in one encoder, where the image and text are usually tokenized into sequences
[60, 61, 65, 66]. Another line of research represents the im-age and text modality separately with modality-speciﬁc en-coders and utilizes contrastive learning to align the modal-ities, achieving state-of-the-art performance on multiple downstream applications [13, 26, 31, 32, 41, 49, 53, 54, 70].
Despite the successful empirical practice of contrastive loss in multi-modal representation learning, it remains an open question whether bridging and aligning the two modalities always brings beneﬁts to downstream tasks.
One concept closely related to this question is the modal-ity gap [35, 49, 68, 72], where it is deﬁned as the dis-tance between the feature distributions of the two modali-ties. Modality alignment can be considered as reducing the modality gap. At a ﬁrst glance, one would conjecture that contrastive loss would reduce the modality gap by pulling positive (paired) image and text data together for better rep-resentation. However, a recent study [35] shows evidence that contrastive learning does not always reduce the modal-ity gap. Furthermore, we also show in our empirical anal-ysis that a reduced modality gap does not always guaran-tee better performance in downstream applications. Moti-vated by these empirical observations, in this paper we ﬁrst theoretically study the modality gap problem, by showing that when the modality gap is zero, i.e., exact alignment be-tween the two modalities, the learned representations nec-essarily have to pay a price for the downstream prediction task, which we term as the information gap between the two modalities (Theorem 3.1). Intuitively, this is because that representations with zero modality gap can only preserve predictive information present in both of the modalities at the cost of losing the modality-speciﬁc information.
Our theory then suggests that instead of exact modality matching, whether learned representations are meaningful is an important factor in multi-modal representation learn-ing. In particular, we propose to improve on top of con-trastive learning with regularizations to construct better la-tent structures. We consider intra-modality, inter-modality, and intra-inter-modality regularizations. These regulariza-tions are generalizable and can be applied to various vision-language models with modality-speciﬁc encoders. Specif-ically, for intra-modality regularization, motivated by our theoretic result, we propose deep feature separation to en-courage the model to preserve both the modality-shared and modality-speciﬁc information in different components.
For inter-modality regularization, we aim to bridge two modalities with their augmentations. Consequently, we pro-posed a Brownian bridge loss between the triplet of (text, augmented image, image) to regularize the inter-modality structures. For intra-inter-modality regularization, we in-troduce the geometric consistency loss that promotes geo-metric symmetry in the latent space. In summary, the main contributions of this paper are:
• We conduct empirical and theoretical analysis on un-derstanding the impact of the modality alignment on downstream tasks. We show that a reduced modal-ity gap does not always guarantee better performance, and can instead hurt the performance when the infor-mation gap between the two modalities is large (The-orem 3.1). Combined with the existing theory of con-trastive learning, our theory suggests preserving both modality-shared and modality-speciﬁc information.
• Inspired by our theory, we propose three instrumental regularizations on top of the contrastive loss, i.e., the intra-modality, inter-modality, and intra-inter-modality regularizations to improve latent modality structures. 2.