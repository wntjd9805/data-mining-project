Abstract 1.

Introduction
Geometric camera calibration is often required for ap-plications that understand the perspective of the image. We propose Perspective Fields as a representation that mod-els the local perspective properties of an image. Perspec-tive Fields contain per-pixel information about the cam-era view, parameterized as an Up-vector and a Latitude value. This representation has a number of advantages; it makes minimal assumptions about the camera model and is invariant or equivariant to common image editing op-erations like cropping, warping, and rotation.
It is also more interpretable and aligned with human perception. We train a neural network to predict Perspective Fields and the predicted Perspective Fields can be converted to cali-bration parameters easily. We demonstrate the robustness of our approach under various scenarios compared with camera calibration-based methods and show example ap-plications in image compositing. Project page: https:
//jinlinyi.github.io/PerspectiveFields/
* Work partially done during internship at Adobe.
Take a look at the left-most photo in the teaser (Fig. 1-A). Can you tell if the photo is captured from an everyday camera and if it has been geometrically edited? The horizon location at the bottom of the image and the parallel vertical lines of the buildings do not follow a typical camera model:
The horizon at the bottom of the image indicates the cam-era was tilted up (pitch ̸= 0), but this would instead pro-duce converging vertical lines in the image due to perspec-tive projection (Fig. 1-B). Alternatively suppose the camera has 0 pitch, preserving the vertical lines of the buildings, the horizon line would instead be in the middle of the image (Fig. 1-C). This contradiction is explained by the shift of the photo, yielding a non-center principal point, and breaking a usual assumption of many camera calibration systems.
Many single-image camera calibration works make use of a simplified pinhole camera model [22] that assumes a centered principal point [10,24,29] and is parameterized by extrinsic properties such as roll, pitch, and intrinsic proper-ties such as field of view. However, estimating the calibra-tion of a camera is challenging for images in the wild since they are captured by various types of cameras and lenses.
Moreover, like the example in Fig. 1, the images are often
cropped [15] or warped for aesthetic composition, which may shift the image center.
In this work, we propose Perspective Fields, an over-parameterized per-pixel image-based camera representa-tion. Perspective Fields consist of per-pixel Up-vectors and
Latitude values that are useful on their own for alignment and can be converted to calibration parameters easily by solving a simple inverse problem. The Up-vector gives the world-coordinate up direction at each pixel, which equals the inverse gravity direction of the 3D scene projected onto the image. The Latitude is the angle between the incom-ing light ray and the horizontal plane (see Fig. 1-D). This enables our method to be robust to cropping and we show results on multiple camera projection models.
Perspective Fields have a strong correlation with local image features. For example, the Up-vectors can be in-ferred by vertical edges in the image, and the Latitude is 0 at the horizon, positive above, and negative below. Since
Perspective Fields have this translation-equivariance prop-erty, they are especially well suited to prediction by con-volutional neural networks. We train a neural network to predict Perspective Fields from a single image by extracting crops from 360◦ panoramas where ground truth supervision can be easily obtained (see Fig. 2). We also use a teacher-student distillation method to transfer Perspective Fields to object-cutouts, which lets us train models to predict Per-spective Fields for object-centric images.
For applications that require traditional camera parame-ters (e.g. roll, pitch, field of view and principal point), we propose ParamNet to efficiently derive camera parameters from Perspective Fields. Our method works on image crops and outperforms existing methods in single image camera parameter estimation. In addition, Perspective Fields can be used in image compositing to align the camera view be-tween a foreground object and the background based on a local Perspective Field matching metric. We show with a user study that this metric for view alignment more closely matches human perspective than existing camera models.
Our contributions are summarized as follows.
• We propose Perspective Fields, a local and non-parametric representation of images with no assump-tion of camera projection models.
• We train a network to predict Perspective Fields that works on both scene-level and object-centric images, and we propose ParamNet to efficiently derive camera parameters from Perspective Fields. Our Perspective
Fields achieve better performance on recovering cam-era parameters than existing approaches. On cropped images, we reduce the pitch error by 40% over [29].
• We propose a metric of Perspective Fields to esti-mate the low-level perspective consistency between two images. We show that this consistency measure is stronger in correlation with human perception of per-spective mismatch than previous metrics such as Hori-zon line [24, 47]. 2.