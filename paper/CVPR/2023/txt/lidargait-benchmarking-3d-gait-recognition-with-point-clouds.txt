Abstract
Video-based gait recognition has achieved impressive re-sults in constrained scenarios. However, visual cameras neglect human 3D structure information, which limits the
In-feasibility of gait recognition in the 3D wild world. stead of extracting gait features from images, this work explores precise 3D gait features from point clouds and proposes a simple yet efficient 3D gait recognition frame-work, termed LidarGait. Our proposed approach projects sparse point clouds into depth maps to learn the represen-tations with 3D geometry information, which outperforms existing point-wise and camera-based methods by a sig-nificant margin. Due to the lack of point cloud datasets, we build the first large-scale LiDAR-based gait recognition dataset, SUSTech1K, collected by a LiDAR sensor and an
RGB camera. The dataset contains 25,239 sequences from 1,050 subjects and covers many variations, including vis-ibility, views, occlusions, clothing, carrying, and scenes.
Extensive experiments show that (1) 3D structure informa-tion serves as a significant feature for gait recognition. (2)
LidarGait outperforms existing point-based and silhouette-based methods by a significant margin, while it also offers stable cross-view results. (3) The LiDAR sensor is superior to the RGB camera for gait recognition in the outdoor en-vironment. The source code and dataset have been made available at https://lidargait.github.io. 1.

Introduction
Gait is an essential biometric, which has the unique advantage of human identification at a distance without physical contact. Gait empowers real-world applications such as human retrieval, forensic identification, and serv-ing robots. Recently, great progress has been made to pro-*Corresponding Author (a) Camera-based gait recognition with silhouettes (b) LiDAR-based gait recognition with point clouds
Figure 1. Illustration of (a) camera-based and (b) LiDAR-based gait recognition. Camera-based gait recognition commonly uses silhouettes to learn shape information from a single view. LiDAR-based gait recognition can use 3D structure, shape, and scale in-formation to identify a subject. mote gait recognition from in-the-lab setting [19, 42, 50] to in-the-wild scenario [17, 52, 56, 58]. Despite these studies have made significant contributions to recent ad-vances [5, 7, 11, 30, 31, 38, 55], two inherent problems still remain (1) lack of 3D geometry information, and (2) poor feasibility in the real-world scenario.
Existing camera-based methods [18, 53] are counterintu-itive to human nature. When recognizing a subject [56, 57], humans consider not only the 2D appearance character-istics, but also 3D geometry structure information like height, shape, and viewpoints. Differently, camera-based gait recognition methods [5, 26, 31] either capture 2D rep-resentations from a single viewpoint, as shown in Fig. 1a, or exploit 3D representations from estimated 3D pose/mesh
models [23, 27, 56], which is usually imprecise in various challenging conditions of low resolution, poor illumination, untrained posture, etc. Fortunately, 3D sensors provide pre-cise 3D perception like human nature, e.g. recognizing a subject from multiple views as illustrated in Fig. 1b.
Visual ambiguity is the alternative limitation of camera-based approaches. To our knowledge, most existing gait datasets [42,56,58] only consider camera-based modalities, and fail to acknowledge the challenges of visual ambiguity caused by poor illumination and complex backgrounds in outdoor environments. These factors can significantly harm the performance of upstream tasks like pedestrian detection and segmentation, which in turn affects the accuracy of the gait system in real-world applications. Thus, obtaining pre-cise 3D information for gait description is highly desirable to eliminate visual ambiguity in RGB images.
The remarkable success of 3D applications [6, 14, 33] motivates us to endow gait recognition with precise 3D structural information and accurate human perception, by utilizing LiDAR sensors in challenging outdoor environ-ments. In addition to improving gait recognition, LiDAR sensors offer potential benefits in many scenarios, including robotics, healthcare, social security, and surveillance. For example, robots equipped with LiDAR-based gait recogni-tion can function as 24 Ã— 7 security guards, enhancing com-munity safety. Vehicles fitted with LiDAR sensors can aid in locating lost orders and children. Furthermore, LiDAR is more privacy-preserving than cameras, making it suitable for sensitive scenarios such as nursing homes and kinder-gartens. Additionally, LiDAR has the potential to enhance biometric security by protecting against Deepfake attacks compared to cameras.
This paper introduces SUSTech1K, the first large-scale
LiDAR-based gait dataset to facilitate 3D gait recognition with point clouds. The dataset is captured outdoors using a Velodyne VLS128 LiDAR sensor and an RGB camera mounted together on a robot. Compared to existing datasets listed in Tab. 1, SUSTech1K offers several distinctive fea-tures: (1) Precision. The SUSTech1K dataset provides 3D point clouds as gait representations with high precision and density, providing precise and robust 3D structure informa-tion for recognition. (2) Scalability. The dataset captures 25,239 sequences from 1,050 subjects, providing scalabil-ity for statistical evaluation. (3) Diversity. The dataset in-cludes diverse and realistic challenges, such as illumination, occlusion, dressing, carrying, and more, along with detailed annotations, enabling the community to study the impact of (4) Multimodality. different factors on gait recognition.
The dataset captures data streams from LiDAR and cam-era sensors, opening up opportunities for exploring sensor fusion approaches for robust gait recognition.
Given that 3D point clouds are formatted differently from pixels in images and that point-based gait recogni-tion has received little attention, we investigate four cutting-edge methods [15, 36, 37, 54] from the study of point-based object classification [36]. However, we observed that all the implemented point-based methods performed sub-optimally when compared to methods using camera-based silhouettes. We believe the performance gap is pri-marily due to the difference in feature granularity of the task. The aforementioned point-based methods are primar-ily designed for coarse-grained object classification, focus-In contrast, gait ing more on global context modeling. recognition requires extracting fine-grained local informa-tion to achieve high accuracy.
To address this issue, we propose a simple yet effective baseline method named the LidarGait. Specifically, Lidar-Gait first projects 3D point clouds into depth images from the LiDAR range view and then employs convolutional net-works to extract gait features with 3D structural informa-tion from the projection. This approach contrasts point-wise methods that learn global context from sparse point clouds with limited local connectivity. Using convolutional neu-ral networks on projection, LidarGait can efficiently capture the fine-grained and discriminative gait features from sparse point clouds. Extensive experiments demonstrate that (1)
LidarGait is effective in maintaining 3D structural informa-tion for gait recognition, and including 3D information can significantly contributes to performance improvement, (2) point-based gait recognition equipped with a LiDAR sensor performs stably well on various challenges, convincingly demonstrating its practical significance.
To summarize, our main contributions are as follows: (1)
We carry out one of the first studies of 3D gait recognition with point clouds, bringing precise perception and 3D ge-ometry of humans for better practicality in real-world sce-narios. (2) We introduce SUSTech1K, the first large-scale
LiDAR-based gait recognition benchmark, which includes a range of annotations covering occlusions, viewpoints, car-rying, clothing, and distance. (3) We propose a novel point cloud gait recognition framework, LidarGait, outperform-ing camera-based methods by a large margin. 2.