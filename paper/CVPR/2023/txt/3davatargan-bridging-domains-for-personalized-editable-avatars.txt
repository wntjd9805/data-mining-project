Abstract
Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure.
Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera informa-tion has not yet been shown possible. Can we train a 3D
GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowl-edge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across do-mains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate ge-ometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enablingÐas a byproductÐ personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributionsÐfor the first timeÐallow for the generation, editing, and anima-tion of personalized artistic 3D avatars on artistic datasets.
Project Page: https:/rameenabdal.github.io/3DAvatarGAN 1.

Introduction
Photo-realistic portrait face generation is an iconic ap-plication demonstrating the capability of generative models especially GANs [28,30,31]. A recent development has wit-nessed an advancement from straightforwardly synthesizing 2D images to learning 3D structures without 3D supervi-sion, referred to as 3D-GANs [10, 41, 55, 64]. Such training
² Part of the work was done during an internship at Snap Inc.
is feasible with the datasets containing objects with highly consistent geometry, enabling a 3D-GAN to learn a distri-bution of shapes and textures. In contrast, artistically styl-ized datasets [25, 65] have arbitrary exaggerations of both geometry and texture, for example, the nose, cheeks, and eyes can be arbitrarily drawn, depending on the style of the artist as well as on the features of the subject, see Fig. 1.
Training a 3D-GAN on such data becomes problematic due to the challenge of learning such an arbitrary distribution of geometry and texture. In our experiments (Sec. 5.1), 3D-GANs [10] generate flat geometry and become 2D-GANs essentially. A natural question arises, whether a 3D-GAN can synthesize consistent novel views of images belonging to artistically stylized domains, such as the ones in Fig. 1.
In this work, we propose a domain-adaption framework that allows us to answer the question positively. Specifi-cally, we fine-tune a pre-trained 3D-GAN using a 2D-GAN trained on a target domain. Despite being well explored for 2D-GANs [25, 65], existing domain adaptation techniques are not directly applicable to 3D-GANs, due to the nature of 3D data and characteristics of 3D generators.
The geometry and texture of stylized 2D datasets can be arbitrarily exaggerated depending on the context, artist, and production requirements. Due to this, no reliable way to estimate camera parameters for each image exists, whether using an off-the-shelf pose detector [72] or a manual label-ing effort. To enable the training of 3D-GANs on such chal-1 An lenging datasets, we propose three contributions. optimization-based method to align distributions of cam-era parameters between domains. 2 Texture, depth, and geometry regularizations to avoid degenerate, flat solutions and ensure high visual quality. Furthermore, we redesign the discriminator training to make it compatible with our task. We then propose 3 a Thin Plate Spline (TPS) 3D deformation module operating on a tri-plane representation to allow for certain large and sometimes extreme geometric deformations, which are so typical in artistic domains.
The proposed adaptation framework enables the train-ing of 3D-GANs on complex and challenging artistic data.
The previous success of domain adaptation in 2D-GANs un-leashed a number of exciting applications in the content cre-ation area [25, 65]. Given a single image such methods first find a latent code corresponding to it using GAN inversion, followed by latent editing producing the desired effect in the image space. Compared to 2D-GANs, the latent space of 3D-GANs is more entangled, making it more challeng-ing to link the latent spaces between domains, rendering the existing inversion and editing techniques not directly appli-cable. Hence, we take a step further and explore the use of our approach to 3D artistic avatar generation and editing.
Our final contribution to enable such applications is 4 a new inversion method for coupled 3D-GANs.
In summary, the proposed domain-adaption framework allows us to train 3D-GANs on challenging artistic datasets with exaggerated geometry and texture. We call our method 3DAvatarGAN as itÐfor the first timeÐoffers generation, editing, and animation of personalized stylized, artistic avatars obtained from a single image. Our results (See
Sec. 5.2) show the high-quality 3D avatars possible by our method compared to the naive fine-tuning. 2.