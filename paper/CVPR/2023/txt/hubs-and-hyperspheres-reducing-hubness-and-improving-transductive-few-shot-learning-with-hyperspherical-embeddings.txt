Abstract
Distance-based classification is frequently used in trans-ductive few-shot learning (FSL). However, due to the high-dimensionality of image representations, FSL classifiers are prone to suffer from the hubness problem, where a few points (hubs) occur frequently in multiple nearest neighbour lists of other points. Hubness negatively impacts distance-based classification when hubs from one class appear often among the nearest neighbors of points from another class, degrading the classifier’s performance. To address the hubness prob-lem in FSL, we first prove that hubness can be eliminated by distributing representations uniformly on the hypersphere.
We then propose two new approaches to embed representa-tions on the hypersphere, which we prove optimize a tradeoff between uniformity and local similarity preservation – reduc-ing hubness while retaining class structure. Our experiments show that the proposed methods reduce hubness, and signifi-cantly improves transductive FSL accuracy for a wide range of classifiers1. 1.

Introduction
While supervised deep learning has made a significant impact in areas where large amounts of labeled data are available [6, 11], few-shot learning (FSL) has emerged as a promising alternative when labeled data is limited [3, 12, 14, 16, 21, 26, 28, 31, 33, 39, 40]. FSL aims to design classifiers that can discriminate between novel classes based on a few labeled instances, significantly reducing the cost of the labeling procedure.
In transductive FSL, one assumes access to the entire
*Equal contributions.
†UiT Machine Learning group (machine-learning.uit.no) and
Visual Intelligence Centre (visual-intelligence.no).
‡Norwegian Computing Center.
§Department of Computer Science, University of Copenhagen.
¶Pioneer Centre for AI (aicentre.dk). 1Code available at https://github.com/uitml/noHub.
Figure 1. Few-shot accuracy increases when hubness decreases.
The figure shows the 1-shot accuracy when classifying different embeddings with SimpleShot [33] on mini-ImageNet [29]. query set during evaluation. This allows transductive FSL classifiers to learn representations from a larger number of samples, resulting in better performing classifiers. However, many of these methods base their predictions on distances to prototypes for the novel classes [3, 16, 21, 28, 39, 40].
This makes these methods susceptible to the hubness prob-lem [10, 22, 24, 25], where certain exemplar points (hubs) appear among the nearest neighbours of many other points.
If a support sample is a hub, many query samples will be assigned to it regardless of their true label, resulting in low accuracy. If more training data is available, this effect can be reduced by increasing the number of labeled samples in the classification rule – but this is impossible in FSL.
Several approaches have recently been proposed to embed samples in a space where the FSL classifier’s performance is improved [4, 5, 7, 17, 33, 35, 39]. However, only one of these directly addresses the hubness problem. Fei et al. [7] show that embedding representations on a hypersphere with zero mean reduces hubness. They advocate the use of Z-score normalization (ZN) along the feature axis of each representation, and show empirically that ZN can reduce hubness in FSL. However, ZN does not guarantee a data mean of zero, meaning that hubness can still occur after ZN.
In this paper we propose a principled approach to em-bed representations in FSL, which both reduces hubness and improves classification performance. First, we prove that hubness can be eliminated by embedding representa-tions uniformly on the hypersphere. However, distributing representations uniformly on the hypersphere without any additional constraints will likely break the class structure which is present in the representation space – hurting the performance of the downstream classifier. Thus, in order to both reduce hubness and preserve the class structure in the representation space, we propose two new embedding methods for FSL. Our methods, Uniform Hyperspherical
Structure-preserving Embeddings (noHub) and noHub with
Support labels (noHub-S), leverage a decomposition of the
Kullback-Leibler divergence between representation and em-bedding similarities, to optimize a tradeoff between Local
Similarity Preservation (LSP) and uniformity on the hyper-sphere. The latter method, noHub-S, also leverages label information from the support samples to further increase the class separability in the embedding space.
Figure 1 illustrates the correspondence between hubness and accuracy in FSL. Our methods have both the least hub-ness and highest accuracy among several recent embedding techniques for FSL.
Our contributions are summarized as follows.
• We prove that the uniform distribution on the hyper-sphere has zero hubness and that embedding points uni-formly on the hypersphere thus alleviates the hubness problem in distance-based classification for transduc-tive FSL.
• We propose noHub and noHub-S to embed representa-tions on the hypersphere, and prove that these methods optimize a tradeoff between LSP and uniformity. The resulting embeddings are therefore approximately uni-form, while simultaneously preserving the class struc-ture in the embedding space.
• Extensive experimental results demonstrate that noHub and noHub-S outperform current state-of-the-art em-bedding approaches, boosting the performance of a wide range of transductive FSL classifiers, for multiple datasets and feature extractors. 2.