Abstract
We present DejaVu, a novel framework which leverages conditional image regeneration as additional supervision during training to improve deep networks for dense pre-diction tasks such as segmentation, depth estimation, and surface normal prediction. First, we apply redaction to the input image, which removes certain structural information by sparse sampling or selective frequency removal. Next, we use a conditional regenerator, which takes the redacted image and the dense predictions as inputs, and reconstructs the original image by filling in the missing structural in-formation. In the redacted image, structural attributes like boundaries are broken while semantic context is largely preserved. In order to make the regeneration feasible, the conditional generator will then require the structure infor-mation from the other input source, i.e., the dense predic-tions. As such, by including this conditional regeneration objective during training, DejaVu encourages the base net-work to learn to embed accurate scene structure in its dense prediction. This leads to more accurate predictions with clearer boundaries and better spatial consistency. When it is feasible to leverage additional computation, DejaVu can be extended to incorporate an attention-based regenera-tion module within the dense prediction network, which fur-ther improves accuracy. Through extensive experiments on multiple dense prediction benchmarks such as Cityscapes,
COCO, ADE20K, NYUD-v2, and KITTI, we demonstrate the efficacy of employing DejaVu during training, as it out-performs SOTA methods at no added computation cost. 1.

Introduction
Dense prediction tasks produce per-pixel classification or regression results, such as semantic or panoptic class la-bels, depth or disparity values, and surface normal angles.
These tasks are critical for many vision applications to bet-ter perceive their surroundings for XR, autonomous driving,
*These authors contributed equally to this work.
†Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.
Figure 1. Training within the DejaVu framework enables dense prediction models to improve their initial predictions using our proposed loss. The segmentation results are for the same OCR [87] model with and without DejaVu. The surface normal results are for
SegNet-XTC [43]. robotics, visual surveillance, and so on. There has been sig-nificant success in adopting neural networks to solve dense prediction tasks through innovative architectures, data aug-mentations and training optimizations. For example, [46] addresses pixel level sampling bias and [7] incorporates boundary alignment objectives. Orthogonal to existing methods, we explore novel regeneration-based ideas to un-derstand how additional gradients from reconstruction tasks complement the established training pipelines for dense prediction tasks and input representations.
There are works [69, 94] in classification settings that leverage reconstructions and likelihood-based objectives as auxiliary loss functions to enhance the quality of feature representations and also improve Open-set/OOD Detection
[25, 48, 49, 55]. The core intuition is that, for discrimina-tive tasks the model needs a minimal set of features to solve the task and any feature which does not have discriminative
power for the target subset of data are ignored. Another line of work for dense predictions [54] focuses on depth comple-tion and leverages reconstruction-based loss to learn com-plementary image features that aid better capture of object structures and semantically consistent features. Following such intuitions, we can see that reconstruction-based auxil-iary loss should capture more information in representation than discriminative-only training.
Here, we introduce a novel training strategy, DejaVu1, for dense prediction tasks with an additional, conditional re-construction objective to improve the generalization capac-ity of the task-specific base networks as illustrated in Fig. 1.
We redact the input image to remove structure information (e.g., boundaries) while retaining contextual information.
We adopt various redaction techniques that drop out com-ponents in spatial or spectral domains. Then, we enforce a conditional regeneration module (CRM), which takes the redacted image and the base network’s dense predictions, to reconstruct the missing information. For regeneration feasi-bility, the CRM will require structure information from the dense predictions. By including this conditional regenera-tion objective during training, we encourage the base net-work to learn and use such structure information, which leads to more accurate predictions with clearer boundaries and better spatial consistency, as shown in the experimental section. In comparison, the supervised loss cannot capture this information alone since the cross-entropy objective (for segmentation, as an example) looks at the probability distri-bution of every pixel. In this sense, DejaVu can implicitly provide cues to the dense prediction task from the recon-struction objective depending on the type of redaction we select. We also note that using the same number of addi-tional regenerated images as a data augmentation scheme does not provide the performance improvements that De-jaVu can achieve (as reported in the Appendix). This shows that DejaVu conditions the training process more effectively than any data augmentation technique.
Our DejaVu loss can be applied to train any dense pre-diction network and does not incur extra computation at test time. When it is feasible to leverage additional com-putation, DejaVu can be extended where we incorporate an attention-based regeneration module within the dense pre-diction network, further improving accuracy. An advantage of regenerating the original image from predictions is that we can additionally use other losses including text supervi-sion and cyclic consistency, as described in Section 3.4.
Our extensive experiments on multiple dense prediction tasks, including semantic segmentation, depth estimation, and surface normal prediction, show that employing DejaVu during training enables our trained models to outperform 1In training, DejaVu redacts the input image and constructs its regener-ated versions, in a way, these regenerated versions are ”already seen” yet not exactly the same due to initial redaction. the latest state of the art on several large-scale benchmarks.
Our main contributions are summarized as follows:
• We devise a novel learning strategy, DejaVu, that lever-ages conditional image regeneration from redacted in-put images to improve the overall performance on dense prediction tasks. (Sec. 3.3)
• We propose redacting the input image to enforce the base networks to learn accurate dense predictions such that these tasks can precisely condition the regenera-tive process. (Sec. 3.1)
• We devise a novel shared attention scheme, DejaVu-SA, by incorporating the regeneration objective into the parameters of the network. (Sec. 3.4)
• We further provide extensions to DejaVu, such as the text supervision loss DejaVu-TS and Cyclic consis-tency loss DejaVu-CL, further improving performance when additional data is available. (Sec. 3.5)
• DejaVu is a universal framework that can enhance the performance of multiple networks for essential dense prediction tasks on numerous datasets with no added inference cost. (Sec. 4) 2.