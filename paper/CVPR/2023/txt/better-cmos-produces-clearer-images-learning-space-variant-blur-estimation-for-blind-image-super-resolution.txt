Abstract
Most of the existing blind image Super-Resolution (SR) methods assume that the blur kernels are space-invariant.
However, the blur involved in real applications are usu-ally space-variant due to object motion, out-of-focus, etc., resulting in severe performance drop of the advanced SR methods. To address this problem, we firstly introduce two new datasets with out-of-focus blur, i.e., NYUv2-BSR and
Cityscapes-BSR, to support further researches of blind SR with space-variant blur. Based on the datasets, we design a novel Cross-MOdal fuSion network (CMOS) that estimate both blur and semantics simultaneously, which leads to im-proved SR results. It involves a feature Grouping Interac-tive Attention (GIA) module to make the two modalities in-teract more effectively and avoid inconsistency. GIA can also be used for the interaction of other features because of the universality of its structure. Qualitative and quanti-tative experiments compared with state-of-the-art methods on above datasets and real-world images demonstrate the superiority of our method, e.g., obtaining PSNR/SSIM by
+1.91↑/+0.0048↑ on NYUv2-BSR than MANet1. 1.

Introduction
Blind image SR, with the aim of reconstructing High-Resolution (HR) images from Low-Resolution (LR) images with unknown degradations, has attracted great attention due to its significance for practical use [2,5,6,12,15,22–24, 29]. Two degradation models, bicubic downsampling [35] and traditional degradation [26,32], are usually used to gen-erate LR images from HR images. The latter can be mod-eled by: (cid:79) y = (x k) ↓s +n. (1)
It assumes the LR image y is obtained by first convolving the HR image x with a blur kernel k, followed by a down-*Equal contribution.
†Corresponding author. 1https://github.com/ByChelsea/CMOS.git
Figure 1. SR results of KernelGAN [1], DCLS [28] and the pro-posed CMOS on a space-variant blurred LR image. For Kernel-GAN and DCLS, patches are blurry in the first row and have arti-facts in the second row, while CMOS performs well in both cases. sampling operation with scale factor s and an addition of noise n. On top of that, some works [38, 48] propose more complex and realistic degradation models, which also as-sume that blur is space-invariant. However, in real-world applications, blur usually changes spatially due to factors such as out-of-focus and object motion, so that the mis-matches will greatly degrade the performance of existing
SR methods. Fig. 1 gives an example when the LR image suffers from space-variant blur. Since both KernelGAN [1] and DCLS [28] estimate only one blur kernel for an image, there are a lot of mismatches.
In the first row of Fig. 1, where the kernel estimated by the two methods are sharper than the real one of the patch, SR results are over smoothing and high frequency textures are significantly blurred. In the second row, where the kernels estimated are smoother than the correct one, SR results show ringing artifacts caused by over-enhancing high-frequency edges. This phenomenon il-lustrates that mismatch of blur will significantly affect SR results, leading to unnatural outputs. In this paper, we fo-cus on the space-variant blur estimation to ensure that the estimated kernel is correct for each pixel in the images.
A few recent works [15, 23, 43] have taken space-variant blur into account. Among them, MANet [23] is the most representative model, which assumes that blur is space-invariant within a small patch. Based on this, MANet uses a moderate receptive field to keep the locality of degradations.
However, there are still two critical issues. 1) Because there
Figure 2. A condition in which blur and semantic information are inconsistent. This image comes from our dataset NYUv2-BSR. is no available dataset containing space-variant blur in SR field, MANet is trained on space-invariant images, resulting in blur deviation of the training and testing phase. 2) Even limiting the size of the receptive field, the estimation results are still poor at the boundaries of different kernels, leading to mean value prediction of space-variant blur.
To address the aforementioned challenges, we first in-troduce a new degradation method and propose two corre-sponding datasets, i.e., NYUv2-BSR and Cityscapes-BSR to support relevant researches of space-variant blur in the
SR domain. As a preliminary exploration, out-of-focus blur is studied as an example in this paper and it is generated according to the depth of the objects using the method pro-posed in [19]. Besides, we also add some space-invariant blur into the datasets so that the models trained on them can cope with both spatially variant and invariant situations.
Furthermore, to improve the performance at the bound-aries of different blur regions, we present a novel model named Cross-MOdal fuSion network (CMOS). Our intu-ition is that the sharp semantic edges are usually aligned with out-of-focus blur boundaries and it can help to distin-guish different blur amounts. This raises a critical concern that how to effectively introduce semantics into the process.
Specifically, we firstly predict blur and semantics simulta-neously instead of using the semantics as an extra input, which not only avoids using extra information during test phase, but also enables non-blind SR methods to recover finer textures with the two modalities. Secondly, to enhance accuracy at the blur boundaries, we conduct interaction be-tween the semantic and blur features for complementary in-formation learning inspired by multi-task learning [36, 42].
However, in some cases these two modalities are inconsis-tent. As shown in Fig. 2, the wall and the picture on it are completely different in the semantic map, with clear bound-aries. But the depth of them are almost the same, so the blur amounts depending on depth are also very similar. In this case, not only can the two modalities fail to use common features, but they can also negatively influence each other.
Besides, since we add some space-invariant blurred images with uniform blur maps in the datasets, it will also greatly increase the inconsistency.
Motivated by these observations, we propose a feature
Grouping Interactive Attention (GIA) module to help the interaction of the two modalities. GIA has two parallel streams: one operating along the spatial dimension and the other along the channel dimension. Both streams employ group interactions to process the input features and make adjustments. Moreover, GIA has an upsampling layer based on the flow field [21] to support inputs of different resolu-tions. Its universal structure allows it to be used for more than just interactions between the two modalities.
The main contributions of this work are as follows:
• To support researches on space-variant blur in the field of SR, we introduce a new degradation model of out-of-focus blur and propose two new datasets, i.e.,
NYUv2-BSR and Cityscapes-BSR.
• We design a novel model called CMOS for estimating space-variant blur, which leverages extra semantic in-formation to improve the accuracy of blur prediction.
The proposed GIA module is used to make the two modalities interact effectively. Note that GIA is uni-versal and can be used between any two features.
• Combined with existing non-blind SR models, CMOS can estimate both space-variant and space-invariant blur and achieve SOTA SR performance in both cases. 2.