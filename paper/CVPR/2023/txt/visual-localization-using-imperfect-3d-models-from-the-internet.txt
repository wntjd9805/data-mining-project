Abstract
Visual localization is a core component in many appli-cations, including augmented reality (AR). Localization al-gorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images.
This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) al-gorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imper-fect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched.
This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc. 1.

Introduction
Visual localization is the task of estimating the pre-cise position and orientation, i.e., the camera pose, from which a given query image was taken. Localization is a core capability for many applications, including self-driving cars [35], drones [54], and augmented reality (AR) [16, 57].
Visual localization algorithms mainly differ in the way they represent the scene, e.g., explicitly as a 3D model [27, 28, 42, 53, 56, 57, 73, 75, 76, 78, 83, 92, 107] or a set of
Figure 1. We evaluate the use of 3D models from the Internet for visual localization. Such models can differ significantly from the real world in terms of geometry and appearance. posed images [10,66,108,110], or implicitly via the weights of a neural network [5, 12â€“14, 17, 46, 47, 59, 101, 104], and in the way they estimate the camera pose, e.g., based on 2D-3D [27, 28, 73, 75, 78, 92, 107] or 2D-2D [10, 110] matches or as a weighted combination of a set of base poses [46, 47, 59, 66, 81, 104]. Existing visual localization approaches typically use RGB(-D) images to construct their scene representations. Yet, capturing sufficiently many im-ages and estimating their camera poses, e.g., via Structure-from-Motion (SfM) [82,90], are challenging tasks by them-selves, especially for non-technical users unfamiliar with how to best capture images for the reconstruction task.
As shown in [4, 63, 85, 88, 109], modern local features such as [21,23,91,111] are capable of matching real images with renderings of 3D models, even though they were never explicitly trained for this task. This opens up a new way of obtaining the scene representations needed for visual local-ization: rather than building the representation from images captured in the scene, e.g., obtained via a service such as
Google Street View, crowd-sourcing [99], or from photo-sharing websites [52], one can simply download a ready-made 3D model from the Internet (cf . Fig. 1), e.g., from 3D model sharing websites such as Sketchfab and 3D Ware-house. This removes the need to run a SfM system such as
COLMAP [82] or RealityCapture on image data, which can be a very complicated step, especially for non-experts such
as artists designing AR applications. As such, this approach has the potential to significantly simplify the deployment of visual localization-based applications.
However, using readily available 3D models from the In-ternet to define the scene representation also comes with its own set of challenges (cf . Fig. 2): (1) fidelity of ap-pearance: the 3D models might not be colored / textured, thus resulting in very abstract representations that are hard to match to real images [63]. Even if a model is textured, the texture might be generic and repetitive rather than based on the real appearance of the scene. In other cases, the tex-ture might be based on real images, but severely distorted or stretched if these images were captured from drones or planes. (2) fidelity of geometry: some 3D models might be obtained via SfM and Multi-View Stereo (MVS), resulting in 3D models that accurately represent the underlying scene geometry. Yet, this does not always need to be the case.
For example some models might be obtained by extruding building outlines, resulting in a very coarse model of the scene geometry. Others might be created by artists by hand, resulting in visually plausible models with overly simplified geometry, or with wrong aspect ratios, e.g., a model might be too high compared to the width of the building.
Naturally, the imperfections listed above negatively af-fect localization accuracy. The goal of this work is to quan-tify the relation between model inaccuracy and localiza-tion accuracy. This will inform AR application designers which 3D models are likely to provide precise pose esti-mates. Looking at Fig. 2, humans seem to be able to estab-lish correspondences between the models, even if the mod-els are very coarse and untextured. Similarly, humans are able to point out correspondences between coarse and un-textured models and real images that can be used for pose estimation in the context of visual localization. As such, we expect that it is possible to teach the same to a machine. We thus hope that this paper will help researchers to develop algorithms that close the gap between human and machine performance for this challenging matching task.
In summary, this paper makes the following contribu-tions: (1) we introduce the challenging and interesting tasks of visual localization w.r.t. to 3D models downloaded from (2) we provide a new benchmark for this the Internet. task that includes multiple scenes and 3D models of dif-ferent levels of fidelity of appearance and geometry. (3) we present detailed experiments evaluating how these different levels of fidelity affect localization performance. We show that 3D models from the Internet represent a promising new category of scene representations. Still, our results show that there is significant room for improvement, especially for less realistic models (which often are very compact to store). (4) We make our benchmark publicly available (v-pnk.github.io/cadloc) to foster research on visual localiza-tion algorithms capable of handling this challenging task. 2.