Abstract 1.

Introduction
Video motion magnification makes subtle invisible mo-tions visible, such as small chest movements while breath-ing, subtle vibrations in the moving objects etc. But small motions are prone to noise, illumination changes, large mo-tions, etc. making the task difficult. Most state-of-the-art methods use hand-crafted concepts which result in small magnification, ringing artifacts etc. The deep learning-based approach has higher magnification but is prone to severe artifacts in some scenarios. We propose a new phase-based deep network for video motion magnification that op-erates in both domains (frequency and spatial) to address this issue. It generates motion magnification from frequency domain phase fluctuations and then improves its quality in the spatial domain. The proposed models are lightweight networks with fewer parameters (∼ 0.11M and ∼ 0.05M ).
Further, the proposed networks performance is compared to the SOTA approaches and evaluated on real-world and syn-thetic videos. Finally, an ablation study is also conducted to show the impact of different parts of the network.
Subtle movements in real-world circumstances contain significant information and these motions are translated into larger motions using video motion magnification. This turned out to be useful in a variety of applications, includ-ing the categorization of micro-expression [2], [12], [10],
[27], [18], taking a person’s vital signs [3], [20], [16], [9], analysing vibrations [19], [11], [6], [4] etc. However, be-cause little movements are often at the same level as pho-tographic noise (introduced during image acquisition e.g. small illumination changes which are invisible to the naked eye etc.) [22], magnifying them presents a difficult problem.
Second, it might be difficult to magnify minor changes in dynamic situations or when there are large motions present since magnifying large changes produces hazy output and obscures the subtle changes. Third, higher magnification causes issues with texture synthesis problems and results in artifacts, distortions etc. in the output.
The traditional technique [28] comprises of handcrafted filter-based algorithms, that rely on steerable pyramids for image decomposition and filters for motion magnification.
However, it produced noisy output. Through phase vari-Table 1. Conceptual differences between the proposed approach and existing methods for motion magnification
Methods
Motion
Manipulation
Hand-crafted Methods
[29], [24], [22], [23]
Deep-learning Method
Oh et al. [17]
Proposed Method
Phase-Variation based
Linear / Non-Linear filters (Frequency Domain)
Shape Feature Differences based learnable filters (Spatial Domain operations)
Phase and Spatial Variation based learnable filters (Frequency and Spatial Domain)
Magnified Frame
Texture Generation
Steerable Pyramid (wavelet-based reconstruction)
Residual blocks based simple decoder (learnable filters in spatial domain)
Multi-scale texture
Correction block (learnable filters in spatial domain) ations, [25] propose a complex steerable pyramid for im-age decomposition and magnification. This resulted in im-proved magnification while lowering the effects of noise in the magnification process. However, they function poorly in scenarios containing dynamic or large motion. Two dif-ferent approaches are proposed to tackle these issues: hand design filtering and deep learning models. In the first ap-proach, authors propose hand-design filters [29], [24], [22],
[23] compatible with earlier methods, to work both in static and dynamic scenarios. But, they have small amount of magnification and are prone to ringing artifacts. The second technique is deep learning, which is based on the notion that deep convolutional networks may produce a more optimal solution [17]. Oh et. al [17] proposed a deep network with more magnification, compared to handcrafted methods but it’s solution is computationally challenging, prone to distor-tions, artefacts, and texture generation-related problems.
We propose a phase-based deep network for video mo-tion magnification to address these concerns. It combines the handcrafted approach of phase-based motion magnifi-cation [25] with deep learning-based spatial magnification
In addition, for
[17] to overcome each other limitations. real-time applications, lightweight networks D1 and D2 are proposed. The following are the key contributions:-• A novel multi-domain lightweight networks (D1 and
D2) is proposed for video motion magnification.
• A frequency domain-based motion magnification block is proposed for motion synthesis. It directly es-timates the phase and amplitude changes for the mag-nification according to the provided magnification fac-tor. This helps to reduce noise effects in the magnifica-tion process and generates motion (which depends on phase variations).
• A spatial domain-based multi-scale texture correction block is proposed to improve the texture quality.
It estimates the texture component at each scale using information from input frames and magnified motion features in the spatial domain.
The proposed networks (D1 and D2) are evaluated qualita-tively and quantitatively on real-world and synthetic videos on different tasks. Additionally, an experiment is conducted to check the physical sccuracy of the proposed method. An ablation study is also conducted to see the effects of differ-ent parts of the proposed network. 2.