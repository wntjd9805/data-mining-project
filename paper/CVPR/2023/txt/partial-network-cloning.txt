Abstract
In this paper, we study a novel task that enables par-tial knowledge transfer from pre-trained models, which we term as Partial Network Cloning (PNC). Unlike prior meth-ods that update all or at least part of the parameters in the target network throughout the knowledge transfer process,
PNC conducts partial parametric “cloning” from a source network and then injects the cloned module to the target, without modifying its parameters. Thanks to the transferred module, the target network is expected to gain additional functionality, such as inference on new classes; whenever needed, the cloned module can be readily removed from the target, with its original parameters and competence kept intact. Specifically, we introduce an innovative learning scheme that allows us to identify simultaneously the com-ponent to be cloned from the source and the position to be inserted within the target network, so as to ensure the opti-mal performance. Experimental results on several datasets demonstrate that, our method yields a significant improve-ment of 5% in accuracy and 50% in locality when com-pared with parameter-tuning based methods. Our code is available at https://github.com/JngwenYe/PNCloning. 1.

Introduction
With the recent advances in deep learning, an increas-ingly number of pre-trained models have been released online, demonstrating favourable performances on various computer vision applications. As such, many model-reuse approaches have been proposed to take advantage of the pre-trained models.
In practical scenarios, users may re-quest to aggregate partial functionalities from multiple pre-trained networks, and customize a target network whose competence differs from any network in the model zoo.
A straightforward solution to the functionality dynamic changing is to re-train the target network using the origi-nal training dataset, or to conduct finetuning together with regularization strategies to alleviate catastrophic forget-† Corresponding author.
Figure 1. Illustration of partial network cloning. Given a set of pre-trained source models, we “clone” the transferable modules from the source, and insert them into the target model (left) while preserving the functionality (right). ting [3,19,39], which is known as continual learning. How-ever, direct re-training is extremely inefficient, let alone the fact that original training dataset is often unavailable. Con-tinual learning, on the other hand, is prone to catastrophic forgetting especially when the amount of data for finetun-ing is small, which, unfortunately, often occurs in practice.
Moreover, both strategies inevitably overwrite the original parameters of the target network, indicating that, without explicitly storing original parameters of the target network, there is no way to recover its original performance or com-petence when this becomes necessary.
In this paper, we investigate a novel task, termed as
Partial Network Cloning (PNC), to migrate knowledge from the source network, in the form of a transferable mod-ule, to the target one. Unlike prior methods that rely on updating parameters of the target network, PNC attempts to clone partial parameters from the source network and then directly inject the cloned module into the target, as shown in Fig. 1. In other words, the cloned module is transferred to the target in a copy-and-paste manner. Meanwhile, the
original parameters of the target network remain intact, in-dicating that whenever necessary, the newly added module can be readily removed to fully recover its original function-ality. Notably, the cloned module per se is a fraction of the source network, and therefore requirements no additional storage expect for the lightweight adapters. Such flexibil-ity to expand the network functionality and to detach the cloned module without altering the base of the target or al-locating extra storage, in turn, greatly enhances the utility of pre-trained model zoo and largely enables plug-and-play model reassembly.
Admittedly, the ambitious goal of PNC comes with sig-nificant challenges, mainly attributed to the black-box na-ture of the neural networks, alongside our intention to pre-serve the performances on both the previous and newly-added tasks of the target. The first challenge concerns the localization of the to-be-cloned module within the source network, since we seek discriminant representations and good transferability to the downstream target task. The sec-ond challenge, on the other hand, lies in how to inject the cloned module to ensure the performance.
To solve these challenges, we introduce an innovative strategy for PNC, through learning the localization and in-sertion in an intertwined manner between the source and tar-get network. Specifically, to localize the transferable mod-ule in the source network, we adopt a local-performance-based pruning scheme for parameter selection. To adap-tively insert the module into the target network, we utilize a positional search method in the aim to achieve the optimal performance, which, in turn, optimizes the localization op-eration. The proposed PNC scheme achieves performances significantly superior to those of the continual learning set-ting (5% ∼ 10%), while reducing data dependency to 30%.
Our contributions are therefore summarized as follows.
• We introduce a novel yet practical model re-use setup, termed as partial network cloning (PNC). In contrast to conventional settings the rely on updating all or part of the parameters in the target network, PNC migrates parameters from the source in a copy-and-paste man-ner to the target, while preserving original parameters of the target unchanged.
• We propose an effective scheme towards solving PNC, which conducts learnable localization and insertion of the transferable module jointly between the source and target network. The two operations reinforce each other and together ensure the performance of the tar-get network.
• We conduct experiments on four widely-used datasets and showcase that the proposed method consis-tently achieves results superior to the conventional knowledge-transfer settings, including continual learn-ing and model ensemble. 2.