Abstract
Optical flow estimation is a challenging problem remain-ing unsolved. Recent deep learning based optical flow mod-els have achieved considerable success. However, these models often train networks from the scratch on standard optical flow data, which restricts their ability to robustly and geometrically match image features.
In this paper, we propose a rethinking to previous optical flow estima-tion. We particularly leverage Geometric Image Matching (GIM) as a pre-training task for the optical flow estimation (MatchFlow) with better feature representations, as GIM shares some common challenges as optical flow estimation, and with massive labeled real-world data. Thus, match-ing static scenes helps to learn more fundamental feature correlations of objects and scenes with consistent displace-ments. Specifically, the proposed MatchFlow model em-ploys a QuadTree attention-based network pre-trained on
MegaDepth to extract coarse features for further flow re-gression. Extensive experiments show that our model has great cross-dataset generalization. Our method achieves 11.5% and 10.1% error reduction from GMA on Sintel clean pass and KITTI test set. At the time of anony-mous submission, our MatchFlow(G) enjoys state-of-the-art performance on Sintel clean and final pass compared to published approaches with comparable computation and memory footprint. Codes and models will be released in https://github.com/DQiaole/MatchFlow. 1.

Introduction
This paper studies optical flow estimation, which is the problem of estimating the per-pixel displacement vector be-tween two frames. It is very useful to various real-world applications, such as video frame interpolation [24], video inpainting [17], and action recognition [48]. The recent direct-regression based methods [16, 22, 25, 34, 44, 50] have achieved great success by using powerful deep models, es-pecially the recent transformers [20, 54]. Among them,
RAFT [50] employs a convolutional GRU for iterative re-finements, which queries local correlation features from a multi-scale 4D correlation volume. And GMA [25] fur-ther proposes a global motion aggregation module based on the self-similarity of image context, which greatly im-proves the performance within the occluded regions without degrading the performance in non-occluded regions. Typi-cally, these models often train networks from the scratch on standard optical flow data, with the matching module (correlation volume) to help align the features of different images/frames. Generally, these current optical flow esti-mation algorithms still can not robustly handle several in-tractable cases, e.g., small and fast-moving objects, occlu-sions, and textureless regions, as these estimators have very restricted ability of robustly learning the local image feature correspondence of different frames.
In this paper, we aim to provide a rethinking to the im-portance of Geometric Image Matching (GIM) to the opti-cal flow estimation. In particular, despite GIM being de-signed to deal with the geometrically matching of static scenes, it indeed shares some common challenges with opti-cal flow estimation, such as large displacement and appear-ance change [52]. Thus, we advocate that the deep models for optical flow estimation should be trained from match-ing static scene pairs with consistent displacements. This can potentially help these models to learn the local low-level features and color correlations at the early stages of networks, before extracting the priors for 3D multi-object motion. Furthermore, compared to optical flow data, it is much easier and simple to collect the real-world GIM data [14, 27], labeled by camera poses and depth computed from ground-truth or pioneering multi-view stereo man-ners [40]. Such extensive real-world data largely improves the generalization of optical flow.
*Equal contributions.
†Corresponding author. The author is also with Shanghai Key Lab of Intelligent Information Processing, and Fudan ISTBI-ZJNU Algorithm
Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jin-hua, China.
On the other hand, we can also emphasize a rethinking of the general training pipeline of optical flow. Specifi-cally, since the creative work of FlowNet2 [22], optical flow models [25, 42, 42] are often trained following the schedule
Figure 1. Qualitative results on KITTI test set. Red dashed boxes mark the regions of substantial improvements. Please zoom in for details. of Curriculum Learning [2], i.e., from FlyingChair [16] to
FlyThings3D [31] and finally to Sintel [7] or KITTI [18].
Nevertheless, the motion contained in FlyingChair is still far from the simplest scenario. Empirically, the static scene under viewpoint changes of GIM [46, 49], can also be taken as one special type of optical flow estimation, which is even much simpler than FlyingChair. Therefore, it is reason-able to take GIM amenable for being the very first stage of the curriculum learning pipeline. Essentially, as mentioned above, the GIM data can be easily collected at large-scale, and thus will greatly benefit the learning of deep models.
Formally, this paper well elaborates on the key idea of taking GIM as the prefixed task for optical flow. We draw the inspiration from recent GIM works, and present a novel MatchFlow that can effectively generalize the pre-trained image matching module to estimate optical flow.
The key component of MatchFlow is a new module of Fea-ture Matching Extractor, composed of Resnet-16 and 8 in-terleaving self/cross-Quadrtee attention blocks, trained on
GIM task and used to get the 4D correlation volume. After
Feature Matching Extractor, our MatchFlow still takes the common practice of using GRUs module to handle optical flow estimation, with an optional GMA modelling the con-text information. In this paper, we denote the full model based on GMA as MatchFlow(G), and the model without
GMA module (a.k.a. RAFT) as MatchFlow(R).
Following the standard optical flow training proce-dure [25, 50], we conduct extensive experiments on Fly-ingChair
[7], and
[16], FlyingThings3D [31], Sintel
KITTI [18]. Experiments results show that MatchFlow en-joys good performance and great cross-dataset generaliza-tion. Formally, RAFT-based MatchFlow(R) obtains an Fl-all error 13.6% on KITTI training set after being trained on synthetic datasets. In addition, GMA-based MatchFlow(G) achieves 11.5% and 10.1% error reduction from GMA on the Sintel clean pass and KITTI test set. The qualitative comparison on KITTI also shows the superior performance of MatchFlow as in Fig. 1. Ablation studies verify that GIM pre-training indeed helps to learn better feature representa-tion for optical flow estimation.
We highlight our contributions as follows. (1) We re-formulate the optical flow pipeline, and propose the idea of employing GIM as the preluding task for optical flow.
This offers a rethinking to the learning based optical flow estimation. (2) We further present a novel matching-based optical flow estimation model – MatchFlow, which has the new module of Feature Matching Extractor, learned by the
GIM pre-training task for optical flow estimation. Accord-ingly, the pipeline of curriculum learning has also been up-dated to effectively train our MatchFlow. (3) We introduce the massive real-world matching data to train our model.
And thus our model can extract robust features to handle with the consistent motion of scenes, and common chal-lenges faced by both tasks. (4) We conduct extensive ex-periments and ablation studies to show that both the match-ing based pre-training and interleaving self/cross-attention modules are critical for the final optical flow performance.
The proposed model shows great cross-dataset generaliza-tion and better performance over several optical flow com-petitors on several standard benchmarks. 2.