Abstract
Self-supervised pretraining on large unlabeled datasets has shown tremendous success in improving the task per-formance of many 2D and small scale 3D computer vi-sion tasks. However, the popular pretraining approaches have not been impactfully applied to outdoor LiDAR point cloud perception due to the latter’s scene complexity and wide range. We propose a new self-supervised pretrain-ing method ISCC with two novel pretext tasks for LiDAR point clouds. The first task uncovers semantic information by sorting local groups of points in the scene into a glob-ally consistent set of semantically meaningful clusters using contrastive learning, complemented by a second task which reasons about precise surfaces of various parts of the scene through implicit surface reconstruction to learn geomet-ric structures. We demonstrate their effectiveness through transfer learning on 3D object detection and semantic seg-mentation in real world LiDAR scenes. We further design an unsupervised semantic grouping task to show that our approach learns highly semantically meaningful features. 1.

Introduction
Robust and reliable 3D LiDAR perception is critical for autonomous driving systems. Unlike images, LiDAR pro-vides unambiguous measurements of the vehicle’s 3D en-vironment. A plethora of perception models have arisen in recent years to enable a variety of scene understanding tasks using LiDAR input, such as object detection and semantic segmentation. However, training these models generally re-lies on a large quantity of human annotated data, which is tedious and expensive to produce.
Recently, self-supervised learning has attracted signifi-cant research attention [5, 7, 16–18], as it has the potential to increase performance on downstream tasks with limited quantities of annotated data in the image domain. However, self-supervised learning has shown less impact for outdoor
*Work done at AWS AI.
Figure 1. With frame t as input, we 1) apply contrastive cluster learning to reason about unsupervised semantic clustering and also enforce feature consistency across different views, and 2) conduct implicit surface reconstruction in randomly cropped out regions. 3D point clouds. A core difficulty stems from the rela-tive difficulty in designing appropriate pretext tasks used for self-supervision.
In the image domain, the ImageNet dataset [35] provides millions of canonical images of ev-eryday objects, allowing for straightforward manipulations to generate pretext tasks that lead to strong object-centric or semantic group-centric feature learning. While large-scale unlabeled outdoor LiDAR datasets are relatively easy to col-lect, the data samples exhibit a high level of scene complex-ity, sparsity of measurements, and heavy dependency on the observer positioning and sensor type. These factors pose great challenges for creating useful pretext tasks.
Recent works have proposed 3D-specific self-supervised learning, starting with scene-level contrastive learning [44, 49], followed by the work of [30] and [47], which use finer, region-level granularity to better encode individual compo-nents of large scale LiDAR point clouds. However, these techniques do not explicitly make use of regularities in 3D shapes and surfaces.
In our work, we propose ISCC (Implicit Surface Con-trastive Clustering) which consists of two new pretext tasks to automatically learn semantically meaningful feature ex-traction without annotations for LiDAR point clouds. The first task focuses on learning semantic information by sort-ing local groups of points in the scene into a globally consis-tent set of semantically meaningful clusters using the con-trastive learning setup [5]. This is augmented with a second task which reasons about precise surfaces of various parts of the scene through implicit surface reconstruction to learn geometric regularities. A high level overview is found in
Figure 1. Furthermore, we showcase a novel procedure to generate training signals for implicit surface reasoning in the absence of dense 3D surface meshes which are difficult to obtain for large scale LiDAR point clouds.
Using the large real world KITTI [13] and Waymo [37] datasets, we show that our approach is superior to re-lated state-of-the-art self-supervised learning techniques in downstream finetuning performance in semantic segmenta-tion and object detection. For example, we see a 72% gain in segmentation performance on SemanticKITTI versus the state-of-the-art [48] when fine-tuned with 1% of the anno-tations, and exceeds the accuracy achieved by using twice the annotations with random initialized weights. As well, we analyze the semantic consistency of the learned features through a new unsupervised semantic grouping task, and show that our learned features are able to form semantic groups even in the absence of supervised fine-tuning. 2.