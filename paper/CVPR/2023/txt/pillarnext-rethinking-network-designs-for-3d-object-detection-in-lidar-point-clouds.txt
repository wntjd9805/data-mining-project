Abstract
In order to deal with the sparse and unstructured raw point clouds, most LiDAR based 3D object detection re-search focuses on designing dedicated local point aggre-gators for fine-grained geometrical modeling. In this pa-per, we revisit the local point aggregators from the perspec-tive of allocating computational resources. We find that the simplest pillar based models perform surprisingly well con-sidering both accuracy and latency. Additionally, we show that minimal adaptions from the success of 2D object detec-tion, such as enlarging receptive field, significantly boost the performance. Extensive experiments reveal that our pil-lar based networks with modernized designs in terms of ar-chitecture and training render the state-of-the-art perfor-mance on two popular benchmarks: Waymo Open Dataset and nuScenes. Our results challenge the common intuition that detailed geometry modeling is essential to achieve high performance for 3D object detection. 1.

Introduction 3D object detection in LiDAR point clouds is an essential task in an autonomous driving system, as it provides crucial information for subsequent onboard modules, ranging from perception [24, 25], prediction [27, 42] to planning [2, 23].
There have been extensive research efforts on developing sophisticated networks that are specifically designed to cope with point clouds in this field [14, 32, 33, 43, 47].
Due to the sparse and irregular nature of point clouds, most existing works adopt the grid based methods, which convert point clouds into regular grids, such as pillar [14], voxel [47] and range view [26], such that regular operators can be applied. However, it is a common belief that the grid based methods (especially for pillar) inevitably introduce information loss, leading to inferior results, in particular for small objects (e.g., pedestrians). Recent research [33] pro-poses the hybrid design for fine-grained geometrical mod-eling to combine the point and gird based representations.
We name all the above operators as local point aggregators because they aim to aggregate point features in a certain
*Corresponding author xiaodong@qcraft.ai
Figure 1. Overview of pillar, voxel and multi-view fusion (MVF) based 3D object detection networks under different GFLOPs. The
Pillar/Voxel+ denote the corresponding models, which are trained with an enhanced strategy. We report the L2 BEV and 3D APH of vehicle on the validation set of Waymo Open Dataset. neighborhood. We observe that the current mainstream of 3D object detection is to develop more specific operators for point clouds, while leaving the network architectures al-most unexplored. Most existing works are still built upon the very original architectures of SECOND [44] or Point-Pillars [14], which lacks of modernized designs.
Meanwhile, in a closely related area, 2D object detec-tion in images has achieved remarkable progress in both accuracy and efficiency, which can be largely attributed to the advances in network architectures. Among them, the powerful backbones (e.g., ResNet [12], ViT [10] and
Swin Transformers [20]) as well as the effective necks (e.g.,
FPN [17], BiFPN [40] and YOLOF [6]) are in particular no-table. Therefore, the research focus of 2D object detection is largely different from that of 3D object detection.
In light of the aforementioned observations, we rethink what should be the focus for 3D object detection in LiDAR point clouds. Specifically, we revisit the two fundamental issues in designing a 3D object detection model: the local point aggregator and the network architecture.
First, we compare local point aggregators from a new perspective, i.e., the computational budget. A fine-grained aggregator usually demands more extensive computing re-sources than the coarse one. For instance, the voxel based aggregator employs 3D convolutions, which require more network parameters and run much slower than 2D convo-lutions. This raises a question of how to effectively allo-cate the computational budget or network capacity. Should we spend the resources on fine-grained structures or assign them to coarse grids? Surprisingly, as shown in Figure 1, when training with an enhanced strategy and under a com-parable budget, a simple pillar based model can achieve su-perior or on par performance with the voxel based model, even for small objects such as pedestrians, while signifi-cantly outperform the multi-view fusion based model. This challenges the actual performance gain and the necessity of fine-grained local 3D structures. Our finding is also con-sistent with the recent works [19, 31] in general point cloud analysis, demonstrating that different local point aggrega-tors perform similarly under strong networks.
Second, for the network architecture, we do not aim to propose any domain specific designs for point clouds, in-stead, we make minimal adaptations from the success of 2D object detection and show that they already outperform most of the existing methods with specific designs for point clouds. One key finding is that enlarging receptive field properly brings significant improvement. Unlike previous works [32, 47] that rely on multi-scale feature fusion, we show that a single scale at the final stage with sufficient re-ceptive field obtains better performance. Such promising results suggest that 3D object detection can inherit the suc-cessful practices well developed in 2D domain.
Levaraging on the findings above, we propose a pillar based network, dubbed as PillarNeXt, which leads to the state-of-the-art results on two popular benchmarks [3, 36].
Our approach is simple yet effective, and enjoys strong scal-ability and generalizability. We develop a series of networks with different trade-offs between accuracy and latency by tuning the number of network parameters, which can be used for both on-board [14] and off-board [30] applications in autonomous driving.
Our main contributions can be summarized as follows. (1) To our knowledge, this is the first work that compares different local point aggregators (pillar, voxel and multi-view fusion) from the perspective of computational bud-get allocation. Our findings challenge the common belief by showing that pillar can achieve comparable 3D mAP and better birdâ€™s eye view (BEV) mAP compared to voxel, and substantially outperform multi-view fusion in both 3D and BEV mAP. (2) Inspired by the success of 2D object detection, we find that enlarging receptive field is crucial for 3D object detection. With minimal adaptions, our de-tectors outperform existing methods with sophisticated de-signs for point clouds. (3) Our networks with appropriate training achieve superior results on two large-scale bench-marks. We hope our models and related findings can serve as a strong and scalable baseline for future research in this community. Our code and model will be made available at https://github.com/qcraftai/pillarnext. 2.