Abstract
Neural Radiance Fields (NeRF) have demonstrated su-perior novel view synthesis performance but are slow at ren-dering. To speed up the volume rendering process, many ac-celeration methods have been proposed at the cost of large memory consumption. To push the frontier of the efficiency-memory trade-off, we explore a new perspective to accel-erate NeRF rendering, leveraging a key fact that the view-point change is usually smooth and continuous in interac-tive viewpoint control. This allows us to leverage the in-formation of preceding viewpoints to reduce the number of rendered pixels as well as the number of sampled points
In our pipeline, a along the ray of the remaining pixels. low-resolution feature map is rendered first by volume ren-dering, then a lightweight 2D neural renderer is applied to generate the output image at target resolution leverag-ing the features of preceding and current frames. We show that the proposed method can achieve competitive render-ing quality while reducing the rendering time with little memory overhead, enabling 30FPS at 1080P image reso-lution with a low memory footprint. 1.

Introduction
Novel View Synthesis (NVS) is a long-standing problem in computer vision and computer graphics with applications in navigation [40], telepresence [60], and free-viewpoint video [51]. Given a set of posed images, the goal is to ren-der the scene from unseen viewpoints to facilitate viewpoint control interactively.
Recently, Neural Radiance Fields (NeRF) have emerged as a popular representation for NVS due to the capacity to render high-quality images from novel viewpoints. NeRF represents a scene as a continuous function, parameterized by a multilayer perceptron (MLP), that maps a continuous 3D position and a viewing direction to a density and view-dependent radiance [23]. A 2D image is then obtained via volume rendering, i.e., accumulating colors along each ray.
∗ Corresponding author. ∗∗ Co-corresponding author.
Figure 1. Illustration. We exploit smooth viewpoint trajectory to accelerate NeRF rendering, achieved by performing volume ren-dering at a low resolution and recovering the target image guided by multiple viewpoints. Our method enables fast rendering with a low memory footprint.
However, NeRF is slow at rendering as it needs to query the MLP millions of times to render a single image, pre-venting NeRF from interactive view synthesis. Many recent works have focused on improving the rendering speed of
NeRF, yet there is a trade-off between rendering speed and memory cost. State-of-the-art acceleration approaches typi-cally achieve fast rendering at the expense of large memory consumption [13, 56], e.g., by pre-caching the intermedi-ate output of the MLP, leading to hundreds of megabytes to represent a single scene. While there are some attempts to accelerate NeRF rendering with a low memory foot-print [16, 25], the performance has yet to reach cache-based methods. In practice, it is desired to achieve faster rendering at a lower memory cost.
To push the frontier of this trade-off, we propose to speed up NeRF rendering from a new perspective, leveraging the critical fact that the viewpoint trajectory is usually smooth and continuous in interactive control. Unlike existing NeRF acceleration methods that reduce the rendering time of each viewpoint individually, we accelerate the rendering by ex-ploiting the information overlap between multiple consecu-tive viewpoints. Fig. 1 illustrates our SteerNeRF, a simple yet effective framework leveraging the SmooTh viEwpoint trajEctoRy to speed up NeRF rendering. Here, “steer” also refers to a user smoothly controlling the movement of a camera during interactive real-time rendering.
Exploiting the smooth view trajectory, we can acceler-ate volume rendering by reducing the number of sample points while maintaining image fidelity using efficient 2D neural rendering. More specifically, our method comprises a rendering buffer, neural feature fields, and a lightweight 2D neural renderer. We first render a low-resolution feature map at a given viewpoint via volume rendering. The sam-pling range along each ray is reduced by fetching a depth map from the rendering buffer and projecting it to the cur-rent view. This effectively reduces the volume rendering computation as both the number of pixels and the number of samples for the remaining pixels are reduced. Next, we combine preceding and current feature maps to recover the image at the target resolution using a 2D neural renderer, i.e., a 2D convolutional neural network. The neural feature fields and the 2D neural renderer are trained jointly end-to-end. The combination of low-resolution volume rendering and high-resolution neural rendering leads to fast render-ing, yet maintains high fidelity and temporal consistency at a low memory cost.
We summarize our contributions as follows. i) We pro-vide a new perspective on NeRF rendering acceleration based on the assumption of smooth viewpoint trajectory.
Our method is orthogonal to existing NeRF rendering accel-eration methods and can be combined with existing work to achieve real-time rendering at a low memory footprint. ii)
To fully exploit information from preceding viewpoints, we propose a simple framework that combines low-resolution volume rendering and high-resolution 2D neural render-ing. With end-to-end joint training, the proposed frame-work maintains high image fidelity. iii) Our experiments on synthetic and real-world datasets show that our method achieved a rendering speed of nearly 100 FPS at an image resolution of 800 × 800 pixels and 30 FPS at 1920 × 1080
It is faster than other low-memory NeRF accel-pixels. eration methods and narrows the speed gap between low-memory and cache-based methods. 2.