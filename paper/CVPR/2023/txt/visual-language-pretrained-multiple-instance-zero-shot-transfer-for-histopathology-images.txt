Abstract
Contrastive visual language pretraining has emerged as a powerful method for either training new language-aware image encoders or augmenting existing pretrained models with zero-shot visual recognition capabilities. However, existing works typically train on large datasets of image-text pairs and have been designed to perform downstream tasks involving only small to medium sized-images, neither of which are applicable to the emerging field of computa-tional pathology where there are limited publicly available paired image-text datasets and each image can span up to 100,000 × 100,000 pixels.
In this paper we present MI-Zero, a simple and intuitive framework for unleashing the zero-shot transfer capabilities of contrastively aligned im-age and text models on gigapixel histopathology whole slide images, enabling multiple downstream diagnostic tasks to be carried out by pretrained encoders without requiring any additional labels. MI-Zero reformulates zero-shot trans-fer under the framework of multiple instance learning to overcome the computational challenge of inference on ex-tremely large images. We used over 550k pathology re-ports and other available in-domain text corpora to pre-train our text encoder. By effectively leveraging strong pre-trained encoders, our best model pretrained on over 33k histopathology image-caption pairs achieves an average median zero-shot accuracy of 70.2% across three different real-world cancer subtyping tasks. Our code is available at: https://github.com/mahmoodlab/MI-Zero. 1.

Introduction
Weakly-supervised deep learning for computational pathology (CPATH) has rapidly become a standard ap-proach for modelling whole slide image (WSI) data [9, 30, 47,71,73]. To obtain “clinical grade” machine learning per-formance on par with human experts for a given clinical task, many approaches adopt the following model develop-ment life cycle: 1) curate a large patient cohort (N > 1000 samples) with diagnostic whole-slide images and clinical labels, 2) unravel and tokenize the WSI into a sequence of patch features, 3) use labels to train a slide classifier that learns to aggregate the patch features for making a predic-tion, and 4) transfer the slide classifier for downstream clin-ical deployment [9, 43, 91].
Successful examples of task-specific model development (e.g. training models from scratch for each task) include prostate cancer grading and lymph node metastasis detec-tion [5, 7–9, 50, 70]. However, this paradigm is intractable if one wishes to scale across the hundreds of tumor types across the dozens of different organ sites in the WHO clas-sification system1, with most tumor types under-represented in public datasets or having inadequate samples for model development [41, 92]. To partially address these limita-tions, self-supervised learning has been explored for learn-ing the patch representations within the WSI with the idea that certain local features, such as tumor cells, lymphocytes, and stroma, may be conserved and transferred across tis-sue types [10, 16, 39, 40, 44, 64, 77]. Though morphological features at the patch-level are captured in a task-agnostic fashion, developing the slide classifier still requires supervi-sion, which may not be possible for disease types with small sample sizes. To scale slide classification across the vast number of clinical tasks and possible findings in CPATH, an important shift needs to be made from task-specific to task-agnostic model development.
Recent works [33,55] have demonstrated that large-scale pretraining using massive, web-sourced datasets of noisy image-text pairs can not only learn well-aligned represen-tation spaces between image and language, but also transfer the aligned latent space to perform downstream tasks such as image classification. Specifically for CLIP [55], after pretraining a vision encoder in a task-agnostic fashion, the vision encoder can be “prompted” with text from the label
†These authors contributed equally to this work. 1tumourclassification.iarc.who.int/
space (referred to as “zero-shot transfer”, as no labeled ex-amples are used in the transfer protocol). Despite the vol-ume of zero-shot transfer applications developed for natural images [21, 33, 45, 49, 57, 80, 82] and certain medical imag-ing modalities (e.g. radiology [29, 60, 68, 78, 90]), zero-shot transfer for pathology has not yet been studied2. We believe this is due to 1) the lack of large-scale, publicly available datasets of paired images and captions in the highly special-ized field of pathology, and 2) fundamental computational challenges associated with WSIs, as images can span up to 100, 000 × 100, 000 pixels and do not routinely come with textual descriptions, bounding box annotations or even re-gion of interest labels.
In this work, we overcome the above data and compu-tational challenges and develop the first zero-shot transfer framework for the classification of histopathology whole slide images. On the data end, we curated the largest known dataset of web-sourced image-caption pairs specifically for pathology. We propose “MI-Zero”, a simple and intuitive multiple instance learning-based [3, 30] method for utiliz-ing the zero-shot transfer capability of pretrained visual-language encoders for gigapixel-sized WSIs that are rou-tinely examined during clinical practice. We validate our approach on 3 different real-world cancer subtyping tasks, and perform multiple ablation experiments that explore im-age pretraining, text pretraining, pooling strategies, and sample size choices for enabling zero-shot transfer in MI-Zero. 2.