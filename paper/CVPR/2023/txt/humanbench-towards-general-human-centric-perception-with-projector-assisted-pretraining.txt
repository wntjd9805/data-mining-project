Abstract
Human-centric perceptions include a variety of vision tasks, which have widespread industrial applications, in-cluding surveillance, autonomous driving, and the meta-It is desirable to have a general pretrain model verse. for versatile human-centric downstream tasks. This pa-per forges ahead along this path from the aspects of both benchmark and pretraining methods. Specifically, we pro-pose a HumanBench based on existing datasets to com-prehensively evaluate on the common ground the gener-alization abilities of different pretraining methods on 19 datasets from 6 diverse downstream tasks, including person
ReID, pose estimation, human parsing, pedestrian attribute recognition, pedestrian detection, and crowd counting. To learn both coarse-grained and fine-grained knowledge in human bodies, we further propose a Projector AssisTed
Hierarchical pretraining method (PATH) to learn diverse knowledge at different granularity levels. Comprehensive evaluations on HumanBench show that our PATH achieves new state-of-the-art results on 17 downstream datasets and on-par results on the other 2 datasets. The code will be publicly at https://github.com/OpenGVLab/HumanBench. 1.

Introduction
Human-centric perception has been a long-standing pur-suit for computer vision and machine learning communi-ties.
It encompasses massive research tasks and applica-tions including person ReID in surveillance [16, 17, 60, 96, 112], human parsing and pose estimation in the meta-verse [47, 48, 61, 79, 90, 92], and pedestrian detection in au-tonomous driving [8, 51, 87]. Although significant progress has been made, most existing human-centric studies and pipelines are task-specific for better performances, leading
*Equal contribution. This work was done in SenseTime.
†Corresponding author. to huge costs in representation/network design, pretraining, parameter-tuning, and annotations. To promote real-world deployment, we ask: whether a general human-centric pre-training model can be developed that can benefit diverse human-centric tasks and be efficiently adapted to down-stream tasks?
Intuitively, we argue that pretraining such general human-centric models is possible for two reasons. First, there are obvious correlations among different human-centric tasks. For example, both human parsing and pose es-timation predict the fine-grained parts of human bodies [29, 49] with differences in annotation granularities. Thus, the annotations in one human-centric task may benefit other human-centric tasks when trained together. Second, recent achievements in foundation models [5, 11, 38, 65, 66, 80] have shown that large-scale deep neural networks (e.g., transformers [13]) have the flexibility to handle diverse in-put modalities and the capacity to deal with different tasks.
For example, Uni-Percevier [115] and BEITv3 [85] are ap-plicable to multiple vision and language tasks.
Despite the opportunities of processing multiple human-centric tasks with one pretraining model, there are two ob-stacles for developing general human-centric pretraining models. First, although there are many benchmarks for ev-ery single human-centric task, there is still no benchmark to fairly and comprehensively compare various pretraining methods on a common ground for a broad range of human-centric tasks, data distributions, and application scenarios.
Second, different from most existing general foundation models trained by unified global vision-language consis-tencies, pretraining human-centric models are required to learn both global (e.g., person ReID and pedestrian detec-tion) and fine-grained semantic features (e.g., pose estima-tion and human parsing) of human bodies from diverse an-notation granularity simultaneously.
In this paper, we first build a benchmark, called Hu-manBench, based on existing datasets to enable pretrain-Figure 1. (a-b) Overview of our proposed HumanBench. HumanBench includes diverse images, including scene images and person-centric images. Our HumanBench also has comprehensive evaluation. Specifically, it evaluates pretraining models on 6 tasks, including pedestrian detection, human parsing, pose estimation, pedestrian attribute recognition, person ReID, and crowd counting. (c) High performances are achieved by our pretraining method on HumanBench. We report 1-heavy occluded MR−2 and 1-EPE for Caltech and H3.6pose. ing and evaluating human-centric representations that can be generalized to various downstream tasks. HumanBench has two appealing properties. (1) Diversity. The images in our HumanBench include diverse image properties, rang-ing from person-centric cropped images to scene images with crowd pedestrians, ranging from indoor scenes to out-door scenes (Fig. 1(a)), and from surveillance to metaverse. (2) Comprehensiveness. Humanbench covers comprehen-sive image-based human-centric tasks in both pretraining datasets and downstream tasks (Fig. 1(b)). For pretrain-ing, we include 11 million images from 37 datasets across five representative human-centric tasks, i.e., person ReID, pose estimation, human parsing, pedestrian attribute recog-nition, and pedestrian detection. For evaluation, Human-Bench evaluates the generalization abilities on 12 pretrain-ing datasets, 6 unseen datasets of pretraining tasks, and 2 datasets out of pretraining tasks, ranging from global pre-diction, i.e., ReID, to local prediction, i.e., human pars-ing and pose estimation. Results on our HumanBench (Fig. 1(c)) lead to two interesting findings. First, compared with datasets with natural images for general pretrained models, HumanBench is more effective for human-centric perception tasks. Second, as human-centric pretraining re-quires to learn features of diverse granularity, supervised pretraining methods with proper designs can learn from di-verse annotations in HumanBench and perform better than the existing unsupervised pretraining methods, for which details will be shown in Sec. 5.3.
Based on HumanBench, we further investigate how to learn a better human-centric supervised pretraining model from diverse datasets with various annotations. How-ever, naive multitask pretraining may easily suffer from the task conflicts [53, 97] or overfitting to pretrained annota-tions [67, 107], losing the desirable generalization ability of pretraining. Inspired by [86], which suggests adding an
MLP projector before the task head can significantly en-hance the generalization ability of supervised pretraining, we propose Projector AssisTed Hierarchical Pre-training (PATH), a projector assisted pretraining method with hi-erarchical weight sharing to tackle the task conflicts of su-pervised pretraining from diverse annotations. Specifically, the weights of backbones are shared among all datasets, and the weights of projectors are shared only for datasets of the same tasks, while the weights of the heads are shared only for a single dataset – forming a hierarchical weight-sharing structure. During the pretraining stage, we insert the task-specific projectors before dataset heads but discard them when evaluating models on downstream tasks. With the hi-erarchical weight-sharing strategy, our pretraining method enforces the backbone to learn the shared knowledge pool, the projector to attend to the task-specific knowledge, and the head to focus on the dataset with specific annotation and data distribution.
In summary, our contributions are two folds: (1) we build HumanBench, a large-scale dataset for human-centric pretraining including diverse images and comprehensive evaluations. (2) To tackle the diversity of input images and annotations of various human-centric datasets, we pro-pose PATH, a projector-assisted hierarchical weight-sharing method for pretraining the general human-centric represen-tations. We achieve state-of-the-art results by PATH on 15 datasets throughout 6 downstream tasks (Fig. 1(c)), on-par results on 2 datasets, and slightly lower results on 2 datasets on HumanBench when using ViT-Base. Experi-ments with ViT-Large backbone show that our method can further achieve considerable gains over ViT-Base, achiev-ing another 2 new state-of-the-art results and showing the promising scalability of our method. We hope our work can shed light on future research on pretraining human-centric representations, such as unified structures.
Table 1. Statistics of Pretraining Datasets
Task
Number of datasets
Number of Images
Person ReID
Pose estimation
Human parsing
Pedestrian Attribute
Pedestrian Detection
In total 7 11 7 6 6 37 5,446,419 3,739,291 1,419,910 242,880 170,687 11,019,187 2.