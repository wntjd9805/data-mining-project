Abstract
Semantically coherent out-of-distribution (SCOOD) de-tection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The co-existence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrim-ination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Ex-tensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@95, respectively. Code is available at https://github.com/LuFan31/ET-OOD. 1.

Introduction
Deep learning models trained in close-set world often suffer from performance degradation in real-world scenar-ios due to the interference of out-of-distribution (OOD) inputs—unknown samples whose classes don’t overlap with those already seen during training [33]. Full-supervised models tend to make overconfident predictions on OOD inputs [20, 21] , which severely limits the application in high-risk fields such as autonomous driving [7] and medical analysis [22]. To this end, OOD detection [10,17,18,28,31] aims to obtain a reliable model to identify these unknown
∗Co-first Author. †Corresponding Author. ‡Work done during an internship at Ant Group.
)
% ( 5 9
@
R
P
F
Number of Accurately Assigned Labels
Figure 1. The effect of the number of accurately assigned labels. We gradually increase the ground-truth labels of unlabeled
ID samples during the baseline [11] training and report the
FPR@95% (lower value is better) results averaged by multiple tests. The trend shows that more accurately assigned labels enhance the OOD detection capability of the model. samples as abnormal ones and reject them at test time.
To learn the discrepancy between ID/OOD samples better, a rich line of OOD detection methods like OE [11] and MCD [31] utilizing extra OOD data during training has been developed. OE flattens the prediction probability of
OOD samples among all ID classes, and MCD maximizes the entropy discrepancy between ID and OOD samples outputted by two parallel classifiers. Although extra data greatly enhances the performance of models across tradi-tional OOD benchmarks, these benchmarks still face two problems: 1) to ensure the sampling effectiveness, ID samples in extra unlabeled datasets are manually removed to obtain the pure OOD set, which does not correspond to reality since ID samples are inevitably mixed into the unlabeled set and expensive to be purified. 2) the great improvement on them is most likely due to the overfitting on the training OOD samples [28], so the model paying atten-tion to the lower-level covariate shifts between datasets has difficulty generalizing to other OOD domains, especially when the scale of the dataset is not large.
To overcome these challenges, the SCOOD benchmarks
Figure 2. Motivation of the proposed method. When an extra unlabeled dataset is introduced into training, the resulting covariate shifts will easily confuse the minimum feature distance (e.g., Euclidean distance) from the correct class mean with other wrong classes. The left histogram reflects the minimum Euclidean distance distribution from the class mean for the ‘dog’ images in both CIFAR-10 (blue) and tiny-ImageNet (green). Unfortunately, although belonging to the same class, their feature distance distributions differ greatly, which limits the ability of cluster-based methods to mine unlabeled ID semantic knowledge in the SCOOD setting. In contrast, as shown in the right histogram, the distributions of the ‘dog’ images in the two different datasets maintain good consistency on the energy metric, facilitating a more stable semantic assignment process.
[28] are proposed to urge OOD detection models to focus on distinguishing semantic discrepancy between ID/OOD samples and avoid overfitting low-level covariate shifts on different data sources. K-means clustering is adopted to dynamically filter ID samples from the unlabeled set during training, enhancing the optimization for ID classi-fication [28]. Unfortunately, we statistically observe that this strategy can only assign correct semantic labels to less than 1% unlabeled ID samples at each epoch, which means a large number of samples are added to the ID classification training with wrong labels and undoubtedly limits the ability of the model to fully understand the semantic discrepancy between ID/OOD samples.
To investigate the impact of accurately assigned se-mantic labels on SCOOD tasks, we gradually increase the ground-truth labels of unlabeled ID samples used in training with the baseline in [28], and then observe the variation trend of model performance shown in Fig. 1. The model performance consistently improves as the correctly assigned labels increase, leading the key question of how to allocate as many ID-related samples in the unlabeled set correctly as possible. Our further analysis indicates that compared to the traditional feature distance (e.g., Euclidean distance), the class-specific energy score remains consistent even when the assigned cluster contains shifted unlabeled samples with an ID class. As shown in Fig. 2, due to the covariate shifts of the same class samples from another dataset with a black circle, the minimum distance with the correct class mean is easily confused with other clusters. In contrast, the energy-based uncertainty prior can effectively mitigate the corresponding interference owing to the intra-class interaction. (ET) mechanism estimates the energy transport cost to encourage lower-cost samples to be assigned to the same cluster and the higher-cost ones to be split uniformly among all clusters, leading to a semantic-consistent distribution.
Specifically, the energy score is obtained by performing the aggregation operation in logit space.
To further enhance the assignment efficiency of the aforementioned transport mechanism, an inter-cluster ex-tension strategy is proposed to narrow the margin of the same ID class while widening the margin between ID and
OOD samples. An enhanced global feature representation will be mapped to the lower-dimensional logit space, from where we can obtain a more discriminative prediction distri-bution for each cluster and class-specific energy score better reflecting the semantic information of samples. By utilizing both of them, the semantics among samples assigned to different clusters by ET will be more discriminative. During inference, we also choose the energy score with temperature scaling produced by the classifier as the detection metric, aligning with the transport optimization more closely.
Our contributions are summarized as follows: 1) We analyze the limitations of the clustering strategy for the
SCOOD task and further explore how to address the fun-damental challenge of OOD detection. 2) We propose a novel uncertainty-aware optimal transport scheme to fully utilize the semantic consistency hidden in the unlabeled set, resulting in the covariate-invariant assignment. 3) Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on SCOOD benchmarks. 2.