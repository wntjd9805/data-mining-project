Abstract
In this paper, we introduce SDM-UniPS, a groundbreak-ing Scalable, Detailed, Mask-free, and Universal Photomet-ric Stereo network. Our approach can recover astonishingly intricate surface normal maps, rivaling the quality of 3D scanners, even when images are captured under unknown, spatially-varying lighting conditions in uncontrolled envi-ronments. We have extended previous universal photometric stereo networks to extract spatial-light features, utilizing all available information in high-resolution input images and accounting for non-local interactions among surface points.
Moreover, we present a new synthetic training dataset that encompasses a diverse range of shapes, materials, and illu-mination scenarios found in real-world scenes. Through ex-tensive evaluation, we demonstrate that our method not only surpasses calibrated, lighting-specific techniques on pub-lic benchmarks, but also excels with a significantly smaller number of input images even without object masks. 1.

Introduction
Photometric stereo [52] aims to deduce the surface nor-mal map of a scene by analyzing images captured from a fixed perspective under diverse lighting conditions. Until very recently, all photometric stereo methods assumed their specific lighting conditions, which led to limitations in their applicability. For instance, methods that assumed direc-tional lighting conditions (e.g., [20,24,25]) were unsuitable under natural illumination, and vice versa (e.g., [15, 38]).
To overcome this limitation, the “universal” photomet-ric stereo method (UniPS) [22] has been introduced, de-signed to operate under unknown and arbitrary lighting con-ditions. In contrast to prior uncalibrated photometric stereo methods [7,9,27], which assumed specific physically-based lighting models, this method encodes a non-physical fea-ture at each pixel for representing spatially-varying illumi-nation, which is served as a substitute for physical light-ing parameters within the calibrated photometric stereo net-work [21]. This method has taken the first step towards dealing with unknown, spatially-varying illumination that none of the existing methods could handle.
However, the surface normal map recovered by UniPS, while not en-tirely inaccurate, appears blurry and lacks fine detail (see the top-right corner of Fig. 1). Upon investigation, we pin-pointed three fundamental factors contributing to the subpar reconstruction performance. Firstly, extracting illumination
Supported by JSPS KAKENHI Grant Number 22K17919.
features (i.e., global lighting contexts) from downsampled images caused a loss of information at higher input resolu-tions and produced blurry artifacts. Secondly, UniPS em-ploys a pixel-wise calibrated photometric stereo network to predict surface normals using illumination features, which leads to imprecise overall shape recovery. Although pixel-wise methods [20,21,25] offer advantages in capturing finer details compared to image-wise methods [8, 29, 49], they suffer from an inability to incorporate global information.
Lastly, the third issue lies in the limited variety of shape, material, and illumination conditions present in the training data, which hampers its capacity to adapt to a diverse range of real-world situations. This limitation primarily stems from the fact that current datasets (i.e., PS-Wild [22]) do not include renderings under light sources with high-frequency components focused on specific incident angles, such as point or directional sources. Consequently, the method ex-hibits considerable performance degradation when exposed to directional lighting setups like DiLiGenT [46], as will be demonstrated later in this paper.
In this paper, we present a groundbreaking photometric stereo network, the Scalable, Detailed, and Mask-Free Uni-versal Photometric Stereo Network (SDM-UniPS), which recovers normal maps with remarkable accuracy from im-ages captured under extremely uncontrolled lighting condi-tions. As shown in Fig. 1, SDM-UniPS is scalable, enabling the generation of normal maps from images with substan-tially higher resolution (e.g., 2048x2048) than the training data (e.g., 512x512); it is detailed, providing more accu-rate normal maps on DiLiGenT [46] with a limited number of input images than most existing orthographic photomet-ric stereo techniques, including calibrated methods, and in some cases, surpassing 3D scanners in detail; and it is mask-free, allowing for application even when masks are absent, unlike many conventional methods. Our technical novelties include: 1. The development of a scale-invariant spatial-light fea-ture encoder that efficiently extracts illumination fea-tures while utilizing all input data and maintaining scalability with respect to input image size. Our en-coder, based on the "split-and-merge" strategy, accom-modates varying input image sizes during training and testing without sacrificing performance. 2. The development of a surface normal decoder utilizing our novel pixel-sampling transformer. By randomly sampling pixels of fixed size, we simultaneously pre-dict surface normals through non-local interactions among sampled pixels using Transformers [51], effec-tively accounting for global information. 3. The creation of a new synthetic training dataset, com-prising multiple objects with diverse textures within a scene, rendered under significantly varied lighting con-ditions that include both low and high-frequency illu-minations.
We believe that the most significant contribution is the ex-traordinary time savings from data acquisition to normal map recovery compared to existing photometric stereo al-gorithms requiring meticulous lighting control, even in the uncalibrated setup. This progress allows photometric stereo to be executed at home, literally “in the wild” setup. 2.