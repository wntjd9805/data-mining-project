Abstract
Gloss
HOUSE3
Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create mean-ingful representations of the text and poses while consider-ing their spatial and temporal information. We use weak su-pervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Ad-ditionally, we offer a new distance measurement that con-siders missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correct-ness using AUTSL, a large-scale Sign language dataset, show that it measures the distance between pose sequences more accurately than existing measurements, and use it to assess the quality of our generated pose sequences. Code for the data pre-processing, the model, and the distance measurement is publicly released for future research. 1.

Introduction
Sign languages are an important communicative tool within the deaf and hard-of-hearing (DHH) community and a central property of Deaf culture. According to the World
Health Organization, there are more than 70 million deaf people worldwide [56], who collectively use more than 300 different Sign languages [29]. Using the visual-gestural modality to convey meaning, Sign languages are considered natural languages [40], with their own grammar and lexi-cons. They are not universal and are mostly independent of spoken languages. For example, American Sign Lan-guage (ASL)—used predominantly in the United States— and British Sign Language (BSL)—used predominantly in
HamNoSys
SignWriting
Figure 1. German Sign Language sign for “Haus”.
Gloss is a unique semantic identifier; HamNoSys and SignWrit-ing describe the phonology of a sign: Two flat hands with fingers closed, rotated towards each other, touching, then symmetrically moving diagonally downwards. the United Kingdom—are entirely different, despite English being the predominant spoken language in both. As such, the translation task between each signed and spoken lan-guage pair is different and requires different data. Build-ing a robust system that translates spoken languages into
Sign languages and vice versa is fundamental to alleviate communication gaps between the hearing-impaired and the hearing communities.
While translation research from Sign languages into spoken languages has rapidly advanced in recent years
[2, 5, 6, 30, 36, 37], translating spoken languages into Sign languages, also known as Sign Language Production (SLP), remains a challenge [41, 42, 47, 48]. This is partially due to a misconception that deaf people are comfortable reading spoken language and do not require translation into Sign language. However, there is no guarantee that someone whose first language is, for example, BSL, exhibits high literacy in written English. SLP is usually done through an intermediate notation system such as a semantic nota-tion system, e.g. gloss (Sec. 2.1), or a lexical notation sys-tem, e.g. HamNoSys, SignWriting (Sec. 2.2). The spoken language text is translated into the intermediate notation, which is then translated into the relevant signs. The signs can either be animated avatars or pose sequences later con-verted into videos. Previous work has shown progress in translating spoken language text to Sign language lexical notations, namely HamNoSys [54] and SignWriting [21], and in converting pose sequences into videos [8, 43, 55].
There has been some work on animating HamNoSys into avatars [3, 12, 13, 58], with unsatisfactory results (Sec. 3.1),
but no work on the task of animating HamNoSys into pose sequences. Hence, in this work, we focus on animating
HamNoSys into signed pose sequences, thus facilitating the task of SLP with a generic solution for all Sign lan-guages. To do this, we collect and combine data from mul-tiple HamNoSys-to-video datasets [25, 26, 32], extract pose keypoints from the videos using a pose estimation model, and process these further as detailed in Sec. 4.1. We use the pose features as weak labels to train a model that gets
HamNoSys text and a single pose frame as inputs and grad-ually generates the desired pose sequence from them. De-spite the pose features being inaccurate and incomplete, our model still learns to produce the correct motions. Addition-ally, we offer a new distance measurement that considers missing keypoints, to measure the distance between pose se-quences using DTW-MJE [20]. We validate its correctness using AUTSL, a large-scale Sign language dataset [44], and show that it measures pose sequences distance more accu-rately than currently used measurements. Overall, our main contributions are: 1. We propose the first method for animating HamNoSys into pose sequences. 2. We offer a new pose sequences distance measurement, validated on a large annotated dataset. 3. We combine existing datasets, converting them to one enhanced dataset with processed pose features. 2.