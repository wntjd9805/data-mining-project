Abstract
Understanding the hand-object interactions from an egocentric video has received a great attention recently. So far, most approaches are based on the convolutional neural network (CNN) features combined with the temporal encoding via the long short-term memory (LSTM) or graph convolution network (GCN) to provide the unified understanding of two hands, an object and their interactions. In this paper, we propose the Transformer-based unified framework that provides better understanding of two hands manipulating objects. In our framework, we insert the whole image depicting two hands, an object and their interactions as input and jointly estimate 3 information from each frame: poses of two hands, pose of an object and object types. Afterwards, the action class defined by the hand-object interactions is predicted from the entire video based on the estimated information combined with the contact map that encodes the interaction between two hands and an object. Experiments are conducted on H2O and FPHA benchmark datasets and we demonstrated the superiority of our method achieving the state-of-the-art accuracy. Ablative studies further demonstrate the effectiveness of each proposed module. 1.

Introduction
Estimating poses and actions of an egocentric video involving two hands and an object is an important factor of various appli-cations such as augmented reality (AR), virtual reality (VR) and human computer interaction (HCI). Previously, there has been much progress in the hand pose estimation [3–5,11,12,18,31,33, 38,43,53,61] and in the object 6D pose estimation [10,26,28,36, 51, 57, 58] separately from each other. Recently, there has been a surge in demand for understanding hand-object interactions, leading to the emergence of methods for joint pose estimation of hands and objects [22,23,39]. However, most methods focus on the separate problem either for the pose estimation [9,13,20,39] or for the interaction recognition [6, 42, 48]. Furthermore, most approaches developed the pose estimation method based on the already cropped tight bounding boxes of hands and objects which are not realistic. Therefore, the pose estimation accuracy
Figure 1. Example results of pose estimation and interaction recognition for two hands manipulating objects. Our method first estimates hand poses, object poses and object types. Then, interaction class is estimated using estimated information combined with contact maps. (Row 1) example input video v for open chips and grap cappuccino; (Row 2) contact maps for left hand mLeft, object mO, and right hand mRight; (Row 3) estimated 3D poses of hands h, a 3D object pose o and the estimated interaction class a. is frequently affected by the performance of the detector.
To tackle the issue, Tekin et al. [50] proposed an unified framework that estimates the 3D hand pose, the object 6D pose and their action classes. They developed the pose estimator extending the architecture of [45] towards 3D space and recognize actions using estimated hand and object poses. The long short-term memory (LSTM) [25]-based architecture is further used to map the information towards the action classes.
Kwon et al. [32] further extended the framework towards involving two hands rather than one hand: They estimated 3D poses of two hands, 6D pose of an object and their action classes.
The proposed method involves the graph convolutional network (GCN) to model the hand-object interaction considering the geometric relation between hand and object. In both works, estimated hand and object poses (i.e. skeletons) were used as the cue to the interaction recognition.
In this paper, we propose the Transformer-based unified framework (H2OTR) to estimate poses of two hands, object pose, object types and interaction classes between hands and ob-ject. We construct the Transformer-based architecture similarly to [7, 60] and it is able to predict the poses from each frame without hand/object detectors or any additional post-processing such as non-maximal suppression (NMS). It also estimates hand-object interaction classes from the entire videos. We additionally exploit the contact map between hand and object meshes by recovering hand meshes from hand poses via inverse kinematics.
We demonstrated that the contact map expresses the explicit relational information between hands and object and is used as the crucial cue for the hand-object interaction recognition task. We summarize our contributions in this paper as follows:
• We propose the Transformer-based unified framework for estimating poses of two hands, object poses, object types and hand-object interaction classes at a single inference step.
• We introduce a novel interaction recognition method which utilizes a contact map. To the best of our knowledge, this is the first work to exploit the contact map as a cue for interaction recognition.
• We achieve the state-of-the-art performance in pose estimation and interaction recognition tasks using
H2O [32] and FPHA [18] datasets. 2.