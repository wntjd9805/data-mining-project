Abstract
A key goal for the advancement of AI is to develop technologies that serve the needs not just of one group but of all communities regardless of their geographical re-gion. In fact, a signiﬁcant proportion of knowledge is lo-cally shared by people from certain regions but may not apply equally in other regions because of cultural dif-If a model is unaware of regional character-ferences. istics, it may lead to performance disparity across re-gions and result in bias against underrepresented groups.
We propose GIVL, a Geographically Inclusive Vision-and-Language Pre-trained model. There are two attributes of geo-diverse visual concepts which can help to learn geo-diverse knowledge: 1) concepts under similar categories have unique knowledge and visual characteristics, 2) con-cepts with similar visual features may fall in completely different categories. Motivated by the attributes, we de-sign new pre-training objectives Image-Knowledge Match-ing (IKM) and Image Edit Checking (IEC) to pre-train
GIVL. Compared with similar-size models pre-trained with similar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced performance on geo-diverse V&L tasks.
Code and data are released at https://github.com/
WadeYin9712/GIVL. 1.

Introduction
Vision-Language Pre-trained Models (VLPs) [9, 23, 24, 29, 53] have achieved remarkable performance on Vision-Language (V&L) tasks including visual question answer-ing [11, 12, 15], image-text retrieval [22], and image cap-tioning [19, 27]. Pre-trained with large-scale corpora of image-text pairs, e.g. COCO [27], OpenImages [21]. VLPs are capable of learning multi-modal representations and can be effectively ﬁne-tuned on downstream V&L tasks.
While VLPs can solve a broad range of V&L tasks, to deploy VLPs in real-world applications, it is essential to consider the geographical inclusivity1 of VLPs. Because of geographic differences, images from different regions em-body a large amount of knowledge that is locally shared but cannot be applied in other regions, i.e. geographically di-verse. For example, in Figure 1, the festivals in different 1We use regions as a proxy to estimate inclusivity of V&L models.
People in the same regions may have different cultures and traditions.
regions look different.
Ideally, a geographically inclusive VLP should be capa-ble of achieving comparable performance over all the im-ages, regardless of their origins. However, current VLPs does not perform equally well on data from different re-gions. For example, prior works [28, 49] show that on geo-diverse V&L tasks, there is nearly a 20% performance dis-crepancy between Western and East Asian images when current VLPs are applied. To combat such geographical bias, we aim to design methods to make VLPs achieve more balanced performance across regions.
One solution to mitigating bias is to obtain diverse task-speciﬁc annotations for each region and ﬁne-tune VLPs on the new annotations. However, according to [17], most
Amazon MTurk annotators are from US and India, and may be unfamiliar with the cultures of other regions. Thus, it is unrealistic to obtain large-scale geo-diverse annotations even in such a popular crowdsourcing platform.
Pre-training a uniﬁed VLP with large-scale unannotated geo-diverse images and corresponding knowledge could make the VLP a foundation to provide more generalizable representations and help to transfer on comprehending im-ages from various regions easier. In this paper, we propose
GIVL, a Geographically Inclusive Vision-and-Language
Pre-trained model. We focus on how to encourage GIVL to better learn geo-diverse knowledge on images from different regions during its pre-training stage.
We observe two attributes of geo-diverse visual concepts that can contribute to learning geo-diverse knowledge:
A1: Concepts under similar categories have unique knowledge and visual characteristics. For example, tra-ditional Western and Chinese festivals, like Christmas and
Chinese New Year in Figure 1, are held with different rit-uals and their decoration style differs as well. It is neces-sary for GIVL to learn the difference between their corre-sponding knowledge and precisely distinguish these visual concepts. On the other hand, Christmas and Chinese New
Year are both festivals. Learning the commonalities of vi-sual concepts (e.g., both images in Figure 1 belong to the same category “festival”) would help model connect West-ern and non-Western concepts and contribute to more effec-tive transfer on geo-diverse images.
A2: Concepts with similar visual features may lie in completely different categories. In Figure 2, Chinese pa-per cuttings share visual features (e.g., color, shape) with red frisbee. Similarly. sugar cane and ﬂute share visual fea-tures. However, these concepts are not related to each other.
Since geo-diverse images cover a broader range of visual concepts, differentiating visually similar concepts given vi-sual contexts is also essential.
To this end, besides common objectives Masked Lan-guage Modeling (MLM) and Image-Text Matching (ITM) for pre-training VLPs, we propose two additional pre-(a) (b)
Figure 2. Example of Chinese paper cuttings and red frisbee (left), sugar cane and ﬂute (right). Different concepts may be visually similar, but they may have completely different functionalities. training objectives, Image-Knowledge Matching (IKM) and Image Edit Checking (IEC).
IKM is used to learn the alignment between images and corresponding textual knowledge in Wikipedia. It requires GIVL to not only judge if the input textual knowledge matches input images, but also identify whether the visual concepts described in input knowledge falls into similar categories of the concepts in in-put images. This encourages GIVL to learn corresponding relationship between knowledge and images as well as rec-ognize similarity among geo-diverse visual concepts. IEC is proposed to identify whether a visual concept in input image is replaced by another concept that is visually similar but lies in an irrelevant category (see Fig.3 for an example).
It enables GIVL to capture nuances between visually simi-lar concepts after the replacement given visual contexts.
Our contributions and empirical results are as follows:
• By considering the attributes of geo-diverse visual con-cepts, we propose two novel V&L pre-training objec-tives Image-Knowledge Matching (IKM) and Image
Edit Checking (IEC) that can greatly improve the geo-graphical inclusivity of VLPs.
• Compared with similar-size VLPs pre-trained with similar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced performance over dif-ferent regions on geo-diverse V&L tasks including
MaRVL [28], GD-VCR [49] and WIT Image-Text Re-trieval [38]. For geo-diverse zero-shot image classi-ﬁcation on Dollar Street dataset2, GIVL outperforms
VinVL [53] 26%. 2.