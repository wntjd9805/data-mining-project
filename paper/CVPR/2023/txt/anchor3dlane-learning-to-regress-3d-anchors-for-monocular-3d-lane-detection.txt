Abstract
Monocular 3D lane detection is a challenging task due to its lack of depth information. A popular solution is to first transform the front-viewed (FV) images or features into the bird-eye-view (BEV) space with inverse perspective map-ping (IPM) and detect lanes from BEV features. However, the reliance of IPM on flat ground assumption and loss of context information make it inaccurate to restore 3D in-formation from BEV representations. An attempt has been made to get rid of BEV and predict 3D lanes from FV repre-sentations directly, while it still underperforms other BEV-based methods given its lack of structured representation for 3D lanes. In this paper, we define 3D lane anchors in the 3D space and propose a BEV-free method named An-chor3DLane to predict 3D lanes directly from FV represen-tations. 3D lane anchors are projected to the FV features to extract their features which contain both good structural and context information to make accurate predictions. In addition, we also develop a global optimization method that makes use of the equal-width property between lanes to reduce the lateral error of predictions. Extensive experi-ments on three popular 3D lane detection benchmarks show that our Anchor3DLane outperforms previous BEV-based methods and achieves state-of-the-art performances. The code is available at: https://github.com/tusen-ai/Anchor3DLane. 1.

Introduction
Monocular 3D lane detection, which aims at estimating the 3D coordinates of lane lines from a frontal-viewed im-age, is one of the essential modules in autonomous driv-*Work done while at TuSimple
Figure 1. (a) BEV-based methods, which perform lane detection in the warped BEV images or features. (b) Non-BEV method, which projects 2D lane predictions back to 3D space with esti-mated depth. (c) Our Anchor3DLane projects 3D anchors into FV features to sample features for 3D prediction directly. ing systems. Accurate and robust perception of 3D lanes is not only critical for stable lane keeping, but also serves as an important component for downstream tasks like high-definition map construction [21, 25], and trajectory plan-ning [1, 40]. However, due to the lack of depth informa-tion, estimating lanes in 3D space directly from 2D image domain still remains very challenging.
A straightforward way to tackle the above challenges is to detect lanes from the bird-eye-viewed (BEV) space.
As illustrated in Figure 1(a), a common practice of BEV-based methods [5, 7, 8, 20] is to warp images or features from frontal-viewed (FV) space to BEV with inverse per-spective mapping (IPM), thereby transforming the 3D lane detection task into 2D lane detection task in BEV. To project the detected BEV lanes back into 3D space, coordinates of the lane points are then combined with their corresponding height values which are estimated by a height estimation head. Though proven effective, their limitations are still ob-vious: (1) IPM relies on a strict assumption of flat ground, which does not hold true for uphill or downhill cases. (2)
Since IPM warps the images on the basis of ground, some useful height information as well as the context information above the road surface are lost inevitably. For example, ob-jects like vehicles on the road are severely distorted after warping. Therefore, information lost brought by IPM hin-ders the accurate restoration of 3D information from BEV representations.
Given the above limitations of BEV, some works tried to predict 3D lanes from FV directly. As illustrated in Fig-ure 1(b), SALAD [36] decomposes 3D lane detection task into 2D lane segmentation and dense depth estimation. The segmented 2D lanes are projected into 3D space with cam-era intrinsic parameters and the estimated depth informa-tion. Even though getting rid of the flat ground assumption,
SALAD lacks structured representations of 3D lanes. As a result, it is unnatural to extend it to more complex 3D lane settings like multi-view or multi-frame. Moreover, their performance is still far behind the state-of-the-art methods due to the unstructured representation.
In this paper, we propose a novel BEV-free method named Anchor3DLane to predict 3D lanes directly from
FV concisely and effectively. As shown in Figure 1(c), our Anchor3DLane defines lane anchors as rays in the 3D space with given pitches and yaws. Afterward, we first project them to corresponding 2D points in FV space us-ing camera parameters, and then obtain their features by bilinear sampling. A simple classification head and a re-gression head are adopted to generate classification proba-bilities and 3D offsets from anchors respectively to make final predictions. Unlike the information loss in IPM, sam-pling from original FV features retains richer context infor-mation around lanes, which helps estimate 3D information more accurately. Moreover, our 3D lane anchors can be it-eratively refined to sample more accurate features to better capture complex variations of 3D lanes. Furthermore, An-chor3DLane can be easily extended to the multi-frame set-ting by projecting 3D anchors to adjacent frames with the assistance of camera poses between frames, which further improves performances over single-frame prediction.
In addition, we also utilize global constraints to refine the challenging distant parts due to low resolution. The moti-vation is based on an intuitive insight that lanes in the same image appear to be parallel in most cases except for the fork lanes, i.e., distances between different point pairs on each lane pair are nearly consistent. By applying a global equal-width optimization to non-fork lane pairs, we adjust 3D lane predictions to make the width of lane pairs consistent from close to far. The lateral error of distant parts of lane lines can be further reduced through the above adjustment.
Our contributions are summarized as follows:
• We propose a novel Anchor3DLane framework that di-rectly defines anchors in 3D space and regresses 3D lanes directly from FV without introducing BEV. An extension to the multi-frame setting of Anchor3DLane is also proposed to leverage the well-aligned temporal information for further performance improvement.
• We develop a global optimization method to utilize the equal-width properties of lanes for refinement.
• Without bells and whistles, our Anchor3DLane out-performs previous BEV-based methods and achieves state-of-the-art performances on three popular 3D lane detection benchmarks. 2.