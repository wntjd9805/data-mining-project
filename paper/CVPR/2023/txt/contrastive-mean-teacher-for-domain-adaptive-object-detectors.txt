Abstract
Object detectors often suffer from the domain gap be-tween training (source domain) and real-world applications (target domain). Mean-teacher self-training is a powerful paradigm in unsupervised domain adaptation for object de-tection, but it struggles with low-quality pseudo-labels. In this work, we identify the intriguing alignment and syn-ergy between mean-teacher self-training and contrastive learning. Motivated by this, we propose Contrastive Mean
Teacher (CMT) – a unified, general-purpose framework with the two paradigms naturally integrated to maximize beneficial learning signals. Instead of using pseudo-labels solely for final predictions, our strategy extracts object-level features using pseudo-labels and optimizes them via contrastive learning, without requiring labels in the target domain. When combined with recent mean-teacher self-training methods, CMT leads to new state-of-the-art target-domain performance: 51.9% mAP on Foggy Cityscapes, outperforming the previously best by 2.1% mAP. Notably,
CMT can stabilize performance and provide more signifi-cant gains as pseudo-label noise increases. 1.

Introduction
The domain gap between curated datasets (source do-main) and real-world applications (target domain, e.g., on edge devices or robotic systems) often leads to deteriorated performance for object detectors. Meanwhile, accurate la-bels provided by humans are costly or even unavailable in practice. Aiming at maximizing performance in the target domain while minimizing human supervision, unsupervised domain adaptation mitigates the domain gap via adversarial training [7, 30], domain randomization [23], image transla-tion [4, 20, 21], etc.
In contrast to the aforementioned techniques that explic-itly model the domain gap, state-of-the-art domain adaptive object detectors [5, 27] follow a mean-teacher self-training paradigm [2, 9], which explores a teacher-student mutual
Code available at https://github.com/Shengcao-Cao/CMT
Figure 1. Overview of Contrastive Mean Teacher. Top: Mean-teacher self-training [2, 5, 9, 27] for unsupervised domain adap-tation (left) and Momentum Contrast [16] for unsupervised rep-resentation learning (right) share the same underlying structure, and thus can be naturally integrated into our unified framework,
Contrastive Mean Teacher. Bottom: Contrastive Mean Teacher benefits unsupervised domain adaptation even when pseudo-labels are noisy. In this example, the teacher detector incorrectly detects the truck as a train and the bounding box is slightly off. Rein-forcing this wrong pseudo-label in the student harms the perfor-mance. Contrarily, our proposed object-level contrastive learn-ing still finds meaningful learning signals from it, by enforc-ing feature-level similarities between the same objects and dis-similarities between different ones. learning strategy to gradually adapt the object detector for cross-domain detection. As illustrated in Figure 1-top, the teacher generates pseudo-labels from detected objects in the target domain, and the pseudo-labels are then used to su-In return, the teacher’s pervise the student’s predictions. weights are updated as the exponential moving average (EMA) of the student’s weights.
Outside of unsupervised domain adaptation, contrastive learning [3, 6, 14, 16] has served as an effective approach to learning from unlabeled data. Contrastive learning opti-mizes feature representations based on the similarities be-tween instances in a fully self-supervised manner. Intrigu-ingly, as shown in Figure 1-top, there in fact exist strong alignment and synergy between the Momentum Contrast paradigm [16] from contrastive learning and the mean-teacher self-training paradigm [2, 9] from unsupervised do-main adaptation: The momentum encoder (teacher detec-tor) provides stable learning targets for the online encoder (student detector), and in return the former is smoothly up-dated by the latter’s EMA. Inspired by this observation, we propose Contrastive Mean Teacher (CMT) – a unified framework with the two paradigms naturally integrated. We find that their benefits can compound, especially with con-trastive learning facilitating the feature adaptation towards the target domain from the following aspects.
First, mean-teacher self-training suffers from the poor quality of pseudo-labels, but contrastive learning does not rely on accurate labels. Figure 1-bottom shows an il-the teacher de-lustrative example: On the one hand, tector produces pseudo-labels in the mean-teacher self-training framework, but they can never be perfect (other-wise, domain adaptation would not be needed). The stu-dent is trained to fit its detection results towards these noisy pseudo-labels. Consequently, mis-predictions in the pseudo-labels become harmful learning signals and limit the target-domain student performance. On the other hand, contrastive learning does not require accurate labels for learning. Either separating individual instances [6, 16] or separating instance clusters [3] (which do not necessarily coincide with the actual classes) can produce powerful rep-resentations. Therefore, CMT effectively learns to adapt its features in the target domain, even with noisy pseudo-labels.
Second, by introducing an object-level contrastive learn-ing strategy, we learn more fine-grained, localized repre-sentations that are crucial for object detection. Tradition-ally, contrastive learning treats data samples as monolithic instances but ignores the complex composition of objects in natural scenes. This is problematic as a natural image consists of multiple heterogeneous objects, so learning one homogeneous feature may not suffice for object detection.
Hence, some recent contrastive learning approaches learn representations at the pixel [35], region [1], or object [38] levels, for object detection yet without considering the chal-lenging scenario of domain adaptation. Different from such prior work, in CMT we propose object-level contrastive learning to precisely adapt localized features to the target
In addition, we exploit predicted classes from domain. noisy pseudo-labels, and further augment our object-level contrastive learning with multi-scale features, to maximize the beneficial learning signals.
Third, CMT is a general-purpose framework and can be readily combined with existing work in mean-teacher self-training. The object-level contrastive loss acts as a drop-in enhancement for feature learning, and does not change the original training pipelines. Combined with the most recent methods (e.g., Adaptive Teacher [27], Probabilistic
Teacher [5]), we achieve new state-of-the-art performance in unsupervised domain adaptation for object detection.
To conclude, our contributions include:
• We identify the intrinsic alignment and synergy between contrastive learning and mean-teacher self-training, and propose an integrated unsupervised domain adaptation framework, Contrastive Mean Teacher (CMT).
• We develop a general-purpose object-level contrastive learning strategy to enhance the representation learning in unsupervised domain adaptation for object detection.
Notably, the benefit of our strategy becomes more pro-nounced with increased pseudo-label noise (see Figure 3).
• We show that our proposed framework can be combined with several existing mean-teacher self-training methods without effort, and the combination achieves state-of-the-art performance on multiple benchmarks, e.g., improv-ing the adaptation performance on Cityscapes to Foggy
Cityscapes from 49.8% mAP to 51.9% mAP. 2.