Abstract
Typical layout-to-image synthesis (LIS) models gener-ate images for a closed set of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work, we explore the freestyle capability of the model, i.e., how far can it generate unseen semantics (e.g., classes, attributes, and styles) onto a given layout, and call the task Freestyle
LIS (FLIS). Thanks to the development of large-scale pre-trained language-image models, a number of discrimina-tive models (e.g., image classification and object detection) trained on limited base classes are empowered with the
*Corresponding author. Part of this work was done when Han Xue served as a visiting Ph.D. student at Singapore Management University. ability of unseen class prediction. Inspired by this, we opt to leverage large-scale pre-trained text-to-image diffusion models to achieve the generation of unseen semantics. The key challenge of FLIS is how to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-learned knowledge, e.g., the model never sees “a unicorn sitting on a bench” during its pre-training.
To this end, we introduce a new module called Rectified
Cross-Attention (RCA) that can be conveniently plugged in the diffusion model to integrate semantic masks. This
“plug-in” is applied in each cross-attention layer of the model to rectify the attention maps between image and text tokens. The key idea of RCA is to enforce each text token to act on the pixels in a specified region, allowing us to freely
put a wide variety of semantics from pre-trained knowledge (which is general) onto the given layout (which is specific).
Extensive experiments show that the proposed diffusion net-work produces realistic and freestyle layout-to-image gen-eration results with diverse text inputs, which has a high po-tential to spawn a bunch of interesting applications. Code is available at https://github.com/essunny310/FreestyleNet. 1.

Introduction
Layout-to-image synthesis (LIS) is one of the prevailing topics in the research of conditional image generation. It aims to generate complex scenes where a user requires fine controls over the objects appearing in a scene. There are different types of layouts including bboxes+classes [13], se-mantic masks+classes [29, 37], key points+attributes [33], skeletons [26], to name a few. In this work, we focus on us-ing semantic masks to control object shapes and positions, and using texts to control classes, attributes, and styles. We aim to offer the user the most fine-grained controls on out-put images. For example, in Figure 1 (2nd column), the train and the cat are generated onto the right locations de-fined by the input semantic layouts, and they should also be aligned well with their surroundings.
However, most existing LIS models [24, 29, 37, 49, 52] are trained on a specific dataset from scratch. Therefore, the generated images are limited to a closed set of seman-tic classes, e.g., 182 objects on COCO-Stuff [3]. While in this paper, we explore the freestyle capability of the LIS models for the generation of unseen classes (including their attributes and styles) onto a given layout. For instance, in
Figure 1 (5th column), our model stretches and squeezes a
“warehouse” into a mask of “train”, and the “warehouse” is never seen in the training dataset of LIS. In addition, it is also able to introduce novel attributes and styles into the synthesized images (in the 3rd and 4th columns of Figure 1).
The freestyle property aims at breaking the in-distribution limit of LIS, and it has a high potential to enlarge the appli-cability of LIS results to out-of-distribution domains such as data augmentation in open-set or long-tailed semantic seg-mentation tasks.
To overcome the in-distribution limit, large-scale pre-trained language-image models (e.g., CLIP [31]) have shown the ability of learning general knowledge on an enor-mous amount of image-text pairs. The learned knowledge enables a multitude of discriminative methods trained with limited base classes to predict unseen classes. For instance, the trained models in [31,51, 60] based on pre-trained CLIP are able to do zero-shot or few-shot classification, which is achieved by finding the most relevant text prompt (e.g., a photo of a cat) given an image. However, it is non-trivial to apply similar ideas to generative models. In classifica-tion, simple texts (e.g., a photo of a cat) can be conveniently mapped to labels (e.g., cat). It is, however, not intuitive to achieve this mapping from compositional texts (e.g., a train is running on the railroad with grass on both sides) to syn-thesized images. Thanks to the development of pre-training in image generation tasks, we opt to leverage large-scale pre-trained language-image models to the downstream gen-eration tasks.
To this end, we propose to leverage the large-scale pre-trained text-to-image diffusion model to achieve the LIS of unseen semantics. These diffusion models are powerful in generating images of high quality that are in line with the input texts. This can provide us with a generative prior with a wide range of semantics. However, it is challeng-ing to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-trained knowledge, e.g., the model never sees “a unicorn sitting on a bench” during its pre-training. Textual semantics should be correctly arranged in space without impairing their expres-siveness. A seminal attempt, PITI [48], learns an encoder to map the layout to the textual space of a pre-trained text-to-image model. Although it demonstrates improved quality of the generated images, PITI has two main drawbacks. First, it abandons the text encoder of the pre-trained model, losing the ability to freely control the generation results using dif-ferent texts. Second, its introduced layout encoder implic-itly matches the space of the pre-trained text encoder and hence incurs inferior spatial alignment with input layouts.
In this paper, we propose a simple yet effective frame-work for the suggested freestyle layout-to-image synthesis (FLIS) task. We introduce a new module called Rectified
Cross-Attention (RCA) and plug it into a pre-trained text-to-image diffusion model. This “plug-in” is applied in each cross-attention layer of the diffusion model to integrate the input layout into the generation process. In particular, to ensure the semantics (described by text) appear in a region (specified by layout), we find that image tokens in the re-gion should primarily aggregate information from the cor-responding text tokens. In light of this, the key idea of RCA is to utilize the layout to rectify the attention maps com-puted between image and text tokens, allowing us to put desired semantics from the text onto a layout. In addition,
RCA does not introduce any additional parameters into the pre-trained model, making our framework an effective so-lution to freestyle layout-to-image synthesis.
As shown in Figure 1, the proposed diffusion network (FreestyleNet) allows the freestyle synthesis of high-fidelity images with novel semantics, including but not limited to: synthesizing unseen objects, binding new attributes to an object, and rendering the image with various styles. Our main contributions can be summarized as follows:
• We propose a novel LIS task called FLIS. In this task, we exploit large-scale pre-trained text-to-image diffu-sion models with layout and text as conditions.
• We introduce a parameter-free RCA module to plug in the diffusion model, allowing us to generate images from a specified layout effectively while taking full ad-vantage of the model’s generative priors.
• Extensive experiments demonstrate the capability of our approach to translate a layout into high-fidelity im-ages with a wide range of novel semantics, which is beyond the reach of existing LIS models. 2.