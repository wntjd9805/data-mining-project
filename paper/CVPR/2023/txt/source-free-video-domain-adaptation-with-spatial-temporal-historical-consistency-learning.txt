Abstract
Source-free domain adaptation (SFDA) is an emerging research topic that studies how to adapt a pretrained source model using unlabeled target data. It is derived from unsu-pervised domain adaptation but has the advantage of not requiring labeled source data to learn adaptive models.
This makes it particularly useful in real-world applications where access to source data is restricted. While there has been some SFDA work for images, little attention has been paid to videos. Naively extending image-based methods to videos without considering the unique properties of videos often leads to unsatisfactory results. In this paper, we pro-pose a simple and highly flexible method for Source-Free
Video Domain Adaptation (SFVDA), which extensively ex-ploits consistency learning for videos from spatial, tempo-ral, and historical perspectives. Our method is based on the assumption that videos of the same action category are drawn from the same low-dimensional space, regardless of the spatio-temporal variations in the high-dimensional space that cause domain shifts. To overcome domain shifts, we simulate spatio-temporal variations by applying spatial and temporal augmentations on target videos and encour-age the model to make consistent predictions from a video and its augmented versions. Due to the simple design, our method can be applied to various SFVDA settings, and ex-periments show that our method achieves state-of-the-art performance for all the settings. 1.

Introduction
Action recognition is a crucial task in video understand-ing and has been receiving tremendous attention from the vision community. In recent years, it has made significant progress, primarily due to the development of deep learning techniques [11, 43, 45] and the establishment of large-scale annotated datasets [2, 13, 42]. However, it is acknowledged that an action recognition model trained with annotated data drawn from one distribution typically experiences a per-formance drop when tested on out-of-distribution data [4].
This is the so-called domain shift problem.
To tackle this problem, Unsupervised Video Domain
Adaptation (UVDA) has been proposed. The goal is to learn an adaptive model using labeled video data from one domain (source) and unlabeled video data from another do-main (target). Typical UVDA methods use videos from both domains as input and train a model by minimizing the clas-sification risk on labeled source videos and explicitly align-ing videos from both domains in a class-agnostic fashion.
Although most image-based domain alignment techniques can be applied to video domain alignment, such as adver-sarial learning [22, 37, 44], methods that align domains by considering the richer temporal information in videos have shown superior performance [6, 33, 36].
While UVDA methods help alleviate the domain shift problem, their assumption that labeled source videos are available for domain alignment can be problematic in real-world applications where access to source videos is re-stricted due to privacy or commercial reasons [24, 50]. This motivates a new research topic, Source-Free Video Domain
Adaptation (SFVDA) [50], which aims to learn an adap-tive action recognition model using unlabeled target videos and a source model pre-trained with labeled source videos.
SFVDA is similar to UVDA in learning an adaptive model using labeled source and unlabeled target videos but dif-fers in that labeled source videos are only used for learning the source model. Adaptation only involves target videos, which avoids leaking annotated source videos. However, the absence of labeled source videos makes SFVDA a more challenging problem than UVDA since there is no reliable supervision signal, and no data drawn from the distribution to be aligned, which makes it even more challenging.
Very recently, Xu et al. [50] proposed a pioneering ap-proach to SFVDA based on temporal consistency. They adapt the source model by encouraging it to keep the ca-pability of understanding motion dynamics despite domain shifts. They train the model to produce features/predictions for a video clip consistent with those of other clips within the same video or that of the entire video. Despite im-                               
we implement this in a nearly no-cost way: We store his-torical predictions of all the clips (with randomly sampled frames) from each video in a memory bank and retrieve pre-dictions from the bank to enforce prediction consistency for the current clip. This technique reinforces temporal consis-tency and we call it historical consistency (HC). Notably,
TC and SC produce “hard” versions of a clip and encourage the model to overcome the hard factors and make consis-tent predictions. Therefore, the model must have a strong understanding of the target domain to fulfill these tasks, fa-cilitating model adaptation.
Thanks to simplicity in design, our STHC method can be easily extended to other SFVDA settings, including the open-set setting where the target domain contains classes that are absent in the source domain, the partial setting where the source domain contains classes that are absent in the target domain, and the black-box setting where only outputs of the source model are available and the model weights are not accessible. Experiments show that STHC outperforms existing methods for all the SFVDA settings.
Our contributions can be summarized as follows:
• We comprehensively exploit consistency learning for videos and propose STHC model for SFVDA. STHC performs stochastic spatio-temporal augmentations on each video and enforces prediction consistency from spatial, temporal, and historical perspectives.
• We extend STHC to address various domain adap-tation problems under the SFVDA setting. To our best knowledge, most of these problems have not been studied before and we establish the evaluation bench-marks that will help future development.
• STHC achieves state-of-the-art performance for
SFVDA in various problem settings. 2.