Abstract
Supervised keypoint localization methods rely on large manually labeled image datasets, where objects can deform, articulate, or occlude. However, creating such large keypoint labels is time-consuming and costly, and is often error-prone due to inconsistent labeling. Thus, we desire an approach that can learn keypoint localization with fewer yet consis-tently annotated images. To this end, we present a novel formulation that learns to localize semantically consistent keypoint deﬁnitions, even for occluded regions, for varying object categories. We use a few user-labeled 2D images as input examples, which are extended via self-supervision us-ing a larger unlabeled dataset. Unlike unsupervised methods, the few-shot images act as semantic shape constraints for object localization. Furthermore, we introduce 3D geometry-aware constraints to uplift keypoints, achieving more ac-curate 2D localization. Our general-purpose formulation paves the way for semantically conditioned generative mod-eling and attains competitive or state-of-the-art accuracy on several datasets, including human faces, eyes, animals, cars, and never-before-seen mouth interior (teeth) localiza-tion tasks, not attempted by the previous few-shot methods.
Project page: https://xingzhehe.github.io/FewShot3DKP/ 1.

Introduction
Keypoint localization is a long-standing problem in com-puter vision with applications in classiﬁcation [7, 8], image generation [45, 66], character animation [64, 65], 3D model-ing [15,55], and anti-spooﬁng [9], among others. Traditional supervised keypoint localization approaches require a large dataset of annotated images with balanced data distributions to train robust models that generalize to unseen observa-tions [18, 84, 87]. However, annotating keypoints in images and videos is expensive, and usually requires several annota-tors with domain expertise [44, 80, 83]. Manual annotations can be inaccurate due to low resolution imagery [5] and tem-poral variations in illumination and appearance [29, 79], or even subjective, especially in presence of external occlusions
*Work was done while interning at Flawless AI
Figure 1. All results are obtained by 10-shot learning except Tigers where 20 examples are used. The left/right/middle keypoints are marked in red/blue/green. Using only a few shots, the model learns semantically consistent and human interpretable keypoints. Un-certainty modeling helps us identify occlusions and ambiguous boundaries, as shown in the mouth, eye, and car example.
[38, 89] and image blur effects [68, 96]. Besides, modeling self-occluded object parts is proven to be an ambiguous task since 3D consistent keypoint annotations are needed [97]. As a consequence, supervised approaches are prone to learning suboptimal models from noisy training data.
Unsupervised keypoint detection methods can predict con-sistent keypoint structures [19–21, 27, 43, 98], but they lack human interpretability or may be insufﬁcient, e.g., for editing tasks requiring detailed manipulation of object parts. Jakab
et al. [28] pioneered adding interpretability with a cycle loss between unsupervised images and unpaired pose examples.
However, their focus is different and the proposed cycleGAN struggles in a few-shot setting as it does not exploit paired ex-amples. Unsupervised methods can be extended to few-shot setups either by learning a mapping that regresses detected keypoints to human-labeled annotations [73], which requires hundreds or thousands of examples, or by attaching few-shot annotated examples to the unsupervised training batch as weak supervision [51]. However, we show that neither approach produces competitive predictions when added to state-of-the-art unsupervised keypoint localization methods.
Our contributions focus on making the latter approach work, building upon the unsupervised reconstruction in [20] and the skeleton formulation in [28].
Recent advances in semi-supervised keypoint localization has shown signiﬁcant progress in the ﬁeld. Still, most ex-isting methods are specialized for a single object category such as faces [3,57,85] and X-rays [6,94,95,100], or require hundreds or thousands of annotated examples to achieve competitive performance [51, 56, 81]. Our approach, how-ever, only needs a few dozens of examples. In an orthogonal direction, Honari et al. [24] assist keypoint localization via equivariance transforms and classiﬁcation labels, though the latter are not always available. Generative image labeling has also shown great promise [69, 90, 99]. However, an-notating StyleGAN-generated images is prone to artifacts and noise. Besides, generative approaches are limited to the underlying data distribution biases [53, 71], thus decreasing overall keypoint localization performance.
Current limitations create the need for an approach that can leverage a smaller yet semantically consistent corpus of human labeled annotations while generalizing to a much larger unlabelled image set. This paper presents a novel formulation that learns to localize semantically consistent keypoint deﬁnitions, even for occluded regions, for various object categories with complex geometry using only a few user-labeled images. We use as input a few example-based user-labeled 2D images with predeﬁned keypoint deﬁnitions and their linkages to learn to localize keypoints. Unlike un-supervised methods, the user-selected few-shot images act as semantic shape constraints for human-interpretable key-point localization. To enable generalization to the target data distribution, we extend our approach via self-supervision using a larger unlabeled dataset. In addition, we introduce 3D geometry-aware constraints to model depth and uplift 2D keypoints in 3D with viewpoint consistency, thus achieving more accurate 2D localization.
Experimental results demonstrate that our proposed ap-proach competes with or outperforms state-of-the-art meth-ods in few-shot keypoint localization for human faces, eyes, animals, and cars using a only few user-deﬁned semantic examples. We also show the capabilities of our keypoint localization approach on a novel data distribution, speciﬁ-cally the mouth interior, which has not been attempted with previous few-shot localization approaches. Thus, our novel general-purposed formulation paves the way for semantically conditional generative modeling with a few user-labeled ex-amples. We hope it will enable a broader set of downstream applications, including fast dataset labeling, and in-the-wild modeling and tracking of complex objects, among others.
Our key contributions are summarized as follows: 1. A novel formulation for few-shot 3D geometry-aware keypoint localization that works on diverse data distributions. 2. We introduce keypoint uncertainty and local 3D aware geometry constraints for better keypoint localization. 3. We adapt techniques of transformation equivariance and image reconstruction from unsupervised methods. 4. Our approach enables ﬂexible modeling of complex deformable objects and geometric parts, such as mouth inte-rior, faces, eyes, cars, and animals via a few user examples with consistent semantic deﬁnitions. 2.