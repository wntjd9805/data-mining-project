Abstract
High-quality 3D human body reconstruction requires high-ﬁdelity and large-scale training data and appropriate network design that effectively exploits the high-resolution input images. To tackle these problems, we propose a sim-ple yet effective 3D human digitization method called 2K2K, which constructs a large-scale 2K human dataset and in-fers 3D human models from 2K resolution images. The proposed method separately recovers the global shape of a human and its details. The low-resolution depth network predicts the global structure from a low-resolution image, and the part-wise image-to-normal network predicts the de-tails of the 3D human body structure. The high-resolution depth network merges the global 3D shape and the detailed structures to infer the high-resolution front and back side depth maps. Finally, an off-the-shelf mesh generator recon-structs the full 3D human model, which are available at https://github.com/SangHunHan92/2K2K. In addition, we also provide 2,050 3D human models, including texture maps, 3D joints, and SMPL parameters for research purposes. In experiments, we demonstrate competitive per-formance over the recent works on various datasets. 1.

Introduction
Reconstructing photo-realistic 3D human models is one of the actively researched topics in computer vision and graph-ics. Conventional approaches search for correspondences across multiple views. Therefore, it was necessary to em-ploy multiple camera systems [9, 11] to acquire high-quality human models. However, the bulky and expensive camera systems limit the usage of normal users such as personal content creators and inﬂuencers. Recent progress in deep learning has shown the possibility of reconstructing human models from a single image [1,15,21,30,35,40,46,48,54,55].
Nevertheless, there still exists room to improve the quality of 3D human models, especially given an input single image.
Existing approaches fall into two categories; the ﬁrst is to predict a deep implicit volume [15, 40, 41] and the second is to infer multiple depth maps [13] from an image. In the case of the ﬁrst approach, the implicit volume can be directly pre-dicted through a deep learning network [46] or the volume can be constructed by predicting each voxel [40, 41] mil-lions of times. Therefore, these approaches are demanding either in memory or in time. The second approach requires to predict at least two depth maps, one for the front and the other for the back, to build a complete 3D human model.
Gabeur et al. [13] propose an adversarial framework to pre-dict double-sided depth maps; however, it shows poor results owing to inadequate training data generated by using 19 human scan models in addition to the synthetic dataset [47].
Here, researchers have paid attention to recover not only geometric details [41] such as facial regions but also people in various postures by predicting surface normal maps and parametric template models [33].
We claim that the prediction quality of 3D human mod-els is primarily affected by suitably designed deep neu-ral networks and high-quality training datasets, i.e., high-resolution images and 3D human models. Existing ap-proaches, however, could not handle high-resolution images, e.g., 2048×2048, because it requires massive learnable pa-rameters to train, and there was no such large-scale dataset for 3D human digitization.
There are human datasets opened publicly for research purposes [49–51, 55]. These datasets, however, lack both quality and quantity to train a network generating high-ﬁdelity human models. In this paper, we present a practical approach to reconstructing high-quality human models from high-resolution images and a large-scale human scan dataset consisting of more than 2,000 human models, where existing methods utilize a few hundred human scans to train their networks.
Our framework takes high-resolution images as input up to 2K, 2048×2048, and it is the ﬁrst to predict high-resolution depth maps for the task of 3D human reconstruction, named 2K2K. To minimize the number of learnable parameters and memory usage, we split the human body into multiple body parts such as arms, legs, feet, head, and torso, with the aid of a 2D pose human detector [8]. In addition, we align each body part by rotating and scaling to a canonical position which makes the proposed method robust under human pose variation while excluding background regions from the computation. By doing this, the part-wise image-to-normal prediction network can predict accurate surface normals even in the presence of pose variations. Afterward, we merge predicted normal maps into a single normal map and feed it to the normal-to-depth prediction network. Note that it is hard to predict depth maps directly for each body part because of the scale ambiguity; predicting the depth map from a merged normal map can alleviate this problem. We also predict a coarse depth map and feed it to the normal-to-depth prediction network to obtain consistent depth maps over different body parts. Finally, we generate high-ﬁdelity human meshes through Marching cubes [26], whose example is shown in Fig. 1.
To summarize, the contributions of this paper are: 1. Accuracy. Our method recovers the details of a hu-man from high-resolution images up to a resolution of 2048×2048. 2. Efﬁciency. The part-wise normal prediction scheme naturally excludes background regions from computa-tion, which results in a reduced memory footprint. 3. Data. We release a large-scale 3D human dataset con-sisting of 2,050 human models with synthesized images. 2.