Abstract
As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolu-tion upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and over-fitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency.
However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. On the other hand, decreasing the number of chunks through training optimization techniques usually re-quires high model capacity, which significantly slows down execution speed. To reconcile such, we propose a novel method for high-quality and efficient video resolution up-scaling tasks, which leverages the spatial-temporal infor-mation to accurately divide video into chunks, thus keep-ing the number of chunks as well as the model size to min-imum. Additionally, we advance our method into a sin-gle overfitting model by a data-aware joint training tech-nique, which further reduces the storage requirement with negligible quality drop. We deploy our models on an off-the-shelf mobile phone, and experimental results show that our method achieves real-time video super-resolution with high video quality. Compared with the state-of-the-art, our method achieves 28 fps streaming speed with 41.6 PSNR, which is 14× faster and 2.29 dB better in the live video resolution upscaling tasks. Code available in https:// github.com/coulsonlee/STDO-CVPR2023.git. 1.

Introduction
Being praised by its high image quality performance and wide application scenarios, deep learning-based super-resolution (SR) becomes the core enabler of many incred-† Equal Contribution.
Figure 1. Patch PSNR heatmap of two frames in a 15s video when super-resolved by a general WDSR model. A clear bound-ary shows that PSNR is strongly related to video content. ible, cutting-edge applications in the field of image/video reparation [10, 11, 39, 40], surveillance system enhance-ment [9], medical image processing [35], and high-quality video live streaming [20]. Distinct from the traditional methods that adopt classic interpolation algorithms [15, 45] to improve the image/video quality, the deep learning-based
[10, 11, 21, 24, 28, 40, 44, 47, 57, 60] exploit approaches the advantages of learning a mapping function from low-resolution (LR) to high-resolution (HR) using external data, thus achieving better performance due to better generaliza-tion ability when meeting new data.
Such benefits have driven numerous interests in design-ing new methods [5, 17, 50] to deliver high-quality video stream to users in the real-time fashion, especially in the context of massive online video and live streaming avail-able. Among this huge family, an emerging representa-tive [13,16,31,38] studies the prospect of utilizing SR model to upscale the resolution of the LR video in lieu of transmit-ting the HR video directly, which in many cases, consumes tremendous bandwidth between servers and clients [19].
One practical method is to deploy a pretrained SR model on the devices of the end users [25, 54], and perform res-olution upscaling for the transmitted LR videos, thus ob-taining HR videos without causing bandwidth congestion.
However, the deployed SR model that is trained with lim-ited data usually suffers from limited generalization abil-Figure 2. Overview of the proposed STDO method. Each video frame is sliced into patches, and all patches across time dimension are divided and grouped into chunks. Here we set the number of chunks to 2 for clear illustration. Then each chunk is overfitted by independent
SR models, and delivered to end-user for video super-resolution. ity, and may not achieve good performance at the presence of new data distribution [55]. To overcome this limitation, new approaches [4, 8, 20, 30, 51, 53, 55] exploit the overfit-ting property of DNN by training an SR model for each video chunk (i.e., a fragment of the video), and deliver-ing the video alongside the corresponding SR models to the clients. This trade-off between model expressive power and the storage efficiency significantly improves the quality of the resolution upscaled videos. However, to obtain bet-ter overfitting quality, more video segments are expected, which notably increase the data volume as well as system overhead when processing the LR videos [55]. While ad-vanced training techniques are proposed to reduce the num-ber of SR models [30], it still requires overparameterized
SR backbones (e.g., EDSR [28]) and handcrafted modules to ensure sufficient model capacity for the learning tasks, which degrades the execution speed at user-end when the device is resource-constraint.
In this work, we present a novel approach towards high-quality and efficient video resolution upscaling via Spatial-Temporal Data Overfitting, namely STDO, which for the first time, utilizes the spatial-temporal information to accu-rately divide video into chunks. Inspired by the work pro-posed in [1, 14, 23, 46, 58] that images may have different levels of intra- and inter-image (i.e., within one image or between different images) information density due to var-ied texture complexity, we argue that the unbalanced infor-mation density within or between frames of the video uni-versally exists, and should be properly managed for data overfitting. Our preliminary experiment in Figure 1 shows that the PSNR values at different locations in a video frame forms certain pattern regarding the video content, and ex-hibits different patterns along the timeline. Specifically, at the server end, each frame of the video is evenly divided into patches, and then we split all the patches into multi-ple chunks by PSNR regarding all frames. Independent SR models will be used to overfit the video chunks, and then de-livered to the clients. Figure 2 demonstrates the overview of our proposed method. By using spatial-temporal informa-tion for data overfitting, we reduce the number of chunks as well as the overfitting models since they are bounded by the nature of the content, which means our method can keep a minimum number of chunks regardless the dura-tion of videos.
In addition, since each chunk has similar data patches, we can actually use smaller SR model without handcrafted modules for the overfitting task, which reduces the computation burden for devices of the end-user. Our experimental results demonstrate that our method achieves real-time video resolution upscaling from 270p to 1080p on an off-the-shelf mobile phone with high PSNR.
Note that STDO encodes different video chunks with independent SR models, we further improve it by a Joint training technique (JSTDO) that results in one single SR model for all chunks, which further reduces the storage requirement. We design a novel data-aware joint training technique, which trains a single SR model with more data from higher information density chunks and less data from their counterparts. The underlying rationale is consistent with the discovery in [46, 58], that more informative data contributes majorly to the model training. We summarize
our contributions as follows:
• We discover the unbalanced information density within video frames, and it universally exists and constantly changes along the video timeline.
• By leveraging the unbalanced information density in the video, we propose a spatial-temporal data overfit-ting method STDO for video resolution upscaling, which achieves outperforming video quality as well as real-time execution speed.
• We propose an advanced data-aware joint training tech-nique which takes different chunk information density into consideration, and reduces the number of SR mod-els to a single model with negligible quality degradation.
• We deploy our models on an off-the-shelf mobile phone, and achieve real-time super-resolution performance. 2.