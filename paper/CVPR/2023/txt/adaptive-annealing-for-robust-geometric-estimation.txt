Abstract
Geometric estimation problems in vision are often solved via minimization of statistical loss functions which account for the presence of outliers in the observations. The corre-sponding energy landscape often has many local minima.
Many approaches attempt to avoid local minima by an-nealing the scale parameter of loss functions using methods such as graduated non-convexity (GNC). However, little at-tention has been paid to the annealing schedule, which is of-ten carried out in a fixed manner, resulting in a poor speed-accuracy trade-off and unreliable convergence to the global minimum. In this paper, we propose a principled approach for adaptively annealing the scale for GNC by tracking the positive-definiteness (i.e. local convexity) of the Hessian of the cost function. We illustrate our approach using the classic problem of registering 3D correspondences in the presence of noise and outliers. We also develop approxima-tions to the Hessian that significantly speeds up our method.
The effectiveness of our approach is validated by compar-ing its performance with state-of-the-art 3D registration ap-proaches on a number of synthetic and real datasets. Our approach is accurate and efficient and converges to the global solution more reliably than the state-of-the-art meth-ods. 1.

Introduction
Geometric estimation problems in computer vision use point locations or correspondences extracted from images or scans, e.g. epipolar geometry, bundle adjustment, 3D registration etc. Such point observations are noisy and also have a number of corrupting outliers. While RANSAC and related approaches [11, 22, 40, 53] work by removing obser-vations classified as outliers, an important class of methods use M-estimation [2,52,56,57], which reduces the influence of outliers. Denoting the geometric model parameters as x and the residual of the model fitting of the i-th observation as ri(x), the robust estimation problem is min x
N (cid:88) i=1
ρσ(∥ri(x)∥) (1)
Figure 1. Annealing of scale σ in GNC: We start with a large value of σ that makes the problem easy to solve. The red curve indicates the path of the minimum solution for robust estimation as σ is gradually reduced. We desire an annealing scheme that will ensure that we reach the final global minimum for σf inal. where ρ(.) is a statistical loss function and σ is a scale parameter. σ signifies the scale of the noise in the observa-tions and defines the distinction between what we classify as inliers and outliers.
Eqn. 1 is a non-linear optimization problem that is usu-ally solved using a series of local optimization steps. A common choice is the Iteratively Reweighted Least Squares (IRLS) approach [28, 31]. In IRLS, we take the gradient of the cost function and equate it to zero, resulting in
∇ρσ(x) = (cid:80)N i=1
⇒ (cid:80)N i=1
ρ′(∥ri∥)∇(∥ri∥) = 0 (cid:19)
ρ′(∥ri∥)
∥ri∥
∇ (cid:18) ∥ri∥2 2 (cid:18) ∥ri∥2 2 (cid:19) (2)
= 0 (3)
= 0 (4)
⇒ (cid:80)N i=1 ϕ(∥ri∥)∇
In a given iteration, we treat the weights ϕi = ϕ(∥ri∥) as constants, which gives us a weighted least squares problem.
Given a new estimate of x, these weights are updated, and the process is repeated until convergence.
Graduated Non-Convexity: Since the cost function in
Eqn. 1 can have a large number of local minima, a careful initialization of IRLS is necessary to reach the global minimum that we desire. To this end, a popular approach developed in the computer vision literature is Graduated
Non-Convexity (GNC) [10, 47, 66, 71, 72].
In GNC, we start with a large value, σ0, that smoothens the underlying cost function making it easier to optimize. When σ0 → ∞,
ρσ0 (∥r∥) → ∥r∥2/2 leading to a least squares problem.
The initial cost is, thus, very smooth and is either convex or has a large basin of convexity around the global minimum, thereby enabling practical optimization algorithms to reliably find the global minimum.
In many cases, there exist certifiable global solvers [12, 74], and in the special case of 3D registration, there exists a closed-form solution for the global minimum [55] of the initial cost. After obtaining the global minimum for the initial smoothened cost, we proceed by progressively annealing or reducing σ and re-solving Eqn. 1. Given an estimate xk at scale σk at the k-th stage, we update scale to σk+1 and solve Eqn. 1 using xk as the initialization, yielding the updated estimate xk+1. This is repeated till we terminate our procedure at a desired final scale σf inal.
If we incrementally update σ from σk to σk+1, we can ensure that the resulting local minimum of the new cost function, obtained using the previous solution as the initialization, is a very good local minimum if not the global minimum of the new cost. When the data is corrupted with more than 50% outliers in an adversarial manner, any robust method would give a model that is biased towards the majority outlier observations. This is because the global minimum of the robust cost is not close to the inlier model parameters. Thus, it is reasonable for any GNC strategy to focus on obtaining the global minimum of the robust cost on data corrupted with < 50% outliers.
Fig. 1 illustrates a family of cost functions parametrized by σ used in GNC. Starting from a very large σ which gives us a smooth, easily optimizable cost function in Eqn. 1, we can move x along the smooth red curve which is the locus of the minimum of the cost function as we drive σ down to its final value σf inal. The GNC procedure seeks to ensure that at every stage (k + 1), the initialization xk falls within the basin of convergence for the global minimum of the current cost function for σk+1. This, in turn, increases the likelihood that the final estimate of x at σf inal is the global minimum, i.e. we try to avoid falling into the basin of a local minimum. We note that as we anneal σ, the cost function starts becoming non-convex with multiple local minima in the cost function landscape. However, if throughout we remain in the same basin of convergence as we modify σ, we can reach the global minimum more
Figure 2. Adaptive annealing for GNC for line fitting in the pres-ence of outliers. See text for details. reliably for the desired cost function specified by σf inal, in most of the cases with outliers < 50%. Even in cases where the outlier corruption is ≥ 50%, the real-world data is not always adversarial, thereby enabling GNC to perform well. It will be noted that this is akin to the approach of homotopy continuation [24, 37, 38, 41] in optimization.
Annealing Scheme: The inset in Fig. 1 further illus-trates the GNC procedure. At the k-th stage, we are at the minimum of Eqn. 1 for σk. We keep the current xk and update σ to σk+1, as indicated by the downward vertical arrow. Given this scale σk+1, we solve Eqn. 1, which moves the current estimate of the model parameters to xk+1 as indicated by the leftward horizontal arrow. This procedure is repeated till we reach the desired scale σf inal.
The performance of this scheme critically depends on the annealing steps used. While GNC is used in many geometric estimation approaches [66, 68, 75], the majority of them use a simple annealing scheme, wherein, at each stage, the scale σ is decremented by a fixed factor, i.e.
σk+1 = σk
γ where γ > 1. It will be immediately noted that the choice of γ has a significant impact on the performance of GNC. If we take γ close to 1, say γ = 1 + ϵ for very small ϵ > 0, we move slowly in the cost function landscape.
This will ensure that we stay on the red curve leading to the global minimum for σf inal. However, this comes at the cost of using a large number of annealing stages, thereby making the computational cost prohibitive. In contrast, if we take a large γ, we move rapidly in the scale space σ but can possibly end up in a poor quality local minimum.
In this paper, we propose a principled approach to adap-tively anneal the scale σ. At each stage, we seek to use
GNC strategy
Small γ (fixed)
Large γ (fixed)
Adaptive γ [Ours]
Desirable
# of stages in GNC Accuracy
↑
↓
↓
↓
↑
↓
↑
↑
Table 1. approach achieves high accuracy with fewer annealing stages.
Impact of different annealing strategies. Our adaptive as large a γ as possible while ensuring that we always stay within the desired basin of convergence of the global minimum. While our approach is general, for simplic-ity, throughout this paper, we use the Geman-McClure loss function:
ρσ(e) = e2 2(1 + e2
σ2 ) (5)
In Fig. 2, we illustrate the impact of our adaptive an-nealing scheme for a simple problem of 2D line fitting in the presence of outliers. When we set the annealing fac-tor to a small value (γ = 2), GNC converges to the global minimum but needs 14 stages. For a large annealing fac-tor (γ = 10), GNC terminates in 4 stages but converges to a poor local minimum. In contrast, our adaptive approach uses 8 stages and converges to the correct solution at the global minimum. We can summarize our observations as shown in Table. 1. 1.1. Adaptive Annealing
In this subsection, we describe our adaptive annealing scheme. From the above discussion, we have noted that at each stage of GNC, we seek to anneal σ while ensuring that we remain in the basin of convergence of the global mini-mum. We propose to achieve this objective by examining the Hessian of the cost function in Eqn. 1 with respect to the parameters x. For an Np parameter model, the Np × Np
Hessian matrix is
[Hi](r,s) =
∂2ρσ(∥ri(x)∥)
∂xr∂xs (cid:12) (cid:12) (cid:12) (cid:12)xk (6)
⇒ H =
N (cid:88) i=1
Hi where i is the index for the individual observations in
Eqn. 1. Thus, H is the Hessian of the robust cost function evaluated at the k-th stage estimate xk. Since xk is a minimum of the cost function in Eqn. 1 evaluated for σk, we note that H is locally convex, i.e. positive-definite.
This is true since we have converged to xk through an optimization of Eqn. 1.1 Additionally, in the (k + 1)-th 1We emphasize that the positive-definiteness of H indicates local con-vexity of our cost function and does not imply that our cost function is globally convex. iteration, if we update the scale to σk+1 ensuring that the
Hessian H evaluated at σk+1 remains positive-definite, then we have likely ensured that the new estimate, xk+1, is in the same basin of convergence as that of the previous iteration. This is true since we initialize our optimization for the (k + 1)-th iteration at xk and converge to the local minimum using IRLS. We finally note that we begin the adaptive GNC procedure for a very large σ, thereby ensuring that the cost function is very smooth and convex over a larger domain. This makes it easier to obtain the global minimum of the smoothened cost function, which serves as the first estimate x0. To summarize, if we iteratively choose σk and solve for xk, such that at each stage, the corresponding Hessian H, in Eqn. 6, re-mains positive-definite, then the resulting solution, xf inal, is very likely to be the global minimum of the cost at σf inal.
In the (k + 1)-th iteration, in the interest of a faster estimate, we seek to reduce σk+1 as much as possible while maintaining local convexity of H. Since a positive-definite
H has all positive eigen values, we achieve our objective by tracking the sign of the smallest eigen value λmin of
H. Therefore, our criteria for maintaining local convexity translates to one of seeking the smallest σk+1 (i.e. the largest annealing) while maintaining λmin(H) > 0. While a λmin(H) close to zero can make H highly ill-conditioned, we note that we never use H in our estimation process and only use the criteria of λmin(H) > 0 to determine σk+1.
We can also choose to terminate our search when λmin(H) is close to a threshold λT > 0, which will ensure that H is never ill-conditioned and the optimization of Eqn. 1 using
σk+1 is a well-defined problem.
Binary Search: In general, we do not have closed-form expressions for λmin(H). Consequently, we estimate σk+1 using a divide-and-conquer approach using the criteria that λmin(H) > 0. We define a search interval below
σk and perform a binary search. We note that the cost of evaluating λmin(H) is cheap since H is a small Np × Np
In the problem of interest in this paper, i.e. 3D matrix. registration, Np = 6. We note here that while the cost in Eqn. 1 is non-linear, it is smooth and differentiable.
Implicit to our binary search strategy is the assumption that λmin is monotonically decreasing as we reduce σ in the binary search interval. We observe that, in practice, this assumption is true in most cases. We can also ensure the reliability of this assumption by making the search interval smaller, although that would increase the overall computational time since now we will need more stages of
GNC to reach the desired scale σf inal.
Our approach of adaptive annealing of scale σ to solve
Eqn. 1 is general in nature. In the remainder of this paper,
we illustrate our approach with a concrete example of solv-ing for robust 3D registration given point correspondences that are noisy and include a large number of outliers. Be-fore we develop our method for 3D registration, we briefly discuss some of the relevant literature. 2.