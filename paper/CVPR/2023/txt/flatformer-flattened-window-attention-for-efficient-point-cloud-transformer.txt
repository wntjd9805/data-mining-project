Abstract
Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3× slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving).
This inefficiency comes from point clouds’ sparse and irreg-ular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better com-putational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effec-tively avoids expensive structuring and padding overheads.
We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from differ-ent directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on
Waymo Open Dataset with 4.6× speedup over (transformer-based) SST and 1.4× speedup over (sparse convolutional)
CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks. 1.

Introduction
Transformer [75] has become the model of choice in nat-ural language processing (NLP), serving as the backbone of many successful large language models (LLMs) [2, 17].
Recently, its impact has further been expanded to the vision community, where vision transformers (ViTs) [18, 45, 74] have demonstrated on-par or even superior performance com-pared with CNNs in many visual modalities (e.g., image and video). 3D point cloud, however, is not yet one of them.
∗ indicates equal contributions.
Figure 1. Previous point cloud transformers (⋆) are 3-4× slower than sparse convolution-based models (•) despite achieving similar detection accuracy. FlatFormer is the first point cloud transformer that is faster than sparse convolutional methods with on-par accu-racy. Latency is measured on an NVIDIA Quadro RTX A6000.
Different from images and videos, 3D point clouds are intrinsically sparse and irregular. Most existing point cloud models [94] are based on 3D sparse convolution [24], which computes convolution only on non-zero features. They re-quire dedicated system support [14, 71, 91] to realize high utilization on parallel hardware (e.g., GPUs).
Many efforts have been made toward point cloud trans-formers (PCTs) to explore their potential as an alternative to sparse convolution. Global PCTs [26] benefit from the reg-ular computation pattern of self-attention but suffer greatly from the quadratic computational cost (w.r.t. the number of points). Local PCTs [50, 98] apply self-attention to a local neighborhood defined in a similar way to point-based mod-els [57] and are thus bottlenecked by the expensive neighbor gathering [47]. These methods are only applicable to single objects or partial indoor scans (with <4k points) and cannot be efficiently scaled to outdoor scenes (with >30k points).
Inspired by Swin Transformer [45], window PCTs [19,70] compute self-attention at the window level, achieving im-pressive accuracy on large-scale 3D detection benchmarks.
Despite being spatially regular, these windows could have drastically different numbers of points (which differ by more than 80×) due to the sparsity. This severe imbalance results in redundant computation with inefficient padding and par-titioning overheads. As a result, window PCTs can be 3× slower than sparse convolutional models (Figure 1), limiting their applications in resource-constrained, latency-sensitive scenarios (e.g., autonomous driving, augmented reality).
This paper introduces FlatFormer to close this huge la-tency gap. Building upon window PCTs, FlatFormer trades spatial proximity for better computational regularity by par-titioning 3D point cloud into groups of equal sizes instead of windows of equal shapes. It applies self-attention within groups to extract local features, alternates the sorting axis to aggregate features from different orientations, and shifts windows to exchange features across groups. Benefit from the regular computation pattern, FlatFormer achieves 4.6× speedup over (transformer-based) SST and 1.4× speedup over (sparse convolutional) CenterPoint while delivering the state-of-the-art accuracy on Waymo Open Dataset.
To the best of our knowledge, FlatFormer is the first point cloud transformer that achieves on-par or superior accuracy than sparse convolutional methods with lower latency. It is also the first to achieve real-time performance on edge GPUs.
With better hardware support for transformers (e.g., NVIDIA
Hopper), point cloud transformers will have a huge potential to be the model of choice in 3D deep learning. We believe our work will inspire future research in this direction. 2.