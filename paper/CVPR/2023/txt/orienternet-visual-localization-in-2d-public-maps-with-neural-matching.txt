Abstract
Humans can orient themselves in their 3D environments using simple 2D maps. Differently, algorithms for visual lo-calization mostly rely on complex 3D point clouds that are expensive to build, store, and maintain over time. We bridge this gap by introducing OrienterNet, the first deep neural network that can localize an image with sub-meter accuracy using the same 2D semantic maps that humans use. Orienter-Net estimates the location and orientation of a query image by matching a neural Bird’s-Eye View with open and globally available maps from OpenStreetMap, enabling anyone to lo-calize anywhere such maps are available. OrienterNet is su-pervised only by camera poses but learns to perform seman-tic matching with a wide range of map elements in an end-to-end manner. To enable this, we introduce a large crowd-sourced dataset of images captured across 12 cities from the diverse viewpoints of cars, bikes, and pedestrians. Orienter-Net generalizes to new datasets and pushes the state of the art in both robotics and AR scenarios. The code is available at github.com/facebookresearch/OrienterNet. 1.

Introduction
As humans, we intuitively understand the relationship be-tween what we see and what is shown on a map of the scene we are in. When lost in an unknown area, we can accurately pinpoint our location by carefully comparing the map with our surroundings using distinct geographic features.
Yet, algorithms for accurate visual localization are typi-cally complex, as they rely on image matching and require detailed 3D point clouds and visual descriptors [18,31,38,39, 53, 57, 60]. Building 3D maps with LiDAR or photogramme-try [2, 22, 43, 61, 67] is expensive at world scale and requires costly, freshly-updated data to capture temporal changes in visual appearance. 3D maps are also expensive to store, as they are orders of magnitude larger than basic 2D maps.
This prevents executing localization on-device and usually requires costly cloud infrastructure. Spatial localization is thus a serious bottleneck for the large-scale deployment of robotics and augmented reality devices. This disconnect be-Figure 1. Towards human-like localization. Humans can eas-ily orient themselves with basic 2D maps while state-of-the-art algorithms for visual localization require complex 3D cues. Ori-enterNet can localize an image using only compact maps from
OpenStreetMap by matching Bird’s-Eye View and neural maps. tween the localization paradigms of humans and machines leads to the important research question of How can we teach machines to localize from basic 2D maps like humans do?
This paper introduces the first approach that can localize single images and image sequences with sub-meter accuracy given the same maps that humans use. These planimetric maps encode only the location and coarse 2D shape of few important objects but not their appearance nor height. Such maps are extremely compact, up to 104 times smaller in size than 3D maps, and can thus be stored on mobile devices and used for on-device localization within large areas. We demon-strate these capabilities with OpenStreetMap (OSM) [46], an openly accessible and community-maintained world map, enabling anyone to localize anywhere for free. This solution does not require building and maintaining costly 3D maps over time nor collecting potentially sensitive mapping data.
Concretely, our algorithm estimates the 3-DoF pose, as position and heading, of a calibrated image in a 2D map.
The estimate is probabilistic and can therefore be fused with an inaccurate GPS prior or across multiple views from a multi-camera rig or image sequences. The resulting solution is significantly more accurate than consumer-grade GPS sensors and reaches accuracy levels closer to the traditional pipelines based on feature matching [57, 60].
Map type
What?
Explicit geometry?
Visual appearance?
Freely available
Storage for 1 km2
Size reduction vs SfM
SfM
SLAM 3D points
+features 3D
✓
✗ 42 GB
-Satellite images pixel intensity
✗
✓
✗ 75 MB 550×
OpenStreetMap (our work) polygons, lines, points 2D
✗
✓ 4.8 MB 8800×
Table 1. Types of maps for visual localization. Planimetric maps from OpenStreetMap consist of polygons and lines with metadata.
They are publicly available for free and do not store sensitive ap-pearance information, as opposed to satellite images and 3D maps built with SfM. They are also compact: a large area can be down-loaded and stored on a mobile device. We show that they encode sufficient geometric information for accurate 3-DoF localization.
Our approach, called OrienterNet, is a deep neural net-work that mimics the way humans orient themselves in their environment when looking at maps, i.e., by matching the metric 2D map with a mental map derived from visual obser-vations [37,45]. OrienterNet learns to compare visual and se-mantic data in an end-to-end manner, supervised by camera poses only. This yields accurate pose estimates by leveraging the high diversity of semantic classes exposed by OSM, from roads and buildings to objects like benches and trash cans.
OrienterNet is also fast and highly interpretable. We train a single model that generalizes well to previously-unseen cities and across images taken by various cameras from di-verse viewpoints – such as car-, bike- or head-mounted, pro or consumer cameras. Key to these capabilities is a new, large-scale training dataset of images crowd-sourced from cities around the world via the Mapillary platform.
Our experiments show that OrienterNet substantially out-performs previous works on localization in driving scenarios and vastly improves its accuracy in AR use cases when ap-plied to data recorded by Aria glasses. We believe that our approach constitutes a significant step towards continuous, large scale, on-device localization for AR and robotics. 2.