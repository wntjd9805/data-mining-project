Abstract available at https://chengwei-zheng.github. io/EditableNeRF/.
Neural radiance ﬁelds (NeRF) achieve highly photo-realistic novel-view synthesis, but it’s a challenging prob-lem to edit the scenes modeled by NeRF-based methods, es-pecially for dynamic scenes. We propose editable neural radiance ﬁelds that enable end-users to easily edit dynamic scenes and even support topological changes. Input with an image sequence from a single camera, our network is trained fully automatically and models topologically vary-ing dynamics using our picked-out surface key points. Then end-users can edit the scene by easily dragging the key points to desired new positions. To achieve this, we propose a scene analysis method to detect and initialize key points by considering the dynamics in the scene, and a weighted key points strategy to model topologically varying dynamics by joint key points and weights optimization. Our method supports intuitive multi-dimensional (up to 3D) editing and can generate novel scenes that are unseen in the input se-quence. Experiments demonstrate that our method achieves high-quality editing on various dynamic scenes and outper-forms the state-of-the-art. Our code and captured data are 1.

Introduction
Neural radiance ﬁelds (NeRF) [23] have shown great power in novel-view synthesis and enable many applica-tions as this method achieves photo-realistic rendering [9].
Recent techniques have further improved NeRF by extend-ing it to handle dynamic scenes [27, 30, 40] and even topo-logically varying scenes [28]. However, these works mainly focus on reconstruction itself but do not consider scene edit-ing. Thus, for rendering, only the camera views can be changed, while the modeled scenes cannot be edited.
Recently, some frameworks have been proposed to make neural radiance ﬁelds editable in different aspects. Some of them aim to edit the reconstructed appearance and enable relighting [2, 35, 54]; some allow controlling the shapes and colors of objects from a speciﬁc category [15, 20, 44, 47]; and some divide the scene into different parts and the loca-tion of each part can be modiﬁed [48, 49, 52]. However, the
dynamics of moving objects cannot be edited by the previ-ous methods. And this task becomes much more challeng-ing when the dynamics contain topological changes. Topo-logical changes can lead to motion discontinuities (e.g., be-tween the hammer and the piano keys, between the cups and the table in Fig. 1) in 3D space and further cause notice-able artifacts if they are not modeled well. A state-of-the-art framework CoNeRF [16] tries to resolve this problem by us-ing manual supervision. However, it only supports limited and one-dimensional editing for each scene part, requiring user annotations as supervision.
We propose EditableNeRF, editable topologically vary-ing neural radiance ﬁelds that are trained without manual supervision and support intuitive multi-dimensional (up to three-dimensional) editing. The key of our method is to rep-resent motions and topological changes by the movements of some sparse surface key points. Each key point is able to control the topologically varying dynamics of a mov-ing part, as well as other effects like shadow and reﬂection changes through the neural radiance ﬁelds. This key-point-based method enables end-users to edit the scene by easily dragging the key points to their desired new positions.
To achieve this, we ﬁrst apply a scene analysis method to detect key points in the canonical space and track them in the full sequence for key point initialization. We introduce a network to estimate spatially-varying weights for all scene points and use the weighted key points to model the dynam-ics in the scene, including topological changes. In the train-ing stage, our network is trained to reconstruct the scene using the supervision from the input image sequence, and the key point positions are also optimized by taking motion (optical ﬂow) and geometry (depth maps) constraints as ad-ditional supervision. After training, the scene can be edited by controlling the key points’ positions, and novel scenes that are unseen during training can also be generated.
The contribution of this paper lies in the following as-pects:
• Key-point-driven neural radiance ﬁelds achieving intu-itive multi-dimensional editing even with topological changes, without requiring annotated training data.
• A weighted key points strategy modeling topologically varying dynamics by joint key points and weights op-timization.
• A scene analysis method to detect and initialize key points by considering the dynamics in the scene. 2.