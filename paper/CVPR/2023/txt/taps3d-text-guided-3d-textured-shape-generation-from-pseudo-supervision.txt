Abstract
In this paper, we investigate an open research task of generating controllable 3D textured shapes from the given textual descriptions. Previous works either require ground truth caption labeling or extensive optimization time. To resolve these issues, we present a novel frame-work, TAPS3D, to train a text-guided 3D shape generator with pseudo captions. Specifically, based on rendered 2D images, we retrieve relevant words from the CLIP vocab-ulary and construct pseudo captions using templates. Our constructed captions provide high-level semantic supervi-sion for generated 3D shapes. Further, in order to pro-duce fine-grained textures and increase geometry diversity, we propose to adopt low-level image regularization to en-able fake-rendered images to align with the real ones. Dur-ing the inference phase, our proposed model can generate 3D textured shapes from the given text without any addi-tional optimization. We conduct extensive experiments to analyze each of our proposed components and show the efficacy of our framework in generating high-fidelity 3D textured and text-relevant shapes. Code is available at https://github.com/plusmultiply/TAPS3D 1.

Introduction 3D objects are essential in various applications [21–24], such as video games, film special effects, and virtual real-∗Equal contribution. Work done during an internship at Bytedance.
†Corresponding author. ity. However, realistic and detailed 3D object models are usually hand-crafted by well-trained artists and engineers slowly and tediously. To expedite this process, many re-search works [3,9,13,34,48,49] use deep generative models to achieve automatic 3D object generation. However, these models are primarily unconditioned, which can hardly gen-erate objects as humans will.
In order to control the generated 3D objects from text, prior text-to-3D generation works [43, 44] leverage the pre-trained vision-language alignment model CLIP [37], such that they can only use 3D shape data to achieve zero-shot learning. For example, Dream Fields [13] combines the ad-vantages of CLIP and NeRF [27], which can produce both 3D representations and renderings. However, Dream Fields costs about 70 minutes on 8 TPU cores to produce a sin-gle result. This means the optimization time during the inference phase is too slow to use in practice. Later on,
GET3D [9] is proposed with faster inference time, which incorporates StyleGAN [15] and Deep Marching Tetrahe-dral (DMTet) [46] as the texture and geometry generators respectively. Since GET3D adopts a pretrained model to do text-guided synthesis, they can finish optimization in less time than Dream Fields. But the requirement of test-time optimization still limits its application scenarios. CLIP-NeRF [50] utilizes conditional radiance fields [45] to avoid test-time optimization, but it requires ground truth text data for the training purpose. Therefore, CLIP-NeRF is only ap-plicable to a few object classes that have labeled text data for training, and its generation quality is restricted by the
NeRF capacity.
To address the aforementioned limitations, we propose
to generate pseudo captions for 3D shape data based on their rendered 2D images and construct a large amount of
⟨3D shape, pseudo captions⟩ as training data, such that the text-guided 3D generation model can be trained over them. To this end, we propose a novel framework for
Text-guided 3D textured shApe generation from Pseudo
Supervision (TAPS3D), in which we can generate high-quality 3D shapes without requiring annotated text training data or test-time optimization.
Specifically, our proposed framework is composed of two modules, where the first generates pseudo captions for 3D shapes and feeds them into a 3D generator to con-duct text-guided training within the second module. In the pseudo caption generation module, we follow the language-free text-to-image learning scheme [20, 54]. We first adopt the CLIP model to retrieve relevant words from given ren-dered images. Then we construct multiple candidate sen-tences based on the retrieved words and pick sentences hav-ing the highest CLIP similarity scores with the given im-ages. The selected sentences are used as our pseudo cap-tions for each 3D shape sample.
Following the notable progress of text-to-image gener-ation models [29, 38, 39, 42, 52], we use text-conditioned
GAN architecture in the text-guided 3D generator training part. We adopt the pretrained GET3D [9] model as our backbone network since it has been demonstrated to gen-erate high-fidelity 3D textured shapes across various object classes. We input the pseudo captions as the generator con-ditions and supervise the training process with high-level
CLIP supervision in an attempt to control the generated 3D shapes. Moreover, we introduce a low-level image regu-larization loss to produce fine-grained textures and increase geometry diversity. We empirically train the mapping net-works only of a pretrained GET3D model so that the train-ing is stable and fast, and also, the generation quality of the pretrained model can be preserved.
Our proposed model TAPS3D can produce high-quality 3D textured shapes with strong text control as shown in
Fig. 1, without any per prompt test-time optimization. Our contribution can be summarized as:
• We introduce a new 3D textured shape generative framework, which can generate high-quality and fi-delity 3D shapes without requiring paired text and 3D shape training data.
• We propose a simple pseudo caption generation method that enables text-conditioned 3D generator training, such that the model can generate text-controlled 3D textured shapes without test time opti-mization, and significantly reduce the time cost.
• We introduce a low-level image regularization loss on top of the high-level CLIP loss in an attempt to produce fine-grained textures and increase geometry diversity. 2.