Abstract 1.

Introduction
Blind text image super-resolution (SR) is challenging as one needs to cope with diverse font styles and unknown degradation. To address the problem, existing methods perform character recognition in parallel to regularize the
SR task, either through a loss constraint or intermediate feature condition. Nonetheless, the high-level prior could still fail when encountering severe degradation. The prob-lem is further compounded given characters of complex structures, e.g., Chinese characters that combine multiple pictographic or ideographic symbols into a single charac-ter.
In this work, we present a novel prior that focuses more on the character structure. In particular, we learn to encapsulate rich and diverse structures in a StyleGAN and exploit such generative structure priors for restora-tion. To restrict the generative space of StyleGAN so that it obeys the structure of characters yet remains flexible in handling different font styles, we store the discrete fea-tures for each character in a codebook. The code subse-quently drives the StyleGAN to generate high-resolution structural details to aid text SR. Compared to priors based on character recognition, the proposed structure prior ex-erts stronger character-specific guidance to restore faithful and precise strokes of a designated character. Extensive experiments on synthetic and real datasets demonstrate the compelling performance of the proposed generative structure prior in facilitating robust text SR. Our code is available at https://github.com/csxmli2016/MARCONet.
Blind text super-resolution (SR) aims at restoring a high-resolution (HR) image from a low-resolution (LR) text image that is corrupted by unknown degradation. The problem is relatively less explored than SR of natural and face images.
This seemingly easy task actually possesses many unique challenges. In particular, each character owns its unique structure. A suboptimal restoration that destroys the struc-ture with, e.g., distorted, missing or additional strokes, is easily perceptible and may alter the meaning of the original word. The task becomes more difficult when dealing with writing systems such as Chinese, Kanji and Hanja characters, in which the patterns of thousands of characters vary in com-plex ways and are mostly composed of different subunits (or compound ideographs). A slight deviation in strokes, say
‘太’ and ‘大’ carries entirely different meanings. The prob-lem becomes more intricate given the different typefaces.
In addition, complex and unknown degradation make the data-driven convolutional neural networks (CNNs) incapable of generalizing well to real-world degraded observations.
To ameliorate the difficulties in blind text SR, espe-cially in retaining the unique structure of each character, recent studies design new loss functions to constrain SR re-sults semantically close to the high-resolution (HR) ground-truth [7,8,45,50,79], or incorporate the recognition prior into the intermediate features [40, 41, 44]. Figure 1 shows several representative results of these two types of methods, i.e., TB-SRN [7] that applies content-aware constraint and TATT [41]
that embeds the recognition prior into the SR process. The results are not satisfactory despite the models were retrained with data synthesized with sophisticated degradation models, i.e., BSRGAN [75] and Real-ESRGAN [66]. TBSRN and
TATT perform well when the LR character has fewer strokes.
However, they both fail to generate the desired structures when the character is composed of complex strokes, e.g., the 4, 6, 8, 11-th characters in (a). The results suggest that high-level recognition prior cannot well benefit the SR task in restoring faithful and accurate structures.
When dealing with complex characters, we can identify some commonly used strokes as constituents of characters or their subparts. These obvious structural regularities have not been studied thoroughly in the literature. In particular, most existing text SR studies [7, 40, 41, 45, 50, 79] focus on scene text images that are presented in different view perspectives or arranged in irregular patterns. In contrast, our goal is to explore structural regularities specific to complex char-acters and investigate an effective way to exploit the prior.
The investigation has practical values in many applications, including the restoration of old printed media (e.g., newspa-pers or books) for digitalization and preservation, and the enhancement of captions in digital media.
The key idea of our solution is to mine structure prior from a generative network, and use the prior to facilitate blind SR of text images. To capture the structure prior of characters, we train a StyleGAN to generate characters of different styles. As our intent is not to generate infinite and non-repetitive results, we restrict the generative space of
StyleGAN to obey the structure of characters by constraining the generation with a codebook. Specifically, the codebook stores the discrete code of each character, and each code serves as a constant to StyleGAN for generating a specific high-resolution character. The font style is governed by the
W latent space of StyleGAN. Once the character StyleGAN is trained, we can extract its intermediate features serving as the generative structure prior, which encapsulates the intrinsic structure and style of the LR character.
Using the prior for text SR can be conveniently achieved through an additional Transformer-based encoder and a text
SR network that accepts prior. In particular, given a text image that constitutes multiple characters (e.g., Figure 1), we use a Transformer-based encoder to predict the font style, character bounding boxes, and their respective indexes in the codebook jointly. The codebook indexes drive the structure prior generation for each character. In the text SR network, each LR character is super-resolved, guided by the respective priors that are aligned using their bounding boxes.
Experiments demonstrate that our design can generate robust and consistent structure priors for diverse and severely degraded text input. With the embedded structure prior, our approach performs superior against state-of-the-art methods in generating photo-realistic text results and shows excellent generalization to real-world LR text images. While our study mainly employs Chinese characters as examples, the proposed method can be readily extended to characters of other languages. We call our approach as MARCONet. The main contributions are summarized as follows:
• We show that blind SR task, especially for characters with complex structures, can be restored by using their structure prior encapsulated in a generative network.
• We present a viable way of learning such generative structure prior through reformulating a StyleGAN by replacing its single constant with discrete codes that represent different characters.
• To retrieve the prior accurately, we propose a
Transformer-based encoder to jointly predict the font styles, character bounding boxes and their indexes in codebook from the LR input. 2.