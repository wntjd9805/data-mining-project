Abstract
To apply optical flow in practice, it is often necessary to resize the input to smaller dimensions in order to reduce computational costs. However, downsizing inputs makes the estimation more challenging because objects and motion ranges become smaller. Even though recent approaches have demonstrated high-quality flow estimation, they tend to fail to accurately model small objects and precise bound-aries when the input resolution is lowered, restricting their applicability to high-resolution inputs.
In this paper, we introduce AnyFlow, a robust network that estimates accu-rate flow from images of various resolutions. By repre-senting optical flow as a continuous coordinate-based rep-resentation, AnyFlow generates outputs at arbitrary scales from low-resolution inputs, demonstrating superior perfor-mance over prior works in capturing tiny objects with de-tail preservation on a wide range of scenes. We establish a new state-of-the-art performance of cross-dataset gener-alization on the KITTI dataset, while achieving comparable accuracy on the online benchmarks to other SOTA methods. 1.

Introduction
Optical flow seeks to estimate per-pixel correspon-dences, characterized as the horizontal and vertical shift, from a pair of images. Specifically, it aims to identify the correspondence across pixels in different images, which is at the heart of numerous computer vision tasks such as video denoising [4, 33, 70], action recognition [55, 60, 62] and object tracking [11, 26, 46]. This is particularly chal-lenging due to the fact that the scene geometry and object motion are combined into a single observation, making the inverse problem of estimating motion highly ill-posed.
A common assumption for enabling computationally tractable optical flow is to explicitly account for small mo-tion in local neighborhood and incorporate additional prior to constrain the solution space, such as using total variation
*This work was done during the author’s internship at Meta.
†Affiliated with Meta at the time of this work.
Figure 1. Predicting 50% downsized images on Sintel [5], we compare AnyFlow to RAFT [57]. AnyFlow shows clearer bound-aries, accurate shapes, and better detection of small objects. prior [15, 71] and smooth prior [2, 3, 51]. While effective, these techniques are significantly restricted by the assump-tion and do not generalize well for real-life scenes with so-phisticated geometry and motions.
An alternative approach is motivated by the success of deep neural networks. Different from using handcrafted priors, learning-based methods [14, 67, 74] design end-to-end networks to regress the motion field. Sun et al. [53] use the coarse-to-fine approach to characterize the pixel-to-pixel mapping and several methods [20, 22, 44, 67] are de-veloped along this line. The drawback of these approaches is that the accuracy of flow estimation is not only limited by the sampling but also has a critical dependence on a good initial solution because the pixel correspondence is funda-mentally ambiguous. Tedd and Deng [57] resort to iterative update module on top of the correlation volumes, allowing better estimates in tiny and fast-moving objects. Indeed the success of this model relies on having a correlation volume that is high quality in characterizing the feature correspon-dence for sub-pixel level. Inspired by its success, follow-up methods [23, 25, 36, 50, 52] have further improved it in multiple ways. One popular direction is to introduce atten-tion mechanisms [59] for efficiency [64,72], occlusion han-dling [24], and large displacements [49]. As attention has been found to be effective in estimating long-range depen-dencies, Xu et al. [65] and Huang et al. [19] adopt Vision
Transformers [34] to perform global matching and demon-strate their effectiveness. However, these methods naturally become less effective for low-resolution images, where in-accuracy is introduced when computing the correlation vol-umes and the iterative refinements. They tend to fail in ac-curately modeling small objects and precise boundary when the input resolution is lowered. This inherently limits the applicability for mobile devices in particular, where resiz-ing the input to small size is often necessary to reduce the computational cost.
In this paper, we propose AnyFlow, a novel method that is agnostic to the resolution of images. The key insight behind AnyFlow is the use of implicit neural representa-INRs, such as LIIF [8], tion (INR) for flow estimation. have been demonstrated to be effective for image super-resolution, as they model an image as a continuous function without limiting the output to a fixed resolution. This en-ables image generation at arbitrary scales. Inspired by this capability, we design a continuous coordinate-based flow upsampler to infer flow at any desired scale. In addition, we propose a novel warping scheme utilizing multi-scale feature maps together with dynamic correlation lookup to generalize well on diverse shapes of input. As the proposed methods produce a synergistic effect, we demonstrate su-perior performance for tiny objects and detail preservation on a wide range of scenes involving complex geometry and motions. Specifically, we verify the robustness for the input with low resolution. Fig. 1 showcases our technique against
RAFT [57]. Our contributions are summarized as follows:
• We introduce AnyFlow, a novel network to produce high quality flow estimation for arbitrary size of image.
• We present the methods to utilize multi-scale feature maps and search optimal correspondence within pre-dicted range, enabling robust estimation in wide ranges of motion types.
• We demonstrate strong performance on cross-dataset generalization by achieving more than 25% of error reduction with only 0.1M additional parameters from the baseline and rank 1st on the KITTI dataset.
Together, our contributions provide, for the first time, an approach for arbitrary size of input, which significantly im-proves the quality for low-resolution images and extends the applicability for portable devices. 2.