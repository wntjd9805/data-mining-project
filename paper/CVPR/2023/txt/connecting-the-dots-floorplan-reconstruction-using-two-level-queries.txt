Abstract
Input 3D Point Cloud
Reconstructed Floorplan
We address 2D floorplan reconstruction from 3D scans.
Existing approaches typically employ heuristically de-signed multi-stage pipelines. Instead, we formulate floor-plan reconstruction as a single-stage structured predic-tion task: find a variable-size set of polygons, which in turn are variable-length sequences of ordered vertices.
To solve it we develop a novel Transformer architec-ture that generates polygons of multiple rooms in paral-lel, in a holistic manner without hand-crafted intermedi-ate stages. The model features two-level queries for poly-gons and corners, and includes polygon matching to make the network end-to-end trainable. Our method achieves a new state-of-the-art for two challenging datasets, Struc-tured3D and SceneCAD, along with significantly faster in-ference than previous methods. Moreover, it can read-ily be extended to predict additional information, i.e., se-mantic room types and architectural elements like doors and windows. Our code and models are available at: https://github.com/ywyue/RoomFormer. 1.

Introduction
The goal of floorplan reconstruction is to turn observa-tions of an (indoor) scene into a 2D vector map in birds-eye view. More specifically, we aim to abstract a 3D point cloud into a set of closed polygons corresponding to rooms, optionally enriched with further structural and semantic el-ements like doors, windows and room type labels.
Floorplans are an essential representation that enables a wide range of applications in robotics, AR/VR, inte-rior design, etc. Like prior work [2, 3, 8, 9, 29], we start from a 3D point cloud, which can easily be captured with
RGB-D cameras, laser scanners or SfM systems. Several works [8, 9, 21, 29] have shown the effectiveness of project-ing the raw 3D point data along the gravity axis, to obtain a 2D density map that highlights the building’s structural el-ements (e.g., walls). We also employ this early transition to 2D image space. The resulting density maps are compact and computationally efficient, but inherit the noise and data gaps of the underlying point clouds, hence floorplan recon-Figure 1. Semantic floorplan reconstruction. Given a point cloud of an indoor environment, RoomFormer jointly recovers multiple room polygons along with their associated room types, as well as architectural elements such as doors and windows. struction remains a challenging task.
Existing methods can be split broadly into two categories that both operate in two stages: Top-down methods [8, 29] first extract room masks from the density map using neu-ral networks (e.g., Mask R-CNN [15]), then employ opti-mization/search techniques (e.g., integer programming [28],
Monte-Carlo Tree-Search [4]) to extract a polygonal floor-plan. Such techniques are not end-to-end trainable, and their success depends on how well the hand-crafted opti-mization captures domain knowledge about room shape and layout. Alternatively, bottom-up methods [9, 21] first de-tect corners, then look for edges between corners (i.e., wall segments) and finally assemble them into a planar floorplan graph. Both approaches are strictly sequential and therefore dependent on the quality of the initial corner, respectively room, detector. The second stage starts from the detected entities, therefore missing or spurious detections may sig-nificantly impact the reconstruction.
We address those limitations and design a model that directly maps a density image to a set of room polygons.
Our model, named RoomFormer, leverages the sequence prediction capabilities of Transformers and directly out-puts a variable-length, ordered sequence of vertices per room. RoomFormer requires neither hand-crafted, domain-specific intermediate products nor explicit corner, wall or room detections. Moreover, it predicts all rooms that make up the floorplan at once, exploiting the parallel nature of the
Transformer architecture.
In more detail, we employ a standard CNN backbone to extract features from the birds-eye view density map, fol-lowed by a Transformer encoder-decoder setup that con-sumes image features (supplemented with positional encod-ings) and outputs multiple ordered corner sequences, in par-allel. The floorplan is recovered by simply connecting those corners in the predicted order. Note that the described pro-cess relies on the ability to generate hierarchically struc-tured output of variable and a-priori unknown size, where each floorplan has a different number of rooms (with no nat-ural order), and each room polygon has a different number of (ordered) corners. We address this challenge by introduc-ing two-level queries with one level for the room polygons and one level for their corners. The varying numbers of both rooms and corners are accommodated by additionally classifying each query as valid or invalid. The decoder it-eratively refines the queries, through self-attention among queries and cross-attention between queries and image fea-tures. To enable end-to-end training, we propose a poly-gon matching strategy that establishes the correspondence between predictions and targets, at both room and corner levels. In this manner, we obtain an integrated model that holistically predicts a set of polygons to best explain the ev-idence in the density map, without hand-tuned intermediate rules of which corners, walls or rooms to commit to along the way. The model is also fast at inference, since it operates in single-stage feed-forward mode, without optimization or search and without any post-processing steps. Moreover, it is flexible and can, with few straight-forward modifications, predict additional semantic and structural information such as room types, doors and windows (Fig. 1).
We evaluate our model on two challenging datasets,
Structured3D [37] and SceneCAD [2]. For both of them,
RoomFormer outperforms the state of the art, while at the same time being significantly faster than existing methods.
In summary, our contributions are:
• A new formulation of floorplan reconstruction, as the simultaneous generation of multiple ordered se-quences of room corners.
• The RoomFormer model, an end-to-end trainable,
Transformer-type architecture that implements the proposed formulation via two-level queries that pre-dict a set of polygons each consisting of a sequence of vertex coordinates.
• Improved floorplan reconstruction scores on both
Structured3D [37] and SceneCAD [2], with faster in-ference times.
• Model variants able to additionally predict semantic room type labels, doors and windows. 2.