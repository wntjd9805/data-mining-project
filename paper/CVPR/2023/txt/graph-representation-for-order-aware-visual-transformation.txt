Abstract
This paper proposes a new visual reasoning formula-tion that aims at discovering changes between image pairs and their temporal orders. Recognizing scene dynamics and their chronological orders is a fundamental aspect of human cognition. The aforementioned abilities make it possible to follow step-by-step instructions, reason about and analyze events, recognize abnormal dynamics, and restore scenes to their previous states. However, it remains unclear how well current AI systems perform in these capabilities. Al-though a series of studies have focused on identifying and describing changes from image pairs, they mainly consider those changes that occur synchronously, thus neglecting po-tential orders within those changes. To address the above issue, we first propose a visual transformation graph struc-ture for conveying order-aware changes. Then, we bench-marked previous methods on our newly generated dataset and identified the issues of existing methods for change order recognition. Finally, we show a significant improvement in order-aware change recognition by introducing a new model that explicitly associates different changes and then identi-fies changes and their orders in a graph representation. 1.

Introduction
The Only Constant in Life Is Change.
- Heraclitus
Humans conduct numerous reasoning processes beyond object and motion recognition. Through these processes, we can capture a wide range of information with just a glimpse of a scenario. To achieve human-level visual understanding, various studies have recently focused on different aspects of visual reasoning, such as compositional [1–4], causal [5, 6], abstract [7–9], abductive [10, 11], and commonsense visual reasoning [12, 13]. Due to the ever-changing visual sur-rounding, perceiving and reasoning over scene dynamics are essential. However, most existing visual reasoning studies focus on scenes in fixed periods of time. Therefore, this study focuses on a new formulation of visual reasoning for identifying scene dynamics.
Figure 1. Overview of the proposed order-aware change recog-nition model VTGen (top). From an image pair observed before and after multiple synchronous and asynchronous changes, VTGen generates a visual transformation graph (bottom) where nodes in-dicate change contents (including type, object attributes, original position described by what is underneath it, and new position, and directed edges indicate temporal orders of changes.
Due to variations in the spatial positions of objects and the temporal order of human activities, changes within a pair of observations could occur simultaneously or asyn-chronously. Several recent studies have already discussed recognizing and describing synchronous changes from a pair of images via natural language texts [14–16] while neglect-ing the potential orders between changes. However, iden-tifying temporal orders is an integral aspect of revealing how scene dynamics occur in time, making it possible to restore scenes to their previous states. Temporal orders are also critical in a variety of applications, such as room rear-rangement [17], assembly operation [18,19], and instruction following [20, 21]. Change order recognition presents new challenges as it requires reasoning over underlying tempo-ral events, and it also complicates single change recognition due to entangled appearances and localization. However, despite its complexity, humans exhibit high performance in finding multiple changes and determining their temporal or-ders from a pair of images. For example, even human chil-dren under age five can perform assembly operations with toys in games like LEGO building blocks. Therefore, in this work, we are particularly interested in how well the current
AI methods perform in order-aware change recognition.
Similar to discovering dynamics and orders from a pair of scene observations, there is a group of works that discusses the step-by-step assembly of objects from their component parts [18, 22–24]. Such part assembly studies tend to focus on recovering the sequence steps for rebuilding objects and are thus highly useful in robotic applications used for as-sembly operations or instruction following. However, part assembly operations focus on the reconstruction of objects from their parts, and not finding the differences between two discrete scene observations. Moreover, instead of directly recovering all steps from two single observations, existing part assembly methods require additional information, such as language instructions or demonstration videos, for their step generation processes.
As shown in Figure 1, this study proposes a new task to identify order-aware changes directly from a pair of images.
Most existing studies generate a single sentence [14, 15], paragraph [16], or triplets [25] for describing changes. How-ever, sentences are lengthy and less suitable for simultane-ously indicating change contents and their orders, and make model analysis and evaluation opaque. Hence, we propose the use of an order-aware transformation graph (Figure 1 bottom). Change contents are represented by nodes and their chronological orders by directed edges. To diagnose model performance, we generated a dataset, named order-aware vi-sual transformation (OVT), consisting of asynchronous and synchronous changes between scene observations.
We then conducted benchmark experiments using ex-isting methods and found they showed seriously degraded performance in terms of order-aware change recognition.
Although neglected by existing methods, associations be-tween changes, and disentangled representations of change contents and orders are useful in identifying order-aware changes. Therefore, we propose a novel method called vi-sual transformation graph generator (VTGen) that explicitly associates different changes and generates a graph that de-scribes change contents and their orders in a disentangled manner. VTGen achieved state-of-the-art performance in the OVT dataset and an existing benchmark CLEVR-Multi-Change [16], and outperformed existing methods by large margins. However, we also found a significant performance gap between the best-performing model and humans. We hope our research and OVT dataset can contribute to achiev-ing human-level visual reasoning in scene dynamics.
Our contributions are three-fold: i. We propose a novel task and a dataset named OVT for order-aware visual trans-formation. ii. We report on benchmark evaluations of ex-isting change recognition methods in order-aware change recognition and discuss their shortcomings. iii. We propose a novel method VTGen that achieves state-of-the-art perfor-mance in the OVT dataset and an existing change recogni-tion benchmark. 2.