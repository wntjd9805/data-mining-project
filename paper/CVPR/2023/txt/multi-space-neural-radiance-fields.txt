Abstract
Existing Neural Radiance Fields (NeRF) methods suf-fer from the existence of reflective objects, often result-ing in blurry or distorted rendering.
Instead of calculat-ing a single radiance field, we propose a multi-space neu-ral radiance field (MS-NeRF) that represents the scene us-ing a group of feature fields in parallel sub-spaces, which leads to a better understanding of the neural network to-ward the existence of reflective and refractive objects. Our multi-space scheme works as an enhancement to existing
NeRF methods, with only small computational overheads needed for training and inferring the extra-space outputs.
We demonstrate the superiority and compatibility of our ap-proach using three representative NeRF-based models, i.e.,
NeRF, Mip-NeRF, and Mip-NeRF 360. Comparisons are performed on a novelly constructed dataset consisting of 25 synthetic scenes and 7 real captured scenes with complex reflection and refraction, all having 360-degree viewpoints.
Extensive experiments show that our approach significantly outperforms the existing single-space NeRF methods for rendering high-quality scenes concerned with complex light paths through mirror-like objects. Our code and dataset will be publicly available at https://zx-yin.github.io/msnerf. 1.

Introduction
Neural Radiance Fields (NeRF) [25] and its variants are refreshing the community of neural rendering, and the po-tential for more promising applications is still under ex-ploration. NeRF represents scenes as continuous radiance fields stored by simple Multi-layer Perceptrons (MLPs) and renders novel views by integrating the densities and radi-ance, which are queried from the MLPs by points sampled along the ray from the camera to the image plane. Since its first presentation [25], many efforts have been investigated to enhance the method, such as extending to unbounded scenes [2, 50], handling moving objects [29, 30, 37], or re-constructing from pictures in the wild [6, 21, 35, 49].
*Bo Ren is the corresponding author. (a) Mip-NeRF 360 [2], SSIM=0.825 (b) Our Model, SSIM=0.881
Figure 1. (a) Though Mip-NeRF 360 can handle unbounded scenes, it still suffers from reflective surfaces, as the virtual images violate the multi-view consistency, which is of vital importance to NeRF-based methods. (b) Our method can help conventional
NeRF-like methods learn the virtual images with little extra cost.
However, rendering scenes with mirrors is still a chal-lenging task for state-of-the-art NeRF-like methods. One of the principle assumptions for the NeRF method is the multi-view consistency property of the target scenes [16, 20, 36].
When there are mirrors in the space, if one allows the view-points to move 360-degree around the scene, there is no consistency between the front and back views of a mirror, since the mirror surface and its reflected virtual image are only visible from a small range of views. As a result, it is often required to manually label the reflective surfaces in order to avoid falling into sub-optimal convergences [12].
In this paper, we propose a novel multi-space NeRF-based method to allow the automatic handling of mirror-like objects in the 360-degree high-fidelity rendering of scenes without any manual labeling. Instead of regarding the Eu-clidean scene space as one single space, we treat it as com-posed of multiple virtual sub-spaces, whose composition changes according to location and view direction. We show that our approach using such a multi-space decomposition leads to successful handlements of complex reflections and refractions where the multi-view consistency is heavily vi-olated in the Euclidean real space. Furthermore, we show that the above benefits can be achieved by designing a low-cost multi-space module and replacing the original output layer with it. Therefore, our multi-space approach serves as
a general enhancement to the NeRF-based backbone, equip-ping most NeRF-like methods with the ability to model complex reflection and refraction, as shown in Fig. 1.
Existing datasets have not paid enough attention to the 360-degree rendering of scenes containing mirror-like ob-jects, such as RFFR [12] just has forward-facing scenes, and the Shiny dataset in [42] with small viewpoints changes and cannot exhibit view-dependent effects in large angle scale. Therefore we construct a novel dataset dedicated to evaluation for the 360-degree high-fidelity rendering of scenes containing complex reflections and refractions. In this dataset, we collect 25 synthesized scenes and 7 captured real-world scenes. Each synthesized scene consists of 120 images of 360-degree around reflective or refractive objects, with 100 randomly split for training, 10 for validation, and 10 for evaluation. Each real-world scene is captured ran-domly around scenes with reflective and refractive objects, consisting of 62 to 118 images, and organized under the convention of LLFF [24]. We then demonstrate the supe-riority and compatibility of our approach by comparisons, using three representative baseline models, i.e., NeRF [25],
Mip-NeRF [1], and Mip-NeRF 360 [2], with and without our multi-space module. Experiments show that our ap-proach improves performance by a large margin both quan-titatively and qualitatively on scenes with reflection and re-fraction. Our main contributions are as follows:
• We propose a multi-space NeRF method that auto-matically handles mirror-like objects in 360-degree high-fidelity scene rendering, achieving significant im-provements over existing representative baselines both quantitatively and qualitatively.
• We design a lightweight module that can equip most
NeRF-like methods with the ability to model reflection and refraction with small computational overheads.
• We construct a dataset dedicated to evaluation for the 360-degree high-fidelity rendering of scenes contain-ing complex reflections and refractions, including 25 synthesized scenes and 7 real captured scenes. 2.