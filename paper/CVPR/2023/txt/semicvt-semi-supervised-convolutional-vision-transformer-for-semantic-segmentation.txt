Abstract
Semi-supervised learning improves data efficiency of deep models by leveraging unlabeled samples to alleviate the reliance on a large set of labeled samples. These suc-cesses concentrate on the pixel-wise consistency by using convolutional neural networks (CNNs) but fail to address both global learning capability and class-level features for unlabeled data. Recent works raise a new trend that Trans-former achieves superior performance on the entire feature map in various tasks. In this paper, we unify the current dominant Mean-Teacher approaches by reconciling intra-model and inter-model properties for semi-supervised seg-mentation to produce a novel algorithm, SemiCVT, that absorbs the quintessence of CNNs and Transformer in a comprehensive way. Specifically, we first design a paral-lel CNN-Transformer architecture (CVT) with introducing an intra-model local-global interaction schema (LGI) in
Fourier domain for full integration. The inter-model class-wise consistency is further presented to complement the class-level statistics of CNNs and Transformer in a cross-teaching manner. Extensive empirical evidence shows that
SemiCVT yields consistent improvements over the state-of-the-art methods in two public benchmarks. 1.

Introduction
Semantic segmentation [4, 25, 41, 44] is a foundational problem in computer vision and has attracted tremendous interests for assigning pixel-level semantic labels in an im-age. Despite remarkable successes of convolutional neural network (CNN), collecting a large quantity of pixel-level annotations is quite expensive and time-consuming. Re-cently, semi-supervised learning (SSL) provides an alterna-tive way to infer labels by learning from a small number of images annotated to fully explore those unlabeled data.
The main stream of semi-supervised learning relies on
*Corresponding Authors: Lanfen Lin (llf@zju.edu.cn), Yawen Huang (yawenhuang@tencent.com).
†Huimin Huang and Shiao Xie are co-first authors, and this work is done during the internship at Tencent Jarvis Lab.
Figure 1. Visualizations of class activation maps generated by
Grad-CAM [31] and segmentation results of MT [33] and Our
SemiCVT. Current MT-based SSLs suffer from limited global de-pendency (e.g., incomplete human leg in (a), (c)) and class confu-sion (e.g., mis-classify bottle as person in (e), (g)). Our SemiCVT improves the performance from intra-model and inter-model per-spective, achieving better compactness and accurate localization. consistency regularization [13, 30, 33, 38], pseudo label-ing [29, 32], entropy minimization [3, 14] and bootstrap-ping [15]. For semantic segmentation, a typical approach is to build a Mean-Teacher (MT) model [33] (in Fig. 2 (a)), which allows the predictions generated from either teacher or student model as close as possible. However, such a clas-sic structure still suffers from two limitations: 1) Most of
MT-based frameworks are built upon stacking convolutional layers, while the dilemma of CNNs is to capture global rep-resentations in the limited receptive field [11]. It results in the neglected ability of aggregating global context and local features, as depicted in Figs. 1 (a) and (c). 2) These ap-proaches usually leverage the pixel-wise predictions from
CNNs to enforce consistency regularization with their fo-cuses on fine-level pair-wise similarity. It may fail to ex-plore rich information in feature space and also overlook the global feature distribution, as shown in Figs. 1 (e), (g).
On the other hand, Transformer [34] has achieved no-table performance on vision tasks [2, 24, 37], owing to their strong capability in multi-head self-attention for capturing long-distance dependencies. However, pure Transformer-based architectures cannot achieve satisfactory perfor-pared with the MT-based SSL, SemiCVT can attend to full object extent with in various sizes and long-range scenarios (e.g., full extent of the people’s leg in Fig. 1. (b), as well as the feature discriminability between different classes (e.g., activated small-size bottle in Fig. 1. (f)), achieving accurate segmentation shown in Fig. 1 (d) and (h).
In summary, the main contributions of this work are four-fold: (i) We analyze the intra-model and inter-model prob-lems faced by the existing CNN-based Mean-Teacher meth-ods for semi-supervised segmentation, and propose a novel scheme, named SemiCVT, to fully capitalize the unlabeled (ii) We introduce an intra-model local-global inter-data. action strategy for chaining both CNN and Transformer in the Fourier domain. (iii) We propose an inter-model class-wise consistency to learn complementary class-level statis-tics in a cross-teaching manner. (iv) Extensive experiments are performed on two public datasets, resulting in the new state-of-the-art performances consistently. 2.