Abstract
The performance of video prediction has been greatly boosted by advanced deep neural networks. However, most of the current methods suffer from large model sizes and require extra inputs, e.g., semantic/depth maps, for promis-ing performance. For efficiency consideration, in this pa-per, we propose a Dynamic Multi-scale Voxel Flow Net-work (DMVFN) to achieve better video prediction perfor-mance at lower computational costs with only RGB images, than previous methods. The core of our DMVFN is a dif-ferentiable routing module that can effectively perceive the motion scales of video frames. Once trained, our DMVFN selects adaptive sub-networks for different inputs at the in-ference stage. Experiments on several benchmarks demon-strate that our DMVFN is an order of magnitude faster than
Deep Voxel Flow [35] and surpasses the state-of-the-art iterative-based OPT [63] on generated image quality. 1.

Introduction
Video prediction aims to predict future video frames from the current ones. The task potentially benefits the study on representation learning [40] and downstream fore-casting tasks such as human motion prediction [39], au-tonomous driving [6], and climate change [48], etc. Dur-ing the last decade, video prediction has been increasingly studied in both academia and industry community [5, 7].
Video prediction is challenging because of the diverse and complex motion patterns in the wild, in which accurate motion estimation plays a crucial role [35, 37, 58]. Early methods [37, 58] along this direction mainly utilize recur-rent neural networks [19] to capture temporal motion infor-mation for video prediction. To achieve robust long-term prediction, the works of [41, 59, 62] additionally exploit the semantic or instance segmentation maps of video frames for semantically coherent motion estimation in complex scenes.
*Corresponding authors.
Figure 1. Average MS-SSIM and GFLOPs of different video prediction methods on Cityscapes [9]. The parameter amounts are provided in brackets. DMVFN outperforms previous methods in terms of image quality, parameter amount, and GFLOPs.
However, the semantic or instance maps may not always be available in practical scenarios, which limits the application scope of these video prediction methods [41,59,62]. To im-prove the prediction capability while avoiding extra inputs, the method of OPT [63] utilizes only RGB images to esti-mate the optical flow of video motions in an optimization manner with impressive performance. However, its infer-ence speed is largely bogged down mainly by the computa-tional costs of pre-trained optical flow model [54] and frame interpolation model [22] used in the iterative generation.
The motions of different objects between two adjacent frames are usually of different scales. This is especially ev-ident in high-resolution videos with meticulous details [49].
The spatial resolution is also of huge differences in real-world video prediction applications. To this end, it is es-sential yet challenging to develop a single model for multi-scale motion estimation. An early attempt is to extract
multi-scale motion cues in different receptive fields by em-ploying the encoder-decoder architecture [35], but in prac-tice it is not flexible enough to deal with complex motions.
In this paper, we propose a Dynamic Multi-scale Voxel
Flow Network (DMVFN) to explicitly model the com-plex motion cues of diverse scales between adjacent video frames by dynamic optical flow estimation. Our DMVFN is consisted of several Multi-scale Voxel Flow Blocks (MVFBs), which are stacked in a sequential manner. On top of MVFBs, a light-weight Routing Module is pro-posed to adaptively generate a routing vector according to the input frames, and to dynamically select a sub-network for efficient future frame prediction. We con-duct experiments on four benchmark datasets, including
Cityscapes [9], KITTI [12], DAVIS17 [43], and Vimeo-Test [69], to demonstrate the comprehensive advantages of our DMVFN over representative video prediction meth-ods in terms of visual quality, parameter amount, and computational efficiency measured by floating point oper-ations (FLOPs). A glimpse of comparison results by differ-ent methods is provided in Figure 1. One can see that our
DMVFN achieves much better performance in terms of ac-curacy and efficiency on the Cityscapes [9] dataset. Exten-sive ablation studies validate the effectiveness of the com-ponents in our DMVFN for video prediction.
In summary, our contributions are mainly three-fold:
• We design a light-weight DMVFN to accurately pre-dict future frames with only RGB frames as inputs.
Our DMVFN is consisted of new MVFB blocks that can model different motion scales in real-world videos.
• We propose an effective Routing Module to dynam-ically select a suitable sub-network according to the input frames. The proposed Routing Module is end-to-end trained along with our main network DMVFN.
• Experiments on four benchmarks show that our
DMVFN achieves state-of-the-art results while being an order of magnitude faster than previous methods. 2.