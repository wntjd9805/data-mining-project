Abstract
Learning an accurate entropy model is a fundamental way to remove the redundancy in point cloud compression.
Recently, the octree-based auto-regressive entropy model which adopts the self-attention mechanism to explore de-pendencies in a large-scale context is proved to be promis-ing. However, heavy global attention computations and auto-regressive contexts are inefficient for practical appli-cations. To improve the efficiency of the attention model, we propose a hierarchical attention structure that has a lin-ear complexity to the context scale and maintains the global receptive field. Furthermore, we present a grouped context structure to address the serial decoding issue caused by the auto-regression while preserving the compression perfor-mance. Experiments demonstrate that the proposed entropy model achieves superior rate-distortion performance and significant decoding latency reduction compared with the state-of-the-art large-scale auto-regressive entropy model. 1.

Introduction
Point cloud is a fundamental data structure to represent 3D scenes. It has been widely applied in 3D vision systems such as autonomous driving and immersive applications.
The large-scale point cloud typically contains millions of points [36].
It is challenging to store and transmit such massive data. Hence, efficient point cloud compression that reduces memory footprints and transmission bandwidth is necessary to develop practical point cloud applications.
Recently, deep learning methods have promoted the de-velopment of point cloud compression [4, 9, 10, 16, 31, 36, 40,43]. It is a common pipeline to learn an octree-based en-tropy model to estimate octree node symbol (i.e., occupancy symbol) distributions. The point cloud is first organized as an octree, and occupancy symbols are then encoded into the bitstream losslessly by an entropy coder (e.g., arithmetic
*Corresponding author.
Figure 1. Bitrate and decoding speed in the log scale for lossless compression on 16-bit quantized SemanticKITTI. The proposed method EHEM yields state-of-the-art performance with a compa-rable decoding latency to the efficient traditional method G-PCC. coder [47]). An accurate entropy model is required since it reduces the cross entropy between the estimated distribution and ground truth, which is corresponding to actual bitrates.
Various attempts have been made to improve the accuracy by designing different context structures [4, 10, 16, 37]. The key of these advances is to increase the context capacity and introduce references from high-resolution octree represen-tations. For example, the context in OctAttention [10] in-cludes hundreds of previously decoded siblings (i.e., nodes at the same octree level). The large-scale context incorpo-rates more references for the entropy coder, and the high-resolution context preserves detailed features of the point cloud. Both of them contribute to building informative con-texts and effective entropy models.
However, large-scale context requires heavy computa-tions to model dependencies among numerous references.
The previous work [10] uses the global self-attention mech-anism to model long-range dependencies within the con-text, and its complexity is quadratic to the length of the
large-scale context. Furthermore, considering the efficiency issue, it is infeasible to build a deeper entropy model or extend the context scale to enhance the modeling capabil-ity based on global attention. Another concern is the serial decoding process caused by the inclusion of previously de-coded siblings. This auto-regressive context structure incurs a practically unacceptable decoding latency (e.g., around 700 seconds per frame).
To address these issues, we build an entropy model with an efficient hierarchical attention model and a parallel-friendly grouped context structure. The hierarchical atten-tion model partitions the context into local windows and computes attention within these windows independently.
Therefore, the complexity is linear to the context scale, which allows the further extension of network depth and context capacity to improve performance. Since the recep-tive field of the localized attention is limited, we adopt a multi-scale network structure to query features across dif-ferent windows. The context is progressively downsam-pled by merging neighboring nodes to generate a new to-ken. Then, cross-window dependencies can be captured by incorporating these new tokens in the same window. The grouped context divides the occupancy symbol sequence into two groups. Each group is conditioned on ancestral features and previously decoded groups, and hence nodes in the same group can be coded in parallel. Furthermore, in contrast to the previous auto-regressive context that only ex-ploits causal parts of the ancestral context [10], the grouped context allows to make use of a complete ancestral context.
The proposed efficient hierarchical entropy model called
EHEM is evaluated on SemanticKITTI [3] and Ford [32] benchmarks. It surpasses state-of-the-art methods in terms of both compression performance and efficiency. Contribu-tions of this work can be summarized from the following perspectives:
• We propose a hierarchical attention model, which yields improved compression performance by extend-ing model and context while keeping the efficiency.
• We design a grouped context structure that enables par-allel decoding. It adapts the entropy model using high-resolution references to practical applications.
• The proposed method achieves state-of-the-art RD per-formance with a practically applicable coding speed. 2.