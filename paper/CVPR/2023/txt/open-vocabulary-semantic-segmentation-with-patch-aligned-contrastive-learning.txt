Abstract
We introduce Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP’s con-trastive loss, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. With such an alignment, a model can identify regions of an image corresponding to a given text input, and therefore transfer seamlessly to the task of open vocabulary semantic segmentation without requiring any segmentation annotations during training. Using pre-trained CLIP en-coders with PACL, we are able to set the state-of-the-art on the task of open vocabulary zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC, Pascal
Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also applicable to image-level predictions and when used with a CLIP backbone, provides a general im-provement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets. 1.

Introduction
Understanding the semantic content in visual scenes has been one of the most important problems studied in com-puter vision at various levels of granularity. Work on this problem has led to significant improvements along several threads including image level predictions like image classi-fication [13, 54, 58], object level predictions like object de-tection [33, 51, 53, 59–61], as well as pixel level predictions like semantic segmentation [10,29,51,53]. Although in im-age classification we require only a single label per image for prediction, for scene understanding at a higher level of granularity like segmentation, supervised training requires annotations at a pixel level. Such annotations require sig-nificant human effort and are often very expensive to ob-tain. This impedes training on a large scale with millions of
*Corresponding author.
Figure 1. High level overview of our model. We train an align-ment between the patch level embeddings from the image encoder and the CLS embedding from the text encoder. This alignment can then be used to perform open-vocabulary semantic segmenta-tion in a zero-shot manner. images.
One way to tackle this problem could be to train mod-els in an unsupervised manner without requiring any seg-mentation annotations. The best methods [11, 19] in this category exploit the similarity between internal representa-tions of self-supervised image encoders [5]. This similarity is then used to identify and cluster similar regions of the image as segmentations. These models however are signif-icantly outperformed by their fully supervised counterparts on most segmentation benchmarks.
Recent improvements in multi-modal foundation mod-els has led to the possibility of training on very large scale datasets scraped off the internet [41]. These datasets contain pairs of images and their corresponding natural language text descriptions. Models like CLIP [41], ALIGN [23],
Florence [59] and CoCa [58] trained on such large internet scale datasets have been shown to transfer very well to sev-eral downstream tasks. Furthermore, having been trained on natural language textual descriptions, these models are often expected to recognize a wide variety of real-world vi-sual concepts which can be expressed in natural language, a setting better known as open vocabulary prediction.
The natural question then is whether these multi-modal models can be used for pixel level predictions, i.e., semantic
segmentation in the open vocabulary setting. Prior works on this topic [17, 28, 32, 56, 57] show that this is indeed possible. However, 3 of these works use either fully su-pervised segmentation annotations [32], class-agnostic seg-mentation masks [17] or a region proposal model trained using segmentation annotations [57], thereby being lim-ited by the availability of expensive segmentation anno-tations/masks. To the best of our knowledge, only two models: ViL-Seg [32] and GroupViT [56] perform the task of open-vocabulary semantic segmentation while be-ing trained solely on image-text data. Among these two, the better performer, GroupViT, defines a modified vision transformer (ViT) [15] architecture to naturally find seman-tic clusters within an image. Due to a different architecture, their model has to be trained end-to-end from scratch and cannot leverage pre-trained vision encoders.
In this work, we tackle the problem of open-vocabulary semantic segmentation without using any segmentation an-notations or masks, with a model purely trained on image-text data. We start with the observation in [19] that self-supervised ViT models like DINO [5], have similar patch representations for semantically similar regions of an im-age. We find this observation to be true for CLIP’s ViT based vision encoders as well. However, we also find that
CLIP does not exhibit a patch level alignment between its vision and text encoders, primarily owing to the fact that its contrastive loss only aligns the CLS image and text tokens.
Inspired from previous work on contrastive learning for weakly supervised phrase grounding [18], we define a new compatibility function for contrastive loss to train an align-ment between the patch tokens of the vision encoder and the CLS token of the text encoder. In particular, we take the cosine similarity between the text CLS token and the vision patch tokens and use these similarities as weights to com-pute a weighted sum over vision tokens. The final compat-ibility function is then simply the cosine similarity between the weighted sum of the vision patch tokens thus obtained and the CLS text token. We find that models trained on our
Patch Aligned Contrastive Learning loss indeed exhibit the desired patch level fine-grained alignment. Thus, at infer-ence time, the compatibility function can be used to make image level predictions and the patch level alignment can be used for zero-shot transfer to semantic segmentation. A high level overview of our model is shown in Fig. 1.
Note that unlike GroupViT, our PACL method is more flexible and general and can be used with any pre-trained
ViT based encoders as well. We evaluate PACL with a pre-trained CLIP encoder on the task of zero-shot semantic segmentation using 4 different datasets: Pascal VOC [16],
Pascal Context [36], COCO Stuff [4] and ADE20K [63].
On all 4 datasets, PACL consistently beats previous base-lines [17, 28, 32, 56], even the ones which use segmentation annotations or segmentation masks for training. We also find that PACL trained on top of a CLIP backbone leads to a general improvement in zero-shot classification perfor-mance across a suite of 12 image classification datasets.
In a nutshell, our contributions are as follows. Firstly, we propose Patch Aligned Contrastive Learning (PACL), a modified compatibility function for contrastive loss in or-der to train an alignment between the patch representations of a ViT based vision encoder and the CLS text represen-tation of a text encoder. We show that this alignment can be used to find regions within an image corresponding to a given text input and hence, can be used for zero-shot trans-fer to open-vocabulary semantic segmentation. Secondly, we show that PACL with a pre-trained CLIP encoder ob-tains state-of-the-art scores on zero-shot semantic segmen-tation across 4 different segmentation benchmarks: Pascal
VOC, Pascal Context, COCO Stuff and ADE20K. Finally,
PACL with a CLIP backbone also shows a general improve-ment in performance on zero-shot classification tasks across 12 different image classification datasets. 2.