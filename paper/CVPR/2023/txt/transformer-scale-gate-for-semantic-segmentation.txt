Abstract
Effectively encoding multi-scale contextual information is crucial for accurate semantic segmentation. Most of the existing transformer-based segmentation models combine features across scales without any selection, where features on sub-optimal scales may degrade segmentation outcomes.
Leveraging from the inherent properties of Vision Trans-formers, we propose a simple yet effective module, Trans-former Scale Gate (TSG), to optimally combine multi-scale features. TSG exploits cues in self and cross attentions in
Vision Transformers for the scale selection. TSG is a highly flexible plug-and-play module, and can easily be incorpo-rated with any encoder-decoder-based hierarchical vision
Transformer. Extensive experiments on the Pascal Context,
ADE20K and Cityscapes datasets demonstrate that the pro-posed feature selection strategy achieves consistent gains. 1.

Introduction
Semantic segmentation aims to segment all objects in-cluding ‘things’ and ‘stuff’ in an image and determine their categories. It is a challenging task in computer vision, and serves as a foundation for many higher-level tasks, such as scene understanding [15,34], object recognition [23,29] and vision+language [30, 33].
In recent years, Vision Trans-formers based on encoder-decoder architectures have be-come a new paradigm for semantic segmentation. The en-coder consists of a series of multi-head self-attention mod-ules to capture features of image patches, while the decoder has both self- and cross-attention modules to generate seg-mentation masks. Earlier works [47] usually use Vision
Transformers designed for image classification to tackle se-mantic segmentation, and only encode single-scale features.
However, different from image classification where we only need to recognize one object in an image, semantic segmen-tation is generally expected to extract multiple objects of different sizes. It is hard to segment and recognize these varying sized objects by only single-scale features.
Some methods [20, 37] attempt to leverage multi-scale features to solve this problem. They first use hierarchical transformers such as Swin Transformer [20] and PVT [37] to extract multi-scale image features, and then combine them, e.g. by the pyramid pooling module (PPM) [46] or the seminal feature pyramid network (FPN) [18] borrowed from CNNs. We argue that such feature combinations can-not effectively select an appropriate scale for each image patch. Features on sub-optimal scales may impact the seg-mentation performance.To address this issue, CNN-based methods [3, 8, 14, 32] design learnable models to select the optimal scales. Nevertheless, these models are complex, ei-ther use complex mechanisms [3, 8, 14] and/or require scale labels [32], which decrease the network efficiency and may cause over-fitting.
In this paper, we exploit the inherent characteristics of
Vision Transformers to guide the feature selection process.
Specifically, our design is inspired from the following ob-(1) As shown in Fig. 1(a), the self-attention servations: module in the transformer encoder learns the correlations
If an image patch is correlated to among image patches. many patches, small-scale (low-resolution) features may be preferred for segmenting this patch, since small-scale fea-tures have large effective receptive fields [42] to involve as many of these patches as possible, and vice versa. (2) In the transformer decoder, the cross-attention module mod-els correlations between patch-query pairs, as shown in
Fig. 1(b), where the queries are object categories.
If an image patch is correlated to multiple queries, it indicates that this patch may contain multiple objects and need large-scale (high-resolution) features for fine-grained segmenta-tion. On the contrary, an image patch correlated to only a few queries may need small-scale (low-resolution) features (3) The above observations to avoid over-segmentations. can guide design choices for not only category-query-based decoders but also object-query-based decoders. In object-query-based decoders, each query token corresponds to an object instance. Then, the cross-attention module extracts relationships between patch-object pairs. Many high cross-attention values also indicate that the image patch may con-Figure 1. Illustration of Vision Transformers for semantic segmentation. (a) The image is divided into multiple patches and input into the encoder. The encoder contains Lenc blocks and outputs features for every image patch. (b) The decoder takes learnable query tokens as inputs, where each query is corresponding to an object category. The decoder with Ldec blocks outputs query embeddings. Finally, segmentation results are generated by multiplying the image patch features and the query embeddings. In this paper, we exploit inner properties of these attention maps for multi-scale feature selection. tain multiple objects and require high-resolution features.
Note that these are general observations, which do not mean that these are the only relationships between transformer correlation maps and feature scales. which leverage our TSG to improve the semantic segmen-tation performance. (3) Our extensive experiments and ab-lations show that the proposed modules obtain significant improvements on three semantic segmentation datasets.
From these observations and analyses, we propose a novel Transformer Scale Gate (TSG) module that takes cor-relation maps in self- and cross-attention modules as inputs, and predicts weights of multi-scale features for each image patch. Our TSG is a simple design, with a few lightweight linear layers, and thus can be plug-and-play across diverse architectures. We further extend TSG to a TSGE module and a TSGD module, which leverage our TSG weights to optimize multi-scale features in transformer encoders and decoders, respectively. TSGE employs a pyramid struc-ture to refine multi-scale features in the encoder by the self-attention guidance, while TSGD fuses these features in the decoder based on the cross-attention guidance. Experi-mental results on three datasets, Pascal Context, ADE20K,
Cityscapes show that the proposed modules consistently achieve gains, up to 4.3% in terms of mIoU, compared with
Swin Transformer based baseline [20].
Our main contributions can be summarized as follows: (1) To the best of our knowledge, this is the first work to exploit inner properties of transformer attention maps for multi-scale feature selection in semantic segmentation. We analyze the properties of Vision Transformers and design
TSG for the selection. (2) We propose TSGE and TSGD in the encoder and decoder in transformers, respectively, 2.