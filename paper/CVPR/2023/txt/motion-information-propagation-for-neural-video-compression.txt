Abstract
In most existing neural video codecs, the information flow therein is uni-directional, where only motion coding provides motion vectors for frame coding. In this paper, we argue that, through information interactions, the synergy between motion coding and frame coding can be achieved.
We effectively introduce bi-directional information interac-tions between motion coding and frame coding via our Mo-tion Information Propagation. When generating the tempo-ral contexts for frame coding, the high-dimension motion feature from the motion decoder serves as motion guidance to mitigate the alignment errors. Meanwhile, besides as-sisting frame coding at the current time step, the feature from context generation will be propagated as motion con-dition when coding the subsequent motion latent. Through the cycle of such interactions, feature propagation on mo-tion coding is built, strengthening the capacity of exploiting long-range temporal correlation. In addition, we propose hybrid context generation to exploit the multi-scale context features and provide better motion condition. Experiments show that our method can achieve 12.9% bit rate saving over the previous SOTA neural video codec. 1.

Introduction
How to better utilize the temporal correlation across video frames is one of the core problems throughout the line of video compression research, for both traditional video codec and neural video codec.
In traditional video codec, such as H.264 [41] and
HEVC [36], temporal correlation is captured by rule-based motion-compensated prediction. It is assumed that the pix-els within a block share the same motion. For the cur-rent coding block, the best motion vector corresponds to the matching block that results in the least bit cost.
It is searched and then is used for motion compensation. Such motion-compensated prediction is usually a local optimiza-tion limited to two frames. The correlation across multiple
*This work was done when Linfeng Qi was an intern at Microsoft Re-search Asia.
Figure 1. Compared with previous methods, besides the propaga-tion for frame coding, our method: 1. introduces bi-directional in-formation interactions between motion coding and frame coding. 2. enables propagation for motion coding through the interactions. frames is neglected, which limits the capacity to utilize the temporal characteristics of videos.
With the flourish of deep learning, it is possible for neural video codec [10, 12–14, 18, 25, 26, 45] to access pixel-wise optical flow and long-range temporal informa-tion. Typically, the entire coding process can be roughly divided into three steps: motion coding, context genera-tion, and frame coding. The motion is firstly estimated and transmitted. Then the temporal context (prediction is also a kind of context in residual coding framework) is obtained through motion compensation, where handcrafted block-wise matching is replaced by optical flow based pixel-wise warping. Finally, with the assistance of temporal context, the frame is encoded and decoded. Frame coding draws more attention in most existing methods.
As shown in Figure 1, in the previous methods, the in-formation flow therein is uni-directional, where only mo-tion coding provides motion vectors for frame coding. By contrast, we emphasize the synergy between motion coding and frame coding. As far as we know, we are the first to in-troduce effective bi-directional information interactions be-tween motion coding and frame coding. Through the cycle of such interactions, feature propagation on motion coding
(a) VCT [29] (b) DCVC [18] (c) DCVC-HEM [19] (d) Our DCVC-MIP
Figure 2. Comparison for several recent methods. MIP stands for motion information propagation. is enabled to exploit long-range motion information.
Figure 2 provides a more detailed comparison for several recent methods. VCT [29] uses transformers [38] to capture the contexts among latent representations. It is free from motion coding and beyond our discussion. DCVC [18] uses the motion to extract context from the previously decoded frame. Its following work DCVC-TCM [35] and DCVC-HEM [19] further use feature propagation to improve frame coding. However, in these existing works, there are no ef-fective information interactions between motion coding and frame coding. Furthermore, propagation mechanism which captures long-range temporal correlation is also neglected for motion coding.
Thus, based on previous SOTA DCVC-HEM [19], we build our DCVC-MIP framework, which introduces bi-directional information interactions via proposed Motion
Information Propagation. When generating the temporal contexts for frame coding, the information is passed from motion coding to context generation via motion guidance, which aims at mitigating the alignment errors. Meanwhile, besides the feature propagation between context generation and frame coding, information is also passed from context generation to motion coding, and it is as motion condition to help reduce the bit cost for coding the subsequent motion latent. The bi-directional information flow can be described as follows: motion coding → context generation → frame coding, and frame coding → context generation → mo-tion coding. Through the cycle of such interactions, feature propagation on motion coding is implicitly built, strength-ening the capacity of exploiting long-range temporal corre-lation. We emphasize the importance of introducing Mo-tion Information Propagation as follows: 1. Although the bit cost for motion coding is less than frame coding, it is still not neglectable. Motion Information Propagation can implicitly capture long-range motion information, helping save the bit cost for motion coding. 2. By introducing the information interactions, the synergy between motion cod-ing and frame coding could be achieved, leading to mutual improvement.
Another advance of DCVC-MIP is the hybrid context generation. There are various types of motion in the video, from simple camera motion to sophisticated human activi-ties. In order to handle such motion information, the capac-ity of learning hybrid temporal context is important. Thus, we propose our hybrid context generation module as fol-lows: for the high-resolution context feature, we adopt off-set diversity [6], which predicts multiple offsets and masks to get more accurate feature alignment and enhance the re-construction details. The inner feature, which is used for generating offsets and masks, contains rich motion infor-mation and is suitable to be propagated as motion condi-tion; for the low-resolution context feature, we introduce transformer based context refinement, which enlarges the receptive field and captures better correlation to refine the context. Our contributions can be summarized as follows:
• We are the first to introduce bi-directional information interactions between motion coding and frame coding, which assist to achieve the synergy between them.
• Our Motion Information Propagation not only miti-gates the alignment errors, but also saves the bit cost for motion coding. With the propagated features, long-range motion information is implicitly utilized, leading to a better rate distortion trade-off.
• We propose a hybrid context generation module, which not only strengthens the multi-scale context feature mining, but also provides better motion condition for
Motion Information Propagation.
• Compared with the previous SOTA neural video codec,
our method achieves 12.9% bit rate saving, which demonstrates the effectiveness of our method. 2.