Abstract reconstruct
The popular VQ-VAE models images through learning a discrete codebook but suffer from a sig-nificant issue in the rapid quality degradation of image re-construction as the compression rate rises. One major rea-son is that a higher compression rate induces more loss of visual signals on the higher frequency spectrum which re-flect the details on pixel space. In this paper, a Frequency
Complement Module (FCM) architecture is proposed to capture the missing frequency information for enhancing reconstruction quality. The FCM can be easily incorpo-rated into the VQ-VAE structure, and we refer to the new model as Frequancy Augmented VAE (FA-VAE). In ad-dition, a Dynamic Spectrum Loss (DSL) is introduced to guide the FCMs to balance between various frequencies dynamically for optimal reconstruction. FA-VAE is further extended to the text-to-image synthesis task, and a Cross-attention Autoregressive Transformer (CAT) is proposed to obtain more precise semantic attributes in texts. Extensive reconstruction experiments with different compression rates are conducted on several benchmark datasets, and the re-sults demonstrate that the proposed FA-VAE is able to re-store more faithfully the details compared to SOTA meth-ods. CAT also shows improved generation quality with bet-ter image-text semantic alignment. 1.

Introduction
VQ-VAE models [6,11,25,29,39,46] reconstruct images through learning a discrete codebook of latent embeddings.
They gained wide popularity due to the scalable and versa-tile codebook, which can be broadly applied to many visual tasks such as image synthesis [11,49] and inpainting [4,31].
A higher compression rate is typically preferable in VQ-Figure 1. Images and their frequency maps. Row 1: original and reconstructed images. Row 2: the frequency maps of images, fre-quency increases in any direction away from the center. f is the compression rate. With a greater compression rate, more details are lost during reconstruction, i.e. eyes and mouth shape, and hair texture (pointed with red arrows) which align with the loss of high-frequency features. All frequency figures in this paper use the same colormap. rFID [14] and lpips [16] are lower the bet-ter, and frequency values increase from red to green, zoom in for better visualization.
VAE models since it provides memory efficiency and better learning of coherent semantics structures [11, 40].
One main challenge quickly arises for a higher com-pression rate, which severely compromises reconstruction accuracy. Figure 1 row 1 shows that although the recon-structed images at higher compression rates appear consis-tent with the original image, details inconsistencies such as the color and contour of the lips become apparent upon closer scrutiny. Figure 1 row 2 reveals that similar degra-dation also manifests on the frequency domains where fea-tures towards the middle and higher frequency spectrum are the least recoverable with greater compression rate.
Several causes stand behind this gap between pixel and frequency space. The convolutional nature of autoen-coders is prone to spectral bias, which favors learning low-frequency features [22, 36]. This challenge is further ag-gravated when current methods exclusively design losses or improve model architecture for better semantics resem-blance [11,16,25] but often neglect the alignment on the fre-quency domain [12,15]. On top of that, it is intuitively more challenging for a decoder to reconstruct an image patch from a single codebook embedding (high compression) than multiple embeddings (less compression). The reason is that the former mixes up features of incomplete and diverse fre-quencies, while the latter could preserve more fine-grained and complete features at various frequencies.
Inspired by these insights, the Frequency Augmented
VAE (FA-VAE) model is proposed, which aims to improve reconstruction quality by achieving better alignment on the frequency spectrums between the original and reconstructed images. More specifically, new modules named Frequency
Complement Modules (FCM) are crafted and embedded at multiple layers of FA-VAE’s decoder to learn to comple-ment the decoder’s features with missing frequencies.
We observe that valuable middle and high frequencies are mingled with the encoder’s feature maps during the compression via an encoder, shown in Figure 3 row 4.
Therefore, a new loss termed Spectrum Loss (SL) is pro-posed to guide FCMs to generate missing features that align with the same level’s encoder features on the fre-quency domain. Since most image semantics reside on the low-frequency spectrum [48], SL prioritizes learning lower-frequency features with diminishing weights as frequencies increase.
Interestingly, we discover that checkerboard patterns ap-pear in the complemented decoder’s features with SL, al-though better reconstruction performance is achieved (Fig-ure 3 column 4). We speculate that because SL sets a de-terministic range for the low-frequency spectrum when ap-plying weights on the frequencies without considering that the importance of a frequency can vary from layer to layer.
Thus, an improved loss function Dynamic Spectrum Loss (DSL) is crafted on top of SL with a learnable component to adjust the range of low-frequency spectrum dynamically for optimal reconstruction. DSL can improve reconstruction quality even further than SL without the unnatural checker-board artifacts in the features (Figure 3 column 5).
We further extend FA-VAE to the text-to-image gener-ation task and propose the Cross-attention Autoregressive
Transformer (CAT) model. We first observe that only using one or a few token embeddings is a coarse representation of lengthy texts [8, 27, 33]. Thus CAT uses all token embed-dings as a condition for more precise guidance. Moreover, existing works typically use self-attention, and the text con-dition is embedded merely at the beginning of the genera-tion [11, 49]. This mechanism becomes problematic in the autoregressive generation because one image token is gen-erated at a time, thus the text condition gradually loosens its connection with the generated tokens. To circumvent this issue, CAT embeds a cross-attention mechanism that allows the text condition to guide each step generation.
To summarize, our work includes the following contri-butions:
• We propose a new type of architecture called Fre-quency Augmented VAE (FA-VAE) for improving im-age reconstruction through achieving more accurate details reconstruction.
• We propose a new loss called Spectrum Loss (SL) and its enhanced version Dynamic Spectrum Loss (D
SL), which guides the Frequency Complement Mod-ules (FCM) in FA-VAE to adaptively learn different low/high frequency mixtures for optimal reconstruc-tion.
• We propose a new Cross-attention Autoregressive
Transformer (CAT) for text-to-image generation using more fine-grained textual embeddings as a condition with a cross-attention mechanism for better image-text semantic alignment. 2.