Abstract
We introduce a new family of deep neural networks, where instead of the conventional representation of net-work layers as N -dimensional weight tensors, we use a continuous layer representation along the filter and chan-nel dimensions. We call such networks Integral Neural Net-works (INNs). In particular, the weights of INNs are rep-resented as continuous functions defined on N -dimensional hypercubes, and the discrete transformations of inputs to the layers are replaced by continuous integration opera-tions, accordingly. During the inference stage, our con-tinuous layers can be converted into the traditional tensor representation via numerical integral quadratures. Such kind of representation allows the discretization of a net-work to an arbitrary size with various discretization in-tervals for the integral kernels. This approach can be ap-plied to prune the model directly on an edge device while suffering only a small performance loss at high rates of structural pruning without any fine-tuning. To evaluate the practical benefits of our proposed approach, we have con-ducted experiments using various neural network architec-tures on multiple tasks. Our reported results show that the proposed INNs achieve the same performance with their conventional discrete counterparts, while being able to pre-serve approximately the same performance (2% accuracy loss for ResNet18 on Imagenet) at a high rate (up to 30%) of structural pruning without fine-tuning, compared to 65% accuracy loss of the conventional pruning methods under the same conditions. Code is available at gitee. 1.

Introduction
Recently, deep neural networks (DNNs) have achieved impressive breakthroughs in a wide range of practical ap-plications in both computer vision [13, 20, 32] and natural
*The authors contributed equally to this work.
â€ Currently affiliated with Garch Lab. language processing [7] tasks. This state-of-the-art perfor-mance is mainly attributed to the huge representation ca-pacity [2] of DNNs. According to the Kolmogorov su-perposition theorem [14] and the universal approximation theorem [29], a DNN is capable of approximating uni-formly any continuous multivariate function with appropri-ate weights. To achieve better performance, a large num-ber of parameters and computations are assigned to the
DNN [10, 40], which seriously limits its application on memory- and computation-constrained devices. Hence, nu-merous approaches have been proposed to compress and ac-celerate neural networks, including pruning [26, 38], quan-tization [35, 41] and neural architecture search [34, 37].
DNNs are particularly successful in dealing with chal-lenging transformations of natural signals such as images or audio signals [27]. Since such analogue signals are in-evitably discretized, neural networks conventionally per-form discrete representations and transformations, such as matrix multiplications and discrete convolutions. However, for such kind of representations the size of neural networks cannot be adjusted without suffering severe performance degradation during the inference stage, once the training procedure is completed. Although several network prun-ing methods [26, 38] have been proposed to extract crucial channels from the trained model and generate efficient mod-els, they either suffer from a significant accuracy degrada-tion or require to fine-tune the model on the whole train-ing database. Along with the development of hardware, there have been diverse edge devices with various capacities for memory and computation, from ordinary processors to dedicated neural network accelerators. The model size for different devices varies significantly [4]. Moreover, many tasks (e.g. autonomous driving) require different response speeds on the same hardware according to various scenar-ios or conditions (e.g. driving speed and weather condition).
The conventional way to deal with such problems is to de-sign multiple model architectures for all possible scenarios and store them together. However, the downside of such
strategy is that it requires huge resources for training and memory space for storage. Hence, it is crucial to design neural networks that feature a self-resizing ability during inference, while preserving the same level of performance.
Inspired by the inherently continuous nature of the in-put signals, we challenge the discrete representation of neu-ral networks by exploring a continuous representation along the filters and channel dimensions. This leads to a new class of networks which we refer to as Integral Neural Networks (INNs). INNs employ the high-dimensional hypercube to present the weights of one layer as a continuous surface.
Then, we define integral operators analogous to the conven-tional discrete operators in neural networks. INNs can be converted into the conventional tensor representation by nu-merical integration quadratures for the forward pass. At the inference stage, it is convenient to discretize such networks into arbitrary size with various discretization intervals of the integral kernels. Since the representation is composed of in-tegral operators, discretizing the continuous networks could be considered as the numerical quadrature approximation procedure [9]. The estimated values with various discretiza-tion intervals are close to the integral value when the inter-val is small enough. Hence, when we discretize an INN with different intervals to generate networks of various sizes, it is capable of preserving the original performance to some extent without the need of additional fine-tuning. Such kind of representation of neural networks can play a crucial role in dealing with the important problem of efficient network deployment in diverse conditions and hardware setups.
To evaluate the performance of INNs, extensive exper-iments were conducted on image classification and super-resolution tasks. The results show that the proposed contin-uous INNs achieve the same performance with their discrete
DNN counterparts, when the training procedure is finished.
Moreover, such kind of networks approximately preserve the performance at a high rate of structural pruning without the aid of additional fine-tuning. 2.