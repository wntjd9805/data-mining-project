Abstract
Human-centric perceptions (e.g., pose estimation, hu-man parsing, pedestrian detection, person re-identification, etc.) play a key role in industrial applications of visual mod-els. While specific human-centric tasks have their own rel-evant semantic aspect to focus on, they also share the same underlying semantic structure of the human body. However, few works have attempted to exploit such homogeneity and design a general-propose model for human-centric tasks.
In this work, we revisit a broad range of human-centric tasks and unify them in a minimalist manner. We propose
UniHCP, a Unified Model for Human-Centric Perceptions, which unifies a wide range of human-centric tasks in a sim-plified end-to-end manner with the plain vision transformer architecture. With large-scale joint training on 33 human-centric datasets, UniHCP can outperform strong baselines on several in-domain and downstream tasks by direct eval-uation. When adapted to a specific task, UniHCP achieves new SOTAs on a wide range of human-centric tasks, e.g., 69.8 mIoU on CIHP for human parsing, 86.18 mA on PA-100K for attribute prediction, 90.3 mAP on Market1501 for
ReID, and 85.8 JI on CrowdHuman for pedestrian detec-tion, performing better than specialized models tailored for each task. The code and pretrained model are available at https://github.com/OpenGVLab/UniHCP. 1.

Introduction
Research on human-centric perceptions has come a long way with tremendous advancements in recent years. Many methods have been developed to enhance the performance of pose estimation [9, 25, 60, 91], pedestrian detection [4, 62, 63, 76], person re-identification [42, 86, 101] (ReID), and many other human-centered tasks. These significant progress play a key role in advancing the applications of vi-*Equal contribution.
†Corresponding author.
Figure 1. UniHCP unifies 5 human-centric tasks under one model and is trained on a massive collection of human-centric datasets. sual models in numerous fields, such as sports analysis [11], autonomous driving [97], and electronic retailing [27].
Although different human-centric perception tasks have their own relevant semantic information to focus on, those semantics all rely on the same basic structure of the human body and the attributes of each body part [64, 81]. In light of this, there have been some attempts trying to exploit such homogeneity and train a shared neural network jointly with distinct human-centric tasks [28,29,46,48,61,71,77,87,98].
For instance, human parsing has been trained in conjunc-tion with human keypoint detection [46, 61, 98], pedestrian attribute recognition [87], pedestrian detection [48] or per-son re-identification [28]. The experimental results of these works empirically validate that some human-centric tasks may benefit each other when trained together. Motivated by these works, a natural expectation is that a more ver-satile all-in-one model could be a feasible solution for gen-eral human-centric perceptions, which can utilize the homo-geneity of human-centric tasks for improving performance, enable fast adaption to new tasks, and decrease the burden of memory cost in large-scale multitask system deployment compared with specific models to specific tasks.
However, unifying distinct human-centric tasks into a
general model is challenging considering the data diversity and output structures. From the data’s perspective, images in different human-centric tasks and different datasets have different resolutions and characteristics (e.g., day and night, indoor and outdoor), which calls for a robust representative network with the capability to accommodate them. From the perspective of output, the annotations and expected out-puts of different human-centric tasks have distinct structures and granularities. Although this challenge can be bypassed via deploying separate output heads for each task/dataset, it is not scalable when the number of tasks and datasets is large.
In this work, we aim to explore a simple, scalable for-mulation for unified human-centric system and, for the first time, propose a Unified model for Human-Centric
Perceptions (UniHCP). As shown in Figure.1, UniHCP uni-fies and simultaneously handles five distinct human-centric tasks, namely, pose estimation, semantic part segmentation, pedestrian detection, ReID, and person attribute recogni-tion. Motivated by the extraordinary capacity and flexibil-ity of the vision transformers [43, 94], a simple yet unified encoder-decoder architecture with the plain vision trans-former is employed to handle the input diversity, which works in a simple feedforward and end-to-end manner, and can be shared across all human-centric tasks and datasets to extract general human-centric knowledge. To gener-ate the output for different tasks with the unified model,
UniHCP defines Task-specific Queries, which are shared among all datasets with the same task definition and inter-preted into different output units through a Task-guided In-terpreter shared across different datasets and tasks. With task-specific queries and the versatile interpreter, UniHCP avoids the widely used task-specific output heads, which minimizes task-specific parameters for knowledge sharing and make backbone-encoded features reusable across tasks.
Own to these designs, UniHCP is suitable and easy to perform multitask pretraining at scale. To this end, we pre-trained an UniHCP model on a massive collection of 33 labeled human-centric datasets. By harnessing the abun-dant supervision signals of each task, we show such a model can simultaneously handle these in-pretrain tasks well with competitive performance compared to strong baselines rely-ing on specialized architectures. When adapted to a specific task, both in-domain and downstream, our model achieves new SOTAs on several human-centric task benchmarks. In summary, the proposed model has the following properties:
■ Unifying five distinct human-centric tasks and han-dling them simultaneously.
■ Shared encoder-decoder network based on plain trans-former.
■ Simple task-specific queries identifying the outputs.
■ Maximum weight sharing (99.97% shared parameters) with a task-guided interpreter.
■ Trainable at scale and demonstrates competitive per-formance compared to task-specialized models. 2.