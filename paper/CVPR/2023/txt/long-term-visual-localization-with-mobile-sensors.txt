Abstract
Despite the remarkable advances in image matching and pose estimation, image-based localization of a camera in a temporally-varying outdoor environment is still a chal-lenging problem due to huge appearance disparity between query and reference images caused by illumination, seasonal and structural changes. In this work, we propose to leverage additional sensors on a mobile phone, mainly GPS, compass, and gravity sensor, to solve this challenging problem. We show that these mobile sensors provide decent initial poses and effective constraints to reduce the searching space in image matching and final pose estimation. With the initial pose, we are also able to devise a direct 2D-3D matching network to efficiently establish 2D-3D correspondences in-stead of tedious 2D-2D matching in existing systems. As no public dataset exists for the studied problem, we collect a new dataset that provides a variety of mobile sensor data and significant scene appearance variations, and develop a system to acquire ground-truth poses for query images. We benchmark our method as well as several state-of-the-art baselines and demonstrate the effectiveness of the proposed approach. Our code and dataset are available on the project page: https://zju3dv.github.io/sensloc/. 1.

Introduction
Visual localization aims at estimating the camera trans-lation and orientation for a given image relative to a known scene. Solving this problem is crucial for many applications such as autonomous driving [11], robot navigation [37] and augmented and virtual reality [7, 41].
State-of-the-art approaches to visual localization typi-cally involve matching 3D points in a pre-built map and 2D pixels in a query image [4, 22, 42, 50–52, 67, 69]. An intermediate image retrieval step [2, 21, 23, 46] is often ap-plied to determine which parts of the scene are likely visible
The authors from Zhejiang University are affiliated with the State
Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D Vision.
†Corresponding author: Xiaowei Zhou.
Figure 1. Visual localization under extremely challenging condi-tions. The proposed benchmark dataset SensLoc exhibits long-term appearance changes due to illumination, weather, season, day-night alternation, and new constructions. in the query image, in order to handle large-scale scenes.
The resulting camera poses are estimated using a standard
Perspective-n-Point (PnP) solver [24, 32] inside a robust
RANSAC [3, 12, 13, 18] loop. However, in real-world out-door scenarios, obtaining such correspondences and further recovering the 6-DoF pose are difficult, since the outdoor scenes can experience large appearance variations caused by illumination (e.g., day and night), seasonal (e.g., summer and winter) and structure changes. Visual localization under such challenging conditions remains an unsolved problem, as reported by recent benchmarks [48, 53, 76].
Fortunately, nowadays, with the popularity of smart de-vices that come equipped with various sensors such as In-ertial Measurement Unit (IMU), gravity, compass, GPS, or radio signals (like WiFi and Bluetooth), new possibilities arise for mobile phone pose estimation exploiting these addi-tional multi-modality sensors. Nevertheless, previous works
only take independent sensors into consideration. For ex-ample, some methods utilize GPS as a prior to bound the
Visual-Inertial Odometry (VIO) drift [31, 45, 56, 75] or sim-plify the image retrieval process [35, 72, 73], while others focus on employing the gravity direction as a reliable prior to benefit the PnP solver [1, 20, 33, 52, 63–65].
In this paper, we introduce a novel framework, named
SensLoc, that localizes an image by tightly coupling visual perception with complementary mobile sensor information for robust localization under extreme conditions.
In the first stage, our approach leverages GPS and compass to constrain the search space for image retrieval, which not only reduces the risk of misrepresentation of global features but also speedups the retrieval procedure with fewer database candidates. In the second stage, inspired by the recent CAD-free object pose estimation method, OnePose++ [26], we design a transformer-based network to directly match 3D points of the retrieved sub-map to dense 2D pixels of the query photo in a coarse-to-fine manner. Compared to the modern visual localization pipeline, which establishes 2D-3D correspondences by repeatedly matching 2D-2D local features between query and retrieve images, our solution shows a significant acceleration and a better performance especially under challenging appearance changes. In the last stage, we implement a simple yet effective gravity validation algorithm and integrate it into the RANSAC loop to filter the wrong pose hypotheses, leveraging the precise roll and pitch angles from mobile gravity sensors. The gravity validation leads to an improvement of RANSAC in terms of efficiency and accuracy, as false hypotheses can be removed in advance.
To the best of our knowledge, there is no public dataset for multi-sensor localization under strong visual changes. To facilitate the research of this area, we created a new bench-mark dataset SensLoc, as shown in Fig. 1. Specifically, we first used a consumer-grade panoramic camera (Insta360) and a handheld Real-time Kinematic (RTK) recorder, to cap-ture and reconstruct a large-scale reference map. Half a year later, we collected query sequences with large scene appear-ance changes through a mobile phone bounded with a RTK and recorded all available built-in sensor data. As direct registration between the query sequences and the reference map is difficult, we rebuilt an auxiliary map at the same time as acquiring the query sequences using Insta360 and
RTK and aligned the auxiliary map with the reference map through ICP. Thus, we only needed to register the query im-ages with the auxiliary map, which was easier as they were captured at the same time. To achieve this, we developed a pseudo-ground-truth (GT) generation algorithm to accu-rately register each query sequence against the auxiliary map by incorporating feature matching, visual-inertial-odometry,
RTK positions and gravity directions. The GT generation algorithm does not ask for any manual intervention or extra setup in the environment, enabling scalable pose labeling.
We evaluate several state-of-the-art image retrieval and localization methods on our proposed dataset. We show that the performance of the existing methods can be drastically improved by considering sensor priors available in mobile devices, such as GPS, compass, and gravity direction. The experiments also demonstrate that our method outperforms the state-of-the-art approach HLoc [50,51] by a large margin in challenging night-time environments, while taking only 66ms to find 2D-3D correspondences on a GPU and 8ms for PnP RANSAC.
In summary, our main contributions include:
• A novel outdoor visual localization framework with multi-sensor prior for robust and accurate localization under extreme visual changes.
• A new dataset for multi-sensor visual localization with seasonal and illumination variations.
• Benchmarking existing methods and demonstrating the effectiveness of the proposed approach. 2.