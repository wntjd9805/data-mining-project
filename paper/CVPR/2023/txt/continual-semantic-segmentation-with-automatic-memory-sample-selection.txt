Abstract
Continual Semantic Segmentation (CSS) extends static semantic segmentation by incrementally introducing new classes for training. To alleviate the catastrophic forgetting issue in CSS, a memory buffer that stores a small number of samples from the previous classes is constructed for re-play. However, existing methods select the memory samples either randomly or based on a single-factor-driven hand-crafted strategy, which has no guarantee to be optimal. In this work, we propose a novel memory sample selection mechanism that selects informative samples for effective re-play in a fully automatic way by considering comprehen-sive factors including sample diversity and class perfor-mance. Our mechanism regards the selection operation as a decision-making process and learns an optimal selection policy that directly maximizes the validation performance on a reward set. To facilitate the selection decision, we de-sign a novel state representation and a dual-stage action space. Our extensive experiments on Pascal-VOC 2012 and
ADE 20K datasets demonstrate the effectiveness of our ap-proach with state-of-the-art (SOTA) performance achieved, outperforming the second-place one by 12.54% for the 6-stage setting on Pascal-VOC 2012. 1.

Introduction
Semantic segmentation is an important task with a lot of applications. The rapid development of algorithms
[11, 20, 22, 30, 32, 56] and the growing number of publicly available large datasets [14, 55] have led to great success in the field. However, in many scenarios, the static model cannot always meet real-world demands, as the constantly changing environment calls for the model to be constantly updated to deal with new data, sometimes with new classes.
A naive solution is to apply continual learning by incre-mentally adding new classes to train the model. However, it
*Equal Contribution
†Corresponding Author is not simple as it looks – almost every time, since the pre-vious classes are inaccessible in the new stage, the model forgets the information of them after training for the new classes. This phenomenon, namely catastrophic forgetting, has been a long-standing issue in the field. Furthermore, the issue is especially severe in dense prediction tasks like semantic segmentation.
Facing the issue, existing works [1, 4, 5, 7, 17, 25, 26, 38, 43] propose to perform exemplar replay by introducing a memory buffer to store some samples from previous classes.
By doing so, the model can be trained with samples from both current and previous classes, resulting in better gener-alization. However, since the number of selected samples in the memory is much smaller than those within the new classes, the selected samples are easy to be ignored or cause overfitting when training due to the small number. Careful selection of the samples is required, which naturally brings the question: How to select the best samples for replay?
Some attempts have been made to answer the question, aiming to seek the most effective samples for replay. Re-searchers propose different criteria that are mostly manu-ally designed based on some heuristic factors like diver-sity [1, 4, 5, 25, 26, 38, 43]. For example, [33] selects the most common samples with the lowest diversity for replay, believing that the most representative samples will elevate the effectiveness of replay. However, the most common samples may not always be the samples being forgotten in later stages. [4] proposes to save both the low-diversity samples near the distribution center and high-diversity sam-ples near the classification boundaries. However, new chal-lenges arise since the memory length is limited, so it is challenging to find the optimal quotas for the two kinds of samples to promote replay effectiveness to the greatest ex-tent. Moreover, most of the existing methods are designed based on a single factor, the selection performance, how-ever, can be influenced by many factors with complicated relationships. For example, besides diversity, memory sam-ple selection should also be class-dependent because the hard classes need more samples to replay in order to allevi-ate the more severe catastrophic forgetting issue. Therefore,
we argue that it is necessary to select memory samples in a more intelligent way by considering the more comprehen-sive factors and their complicated relationships.
Witnessing the challenge, in this work, we propose a novel automatic sample selection mechanism for CSS.
Our key insight is that selecting memory samples can be regarded as a decision-making task in different training stages, so we formulate the sample selection process as a
Markov Decision Process, and we propose to solve it au-tomatically with a reinforcement learning (RL) framework.
Specifically, we employ an agent network to make the se-lection decision, which receives the state representation as the input and selects optimal samples for replay. To help the agent make wiser decisions, we construct a novel and com-prehensive state combined with the sample diversity and class performance features.
In the process of state com-putation, the inter-sample similarity needs to be measured.
We found the naive similarity measurement by computing the prototype distance is ineffective in segmentation, as the prototype losses the local structure details that are important for making pixel-level predictions. Therefore, we propose a novel similarity measured in a multi-structure graph space to get a more informative state. We further propose a dual-stage action space, in which the agent not only selects the most appropriate samples to update the memory, but also enhances the selected samples to have better replay effec-tiveness in a gradient manner. All the careful designs allow the RL mechanism to be effective in solving the sample se-lection problem for CSS.
We perform extensive experiments on Pascal-VOC 2012 and ADE 20K datasets, which demonstrate the effective-ness of our proposed novel paradigm for CSS. Benefit-ing from the reward-driven optimization, the automatically learned policy can help select the more effective samples, thus resulting in better performance than the previous strate-gies. On both datasets, our method achieves state-of-the-art (SOTA) performance. To summarize, our contributions are as follows:
• We formulate the sample selection of CSS as a Markov
Decision Process, and introduce a novel and effective automatic paradigm for sample replay in CSS enabled by reinforcement learning.
• We design an effective RL paradigm tailored for CSS, with novel state representations containing multiple factors that can guide the selection decision, and a dual-stage action space to select samples and boost their replay effectiveness.
• Extensive experiments demonstrate our automatic paradigm for sample replay can effectively alleviate the catastrophic forgetting issue with state-of-the-art (SOTA) performance achieved. 2.