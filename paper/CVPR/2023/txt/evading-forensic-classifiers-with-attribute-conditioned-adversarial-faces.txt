Abstract
The ability of generative models to produce highly real-istic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been devel-oped. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are de-tectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly per-turb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to suc-cessfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gender, etc.).
To achieve this goal, we leverage the state-of-the-art gen-erative model StyleGAN with disentangled representations, which enables a range of modifications without leaving the manifold of natural images. We propose a framework to search for adversarial latent codes within the feature space of StyleGAN, where the search can be guided either by a text prompt or a reference image. We also propose a meta-learning based optimization strategy to achieve transfer-able performance on unknown target models. Extensive experiments demonstrate that the proposed approach can produce semantically manipulated adversarial fake faces, which are true to the specified attribute set and can suc-cessfully fool forensic face classifiers, while remaining un-detectable by humans. Code: https://github.com/ koushiksrivats/face_attribute_attack. 1.

Introduction
Recent advances in deep learning such as generative ad-versarial networks (GAN) [9, 30] have enabled the genera-tion of highly realistic and diverse human faces that do not exist in reality [2]. While these synthetic/fake faces have many positive applications in video games, make-up indus-try, and computer-aided designs [11], they can also be mis-used for malicious purposes causing serious security and ethical problems. For instance, recent news reports have revealed the potential use of GAN-generated face images in the US elections for creating fake social media profiles to rapidly disseminate misinformation among the targeted groups [6, 24].
In a similar incident, a 17-year-old high school student successfully tricked Twitter into verifying a fake face profile picture of a US Congress candidate [1] us-ing a powerful generative model called StyleGAN2 [19].
To combat this problem of synthetically-generated fake faces using GANs, several methods have been proposed to distinguish fake GAN-generated faces from real ones [15, 36]. The results reported in these works tend to indi-cate that simple, supervised deep learning-based classifiers are often highly effective at detecting GAN-generated im-ages [35, 43]. Such classifiers can be referred to as forensic classifiers/models. However, a smart attacker may attempt to manipulate these fake images using tools from adversar-ial machine learning [4] to bypass forensic classifiers, while maintaining high visual quality. A recent attempt in this di-rection is the work of Li et al. [20], which demonstrates that adversarially exploring the manifold of the generative model by optimizing over the latent space can generate real-istic faces that are misclassified by the target forensic detec-tors. Furthermore, they show that the resulting adversarial fake faces contain fewer artifacts compared to traditional norm-constrained adversarial attacks in the image space.
While being the first to introduce the generative model based adversarial attack to fool forensic face classifiers, the work of Li et al. [20] suffers from one key limitation: the in-ability to control the attributes of the generated adversarial faces like skin color, expression, or age. This kind of con-trol over face attributes is essential for attackers to rapidly disseminate false propaganda via social media to specific ethnic or age groups. Therefore, it is imperative for image forensics researchers to study and develop these attribute-conditioned attacks to expose the vulnerabilities of existing forensic face classifiers, with the eventual goal of designing effective defense mechanisms in the future.
In this paper, we propose a method to generate realistic image-driven or text-guided adversarial fake faces that are misclassified by forensic face detectors. Our method lever-Figure 1. Left: An illustration of our attribute-conditioned adversarial face image generation approach. Instead of generating face image randomly (purple dot) and making it adversarial using approach of Li et al. [20] (red dot), we leverage the highly disentangled latent space of the StyleGAN2 to generate attribute-conditioned adversarial face images (green dot) that are misclassified by the target forensic classifier. Right: Generated adversarial face images via our proposed image-driven and text-guided method. ages the highly disentangled latent space of StyleGAN2 to craft attribute-conditioned unrestricted attacks in a unified framework, as illustrated in Figure 1. Specifically, for gen-erating a fake face that matches the attributes of a given ref-erence image, we propose an efficient algorithm to optimize attribute-specific latent variables in an adversarial manner.
This results in an effective transfer of the desired coarse or fine-grained attributes from the reference image to the gen-erated fake image. For generating fake faces that match given text descriptions, we leverage the joint image-text representation capabilities of Contrastive Language-Image
Pre-training (CLIP) [28] to enforce consistency between the generated adversarial face image and text description. Our contributions can be summarized as follows:
• We propose a framework to generate adversarial fake faces with a specific set of attributes defined using a reference image or a text prompt. This is achieved by performing both semantic manipulations and adversar-ial perturbations to a randomly generated initial image in the latent space of StyleGAN2 [19]. For image-based attribute conditioning, we transfer semantic attributes from the given reference image by searching the adversarial space guided by perceptual loss. For text-based attribute conditioning, we take advantage of multimodal CLIP [28] and use its text-guided fea-ture space to search for adversarial latent codes within the feature space of StyleGAN2 to fool forensic clas-sifiers.
• We leverage a meta-learning-based optimization strat-egy to generate adversarial images that are more trans-ferable to the unknown black-box forensic classifiers compared to ensemble-based approach.
• We show that semantic changes (e.g. hair color changes, manipulating eye sizes, etc.) introduced by our method appears benign to human subjects while being adversarial to deep forensic classifiers. We fur-ther validate this claim by measuring the perceptual similarity between the original and adversarial sam-ples. We also conduct experiments to show that faces generated using the proposed approach not only ex-hibit the desired attributes but are also able to fool the target forensic classifier with high success rate.
At the outset, we would like to strongly emphasize that our goal is not to circumvent a face recognition system. Our only objective is to generate realistic synthetic faces with specific attributes that can evade a forensic classifier, which is trained to detect synthetic images. The reference image or text prompt merely guides the generation process towards the required attributes. Hence, we make no explicit attempt to preserve the identity of either the randomly generated ini-tial image or the reference image in the final generated im-age. However, we experimentally show that, it is also pos-sible to preserve the identity of the generated image without sacrificing its adversarial nature.
2.