Abstract
Formula-driven supervised learning (FDSL) has been shown to be an effective method for pre-training vision transformers, where ExFractalDB-21k was shown to exceed the pre-training effect of ImageNet-21k. These studies also indicate that contours mattered more than textures when pre-training vision transformers. However, the lack of a systematic investigation as to why these contour-oriented synthetic datasets can achieve the same accuracy as real datasets leaves much room for skepticism. In the present work, we develop a novel methodology based on circular harmonics for systematically investigating the design space of contour-oriented synthetic datasets. This allows us to efﬁciently search the optimal range of FDSL parameters and maximize the variety of synthetic images in the dataset, which we found to be a critical factor. When the resulting new dataset VisualAtom-21k is used for pre-training ViT-Base, the top-1 accuracy reached 83.7% when ﬁne-tuning on ImageNet-1k. This is only 0.5% difference from the top-1 accuracy (84.2%) achieved by the JFT-300M pre-training, even though the scale of images is 1/14. Unlike JFT-300M which is a static dataset, the quality of synthetic datasets will continue to improve, and the current work is a testa-ment to this possibility. FDSL is also free of the common issues associated with real images, e.g. privacy/copyright issues, labeling costs/errors, and ethical biases. 1.

Introduction
Vision transformers [10] have made a signiﬁcant im-pact on the entire ﬁeld of computer vision, and state of the art models in classiﬁcation [37, 41, 42], object detec-tion [25, 36], and segmentation [8, 15, 23, 36] are now based on vision transformers. The accuracy of vision transform-ers exceeds that of convolutional neural networks by a con-siderable margin when the model is pre-trained on huge datasets, such as JFT-300M [32]. However, the JFT-300M dataset contains 300M images and 375M labels. It is impos-Figure 1. VisualAtom: a new FDSL dataset (a) Inspired by de
Broglie’s atomic model, we propose VisualAtom dataset contain-ing shapes from two sinusoidal waves. (b) When ﬁne-tuning on
ImageNet-1k, ViT-B pre-trained with VisualAtom achieved the ac-curacy of only 0.5% lower than JFT-300M using 1/14 images. sible to manually label all of these images. Efforts to auto-matically label such datasets is still not as accurate as man-ual labeling. Self-supervised learning (SSL) is increasing in popularity, as datasets do not need to be labeled for this
mode of training [16]. Although SSL removes the burden of labeling large datasets, the effort to collect/download, store, and load these large datasets remains a challenge.
One of the major issues in computer vision is that ac-cess to huge datasets, such as JFT-300M/3B [32, 42] and
IG-3.5B [27], is limited to certain institutions. This makes it difﬁcult for the rest of the community to build upon, or even reproduce, existing works. This limitation has prompted the creation of open datasets, such as LAION-5B [30]. How-ever, the LAION dataset is not curated, which means that it could contain inappropriate content, or be subject to soci-etal bias and/or privacy/copyright issues [5, 39, 40]. How to curate such large datasets from the perspective of AI ethics and safety is an open area of research, but, in the meantime, an alternative approach to creating large datasets for com-puter vision is needed.
Formula-Driven Supervised Learning (FDSL) [19, 20] has been proposed as an alternative to supervised learn-ing (SL) and SSL with real images. The term “formula-driven” encompasses a variety of techniques for generating synthetic images from mathematical formulae. The ratio-nale here is that, during the pre-training of vision trans-formers, feeding such synthetic patterns are sufﬁcient to ac-quire the necessary visual representations. These images include various types of fractals [1, 17, 19, 20, 28], geomet-ric patterns [18], polygons and other basic shapes [17]. The complexity and smoothness of these shapes can be adjusted along with the brightness, texture, ﬁll-rate, and other fac-tors that affect the rendered image. Labels can be assigned based on the combination of any of these factors, so a la-beled dataset of arbitrary quantity can be generated without human intervention. Furthermore, there is close-to-zero risk of generating images with ethical implications, such as so-cietal bias or copyright infringement.
Another major advantage of synthetic datasets is that the quality of images can be improved continuously, unlike natural datasets which can only be enhanced in quantity.
Therefore, we anticipate that we could eventually create a synthetic image dataset that surpass the pre-training effect of JFT-300M, by understanding which properties of syn-thetic images that contribute to pre-training and improving them. Nakashima et al. [28] used fractal images to pre-train vision transformers and found that the attention maps tend to focus on the contours (outlines) rather than the textures.
Kataoka et al. [17] veriﬁed the importance of contours by creating a new dataset from well-designed polygons, and exceeded the pre-training effect of ImageNet-21k with this dataset that consists of only contours. These studies indi-cate that contours are what matter when pre-training vision transformers. However, these studies covered only a lim-ited design space, due to the difﬁculty of precisely control-ling the geometric properties of the contours in each image.
The present study aims to conduct a systematic and thor-ough investigation of the design space of contour-oriented synthetic images. We systematically investigate the design space of contours by expressing them as a superposition of sinusoidal waves onto ellipses, as shown in Figure 1a. In the same way that a Fourier series can express arbitrary func-tions, we can express any contour shape with such a super-position of waves onto an ellipse. Such geometrical con-cepts have appeared in classical physics, e.g. de Broglie’s atomic model [6]. Therefore, we name this new dataset
“VisualAtom”, and the method to generate the images “vi-sual atomic renderer”. The visual atomic renderer allows us to exhaustively cover the design space of contour-oriented synthetic images, by systematically varying the frequency, amplitude, and phase of each orbit, along with the number of orbits and degree of quantization the orbits. We vary the range of these parameters to generate datasets with different variety of images. We found that variety of contour shapes is a crucial factor for achieving a superior pre-training ef-fect. Our resulting dataset was able to nearly match the pre-training effect of JFT-300M when ﬁne-tuned on ImageNet-1k, while using only 21M images. We summarize the con-tributions as follows:
Investigative contribution (Figure 1a): We propose a novel methodology based on circular harmonics that allows us to systematically investigate the design space of contour-oriented synthetic datasets. Identifying the optimal range of frequency, amplitude, and quantization of the contours lead to the creation of a novel synthetic dataset VisualAtom with unprecedented pre-training effect on vision transformers.
Experimental contribution (Figure 1b): We show that pre-training ViT-B with VisualAtom can achieve compara-ble accuracy to pre-training on JFT-300M, when evaluated on ImageNet-1k ﬁne-tuning. Notably, the number of im-ages used to achieve this level of accuracy was approxi-mately 1/14 of JFT-300M. We also show that VisualAtom outperforms existing state-of-the-art FDSL methods.
Ethical contribution: We will release the synthesized im-age dataset, pre-trained models, and the code to generate the dataset. This will also allow users with limited inter-net bandwidth to generate the dataset locally. Moreover, the dataset and model will be released publicly as a com-mercially available license and not limited to educational or academic usage. 2.