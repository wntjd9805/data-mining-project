Abstract
Deep neural networks (DNN) have outstanding perfor-mance in various applications. Despite numerous efforts of the research community, out-of-distribution (OOD) samples remain a significant limitation of DNN classifiers. The abil-ity to identify previously unseen inputs as novel is crucial in safety-critical applications such as self-driving cars, un-manned aerial vehicles, and robots. Existing approaches to detect OOD samples treat a DNN as a black box and evalu-ate the confidence score of the output predictions. Unfortu-nately, this method frequently fails, because DNNs are not trained to reduce their confidence for OOD inputs. In this work, we introduce a novel method for OOD detection. Our method is motivated by theoretical analysis of neuron ac-tivation patterns (NAP) in ReLU-based architectures. The proposed method does not introduce a high computational overhead due to the binary representation of the activation patterns extracted from convolutional layers. The extensive empirical evaluation proves its high performance on vari-ous DNN architectures and seven image datasets. 1.

Introduction
Even the most efficient deep neural network (DNN) ar-chitectures, designed for image recognition tasks, cannot ensure that they will not malfunction during their opera-tion. Thus, deployment of those safety-critical applications, such as in self-driving cars, unmanned aerial vehicles, and robots, is still an unresolved problem [9, 35]. The use of safety mechanisms, such as runtime monitors, is a viable strategy to keep the system in a safe state despite of DNN failure. The design and development of such monitors in the context of safety-critical applications is a significant chal-lenge [10,48]. Therefore, it is required to define robust met-rics that can allow to detect and control of DNN’s failures at runtime and mitigate potential hazards caused by their performance limitations.
DNNs are trained over a set of inputs sampled from real-world scenarios. However, due to the large variation of the input images, the training dataset cannot contain all possi-ble variants of input samples. Although it is expected that the trained models can perform well on unknown inputs, es-pecially those that are similar to the training data, it cannot be guaranteed that they will perform well for OOD noisy samples that present the objects not considered before [15].
While DNN training techniques should allow a network to achieve high generalization capabilities, it is also crucial to ensure the dependability of safety-critical systems to train a model so that any outlying input will result in low confi-dence of the network’s decision.
The fundamental challenge to ensure the safety of DNNs is to estimate if a given input sample comes from the same data distribution for which a DNN was trained. This is very hard to estimate because the network usually extrapolates its decision while receiving new image samples. Another cause of the incorrect recognition of outlying samples can be the distributional shift of input data over time (e.g. the time-dependent variations of an object’s appearance) [19].
In the vast literature, this problem has been formulated as a problem of detecting whether input data are from an in-distribution (ID) or out-of-distribution (OOD). This has been studied for many years and discussed in the follow-ing aspects: sample rejection, anomaly detection, open-set samples recognition, familiar vs unfamiliar samples or un-certainty estimation [2, 24, 38].
In this work, we present a novel algorithm for the identi-fication of OOD image samples. In our method, we extract the binary neuron activation patterns on various hidden lay-ers of a DNN and compare them with the ones collected in the training procedure. By measuring the Hamming dis-tances between extracted binary patterns of any test sample and the patterns extracted during the training, we can iden-tify OOD samples.
The main contributions of this paper are the following:
• We introduce NAP - an algorithm that extracts bi-nary patterns from both fully connected and convolu-tional layers and estimates a classifier’s predictive un-certainty based on the patterns. The proposed method outperforms state-of-the-art OOD detection methods.
Moreover, the algorithm is straightforward, making it simple to incorporate into existing DNN architectures.
• We provide an extended empirical evaluation compar-ing the impact of the activation patterns collected from different layers of DNN which may inspire future re-search in this area.
• We publish the largest evaluation framework for OOD detection. This framework contains 17 OOD methods (including the proposed NAP-based method) that can be directly tested on two state-of-the-art DNN archi-tectures and 7 datasets allowing for simple extension of the framework for new methods, architectures, and datasets.1 2.