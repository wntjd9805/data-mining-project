Abstract
Continual learning aims to learn on non-stationary data streams without catastrophically forgetting previous knowl-edge. Prevalent replay-based methods address this chal-lenge by rehearsing on a small buffer holding the seen data, for which a delicate sample selection strategy is required.
However, existing selection schemes typically seek only to maximize the utility of the ongoing selection, overlooking the interference between successive rounds of selection.
Motivated by this, we dissect the interaction of sequential selection steps within a framework built on influence func-tions. We manage to identify a new class of second-order influences that will gradually amplify incidental bias in the replay buffer and compromise the selection process. To reg-ularize the second-order effects, a novel selection objective is proposed, which also has clear connections to two widely adopted criteria. Furthermore, we present an efficient im-plementation for optimizing the proposed criterion. Exper-iments on multiple continual learning benchmarks demon-strate the advantage of our approach over state-of-the-art methods. Code is available at https://github.com/ feifeiobama/InfluenceCL. 1.

Introduction
The ability to continually accumulate knowledge is a hallmark of intelligence, yet prevailing machine learning systems struggle to remember old concepts after acquir-ing new ones. Continual learning has hence emerged to tackle this issue, also known as catastrophic forgetting, and thereby enable the learning on long task sequences which are frequently encountered in real-world applica-tions [15, 17]. Amongst a variety of valid methods, the replay-based approaches [33, 39] achieve evidently strong results by buffering a small coreset of previously seen data for rehearsal in later stages. Due to the rigorous constraints on memory overhead, the replay buffer needs to be carefully maintained such that only the samples deemed most critical
*Corresponding author.
Figure 1. (a) In continual learning, earlier coreset selection exerts a profound influence on subsequent steps through the data flow. (b) By ignoring this, a greedy selection strategy can degrade over time. Therefore, we propose to model and regularize the influence of each selection on the future. for preserving overall performance are selected during the learning procedure.
Despite much effort in sophisticated design of coreset selection criteria [3,7,47,52], existing works are mostly de-veloped for an oversimplified scenario, where each round of selection is considered isolated and primarily focused on refining single-round performance. For example, Borsos et al. [7] greedily minimize the empirical risk on the currently available subset of data. However, as shown in Fig. 1a, the actual selection process within continual learning is rather different in that each prior selection defines the input for subsequent selection and therefore has an inevitable impact on the later decision. Neglecting such interactions between successive selection steps, as in previous practice, may re-sult in a degraded coreset after the prolonged selection pro-cess. To maximize overall performance, an ideal continual learning method should take into account the future influ-ence of each round of selection, which remains unresolved due to the obscure role of intermediate selection results.
This work models the interplay among multiple selection steps with the classic tool of influence functions [21, 29], which estimates the effect of each training point by perturb-ing sample weights. Similarly, we begin by upweighting two samples from consecutive time steps, and uncover that the latter one’s influence evaluation is biased due to the con-sequent perturbations in test gradient and coreset Hessian.
This gives rise to a new class of second-order influences that interfere with the initial influence-based selection strat-egy. To be specific, the selection tends to align across dif-ferent rounds, favoring those samples with a larger gradient projection along the previously upweighted ones. It further suggests that some unintended adverse effects of the early selection steps, such as incidental bias introduced into the replay buffer, will be amplified after rounds of selection and impair the final performance.
To address the newly discovered disruptive effects, we propose to regularize each round of selection beforehand based on its expected second-order influence, as illustrated in Fig. 1b. Intuitively, the selection with a large magnitude of second-order influence will substantially interfere with future influence estimates, and therefore should be avoided.
However, the magnitude itself cannot be precalculated for direct guidance of the selection, since it is related to some unknown future data. We instead derive a tractable upper bound for the magnitude which results in the form of ℓ2-norm, and integrated it with the vanilla influence functions to serve as the final selection criterion. The proposed regu-larizer can be interpreted with clarity as it equates to a cou-ple of existing criteria ranging from gradient matching to diversity, under varying simplifications. Finally, we present an efficient implementation that tackles the technical chal-lenges in selecting with neural networks.
Our contributions are summarized as below:
• We investigate the previously-neglected interplay be-tween consecutive selection steps within an influence-based selection framework. A new type of second-order influences is identified, and further analysis states its harmfulness in continual learning.
• A novel regularizer is proposed to mitigate the second-order interference. The regularizer is associated with two other popular selection criteria and can be effi-ciently optimized with our implementation.
• Comprehensive experiments on three continual learn-ing benchmarks (Split CIFAR-10, Split CIFAR-100 and Split miniImageNet) clearly illustrates that our method achieves new state-of-the-art performance. 2.