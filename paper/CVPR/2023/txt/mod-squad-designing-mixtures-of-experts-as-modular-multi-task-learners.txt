Abstract
Optimization in multi-task learning (MTL) is more chal-lenging than single-task learning (STL), as the gradient from different tasks can be contradictory. When tasks are re-lated, it can be beneficial to share some parameters among them (cooperation). However, some tasks require additional parameters with expertise in a specific type of data or dis-crimination (specialization). To address the MTL challenge, we propose Mod-Squad, a new model that is Modularized into groups of experts (a ‘Squad’). This structure allows us to formalize cooperation and specialization as the pro-cess of matching experts and tasks. We optimize this match-ing process during the training of a single model. Specifi-cally, we incorporate mixture of experts (MoE) layers into a transformer model, with a new loss that incorporates the mutual dependence between tasks and experts. As a result, only a small set of experts are activated for each task. This prevents the sharing of the entire backbone model between all tasks, which strengthens the model, especially when the training set size and the number of tasks scale up. More interestingly, for each task, we can extract the small set of experts as a standalone model that maintains the same per-formance as the large model. Extensive experiments on the
Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5 vision tasks show the superiority of our approach. The project page can be accessed at https://vis-www.cs.umass.edu/mod-squad. 1.

Introduction
Computer vision involves a great number of tasks includ-ing recognition, depth estimation, edge detection, etc. Some of them have a clear and strong relationship: they are likely to benefit from shared features. An example would be a task to classify cars and pedestrians and a task to segment the same classes. Other tasks appear to be less related: it is not clear what features they would share. An example could be tumor detection in medical images and face recognition.
Multi-task learning (MTL) aims to model the relation-ships among tasks and build a unified model for a diverse
Figure 1. A comparison between Mod-Squad and MoE ViT.
Our key motivation is that experts should leverage commonalities in some tasks (cooperation) but focus on a subset of tasks that require specific features and do not interfere with each other (spe-cialization). set of tasks. On the one hand, tasks often benefit by shar-ing parameters, i.e., cooperation. On the other hand, some tasks may require specialized expertise that only benefits that single task, i.e., specialization. A good MTL system should be flexible to optimize experts for the dual purposes of cooperation and specialization.
There are two well-known challenges in MTL: (1) gradi-ent conflicts across tasks [5, 38]; and (2) how to design ar-chitectures that have both high accuracy and computational efficiency.
To address these challenges, we introduce Mod-Squad, a new model that constructs a Mixture of Ex-perts (MoE) [31] to be modularized multi-task learners (a squad). Our design allows experts to cooperate on tasks when it is helpful, rather than penalizing experts that do not participate in every task. At the same time, some experts naturally develop a deep specialization in particular tasks, improving performance. The left figure in Fig. 1 shows an example of the specialization and cooperation of experts in
Mod-Squad. A further and important side benefit, discussed below, is that this sparsification of experts allows our model to be decomposed into much smaller single-task models that perform extremely well.
We achieve these goals by first integrating mixture of ex-perts (MoE) layers into our vision transformer [6] backbone network. The motivation is to divide the model into groups of experts, and for each expert to construct a minimum part of the model that can be shared among tasks or be special-ized for one task. The experts can have any network struc-ture (e.g., MLP or attention network [40]) so that we can incorporate advanced model designs. Our modular design allows cooperation and specialization via the distribution of tasks to experts and also experts to tasks. Below, we for-malize this idea mathematically by analyzing the probabil-ity distribution over tasks and experts, and using a novel loss function to induce a specific structure on this distribution.
Many previous MoE works [29, 31, 40] use a load-balancing loss that encourages the frequency of expert us-age (across all tasks and batches) to be highly similar. Some
MoE methods [18, 26] directly apply this loss after the forward pass of each task on the multi-task scenario so that each task evenly uses all experts. However, this ap-proach may force experts to set parameters on conflicting tasks with learning gradients that counteract each other. In other words, while an expert may benefit from being shared among certain pairs of tasks, it may be harmed by being forced to share among other pairs of tasks. This is an expla-nation for the difficulty of training multi-task models under such an expert-balancing loss.
In comparison, we contend that experts should leverage commonalities in some tasks (cooperation) but also create a subset of experts that learn specific features (as needed by some tasks) and do not interfere with each other (spe-cialization). Such an assignment of tasks to experts can be represented via a sparse but strong dependence between experts and tasks. Fig. 1 illustrates this key difference be-tween our model and previous MoE work, showing how our model induces a sparser structure in the assignment of ex-perts to tasks. To implement this idea, we add a loss term to maximize the mutual information between experts and tasks. This induces a strong dependency between experts and tasks, with each task heavily related to a small set of experts and vice versa.
Interestingly, we find that our model converges to a state in which, after training, most experts are never or rarely used for many tasks (evidence of specialization), but the ex-perts are still balanced in their activation frequency. This property enables us to extract a compact sub-network from the giant model for each task. The small networks extracted in this fashion work independently as standalone models for individual tasks with no performance drop. This prop-erty enables us to train a giant, sparse model in a scaled-up multi-task learning scenario and later get compact sub-networks for each task with high performance.
Our main contributions can be summarized as follows:
• Modular multi-task learner. We propose a new modular backbone model, Mod-Squad, that is composed of a large group of attention and feed-forward experts. The experts can be flexibly assigned a subset of tasks to achieve spe-cialization and cooperation.
• Optimizing the joint distribution over tasks and ex-perts. Mod-Squad includes a new loss term that encour-ages a sparse but strong dependence between experts and tasks. This is done by measuring and maximizing the mu-tual information between tasks and experts.
• Effective and Efficient multi-task learners at scale. Ex-periment results show that Mod-Squad achieves state-of-the-art performance on two major multi-task datasets while maintaining its computational efficiency.
• Extracting small sets of experts as standalone models with no performance drop. We further show that Mod-Squad can be effectively pruned for a designated task with-out sacrificing performance. 2.