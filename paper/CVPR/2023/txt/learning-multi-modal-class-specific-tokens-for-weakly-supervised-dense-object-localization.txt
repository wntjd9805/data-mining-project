Abstract
Weakly supervised dense object localization (WSDOL) relies generally on Class Activation Mapping (CAM), which exploits the correlation between the class weights of the im-age classifier and the pixel-level features. Due to the lim-ited ability to address intra-class variations, the image clas-sifier cannot properly associate the pixel features, leading to inaccurate dense localization maps.
In this paper, we propose to explicitly construct multi-modal class represen-tations by leveraging the Contrastive Language-Image Pre-training (CLIP), to guide dense localization. More specifi-cally, we propose a unified transformer framework to learn two-modalities of class-specific tokens, i.e., class-specific visual and textual tokens. The former captures semantics from the target visual data while the latter exploits the class-related language priors from CLIP, providing complemen-tary information to better perceive the intra-class diversi-In addition, we propose to enrich the multi-modal ties. class-specific tokens with sample-specific contexts compris-ing visual context and image-language context. This en-ables more adaptive class representation learning, which further facilitates dense localization. Extensive experiments show the superiority of the proposed method for WSDOL on two multi-label datasets, i.e., PASCAL VOC and MS COCO, and one single-label dataset, i.e., OpenImages. Our dense localization maps also lead to the state-of-the-art weakly supervised semantic segmentation (WSSS) results on PAS-CAL VOC and MS COCO. 1 1.

Introduction
Fully supervised dense prediction tasks have achieved great success, which however comes at the cost of expensive pixel-level annotations. To address this issue, recent works have investigated the use of weak labels, such as image-1https://github.com/xulianuwa/MMCST
Figure 1. (a) CAM exploits the correlation between the image classifier and the pixel features. (b) We propose to construct multi-modal class-specific tokens to guide dense object localization. level labels, to generate dense object localization maps as pseudo labels for those tasks. For the weakly supervised ob-ject localization (WSOL) task, most methods evaluate local-ization results on the bounding-box level and a few recent methods [5] evaluate on the pixel level. We use WSDOL to focus on the pixel-level evaluation, which is critical for downstream dense prediction tasks such as WSSS.
Previous works have exploited Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) [7] for
WSDOL with image-level labels [21, 40]. These meth-ods have generally relied on Class Activation Mapping (CAM) [48], which generates class-specific localization maps by computing the correlation between the class-specific weight vectors of the image classifier and every pixel feature vector. However, image classifiers generally have a limited ability to address the intra-class variation, let alone at the pixel level. This thus leads to inaccurate dense localization results.
In the conventional fully supervised learning paradigm, the image classification model aims to convert images to numeric labels, ignoring the context of the labels. Hence, it tends to learn the pattern that max-imizes the inter-class differences but disregards the intra-class diversities. This largely restricts the model’s ability of
understanding semantic objects.
Recently, Vision-Language (VL) models have attracted much attention.
In particular, CLIP, a representative VL model, pre-trained on 400 million image-text pairs that are readily available publicly, has been successfully applied to a number of downstream tasks, due to its strong general-ization ability. CLIP introduces a contrastive representation learning method that constrains an image to match its re-lated text while dis-matching the remaining texts from the same batch, in a multi-modal embedding space. This en-ables the model to perceive the differences across images, thus facilitating it to better discriminate intra-class samples.
Motivated by these observations, we propose to lever-age the strong representations of visual concepts encoded by the pre-trained CLIP language model to guide the dense object localization. More specifically, we extract the class-related text embeddings by feeding the label prompts to the pre-trained CLIP language model. As shown in Figure 1, we propose a unified transformer framework which includes multi-modal class-specific tokens, i.e., class-specific visual tokens and class-specific textual tokens. The class-specific visual tokens aim to capture visual representations from the target image dataset, while the class-specific textual tokens take the rich language semantics from the CLIP label text embeddings. These two modalities of class-specific tokens, with complementary information, are jointly used to corre-late pixel features, contributing to better dense localization.
In order to construct more adaptive class representations, which can better associate the sample-specific local features for dense localization, we propose to enhance the global multi-modal class-specific tokens with sample-specific con-textual information. To this end, we introduce two designs: (i) at the feature level, we use the sample-specific visual context to enhance both the class-specific visual and tex-tual tokens. This is achieved by combining these global to-kens with their output local counterparts which aggregate the patch tokens of the image through the self-attention lay-ers; (ii) at the loss level, we introduce a regularization con-trastive loss to encourage the output text tokens to match the
CLIP image embeddings. This allows the CLIP model to be better adapted to our target datasets. Moreover, due to its image-language matching pre-training objective, the CLIP image encoder is learned to extract the image embeddings that match the CLIP text embeddings of their corresponding image captions. We thus argue that through this contrastive loss, the rich image-related language context from the CLIP could be implicitly transferred to the text tokens, which are more beneficial for guiding the dense object localization, compared to the simple label prompts.
In summary, the contribution of this work is three-fold:
• We propose a new WSDOL method by explicitly con-structing multi-modal class representations in a unified transformer framework.
• The proposed transformer includes class-specific vi-sual tokens and class-specific textual tokens, which are learned from different data modalities with diverse super-visions, thus providing complementary information for more discriminative dense localization.
• We propose to enhance the multi-modal global class rep-resentations by using sample-specific visual context via the global-local token fusion and transferring the image-language context from the pre-trained CLIP via a regular-ization loss. This enables more adaptive class representa-tions for more accurate dense localization.
The proposed method achieved the state-of-the-art re-sults on PASCAL VOC 2012 (72.2% on the test set) and
MS COCO 2014 (45.9% on the validation set) for WSSS. 2.