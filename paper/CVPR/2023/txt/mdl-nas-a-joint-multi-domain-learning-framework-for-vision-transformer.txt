Abstract
In this work, we introduce MDL-NAS, a unified frame-work that integrates multiple vision tasks into a manage-able supernet and optimizes these tasks collectively un-der diverse dataset domains. MDL-NAS is storage-efficient since multiple models with a majority of shared parame-ters can be deposited into a single one. Technically, MDL-NAS constructs a coarse-to-fine search space, where the coarse search space offers various optimal architectures for different tasks while the fine search space provides fine-grained parameter sharing to tackle the inherent obstacles of multi-domain learning. In the fine search space, we sug-gest two parameter sharing policies, i.e., sequential shar-ing policy and mask sharing policy. Compared with pre-vious works, such two sharing policies allow for the par-tial sharing and non-sharing of parameters at each layer of the network, hence attaining real fine-grained parameter sharing. Finally, we present a joint-subnet search algorithm that finds the optimal architecture and sharing parameters for each task within total resource constraints, challeng-ing the traditional practice that downstream vision tasks are typically equipped with backbone networks designed for image classification. Experimentally, we demonstrate that
MDL-NAS families fitted with non-hierarchical or hierar-chical transformers deliver competitive performance for all tasks compared with state-of-the-art methods while main-taining efficient storage deployment and computation. We also demonstrate that MDL-NAS allows incremental learn-ing and evades catastrophic forgetting when generalizing to a new task. 1.

Introduction
Recently, transformers have become the standard pattern for natural language processing (NLP) tasks due to their efficacy in modelling long-range relationships via the self-*Corresponding author.
Figure 1. Illustration of differences between multi-domain learn-ing (MDL) and other learning paradigms. MDL-NAS jointly op-timizes multiple vision tasks under different dataset domains. L denotes the layer numbers in the backbone. attention mechanism [41]. Such success and good prop-erties of transformers have spawned a slew of subsequent works that apply them to a wide variety of computer vision tasks, such as image classification [6, 27, 32, 49], object de-tection [5, 57], semantic segmentation [55], and video un-derstanding [56], achieving impressive results. However, these methods only apply transformer to a specific domain.
After observing the success of transformers, a naive ques-tion arises: could a transformer simultaneously handle mul-tiple vision tasks under a variety of dataset domains ?
While a few works have investigated the usage of trans-formers to handle multiple input modalities (i.e., images and text), they typically concentrate on a particular task, such as visual question answering [20, 24], i.e., many-to-one mapping. Also, some methods [2, 26] explore to simul-taneously performing depth estimation, surface normal esti-mation, and semantic segmentation on a given input image.
However, these methods are restricted to a single-domain setting, where all inputs are the same, i,e,, one-to-many mapping. In addition, there are some works [19, 28] that employ transformers to solve different tasks under multiple domains (multi-domain learning), which is more realistic, i.e., many-to-many mapping, as shown in Fig. 1. Never-(a) Image classification (b) Object detection (c) Semantic segmentation
Figure 2. Adjust baselines that shares all parameters in backbone with task-specific normalization (Spe-norm), task-specific self-attention (Spe-attn), task-specific feed forward network (Spe-ffn) and all task-specific parameters (All-spe) under the same training recipe. theless, these methods utilize diverse encoders to manage different dataset domains, which is inefficient in terms of storage deployment. In this work, we investigate a unified network that optimizes multiple vision tasks over multiple dataset domains to enable all tasks to share as many param-eters as feasible while maintaining promising performance.
As a preliminary step, we conduct experiments to observe the performance impact of treating various components of vision transformers as task-specific parameters.
As depicted in Fig. 2, considering the multihead self-attention (MHSA) layer or feed-forward network (FFN) or
LayerNorm (LN) throughout the backbone as task-specific parameters can all achieve a certain performance gain for classification and detection tasks over a baseline that shares all parameters. Besides, we observe that the performance of semantic segmentation is elevated when all parameters are shared, indicating that closely-related tasks have mu-tual benefits whereas some tasks have conflicts against each other under multi-domain learning setting. Consequently, to use task-shared parameters for learning task-reciprocal features while using task-specific parameters for mitigating conflicts, sharing parameters with various proportions in-side each layer is an immediate thought, which motivates us to find a method to supply different share ratios for different layers in the network. Moreover, when optimizing multiple tasks collectively, we typically equip these tasks with the backbone designed for image classification, which may be sub-optimal due to the gap between the image classification task and other vision tasks.
To tackle these issues, we introduce MDL-NAS, a uni-fied framework based on vision transformers, which accom-modates multiple vision tasks under heterogeneous dataset domains into a modest supernet and jointly optimizes these
Specifically, we first construct a coarse search tasks. space comprising embedding dimension, heads number, query/key/value dimension, and MLP ratios for each trans-former block to discover different optimal architectures for diverse tasks. Moreover, such space comprises candidate ar-chitectures with a wide spectrum of model size, which pro-vides certain flexibility for final model deployment. Based on the coarse search space, we design a fine search space that offers fine-grained parameter sharing for all tasks to resolve the inherent challenges of multi-domain learning.
In the fine search space, we suggest two parameter sharing policies, namely sequential sharing policy and mask shar-ing policy. Sequential sharing policy enables all tasks to share parameters for each layer in order, which allows to customize the parameter share ratio. Mask sharing policy provides maximum flexibility for different tasks to share pa-rameters with various proportions and channels inside each layer. Following Autoformer [6], to address the efficiency issue, we leverage the weight entanglement training strat-egy to train MDL-NAS, allowing thousands of subnets to be extremely well-trained.
During the search stage, we propose a joint-subnet search algorithm that finds the optimal architecture and sharing parameters for each task under total resource con-straints. The searched subnets with various architectures share as many parameters as possible in the backbone, guar-anteeing excellent performance for each task while keeping storage-efficient for model deployment.
Experiments show that the searched models with weights inherited from the supernet outperform several baselines and are comparable with the state-of-the-art methods that are trained individually for specific tasks. We also demon-strate that MDL-NAS allows incremental learning and evades catastrophic forgetting when generalizing to a new task. Thus, MDL-NAS is more parameter-efficient and can scale up more gracefully with the number of tasks increas-ing, as illustrated in Sec. 4.4.
The key contributions of this work can be summarized as: (1) We propose MDL-NAS that accepts multiple dataset domains as input to optimize multiple vision tasks concur-rently. (2) We construct a coarse-to-fine search space, with the coarse search space finding optimal architectures for all tasks and the fine search space coupled with sequential or mask sharing policy providing fine-grained shared parame-ters to learn task-reciprocal features and extra task-specific parameters for learning task-related features. (3) We intro-duce a subnet search algorithm to jointly search architec-tures and share ratios, enabling all tasks to share as many parameters as feasible while ensuring high performance for each task. (4) We demonstrate that MDL-NAS allows in-cremental learning with fewer parameters. 2.