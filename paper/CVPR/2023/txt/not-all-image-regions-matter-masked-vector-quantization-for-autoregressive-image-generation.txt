Abstract
Existing autoregressive models follow the two-stage gen-eration paradigm that first learns a codebook in the la-tent space for image reconstruction and then completes the image generation autoregressively based on the learned codebook. However, existing codebook learning simply models all local region information of images without dis-tinguishing their different perceptual importance, which brings redundancy in the learned codebook that not only limits the next stage’s autoregressive model’s ability to model important structure but also results in high train-ing cost and slow generation speed. In this study, we bor-row the idea of importance perception from classical im-age coding theory and propose a novel two-stage frame-work, which consists of Masked Quantization VAE (MQ-VAE) and Stackformer, to relieve the model from model-ing redundancy.
Specifically, MQ-VAE incorporates an adaptive mask module for masking redundant region fea-tures before quantization and an adaptive de-mask mod-ule for recovering the original grid image feature map to faithfully reconstruct the original images after quantiza-tion. Then, Stackformer learns to predict the combination of the next code and its position in the feature map. Com-prehensive experiments on various image generation vali-date our effectiveness and efficiency. Code will be released at https : / / github . com / CrossmodalGroup /
MaskedVectorQuantization. 1.

Introduction
Deep generative models of images have received signif-icant improvements over the past few years and broadly fall into two categories: likelihood-based models, which include VAEs [24], flow-based [36], diffusion models [17] and autoregressive models [40], and generative adversarial
*Zhendong Mao is the corresponding author.
Figure 1. Illustration of our motivation. (a) Existing works model all local regions without distinguishing their perceptual impor-tance in stage 1, which not only brings redundancy (e.g., the textu-ral regions like the background) in the learned codebook but also make the autoregressive models overly focus on modeling this re-dundancy and hinder other important structural regions modeling. (b) The codebook learning in our method only includes the im-portant regions, e.g., the structural regions like corners and edges, since other unimportant ones can be restored even if missing, and thus autoregressive model could focus on modeling these impor-tant regions in stage 2 and results in better generation quality. networks (GANs) [14], which use discriminator networks to distinguish samples from generator networks and real ex-amples. Compared with GANs, likelihood-based models’ training objective, i.e., the negative log-likelihood (NLL) or its upper bound, incentives learning the full data distribution and allows for detecting overfitting.
Among the likelihood-based models, autoregressive models have recently attracted increasing attention for their impressive modeling ability and scalability. Recent autore-gressive image generation [10, 12, 13, 28, 28, 34, 35, 37, 39] follows the two-stage generation paradigm, i.e., the first stage learns a codebook in the latent space for image recon-struction and the second stage completes the image genera-tion in the raster-scan [13] order by autoregressive models
based on the learned codebook. Since codebook learning in the first stage defines the discrete image representation for the next autoregressive modeling, a high-quality code-book is the key to generate high-quality images. Several recent works focus on improving the codebook learning in the first stage, e.g., VQGAN [13] introduces adversarial loss and perceptual loss. ViT-VQGAN [42] introduces a more expressive transformer backbone. RQ-VAE [28] introduces the residual quantization to reduce the resolution of the la-In general, the essence of existing codebook tent space. learning is the modeling of all local region information (i.e., an 8 × 8 or 16 × 16 patch) of images in the dataset, without distinguishing their different perceptual importance.
In this study, we point out that existing codebook learn-ing exists gaps with classical image coding theory [20, 25, 26], the basic idea of which is to remove redundant infor-mation by perceiving the importance of different regions in images. The image coding theory reveals that an ideal image coding method should only encode images’ percep-tually important regions (i.e., which cannot be restored if missing) while discarding the unimportant ones (i.e., which can be restored by other image regions even if missing). The neglect of considering such perceptual importance in exist-ing works poses problems in two aspects, as illustrated in
Figure 1(a): (1) the existence of this large amount of repet-itive and redundant information brings redundancy to the learned codebook, which further makes the autoregressive model in the next stage overly focus on modeling this redun-dancy while overlooking other important regions and finally (2) the redundancy makes degrades generation quality. the autoregressive model need to predict more (redundant) quantized codes to generate images, which significantly in-creases the training cost and decreases the generating speed.
Although the effectiveness and efficiency of image coding theory have been widely validated, how to introduce this idea into codebook learning remains unexplored.
The key of applying image coding theory to codebook learning is to distinguish important image parts from unim-portant ones correctly. Considering that the essential dif-ference between these two sets lies in whether they can be restored if missing, we found that this distinction can be re-alized through the mask mechanism, i.e., the masked part is important if it cannot be faithfully restored, and otherwise unimportant. Based on the above observation, we thereby propose a novel two-stage generation paradigm upon the mask mechanism to relieve the model from modeling redun-dant information. Specifically, we first propose a Masked
Quantization VAE (MQ-VAE) with two novel modules, i.e., an adaptive mask module for adaptively masking redun-dant region features before quantization, and an adaptive de-mask module for adaptively recovering the original grid image feature map to faithfully reconstruct original images after quantization. As for the adaptive mask module, it in-corporates a lightweight content-aware scoring network that learns to measure the importance of each image region fea-ture. The features are then ranked by the importance scores and only a subset of high-scored features will be quantized further. As for the adaptive de-mask module, we design a direction-constrained self-attention to encourage the in-formation flow from the unmasked regions to the masked regions while blocking the reverse, which aims to infer the original masked region information based on unmasked ones. Thanks to the adaptive mask and de-mask mecha-nism, our MQ-VAE removes the negative effects of redun-dant image regions and also shortens the sequence length to achieve both effectiveness and efficiency.
Moreover, since different images have different impor-tant regions, the position of quantized codes in the feature map also dynamically changed. Therefore, we further pro-pose Stackformer for learning to predict the combination of both codes and their corresponding positions. Concretely, the proposed Stackformer stacks a Code-Transformer and a
Position-Transformer, where the Code-Transformer learns to predict the next code based on all previous codes and their positions, and the Position-Transformer learns to pre-dict the next code’s position based on all previous codes’ positions and current code.
With our method, as shown in Figure 1(b), the codebook learning only includes the important regions, e.g., the struc-tural regions, since unimportant ones like the background can be restored even if missing. And therefore the autore-gressive model in the second stage could focus on modeling these important regions and brings better generation quality.
In a nutshell, we summarize our main contributions as:
Conceptually, we point out that existing codebook learning ignores distinguishing the perceptual importance of different image regions, which brings redundancy that degrades generation quality and decreases generation speed.
Technically, (i) we propose MQ-VAE with a novel adaptive mask module to mask redundant region features before quantization and a novel adaptive de-mask module to recover the original feature map after quantization; (ii) we propose a novel Stackformer to predict the combination of both codes and their corresponding positions.
Experimentally, comprehensive experiments on various generations validate our effectiveness and efficiency, i.e., we achieve 8.1%, 2.3%, and 18.6% FID improvement on un-, class-, and text-conditional state-of-the-art at million-level parameters, and faster generation speed compared to existing autoregressive models. 2.