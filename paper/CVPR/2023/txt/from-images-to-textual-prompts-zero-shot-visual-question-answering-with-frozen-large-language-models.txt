Abstract
Large language models (LLMs) have demonstrated ex-cellent zero-shot generalization to new language tasks.
However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnect and task disconnect be-tween the LLM and VQA tasks. End-to-end training on multimodal data may bridge the disconnects, but is inflex-ible and computationally expensive. To address this is-sue, we propose Img2LLM, a plug-and-play module that provides LLM prompts to enable LLMs to perform zero-shot VQA tasks without end-to-end training. We develop
LLM-agnostic models describe image content as exemplar question-answer pairs, which prove to be effective LLM prompts.
Img2LLM offers the following benefits: 1) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outper-form Flamingo [3] by 5.6% on VQAv2. On the challeng-ing A-OKVQA dataset, our method outperforms few-shot methods by as much as 20%. 2) It flexibly interfaces with a wide range of LLMs to perform VQA. 3) It eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reduc-ing cost. Code is available via the LAVIS [28] framework at https://github.com/salesforce/LAVIS/ tree/main/projects/img2llm-vqa. 1.

Introduction
Visual question answering (VQA) [5] is a prominent vision-language task that finds a broad range of real-world applications, such as assisting blind individuals in under-standing their environments. A diverse set of VQA datasets have been proposed, some focusing on image recognition
*Work done while Jiaxian Guo was an intern at Salesforce Research.
[5, 17] and others on logical reasoning [39]. However, hu-man annotations are expensive to obtain and may introduce a variety of human biases [6, 10, 63], making the VQA sys-tem brittle towards new answer styles and question types
[1, 21]. This has led researchers to zero-shot VQA meth-ods [6, 10, 21] that do not require ground-truth question-answer annotations, thereby facilitating more generalizable
VQA systems.
Recently, large language models (LLMs) (e.g., [8, 66]) have demonstrated excellent capabilities to perform tasks with zero in-domain data, conduct logical reasoning, and apply commonsense knowledge in NLP tasks [26, 55, 57].
As a result, recent approaches [3, 52, 61] have resorted to leverage LLMs in zero-shot VQA.
However, applying LLMs to VQA tasks is less than straightforward, due to (1) the modality disconnect between vision and language and (2) the task disconnect between language modeling and question answering. A common technique is to finetune a vision encoder jointly with the
LLM [3, 20, 52] to align the vision and language represen-tation spaces, but this can incur prohibitive computational and data cost. For example, Flamingo [3] finetunes on bil-lions of image-text pairs with thousands of TPUs. Further, the finetuning specializes and introduces strong interdepen-dence between the vision encoder and the LLM. If we need to upgrade the LLM as new versions emerge, the entire model needs to undergo expensive re-training.
In contrast to the end-to-end integration of LLM into a VQA system, this paper proposes a modular VQA sys-tem built on top of frozen off-the-shelf LLMs. This brings two benefits. First, it can reduce the deployment cost and simplify the deployment. Second, upgrading the LLM is straightforward. However, it is challenging to bridge the modality disconnect and task disconnect without end-to-end training. PICa [61] converts images into captions, and pro-vides exemplar QA pairs from training data as prompt to the LLM. However, doing so assumes the existence of an-notated training data and the performance is sensitive to the selection of few-shot exemplars.
We propose Img2LLM, a plug-and-play module that en-ables off-the-shelf LLMs to perform zero-shot VQA. The central insight of Img2LLM is that we can utilize a vision-language model (e.g. BLIP [30]) and a question-generation model to translate the image content into synthetic question-answer (QA) pairs, which are fed to the LLM as part of the prompt. These exemplar QA pairs tackle the modal-ity disconnect by describing the image content verbally, and tackle the task disconnect by demonstrating the QA task to the LLM. Notably, the exemplar QA pairs are con-structed entirely based on the test image and question, ob-viating the need for similar few-shot examples as required by PICa [61], which are not always available in practical zero-shot scenarios. When applied to the open-source OPT language models [66], Img2LLM achieves comparable or superior zero-shot VQA performance to methods that per-form costly end-to-end training.
With this paper, we make the following contributions.
• We propose Img2LLM, a plug-and-play module that converts an image into synthetic question-answer pairs based solely on the current image of the question.
Img2LLM bridges the modality disconnect between language and vision as well as the task discon-nect between language modeling and visual question-answering.
• Img2LLM enables off-the-shelf LLMs to perform zero-shot VQA without costly end-to-end training or specialized textual QA networks [40], thereby allow-ing low-cost and flexible model deployment and pain-less LLM upgrades (Table 3).
• Our experimental results show that the OPT models equipped with Img2LLM achieve zero-shot VQA per-formance that is competitive or superior to the end-to-end trained models. For example, we outperform
Flamingo [3] by 5.6% on VQAv2. We even outper-form many few-shot VQA methods. 2.