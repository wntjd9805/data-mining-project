Abstract
We address the problem of synthesizing novel views from a monocular video depicting a complex dynamic scene. State-of-the-art methods based on temporally varying Neural Ra-diance Fields (aka dynamic NeRFs) have shown impressive results on this task. However, for long videos with com-plex object motions and uncontrolled camera trajectories, these methods can produce blurry or inaccurate renderings, hampering their use in real-world applications. Instead of encoding the entire dynamic scene within the weights of
MLPs, we present a new approach that addresses these lim-itations by adopting a volumetric image-based rendering framework that synthesizes new viewpoints by aggregating features from nearby views in a scene motion–aware manner.
Our system retains the advantages of prior methods in its ability to model complex scenes and view-dependent effects, but also enables synthesizing photo-realistic novel views from long videos featuring complex scene dynamics with unconstrained camera trajectories. We demonstrate signiﬁ-cant improvements over state-of-the-art methods on dynamic scene datasets, and also apply our approach to in-the-wild videos with challenging camera and object motion, where prior methods fail to produce high-quality renderings. 1.

Introduction
Computer vision methods can now produce free-viewpoint renderings of static 3D scenes with spectacular quality. What about moving scenes, like those featuring peo-ple or pets? Novel view synthesis from a monocular video of a dynamic scene is a much more challenging dynamic scene reconstruction problem. Recent work has made progress towards synthesizing novel views in both space and time, thanks to new time-varying neural volumetric representa-tions like HyperNeRF [50] and Neural Scene Flow Fields (NSFF) [35], which encode spatiotemporally varying scene content volumetrically within a coordinate-based multi-layer perceptron (MLP).
However, these dynamic NeRF methods have limitations
that prevent their application to casual, in-the-wild videos.
Local scene ﬂow–based methods like NSFF struggle to scale to longer input videos captured with unconstrained camera motions: the NSFF paper only claims good perfor-mance for 1-second, forward-facing videos [35]. Methods like HyperNeRF that construct a canonical model are mostly constrained to object-centric scenes with controlled camera paths, and can fail on scenes with complex object motion.
In this work, we present a new approach that is scalable to dynamic videos captured with 1) long time duration, 2) un-bounded scenes, 3) uncontrolled camera trajectories, and 4) fast and complex object motion. Our approach retains the ad-vantages of volumetric scene representations that can model intricate scene geometry with view-dependent effects, while signiﬁcantly improving rendering ﬁdelity for both static and dynamic scene content compared to recent methods [35, 50], as illustrated in Fig. 1.
We take inspiration from recent methods for rendering static scenes that synthesize novel images by aggregating local image features from nearby views along epipolar lines [39, 64, 70]. However, scenes that are in motion vi-olate the epipolar constraints assumed by those methods. We instead propose to aggregate multi-view image features in scene motion–adjusted ray space, which allows us to cor-rectly reason about spatio-temporally varying geometry and appearance.
We also encountered many efﬁciency and robustness chal-lenges in scaling up aggregation-based methods to dynamic scenes. To efﬁciently model scene motion across multiple views, we model this motion using motion trajectory ﬁelds that span multiple frames, represented with learned basis functions. Furthermore, to achieve temporal coherence in our dynamic scene reconstruction, we introduce a new tem-poral photometric loss that operates in motion-adjusted ray space. Finally, to improve the quality of novel views, we pro-pose to factor the scene into static and dynamic components through a new IBR-based motion segmentation technique within a Bayesian learning framework.
On two dynamic scene benchmarks, we show that our approach can render highly detailed scene content and sig-niﬁcantly improves upon the state-of-the-art, leading to an average reduction in LPIPS errors by over 50% both across entire scenes, as well as on regions corresponding to dynamic objects. We also show that our method can be applied to in-the-wild videos with long duration, complex scene motion, and uncontrolled camera trajectories, where prior state-of-the-art methods fail to produce high quality renderings. We hope that our work advances the applicability of dynamic view synthesis methods to real-world videos. 2.