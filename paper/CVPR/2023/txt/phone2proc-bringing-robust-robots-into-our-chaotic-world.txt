Abstract
Training embodied agents in simulation has become mainstream for the embodied AI community. However, these agents often struggle when deployed in the physical world due to their inability to generalize to real-world envi-ronments. In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan and conditional proce-dural generation to create a distribution of training scenes that are semantically similar to the target environment. The generated scenes are conditioned on the wall layout and arrangement of large objects from the scan, while also sampling lighting, clutter, surface textures, and instances of smaller objects with randomized placement and materi-als. Leveraging just a simple RGB camera, training with
Phone2Proc shows massive improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav performance
∗ Equal contribution. across a test suite of over 200 trials in diverse real-world environments, including homes, offices, and RoboTHOR.
Furthermore, Phone2Proc’s diverse distribution of gener-ated scenes makes agents remarkably robust to changes in the real world, such as human movement, object rearrange-ment, lighting changes, or clutter. 1.

Introduction
The embodied AI research community has increasingly relied on visual simulators [30, 49, 61] to train embodied agents, with the expectation that the resulting policies can be transferred onto robots in the physical world. While agents trained within simulated environments have shown increased capabilities, progress in successfully deploying these policies onto physical robots has been limited.
Robots trained in simulation must overcome daunting challenges if they are to work effectively in a real space such as our home. First, they must overcome the generalization
gap between the limited set of simulated environments they are trained on and the test scene of interest. In practice, poli-cies trained to perform complex visual tasks with reinforce-ment learning struggle to perform well in novel scenes with novel layouts and object instances. Second, they must work in realistic environments where we live and work, which are often full of clutter, with objects that keep being moved around, with people in and out of the scene and with light-ing changes. In short, we expect our agents to learn from a small set of training data points and generalize not just to a single test data point, but to a distribution of test data that is often semantically distant from the training data. Today’s methods are a ways away from delivering such performant, robust, and resilient robots [9, 12].
In this work, we present PHONE2PROC, which repre-sents a significant advancement towards the goal of creating performant, robust, and resilient robots.
Instead of train-ing policies in simulated environments that may be seman-tically distant from the target physical scene, PHONE2PROC efficiently generates a distribution of training environments that are semantically similar to the target environment. This significantly reduces the generalization gap between the training and target distributions, resulting in more capable robots.
PHONE2PROC utilizes a freely available mobile appli-cation to quickly scan a target environment and create a template of the surroundings, including the scene layout and 3D placements of large furniture. This template is then used to conditionally generate a fully interactive sim-ulated world using ProcTHOR [13], closely mirroring the real-world space. Importantly, this single simulated envi-ronment is then transformed into a distribution of simulated worlds by randomizing objects, their placements, materi-als, textures, scene lighting, and clutter. This allows for the creation of arbitrary large training datasets that are seman-tically similar to the desired real-world scene.
We produce policies for object goal navigation using
PHONE2PROC and deploy them onto a LoCoBot robot in the physical world. We conduct extensive evaluations with 234 episodes in five diverse physical environments: a 3-room and 6-room apartment, a test scene from RoboTHOR-real, a conference room, and a cafeteria. This represents one of the largest and most diverse studies of sim-to-real indoor navigation agents to date. Across all environments,
PHONE2PROC significantly outperforms the state-of-the-art embodied AI model built with ProcTHOR, with an average improvement in success rate from 34.7% to 70.7%. Our robot is able to explore the scene efficiently and effectively navigate to objects of interest, even in the presence of clut-ter, lighting changes, shifts in furniture, and human move-ment. These strong navigation results are achieved using an RGB-only camera, no depth sensors, no localization sensors, and no explicit mapping components.
In summary, we present: (1) PHONE2PROC, a simple and highly effective method for reducing the generalization gap between datasets of simulated environments and a tar-get environment in the real world, (2) large-scale real-world robotics experiments with 234 trials showing significant im-provements for PHONE2PROC compared to state-of-the-art models, and (3) experiments demonstrating the robustness of PHONE2PROC in the face of variations such as changes in lighting, clutter, and human presence. 2.