Abstract
Generative modeling with ordinary differential equa-tions (ODEs) has achieved fantastic results on a variety of applications. Yet, few works have focused on controlling the generated content of a pre-trained ODE-based genera-tive model. In this paper, we propose to optimize the out-put of ODE models according to a guidance function to achieve controllable generation. We point out that, the gra-dients can be efficiently back-propagated from the output to any intermediate time steps on the ODE trajectory, by decomposing the back-propagation and computing vector-Jacobian products. To further accelerate the computation of the back-propagation, we propose to use a non-uniform discretization to approximate the ODE trajectory, where we measure how straight the trajectory is and gather the straight parts into one discretization step. This allows us to save âˆ¼ 90% of the back-propagation time with ignor-able error. Our framework, named FlowGrad, outperforms the state-of-the-art baselines on text-guided image manip-ulation. Moreover, FlowGrad enables us to find global se-mantic directions in frozen ODE-based generative models that can be used to manipulate new images without extra optimization. 1.

Introduction
Controllable generation is very important for image edit-ing [2, 11, 43], text-guided image manipulation [19, 30, 33], etc.. Traditionally, we use GAN and optimize the latent em-bedding with the desirable objective functions [2, 7, 11, 29, 31, 38, 49]. But the disadvantage is that, it is difficult to embed the image into the GAN space honestly and the per-formance is limited by the pre-trained GANs, which suffer from training instability and mode collapse.
Recently, diffusion models (or stochastic differential equation (SDE)-based generative models) has been popu-lar [9, 12, 30, 39, 40, 42, 45], and there has been a number of works on controlled generation based on diffusion models, such as [9, 28]. But due to the diffusion noise, it is hard to accurately control the output, especially when it comes to optimizing a complex loss function such as CLIP [35]. To achieve controlled generation, existing methods either re-quires training a noised version of the guidance [9, 30, 40], or fine-tuning the whole diffusion model [13, 19].
In contrast, ordinary differential equation (ODE)-based generative models represent a simpler alternative than dif-fusion without involving the diffusion noise and the Ito calculus machinery. Recently, it has been shown that 1)
ODEs can be trained directly without resorting to SDEs, and 2) ODE can perform comparable or even better than
SDE [16, 21, 36, 44].
Due to the deterministic nature of ODEs, they form an ideal model for controlled generation, as they enjoy both the rich latent space as SDEs and the explicit optimization framework as GANs. The goal of this work is to fully ex-plore its potential in terms of controlled generation, with un-conditioned pre-trained ODEs. Technically, 1) We present a simple way to control the output of ODE-based deep gener-ative models with gradients; 2) We present a novel strategy to speedup the gradient computation by explore the straight-ness of ODE trajectories. By measuring the straightness at each time step during the simulation with Euler discretiza-tion, we can approximate the ODE trajectory with a few-step non-uniform discretization, and consequently reduce a great amount of time in back-propagation.
Our fast gradient computation scheme, named Flow-Grad, allows us to efficiently control the generated con-tents of ODEs with any differentiable loss functions.
In particular, we test FlowGrad on a challenging objective function, the CLIP loss, to manipulate user-provided im-ages with text prompts. Moreover, by optimizing a set of training images together, FlowGrad can find semantically meaningful global directions in pre-trained ODE models, which allow manipulating new images for free. Equipped with advanced ODE-based generative models, FlowGrad outperforms state-of-the-art CLIP-guided diffusion models and GANs. 2.