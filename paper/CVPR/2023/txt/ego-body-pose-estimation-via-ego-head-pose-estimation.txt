Abstract
Estimating 3D human motion from an egocentric video sequence plays a critical role in human behavior understand-ing and has various applications in VR/AR. However, naively learning a mapping between egocentric videos and human motions is challenging, because the user’s body is often un-observed by the front-facing camera placed on the head of the user. In addition, collecting large-scale, high-quality datasets with paired egocentric videos and 3D human mo-tions requires accurate motion capture devices, which often limit the variety of scenes in the videos to lab-like environ-ments. To eliminate the need for paired egocentric video and human motions, we propose a new method, Ego-Body Pose
Estimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem into two stages, connected by the head motion as an intermediate representation. EgoEgo first integrates SLAM and a learning approach to estimate accu-rate head motion. Subsequently, leveraging the estimated head pose as input, EgoEgo utilizes conditional diffusion
† indicates equal contribution. to generate multiple plausible full-body motions. This dis-entanglement of head and body pose eliminates the need for training datasets with paired egocentric videos and 3D human motion, enabling us to leverage large-scale egocen-tric video datasets and motion capture datasets separately.
Moreover, for systematic benchmarking, we develop a syn-thetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric videos and human motion. On both ARES and real data, our EgoEgo model performs significantly better than the current state-of-the-art methods. 1.

Introduction
Estimating 3D human motion from an egocentric video, which records the environment viewed from the first-person perspective with a front-facing monocular camera, is criti-cal to applications in VR/AR. However, naively learning a mapping between egocentric videos and full-body human motions is challenging for two reasons. First, modeling this complex relationship is difficult; unlike reconstruction motion from third-person videos, the human body is often out of view of an egocentric video. Second, learning this
mapping requires a large-scale, diverse dataset containing paired egocentric videos and the corresponding 3D human poses. Creating such a dataset requires meticulous instru-mentation for data acquisition, and unfortunately, such a dataset does not currently exist. As such, existing works have only worked on small-scale datasets with limited mo-tion and scene diversity [22, 47, 48].
We introduce a generalized and robust method, EgoEgo, to estimate full-body human motions from only egocentric video for diverse scenarios. Our key idea is to use head motion as an intermediate representation to decompose the problem into two stages: head motion estimation from the input egocentric video and full-body motion estimation from the estimated head motion. For most day-to-day activities, humans have an extraordinary ability to stabilize the head such that it aligns with the center of mass of the body [13], which makes head motion an excellent feature for full-body motion estimation. More importantly, the decomposition of our method removes the need to learn from paired egocentric videos and human poses, enabling learning from a combina-tion of large-scale, single-modality datasets (e.g., datasets with egocentric videos or 3D human poses only), which are commonly and readily available.
The first stage, estimating the head pose from an ego-centric video, resembles the localization problem. However, directly applying the state-of-the-art monocular SLAM meth-ods [33] yields unsatisfactory results, due to the unknown gravity direction and the scaling difference between the es-timated space and the real 3D world. We propose a hybrid solution that leverages SLAM and learned transformer-based models to achieve significantly more accurate head motion estimation from egocentric video. In the second stage, we generate the full-body motion based on a diffusion model conditioned on the predicted head pose. Finally, to evaluate our method and train other baselines, we build a large-scale synthetic dataset with paired egocentric videos and 3D hu-man motions, which can also be useful for future work on visuomotor skill learning and sim-to-real transfer.
Our work makes four main contributions. First, we pro-pose a decomposition paradigm, EgoEgo, to decouple the problem of motion estimation from egocentric video into two stages: ego-head pose estimation, and ego-body pose estimation conditioned on the head pose. The decomposi-tion lets us learn each component separately, eliminating the need for a large-scale dataset with two paired modalities.
Second, we develop a hybrid approach for ego-head pose estimation, integrating the results of monocular SLAM and learning. Third, we propose a conditional diffusion model to generate full-body poses conditioned on the head pose.
Finally, we contribute a large-scale synthetic dataset with both egocentric videos and 3D human motions as a test bed to benchmark different approaches and showcase that our method outperforms the baselines by a large margin. 2.