Abstract
Most neural networks for computer vision are designed to infer using RGB images. However, these RGB images are commonly encoded in JPEG before saving to disk; decoding them imposes an unavoidable overhead for RGB networks.
Instead, our work focuses on training Vision Transformers (ViT) directly from the encoded features of JPEG. This way, we can avoid most of the decoding overhead, accelerating data load. Existing works have studied this aspect but they focus on CNNs. Due to how these encoded features are structured, CNNs require heavy modification to their archi-tecture to accept such data. Here, we show that this is not the case for ViTs. In addition, we tackle data augmentation directly on these encoded features, which to our knowledge, has not been explored in-depth for training in this setting.
With these two improvements – ViT and data augmentation – we show that our ViT-Ti model achieves up to 39.2% faster training and 17.9% faster inference with no accuracy loss compared to the RGB counterpart. 1 1.

Introduction
Neural networks that process images typically receive their inputs as regular grids of RGB pixel values. This spatial-domain representation is intuitive, and matches the way that images are displayed on digital devices (e.g. LCD panels with RGB sub-pixels). However, images are often stored on disk as compressed JPEG files that instead use frequency-domain representations for images. In this paper we design neural networks that can directly process images encoded in the frequency domain.
Networks that process frequency-domain images have the potential for much faster data loading. JPEG files store image data using Huffman codes; these are decoded to (frequency-domain) discrete cosine transform (DCT) coef-ficients then converted to (spatial-domain) RGB pixels be-fore being fed to the neural network (Fig. 1). Networks that process DCT coefficients can avoid the expensive DCT to
RGB conversion; we show in Sec. 3 that this can reduce the theoretical cost of data loading by up to 85%. Data is typi-Figure 1. Our proposed training process. The typical process re-quires full decoding as well as patch extraction to train. In contrast, our process does not since the DCT coefficients are already saved in block-wise fashion. As ViTs work on image patches, we can directly feed these coefficients to the network. cally loaded by the CPU while the network runs on a GPU or other accelerator; more efficient data loading can thus reduce CPU bottlenecks and accelerate the entire pipeline.
We are not the first to design networks that process frequency-domain images. The work of Gueguen et al. [1] and Xu et al. [2] are most similar to ours: they show how standard CNN architectures such as ResNet [3] and Mo-bileNetV2 [4] can be modified to input DCT rather than
RGB and trained to accuracies comparable to their standard formulations. We improve upon these pioneering efforts in two key ways: architecture and data augmentation.
Adapting a CNN architecture designed for RGB inputs to instead receive DCT is nontrivial. The DCT representa-tion of an H ×W ×3 RGB image consists of a H 8 ×8×8 tensor of luma data and two H 16× W 16 ×8×8 tensors of chroma data. The CNN architecture must be modified both to accept lower-resolution inputs (e.g. by skipping the first few stages of a ResNet50 and adding capacity to later stages) and to ac-cept heterogeneously-sized luma and chroma data (e.g. by encoding them with separate pathways). 8 × W 1Code available at https://github.com/JeongsooP/RGB-no-more
We overcome these challenges by using Vision Trans-formers (ViTs) [5] rather than CNNs. ViTs use a patch embedding layer to encode non-overlapping image patches into vectors, which are processed using a Transformer [6].
This is a perfect match to DCT representations, which also represent non-overlapping RGB image patches as vectors.
We show that ViTs can be easily adapted to DCT inputs by modifying only the initial patch embedding layer and leav-ing the rest of the architecture unchanged.
Data augmentation is critical for training accurate net-works; this is especially true for ViTs [7–9]. However, standard image augmentations such as resizing, cropping, flipping, color jittering, etc. are expressed as transforma-tions on RGB images; prior work [1, 2] on neural networks with DCT inputs thus implement data augmentation by con-verting DCT to RGB, augmenting in RGB, then converting back to DCT before passing the image to the network. This negates all of the potential training-time efficiency gains of using DCT representations; improvements can only be real-ized during inference when augmentations are not used.
We overcome this limitation by augmenting DCT image representations directly, avoiding any DCT to RGB conver-sions during training. We show how all image augmenta-tions used by RandAugment [10] can be implemented on
DCT representations. Some standard augmentations such as image rotation and shearing are costly to implement in DCT, so we also introduce several new augmentations which are natural for DCT.
Using these insights, we train ViT-S and ViT-Ti mod-els on ImageNet [11, 12] which match the accuracy of their
RGB counterparts. Compared to an RGB equivalent, our
ViT-Ti model is up to 39.2% faster per training iteration and 17.9% faster during inference. We believe that these results demonstrate the benefits of neural networks that in-gest frequency-domain image representations.
Figure 2. Process of applying 8 × 8 DCT to the input image. The input image is sliced into 8 × 8 patches and the DCT is applied to each patch. The DCT bases are shown on the right.
Data augmentation directly on the frequency domain has been studied in several works. Gueguen et al. [1] sug-gested augmenting on RGB and converting back to DCT.
Wiles et al. [34] used an augmentation network that is tai-lored towards their neural compressor. Others focus on a more classical approach such as sharpening [51–57], resiz-ing [58–65], watermarking [66–72], segmentation [73–77], flip and 90-degree rotation [21, 78], scalar operations [79], and forgery detection [80–82] via analyzing the properties of JPEG and DCT. However, to our knowledge, no other works have thoroughly studied the effect of DCT augmen-tation during frequency-domain training.
Speeding up ViTs has been rigorously studied. These ei-ther utilize CNN-hybrid architectures to reduce computa-tion [83–89], speed up attention by sparsifying [90–93], lin-earizing [94, 95], or through other techniques such as prun-ing [96–99], bottlenecking [100], or approximation [101].
While we only consider the plain ViT architecture in this paper, we want to emphasize that faster data loading is im-perative to fully take advantage of these speed-ups as mod-els can only infer as fast as the data loading speed. 2.