Abstract
Interpretable methods based on prototypical patches rec-ognize various components in an image in order to explain their reasoning to humans. However, existing prototype-based methods can learn prototypes that are not in line with human visual perception, i.e., the same prototype can refer to different concepts in the real world, making interpretation not intuitive. Driven by the principle of explainability-by-design, we introduce PIP-Net (Patch-based Intuitive Proto-types Network): an interpretable image classification model that learns prototypical parts in a self-supervised fashion which correlate better with human vision. PIP-Net can be interpreted as a sparse scoring sheet where the pres-ence of a prototypical part in an image adds evidence for a class. The model can also abstain from a decision for out-of-distribution data by saying “I haven’t seen this before”. We only use image-level labels and do not rely on any part an-notations. PIP-Net is globally interpretable since the set of learned prototypes shows the entire reasoning of the model.
A smaller local explanation locates the relevant prototypes in one image. We show that our prototypes correlate with ground-truth object parts, indicating that PIP-Net closes the “semantic gap” between latent space and pixel space.
Hence, our PIP-Net with interpretable prototypes enables users to interpret the decision making process in an intuitive, faithful and semantically meaningful way. Code is available at https://github.com/M-Nauta/PIPNet. 1.

Introduction
Deep neural networks are dominant in computer vision, but there is a high demand for understanding the reasoning of such complex models [23,30]. Consequently, interpretability and explainability have grown in importance. In contrast to the common post-hoc explainability that reverse-engineers a black box, we argue that we should take interpretability as a design starting point for in-model explainability. The recognition-by-components theory [1] describes how hu-mans recognize objects by segmenting them into multiple components. We mimic this intuitive line of reasoning in an intrinsically interpretable image classifier. Specifically, our PIP-Net (Patch-based Intuitive Prototypes Network) au-tomatically identifies semantically meaningful components, while only having access to image-level class labels and not relying on additional part annotations. The components are “prototypical parts” (prototypes) visualized as image patches, since exemplary natural images are more informa-tive to humans than generated synthetic images [2]. PIP-Net is globally interpretable and designed to be highly intuitive as it uses simple scoring-sheet reasoning: the more relevant prototypical parts for a specific class are present in an image, the more evidence for that class is found, and the higher its score. When no relevant prototypes are present in the image, with e.g. out-of-distribution data, PIP-Net will abstain from a decision. PIP-Net is therefore able to say “I haven’t seen this before” (see Fig. 2). Additionally, following the principle of isolation of functional properties for aligning human and machine vision [5], the reasoning of PIP-Net is separated into multiple steps. This simplifies human identification of reasons for (mis)classification.
Recent interpretable part-prototype models are ProtoP-Net [3], ProtoTree [24], ProtoPShare [29] and ProtoPool [28].
These part-prototype models are only designed for fine-grained image recognition tasks (birds and car types) and lack “semantic correspondence” [17] between learned proto-types and human concepts. This “semantic gap” in prototype-based methods between similarity in latent space and input space was also found by others [9, 14]. We hypothesize that the main cause of the semantic gap is the fact that exist-ing part-prototype models only regularize interpretability on class-level, since their underlying assumption is that (parts
Figure 1. Toy dataset with two classes (left). Existing models can learn representations of prototypes that do not align with human visually perceived similarity (center). Our objective is to learn prototypes that represent concepts that also look similar to humans (right). of) images from the same class have the same prototypes.
This assumption may however not hold, leading to similarity in latent space which does not correspond to visually per-ceived similarity. Consider the example in Fig. 1, where we have re-labeled images from a clipart dataset [43] to create a binary classification task: the two kids are happy when the sun or dog is present, and sad when there is neither a sun nor a dog. Hence, the classes are ‘sun OR dog’ and ‘NOT (sun OR dog)’. Intuitively, an easy-to-interpret model should learn two prototypes: one for the sun and one for the dog.
However, existing interpretable part-prototype models, such as ProtoPNet [3] and ProtoTree [24], optimize images of the same class to have the same prototypes. They could, there-fore, learn a single prototype that represents both the sun and the dog, especially when the model is optimized to have few prototypes (see Fig. 1, center). The model’s perception of patch similarity may thus not be in line with human visual perception, leading to the perceived “semantic gap”.
To address the gap between latent and pixel space, we present PIP-Net: an interpretable model that is designed to be intuitive and optimized to correlate with human vision.
A sparse linear layer connects learned interpretable proto-typical parts to classes. A user only needs to inspect the prototypes and their relation to the classes in order to inter-pret the model. We restrict the weights of the linear layer to be non-negative, such that the presence of a class-relevant prototype increases the evidence for a class. The linear layer can be interpreted as a scoring sheet: the score for a class is the sum of all present prototypes multiplied by their weights.
A local explanation (Fig. 2 and Fig. 3) explains a specific prediction and shows which prototypes were found at which locations in the image. The global explanation provides an overall view of the model’s decision layer, consisting of the sparse weights between classes and their relevant prototypes.
Because of this interpretable and predictive linear layer, we ensure a direct relation between the prototypes and the clas-sification, and thereby prevent unfaithful explanations which can arise with local or post-hoc XAI methods [16].
Our Contributions: 1. We present the Patch-based Intuitive Prototypes Net-work (PIP-Net): an intrinsically interpretable image classifier, driven by three explainability requirements: the model should be intuitive, compact and able to han-dle out-of-distribution data. 2. PIP-Net has a surprisingly simple architecture and is trained with novel regularization for learning prototype similarity that better correlates with human visual per-ception, thereby closing a perceived semantic gap. 3. PIP-Net acts as a scoring sheet and therefore can detect that an image does not belong to any class or that it belongs to multiple classes. 4. Instead of specifying the number of prototypes be-forehand as in ProtoPNet [3], ProtoPool [28] and Tes-Net [35], PIP-Net only needs an upper bound on the number of prototypes and selects as few prototypes as possible for good classification accuracy with compact explanations, reaching sparsity ratios > 99%. 2.