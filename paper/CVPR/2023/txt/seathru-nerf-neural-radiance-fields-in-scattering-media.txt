Abstract
Research on neural radiance fields (NeRFs) for novel view generation is exploding with new models and exten-sions. However, a question that remains unanswered is what happens in underwater or foggy scenes where the medium strongly influences the appearance of objects. Thus far,
NeRF and its variants have ignored these cases. However, since the NeRF framework is based on volumetric render-ing, it has inherent capability to account for the medium’s effects, once modeled appropriately. We develop a new ren-dering model for NeRFs in scattering media, which is based on the SeaThru image formation model, and suggest a suit-able architecture for learning both scene information and medium parameters. We demonstrate the strength of our method using simulated and real-world scenes, correctly rendering novel photorealistic views underwater. Even more excitingly, we can render clear views of these scenes, removing the medium between the camera and the scene and reconstructing the appearance and depth of far objects, which are severely occluded by the medium. Our code and unique datasets are available on the project’s website. 1.

Introduction
The pioneering work of Mildenhall et al. [25] on Neural
Radiance Fields (NeRFs) has tremendously advanced the field of Neural Rendering, due to its flexibility and unprece-dented quality of synthesized images. Yet, the formulations of the original NeRF [25] and its followup variants assume that images were acquired in clear air, i.e., in a medium that does not scatter or absorb light in a significant man-ner and that the acquired image is composed solely of the object radiance. The NeRF formulation is based on volu-metric rendering equations that take into account sampled points along 3D rays. Assuming a clear air environment, an implicit assumption, which is often enforced explicitly with dedicated loss components [5], is that a single opaque (high density) object is encountered per ray, with zero density be-tween the camera and the object.
In stark contrast to clear air case, when the medium is absorbing and / or scattering (e.g., haze, fog, smog, and all aquatic habitats), the volume rendering equation has a true volumetric meaning, as the entire volume, and not only the object, contributes to image intensity. As the NeRF model estimates color and density at every point of a scene, it lends itself perfectly to general volumetric rendering, given that the appropriate rendering model is used. Here, we bridge this gap with SeaThru-NeRF, a framework that incorporates a rendering model that takes into account scattering media.
This is achieved by assigning separate color and density parameters to the object (scene) and the medium, within the
NeRF framework. Our approach adopts the SeaThru un-derwater image formation model [1, 3] to account for scat-tering media. SeaThru is a generalization of the standard wavelength-independent attenuation (e.g., fog) image for-mation model, where two different wideband coefficients are used to represent the medium, which is more accurate when attenuation is wavelength-dependent (as in all wa-ter bodies and under some atmospheric conditions). In our model, the medium parameters are separate per color chan-nel, and are learned functions of the viewing angles, enforc-ing them to be constant only along 3D rays in the scene.
Attempting to optimize existing NeRFs on scenes with scattering medium results in cloud-like objects floating in space, while our formulation enables the network to learn the correct representation of the entire 3D volume, that con-sists of both the scene and the medium. Our experiments demonstrate that SeaThru-NeRF produces state-of-the-art photorealistic novel view synthesis on simulated and chal-lenging real-world scenes (see Fig. 1) with scattering media, that include complex geometries and appearances. In addi-tion, it enables: 1. Color restoration of the scenes as if they were not im-aged through a medium, as our modeling allows full sepa-ration of object appearance from the medium effects. 2. Estimation of 3D scene structure which surpasses that of structure-from-motion (SFM) or current NeRFs, espe-cially in far areas of bad visibility, as we jointly reconstruct and reason for the geometry and medium. 3. Estimation of wideband medium parameters, which are informative properties of the captured environment, and potentially allowing simulation under different conditions. 2.