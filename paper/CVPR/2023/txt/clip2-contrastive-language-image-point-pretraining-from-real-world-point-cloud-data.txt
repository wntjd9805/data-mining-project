Abstract
Contrastive Language-Image Pre-training, benefiting from large-scale unlabeled text-image pairs, has demon-strated great performance in open-world vision understand-ing tasks. However, due to the limited Text-3D data pairs, adapting the success of 2D Vision-Language Models (VLM) to the 3D space remains an open problem. Existing works that leverage VLM for 3D understanding generally resort to constructing intermediate 2D representations for the 3D data, but at the cost of losing 3D geometry information.
To take a step toward open-world 3D vision understand-ing, we propose Contrastive Language-Image-Point Cloud
Pretraining (CLIP2) to directly learn the transferable 3D point cloud representation in realistic scenarios with a novel proxy alignment mechanism. Specifically, we exploit naturally-existed correspondences in 2D and 3D scenarios, and build well-aligned and instance-based text-image-point proxies from those complex scenarios. On top of that, we propose a cross-modal contrastive objective to learn se-mantic and instance-level aligned point cloud representa-∗Equal contribution.
University of Science and Technology
Kong huawei.com 4Sun Yat-san University 1Huawei Noah’s Ark Lab 2Hong Kong 3The Chinese University of Hong
†Corresponding Author: xu.hang@ tion. Experimental results on both indoor and outdoor sce-narios show that our learned 3D representation has great transfer ability in downstream tasks, including zero-shot and few-shot 3D recognition, which boosts the state-of-the-art methods by large margins. Furthermore, we provide analyses of the capability of different representations in real scenarios and present the optional ensemble scheme. 1.

Introduction
Powerful 3D point cloud representation plays a cru-cial role in various real-world applications, e.g., 3D object recognition and detection [10, 20, 31, 40, 44]. Compared to 2D images, 3D point cloud provides specific informa-tion like accurate geometry that is robust to illumination changes. However, current methods [25, 40] that learn 3D representations generally rely on the predefined number of object categories and require plenty of labor-intensive an-notations. Those learned 3D representations are insufficient for safety-critical scenarios like self-driving which includes a long-tail class distribution far beyond the predefined tax-onomy. Therefore, it is highly demanded to learn a transfer-able 3D representation equipped with zero-shot recognition ability in vocabulary scalable real-world scenes. Figure 1 shows an open-world recognition example by our CLIP2 in
outdoor and indoor scenes, where the 3D objects can be classified with the correlation alignment between 3D repre-sentations and open-world vocabularies.
The critical ingredient of open-world understanding is that the models learn sufficient knowledge to obtain general representations. To achieve this, recent Vision-Language
Models (VLM) [14, 27, 38] leverage Internet-scale text-image pairs to conduct vision-language pretraining, which facilitates transferable 2D representation and demonstrates promising performance in 2D open-vocabulary tasks. How-ever, 3D vision-language pretraining remains unexplored due to the limitation of existing 3D datasets in diversity and scale compared to the massive data sources in 2D counter-parts [14,15,27,38]. Though some recent works [12,13,43] try to avoid this problem by transferring the pretrained 2D
VLM into the intermediate representation including pro-jected image patches [12, 18] or depth maps [1, 43], those representations suffer from the loss of 3D geometric infor-mation and limited viewpoints under realistic scenarios. Es-pecially the camera images are only sometimes available due to the sensor failure in 3D scenes. We believe the 3D representation based on original point cloud data retains most information and is the optimal solution for 3D real world understanding, which requires a rethink of learning the transferable 3D representation under realistic scenarios.
To this end, we propose a Contrastive Language-Image-Point cloud Pretraining framework, short for CLIP2, which directly aligns 3D space with broader raw text and advances the 3D representation learning into an open-world era.
Our learning process can be decomposed into two stages:
Firstly, we introduce a Triplet Proxy Collection to alleviate the limitation of accessible pretraining data by construct-ing language-image-point triplets from real-world scenes.
Since the large-scale realistic 3D datasets for outdoor driv-ing [2,19] and indoor scenarios [9,32] are collected in open-world, it contains huge amounts of realistic objects that vary in semantics and diversity. Thus we consider them as potential pretraining data sources without extra human supervision. Specifically, we propose “Proxy” instances as the bridges between language descriptions, 2D images and 3D point clouds. Enabled by a well-aligned VLM, a scal-able caption list and the geometry transformation between 2D and 3D, we automatically create more than 1 million triplets to facilitate pretraining. Secondly, we further pro-pose a Cross-Modal Pretraining scheme to jointly optimize the feature space alignments of three modalities, i.e.point cloud, language and image. It contains both the contrastive learning objective of semantic-level text-3D correlation and instance-level image-3D correlation, which contributes to better transferability of learned 3D representation.
We study the transferable capability of CLIP2 by bench-marking the zero-shot recognition performance on four pop-ular indoor and outdoor real-world datasets, and find a sig-nificant improvement over current methods, achieving Top1 accuracy 61.3% on SunRGBD [32], 43.8% on ScanNet [9]), 28.8% on nuScenes [2] and 56.0% on ONCE [19]. For a fair comparison with existing methods [1, 13, 36, 43], we conduct zero-shot and few-shot classification on single ob-ject dataset ScanObjectNN [34] and find consistent dom-inance, 16.1% relative improvement on zero-shot classifi-cation over previous state-of-the-art method [13]. To vali-date the vocabulary-increasing ability of CLIP2, we report the quantity results and visualizations to show the improved discovery of the long-tail categories. Moreover, we make ablations and analisis on different representations, and in-vestigate ensembling alternatives to merge complementary knowledge of all available representations in realistic appli-cations. Our contributions can be summarized as follows:
• We propose a novel CLIP2 framework that aligns 3D space with open-world language representation, facili-tating zero-shot transfer in realistic scenarios.
• We present a Triplet Proxies Collection scheme in real-world scenes, which alleviates the shortage of text-3D data sources and facilitates the pretraining methods.
• CLIP2 jointly optimizes the correlation alignment be-tween point cloud, language and image by proposed cross-modal pretraining mechanism, which enhances the transferability of learned 3D representation.
• Our CLIP2 achieves the state-of-the-art zero-shot transfer performance on 5 datasets (indoor/outdoor scenes and single-object) and shows quality results on vocabulary-increasing discovery in real world. 2.