Abstract
Federated learning-based semantic segmentation (FSS) has drawn widespread attention via decentralized training on local clients. However, most FSS models assume cate-gories are fixed in advance, thus heavily undergoing forget-ting on old categories in practical applications where local clients receive new categories incrementally while have no memory storage to access old classes. Moreover, new clients collecting novel classes may join in the global training of
FSS, which further exacerbates catastrophic forgetting. To surmount the above challenges, we propose a Forgetting-Balanced Learning (FBL) model to address heterogeneous forgetting on old classes from both intra-client and inter-client aspects. Specifically, under the guidance of pseudo labels generated via adaptive class-balanced pseudo label-ing, we develop a forgetting-balanced semantic compensa-tion loss and a forgetting-balanced relation consistency loss to rectify intra-client heterogeneous forgetting of old cate-gories with background shift. It performs balanced gradient propagation and relation consistency distillation within lo-cal clients. Moreover, to tackle heterogeneous forgetting from inter-client aspect, we propose a task transition mon-itor. It can identify new classes under privacy protection and store the latest old global model for relation distillation.
Qualitative experiments reveal large improvement of our model against comparison methods. The code is available at https://github.com/JiahuaDong/FISS. 1.

Introduction
Federated learning (FL) [13,20,22,44] is a remarkable de-centralized training paradigm to learn a global model across
*Equal contributions. †The corresponding author is Prof. Yang Cong.
‡This work was supported in part by the National Nature Science Foun-dation of China under Grant 62127807, 62225310 and 62133005.
Figure 1. Exemplary FISS setting for medical diagnosis. Hundreds of hospitals including newly-joined ones receive new classes incre-mentally according to their own preference. FISS aims to segment new diseases consecutively via collaboratively learning a global segmentation model on private medical data of different hospitals. distributed local clients without accessing their private data.
Under privacy preservation, it has achieved rapid develop-ment in semantic segmentation [4, 8, 30] by training on mul-tiple decentralized local clients to alleviate the constraint of data island that requires enormous finely-labeled pixel annotations [25]. As a result, federated learning-based se-mantic segmentation (FSS) [28,29] significantly economizes annotation costs in data-scarce scenarios via training a global segmentation model on private data of different clients [29].
However, existing FSS methods [16, 25, 28, 29] unrealisti-cally assume that the learned foreground classes are static and fixed over time, which is impractical in real-world dy-namic applications where local clients receive streaming data of new categories consecutively. To tackle this issue, existing
FSS methods [28, 29, 35] typically enforce local clients to store all samples of previously-learned old classes, and then learn a global model to segment new categories continually via FL. Nevertheless, it requires large computation and mem-ory overhead as new classes arrive continuously, limiting the application ability of FSS methods [16, 28]. If local clients have no memory to store old classes, existing FSS meth-ods [16, 29] significantly degrade segmentation behavior on old categories (i.e., catastrophic forgetting [40, 47, 48]) when learning new classes incrementally. In addition, the pixels la-beled as background in the current learning task may belong to old classes from old tasks or new foreground classes from future tasks. This phenomenon is also known as background shift [11, 36] that heavily aggravates heterogeneous forget-ting speeds on old categories. More importantly, in practical scenarios, new local clients receiving new categories incre-mentally may join in global FL training irregularly, thus further exacerbating catastrophic forgetting to some extent.
To surmount the above real-world scenarios, we pro-pose a novel practical problem called Federated Incremental
Semantic Segmentation (FISS), where local clients collect new categories consecutively according to their preferences, and new local clients collecting unseen novel classes partici-pate in global FL training irregularly. In the FISS settings, the class distributions are non-independent and identically distributed (Non-IID) across different clients, and training data of old classes is unavailable for all local clients. FISS aims to train a global incremental segmentation model via collaborative FL training on local clients while addressing catastrophic forgetting. In this paper, we use medical lesions segmentation [25, 29] as an example to better illustrate FISS, as shown in Figure 1. Hundreds of hospitals, as well as newly joined ones, collect unseen/new medical lesions continuously in clinical diagnosis. Considering privacy preservation, it is desired for these hospitals to learn a global segmentation modal via FL without accessing each other’s data [44, 56].
A naive solution for FISS problem is to directly integrate incremental semantic segmentation [1,11,53] and FL [19,50] together. Nevertheless, such a trivial solution requires global server to have strong human prior about which and when local clients can collect new categories, so that global model learned in the latest old task can be stored by local clients to address forgetting on old classes via knowledge distillation
[18, 43]. Considering privacy preservation in the FISS, this privacy-sensitive prior knowledge cannot be shared between local clients and global server. As a result, this naive solution severely suffers from intra-client heterogeneous forgetting on different old classes caused by background shift [1,11,36,53], and inter-client heterogeneous forgetting across different clients brought by Non-IID class distributions.
To overcome the above-mentioned challenges, we de-velop a novel Forgetting-Balanced Learning (FBL) model, which alleviates heterogeneous forgetting on old classes from intra-client and inter-client perspectives. Specifically, to tackle intra-client heterogeneous forgetting caused by background shift, we propose an adaptive class-balanced pseudo labeling to adaptively generate confident pseudo la-bels for old classes. Under the guidance of pseudo labels, we propose a forgetting-balanced semantic compensation loss to rectify different forgetting of old classes with background shift via considering balanced gradient propagation of local clients. In addition, a forgetting-balanced relation consis-tency loss is designed to distill underlying category-relation consistency between old and new classes for intra-client het-erogeneous forgetting compensation. Moreover, considering addressing heterogeneous forgetting from inter-client aspect, we develop a task transition monitor to automatically identify new classes without any human prior, and store the latest old model from global perspective for relation consistency dis-tillation. Experiments on segmentation datasets reveal large improvement of our model over comparison methods. We summarize the main contributions of this work as follows:
• We propose a novel practical problem called Federated
Incremental Semantic Segmentation (FISS), where the major challenges are intra-client and inter-client heteroge-neous forgetting on old categories caused by intra-client background shift and inter-client Non-IID distributions.
• We propose a Forgetting-Balanced Learning (FBL) model to address the FISS problem via surmounting heteroge-neous forgetting from both intra-client and inter-client aspects. As we all know, in the FL field, this is a pioneer attempt to explore a global continual segmentation model.
• We develop a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to tackle intra-client heterogeneous forgetting across old classes, under the guidance of confident pseudo labels generated via adaptive class-balanced pseudo labeling.
• We design a task transition monitor to surmount inter-client heterogeneous forgetting by accurately recognizing new classes under privacy protection and storing the latest old model from global aspect for relation distillation. 2.