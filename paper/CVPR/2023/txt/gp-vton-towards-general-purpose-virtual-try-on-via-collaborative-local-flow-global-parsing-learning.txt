Abstract
Image-based Virtual Try-ON aims to transfer an in-shop garment onto a specific person. Existing methods employ a global warping module to model the anisotropic defor-mation for different garment parts, which fails to preserve the semantic information of different parts when receiving challenging inputs (e.g, intricate human poses, difficult gar-ments). Moreover, most of them directly warp the input garment to align with the boundary of the preserved re-gion, which usually requires texture squeezing to meet the boundary shape constraint and thus leads to texture dis-tortion. The above inferior performance hinders existing methods from real-world applications. To address these problems and take a step towards real-world virtual try-on, we propose a General-Purpose Virtual Try-ON framework, named GP-VTON, by developing an innovative Local-Flow
Global-Parsing (LFGP) warping module and a Dynamic
Gradient Truncation (DGT) training strategy. Specifically, compared with the previous global warping mechanism,
LFGP employs local flows to warp garments parts individ-ually, and assembles the local warped results via the global garment parsing, resulting in reasonable warped parts and a semantic-correct intact garment even with challenging in-puts.On the other hand, our DGT training strategy dynam-ically truncates the gradient in the overlap area and the warped garment is no more required to meet the bound-ary constraint, which effectively avoids the texture squeez-ing problem. Furthermore, our GP-VTON can be easily extended to multi-category scenario and jointly trained by using data from different garment categories. Extensive ex-periments on two high-resolution benchmarks demonstrate our superiority over the existing state-of-the-art methods. 1 1Corresponding author is Xiaodan Liang. Code is available at gp-vton.
1.

Introduction
The problem of Virtual Try-ON (VTON), aiming to transfer a garment onto a specific person, is of particu-lar importance for the nowadays e-commerce and the fu-ture metaverse. Compared with the 3D-based solutions [2, 14, 16, 28, 34] which rely upon 3D scanning equipment or labor-intensive 3D annotations, the 2D image-based meth-ods [1, 9, 12, 17, 19, 20, 29, 30, 32, 36, 38–40, 43], which directly manipulate on the images, are more practical for the real world scenarios and thus have been intensively ex-plored in the past few years.
Although the pioneering 2D image-based VTON meth-ods [12, 19, 29] can synthesize compelling results on the widely used benchmarks [6, 8, 18], there still exist some deficiencies preventing them from the real-world scenarios, which we argue mainly contain three-folds. First, existing methods have strict constraints on the input images, and are prone to generate artifacts when receiving challenging in-puts. To be specific, as shown in the 1st row of Fig. 1(A), when the pose of the input person is intricate, existing methods [12, 19] fail to preserve the semantic information of different garment parts, resulting in the indistinguish-able warped sleeves. Besides, as shown in the 2nd row of
Fig. 1(A), if the input garment is a long sleeve without ob-vious seam between the sleeve and torso, existing methods will generate adhesive artifact between the sleeve and torso.
Second, most of the existing methods directly squeeze the input garment to make it align with the preserved region, leading to the distorted texture around the preserved region (e.g., the 3rd row of Fig. 1(A)). Third, most of the exist-ing works only focus on the upper-body try-on and neglect other garment categories (i.e, lower-body, dresses), which further limits their scalability for real-world scenarios.
To relieve the input constraint for VTON systems and fully exploit their application potential, in this paper, we take a step forwards and propose a unified framework, named GP-VTON, for the General-Purposed Virtual Try-ON, which can generate realistic try-on results even for the challenging scenario (Fig. 1(A)) (e.g., intricate human poses, difficult garment inputs, etc.), and can be easily ex-tended to the multi-category scenario (Fig. 1(B)).
The innovations of our GP-VTON lie in a novel Local-Flow Global-Parsing (LFGP) warping module and a Dy-namic Gradient Truncation (DGT) training strategy for the warping network, which enable the network to generate high fidelity deformed garments, and further facilitate our
GP-VTON to generate photo-realistic try-on results.
Specifically, most of the existing methods employ neural network to model garment deformation by introducing the
Thin Plate Splines (TPS) transformation [3] or the appear-ance flow [44] into the network, and training the network in a weakly supervised manner (i.e., without ground truth for the deformation function). However, both of the TPS-based methods [6, 18, 30, 36, 40] and the flow-based meth-ods [1, 12, 17, 19, 29] directly learn a global deformation field, therefore fail to represent complicated non-rigid gar-ment deformation that requires diverse transformation for different garment parts. Taking the intricate pose case in
Fig. 1(A) as an example, existing methods [12, 19] can not simultaneously guarantee accurate deformation for the torso region and sleeve region, and lead to exceeding distorted sleeves. In contrast, our LFGP warping module chooses to learn diverse local deformation fields for different garment parts, which is capable of individually warping each gar-ment part, and generating semantic-correct warped garment even for intricate pose case. Besides, since each local defor-mation field merely affects one corresponding garment part, garment texture from other parts is agnostic to the current deformation field and will not appear in the current local warped result. Therefore, the garment adhesion problem in the complex garment scenario can be completely addressed (as demonstrated in the 2nd row of Fig. 1(A)). However, directly assembling the local warped parts together can not obtain realistic warped garments, because there would be overlap among different warped parts. To deal with this, our
LFGP warping module collaboratively estimates a global garment parsing to fuse different local warped parts, result-ing in a complete and unambiguous warped garment.
On the other hand, the warping network in existing meth-ods [12,19,29] takes as inputs the flat garment and the mask of the preserved region (i.e., region to be preserved during the try-on procedure, such as the lower garment for upper-body VTON), and force the input garment to align with the boundary of the preserved region (e.g., the junction of the upper and lower garment), which usually require garment squeezing to meet the shape constraint and lead to texture distortion around the garment junction (please refer to the 3rd row of Fig. 1(A)). An effective solution to this problem is exploiting the gradient truncation strategy for the network training, in which the warped garment will be processed by the preserved mask before calculating the warping loss and the gradient in the preserved region will not be back-propagated. By using such a strategy, the warped garment is no longer required to strictly align with the preserved boundary, which largely avoids the garment squeezing and texture distortion. However, due to the poor supervision of warped garments in the preserved region, directly em-ploying the gradient truncation for all training data will lead to excessive freedom for the deformation field, which usu-ally results in texture stretching in the warped results. To tackle this problem, we proposed a Dynamic Gradient Trun-cation (DGT) training strategy which dynamically conducts gradient truncation for different training samples accord-ing to the disparity of height-width ratio between the flat garment and the warped garment. By introducing the dy-namic mechanism, our LFGP warping module can alleviate
Figure 2. Overview of GP-VTON. The LFGP warping module aims to estimate the local flows {f k}3 which is used to warp different garment parts {Gk}3
The generator G takes as inputs G′ and the other person-related conditions to generate the final try-on result I ′. k=1 and assembles warped parts {G′k}3 k=1 and global garment parsing S′, k=1 into the intact garment G′, respectively. the texture stretching problem and obtain realistic warped garments with better texture preservation.
Overall, our contributions can be summarized as fol-lows: (1) We propose a unified try-on framework, named
GP-VTON, to generate photo-realistic results for diverse scenarios. (2) We propose a novel LFGP warping mod-ule to generate semantic-correct deformed garments even with challenging inputs. (3) We introduce a simple, yet ef-fective DGT training strategy for the warping network to obtain distortion-free deformed garments. (4) Extensive ex-periments on two challenging high-resolution benchmarks show the superiority of GP-VTON over existing SOTAs. 2.