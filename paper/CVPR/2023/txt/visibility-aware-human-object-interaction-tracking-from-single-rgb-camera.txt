Abstract
Capturing the interactions between humans and their environment in 3D is important for many applications in robotics, graphics, and vision. Recent works to reconstruct the 3D human and object from a single RGB image do not have consistent relative translation across frames because they assume a fixed depth. Moreover, their performance drops significantly when the object is occluded.
In this work, we propose a novel method to track the 3D human, object, contacts, and relative translation across frames from a single RGB camera, while being robust to heavy occlu-sions. Our method is built on two key insights. First, we condition our neural field reconstructions for human and object on per-frame SMPL model estimates obtained by pre-fitting SMPL to a video sequence. This improves neu-ral reconstruction accuracy and produces coherent relative translation across frames. Second, human and object mo-tion from visible frames provides valuable information to infer the occluded object. We propose a novel transformer-based neural network that explicitly uses object visibil-ity and human motion to leverage neighboring frames to make predictions for the occluded frames. Building on these insights, our method is able to track both human and object robustly even under occlusions. Experiments on two datasets show that our method significantly im-proves over the state-of-the-art methods. Our code and pre-trained models are available at: https://virtualhumans.mpi-inf.mpg.de/VisTracker. 1.

Introduction
Perceiving and understanding human as well as their in-teraction with the surroundings has lots of applications in robotics, gaming, animation and virtual reality etc. Ac-curate interaction capture is however very hard. Early works employ high-end systems such as dense camera ar-rays [8, 17, 37] that allow accurate capture but are expen-sive to deploy. Recent works [6, 34, 36] reduce the require-Figure 1. From a monocular RGB video, our method tracks the human, object and contacts between them even under occlusions. ment to multi-view RGBD cameras but it is still compli-cated to setup the full capture system hence is not friendly for consumer-level usage. This calls for methods that can capture human-object interaction from a single RGB cam-era, which is more convenient and user-friendly.
However, reasoning about the 3D human and object from monocular RGB images is very challenging. The lack of depth information makes the predictions suscepti-ble to depth-scale ambiguity, leading to temporally inco-herent tracking. Furthermore, the object or human can get heavily occluded, making inference very hard. Prior work
PHOSA [89] relies on hand-crafted heuristics to reduce the ambiguity but such heuristic-based method is neither very accurate nor scalable. More recently, CHORE [79] com-bines neural field reconstructions with model based fitting obtaining promising results. However, CHORE, assumes humans are at a fixed depth from the camera and predicts scale alone, thereby losing the important relative transla-tion across frames. Another limitation of CHORE is that it is not robust under occlusions as little information is avail-able from single-frame when the object is barely visible.
Hence CHORE often fails in these cases, see Fig. 3.
In this work, we propose the first method that can track
both human and object accurately from monocular RGB videos. Our approach combines neural field predictions and model fitting, which has been consistently shown to be more effective than directly regressing pose [4–6, 79]. In contrast to existing neural field based reconstruction meth-ods [60,79], we can do tracking including inference of rela-tive translation. Instead of assuming a fixed depth, we con-dition the neural field reconstructions (for object and hu-man) on per frame SMPL estimates (SMPL-T) including translation in camera space obtained by pre-fitting SMPL to the video sequence. This results in coherent translation and improved neural reconstruction. In addition, we argue that during human-object interaction, the object motion is highly correlated with the human motion, which provides us valu-able information to recover the object pose even when it is occluded (see Fig. 1 column 3-4). To this end, we propose a novel transformer based network that leverages the hu-man motion and object motion from nearby visible frames to predict the object pose under heavy occlusions.
We evaluate our method on the BEHAVE [6] and Inter-Cap dataset [35]. Experiments show that our method can robustly track human, object and realistic contacts between them even under heavy occlusions and significantly outper-forms the currrent state of the art method, CHORE [79].
We further ablate the proposed SMPL-T conditioning and human and visibility aware object pose prediction network and demonstrate that they are key for accurate human-object interaction tracking.
In summary, our key contributions include:
• We propose the first method that can jointly track full-body human interacting with a movable object from a monocular RGB camera.
• We propose SMPL-T conditioned interaction fields, predicted by a neural network that allows consistent 4D tracking of human and object.
• We introduce a novel human and visibility aware ob-ject pose prediction network along with an object visi-bility prediction network that can recover object poses even under heavy occlusions. (HMR) from single images [2, 3, 7, 18, 43, 51, 54] or videos
[20, 40, 55, 56, 87]. We refer readers to a recent review of
HMR methods in [69]. On the other hand, deep learning method has also significantly improved object 6D pose es-timation from single RGB images [22,25,33,45,50,52,72].
However, object pose tracking has received less attention and most works focus on RGBD inputs [21, 64, 74, 75, 92].
Two works explore the camera localization ideas from
SLAM communities and can track object from RGB videos
[48, 67]. Nevertheless, they heavily rely on visual evidence and the performance is unknown under heavy occlusions.
They also do not track human-object interactions.
Human-object interaction. Modelling human object interaction is an emerging research topic in recent years.
Hand-object interaction is studied with works modelling hand-object interaction from RGB [19, 24, 30, 38, 83],
RGBD [9, 11, 28] or 3D inputs [10, 53, 68, 93]. There are also works that model human interacting with a static scene [27, 29, 34, 62, 76, 84, 85] or deformable surface [46].
More recently, the release of BEHAVE [6] and InterCap
[35] datasets allows bench-marking of full-body interact-ing with a movable object. However, human-object in-teraction capture usually deploys multi-view RGB [66] or
RGBD [6, 9, 20, 28, 28, 34, 35, 91] cameras. Only a few works [73, 79, 89] reconstruct dynamic human and object from monocular RGB input and our experiments show that they are not suitable for tracking.
Pose estimation under occlusion. Most existing meth-ods assume occlusion-free input images hence are not ro-bust under occlusions. Only a few methods address hu-man pose estimation under partial occlusions [26, 41, 42, 57, 58, 90] or long term occlusions [87]. For object pose estimation, pixel-wise voting [52] and self-occlusion [22] are explored for more robust prediction under occlusions.
More recently, TP-AE [92] predicts object occlusion ratio to guide pose estimation but relies on depth input. Although being impressive on separate human or object pose estima-tion, these methods do not reason about human-object in-teraction. Our method is the first one that takes both human and object visibility into account for interaction tracking.
• Our code and pretrained models are publicly available to foster future research in this direction. 3. Method 2.