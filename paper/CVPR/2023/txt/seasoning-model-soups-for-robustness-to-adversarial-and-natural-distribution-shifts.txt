Abstract
Adversarial training is widely used to make classifiers robust to a specific threat or adversary, such as ℓp-norm bounded perturbations of a given p-norm. However, ex-isting methods for training classifiers robust to multiple threats require knowledge of all attacks during training and remain vulnerable to unseen distribution shifts.
In this work, we describe how to obtain adversarially-robust model soups (i.e., linear combinations of parameters) that smoothly trade-off robustness to different ℓp-norm bounded adversaries. We demonstrate that such soups allow us to control the type and level of robustness, and can achieve robustness to all threats without jointly training on all of them. In some cases, the resulting model soups are more robust to a given ℓp-norm adversary than the constituent model specialized against that same adversary. Finally, we show that adversarially-robust model soups can be a viable tool to adapt to distribution shifts from a few examples. 1.

Introduction
Deep networks have achieved great success on several computer vision tasks and have even reached super-human accuracy [19, 31]. However, the outputs of such models are often brittle, and tend to perform poorly on inputs that differ from the distribution of inputs at training time, in a condition known as distribution shift [37]. Adversarial perturbations are a prominent example of this condition: small, even imperceptible, changes to images can alter pre-dictions to cause errors [2, 46]. In addition to adversarial inputs, it has been noted that even natural shifts, e.g. dif-ferent weather conditions, can significantly reduce the ac-curacy of even the best vision models [13, 21, 40]. Such drops in accuracy are undesirable for robust deployment, and so a lot of effort has been invested in correcting them.
Adversarial training [34] and its extensions [15, 39, 58] are currently the most effective methods to improve empirical robustness to adversarial attacks. Similarly, data augmen-*Work done during an internship at DeepMind. tation is the basis of several techniques that improve ro-bustness to non-adversarial/natural shifts [3, 11, 22]. While significant progress has been made on defending against a specific, selected type of perturbations (whether adversar-ial or natural), it is still challenging to make a single model robust to a broad set of threats and shifts. For example, a classifier adversarially-trained for robustness to ℓp-norm bounded attacks is still vulnerable to attacks in other ℓq-threat models [29, 47]. Moreover, methods for simultane-ous robustness to multiple attacks require jointly training on all [33, 35] or a subset of them [8]. Most importantly, con-trolling the trade-off between different types of robustness (and nominal performance) remains difficult and requires training several classifiers.
Inspired by model soups [52], which interpolate the pa-rameters of a set of vision models to achieve state-of-the-art accuracy on IMAGENET, we investigate the effects of in-terpolating robust image classifiers. We complement their original recipe for soups by own study of how to pre-train, fine-tune, and combine the parameters of models adversarially-trained against ℓp-norm bounded attacks for different p-norms. To create models for soups, we pre-train a single robust model and fine-tune it to the target threat models (using the efficient technique of [8]). We then es-tablish that it is possible to smoothly trade-off robustness to different threat models by moving in the convex hull of the parameters of each robust classifier, while achieving competitive performance with methods that train on multi-ple p-norm adversaries simultaneously. Unlike alternatives, our soups can uniquely (1) choose the level of robustness to each threat model without any further training and (2) quickly adapt to new unseen attacks or shifts by simply tun-ing the weighting of the soup.
Previous works [24, 30, 54] have shown that adversarial training with ℓp-norm bounded attacks can help to improve performance on natural shifts if carefully tuned. We show that model soups of diverse classifiers, with different types of robustness, offer greater flexibility for finding models that perform well across various shifts, such as IMAGENET variants. Furthermore, we show that a limited number of images of the new distribution are sufficient to select the
weights of such a model soup. Examining the composition of the best soups brings insights about which features are important for each dataset and shift. Finally, while the ca-pability of selecting a model specific to each image distribu-tion is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accu-racy than adversarial and self-supervised baselines [18, 24].
Contributions. In summary, we show that soups
• can merge nominal and ℓp-robust models (for various p): efficient fine-tuning from one robust model obtains a set of models with diverse robustness [8] and compatible pa-rameters for creating model soups [52] (Sec. 3),
• can control the level of robustness to each threat model and achieve, without more training, competitive perfor-mance against multi-norm robustness training (Sec. 4),
• are not limited to interpolation, but can find more effec-tive classifiers by extrapolation (Sec. 4.3),
• enable adaptation to unseen distribution shifts on only a few examples (Sec. 5). 2.