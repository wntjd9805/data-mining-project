Abstract
Despite the great success in 2D editing using user-friendly tools, such as Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D areas are still lim-ited, either relying on 3D modeling skills or allowing edit-ing within only a few categories. In this paper, we present a novel semantic-driven NeRF editing approach, which en-ables users to edit a neural radiance field with a single im-age, and faithfully delivers edited novel views with high fi-delity and multi-view consistency. To achieve this goal, we propose a prior-guided editing field to encode fine-grained geometric and texture editing in 3D space, and develop a series of techniques to aid the editing process, including cyclic constraints with a proxy mesh to facilitate geomet-ric supervision, a color compositing mechanism to stabi-lize semantic-driven texture editing, and a feature-cluster-based regularization to preserve the irrelevant content un-changed. Extensive experiments and editing examples on both real-world and synthetic data demonstrate that our method achieves photo-realistic 3D editing using only a single edited image, pushing the bound of semantic-driven editing in 3D real-world scenes. 1.

Introduction
Semantic-driven editing approaches, such as stroke-based scene editing [34, 39, 66], text-driven image synthe-sis and editing [1, 50, 53], and attribute-based face edit-ing [27, 60], have greatly improved the ease of artistic cre-ation. However, despite the great success of 2D image edit-*Authors contributed equally.
†Corresponding authors. ing and neural rendering techniques [14, 42], similar edit-ing abilities in the 3D area are still limited: (1) they re-quire laborious annotation such as image masks [27, 71] and mesh vertices [69, 74] to achieve the desired manipula-tion; (2) they conduct global style transfer [12,13,16,20,75] while ignoring the semantic meaning of each object part (e.g., windows and tires of a vehicle should be textured differently); (3) they can edit on categories by learning a textured 3D latent representation (e.g., 3D-aware GANs with faces and cars etc.) [6, 8, 9, 17, 45, 56, 59, 60], or at a coarse level [35, 64] with basic color assignment or object-level disentanglement [30], but struggle to conduct texture editing on objects with photo-realistic textures or out-of-distribution characteristics.
Based on this observation, we believe that, on the way toward semantic-driven 3D editing, the following proper-the operation of editing ties should be ensured. First, should be effortless, i.e., users can edit 3D scenes on a sin-gle 2D image in convenient ways, e.g., using off-the-shelf tools such as GAN-based editing [28, 34], text-driven edit-ing [1, 53], Photoshop, or even a downloaded Internet im-age without pixel-wise alignment, rather than steering 3D modeling software with specific knowledge [69], or repeat-edly editing from multi-view images. Second, the editing method should be applicable to real-world scenes or objects and preserve vivid appearances, which is beyond existing 3D-aware generative models [8, 9] due to the limited cate-gories and insufficient data diversity on real-world objects.
To fulfill this goal, we propose a novel Semantic-driven
Image-based Editing approach for Neural radiance field in real-world scenes, named SINE. Specifically, our method allows users to edit a neural radiance field with a sin-gle image, i.e., either by changing a rendered image us-ing off-the-shelf image editing tools or providing an im-age for texture transferring (see Sec. 4.4), and then deliv-ers edited novel views with consistent semantic meaning.
Unlike previous works that directly fine-tune the existing
NeRF model [30, 35, 64], SINE learns a prior-guided edit-ing field to encode geometric and texture changes over the original 3D scene (see Fig. 2), thus enabling fine-grained editing ability. By leveraging guidance from existing neu-ral priors (shape prior models [15] and Vision Transformer models [7], etc.), SINE can directly perform semantic-driven editing on photo-realistic scenes without pre-training a category-level latent space. For example, in Fig. 1, users can stretch a car’s back or change all four tires to cook-ies by only editing a single image, and can even cooperate with text-prompts editing [1] to modify a specific object of a scene with vivid appearances.
However, even when guided with neural priors, editing
NeRF from a single image with multi-view consistency and accuracy is still challenging. (1) The generic NeRF does not necessarily provide an explicit surface or signed dis-tance field, such that it cannot directly work with shape pri-ors [15]. Therefore, we propose to use cyclic constraints with a proxy mesh to represent the edited NeRF’s geom-etry, which facilitates guided editing using coarse shape prior. (2) Learning a coordinate-based 3D editing field us-ing a single edited view is not sufficient to capture fine-grained details, and applying semantic supervision [7, 52] directly to the editing field leads to sub-optimal conver-gence (see Sec. 4.5). To tackle these challenges, we propose a color compositing mechanism by first rendering the tem-plate NeRF color and modification color individually, and then deferred blending them to yield the edited view, which significantly improves semantic-driven texture editing. (3)
Ideally, a user’s editing should only affect the desired re-gions while maintaining other parts untouched. However, in semantic-driven editing, the prior losses require taking the full shape or image as input, which leads to appearance or shape drifting at the undesired area. To precisely con-trol the editing while excluding irrelevant parts from being affected, we generate feature clusters of the editing area us-ing the ViT-based feature field [7,30], and use these clusters to distinguish whether a location is allowed to be edited or should remain unchanged.
In summary, the contributions of our paper are as fol-lows. (1) We propose a novel semantic-driven image-based
NeRF editing approach, called SINE, which allows users to edit a neural radiance field simply on just a single view of the rendering. SINE leverages a prior-guided editing field to encode fine-grained geometry and texture changes over the given pre-trained NeRF, thus delivering multi-view consis-tent edited views with high fidelity. (2) To achieve seman-tic editing functionality, we develop a series of techniques, including cyclic constraints with a proxy mesh for geomet-ric editing, the color compositing mechanism to enhance texture editing, and the feature-cluster-based regularization to control the affected editing area and maintain irrelevant (3) Experiments and editing examples parts unchanged. on both real-world/synthetic and object-centric/unbounded 360◦ scenes data demonstrate superior editing capabilities and quality with effortless operations. 2.