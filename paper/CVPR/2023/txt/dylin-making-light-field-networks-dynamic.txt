Abstract
Light Field Networks, the re-formulations of radiance fields to oriented rays, are magnitudes faster than their co-ordinate network counterparts, and provide higher fidelity with respect to representing 3D structures from 2D obser-vations. They would be well suited for generic scene rep-resentation and manipulation, but suffer from one problem: they are limited to holistic and static scenes.
In this pa-per, we propose the Dynamic Light Field Network (DyLiN) method that can handle non-rigid deformations, including topological changes. We learn a deformation field from in-put rays to canonical rays, and lift them into a higher di-mensional space to handle discontinuities. We further in-troduce CoDyLiN, which augments DyLiN with controllable attribute inputs. We train both models via knowledge distil-lation from pretrained dynamic radiance fields. We eval-uated DyLiN using both synthetic and real world datasets that include various non-rigid deformations. DyLiN qual-itatively outperformed and quantitatively matched state-of-the-art methods in terms of visual fidelity, while being 25 − 71× computationally faster. We also tested CoDyLiN on at-tribute annotated data and it surpassed its teacher model.
Project page: https://dylin2023.github.io. 1.

Introduction
Machine vision has made tremendous progress with re-spect to reasoning about 3D structure using 2D observa-tions. Much of this progress can be attributed to the emer-gence of coordinate networks [6,21,26], such as Neural Ra-diance Fields (NeRF) [23] and its variants [2, 20, 22, 39].
They provide an object agnostic representation for 3D scenes and can be used for high-fidelity synthesis for unseen views. While NeRFs mainly focus on static scenes, a series of works [10,27,29,34] extend the idea to dynamic cases via additional components that map the observed deformations to a canonical space, supporting moving and shape-evolving objects. It was further shown that by lifting this canonical space to higher dimensions the method can handle changes in scene topology as well [28].
However, the applicability of NeRF models is consid-erably limited by their computational complexities. From each pixel, one typically casts a ray from that pixel, and nu-merically integrates the radiance and color densities com-puted by a Multi-Layer Perceptron (MLP) across the ray, approximating the pixel color. Specifically, the numeri-cal integration involves sampling hundreds of points across the ray, and evaluating the MLP at all of those locations.
Several works have been proposed for speeding up static
NeRFs. These include employing a compact 3D represen-tation structure [9, 18, 43], breaking up the MLP into multi-ple smaller networks [30, 31], leveraging depth information
[7, 24], and using fewer sampling points [17, 24, 42]. Yet, these methods still rely on integration and suffer from sam-pling many points, making them prohibitively slow for real-time applications. Recently, Light Field Networks (LFNs)
[32] proposed replacing integration with a direct ray-to-color regressor, trained using the same sparse set of images, requiring only a single forward pass. R2L [36] extended
LFNs to use a very deep residual architecture, trained by distillation from a NeRF teacher model to avoid overfit-ting. In contrast to static NeRF acceleration, speeding up dynamic NeRFs is a much less discussed problem in the literature. This is potentially due to the much increased dif-ficulty of the task, as one also has to deal with the high variability of motion. In this direction, [8, 38] greatly re-duce the training time by using well-designed data struc-tures, but their solutions still rely on integration. LFNs are clearly better suited for acceleration, yet, to the best of our knowledge, no works have attempted extending LFNs to the dynamic scenario.
In this paper, we propose 2 schemes extending LFNs to dynamic scene deformations, topological changes and controllability. First, we introduce DyLiN, by incorpo-rating a deformation field and a hyperspace representa-tion to deal with non-rigid transformations, while distilling knowledge from a pretrained dynamic NeRF. Afterwards, we also propose CoDyLiN, via adding controllable input attributes, trained with synthetic training data generated by a pretrained Controllable NeRF (CoNeRF) [13] teacher model. To test the efficiencies of our proposed schemes, we perform empirical experiments on both synthetic and real datasets. We show that our DyLiN achieves better image quality and an order of magnitude faster rendering speed than its original dynamic NeRF teacher model and the state-of-the-art TiNeuVox [8] method. Similarly, we also show that CoDyLiN outperforms its CoNeRF teacher. We further execute ablation studies to verify the individual effective-ness of different components of our model. Our methods can be also understood as accelerated versions of their re-spective teacher models, and we are not aware of any prior works that attempt speeding up CoNeRF.
Our contributions can be summarized as follows:
• We propose DyLiN, an extension of LFNs that can handle dynamic scenes with topological changes.
DyLiN achieves this through non-bending ray defor-mations, hyperspace lifting for whole rays, and knowl-edge distillation from dynamic NeRFs.
• We show that DyLiN achieves state-of-the-art results on both synthetic and real-world scenes, while being an order of magnitude faster than the competition. We also include an ablation study to analyze the contribu-tions of our model components.
• We introduce CoDyLiN, further extending our DyLiN to handle controllable input attributes. 2.