Abstract
Vision Transformers (ViTs) have gained signiﬁcant pop-ularity in recent years and have proliferated into many applications. However, their behavior under different learning paradigms is not well explored. We compare
ViTs trained through different methods of supervision, and show that they learn a diverse range of behaviors in terms of their attention, representations, and downstream performance. We also discover ViT behaviors that are consistent across supervision, including the emergence of Offset Local Attention Heads. These are self-attention heads that attend to a token adjacent to the current token with a ﬁxed directional offset, a phenomenon that to the best of our knowledge has not been highlighted in any prior work. Our analysis shows that ViTs are highly ﬂexible and learn to process local and global information in different orders depending on their training method. We ﬁnd that contrastive self-supervised methods learn features that are competitive with explicitly supervised features, and they can even be superior for part-level tasks. We also ﬁnd that the representations of reconstruction-based models show non-trivial similarity to contrastive self-supervised models. 1.

Introduction
The ﬁeld of Computer Vision has advanced massively in the past decade, largely built on the backbone of Con-volutional Neural Networks (CNNs). More recently, Vi-sion Transformers (ViTs) [18] have shown the potential to overtake CNNs as the go-to visual processing model.
Prior works have asked the question do ViTs see like CNNs do? [52], but in this work, we ask: how do ViTs learn un-der different supervision? Past examinations of ViTs have largely focused on models trained through full supervision.
Instead, we aim to characterize the differences and similar-ities of ViTs trained through varying training methods, in-cluding self-supervised methods. Unlike CNNs, the ViT ar-chitecture imposes few structural biases to guide the learn-ing of representations. This gives them the ﬂexibility to
*Equal contributors.
Web: www.cs.umd.edu/˜sakshams/vit_analysis
Code: www.github.com/mwalmer-umd/vit_analysis
Figure 1. ViTs exhibit highly varied behaviors depending on their method of training. In this work, we compare ViTs through three domains of analysis representing the How, What, and Why of
ViTs. How do ViTs process information through attention? (Top)
Attention maps averaged over 5000 images show clear differences in the mid-to-late layers. What do ViTs learn to represent? (Left)
Contrastive self-supervised ViTs have a greater feature similarity to explicitly supervised ViTs, but also have some similarity with
ViTs trained through masked reconstruction. Why do we care about using ViTs? (Right) We evaluate ViTs on a variety of global and local tasks and show that the best model and layer vary greatly. learn diverse information processing strategies, and through our analyses, we uncover a wide array of ViT behaviors.
There are countless ways to analyze ViTs, so to guide this analysis we choose three major domains which corre-spond to the How, What, and Why of ViTs. For the How, we focus on how ViTs process information through Atten-tion. Multi-Headed Attention (MHA) layers are arguably the key element of ViTs, and they most distinguish them
from CNNs. For the What, we examine the Features of
ViTs, as these are typically what practitioners take away from them. Finally for the Why, we focus on Downstream
Tasks, which are why we care about using ViTs.
Our work unveils that a powerful aspect of the ViT ar-chitecture is its local-global dual nature, which plays a role in all three aspects of our analyses. While standard
CNNs are restricted to building representations hierarchi-cally from local to global, in a ViT each token can attend to information from any other image region at any time.
And unlike popular CNN modiﬁcations like Spatial Pyra-mids [20, 28, 33, 35] and top-down strategies [6, 47, 56],
ViTs have the freedom to decide when and where global in-formation should be integrated. In this study, we show that the order and the relative ratio of local and global attention in ViTs varies dramatically based on the method of supervi-sion. We also ﬁnd clearly different trends in the allocation of attention in the mid-to-late layers of these networks, as highlighted in Figure 2. This local-global dual nature is also embedded into the structure and features of the ViT, which encodes both local spatial tokens and a non-local classiﬁer (CLS) token throughout its entire depth. We analyze the features of ViTs for both the CLS and spatial tokens, and assess how they align with semantics at the image, object, part, and pixel-level. We perform this analysis at every layer of the ViT to show the emergence of different levels of se-mantic information. Finally, we assess ViTs on a number of local and global downstream tasks.
Overall, our contributions are: [1] A detailed comparison of ViTs trained with six different methods, including both fully supervised and self-supervised training. [2] A cross-cutting analysis spanning three major domains: Attention,
Features, and Downstream Tasks. [3] Multiple insights into the inner workings of ViTs to guide future development of
ViT variants, training strategies, and applications.
In addition, we summarize some of our key observations about ViT behavior: [1] The attention maps of explicitly supervised ViTs devolve into Sparse Repeating Patterns in the mid-to-late layers, but the quality of features con-tinues to improve in these layers (Section 4.1).
[2] All
ViTs studied learn to use Offset Local Attention Heads, suggesting they are fundamentally necessary in ViTs (Sec-tion 4.2). To the best of our knowledge, no prior work has brought attention to this phenomenon. [3] ViTs learn to pro-cess local and global information in different orders depend-ing on their method of supervision (Section 4.3). [4] All
ViTs studied differentiate salient foreground objects by the early-to-mid layers (Section 4.4). [5] Reconstruction-based self-supervised methods can learn semantically meaningful
CLS representations, even when the CLS token is only a placeholder (Section 5.1, 5.2). [6] Supervised method’s fea-tures are the most semantically rich, but contrastive self-supervised methods are comparable or even superior in
Figure 2. Clear differences in attention emerge in the mid-to-late layers under different supervision methods. These plots show the attention maps of CLS tokens averaged over 5000 Ima-geNet images. Rows indicate layers and columns indicate heads.
For brevity, we show only three heads per layer. The bracketed numbers in the lower half denote the layer and head. some cases (Section 5.2, 5.3).
[7] For localized tasks, the best performance often comes from a mid-to-late layer (Section 6.2). [8] There is no single “best” training method or layer for all downstream tasks (Section 6.3). 2.