Abstract
Controllable image synthesis with user scribbles has gained huge public interest with the recent advent of text-conditioned latent diffusion models. The user scribbles con-trol the color composition while the text prompt provides control over the overall image semantics. However, we note that prior works in this direction suffer from an intrinsic domain shift problem wherein the generated outputs often lack details and resemble simplistic representations of the target domain. In this paper, we propose a novel guided im-age synthesis framework, which addresses this problem by modelling the output image as the solution of a constrained optimization problem. We show that while computing an exact solution to the optimization is infeasible, an approx-imation of the same can be achieved while just requiring a single pass of the reverse diffusion process. Additionally, we show that by simply defining a cross-attention based correspondence between the input text tokens and the user stroke-painting, the user is also able to control the seman-tics of different painted regions without requiring any con-ditional training or finetuning. Human user study results show that the proposed approach outperforms the previous state-of-the-art by over 85.32% on the overall user satis-faction scores. Project page for our paper is available at https://1jsingh.github.io/gradop. 1.

Introduction
Guided image synthesis with user scribbles has gained widespread public attention with the recent advent of large-scale language-image (LLI) models [23, 26, 28, 30, 40]. A
Existing methods often aim attempt novice user can gain significant control over the final image contents by combining text-based conditioning with unsu-pervised guidance from a reference image (usually a coarse stroke painting). The text prompt controls the overall image semantics, while the provided coarse stroke painting allows the user to define the color composition in the output scene. to achieve this through two means. The first category leverages condi-tional training using semantic segmentation maps [8, 28, 39]. However, the conditional training itself is quite time-consuming and requires a large scale collection of dense se-mantic segmentation labels across diverse data modalities.
The second category, typically leverages an inversion based approach for mapping the input stroke painting to the tar-get data manifold without requiring any paired annotations.
For instance, a popular solution by [22, 35] introduces the painting based generative prior by considering a noisy ver-sion of the original image as the start of the reverse diffusion process. However, the use of an inversion based approach causes an intrinsic domain shift problem if the domain gap between the provided stroke painting and the target domain is too high. In particular, we observe that the resulting out-puts often lack details and resemble simplistic representa-tions of the target domain. For instance, in Fig. 1, we notice that while the target domain consists of realistic photos of a landscape, the generated outputs resemble simple pictorial arts which are not very realistic. Iteratively reperforming the guided synthesis with the generated outputs [4] seems to improve realism but it is costly, some blurry details still persist (refer Fig. 4), and the generated outputs tend to lose faithfulness to the reference with each successive iteration.
To address this, we propose a diffusion-based guided im-age synthesis framework which models the output image as the solution of a constrained optimization problem (Sec. 3).
Given a reference painting y, the constrained optimization is posed so as to find a solution x with two constraints: 1) upon painting x with an autonomous painting function we should recover a painting similar to reference y, and, 2) the output x should lie in the target data subspace defined by the text prompt (i.e., if the prompt says “photo” then we want the output images to be realistic photos instead of cartoon-like representations of the same concept). Subsequently, we show that while the computation of an exact solution for this optimization is infeasible, a practical approximation of the same can be achieved through simple gradient descent.
Finally, while the proposed optimization allows the user to generate image outputs with high realism and faithful-ness (with reference y), the fine-grain semantics of differ-ent painting regions are inferred implicitly by the diffusion model. Such inference is typically dependent on the gener-ative priors learned by the diffusion model, and might not accurately reflect the user’s intent in drawing a particular region. For instance, in Fig. 1, we see that the light blue re-gions can be inferred as blue-green grass instead of a river.
To address this, we show that by simply defining a cross-attention based correspondence between the input text to-kens and user stroke-painting, the user can control seman-tics of different painted regions without requiring semantic-segmentation based conditional training or finetuning. 2.