Abstract
Motion segmentation is one of the main tasks in com-puter vision and is relevant for many applications. The op-tical flow (OF) is the input generally used to segment every frame of a video sequence into regions of coherent motion.
Temporal consistency is a key feature of motion segmenta-tion, but it is often neglected. In this paper, we propose an original unsupervised spatio-temporal framework for mo-tion segmentation from optical flow that fully investigates the temporal dimension of the problem. More specifically, we have defined a 3D network for multiple motion segmen-tation that takes as input a sub-volume of successive opti-cal flows and delivers accordingly a sub-volume of coher-ent segmentation maps. Our network is trained in a fully unsupervised way, and the loss function combines a flow reconstruction term involving spatio-temporal parametric motion models, and a regularization term enforcing tempo-ral consistency on the masks. We have specified an easy temporal linkage of the predicted segments. Besides, we have proposed a flexible and efficient way of coding U-nets.
We report experiments on several VOS benchmarks with convincing quantitative results, while not using appearance and not training with any ground-truth data. We also high-light through visual results the distinctive contribution of the short- and long-term temporal consistency brought by our OF segmentation method. 1.

Introduction
Motion segmentation is a key topic in computer vision that arises as soon as videos are processed.
It may be a goal in itself. More frequently, it is a prerequisite for dif-ferent objectives as independent moving object detection, object tracking, or motion recognition, to name a few. It is also widely leveraged in video object segmentation (VOS), but most often coupled with appearance. Motion segmen-tation is supposed to rely on optical flow as input. Indeed, the optical flow carries all the information on the movement between two successive images of the video.
Clearly, the motion segmentation problem has a strong temporal dimension, as motion is generally consistent throughout the video, at least in part of it within video shots.
The use of one optical flow field at each given time instant may be sufficient to get the segmentation at frame t of the video. However, extending the temporal processing win-dow can be beneficial.
Introducing temporal consistency in the motion segmentation framework is certainly useful from an algorithmic perspective: it may allow to correct lo-cal errors or to predict the segmentation map at the next time instant. Beyond that, temporal consistency is an in-trinsic property of motion that is essential to involve in the formulation of the motion segmentation problem.
In this paper, we propose an original method for multiple motion segmentation from optical flow, exhibiting temporal consistency, while ensuring accuracy and robustness. To the best of our knowledge, our optical flow segmentation (OFS) method is the first one to involve short- and long-term tem-poral consistency. We are considering a fully unsupervised method, which overcomes tedious or even unfeasible man-ual annotation and provides a better generalization power to any type of video sequences.
The main contributions of our work are as follows. We adopt an explicit space-time approach. More specifically, our network takes as input a sub-volume of successive opti-cal flows and delivers accordingly a sub-volume of coherent segmentation maps. Our network is trained in a completely unsupervised manner, without any manual annotation or ground truth data of any kind. The loss function combines a flow reconstruction term involving spatio-temporal para-metric motion models defined over the flow sub-volume, and a regularization term enforcing temporal consistency on the masks of the sub-volume. Our method also introduces a latent represention of each segment motion and enables an easy temporal linkage between predictions. In addition, we have designed a flexible and efficient coding of U-nets.
The rest of the paper is organized as follows. Section 2 is devoted to related work. In Section 3, we describe our unsu-pervised 3D network for multiple motion segmentation em-bedding temporal consistency. Section 4 collects details on our implementation. In Section 5, we report results on sev-eral VOS benchmarks with a comparison to several existing methods. Finally, Section 6 contains concluding remarks. estimation of optical flow, depth and camera pose, achieved by dedicated networks, and the computation of the resulting static scene flow, i.e., the apparent motion of the static parts of the scene viewed by a moving camera. 2.