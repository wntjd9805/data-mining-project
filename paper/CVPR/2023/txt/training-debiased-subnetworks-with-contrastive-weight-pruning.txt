Abstract
Neural networks are often biased to spuriously corre-lated features that provide misleading statistical evidence that does not generalize. This raises an interesting ques-tion: “Does an optimal unbiased functional subnetwork ex-ist in a severely biased network? If so, how to extract such subnetwork?” While empirical evidence has been accumu-lated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then fur-ther elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbiased subnetworks without ex-pensive group annotations. Experimental results demon-strate that our approach significantly outperforms state-of-the-art debiasing methods despite its considerable reduc-tion in the number of parameters. 1.

Introduction
While deep neural networks have made substantial progress in solving challenging tasks, they often undesir-ably rely on spuriously correlated features or dataset bias, if present, which is considered one of the major hurdles in deploying models in real-world applications. For example, consider recognizing desert foxes and cats from natural im-ages. If the background scene (e.g., a desert) is spuriously correlated to the type of animal, the neural networks might use the background information as a shortcut to classifica-tion, resulting in performance degradation in different back-grounds (e.g., a desert fox in the house).
To investigate the origin of the spurious correlations, this paper considers shortcut learning as a fundamental architec-tural design issue of neural networks. Specifically, if any available information channels in deep networks’ structure could transmit the information of spuriously correlated fea-tures (spurious features from now on), networks would ex-ploit those features as long as they are sufficiently predic-tive. It naturally follows that pruning weights on spurious features can purify the biased latent representations, thereby improving performances on bias-conflicting samples1. We conjecture that this neural pruning may improve the gener-alization of the network in a way that reduces the effective dimension of spurious features, considering that the failure of Out-of-Distribution (OOD) generalization may arise due to high-dimensional spurious features [24, 31].
Recently, Zhang et al. [33] has empirically demonstrated the existence of subnetworks that are less susceptible to spurious features. Based on the modular property of neu-ral networks [5], they prune out weights that are closely related to the spurious attributes. While [33] affords us valuable insights on the importance of neural architectures, the study has limitation in that such neural pruning requires sufficient number of ground-truth bias-conflicting samples.
Thus, how to discover the optimal subnetworks in practice when the dataset is highly biased?
To address this, we first present a simple theoretical ob-servation that reveals the limitations of existing substruc-ture probing methods in searching unbiased subnetworks.
Specifically, we reveal that there exists an unavoidable gen-eralization gap in the subnetworks obtained by standard pruning algorithms in the presence of strong spurious corre-lations. Our analysis also shows that trained models may in-evitably rely on the spuriously correlated features in a prac-tical training setting with finite training time and a number of samples.
In addition, we show that sampling more bias-conflicting data makes it possible to identify spurious weights. Specifi-cally, bias-conflicting samples require that the weights as-1The bias-aligned samples refer to data with a strong correlation be-tween (potentially latent) spurious features and target labels (e.g., cat in the house). The bias-conflicting samples refer to the opposite cases where spurious correlations do not exist (e.g., cat in the desert).
Figure 1. Concept: We demonstrate an inevitable generalization gap of subnetworks obtained by standard pruning methods including [33].
Based on these observations, we design a novel subnetwork probing framework by fully exploiting unbiased samples. sociated with spurious features should be pruned out as the spurious features do not help predict bias-conflicting samples. Our theoretical observations suggest that balanc-ing the ratio between the number of bias-aligned and bias-conflicting samples is crucial in finding the optimal unbi-ased subnetworks.
In practice, the dataset may severely lack diversity for bias-conflicting samples due to the potential pitfalls in data collection protocols or human prejudice. Since it is of-ten highly laborious to supplement enough bias-conflicting samples, we propose a novel debiasing scheme called De-biased Contrastive Weight Pruning (DCWP) that uses the oversampled bias-conflicting data to search unbiased sub-networks.
As shown in Fig. 1, DCWP is comprised of two stages: (1) identifying the bias-conflicting samples without expen-sive annotations on spuriously correlated attributes, and (2) training the pruning parameters to obtain weight pruning masks with the sparsity constraint and debiased loss func-tion. Here, the debiased loss includes a weighted cross-entropy loss for the identified bias-conflicting samples and an alignment loss to further reduce the geometrical align-ment gap between bias-aligned and bias-conflicting samples within each class.
We demonstrate that DCWP consistently outperforms state-of-the-art debiasing methods across various biased datasets, including the Color-MNIST [21, 25], Corrupted
CIFAR-10 [13], Biased FFHQ [19] and CelebA [23], even without direct supervision on the bias type. Our approach improves the accuracy on the unbiased evaluation dataset by 86.74% → 93.41%, 27.86% → 35.90% on Colored-MNIST and Corrupted CIFAR-10 compared to the second best model, respectively, even when 99.5% of samples are bias-aligned. 2.