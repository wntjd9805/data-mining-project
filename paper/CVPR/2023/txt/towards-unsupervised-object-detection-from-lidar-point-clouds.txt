Abstract
In this paper, we study the problem of unsupervised ob-ject detection from 3D point clouds in self-driving scenes.
We present a simple yet effective method that exploits (i) point clustering in near-range areas where the point clouds are dense, (ii) temporal consistency to ﬁlter out noisy unsu-pervised detections, (iii) translation equivariance of CNNs to extend the auto-labels to long range, and (iv) self-supervision for improving on its own. Our approach, OYS-TER (Object Discovery via Spatio-Temporal Reﬁnement), does not impose constraints on data collection (such as repeated traversals of the same location), is able to de-tect objects in a zero-shot manner without supervised ﬁne-tuning (even in sparse, distant regions), and continues to self-improve given more rounds of iterative self-training. To better measure model performance in self-driving scenar-ios, we propose a new planning-centric perception metric based on distance-to-collision. We demonstrate that our unsupervised object detector signiﬁcantly outperforms un-supervised baselines on PandaSet and Argoverse 2 Sen-sor dataset, showing promise that self-supervision com-bined with object priors can enable object discovery in the wild. For more information, visit the project website: https://waabi.ai/research/oyster. 1.

Introduction
When a large set of annotations are available, supervised learning can solve the task of 3D object detection remark-ably well thanks to the power of neural networks, as proven by many successful object detectors developed in the past decade [24,36,47,70]. However, since most existing data is unlabeled, human annotations are currently the bottleneck for data-driven learning algorithms, as they require tedious manual effort that is very costly in practice. While there has been some effort on using weaker supervision to train ob-ject detectors [4, 57, 68], it is worth noting that human and animal brains are able to perceive objects without explicit
†Work done at Waabi. Mengye is now at New York University.
Figure 1. Visualization of a sequence of ﬁve 3D point clouds. At near range, the point clouds are very dense and we can clearly distinguish clusters, motivating us to use prior knowledge. At far range, the point clouds are quite sparse and the objects do not ap-pear as obvious, leading us to explore zero-shot generalization. labels at all [2]. This naturally inspires us to ask whether we can design unsupervised learning algorithms that dis-cover objects from raw streams of sensor data on their own.
Unsupervised object detection has long been studied in computer vision, albeit in various forms. For instance, nu-merous ways of unsupervised object proposals were consid-ered as the ﬁrst stage of an object detector [20]. These meth-ods leverage a variety of cues including colors and edge boundaries [1], graph structures [18], and motion cues [50].
While those methods are no longer popular in today’s object detectors due to end-to-end supervised training, they offer important intuitions of what an object is.
In recent years, unsupervised object detection has made a comeback under the name of object-centric models [6, 15, 33, 34, 38, 39, 58]. The essential idea is to train an auto-encoder with a structured decoder such that the network is forced to decompose the scene into a set of individual objects during reconstruction. Most of those models only show experiments on synthetic toy datasets, where the back-ground lacks any ﬁne-grained details, and the foreground objects have simplistic shapes and distinct colors. The rea-son why object-centric models have struggled to scale to re-alistic data is that their mechanism of object decomposition is based on careful balancing of model capacities between the foreground and background modules. Consequently, an increase in model capacity, which is needed for real-world data, breaks the brittle balance between modules and result in a failure in scene decomposition. Unsupervised object discovery in the wild remains an open challenge.
In this work, we study unsupervised object detection from point clouds in the context of self-driving vehicles (SDVs). This is a challenging task due to occlusion as well as the sparsity of the observations particularly at range. We refer the reader to Fig. 1 for an example. Despite its appeal, unsupervised object detection from LiDAR has received lit-tle attention. Recent work [66] exploits repeated traver-sals of the same region to understand the persistence of a point over time and discover mobile objects from that in-formation. However, the assumption of repeated traversals in acquiring point clouds restricts the applicability of the method. Instead, we study the most generic setting of unsu-pervised detection given any raw sequence of point clouds.
Our method OYSTER (Object Discovery via Spatio-Temporal Reﬁnement) carefully combines key ideas from density-based spatial clustering, temporal consistency, equivariance and self-supervised learning in a uniﬁed framework that exploits their strengths while overcoming their shortcomings. Firstly, we exploit point clustering to obtain initial pseudo-labels to bootstrap an object detector in the near range, where point density is high (see Fig. 1).
We then employ unsupervised tracking to ﬁlter out tempo-rally inconsistent objects. Since point clustering does not work well in the long-range where observations are sparse, we exploit the translation equivariance of CNNs to train on high-quality, near-range pseudo-labels and zero-shot gen-eralize to long range. To bridge the density gap between short and long-range at training vs. inference, we propose a novel random LiDAR ray dropping strategy. Finally, we design a self-improvement loop in which this bootstrapped model can self-train. At every round of self-improvement, we utilize the temporal consistency of objects to automati-cally reﬁne the detections from the model at the previous iteration, and use these reﬁned outputs as pseudo-labels for training. Our experiments on Pandaset [63] and Ar-goverse V2 Sensor [60] demonstrate that OYSTER clearly outperforms other unsupervised methods, both under stan-dard metrics based on intersection-over-union (IoU) and our proposed metric based on distance-to-collision (DTC). We hope that our work serves as a step towards building percep-tion systems that automatically improve with more data and compute without being bottlenecked by human supervision. 2.