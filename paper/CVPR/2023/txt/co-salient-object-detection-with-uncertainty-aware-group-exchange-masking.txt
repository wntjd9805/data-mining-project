Abstract
The traditional definition of co-salient object detection (CoSOD) task is to segment the common salient objects in a group of relevant images. Existing CoSOD models by-default adopt the group consensus assumption. This brings about model robustness defect under the condition of ir-relevant images in the testing image group, which hinders the use of CoSOD models in real-world applications. To address this issue, this paper presents a group exchange-masking (GEM) strategy for robust CoSOD model learn-ing. With two group of image containing different types of salient object as input, the GEM first selects a set of images from each group by the proposed learning based strategy, then these images are exchanged. The proposed feature ex-traction module considers both the uncertainty caused by the irrelevant images and group consensus in the remain-ing relevant images. We design a latent variable genera-tor branch which is made of conditional variational autoen-coder to generate uncertainly-based global stochastic fea-tures. A CoSOD transformer branch is devised to capture the correlation-based local features that contain the group consistency information. At last, the output of two branches are concatenated and fed into a transformer-based decoder, producing robust co-saliency prediction. Extensive evalua-tions on co-saliency detection with and without irrelevant images demonstrate the superiority of our method over a variety of state-of-the-art methods. 1.

Introduction
Co-salient object detection (CoSOD) is to segment the common salient objects in a group of relevant images.
By detecting the co-salient object in a group of images, the images’ background and redundant content are re-∗Corresponding author. This work is supported in part by Na-tional Key Research and Development Program of China under Grant No. 2018AAA0100400, in part by the NSFC under Grant Nos. 62276141, 61872189.
Figure 1. (a) When there exists an irrelevant image in the test group, the state-of-the-art CoSOD models such as DCFM [39] and
GCoNet [9] tend to generate false positive predictions for the noisy image, yet our method can achieve an accurate prediction due to the use of group exchange-masking strategy in (b). moved, which helps the downstream tasks such as ob-ject tracking [38], co-segmentation [48] and video co-localization [14], to name a few.
Group consensus assumption is widely used by exist-ing CoSOD models, i.e., these models presume that all im-ages in the same group contain the common salient tar-gets. The widely-used CoSOD benchmark datasets, such as
COCO-SEG [33], CoCA [50], and CoSOD3k [8] organize the training and testing images that contain the same ob-ject as a group. Many existing CoSOD models consider the group consensus characteristic in modeling. For example, early works such as [2, 10, 13] extract hand-crafted features for inter-image co-object correspondence discovery. The deep learning models proposed in [15, 20, 36, 39, 44, 46, 48] use one group of relevant images as training data input for consensus representation learning. Among them, a va-riety of novel model design techniques have been devel-oped to make full use of the group consistency characteris-tic, such as the low-rank feature learning [46], co-attention model [20] and intra-saliency correlation learning [15]. The issues of the group consensus assumption are partially stud-ied in recent literature [9], which further models the inter-1
group separability for more discriminative feature learning by a group collaborating module.
In this paper, we find that the group consensus assump-tion also restricts the CoSOD model’s robustness against the images without common object. As illustrated by Fig-ure 1(a), state-of-the-art CoSOD models tend to output false positive predictions for the irrelevant image. This issue hinders the use of CoSOD models in real-world applica-tions where the testing inputs are likely to contain irrele-vant images. To enhance the model’s robustness, we pro-pose a learning framework called group exchange-masking (GEM). The GEM is illustrated by Figure 1(b). Given two image groups that contain different types of co-salient ob-jects, we exchange several images between one group and the other. Those exchanged images are called noisy images.
The number of noisy images is chosen to be less than the number of remaining relevant images in the group, so that the co-salient object in the noisy images forms a negative object but not the dominant co-salient object. The “mask-ing” strategy refers to the label regeneration of the noisy images. Because there is no co-salient object, in the regen-erated label, the original ground-truth object is masked. The learning objective is to correctly predict both the co-salient objects in the original relevant images and the added noisy images.
Adding noisy images to the training image group brings about uncertainty to the CoSOD model learning since there is some probability of no expected common object in each image. We design a dual-path image feature extrac-tion module to model the group uncertainty in addition to the group consensus property. Specifically, we design a latent variable generator branch (LVGB) to extract the uncertainty-based global image features. The LVGB mod-ule is motivated by the conditional variational autoencoder (CVAE) [32] that is widely used to address the uncertainty in vision applications including image background model-ing [19], RGB-D saliency detection [43] and image recon-struction [41].
In parallel with LVGB, we feed the im-age group into a CoSOD Transformer Branch (CoSOD-TB). The CoSOD-TB partitions each image group into lo-cal patches, and the attention mechanism in the transformer enables this branch to model patch-wise correlation-based local features. As a result, the group consistency informa-tion can be captured by this branch. The outputs of the two branches are concatenated and fed into a transformer-based decoder for co-saliency prediction. The proposed model has the following technical contributions.
• A robust CoSOD model learning mechanism, called group exchange-masking is proposed. By exchanging images between two groups, we augment the training data containing irrelevant images as noise to enhance model’s robustness. This is different from the tra-ditional CoSOD model learning frameworks that use groups of relevant images as training data.
• We propose a dual-path feature extraction module composed of the LVGB and the CoSOD-TB. The
LVGB is designed to model the uncertainty of co-salient object existence. The CoSOD-TB is for the consensus feature extraction of the salient object in the relevant images.
[42], CoCA [15],
• Extensive evaluations on three benchmark datasets, and including CoSal2015 the
CoSOD3k [7] show that proposed model to the state-of-the-art methods in terms of all evaluation metrics. Besides, the proposed model demonstrates good robustness for dealing with noisy data without co-salient objects. the superiority of 2.