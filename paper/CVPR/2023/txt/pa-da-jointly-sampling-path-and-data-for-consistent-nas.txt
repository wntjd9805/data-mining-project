Abstract
Based on the weight-sharing mechanism, one-shot NAS methods train a supernet and then inherit the pre-trained weights to evaluate sub-models, largely reducing the search cost. However, several works have pointed out that the shared weights suffer from different gradient descent direc-tions during training. And we further find that large gra-dient variance occurs during supernet training, which de-grades the supernet ranking consistency. To mitigate this issue, we propose to explicitly minimize the gradient vari-ance of the supernet training by jointly optimizing the sam-pling distributions of PAth and DAta (PA&DA). We theoret-ically derive the relationship between the gradient variance and the sampling distributions, and reveal that the optimal sampling probability is proportional to the normalized gra-dient norm of path and training data. Hence, we use the normalized gradient norm as the importance indicator for path and training data, and adopt an importance sampling strategy for the supernet training. Our method only requires negligible computation cost for optimizing the sampling dis-tributions of path and data, but achieves lower gradient variance during supernet training and better generalization performance for the supernet, resulting in a more consistent
NAS. We conduct comprehensive comparisons with other improved approaches in various search spaces. Results show that our method surpasses others with more reliable ranking performance and higher accuracy of searched ar-chitectures, showing the effectiveness of our method. Code is available at https://github.com/ShunLu91/PA-DA. 1.

Introduction
Neural architecture search (NAS) aims to automate the process of designing architectures. Conventional NAS
*Corresponding author. methods [2,58] separately train each sub-model to guide the controller for better architectures, demanding prohibitive computational complexity. ENAS [34] explores the weight-sharing mechanism across sub-models and One-Shot NAS
[3] proposes to train a supernet to share weights for sub-models to achieve higher efficiency. Later on, many follow-ups [9, 18, 30, 49] adopt this vein to perform NAS.
Though the weight-sharing mechanism greatly improves
NAS efficiency, many works [19, 20, 41, 50, 53, 56, 57] point out that the shared weights suffer from different gradient descent directions in different sub-models, leading to large gradient variance and poor ranking consistency. To mit-igate this issue, they [20, 41, 56, 57] propose to maintain multi-copies of supernet weights to decrease the weight-sharing extent, manually elaborate a better path sampling strategy [9, 50], or introduce additional loss regularizations
[19, 50, 53]. However, they typically require multiple com-putation burdens for the supernet training and obtain unsat-isfying results, motivating us to explore a better solution.
Notice that significant efforts [13, 17, 22, 36, 38, 40] have been dedicated to reducing the variance of the stochastic gradient descent (SGD) for minimizing finite sums. Many works [1, 5, 16, 23, 25, 45, 55] focus on optimizing the data sampling distribution to reduce the gradient variance (GV) for training deep models. These methods generally enjoy faster convergence and better generalization performance, inspiring us to improve the supernet training from the per-spective of gradient variance reduction.
We conduct a toy experiment on NAS-Bench-201 [15] using CIFAR-10 to investigate the GV for the supernet training. We use SPOS [18] algorithm to train the super-net and gradually increase the candidate operations on each edge to change the weight-sharing extent. We record the average GV of all candidate operation weights during train-ing and evaluate the supernet performance by measuring the ranking results of the same 64 sub-models (corresponding to the smallest search space with 2 candidate operations on
2.