Abstract
LiDAR has become a cornerstone sensing modality for 3D vision. LiDAR systems emit pulses of light into the scene, take measurements of the returned signal, and rely on hardware digital signal processing (DSP) pipelines to construct 3D point clouds from these measurements. The resulting point clouds output by these DSPs are input to downstream 3D vision models – both, in the form of train-ing datasets or as input at inference time. Existing LiDAR
DSPs are composed of cascades of parameterized opera-tions; modifying configuration parameters results in signif-icant changes in the point clouds and consequently the out-put of downstream methods. Existing methods treat LiDAR systems as fixed black boxes and construct downstream task networks more robust with respect to measurement fluctua-tions. Departing from this approach, the proposed method directly optimizes LiDAR sensing and DSP parameters for downstream tasks. To investigate the optimization of LiDAR system parameters, we devise a realistic LiDAR simulation method that generates raw waveforms as input to a LiDAR
DSP pipeline. We optimize LiDAR parameters for both 3D object detection IoU losses and depth error metrics by solv-ing a nonlinear multi-objective optimization problem with a 0th-order stochastic algorithm. For automotive 3D object detection models, the proposed method outperforms manual expert tuning by 39.5% mean Average Precision (mAP). 1.

Introduction
Environment perception for autonomous drones [12, 53] and vehicles [72] requires precise depth sensing for safety-critical control decisions. Scanning LiDAR sensors have been broadly adopted in autonomous driving [3, 7, 56] as they provide high temporal and spatial resolution, and re-cent advances in MEMS scanning [67] and photodiode tech-nology [64] have reduced their cost and form factor.
The 3D LiDAR point cloud (PC) data that existing 3D detection methods take as input is produced by a LiDAR and digital signal processor (DSP) pipeline with many mea-surement and processing steps. Typical LiDAR sensors op-erate by sending out a laser pulse and measuring the tem-poral response through an Avalanche Photo Diode (APD).
This temporal wavefront signal is fed to a DSP that extracts peaks corresponding to echos from candidate targets within the scene [70]. As such, DSP processing results in a 1000-fold data reduction for a single emitted beam, producing single or multiple 3D points per beam. Compressing the waveform into points in 3D space with minimal informa-tion loss is challenging because of object discontinuities, sub-surface scattering, multipath reflections, and scattering media, see Fig. 1. In particular, significant scattering occurs in adverse weather conditions like fog [3,4,6,20,23,25,66], rain [6,18,66] and snow [19,25,27,34]. LiDAR manufactur-ers currently handle such complications by manually adjust-ing internal sensing and DSP parameters in controlled envi-ronments and restricted real-world scenarios using a com-bination of visual inspection and depth quality metrics.
Generally, LiDAR production units are black boxes with configuration parameters hidden from the user. To ac-count for noisy point cloud measurements with spurious ar-tifacts, existing work has explored methods that add sim-ulated adverse effects and point cloud degradations that model rain [18, 26, 58], fog [3, 20, 50] and snow [19] to real black-box LiDAR datasets. Augmented point clouds in hand, downstream vision models are retrained for pre-dictions more robust to point cloud data corruption. An-other approach consists of generating synthetic measure-ments from 3D scenes using rendering engines [24, 57, 69].
Such existing methods typically avoid simulating transient light propagation and signal processing by converting 3D scene depth directly into a point cloud, thus lack physi-cally realistic modeling of fluctuations arising from mul-tipath effects or measurement noise. Notably, existing sim-ulation methods that alter measurements or generate syn-thetic point clouds do not optimize sensing or DSP param-eters for downstream vision performance.
In this work, we directly optimize LiDAR pulse con-figuration and DSP hyperparameters for end-to-end down-stream 3D object detector losses and PC depth quality met-rics, a challenging task because hyperparameter space in-volves tens to hundreds of categorical, discrete and effec-tively continuous parameters affecting downstream tasks in complex nonlinear ways via the intermediate point cloud.
Examples of categorical hyperparameters are Velodyne
LiDAR sensor return modes that configure internal wave-front peak selection algorithms for point cloud formation;
Figure 1. LiDAR Point Cloud Formation. Typical LiDAR sensor PC measurements are produced by a multi-stage measurement and signal processing chain: The LiDAR emits a laser pulse. This signal travels through the scene and returns to the detector after single or multiple reflections. Cluttered surfaces (a), strong retroreflectors (b) and ambient light are introduced (c) in the returned signals. Thus, the full transient waveform read by sensors is the superposition of multiple return paths. The DSP, itself a chain of processing blocks, processes all temporal waveforms and extracts a continuous stream of 3D points that forms the final point cloud (bottom). rotation velocity, which impacts angular resolution, is an example of a continuous hyperparameter [4].
Grid search optimization is impractical here because of combinatorial explosion. Orthogonally to LiDAR, it was recently shown that 0th order solvers can find camera DSP hyperparameters that improve downstream 2D object de-tectors [36, 48]. We propose an optimization method for
LiDAR sensing and DSP hyperparameters that minimizes end-to-end domain-specific losses such as RMSE of the measured depth against ground truth and IoU measured on downstream 3D object detection. To assess the proposed method, we devised a LiDAR simulation method based on the CARLA engine [13] that models a LiDAR DSP as well as the full transient noisy waveform formed by multiple laser echoes. We optimize sensing and DSP hyperparam-eters by solving a Multi-Objective black-box Optimization (MOO) problem with a novel CMA-ES (Covariance Matrix
Adaptation-Evolution Strategy [21]) that relies on a max-rank multi-objective scalarization loss [36] to dynamically improve scale matching between different loss components.
In combination with a novel champion selection method, it finds a balanced Pareto-optimal solution for which no loss component has a comparatively poor value for LiDAR op-timization with multiple objectives. We validate the pro-posed optimization method for 3D object detection and point cloud depth estimation, both in simulation and using an off-the-shelf experimental LiDAR sensor.
Specifically, we make the following contributions:
• We introduce a LiDAR wavefront simulation for the
CARLA simulation environment that models realistic transient scene responses.
• We devise a novel multi-objective optimization method for balanced MOO of LiDAR parameters.
• We validate end-to-end LiDAR sensing and DSP opti-mization for 3D object detection and depth estimation through simulation and with a real system. For all ap-plications, our approach outperforms existing state-of-the-art methods, including expert tuning.
Simulator code and DSP models are published here1.
Limitations Because commercial LiDAR units are IP-protected black boxes, interfacing their DSP hyperparam-eters is not straightforward. While the off-the-shelf LiDAR system used in this work makes some DSP hyperparame-ters accessible, most LiDAR systems are completely closed.
We hope that these findings spur LiDAR vendors to follow the lead of digital camera and ISP (Image Signal Processor) vendors and open their processing pipelines. 2.