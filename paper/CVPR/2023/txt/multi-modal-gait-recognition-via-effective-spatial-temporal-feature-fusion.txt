Abstract
Gait recognition is a biometric technology that iden-tifies people by their walking patterns. The silhouettes-based method and the skeletons-based method are the two most popular approaches. However, the silhouette data are easily affected by clothing occlusion, and the skeleton data lack body shape information. To obtain a more ro-bust and comprehensive gait representation for recognition, we propose a transformer-based gait recognition frame-work called MMGaitFormer, which effectively fuses and ag-gregates the spatial-temporal information from the skele-tons and silhouettes. Specifically, a Spatial Fusion Mod-ule (SFM) and a Temporal Fusion Module (TFM) are pro-posed for effective spatial-level and temporal-level feature fusion, respectively. The SFM performs fine-grained body parts spatial fusion and guides the alignment of each part of the silhouette and each joint of the skeleton through the at-tention mechanism. The TFM performs temporal modeling through Cycle Position Embedding (CPE) and fuses tempo-ral information of two modalities. Experiments demonstrate that our MMGaitFormer achieves state-of-the-art perfor-mance on popular gait datasets. For the most challenging
“CL” (i.e., walking in different clothes) condition in CASIA-B, our method achieves a rank-1 accuracy of 94.8%, which outperforms the state-of-the-art single-modal methods by a large margin. 1.

Introduction
Gait recognition is a biometric technology that identi-fies people by their walking patterns, which is one of the most promising video-based biometric technologies in the long-distance recognition system. However, it is still chal-lenging to perform reliable gait recognition, as its perfor-mance is severely affected by many complex factors, in-cluding clothing, carrying conditions, cross-view, etc.. To alleviate these issues, various methods have been proposed.
The appearance-based and model-based methods are the
†Corresponding Author.
Figure 1. Comparison of different gait representations of a subject from the CASIA-B gait dataset at different timesteps of normal walks (a) and walking in different clothes (b). Each row depicts the same frames as silhouette image, and 2D skeleton pose, the combination of skeletons and silhouettes, respectively, from top-to-bottom. Combines the complementary strengths of silhouette and skeleton, it is expected to be a more comprehensive represen-tation for gait. two most popular approaches for video-based gait recogni-tion. The appearance-based (i.e., silhouettes-based) meth-ods [5, 9, 14, 19, 27] rely on binary human silhouette im-ages segmented from the original video frame to eliminate the influence of external factors. They utilized convolu-tional neural networks (CNN) to extract spatio-temporal features and achieved state-of-the-art performance. The model-based methods [2,16,17,23] consider the underlying physical structure of the body and express the gait in a more comprehensible model. The most recent model-based ap-proaches are skeletons-based, in which they represent gait with the skeletons obtained from videos through pose esti-mation models. With clear and robust skeleton representa-tion, recent skeletons-based methods could even show com-petitive results compared to appearance-based methods.
Although both silhouette-based and skeletons-based methods have their advantages, we argue that the incom-pleteness of both input representations of the gait infor-mation limits further improvement of these methods. As shown in Fig.1(a), although the silhouettes retain most body shape information, the self-obscuring problem occurs when body areas overlap. Moreover, when clothing condition changes, as shown in Fig.1(b), the external body shape is significantly changed by clothing obscuration. However,
skeletons only keep the internal body structure information which effectively solves the clothing-obscuring and self-obscuring problems, but completely ignoring the discrim-inative body shape information leads to poor performance.
Thus, we could observe that the silhouette retains the exter-nal body shape information and omits some body-structure clues, and the skeleton preserves the internal body structure information. The two data modalities are complementary to each other, and their combination is expected to be a more comprehensive representation of gait.
Motivated by the observations above, to obtain robust and comprehensive gait representation for recognition, we propose a transformer-based gait recognition framework called MMGaitFormer, which effectively fuses and aggre-gates the spatial-temporal information from the skeletons and silhouettes. Precisely, the proposed framework consists of four main modules at three stages. Firstly, the silhou-ette sequence and skeleton sequence are extracted from the original RGB video by segmentation and pose estimation methods, respectively. After that, we feed the silhouettes and skeletons into independent encoding modules to ex-tract unique spatio-temporal feature maps for each modal.
Finally, we propose a Spatial Fusion Module (SFM) and a Temporal Fusion Module (TFM) for spatial and tempo-ral feature fusion, respectively. As a video-based recog-nition task, how to effectively extract discriminative gait features from spatio-temporal information is the most crit-ical issue. In this work, we consider both fine-grained fu-sion at the spatial level and fine-aligned fusion at the tem-In the SFM, we design a co-attention mod-poral level. ule to enable the interactions between the silhouettes and skeletons. Specifically, we construct strategies called Fine-grained Body Parts Fusion (FBPF) to guide SFM for fine-grained feature fusion learning based on prior positional re-lationships between joints in the skeleton and corresponding parts in the silhouette. In the TFM, we introduced an em-bedding modeling operation for fine-aligned temporal mod-eling, in which we design the Cycle Position Embedding (CPE) to efficiently capture gait cycle features and better model the temporal information for gait sequences.
The main contributions of the proposed method are sum-marized as follows: (1) We propose an effective and novel multi-modal gait recognition framework called MMGait-Former, which utilizes a more comprehensive gait represen-tation constructed from silhouettes and skeletons for better recognition. (2) A co-attention-based Spatial Fusion Mod-ule is proposed to perform a fine-grained body parts fusion (FBPF) of spatial gait features by using the prior positional relationships of each skeleton joint and each silhouette part. (3) We propose a novel Temporal Fusion Module for feature fusion at the temporal level, in which we design the Cycle
Position Embedding (CPE) to model temporal relationships for gait sequences of arbitrary length. Experiments demon-strate that our MMGaitFormer achieves state-of-the-art per-formance on popular gait datasets. For the most challeng-ing condition (i.e., walking in different clothes) in CASIA-B [26], our method achieves a rank-1 accuracy of 94.8%, which outperforms the state-of-the-art Single-modal meth-ods by a large margin (+11.2% accuracy improvement ). 2.