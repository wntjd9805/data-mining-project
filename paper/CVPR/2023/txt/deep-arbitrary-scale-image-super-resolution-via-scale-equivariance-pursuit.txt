Abstract
The ability of scale-equivariance processing blocks plays a central role in arbitrary-scale image super-resolution tasks. Inspired by this crucial observation, this work proposes two novel scale-equivariant modules within a transformer-style framework to enhance arbitrary-scale image super-resolution (ASISR) performance, especially in high upsampling rate image extrapolation. In the feature extraction phase, we design a plug-in module called Adap-tive Feature Extractor, which injects explicit scale informa-tion in frequency-expanded encoding, thus achieving scale-adaption in representation learning.
In the upsampling phase, a learnable Neural Kriging upsampling operator is introduced, which simultaneously encodes both relative distance (i.e., scale-aware) information as well as feature similarity (i.e., with priori learned from training data) in a bilateral manner, providing scale-encoded spatial feature fusion. The above operators are easily plugged into mul-tiple stages of a SR network, and a recent emerging pre-training strategy is also adopted to impulse the model’s performance further. Extensive experimental results have demonstrated the outstanding scale-equivariance capabil-ity offered by the proposed operators and our learning framework, with much better results than previous SOTA methods at arbitrary scales for SR. Our code is available at https://github.com/neuralchen/EQSR. 1.

Introduction
Arbitrary-scale image super-resolution (ASISR), which aims at upsampling the low-resolution (LR) images to high-resolution (HR) counterparts by any proper real-valued magnifications with one single model, has become one of the most interesting topics in low-level computer vision research for its flexibility and practicality. Unfortunately, compared with fixed-scale SISR models [2,3,11,19,20,22], existing methods on ASISR [4,9,18,33] usually offer much lower SR performances (e.g., PSNR), hindering their prac-*Equal Contribution.
†Corresponding author: Bingbing Ni.
Figure 1. Scale-equivariance of our network. We compare the
PSNR degradation rate of our method and ArbSR [33]. Taking the SOTA fixed-scale method HAT [3] as reference, our model presents a more stable degradation as the scale increases, reflecting the equivariance of our method. Please enlarge the pdf for details. tical applications. The major causes of the defects of pre-vious ASISR methods are due to lack of scale-equivariance in dealing with scale-varying image features, as explained in detail as follows.
On the one hand, the model’s backbone should possess the capability of adaptively processing features according to the sampling scale in order to achieve scale-equivariance in the feature extraction phase. To be concrete, since ASISR models rely on only one single backbone to handle different scales, designing a scale-equivariant feature learning mod-ule that extracts, transforms, and pool image information from adaptively adjusted sampling positions (i.e., accord-ing to the scaled distance) to obtain scale-equivariant fea-ture maps from the same source image with different scales is essential. As shown in Figure 2, we analyze a series of fixed-scale HAT [3] models and find that the extracted fea-tures show apparent divergences from the middle of the net-work to handle different scale factors, demonstrating that features for different scales should be extracted adaptively.
On the other hand, we also expect the model to possess suitable scale-equivariant properties in the image/feature upsampling stage. Namely, the upsampling module should
be designed to perform adaptive interpolation operations ac-cording to arbitrary scaling factors. It is worth noting that the ability to handle out-of-distribution scales (i.e., scales that the model has never seen in training) is crucial for
ASISR models since the training process is impossible to cover all possible upsampling scales. However, existing methods commonly use 3 × 3 convolution layers for up-sampling, which has been proved to lack scale equivari-ance by many studies [35–37], leading to insensitivity to the changes of the scale factor. Some implicit-field-based methods, such as LIIF [4], adopt a channel-separated MLP to enhance the scale equivariance; however, additional oper-ations, including feature unfolding and local ensemble, are needed, resulting in a cumbersome upsampler. Alias-Free
StyleGAN [12] points out that 1 × 1 convolution could be regarded as an instance of a continuously E(2)-equivariant model [34] in image generation, but 1 × 1 receptive field cannot aggregate the crucial local information for SR.
Motivated by the above analysis, this work proposes two novel scale-equivariant modules within a transformer-style framework for enhancing arbitrary-scale image super-In the feature extraction phase, resolution performance. we design a novel module called Adaptive Feature Ex-tractor (AFE), which explicitly injects scale information in the form of frequency-expanded encoding to modulate the weights of subsequent convolution. Combined with the traditional self-attention mechanism, this operator can be plugged into multiple stages of the feature extraction sub-network and achieves a large receptive field as well as good scale-adaption properties in representation learning.
When upsampling, instead of monotonically using pixel-independent convolutions (e.g., Alias-Free StyleGAN [12],
LIIF [4]), we propose a brand-new paradigm, i.e., Neural
Kriging upsampler, which endows vanilla K × K convolu-tions with highly competitive equivariance while maintain-ing excellent spatial aggregation. Specifically, the Neural
Kriging upsampler simultaneously encodes geometric in-formation (i.e., relative position) and feature similarity (i.e., prior knowledge learned from training data) in a bilateral manner, providing scale-encoded spatial feature fusion.
Combining the above modules, we construct a model with a certain equivariance named EQSR, which can adap-tively handle different sampling rates. We conduct exten-sive qualitative and quantitative experiments to verify the superiority of our method on ASISR benchmarks. Com-pared with state-of-the-art methods, average PSNRs of our model have shown significant advantages in both in-distribution and out-of-distribution cases. Under the ×2 and
×3 configurations, we surpass the previous SOTA LTE [18] by 0.33dB (34.83dB v.s. 34.50dB) and 0.35dB (29.76dB v.s. 29.41dB) on the Urban100 dataset. Under the ×6 con-figuration, we also achieve a large gap of 0.21dB, proving the effectiveness of our scale-equivariant operator.
Figure 2. Feature similarity of Different models. We compare the recent SOTA fixed-scale model HAT [3] and arbitrary-scale model
ArbSR [33]. (a) shows the CKA similarity of ×2/3/4 features at each layer; (b) compares the performance of these methods on the
Urban100 dataset. 2.