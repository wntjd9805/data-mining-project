Abstract
Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, con-ventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and mod-els. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more ef-ficient and promising. Specifically, we identify two impor-tant shortcomings of naive distribution matching (i.e., im-balanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware dis-tribution regularization). Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scal-ing data condensation to larger datasets and models. Ex-tensive experiments demonstrate the effectiveness of our method. Codes are available at https://github. com/uitrbn/IDM 1.

Introduction
Deep learning [23, 25, 57] is notoriously data-hungry, which poses challenges for both its training and data stor-age. To improve data storage efficiency, Dataset Conden-sation (DC) [47, 56] aims to condense large datasets into smaller ones while retaining their validity for model train-ing. Unlike traditional coreset selection methods [38, 44, 49,50], such dataset condensation is often achieved through image synthesis and yields better performance.
If prop-erly condensed, the resulting datasets not only consume less storage space, but can also benefit various downstream tasks
*Corresponding authors are Guanbin Li and Yizhou Yu.
Figure 1. Illustration of optimization-oriented methods and distri-bution matching methods. L: classification loss; g: gradient; O: output of models; Lmatching: the matching loss for condensation. such as network architecture search and continual learning by reducing their computational costs.
The pioneering DC approach [56, 56] guarantees the va-lidity of a condensed dataset by imposing a strong assump-tion that a model trained on it should be identical to that trained on the real dataset. However, naive matching be-tween these converged models can be too challenging due to their large parameter space and long optimization towards convergence. To this end, they impose an even stronger as-sumption that the two models should share an identical or similar optimization path, which can be achieved by match-ing either their gradients [53,56] or their intermediate model parameters [8] during training. We therefore refer to them as optimization-oriented methods.
However, despite their success, the unique characteris-tics of optimization-oriented DC methods imply that they inevitably suffer from high computational costs and thus scale poorly to large datasets and models. Specifically, they all involve the optimization of models (either randomly-initialized [53, 56] or initialized with parameters of pre-trained models [8]) against the condensed dataset, and thus rely on a nested loop that optimizes the condensed dataset and model parameters in turn. Note that the use of pre-trained models require additional computation and storage space [8]. As a result, existing optimization-oriented DC methods are only applicable to “toy” networks (e.g., three-layer convolutional networks) and small datasets (e.g., CI-FAR10, CIFAR100). Whether they can be scaled to real-world scenarios is still an open question.
To scale DC to large models and datasets, distribution matching (DM) [54] proposes to match the output feature distributions of the real and condensed datasets extracted by randomly-initialized models. This stems from the fact that the validity of a condensed dataset can also be guar-anteed if it produces the same feature distribution as the real dataset. Since DM does not involve the optimization of models against the condensed dataset, it avoids the expen-sive nested loops in optimization-oriented methods, and is thus highly efficient and scalable. However, despite being promising, experimental results show that its performance still lags behind that of the state-of-the-art optimization-oriented methods.
In this paper, we perform an in-depth analysis on DM’s unsatisfactory performance and propose a set of remedies that can significantly improve it, namely improved distribu-tion matching (IDM). Specifically, we analyzed the output feature distributions of DM and observed that although their means match, the features of the condensed dataset scatter around, causing severe class misalignment problems, which accounts for its impaired performance. We ascribe such scattered features to two shortcomings of DM as follows: i) DM suffers from the imbalanced number of features. In-tuitively, DM uses the features of small condensed datasets to match those of large real datasets, which is inherently intractable. Addressing this shortcoming, we propose Par-titioning and Expansion augmentation, which augments the condensed dataset by evenly splitting each image into l × l parts and expanding each part to the size of the original im-age, resulting in l2 features per image and a better match to the features of the real dataset. ii) Randomly initialized models are not valid embedding functions for the Maximum Mean Discrepancy (MMD) [18] estimation used in DM. Specifically, DM justifies the valid-ity of randomly-initialized models by their intrinsic clas-sification power observed in tasks such as deep cluster-ing [3,5,6,36]. However, we believe that this does not apply to DM as randomly-initialized models do not satisfy the re-quirement of embedding functions used in MMD and makes it an invalid measure of distribution distance. Since it is too challenging to design neural network based embedding functions that are valid for MMD, we propose two simple yet effective remedies: 1) Efficient and enriched model sam-pling. We enrich the embedding functions in MMD with semi-trained models as additional feature extractors, and develop a memory-efficient model queue to facilitate their sampling. 2) Class-aware distribution regularization. We explicitly regularize the feature distributions of condensed datasets to further alleviate class misalignment.
These three novel techniques together help to extract better feature distributions for DM. Our contributions include:
• We deeply analyze the shortcomings of the Distributed
Matching [54] algorithm and reveal that the root of its impaired performance lies in the problem of class mis-alignment.
• We propose improved distribution matching (IDM), consisting of three novel techniques that address the shortcomings of DM and help to learn better feature distributions.
• Experimental results show that our IDM achieves sig-nificant improvement over DM and surpasses the per-formance of most optimization-oriented methods.
• We show that our IDM method is highly efficient and scalable, and can be applied to large datasets such as
ImageNet Subset [10, 43]. 2.