Abstract 1.

Introduction
Limited by the trade-off between frame rate and exposure time when capturing moving scenes with conventional cam-eras, frame based HDR video reconstruction suffers from scene-dependent exposure ratio balancing and ghosting ar-tifacts. Event cameras provide an alternative visual repre-sentation with a much higher dynamic range and temporal resolution free from the above issues, which could be an effective guidance for HDR imaging from LDR videos. In this paper, we propose a multimodal learning framework for event guided HDR video reconstruction.
In order to better leverage the knowledge of the same scene from the two modalities of visual signals, a multimodal representa-tion alignment strategy to learn a shared latent space and a fusion module tailored to complementing two types of sig-nals for different dynamic ranges in different regions are proposed. Temporal correlations are utilized recurrently to suppress the ﬂickering effects in the reconstructed HDR video. The proposed HDRev-Net demonstrates state-of-the-art performance quantitatively and qualitatively for both synthetic and real-world data.
The dynamic range of the real world usually exceeds what a conventional camera and 8-bit image can record by a large margin. High dynamic range (HDR) imaging, which expands the luminance range limited by low dynamic range (LDR) images or videos, is a broadly used technique with extensive applications in photography/videography, video games, and high-end display.
Most HDR imaging methods for conventional cameras rely on capturing and merging multiple snapshots with dif-ferent exposure times [9, 49], which is challenging for cap-turing videos. There have been enduring efforts for sophisti-cated modiﬁcation on conventional frame based cameras to capture multi-exposure sequences (nearly) simultaneously, e.g., beam splitting with three or more sensors [69,70], tem-porally [7, 30, 32] or spatially [1, 8, 22, 28, 53, 54] varying exposure. Nevertheless, their abilities for HDR video re-construction are limited by the trade-off between a higher frame rate (for a smooth viewing experience) and a higher dynamic range (for capturing details in dark regions with
∗ Corresponding author
Project page: https://yixinyang-00.github.io/HDRev/
prolonged exposure time). Moreover, the optimal expo-sure ratio between LDR frame sequences with different ex-posure settings is scene-dependent and temporally-varying, whose balancing is difﬁcult for diverse scenes captured in videos. Even worse, moving objects or camera shaking during video capture can lead to ghosting effects in frames generated by long exposure shots. An HDR video could also be hallucinated from LDR inputs in a frame-by-frame manner by leveraging prior knowledge of tone-mapping op-erators [61] or data modeling powers of deep learning [11].
However, due to the highly ill-posed nature of the hallucina-tion process, it inevitably leads to severe ﬂickering effects.
In recent years, the event camera [16] has drawn increas-ing attention of researchers, due to its advantages over con-ventional frame based ones in sensing fast motions and ex-tended dynamic ranges (e.g., 120dB for DAVIS346). Unlike using multi-exposure frames, events recorded along with an
LDR video encode HDR irradiance changes without sacri-ﬁcing the frame rate/exposure time of the LDR video, which avoids ghosting artifacts as well, and is very promising as guidance for HDR video reconstruction.
However, integrating events with LDR video for HDR video reconstruction is challenging due to inconsistency be-tween events and frames in three aspects: 1) Modality mis-alignment: Frames and events are completely different rep-resentations of visual information, and “fusing” them by
ﬁrst translating events into intensity values [60,80] like [24] often includes artifacts from solving the ill-posed event in-tegration problem. 2) Dynamic range gap: Performing im-age/video reconstruction under the guidance of events [75], i.e., doubly integrating events as intensity changes within the exposure time [56, 57], ignores the dynamic range clip-ping in the capturing process of LDR frames, which leads to uncertainties in under/over-exposed regions. 3) Tex-ture mismatching: Regions with smooth textures and slow motion hardly produce effective event observations, which results in inconsistent textures among consecutive event stacks and ﬂickering effects in the reconstructed videos.
We propose HDRev-Net, a multimodal learning frame-work for event guided HDR video reconstruction to tackle the challenges by the following strategies: 1) To achieve multimodal representation alignment for the two modalities of the same scene, we propose a learning strategy to pro-gressively project them onto a shared representation space. 2) To reliably complement information from the two modal-ities in over/under-exposed regions, the representations pro-duced by the two modality-speciﬁc encoders are fused for an expressive joint representation using a conﬁdence guided multimodal fusion module. 3) To effectively suppress the
ﬂickering effects, we utilize the temporal redundant infor-mation between consecutive frames and events via the pro-posed recurrent convolutional encoders.
As shown in Fig. 1, HDRev-Net can successfully fuse
LDR frames and events to obtain HDR frames with more details and less ﬂickering effects. It demonstrates state-of-the-art HDR video reconstruction performance on both syn-thetic and real data by making the following contributions:
• We design a multimodal alignment strategy to bridge the gap between events and frames by aligning their representation in a shared latent space.
• We develop a conﬁdence guided fusion module to complement HDR information from events and ﬁner details from well-exposed regions in LDR frames.
• We utilize the temporal correlation from consecutive events and LDR frames in a recurrent fashion to alle-viate the ﬂickering effects for recovered HDR videos. 2.