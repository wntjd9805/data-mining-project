Abstract
We propose a novel visual-inertial odometry (VIO) ini-tialization method, which decouples rotation and transla-tion estimation, and achieves higher efficiency and bet-ter robustness. Existing loosely-coupled VIO-initialization methods suffer from poor stability of visual structure-from-motion (SfM), whereas those tightly-coupled methods of-ten ignore the gyroscope bias in the closed-form solution, resulting in limited accuracy. Moreover, the aforemen-tioned two classes of methods are computationally expen-sive, because 3D point clouds need to be reconstructed si-multaneously. In contrast, our new method fully combines inertial and visual measurements for both rotational and translational initialization. First, a rotation-only solution is designed for gyroscope bias estimation, which tightly couples the gyroscope and camera observations. Second, the initial velocity and gravity vector are solved with lin-ear translation constraints in a globally optimal fashion and without reconstructing 3D point clouds. Extensive ex-periments have demonstrated that our method is 8 ∼ 72 times faster (w.r.t. a 10-frame set) than the state-of-the-art methods, and also presents significantly higher robustness and accuracy. The source code is available at https:
//github.com/boxuLibrary/drt-vio-init. 1.

Introduction
Visual-inertial odometry (VIO) aims to estimate camera motion and recover 3D scene structure by fusing both im-age and IMU measurements. The low-cost and compactness of the camera module and IMU sensors make VIO widely used in virtual or augmented reality systems (VR/AR) and various autonomous navigation systems. Currently, most
VIO systems track camera motion by minimizing nonlinear
*Equal contribution.
Figure 1. Comparison of computational cost and scale factor er-rors on EuRoC dataset. Different colors indicate different types of methods. Our proposed initialization method for decoupling rotation and translation (DRT) is accurate and computationally ef-ficient. visual re-projection errors [14, 30], so the accuracy of the initial value will affect the convergence. In addition, the ro-bustness and lower latency of the initialization are also very important for the downstream application, e.g. AR develop-ers need accurate camera tracking within a few hundred mil-liseconds after launching VIO, regardless of the use case.
For the sensor that has calibrated intrinsic and extrinsic pa-rameters, the initial variables for VIO include the gravity vector, initial velocity, gyroscope and accelerometer biases.
Many VIO systems are initialized by setting the initial velocity to zero, then calculating the gravity vector and gy-roscope bias with IMU measurements [14,20,36]. However, this method only works when the system is strictly static.
For sensors in motion, loosely-coupled and tightly-coupled initialization methods are widely studied. As shown in Fig. 2, the loosely-coupled methods [5,28,30] combine the cam-era poses estimated by visual SfM and the IMU measure-ments to estimate the initial state variables. However, vi-sual SfM is prone to inaccuracy or failure when co-viewed
Figure 2. Comparison between our method and previous VIO initialization methods. Different colored arrows indicate different information flows for VI fusion. Our method takes full advantage of the complementary information between vision and IMU. In contrast, previous loosely-coupled methods do not incorporate IMU information into visual SfM, and previous tightly-coupled methods do not use visual observations to remove gyroscope bias, either of which affects the robustness and accuracy of VIO initialization. frames are insufficient or the camera rotates rapidly. The motion information measured by IMU is not used to im-prove the robustness of visual SfM. The tightly-coupled methods [8, 9, 24, 25] firstly use gyroscope measurements and calibrated extrinsic parameters to estimate camera rota-tion, then use closed-form solution constructed with vision and accelerometer observations to solve for the initial ve-locity and gravity vector. However, this type of method has poor accuracy on systems equipped with inexpensive and noisy IMU (e.g. cell phones), because no visual observa-tions are used to estimate the gyroscope bias. Moreover, the three-dimensional coordinates of point clouds are ob-tained with the closed-form solution, resulting in a large and time-consuming solution matrix. Both the above two kinds of methods under-utilize the complementary advantages be-tween visual and inertial sensors, resulting in limited accu-racy and robustness.
According to [17, 18, 26, 38], image observations could be directly used to optimize frame-to-frame rotation and camera poses could be efficiently solved with linear global translation constraints [3].
Inspired by this, we propose a novel rotation-translation-decoupled VIO initialization framework. Gyroscope measurements are directly inte-grated into the camera rotation estimation, which greatly improves the robustness of initialization, and the translation related initial variables are solved efficiently without esti-mating the 3D structure. As shown in Fig. 1, our method achieves the lowest scale error and is significantly faster than previous methods. The scale factor error is one of the metrics for evaluating the initialization. Our main contribu-tions are
- We propose a rotation-only solution to directly opti-mize gyroscope bias using image observations, which can obtain camera rotation more efficiently and more robustly compared to vision-only methods.
- We propose a globally optimal solution for estimating the initial velocity and gravity vector based on linear translation constraints. Its linearity and independence of scene structure significantly benefit computational efficiency.
- Our proposed initialization framework outperforms the state-of-the-art in both accuracy and robustness on public datasets while being 8 ∼ 72 times faster in cal-culation time for a 10-frame set. We published our code to facilitate communication. 2.