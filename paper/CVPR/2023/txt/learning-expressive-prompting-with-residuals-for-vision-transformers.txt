Abstract
Prompt learning is an efficient approach to adapt trans-formers by inserting learnable set of parameters into the input and intermediate representations of a pre-trained model. In this work, we present Expressive Prompts with
Residuals (EXPRES) which modifies the prompt learn-ing paradigm specifically for effective adaptation of vi-sion transformers (ViT). Our method constructs down-stream representations via learnable “output” tokens (shal-low prompts), that are akin to the learned class tokens of the
ViT. Further for better steering of the downstream repre-sentation processed by the frozen transformer, we introduce residual learnable tokens that are added to the output of various computations. We apply EXPRES for image classi-fication and few-shot semantic segmentation, and show our method is capable of achieving state of the art prompt tun-ing on 3/3 categories of the VTAB benchmark. In addition to strong performance, we observe that our approach is an order of magnitude more prompt efficient than existing vi-sual prompting baselines. We analytically show the compu-tational benefits of our approach over weight space adap-tation techniques like finetuning. Lastly we systematically corroborate the architectural design of our method via a series of ablation experiments. 1.

Introduction
Scaling up of neural nets in the past few years has steadily improved performance on wide variety of downstream vi-sual tasks. However, model adaptation is often necessary to achieve the best performance in downstream tasks like fine-grained recognition [89], semantic segmentation [8] or object recognition [34]. While traditional techniques like full-model finetuning have become the de-facto approach to adaptation, they are not well suited for many scenarios.
For example, finetuning is susceptible to catastrophic for-getting [37] as it modifies model parameters without the knowledge of future domains, and potentially losing prior
∗Work conducted while interning at AWS AI Labs.
∗∗Work conducted while at AWS AI Labs.
Figure 1. Adapting large vision models is crucial to solving down-stream tasks with wide variety of semantics (e.g., image classifica-tion, semantic segmentation etc.) as well as dataset sizes (few-shot, low-shot, full-shot). In this work, we propose a novel adaptation technique for large vision models that is capable of achieving the desired goal. knowledge of current adaptation. Moreover, finetuning all of the model parameters of a large vision model with just a few training examples can lead to poor generalization. This is in contrast to human intelligence that is capable of solv-ing wide variety of downstream tasks with extremely few exemplars.
Motivated by the need for better adaptation, parameter effi-cient techniques like partial-finetuning or adapters [64,101] have been developed to constructively adapt large models without significant parameter overhead. While serving as effective alternatives to finetuning, most parameter efficient techniques have been designed with convolutional architec-tures in mind. In light of recent works [15] that demonstrate that Vision Transformers are more suitable for scaling up than CNNs, designing adaptation techniques that exploit the
Transformer architecture can be extremely useful. To that end, visual prompt tuning (VPT) [32] has been proposed as a way to constructively adapt transformers by introducing learnable tokens at every layer that interact with the patch and class tokens and are optimized together with a classi-fier head. While being effective in practice, VPT allows only partial interactions between prompts and the remain-ing tokens, thus, leveraging only a part of the prompt capac-ity. Moreover, it often requires a large number of inserted prompts to achieve optimal performance but that signifi-cantly increases the computation costs due to the quadratic computational complexity of the self-attention layer.
In this work, we explore an alternate design to prompting motivated by the potential for greater prompt capacity. We propose ExPRes, an expressive prompt tuning method with residual tokens that inherits the strengths of parameter effi-cient adaptation while significantly improving downstream performance. Our prompt design is inspired by the two key observations - propagation of prompts by multilayered in-teraction with other tokens is crucial for strong capacity and learnable residual tokens can modulate the propagated prompts to favour task-specific relations (unlike in [32]).
We first propagate shallow prompts through the encoder that are average pooled at the last layer to yield seman-tic image-level representations. Shallow prompts by them-selves have limited capacity since they cannot specifically modulate token-token relations at higher layers. Therefore to harness the prompts, we add residual tokens to prop-agated prompts at various layerwise computations of the
Transformer encoder including LayerNorm, self-attention and multi-head projection to facilitate layerwise modulation without increasing the number of prompts per layer. This results in enhanced prompt capacity at almost no additional computational cost.
We empirically validate the effectiveness of our method on a variety of downstream tasks including fine-grained recog-nition and semantic segmentation. Our use of additional learnable parameters in the form of residual and shallow prompts allows the retention of prior knowledge in the form of frozen encoder weights while being extremely parameter efficient (prompts are ≤ 1% of the total parameters). Thus, our method is highly suited for real world adaptation that requires information retention at low memory and computa-tional overheads. Additionally, we show that in most cases we require fewer prompts than VPT to achieve the same or better performance, making it more suitable for limited data settings. Our main contributions can be summarized as fol-lows:
• We propose a novel prompting technique: EXPRES, that uses a combination of shallow and deep residual prompts to facilitate constructive adaptation to down-stream tasks with limited labelled datasets.
• Our method significantly outperforms full-finetuning based adaptation by 4.6% on VTAB-1k. Moreover, our method outperforms state-of-the-art prompting ap-proach [32] on the same benchmarks with significantly fewer prompts, suggesting that prompt design is cru-cial to extracting more capacity at a given parame-ter/computational budget.
• To the best of our knowledge, we are the first to demonstrate the effectiveness of prompting for diverse applications such as few-shot semantic segmentation.
Our method outperforms strong adaptation baselines by 25% and achieves competitive performance with respect to language-assisted segmentation [40] despite training on significantly less data with dense annota-tions. 2.