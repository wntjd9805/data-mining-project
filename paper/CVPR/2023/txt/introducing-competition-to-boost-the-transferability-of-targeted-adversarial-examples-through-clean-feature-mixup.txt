Abstract
Deep neural networks are widely known to be suscep-tible to adversarial examples, which can cause incorrect predictions through subtle input modifications. These ad-versarial examples tend to be transferable between mod-els, but targeted attacks still have lower attack success rates due to significant variations in decision boundaries.
To enhance the transferability of targeted adversarial ex-amples, we propose introducing competition into the op-timization process. Our idea is to craft adversarial per-turbations in the presence of two new types of competi-tor noises: adversarial perturbations towards different tar-get classes and friendly perturbations towards the correct class. With these competitors, even if an adversarial ex-ample deceives a network to extract specific features lead-ing to the target class, this disturbance can be suppressed by other competitors. Therefore, within this competition, adversarial examples should take different attack strategies by leveraging more diverse features to overwhelm their in-terference, leading to improving their transferability to dif-ferent models. Considering the computational complexity, we efficiently simulate various interference from these two types of competitors in feature space by randomly mixing up stored clean features in the model inference and named this method Clean Feature Mixup (CFM). Our extensive exper-imental results on the ImageNet-Compatible and CIFAR-10 datasets show that the proposed method outperforms the ex-isting baselines with a clear margin. Our code is available at https://github.com/dreamflake/CFM . 1.

Introduction
Although deep neural networks have excelled in various computer vision tasks such as image classification [10, 12] and object detection [19, 22], they are vulnerable to mali-ciously crafted inputs called adversarial examples [8, 37].
These adversarial examples are generated by optimizing im-perceptible perturbations to mislead a model to incorrect predictions. Intriguingly, these adversarial examples tend to be transferable between models, and this unique char-acteristic allows adversaries to attempt adversarial attacks on a black-box model without knowing its interior. How-ever, targeted adversarial attacks, which have a specific tar-get class, still have lower attack success rates due to sig-nificant differences in decision boundaries [17, 37]. Never-theless, targeted attacks can pose more serious risks as they can deceive models into predicting a specific harmful target class. Therefore, preemptive research on developing a novel transfer-based attack is crucial because it can assist service providers in preparing their models for these forthcoming risks and evaluating their models’ robustness.
In this work, we aim to further improve the transferabil-ity of targeted adversarial examples by introducing compe-tition into their optimization. Our approach involves craft-ing adversarial perturbations in the presence of two new types of noises: (a) adversarial perturbations towards dif-ferent target classes; and (b) friendly perturbations towards the correct class. With these competitors and a source model, even if an adversarial example deceives the source model into extracting certain features that lead to the tar-get class, this disturbance may be suppressed by interfer-ence from competitors. Consequently, adversarial pertur-bations should take various attack strategies, leveraging a wider range of features to overcome interference, which en-hances their transferability to different models. In the fol-lowing, we will further discuss why employing a diverse set of features for attack can boost the transferability of targeted adversarial examples.
In image classification, deep learning models extract a variety of features from images across multiple layers and comprehensively evaluate them to calculate prediction probabilities for each class. As numerous features can con-tribute to the final output, even when two images are recog-nized as the same class, the contributing features can signifi-cantly differ. Taking this into account, optimizing adversar-ial examples to utilize as many distinct feature combinations as possible would effectively enhance their transferability.
Conversely, in existing frameworks, an adversarial ex-Figure 1. Overview of the Clean Feature Mixup (CFM) method. ample may be optimized to intensely distract a limited num-ber of features identified in the early stages. However, the target model, unlike the source model, might be insensi-tive to such feature distractions, leading to the failure of transfer-based attacks.
Meanwhile, if we model the competitors as noises that should also be optimized, they require additional backward passes, substantially increasing the computational burden.
To address this challenge and enhance interference diver-sity, we propose Clean Feature Mixup (CFM), a method that efficiently mimics the behaviors of the two types of pertur-bations in feature space by mixing stored clean features of the images within a batch. A detailed description of their similarities can be seen in Section 3.3.
The overview of the proposed CFM method is illustrated in Fig. 1. Specifically, this method converts a pre-trained source model by attaching our specially designed CFM modules to convolution and fully-connected layers. After that, the attached CFM modules randomly mix the features of the clean images (i.e., clean features) with current in-put features at each inference. This process can effectively mitigate the overfitting of adversarial examples in their op-timization by preventing them from focusing on particular features in their targeted attacks on the source model. Un-like many existing techniques [18, 30, 31] that significantly increase the computational cost by multiplying the required number of forward/backward passes, CFM adds just one ad-ditional forward pass for storing clean features and requires a marginal amount of computation for feature mixup at each inference.
Our contributions can be summarized as follows:
• We propose the idea of introducing competition into the optimization of targeted adversarial examples with two types of competitor noises to encourage the uti-lization of various features in their attacks, ultimately boosting their transferability.
• Motivated by the above idea, we propose the Clean
Feature Mixup (CFM) method to improve the trans-ferability of adversarial examples. This method effi-ciently simulates the competitor noises by randomly mixing up stored clean features of the images in a batch.
• We performed extensive experiments with 20 models, including four defensive models and five Transformer-based classifiers. Our experimental results on the
ImageNet-Compatible and CIFAR-10 datasets demon-strate that CFM outperforms state-of-the-art baselines. 2.