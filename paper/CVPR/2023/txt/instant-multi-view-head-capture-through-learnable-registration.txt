Abstract
Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow and commonly ad-dress the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Es-timation of 3D Meshes from Performances of Expressive
Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans’ sur-faces and being robust to scanning noise and outliers. In-stead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training, we mini-mize a geometric loss commonly used for surface registra-tion, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly pre-dicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient cap-ture of large datasets containing multiple people and di-verse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de. 1.

Introduction
Capturing large datasets containing 3D heads of multi-ple people with varying facial expressions and head poses is a key enabler for modeling and synthesizing realistic head avatars. Typically, building such datasets is done in two steps: unstructured 3D scans are captured with a calibrated multi-view stereo (MVS) system, followed by a non-rigid registration step to unify the mesh topology [23]. This two-stage process has major drawbacks. MVS reconstruction requires cameras with strongly overlapping views and the resulting scans frequently contain holes and noise. Reg-istering a template mesh to these scans typically involves manual parameter tuning to balance the trade-off between accurately fitting the scan’s surface and being robust to scan artifacts. Both stages are computationally expensive, each taking several minutes per scan. For professional captures, both steps are augmented with manual clean-up to enhance
≫ the quality of the output meshes [3, 67]. Such manual edit-ing is infeasible for large-scale captures ( 10K scans).
Instead, we advocate for a more practical setting that di-rectly predicts 3D heads in dense correspondence from cal-ibrated multi-view images, effectively bypassing the MVS step. We achieve this with TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads), 0.3 seconds per head on a NVIDIA A100-which quickly ( 0.26 mm median
SXM GPU) infers accurate 3D heads ( error) in correspondence, without manual user input.
∼
∼
While several methods exist that directly recover 3D faces in correspondence from calibrated multi-view im-ages, they have high computational cost and require care-ful selection of optimization parameters per capture subject
[9, 14, 29, 62]. These remain major obstacles for large-scale data captures. A few learning-based methods directly regress parameters of a 3D morphable model (3DMM) [80] or iteratively refine 3DMM meshes from multi-view images
[6]. As shown by Li et al. [50], this 3DMM dependency constrains the quality and expressiveness of these methods.
The recent ToFu [50] method goes beyond these 3DMM-based approaches with a volumetric feature sampling framework to infer face meshes from calibrated multi-view images. While demonstrating high-quality predic-tions, ToFu has several limitations. (a) The training is fully-supervised with paired data of multi-view images and high-quality registered meshes; creating such data requires ex-tensive manual input. (b) Only the face region is predicted; ears, neck, and the back of the head are manually completed in an additional fitting step. (c) Self-occlusions in scanner setups designed to capture the entire head result in mediocre predictions due to the na¨ıve feature aggregation strategy that ignores the surface visibility. (d) Only a small capture vol-ume is supported and increasing the size of the capture vol-ume to cover head movements reduces the accuracy.
TEMPEH adapts ToFu’s volumetric feature sampling framework but goes beyond it in several ways: (a) The train-ing requires no manually curated data as we jointly optimize
TEMPEH’s weights and register the raw scans. Obtaining the clean, registered meshes required by ToFu is a key prac-tical hurdle. TEMPEH learns from raw scans and is robust to their noise and missing data. This is done by directly minimizing the point-to-surface distance between scans and predicted meshes. (b) At run time the entire head is inferred from images alone and includes the ears, neck, and back of the head. (c) The feature aggregation accounts for surface visibility. (d) A spatial transformer module [42] localizes the head in the feature volume to only sample regions rele-vant for prediction, improving the accuracy.
In summary, TEMPEH is the first framework to accu-rately capture the entire head from multi-view images at near interactive rates. During training, TEMPEH jointly learns to predict 3D heads from multi-view images, and reg-isters unstructured scans. Once trained, it only requires cal-ibrated camera input and it generalizes to diverse extreme expressions and head poses for subjects unseen during train-ing (see Fig. 1). TEMPEH is trained and evaluated on a dy-namic 3D head dataset of 95 subjects, each performing 28 facial motions, totalling about 600K 3D head meshes. The registered dataset meshes, raw images, camera calibrations, trained model, and training code are publicly available. 2.