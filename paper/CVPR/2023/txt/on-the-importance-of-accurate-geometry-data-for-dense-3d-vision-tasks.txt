Abstract
Learning-based methods to solve dense 3D vision prob-lems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are in-tricate to measure with existing hardware. Training on in-accurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estima-tion and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various tech-nologies in everyday household environments. For evalu-ation, we introduce a carefully designed dataset1 compris-ing measurements from commodity sensors, namely D-ToF,
I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and tar-geted data fusion. 1.

Introduction
Our world is 3D. Distance measurements are essential for machines to understand and interact with our environ-ment spatially. Autonomous vehicles [23, 30, 50, 58] need this information to drive safely, robot vision requires dis-tance information to manipulate objects [15, 62, 72, 73], and
AR realism benefits from spatial understanding [6, 31].
A variety of sensor modalities and depth predic-1dataset available at https://github.com/Junggy/HAMMER-dataset
Figure 1. Other datasets for dense 3D vision tasks reconstruct the scene as a whole in one pass [8,12,56], resulting in low quality and accuracy (cf. red boxes). On the contrary, our dataset scans the background and every object in the scene separately a priori and annotates them as dense and high-quality 3D meshes. Together with precise camera extrinsics from robotic forward-kinematics, this enables a fully dense rendered depth as accurate pixel-wise ground truth with multimodal sensor data, such as RGB with po-larization, D-ToF, I-ToF and Active Stereo. Hence, it allows quan-tifying different downstream 3D vision tasks such as monocular depth estimation, novel view synthesis, or 6D object pose estima-tion. tion pipelines exist.
The computer vision community thereby benefits from a wide diversity of publicly available datasets [23, 51, 52, 57, 60, 61, 65], which allow for evalua-tion of depth estimation pipelines. Depending on the setup, different sensors are chosen to provide ground truth (GT) depth maps, all of which have their respective advantages and drawbacks determined by their individual principle of distance reasoning. Pipelines are usually trained on the data without questioning the nature of the depth sensor used for supervision and do not reflect areas of high or low confi-dence of the GT.
Popular passive sensor setups include multi-view stereo cameras where the known or calibrated spatial relationship
between them is used for depth reasoning [51]. Correspond-ing image parts or patches are photometrically or struc-turally associated, and geometry allows to triangulate points within an overlapping field of view. Such photometric cues are not reliable in low-textured areas and with little ambient light where active sensing can be beneficial [52,57]. Active stereo can be used to artificially create texture cues in low-textured areas and photon-pulses with a given sampling rate are used in Time-of-Flight (ToF) setups either directly (D-ToF) or indirectly (I-ToF) [26]. With the speed of light, one can measure the distance of objects from the return time of the light pulse, but unwanted multi-reflection artifacts also arise. Reflective and translucent materials are measured at incorrect far distances, and multiple light bounces distort measurements in corners and edges. While ToF signals can still be aggregated for dense depth maps, a similar setup is used with LiDAR sensors which sparsely measure the dis-tance using coordinated rays that bounce from objects in the surrounding. The latter provides ground truth, for instance, for the popular outdoor driving benchmark KITTI [23].
While LiDAR sensing can be costly, radar [21] provides an even sparser but more affordable alternative. Multiple modalities can also be fused to enhance distance estimates.
A common issue, however, is the inherent problem of warp-ing onto a common reference frame which requires the in-formation about depth itself [27, 37]. While multi-modal setups have been used to enhance further monocular depth estimation using self-supervision from stereo and temporal cues [25, 60], its performance analysis is mainly limited to average errors and restricted by the individual sensor used.
An unconstrained analysis of depth in terms of RMSE com-pared against a GT sensor only shows part of the picture as different sensing modalities may suffer from drawbacks.
Where are the drawbacks of current depth-sensing modalities - and how does this impact pipelines trained with this (potentially partly erroneous) data? Can self- or semi-supervision overcome some of the limitations posed currently? To objectively investigate these questions, we provide multi modal sensor data as well as highly accurate annotated depth so that one can analyse the deterioration of popular monocular depth estimation and 3D reconstruc-tion methods (see Fig. 1) on areas of different photometric complexity and with varying structural and material prop-erties while changing the sensor modality used for training.
To quantify the impact of sensor characteristics, we build a unique camera rig comprising a set of the most popular in-door depth sensors and acquire synchronised captures with highly accurate ground truth data using 3D scanners and aligned renderings. To this end, our main contributions can be summarized as follows: 1. We question the measurement quality from commodity depth sensor modalities and analyse their impact as supervision signals for the dense 3D vision tasks of depth estimation and reconstruction. 2. We investigate performance on texture-varying mate-rial as well as photometrically challenging reflec-tive, translucent and transparent areas where learning methods systematically reproduce sensor errors. 3. To objectively assess and quantify different data sources, we contribute an indoor dataset compris-ing an unprecedented combination of multi-modal sensors, namely I-ToF, D-ToF, monocular RGB+P, monochrome stereo, and active light stereo together with highly accurate ground truth. 2.