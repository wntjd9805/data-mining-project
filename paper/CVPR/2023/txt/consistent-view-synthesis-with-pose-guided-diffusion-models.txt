Abstract against state-of-the-art transformer-based and GAN-based approaches. More qualitative results are available at https://poseguided-diffusion.github.io/.
Novel view synthesis from a single image has been a cornerstone problem for many Virtual Reality applications that provide immersive experiences. However, most exist-ing techniques can only synthesize novel views within a limited range of camera motion or fail to generate consis-tent and high-quality novel views under signiﬁcant cam-In this work, we propose a pose-guided era movement. diffusion model to generate a consistent long-term video of novel views from a single image. We design an atten-tion layer that uses epipolar lines as constraints to facili-tate the association between different viewpoints. Experi-mental results on synthetic and real-world datasets demon-strate the effectiveness of the proposed diffusion model 1.

Introduction
Offering immersive 3D experiences from daily photos has attracted considerable attention.
It is a cornerstone technique for a wide range of applications such as 3D photo [18, 49], 3D asset generation [35], and 3D scene nav-igation [4]. Notably, rapid progress has been made in ad-dressing the single-image view synthesis [40, 50, 61, 69] is-sue. Given an arbitrarily narrow ﬁeld-of-view image, these frameworks can produce high-quality images from novel viewpoints. However, these methods are limited to view-points that are within a small range of the camera motion.
The long-term single-image view synthesis task is re-cently proposed to address the limitation of small camera motion range. As demonstrated in Figure 1, the task at-tempts to generate a video from a single image and a se-quence of camera poses. Note that different from the single-image view synthesis problem, the viewpoints of the last few video frames produced under this setting may be far away from the original viewpoint. Take the results shown in Figure 1, for instance, the cameras are moving into dif-ferent rooms that were not observed in the input images.
Generating long-term view synthesis results from a sin-gle image is challenging for two main reasons. First, due to the large range of the camera motion, e.g., moving into a new room, a massive amount of new content needs to be hallucinated for the regions that are not observed in the in-put image. Second, the view synthesis results should be consistent across viewpoints, particularly in the regions ob-served in the input viewpoint or previously hallucinated in the other views.
Both explicit- and implicit-based solutions are proposed to handle these issues. Explicit-based approaches [17, 24, 25, 40] use a “warp and reﬁne” strategy. Speciﬁcally, the image is ﬁrst warped from the input to novel viewpoints according to some 3D priors, i.e., monocular depth estima-tion [37, 38]. Then a transformer or GAN-based generative model is designed to reﬁne the warped image. However, the success of the explicit-based schemes hinges on the ac-curacy of the monocular depth estimation. To address this limitation, Rombach et al. [42] designed a geometry-free transformer to implicitly learn the 3D correspondences be-tween the input and output viewpoints. Although reason-able new content is generated, the method fails to produce coherent results across viewpoints. The LoR [39] frame-work leverages the auto-regressive transformer to further improve the consistency. Nevertheless, generating consis-tent, high-quality long-term view synthesis results remains challenging.
In this paper, we propose a framework based on dif-fusion models for consistent and realistic long-term novel view synthesis. Diffusion models [14,52,54] have achieved impressive performance on many content creation applica-tions, such as image-to-image translation [44] and text-to-image generation [2, 36, 45]. However, these methods only work on 2D images and lack 3D controllability. To this end, we develop a pose-guided diffusion model with the epipolar attention layers. Speciﬁcally, in the UNet [43] network of the proposed diffusion model, we design the epipolar attention layer to associate the input view and out-put view features. According to the camera pose informa-tion, we estimate the epipolar line on the input view feature map for each pixel on the output view feature map. Since these epipolar lines indicate the candidate correspondences, we use the lines as the constraint to compute the attention weight between the input and output views.
We conduct extensive quantitative and qualitative stud-ies on real-world Realestate10K [76] and synthetic Mat-terport3D [7] datasets to evaluate the proposed approach.
With the epipolar attention layer, our pose-guided diffusion model is capable of synthesizing long-term novel views that 1) have realistic new content in unseen regions and 2) are consistent with the other viewpoints. We summarize the contributions as follows:
• We propose a pose-guided diffusion model for the long-term single-image view synthesis task.
• We consider the epipolar line as the constraint and de-sign an epipolar attention to associate pixels in the im-ages at input and output views for the UNet network in the diffusion model.
• We validate that the proposed method synthesizes re-alistic and consistent long-term view synthesis results on the Realestate10K and Matterport3D datasets. 2.