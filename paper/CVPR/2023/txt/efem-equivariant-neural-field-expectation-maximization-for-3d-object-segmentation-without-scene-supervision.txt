Abstract
We introduce Equivariant Neural Field Expectation
Maximization (EFEM), a simple, effective, and robust ge-ometric algorithm that can segment objects in 3D scenes without annotations or training on scenes. We achieve such unsupervised segmentation by exploiting single ob-ject shape priors. We make two novel steps in that direc-tion. First, we introduce equivariant shape representations to this problem to eliminate the complexity induced by the variation in object configuration. Second, we propose a novel EM algorithm that can iteratively refine segmenta-tion masks using the equivariant shape prior. We collect a novel real dataset Chairs and Mugs that contains vari-ous object configurations and novel scenes in order to verify the effectiveness and robustness of our method. Experimen-tal results demonstrate that our method achieves consis-tent and robust performance across different scenes where the (weakly) supervised methods may fail. Code and data available at https://www.cis.upenn.edu/Ëœleijh/ projects/efem 1.

Introduction
Learning how to decompose 3D scenes into object in-stances is a fundamental problem in visual perception sys-tems.
Past developments in 3D computer vision have made huge strides on this problem by training neural net-works on 3D scene datasets with segmentation masks [55, 63, 67]. However, these works heavily rely on large la-beled datasets [3, 15] that require laborious 3D annotation based on special expertise. Few recent papers alleviate this problem by reducing the need to either sparse point label-ing [24, 60] or bounding boxes [12].
In this work, we follow an object-centric approach in-spired by the Gestalt school of perception that captures an object as a whole shape [32, 47] invariant to its pose and scale [31]. A holistic approach builds up a prior for each object category, that then enables object recognition in dif-ferent complex scenes with varying configurations. Directly learning object-centric priors instead of analyzing each 3D
Figure 1. We present EFEM, an unsupervised 3D object segmen-tation method applicable to real-world scenes (results on the right) by only training on ShapeNet single object reconstruction. scene inspires a more efficient way of learning instance seg-mentation: both a mug on the table and a mug in the dish-washer are mugs, and one does not have to learn to seg-ment out a mug in all possible environmental contexts if we have a unified shape concept for mugs. Such object-centric recognition facilitates a robust scene analysis for au-tonomous systems in many interactive real-world environ-ments with a diversity of object configurations: Imagine a scenario where a robot is doing the dishes in the kitchen.
Dirty bowls are piled in the sink and the robot is clean-ing them and placing them into a cabinet. Objects of the same category appear in the scene repeatedly under differ-ent configurations (piles, neat lines in the cabinet). What is even more challenging is that even within this one single task (doing dishes) the scene configuration can drastically change when objects are moved. We show that such scenar-ios cannot be addressed by the state-of-the-art strongly or weakly supervised methods that struggle under such scene configuration variations.
In this paper, we introduce a method that can segment 3D object instances from 3D static scenes by learning pri-ors of single object shapes (ShapeNet [4]) without using any scene-level labels. Two main challenges arise when we re-move the scene-level annotation. First, objects in the scene can have a different position, rotation, and scale than the canonical poses where the single object shapes were trained.
Second, the shape encoder which is trained on object-level input cannot be directly applied to the scene observations unless the object masks are known. We address the first challenge by introducing equivariance to this problem. By
learning a shape prior that is equivariant to the similitude group SIM(3), the composition of a rotation, a translation, and a uniform scaling in 3D (Sec. 3.1), we address the com-plexity induced by the SIM(3) composition of objects. For the second challenge, we introduce a simple and effective iterative algorithm, Equivariant neural Field Expectation
Maximization (EFEM), that refines the object segmentation mask, by alternately iterating between mask updating and shape reconstruction (Sec. 3.2). The above two steps en-able us to directly exploit the learned single instance shape prior to perform segmentation in real-world scenes. We col-lected and annotated a novel real-world test set (240 scenes) (Sec. 4.4) that contains diverse object configurations and novel scenes to evaluate the generalizability and robustness to novel object instances and object configuration changes.
Experiments on both synthetic data (Sec. 4.3) and our novel real dataset (Sec. 4.4) give us an insight to the effectiveness of the method. Compared to weakly supervised methods, when the testing scene setup is similar to the training setup, our method has a small performance gap to the (weakly) supervised baselines. However, when the testing scenes are drawn from novel object configurations, our method consis-tently outperforms the (weakly) supervised baselines.
Our paper makes the following novel contributions to the 3D scene segmentation problem: (1) a simple and effective iterative EM algorithm that can segment objects from the scenes using only single object shape priors. (2) addressing the diversity of object composition in 3D scenes by combin-ing representations equivariant to rotation, translation, and scaling of the objects. (3) an unsupervised pipeline for 3D instance segmentation that works in real-world data and can generalize to novel setups. (4) a novel real-world test set
Chairs and Mugs that contains diverse object configura-tions and scenes. 2.