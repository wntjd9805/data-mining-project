Abstract
Diffusion-based generative models have achieved re-markable success in image generation. Their guidance for-mulation allows an external model to plug-and-play con-trol the generation process for various tasks without fine-tuning the diffusion model. However, the direct use of pub-licly available off-the-shelf models for guidance fails due to their poor performance on noisy inputs. For that, the existing practice is to fine-tune the guidance models with labeled data corrupted with noises. In this paper, we ar-gue that this practice has limitations in two aspects: (1) performing on inputs with extremely various noises is too hard for a single guidance model; (2) collecting labeled datasets hinders scaling up for various tasks. To tackle the limitations, we propose a novel strategy that lever-ages multiple experts where each expert is specialized in a particular noise range and guides the reverse process of the diffusion at its corresponding timesteps. However, as it is infeasible to manage multiple networks and uti-lize labeled data, we present a practical guidance frame-work termed Practical Plug-And-Play (PPAP), which lever-ages parameter-efficient fine-tuning and data-free knowl-edge transfer. We exhaustively conduct ImageNet class con-ditional generation experiments to show that our method can successfully guide diffusion with small trainable pa-rameters and no labeled data. Finally, we show that im-age classifiers, depth estimators, and semantic segmenta-tion models can guide publicly available GLIDE through our framework in a plug-and-play manner. Our code is available at https://github.com/riiid/PPAP. 1.

Introduction
Recently, diffusion-based generative models [49] have shown great success in various domains, including image generation [14, 44, 45], text-to-speech [21, 40], and text
*Co-first autor.
â€ Corresponding author.
Figure 1. Overview of our framework. Practical Plug-And-Play (PPAP) enables the diffusion model to be guided by lever-Images shown below are generated aging off-the-shelf models. by guiding the unconditional GLIDE [37] with DeepLabV3 [4],
ResNet50 [15], and MiDaS [43] in a plug-and-play manner. generation [32]. Specifically, for image generation, recent works have shown that diffusion models are capable of gen-erating high-quality images comparable to those generated by GANs [8,12], while not suffering from mode collapse or training instabilities [38].
In addition to these advantages, their formulation allows the external model guidance [8, 49, 53], which guides the generation process of diffusion models towards the desired condition. Since guided diffusion leverages external guid-ance models and does not require further fine-tuning of the diffusion model, it holds the potential for cheap and con-trollable generation in a plug-and-play manner. For exam-ple, previous approaches use an image classifier for class-conditional image generation [8, 53], a fashion understand-ing model for fashion image editing [28], and a vision-language model for text-based image generation [1, 37].
From these, if the publicly available off-the-shelf model can be used for guidance, one can easily apply one diffusion to various generation tasks.
For this purpose, an existing practice is to fine-tune the external off-the-shelf model on a noisy version of the train-ing dataset [8, 12], to adapt the model on the noisy latent images encountered during the diffusion process. However, we argue that such a practice has two challenges for plug-and-play generation: (1) A single guidance model is insuf-ficient to make predictions on inputs corrupted with varying degrees of noise, namely a too difficult task; and (2) It re-quires a labeled training dataset, which becomes a major hurdle whenever leveraging the off-the-shelf model.
In this paper, we first investigate the behaviors of classi-fiers by varying degrees of noise to understand the first chal-lenge. On one hand, guidance models trained on corrupted images with heavy noise categorize images based on coarse structures. As a result, such a model would guide the dif-fusion model to generate essential skeletal features. Mean-while, guidance models trained on cleaner images capture finer details in the images, guiding the diffusion model to work on finishing touches.
Based on these key observations, we propose a novel multi-experts strategy that uses multiple guidance models, each fine-tuned to specialize in a specific noise region.
Despite the effectiveness of the multi-experts strategy, it should manage multiple networks and utilize the labeled data whenever applying new off-the-shelf models for var-ious generation tasks.
For more practical plug-and-play guidance of the diffu-sion model with multi-experts strategy, we introduce the framework called Practical Plug-And-Play (PPAP). First, to prevent the size of guidance models from growing pro-hibitively large due to the multi-experts strategy, we lever-age a parameter-efficient fine-tuning scheme that can adapt off-the-shelf models to noisy images while preserving the number of parameters. Second, we transfer the knowledge of the off-the-shelf model on clean diffusion-generated data to the expert guidance models, thereby circumventing the need for collecting labeled datasets.
Our empirical results validate that our method signifi-cantly improves performance on conditional image genera-tion with off-the-shelf models with only small trainable pa-rameters and no labeled data. We also showcase various applications with the publicly available diffusion model,
GLIDE [37], by leveraging off-the-shelf image classifiers, depth estimators, and semantic segmentation models in a plug-and-play manner. 2.