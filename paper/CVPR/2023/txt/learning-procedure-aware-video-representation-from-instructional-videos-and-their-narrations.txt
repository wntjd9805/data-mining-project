Abstract
The abundance of instructional videos and their narra-tions over the Internet offers an exciting avenue for un-In this work, we pro-derstanding procedural activities. pose to learn video representation that encodes both ac-tion steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narra-tions, without using human annotations. Our method jointly learns a video representation to encode individual step con-cepts, and a deep probabilistic model to capture both tem-poral dependencies and immense individual variations in the step ordering. We empirically demonstrate that learn-ing temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8%/+3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising re-sults in zero-shot inference for step classification and fore-casting, as well as in predicting diverse and plausible steps for incomplete procedures. Our code is available at https://github.com/facebookresearch/ProcedureVRL. 1.

Introduction
Many of our daily activities (e.g. cooking or crafting) are highly structured, comprising a set of action steps con-ducted in a certain ordering. Yet how these activities are performed varies among individuals. Consider the exam-ple of making scrambled eggs as shown in Fig. 1. While most people tend to whisk eggs in a bowl, melt butter in a pan, and cook eggs under medium heat, expert chefs have recommended to crack eggs into the pan, add butter, and stir them under high heat. Imagine a vision model that can account for the individual variations and reason about the temporal ordering of action steps in a video, so as to infer prior missing steps, recognize the current step, and forecast
*Work done while Yiwu Zhong was an intern at Meta.
â€ Co-corresponding authors.
Figure 1. Top: During training, our model learns from procedu-ral videos and step descriptions to understand individual steps and capture temporal ordering and variations among steps. Bottom:
Once trained, our model supports zero-shot step classification and forecasting, yielding multiple credible predictions. a future step. Such a model will be immensely useful for a wide range of applications including augmented reality, virtual personal assistant, and human-robot interaction.
Understanding complex procedural activities has been a long-standing challenge in the vision community [7, 18, 22, 39, 41, 46]. While many prior approaches learn from anno-tated videos following a fully supervised setting [13,27,69], this paradigm is difficult to scale to a plethora of activ-ities and their variants among individuals. A promising solution is offered by the exciting advances in vision-and-language pre-training, where models learn from visual data (images or videos) and their paired text data (captions or narrations) [30, 43, 54, 70] in order to recognize a variety of concepts. This idea has recently been explored to analyze instructional videos [33, 37], yet existing methods are lim-ited to recognize single action steps in procedural activities.
In this paper, we present a first step towards modeling temporal ordering of action steps in procedural activities by learning from instructional videos and their narrations. Our key innovation lies in the joint learning of a video repre-sentation aiming to encode individual step concepts, and a deep probabilistic model designed to capture temporal de-pendencies and variations among steps. The video represen-tation, instantiated as a Transformer network, is learned by
matching a video clip to its corresponding narration. The probabilistic model, built on a diffusion process, is tasked to predict the distribution of the video representation for a missing step, given steps in its vicinity. With the help of a pre-trained vision-and-language model [43], our model is trained using only videos and their narrations from auto-matic speech recognition (ASR), and thus does not require any manual annotations.
Once learned, our model celebrates two unique bene-fits thanks to our model design and training framework.
First, our model supports zero-shot inference given an input video, including the recognition of single steps and fore-casting of future steps, and can be further fine-tuned on downstream tasks. Second, our model allows sampling mul-tiple video representations when predicting a missing action step, with each presenting a possibly different hypothesis of the step ordering. Instead of predicting a single represen-tation with the highest probability, sampling from a proba-bilistic model provides access to additional high-probability solutions that might be beneficial to prediction tasks with high ambiguity or requiring user interactions.
We train our models on a large-scale instructional video dataset collected from YouTube (HowTo100M [38]), and evaluate them on two public benchmarks (COIN [55] and
EPIC-Kitchens-100 [10]) covering a wide range of pro-cedural videos and across the tasks of step classification and step forecasting. Through extensive experiments, we demonstrate that (1) our temporal model is highly effec-tive in forecasting future steps, outperforming state-of-the-art methods by a large margin of +7.4% in top-1 accu-racy on COIN; (2) modeling temporal ordering reinforces video representation learning, leading to improved clas-sification results (+2.8%/+3.3% for step classification on
COIN/EPIC-Kitchens) when probing the learned represen-tations; (3) our training framework offers strong results for zero-shot step classification and forecasting; and (4) sam-pling from our probabilistic model yields diverse and plau-sible predictions of future steps.
Contributions. Our work presents the first model that leverages video-and-language pre-training to capture the temporal ordering of action steps in procedural activities.
Our key technical innovation lies in the design of a deep probabilistic model using a diffusion process, in tandem with video-and-language representation learning. The re-sult is a model and a training framework that establish new state-of-the-art results on both step classification and fore-casting tasks across the major benchmarks. Besides, our model is capable of generating diverse step predictions and supports zero-shot inference. 2.