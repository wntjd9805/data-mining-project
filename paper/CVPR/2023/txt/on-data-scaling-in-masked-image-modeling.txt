Abstract
Scaling properties have been one of the central issues in self-supervised pre-training, especially the data scalabil-ity, which has successfully motivated the large-scale self-supervised pre-trained language models and endowed them with significant modeling capabilities. However, scaling properties seem to be unintentionally neglected in the re-cent trending studies on masked image modeling (MIM), and some arguments even suggest that MIM cannot ben-efit from large-scale data. In this work, we try to break down these preconceptions and systematically study the scal-ing behaviors of MIM through extensive experiments, with data ranging from 10% of ImageNet-1K to full ImageNet-22K, model parameters ranging from 49-million to one-billion, and training length ranging from 125K to 500K iterations. And our main findings can be summarized in two folds: 1) masked image modeling remains demand-ing large-scale data in order to scale up computes and model parameters; 2) masked image modeling cannot bene-fit from more data under a non-overfitting scenario, which diverges from the previous observations in self-supervised pre-trained language models or supervised pre-trained vi-sion models. In addition, we reveal several intriguing prop-erties in MIM, such as high sample efficiency in large MIM models and strong correlation between pre-training vali-dation loss and transfer performance. We hope that our findings could deepen the understanding of masked im-age modeling and facilitate future developments on large-scale vision models. Code and models will be available at https://github.com/microsoft/SimMIM . 1.

Introduction
Masked Image Modeling (MIM) [3, 18, 44], which has recently emerged in the field of self-supervised visual pre-training, has attracted widespread interest and extensive ap-plications throughout the community for unleashing the su-perior modeling capacity of attention-based Transformer
The work is done when Zhenda Xie, Yutong Lin, and Yixuan Wei are interns at Microsoft Research Asia. † Project co-leaders. architectures [13, 26] and demonstrating excellent sample efficiency and impressive transfer performance on a vari-ety of vision tasks. However, most recent practices are focused on the design of MIM methods, while the study of scaling properties of MIM is unintentionally neglected, especially the data scaling property, which successfully mo-tivated the large-scale self-supervised pre-trained language models and endowed them with significant modeling capa-bilities. Although previous works [8, 10, 17, 37, 45] have explored several conclusions about the scaling properties of vision models, most of their findings were obtained under a supervised pre-training scheme or under a contrastive learn-ing framework, so the extent to which these findings could be transferred to MIM still needs to be investigated.
Meanwhile, with the emergence of Transformers [41] and masked language modeling (MLM) [12, 31], the systematic studies of scaling laws have already been explored in natural language processing field [21, 23, 35], which provided ample guidance for large models in recent years. The core finding drawn from scaling laws [21] for neural language models is that the performance has a power-law relationship with each of the three scale factors – model parameters N, size of dataset D, and amount of compute C respectively – when not bottlenecked by the other two. This conclusion implies that better performance can be obtained by scaling up these three factors to the extent that the scaling laws are in effect, which led to the subsequent developments of large scale language models [16, 28, 29, 32, 33] that exhibit excellent modeling ca-pabilities [4] on most language tasks. Therefore, it is natural to ask whether MIM possesses the same scaling signatures for vision models as the MLM method for language models, so that the scaling up of vision models can catch up with language models.
Though masked image modeling and masked language modeling both belong to masked signal prediction, their prop-erty differences are also non-negligible due to the different nature of vision and language. That is, the images are highly redundant raw signals and words/sentences are semantically rich tokens, which may result in different abilities of data utilization, and it is thus debatable whether the observations from language models could be reproduced in vision models.
Figure 1. The curves of validation loss of pre-training models w.r.t. the relative compute, dataset size and model size. MIM performance improves smoothly as we increase the relative compute and model size, but not improve when the dataset size is sufficient to prevent model from overfitting. We set the relative compute of SwinV2-S for 125K iterations as the value of 1. Best viewed in color.
Moreover, recent studies [14, 38] have shown that using a small amount of training data in masked image modeling can achieve comparable performance to using large datasets, which also motivates our explorations as it disagrees with previous findings and intuitions.
In this paper, we systematically investigate the scaling properties, especially the data scaling capability of masked image modeling in terms of different dataset sizes ranging from 10% of ImageNet (∼ 0.1 million) to full ImageNet-22K (∼ 14 million), model sizes ranging from 49-million Swin-V2 Small to 1-billion Swin-V2 giant and training lengths ranging from 125K to 500K iterations. We use Swin Trans-former V2 [25] as the vision encoder for its proven train-ability of large models and applicability to a wide range of vision tasks, and adopt SimMIM [44] for masked image modeling pre-training because it has no restrictions on en-coder architectures. We also conduct experiments with other
MIM frameworks like MAE [18] and other vision encoder like the widely used ViT [13] to verify the generalizability of our findings. With these experimental setups, our main findings could be summarized into two folds: i) Masked image modeling remains demanding large-scale data in order to scale up computes and model parame-ters. We empirically find that MIM performance has a power-law relationship with relative compute and model size when not bottlenecked by the dataset size (Figure 1). Besides, we observe that smaller datasets lead to severe overfitting phe-nomenon for training large models (Figure 2), and the size of the dataset to prevent model from overfitting increases clearly as the model increases (Figure 5-Left). Therefore, from the perspective of scaling up model, MIM still demands large-scale data. Furthermore, if we train large-scale models of different lengths, we find that relatively small datasets are adequate at shorter training lengths, but still suffer from overfitting at longer training lengths(Figure 5-Right), which further demonstrate the data scalability of MIM for scaling up compute. 2) Masked image modeling cannot benefit from more data under a non-overfitting scenario, which diverges from the previous observations in self-supervised pre-traind language models or supervised pre-trained vision models. In MIM, we find that increasing the number of unique samples for a non-overfitting model does not provide additional bene-fits to performance (Figure 1). This behavior differs from previous observations in supervised vision transformers and self-supervised language models, where the model perfor-mance increases as the number of unique samples increases.
In addition, we also demonstrate some intriguing proper-ties about masked image modeling, such as larger models possessing higher sample efficiency, i.e., fewer optimization steps are required for larger models to achieve same perfor-mance (Section 4.2); and the consistency between transfer and test performance, i.e., test performance could be used to indicate the results on downstream tasks (Section 4.3). These observations also correspond to those in previous practices.
These findings on the one hand confirm the effect of MIM on scaling up model size and compute, and raise new con-cerns and challenges on the data scalability of MIM on the other. We hope that our findings could deepen the under-standing of masked image modeling and facilitate future developments on large-scale vision models. 2.