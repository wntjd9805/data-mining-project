Abstract
A recent study has shown a phenomenon called neural collapse in that the within-class means of features and the classifier weight vectors converge to the vertices of a sim-plex equiangular tight frame at the terminal phase of train-ing for classification.
In this paper, we explore the cor-responding structures of the last-layer feature centers and classifiers in semantic segmentation. Based on our empir-ical and theoretical analysis, we point out that semantic segmentation naturally brings contextual correlation and imbalanced distribution among classes, which breaks the equiangular and maximally separated structure of neural collapse for both feature centers and classifiers. However, such a symmetric structure is beneficial to discrimination for the minor classes. To preserve these advantages, we in-troduce a regularizer on feature centers to encourage the network to learn features closer to the appealing struc-ture in imbalanced semantic segmentation. Experimental results show that our method can bring significant improve-ments on both 2D and 3D semantic segmentation bench-marks. Moreover, our method ranks 1st and sets a new record (+6.8% mIoU) on the ScanNet200 test leaderboard. 1.

Introduction
The solution structures of the last-layer representation and classifier provide a geometric perspective to delve into the learning behaviors in a deep neural network. The neu-ral collapse phenomenon discovered by Papyan et al. [49] reveals that as a classification model is trained towards con-vergence on a balanced dataset, the last-layer feature centers of all classes will be located on a hyper-sphere with maxi-mal equiangular separation, as known as a simplex equian-gular tight frame (ETF), which means that any two centers have an equal cosine similarity, as shown in Fig. 1a. The final classifiers will be formed as the same structure and aligned with the feature centers. The following studies try to theoretically explain this elegant phenomenon, showing
* Equal contribution. Part of the work was done in MEGVII. (a) (b)
Figure 1.
Illustration of equiangular separation (a) and non-equiangular separation (b) in a 3D space. Neural collapse reveals the structure in (a), where features are collapsed into their within-class centers with maximal equiangular separation as a simplex
ETF, and classifiers are aligned with the same structure. We ob-serve that in semantic segmentation the feature centers and clas-sifiers do not satisfy such a structure, as illustrated in (b) for an example. As some minor class features and classifier vectors lie in a close position, the discriminate ability of the network degrades. that neural collapse is the global optimality under the cross-entropy (CE) and mean squared error (MSE) loss functions in an approximated model [21, 22, 27, 32, 44, 47, 51, 64, 66, 80]. However, all the current studies on neural collapse fo-cus on the training in image recognition, which performs classification for each image. Semantic segmentation as an important pixel-wise classification problem receives no at-tention from the neural collapse perspective yet.
In this paper, we explore the solution structures of fea-ture centers and classifiers in semantic segmentation. Sur-prisingly, it is observed that the symmetric equiangular sep-aration as instructed by the neural collapse phenomenon in image recognition does not hold in semantic segmentation for both feature centers and classifiers. An example of non-equiangular separation is illustrated in Fig. 1b. We point out two reasons that may explain the difference.
First, classification benchmark datasets usually have low correlation among classes. In contrast, different classes in the semantic segmentation task are contextually related. In this case, the classifier needs to be adaptable to class cor-relation, so does not necessarily equally separate the label space. We conduct a simple experiment to verify it:
Classifier
Learned
Fixed
ScanNet200 27.8↓ 26.5↓
ADE20K 44.5↓ 43.6↓
It is shown that a semantic segmentation model with the classifier fixed as a simplex ETF performs much worse than a learnable classifier. Although using a fixed classifier of the simplex ETF structure has been proven to be effective for image recognition [22, 70, 80], we hold that in semantic segmentation the classifier needs to be learnable and does not have to be equiangular.
Second, the neural collapse phenomenon observed in im-age recognition highly relies on a balanced class distribu-tion of training samples. It is indicated that neural collapse will be broken when data imbalance emerges, which ex-plains the deteriorated performance of training on imbal-anced data [21]. We notice that semantic segmentation nat-urally suffers from data imbalance because some semantic classes are prone to cover a large area with significantly more points/pixels. Under the point/pixel-wise classifica-tion loss, the gradients will be also extremely imbalanced with respect to the backbone parameters, which breaks the equiangular separation structure for feature centers. In this case, the network makes the feature and classifier of minor classes lie in a close position and does not have the ability to discriminate the minor classes. However, the simplex ETF structure in neural collapse renders feature centers equian-gular separation and the maximal discriminative ability, which is able to effectively improve the performance of mi-nor classes in imbalanced recognition [36, 70, 79].
Inspired by our observations and analyses, we propose to induce the simplex ETF structure for feature centers, but keep a learnable classifier to enable adaptive class correla-tion for semantic segmentation. To this end, we propose an accompanied center regularization branch that extracts the feature centers of each semantic class. We regularize them by another classifier layer that is fixed as a simplex
ETF. The fixed classifier forces feature centers to be aligned with the appealing structure, which enjoys the equiangu-lar separation and the maximal discriminative ability. It in turn helps the feature learning in the original branch to im-prove the performance of minor classes for better semantic segmentation quality. We also provide theoretical results for a rigorous explanation. Our method can be easily inte-grated into any segmentation architecture and experimental results also show that our simple method consistently brings improvements on multiple image and point cloud semantic segmentation benchmarks.
Our overall contributions can be listed as follows:
• We are the first to explore neural collapse in seman-tic segmentation. We show that semantic segmentation naturally brings contextual correlation and imbalanced distribution among classes, which breaks the symmet-ric structure of neural collapse for both feature centers and classifiers.
• We propose a center collapse regularizer to encour-age the network to learn class-equiangular and class-maximally separated structured features for imbal-anced semantic segmentation.
• Our method is able to bring significant improvements on both point cloud and image semantic segmentation.
Moreover, our method ranks 1st and sets a new record (+6.8 mIoU) on the ScanNet200 test leaderboard. 2.