Abstract 1.

Introduction
Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in
AR/VR, video games, and robotics. Towards this goal, we address the task of generating a virtual human – hands and full body – grasping everyday objects. Existing methods ap-proach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object po-sitions and orientations or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasp-ing data. Our key insight is to leverage the existence of both full-body pose and hand-grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are supe-rior to baselines both quantitatively and qualitatively. See our webpage for more details: flex.cs.columbia.edu.
Generating realistic virtual humans is an exciting step towards building better animation tools, video games, im-mersive VR technology and more realistic simulators with human presence. Towards this goal, the research commu-nity has invested a lot of effort in collecting large-scale 3D datasets of humans [1–18]. However, the reliance on data collection will be a major bottleneck when scaling to broader scenarios, for two main reasons. First, data col-lection using optical marker-based motion capture (MoCap) systems is quite tedious to work with. This becomes even more complicated when objects [19] or scenes [20] are in-volved, requiring expertise in specialized hardware systems
[21–23], as well as commercial software [24–27]. Even with the best combination of state-of-the-art solutions, this process often requires multiple skilled technicians to ensure clean data [19].
Second, it is practically impossible to capture all possible ways of interacting with the ever-changing physical world.
The number of scenarios grows exponentially with every considered variable (such as human pose, object class, task,
or scene characteristics). For this reason, models trained on task-specific datasets suffer from the limitations of the data.
For example, methods that are supervised on the GRAB dataset [19] for full-body grasping [28, 29] fail to grasp ob-jects when the object position and/or orientation is changed, and generate poses with virtually no diversity. This is under-standable since the GRAB dataset mostly consists of stand-ing humans grasping objects at a fixed height, interacting with them in a relatively small range of physical motions.
However in realistic scenarios, we expect to see objects in all sorts of configurations - lying on the floor, on the top shelf of a cupboard, inside a kitchen sink, etc.
To build human models that work in realistic scene con-figurations, we need to fundamentally re-think how to solve 3D tasks without needing any additional data, effectively utilizing existing data. In this paper, we address the task of generating full-body grasps for everyday objects in realistic household environments, by leveraging the success of hand grasping models [19, 30–32] and recent advances in human body pose modeling [1, 33].
Our key observation is that we can compose different 3D generative models via geometrical and anatomical con-straints. Having a strong prior over full-body human poses (knowing what poses are feasible and natural), when com-bined with strong grasping priors, allows us to express full-body grasping poses. This combination leads to full-body poses which satisfy both priors resulting in natural poses that are suited for grasping objects, as well as hand grasps that human poses can easily match.
Our contributions are as follows. First, we propose
FLEX, a framework to generate full-body grasps without full-body grasping data. Given a pre-trained hand-only grasping model as well as a pre-trained body pose prior, we search in the latent spaces of these models to generate a human mesh whose hand matches that of the hand grasp, while simultaneously handling the constraints imposed by the scene, such as avoiding obstacles. To achieve this, we introduce a novel obstacle-avoidance loss that treats the hu-man as a connected graph which breaks when intersected by an obstacle. In addition, we show both quantitatively and qualitatively that FLEX allows us to generate a wide range of natural grasping poses for a variety of scenarios, greatly improving on previous approaches. Finally, we introduce the ReplicaGrasp dataset, built by spawning 3D objects in-side ReplicaCAD [34] scenes using Habitat [35]. 2.