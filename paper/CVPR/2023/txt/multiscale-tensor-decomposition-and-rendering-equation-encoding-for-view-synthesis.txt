Abstract
Rendering novel views from captured multi-view images has made considerable progress since the emergence of the neural radiance ﬁeld. This paper aims to further ad-vance the quality of view synthesis by proposing a novel approach dubbed the neural radiance feature ﬁeld (NRFF).
We ﬁrst propose a multiscale tensor decomposition scheme to organize learnable features so as to represent scenes from coarse to ﬁne scales. We demonstrate many beneﬁts of the proposed multiscale representation, including more accurate scene shape and appearance reconstruction, and faster convergence compared with the single-scale repre-sentation.
Instead of encoding view directions to model view-dependent effects, we further propose to encode the rendering equation in the feature space by employing the anisotropic spherical Gaussian mixture predicted from the proposed multiscale representation. The proposed NRFF improves state-of-the-art rendering results by over 1 dB in
PSNR on both the NeRF and NSVF synthetic datasets. A signiﬁcant improvement has also been observed on the real-world Tanks & Temples dataset. Code can be found at https://github.com/imkanghan/nrff. 1.

Introduction
View synthesis aims to synthesize unrecorded views from multiple captured views using computer vision tech-niques. A great deal of effort has been made to solve this problem in the past few decades [29]. The recently proposed neural radiance ﬁeld (NeRF) [18] made a break-through in this area by modeling a scene via a multi-layer perceptron (MLP). The NeRF achieves an impressive photo-realistic view synthesis quality with 6 degrees of free-dom for the ﬁrst time. The NeRF also represents a scene in a very compact form. That is, only a small number of pa-rameters in the MLP, whose size is even smaller than the captured images. However, this advantage in model size
*Corresponding author. comes at the expense of extensive computations. Numerous evaluations of the MLP are required to render a single pixel, incurring a challenge for both training and testing.
Representing a scene via learnable features is shown to be an effective alternative approach for photo-realistic view synthesis [6,7,19,25]. Several data structures are employed to efﬁciently organize learnable features to achieve compact representations. Multiresolution hash encoding (MHE) [19] and tensor decomposition in TensoRF [6] are two typical works in this direction. MHE organizes learnable features in multiresolution hash tables. As each hash table corre-sponds to a distinct grid resolution, a point is thus indexed into different positions of the hash tables to mitigate the negative effects of hash collisions. However, this structure breaks the local coherence in nature scenes, even though the spatial hash function in MHE preserves the coherence to some extent. By comparison, TensoRF decomposes a 3D tensor into 2D plane and 1D line tensors, where the local co-herence is largely preserved. However, TensoRF’s decom-position is performed only in a single scale, whereas mul-tiscale methods are much more desirable for wide-ranging computer vision tasks [1, 14, 16, 26, 27]. We thus propose a multiscale tensor decomposition (MTD) method to rep-resent scenes from coarse to ﬁne scales. We show that the proposed MTD method is able to reconstruct more accurate scene shapes and appearances, and also converges faster than the single-scale TensoRF. As a result, the proposed
MTD method achieves better view synthesis quality than
TensoRF, even with fewer learnable features.
View direction encoding is the key to the success of neu-ral rendering in modeling complex view-dependent effects.
Frequency (or position encoding) [18] and spherical har-monics [30] are the two mostly used view direction en-coding methods. The encoded feature vector of a view di-rection is then fed to an MLP to predict a view-dependent color. This approach models the 5D light ﬁeld function (3D spatial position with 2D view direction) [13]. In computer graphics, the light ﬁeld is usually modeled by the rendering equation [10], where the outgoing radiance is the interaction result of the incoming light at a point with a speciﬁc mate-rial. An accurate solution to the rendering equation involves
Monte Carlo sampling and integration, which is compu-tationally expensive, especially for the scenario of inverse rendering [9]. In this paper, we propose to encode the ren-dering equation in the feature space in lieu of the color space using the predicted anisotropic spherical Gaussian mixture.
In this way, the following MLP is aware of the rendering equation so as to better model complex view-dependent ef-fects. As we use both neural and learnable feature represen-tations as well as the rendering equation encoding (REE) in the feature space, we dub the proposed method the neural radiance feature ﬁeld (NRFF). In summary, we make the following contributions:
• We propose a novel multiscale tensor decomposition scheme to represent scenes from coarse to ﬁne scales, enabling better rendering quality and faster conver-gence with fewer learnable features;
• In lieu of direct encoding of view directions, we pro-pose to encode the rendering equation in the feature space to facilitate the modeling of view-dependent ef-fects. 2.