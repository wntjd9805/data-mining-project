Abstract
Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as vir-tual reality and robotics simulation. Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. We leverage Latent
Diffusion Models that have been successfully utilized for ef-ficient high-quality 2D content creation. We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of la-tent representations. A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline.
We achieve a substantial improvement over existing state-of-the-art scene generation models. Additionally, we show how NeuralField-LDM can be used for a variety of 3D con-tent creation applications, including conditional scene gen-eration, scene inpainting and scene style manipulation. 1.

Introduction
There has been increasing interest in modelling 3D real-world scenes for use in virtual reality, game design, digi-*Equal contribution.
â€ Work done during an internship at NVIDIA. tal twin creation and more. However, designing 3D worlds by hand is a challenging and time-consuming process, re-quiring 3D modeling expertise and artistic talent. Recently, we have seen success in automating 3D content creation via 3D generative models that output individual object as-sets [15, 46, 73]. Although a great step forward, automating the generation of real-world scenes remains an important open problem and would unlock many applications ranging from scalably generating a diverse array of environments for training AI agents (e.g. autonomous vehicles) to the de-sign of realistic open-world video games. In this work, we take a step towards this goal with NeuralField-LDM (NF-LDM), a generative model capable of synthesizing complex real-world 3D scenes. NF-LDM is trained on a collection of posed camera images and depth measurements which are easier to obtain than explicit ground-truth 3D data, offering a scalable way to synthesize 3D scenes.
Recent approaches [2, 6, 8] tackle the same problem of generating 3D scenes, albeit on less complex data. In [6, 8], a latent distribution is mapped to a set of scenes using ad-versarial training, and in GAUDI [2], a denoising diffusion model is fit to a set of scene latents learned using an auto-decoder. These models all have an inherent weakness of attempting to capture the entire scene into a single vector that conditions a neural radiance field. In practice, we find that this limits the ability to fit complex scene distributions.
Recently, diffusion models have emerged as a very pow-erful class of generative models, capable of generating high-quality images, point clouds and videos [18, 24, 39, 48, 53, 73, 78]. Yet, due to the nature of our task, where image data must be mapped to a shared 3D scene without an explicit ground truth 3D representation, straightforward approaches fitting a diffusion model directly to data are infeasible.
In NeuralField-LDM, we learn to model scenes using a three-stage pipeline. First, we learn an auto-encoder that en-codes scenes into a neural field, represented as density and feature voxel grids. Inspired by the success of latent diffu-sion models for images [53], we learn to model the distribu-tion of our scene voxels in latent space to focus the genera-tive capacity on core parts of the scene and not the extrane-ous details captured by our voxel auto-encoders. Specif-ically, a latent-autoencoder decomposes the scene voxels into a 3D coarse, 2D fine and 1D global latent. Hierar-chichal diffusion models are then trained on the tri-latent representation to generate novel 3D scenes. We show how
NF-LDM enables applications such as scene editing, birds-eye view conditional generation and style adaptation. Fi-nally, we demonstrate how score distillation [46] can be used to optimize the quality of generated neural fields, al-lowing us to leverage the representations learned from state-of-the-art image diffusion models that have been exposed to orders of magnitude more data.
Our contributions are: 1) We introduce NF-LDM, a hi-erarchical diffusion model capable of generating complex open-world 3D scenes and achieving state of the art scene generation results on four challenging datasets. 2) We ex-tend NF-LDM to semantic birds-eye view conditional scene generation, style modification and 3D scene editing. 2.