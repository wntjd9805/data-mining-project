Abstract
Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dy-namic scene. Existing methods, however, assume that ac-curate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are un-reliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this robustness issue by jointly estimating the static and dynamic radiance fields along with the camera param-eters (poses and focal length). We demonstrate the robust-ness of our approach via extensive quantitative and qualita-tive experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods. 1.

Introduction
Videos capture and preserve memorable moments of our lives. However, when watching regular videos, viewers ob-*This work was done while Yu-Lun and Andreas were interns at Meta. serve the scene from fixed viewpoints and cannot interac-tively navigate the scene afterward. Dynamic view synthe-sis techniques aim to create photorealistic novel views of dynamic scenes from arbitrary camera angles and points of view. These systems are essential for innovative ap-plications such as video stabilization [33, 42], virtual real-ity [7, 15], and view interpolation [13, 85], which enable free-viewpoint videos and let users interact with the video sequence. It facilitates downstream applications like virtual reality, virtual 3D teleportation, and 3D replays of live pro-fessional sports events.
Dynamic view synthesis systems typically rely on ex-pensive and laborious setups, such as fixed multi-camera capture rigs [7, 10, 15, 50, 85], which require simultaneous capture from multiple cameras. However, recent advance-ments have enabled the generation of dynamic novel views from a single stereo or RGB camera, previously limited to human performance capture [16, 28] or small animals [65].
While some methods can handle unstructured video in-put [1, 3], they typically require precise camera poses es-timated via SfM systems. Nonetheless, there have been
Table 1. Categorization of view synthesis methods.
Known camera poses
Unknown camera poses
NeRF [44], SVS [59], NeRF++ [82],
Mip-NeRF [4], Mip-NeRF 360 [5], DirectVoxGO [68],
Plenoxels [23], Instant-ngp [45], TensoRF [12]
NeRF - - [73], BARF [40],
SC-NeRF [31],
NeRF-SLAM [60]
NV [43], D-NeRF [56], NR-NeRF [71],
NSFF [39], DynamicNeRF [24], Nerfies [52],
HyperNeRF [53], TiNeuVox [20], T-NeRF [25]
Ours c i t a t
S e n e c s c i m a n y
D e n e c s many recent dynamic view synthesis methods for unstruc-tured videos [24, 25, 39, 52, 53, 56, 71, 76] and new methods based on deformable fields [20]. However, these techniques require precise camera poses typically estimated via SfM systems such as COLMAP [62] (bottom left of Table 1).
However, SfM systems are not robust to many issues, such as noisy images from low-light conditions, motion blur caused by users, or dynamic objects in the scene, such as people, cars, and animals. The robustness problem of the
SfM systems causes the existing dynamic view synthesis methods to be fragile and impractical for many challenging videos. Recently, several NeRF-based methods [31, 40, 60, 73] have proposed jointly optimizing the camera poses with the scene geometry. Nevertheless, these methods can only handle strictly static scenes (top right of Table 1).
We introduce RoDynRF, an algorithm for reconstructing dynamic radiance fields from casual videos. Unlike exist-ing approaches, we do not require accurate camera poses as input. Our method optimizes camera poses and two ra-diance fields, modeling static and dynamic elements. Our approach includes a coarse-to-fine strategy and epipolar ge-ometry to exclude moving pixels, deformation fields, time-dependent appearance models, and regularization losses for improved consistency. We evaluate the algorithm on multi-ple datasets, including Sintel [9], Dynamic View Synthe-sis [79], iPhone [25], and DAVIS [55], and show visual comparisons with existing methods.
We summarize our core contributions as follows:
• We present a space-time synthesis algorithm from a dynamic monocular video that does not require known camera poses and camera intrinsics as input.
• Our proposed careful architecture designs and auxil-iary losses improve the robustness of camera pose es-timation and dynamic radiance field reconstruction.
• Quantitative and qualitative evaluations demonstrate the robustness of our method over other state-of-the-art methods on several challenging datasets that typical
SfM systems fail to estimate camera poses. 2.