Abstract
Real-time efficient perception is critical for autonomous navigation and city scale sensing. Orthogonal to archi-tectural improvements, streaming perception approaches have exploited adaptive sampling improving real-time de-tection performance. In this work, we propose a learnable geometry-guided prior that incorporates rough geometry of the 3D scene (a ground plane and a plane above) to re-sample images for efficient object detection. This signifi-cantly improves small and far-away object detection per-formance while also being more efficient both in terms of latency and memory. For autonomous navigation, using the same detector and scale, our approach improves detection rate by +4.1 APS or +39% and in real-time performance by +5.3 sAPS or +63% for small objects over state-of-the-art (SOTA). For fixed traffic cameras, our approach detects small objects at image scales other methods cannot. At the same scale, our approach improves detection of small ob-jects by 195% (+12.5 APS) over naive-downsampling and 63% (+4.2 APS) over SOTA. 1.

Introduction
Visual perception is important for autonomous driving and decision-making for smarter and sustainable cities. Real-time efficient perception is critical to accelerate these ad-vances. For instance, a single traffic camera captures half a million frames every day or a commuter bus acting as a city sensor captures one million frames every day to mon-itor road conditions [10] or to inform public services [22].
There are thousands of traffic cameras [24] and nearly a mil-lion commuter buses [53] in the United States. It is infeasi-ble to transmit and process visual data on the cloud, leading to the rise of edge architectures [43]. However, edge de-vices are severely resource constrained and real-time infer-ence requires down-sampling images to fit both latency and memory constraints severely impacting accuracy.
On the other hand, humans take visual shortcuts [19] to
Figure 1. Geometric cues (black dashed lines) are implicitly present in scenes. Our Perspective based prior exploits this ge-ometry. Our method (a) takes an image and (b) warps them, and performs detection on warped images. Small objects which are (d) not detected when naively downsampled but (e) are detected when enlarged with our geometric prior. Our method (f) uses a geomet-ric model to construct a saliency prior to focus on relevant areas and (g) enables sensing on resource-constrained edge devices. recognize objects efficiently and employ high-level seman-tics [19, 52] rooted in scene geometry to focus on relevant parts. Consider the scene in Figure 1 (c), humans can rec-ognize the distant car despite its small appearance (Figure 1 (d)). We are able to contextualize the car in the 3D scene, namely (1) it’s on the road and (2) is of the right size we’d expect at that distance. Inspired by these observations, can we incorporate semantic priors about scene geometry in our neural networks to improve detection?
In this work, we develop an approach that enables object de-tectors to “zoom” into relevant image regions (Figure 1 (d) and (e)) guided by the geometry of the scene. Our approach considers that most objects of interests are present within two planar regions, either on the ground plane or within another plane above the ground, and their size in the im-age follow a geometric relationship. Instead of uniformly downsampling, we sample the image to enlarge far away regions more and detect those smaller objects.
While methods like quantization [17], pruning [18], dis-tillation [6] and runtime-optimization [16] improve model efficiency (and are complementary), approaches exploiting spatial and temporal sampling are key for enabling effi-cient real-time perception [21, 27]. Neural warping mecha-nisms [23, 36] have been employed for image classification and regression, and recently, detection for self-driving [50].
Prior work [50] observes that end-to-end trained saliency networks fail for object detection. They instead turn to heuristics such as dataset-wide priors and object locations from previous frames, which are suboptimal. We show that formulation of learnable geometric priors is critical for learning end-to-end trained saliency networks for detection.
We validate our approach in a variety of scenarios to show-case the generalizability of geometric priors for detection in self-driving on Argoverse-HD [27] and BDD100K [57] datasets, and for traffic-cameras on WALT [37] dataset.
• On Argoverse-HD, our learned geometric prior im-proves performance over naive downsampling by +6.6
AP and +2.7 AP over SOTA using the same detection architecture. Gains from our approach are achieved by detecting small far-away objects, improving by 9.6
APS (or 195%) over naive down-sampling and 4.2
APS (or 63%) over SOTA.
• On WALT, our method detects small objects at image scales where other methods perform poorly. Further, it significantly improves detection rates by 10.7 APS over naive down-sampling and 3 APS over SOTA.
• Our approach improves object tracking (+4.8%
MOTA) compared to baseline. It also improves track-ing quality, showing increase of +7.6% M T % and re-duction of -6.7% M L%.
• Our approach can be deployed in resource constrained edge devices like Jetson AGX to detect 42% more rare instances while being 2.2X faster to enable real-time sensing from buses. 2.