Abstract
Pre-trained vision-language models have inspired much research on few-shot learning. However, with only a few training images, there exist two crucial problems: (1) the visual feature distributions are easily distracted information in images, and (2) the by class-irrelevant alignment between the visual and language feature distri-butions is difficult. To deal with the distraction problem, we propose a Selective Attack module, which consists of trainable adapters that generate spatial attention maps of images to guide the attacks on class-irrelevant image areas. By messing up these areas, the critical features are captured and the visual distributions of image features are calibrated. To better align the visual and language feature distributions that describe the same object class, we pro-pose a cross-modal distribution alignment module, in which we introduce a vision-language prototype for each class to align the distributions, and adopt the Earth Mover’s
Distance (EMD) to optimize the prototypes. For efficient computation,
In addition, we propose an augmentation strategy to increase the diversity of the images and the text prompts, which can reduce overfitting to the few-shot training images.
Extensive experiments on 11 datasets demonstrate that our method consistently outperforms prior arts in few-shot learning. The implementation code will be available at https://gitee.com/mindspore/models/tree/master/research/cv
/SADA. the upper bound of EMD is derived. 1.

Introduction
Thanks to the availability of large-scale datasets and well-designed training strategies, the performances of many computer vision tasks have been greatly improved. Re-cent progress in vision-language models (VLMs), such as
*Co-first author.
†Corresponding author.
CLIP [29] and ALIGN [17], provides a promising way towards utilizing human language to address downstream recognition tasks efficiently. As vision and language usually contain complementary information, joint learning of image and text representations has proven quite effective. Al-though CLIP has demonstrated impressive zero-shot learn-ing capability, it is still challenging to better adapt it to downstream tasks. Naively fine-tuning CLIP on down-stream datasets has limited effect, since it may destroy the prior learned from the massive data during pre-training.
Therefore, effective transfer methods are needed to boost the downstream performances of CLIP. In order to main-tain the capability of pre-trained VLMs and further boost downstream performances, different approaches have been proposed to fine-tune a small proportion of additional pa-rameters while keeping the pre-trained parameters frozen.
Among these approaches, prompt learning [42, 43] and vi-sual adapters [13, 41] are two common approaches. How-ever, the lack of training samples in few-shot settings increases the risk of overfitting the trained prompts or adapters. The class-irrelevant features (e.g., the cluttered image backgrounds) drive the image features far away from their true distributions of the same category. Besides, VLMs such as CLIP have such a problem that the distributions of the image and text features are not really aligned [30], and the problem becomes more challenging in few-shot settings.
Therefore, the visual distributions should be calibrated by reducing class-irrelevant image contents, and the distribu-tions of image and text features should be further aligned, so as to promote the model’s learning of class-relevant crit-ical features. The purpose of this paper is to develop an ef-fective VLM transfer strategy for few-shot learning to solve the above problems with Selective Attack (SA) and Cross-Modal Distribution Alignment (CMDA).
Images often contain class-irrelevant information, which is also embedded into the image representations. With only a few samples, the model can easily learn these cluttered representations, resulting in overfitting. This seriously hin-ders the learning of critical features that help the model rec-Figure 1. (a) The t-SNE [35] visualization of the image feature distribution before Selective Attack, where the features are obtained by the CLIP image encoder on the CIFAR10 dataset. The dots in different colors represent different classes of the image features. (b) After
Selective Attack, the intra-class distribution is significantly more compact. (c) The distribution histograms of image features and text features of the same class (‘bird’) on CIFAR10 before CMDA, where the horizontal axis denotes the value of each element of the feature vectors, and the vertical axis denotes the number of elements. (d) After CMDA, the difference between the two distributions is significantly reduced. ognize unseen samples. To solve this problem, we propose the SA module, which consists of two trainable adapters that generate a kernelized attention map to locate the class-irrelevant areas of the images. The attention is adopted to guide Gaussian perturbations to attack images before they are fed into the image encoder. By messing up these class-irrelevant image contents through SA, we facilitate the model’s learning of truly critical features that can be trans-ferred to recognize new samples within the same category.
As an example in Figs. 1 (a) and (b), after Selective Attack (SA), the distributions of the image features are calibrated, and the intra-class features become obviously more clus-tered.
Another challenge is that the distributions of the image and the text representations of the same class are not truly aligned in CLIP [30] as shown in Fig. 1(c). The unaligned distributions lead to inaccurate similarity calculations be-tween image features and text features during inference, re-sulting in incorrect predictions. The lack of samples in few-shot settings further makes the problem even more serious.
To address it, we propose a CMDA module, in which we construct a Vision-Language Prototype (VLP) for each class to promote the cross-modal distribution alignment. Specif-ically, the element values of VLP are initialized by aver-aging all the image representations from the corresponding class. During training, each VLP is optimized by reducing its distance to the language prototype (defined in Sec. 3.4) of the same class, thus promoting the cross-modal distri-bution alignment. The Earth Mover’s Distance (EMD) is a suitable metric for the alignment, which can not only reflect the similarity between two distributions but also represent the minimal transmission cost [40]. We derive a concise upper bound of the EMD distance, which can balance the performance and computational consumption. As shown in Figs. 1 (c) and (d), the effect of Cross-Modal Distribu-tion Alignment (CMDA) is obvious that the difference be-tween the image and text feature distributions is effectively reduced. In this way, the image features after CMDA can be better predicted by the text features.
Automatic prompt learning for pre-trained VLMs has been proposed to reduce the expensive cost of hand-crafted prompt engineering [43]. However, the learned prompts may suffer from more overfitting than manual prompts [42].
Therefore, instead of learning one soft prompt, we learn a distribution over a collection of prompts, as in ProDA [23].
Moreover, we introduce an augmentation strategy to in-crease the diversity of the images and the prompts. Specifi-cally, we search for the four best augmentations from a col-lection of predefined ones. Using these operations, each image is augmented into four different forms. The collec-tion of prompts is also divided into four groups, with each group trained by images in the corresponding augmentation form. Through the strategy, we improve the diversity of the images and the prompts, and fully excavate the semantic in-formation in the prompts. The framework of our method is shown in Fig. 2. Our contributions are summarized as follows:
• We conduct Selective Attack on the class-irrelevant regions of images with the guidance of the attention generated by two trainable adapters to facilitate the model’s learning of class-related features, which cal-ibrates the visual distributions.
• We propose Cross-Modal Distribution Alignment op-timized by an EMD loss. The upper bound of EMD for Gaussian distribution is further derived for compu-tation efficiency.
Figure 2. Overview of our framework. We introduce a Selective Attack module to reduce the intra-class distances of image features during training. We also design a Cross-Modal Distribution Alignment (CMDA) module to align the distributions of image and text representations. During training, the trainable parameters are denoted in orange and the encoders of CLIP are frozen. J: the number of augmentations; Ⓢ: cosine similarity computation; ⊛: element-wise product.
• We present an augmentation strategy to reduce overfit-ting and increase the diversity of images and prompts.
• Our method outperforms prior arts in few-shot learning on 11 benchmarks. 2.