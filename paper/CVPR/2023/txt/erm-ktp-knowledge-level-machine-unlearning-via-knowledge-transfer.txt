Abstract
Machine unlearning can fortify the privacy and security of machine learning applications. Unfortunately, the ex-act unlearning approaches are inefficient, and the approxi-mate unlearning approaches are unsuitable for complicated
CNNs. Moreover, the approximate approaches have serious security flaws because even unlearning completely different data points can produce the same contribution estimation as unlearning the target data points. To address the above problems, we try to define machine unlearning from the knowledge perspective, and we propose a knowledge-level machine unlearning method, namely ERM-KTP. Specifi-cally, we propose an entanglement-reduced mask (ERM) structure to reduce the knowledge entanglement among classes during the training phase. When receiving the un-learning requests, we transfer the knowledge of the non-target data points from the original model to the unlearned model and meanwhile prohibit the knowledge of the tar-get data points via our proposed knowledge transfer and prohibition (KTP) method. Finally, we will get the un-learned model as the result and delete the original model to accomplish the unlearning process. Especially, our pro-posed ERM-KTP is an interpretable unlearning method be-cause the ERM structure and the crafted masks in KTP can explicitly explain the operation and the effect of un-learning data points. Extensive experiments demonstrate the effectiveness, efficiency, high fidelity, and scalability of the ERM-KTP unlearning method. Code is available at https://github.com/RUIYUN-ML/ERM-KTP 1.

Introduction
In recent years, many countries have raised concerns about protecting personal privacy. The privacy legislation, e.g., the well-known European Union’s GDPR [19], have been promulgated to oblige information service providers
*Both authors contributed equally
†Corresponding author to remove personal data when receiving a request from the data owner, i.e., the right-to-be-forgotten. Besides, the
GDPR stipulates that the service providers should remove the corresponding impact of the data requested by the data owner, in which the machine learning models are the most representative. Many kinds of research demonstrate that the machine learning models can memorize knowledge of the data points, e.g., the membership inference attack [7,14,15] can infer whether a data point is in the training set or not.
A naive approach is retraining the model after remov-ing the target data points from the training set, but the hu-man resources and materials consumed are costly. Thus, aiming to efficiently remove data as well as their gener-ated contribution to the model, a new ML privacy protec-tion research direction emerged, called machine unlearning.
A good deal of related work attempted to solve the data-removing challenge of inefficient retraining, and there are two representative research fields, including exact unlearn-ing [1] and approximate unlearning [3–5]. Unfortunately, these approaches usually require huge computational, stor-age overhead, and memory requirements for class-specific machine unlearning tasks on the complicated convolutional neural networks (CNNs). Furthermore, they may compro-mise the model’s performance and even cause disastrous forgetting. Most significantly, Thudi et al. [18] believed that such approximate unlearning approaches have serious secu-rity flaws because they usually define machine unlearning as the distribution difference between the unlearned model and the retrained model. According to this definition, even unlearning completely different data points can produce the same contribution estimation as unlearning the target data points.
Unlearning algorithms are difficult to implement on deep learning models because these models are often seen as a black box and lack interpretability, which makes data points’ contributions challenging to estimate. Though many related works [2, 10, 16, 21] attempted to improve the inter-pretability of deep learning models, they can not apply to machine unlearning directly. For example, Zhou et al. [22]
leveraged the global average pooling (GAP) in CNNs to generate a class activation mapping (CAM) to indicate the discriminative image regions used by the CNNs to identify the class. Liang et al. [12] proposed class-specific filters to transform the complex representations in convolutional layers into interpretable graphs.
To address the above problems, for the first time, we con-sider the need for subsequent unlearning tasks during the model training phase. We define machine unlearning in the knowledge perspective, i.e., a reliable machine unlearning approach should satisfy that the knowledge of the unlearned model should be identical to the retrained model. Further-more, we propose an interpretable unlearning method, i.e.,
ERM-KTP. The ERM structure can interpret the relation be-tween data points and channels in the activation maps. Such a relationship can describe which filters the data points con-tribute to the convolutional layer. Then, KTP utilizes the re-lation to transfer the knowledge from corresponding filters to the unlearned model. Furthermore, the crafted masks in
KTP interpret the operation of the knowledge transfer. With these interpretable methods, we can precisely unlearn data points and enhance the reliability of our proposed unlearn-ing approach. We summarize our contributions of this paper as follows:
• We give a novel definition of machine unlearning from the knowledge perspective, and we further propose an interpretable knowledge-level machine unlearning method, i.e., ERM-KTP. Especially, the interpretabil-ity of ERM-KTP enhances the reliability of the un-learning operation.
• We introduce an ERM structure that reduces the knowledge entanglement among classes to get a pre-trained model. When receiving the unlearning re-quests, we use the proposed KTP method to transfer the knowledge of the remaining set from the original model to the unlearned model and, meanwhile, pro-hibit the knowledge of the unlearning set. Finally, we will get the unlearned model as the result and delete the original model to complete the unlearning process.
• We conduct experiments on three different scales of image classification datasets and three complicated
CNNs. Extensive experimental results demonstrate the effectiveness, efficiency, high fidelity, and scalability of ERM-KTP. 2. Problem Formulation
In this paper, we primarily consider a class-specific ma-chine unlearning scenario. We assume a sample space
X ⊆ Rd and corresponding labels Y = {1, 2, · · · , C}, where d is the number of dimensions and C is the num-ber of classes. The training set can be represented as
D = {D1, D2, · · · , DC}. To unlearn the target class’s data points Du ⊆ D exactly, we need to apply a removal method, which should be equivalent to applying the training algo-rithm to the dataset without target data points.
Definition 1. We define a learning algorithm, A : D → W, as a function from a dataset to a model in hypothesis space
W. A removal method, R : A(D) × D × Du → W, is a function from an original model A(D), training dataset
D, an unlearning dataset Du to remove from the training dataset D to an unlearned model in W, and a remaining dataset Dr = D\Du. We define equivalence as having iden-tical knowledge for each model in W:
K(A(Dr)) = K(R(A(D), D, Du)), (1) in which K(·) is a knowledge measuring function.
If an exact unlearning method satisfy the above definition, it can be defined as a knowledge-level unlearning method. 3.