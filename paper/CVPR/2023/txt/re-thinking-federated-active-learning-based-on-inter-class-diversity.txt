Abstract
Although federated learning has made awe-inspiring ad-vances, most studies have assumed that the client’s data are fully labeled. However, in a real-world scenario, ev-ery client may have a significant amount of unlabeled in-stances. Among the various approaches to utilizing un-labeled data, a federated active learning framework has emerged as a promising solution. In the decentralized set-ting, there are two types of available query selector models, namely ‘global’ and ‘local-only’ models, but little litera-ture discusses their performance dominance and its causes.
In this work, we first demonstrate that the superiority of two selector models depends on the global and local inter-class diversity. Furthermore, we observe that the global and local-only models are the keys to resolving the imbalance of each side. Based on our findings, we propose LoGo, a FAL sampling strategy robust to varying local heterogeneity lev-els and global imbalance ratio, that integrates both models by two steps of active selection scheme. LoGo consistently outperforms six active learning strategies in the total num-ber of 38 experimental settings. The code is available at: https://github.com/raymin0223/LoGo. 1.

Introduction
Federated learning (FL) is a distributed framework that allows multiple parties to learn a unified deep learning model cooperatively with preserving the privacy of the local client [23,28,31]. Typically, FL has been actively studied in a standard supervised learning setting, where all the train-ing instances are labeled, but it is more realistic for each client to contain both labeled and unlabeled data due to the high labeling cost [29, 48]. Here, active learning (AL) can be a promising solution to improve the performance of a cooperated model with the pool of unlabeled data. In practice, federated active learning (FAL) framework has re-cently attempted to bridge two different philosophies in FL and AL [3,4]. As illustrated in Figure 1-(a) FAL framework alternates an FL procedure (red line) of training a predictive
*equal contribution
†corresponding authors model collaboratively through local updates and aggrega-tion phases, and an AL procedure (green line) of querying and annotating informative instances separately per client.
Although the overall framework just appears to be a straightforward fusion of two research fields, FL factors in-troduce two major challenges to the AL procedure. First, the class imbalance of the local dataset originates from heterogeneous distribution across local clients [15, 23, 28].
Hence, in the FAL framework, the active selection algo-rithm has to ensure inter-class diversity from both local and global perspectives. Second, there are two available types of query-selecting models, a global model, which is globally optimized through the FL pipeline, and a local-only model [9, 33], which can be separately trained only for each client. In the query selection phase, the global model can leverage the aggregated knowledge of all clients, while the local-only model is able to detect the most valuable in-stances for the local updates.
Prior FAL literature [3, 4], which simply adapt conven-tional AL strategies, had little discussion on these chal-lenges. As our first contribution, we found the significant performance gap between two types of query selector (see
Figure 1-(b)), and it is the first study to solve a conundrum of dominance trend by introducing two indicators of inter-class diversity1– local heterogeneity level (α) and global im-balance ratio (ρ). The first indicator α is the concentration parameter of Dirichlet distribution, commonly seen in the
FL literature [1, 15, 23], resulting in more locally imbal-anced class distribution at lower values. Besides, ρ indica-tor is the ratio of class imbalance for the aggregated global data of all clients [7, 21]. We discovered three meaningful insights on selector dominance: (Obs. 1) Interestingly, the superiority of the two selectors varies depending on the two indicators of inter-class diversity, α and ρ. (Obs. 2) When local heterogeneity is severe (α is low), a local-only model is preferred for weighing minority instances for each client, and (Obs. 3) when globally minor classes exist (ρ is high), the knowledge of the entire data distribution, inherent in a global model, is more essential. ▷ See Section 4 1We use the term inter-class diversity interchangeably with class bal-ance throughout the paper.
(a) Federated Active Learning framework. (b) Superiority change of query selector.
Figure 1. Motivation: (a) The red and green lines correspond to the conventional FL and AL framework. We focus on a sampling strategy for FAL (green box) with the considerations of hierarchy structure and two available query-selecting models. (b) For a fixed querying strategy (Entropy sampling), the performance gap occurs only by changing the query selector. The y-axis is the gap in the winning rate in total active rounds (refer to Section 4). Closer to 1 indicates that global models outperforms than local-only models, and -1 is the opposite.
In a real FAL scenario, the superiority of two query mod-els for a given dataset cannot be known in advance due to privacy preservation. Therefore, as our second contri-bution, we design a simple yet effective FAL querying ap-proach, LoGo, that simultaneously leverages local-only and global models, to be robust to varying heterogeneity lev-els and global imbalance ratios. LoGo is a clutering-based sampling strategy, which is composed of macro and mi-cro step exploiting local-only and global models, respec-tively. The rationale behind our method is that the opti-mal querying policy needs to evaluate the informativeness of instances with both models, which implicitly learn lo-In a macro cal and global data distribution, respectively. step, to improve the local inter-class diversity first, we per-form k-means clustering [30] in hallucinated gradient space generated from local-only models. Then, in a micro step, the final query set is determined via one step of the EM algorithm [10], making cluster boundaries using instances from macro step (E-step) and cluster-wise sampling with the global model (M-step). The proposed cluster-wise sampling conservatively guarantees the diversity information of the macro step, i.e., the local inter-class diversity obtained by local-only models, while also considering the global minor-ity classes via the global model. ▷ See Section 5
As our third contribution, we conduct a total number of 38 experiments on five datasets using seven AL strate-gies including our LoGo algorithm. To verify the superi-ority of our method in real-world scenarios, we build com-prehensive combinations of six categories, including query selector types (local-only vs. global models), local het-erogeneity levels (α ∈ [0.1, ∞)), global imbalance ratios (ρ ∈ [1, 58]), model architectures, budget sizes, and model initialization scheme. As a result, the experimental results empirically prove our three observations (Obs. 1–3). Be-sides, our method outperforms all other AL baselines and na¨ıve implementations for an ensemble of two query selec-tors in extensive experimental settings. ▷ See Section 6 2.