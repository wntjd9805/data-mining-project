Abstract
We present ARTrack, an autoregressive framework for visual object tracking. ARTrack tackles tracking as a co-ordinate sequence interpretation task that estimates object trajectories progressively, where the current estimate is in-duced by previous states and in turn affects subsequences.
This time-autoregressive approach models the sequential evolution of trajectories to keep tracing the object across frames, making it superior to existing template matching based trackers that only consider the per-frame localiza-tion accuracy. ARTrack is simple and direct, eliminating customized localization heads and post-processings. Despite its simplicity, ARTrack achieves state-of-the-art performance on prevailing benchmark datasets. Source code is available at https://github.com/MIV-XJTU/ARTrack. 1.

Introduction
Visual object tracking [5, 20, 34, 38, 48, 52] is a founda-tional objective in the realm of computer vision, whereby the tracker endeavors to estimate the location of an arbitrary target in each video frame, based on its initial state. Despite its ostensibly straightforward definition, the tracking task poses a significant challenge in real-world settings due to a variety of issues including but not limited to object de-formation, scale variation, occlusion, and distraction from similar objects. Fortunately, visual tracking capitalizes on abundant temporal data as its input comprises a sequence of video frames. Observationally, humans leverage temporal information to gain a perception of the target’s deformation, velocity, and acceleration trends, enabling them to maintain consistent tracking results in the face of indiscriminative or temporarily unavailable visual information.
The present mainstream approaches [10,13,45,61,64] for visual object tracking typically view it as a per-frame tem-plate matching problem, neglecting the potential temporal dependencies among the video frames. These methods gen-erally follow three primary stages: (i) deep neural network-based feature extraction from the search and template images,
Figure 1. Our ARTrack framework. First, we embed visual fea-tures of template and search by an encoder. Then the coordinate tokens at current time step are interpreted by the decoder, condi-tioning on the previous estimates (as spatio-temporal prompts), and the command and visual tokens. (ii) an integration module using either convolution [2,4] or at-tention mechanisms [10,61] for feature matching/fusion, and (iii) bounding-box localization through customized heads for corner [13, 61], center/scale [64] estimation, and target classification [4, 61]. In some cases, the first two stages can be combined using a unified architecture [13, 64]. Post-processing techniques are usually employed during the local-ization step, such as Hanning window penalty [10,56,64,68] and box optimization [4, 56]. Some methods incorporate a template update mechanism to improve the target feature representation. Representative techniques in this category in-clude template image selection [61], feature integration [56], and time evolution [62, 66]. However, customized heads and post-processing techniques are complex and may require individual training and inference, which compromises the simple end-to-end framework. Moreover, tracking empha-sizes preserving localization accuracy across the sequence, while conventional per-frame training methods prioritize immediate localization accuracy, resulting in an objective mismatch between training and inference [35].
This study proposes a novel framework to visual object
tracking that differs from the mainstream methods, which typically employ a per-frame template matching task. In-stead, the authors propose to consider tracking as coordi-nate sequence interpretation, with the objective of learning a simple end-to-end model for direct trajectory estimation.
The proposed approach is based on the idea that given a se-quence of frames and an initial object box, the tracker should
“interpret” a sequence of coordinates that trace the object, in a manner similar to a language modeling task. The pro-posed framework models the sequential evolution of object trajectories across frames by decoding the entire trajectory sequence step by step. The current estimate is influenced by previous states and in turn influences the subsequences, thus unifying the task objectives of training and inference.
Furthermore, the proposed approach simplifies the tracking pipeline by avoiding customized heads and post-processings, relying instead on direct coordinate regression.
The proposed autoregressive visual tracking framework, called ARTrack, is depicted in Figure 1. The first step in this framework is to construct discrete token sequences from object trajectories using a quantization and serialization scheme [8]. The framework then adopts an encoder-decoder architecture to perceive visual information and generate tar-get sequences gradually. In this autoregressive framework, the previous outcomes serve as spatio-temporal prompts, propagating preceding motion dynamics into succeeding frames for more coherent tracking results. Notably, the model is trained using a structured loss function that maxi-mizes the likelihood of the target sequence, consistent with the task objective at test time. The authors demonstrate the ef-ficacy of this approach through extensive experiments, show-ing that the simple and neat ARTrack framework achieves state-of-the-art results on prevailing tracking benchmarks, outperforming other highly customized trackers. 2.