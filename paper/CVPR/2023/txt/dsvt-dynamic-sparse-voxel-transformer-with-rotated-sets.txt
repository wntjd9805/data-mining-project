Abstract
Designing an efficient yet deployment-friendly 3D back-bone to handle sparse point clouds is a fundamental problem in 3D perception. Compared with the customized sparse convolution, the attention mechanism in Transformers is more appropriate for flexibly modeling long-range relation-ships and is easier to be deployed in real-world applications.
However, due to the sparse characteristics of point clouds, it is non-trivial to apply a standard transformer on sparse
In this paper, we present Dynamic Sparse Voxel points.
Transformer (DSVT), a single-stride window-based voxel
Transformer backbone for outdoor 3D perception. In order to efficiently process sparse points in parallel, we propose
Dynamic Sparse Window Attention, which partitions a series of local regions in each window according to its sparsity and then computes the features of all regions in a fully par-allel manner. To allow the cross-set connection, we design a rotated set partitioning strategy that alternates between two partitioning configurations in consecutive self-attention layers. To support effective downsampling and better en-code geometric information, we also propose an attention-style 3D pooling module on sparse points, which is powerful and deployment-friendly without utilizing any customized
CUDA operations. Our model achieves state-of-the-art per-formance with a broad range of 3D perception tasks. More importantly, DSVT can be easily deployed by TensorRT with real-time inference speed (27Hz). Code will be available at https://github.com/Haiyang-W/DSVT. 1.

Introduction 3D perception is a crucial challenge in computer vision, garnering increased attention thanks to its potential appli-cations in various fields such as autonomous driving sys-*Equal contribution.
†Corresponding author.
Figure 1. Detection performance (mAPH/L2) vs speed (Hz) of different methods on Waymo [36] validation set. All the speeds are evaluated on an NVIDIA A100 GPU with AMD EPYC 7513 CPU. tems [2, 41] and modern robotics [44, 53]. In this paper, we propose DSVT, a general-purpose and deployment-friendly
Transformer-only 3D backbone that can be easily applied to various 3D perception tasks for point clouds processing.
Unlike the well-studied 2D community where the input image is in a densely packed array, 3D point clouds are sparsely and irregularly distributed in continuous space due to the nature of 3D sensors, which makes it challenging to directly apply techniques used for traditional regular data. To support sparse feature learning from raw point clouds, pre-vious methods mainly apply customized sparse operations, such as PointNet++ [27, 28] and sparse convolution [12, 13].
PointNet based methods [27, 28, 39] use point-wise MLPs with the ball-query and max-pooling operators to extract features. Sparse convolution based methods [6, 12, 13] first convert point clouds to regular grids and handle the sparse volumes efficiently. Though impressive, they suffer from either the intensive computation of sampling and group-ing [28] or the limited representation capacity due to sub-manifold dilation [13]. More importantly, these specifically-designed operations generally can not be implemented with well-optimized deep learning tools (e.g., PyTorch and Tensor-Flow) and require writing customized CUDA codes, which
greatly limits their deployment in real-world applications.
Witnessing the success of transformer [40] in the 2D do-main, numerous attention-based 3D vision methods have been investigated in point cloud processing. However, be-cause of the sparse characteristic of 3D points, the number of non-empty voxels in each local region can vary significantly, which makes directly applying a standard Transformer non-trivial. To efficiently process the attention on sparse data, many approaches rebalance the token number by random sampling [25,49] or group local regions with similar number of tokens together [10, 37]. These methods are either insep-arable from superfluous computations (e.g., dummy token padding and non-parallel attention) or noncompetitive per-formance (e.g., token random dropping). Alternatively, some approaches [15, 24] try to solve these problems by writing customized CUDA operations, which require heavy opti-mization to be deployed on modern devices. Hence, building an efficient and deployment-friendly 3D transformer back-bone is the main challenge we aim to address in this paper.
In this paper, we seek to expand the applicability of Trans-former such that it can serve as a powerful backbone for out-door 3D perception, as it does in 2D vision. Our backbone is efficient yet deployment-friendly without any customized
CUDA operations. To achieve this goal, we present two major modules, one is the dynamic sparse window attention to support efficient parallel computation of local windows with diverse sparsity, and the other one is a novel learnable 3D pooling operation to downsample the feature map and better encode geometric information.
Specifically, as illustrated in Figure 2, given the sparse voxelized representations and window partition, we first di-vide each window’s sparse voxels into some non-overlapped subsets, where each subset is guaranteed to have the same number of voxels for parallel computation. The partitioning configuration of these subsets will be changed in consecu-tive self-attention layers based on the rotating partition axis between the X-Axis and Y-Axis. It bridges the subsets of preceding layers for intra-window fusion, providing connec-tions among them that significantly enhance modeling power (see Table 6). To better process the inter-window fusion and encode multi-scale information, we propose the hybrid window partition, which alternates its window shape in suc-cessive transformer blocks. It leads to a drop in computation cost with even better performance (see Table 7). With the above designs, we process all regions in a fully parallel man-ner by calculating them in the same batch. This strategy is efficient in regards to real-world latency: i) all the windows are processed in parallel, which is nearly independent of the voxel distribution, ii) using self-attention without key dupli-cation, which facilitates memory access in hardware. Our experiments show that the dynamic sparse window attention approach has much lower latency than previous bucketing-based strategies [10, 37] or vanilla window attention [20],
Figure 2. A demonstration of dynamic sparse window attention in our DSVT block. In the X-Axis DSVT layer, the sparse voxels will be split into a series of window-bounded and size-equivalent subsets in X-Axis main order, and self-attention is computed within each set. In the next layer, the set partition is switched to Y-Axis, providing connections among the previous sets. yet is similar in modeling power (see Table 5).
Secondly, we present a powerful yet deployed-friendly 3D sparse pooling operation to efficiently process downsam-pling and encode better geometric representation. To tackle the sparse characteristic of 3D point clouds, previous meth-ods adopt some custom operations, e.g., customized scatter function [11] or strided sparse convolution to generate down-sampled feature volumes [46, 50]. The requirement of heavy optimization for efficient deployment limits their real-world applications. More importantly, we empirically find that inserting some linear or max-pooling layers between our transformer blocks also harms the network convergence and the encoding of geometric information. To address the above limitations, we first convert the sparse downsampling region into dense and process an attention-style 3D pooling oper-ation to automatically aggregate the local spatial features.
Our 3D pooling module is powerful and deployed-friendly without any self-designed CUDA operations, and the perfor-mance gains (see Table 8) demonstrate its effectiveness.
In a nutshell, our contributions are four-fold: 1) We pro-pose Dynamic Sparse Window Attention, a novel window-based attention strategy for efficiently handling sparse 3D voxels in parallel. 2) We present a learnable 3D pooling operation, which can effectively downsample sparse voxels and encode geometric information better. 3) Based on the above key designs, we introduce an efficient yet deployment-friendly transformer 3D backbone without any customized
CUDA operations. It can be easily accelerated by NVIDIA
TensorRT to achieve real-time inference speed (27Hz), as shown in Figure 1. 4) Our approach outperforms previous state-of-the-art methods on the large-scale Waymo [35] and nuScenes [3] datasets across various 3D perception tasks. 2.