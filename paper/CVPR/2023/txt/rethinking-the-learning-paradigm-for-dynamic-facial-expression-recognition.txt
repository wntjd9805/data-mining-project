Abstract
Dynamic Facial Expression Recognition (DFER) is a rapidly developing field that focuses on recognizing facial expressions in video format. Previous research has con-sidered non-target frames as noisy frames, but we pro-pose that it should be treated as a weakly supervised prob-lem. We also identify the imbalance of short- and long-term temporal relationships in DFER. Therefore, we in-troduce the Multi-3D Dynamic Facial Expression Learn-ing (M3DFEL) framework, which utilizes Multi-Instance
Learning (MIL) to handle inexact labels. M3DFEL gen-erates 3D-instances to model the strong short-term tem-poral relationship and utilizes 3DCNNs for feature extrac-tion. The Dynamic Long-term Instance Aggregation Mod-ule (DLIAM) is then utilized to learn the long-term temporal relationships and dynamically aggregate the instances. Our experiments on DFEW and FERV39K datasets show that
M3DFEL outperforms existing state-of-the-art approaches with a vanilla R3D18 backbone. The source code is avail-able at https://github.com/faceeyes/M3DFEL. 1.

Introduction
Facial expressions are essential in communication [26, 27, 45]. Understanding the emotions of others through their facial expressions is critical during conversations. Thus, automated recognition of facial expressions is a significant challenge in various fields, such as human-computer inter-action (HCI) [25, 34], mental health diagnosis [12], driver fatigue monitoring [24], and metahuman [6]. While signif-icant progress has been made in Static Facial Expression
Recognition (SFER) [23, 43, 44, 55], there is increasing at-tention on Dynamic Facial Expression Recognition.
*Both author contributed equally to this work. Work done during
Hanyang Wang’s internship at Tencent Youtu Lab and Bo Li is the project lead.
†Corresponding authors. libraboli@tencent.com, lsttoy@163.com, amzhou@cs.ecnu.edu.cn
Figure 1. In-the-wild Dynamic Facial Expressions. In the first row of images, the subject appear predominantly Neutral, yet the video is labeled as happy without specifying the exact moment when the emotion is expressed. In the second row, the emotion is evident from the perspective of a few figures, but any single one of them is noisy and unclear. In the third row, all frames appear Neutral, but a closer analysis of facial movement over time reveals a rising of the corner of the mouth, indicating a smile.
With the availability of large-scale in-the-wild datasets like DFEW [11] and FERV39K [46], several methods have been proposed for DFER [21, 22, 31, 47, 54]. Previous works [31, 54] have simply applied general video under-standing methods to recognize dynamic facial expressions.
Later on, Li et al. [22] observe that DFER contains a large number of noisy frames and propose a dynamic class token and a snippet-based filter to suppress the impact of these frames. Li et al. [21] propose an Intensity Aware Loss to account for the large intra-class and small inter-class differ-ences in DFER and force the network to pay extra atten-tion to the most confusing class. However, we argue that
DFER requires specialized designs rather than being con-sidered a combination of video understanding and SFER.
Although these works [21, 22, 47] have identified some is-sues in DFER, their models have only addressed them in a rudimentary manner.
Firstly, these works fail to recognize that the existence of non-target frames in DFER is actually caused by weak su-pervision. When collecting large-scale video datasets, an-notating the precise location of labels is labor-intensive and challenging. A dynamic facial expression may contain a change between non-target and target emotions, as shown in
Figure 1. Without a location label that can guide the model to ignore the irrelevant frames and focus on the target, mod-els are likely to be confused by the inexact label. Therefore, modeling these non-target frames as noisy frames directly is superficial, and the underlying weakly supervised problem remains unsolved.
Secondly, the previous works directly follow to use se-quence models without a dedicated design for DFER. How-ever, we find that there is an imbalance between short-and long-term temporal relationships in DFER. For exam-ple, some micro-expressions may occur within a short clip, while some facial movements between expressions may dis-rupt individual frames, as shown in Figure 1. In contrast, there is little temporal relationship between a Happy face at the beginning of a video and another Happy face at the end. Therefore, neither modeling the entire temporal rela-tionship nor using completely time-irrelevant aggregation methods is suitable for DFER. Instead, a method should learn to model the strong short-term temporal relationship and the weak long-term temporal relationship differently.
To address the first issue, we suggest using weakly su-pervised strategies to train DFER models instead of treating non-target frames as noisy frames. Specifically, we propose modeling DFER as a Multi-Instance Learning (MIL) prob-lem, where each video is considered as a bag containing a set of instances.
In this MIL framework, we disregard non-target emotions in a video and only focus on the target emotion. However, most existing MIL methods are time-independent, which is unsuitable for DFER. Therefore, a dedicated MIL framework for DFER is necessary to address the imbalanced short- and long-term temporal relationships.
The M3DFEL framework proposed in this paper is de-signed to address the imbalanced short- and long-term tem-poral relationships and the weakly supervised problem in
DFER in a unified manner. It uses a combination of 3D-Instance and R3D18 models to enhance short-term tem-poral learning. Once instance features are extracted, they are fed into the Dynamic Long-term Instance Aggregation
Module (DLIAM), which aggregates the features into a bag-level representation. The DLIAM is specifically de-signed to capture long-term temporal relationships between instances. Additionally, the Dynamic Multi-Instance Nor-malization (DMIN) is employed to maintain temporal con-sistency at both the bag-level and instance-level by perform-ing dynamic normalization.
Overall, our contributions can be summarized as follows:
• We propose a weakly supervised approach to model
Dynamic Facial Expression Recognition (DFER) as a Multi-Instance Learning (MIL) problem. We also identify an imbalance between short- and long-term temporal relationships in DFER, which makes it inap-propriate to model the entire temporal relationship or use time-irrelevant methods.
• We propose the Multi-3D Dynamic Facial Expression
Learning (M3DFEL) framework to provide a unified solution to the weakly supervised problem and model the imbalanced short- and long-term temporal relation-ships in DFER.
• We conduct extensive experiments on DFEW and
FERV39K, and our proposed M3DFEL achieves state-of-the-art results compared with other methods, even when using a vanilla R3D18 backbone. We also con-duct visualization experiments to analyze the perfor-mance of M3DFEL and uncover unsolved problems. 2.