Abstract
Learnable prompt:  “? ? ? ? [class_name]”
Semi-supervised class-conditional image synthesis is typically performed by inferring and injecting class labels into a conditional Generative Adversarial Network (GAN).
The supervision in the form of class identity may be inade-quate to model classes with diverse visual appearances. In this paper, we propose a Learnable Cluster Prompt-based
GAN (LCP-GAN) to capture class-wise characteristics and intra-class variation factors with a broader source of su-pervision. To exploit partially labeled data, we perform soft partitioning on each class, and explore the possibility of as-sociating intra-class clusters with learnable visual concepts in the feature space of a pre-trained language-vision model, e.g., CLIP. For class-conditional image generation, we de-sign a cluster-conditional generator by injecting a combi-nation of intra-class cluster label embeddings, and further incorporate a real-fake classiﬁcation head on top of CLIP to distinguish real instances from the synthesized ones, con-ditioned on the learnable cluster prompts. This signiﬁcant-ly strengthens the generator with more semantic language supervision. LCP-GAN not only possesses superior gen-eration capability but also matches the performance of the fully supervised version of the base models: BigGAN and
StyleGAN2-ADA, on multiple standard benchmarks. 1.

Introduction
Generative Adversarial Networks (GANs) have achieved considerable success in modeling complex data distribu-tions and generating high-ﬁdelity images from random vec-tors [2, 18, 25]. To control class semantics in the generation
∗Joint ﬁrst authors.
†Corresponding author.
“? ? ? ?” (cid:3406)“Wavy Hair”
“? ? ? ?” (cid:3406) “Smiling”
Class
Intra-class cluster
Figure 1. Different from generic class-conditional GANs con-ditioned on discrete class labels, LCP-GAN learns intra-class cluster-speciﬁc prompts to guide the generation process and cap-ture underlying variation factors. process, object category is typically represented in the form of a discrete label, which is injected into both generator and discriminator through learnable embedding layers. Howev-er, sufﬁcient labeled training data may be difﬁcult to collect in real-world applications. Signiﬁcant efforts have been de-voted to semi-supervised generative learning that aims to re-duce the dependence of class-conditional GANs on labeled training data [6, 15, 29, 33].
In the semi-supervised setting, the amount of unlabeled training samples can be signiﬁcantly greater than that of la-beled ones. As one of the early attempts, CatGAN [44] trained a discriminator to infer the class labels of real im-ages with high conﬁdence, but not for the synthesized ones. Both TripleGAN [29] and Δ-GAN [15] incorpo-rated an auxiliary classiﬁer to focus on class label pre-diction in the adversarial training process. The unlabeled images with pseudo labels were used to train the class-(cid:2163)(cid:2187)(cid:2196)(cid:2187)(cid:2200)(cid:2183)(cid:2202)(cid:2197)(cid:2200) (cid:2163) (cid:1878) (cid:4666)(cid:3036)(cid:4667)(cid:2013)(cid:4666)(cid:1855)(cid:2291)(cid:820)(cid:3036)(cid:4667) (cid:1873)(cid:3053) (cid:3533) (cid:3036) (cid:2161)(cid:2195)(cid:2184)(cid:2187)(cid:2186)(cid:2186)(cid:2191)(cid:2196)(cid:2189) (cid:485) (cid:2013)(cid:4666)(cid:1855)(cid:2291)(cid:820)(cid:2870)(cid:28604) (cid:1620) (cid:2013)(cid:4666)(cid:1855)(cid:2291)(cid:820)(cid:2869)(cid:4667) (cid:2013)(cid:4666)(cid:1855)(cid:3052)(cid:820)(cid:3038)(cid:2291)(cid:4667) (cid:2869) (cid:1873)(cid:3053) (cid:2870) (cid:485) (cid:1873)(cid:3053) (cid:3038)(cid:2291) (cid:1873)(cid:3053) (cid:2162)(cid:2183)(cid:2193)(cid:2187) (cid:2165)(cid:2195)(cid:2183)(cid:2189)(cid:2187)(cid:2201) (cid:2174)(cid:2187)(cid:2183)(cid:2194) (cid:2165)(cid:2195)(cid:2183)(cid:2189)(cid:2187)(cid:2201) (cid:2159)(cid:2194)(cid:2183)(cid:2201)(cid:2201) (cid:2170)(cid:2183)(cid:2195)(cid:2187) (cid:1834)(cid:1857)(cid:1870)(cid:1870)(cid:1861)(cid:1866)(cid:1859) (cid:1833)(cid:1873)(cid:1864)(cid:1864) (cid:1832)(cid:1861)(cid:1866)(cid:1855)(cid:1860) (cid:1842)(cid:1873)(cid:1870)(cid:1868)(cid:1864)(cid:1857) (cid:485) (cid:485) (cid:485) (cid:2160)(cid:2191)(cid:2201)(cid:2185)(cid:2200)(cid:2191)(cid:2195)(cid:2191)(cid:2196)(cid:2183)(cid:2202)(cid:2197)(cid:2200) (cid:2160)(cid:2185)(cid:2194)(cid:2183)(cid:2201)(cid:2201) (cid:2291) (cid:1622) (cid:1618) (cid:1844)(cid:1857)(cid:1853)(cid:1864) (cid:512)(cid:1832)(cid:1853)(cid:1863)(cid:1857) (cid:2161)(cid:2191)(cid:2195)(cid:2189) (cid:1838)(cid:1853)(cid:1878)(cid:1873)(cid:1864)(cid:1861) (cid:1828)(cid:1873)(cid:1866)(cid:1872)(cid:1861)(cid:1866)(cid:1859) (cid:2160)(cid:2191)(cid:2201)(cid:2185)(cid:2200)(cid:2191)(cid:2195)(cid:2191)(cid:2196)(cid:2183)(cid:2202)(cid:2197)(cid:2200) (cid:2160)(cid:2198)(cid:2200)(cid:2197)(cid:2195)(cid:2198)(cid:2202) (cid:2190)(cid:2191)(cid:2195)(cid:2189) (cid:2205)(cid:2203)(cid:2196)(cid:2185) (cid:1618) (cid:1844)(cid:1857)(cid:1853)(cid:1864) (cid:512)(cid:1832)(cid:1853)(cid:1863)(cid:1857) (cid:2194)(cid:2187)(cid:2183)(cid:2200)(cid:2196)(cid:2183)(cid:2184)(cid:2194)(cid:2187) (cid:2198)(cid:2200)(cid:2197)(cid:2195)(cid:2198)(cid:2202) (cid:2869) (cid:1874)(cid:2291)(cid:820)(cid:2869) (cid:2869) (cid:1874)(cid:2291)(cid:820)(cid:3038)(cid:2291) (cid:3019) (cid:1874)(cid:2291)(cid:820)(cid:2869) (cid:2870) (cid:1710) (cid:1874)(cid:2291)(cid:3117) (cid:485) (cid:2870) (cid:1710) (cid:1874)(cid:2291)(cid:3286)(cid:2291) (cid:1874)(cid:2291)(cid:820)(cid:3038)(cid:2291) (cid:3019) (cid:1855)(cid:1864)(cid:1853)(cid:1871)(cid:1871)(cid:820)(cid:1866)(cid:1853)(cid:1865)(cid:1857) (cid:484) (cid:1855)(cid:1864)(cid:1853)(cid:1871)(cid:1871)(cid:820)(cid:1866)(cid:1853)(cid:1865)(cid:1857) (cid:484) (cid:2161)(cid:2202)(cid:2206)(cid:2202) (cid:1620) (cid:1875)(cid:2291) (cid:1622) (cid:2190)(cid:2202)(cid:2206)(cid:2202)
Figure 2. An overview of the proposed LCP-GAN. The generator G synthesizes class-speciﬁc images, conditioned on the combination z E(cy i). In addition to class-conditional adversarial training via the discriminator Dclass, we of intra-cluster label embeddings: incorporate an additional discriminator Dprompt to distinguish real images from the synthesized ones, conditioned on the combination of the learnable cluster-speciﬁc prompts {ty 1, . . . , ty ky }. By competing with Dclass and Dprompt, G learns to associate the cluster label embeddings with the underlying visual concepts described by the prompts. i u(i) (cid:2) conditional discriminators. There have also been some at-tempts at improving GANs via unsupervised data partition-ing [14, 16, 21, 31, 39]. SphericGAN [6] imposed hard par-titioning on the training data, and aligned the real and syn-thesized data clusters in a hyper-spherical latent space. The existing semi-supervised GANs are conditioned on class i-dentity, while the semantics encapsulated in the class names is overlooked. This impedes the generative model from using prior knowledge in natural language as humans do.
Furthermore, only one single label embedding is learnt per class, which is insufﬁcient to account for large intra-class variance. To address these issues, we explore intra-class variation factors by performing soft and ﬁner partitions on each class and learning cluster-speciﬁc prompts to represent underlying visual concepts as shown in Figure 1.
More speciﬁcally, we propose a Learnable Cluster
Prompt-based GAN to facilitate semi-supervised class-conditional image generation, and our model is referred to as LCP-GAN. To better match class-speciﬁc data distri-bution, the generator learns to synthesize high-ﬁdelity im-ages, conditioned on a combination of intra-class cluster la-bel embeddings. Capturing intra-class variation factors is a non-trivial task, since the semantics reﬂected by the clusters may not be well-deﬁned. Considering that natural language can express a wide range of visual concepts, we make an attempt to learn from CLIP [42], which provides an effec-tive way to understand the content of images. Inspired by
CoOp [56], we can model the cluster-speciﬁc context words with learnable vectors, and further learn a mapping to adapt the CLIP representation to our generation task. As a re-sult, the generator is guided to capture cluster semantics in the adversarial training process. The framework of LCP-GAN is illustrated in Figure 2. We adopt the state-of-the-art architectures: BigGAN [2] and StyleGAN2-ADA [23], and achieve signiﬁcant improvements over them, suggest-ing that semi-supervised image generation can beneﬁt from modeling intra-class variation with the language-vision pre-training.
The main contributions of this work are summarized as follows: (a) We associate the intra-class cluster label em-beddings with the cluster semantics, and the expressiveness of their combination is higher than that of a single class label embedding for capturing multiple underlying modes with diverse visual appearances. (b) To address the issue that the visual concepts reﬂected by the clusters may not be well deﬁned, we leverage the language-vision pre-training and represent the clusters with learnable prompts. (c) To guide the generator to capture intra-class variation factors, the cluster prompts serve as conditional information and are jointly learnt in the adversarial training process. 2.