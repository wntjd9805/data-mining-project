Abstract
Graph matching (GM) aims at discovering node match-ing between graphs, by maximizing the node- and edge-wise afﬁnities between the matched elements. As an NP-hard problem, its challenge is further pronounced in the existence of outlier nodes in both graphs which is ubiqui-tous in practice, especially for vision problems. However, popular afﬁnity-maximization-based paradigms often lack a principled scheme to suppress the false matching and resort to handcrafted thresholding to dismiss the outliers.
This limitation is also inherited by the neural GM solvers though they have shown superior performance in the ideal no-outlier setting. In this paper, we propose to formulate the partial GM problem as the top-k selection task with a given/estimated number of inliers k. Speciﬁcally, we devise a differentiable top-k module that enables effective gradi-ent descent over the optimal-transport layer, which can be readily plugged into SOTA deep GM pipelines including the quadratic matching network NGMv2 as well as the linear matching network GCAN. Meanwhile, the attention-fused aggregation layers are developed to estimate k to enable automatic outlier-robust matching in the wild. Last but not least, we remake and release a new benchmark called
IMC-PT-SparseGM, originating from the IMC-PT stereo-matching dataset. The new benchmark involves more scale-varying graphs and partial matching instances from the real world. Experiments show that our methods outperform other partial matching schemes on popular benchmarks. 1.

Introduction
The importance of graph matching (GM) is recognized by its successful applications in machine learning [26], molecule matching [47], and various vision tasks [15, 30,
⇤Runzhong Wang and Ziao Guo contributed equally. Junchi Yan is the correspondence author who is also with Shanghai AI Laboratory. The work was in part supported by National Key Research and Development
Program of China (2020AAA0107600), NSFC (62222607, U19B2035),
Shanghai Committee Science and Technology Project (22511105100).
IMC-PT-SparseGM dataset: https://github.com/Thinklab-SJTU/IMCPT- SparseGM- dataset; Code: https://github. com/Thinklab-SJTU/ThinkMatch.
Illustrative comparison of injective graph matching
Figure 1. (left) and partial graph matching (right) on our remade and re-leased benchmark: IMC-PT-SparseGM (originated from IMC-PT [18], see Sec. 4 for details). Green for correct matches, red for wrong matches. Without partial matching, the GM solver greed-ily matches all nodes, leading to inferior accuracy. This paper presents a general learning method to mitigate this issue. 49]. Existing GM methods mainly follow the optimiza-tion formulation by maximizing the node-wise and edge-wise afﬁnities of the matched elements, yielding an NP-hard problem known as Quadratic Assignment Problem [22].
The ubiquitous challenge of partial matching, how-ever, is less addressed by the existing afﬁnity-maximization pipeline. Partial matching means only a partial set of the nodes are matched, due to the existence of outliers on both sides. As shown in Fig. 1, this is commonly encountered in (visual) graph matching [13, 16, 52, 53], where the exis-tence of outliers is usually unavoidable due to the errors in keypoint detectors and (self-)occlusion of objects.
The recent line of deep graph matching papers [11, 45, 52] sheds light on the partial GM, whereby the higher ca-pacity offered by deep neural networks on both the feature stage [52] and the solver stage [27, 45] will hopefully dis-tinguish the outliers from inliers. However, there still lacks a general, principled approach that could enable the par-tial matching capability of existing graph matching neural networks. Some straightforward treatments such as thresh-olding [30], and adding dummy rows and columns [17] are relatively inﬂexible because their threshold and dummy val-ues are manually assigned.
We aim at developing a general, uniﬁed partial match-ing handling approach, which can be readily integrated into
SOTA GM networks. However, it is still an open ques-tion about how to determine whether or not a matching pair should be discarded. Besides, directly discarding the unwanted matching pairs causes non-differentiability and
(cid:54)(cid:72)(cid:70)(cid:17)(cid:3)(cid:22)(cid:17)(cid:21)(cid:17)(cid:21) (cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:16)(cid:73)(cid:88)(cid:86)(cid:72)(cid:71)(cid:3)(cid:36)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:56)(cid:81)(cid:76)(cid:73)(cid:76)(cid:72)(cid:71)(cid:3)(cid:37)(cid:76)(cid:83)(cid:68)(cid:85)(cid:87)(cid:76)(cid:87)(cid:72)(cid:3)(cid:42)(cid:85)(cid:68)(cid:83)(cid:75)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:76)(cid:81)(cid:74) (cid:76)(cid:80)(cid:68)(cid:74)(cid:72)(cid:3)(cid:83)(cid:68)(cid:76)(cid:85)(cid:86) (cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:72)(cid:91)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85) (cid:74)(cid:85)(cid:68)(cid:83)(cid:75)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86) (cid:74)(cid:85)(cid:68)(cid:83)(cid:75)(cid:3)(cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:81)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78) (cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:85)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87) (cid:36)(cid:41)(cid:36)(cid:16)(cid:56)(cid:3)(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72) (cid:38)(cid:49)(cid:49) (cid:42)(cid:48)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78) (cid:55)(cid:82)(cid:83)(cid:16) (cid:16)(cid:42)(cid:48)(cid:3)(cid:36)(cid:79)(cid:74)(cid:82)(cid:85)(cid:76)(cid:87)(cid:75)(cid:80)(cid:3)(cid:3)(cid:54)(cid:72)(cid:70)(cid:17)(cid:3)(cid:22)(cid:17)(cid:20)(cid:3) (cid:3) (cid:193)(cid:68)(cid:87)(cid:87)(cid:72)(cid:81) (cid:85)(cid:72)(cid:86)(cid:75)(cid:68)(cid:83)(cid:72) (cid:71)(cid:82)(cid:88)(cid:69)(cid:79)(cid:92)(cid:16)(cid:86)(cid:87)(cid:82)(cid:70)(cid:75)(cid:68)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:80)(cid:68)(cid:87)(cid:85)(cid:76)(cid:91)(cid:3) (cid:85)(cid:72)(cid:73)(cid:76)(cid:81)(cid:72)(cid:71)(cid:3)(cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:68)(cid:87)(cid:85)(cid:76)(cid:91) (cid:71)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:54)(cid:50)(cid:41)(cid:55)(cid:16)(cid:55)(cid:82)(cid:83) (cid:36)(cid:41)(cid:36)(cid:16)(cid:44)(cid:3)(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72) (cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:16)(cid:73)(cid:88)(cid:86)(cid:72)(cid:71)(cid:3)(cid:36)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:44)(cid:81)(cid:71)(cid:76)(cid:89)(cid:76)(cid:71)(cid:88)(cid:68)(cid:79)(cid:3)(cid:42)(cid:85)(cid:68)(cid:83)(cid:75)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3) (cid:54)(cid:72)(cid:70)(cid:17)(cid:3)(cid:22)(cid:17)(cid:21)(cid:17)(cid:20) (cid:36)(cid:41)(cid:36)(cid:16)(cid:56) (cid:36)(cid:41)(cid:36)(cid:16)(cid:44) (cid:50)(cid:88)(cid:85)(cid:3)(cid:51)(cid:85)(cid:82)(cid:83)(cid:82)(cid:86)(cid:72)(cid:71)(cid:3)(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)(cid:86)
Figure 2. Deep learning paradigm for partial graph matching. The input images are sent to CNN to extract features and build the input graphs. The GM network predicts a doubly-stochastic matrix based on graphs with features, followed by a differentiable top-k algorithm.
The number of inliers k is estimated by our proposed attention-fused aggregation networks. truncated gradient. We resort to the doubly-stochastic ma-trix available in most SOTA GM pipelines [17, 44, 45], whereby the values in the doubly-stochastic matrix could be viewed as the matching conﬁdence. The matchings with the top-k conﬁdence values should be preserved, assuming that the number of inliers k is known. To this end, we present a top-k deep graph matching approach to address the partial matching challenge, whereby the differentiable top-k for-mulation [48] is followed to enable the end-to-end training.
Another challenge that naturally arises is how to estimate the value of k (i.e. number of inliers) from scratch. We identify the connection between the k-estimation problem and the graph-level similarity learning problem, whereby some prior successful models [1, 2] could be further ex-In this paper, we present two networks to ef-ploited.
ﬁciently reuse the mid-layers that are available in most deep GM pipelines, namely Attention-Fused Aggregation (AFA) modules, based on the similarity of graphs aggre-gated from either individual graph features (Sec. 3.2.1) or the doubly-stochastic similarities on a uniﬁed bipartite graph (Sec. 3.2.2). The AFA modules are further integrated into the learning pipeline.
Besides, the severe partial matching issue that usually exists in downstream vision tasks is less addressed by exist-ing graph matching evaluation protocols. We are thus moti-vated to collect and relabel a new benchmark, namely IMC-PT-SparseGM, which is originated from the stereo matching task in Image Matching Challenge PhotoTourism (IMC-PT) 2020 [18]. As summarized in Tbl. 1, the new benchmark ad-dresses the severe partial matching challenge, and its graphs are of larger sizes than existing benchmarks.
The main contributions of the paper are as follows. 1) We formulate the partial (graph) matching problem as a top-k scheme, i.e. in the presence of outliers in both
Table 1. Visual GM datasets. “partial rate” means the mean per-centage of occluded keypoints w.r.t. the universe of all keypoints. partial rate 0.0% 28.5% 57.3% 55.5% dataset name
Willow Object Class [6]
Pascal VOC Keypoint [5]
IMC-PT-SparseGM-50 (ours)
IMC-PT-SparseGM-100 (ours)
# visual graphs 404 8702 25765 25765 avg # nodes 10 9.07 21.36 44.48
# universe 10 6 to 23 50 100 graphs. The scheme can be integrated into SOTA GM neu-ral networks e.g. [17, 45] as a plugin. 2) Based on the top-k formulation, we devise an end-to-end and outlier-aware neural pipeline for partial GM learn-ing and show its effectiveness. In contrast, we show that di-rectly combining the top-k scheme with either a traditional solver like RRWM [7] or an outlier-blind neural solver e.g. NGMv2 [45] still produce poor results (see Tbl. 6), sug-gesting the necessity of our integrated learning paradigm. 3) To estimate the number of inliers k which is often un-known in practice, we devise an attention-based supervised graph neural network whose input consists of similarity in-formation available in GM networks. With this estimation module, we enable a fully automatic neural solver for partial
GM which to our best knowledge is new in the literature. 4) Our approach outperforms existing methods on pop-ular GM benchmarks notably in the setting of partial GM.
Last but not least, we remake and release a new benchmark to advance the research in GM by introducing larger graphs for matching and more partial matching instances. 2.