Abstract
Self-attention mechanism has been a key factor in the re-cent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learn-ing or subject to some handcrafted designs.
In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col func-tion or rely on specific CUDA kernels that are hard to gen-eralize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolu-tion as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value posi-tions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently im-proved performances on comprehensive benchmarks. 1.

Introduction
Transformer was originally proposed for natural lan-guage processing [5, 29] and has gained increasing research interest in recent years. With the advent of Vision Trans-former [9], researchers begin to realize its great potential on processing vision data, and further extend Transformer models to a variety of vision tasks including image classi-*Equal contribution.
â€ Corresponding author.
Figure 1. Comparison of our model and other attention pat-terns. Comparing to the global attention in PVT and window at-tention in Swin-Transformer, we propose a novel Slide Attention module that not only imposes local inductive bias like local atten-tion, but also has high efficiency and flexibility. fication [20, 21, 30], semantic segmentation [26, 36], object detection [2, 16, 22, 39], and multi-modal tasks [23, 24].
Nevertheless, adapting Transformer to vision is a non-trivial task. The computation complexity of self-attention with global receptive field grows quadratically with the se-quence length, which leads to excessive computation costs and makes it impractical for vision models that require high-resolution inputs and large memory consumption.
To overcome this challenge, existing works have pro-posed to limit the global receptive field to smaller regions.
For example, PVT [30] and DAT [33] use sparse global at-tention to select sparse key and value positions from the feature map and share them across all queries. Another line of research including Swin Transformer [20] and CSwin
Transformer [8] follow the window attention paradigm. The input is divided into specially designed windows, where features are extracted and aggregated within. Despite be-ing efficient, these carefully designed attention patterns still suffer from several limitations. On one hand, sparse global attention tends to be inferior in capturing local features, and is susceptible to key and value positions where informative features in other regions may be discarded. On the other hand, window attentions may hinder cross-window commu-nication, and involve extra designs like window shifts that set restrictions on the model structure.
Instead of shrinking the global receptive field, a natu-ral and effective alternative is adopting local attention by constraining receptive field of each query in its own neigh-boring pixels, where similar pattern has been widely used in traditional convolution design [6, 13]. Compared with the aforementioned attention patterns, local attention has the advantages of convolution with translation-equivariance and local inductive bias, while also enjoying the flexibility and data-dependency of the self-attention mechanism. Sev-eral works have already investigated applying local atten-tion to modern convolution or Transformer models. How-ever, they either use the inefficient Im2Col function [25] which results in huge increase in inference time, or rely on carefully written CUDA kernels [11, 37] which restrict the applicability on devices without CUDA support. There-fore, developing a local attention module with both high efficiency and high generalizability remains challenging.
In this paper, we present a novel local attention module, dubbed Slide Attention, that can be efficiently integrated with various Vision Transformer models and hardware de-vices. We target the inefficient Im2Col function that was adopted in the previous local attention module and view the process from a new perspective. Specifically, the original
Im2Col generates the key and value matrix from a column-based view, where each column represents a local region centered at a certain position of the input. Alternatively, we re-formulate the key and value matrix from a row-based view and show that each row corresponds to the input fea-ture shifted in different directions. This new insight gives us the chance to take a further step, that allows us to re-place the shifting operation with carefully designed Depth-wise Convolutions.
In this way, the Im2Col function is replaced with standard convolution operations, which can be realized in a more efficient manner and easily imple-mented on different hardware devices. To further enhance flexibility, we introduce a novel deformed shifting module that relaxes fixed key and value positions (Fig.1(c)) to de-formed features within the local region (Fig.1(d)). By using a re-parameterization technique, we effectively increase the model capacity while preserving inference efficiency.
We empirically validate our module on image classifi-cation, semantic segmentation, and object detection tasks under five advanced Vision Transformer models, and show consistent improvements over all baselines. When adopted on devices without CUDA support like Metal Performance
Figure 2. Performance and inference speed comparison on local attention implementations. Results are based on Swin-Tiny [20]. Previous works mainly use Im2Col function [25] or carefully designed CUDA kernels [11], where the former is highly inefficient and the latter only shows marginal improve-ments over other attention patterns, e.g., window attention in
Swin-Transformer, and hard to generalize to other devices. Our work first re-interprets the Im2Col function as feature shift oper-ations, then substitute shifts with more efficient depthwise convo-lutions. When further equipped with a deformed shifting module, our model achieves significant improvements over baselines under competitive inference time. FPS is tested on an RTX3090 GPU.
Shader (MPS) or iPhone 12, our method also proves to be efficient. For instance, our Slide Attention based on
Swin-small outperforms the vanilla Swin-base model while achieving 1.7x inference speedup on iPhone 12. 2.