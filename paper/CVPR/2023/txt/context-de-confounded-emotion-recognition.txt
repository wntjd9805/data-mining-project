Abstract
Context-Aware Emotion Recognition (CAER) is a cru-cial and challenging task that aims to perceive the emo-tional states of the target person with contextual informa-tion. Recent approaches invariably focus on designing so-phisticated architectures or mechanisms to extract seem-ingly meaningful representations from subjects and con-texts. However, a long-overlooked issue is that a con-text bias in existing datasets leads to a significantly unbal-anced distribution of emotional states among different con-text scenarios. Concretely, the harmful bias is a confounder that misleads existing models to learn spurious correlations based on conventional likelihood estimation, significantly limiting the models’ performance. To tackle the issue, this paper provides a causality-based perspective to disentan-gle the models from the impact of such bias, and formu-late the causalities among variables in the CAER task via a tailored causal graph. Then, we propose a Contextual
Causal Intervention Module (CCIM) based on the backdoor adjustment to de-confound the confounder and exploit the true causal effect for model training. CCIM is plug-in and model-agnostic, which improves diverse state-of-the-art ap-proaches by considerable margins. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our CCIM and the significance of causal insight. 1.

Introduction
As an essential technology for understanding human in-tentions, emotion recognition has attracted significant at-tention in various fields such as human-computer interac-tion [1], medical monitoring [28], and education [40]. Pre-vious works have focused on extracting multimodal emo-tion cues from human subjects, including facial expres-sions [9, 10, 49], acoustic behaviors [2, 50, 52], and body
§Corresponding Author.
Figure 1. Illustration of the context bias in the CAER task. GT means the ground truth. Most images contain similar contexts in the training data with positive emotion categories.
In this case, the model learns the spurious correlation between specific contexts and emotion categories and gives wrong results. Thanks to CCIM, the simple baseline [19] achieves more accurate predictions. postures [25, 53], benefiting from advances in deep learn-ing algorithms [6, 7, 21, 26, 27, 43, 44, 46, 47, 54, 55, 59].
Despite the impressive improvements achieved by subject-centered approaches, their performance is limited by natu-ral and unconstrained environments. Several examples in
Figure 1 (left) show typical situations on a visual level. In-stead of well-designed visual contents, multimodal repre-sentations of subjects in wild-collected images are usually indistinguishable (e.g., ambiguous faces or gestures), which forces us to exploit complementary factors around the sub-ject that potentially reflect emotions.
Inspired by psychological study [3], recent works [19,22, 23, 29, 56] have suggested that contextual information con-tributes to effective emotion cues for Context-Aware Emo-tion Recognition (CAER). The contexts are considered to include the place category, the place attributes, the objects, or the actions of others around the subject [20]. The major-ity of such research typically follows a common pipeline: (1) Obtaining the unimodal/multimodal representations of the recognized subject; (2) Building diverse contexts and extracting emotion-related representations; (3) Designing
it shows the presence of these scenes only in the positive or negative set of emotions. Concretely, for the EMOTIC dataset [20], about 40% of scene categories for anger have zero conditional entropy while about 45% of categories for happy (i.e., happiness) have zero conditional entropy. As an intuitive example, most party-related scene contexts are present in the samples with the happy category and almost non-existent in the negative categories. These observations confirm the severe context bias in CAER datasets, leading to distribution gaps in emotion categories across contexts and uneven visual representations.
Motivated by the above observation, we attempt to em-brace causal inference [31] to reveal the culprit that poi-sons the CAER models, rather than focusing on beating them. As a revolutionary scientific paradigm that facili-tates models toward unbiased prediction, the most impor-tant challenge in applying classical causal inference to the modern CAER task is how to reasonably depict true causal effects and identify the task-specific dataset bias. To this end, this paper attempts to address the challenge and rescue the bias-ridden models by drawing on human instincts, i.e., looking for the causality behind any association. Specifi-cally, we present a causality-based bias mitigation strategy.
We first formulate the procedure of the CAER task via a proposed causal graph. In this case, the harmful context bias in datasets is essentially an unintended confounder that misleads the models to learn the spurious correlation between similar contexts and specific emotion semantics.
From Figure 3, we disentangle the causalities among the input images X, subject features S, context features C, confounder Z, and predictions Y . Then, we propose a simple yet effective Contextual Causal Intervention Module (CCIM) to achieve context-deconfounded training and use the do-calculus P (Y |do(X)) to calculate the true causal ef-fect, which is fundamentally different from the conventional likelihood P (Y |X). CCIM is plug-in and model-agnostic, with the backdoor adjustment [14] to de-confound the con-founder and eliminate the impact of the context bias. We comprehensively evaluate the effectiveness and superiority of CCIM on three standard and biased CAER datasets. Nu-merous experiments and analyses demonstrate that CCIM can significantly and consistently improve existing base-lines, achieving a new state-of-the-art (SOTA).
The main contributions can be summarized as follows:
• To our best knowledge, we are the first to investigate the adverse context bias of the datasets in the CAER task from the causal inference perspective and iden-tify that such bias is a confounder, which misleads the models to learn the spurious correlation.
• We propose CCIM, a plug-in contextual causal inter-vention module, which could be inserted into most
CAER models to remove the side effect caused by the
Figure 2. We show a toy experiment on the EMOTIC [20] and CAER-S [22] datasets for scene categories of angry and happy emotions. More scene categories with normalized zero-conditional entropy reveal a strong presence of the context bias. fusion strategies to combine these features for emotion la-bel predictions. Although existing methods have improved modestly through complex module stacking [12,23,51] and tricks [16, 29], they invariably suffer from a context bias of the datasets, which has long been overlooked. Recall-ing the process of generating CAER datasets, different an-notators were asked to label each image according to what they subjectively thought people in the images with diverse contexts were feeling [20]. This protocol makes the prefer-ence of annotators inevitably affect the distribution of emo-tion categories across contexts, thereby leading to the con-text bias. Figure 1 illustrates how such bias confounds the predictions. Intrigued, most of the images in training data contain vegetated scenes with positive emotion categories, while negative emotions in similar contexts are almost non-existent. Therefore, the baseline [19] is potentially misled into learning the spurious dependencies between context-specific features and label semantics. When given test im-ages with similar contexts but negative emotion categories, the model inevitably infers the wrong emotional states.
More intrigued, a toy experiment is performed to ver-ify the strong bias in CAER datasets. This test aims to ob-serve how well emotions correlate with contexts (e.g., scene categories). Specifically, we employ the ResNet-152 [15] pre-trained on Places365 [58] to predict scene categories from images with three common emotion categories (i.e.,
“anger”, “happy”, and “fear”) across two datasets. The top 200 most frequent scenes from each emotion category are selected, and the normalized conditional entropy of each scene category across the positive and negative set of a spe-cific emotion is computed [30]. While analyzing correla-tions between scene contexts and emotion categories in Fig-ure 2 (e.g., “anger” and “happy”), we find that more scene categories with the zero conditional entropy are most likely to suggest the significant context bias in the datasets, as
confounder and facilitate a fair contribution of diverse contexts to emotion understanding.
• Extensive experiments on three standard CAER datasets show that the proposed CCIM can facilitate existing models to achieve unbiased predictions. 2.