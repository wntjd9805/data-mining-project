Abstract 1.

Introduction
We explore a new task for audio-visual-language model-ing called fine-grained audible video description (FAVD). It aims to provide detailed textual descriptions for the given audible videos, including the appearance and spatial lo-cations of each object, the actions of moving objects, and the sounds in videos. Existing visual-language modeling tasks often concentrate on visual cues in videos while un-dervaluing the language and audio modalities. On the other hand, FAVD requires not only audio-visual-language modeling skills but also paragraph-level language gener-ation abilities. We construct the first fine-grained audi-ble video description benchmark (FAVDBench) to facili-tate this research. For each video clip, we first provide a one-sentence summary of the video, i.e., the caption, fol-lowed by 4-6 sentences describing the visual details and 1-2 audio-related descriptions at the end. The descriptions are provided in both English and Chinese. We create two new metrics for this task: an EntityScore to gauge the com-pleteness of entities in the visual descriptions, and an Au-dioScore to assess the audio descriptions. As a prelimi-nary approach to this task, we propose an audio-visual-language transformer that extends existing video caption-ing model with an additional audio branch. We combine the masked language modeling and auto-regressive lan-guage modeling losses to optimize our model so that it can produce paragraph-level descriptions. We illustrate the efficiency of our model in audio-visual-language mod-eling by evaluating it against the proposed benchmark us-ing both conventional captioning metrics and our proposed metrics. We further put our benchmark to the test in video generation models, demonstrating that employing fine-grained video descriptions can create more intricate videos than using captions. Code and dataset are available at https://github.com/OpenNLPLab/FAVDBench. Our online benchmark is available at www.avlbench.opennlplab.cn.
⋆These authors have equal contributions. (cid:0)Yiran Zhong is the corre-sponding author (e-mail: zhongyiran@gmail.com).
Language serves as the primary form of human commu-nication, providing not only complementary information to other modalities such as vision and audio, but also an ef-ficient means of exchanging information [17, 27, 46]. For example, we can use voice navigation to guide us to our destination. Visually impaired people can watch a movie by listening to its narration. The former shows that the language can provide complementary information to other modalities, while the latter indicates that the language can carry the most information in other modalities.
Recent multi-modal modeling tasks attempt to connect the language to other modalities, including image/video captioning [16, 32, 45, 48, 51, 64, 65, 74, 75, 84, 91], text-to-image/video synthesis [15,19–21,58,59,62,63,73,79], text-driven image/video manipulation [3, 30, 44, 87, 88], and etc.
However, in these tasks, since the language is frequently used to provide complementary information to other modal-ities, they often fail to give a fine-grained description of the exchange of information between modalities, and only con-sider the simplified language, i.e., one-sentence captions.
Due to the conciseness of captions, only salient objects and activities have got descriptions. As a result, the information carried by the captions will be much less than that carried by other modalities, causing significant information loss when exchanging information from other modalities to language.
In this paper, we consider language as a tool for con-veying information from other modalities in multi-modal modeling and propose a new task called Fine-grained Audi-ble Video Description (FAVD). We compare it with existing captioning tasks in Fig. 1. Video captioning aims to gen-erate one concise sentence to summarize the whole video.
Dense video captioning first detects the multiple temporal events and then provides one caption for each event. The generated sentences of these two tasks merely cover the main objects or movements while losing many details when translating the video information into language. Unlike this, the FAVD task requires a model to describe videos in a simi-lar way that humans do, i.e., starting with an overview of the video and then focusing on each fine-grained detail to con-Figure 1. Comparison of the proposed FAVD task with existing captioning tasks. (a) Video captioning (VC) uses one sentence to describe the main content of the video. (b) Dense video captioning aims to localize the multiple temporal events and generate corresponding descriptions. Both VC and DVC describe the salient events in videos while losing many details, such as the appearance of objects, spatial relations, and sounds. (c) The proposed FAVD tries to generate a paragraph-level description that contains the caption, named as Summary, and the audio-visual descriptions, abbreviated as A. and V.. crete the description. In this case, most video information can be preserved in the language modality. Since a video contains both visual and audio signals, we also include au-dio descriptions in our task.
FAVD is a non-trivial task. In addition to long-text mod-eling abilities, it calls for a finer-grained visual understand-ing than earlier video captioning tasks, i.e., it needs to rec-ognize all objects and actions in videos, as well as de-scribe the appearance and spatial locations of each object and the sounds in videos. To facilitate this task, we con-struct the first fine-grained audible video description bench-mark (FAVDBench), which allows the model to be trained in a supervised manner.
FAVDBench is made up of over 11,000 video clips culled from millions of YouTube videos and each clip is sourced by querying more than 70 life-related categories to fulfill the diversity of the benchmark. We annotate the video descrip-tions based on human behaviors. The annotations of each video clip begin with a one-sentence caption that describes the salient objects and activities, followed by 4-6 sentences that describe visual details including the appearance and spatial locations of each object, and the actions or move-ments of moving objects. To make the task consider au-dio information, we include 1-2 sentences of audio-related descriptions at the end of the whole descriptions. The de-scriptions are provided in both English and Chinese with human translation. An annotation example can be found in
Fig. 1 and Fig. 3. We design two new metrics for the FAVD task. One called EntityScore, which assesses the complete-ness of the information that is transferred from the videos to the descriptions by gauging the completeness of entities in the visual descriptions. The other is AudioScore, which measures the audio description in the feature space of a pre-trained audio-visual-language model.
We provide a baseline model for this new task. The model is based on an existing end-to-end video caption-ing model [35] with an additional audio branch. We also extend the visual-language transformer to the audio-visual-language transformer in order to mix multi-modal tokens.
Existing video captioning models often adopt Masked Lan-guage Modelling (MLM) loss to optimize the caption gen-eration. However, because our task requires the model to describe fine-grained details, the model should also be ca-pable of general language modeling. To this end, we in-clude the auto-regressive language modeling (ALM) loss in our baseline model.
We extensively evaluate the baseline model against the proposed FAVDBench using both the conventional caption-ing metrics and our proposed metrics and demonstrate the effectiveness of our model in audio-visual-language model-ing and fine-grained description generation. To qualitatively illustrate the information preservation in modality informa-tion exchange, we further put our benchmark to the test in video generation models, showing more intricate videos than using captions. 2.