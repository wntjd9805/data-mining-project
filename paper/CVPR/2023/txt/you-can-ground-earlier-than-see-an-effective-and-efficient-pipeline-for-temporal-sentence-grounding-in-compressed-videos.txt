Abstract
Given an untrimmed video, temporal sentence ground-ing (TSG) aims to locate a target moment semantically ac-cording to a sentence query. Although previous respectable works have made decent success, they only focus on high-level visual features extracted from the consecutive de-coded frames and fail to handle the compressed videos for query modelling, suffering from insufﬁcient representation capability and signiﬁcant computational complexity during training and testing.
In this paper, we pose a new set-ting, compressed-domain TSG, which directly utilizes com-pressed videos rather than fully-decompressed frames as the visual input. To handle the raw video bit-stream in-put, we propose a novel Three-branch Compressed-domain
Spatial-temporal Fusion (TCSF) framework, which extracts and aggregates three kinds of low-level visual features (I-frame, motion vector and residual features) for effective and efﬁcient grounding. Particularly, instead of encoding the whole decoded frames like previous works, we capture the appearance representation by only learning the I-frame feature to reduce delay or latency. Besides, we explore the motion information not only by learning the motion vector feature, but also by exploring the relations of neighboring frames via the residual feature. In this way, a three-branch spatial-temporal attention layer with an adaptive motion-appearance fusion module is further designed to extract and aggregate both appearance and motion information for the ﬁnal grounding. Experiments on three challenging datasets shows that our TCSF achieves better performance than other state-of-the-art methods with lower complexity. 1.

Introduction
As a signiﬁcant yet challenging computer vision task, temporal sentence grounding (TSG) has drawn increasing
⇤Equal contributions.
†Corresponding author.
Figure 1. (a) Example of the temporal sentence grounding (TSG). (b) Comparison between previous supervised TSG models and our compressed-domain TSG model. Previous models ﬁrst decode the video into consecutive frames and then feed them into their net-works, while our compressed-domain model directly leverages the compressed video as the visual input. attention due to its various applications, such as video un-derstanding [12–17, 25, 44, 76, 77, 79–81, 84] and temporal action localization [63, 71]. Given a long untrimmed video, the TSG task aims to locate the speciﬁc start and end times-tamps of a video segment with an activity that semantically corresponds to a given sentence query. As shown in Fig-ure 1(a), most of video contents are query-irrelevant, where only a short video segment matches the query.
It is sub-stantially more challenging since a well-designed method needs to not only model the complex multi-modal interac-tion among video and query, but also capture complicated context information for cross-modal semantics alignment.
By treating a video as a sequence of independent frames, most TSG methods [3,17,29,34,36–41,43,46,65,90,93,98] refer to the fully-supervised setting, where each frame is
ﬁrstly fully decompressed from a video bit-stream and then manually annotated as query-relevant or query-irrelevant.
Despite the decent progress on the grounding performance, these data-hungry methods severely rely on the fully de-compression and numerous annotations, which are signif-icantly labor-intensive and time-consuming to obtain from real-word applications. To alleviate this dense reliance to a certain extent, some weakly-supervised works [8,11,16,33, 35, 49, 51, 62, 64, 96, 97] are proposed to only leverage the coarse-grained video-query annotations instead of the ﬁne-grained frame-query annotations. Unfortunately, this weak supervision still requires the fully-decompressed video for visual feature extraction.
Based on the above observation, in this paper, we make the ﬁrst attempt to explore if an effective and efﬁcient
TSG model can be learned without the limitation of the fully decompressed video input. Considering that the real-world video always stored and transmitted in a compressed data format, we explore a more practical but challenging task: compressed-domain TSG, which directly leverages the compressed video instead of obtaining consecutive de-coded frames as visual input for grounding. As shown in the Figure 1(b), a compressed video is generally parsed by a stream of Group of successive Pictures (GOPs) and each
GOP starts with one intra-frame (I-frame) followed by a variable number of predictive frames (P-frames) [30, 74].
Speciﬁcally, the I-frame contains complete RGB informa-tion of a video frame, while each P-frame contains a mo-tion vector and a residual. The motion vectors store 2D dis-placements between I-frame and its neighbor frames, and the residuals store the RGB differences between I-frame and its reconstructed frame calculated by Motion Vectors in the
P-frames after motion compensation. The I-frame can be decoded itself, while these P-frames only store the changes from the previous I-frame by motion vectors and residuals.
Given the compressed video, our main challenge is how to effectively and efﬁciently extract contextual visual fea-tures from the above three low-level visual information for query alignment. Existing TSG works [3, 29, 34, 39, 65, 90, 93, 98] cannot be applied directly to the compressed video because their video features (e.g., C3D and I3D) can only be extracted if all complete video frames are available after de-compression. Moreover, decompressing all the frames will signiﬁcantly increase computational complexity for feature extraction, leading to extra latency and extensive storage.
To address this challenging task, we propose the ﬁrst and novel approach for compressed-domain TSG, called
Three-branch Compressed-domain Spatial-temporal Fusion (TCSF). Given a group of successive picture (GOP) in a compressed video, we ﬁrst extract the visual features from each I-frame to represent the appearance at its timestamp, and then extract the features of its P-frames to capture the motion information near the I-frame. In this way, we can model the activity content with above simple I-frame and P-frames instead of using their corresponding consecutive de-coded frames. Speciﬁcally, we design a spatial attention and a temporal attention to integrate the appearance and motion features for activity modelling. To adaptively handle differ-ent fast-motion (P-frame guided) or slow-motion (I-frame guided) cases, we further design an adaptive appearance and motion fusion module to integrate the appearance and mo-tion information by learning a balanced weight through a residual module. Finally, a query-guided multi-modal fu-sion is exploited to integrate the visual and textual features for ﬁnal grounding.
Our contributions are summarized as follows:
• We propose a brand-new and challenging task: compressed-domain TSG, which aims to directly leverage the compressed video for TSG. To our best knowledge, we make the ﬁrst attempt to locate the tar-get segment in the compressed video.
• We present a novel pipeline for compressed-domain
TSG, which can efﬁciently and effectively integrate both appearance and motion information from the low-level visual information in the compressed video.
• Extensive experiments on three challenging datasets (ActivityNet Captions, Charades-STA and TACoS) validate the effectiveness and efﬁciency of our TCSF. 2.