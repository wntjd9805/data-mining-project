Abstract
Bird’s-Eye-View (BEV) semantic maps have become an essential component of automated driving pipelines due to the rich representation they provide for decision-making tasks. However, existing approaches for generating these maps still follow a fully supervised training paradigm and hence rely on large amounts of annotated BEV data. In this work, we address this limitation by proposing the first self-supervised approach for generating a BEV semantic map using a single monocular image from the frontal view (FV).
During training, we overcome the need for BEV ground truth annotations by leveraging the more easily available FV se-mantic annotations of video sequences. Thus, we propose the SkyEye architecture that learns based on two modes of self-supervision, namely, implicit supervision and explicit supervision. Implicit supervision trains the model by enforc-ing spatial consistency of the scene over time based on FV semantic sequences, while explicit supervision exploits BEV pseudolabels generated from FV semantic annotations and self-supervised depth estimates. Extensive evaluations on the KITTI-360 dataset demonstrate that our self-supervised approach performs on par with the state-of-the-art fully su-pervised methods and achieves competitive results using only 1% of direct supervision in BEV compared to fully su-pervised approaches. Finally, we publicly release both our code and the BEV datasets generated from the KITTI-360 and Waymo datasets. 1.

Introduction
Bird’s-Eye-View (BEV) maps are an integral part of an autonomous driving pipeline as they allow the vehi-cle to perceive the environment using a feature-rich yet computationally-efficient representation. These maps cap-ture both static and dynamic obstacles in the scene while encoding their absolute distances in the metric scale using a low-cost 2D representation. Such characteristics allow them
*Equal contribution
Figure 1. SkyEye: The first self-supervised framework for semantic BEV mapping. We use sequences of FV semantic annotations to train the network to estimate a semantic map in BEV using a single RGB input. to be used in many distance-based time-sensitive applications such as trajectory estimation and collision avoidance [12,14].
Existing approaches that estimate BEV maps from frontal view (FV) images and/or LiDAR scans require large datasets annotated in the BEV as they are trained in a fully super-vised manner [6, 19, 23, 43]. However, BEV ground truth generation relies on the presence of HD maps, annotated 3D point clouds, and/or 3D bounding boxes, which are ex-tremely arduous to obtain [27]. Recent approaches [29, 36] circumvent this problem of requiring BEV ground truths by leveraging data from simulation environments. However, these approaches suffer from the large domain gap between simulated and real-world images, which results in their re-duced performance in the real world.
In this work, we address the aforementioned limitations by proposing SkyEye, the first self-supervised learning frame-work for generating an instantaneous semantic map in BEV, given a single monocular FV image. During training, our approach, depicted in Fig. 1, overcomes the need for BEV ground truths by leveraging FV semantic ground truth labels along with the spatial and temporal consistency offered by video sequences. FV semantic ground truth labels can easily be obtained with reduced human annotation effort due to the relatively small domain gap between FV images of different datasets which allows for efficient label transfer [15, 17, 37].
Additionally, no range sensor is required for data recording.
During inference, our model only uses a single monocular
FV image to generate the semantic map in BEV.
Our proposed self-supervised learning framework lever-ages two supervision signals, namely, implicit and explicit supervision. Implicit supervision generates the training sig-nal by enforcing spatial and temporal consistency of the scene. To this end, our model generates the FV semantic predictions for the current and future time steps using the
FV image of only the current time step. These predictions are supervised using the corresponding ground truth labels in FV. Explicit supervision, in contrast, supervises the net-work using BEV semantic pseudolabels generated from FV semantic ground truths using a self-supervised depth estima-tion network augmented with a dedicated post-processing procedure. We perform extensive evaluations of SkyEye on the KITTI-360 dataset and demonstrate its generalizability on the Waymo dataset. Results demonstrate that SkyEye performs on par with the state-of-the-art fully-supervised approaches and achieves competitive performance with only 1% of pseudolabels in BEV. Further, we outperform all base-line methods w.r.t. generalization capabilities.
Our main contributions can thus be stated as follows:
• The first self-supervised framework for generating se-mantic BEV maps from monocular FV images.
• An implicit supervision strategy that leverages seman-tic annotations in FV to encode semantic and spatial information into a latent voxel grid.
• A pseudolabel generation pipeline to create BEV pseu-dolabels from FV semantic ground truth labels.
• A novel semantic BEV dataset derived from Waymo.
• Extensive evaluations as well as ablation studies to show the impact of our contributions.
• Publicly available code for our SkyEye framework at http://skyeye.cs.uni-freiburg.de. 2.