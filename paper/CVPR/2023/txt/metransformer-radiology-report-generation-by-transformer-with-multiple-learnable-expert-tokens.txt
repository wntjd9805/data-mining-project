Abstract
In clinical scenarios, multi-specialist consultation could significantly benefit the diagnosis, especially for intricate cases. This inspires us to explore a “multi-expert joint diagnosis” mechanism to upgrade the existing “single ex-pert” framework commonly seen in the current literature.
To this end, we propose METransformer, a method to real-ize this idea with a transformer-based backbone. The key design of our method is the introduction of multiple learn-able “expert” tokens into both the transformer encoder and decoder. In the encoder, each expert token interacts with both vision tokens and other expert tokens to learn to attend different image regions for image representation. These ex-pert tokens are encouraged to capture complementary in-formation by an orthogonal loss that minimizes their over-lap. In the decoder, each attended expert token guides the cross-attention between input words and visual tokens, thus influencing the generated report. A metrics-based expert voting strategy is further developed to generate the final re-port. By the multi-experts concept, our model enjoys the merits of an ensemble-based approach but through a man-ner that is computationally more efficient and supports more sophisticated interactions among experts. Experimental re-sults demonstrate the promising performance of our pro-posed model on two widely used benchmarks. Last but not least, the framework-level innovation makes our work ready to incorporate advances on existing “single-expert” models to further improve its performance. 1.

Introduction
Interpreting radiology images (e.g., chest X-ray) and writing diagnostic reports are essential operations in clinical practice and normally require a considerable manual work-load. Therefore, radiology report generation, which aims to automatically generate a free-text description based on a radiograph, is highly desired to ease the burden of radiolo-gists while maintaining the quality of health care. Recently, substantial progress has been made towards research on au-tomated radiology report generation models [4,5,15–17,21, 22,33–35,41,43,44,47]. Most existing studies adopt a con-ventional encoder-decoder architecture following the image captioning paradigm [6, 25, 32, 37, 48] and resort to opti-mizing network structure or introducing external or prior information to aid report generation. These methods, in this paper, are collectively referred to as “single-expert” based diagnostic captioning methods.
However, diagnostic report generation is a very challeng-ing task as disease anomalies usually only occupy a small portion of the whole image and could appear at arbitrary lo-cations. Due to the fine-grained nature of radiology images, it is hard to focus on the correct image regions throughout the report generation procedure despite different attentions developed in recent works [16, 21]. Meanwhile, it is no-ticed that in clinic scenarios, multi-specialist consultation is especially beneficial for those intricate diagnostic cases that challenge a single specialist for a comprehensive and accu-rate diagnosis. The above observations led us to think, could we design a model to simulate the multi-specialist consul-tation scenario? Based on this motivation, we propose a new diagnostic captioning framework, METransformer, to mimic the “multi-expert joint diagnosis” process. Built upon a transformer backbone, METransformer introduces multiple “expert tokens”, representing multiple experts, into both the transformer encoder and decoder. Each expert to-ken learns to attend distinct image regions and interacts with other expert tokens to capture reliable and complementary visual information and produce a diagnosis report in paral-lel. The optimal report is selected through an expert voting strategy to produce the final report. Our design is based on
the assumption that it would be easier for multiple experts than a single one to capture visual patterns of clinical im-portance, which is verified by our experimental results.
Specifically, we feed both the expert tokens (learnable embeddings) and the visual tokens (image patches embed-dings) into the Expert Transformer encoder which is com-prised of a vision transformer (ViT) encoder and a bilinear transformer encoder. In ViT encoder, each expert token in-teracts not only with the visual tokens but also with the other expert tokens. Further, in the bilinear transformer encoder, to enable each “expert” to capture fine-grained image in-formation, we compute higher-order attention between ex-pert tokens and visual tokens, which has proved to be effec-tive in fine-grained classification tasks [20]. It is notewor-thy that the expert tokens in the encoder are encouraged to learn complementary representations by an orthogonal loss so that they attend differently to different image regions.
With these carefully learned expert token embeddings, in the decoder, we take them as a guide to regulate the learn-ing of word embeddings and visual token embedding in the report generation process. This results in M different word and visual token embeddings, thus producing M candidate reports, where M is the number of experts. We further pro-pose a metric-based expert voting strategy to generate the final report from the M candidates.
By utilizing multiple experts, our model, to some extent, is analogous to ensemble-based approaches, while each ex-pert token corresponds to an individual model. While it en-joys the merits of ensemble-based approaches, our model is designed in a manner that is computationally more efficient and supports more sophisticated interactions among ex-perts. Therefore, it can scale up with only a trivial increase of model parameters and achieves better performance, as demonstrated in our experimental study.
Our main contributions are summarized as follows.
First, we propose a new diagnostic captioning frame-work, METransformer, which is conceptually “multi-expert joint diagnosis” for radiology report generation, by intro-ducing learnable expert tokens and encouraging them to learn complementary representations using both linear and non-linear attentions.
Second, our model enjoys the benefits of an ensemble approach. Thanks to the carefully designed network struc-ture and the end-to-end training manner, our model can achieve better results than common ensemble approaches while greatly reducing training parameters and improving training efficiency.
Third, our approach shows promising performance on two widely used benchmarks IU-Xray and MIMIC-CXR over multiple state-of-the-art methods. The clinic relevance of the generated reports is also analyzed. 2.