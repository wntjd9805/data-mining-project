Abstract
Vision transformers have shown great success due to their high model capabilities. However, their remarkable performance is accompanied by heavy computation costs, which makes them unsuitable for real-time applications. In this paper, we propose a family of high-speed vision trans-formers named EfficientViT. We find that the speed of ex-isting transformer models is commonly bounded by mem-ory inefficient operations, especially the tensor reshaping and element-wise functions in MHSA. Therefore, we design a new building block with a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN layers, which improves memory efficiency while enhancing channel communication. Moreover, we discover that the attention maps share high similarities across heads, leading to com-putational redundancy. To address this, we present a cas-caded group attention module feeding attention heads with different splits of the full feature, which not only saves com-putation cost but also improves attention diversity. Compre-hensive experiments demonstrate EfficientViT outperforms existing efficient models, striking a good trade-off between speed and accuracy. For instance, our EfficientViT-M5 sur-passes MobileNetV3-Large by 1.9% in accuracy, while get-ting 40.4% and 45.2% higher throughput on Nvidia V100
GPU and Intel Xeon CPU, respectively. Compared to the recent efficient model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while running 5.8×/3.7× faster on the GPU/CPU, and 7.4× faster when converted to
ONNX format. Code and models are available at here. 1.

Introduction
Vision Transformers (ViTs) have taken computer vision domain by storm due to their high model capabilities and superior performance [18, 44, 69]. However, the constantly improved accuracy comes at the cost of increasing model sizes and computation overhead. For example, SwinV2 [43] uses 3.0B parameters, while V-MoE [62] taking 14.7B pa-rameters, to achieve state-of-the-art performance on Ima-∗Work done when Xinyu was an intern of Microsoft Research.
Figure 1. Speed and accuracy comparisons between EfficientViT (Ours) and other efficient CNN and ViT models tested on an
Nvidia V100 GPU with ImageNet-1K dataset [17]. geNet [17]. Such large model sizes and the accompanying heavy computational costs make these models unsuitable for applications with real-time requirements [40, 78, 86].
There are several recent works designing light and effi-cient vision transformer models [9,19,29,49,50,56,79,81].
Unfortunately, most of these methods aim to reduce model parameters or Flops, which are indirect metrics for speed and do not reflect the actual inference throughput of models.
For example, MobileViT-XS [50] using 700M Flops runs much slower than DeiT-T [69] with 1,220M Flops on an
Nvidia V100 GPU. Although these methods have achieved good performance with fewer Flops or parameters, many of them do not show significant wall-clock speedup against standard isomorphic or hierarchical transformers, e.g., DeiT
[69] and Swin [44], and have not gained wide adoption.
To address this issue, in this paper, we explore how to go faster with vision transformers, seeking to find princi-ples for designing efficient transformer architectures. Based on the prevailing vision transformers DeiT [69] and Swin
[44], we systematically analyze three main factors that af-fect model inference speed, including memory access, com-putation redundancy, and parameter usage.
In particular, we find that the speed of transformer models is commonly memory-bound.
In other words, memory accessing de-lay prohibits the full utilization of the computing power in GPU/CPUs [21, 32, 72], leading to a critically negative impact on the runtime speed of transformers [15, 31]. The
most memory-inefficient operations are the frequent tensor reshaping and element-wise functions in multi-head self-attention (MHSA). We observe that through an appropri-ate adjustment of the ratio between MHSA and FFN (feed-forward network) layers, the memory access time can be re-duced significantly without compromising the performance.
Moreover, we find that some attention heads tend to learn similar linear projections, resulting in redundancy in atten-tion maps. The analysis shows that explicitly decomposing the computation of each head by feeding them with diverse features can mitigate this issue while improving computa-tion efficiency. In addition, the parameter allocation in dif-ferent modules is often overlooked by existing lightweight models, as they mainly follow the configurations in stan-dard transformer models [44,69]. To improve parameter ef-ficiency, we use structured pruning [45] to identify the most important network components, and summarize empirical guidance of parameter reallocation for model acceleration.
Based upon the analysis and findings, we propose a new family of memory efficient transformer models named Effi-cientViT. Specifically, we design a new block with a sand-wich layout to build up the model. The sandwich layout block applies a single memory-bound MHSA layer between
FFN layers. It reduces the time cost caused by memory-bound operations in MHSA, and applies more FFN layers to allow communication between different channels, which is more memory efficient. Then, we propose a new cascaded group attention (CGA) module to improve computation ef-ficiency. The core idea is to enhance the diversity of the fea-tures fed into the attention heads. In contrast to prior self-attention using the same feature for all heads, CGA feeds each head with different input splits and cascades the out-put features across heads. This module not only reduces the computation redundancy in multi-head attention, but also elevates model capacity by increasing network depth. Last but not least, we redistribute parameters through expanding the channel width of critical network components such as value projections, while shrinking the ones with lower im-portance like hidden dimensions in FFNs. This reallocation finally promotes model parameter efficiency.
Experiments demonstrate that our models achieve clear improvements over existing efficient CNN and ViT models in terms of both speed and accuracy, as shown in Fig. 1.
For instance, our EfficientViT-M5 gets 77.1% top-1 accu-racy on ImageNet with throughput of 10,621 images/s on an
Nvidia V100 GPU and 56.8 images/s on an Intel Xeon E5-2690 v4 CPU @ 2.60GHz, outperforming MobileNetV3-Large [26] by 1.9% in accuracy, 40.4% in GPU inference speed, and 45.2% in CPU speed. Moreover, EfficientViT-M2 gets 70.8% accuracy, surpassing MobileViT-XXS [50] by 1.8%, while running 5.8×/3.7× faster on the GPU/CPU, and 7.4× faster when converted to ONNX [3] format. When deployed on the mobile chipset, i.e., Apple A13 Bionic chip
Figure 2. Runtime profiling on two standard vision transformers
Swin-T and DeiT-T. Red text denotes memory-bound operations, i.e., the time taken by the operation is mainly determined by mem-ory accesses, while time spent in computation is much smaller. in iPhone 11, EfficientViT-M2 model runs 2.3× faster than
MobileViT-XXS [50] using the CoreML [1].
In summary, the contributions of this work are two-fold:
• We present a systematic analysis on the factors that affect the inference speed of vision transformers, de-riving a set of guidelines for efficient model design.
• We design a new family of vision transformer models, which strike a good trade-off between efficiency and accuracy. The models also demonstrate good transfer ability on a variety of downstream tasks. 2. Going Faster with Vision Transformers
In this section, we explore how to improve the efficiency of vision transformers from three perspectives: memory ac-cess, computation redundancy, and parameter usage. We seek to identify the underlying speed bottlenecks through empirical studies, and summarize useful design guidelines. 2.1. Memory Efficiency
Memory access overhead is a critical factor affecting model speed [15, 28, 31, 65]. Many operators in transformer
[71], such as frequent reshaping, element-wise addition, and normalization are memory inefficient, requiring time-consuming access across different memory units, as shown in Fig. 2. Although there are some methods proposed to ad-dress this issue by simplifying the computation of standard softmax self-attention, e.g., sparse attention [34, 57, 61, 75] and low-rank approximation [11,51,74], they often come at the cost of accuracy degradation and limited acceleration.
In this work, we turn to save memory access cost by reducing memory-inefficient layers. Recent studies reveal that memory-inefficient operations are mainly located in
MHSA rather than FFN layers [31, 33]. However, most ex-isting ViTs [18, 44, 69] use an equivalent number of these two layers, which may not achieve the optimal efficiency.
We thereby explore the optimal allocation of MHSA and
FFN layers in small models with fast inference. Specifi-cally, we scale down Swin-T [44] and DeiT-T [69] to several small subnetworks with 1.25× and 1.5× higher inference throughput, and compare the performance of subnetworks with different proportions of MHSA layers. As shown in
Fig. 3, subnetworks with 20%-40% MHSA layers tend to get better accuracy. Such ratios are much smaller than the
Figure 3. The accuracy of downscaled baseline models with dif-ferent MHSA layer proportions, where the dots on each line rep-resent subnetworks with similar throughput. Left: Swin-T as the baseline. Right: DeiT-T as the baseline. The 1.25×/1.5× denote accelerating the baseline models by 1.25/1.5 times, respectively.
Figure 4. The average maximum cosine similarity of each head in different blocks. Left: downscaled Swin-T models. Right: down-scaled DeiT-T models. Blue lines denote Swin-T-1.25×/DeiT-T-1.25× model, while darkblue lines denote the variants that feed each head with only a split of the full feature. typical ViTs that adopt 50% MHSA layers. Furthermore, we measure the time consumption on memory-bound op-erations to compare memory access efficiency, including reshaping, element-wise addition, copying, and normaliza-tion. Memory-bound operations is reduced to 44.26% of the total runtime in Swin-T-1.25× that has 20% MHSA lay-ers. The observation also generalizes to DeiT and smaller models with 1.5× speed-up. It is demonstrated that reduc-ing MHSA layer utilization ratio appropriately can enhance memory efficiency while improving model performance. 2.2. Computation Efficiency
MHSA embeds the input sequence into multiple sub-spaces (heads) and computes attention maps separately, which has been proven effective in improving performance
[18, 69, 71]. However, attention maps are computationally expensive, and studies have shown that a number of them are not of vital importance [52, 73]. To save computation cost, we explore how to reduce redundant attention in small
ViT models. We train width downscaled Swin-T [44] and
DeiT-T [69] models with 1.25× inference speed-up, and measure the maximum cosine similarity of each head and the remaining heads within each block. From Fig. 4, we ob-serve there exists high similarities between attention heads, especially in the last blocks. The phenomenon suggests that many heads learn similar projections of the same full fea-ture and incur computation redundancy. To explicitly en-courage the heads to learn different patterns, we apply an intuitive solution by feeding each head with only a split of the full feature, which is similar to the idea of group con-volution in [10, 87]. We train the variants of downscaled models with the modified MHSA, and also compute the at-tention similarities in Fig. 4. It is shown that using different channel-wise splits of the feature in different heads, instead of using the same full feature for all heads as MHSA, could effectively mitigate attention computation redundancy. 2.3. Parameter Efficiency
Typical ViTs mainly inherit the design strategies from
NLP transformer [71], e.g., using an equivalent width for
Q,K,V projections, increasing heads over stages, and set-ting the expansion ratio to 4 in FFN. For lightweight mod-Figure 5. The ratio of the channels to the input embeddings before and after pruning Swin-T. Baseline accuracy: 79.1%; pruned ac-curacy: 76.5%. Results for DeiT-T are given in the supplementary. els, the configurations of these components need to be care-fully re-designed [7, 8, 39]. Inspired by [45, 82], we adopt
Taylor structured pruning [53] to automatically find the im-portant components in Swin-T and DeiT-T, and explore the underlying principles of parameter allocation. The pruning method removes unimportant channels under a certain re-source constraint and keeps the most critical ones to best preserve the accuracy. It uses the multiplication of gradient and weight as channel importance, which approximates the loss fluctuation when removing channels [38].
The ratio between the remaining output channels to the input channels is plotted in Fig. 5, and the original ratios in the unpruned model are also given for reference.
It is observed that: 1) The first two stages preserve more dimen-sions, while the last stage keeps much less; 2) The Q,K and
FFN dimensions are largely trimmed, whereas the dimen-sion of V is almost preserved and diminishes only at the last few blocks. These phenomena show that 1) the typical channel configuration, that doubles the channel after each stage [44] or use equivalent channels for all blocks [69], may produce substantial redundancy in last few blocks; 2)
The redundancy in Q,K is much larger than V when they have the same dimensions. V prefers a relative large chan-nels, being close to the input embedding dimension. 3. Efficient Vision Transformer
Based upon the above analysis, in this section, we pro-pose a new hierarchical model with fast inference named
EfficientViT. The architecture overview is shown in Fig. 6.
Figure 6. Overview of EfficientViT. (a) Architecture of EfficientViT; (b) Sandwich Layout block; (c) Cascaded Group Attention. 3.1. EfficientViT Building Blocks
We propose a new efficient building block for vision transformer, as shown in Fig. 6 (b). It is composed of a memory-efficient sandwich layout, a cascaded group atten-tion module, and a parameter reallocation strategy, which focus on improving model efficiency in terms of memory, computation, and parameter, respectively.
Sandwich Layout. To build up a memory-efficient block, we propose a sandwich layout that employs less memory-bound self-attention layers and more memory-efficient FFN layers for channel communication. Specifically, it applies a single self-attention layer ΦA for spatial mixing, which is i sandwiched between FFN layers ΦF i . The computation can be formulated as:
N (cid:89)
N (cid:89) (1)
ΦF
ΦF
Xi+1 = i (Xi))), i (ΦA i ( where Xi is the full input feature for the i-th block. The block transforms Xi into Xi+1 with N FFNs before and af-ter the single self-attention layer. This design reduces the memory time consumption caused by self-attention layers in the model, and applies more FFN layers to allow com-munication between different feature channels efficiently.
We also apply an extra token interaction layer before each
FFN using a depthwise convolution (DWConv) [27]. It in-troduces inductive bias of the local structural information to enhance model capability [14].
Cascaded Group Attention. Attention head redundancy is a severe issue in MHSA, which causes computation inef-ficiency. Inspired by group convolutions in efficient CNNs
[10, 37, 64, 87], we propose a new attention module named cascaded group attention (CGA) for vision transformers. It feeds each head with different splits of the full features, thus explicitly decomposing the attention computation across heads. Formally, this attention can be formulated as: (cid:101)Xij = Attn(XijW Q ij , XijW V ij , XijW K (cid:101)Xi+1 = Concat[ (cid:101)Xij]j=1:hW P i , ij ), (2) where the j-th head computes the self-attention over Xij, which is the j-th split of the input feature Xi, i.e., Xi =
[Xi1, Xi2, . . . , Xih] and 1 ≤ j ≤ h. h is the total number of heads, W Q ij are projection layers mapping the input feature split into different subspaces, and W P is i a linear layer that projects the concatenated output features back to the dimension consistent with the input. ij , and W V ij , W K
Although using feature splits instead of the full features for each head is more efficient and saves computation over-head, we continue to improve its capacity, by encouraging the Q, K, V layers to learn projections on features with richer information. We compute the attention map of each head in a cascaded manner, as illustrated in Fig. 6 (c), which adds the output of each head to the subsequent head to refine the feature representations progressively:
′
′
X 1 < j ≤ h, ij = Xij + (cid:101)Xi(j−1), (3) where X ij is the addition of the j-th input split Xij and the (j−1)-th head output (cid:101)Xi(j−1) calculated by Eq. (2). It re-places Xij to serve as the new input feature for the j-th head when calculating the self-attention. Besides, another token
Table 1. Architecture details of EfficientViT model variants.
Model
{C1, C2, C3} {L1, L2, L3} {H1, H2, H3}
{64, 128, 192}
EfficientViT-M0
EfficientViT-M1 {128, 144, 192}
EfficientViT-M2 {128, 192, 224}
EfficientViT-M3 {128, 240, 320}
EfficientViT-M4 {128, 256, 384}
EfficientViT-M5 {192, 288, 384}
{1, 2, 3}
{1, 2, 3}
{1, 2, 3}
{1, 2, 3}
{1, 2, 3}
{1, 3, 4}
{4, 4, 4}
{2, 3, 3}
{4, 3, 2}
{4, 3, 4}
{4, 4, 4}
{3, 3, 4} interaction layer is applied after the Q projection, which enables the self-attention to jointly capture local and global relations and further enhances the feature representation.
Such a cascaded design enjoys two advantages. First, feeding each head with different feature splits could im-prove the diversity of attention maps, as validated in Sec. 2.2. Similar to group convolutions [10, 87], the cascaded group attention could save the Flops and parameters by h×, since the input and output channels in the QKV layers are reduced by h×. Second, cascading the attention heads al-lows for an increase of network depth, thus further elevat-ing the model capacity without introducing any extra pa-rameters. It only incurs minor latency overhead since the attention map computation in each head uses smaller QK channel dimensions.
Parameter Reallocation. To improve parameter effi-ciency, we reallocate the parameters in the network by ex-panding the channel width of critical modules while shrink-ing the unimportant ones. Specifically, based on the Tay-lor importance analysis in Sec. 2.3, we set small channel dimensions for Q and K projections in each head for all stages. For the V projection, we allow it to have the same dimension as the input embedding. The expansion ratio in
FFN is also reduced from 4 to 2 due to its parameter redun-dancy. With the proposed reallocation strategy, the impor-tant modules have larger number of channels to learn rep-resentations in a high dimensional space, which prevent the loss of feature information. Meanwhile, the redundant pa-rameters in unimportant modules are removed to speed up inference and enhance the model efficiency. 3.2. EfficientViT Network Architectures
The overall architecture of our EfficientViT is presented in Fig. 6 (a). Concretely, we introduce overlapping patch embedding [20, 80] to embed 16×16 patches into tokens with C1 dimension, which enhances the model capacity in low-level visual representation learning. The architecture contains three stages. Each stage stacks the proposed Ef-ficientViT building blocks and the number of tokens is re-duced by 4× at each subsampling layer (2× subsampling of the resolution). To achieve efficient subsampling, we propose an EfficientViT subsample block which also has the sandwich layout, except that the self-attention layer is replaced by an inverted residual block to reduce the infor-mation loss during subsampling [26, 63]. It is worth not-ing that we adopt BatchNorm (BN) [30] instead of Layer-Norm (LN) [2] throughout the model, as BN can be folded into the preceding convolution or linear layers, which is a runtime advantage over LN. We also use ReLU [54] as the activation function, as the commonly used GELU [25] or
HardSwish [26] are much slower, and sometimes not well-supported by certain inference deployment platforms [1, 3].
We build our model family with six different width and depth scales, and set different number of heads for each stage. We use fewer blocks in early stages than late stages similar to MobileNetV3 [26] and LeViT [20], since that the processing on early stages with larger resolutions is more time consuming. We increase the width over stages with a small factor (≤ 2) to alleviate redundancy in later stages, as analyzed in Sec. 2.3. The architecture details of our model family are presented in Tab. 1. Ci, Li, and Hi refer to the width, depth, and number of heads in the i-th stage. 4. Experiments 4.1. Implementation Details
We image conduct classification experiments on
ImageNet-1K [17]. The models are built with PyTorch 1.11.0 [59] and Timm 0.5.4 [77], and trained from scratch for 300 epochs on 8 Nvidia V100 GPUs using AdamW
[46] optimizer and cosine learning rate scheduler. We set the total batchsize as 2,048. The input images are resized and randomly cropped into 224×224. The initial learning rate is 1×10−3 with weight decay of 2.5×10−2. We use the same data augmentation as [69], including Mixup [85], auto-augmentation [13], and random erasing [88]. In ad-dition, we provide throughput evaluation on different hard-ware. For GPU, we measure the throughput on an Nvidia
V100, with the maximum power-of-two batchsize that fits in memory following [20, 69]. For CPU and ONNX, we measure the runtime on an Intel Xeon E5-2690 v4 @ 2.60
GHz processor, with batchsize 16 and run the model in a single thread following [20]. We also test the transferabil-ity of EfficientViT on downstream tasks. For the experi-ments on downstream image classification, we finetune the models for 300 epochs following [86], using AdamW [46] with batchsize 256, learning rate 1×10−3 and weight-decay 1×10−8. We use RetinaNet [41] for object detection on
COCO [42], and train the models for 12 epochs (1× sched-ule) with the same settings as [44] on mmdetection [6]. For instance segmentation, please refer to the supplementary. 4.2. Results on ImageNet
We compare EfficientViT with prevailing efficient CNN and ViT models on ImageNet [17], and report the results in
Tab. 2 and Fig. 1. The results show that, in most cases, our
EfficientViT achieves the best accuracy and speed trade-off across different evaluation settings.
Comparisons with efficient CNNs. We first compare Ef-ficientViT with vanilla CNN models, such as MobileNets
Table 2. EfficientViT image classification performance on ImageNet-1K [17] with comparisons to state-of-the-art efficient CNN and ViT models trained without extra data. Throughput is tested on Nvidia V100 for GPU and Intel Xeon E5-2690 v4 @ 2.60 GHz processor for
CPU and ONNX, where larger throughput means faster inference speed. ↑: finetune with higher resolution.
Model
EfficientViT-M0
MobileNetV3-Small [26]
EfficientViT-M1
Mobile-Former-52M [9]
MobileViT-XXS [50]
ShuffleNetV2 1.0× [48]
MobileViTV2-0.5 [51]
EfficientViT-M2
MobileOne-S0 [70]
MobileNetV2 1.0× [63]
EfficientViT-M3
GhostNet 1.0× [23]
NASNet-A-Mobile [89]
EfficientViT-M4
EdgeViT-XXS [56]
MobileViT-XS [50]
ShuffleNetV2 2.0× [48]
MobileNetV3-Large [26]
MobileViTV2-0.75 [51]
MobileOne-S1 [70]
GLiT-Tiny [5]
EfficientNet-B0 [67]
EfficientViT-M5
EfficientViT-M4↑384
EfficientViT-M5↑512
Top-1
Top-5
Throughput (images/s)
Flops
Params (%) 63.2 67.4 68.4 68.7 69.0 69.4 70.2 70.8 71.4 72.0 73.4 73.9 74.1 74.3 74.4 74.7 74.9 75.2 75.6 75.9 76.4 77.1 77.1 79.8 80.8 (%) 85.4
-88.7
--88.9
-90.2
-91.0 91.4 91.4
-91.8
--92.4
----93.3 93.4 95.0 95.5
GPU
CPU
ONNX (M) (M) 27644 228.4 340.1 19738 20093 3141 4456 13301 5142 18218 11320 6534 16644 7382 2623 15914 3638 3344 6962 7560 3350 6663 3516 4532 10621 3986 2313 156.5 126.9 32.8 29.4 106.7 34.4 121.2 67.4 32.5 96.4 57.3 19.8 88.5 28.2 11.1 37.9 39.1 16.0 30.7 17.5 30.2 56.8 15.8 8.3 231.7 215.9 21.5 41.7 177.0 44.9 158.7 128.6 80.4 120.8 77.0 25.5 108.6 29.6 20.5 52.3 70.5 22.7 51.1 15.7 29.5 62.5 22.6 10.5 79 57 167 52 410 146 466 201 274 300 263 141 564 299 556 986 591 217 1030 825 1333 390 522 1486 2670 2.3 2.5 3.0 3.5 1.3 2.3 1.4 4.2 2.1 3.4 6.9 5.2 5.3 8.8 4.1 2.3 7.4 5.4 2.9 4.8 7.3 5.3 12.4 12.4 12.4
Input
Epochs 224 224 224 224 256 224 256 224 224 224 224 224 224 224 224 256 224 224 256 224 224 224 224 384 512 300 600 300 450 300 300 300 300 300 300 300 300 300 300 300 300 300 600 300 300 300 350 300 330 360
[26, 63] and EfficientNet [67]. Specifically, compared to
MobileNetV2 1.0× [63], EfficientViT-M3 obtains 1.4% better top-1 accuracy, while running at 2.5× and 3.0× faster speed on V100 GPU and Intel CPU, respectively.
Compared to the state-of-the-art MobileNetV3-Large [26],
EfficientViT-M5 achieves 1.9% higher accuracy yet runs much faster, e.g., 40.5% faster on the V100 GPU and 45.2% faster on the Intel CPU but is 11.5% slower as ONNX mod-els. This may because reshaping is slower in ONNX imple-mentation, which is inevitable in computing self-attention.
Moreover, EfficientViT-M5 achieves comparable accuracy with the searched model EfficientNet-B0 [67], while runs 2.3×/1.9× faster on the V100 GPU/Intel CPU, and 2.1× faster as ONNX models. Although our model uses more pa-rameters, it reduces memory-inefficient operations that af-fect the inference speed and achieves higher throughput.
Comparisons with efficient ViTs. We also compare our models with recent efficient vision transformers [5, 9, 50, 51, 56] in Tab. 2. In particular, when getting similar per-formance on ImageNet-1K [17], our EfficientViT-M4 runs 4.4× and 3.0× faster than the recent EdgeViT-XXS [56] on the tested CPU and GPU devices. Even converted to ONNX runtime format, our model still gets 3.7× higher speed.
Compared to the state-of-the-art MobileViTV2-0.5 [51], our EfficientViT-M2 achieves slightly better performance with higher throughput, e.g., 3.4× and 3.5× higher through-put tested on the GPU and CPU devices, respectively. Fur-thermore, we compare with tiny variants of state-of-the-art large ViTs in Tab. 3. PoolFormer-12S [83] has comparable accuracy with EfficientViT-M5 yet runs 3.0× slower on the
V100 GPU. Compared to Swin-T [44], EfficientViT-M5 is 4.1% inferior in accuracy yet is 12.3× faster on the Intel
CPU, demonstrating the efficiency of the proposed design.
In addition, we present the speed evaluation and comparison on mobile chipsets in the supplementary material.
Finetune with higher resolutions. Recent works on ViTs have demonstrated that finetuning with higher resolutions can further improve the capacity of the models. We also finetune our largest model EfficientViT-M5 to higher res-olutions. EfficientViT-M5↑384 reaches 79.8% top-1 accu-racy with throughput of 3,986 images/s on the V100 GPU, and EfficientViT-M5↑512 further improves the top-1 accu-racy to 80.8%, demonstrating the efficiency on processing images with larger resolutions and the good model capacity.
Table 3. Comparison with the tiny variants of state-of-the-art large-scale ViTs on ImageNet-1K [17].
Table 5. EfficientViT object detection performance on COCO val2017 [42] with comparisons to other efficient models.
Top-1
Throughput (imgs/s)
Flops
Params
Model
PVTV2-B0 [76]
T2T-ViT-7 [84]
DeiT-T [69]
PoolFormer-12S [83]
EffFormer-L1 [40]
Swin-T [44]
EfficientViT-M5 (%) 70.5 71.7 72.2 77.2 79.2 81.2 77.1
GPU 3507 1156 4631 3534 4465 1393 10621
CPU ONNX (G) 12.7 22.5 26.0 10.4 12.9 4.6 56.8 18.5 16.1 25.1 14.6 21.2 6.4 62.5 0.6 1.1 1.3 1.9 1.3 4.5 0.5 (M) 1.4 4.3 5.9 12.0 12.3 29.0 12.4
Model
RetinaNet 1×
Flops Params
AP AP50 AP75 APs APm APl (M) (M)
MobileNetV2 [63]
MobileNetV3 [26]
SPOS [22]
MNASNet-A2 [66]
FairNAS-C [12]
MixNet-M [68]
EfficientViT-M4 28.3 46.7 29.3 14.8 30.7 38.1 300 29.9 49.3 30.8 14.9 33.3 41.1 217 30.7 49.8 32.2 15.4 33.9 41.6 365 30.5 50.2 32.0 16.6 34.1 41.1 340 31.2 50.8 32.7 16.3 34.4 42.3 325 31.3 51.7 32.4 17.0 35.0 41.9 360 32.7 52.2 34.1 17.6 35.3 46.0 299 3.4 5.4 4.3 4.8 5.6 5.0 8.8
Table 4. Results of EfficientViT and other efficient models on downstream image classification datasets.
Table 6. Ablation for EfficientViT-M4 on ImageNet-1K [17] dataset. Top-1 accuracy, GPU and ONNX throughput are reported. t u p h g u o r h
T
Model
MobileNetV1 [27]
MobileNetV2 [63]
MobileNetV3 [26]
NASNet-A-M [89]
ViT-S/16 [18]
EfficientViT-M5 8543 6534 7560 2623 2135 10621 t e
N e g a m
I 70.6 72.9 75.2 74.1 81.4 77.1 0 1
R
A
F
I
C 96.1 95.7 97.6 96.8 97.6 98.0 0 0 1
R
A
F
I
C 82.3 80.8 85.5 83.9 85.7 86.4 s r e w o l
F 96.7 96.6 97.0 96.8 86.4 97.1 s r a
C 91.4 91.0 91.2 88.5
-89.7 s t e
P 89.9 90.5 90.1 89.4 90.4 92.0 4.3. Transfer Learning Results
To further evaluate the transfer ability, we apply Effi-cientViT on various downstream tasks.
Downstream Image Classification. We transfer Effi-cientViT to downstream image classification datasets to test its generalization ability: 1) CIFAR-10 and CIFAR-100 [36]; 2) fine-grained classification: Flowers [55], Stan-ford Cars [35], and Oxford-IIIT Pets [58]. We report the results in Tab. 4. Compared to existing efficient mod-els [18, 26, 27, 63, 89], our EfficientViT-M5 achieves com-parable or slightly better accuracy across all datasets with much higher throughput. An exception lies in Cars, where our model is slightly inferior in accuracy. This may because the subtle differences between classes lie more in local de-tails thus is more feasible to be captured with convolution.
Object Detection. We compare EfficientViT-M4 with ef-ficient models [12, 22, 26, 63, 66, 68] on the COCO [42] ob-ject detection task, and present the results in Tab. 5. Specif-ically, EfficientViT-M4 surpasses MobileNetV2 [63] by 4.4% AP with comparable Flops. Compared to the searched method SPOS [22], our EfficientViT-M4 uses 18.1% fewer
Flops while achieving 2.0% higher AP, demonstrating its capacity and generalization ability in different vision tasks. 4.4. Ablation Study
In this section, we ablate important design elements in the proposed EfficientViT on ImageNet-1K [17]. All mod-els are trained for 100 epochs to magnify the differences and reduce training time [20]. Tab. 6 reports the results.
Impact of the sandwich layout block. We first present an ablation study to verify the efficiency of the proposed sand-wich layout design, by replacing the sandwich layout block with the original Swin block [44]. The depth is adjusted to
Ablation
Top-1 (%)
Throughput (imgs/s)
GPU
ONNX
# 1 5 6 7 8
EfficientViT-M4
Sandwich → Swin [44] 2 3 N = 1 → 2 4 N = 1 → 3
CGA → MHSA [18]
Cascade → None
QKV allocation → None
FFN ratio 2 → 4
DWConv → None 9 10 BN → LN [2] 11 ReLU → HSwish [26] 71.3 68.3 70.2 65.7 70.2 69.8 69.9 69.8 69.9 70.4 72.2 15914 15804 14977 15856 16243 16411 15132 15310 16325 15463 15887 108.6 114.5 112.3 139.7 102.2 111.0 103.1 112.4 110.4 103.6 87.5
{2, 2, 3} to guarantee similar throughput with EfficientViT-M4 for a fair comparison. The top-1 accuracy degrades by 3.0% at a similar speed, verifying that applying more
FFNs instead of memory-bound MHSA is more effective for small models. Furthermore, to analyze the impact of the number of FFNs N before and after self-attention , we change the number from 1 to 2 and 3. The number of blocks is reduced accordingly to maintain similar throughput. As presented in Tab. 6 (#3 and #4), further increasing the num-ber of FFNs is not effective due to the lack of long-range spatial relation and N =1 achieves the best efficiency.
Impact of the cascaded group attention. We have proposed CGA to improve the computation efficiency of
MHSA. As shown in Tab. 6 (#5 and #6), replacing CGA with MHSA decreases the accuracy by 1.1% and ONNX speed by 5.9%, suggesting that addressing head redun-dancy improves the model efficiency. For the model without the cascade operation, its performance is comparable with
MHSA but worse than CGA, demonstrating the efficacy of enhancing the feature representations of each head.
Impact of the parameter reallocation. Our EfficientViT-M4 yields 1.4%/1.5% higher top-1 accuracy, 4.9%/3.8% higher GPU throughput than the models without QKV channel dimension reallocation or FFN ratio reduction, re-spectively, indicating the effectiveness of parameter reallo-cation (#1 vs. #7, #8). Moreover, we study the choices of
QK dimension in each head and the ratio of V dimension to the input embedding in Fig. 7.
It is shown that the perfor-mance is improved gradually as QK dimension increases
Table 7. Performance comparison on ImageNet-1K [17] and
ImageNet-ReaL [4]. Results with † are trained with 1,000 epochs and knowledge distillation following LeViT [20].
ImageNet (%)
Throughput (imgs/s) Flops Params
Model
Top-1 Top-1† ReaL† GPU CPU ONNX (M) (M)
LeViT-128S [20] 73.6
EfficientViT-M4 74.3 76.6 77.1 80.9 82.6 14457 82.3 83.6 15914 88.5 108.6 305 299 7.8 8.8 from 4 to 16, while further increasing it gives inferior per-formance. Besides, the performance improves from 70.3% to 71.3% when increasing the ratio between V dimension and input embedding from 0.4 to 1.0. When further en-larging the ratio to 1.2, it only gets 0.1% improvements.
Therefore, setting the channels of V close to the input em-bedding achieves the best parameter efficiency, which meets our analysis in Sec. 2.3 and design strategy.
Impact of other components. We ablate the impact of us-ing DWConv for token interaction, the normalization layer, and the activation function, as presented in Tab. 6 (#9, #10, and #11). With DWConv, the accuracy improves by 1.4% with a minor latency overhead, demonstrating the effective-ness of introducing local structural information. Replacing
BN with LN decreases accuracy by 0.9% and GPU speed by 2.9%. Using HardSwish instead of ReLU improves ac-curacy by 0.9% but leads to a large drop of 20.0% ONNX speed. The activation functions are element-wise operations that occupy a considerable amount of processing time on
GPU/CPU [15, 48, 72], thus utilizing ReLU instead of more complicated activation functions is of better efficiency.
Results of 1,000 training epochs and distillation. Tab. 7 shows the results with 1,000 training epochs and knowledge distillation using RegNetY-16GF [60] as the teacher model following [20] on ImageNet-1K [17] and ImageNet-ReaL
[4]. Compared to LeViT-128S [20], EfficientViT-M4 sur-passes it by 0.5% on ImageNet-1K and 1.0% on ImageNet-ReaL, respectively. For the inference speed, our model has 34.2% higher throughput on ONNX and also shows supe-riority on other settings. The results demonstrate that the strong capability and generalization ability of EfficientViT can be further explored with longer training schedules. 5.