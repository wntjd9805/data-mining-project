Abstract
Despite recent advances in implicit neural representa-tions (INRs), it remains challenging for a coordinate-based multi-layer perceptron (MLP) of INRs to learn a common representation across data instances and generalize it for unseen instances. In this work, we introduce a simple yet effective framework for generalizable INRs that enables a coordinate-based MLP to represent complex data instances by modulating only a small set of weights in an early MLP layer as an instance pattern composer; the remaining MLP weights learn pattern composition rules for common repre-sentations across instances. Our generalizable INR frame-work is fully compatible with existing meta-learning and hy-pernetworks in learning to predict the modulated weight for unseen instances. Extensive experiments demonstrate that our method achieves high performance on a wide range of domains such as an audio, image, and 3D object, while the ablation study validates our weight modulation. 1.

Introduction
Implicit neural representations (INR) have shown the po-tential to represent complex data as continuous functions.
Assuming that a data instance comprises the pairs of a co-ordinate and its output features, INRs adopt a parameterized neural network as a mapping function from an input coor-dinate into its output features. For example, a coordinate-based MLP [23] predicts RGB values at each 2D coordinate as an INR of an image. Despite the popularity of INRs, a trained MLP cannot be generalized to represent other in-stances, since each MLP learns to memorize each data in-stance. Thus, INRs necessitate separate training of MLPs to represent a lot of data instances as continuous functions.
*Equal contribution
†Corresponding author
Figure 1. The reconstructed images of 178×178 ImageNette by
TransINR [4] (left) and our generalizable INRs (right).
Generalizable INRs aim to learn common representa-tions of a MLP across instances, while modulating fea-tures or weights of the coordinate-based MLP to adapt unseen data instances [4, 7, 24]. The feature-modulation method exploits the latent vector of an instance to con-dition the activations in MLP layers through concatena-tion [17] or affine-transform [8, 18]. Despite the computa-tional efficiency of feature-modulation, the modulated INRs have unsatisfactory results to represent complex data due to their limited modulation capacity. On the other hand, the weight-modulation method learns to update the whole MLP weights to increase the modulation capacity for high perfor-mance. However, modulating whole MLP weights leads to unstable and expensive training [4, 7, 9, 24].
In this study, we propose a simple yet effective frame-work for generalizable INRs via Instance Pattern Com-posers to modulate only a small set of MLP weights. We postulate that a complex data instance can be represented by composing low-level patterns in the instance [23]. Thus, we rethink and categorize the weights of MLP into i) instance pattern composers and ii) pattern composition rule. The in-stance pattern composer is a weight matrix in the early layer of our coordinate-based MLP to extract the instance content patterns of each data instance as a low-level feature. The re-maining weights of MLP is defined as a pattern composition rule, which composes the instance content patterns in an instance-agnostic manner. In addition, our framework can adopt both optimization-based meta-learning and hypernet-works to predict the instance pattern composer for an INR of unseen instance. In experiments, we demonstrate the ef-fectiveness of our generalizable INRs via instance pattern composers on various domains and tasks.
Our main contributions are summarized as follows. 1)
Instance pattern composers enable a coordinate-based MLP to represent complex data by modulating only one weight, while pattern composition rule learns the common repre-sentation across data instances. 2) Our instance pattern composers are compatible with optimization-based meta-learning and hypernetwork to predict modulated wights of unseen data during training. 3) We conduct extensive exper-iments to demonstrate the effectiveness of our framework through quantitative and qualitative analysis. 2.