Abstract
Multi-frame depth estimation generally achieves high accuracy relying on the multi-view geometric consistency.
When applied in dynamic scenes, e.g., autonomous driving, this consistency is usually violated in the dynamic areas, leading to corrupted estimations. Many multi-frame meth-ods handle dynamic areas by identifying them with explicit masks and compensating the multi-view cues with monocu-lar cues represented as local monocular depth or features.
The improvements are limited due to the uncontrolled qual-ity of the masks and the underutilized benefits of the fu-sion of the two types of cues. In this paper, we propose a novel method to learn to fuse the multi-view and monocu-lar cues encoded as volumes without needing the heuristi-cally crafted masks. As unveiled in our analyses, the multi-view cues capture more accurate geometric information in static areas, and the monocular cues capture more useful contexts in dynamic areas. To let the geometric percep-tion learned from multi-view cues in static areas propagate to the monocular representation in dynamic areas and let monocular cues enhance the representation of multi-view cost volume, we propose a cross-cue fusion (CCF) module, which includes the cross-cue attention (CCA) to encode the spatially non-local relative intra-relations from each source to enhance the representation of the other. Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method. 1.

Introduction
Depth estimation is a fundamental and challenging task for 3D scene understanding in various application scenar-ios, such as autonomous driving [10, 16, 27]. With the advent of convolutional neural networks (CNNs) [12, 18], depth estimation methods [2, 24–26, 40, 45, 46] are capable of predicting promising results given either single or mul-Corresponding author: Y. Zhang, D. Gong, W. Yin.
Figure 1. Depth estimation in dynamic scenes. Multi-frame pre-dictions reserve high overall accuracy while degrading in dynamic areas. The monocular method better handles moving areas while suffering in static areas. Our method fuses both multi-frame and monocular depth cues for final prediction, yielding superior per-formance of the whole scene. tiple images. The single image-based methods learn the monocular cues, e.g., the texture or object-level features, to predict the depth [2, 42, 44] , while multi-frame meth-ods [36,37,40] can generally obtain higher overall accuracy relying on the multi-view geometric cues. Specifically, the 3D cost volume has been proved simple and effective for depth estimation, which encodes the multi-frame matching probabilities with a set of depth hypotheses [15, 37, 40].
Although multi-frame methods are widely used in scene reconstruction [15, 34, 40], they encounter non-negligible challenges in the dynamic scenes with dynamic areas (e.g., moving cars and pedestrians). The dynamic areas cause corrupted values in the cost volume due to the violation of multi-view consistency [9,33] and mislead the network pre-dictions. However, depth estimation for the dynamic areas is usually crucial in most applications [10,16,19]. As shown in Fig. 1, multi-frame depth estimation for the dynamic cars is more challenging than the static backgrounds.
To handle the dynamic areas violating the multi-view consistency, a few multi-frame depth estimation methods
[9, 36, 37] try to identify and exclude the dynamic areas
through an explicit mask obtained relying on some assump-tions or heuristics. Specifically, some method [37] excludes the multi-frame cost volume relying on a learned dynamic mask and compensates the excluded areas with monocular features; some methods directly adjust the dynamic object locations in input images [9] or supervise multi-frame depth
[36] with predicted monocular depth. However, these meth-ods are usually sensitive to the explicit mask’s quality, and the masks are obtained from additional networks or manu-ally crafted criteria [9, 36, 37]. Despite better performances than pure multi-frame methods, these methods exhibit lim-ited performance improvement compared with the addition-ally introduced monocular cues (as shown in Tab. 4), im-plying underutilized benefits from the fusion of the multi-view and monocular cues. Although some self-supervised monocular depth estimation methods [4,5,11,14,22,23] also address the multi-view inconsistency issues, they mainly fo-cus on handling the unfaithful self-supervision signals.
To tackle the above issues, we propose a novel method that fuses the respective benefits from the monocular and multi-view cues, leading to significant improvement upon each individual source in dynamic scenes. We first ana-lyze the behaviors of monocular and multi-frame cues in dynamic scenes, that the pure monocular method can gen-erally learn good structural relations around the dynamic areas, and the pure multi-view cue preserves more accurate geometric properties in the static areas. We then unveil the effectiveness of leveraging the benefits of both depth cues by directly fusing depth volumes (Sec. 3). Inspired by the above observations, beyond treating monocular cues as a lo-cal supplement of multi-frame methods [9, 36, 37], we pro-pose a cross-cue fusion (CCF) module to enhance the repre-sentations of multi-view and monocular cues with the other, and fuse them together for dynamic depth estimation. We use the spatially non-local relative intra-relations encoded in cross-observation attention (CCA) weights from each source to guide the representation of the other, as shown in Fig. 4. Specifically, the intra-relations of monocular cues can help to address multi-view inconsistency in dynamic ar-eas, while the intra-relations from multi-view cues help to enhance the geometric property of the monocular represen-tation, as visualized in Fig. 5. Unlike [1, 9, 36, 37], the proposed method unifies the input format of both cues as volumes and conducts fusion on them, which achieves bet-ter performances (as shown in Fig. 6). The proposed fu-sion module is learnable and does not require any heuristic masks, leading to better generalization and flexibility.
Our main contributions are summarized as follows:
• We analyze multi-frame and monocular depth estima-tions in dynamic scenes and unveil their respective
Inspired by advantages in static and dynamic areas. this, we propose a novel method that fuses depth vol-umes from each cue to achieve significant improve-ment upon individual estimations in dynamic scenes.
• We propose a cross-cue fusion (CCF) module that uti-lizes the cross-cue attention to encode non-local intra-relations from one depth cue to guide the representa-tion of the other. Different from methods using local masks, the attention weights learn mask-free global ge-ometric information according to the geometric prop-erties of each depth cue (as shown in Fig. 5).
• The proposed method outperforms the state-of-the-art method in dynamic areas with a significant error reduc-tion of 21.3% while retaining its superiority in overall performance on KITTI. It also achieves the best gener-alization ability on the DDAD dataset in dynamic areas than the competing methods. 2.