Abstract
This paper investigates a phenomenon where query-based object detectors mispredict at the last decoding stage while predicting correctly at an intermediate stage. We review the training process and attribute the overlooked phenomenon to two limitations: lack of training emphasis and cascading errors from decoding sequence. We design and present Selective Query Recollection (SQR), a simple and effective training strategy for query-based object detec-tors. It cumulatively collects intermediate queries as decod-ing stages go deeper and selectively forwards the queries to the downstream stages aside from the sequential struc-ture. Such-wise, SQR places training emphasis on later stages and allows later stages to work with intermediate queries from earlier stages directly. SQR can be easily plugged into various query-based object detectors and sig-nificantly enhances their performance while leaving the in-ference pipeline unchanged. As a result, we apply SQR on
Adamixer, DAB-DETR, and Deformable-DETR across var-ious settings (backbone, number of queries, schedule) and consistently brings 1.4 ∼ 2.8 AP improvement. Code is available at https://github.com/Fangyi-Chen/
SQR 1.

Introduction
Object detection is a long-established topic in computer vision aiming to localize and categorize objects of interest.
Previous methods [4, 7, 10, 11, 16, 18, 21, 25, 26, 29, 32, 33, 35–37] rely on dense priors tiled at feature grids so as to detect in a sliding-window paradigm, and have dominated object detection for the recent decade, but these methods fail to shake off many hand-crafted processing steps such as anchor generation or non-maximum suppression, which block end-to-end optimization.
Recent research attention has been geared towards query-based object detection [3, 17, 20, 23, 28, 31, 38] since the thriving of transformer [30] and DETR [3]. By view-Figure 1. The inference speed and AP for various networks on the MS-COCO val set. The red stars are the results trained with
SQR. The blue circles are the results of baselines without SQR.
SQR enhances the training of query-based object detectors while leaving the inference pipeline unchanged. ing detection as a direct set prediction problem, the new archetype represents the set of objects using a set of learn-able embeddings, termed as queries, which are fed to a decoder consisting of a stack (typically six) of decoding stages. Each stage performs similar operations: (1) in-teracting queries with image features via an attention-like mechanism, so the queries are aggregated with valuable in-formation that represents objects; (2) reasoning the relation among all queries so that global dependency on objects co-occurrence and duplicates could be captured; (3) interpret-ing bounding box and category from each query by a feed forward network. Queries are sequentially processed stage-by-stage, and each stage is formulated to learn a residual function with reference to the former stage’s output, aiming to refine queries in a cascaded style.
As such wise, the decoding procedure implies that de-tection should be stage-by-stage enhanced in terms of IoU and confidence score. Indeed, monotonically improved AP is empirically achieved by this procedure. However, when visualizing the stage-wise predictions, we surprisingly ob-serve that decoder makes mistakes in a decent proportion of
Figure 2. Are query-based object detectors always enhancing predictions stage-by-stage? The traffic light at stage 1 gets a confident score of 0.41, while from stage 2 to 5 the confidence gradually decreases to 0.21 (Upper); the remote at stage 3 was wrongly classified as a cell phone, and from stage 3 to 6 the mistake was amplified from 0.26 to 0.42 (Lower). The visualization is acquired from Adamixer-R50 (42.5
AP) tested on COCO val set. cases where the later stages degrade true-positives and up-grade false-positives from the former stages. As shown in
Fig.2, the traffic light at stage 1 gets categorical confidence of 0.41, while from stage 2 to 5 the confidence gradually decreases to 0.21; the remote at stage 3 was wrongly clas-sified as a cell phone, while from stage 3 to 6 the error was exacerbated from 0.26 to 0.42. We present a more detailed statistic in Section 3.
This phenomenon inspires us to review the current train-ing strategy and bring two conjectures. Firstly, the respon-sibility that each stage takes is unbalanced, while supervi-sion applied to them is analogous. An early stage could make mistakes without causing too much impact because it gets chances to be corrected later, and the later stages are more responsible for the final prediction. But during train-ing, all of these stages are supervised in an equivalent man-ner and there lacks such a mechanism that places particular training emphasis on later stages. Secondly, due to the se-quential structure of the decoder, an intermediate query re-fined by a stage - no matter whether this refinement brings positive or negative effects - will be cascaded to the fol-lowing stages, while the query prior to the refinement never gets an opportunity to be propagated forward even though it emerges unscathed and might be more representative than the refined one. The cascading errors increase the diffi-culty of convergence and the sequential structure impedes the later stages from seeing prior queries during training.
Based on these intuitions, we present Query Recollection (QR) as a training strategy for query-based object detectors.
It cumulatively collects intermediate queries as stages go deeper, and feeds the collected queries to the downstream stages aside from the sequential structure. By each stage, the new add-ins alongside the original inputs are indepen-dently treated among each other, so the attentions and losses are calculated individually. In such a manner, QR enjoys two key features: (1) The number of supervision signals per stage grows in geometric progression, so that later stages get more supervision than the former ones, for example, the sixth stage got 32 times more supervision than the first; (2) Later stages get chance to view the outputs beyond its neighboring stage for training, which mitigates the poten-tial impact due to cascading errors. We further discover that selectively forward queries to each stage, not with the entire query collection but only those from the prior two stages, can raise the number of supervision in a Fibonacci sequence which halves the extra computing cost and brings even bet-ter results. We name it Selective Query Recollection (SQR). (1)
Our contributions are summarized in three folds:
We quantitatively investigate the phenomenon where query-based object detectors mispredict at the last decoding stage
while predicting correctly at an intermediate one. (2) We attribute the overlooked phenomenon to two training limi-tations, and propose a simple and effective training strategy
SQR that elegantly fits query-based object detectors. (3) We conduct experiments on Adamixer, DAB DETR, and De-formable DETR across various training settings that verify its effectiveness (Fig.1). 2.