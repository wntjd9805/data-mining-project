Abstract
DreamFusion [31] has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize
Neural Radiance Fields (NeRF) [23], achieving remarkable text-to-3D synthesis results. However, the method has two in-herent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse repre-sentation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer in-teracting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2× faster than DreamFu-sion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications. 1.

Introduction 3D digital content has been in high demand for a variety of applications, including gaming, entertainment, architec-ture, and robotics simulation. It is slowly finding its way into virtually every possible domain: retail, online conferencing, virtual social presence, education, etc. However, creating professional 3D content is not for anyone Ð it requires immense artistic and aesthetic training with 3D modeling ex-pertise. Developing these skill sets takes a significant amount of time and effort. Augmenting 3D content creation with natural language could considerably help democratize 3D content creation for novices and turbocharge expert artists.
*†: equal contribution.
Image content creation from text prompts [2, 28, 33, 36] has seen significant progress with the advances of diffusion models [13, 41, 42] for generative modeling of images. The key enablers are large-scale datasets comprising billions of samples (images with text) scrapped from the Internet and massive amounts of compute. In contrast, 3D content generation has progressed at a much slower pace. Existing 3D object generation models [4, 9, 47] are mostly categorical.
A trained model can only be used to synthesize objects for a single class, with early signs of scaling to multiple classes shown recently by Zeng et al. [47]. Therefore, what a user can do with these models is extremely limited and not yet ready for artistic creation. This limitation is largely due to the lack of diverse large-scale 3D datasets Ð compared to image and video content, 3D content is much less accessible on the
Internet. This naturally raises the question of whether 3D generation capability can be achieved by leveraging powerful text-to-image generative models.
Recently, DreamFusion [31] demonstrated its remarkable ability for text-conditioned 3D content generation by uti-lizing a pre-trained text-to-image diffusion model [36] that generates images as a strong image prior. The diffusion model acts as a critic to optimize the underlying 3D repre-sentation. The optimization process ensures that rendered images from a 3D model, represented by Neural Radiance
Fields (NeRF) [23], match the distribution of photorealis-tic images across different viewpoints, given the input text prompt. Since the supervision signal in DreamFusion oper-ates on very low-resolution images (64 × 64), DreamFusion cannot synthesize high-frequency 3D geometric and texture details. Due to the use of inefficient MLP architectures for the NeRF representation, practical high-resolution synthesis may not even be possible as the required memory footprint and the computation budget grows quickly with the resolu-tion. Even at a resolution of 64 × 64, optimization times are in hours (1.5 hours per prompt on average using TPUv4).
In this paper, we present a method that can synthesize highly detailed 3D models from text prompts within a re-duced computation time. Specifically, we propose a coarse-a silver platter piled  high with fruits michelangelo style statue of  an astronaut a stuffed grey rabbit  holding a pretend carrot  an iguana holding a balloon a beautiful dress made  out of garbage bags an imperial state  crown of england a blue poison-dart frog  sitting on a water lily neuschwanstein castle, aerial view  a metal bunny  sitting on  top of a  stack of  broccoli
Low resolution bunny before editing  a baby bunny  sitting on  top of a  stack of  pancakes a metal bunny  sitting on  top of a  stack of  chocolate  cookie a sphinx sitting on  top of a  stack of  chocolate  cookie
Figure 1. Results and applications of Magic3D. Top: high-resolution text-to-3D generation. Magic3D can generate high-quality and high-resolution 3D models from text prompts. Bottom: high-resolution prompt-based editing. Magic3D can edit 3D models by fine-tuning with the diffusion prior using a different prompt. Taking the low-resolution 3D model as the input (left), Magic3D can modify different parts of the 3D model corresponding to different input text prompts. Together with various creative controls on the generated 3D models, Magic3D is a convenient tool for augmenting 3D content creation. to-fine optimization approach that uses multiple diffusion priors at different resolutions to optimize the 3D representa-tion, enabling the generation of both view-consistent geome-try as well as high-resolution details. In the first stage, we optimize a coarse neural field representation akin to Dream-Fusion, but with a memory- and compute-efficient scene representation based on a hash grid [25]. In the second stage, we switch to optimizing mesh representations, a critical step that allows us to utilize diffusion priors at resolutions as high as 512 × 512. As 3D meshes are amenable to fast graphics renderers that can render high-resolution images in real-time, we leverage an efficient differentiable rasterizer [9, 26] and make use of camera close-ups to recover high-frequency details in geometry and texture. As a result, our approach
produces high-fidelity 3D content (see Fig. 1) that can con-veniently be imported and visualized in standard graphics software and does so at 2× the speed of DreamFusion. Fur-thermore, we showcase various creative controls over the 3D synthesis process by leveraging the advancements developed for text-to-image editing applications [2, 35]. Our approach, dubbed Magic3D, endows users with unprecedented control in crafting their desired 3D objects with text prompts and reference images, bringing this technology one step closer to democratizing 3D content creation.
In summary, our work makes the following contributions:
• We propose Magic3D, a framework for high-quality 3D content synthesis using text prompts by improving several major design choices made in DreamFusion. It consists of a coarse-to-fine strategy that leverages both low- and high-resolution diffusion priors for learning the 3D representa-tion of the target content. Magic3D, which synthesizes 3D content with an 8× higher resolution supervision, is also 2× faster than DreamFusion. 3D content synthesized by our approach is significantly preferable by users (61.7%).
• We extend various image editing techniques developed for text-to-image models to 3D object editing and show their applications in the proposed framework. 2.