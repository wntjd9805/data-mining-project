Abstract
In this paper we present a new method for deformable
NeRF that can directly use optical flow as supervision. We overcome the major challenge with respect to the compu-tationally inefficiency of enforcing the flow constraints to the backward deformation field, used by deformable NeRFs.
Specifically, we show that inverting the backward deforma-tion function is actually not needed for computing scene flows between frames. This insight dramatically simplifies the problem, as one is no longer constrained to deformation functions that can be analytically inverted. Instead, thanks to the weak assumptions required by our derivation based on the inverse function theorem, our approach can be ex-tended to a broad class of commonly used backward defor-mation field. We present results on monocular novel view synthesis with rapid object motion, and demonstrate signifi-cant improvements over baselines without flow supervision. 1.

Introduction
Reconstructing dynamic scenes from monocular videos is a significantly more challenging task compared to its static-scene counterparts, due to lack of epipolar constraints for finding correspondences and ambiguities between mo-tion and structure. Recent advances in differentiable ren-dering have lead to various solutions using an analysis-by-synthesis strategy â€“ solving the non-rigid deformation and structure by minimizing the difference between synthesized images and input video frames. Among those, deformable neural radiance fields [14, 21, 25, 31] has been a notable technique to represent dynamic scenes and shows plausi-ble space-time view synthesis results. However, the current implementations only warrant success on teleporting-like videos whose camera motions are significantly more rapid than object motions. Quality of their results significantly decrease on videos with more rapid object motions [6].
In this work, we conjecture the deficiency of these de-formable NeRF-based methods is mainly due to lack of
Figure 1. We propose a method to use optical flow supervision for deformable NeRF. It noticeably improves novel view synthesis for monocular videos with rapid object motions. In the figure, we visualize rendered novel view images and depth maps for the first and last frame of the input video. temporal regularization. As they represent deformation as backward warping from the sampled frames to some canon-ical space, the motions or scene flows between temporally adjacent frames is not directly modeled nor supervised. An-other deficiency is that these methods minimize photomet-ric error alone, which is insufficient for gradient descent to overcome poor local minima when the canonical and other frames has little spatial overlap. This deficiency is severe for non-object-centric videos with large translations.
The community has explored optical flow as an addi-tional cue to help supervise the temporal transitions of other motion representations, such as scene flow fields [4, 5, 13, 43] and blend skinning fields [40]. However, enforcing flow constraints with respect to a generic backward warp-ing field as in Nerfies [21] is non-trivial. Intuitively, to com-pute scene flows, it requires inverting the backward warp by having a forward warp which maps points from canonical space to other frames. Then scene flows can be computed either by taking time derivative of the forward warp or pre-dicting the next position of a point through evaluating the forward warp. But this can be problematic since analytical inverse of a complicated non-bijective function (e.g. neu-ral networks) is impossible, and an approximate solution by
having an auxiliary network to represent the forward warp will introduce computational overhead and is theoretically ill-posed. Counter to this intuition, we will show that invert-ing the backward warping function is actually not needed for computing scene flows between frames.
The main contribution of this paper is: we derive an analytical solution to compute velocities of objects directly from the backward warping field of the deformable NeRF.
The velocities are then used to compute scene flows through temporal integration, which allows us to supervise the de-formable NeRF through optical flow. This leads to signif-icant improvement on videos with more rapid motions, as shown in Fig. 1.
The advantage of our approach is twofold: (i) Our method applies to all kinds of backward warping function, thanks to the weak assumptions required by the inverse function theorem which our derivation is based on. Thus our method is more general compared to other works using invertible normalizing flows [11] or blend skinning [3, 40]. (ii) Our method is also computationally more tractable com-pared to neural scene flow fields [4, 12, 13], which would require integrating flows over long period of time to reach some canonical frame. 2.