Abstract
We address the challenge of recovering an underlying scene geometry and colors from a sparse set of RGBD view observations. In this work, we present a new solution termed RGBD2 that sequentially generates novel RGBD views along a camera trajectory, and the scene geometry is simply the fusion result of these views. More speciﬁcally, we maintain an intermediate surface mesh used for render-ing new RGBD views, which subsequently becomes com-plete by an inpainting network; each rendered RGBD view is later back-projected as a partial surface and is supple-mented into the intermediate mesh. The use of intermediate mesh and camera projection helps solve the tough problem of multi-view inconsistency. We practically implement the
RGBD inpainting network as a versatile RGBD diffusion model, which is previously used for 2D generative model-ing; we make a modiﬁcation to its reverse diffusion process to enable our use. We evaluate our approach on the task of 3D scene synthesis from sparse RGBD inputs; extensive experiments on the ScanNet dataset demonstrate the supe-riority of our approach over existing ones. Project page: https://jblei.site/proj/rgbd-diffusion. 1.

Introduction
Scene synthesis is an essential requirement for many practical applications. The resulting scene representation can be readily utilized in diverse ﬁelds, such as virtual re-ality, augmented reality, computer graphics, and game de-velopment. Nevertheless, conventional approaches to scene synthesis usually involve reconstructing scenes (e.g., indoor scenes with varying sizes) by ﬁtting given observations, such as multi-view images or point clouds. The increas-ing prevalence of RGB/RGBD scanning devices has estab-lished multi-view data as a favored input modality, driving and promoting technical advancements in the realm of scene reconstruction from multi-view images.
Neural Radiance Fields (NeRFs) [42] have demonstrated
†Correspondence to Kui Jia: <kuijia@scut.edu.cn>. (cid:1872) (cid:3404) (cid:882) (cid:1872) (cid:3404) (cid:883) (cid:1872) (cid:3404) (cid:884)
… (cid:1872) (cid:3404) (cid:885)
Final Result
Figure 1. Illustration of Our Generative Scene Synthesis. We incrementally reconstruct the scene geometry by inpainting RGBD views as the camera moves in the scene. potential in this regard, yet they are not exempt from limi-tations. NeRFs are designed to reconstruct complete scenes by ﬁtting multi-view images, and they cannot generate or infer missing parts when the input is inevitably incomplete or missing. While recently some studies [3,5,8,56,63] have attempted to equip NeRFs with generative and extrapola-tion capabilities, this functionality relies on a comparatively short representation with limited elements (e.g. typically, the length of a global latent code is much shorter than that of an image: (F = 512) (cid:2) (H ×W = 128×128 = 16, 384)) that signiﬁcantly constrains their capacity to accurately cap-ture ﬁne-grained details in the observed data. Consequently, the effectiveness of these methods has only been established for certain categories of canonical objects, such as faces or cars [8, 63], or relatively small toy scenes [5].
We introduce a novel task of generative scene synthesis from sparse RGBD views, which involves learning across multiple scenes to later enable scene synthesis from a sparse set of multi-view RGBD images. This task presents a chal-lenging setting wherein a desired solution should simulta-neously (1) preserve observed regions, hallucinate missing parts of the scene, (2) eliminate additional computational costs during inference for each individual test scene, (3) en-sure exact 3D consistency, and (4) maintain scalability to scenes with unﬁxed scales.
We will elaborate on them in detail as follows. Firstly, to maximize the preservation of intricate details while simulta-neously hallucinating potentially absent parts that may be-come more pronounced when views are exceedingly sparse, we perform straightforward reconstruction whose details come from images that can describe ﬁne structures using a maximum of H × W elements (i.e. an image size) in a view completion manner. This is particularly compatible with diffusion models that operate at full image resolution with an inpainting mechanism. We also found that RGBD diffusion models greatly simplify the training complexity of a completion model, thanks to their versatile generative ability to inpaint missing RGBD pixels while preserving the integrity of known regions through a convenient train-ing process solely operated on complete RGBD data. Sec-ondly, our method employs back-projection that requires no optimization, thus eliminating the necessity for test-time training for each individual scene, ultimately leading to a signiﬁcant enhancement in test-time efﬁciency. Thirdly, to ensure consistency among multi-view images, an interme-diate mesh representation is utilized as a means of bridging the 2D domain (i.e. multi-view RGBD images) with the 3D domain (i.e. the 3D intermediate mesh) through the aid of camera projection. Fourthly, to enable our method to handle scenes of indeterminate sizes, we utilize images with freely designated poses as the input representation. Such manner naturally ensures SE(3) equivariance, and thus offers scala-bility due to the ease with which the range of the generated content can be controlled by simply specifying their camera extrinsic matrices.
Our proposal involves generating multi-view consistent
RGBD views along a predetermined camera trajectory, us-ing an intermediate mesh to render novel RGBD images that are subsequently inpainted using a diffusion model, and transforming each RGBD view into a 3D partial mesh via back-projection, and ﬁnally merging it with the inter-mediate scene mesh to produce the ﬁnal output. Speciﬁ-cally, our proposed approach initiates by ingesting multiple posed RGBD images as input and utilizing back-projection to construct an intermediate scene mesh. This mesh encom-passes color attributes that facilitate the rendering of RGBD images from the representation under arbitrarily speciﬁed camera viewpoints. Once a camera pose is selected from the test-time rendering trajectory, the intermediate mesh is rendered to generate a new RGBD image for this pose. No-tably, the test-time view typically exhibits only slight over-lap with the known cameras, leading to naturally partially rendered RGBD images. To ﬁll the gaps in the incom-plete view, we employ an inpainting network implemented as an RGBD diffusion model with minor modiﬁcations to its reverse sampling process. The resulting inpainted out-put is then back-projected into 3D space, forming a partial mesh that complements the entire intermediate scene mesh.
We iterate these steps until all test-time camera viewpoints are covered, and the intermediate scene mesh gradually be-comes complete during this process. The ﬁnal output of our pipeline is the mesh outcome acquired from the last step.
Extensive experiments on ScanNet [12] dataset demon-strate the superiority of our approach over existing solutions on the task of scene synthesis from sparse RGBD inputs. 2.