Abstract
Deep Neural Networks show superior performance in various tasks but are vulnerable to adversarial attacks.
Most defense techniques are devoted to the adversarial training strategies, however, it is difficult to achieve satis-factory robust performance only with traditional adversar-ial training. We mainly attribute it to that aggressive per-turbations which lead to the loss increment can always be found via gradient ascent in white-box setting. Although some noises can be involved to prevent attacks from deriv-ing precise gradients on inputs, there exist trade-offs be-tween the defense capability and natural generalization.
Taking advantage of the properties of random projection, we propose to replace part of convolutional filters with ran-dom projection filters, and theoretically explore the geomet-ric representation preservation of proposed synthesized fil-ters via Johnson-Lindenstrauss lemma. We conduct suffi-cient evaluation on multiple networks and datasets. The experimental results showcase the superiority of proposed random projection filters to state-of-the-art baselines. The code is available on GitHub. 1.

Introduction
Although Deep Neural Networks (DNNs) have become a popular technique in various scientific fields [16,46,47,50], the vulnerability of DNNs reveals the high risk of deploy-ment in real scenarios, especially under the attack of ad-versarial examples [11, 28]. Some tiny and imperceptible perturbations to network inputs could result in the major changes of outputs, which can be easily crafted through various adversarial attack strategies [1, 6, 11]. Adversar-ial attacks could be generally categorized into two streams, the white-box and black-box attacks.
In black-box set-ting, the attackers have no knowledge of victim models but can estimate the strong perturbation via surrogate models or huge number of queries [15, 19]. In white-box setting, the attackers have full knowledge of victim model, includ-*Corresponding author. ing the model parameters, network architecture, and infer-ence strategy [28, 39]. Since the gradients of victim models can be directly fetched, the crafted adversarial examples are more aggressive and the performance under white-box at-tacks is one of the key criteria of robustness evaluation.
Seeking adversarial robust networks becomes a key chal-lenge when it comes to the deployment of DNNs. One of the most popular and effective techniques is adversarial train-ing [28], which arguments the training data with adversarial examples within a fixed perturbation size. With the involve-ment of adversarial examples, DNNs are optimized to pre-serve their outputs for perturbed samples within the â„“p ball of all training input data. However, due to the increasingly advanced attack techniques, it is difficult for existing adver-sarially trained networks to achieve satisfactory robustness against all potential attacks. Furthermore, the training on stronger adversarial examples could hurt the natural gen-eralization of models [52], and there exists a trade-off be-tween robustness and accuracy [51].
Besides the traditional adversarial training, the utiliza-tion of randomization in adversarial robustness has been proven effective. For example, Liu et al. [25] propose to inject noise which is sampled from Gaussian distribution to the inputs of convolution layers. Some theoretical analy-ses have shown that randomized classifiers can easily out-perform deterministic ones in defending against adversar-ial attacks [32, 33]. We mainly attribute the improvement of randomization in adversarial robustness evaluation to the fusion of features with noises, which prevents white-box at-tackers from obtaining the precise gradients of loss with re-spect to the inputs. Although the involvement of noise in the networks can be an effective defense mechanism, the design of noises, such as the way of injection, the magnitude of noise, etc., can also significantly influence the natural gen-eralization of networks in practice. The trade-offs between the adversarial robustness and optimization difficulty are al-ways ignored in the randomized techniques, which limits their superiority to deterministic models.
In this paper, we introduce randomness into deep neu-ral networks with the help of random projection filters.
Random projection is a simple yet effective technique
for dimension reduction, which can approximately pre-serve the pairwise distance between any two data points from a higher-dimensional space in the projected lower-dimensional space under certain conditions. The theoretical and empirical advantages offered by random projection thus inspire a new way to explore the potential of noise injection with better trade-offs in Convolutional Neural Networks (CNNs). We propose to partially replace the convolutional filters with the random projection filters. Theoretically, we extend the scope of Johnson-Lindenstrauss Lemma [41] to cover the convolutions, where partial convolutional filters are randomly sampled from a zero-mean Gaussian distribu-tion. Pairwise example distance can also be approximately preserved under the new convolutions defined by random projection filters, if the number of random projection fil-ters is lower bounded in terms of the weight norm of the remaining convolutional filters. Motivated by these obser-vations, we introduce a simple and efficient defense scheme via the proposed Random Projection Filters (RPF). As pa-rameters of random projection filters are randomly sampled during forwarding, the attackers have no knowledge of up-coming sampled parameters even if in white-box attack set-tings. The effectiveness of proposed RPF is verified via ex-tensive empirical evaluations in our experiments. 2.