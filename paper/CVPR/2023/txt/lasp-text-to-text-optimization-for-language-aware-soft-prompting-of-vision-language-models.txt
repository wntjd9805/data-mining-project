Abstract
Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a down-stream task using a few training examples. However, cur-rent methods significantly overfit the training data, suffer-ing from large accuracy degradation when tested on un-seen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alle-viate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the proba-bility of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we pro-pose grouped LASP where each group of prompts is opti-mized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, pro-pose a re-calibration mechanism to address it. (4) We show that LASP is inherently amenable to including, during train-ing, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and sur-passes, for the first time, the accuracy on novel classes ob-tained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made available here. 1.

Introduction
Large-scale pre-training of neural networks has recently resulted in the construction of a multitude of foundation models for Language [7,25] and Vision & Language (V&L) understanding [1, 13, 24, 34]. Unlike the previous genera-tion of neural networks, such models can better capture the distribution of the world from which new favorable prop-erties and characteristics emerge. Of particular interest to this work are V&L models trained with contrastive learn-ing (i.e. CLIP-like models [13, 18, 24, 33, 34]), which have enabled seamless few-shot and even zero-shot adaptation to new downstream tasks and datasets. Specifically, this pa-per proposes a simple yet highly effective way to drastically improve soft prompt learning for the few-shot adaptation of the V&L model to a given downstream task.
Similarly to their NLP counterparts [16, 17, 24], prompt engineering and learning has emerged as one of the most powerful techniques for adapting a V&L to new tasks.
Initially, in [24], a set of manually-defined hand-engineered templates (or prompts) like a photo of a
{cls name}, or a black and white photo of a {cls name} were passed through the text encoder of the V&L model to create class-specific weights for category cls name that can be used for zero-shot recognition. Fol-lowing research in NLP [16, 17], subsequent work [35, 36] has proposed replacing the manually picked templates with a sequence of learnable vectors, also coined soft prompts, which are fed as input to the text encoder along with the class name cls name. The soft prompts are learned from a few training examples with the entire V&L model kept frozen. The whole process can be seen as parameter effi-cient fine-tuning of the model on a small training dataset.
However, a clearly identifiable problem with prompt learning is base class overfitting: while the accuracy on the classes used for training (base classes) significantly in-creases, the accuracy on unseen, during training, (novel) classes significantly drops. This is to some extent expected, as soft prompts are learned from few examples belonging to the base classes. Notably, on novel classes, direct, zero-shot recognition using hand-engineered prompts outperforms all existing soft prompt learning methods.
Key idea: To alleviate base class overfitting, in this work, we propose a solution motivated by the following obser-vation: since prompt learning improves the accuracy on base classes, but prompt engineering is significantly bet-ter on novel classes, we propose to learn the soft prompts by adding a cross entropy text-to-text loss that enforces the learned prompts to be close, in embedding space, to the textual ones, thus exploiting the intrinsic information cap-tured by the text encoder. The proposed text-to-text loss en-ables language-only optimization for V&L model adaption
for the first time. This is in contrast with prior soft-prompt learning methods that only capture V&L interactions.
Key contributions: Based on the above, we propose a novel framework for soft prompt learning which we call
Language-Aware Soft Prompting (LASP). Our main con-tributions within the LASP framework are as follows:
• We propose, for the first time, language-only optimiza-tion for V&L model adaption. Specifically, we propose a novel text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classi-fied with respect to the hand-engineered ones and show its effectiveness in terms of alleviating base-class overfitting.
• To increase the representation capacity of the prompts, and inspired by grouped convolution and multi-head at-tention, we propose a grouped language-aware prompt representation where each group of prompts specializes to a different subset of the pre-defined manual templates.
• We identify a visual-language misalignment introduced by prompt learning and LASP which impacts the gener-alization. More importantly, we propose a re-calibration mechanism based on (a) Layer Normalization fine-tuning and (b) learning a class-agnostic bias to address it.
• Thanks to our language-only learning framework, we pro-pose training LASP with virtual classes by including, dur-ing training, class names for which no visual samples are available. Importantly, we show that this further increases the robustness of the learned prompts.
Main results: Our methods set a new state-of-the-art for few-shot and zero-shot image classification on 11 datasets, significantly outperforming all soft prompting prior works.
Importantly, we present, for the first time, a prompt learn-ing method that outperforms, for the majority of the test datasets (8 out of 11), the very strong baseline based on hand-crafted prompts and CLIP for the recognition of novel classes (i.e. zero-shot setting). 2.