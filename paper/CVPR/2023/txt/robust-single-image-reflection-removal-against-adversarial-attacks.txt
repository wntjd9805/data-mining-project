Abstract
This paper addresses the problem of robust deep single-image reflection removal (SIRR) against adversarial at-tacks. Current deep learning based SIRR methods have shown significant performance degradation due to unno-ticeable distortions and perturbations on input images. For a comprehensive robustness study, we first conduct diverse adversarial attacks specifically for the SIRR problem, i.e. towards different attacking targets and regions. Then we propose a robust SIRR model, which integrates the cross-scale attention module, the multi-scale fusion module, and the adversarial image discriminator. By exploiting the multi-scale mechanism, the model narrows the gap between features from clean and adversarial images. The image dis-criminator adaptively distinguishes clean or noisy inputs, and thus further gains reliable robustness. Extensive ex-periments on Nature, SIR2, and Real datasets demonstrate that our model remarkably improves the robustness of SIRR across disparate scenes. 1.

Introduction
Single image reflection removal (SIRR) is a classic topic in the low-level image processing area, namely a kind of image restoration. When taking an image through a trans-parent surface, a reflection layer would be blended with the original photography (i.e. the transmission layer), resulting in imaging corruptions. The SIRR is devoted to recovering a clear transmission image by removing the reflection layer.
However, the SIRR is fundamentally ill-posed [42] that there could be an infinite number of transmission and re-flection decompositions from a blended image. Therefore, traditional methods often exploit manual priors to optimize the layer separation, such as gradient sparsity prior [25] and
∗ Equal contribution (cid:0) Corresponding Author: < whluo.china@gmail.com >.
Figure 1. The PSNR measurements of our approach under dif-ferent kinds of adversarial attacks. ‘Clean’ indicates no-attacks,
‘MSE FR’ represents attacking on Full Region with MSE objec-tive, and so on. The testing result is from Nature dataset [26]. relative smoothness prior [27]. These priors are often vio-lated when facing complex scenes. Recently, deep learning based methods [10, 16, 55] have attracted considerable at-tention to tackle the SIRR problem. By learning semantic and contextual features, deep SIRR methods have achieved much better quality of the recovered images.
However, deep neural networks are often vulnerable to visually imperceptible adversarial perturbations [14, 29].
The prediction can be totally invalid even with slight and unnoticeable attacks on inputs. Similarly, such vulnerability is also an important issue for the deep SIRR problem, and the robustness of current methods has not been thoroughly studied. There have been no benchmarks and evaluations for the robustness of deep SIRR models against intended attacks. Meanwhile, general defense methods [44] have not been applied to SIRR models. Accordingly, the robust SIRR model is still a crucial and desiderate research problem.
In this paper, we first investigate the robustness of deep
SIRR methods. We apply the widely-used and powerful attack method PGD [29] to generate adversarial samples.
For completeness of the robustness evaluation, we present various attack modes by referring [3, 48]. Specifically, we employ different attack objectives i.e. mean squared er-ror (MSE) and learned perceptual image patch similarity (LPIPS) [52], as well as different attack regions, i.e. the
full image region (FR), the reflection region (RR), and the non-reflection region (NR). Through a systematic analy-sis, the most effective attack mode and currently the most robust SIRR model are identified. Then we conduct ad-versarial training based on this model to enhance its ro-bustness.
In order to develop a furthermore robust SIRR model, we borrow the wisdom of multi-scale image pro-cessing [19] and adversarial discriminating [56] from pre-vious defense methods. Consequently, we build a new robust SIRR model based on the image transformer [41], which integrates the cross-scale attention module, the multi-scale fusion module, and the adversarial image discrimi-nator. The proposed method obtains significant improve-ments in robustness. Fig. 1 reveals the PSNR changes of our model prediction under distinct attack modes on the Na-ture dataset [26]. It is notable that our model yields limited degradations against perturbed images when compared with input clean images.
Overall, our main contributions can be summarized be-low. (1) We present a comprehensive evaluation of existing deep SIRR methods in terms of their robustness against var-ious adversarial attacks on diverse datasets. Extensive ex-perimental studies suggest presently the most effective at-tack and the most robust SIRR model. (2) We propose a novel transformer-based SIRR model, which integrates sev-eral relatively-robust modules to defend against adversarial attacks. The model can mitigate the effects of perturbations and distinguish clean or polluted inputs as well. (3) We carry out sufficient experiments to analyze the robustness of the proposed method, which achieves state-of-the-art sta-bility against adversarial images. The model performs supe-rior reflection removal robustness on distorted images while maintaining favorable accuracy on original clean images. 2.