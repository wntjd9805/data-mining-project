Abstract
Transformers have shown superior performance on var-ious computer vision tasks with their capabilities to cap-it ture long-range dependencies. Despite the success, is challenging to directly apply Transformers on point clouds due to their quadratic cost in the number of points.
In this paper, we present a Self-Positioning point-based
Transformer (SPoTr), which is designed to capture both local and global shape contexts with reduced complex-ity. Specifically, this architecture consists of local self-attention and self-positioning point-based global cross-attention. The self-positioning points, adaptively located based on the input shape, consider both spatial and seman-tic information with disentangled attention to improve ex-pressive power. With the self-positioning points, we propose a novel global cross-attention mechanism for point clouds, which improves the scalability of global self-attention by allowing the attention module to compute attention weights with only a small set of self-positioning points. Experiments show the effectiveness of SPoTr on three point cloud tasks such as shape classification, part segmentation, and scene segmentation. In particular, our proposed model achieves an accuracy gain of 2.6% over the previous best models on shape classification with ScanObjectNN. We also provide qualitative analyses to demonstrate the interpretability of self-positioning points. The code of SPoTr is available at https://github.com/mlvlab/SPoTr. 1.

Introduction
Point clouds have been widely applied in various areas such as autonomous driving, robotics, and augmented re-ality. Since the point cloud is an unordered set of points with irregular structures, adopting convolutional neural net-works (CNNs) on point clouds is challenging. Some works devoted effort to transforming point clouds into regular
*First two authors have equal contribution.
†is the corresponding author. (a) Local attention (b) Global attention (c) SP attention
Figure 1. Comparison of attention methods. (a) Local attention, (b) Global attention, (c) Self-positioning point-based attention (SP attention). structures, such as projection to multi-view images [1, 2] and voxelization [3, 4]. Others have tried to preserve the structure and design a convolution on the point space [5– 11]. However, the ability to capture long-range dependen-cies is limited in most convolution-based approaches, while it is crucial to understand global shape context, especially with real-world data [12].
Transformer [13] tackled the long-range dependency is-sue in natural language processing and later it has been actively extended to 2D image processing [14–17]. Early works tried to replace convolutional layers with self-attention [14, 18–22], but they struggled with the quadratic computational cost of self-attention to the number of pix-els. To mitigate the scalability issue, self-attention in lo-cal neighborhoods [15,17] or approximating a self-attention with a reduced set of queries or keys [16, 23, 24] have been studied. For point clouds, Point Transformer [25] applies a local attention operation (Figure 1a) and PointASNL [26] employs a global attention module in a non-local man-ner (Figure 1b). Still, in point clouds, Transformer, which tackles both long-range dependency and scalability issues, has been less explored.
In this paper, we propose Self-Positioning point-based
Transformer (SPoTr) to capture both local and global shape contexts with reduced complexity. SPoTr block consists of two attention modules: (i) local points attention (LPA) to learn local structures and (ii) self-positioning point-based
attention (SPA) to embrace global information via self-positioning points. SPA performs global attention by com-puting attention weights with only a small set of Self-Positioning points (SP points) instead of the whole input points different from the standard global attention as illus-trated in Figure 1c. Specifically, SP points are adaptively located based on the input shape to cover the overall shape with only a small set of points. SP points learn its repre-sentation considering both spatial and semantic proximity through disentangled attention. Then, SPA non-locally dis-tributes information of SP points to each input point. We also show that our SPoTr block generalizes set abstrac-tion [27] with improved expressive power.
Further, we propose SPoTr architecture for standard point cloud tasks (e.g., shape classification and semantic segmentation). We conduct extensive experiments with three datasets: ScanObjectNN [12], SN-Part [28], and
S3DIS [29]. Our proposed method shows its effective-ness on all datasets compared to other attention-based meth-ods.
In particular, our architecture achieves an accu-racy improvement of 2.6% over the previous best model in shape classification with a real-world dataset ScanOb-jectNN. Additionally, we demonstrate the effectiveness and interpretability of self-positioning point-based attention with qualitative analyses.
The contribution of our paper can be summarized as the following:
• We design a novel Transformer architecture (SPoTr) to tackle the long-range dependency issues and the scala-bility issue of Transformer for point clouds.
• We propose a global cross-attention mechanism with flexible self-positioning points (SPA). SPA aggregates information on a few self-positioning points via disen-tangled attention and non-locally distributes informa-tion to semantically related points.
• SPoTr achieves the best performance on three point cloud benchmark datasets (SONN, SN-Part, and
S3DIS) against strong baselines.
• Our qualitative analyses show the effectiveness and in-terpretability of SPA. 2.