Abstract
Learning-based video compression has been extensively studied over the past years, but it still has limitations in adapting to various motion patterns and entropy models.
In this paper, we propose multi-mode video compression (MMVC), a block wise mode ensemble deep video com-pression framework that selects the optimal mode for fea-ture domain prediction adapting to different motion pat-terns. Proposed multi-modes include ConvLSTM-based fea-ture domain prediction, optical flow conditioned feature do-main prediction, and feature propagation to address a wide range of cases from static scenes without apparent mo-tions to dynamic scenes with a moving camera. We parti-tion the feature space into blocks for temporal prediction in spatial block-based representations. For entropy coding, we consider both dense and sparse post-quantization resid-ual blocks, and apply optional run-length coding to sparse residuals to improve the compression rate. In this sense, our method uses a dual-mode entropy coding scheme guided by a binary density map, which offers significant rate reduc-tion surpassing the extra cost of transmitting the binary se-lection map. We validate our scheme with some of the most popular benchmarking datasets. Compared with state-of-the-art video compression schemes and standard codecs, our method yields better or competitive results measured with PSNR and MS-SSIM. 1.

Introduction
Over the past several years, with the emergence and booming of short videos and video conferences across the world, video has become the major container of informa-tion and interaction among people on a daily basis. Conse-quently, we have been witnessing a vast demand increase on transmission bandwidth and storage space, together with the vibrant growth and discovery of handcrafted codecs such as AVC/H.264 [23], HEVC [23], and the recently released
*Equally contributed authors.
VVC [22], along with a number of learning based meth-ods [1, 7, 9, 11, 12, 15–17, 21, 27, 30, 31].
Prior works in deep video codecs have underlined the importance of utilizing and benefiting from deep neural net-work models, which can exploit complex spatial-temporal correlations and have the ability of ‘learning’ contextual and motion features. The main objective of deep video com-pression is to predict the next frame from previous frames or historical data, which results in the reduction of amount of residual information that needs to be encoded and trans-mitted. This has so far led to two directions: (1) to build ef-ficient prediction or estimation models, and extract motion information by leveraging the temporal correlation across the frames [1, 9, 16, 31]; (2) to make accurate estimation of the distribution of residual data and push down the in-formation entropy statistically by appropriate conditioning
[7, 11, 30]. The existing works usually fall in one or a com-bination of the above two realms. In the light of learning capability that deep neural networks can offer, we argue and demonstrate in this work that some measures of adaptively selecting the right mode among different available models in the encoding path can be advantageous on top of the ex-isting schemes, especially when the adaptive model selec-tion is applied at the block level in the feature space.
Drawing wisdom from conventional video codec stan-dards that typically address various types of motions (in-cluding the unchanged contextual information) in the unit of macroblocks, we present a learning-based, block wise video compression scheme that applies content-driven mode se-lection on the fly. Our proposed method consists of four modes targeting different scenarios:
• Skip mode (S) aims to utilize the frame buffer on the decoder and find the most condensed representation to transmit unchanged blocks to achieve the best possi-ble bitrate. This mode is particularly useful for static scenes where same backgrounds are captured by a fixed view camera.
• Optical Flow Conditioned Feature Prediction mode (OFC) leverages the temporal locality of motions. In
Figure 1. Overview of our proposed multi-mode video coding method. The current and previous frames are fed into the feature extractor and then go through branches of prediction modes followed by residual channel removal, quantization, and entropy coding process. We then select the optimal prediction and entropy coding schemes for each block that lead to the smallest code size. this mode, we capture the optical flow [24] between the past two frames, and the warped new frame is treated as a preliminary prediction to the current frame. This warping serves as the condition to provide guidance to the temporal prediction DNN model.
• Feature Propagation mode (FPG) applies to blocks where changes are detected, but there is no better pre-diction mode available. This mode copies the previ-ously reconstructed feature block as the prediction, and encodes the residual from there.
• For other generic cases, we propose the Feature Pre-diction mode (FP) for feature domain inter-frame pre-diction with a ConvLSTM network to produce a pre-dicted current frame (block).
Prior to the mode selection step, The transmitter pro-duces the optimal low-dimensional representation of each frame using a learned encoder and decoder pair based on the image compression framework in [14] for the mapping from pixel to feature space. The block by block difference between the previous frame and the current frame repre-sents the block wise motion. Unlike some state-of-the-art video compression frameworks that separately encode mo-tions and residuals, our method does not encode the motion as it is automatically generated by the prediction using the information available on both the transmitter and receiver.
To adapt to different dynamics that may exist even within a single frame for different blocks, our method evaluates multiple prediction modes that are listed above at the block level. As a result, we can always obtain residuals that have the highest sparsity thereby the shortest code length per block. Furthermore, we propose a residual channel remov-ing strategy to mask out residual channels that are inessen-tial to frame reconstruction, exploiting favorable tradeoffs between noticeably higher compression ratio and negligible quality degradation.
Technical contributions of this work are summarized as follows:
• We present MMVC, a dynamic mode selection-based video compression approach that adapts to different motion and contextual patterns with Skip mode and different feature-domain prediction paths in the unit of block.
• To improve the residual sparsity without losing much quality while minimizing the bitrate, we propose a block wise channel removal scheme and a density-adaptive entropy coding strategy.
• We perform extensive experiments and a compara-tive study to showcase that MMVC exhibits supe-rior or similar performance compared to state-of-the-art learning-based methods and conventional codecs.
In the ablation study, we show the effectiveness of our scheme by quantifying the utilization of different modes that varies by video contents and scenes.
2.