Abstract
We propose a text-to-image generation algorithm based on deep neural networks when text captions for images are unavailable during training. In this work, instead of simply generating pseudo-ground-truth sentences of training im-ages using existing image captioning methods, we employ a pretrained CLIP model, which is capable of properly align-ing embeddings of images and corresponding texts in a joint space and, consequently, works well on zero-shot recogni-tion tasks. We optimize a text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. To better align data in the two domains, we employ a principled way based on a varia-tional inference, which efficiently estimates an approximate posterior of the hidden text embedding given an image and its CLIP feature. Experimental results validate that the pro-posed framework outperforms existing approaches by large margins under unsupervised and semi-supervised text-to-image generation settings. 1.

Introduction
Recent advances in text-to-image (T2I) generation tech-niques [6, 8, 17, 18, 21, 26–29, 36] have shown promising results by employing generative adversarial networks [9], autoregressive models [33], or diffusion models [11, 32] to synthesize images based on their text captions. However, these approaches require a paired dataset that consists of images and their corresponding text captions, and, conse-quently, incur significant annotation costs, especially for la-beling image captions. To alleviate this limitation, unsu-pervised learning methods for T2I generation have recently drawn attention to the computer vision community, where the models learn to generate images without paired text cap-tions.
Existing T2I models [1, 2, 34, 38] based on unsupervised learning exploit Contrastive Language-Image Pretraining (CLIP) [25] to sidestep the absence of text captions dur-∗This work was partly done during an internship at Kakao Brain. ing training. Specifically, after a text embedding is esti-mated using a given image embedding, the T2I model is trained to synthesize an image conditioned on the estimated text embedding. However, although image and text embed-dings extracted by CLIP are not accurately aligned, exist-ing approaches assume that the distinction is ignorable [34] or simple to recover by just adding Gaussian noises [38] without considering the underlying structure of text embed-dings. Thus, those algorithms may suffer from large dis-crepancies between true and estimated text embeddings at both training and testing.
To tackle the challenge, we propose a variational distri-bution learning technique for unsupervised T2I generation, where the lower-bound of the data log-likelihood is maxi-mized in a principled way. Specifically, we first regard a text embedding as a hidden random variable while an image and its CLIP embedding are observable random variables. Then, we decompose the variational lower-bound into three parts: 1) the similarity between the text embedding prior and pos-terior, 2) the log-likelihood of the image embedding given the text embedding, 3) the log-likelihood of the image given the image and text embeddings in the trained T2I model.
Since the lower-bound formulation enforces the matching between the prior and posterior distributions of text embed-ding, our method achieves a more accurate estimation of the embedding and reduces the discrepancy between the true and estimated embeddings.
For the optimization of the variational objective, we em-ploy a two-stage training strategy for T2I models. In the first stage, we learn an encode-decoder architecture that takes the image embedding as an input and the estimated text em-bedding as a latent bottleneck. Then, our network estimates two conditional distributions of CLIP embeddings, one for the variational distribution of the text embedding given the image embedding and the other for the model distribution of the image embedding given the text embedding. The param-eters of the two distributions are obtained from the first two terms in the variational lower-bound objective. Note that we relax the Kullback-Leibler (KL) divergence term in the training objective of the first stage to an adversarial train-ing loss, specifically, the Jensen-Shannon divergence. Since
the KL divergence is only tractable for a confined family of distributions, this relaxation allows more flexible choices for the conditional and the prior distributions. In the sec-ond stage, a T2I model learns the conditional distribution of the images given the estimated text embeddings and the im-age features. Altogether, the proposed method achieves out-standing performance on widely adopted datasets [19, 31].
The main contributions of our work are summarized below:
• We propose a principled approach for unsupervised and semi-supervised text-to-image generation tasks based on a variational inference technique.
• We theoretically show that our method considers the underlying structure of text embeddings, which can eventually lead to better generalization performance.
• We empirically confirm that the proposed algorithm outperforms the existing methods by large margins.
The rest of our paper is organized as follows. Section 2 overviews the related work about unsupervised training methods in text-to-image generation. Section 3 describes the main idea of our approach while Sections 4 and 5 dis-cuss the procedures of the first and second training stages, respectively. The experimental results are presented in Sec-tion 6, and we finally conclude this paper in Section 7. 2.