Abstract
Recently, transformer-based methods have gained sig-nificant success in sequential 2D-to-3D lifting human pose estimation. As a pioneering work, PoseFormer captures spatial relations of human joints in each video frame and human dynamics across frames with cascaded transformer layers and has achieved impressive performance. However, in real scenarios, the performance of PoseFormer and its follow-ups is limited by two factors: (a) The length of the input joint sequence; (b) The quality of 2D joint detection.
Existing methods typically apply self-attention to all frames of the input sequence, causing a huge computational burden when the frame number is increased to obtain advanced es-timation accuracy, and they are not robust to noise natu-rally brought by the limited capability of 2D joint detectors.
In this paper, we propose PoseFormerV2, which exploits a compact representation of lengthy skeleton sequences in the frequency domain to efficiently scale up the receptive field and boost robustness to noisy 2D joint detection. With min-imum modifications to PoseFormer, the proposed method effectively fuses features both in the time domain and fre-quency domain, enjoying a better speed-accuracy trade-off than its precursor. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demon-strate that the proposed approach significantly outperforms the original PoseFormer and other transformer-based vari-ants. Code is released at https://github.com/
QitaoZhao/PoseFormerV2. 1.

Introduction 3D human pose estimation (HPE) aims at localizing human joints in 3-dimensional space based on monocular videos (without intermediate 2D representations) [23,25] or 2D human joint sequences (referred to as 2D-to-3D lifting
*Work was done while Qitao was an intern mentored by Chen Chen.
Figure 1. Comparisons of PoseFormerV2 and PoseFormerV1 [41] on Human3.6M [12]. RF denotes Receptive Field and k×RF indi-cates that the ratio between the full sequence length and the num-ber of frames as input into the spatial encoder of PoseFormerV2 is k, i.e., the RF of the spatial encoder is expanded by k× with a few low-frequency coefficients of the full sequence. The proposed method outperforms PoseFormerV1 by a large margin in terms of speed-accuracy trade-off, and the larger k brings more significant improvements, e.g., 4.6× speedup with the k of 27. approaches) [5, 17, 33, 39]. With the large availability of 2D human pose detectors [6, 24] plus the lightweight nature of 2D skeleton representation of humans, lifting-based meth-ods are now dominant in 3D human pose estimation. Com-pared to raw monocular videos, 2D coordinates of human joints in each video frame are much more memory-friendly, making it possible for lifting-based methods to utilize a long joint sequence to boost pose estimation accuracy.
Transformers [32] first gain huge success in the field of natural language processing (NLP) [3, 7] and then extend their capacity to the computer vision community, becom-ing the de facto approach for several vision tasks, e.g., im-age classification [8, 18, 31], object detection [4, 42] and video recognition [1,2,38]. The discreteness of human joint representation and the requirement for long-range temporal
for MHFormer [15]. However, densely applying self-attention to such long sequences is highly computation-ally expensive, e.g., the single-epoch wall-time training cost of 3-frame PoseFormer is ∼5 minutes while for 81-frame PoseFormer the cost surges to ∼1.5 hour on an RTX 3090 GPU. (b) The quality of 2D joint detection. 2D joint detectors inevitably introduce noise due to bias in their training dataset and the temporal inconsistency brought by the single-frame estimation paradigm. For example, Pose-Former achieves 31.3mm MPJPE (Mean Per Joint Position
Error) using the ground-truth 2D detection on the Hu-man3.6M dataset [12]. This result drops significantly to 44.3mm when the clean input is replaced by the CPN [6] 2D pose detection. In practice, the long-sequence inference may be unaffordable for hardware deployment on resource-limited devices such as AR/VR headsets and high-quality 2D detection is hard to obtain. More quantitative results about the efficiency to process long sequences and the ro-bustness to noisy 2D joint detection of existing transformer-based methods are available in Table 1.
Driven by these practical concerns, we raise two impor-tant research questions:
• Q1: How to efficiently utilize long joint sequences for bet-ter estimation precision?
• Q2: How to improve the robustness of the model against unreliable 2D pose detection?
Few works have tried to answer either of these two ques-tions by incorporating hand-crafted modules, e.g., the downsampling-and-uplifting module [9] that only processes a proportion of video frames for improved efficiency, the multi-hypothesis module [15] to model the depth ambiguity of body parts and the uncertainty of 2D detectors. How-ever, none of them manages to find a single solution to these two questions simultaneously, and even worse, a paradox seemingly exists between solutions to the questions above, e.g., multiple hypotheses [15] improve robustness but bring additional computation cost (see also Table 1).
In this paper, we present our initial attempt to “kill” two birds with one stone. With restrained modifications to the prior art PoseFormer, we show that the appropriate form of representation for input sequences might be the key to answering these questions simultaneously. Specifically, we shed light on the barely explored frequency domain in 3D
HPE literature and propose to encode the input skeleton se-quences into low-frequency coefficients. The insight be-hind this representation is surprisingly simple: On the one hand, low-frequency components are enough to represent the entire visual identity [34, 37] (e.g., 2D images in im-age compression and joint trajectories in this case), thus re-moving the need for expensive all-frame self-attention; On the other, the low-frequency representation of the skeleton sequence itself filters out high-frequency noise (jitters and outliers) [19, 20] contained in detected joint trajectories.
Figure 2. Overview of PoseFormerV1. PoseFormerV1 mainly consists of two modules: the spatial transformer encoder and the temporal transformer encoder. The temporal encoder of Pose-FormerV1 applies self-attention to all frames given a 2D joint se-quence for human motion modeling.
Table 1. The computational cost and performance drop brought by replacing ground-truth 2D detection with CPN [6] 2D pose detection for the SOTA transformer-based methods. The perfor-mance drop is reported on Human3.6M dataset (Protocol 1) [12].
RF: Receptive Field, sharing the same meaning as that in Fig. 1.
Method
PoseFormerV1 [41]
StridedTransformer [14]
MixSTE [40]
MHFormer [15]
P-STMO [29]
PoseFormerV2 (9×RF)
PoseFormerV2 (27×RF)
ICCV’21
TMM’22
CVPR’22
CVPR’22
ECCV’22
Seq.
Length 81 243 81 81 243 81 81
GFLOPs 1.36 1.37 92.46 3.12 1.74 0.35 0.12
Perform.
Drop (mm) 13.0 15.2 16.5 11.8 13.5 8.2 9.7 dependency modeling in a skeleton sequence make trans-formers an excellent fit for lifting-based human pose esti-mation. Previous works [14, 15, 29, 40, 41] have adopted transformers as the backbone for 3D human pose estima-tion and shown promising results.
As the pioneering work among transformer-based meth-ods, PoseFormer [41] factorizes joint sequence feature ex-traction into two stages (see Fig. 2) and outperforms tradi-tional convolution-based approaches. First, all joints within each frame are linearly projected into high-dimensional vectors (i.e., joint tokens) as input into the spatial trans-former encoder. The spatial encoder builds up inter-joint dependencies in single frames with the self-attention mech-anism. In the second stage, joint tokens of each frame are combined as one frame token, serving as input to the tem-poral encoder for human motion modeling across all frames in sequence. More details are included in Sec. 3.1.
Despite its capacity, the performance of PoseFormer (and other transformer-based methods) is limited by two crucial factors. (a) The length (number of frames) of the input 2D skeleton sequence. State-of-the-art transformer-based methods typically use extremely long sequences to obtain advanced performance, e.g., 81 frames for Pose-Former [41], 243 frames for P-STMO [14] and 351 frames
We inherit the spatial-temporal architecture from Pose-Former but force the spatial transformer encoder to only
“see” a few central frames in a long sequence. Then we complement “short-sighted” frame-level features (the out-put of the spatial encoder) with global features from low-frequency components of the complete sequence. Without resorting to the expensive frame-to-frame self-attention for all time steps, the temporal transformer encoder is reformu-lated as a Time-Frequency Feature Fusion module.
Extensive experiments on two 3D human pose es-timation benchmarks (i.e., Human3.6M [12] and MPI-INF-3DHP [21]) demonstrate that the proposed approach, dubbed as PoseFormerV2, significantly outperforms its precursor (see Fig. 1) and other transformer-based variants in terms of speed-accuracy trade-off and robustness to noise in 2D joint detection. Our contributions are three-fold:
• To the best of our knowledge, we are the first to utilize a frequency-domain representation of input joint sequences for 2D-to-3D lifting HPE. We find this representation an ideal fit to concurrently solve two important issues in the field (i.e., the efficiency to process long sequences and the robustness to unreliable joint detection), and experimen-tal evidence shows that this approach can easily general-ize to other models.
• We design an effective Time-Frequency Feature Fusion module to narrow the gap between features in the time do-main and frequency domain, enabling us to strike a flexi-ble balance between speed and accuracy.
• Our PoseFormerV2 outperforms other transformer-based methods in terms of the speed-accuracy trade-off and ro-bustness on Human3.6M and achieves the state-of-the-art on MPI-INF-3DHP. 2.