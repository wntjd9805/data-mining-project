Abstract
A natural image frequently contains multiple classiﬁ-cation targets, accordingly providing multiple class labels rather than a single label per image. While the single-label classiﬁcation is effectively addressed by applying a softmax cross-entropy loss, the multi-label task is tackled mainly in a binary cross-entropy (BCE) framework. In contrast to the softmax loss, the BCE loss involves issues regarding imbal-ance as multiple classes are decomposed into a bunch of binary classiﬁcations; recent works improve the BCE loss to cope with the issue by means of weighting. In this pa-per, we propose a multi-label loss by bridging a gap be-tween the softmax loss and the multi-label scenario. The proposed loss function is formulated on the basis of relative comparison among classes which also enables us to fur-ther improve discriminative power of features by enhanc-ing classiﬁcation margin. The loss function is so ﬂexible as to be applicable to a multi-label setting in two ways for discriminating classes as well as samples.
In the exper-iments on multi-label classiﬁcation, the proposed method exhibits competitive performance to the other multi-label losses, and it also provides transferrable features on single-label ImageNet training. Codes are available at https:
//github.com/tk1980/TwowayMultiLabelLoss. 1.

Introduction
Deep neural networks are successfully applied to super-vised learning [12, 26, 38] through back-propagation based on a loss function exploiting plenty of annotated samples.
In the supervised learning, classiﬁcation is one of primary tasks to utilize as annotation a class label to which an image sample belongs. As in ImageNet [7], most of image datasets provide a single class label per image, and a softmax loss is widely employed to deal with the single-label annotation, producing promising performance on various tasks.
The single-label setting, however, is a limited scenario from practical viewpoints. An image frequently contains multiple classiﬁcation targets [39], such as objects, requir-ing laborious cropping to construct single-label annotations.
There are also targets, such as visual attributes [25], which are hard to be disentangled and thereby incapable of produc-ing single-label instances. Those realistic situations pose so-called multi-label classiﬁcation where an image sample is equipped with multiple labels beyond a single label.
While a softmax loss works well in a single-label learn-ing, the multi-label tasks are addressed mainly by applying a binary cross-entropy (BCE) loss. Considering multiple labels are drawn from C class categories, the multi-label classiﬁcation can be decomposed into C binary classiﬁca-tion tasks, each of which focuses on discriminating samples in a target class category [28]; the BCE loss is well coupled with the decomposition approach. Such a decomposition, however, involves an imbalance issue. Even in a case of balanced class distribution, the number of positive samples is much smaller than that of negatives, as small portion of whole C-class categories are assigned to each sample as an-notation (positive) labels. The biased distribution is prob-lematic in a naive BCE loss. To cope with the imbalance issue in BCE, a simple weighting approach based on class frequencies [25] is commonly applied and in recent years it is further sophisticated by incorporating adaptive weighting scheme such as in Focal loss [18] and its variant [2]. On the other hand, the softmax loss naturally copes with multi-ple classes without decomposition nor bringing the above-mentioned imbalance issue; it actually works well in the balanced (single-label) class distribution. The softmax loss is intrinsically based on relative comparison among classes (3) which is missed in the BCE-based losses, though being less applicable to multi-label classiﬁcation.
In this paper, we propose a multi-label loss to effec-tively deal with multiple labels in a manner similar to the softmax loss. Through analyzing the intrinsic loss func-tion of the softmax loss, we formulate an efﬁcient multi-label loss function to exploit relative comparison between positive and negative classes. The relative comparison is related to classiﬁcation margin between positive and neg-ative classes, and we propose an approach to enlarge the margin by simply introducing temperature on logits for fur-ther boosting performance. The proposed loss function is regarded as a generalization of the softmax loss, being re-duced into the softmax loss in case of a single-label task.
Thus, the general loss formulation enables us to measure losses in two ways for computing multi-label classiﬁcation loss not only at each sample but also for each class to dis-criminate samples on that class as the BCE focuses on. In summary, our contributions are three-fold as follows.
• We formulate a new loss function to deal with multiple labels per sample while enhancing classiﬁcation margin.
• By using the loss function, we propose a two-way ap-proach for measuring multi-label loss.
• The loss is thoroughly analyzed from various aspects in the experiments and exhibits competitive performance, compared to the other multi-label losses. It also provides transferrable features on single-label ImageNet training. 1.1.