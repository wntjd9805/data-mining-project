Abstract
Recently, CLIP-guided image synthesis has shown ap-pealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual do-main labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some lim-itations in the quality of generated images and may suf-fer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals.
To address this issue, we propose an Image-specific Prompt
Learning (IPL) method, which learns specific prompt vec-*Equal contribution.
†Corresponding authors. tors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evalua-tions on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is inde-pendent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation. 1.

Introduction
In recent years, image synthesis using generative adver-sarial networks (GANs) [11] has been rapidly developed.
The state-of-the-art methods can generate images that are hard to be distinguished from real data [14, 20, 21, 46, 50].
However, the GAN-based methods heavily rely on vast quantities of training examples, and adopt a cumbersome adversarial training scheme which generally costs many hours of training time. Unfortunately, in many real-world scenarios, data acquisition is difficult or expensive. For ex-ample, in the artistic domains, it is impossible to have artists make thousands of creations. The high training cost is also unacceptable on some embedded devices, e.g., cellphones.
To address these issues, researchers begin to focus on the generative model adaptation. The goal of this task is to adapt a pre-trained source-domain generator to a target do-main with limited data. Many few-shot GAN-based meth-ods are proposed, such as TGAN [48], FreezeD [30], Min-GAN [47], ADA [18], DiffAug [53], IDC [33] and RSSA
[49], etc. However, these methods still require some train-ing images of the target domain and follow the adversarial training scheme. As a pioneer work, StyleGAN-NADA [8] (NADA for short) proposes a zero-shot adaptation method, which only requires textual domain labels and discards the cumbersome adversarial training scheme by introducing a pre-trained CLIP model. Although efficient, it still has obvi-ous deficiencies, i.e., the limited quality and mode collapse of generated images. As shown in Fig.1, we adapt a pre-trained generator of “Photo” domain to “Disney”, “Anime painting”, “Wall painting” and “Ukiyo-e” domains. For the results of NADA [8], we notice that the generated im-ages of the same target domain always show some homo-geneous patterns which degrade the image quality and di-versity, such as deep nasolabial folds in “Disney”, squinting eyes in “Anime painting”, red cheeks in “Wall painting” and blue eyebrows in “Ukiyo-e” (yellow box areas).
By exploring the factors behind this phenomenon, we find that the key factor is the fixed adaptation direction pro-duced by manually designed prompts. Sharing the direction for all cross-domain image pairs leads to identical supervi-sion signals for the model adaptation. Consider the exam-ple, adapting a generator of “Human” domain to “Tolkien elf” domain as shown in Fig.2. The previous works [8, 22] adopt manually designed prompts (e.g., “A photo of a”) plus the domain label to produce a fixed adaptation direction, which is shared by all cross-domain image pairs (Fig.2 (a)) in the adaptation process. We argue that the constraint is too restrictive and suppresses the image-specific features, leading to homogeneous generated patterns.
In this paper, we propose an Image-specific Prompt
Learning (IPL) method to address the above issue. The mo-tivation is setting more precise and diversified adaptation directions by customizing more image-specific prompts, for instance “Asian girl”, “Curly hair lady” and “Elder glass man” (Fig.2 (b)). These adaptation directions endow the target-domain generator with high flexibility to synthesize
Figure 2. An illustration of our motivation. The previous meth-ods adopt manual prompts to compute a fixed adaptation direction for all cross-domain image pairs, while our method learns image-specific prompts for producing more precise and diversified adap-tation directions. more diversified images. The proposed IPL is a two-stage method. In Stage 1, a latent mapper is trained to produce an image-specific set of prompt vectors conditioned on each source-domain image by a contrastive training scheme. The learned prompt vectors contain more specific and diversi-fied features of the source-domain images than the fixed prompt vectors. We further propose a domain regularization loss to ensure that the learned prompt vectors are compat-ible with the target domain. In Stage 2, we compute more precise and diversified adaptation directions for each cross-domain image pair, and train the target-domain generator with an adaptive directional CLIP loss, which can be viewed as an improved version of the Directional CLIP Loss [8].
As shown in Fig.1, our method alleviates the mode collapse issue well. Extensive experiments across a wide range of domains demonstrate that the proposed IPL effectively im-proves the quality of synthesized images and overcomes the mode collapse issue. User studies and ablation studies are also conducted to validate the effectiveness of our method.
It is worth noting that our proposed IPL method is inde-pendent of the structure of the generative model, and can be applied to the recent diffusion models [13, 27, 31, 35, 41– 43, 51]. Thus we also combine IPL with diffusion mod-els and get a more robust and stronger generative capacity, especially on complex images, which shows the high effec-tiveness and adaptability of our approach. 2.