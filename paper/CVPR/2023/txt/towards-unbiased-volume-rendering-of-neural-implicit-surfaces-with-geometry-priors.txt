Abstract
Learning surface by neural implicit rendering has been a promising way for multi-view reconstruction in recent years. Existing neural surface reconstruction methods, such as NeuS [24] and VolSDF [32], can produce reliable meshes from multi-view posed images. Although they build a bridge between volume rendering and Signed Distance
Function (SDF), the accuracy is still limited.
In this pa-per, we argue that this limited accuracy is due to the bias of their volume rendering strategies, especially when the view-ing direction is close to be tangent to the surface. We revise and provide an additional condition for the unbiased vol-ume rendering. Following this analysis, we propose a new rendering method by scaling the SDF field with the angle between the viewing direction and the surface normal vec-tor. Experiments on simulated data indicate that our render-ing method reduces the bias of SDF-based volume render-ing. Moreover, there still exists non-negligible bias when the learnable standard deviation of SDF is large at early stage, which means that it is hard to supervise the rendered depth with depth priors. Alternatively we supervise zero-level set with surface points obtained from a pre-trained
Multi-View Stereo network. We evaluate our method on the
DTU dataset and show that it outperforms the state-of-the-arts neural implicit surface methods without mask supervi-sion. 1.

Introduction 3D reconstruction is an important task in 3D games and
AR/VR applications. As a key technique in computer vi-sion and graphics, recovering surfaces and textures from
Multi-View calibrated RGB images has been widely studied in recent decades. Early unsupervised Multi-View Stereo (MVS) approaches [14, 20] provide solutions through a
*Corresponding author certain multistage pipeline, including grouping the related views, depth prediction, filtering with photometric con-sistency and geometry consistency, fusion of points from different views, meshing the dense points by off-the-shelf methods such as screened Poisson Surface Reconstruction
[8], and texture mapping finally.
Later MVS networks [5,20,27,36] are developed rapidly benefiting from the available large-scale 3D datasets. This kind of MVS networks use Convolutional Neural Network (CNN) to predict depth maps effectively, then follow the traditional pipeline to fuse a global dense point cloud and mesh it. However, MVS networks suffer from texture-less regions and sudden depth changes, so there usually exist many holes in the recovered meshes.
Recently, neural implicit surface and differentiable ren-dering methods present a promising way to improve and simplify the progress of the Multi-View 3D reconstruction.
The surfaces are represented as Signed Distance Functions (SDF) [18, 24, 32, 33] or occupancy field [16, 17]. At the same time, neural radiance field [13, 35] are proposed with different volume rendering. The neural surface-based ren-dering method can recover reliable and smooth surfaces, but it is hard to train without mask supervision. On the contrary, the different volume rendering can achieve good 2D views without mask supervision, but the quality of 3D geometry is rather coarse.
Is there some connections between the SDF field and occupancy field? NeuS [24] and VolSDF [32] point that the connection can be conducted with a certain Cumula-tive Distribution Function (CDF). Thanks to this significant progress, it is able to learn 3D surfaces effectively from neu-ral implicit surface with the self-supervised volume render-ing. The necessary input can only be well-posed 2D images.
Masks could be removed, because it is hard to obtain accu-rate masks for many complex objects in the real world.
Although these great methods have made big progress on 3D reconstruction from calibrated multi-view images,
the accuracy of meshes is still limited compared with those methods [5, 14, 20, 20, 27, 36] in a classical pipeline. We find that there exist unexpected convex or concave surfaces in regions with poor texture or strong highlight. In addition, the learned surface and rendered color tend to be smooth, and the high frequency details can not be captured well.
Based on NeuS [24] and VolSDF [32], we further ana-lyze the precision of the bridge between the SDF field and density field, and we found that there exist a bias between real depth and rendered depth by SDF-based volume ren-dering. We argue that there are two factors leading to the bias: (1) The angle between the view direction and the nor-mal vector; (2) The learnable standard deviation of the SDF field. The bias increases with the growth of the angle. It decreases with the descent of the learnable standard devia-tion, but it is still relatively large when the deviation is not small enough at early training stage. More details of the analysis are described in Section 3. In order to reduce the bias caused by the various angles, we modify the transfor-mation between the SDF field and the density field. Further-more, we adopt dense point clouds predicted by an acces-sible MVS network to reduce the bias further. Finally we evaluate our method and compare it with other SDF-based volume rendering methods on the public benchmark.
To summarise, we provides the following three contribu-tions: a) We amend the conditions of unbiased SDF-based volume rendering, and analyze the bias of VolSDF [32] and
NeuS [24]. b) We propose a new transformation between the SDF field and the density field, which does not re-quire a plane assumption and outperforms VolSDF [32] and
NeuS [24] even without geometry priors. In particular, we scale the SDF field by the inverse of the angle between the view direction and the normal vector. c) Geometry priors from a pre-trained MVS network are used with the anneal-ing sampling to further reduce the bias effectively at early training stage and boost the reconstruction quality. 2.