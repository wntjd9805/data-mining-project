Abstract
We introduce the SceneDiffuser, a conditional genera-tive model for 3D scene understanding. SceneDiffuser pro-vides a unified model for solving scene-conditioned gen-eration, optimization, and planning. In contrast to prior work, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy,
SceneDiffuser jointly formulates the scene-aware genera-tion, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differen-tiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previ-ous scene-conditioned generative models. We evaluate the
SceneDiffuser on various 3D scene understanding tasks, in-cluding human pose and motion generation, dexterous grasp
˚ These authors contributed equally to this work. (cid:0) Corresponding authors: Siyuan Huang (syhuang@bigai.ai) and Wei Liang (liangwei@bit.edu.cn). generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant im-provements compared with previous models, demonstrating the tremendous potential of the SceneDiffuser for the broad community of 3D scene understanding. 1.

Introduction
The ability to generate, optimize, and plan in 3D scenes is a long-standing goal across computer vision, graphics, and robotics. Various tasks have been devised to achieve these goals, fostering downstream applications in motion generation [32, 69, 73, 90], motion planning [42, 43, 60, 75], grasp generation [25, 31, 34], navigation [1, 95], scene syn-thesis [24, 47, 72, 82], embodied perception and manipula-tion [30, 39, 61], and autonomous driving [3, 52].
Despite rich applications and great successes, existing models designed for these tasks exhibit two fundamental limitations for real-world 3D scene understanding.
First, most prior work [8, 14, 32, 49, 60, 68–70, 73] lever-ages the conditional Variational Autoencoder (cVAE) for the conditional generation in 3D scenes. cVAE model utilizes an encoder-decoder structure to learn the posterior distri-bution and relies on the learned latent variables to sample.
Although cVAE is easy to train and sample due to its simple architecture and one-step sampling procedure, it suffers from the posterior collapse problem [12, 17, 26, 64, 69, 84, 93]; the learned latent variable is ignored by a strong decoder, leading to limited generation diversity from these collapsed modes. Such collapse is further magnified in 3D tasks with stronger 3D decoders and more complex and noisy input conditions, e.g., the natural 3D scans [9].
Second, despite the close relations among generation, optimization, and planning in 3D scenes, there lacks a uni-fied framework that could address existing discrepancies among these models. Previous work [15, 34, 69] applies off-the-shelf physics-based post-optimization methods over out-puts of generative models and often produces inconsistent and implausible generations, especially when transferring to novel scenes. Similarly, planners are usually standalone modules over results of generative model [8, 14] for trajec-tory planning or learned separately with the reinforcement learning (RL) [95], leading to gaps between planning and other modules (e.g., generation) during inference, especially in novel scenes where explorations are limited.
To tackle the above limitations, we introduce the
SceneDiffuser, a conditional generative model based on the diffusion process. SceneDiffuser eliminates the discrepan-cies and provides a single home for scene-conditioned gener-ation, optimization, and planning. Specifically, with a denois-ing process, it learns a diffusion model for scene-conditioned generation during training. In inference, SceneDiffuser jointly solves the scene-aware generation, physics-based optimization, and goal-oriented planning through a unified iterative guided-sampling framework. Such a design equips the SceneDiffuser with the following three superiority: 1. Generation: Building upon the diffusion model,
SceneDiffuser solves the posterior collapse problem of scene-conditioned generative models. Since the forward diffusion process can be treated as data augmentation in 3D scenes, it helps traverse sufficient scene-conditioned distribution modes. 2. Optimization: SceneDiffuser integrates the physics-based objective into each step of the sampling process as conditional guidance, enabling the differentiable physics-based optimization during both the learning and sampling process. This design facilitates the physically-plausible generation, which is critical for tasks in 3D scenes. 3. Planning: Based on the scene-conditioned trajectory-level generator, SceneDiffuser possesses a physics- and goal-aware trajectory planner, generalizing better to long-horizon trajectories and novel 3D scenes.
As illustrated in Fig. 1, we evaluate the SceneDiffuser on diverse 3D scene understanding tasks. The human pose, motion, and dexterous grasp generation results significantly improve, demonstrating plausible and diverse generations with the 3D scene and object conditions. The results on path planning for 3D navigation and motion planning for robot arms reveal the generalizable and long-horizon planning capability of the SceneDiffuser. 2.