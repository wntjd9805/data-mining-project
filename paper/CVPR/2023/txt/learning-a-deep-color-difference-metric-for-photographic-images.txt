Abstract
Most well-established and widely used color differ-ence (CD) metrics are handcrafted and subject-calibrated against uniformly colored patches, which do not general-ize well to photographic images characterized by natural scene complexities. Constructing CD formulae for photo-graphic images is still an active research topic in imag-ing/illumination, vision science, and color science commu-nities. In this paper, we aim to learn a deep CD metric for photographic images with four desirable properties. First, it well aligns with the observations in vision science that color and form are linked inextricably in visual cortical processing. Second, it is a proper metric in the mathemat-ical sense. Third, it computes accurate CDs between pho-tographic images, differing mainly in color appearances.
Fourth, it is robust to mild geometric distortions (e.g., trans-lation or due to parallax), which are often present in pho-tographic images of the same scene captured by different digital cameras. We show that all four properties can be satisfied at once by learning a multi-scale autoregressive normalizing flow for feature transform, followed by the Eu-clidean distance which is linearly proportional to the hu-man perceptual CD. Quantitative and qualitative experi-ments on the large-scale SPCD dataset demonstrate the promise of the learned CD metric. Source code is available at https://github.com/haoychen3/CD-Flow. 1.

Introduction
For a long time in vision science community, the mod-ular and segregated view of cortical color processing pre-dominated [46]: the visual perception/processing of color-related quantities is separate from and in parallel with the perception/processing of form (i.e., object shape and struc-ture), motion direction, and depth order in natural scenes.
As a result, vision scientists preferred to investigate color
*Corresponding author. perception under minimal conditions on form [20, 46], for example, using uniformly colored patches.
The idea that color as a visual sensation can be analyzed separately had a profound impact on the development of computational formulae for color difference (CD) assess-ment. Till now, the most well-established and widely used
CD metrics are primarily built upon the three-dimensional spatially-isotropic CIELAB coordinate system [32], recom-mended by the International Commission on Illumination (abbreviated as CIE from its French name Commission In-ternationale de l’Èclairage) in 1976. However, the unifor-mity1 of the CIELAB space is not as ideal as intended [28], even for uniformly colored patches. Thus, more complex and parametric formulae are proposed to rectify different as-pects of perceptual non-uniformity. Representative methods include JPC79 [33], CMC(l:c)2 [9], BFD(l:c) [31], CIE94
[34], and CIEDE2000 [29], in which the parameters are cal-ibrated by fitting the human perceptual CD measurements of uniformly colored patches. A naïve application of these metrics to photographic images is to compute the mean of the CDs between co-located pixels, which has been em-pirically shown to correlate poorly to human perception of
CDs [38].
Back to the vision science community, with more sup-porting evidence from psychophysical and perceptual stud-ies [2, 6, 27, 47], vision scientists have gradually come to agree on an alternative and more persuasive view of color perception: color and form (and motion) are inextricably interdependent as a unitary process of perceptual organiza-tion [19,46]. Even the primary visual cortex (i.e., V1) plays a significant role in color perception through two types of color-sensitive neurons: single-opponent and double-opponent cells. The single-opponent cells are sensitive to large areas of color, while the double-opponent cells re-spond to color patterns, textures, and boundaries [24, 46]. 1A system is perceptually uniform if a small perturbation to a compo-nent value is approximately equally perceptible across the range of that value [43]. 2l and c are two multiplicative parameters in the model to be fitted.
At later stages, color is transformed to more complex and abstract features, which represent the integral properties of objects, and remain consistent against the changes of the environmental illumination [17, 37].
Inspired by these scientific findings, researchers and engineers began to take spatial context (i.e., local sur-rounding regions) into account, when designing CD for-mulae. Representative strategies include low-pass spa-tial filtering [57], histograming [18], patch-based compar-ison [49], and texture-based segmentation [38]. Most re-cently, Wang et al. [51] established the largest photographic image dataset, SPCD, for perceptual CD assessment. They further trained a lightweight deep neural network (DNN) for CD assessment of photographic images in a data-driven fashion, as a generalization of several existing CD metrics built on the CIE colorimetry. Nonetheless, the learned for-mula may not be a proper metric, due to reliance on the possibly surjective mapping for feature transform.
In this work, we further pursue the data-driven approach.
We aim to learn a deep CD metric for photographic images with four desirable properties.
• It is conceptually inspired by color perception in the visual cortex. The design of our approach should re-spect the view that color and form interact inextricably through all stages of visual cortical processing.
• It is a proper metric that satisfies non-negativity, sym-metry, identity of indiscernibles, and triangle inequal-ity. Such design has been proven useful for perceptual optimization of image processing systems [11].
• It is accurate in predicting the human perceptual CDs of photographic images, with good generalization to uniformly colored patches.
• It is robust to mild geometric distortions (e.g., trans-lation and dilation), which are often present in photo-graphic images of the same scene captured with differ-ent camera settings or along different lines of sight.
We show that all the four desirable properties can be sat-isfied at once by learning a multi-scale autoregressive nor-malizing flow (a variant of RealNVP [13] to be specific) for feature transform, followed by Euclidean distance mea-sure in the transformed space. More specifically, we achieve the first property by the squeezing operation (also known as invertible downsampling) in the normalizing flow, which trades space size for channel dimension. The second prop-erty is a direct consequence of the bijectivity of the normal-izing flow and the Euclidean distance measure. We achieve the third property by optimizing the model parameters to ex-plain the human perceptual CDs in SPCD [51]. We achieve the fourth property by enforcing the normalizing flow to be multi-scale and autoregressive, in which the features at a particular scale are conditioned on those at a higher (i.e., coarser) scale. By doing so, our metric automatically learns to preferentially rely on coarse-scale feature representations with more built-in tolerance to geometric distortions for CD assessment.
We conduct extensive experiments on the large-scale
SPCD dataset [51], and find that our proposed metric, termed as CD-Flow, outperforms 15 CD formulae in as-sessing CDs of photographic images, produces competitive multi-scale local CD maps without any dense supervision, and is more robust to geometric distortions. Moreover, we empirically verify the perceptual uniformity of the learned color image representation from multiple aspects. 2.