Abstract
Pre-trained vision-language models (VLMs) learn to align vision and language representations on large-scale datasets, where each image-text pair usually contains a bag of semantic concepts. However, existing open-vocabulary object detectors only align region embeddings individually with the corresponding features extracted from the VLMs.
Such a design leaves the compositional structure of semantic concepts in a scene under-exploited, although the structure may be implicitly learned by the VLMs. In this work, we propose to align the embedding of bag of regions beyond in-dividual regions. The proposed method groups contextually interrelated regions as a bag. The embeddings of regions in a bag are treated as embeddings of words in a sentence, and they are sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which is learned to be aligned to the corresponding features extracted by a frozen VLM.
Applied to the commonly used Faster R-CNN, our approach surpasses the previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are available at https://github.com/wusize/ovdet. 1.

Introduction
A traditional object detector can only recognize categories learned in the training phase, restricting its application in the real world with a nearly unbounded concept pool. Open-vocabulary object detection (OVD), a task to detect objects whose categories are absent in training, has drawn increasing research attention in recent years.
A typical solution to OVD, known as the distillation-based approach, is to distill the knowledge of rich and unseen categories from pre-trained vision-language mod-els (VLMs) [21, 38]. In particular, VLMs learn aligned image and text representations on large-scale image-text pairs (Fig. 1(a)). Such general knowledge is beneficial for
OVD. To extract the knowledge, most distillation-based ap-*Corresponding author.
Figure 1. (a) Typical vision-language models (VLMs) learn to align representations of images and captions with rich compositional structure. (b) Existing distillation-based object detectors align each individual region embedding to features extracted by the frozen image encoder of VLMs. (c) Instead, the proposed method aligns the embedding of bag of regions. The region embeddings in a bag are projected to the word embedding space (dubbed as pseudo words), formed as a sentence, and then sent to the text encoder to obtain the bag-of-regions embedding, which is aligned to the corresponding image feature extracted by the frozen VLMs. proaches [10, 15, 52] align each individual region embed-ding to the corresponding features extracted from the VLM (Fig. 1(b)) with some carefully designed strategies.
We believe VLMs have implicitly learned the inherent compositional structure of multiple semantic concepts (e.g., co-existence of stuff and things [1, 23]) from a colossal amount of image-text pairs. A recent study, MaskCLIP [57], leverages such a notion for zero-shot segmentation. Existing distillation-based OVD approaches, however, have yet to fully exploit the compositional structures encapsulated in
VLMs. Beyond the distillation of individual region embed-ding, we propose to align the embedding of BAg of RegiONs, dubbed as BARON. Explicitly learning the co-existence of visual concepts encourages the model to understand the scene beyond just recognizing isolated individual objects.
BARON is easy to implement. As shown in Fig. 1(c),
BARON first samples contextually interrelated regions to form a ‘bag’. Since the region proposal network (RPN) is proven to cover potential novel objects [15,58], we explore a neighborhood sampling strategy that samples boxes around region proposals to help model the co-occurrence of a bag of visual concepts. Next, BARON obtains the bag-of-regions embeddings by projecting the regional features into the word embedding space and encoding these pseudo words with the text encoder (TE) of a frozen VLM [38]. By projecting region features to pseudo words, BARON naturally allows
TE to effectively represent the co-occurring semantic con-cepts and understand the whole scene. To retain the spatial information of the region boxes, BARON projects the box shape and box center position into embeddings and add to the pseudo words before feeding them to TE.
To train BARON, we align the bag-of-regions embed-dings to the teacher embeddings, which are obtained by feeding the image crops that enclose the bag of regions to the image encoder (IE) of the VLM. We adopt a contrastive learning approach [46] to learn the pseudo words and the bag-of-regions embeddings. Consistent with the VLMs’ pre-training (e.g., CLIP [38]), the contrastive loss pulls close corresponding student (the detector) and teacher (IE) embed-ding pairs and pushes away non-corresponding pairs.
We conduct extensive experiments on two challeng-ing benchmarks, OV-COCO and OV-LVIS. The proposed method consistently outperforms existing state-of-the-art methods [10, 52, 58] in different settings. Combined with
Faster R-CNN, BARON achieves a 34.0 (4.6 increase) box
AP50 of novel categories on OV-COCO and 22.6 (2.8 in-crease) mask mAP of novel categories on OV-LVIS. It is noteworthy that BARON can also distill knowledge from caption supervision – it achieves 32.7 box AP50 of novel cat-egories on OV-COCO, outperforming previous approaches that use COCO caption [13, 53, 56, 58]. 2.