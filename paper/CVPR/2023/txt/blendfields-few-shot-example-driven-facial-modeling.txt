Abstract
Generating faithful visualizations of human faces re-quires capturing both coarse and ﬁne-level details of the face geometry and appearance. Existing methods are either data-driven, requiring an extensive corpus of data not pub-licly accessible to the research community, or fail to cap-ture ﬁne details because they rely on geometric face mod-els that cannot represent ﬁne-grained details in texture with a mesh discretization and linear deformation designed to model only a coarse face geometry. We introduce a method that bridges this gap by drawing inspiration from tradi-tional computer graphics techniques. Unseen expressions
†Work done during an internship at Microsoft Research Cambridge.
‡Work done at Simon Fraser University. are modeled by blending appearance from a sparse set of extreme poses. This blending is performed by measuring local volumetric changes in those expressions and locally reproducing their appearance whenever a similar expres-sion is performed at test time. We show that our method generalizes to unseen expressions, adding ﬁne-grained ef-fects on top of smooth volumetric deformations of a face, and demonstrate how it generalizes beyond faces. 1.

Introduction
Recent advances in neural rendering of 3D scenes [53] offer 3D reconstructions of unprecedented quality [36] with
[23, 29]. Human an ever-increasing degree of control faces are of particular interest to the research commu-NeRF [36] NeRFies [42] HyperNeRF [43] NeRFace [13] NHA [17] AVA [7] VolTeMorph [15] Ours
Applicability beyond faces
Interpretable control
Data efﬁciency
Expression-dependent changes
Generalizability to unknown expressions
✓
✗
✗
✗
✗
✓
✗
✓
✗
✗
✓
✗
✓
✓
✗
✗
✓
✗
✓
✓
✗
✓
✓
✓
✓
✗
✗
✗
✓
✗
✓
✓
✓
✗
✓
✓
✓
✓
✓
✓
Table 1. Comparison – We compare several methods to our approach. Other methods fall short in data efﬁciency and applicability. For example, AVA [7] requires 3.1 million training images while VolTeMorph [15] cannot model expression-dependent wrinkles realistically. nity [1, 13–15] due to their application in creating realistic digital doubles [32, 53, 75, 79].
To render facial expressions not observed during train-ing, current solutions [1, 13–15] rely on parametric face models [6]. These allow radiance ﬁelds [36] to be con-trolled by facial parameters estimated by off-the-shelf face trackers [27]. However, parametric models primarily cap-ture smooth deformations and lead to digital doubles that lack realism because ﬁne-grained and expression-dependent phenomena like wrinkles are not faithfully reproduced.
Authentic Volumetric Avatars (AVA) [7] overcomes this issue by learning from a large multi-view dataset of syn-chronized and calibrated images captured under controlled lighting. Their dataset covers a series of dynamic facial expressions and multiple subjects. However, this dataset remains unavailable to the public and is expensive to re-produce. Additionally, training models from such a large amount of data requires signiﬁcant compute resources. To democratize digital face avatars, more efﬁcient solutions in terms of hardware, data, and compute are necessary.
We address the efﬁciency concerns by building on the
In recent works in Neural Radiance Fields [15, 70, 74]. particular, we extend VolTeMorph [15] to render facial de-tails learned from images of a sparse set of expressions.
To achieve this, we draw inspiration from blend-shape cor-rectives [26], which are often used in computer graph-ics as a data-driven way to correct potential mismatches between a simpliﬁed model and the complex phenomena it aims to represent. this mismatch is caused by the low-frequency deformations that the tetrahe-dral mesh from VolTeMorph [15], designed for real-time performance, can capture, and the high-frequency nature of expression wrinkles.
In our setting,
We train multiple radiance ﬁelds, one for each of the K sparse expressions present in the input data, and blend them to correct the low-frequency estimate provided by VolTe-Morph [15]; see Fig. 1. We call our method BlendFields since it resembles the way blend shapes are employed in 3DMMs [6]. To keep K small (i.e., to maintain a few-shot regime), we perform local blending to exploit the known correlation between wrinkles and changes in local differen-tial properties [21,45]. Using the dynamic geometry of [15], local changes in differential properties can be easily ex-tracted by analyzing the tetrahedral representation under-lying the corrective blendﬁelds of our model.
Contributions. We outline the main qualitative differences between our approach and related works in Tab. 1, and our empirical evaluations conﬁrm these advantages.
In sum-mary, we:
• extend VolTeMorph [15] to enable modeling of high-frequency information, such as expression wrinkles in a few-shot setting;
• introduce correctives [6] to neural ﬁeld representations and activate them according to local deformations [45];
• make this topic more accessible with an alternative to techniques that are data and compute-intensive [7];
• show that our model generalizes beyond facial modeling, for example, in the modeling of wrinkles on a deformable object made of rubber. 2.