Abstract more realistic talking face videos compared to previous methods and faithfully generalize to unseen identities.
Generating a talking face video from the input audio sequence is a practical yet challenging task. Most existing methods either fail to capture fine facial details or need to train a specific model for each identity. We argue that a codebook pre-learned on high-quality face images can serve as a useful prior that facilitates high-fidelity and generalizable talking head synthesis. Thanks to the strong capability of the codebook in representing face textures, we simplify the talking face generation task as finding proper lip-codes to characterize the variation of lips during portrait talking. To this end, we propose LipFormer, a
Transformer-based framework to model the audio-visual coherence and predict the lip-codes sequence based on input audio features. We further introduce an adaptive face warping module, which helps warp the reference face to the target pose in the feature space, to alleviate the difficulty of lip-code prediction under different poses.
By this means, LipFormer can make better use of pre-learned priors in images and is robust to posture change.
Extensive experiments show that LipFormer can produce 1.

Introduction
As an ongoing research topic, talking face generation aims to build a cross-modal mapping from an audio se-quence to a face video while maintaining natural speaking styles and audio-visual coherence. It has received growing attention in recent years due to its potential in digital humans, film-making, virtual video conferences, and online education [3, 10, 37–39, 44, 45, 47, 48, 50, 53].
A high-fidelity and generalizable talking face generation model relies heavily on high-quality (HQ) video data with vast identities. However, the existing datasets still suffer from two limitations: (1) low resolution and qualities, e.g.,
LRW [5] and LRS2 [1], leading to the learned model an unsatisfying synthesis quality; (2) a limited number of identities despite the clear videos, e.g., Obama [17, 31] and privately recorded data [28], which requires training a specific model for each person and it is hard to generalize to unseen portraits. their practical applications, and it is also a challenge to collect
These two drawbacks limit
a mass of such high-quality videos because they should simultaneously meet the phoneme balance and audio-visual synchronization demands. In contrast, we notice that there are many publicly available datasets of high-resolution face images, e.g., the FFHQ [15] dataset contains 70,000 identities with 1024 × 1024 resolutions.
It helps raise a question: could these image datasets benefit the generation of a talking portrait?
Fortunately, the answer is a big yes. In this work, we confirm that a high-quality pre-learned facial codebook can serve as a strong prior and facilitate talking head synthesis from the perspectives of visual fidelity and generalizability.
The codebook, which is learned with the objective to reconstruct 2D faces, is capable of representing diverse face details, and hence takes most of the responsibilities to synthesize the appearance of the talking face. That way, the only thing left is to characterize the variation of lips when people talk [24, 25].We can therefore reformulate the task of talking face generation as a simpler problem, namely, finding proper lip-codes for the input face.
To this end, we propose LipFormer, a Transformer-based framework for high-fidelity talking face synthesis.
In particular, LipFormer first encodes the target face us-ing the pre-learned codebook, and then replaces the lip-region features with a new sequence of lip-codes that are predicted by aligning with the reference audio. Before lip-code prediction, we introduce an Adaptive Face Warping
Module, which helps warp the reference face to the target pose in the feature (i.e., codebook) space, to alleviate the texture mismatch problem caused by different poses.
Last but not least, LipFormer is trained in an end-to-end way to make different modules collaborate fully, boosting the final performance. Experiments on multiple datasets confirm the superiority of our approach over state-of-the-art methods [24,51] from the perspectives of both video quality and generalizability to unseen identities. 2.