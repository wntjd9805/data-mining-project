Abstract 1.

Introduction
Cinemagraphs are short looping videos created by adding subtle motions to a static image. This kind of media is popular and engaging. However, automatic generation of cinemagraphs is an underexplored area and current solu-tions require tedious low-level manual authoring by artists.
In this paper, we present an automatic method that allows generating human cinemagraphs from single RGB images.
We investigate the problem in the context of dressed humans under the wind. At the core of our method is a novel cyclic neural network that produces looping cinemagraphs for the target loop duration. To circumvent the problem of collect-ing real data, we demonstrate that it is possible, by working in the image normal space, to learn garment motion dynam-ics on synthetic data and generalize to real data. We evalu-ate our method on both synthetic and real data and demon-strate that it is possible to create compelling and plausible cinemagraphs from single RGB images.
Cinemagraph, a term originally coined by Jamie Beck and Kevin Burg, refers to adding dynamism to still images by adding minor and repeated movements, forming a mo-tion loop, to a still image. Such media format is both en-gaging and intriguing, as adding a simple and subtle motion can bring images to life. Creating such content, however, is challenging as it would require an artist to first set up and capture a suitable video, typically using a tripod, and then carefully mask out most of the movements in a post-processing stage.
We explore the problem of creating human cinemagraphs directly from a single RGB image of a person. Given a dataset of images and corresponding animated video pairs, a straightforward solution would be to train a fully super-vised network to learn to map an input image to a plausible animated sequence. However, collecting such a dataset is extremely challenging and costly, as it would require cap-turing hundreds or thousands of videos of people holding
a perfectly still pose under the influence of the wind from different known directions. While it is possible to simulate different wind force directions using oscillating fans in a lab setup [10], capturing the variability of garment geome-try and appearance types in such a controlled setting is far from trivial. Hence, we explore the alternative approach of using synthetic data where different wind effects can eas-ily be replicated using physically-based simulation. The challenge, then, is to close the synthetic-to-real gap, both in terms of garment dynamics and appearance variations.
We address this generalization concern by operating in the gradient domain, i.e., using surface normal maps. Be-ing robust to lighting or appearance variations, surface nor-mals are arguably easier to generalize from synthetic to real, compared to RGB images. Moreover, surface nor-mals are indicative of the underlying garment geometry (i.e., folds and wrinkles) and hence provide a suitable repre-sentation to synthesize geometric and resultant appearance variations [21, 44] as the garment interacts with the wind.
Further, we make the following technical contributions.
First, we propose a novel cyclic neural network formulation that directly outputs looped videos, with target time periods, without suffering from any temporal jumps. Second, we demonstrate how to condition the model architecture using wind parameters (e.g., direction) to enable control at test time. Finally, we propose a normal-based shading approach that takes the intermediate normals under the target wind attributes to produce RGB image frames. In Figure 1, we show that our method is applicable to a variety of real test images of different clothing types.
We evaluate our method on both synthetic and real im-ages and discuss ablation results to evaluate the various de-sign choices. We compare our approach against alternative approaches [27, 38] using various metrics as well as a user study to evaluate the plausibility of the generated methods.
Our method achieves superior performance both in terms of quantitative metrics as well as the perceptual user study. 2.