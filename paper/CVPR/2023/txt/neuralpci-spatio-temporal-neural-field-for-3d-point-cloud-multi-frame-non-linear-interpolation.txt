Abstract
In recent years, there has been a significant increase in focus on the interpolation task of computer vision. Despite the tremendous advancement of video interpolation, point cloud interpolation remains insufficiently explored. Mean-while, the existence of numerous nonlinear large motions in real-world scenarios makes the point cloud interpolation task more challenging. In light of these issues, we present
NeuralPCI: an end-to-end 4D spatio-temporal Neural field for 3D Point Cloud Interpolation, which implicitly inte-grates multi-frame information to handle nonlinear large motions for both indoor and outdoor scenarios. Further-more, we construct a new multi-frame point cloud interpo-lation dataset called NL-Drive for large nonlinear motions in autonomous driving scenes to better demonstrate the su-periority of our method. Ultimately, NeuralPCI achieves state-of-the-art performance on both DHB (Dynamic Hu-man Bodies) and NL-Drive datasets. Beyond the interpola-tion task, our method can be naturally extended to point cloud extrapolation, morphing, and auto-labeling, which indicates its substantial potential in other domains. Codes are available at https:// github.com/ ispc-lab/ NeuralPCI. 1.

Introduction
In the field of computer vision, sequential point clouds are frequently utilized in many applications, such as VR/AR techniques [11, 38, 49] and autonomous driving [4, 32, 46].
The relatively low frequency of LiDAR compared to other sensors, i.e., 10–20 Hz, impedes exploration for high tem-poral resolution point clouds [47]. Therefore, interpolation tasks for point cloud sequences, which have not been sub-stantially investigated, are receiving increasing attention.
With the similar goal of obtaining a smooth sequence with high temporal resolution, we can draw inspiration from the video frame interpolation (VFI) task. Several VFI meth-ods [6,8,20,35,45,48] concentrate on nonlinear movements in the real world. They take multiple frames as input and
∗ Equal contribution. † Corresponding author.
Figure 1. Common cases of nonlinear motions in autonomous driving scenarios. Spatially uniform linear interpolation ( ) using the middle two frames of the point cloud differs significantly from the actual situation ( ), so it is necessary to take multiple point clouds into consideration for nonlinear interpolation. generate explicit multi-frame fusion results based on flow estimation [6, 8, 20, 45, 48] or transformer [35]. Nonethe-less, due to the unique structure of point clouds [31], it is non-trivial to extend VFI methods to the 3D domain.
Some early works [18,19] rely on stereo images to gener-ate pseudo-LiDAR point cloud interpolation. For pure point cloud input, previous methods [22,47] take two consecutive frames as input and output the point cloud at a given inter-mediate moment. However, with limited two input frames, these approaches can only produce linear interpolation re-sults [22], or perform nonlinear compensation by fusing in-put frames in the feature dimension linearly [47], which is inherently a data-driven approach to learning the dataset-specified distribution of nonlinear motions rather than an actual nonlinear interpolation. Only when the frame rate of the input point cloud sequence is high enough or the object motion is small enough, can the two adjacent point clouds satisfy the linear motion assumption. Nonetheless, there are numerous nonlinear motions in real-world cases. For instance, as illustrated in Fig. 1, the result of linear interpo-lation between two adjacent point cloud frames has a large deviation from the actual situation. A point cloud sequence rather than just two point cloud frames allows us to view further into the past and future. Neighboring multiple point
clouds contain additional spatial-temporal cues, namely dif-ferent perspectives, complementary geometry, and implicit high-order motion information. Therefore, it is time to re-think the point cloud interpolation task with an expanded design space, which is an open challenge yet.
Methods that explicitly fuse multiple point cloud frames generally just approximate the motion model over time, which actually simplifies real-world complex motion. The neural field provides a more elegant way to parameterize the continuous point cloud sequence implicitly. Inspired by
NeRF [25] whose view synthesis of images is essentially an interpolation, we propose NeuralPCI, a neural field to exploit the spatial and temporal information of multi-frame point clouds. We build a 4D neural spatio-temporal field, which takes sequential 3D point clouds and the indepen-dent interpolation time as input, and predicts the in-between or future point cloud at the given time. Moreover, Neu-ralPCI is optimized on runtime in a self-supervised man-ner, without relying on costly ground truths, which makes it free from the out-of-the-distribution generalization prob-lem. Our method can be flexibly applied to segmentation auto-labeling and morphing. Besides, we newly construct a challenging multi-frame point cloud interpolation dataset called NL-Drive from public autonomous driving datasets.
Finally, we achieve state-of-the-art performance on indoor
DHB dataset and outdoor NL-Drive dataset.
Our main contributions are summarized as follows:
• We propose a novel multi-frame point cloud interpola-tion algorithm to deal with the nonlinear complex mo-tion in real-world indoor and outdoor scenarios.
• We introduce a 4D spatio-temporal neural field to in-tegrate motion information implicitly over space and time to generate the in-between point cloud frames at the arbitrary given time.
• A flexible unified framework to conduct both the inter-polation and extrapolation, facilitating several applica-tions as well. 2.