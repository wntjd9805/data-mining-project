Abstract
Mean ensemble (i.e. averaging predictions from multiple models) is a commonly-used technique in machine learning that improves the performance of each individual model. We formalize it as feature alignment for ensemble in open-set face recognition and generalize it into Bayesian Ensemble
Averaging (BEA) through the lens of probabilistic modeling.
This generalization brings up two practical benefits that ex-isting methods could not provide: (1) the uncertainty of a face image can be evaluated and further decomposed into aleatoric uncertainty and epistemic uncertainty, the latter of which can be used as a measure for out-of-distribution detection of faceness; (2) a BEA statistic provably reflects the aleatoric uncertainty of a face image, acting as a mea-sure for face image quality to improve recognition perfor-mance. To inherit the uncertainty estimation capability from
BEA without the loss of inference efficiency, we propose
BEA-KD, a student model to distill knowledge from BEA.
BEA-KD mimics the overall behavior of ensemble members and consistently outperforms SOTA knowledge distillation methods on various challenging benchmarks. 1.

Introduction
Knowledge Distillation (KD) is an active research area that has profound benefits for model compression, wherein competitive recognition performance can be achieved by smaller models (student models) via a distillation process from teacher models. As such, smaller models can be de-ployed into space-constrained environments such as mobile and embedded devices.
There has been abundant literature in KD for face recog-nition [22, 33, 34]. However, all the existing approaches fall into the “one-teacher-versus-one-student” paradigm. This learning paradigm has several limitations. Firstly, a single teacher can be biased, which further results in biased esti-*Equal contribution.
Figure 1. A conceptual illustration of BEA and BEA-KD. Given a face image x, we have n = 5 probabilistic ensemble members
{r-vMF(µi, κi)}n i=1 (marked by light blue). Bayesian ensemble averaging (marked by dark blue) returns a single r-vMF( ˜µx, ¯κ(n) x ) that accounts for the expected positions and confidence by all the ensemble members. To emulate the ensemble’s probabilistic be-havior, we employ a parametrized distribution qϕ,φ(z|x) to ap-proximate BEA. mates of face feature embeddings given by a student after knowledge distillation from the biased teacher. Secondly, it only yields point estimates of face feature embeddings, un-able to provide uncertainty measure for face recognition in a safety-sensitive scenario.
Compared with single-teacher KD, KD from multiple teachers (a.k.a. ensemble KD) is beneficial and has been extensively explored in literature [9, 11, 29, 32, 36, 37].
However, these approaches are designed solely for closed-set classification tasks, distilling logits in a fixed simplex space via KL divergence (as the label set remains the same throughout training and test). In contrast, face recognition is inherently an open-set problem where classes cannot be known a priori. More specifically, face identities appear-ing during the inference stage scarcely overlap with those in the training phase. Consequently, without a fixed sim-plex space for logit distillation, existing approaches cannot be readily applied to face recognition. As will be shown in our empirical studies, existing closed-set KD approaches
exhibit inferior performance in face recognition tasks with million-scale label sets.
How we treat teachers highly affects KD performance.
Unlike prior art [20, 26] that takes average of predictions (termed ‘mean ensemble’ throughout this paper), we treat teachers as draws in a probabilistic manner. We find that this treatment leads to a generalization of mean ensemble, namely Bayesian Ensemble Averaging (BEA), which fur-ther brings up two practical benefits that existing methods could not provide: (1) the uncertainty of a face image can be evaluated and further decomposed into aleatoric uncer-tainty and epistemic uncertainty [6, 7], the latter of which can be used as a measure for out-of-distribution detection of faceness, i.e., distinguishing non-face images from face images; (2) a BEA statistic provably reflects the aleatoric uncertainty of a face image, which acts as a measure for face image quality, thereby improving recognition perfor-mance.
In addition, as ensemble methods are known to be com-putation costly during inference, we expect a more effi-cient model to inherit the uncertainty estimation capabil-ity from BEA. To this end, we propose BEA-KD, a stu-dent model that distills from BEA not only its feature em-beddings but also the BEA statistic (and thus the aleatoric uncertainty). Consequently, BEA-KD consistently outper-forms SOTA KD methods on various benchmarks. Our con-tribution can be summarized as follows: (1) We recognize the benefit of multiple teachers (ensem-ble) for face recognition and present Bayesian Ensemble
Averaging (BEA) as a generalization through the lens of probabilistic modelling. (2) We verify that the proposed BEA can capture aleatoric uncertainty and epistemic uncertainty theoretically and em-pirically. (3) We propose BEA-KD, a single smaller (hence efficient) model that inherits the power of uncertainty estimation from
BEA yet reduces the high computational cost of BEA infer-ence. (4) We verify the BEA and BEA-KD’s superior perfor-mance, respectively, as compared with mean ensemble and
SOTA KD methods through extensive experiments. 2. Preliminaries and