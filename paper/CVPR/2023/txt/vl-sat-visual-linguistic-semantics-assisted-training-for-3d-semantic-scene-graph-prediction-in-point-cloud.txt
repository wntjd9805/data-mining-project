Abstract
The task of 3D semantic scene graph (3DSSG) predic-tion in the point cloud is challenging since (1) the 3D point cloud only captures geometric structures with limited se-mantics compared to 2D images, and (2) long-tailed re-lation distribution inherently hinders the learning of unbi-ased prediction. Since 2D images provide rich semantics and scene graphs are in nature coped with languages, in this study, we propose Visual-Linguistic Semantics Assisted
Training (VL-SAT) scheme that can significantly empower 3DSSG prediction models with discrimination about long-tailed and ambiguous semantic relations. The key idea is to train a powerful multi-modal oracle model to as-sist the 3D model. This oracle learns reliable structural representations based on semantics from vision, language, and 3D geometry, and its benefits can be heterogeneously passed to the 3D model during the training stage. By effectively utilizing visual-linguistic semantics in training, our VL-SAT can significantly boost common 3DSSG pre-diction models, such as SGFN and SGGpoint, only with 3D inputs in the inference stage, especially when dealing with tail relation triplets. Comprehensive evaluations and ab-lation studies on the 3DSSG dataset have validated the ef-fectiveness of the proposed scheme. Code is available at https://github.com/wz7in/CVPR2023-VLSAT. 1.

Introduction
Structurally understanding 3D geometric scenes is par-ticularly important for tasks that require interaction with real-world environments, such as AR/VR [7, 21, 26–28, 49, 51] and navigation [4, 5, 10]. As one vital topic in this field, predicting 3D semantic scene graph (3DSSG) in point cloud [36] has received emerging attention in recent years.
Specifically, given the point cloud of a 3D scene that is as-sociated with class-agnostic 3D instance masks, the 3DSSG
*Lu Sheng and Yang Tang are the corresponding authors.
Figure 1. Comparison between previous method and our VL-SAT. (a) SGPN [36], as the 3D model, fails to find capture predi-cates such as build in. (b) VL-SAT creates an oracle model by het-erogeneously fusing 2D semantics, and language knowledge along with the geometrical features, and the 3D model receives benefits from the oracle model during training. During inference, the en-hanced 3D model can correctly detect the tail predicates. prediction task would like to construct a directed graph whose nodes are semantic labels of the 3D instances and the edges recognize the directional semantic or geometrical relations between connected 3D instances.
However, in addition to common difficulties faced by scene graph prediction, there are several challenges spec-ified to the 3DSSG prediction task. (1) 3D data such as point clouds only capture the geometric structures of each instance and may superficially define the relations by rela-tive orientations or distances. (2) Recent 3DSSG predica-tion datasets [36, 47] are quite small and suffer from long-tailed predicate distributions, where semantic predicates are often rarer than geometrical predicates. For example, as shown in Fig. 1, the pioneering work SGPN [36] usually
prefers a simple and common geometric predicate standing on between sink and bath cabinet, while the ground-truth re-lation triplet ⟨sink, build in, bath cabinet⟩ cares more about the semantics, and the frequency of build in in the training dataset is quite low, as shown in Fig. 3(a). Even though some attempts [38, 47, 48] have been proposed thereafter, the inherent limitations of the point cloud data to some ex-tent hinder the effectiveness of these methods.
Since 2D images provide rich and meaningful seman-tics, and the scene graph prediction task is in nature aligned with natural languages, we explore using visual-linguistic semantics to assist the training, as another pathway to essen-tially enhance the capability of common 3DSSG prediction models with the aforementioned challenges.
How to assist 3D structural understanding with visual-linguistic semantics remains an open problem. Previous studies mainly focus on employing 2D semantics to en-hance instance-level tasks, such as object detection [3, 20, 22,29], visual grounding and dense captioning [4,5,44,50].
Most of them require visual data both in training and in-ference, but a few of them, such as SAT [42] and X -Trans2Cap [44] treat 2D semantics as auxiliary training sig-nals and thus offer more practical inference only with 3D data. But these methods are specified to instance-level tasks and require delicately designed networks for effective as-sistance, thus they are less desirable to our structural pre-diction problem. Thanks to the recent success of large-scale cross-modal pretraining like CLIP [24], 2D semantics in images can be well aligned with linguistic semantics in natural languages, and the visual-linguistic semantics have been applied for alleviating long-tailed issue in tasks related to 2D scene graphs [1,31,32,45] and human-object interac-tion [15]. But how to adapt similar assistance of visual-linguistic semantics to the 3D scenario remains unclear.
In this study, we propose the Visual-Linguistic Seman-tics Assisted Training (VL-SAT) scheme to empower the point cloud-based 3DSSG prediction model (termed as the 3D model) with sufficient discrimination about long-tailed and ambiguous semantic relation triplets. In this scheme, we simultaneously train a powerful multi-modal prediction model as the oracle (termed as oracle model) that is het-erogeneously aligned with the 3D model, which captures reliable structural semantics by extra data from vision, ex-tra training signals from language, as well as the geometri-cal features from the 3D model. These introduced visual-linguistic semantics have been aligned by CLIP. Conse-quently, the benefits of the oracle model, especially the multi-modal structural semantics, can be efficiently embed-ded into the 3D model through the back-propagated gradi-ent flows. In the inference stage, the 3D model can perform superior 3DSSG prediction performance with only 3D in-puts. For example, in Fig. 1(b), the predicate build in can be reliably detected. To our best knowledge, VL-SAT is the first visual-linguistic knowledge transfer work that is applied to 3DSSG prediction in the point cloud. More-over, VL-SAT can successfully enhance SGFN [38] and
SGGpoint [47], validating that this scheme is generalizable to common 3DSSG prediction models.
We benchmark VL-SAT on the 3DSSG dataset [36].
Quantitative and qualitative evaluations, as well as compre-hensive ablation studies, validate that the proposed training scheme leads to significant performance gains, especially for tail relations, as discussed in Sec. 4. 2.