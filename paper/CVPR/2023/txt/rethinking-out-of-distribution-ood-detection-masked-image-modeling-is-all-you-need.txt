Abstract
The core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is dis-tinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive repre-sentations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the per-formance of OOD detection significantly. We deeply ex-plore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to pro-vide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Mod-eling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even defeats the 10-shot-per-class out-lier exposure OOD detection, although we do not include any OOD samples for our detection. Codes are available at https://github.com/lijingyao20010602/MOOD. 1.

Introduction
A reliable visual recognition system not only pro-vides correct predictions on known context (also known as in-distribution data) but also detects unknown out-of-distribution (OOD) samples and rejects (or transfers) them to human intervention for safe handling. This motivates ap-plications of outlier detectors before feeding input to the downstream networks, which is the main task of OOD de-tection, also referred to as novelty or anomaly detection.
OOD detection is the task of identifying whether a test sam-ple is drawn far from the in-distribution (ID) data or not. It is at the cornerstone of various safety-critical applications, including medical diagnosis [5], fraud detection [45], au-tonomous driving [14], etc.
Figure 1. Performance of MOOD compared with current SOTA (indicated by ‘*’) on four OOD detection tasks: (a) one-class OOD detection; (b) multi-class detection; (c) near-distribution detection; and (d) few-shot outlier exposure OOD detection.
Many previous OOD detection approaches depend on outlier exposure [15, 53] to improve the performance of
OOD detection, which turns OOD detection into a simple binary classification problem. We claim that the core of
OOD detection is, instead, to learn the effective ID repre-sentation to discover OOD samples without any known out-lier exposure.
In this paper, we first present our surprising finding – that is, simply using reconstruction-based methods can notably boost the performance on various OOD detection tasks.
Our pioneer work along this line even outperforms previ-ous few-shot outlier exposure OOD detection, albeit we do not include any OOD samples.
Existing methods perform contrastive learning [53,58] or pretrain classification on a large dataset [15] to detect OOD samples. The former methods classify images according to the pseudo labels while the latter classifies images based on ground truth, whose core tasks are both to fulfill the classifi-cation target. However, research on backdoor attack [50,51] shows that when learning is represented by classifying data, networks tend to take a shortcut to classify images.
In a typical backdoor attack scene [51], the attacker adds secret triggers on original training images with the visibly correct label. During the course of testing, the victim model classifies images with secret triggers into the wrong cate-gory. Research in this area demonstrates that networks only learn specific distinguishable patterns of different categories because it is a shortcut to fulfill the classification require-ment.
Nonetheless, learning these patterns is ineffective for
OOD detection since the network does not understand the intrinsic data distribution of the ID images. Thus, learning representations by classifying ID data for OOD detection may not be satisfying. For example, when the patterns sim-ilar to some ID categories appear in OOD samples, the net-work could easily interpret these OOD samples as the ID data and classify them into the wrong ID categories.
To remedy this issue, we introduce the reconstruction-based pretext task. Different from contrastive learning in existing OOD detection approaches [53, 58], our method forces the network to achieve the training purpose of recon-structing the image and thus makes it learn pixel-level data distribution.
Specifically, we adopt the masked image modeling (MIM) [2, 11, 20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natu-ral language processing [11] and computer vision [2,20]. In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the cor-rupted input to the vision transformer. Then we use the tokens from discrete VAE [47] as labels to supervise the network during training. With its procedure, the network learns information from remaining patches to speculate the masked patches and restore tokens of the original image.
The reconstruction process enables the model to learn from the prior based on the intrinsic data distribution of images rather than just learning different patterns among categories in the classification process. it
In our extensive experiments, is noteworthy that masked image modeling for OOD detection (MOOD) out-performs the current SOTA on all four tasks of one-class OOD detection, multi-class OOD detection, near-distribution OOD detection, and even few-shot outlier ex-posure OOD detection, as shown in Fig. 1. A few statistics are the following. 1. For one-class OOD detection (Tab. 6), MOOD boosts the AUROC of current SOTA, i.e., CSI [58], by 5.7% to 94.9%. 2. For multi-class OOD detection (Tab. 7), MOOD out-performs current SOTA of SSD+ [53] by 3.0% and reaches 97.6%. 3. For near-distribution OOD detection (Tab. 2), AUROC of MOOD achieves 98.3%, which is 2.1% higher than the current SOTA of R50+ViT [15]. 4. For few-shot outlier exposure OOD detection (Tab. 9),
MOOD (99.41%) surprisingly defeats current SOTA of R50+ViT [15] (with 99.29%), which makes use of 10 OOD samples per class. It is notable that we do not even include any OOD samples in MOOD. 2.