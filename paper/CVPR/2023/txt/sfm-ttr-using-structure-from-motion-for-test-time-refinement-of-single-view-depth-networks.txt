Abstract
Estimating a dense depth map from a single view is ge-ometrically ill-posed, and state-of-the-art methods rely on learning depth’s relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very ac-curate but sparse maps, as matching across images is typi-cally limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth net-works at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outper-forming previous TTR baselines mainly based on photo-metric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR. 1.

Introduction
Obtaining accurate and dense depth maps from images is a challenging research problem and an essential input in a wide array of fields, like robotics [67], augmented reality [36], endoscopy [42], or autonomous driving [22].
Single-view per-pixel depth estimation is even more chal-lenging, as it is geometrically ill-posed in the general case.
However, in the last decade, intense research on deep mod-els applied to this task has produced impressive results, showing high promise for real-world applications.
Single-view depth learning was initially addressed as a supervised learning problem, in which deep networks were trained using large image collections annotated with ground truth depth from range (e.g., LiDAR) sensors [12, 30]. At present, this line of research keeps improving the accuracy
Figure 1. SfM-TTR overview. Our approach assumes an existing pre-trained depth network and an input sequence at test time. We estimate a SfM 3D reconstruction using the input sequence, and depth maps using a single-view depth network. We align the SfM point cloud with the network’s depth to obtain a pseudo-ground truth to refine the network encoder, improving its representation of the test scene and producing significantly more accurate depth estimates. of single-view depth estimates by better learning models and training methods, as illustrated for example by [6, 59].
In parallel to improving the learning side of the problem, several works are incorporating single- and multi-view ge-ometric concepts to depth learning, extending its reach to more general setups. For example, [3, 15] propose camera intrinsics-aware models, enabling learning and predicting depths for very different cameras. More importantly, many other works (e.g. [20]) use losses based on multi-view pho-tometric consistency, enabling self-supervised learning of depth and even camera intrinsics [21].
Incorporating single- and multi-view geometry into
depth learning naturally links the field to classic research on Structure from Motion (SfM) [23, 45], visual odome-try [14, 44] and visual SLAM [8, 9]. These methods typ-ically produce very accurate but sparse or semi-dense re-constructions of high-gradient points using only multi-view geometry at test time. Among the many opportunities for cross-fertilization of both fields (e.g., using depth net-works in visual SLAM [48] or SfM for training depth net-works [28, 32, 56]), our work focuses on using SfM for re-fining single-view depth networks at test time.
As single-view depth applications typically include a moving camera, several recent works incorporate multiple views at inference or refine single-view depth networks with multi-view consistency cues [4, 11, 36, 37, 46, 49, 52]. Most approaches, however, rely mainly on photometric losses, similar to the ones used for self-supervised training. These losses are limited to be computed between close views, creating weak geometric constraints. Our contribution in this paper is a novel method that, differently from the oth-ers in the literature, uses exclusively a SfM reconstruction for TTR. Although SfM supervision is sparser than typical photometric losses, it is also significantly less noisy as it has been estimated from wider baselines. Our results show that our approach, which we denote as SfM-TTR, provides state-of-the-art results for TTR, outperforming photomet-ric test-time refinement (Ph-TTR) for several state-of the-art supervised and self-supervised baselines. 2.