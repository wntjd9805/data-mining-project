Abstract
Different from conventional image matting, which either requires user-defined scribbles/trimap to extract a specific foreground object or directly extracts all the foreground ob-jects in the image indiscriminately, we introduce a new task named Referring Image Matting (RIM) in this paper, which aims to extract the meticulous alpha matte of the specific object that best matches the given natural language descrip-tion, thus enabling a more natural and simpler instruction for image matting. First, we establish a large-scale challeng-ing dataset RefMatte by designing a comprehensive image composition and expression generation engine to automat-ically produce high-quality images along with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we construct a real-world test set with 100 high-resolution natural im-ages and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods.
Furthermore, we present a novel baseline method CLIPMat for RIM, including a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Exten-sive experiments on RefMatte in both keyword and expres-sion settings validate the superiority of CLIPMat over repre-sentative methods. We hope this work could provide novel insights into image matting and encourage more follow-up studies. The dataset, code and models are available at https://github.com/JizhiziLi/RIM. 1.

Introduction
Image matting refers to extracting the soft alpha matte of the foreground in natural images, which is beneficial for various downstream applications such as video confer-ences, advertisement production, and e-Commerce promo-tion [58]. Typical matting methods can be divided into two groups: 1) the methods based on auxiliary inputs, e.g., scrib-ble [17] and trimap [1,17], and 2) automatic matting methods
*Dr Jing Zhang and Ms Jizhizi Li were supported by Australian Research
Council Projects in part by FL170100117 and IH180100002. that can extract the foreground without any human interven-tion [19,44]. However, the former are not applicable for fully automatic scenarios, while the latter are limited to specific categories, e.g., human [2, 32, 57], animal [19], or the salient objects [40, 60]. It is still unexplored to carry out control-lable image matting on arbitrary objects based on language instructions, e.g., extracting the alpha matte of the specific object that best matches the given language description.
Recently, language-driven tasks such as referring expres-sion segmentation (RES) [55], referring image segmentation (RIS) [12, 25, 54], visual question answering (VQA) [8], and referring expression comprehension (REC) [31] have been widely studied. Great progress in these areas has been made based on many datasets like ReferIt [14], Google Ref-Exp [34], RefCOCO [56], VGPhraseCut [50], and Cops-Ref [3]. However, due to the limited resolution of avail-able datasets, visual grounding methods are restricted to the coarse segmentation level. Besides, most of the meth-ods [13, 30] neglect pixel-level text-visual alignment and cannot preserve sufficient details, making them difficult to be used in scenarios that require meticulous alpha mattes.
To fill this gap, we propose a new task named Referring
Image Matting (RIM), which refers to extracting the metic-ulous high-quality alpha matte of the specific foreground object that can best match the given natural language de-scription from the image. Different from the conventional matting methods, RIM is designed for controllable image matting that can perform a more natural and simpler instruc-tion to extract arbitrary objects. It is of practical significance in industrial application domains and opens up a new re-search direction To facilitate the study of RIM, we establish the first dataset RefMatte, which consists of 230 object categories, 47,500 images, and 118,749 expression-region entities together with the corresponding high-quality alpha mattes and 474,996 expressions. Specifically, to build up
RefMatte, we revisit a lot of prevalent public matting datasets like AM-2k [19], P3M-10k [18], AIM-500 [20], SIM [45] and manually label the category of each foreground object (a.k.a. entity) carefully. We also adopt multiple off-the-shelf deep learning models [27, 51] to generate various attributes for each entity, e.g., gender, age, and clothes type of human.
Figure 1. Some examples from our RefMatte test set (top) and the results of CLIPMat given keyword and expression inputs (bottom).
Figure 2. Some examples from our RefMatte-RW100 test set (top) and the results of CLIPMat given expression inputs (bottom), which also show CLIPMat’s robustness to preserved privacy information.
Then, we design a comprehensive composition and expres-sion generation engine to produce the synthetic images with reasonable absolute and relative positions considering other entities. Finally, we present several expression logic forms to generate varying language descriptions with the use of rich visual attributes. In addition, we propose a real-world test set RefMatte-RW100 with 100 images containing diverse objects and human-annotated expressions, which is used to evaluate the generalization ability of RIM methods. Some examples are shown in Figure 1 and Figure 2.
Since previous visual grounding methods are designed for the segmentation-level tasks, directly applying them [13, 30, 43] to the RIM task cannot produce promising alpha mattes with fine details. Here, we present CLIPMat, a novel baseline method specifically designed for RIM. CLIPMat utilizes the large-scale pre-trained CLIP [41] model as the text and visual backbones, and the typical matting branches [18, 19] as the decoders. An intuitive context-embedded prompt is adopted to provide matting-related learnable features for the text en-coder. To extract high-level visual semantic information for the semantic branch, we pop up the visual semantic feature through the guidance of the text output feature. Additionally, as RIM requires much more visual details compared to the segmentation task, we devise a module to extract multi-level details by exploiting shallow-layer features and the original input image, aiming to preserve the foreground details in the matting branch. Figure 1 and Figure 2 show some promising results of the proposed CLIPMat given different types of language inputs, i.e., keywords and expressions.
Furthermore, to provide a fair and comprehensive evalua-tion of CLIPMat and relevant state-of-the-art methods, we conduct extensive experiments on RefMatte under two differ-ent settings, i.e., the keyword-based setting and expression-based setting, depending on language descriptions’ forms.
Both the subjective and objective results have validated the superiority of CLIPMat over representative methods. The main contribution of this study is three-fold. 1) We de-fine a new task named RIM, aiming to identify and extract the alpha matte of the specific foreground object that best matches the given natural language description. 2) We es-tablish the first large-scale dataset RefMatte, consisting of 47,500 images and 118,749 expression-region entities with high-quality alpha mattes and diverse expressions. 3) We present a novel baseline method CLIPMat specifically de-signed for RIM, which achieves promising results in two different settings of RefMatte, also on real-world images. 2.