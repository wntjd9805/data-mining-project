Abstract
Recently, visual-language navigation (VLN) – entailing robot agents to follow navigation instructions – has shown great advance. However, existing literature put most empha-sis on interpreting instructions into actions, only delivering
“dumb” wayfinding agents. In this article, we devise LANA, a language-capable navigation agent which is able to not only execute human-written navigation commands, but also provide route descriptions to humans. This is achieved by si-multaneously learning instruction following and generation with only one single model. More specifically, two encoders, respectively for route and language encoding, are built and shared by two decoders, respectively for action prediction and instruction generation, so as to exploit cross-task know-ledge and capture task-specific characteristics. Throughout pretraining and fine-tuning, both instruction following and generation are set as optimization objectives. We empirically verify that, compared with recent advanced task-specific solutions, LANA attains better performances on both in-struction following and route description, with nearly half complexity. In addition, endowed with language generation capability, LANA can explain to human its behaviours and as-sist human’s wayfinding. This work is expected to foster fu-ture efforts towards building more trustworthy and socially-intelligent navigation robots. 1.

Introduction
Developing agents that can interact with humans in natu-ral language while perceiving and taking actions in their en-vironments is one of the fundamental goals in artificial intel-ligence. As a small step towards this target, visual-language navigation (VLN) [4] – endowing agents to execute natural language navigation commands – recently received signifi-cant attention. In VLN space, much work has been done on language grounding – teaching agents how to relate human instructions with actions associated with perceptions. How-ever, there has been far little work [27, 70, 1, 77, 23] on the reverse side – language generation – teaching agents how to
*Corresponding author: Yi Yang.
Figure 1: LANA is capable of both instruction following and gen-eration. Its written report benefits human-robot collaboration, and, to some extend, can explain its behavior: it takes a wrong action at step 2 as it mistakes the dining room for bedroom. After gathering more information at step 3, it changes to the correct direction. verbalize a vivid description of navigation routes. More criti-cally, existing VLN literature separately train agents that are specialized for each single task. As a result, the delivered agents are either strong wayfinding actors but never talking, or conversable route instructors but never walking.
This article underlines a fundamental challenge in VLN:
Can we learn a single agent that is capable of both naviga-tion instruction following and route description creation?
We propose LANA, a language-capable navigation agent, that is fully aware of such challenge (Fig. 1). By simultane-ously learning instruction grounding and generation, LANA formalises human-to-robot and robot-to-human communi-cation, conveyed using navigation-oriented natural language, in a unified framework. This is of great importance, because: i) It completes the necessary communication cycle between human and agents, and promotes VLN agent’s real-world utility [58]. For instance, when an agent takes long time to execute a navigation command, during which sustained hu-man attention is infeasible and undesirable, the agent should report its progress [72]. Also, agents are expected to direct human in agents’ explored areas [81], which is relevant for search and rescue robots in disaster regions [71, 19], guide 1
robots in public spaces [77], and navigation devices for the visually impaired [36]. ii) Two-way communication is inte-gral to tight human-robot coordination (i.e., “I will continue this way · · · ”) [7], and boosts human trust in robot [6, 24], hence increasing the acceptance of navigation robots. iii) De-veloping the language generation skill makes for more ex-plainable robots, which can interpret their navigation beha-viors in a form of human-readable route descriptions.
Technically, LANA is built as a Transformer-based, multi-task learning framework. The network consists of two uni-modal encoders respectively for language and route encod-ing, and two multimodal decoders respectively for route-to-instruction and instruction-to-route translation, based on the two encoders. The whole network is end-to-end learned with the tasks of both instruction grounding and generation, dur-ing both pretraining and fine-tuning phases. Taken all these together, LANA provides a unified, powerful framework that explores both task-specific and cross-task knowledge at the heart of model design and network training. LANA thus can better comprehend linguistic cues (e.g., words, phrases, and sentences), visual perceptions, actions over long temporal horizons and their relationships, even in the absence of ex-plicit supervision, and eventually benefits both the two tasks.
We conduct extensive experiments on three famous VLN datasets (i.e., R2R [4], R4R [38], REVERIE [62]), for both instruction following and generation, giving a few intriguing points: First, LANA successfully solves the two tasks using only one single agent, without switching between different models. Second, with an elegant and integrated architecture,
LANA performs comparable, or even better than recent top-leading, task-specific alternatives. Third, compared to learn-ing each task individually, training LANA on the two tasks jointly obtains better performance with reduced complexity and model size, confirming the advantage of LANA in cross-task relatedness modeling and parameter efficiency. Forth,
LANA can explain to human its behavior by verbally describ-ing its navigation routes. LANA can be essentially viewed as an explainable VLN robot, equipped with a self-adaptively trained language explainer. Fifth, subjective analyses reveal our linguistic outputs are of higher quality than the baselines but still lag behind human-generated utterances. While there is still room for improvement, our results shed light on a pro-mising direction of future VLN research, with great poten-tial for explainable navigation agents and robot applications. 2.