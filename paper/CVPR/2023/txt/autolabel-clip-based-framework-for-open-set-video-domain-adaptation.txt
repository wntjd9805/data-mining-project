Abstract
Open-set Unsupervised Video Domain Adaptation (OU-VDA) deals with the task of adapting an action recognition model from a labelled source domain to an unlabelled tar-get domain that contains “target-private” categories, which are present in the target but absent in the source. In this work we deviate from the prior work of training a spe-cialized open-set classifier or weighted adversarial learn-ing by proposing to use pre-trained Language and Vision
Models (CLIP). The CLIP is well suited for OUVDA due to its rich representation and the zero-shot recognition ca-pabilities. However, rejecting target-private instances with the CLIP’s zero-shot protocol requires oracle knowledge about the target-private label names. To circumvent the impossibility of the knowledge of label names, we propose
AutoLabel that automatically discovers and generates object-centric compositional candidate target-private class names. Despite its simplicity, we show that CLIP when equipped with AutoLabel can satisfactorily reject the target-private instances, thereby facilitating better align-ment between the shared classes of the two domains. The code is available1. 1.

Introduction
Recognizing actions in video sequences is an important task in the field of computer vision, which finds a wide range of applications in human-robot interaction, sports, surveillance, and anomalous event detection, among others.
Due to its high importance in numerous practical applica-tions, action recognition has been heavily addressed using deep learning techniques [32, 44, 58]. Much of the success in action recognition have noticeably been achieved in the supervised learning regime [6, 17, 49], and more recently shown to be promising in the unsupervised regime [20, 34, 56] as well. As constructing large scale annotated and cu-rated action recognition datasets is both challenging and ex-pensive, focus has shifted towards adapting a model from a source domain, having a labelled source dataset, to an un-1https://github.com/gzaraunitn/autolabel labelled target domain of interest. However, due to the dis-crepancy (or domain shift) between the source and target domains, naive usage of a source trained model in the target domain leads to sub-optimal performance [51].
To counter the domain shift and and improve the trans-fer of knowledge from a labelled source dataset to an unla-belled target dataset, unsupervised video domain adaptation (UVDA) methods [7, 9, 42] have been proposed in the liter-ature. Most of the prior literature in UVDA are designed with the assumption that the label space in the source and target domain are identical. This is a very strict assumption, which can easily become void in practice, as the target do-main may contain samples from action categories that are not present in the source dataset [43].
In order to make
UVDA methods more useful for practical settings, open-set unsupervised video domain adaptation (OUVDA) meth-ods have recently been proposed [5, 8]. The main task in
OUVDA comprise in promoting the adaptation between the shared (or known) classes of the two domains by excluding the action categories that are exclusive to the target domain, also called as target-private (or unknown) classes. Exist-ing OUVDA prior arts either train a specialized open-set classifier [5] or weighted adversarial learning strategy [8] to exclude the target-private classes.
Contrarily, we address OUVDA by tapping into the very rich representations of the open-sourced foundation Lan-guage and Vision Models (LVMs).
In particular, we use
CLIP (Contrastive Language-Image Pre-training) [45], a foundation model that is trained on web-scale image-text pairs, as the core element of our framework. We argue that the LVMs (e.g., CLIP) naturally lend themselves well to
OUVDA setting due to: (i) the representation learned by
LVMs from webly supervised image-caption pairs comes encoded with an immense amount of prior about the real-world, which is (un)surprisingly beneficial in narrowing the shift in data distributions, even for video data; (ii) the zero-shot recognition capability of such models facilitates identi-fication and separation of the target-private classes from the shared ones, which in turn ensures better alignment between the known classes of the two domains.
(a) Zero-shot prediction using CLIP (b) Rejecting target-private instances with our proposed AutoLabel
Figure 1. Comparison of AutoLabel with CLIP [45] for zero-shot prediction on target-private instances. (a) CLIP assumes the knowledge about the oracle zero-shot classes names (ride horse, shoot ball); (b) Our proposed AutoLabel discovers automatically the candidate target-private classes (horse person, person ball) and extends the known classes label set
Zero-shot inference using CLIP requires multi-modal in-puts, i.e., a test video and a set of all possible prompts “A video of {label}”, where label is a class name, for computing the cosine similarity (see Fig. 1a). However in the OUVDA scenario, except the shared classes, a priori knowledge about the target-private classes label names are not available (the target dataset being unlabelled). Thus, ex-ploiting zero-shot capability of CLIP to identify the target-private instances in an unconstrained OUVDA scenario be-comes a bottleneck. To overcome this issue we propose
AutoLabel, an automatic labelling framework that con-structs a set of candidate target-private class names, which are then used by CLIP to potentially identify the target-private instances in a zero-shot manner.
In details, the goal of AutoLabel is to augment the set of shared class names (available from the source dataset) with a set of candidate target-private class names that best represent the true target-private class names in the tar-get dataset at hand (see Fig. 1b). To this end, we use an external pre-trained image captioning model ViLT [31] to extract a set of attribute names from every frame in a video sequence (see Sec. 3.2.1 for details). This is motivated by the fact that actions are often described by the constituent objects and actors in a video sequence.
As an example, a video with the prompt “A video of {chopping onion}” can be loosely described by the proxy prompt “A video of {knife}, {onion} and {arm}” crafted from the predicted attribute names. In other words, the attributes “knife”, “onion” and “arm” when presented to CLIP in a prompt can elicit similar re-sponse as the true action label “chopping onion”.
Naively expanding the label set using ViLT predicted at-tributes can introduce redundancy because: (i) ViLT pre-dicts attributes per frame and thus, there can be a lot of dis-tractor object attributes in a video sequence; and (ii) ViLT predicted attributes for the shared target instances will be duplicates of the true source action labels. Redundancy in the shared class names will lead to ambiguity in target-private instance rejection.
First, it uses unsupervised clustering (e.g., k-means [36]) on the target dataset to cluster the target samples, and then con-structs the top-k most frequently occurring attributes among the target samples that are assigned to each cluster. This step gets rid of the long-tailed set of attributes, which are inconsequential for predicting an action (see 3.2.2 for de-tails). Second, AutoLabel removes the duplicate sets of attributes that bear resemblance with the source class names (being the same shared underlying class) by using a set matching technique. At the end of this step, the effective label set comprises the shared class names and the candi-date sets of attribute names that represent the target-private class names (see Sec. 3.2.3 for details). Thus, AutoLabel unlocks the zero-shot potential of the CLIP, which is very beneficial in unconstrained OUVDA.
Finally, to transfer knowledge from the source to the tar-get dataset, we adopt conditional alignment using a sim-ple pseudo-labelling mechanism. In details, we provide to the CLIP-based encoder the target samples and the extended label set containing the shared and candidate target-private classes. Then we take the top-k pseudo-labelled samples for each predicted class and use them for optimizing a super-vised loss (see Sec. 3.2.4 for details). Unlike many open-set methods [5, 8] that reject all the target-private into a single unknown category, AutoLabel allows us to discriminate even among the target-private classes. Thus, the novelty of our AutoLabel lies not only in facilitating the rejection of target-private classes from the shared ones, but also opens doors to open world recognition [1].
In summary, our contributions are: (i) We demonstrate that the LVMs like CLIP can be harnessed to address OU-VDA, which can be excellent replacement to complicated alignment strategies; (ii) We propose AutoLabel, an au-tomatic labelling framework that discovers candidate target-private classes names in order to promote better separation of shared and target-private instances; and (iii) We conduct thorough experimental evaluation on multiple benchmarks and surpass the existing OUVDA state-of-the-art methods. 2.