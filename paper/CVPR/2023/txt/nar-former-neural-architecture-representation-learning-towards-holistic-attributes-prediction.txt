Abstract
With the wide and deep adoption of deep learning mod-els in real applications, there is an increasing need to model and learn the representations of the neural networks them-selves. These models can be used to estimate attributes of different neural network architectures such as the accu-racy and latency, without running the actual training or inference tasks.
In this paper, we propose a neural ar-chitecture representation model that can be used to esti-mate these attributes holistically. Specifically, we first pro-pose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fu-sion transformer to build a compact vector representation from the converted sequence. For efficient model train-ing, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less aug-mentation samples compared with previous random aug-mentation strategies. Experiment results on NAS-Bench-101, NAS-Bench-201, DARTS search space and NNLQP show that our proposed framework can be used to pre-dict the aforementioned latency and accuracy attributes of both cell architectures and whole deep neural networks, and achieves promising performance. Code is available at https://github.com/yuny220/NAR-Former. 1.

Introduction
As an ever increasing variety of deep neural network models are widely adopted in academic research and real applications, neural architecture representation is emerg-ing as an universal need to predict model attributes holis-tically. For example, modern neural architecture search (NAS) methods can depend on the neural architecture rep-1This work was done while Yun Yi was an intern at Intellifusion.
∗Corresponding authors. resentation to build good model accuracy predictors [5, 23, 24, 26, 31, 34], which estimate model accuracies without running the expensive training procedure. In order to find faster execution graphs when deploying models to neural network accelerators, neural network compilers [3,20] need it to build network latency predictors, which estimate the real time cost without running it on the corresponding real hardware. As a straightforward approach to solve these holistic prediction tasks, a neural architecture representa-tion model should be built to take the symbolic description of the network as input, and generate its numerical repre-sentation which can be easily used with existing modeling tools to perform the desired downstream tasks.
There are some neural architecture representation mod-els proposed in solving the individual tasks mentioned above, but we find they are rather application specific and have obvious drawbacks when used in new tasks. For ex-ample, the early MLP, LSTM based accuracy prediction approaches [6, 18, 24, 34] improve the efficiency and per-formance of NAS, while there is a limitation of predic-tion performance resulting from the the nonexistent or im-plicit topology encoding. Some later proposed GCN based methods [4, 17, 36] achieve better performance of accuracy prediction than the above methods. However, due to the use of adjacency matrix, the encoding dimension of these methods scales quadratically with the depth of the input ar-chitecture, making them difficult to model large networks.
NNLQP [20] implements latency prediction at the model level, but GNN-based encoding scheme makes it inadequate in modeling long range interactions between nodes.
Inspired by the progress in natural language understand-ing, our network uses a hand designed yet general and ex-tensible token encoding approach to encode the topology information and key parameters of the input neural net-work. Specifically, we design a generic real value tokenizer to encode neural network nodes’ operation type, location, and their inputs into vectors using the positional embed-ding approach that is widely used in transformers [33] and
NERF [25]. This fixed length node encoding scheme makes
the network encoding scale linearly with the size of the in-put network rather than quadratically as in models that rely on taking adjacency matrix as input.
Based on the tokenized input sequence, we design a transformer based model to further encode and fuse the net-work descriptions to generate a compact representation, e.g. a fixed length feature vector. Therefore, long range depen-dencies between nodes can be established by transformer.
Besides the conventional multi-head self attention based transformers, we propose to use a multi-stage fusion trans-former structure at the deep stages of the model to further fuse and refine the representation in a cascade manner.
For the network representation learning task, we also propose a data augmentation method called information flow consistency augmentation. The augmentation per-mutes the encoding order of nodes in the original network under our specified conditions without changing the struc-ture of the network. We find it empirically improves the performance of our method for downstream tasks.
The main contributions of this paper can be summarized as the following points: 1. We propose a simple and effective neural network en-coding approach which tokenizes both operation and topology information of a neural network node into a sequence. When it is used to encode the entire net-work, the tokenized network encoding scheme scales better than adjacency matrix based ones, and builds the foundation for seamlessly using transformer structures for network representation learning. 2. We design a multi-stage fusion transformer to learn feature representations. Benefiting from the proposed tokenizer, a concise pure transformer based neural ar-chitecture representation learning framework (NAR-Former) is proposed for the first time. Our NAR-Former makes full use of the capacity of transformers in handling sequence inputs, and gets promising per-formance. 3. To facilitate efficient model training, we propose an information flow consistency augmentation and cor-respondingly design an architecture consistency loss, which brings more benefits with less augmentation samples compared with the existing random augmen-tation strategy.
We conduct extensive experiments on accuracy predic-tion, neural architecture search as well as latency prediction.
Experiments demonstrate the effectiveness of the proposed representation model on processing both cell leveled net-work components and whole deep neural networks. Specif-ically, our accuracy predictor based on this representation achieves highly competitive accuracy performance on cell-based structures in NAS-Bench-101 [40] and NAS-Bench-201 [10] datasets. Compared with other predictor-based
NAS methods [23, 26], we efficiently find the architecture with 97.52% accuracy in DARTS [19] by only querying 100 neural architectures. We also conduct latency predic-tion experiments on neural networks deeper than 200 layers to demonstrate the universality of our model. 2.