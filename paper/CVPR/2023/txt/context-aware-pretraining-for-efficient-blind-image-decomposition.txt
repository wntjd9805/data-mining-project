Abstract
In this paper, we study Blind Image Decomposition (BID), which is to uniformly remove multiple types of degra-dation at once without foreknowing the noise type. There remain two practical challenges: (1) Existing methods typ-ically require massive data supervision, making them in-feasible to real-world scenarios. (2) The conventional paradigm usually focuses on mining the abnormal pat-tern of a superimposed image to separate the noise, which de facto conflicts with the primary image restoration task.
Therefore, such a pipeline compromises repairing efficiency
In an attempt to solve the two chal-and authenticity. lenges in one go, we propose an efficient and simplified paradigm, called Context-aware Pretraining (CP), with two pretext tasks: mixed image separation and masked im-age reconstruction. Such a paradigm reduces the annota-tion demands and explicitly facilitates context-aware fea-ture learning. Assuming the restoration process follows a structure-to-texture manner, we also introduce a Context-aware Pretrained network (CPNet). In particular, CPNet contains two transformer-based parallel encoders, one in-formation fusion module, and one multi-head prediction module. The information fusion module explicitly utilizes the mutual correlation in the spatial-channel dimension, while the multi-head prediction module facilitates texture-guided appearance flow. Moreover, a new sampling loss along with an attribute label constraint is also deployed to make use of the spatial context, leading to high-fidelity im-age restoration. Extensive experiments on both real and synthetic benchmarks show that our method achieves com-petitive performance for various BID tasks. 1.

Introduction
Different from traditional image restoration, Blind Im-age Decomposition (BID) aims to remove arbitrary degra-∗ Work done during an internship at Baidu.
† Corresponding author: Yi Yang.
Figure 1. The prototype of restoration frameworks: (a) Traditional methods [61,62] require task-specific network design and separate training. (b) All-in-one [28] relies on tedious one-by-one training of multiple heads. (c) TransWeather [49] is ad-hoc to remove one specific noise at a time. (d) IPT [4] extends (c) with a reusable pretrained middle-level transformer, which only works on specific tasks. (e) BIDeN [17] returns to the complex multiple decoders and demands dense supervision from noise labels. (f) The pro-posed method studies removing the general noise combinations by harnessing the prior knowledge learned during pretraining, which largely simplifies the pipeline. (Please zoom in to see the details.) dation combinations without knowing noise type and mix-ing mechanism. This task is challenging due to the huge gap between different noises and varying mixing patterns as amalgamated noises increase. Although many exist-ing methods [30, 54, 61, 62] have been proposed as generic restoration networks, they are still fine-tuned on the indi-vidual datasets and do not use a single general model for all the noise removal tasks (Figure 1 (a)). All-in-one [28] fur-ther proposes a unified model across 3 datasets, but it still uses computationally complex separate encoders (Figure 1 (b)). To ameliorate this issue, TransWeather [49] introduces a single encoder-single decoder transformer network for multi-type adverse weather removal, yet this method is de-signed to restore one specific degradation at one time (Fig-ure 1 (c)), which does not conform to the BID setting.
designed as a dual-branch pattern combining mixed image separation and masked image reconstruction. Our intuition underlying the proposed task is encouraging the network to mine context information (i.e., noise boundaries and types, local and non-local semantics), and such knowledge can be easily transferred to various restoration scenarios. We also develop a pretrained model for BID using the trans-former architecture, namely, Context-aware Pretrained Net-work (CPNet). In contrast to previous methods, the pro-posed CPNet can (1) remove arbitrary types or combina-tions of noises at once, (2) avoid multi-head mask super-vision of each source component (Figure 1 (f)), and (3) be efficiently employed for high-fidelity restoration after fine-tuning, as shown in Figure 2. To our knowledge, this work provides the first framework to apply a self-supervised pre-training learning strategy for BID task. Meanwhile, these two branches are hierarchically connected with an informa-tion fusion module that explicitly facilitates the feature in-teraction through multi-scale self-attention. Furthermore, we empirically subdue the learning difficulty by dividing the restoration process into structure reconstruction dur-ing pretraining and texture refinement during fine-tuning.
Instead of simply learning the mixed pattern and propor-tionally scaling the pixel values in previous methods, our method intuitively gives the model more “imagination” and thus leads to a more compelling and robust performance un-der complex scenes. Moreover, a novel flow sampling loss combing with a conditional attribute loss is further intro-duced for precise and faithful blind image decomposition.
Overall, our contributions are summarized as follows:
• Different from existing BID works, we introduce a new self-supervised learning paradigm, called Context-aware Pretraining (CP) with two pretext tasks: mixed image separation and masked image reconstruction.
To facilitate the feature learning, we also propose
Context-aware Pretrained Network (CPNet), which is benefited from the proposed information fusion mod-ule and multi-head prediction module for texture-guided appearance flow and conditional attribute label.
• Extensive experiments on BID benchmarks substanti-ate that our method achieves competitive performance for blind image restoration. More importantly, our method consistently outperforms competitors by large margins in terms of efficiency, e.g., 3.4 × fewer FLOPs and 50 × faster inference time over BIDeN [17]. 2.