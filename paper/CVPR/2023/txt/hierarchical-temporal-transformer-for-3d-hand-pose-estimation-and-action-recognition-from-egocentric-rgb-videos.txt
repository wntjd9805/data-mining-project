Abstract
Understanding dynamic hand motions and actions from egocentric RGB videos is a fundamental yet challenging task due to self-occlusion and ambiguity. To address occlu-sion and ambiguity, we develop a transformer-based frame-work to exploit temporal information for robust estimation.
Noticing the different temporal granularity of and the se-mantic correlation between hand pose estimation and ac-tion recognition, we build a network hierarchy with two cascaded transformer encoders, where the first one exploits the short-term temporal cue for hand pose estimation, and the latter aggregates per-frame pose and object informa-tion over a longer time span to recognize the action. Our approach achieves competitive results on two first-person hand action benchmarks, namely FPHA and H2O. Exten-sive ablation studies verify our design choices. 1.

Introduction
Perceiving dynamic interacting human hands is funda-mental in fields such as human-robot collaboration, imita-tion learning, and VR/AR applications. Viewing through the egocentric RGB video is especially challenging, as there are frequent self-occlusions between hands and objects, as well as severe ambiguity of action types judged from in-dividual frames (e.g. see Fig. 1 where the actions of pour milk and place milk can only be discerned at complete se-quences).
Recent years have witnessed tremendous improvement in 3D hand pose estimation and action recognition. While many works focus on only one of these tasks [10, 12, 13, 18, 23,39,51,56], unified frameworks [27,47,52] have also been proposed to address both tasks simultaneously, based on the critical observation that the temporal context of hand poses
Work is partially done during the internship of Y. Wen with Microsoft
Research Asia. Code and data are available at https://github.com/ fylwen/HTT.
Figure 1. Image sequences for pour milk and place milk under ego-centric view from H2O [27], with frequently occluded hand joints and ambiguous action type judged by individual frames. Using temporal information can benefit both tasks of 3D hand pose esti-mation and action recognition. helps resolve action ambiguity, implemented via models like LSTM, graph convolutional network or temporal con-volutional network. However, we note that temporal infor-mation can also benefit hand pose estimation: while inter-acting hands are usually under partial occlusion and trunca-tion, especially in the egocentric view, they can be inferred more reliably from neighboring frames with different views by temporal motion continuity.
Indeed, this idea has not been fully utilized yet among the existing works [27,47,52]: e.g., [27, 47] perform hand pose estimation at each frame, leaving the temporal dimension unexplored, and [52] jointly refines action and hand pose through hand-crafted multiple-order motion features and a complex iterative scheme.
We build a simple end-to-end trainable framework to ex-ploit the temporal dimension and achieve effective hand pose estimation and action recognition with a single feed-forward pass. To exploit the relationship among frames, we adopt the transformer architecture [48] which has demon-strated superior performance in sequence modeling. How-ever, action and pose have different temporal granularity: while the action is related to longer time spans lasting for several seconds, the hand pose depicts instantaneous mo-tions. Correspondingly, we use two transformer encoders with different window sizes to respectively leverage the short-term and long-term temporal cues for the per-frame
pose estimation and the action recognition of a whole se-quence. Moreover, we notice that the action has a higher semantic level, which is usually defined in the form of “verb
+ noun” [14, 27], where verb can be derived from the hand motion and the noun is the object being manipulated. We thus follow this pattern to build a hierarchy by cascading the pose and action blocks, where the pose block outputs the per-frame hand pose and object label, which are then aggregated by the action block for action recognition.
We evaluate our approach on FPHA [14] and H2O [27], and achieve state-of-the-art performances for 3D hand pose estimation and action recognition from egocentric RGB videos. Our contribution is summarized as follows:
• We propose a simple but efficient end-to-end trainable framework to leverage the temporal information for 3D hand pose estimation and action recognition from ego-centric RGB videos.
• We build a hierarchical temporal transformer with two cascaded blocks, to leverage different time spans for pose and action estimation, and model their semantic correlation by deriving the high-level action from the low-level hand motion and manipulated object label.
• We show state-of-the-art performance on two public datasets FPHA [14] and H2O [27]. 2.