Abstract
Deep Neural Networks (DNNs) have obtained impres-sive performance across tasks, however they still remain as black boxes, e.g., hard to theoretically analyze. At the same time, Polynomial Networks (PNs) have emerged as an alternative method with a promising performance and improved interpretability but have yet to reach the per-formance of the powerful DNN baselines.
In this work, we aim to close this performance gap. We introduce a class of PNs, which are able to reach the performance of
ResNet across a range of six benchmarks. We demonstrate that strong regularization is critical and conduct an exten-sive study of the exact regularization schemes required to match performance. To further motivate the regularization schemes, we introduce D-PolyNets that achieve a higher-degree of expansion than previously proposed polynomial networks. D-PolyNets are more parameter-efficient while achieving a similar performance as other polynomial net-works. We expect that our new models can lead to an understanding of the role of elementwise activation func-tions (which are no longer required for training PNs). The source code is available at https://github.com/ grigorisg9gr/regularized_polynomials.
Figure 1. The proposed networks (R-PolyNets, D-PolyNets) en-able polynomial networks to reach the performance of the power-ful neural networks across a range of tasks. 1.

Introduction
Deep neural networks (DNNs) are dominating the re-search agenda in computer vision since the previous decade owing to their stellar performance in image recognition [17, 24] and object detection [27, 28]. The design of tailored normalization schemes [21], data augmentation [11] and specific architectural blocks [19, 42] have further fostered this trend. However, our theoretical understanding of DNNs pales in comparison. There is little progress in making
DNNs interpretable, or a principled understanding of the training dynamics or the role of the network depth.
So far, a handful of works have attempted to mitigate that lack of understanding by designing principled archi-tectures. Combining neural networks with the research on kernel methods has emerged for designing principled archi-tectures with guarantees. In [30], the kernel feature map of the training data is used for achieving invariance to certain transformations. Recently, high-performing kernels were used for defining a principled architecture [40]. Using fixed components such as wavelets has been considered for re-placing the learnable convolutions [33]. Another approach approximates the target function with a polynomial expan-sion. Polynomial Nets (PNs) rely on capturing higher-order correlations of the input data for expressing the output with-out the use of elementwise activation functions [37]. De-spite the progress in the principled design of networks, the aforementioned works have yet to achieve a performance comparable to standard baselines, such as the performance of the seminal residual neural networks (ResNet) [17].
In this work, we aim to close the gap between well-established neural network architectures and principled ar-chitectures by focusing on the PNs. In particular, we con-centrate on the recent parametrization of Π-Nets [5] that has outperformed the aforementioned principled methods.
We validate our hypothesis that the performance of PNs can be significantly improved through strong regularization schemes. To this end, we introduce a class of polynomial networks, called R-PolyNets.
In our study, we explore which regularization schemes can improve the performance of PNs. For instance, we find that initializations proposed for neural networks [15, 36] are not optimal for PNs. Over-all, our exploration enables R-PolyNets to achieve perfor-mance on par with the (unregularized) ResNet, which is the de facto neural network baseline.
To further motivate our regularization schemes, we design a new class of polynomial expansions achieving a higher total degree of expansion than previous PNs.
In
R-PolyNets, the final degree of expansion is obtained by a sequential concatenation of a series of lower-degree polyno-mial expansions. That is, R-PolyNets concatenate N poly-nomials of second-degree to obtain a 2N polynomial expan-sion. Instead, we use outputs from previous polynomials in the current expansion, increasing the previous total degree.
Our goals are twofold: a) transfer representations from earlier polynomials, b) increase the total degree of poly-nomial expansion. The proposed regularization schemes are critical for training these dense polynomials, named
D-PolyNets. We showcase that D-PolyNets are more ex-pressive than previously proposed polynomial expansions.
Overall, our contributions can be summarized as follows:
• We introduce a class of regularized polynomial net-works, called R-PolyNets, in sec. 3.
• We propose densely connected polynomials, called D-PolyNets. D-PolyNets use multiple terms from a pre-vious polynomial as input to the current polynomial re-sulting in a higher-degree of expansion than previous
PNs (sec. 4).
• Our thorough validation in both image and audio recognition illustrates the critical components for achieving performance equivalent to vanilla DNNs. 2.