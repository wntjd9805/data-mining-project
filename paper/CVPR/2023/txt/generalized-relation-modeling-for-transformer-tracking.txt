Abstract
Compared with previous two-stream trackers, the recent one-stream tracking pipeline, which allows earlier interac-tion between the template and search region, has achieved a remarkable performance gain. However, existing one-stream trackers always let the template interact with all parts inside the search region throughout all the encoder layers. This could potentially lead to target-background confusion when the extracted feature representations are not sufficiently discriminative. To alleviate this issue, we propose a generalized relation modeling method based on adaptive token division. The proposed method is a generalized formulation of attention-based relation model-ing for Transformer tracking, which inherits the merits of both previous two-stream and one-stream pipelines whilst enabling more flexible relation modeling by selecting ap-propriate search tokens to interact with template tokens.
An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computa-tion and end-to-end learning of the token division module.
Extensive experiments show that our method is superior to the two-stream and one-stream pipelines and achieves state-of-the-art performance on six challenging bench-marks with a real-time running speed. Code and models are publicly available at https://github.com/Little-Podi/GRM. 1.

Introduction
Given the target bounding box in the initial frame of a video, visual tracking [18] aims to localize the target in successive frames. Over the past few years, two-stream trackers [1,21,22,49], which extract features of the template and search region separately and then model cross-relations of the template and search region in a sequential fash-ion, have emerged as a dominant tracking paradigm and made a significant progress. Following this two-stream pipeline, several Transformer-based trackers [4, 11, 38] utilize parallel self-attention blocks to enhance the ex-tracted features by modeling global self-relations within each image as illustrated in Fig. 1(a). Recently, leveraging
Figure 1. Comparison of different relation modeling pipelines of
Transformer-based trackers. The two-stream pipeline uses parallel self-attention blocks to model relations within each set of tokens (template or search tokens). The one-stream pipeline integrates the cross-relation modeling between two sets of tokens and self-relation modeling within each set of tokens via an unified attention block.
In contrast, our proposed pipeline performs an adaptive division of the search tokens, which can degenerate to the two-stream form if no search token is selected to interact with template tokens and to the one-stream form if all the search tokens are se-lected for cross-relation modeling. the flexibility of the attention mechanism, the one-stream pipeline [3, 5, 44] is proposed to jointly extract features and model relations, achieving promising performance. By conducting self-attention among all concatenated tokens, both cross-relation modeling and self-relation modeling can be performed simultaneously as illustrated in Fig. 1(b).
It is demonstrated in [3, 5, 39, 44] that letting the search region interact with the template as early as possible is ben-eficial to target-specific feature generation. However, there is no evidence suggesting that all parts inside the search re-gion should always be forced to interact with the template.
Actually, due to the cropping strategy [1], there is a large proportion of background inside the search region, where distractors with similar appearance to the target may exist.
This would lead to undesired cross-relations between the template and search region as the highly discriminative rep-resentations have not been extracted in some early layers.
Although the attention mechanism can inherently weaken improper cross-relations, applying global cross-relation modeling to all layers may still be more or less disruptive. On the one hand, for the search tokens outside the target region, if undesired cross-relations are modeled between the template and distractors, the aggregated fea-tures of the distractors may contain the target features from the template, which could cause confusion for precisely identifying the actual target in the search region. On the other hand, for the template tokens, their quality could also be degraded by undesired cross-relations during the iterative update since certain features from the background or even distractors could be aggregated into these tokens.
These situations could weaken the target-background discrimination capability of the one-stream pipeline.
Intuitively, only a portion of search tokens, e.g., tokens belonging to the target, are suitable for cross-relation mod-eling when the feature representations are not perfect for target-background discrimination. In some cases, the two-stream relation modeling pipeline could even be better if the feature representations of both the template and search re-gion are imperfect to model cross-relations. The potential limitations of the one-stream pipeline motivates us to pon-der: is it really optimal for the template to interact with all parts inside the search region through all encoder layers in the one-stream pipeline?
In this paper, we answer this question by proposing
GRM, a generalized relation modeling method that can adaptively select the appropriate search tokens to interact with the template. To be specific, we classify the template and search tokens as three categories. The template tokens form one category while the search tokens are divided into another two categories. Instead of modeling relations within all the tokens as the one-stream pipeline, we restrict the interaction among the three token categories. Only the search tokens that are suitable for cross-relation modeling will interact with the template tokens, whilst the interac-tion between the remaining search tokens and the template tokens is blocked. With proper divisions, the two-stream pipeline and one-stream pipeline become two degenerated forms of our relation modeling method as discussed in
Sec. 3.2. Consequently, our method is a generalized formu-lation of attention-based relation modeling for Transformer tracking, which embraces the advantages of both previous pipelines while being more flexible.
The search token division is performed by a lightweight prediction module, which can adaptively determine which search tokens are suitable for cross-relation modeling based on the input tokens. To accomplish this objective, there are two obstacles to overcome. First, the separate relation mod-eling for different token categories makes it hard for paral-lel computation. Second, the discrete token categorization is non-differentiable, thus impeding the end-to-end learning of the token division module. To facilitate parallel compu-tation, we adopt an attention masking strategy to unify the individual attention operations into a single one. Addition-ally, we introduce the Gumbel-softmax technique [17] to make the discrete token categorization differentiable. Con-sequently, the search token division module can be implic-itly optimized in an end-to-end manner, which promotes its adaptability to deal with different situations.
In summary, our main contributions are three-fold:
• We present a generalized formulation of relation mod-eling for Transformer trackers, which divides the input tokens into three categories and enables more flexible interaction between the template and search region.
• To realize the generalized relation modeling, we de-vise a token division module to adaptively classify the input tokens. An attention masking strategy and the
Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the proposed module.
• We conduct extensive experiments and analyses to val-idate the efficacy of our method. The proposed GRM exhibits outstanding results on six challenging visual tracking benchmarks. 2.