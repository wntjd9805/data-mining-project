Abstract
Vision Transformers have shown promising performance in image restoration, which usually conduct window- or channel-based attention to avoid intensive computations.
Although the promising performance has been achieved, they go against the biggest success factor of Transformers to a certain extent by capturing the local instead of global de-pendency among pixels. In this paper, we propose a novel efficient image restoration Transformer that first captures the superpixel-wise global dependency, and then transfers it into each pixel. Such a coarse-to-fine paradigm is imple-mented through two neural blocks, i.e., condensed attention neural block (CA) and dual adaptive neural block (DA). In brief, CA employs feature aggregation, attention computa-tion, and feature recovery to efficiently capture the global dependency at the superpixel level. To embrace the pixel-wise global dependency, DA takes a novel dual-way struc-ture to adaptively encapsulate the globality from superpix-els into pixels. Thanks to the two neural blocks, our method achieves comparable performance while taking only ∼6%
FLOPs compared with SwinIR. 1.

Introduction
Image restoration aims to recover the high-quality im-age from its degraded version, and huge success has been achieved by plentiful methods in the past years [22, 30, 35, 54–56, 60]. In the era of deep learning, Convolutional Neu-ral Networks (CNNs) have shown promising performance in image restoration [25, 52, 67] thanks to the inductive bi-ases of weight sharing and spatial locality [12]. However, although a number of studies have shown the effectiveness of CNNs, it has suffered from the following limitations [12], i.e., i) non-dynamic weights of CNNs limit the model ca-pacity of instance adaption, and ii) the sparse connections of CNNs limit the capture of global dependency.
*indicates equal contribution.
†Corresponding author.
Figure 1. Illustration of the dependency capture in existing vision
Transformers and ours. The red boxes refer to the dependency capture range of a given pixel marked by a red point. We gener-ally summarize the dependency capture in existing vision Trans-formers as the three ways above the dashed line. Obviously, they could only capture the dependency in a local range. In contrast, our method could obtain a pixel-wise global dependency through superpixel-wise dependency computation and distribution.
To overcome these limitations, some solutions [51, 60, 67, 68] have been specifically established, of which
Transformer-based methods [3, 5, 28, 47, 53] have achieved huge success thanks to their high capacities of dynamic weighting and global dependency capturing. Towards the image-restoration-specific Transformers, the biggest road-block might be the unacceptable cost caused by the global attention computation. Therefore, some efficient attentions have been proposed to trade-off the efficiency and depen-dency range, e.g., local window attention [49], shifted win-dow attention [22], and channel attention [56]. Although the promising performance has been achieved, these Trans-formers have still suffered from the following limitations.
First, the computation costs are still very high, thus limit-ing their applications in the mobile scenarios. Second, the attention mechanisms could only capture the dependency from a given range, and thus such a locality might not fully explore the potential of Transformer.
In practice, it is daunting to develop an efficient image restoration Transformer while directly capturing the global dependency, since it necessarily introduces intensive com-putations, which goes against efficiency. Different from ex-isting studies focusing on efficient attention mechanisms, this paper resolves the contradiction from a novel perspec-tive. To be specific, our basic idea is to adaptively aggregate the features at pixel level into a lower dimensional space of superpixel to remove the redundancy in channel [13] and space [50] domains. Through the feature aggregation, the dimension is remarkably reduced, and thus the attention could be computed in a global way with acceptable com-putation costs. After that, the feature recovery is performed to restore the feature distribution in channel and space do-mains. Although the above paradigm shows a feasible so-lution, it is far from the final goal since the obtained de-pendency actually works at the superpixel level. Hence, we need to transfer such a superpixel-wise global dependency to a pixel-wise global dependency. As a result, pixel-wise restoration can depend on the global information from su-perpixels.
Based on the above motivations, we propose an effi-cient Transformer for COmprehensive and DElicate image restoration (CODE). To be specific, CODE consists of a condensed attention neural block (CA) and a dual adap-tive neural block (DA). In brief, CA captures the global dependency at the superpixel level with acceptable compu-tations thanks to the aforementioned paradigm. To obtain the pixel-wise global dependency, DA extracts the global-ity from the CA output and then dynamically encapsulates it into each pixel. Thanks to the two complementary neural blocks, CODE could capture the pixel-wise global depen-dency, while embracing high computational efficiency.
The contributions and novelty of this work could be sum-marized as below.
• Unlike existing efficient Transformers only capture the pixel-wise local dependency, our solution could obtain the pixel-wise global dependency through superpixel-wise dependency computation and transformation.
• The proposed image restoration Transformer (CODE) consists of two neural blocks. In brief, CA employs feature aggregation, attention computation, and fea-ture recovery to efficiently capture the superpixel-wise global dependency. To obtain pixel-wise global depen-dency, DA takes a novel dual-way structure and a dy-namic weighting fashion to distribute the superpixel-wise globality into each pixel.
• Extensive experiments are conducted on four image restoration tasks to demonstrate the efficiency and ef-fectiveness of CODE, e.g., it achieves comparable per-formance while taking only ∼6% FLOPs compared with SwinIR [22]. 2.