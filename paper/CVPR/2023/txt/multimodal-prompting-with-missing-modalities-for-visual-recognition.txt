Abstract
In this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world sit-uations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and miti-gate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into mul-timodal transformers to handle general missing-modality cases, while only requiring less than 1% learnable param-eters compared to training the entire model. We further ex-plore the effect of different prompt configurations and an-alyze the robustness to missing modality. Extensive experi-ments are conducted to show the effectiveness of our prompt learning framework that improves the performance under various missing-modality cases, while alleviating the re-quirement of heavy model re-training. Code is available.1 1.

Introduction
Our observation perceived in daily life is typically mulit-modal, such as visual, linguistic, and acoustic signals, thus modeling and coordinating multimodal information is of great interest and has broad application potentials. Re-cently, multimodal transformers [13, 17, 22, 25, 35] emerge as the pre-trained backbone models in several multimodal downstream tasks, including genre classification [22], mul-timodal sentiment analysis [25, 35], and cross-modal re-trieval [13,15,17,30], etc. Though providing promising per-formance and generalization ability on various tasks, there are still challenges for multimodal transformers being ap-plied in practical scenarios: 1) how to efficiently adapt the multimodal transformers without using heavy computation resource to finetune the entire model? 2) how to ensure the robustness when there are missing modalities, e.g., incom-plete training data or observations in testing? 1https://github.com/YiLunLee/missing aware prompts
Illustration of missing-modality scenarios in training
Figure 1. multimodal transformers. Prior work [22] investigates the robust-ness of multimodal transformers to modality-incomplete test data, with the requirement to finetune the entire model using modality-complete training data. In contrast, our work studies a more gen-eral scenario where various modality-missing cases would occur differently not only for each data sample but also learning phases (training, testing, or both), and we adopt prompt learning to adapt the pre-trained transformer for downstream tasks without requir-ing heavy computations on finetuning the entire model.
Most multimodal transformer-based methods have a common assumption on the data completeness, which may not hold in practice due to the privacy, device, or security constraints. Thus, the performance may degrade when the data is modality-incomplete (regardless of training or test-ing). On the other hand, transformers pretrained on large-scale datasets are frequently adopted as backbone and fine-tuned for addressing various downstream tasks, thanks to the strong generalizability of transformers. However, as the model size of transformers increases (e.g., up to billions of parameters [5,26,27]), finetuning becomes significantly ex-pensive (e.g., up to millions of A100-GPU-hours [31]) and is even not feasible for practitioners due to the limited com-putation resources in most real-world applications. In addi-tion, finetuning a transformer on relatively small-scale tar-get datasets can result in restricted generalizability [9, 10] and stability [24], thus hindering it from being reused for
further learning with new data or in other tasks.
This motivates us to design a method that allows multi-modal transformers to alleviate these two real-world chal-lenges. One pioneer work [22] investigates the sensitiv-ity of vision-language transformers against the presence of modal-incomplete test data (i.e., either texts or images are missing). However, they only consider the case of miss-ing a specific modality for all the data samples, while in real-world scenarios the missing modality for each input data could not be known in advance. Moreover, [22] in-troduces additional task tokens to handle different missing-modal scenarios (e.g., text-only token when missing visual modality) and requires to optimize cross-modal features in the model. Hence finetuning the entire transformer becomes inevitable, leading to significant computation expense.
In this paper, we study multimodal transformers under a more general modality-incomplete scenario, where var-ious missing-modality cases may occur in any data sam-ples, e.g., there can be both text-only and image-only data during training or testing. In particular, we also focus on alleviating the requirement of finetuning the entire trans-formers. To this end, we propose a framework stemmed from prompt learning techniques for addressing the afore-mentioned challenges. Basically, prompt learning meth-ods [2,5,8,16,18,32,42] emerge recently as efficient and ef-fective solutions for adapting pre-trained transformers to the target domain via only training very few parameters (i.e., prompts), and achieve comparable performance with fine-tuning the whole heavy model. As motivated by [29] which shows that prompts are good indicators for different distri-butions of input, we propose to regard different situations of missing modalities as different types of input and adopt the learnable prompts to mitigate the performance drop caused by missing modality. As a result, the size of our learnable prompts can be less than 1% of the entire transformer, and thus the computation becomes more affordable compared to holistic finetuning. The key differences between our work and [22] are illustrated in Figure 1.
In order to further explore the prompt designs for multimodal transformers to tackle the general modality-incomplete scenario, we investigate two designs of inte-grating our missing-aware prompts2 into pre-trained mul-timodal transformers: 1) input-level, and 2) attention-level prompt learning. We find that, the location of attaching prompts to transformers is crucial for the studied missing-modality cases in this paper, which also aligns the findings in [36], though under a different problem setting.
We conduct experiments to explore different prompt configurations and have observations of the impact on the length and location of prompts: 1) As the number of prompting layers increases, the model performs better in-2In this paper, we use “missing-aware prompts” and “modality-missing-aware prompts” interchangeably. tuitively but it is not the most important factor; 2) Attach-ing prompts to the layers near the data input achieves better performance; 3) The prompts’ length has slight impact on model performance for attention-level prompts but may in-fluence input-level prompts more on certain datasets. More-over, we show extensive results to validate the effective-ness of adopting our prompting framework to alleviate the missing-modality issue under various cases, while reducing the learnable parameters to less than 1% compared to the entire model. Our main contributions are as follows:
• We introduce a general scenario for multimodal learn-ing, where the missing modality may occur differently for each data sample, either in training or testing phase.
• We propose to use missing-aware prompts to tackle the missing modality situations, while only requiring less than 1% parameters to adapt pre-trained models, thus avoiding finetuning heavy transformers.
• We further study two designs of attaching prompts onto different locations of a pretrained transformer, input-level and attention-level prompting, where the input-level prompting is generally a better choice but the attention-level one can be less sensitive to certain dataset settings. 2.