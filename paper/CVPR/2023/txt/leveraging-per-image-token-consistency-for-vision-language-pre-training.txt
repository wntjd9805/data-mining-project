Abstract
Most existing vision-language pre-training (VLP) ap-proaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose accord-ing to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Under-utilization of the unmasked tokens: CMLM pri-marily focuses on the masked token but it cannot simul-taneously leverage other tokens to learn vision-language associations.
To handle those limitations, we propose
EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e.,
Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsis-tent Token Generation Procedure), and then the model is re-quired to determine for each token in the sentence whether it is consistent with the image (i.e., Image-Token Consis-tency Task). The proposed EPIC method is easily com-bined with pre-training methods. Extensive experiments show that the combination of the EPIC method and state-of-the-art pre-training approaches, including ViLT, ALBEF,
METER, and X-VLM, leads to significant improvements on downstream tasks. Our coude is released at https:
//github.com/gyhdog99/epic 1.

Introduction
Vision-language pre-training (VLP) [5, 12, 21, 29, 30, 33, 37] aims to learn multi-modal representations from large-scale image-text pairs. A pre-trained vision-language model (VLM) fine-tuned with only a small amount of labeled data has shown state-of-the-art performance in many down-stream tasks such as visual question answering and image-text retrieval.
A primary concern in developing pre-training objectives for VLP models is how to learn better vision-language associations.
In addition to coarse-grained approaches such as image-text matching/contrasting [10, 14, 27] that align concepts from two modalities at the sample level, fine-grained approaches such as cross-modal masked lan-guage/image modeling (CMLM/CMIM) [16, 19, 32] learn vision-language associations at the token-object level. For example, Fig. 1 shows a picture paired with the sentence
“Blue and yellow hydrant on the grass”. When the word
“hydrant” is masked, in order to correctly recover the to-ken, the model has to find the actual object in the image and associate it with the word “hydrant”.
While effective, CMLM is insufficient for learning vision-language associations because of (1) modality bias; and (2) under-utilization of unmasked tokens.
In vision-language understanding, modality bias refers to leveraging only one modality for training/inference and so cross-modal knowledge is not well explored [23]. We argue that modal-ity bias exists in CMLM, and prevents the model from learn-ing sufficient vision-language associations. Specifically, in the CMLM task, we expect to mask salient1 tokens (such as “blue”, “yellow”, “fire-hydrant”, and “grass”) as shown in the left of Fig. 1. These tokens are informative for learn-ing vision-language association because masking them en-forces the model to find the answer from the visual modal-ity. However, in practice, whether a token is salient is un-known as we only have access to image-sentence level an-notations. Given a fixed and relatively small masking ra-tio (typically 15% in CMLM), we might end up masking tokens that are less informative. For example, as shown
* Work was done when the author interned at ByteDance AI Lab.
† The corresponding author. 1The definition of “saliency” is given in Sec. 4.4.
Figure 1. Illustrations of vision-language association learning. Ideal case: Fine-grained annotations (image regions and corresponding text tokens) are given, we can learn explicit associations (solid lines); CMLM: Without fine-grained annotations, we create supervision by masking, but this can be insufficient due to limited masking ratios and modality bias. EPIC: We find salient tokens and corrupt them to learn more associations. Both CMLM and EPIC learn implicit associations due to lack of region annotations. in Fig. 1 (center), when “the” and “and” are masked, the model can predict these masked tokens with only language information. This thus is a form of modality bias as it cir-cumvents using vision-language reasoning. Therefore, the modality bias can make CMLM insufficient to learn vision-language associations.
Another source of insufficiency in CMLM comes from
Similar to the under-utilization of unmasked tokens.
Masked Language Modeling (MLM) [6] in language pre-training, the CMLM loss is computed over masked tokens rather than all tokens in the sentence. As a result, learning of cross-modal association is possible only for the masked tokens but not for the remaining unmasked ones. For ex-ample, in Fig. 1, ideally, there are four associations (shown in black arrows) between text tokens and the correspond-ing regions, while there is only one association for CMLM.
Therefore, CMLM cannot leverage all tokens (including the unmasked ones) for learning vision-language associations.
To expedite the learning of cross-modal associations in VLP, we propose EPIC (lEveraging Per Image-Token
Consistency for vision-language pre-training). For each image-sentence pair, we mask tokens that are salient to the image (Saliency-based Masking Strategy) and make them
“inconsistent”2 with the image by replacing them with al-ternatives from a BERT-like language model (Inconsistent
Token Generation Procedure). The model is then required to determine whether each token in the sentence is consis-tent with the image (Image-Token Consistency (ITC) Task).
As this masks salient tokens and applies a language model to generate inconsistent tokens from them, the model has to refer to the visual modality to determine whether a token is inconsistent. Therefore, the modality bias problem can be alleviated. Moreover, we can make better use of the un-2A formal definition of (in)consistency tokens will be provided in Sec. 4.2. masked tokens for learning vision-language association as the ITC task requires the model to determine whether each token is consistent with the image.
The proposed EPIC method is easy to implement and widely applicable to a lot of vision-language model archi-tectures. We demonstrate the effectiveness of our approach on various pre-training approaches, including ViLT [21],
ALBEF [14], METER [8], and X-VLM [39], and observe significant improvements on downstream tasks. For exam-ple, on MSCOCO image-text retrieval, the proposed EPIC method achieves an absolute gain of 2.5% and 4.7% over
METER and ViLT, respectively, in terms of the Recall@1 score. On visual reasoning tasks (e.g., NLVR2), the pro-posed method improves over ALBEF by 1.8% and X-VLM (the state-of-the-art within its model scale) by 1.3%. The proposed method also allows better generalization of pre-training models. For example, in zero-shot image-text re-trieval, we improve X-VLM by 3.9% (COCO) and ViLT by 9.9% (Flickr30k). 2.