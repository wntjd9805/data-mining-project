Abstract
This paper, for the very first time, introduces human sketches to the landscape of XAI (Explainable Artificial In-telligence). We argue that sketch as a “human-centred” data form, represents a natural interface to study explainability.
We focus on cultivating sketch-specific explainability designs.
This starts by identifying strokes as a unique building block that offers a degree of flexibility in object construction and manipulation impossible in photos. Following this, we de-sign a simple explainability-friendly sketch encoder that accommodates the intrinsic properties of strokes: shape, lo-cation, and order. We then move on to define the first ever
XAI task for sketch, that of stroke location inversion (SLI).
Just as we have heat maps for photos, and correlation ma-trices for text, SLI offers an explainability angle to sketch in terms of asking a network how well it can recover stroke locations of an unseen sketch. We offer qualitative results for readers to interpret as snapshots of the SLI process in the pa-per, and as GIFs on the project page. A minor but interesting note is that thanks to its sketch-specific design, our sketch en-coder also yields the best sketch recognition accuracy to date while having the smallest number of parameters. The code is available at https://sketchxai.github.io. 1.

Introduction
It is very encouraging to witness a recent shift in the vision and language communities towards Explainable AI (XAI) [5,6,40,59,73,75,89]. In a world where “bag of visual words” becomes “bag of tricks”, it is critically important that we understand why and how AI is making the decisions, especially as they overtake humans on a series of tasks [21,
26, 52, 67].
XAI research to date has focused on two modalities: photo [15,38,49,88] and text [16,39,42,66,76]. Great strides have been made in the XAI for the photo domain, with the trend of going from heat/saliency maps [11, 64, 68, 70, 86] to the rules/semantics-oriented approaches [28,29,65]. The text side is captivating due to the flexibility of sentence construc-tion. Early works in text models explainability also started with visualisations [1, 68, 86], moving onto linguistic phe-nomena [8,39,80], and most recently to attention [20,61,71].
In this paper, we make a first attempt at XAI for human freehand sketches. The “why” we hope is obvious – sketches are produced by humans in the first place(!), from thou-sands of years ago in caves, and nowadays on phones and tablets. They are uniquely expressive, not only depicting an object/scene but also conveying stories – see a “Hunter and
Arrows” here for a story dating back 25,000 years in France1.
They, therefore, form an ideal basis for explainability which is also human-facing.
The sketch domain is uniquely different from both of the well-studied photo and text domains. Sketch differs from photo in that it can be freely manipulated, while photos are rigid and hard to manipulate. This is largely thanks to the stroke-oriented nature of sketches – jittering strokes might give the “same” sketch back, jittering pixels gives you a
“peculiar”-looking image. Sketches have the same level of flexibility in semantic construction as text: strokes are the building block for a sketch as words are for text. With these unique traits of sketch, the hope of this paper is to shed some light on what XAI might look for sketch data, and what it can offer as a result to the larger XAI community. This, however, is only the very first stab, the greater hope is to stir up the community and motivate follow-up works in this new direction of “human-centred” data for XAI.
With that in mind, we focus our exploration on what makes sketches unique – yes, strokes. They allow for flexible object construction and make sketches free to manipulate.
We then ask how strokes collectively form objects. For that, we identify three inherent properties associated with strokes: shape, location, and order. These three variables define a particular sketch: shape defines how each stroke looks like, location defines where they reside, and order encodes the temporal drawing sequence.
Our first contribution is a sketch encoder, that factors in all the mentioned essential properties of strokes. We hope that this encoder will build into its DNA how strokes (and in turn sketches) are represented, and therefore be more accommodating when it comes to different explainability tasks (now and in the future) – and for this, we name it
SketchXAINet (“X” for EXplainability). We are acute to the fact that explainability takes simple forms [48], so we refrained from designing a complicated network. In fact, we 1https://www.worldhistory.org/Lascaux_Cave/ did not go any further than introducing a branch to encode each of the three stroke properties (shape, location, and order), and simply feed these into a standard transformer architecture with a cross-entropy loss. Interestingly, however, just with this simple architecture, we already observe state-of-the-art sketch recognition performance improving on all prior arts.
With an explainability-compatible sketch encoder in place, we now want to examine if we can actually make anything explainable. First and foremost, of course, sketch explainability can be performed in the form of a heat map
[11, 64, 70] – just treat sketches as a raster image and we are done. This, however, would be entirely against our very hope of spelling out sketch-specific explainability – the “ex-plainability” one can obtain there is at best at the level of photo heatmaps (see Fig. 1).
Instead, we utilise our sketch encoder and put forward the first XAI task for sketch – that of stroke location inver-sion (SLI) (see Figs. 1 and 3). We study two types of tasks: recovery and transfer. Intuitively, during the recovery, we ask our optimisation procedure to jitter the stroke locations to recover sketch so that it belongs to the same class as the original sketch. During the transfer task, we ask our opti-misation procedure to jitter the stroke locations to obtain a sketch that belongs to a new class that we pass as input to the optimiser. The idea is then that how well the network has learned is positively correlated with how well it does at this inversion task, and that explainability lies in visual-ising this process. So, in addition to heat maps for photos, and correlation matrices for text, for sketch, we now have visualisations, that theoretically be manifested of infinite variety, and in the form of a video/GIF to capture the SLI process. We finish by playing with variants of the proposed
SLI: (i) sketch recovery, to offer insights on category-level understanding of a learned encoder, i.e., reconstructing a sketch to the same category, and (ii) sketch transfer, to shed light on cross-category understanding, i.e., using strokes of one category to reconstruct another.
Our contributions are as follows: (i) we argue for sketches to be introduced to the field of XAI, (ii) we identify strokes as the basic building block and build a sketch encoder, named as SketchXAINet, that encapsulates all unique sketch prop-erties, (iii) we introduce stroke location inversion (SLI) as a first XAI task for sketch, (iv) we offer qualitative results of the inversion process and deliver best sketch recognition performance as a by-product. 2.