Abstract
Can conversational videos captured from multiple egocen-tric viewpoints reveal the map of a scene in a cost-efficient way? We seek to answer this question by proposing a new problem: efficiently building the map of a previously un-seen 3D environment by exploiting shared information in the egocentric audio-visual observations of participants in a natural conversation. Our hypothesis is that as multi-ple people (“egos") move in a scene and talk among them-selves, they receive rich audio-visual cues that can help uncover the unseen areas of the scene. Given the high cost of continuously processing egocentric visual streams, we further explore how to actively coordinate the sampling of visual information, so as to minimize redundancy and re-duce power use. To that end, we present an audio-visual deep reinforcement learning approach that works with our shared scene mapper to selectively turn on the camera to ef-ficiently chart out the space. We evaluate the approach using a state-of-the-art audio-visual simulator for 3D scenes as well as real-world video. Our model outperforms previous state-of-the-art mapping methods, and achieves an excellent cost-accuracy tradeoff. Project: http://vision.cs. utexas.edu/projects/chat2map. 1.

Introduction
The spatial layout of the environment around us is fun-damental to understanding our physical context. By repre-senting the walls, furniture, and other major structures in a space, scene maps ground activity and objects in a persis-tent frame of reference, facilitating high-level reasoning for many downstream applications in augmented reality (AR) and robotics. For example, episodic memory [16, 28] aims to relocalize lost objects observed in first-person video (where are my keys?); floorplan estimation [10, 40, 47] aims to chart out the area and shapes of complex buildings; navigating agents try to discover routes in unfamiliar spaces [4, 11, 53].
While traditional computer vision approaches for map-ping (e.g., visual SLAM) are highly effective when extensive
*Equal contribution
Figure 1. Given egocentric audio-visual observations from multiple people wearing AR glasses and moving and conversing (left), we aim to accurately map the scene (right). To mitigate cost, our model receives audio continuously but learns to selectively employ the ego cameras only when the visual input is expected to be informative. exposure to the environment is possible, in many real-world scenarios only a fraction of the space is observed by the cam-era. Recent work shows the promise of sensing 3D spaces with both sight and sound [8, 13, 24, 26, 52]: listening to echoes bounce around the room can reveal the depth and shape of surrounding surfaces, and even help extrapolate a floorplan beyond the camera’s field of view or behind occluded objects [52].
While we are inspired by these advances, they also have certain limitations. Often systems will emit sounds (e.g., a frequency sweep) into the environment to ping for spatial information [1, 13, 14, 22, 26, 39, 52, 61], which is intrusive if done around people. Furthermore, existing audio-visual models assume that the camera is always on grabbing new frames, which is wasteful if not intractable, particularly on lightweight, low-power computing devices in AR settings.
We introduce Chat2Map, a new scene mapping task aimed at eliminating these challenges. In the proposed set-ting, multiple people converse as they move casually through
the scene while wearing AR glasses equipped with an ego-centric camera, microphones, and potentially other sensors (e.g., for odometry).1 Given their egocentric audio-visual data streams, the goal is to infer the ground-plane occupancy map for the larger environment around them. See Figure 1.
We observe that audio-visual data from the egos’ inter-actions will naturally reflect scene structure. First, as they walk and talk, their movements reveal spaces like corridors, doorways, and large rooms, in both modalities. Second, the speech captured by the device-wearer’s cameras and microphones can be localized to different speakers, which, compared to active sound emission, is non-intrusive.
To realize this vision, we develop a novel approach to efficient scene mapping from multi-ego conversations. Our approach has two key elements: a shared scene mapper and a visual sampling policy. For the former, we devise a transformer-based mapper that incorporates the multiple data streams to infer a map beyond the directly observed areas, and, most importantly, that enables communication among the egos about their observations and states in the 3D space to improve mapping accuracy. For the latter, our idea is to relax the common assumption of an “always-on" camera, and instead actively select when to sample visual frames from any one of the ego cameras. Intuitively, certain regions where the egos move will be more or less important for mapping (e.g., corners of the room, doors). We train a sampling policy with deep reinforcement learning that activates the visual feed only when it is anticipated to complement the continuous audio feed. This is a cost-conscious approach, mindful that switching on a camera is much more power consuming than sensing audio with microphones [2, 37].
We demonstrate our approach using a state-of-the-art audio-visual simulator for 3D scenes [8] as well as real-world video input. We can successfully map an unfamiliar envi-ronment given only partial visibility via multiple conversing people moving about the scene. Compared to sampling all visual frames, our model reduces the visual capture and pro-cessing by 87.5% while the mapping accuracy declines only marginally (∼ 9%).
Our main contributions are 1) we define the task of effi-cient and shared scene mapping from multi-ego conversa-tions, a new direction for AR/VR research; 2) we present the first approach to begin tackling this task, specifically an
RL-based framework that integrates an audio-visual scene mapper and a smart visual sampling policy; and 3) we rig-orously experiment with a variety of environments (both in simulation and the real world), speech sounds, and use cases. 2.