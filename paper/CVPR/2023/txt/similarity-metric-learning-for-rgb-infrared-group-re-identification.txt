Abstract
Group re-identification (G-ReID) aims to re-identify a group of people that is observed from non-overlapping cam-era systems. The existing literature has mainly addressed
RGB-based problems, but RGB-infrared (RGB-IR) cross-modality matching problem has not been studied yet. In this paper, we propose a metric learning method Closest Per-mutation Matching (CPM) for RGB-IR G-ReID. We model each group as a set of single-person features which are ex-tracted by MPANet, then we propose the metric Closest Per-mutation Distance (CPD) to measure the similarity between two sets of features. CPD is invariant with order changes of group members so that it solves the layout change prob-lem in G-ReID. Furthermore, we introduce the problem of G-ReID without person labels. In the weak-supervised case, we design the Relation-aware Module (RAM) that ex-ploits visual context and relations among group members to produce a modality-invariant order of features in each group, with which group member features within a set can be sorted to form a robust group representation against modality change. To support the study on RGB-IR G-ReID, we construct a new large-scale RGB-IR G-ReID dataset
CM-Group. The dataset contains 15,440 RGB images and 15,506 infrared images of 427 groups and 1,013 identi-ties. Extensive experiments on the new dataset demonstrate the effectiveness of the proposed models and the complex-ity of CM-Group. The code and dataset are available at: https://github.com/WhollyOat/CM-Group. 1.

Introduction
Group re-identification (G-ReID) is the problem of as-sociating a group of people that appears in disjoint cam-era views. The significant importance in video surveillance
*Corresponding Author (a) (b)
Figure 1. The members within a group are independent in ap-pearance and the visual similarity of two people does not indicate whether they are in a group. In Figure 1b , images in the same column are from the same person. Images with the same color are from the same group. has yield increasing attention and research efforts by the community [28, 30, 36]. Compared to single-person re-identification (ReID) which deals with a single person, gen-erally G-ReID regards 2 to 6 people as a group and treats two groups with at least 60% the same individuals as the same group. Hence, the main challenge of G-ReID is to construct robust representations of groups with appearance changes and group topology changes.
The existing works based on current G-ReID datasets have made impressive strides, but there still remain sev-eral important issues need to be resolved. First, available datasets for G-ReID are limited in different aspects. For ex-ample, the extant largest dataset for G-ReID City1M [36] is synthesized and has huge domain gap between real im-ages. The commonly used real datasets such as Road
Group [28], DukeMTMC Group [28] and CSG [30] are lim-ited in amount of groups and images. Therefore, for real
applications, it is in need of a simulation of real scenarios which includes large amount of people and variable scenes.
Another challenge we notice is that although G-ReID tackles group matching problem, most deep-learning-based works rely on labels of individuals to train deep nets for fea-ture extraction [8,40]. The fully supervised training scheme requires very large amounts of labour resources to make an-notations, which is cost and time-consuming. This disad-vantage makes it very hard to construct large-scale dataset and impedes development of G-ReID.
To facilitate the study of G-ReID towards real-world applications, we first introduce the RGB-infrared cross-modality group re-identification (RGB-IR G-ReID) prob-lem. Since infrared mode is widely used by surveillance cameras at night, matching infrared images captured in dark scenes with RGB images captured in bright scenes has been an significant problem for ReID and G-ReID research. As shown in Figure 1, RGB images have a huge domain gap between infrared images. Meanwhile the appearances of group members are independent of each other. Therefore
RGB-IR G-ReID not only handles modality discrepancy but also faces challenges from group retrieval. When person labels are available, we propose the Closest Permutation
Matching (CPM) framework. We adopt a state-of-the-art
RGB-IR ReID method MPANet to train a person feature ex-tractor and model each group as a set of group member fea-tures. To measure the similarity of two groups, we calculate the Closest Permutation Distance (CPD) between two sets of extracted features. CPD is a new metric that represents the least distance of two sets of features under all permuta-tions. In the weak-supervised case without person labels, we do not know the identities of group members, which makes it hard to train a person feature extractor. So we propose a Relation-aware Module (RAM) to extract order of group members which is invariant to modality changes.
RAM calculates visual relations between individuals within a group to generate pseudo order and guide the network to learn intrinsic orderings within groups.
Furthermore, we have collected a new dataset called
Cross-Modality Group ReID (CM-Group) dataset. Com-pared to existing G-ReID datasets, CM-Group has several new features. 1) CM-Group contains 15,440 RGB im-ages, 15,506 infrared images, 427 groups and 1,013 per-sons, which is, to our best knowledge, the first RGB-IR cross-modality G-ReID dataset and the largest real-world
G-ReID dataset. 2) The raw videos are captured by 6 cam-eras at 6 different scenes over a time span of 6 months, including large variations of illumination and viewpoint, clothes changes and scale changes. 3) All images are orig-inal frames of raw videos, i.e. all background information is reserved, which enables researchers to mine useful infor-mation in background. More details of CM-Group will be discussed in Section 4.
The main contributions of this work include:
• We propose the Closest Permutation Matching (CPM) to find the best match of group images with the permutation-invariant metric Closest Permutation Dis-tance (CPD). The CPM is resistant to group layout changes and achieves excellent performances on CM-Group.
• We introduce the problem of G-ReID without person labels and propose the Relation-aware Module (RAM) to leverage mutual relations of group members. Our experiments show that RAM can extract a modality-invariant order of members in a group regardless of appearance and layout changes.
• We contribute a large-scale RGB-IR cross-modality G-ReID dataset CM-Group, which supports more com-prehensive study on G-ReID. 2.