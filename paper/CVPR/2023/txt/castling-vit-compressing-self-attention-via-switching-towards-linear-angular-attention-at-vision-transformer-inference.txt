Abstract
Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as com-pared to convolutional neural networks (CNNs), one rea-son is that ViTs’ attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs’ capabilities of cap-turing either global or local context.
In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called
Castling-ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling-ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two tech-niques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling-ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classifi-cation and 1.2 higher mAP on detection under comparable
FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here. 1.

Introduction
Vision Transformers (ViTs) have made significant progress in image classification, object detection, and many
Figure 1. Castling-ViT over SOTA baselines on (1) ImageNet [18] image classification and (2) COCO [36] object detection.
It is well recognized that the supe-other applications. rior performance achieved by ViTs is largely attributed to their self-attention modules that can better capture global context [20, 57, 64]. Nevertheless, ViTs’ powerful self-attention module comes at the cost of quadratic complex-ity with the number of input tokens, causing a major effi-ciency bottleneck to ViTs’ achievable runtime (i.e., infer-ence latency) [3, 8, 32, 58, 66, 69, 76]. To mitigate this is-sue, linear attention designs have been developed to alle-In viate the vanilla ViT attention’s quadratic complexity. particular, existing efforts can be categorized into two clus-ters: (1) ViTs with local attention by restricting the atten-tion window size [38, 53], sharing the attention queries [2], or representing the attention queries/keys with low rank ma-trices [58]; and (2) ViTs with kernel-based linear attention, which approximate the non-linearity softmax function by decomposing it into separate kernel embeddings. This en-ables a change in the matrix computation order for a re-duced computational complexity [5, 6, 14, 29, 37, 39, 43, 66].
*Equal contribution. † Work done while interning at Meta Research.
Despite their promise in alleviating ViTs’ complexity
and thus inference runtime, both the local and linear at-tention compromise ViTs’ performance due to the lack of capabilities to capture global or local context. To marry the best of both worlds, we advocate training ViTs with both (1) efficient but less powerful linear attention, i.e., without the high-order residuals in angular kernel expansion, and (2) powerful yet costly softmax-based masked attention. The latter helps approximate high-order residuals at the early training stage while being dropped during inference, based on an assumption that the remaining networks can gradu-ally learn the high-order components at the later training stage [67]. This concept resembles the “castling” move in chess when two pieces are moved at once. While it sounds promising, there are still two challenges to achieve this.
First, existing linear attention modules still underperform their vanilla softmax-based counterparts. Therefore, a bet-ter linear attention is crucial for the final performance. We find that angular kernels perform equally as softmax-based attentions in terms of similarity measurements. While they still suffer from a quadratic complexity, they can be divided into linear terms and high-order residuals. The challenge is how to construct ViTs with only the linear terms. Second, doing so would require that the trained ViTs merely rely on the linear terms towards the end of training, which would call for an approximation of the above high-order residuals.
The challenge is that how we can resort to costly but pow-erful modules to approximate high-order residuals during training but does not incur extra inference cost.
In this work, we develop techniques to tackle those chal-lenges, and make the following contributions:
• We propose a framework called Castling-ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attentions dur-ing ViT inference to save computational costs.
• We develop a new linear-angular attention leveraging angular kernels to close the accuracy gap between lin-ear attention and softmax-based attention. It expands angular kernels where linear terms are kept while com-plex high-order residuals are approximated.
• We use two parameterized modules to approximate the high-order residuals above: a depthwise convolu-tion and an auxiliary masked softmax-based attention, where the latter’s attention masks are regularized to gradually become zeros to avoid inference overhead.
• We conduct extensive experiments to validate the ef-fectiveness of the proposed Castling-ViT. Results on classification, detection, and segmentation tasks con-sistently demonstrate its superior performance (↑1.8% top-1 accuracy or ↑1.2 mAP) or efficiency (40% MACs savings) over state-of-the-art (SOTA) CNNs and ViTs. 2.