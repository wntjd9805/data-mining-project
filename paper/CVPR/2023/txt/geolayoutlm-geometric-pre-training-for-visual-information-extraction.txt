Abstract
Visual information extraction (VIE) plays an important role in Document Intelligence. Generally, it is divided into two tasks: semantic entity recognition (SER) and rela-tion extraction (RE). Recently, pre-trained models for doc-uments have achieved substantial progress in VIE, partic-ularly in SER. However, most of the existing models learn the geometric representation in an implicit way, which has been found insufficient for the RE task since geometric in-formation is especially crucial for RE. Moreover, we reveal another factor that limits the performance of RE lies in the objective gap between the pre-training phase and the fine-tuning phase for RE. To tackle these issues, we propose in this paper a multi-modal framework, named GeoLay-outLM, for VIE. GeoLayoutLM explicitly models the geo-metric relations in pre-training, which we call geometric pre-training. Geometric pre-training is achieved by three specially designed geometry-related pre-training tasks. Ad-ditionally, novel relation heads, which are pre-trained by the geometric pre-training tasks and fine-tuned for RE, are elaborately designed to enrich and enhance the feature rep-resentation. According to extensive experiments on stan-dard VIE benchmarks, GeoLayoutLM achieves highly com-petitive scores in the SER task and significantly outperforms the previous state-of-the-arts for RE (e.g., the F1 score of
RE on FUNSD is boosted from 80.35% to 89.45%) 1.
Figure 1. Incorrect relation predictions by the previous state-of-the-art model LayoutLMv3 [15]. (a) LayoutLMv3 tends to link two entities relying more on their semantics than the geometric layout, i.e., the entity “212-450-4785” is linked to “Fax Number” regardless of their relationship in layout. (b) LayoutLMv3 suc-cessfully predicts the link in the upper half part but misses the link below, although both links are similar in geometric layout. These two examples clearly show the importance of geometric infor-mation in relation extraction (RE).
LayoutLMv3
+ geometric constraint
Precision Recall 85.45 85.45 75.82 79.87
F1 80.35 82.57
Table 1. The RE performance improvement by introducing a sim-ple geometric restriction (on the FUNSD dataset). 1.

Introduction
Visual information extraction (VIE) is a critical part in
Document AI [3, 29, 47]. It has attracted more and more at-tention from both the academic and industrial community.
VIE involves semantic entity recognition (SER, a.k.a. en-tity labeling) and relation extraction (RE, a.k.a. entity link-ing) from visually-rich documents (VrDs) such as forms and receipts [3, 17, 22, 35, 39, 41, 45, 46]. Recent years have witnessed the great power of pre-trained multi-modal mod-els [1, 7, 8, 12, 15, 20–22, 30, 38, 40, 41, 43] in VIE tasks,
*Both authors contributed equally to this work. 1https://github.com/AlibabaResearch/AdvancedLiterateMachinery especially the SER task. Compared with SER, the RE task, which aims at predicting the relation between semantic en-tities in documents, has not been fully explored and remains a challenging problem [12, 22]. RE is essential to provide additional structural information closer to human compre-hension of the VrDs [45]. It makes the open-layout infor-mation extraction possible, e.g., for open-layout key-value linking and form-like items grouping.
It is widely accepted that document layout understand-ing is crucial for VIE [1, 7, 8, 15, 21, 22, 30, 38, 40, 41, 43], especially for RE [12, 22]. The geometric relationships, a specific form for describing document layout, are impor-tant for document layout representations [22, 27, 31]. Most
previous pre-trained models for VrDs learn layout represen-tations implicitly by adding coordinates into the model in-puts, combining the relative position encoding or supervis-ing by alignment-related pre-training tasks like text-image alignment [15, 30, 43] and masked vision language model-ing [1,7,12,15,21,22,22,40,41,43]. However, it is not guar-anteed that the geometric layout information is well learned in these models. Taking the state-of-the-art model Lay-outLMv3 as an example, we find it would make mistakes in certain relatively simple scenarios, where the geometric relations between entities are not complicated. As shown in
Fig. 1, LayoutLMv3 seems to link two entities depending more on the semantics than the geometric layout. This in-dicates that its layout understanding is not sufficiently dis-criminative. To further verify our conjecture, we conduct an experiment by filtering the false positive relations using a simple geometric restriction (the linkings between entities should not point up beyond a certain distance), the precision would increase by a large margin (more than 4 points) while the recall is controlled unchanged, as detailed in Tab. 1.
This experiment proves that LayoutLMv3 does not fully exploit the useful geometric relationship information. Be-sides, most existing methods did not directly take the rela-tion modeling into consideration in pre-training. They usu-ally adopt token/segment-level classification or regression, which might underperform on downstream tasks related to relation modeling. Therefore, it is necessary to learn a bet-ter layout representation for document pre-trained models by modeling the geometric relationships between entities explicitly during pre-training.
During RE fine-tuning, previous works usually learn a task head like a single linear or bilinear layer [12, 22] from scratch. On the one hand, since the higher-level pair re-lationship features, which are beyond the token or text-segment features in documents, are complex, we argue that a single linear or bilinear layer is not always adequate to make full use of the encoded features for RE. On the other hand, the RE task head initialized randomly is prone to overfitting with limited fine-tuning data. Since the pre-trained backbone has shown tremendous potential [4, 5], why not pre-train the task head in some way simultane-ously? Several works [10, 14, 26] have proved that smaller gap between pre-training and fine-tuning leads to better per-formance for downstream tasks. Hence, there is still consid-erable room for the design and usage of the RE task head.
Based on the above observations, we establish a multi-modal pre-trained framework (termed as GeoLayoutLM) for VIE, in which a geometric pre-training strategy is de-signed to explicitly utilize the geometric relationships be-tween text-segments, and elaborately-designed RE heads are introduced to mitigate the gap between pre-training and fine-tuning on the downstream relation extraction task.
Specifically, three geometric relations are defined: the re-lation between two text-segments (GeoPair), that among multiple text-segment pairs (GeoMPair), and that among three text-segments (GeoTriplet). Correspondingly, three self-supervised pre-training tasks are proposed. GeoPair re-lation is modeled by the Direction and Distance Modeling (DDM) task in which GeoLayoutLM needs to tell the di-rection of a directed pair and identify whether a segment is the nearest to another one in the direction. Furthermore, we design a brand-new pre-training objective called Detection of Direction Exceptions (DDE) for GeoMPair, enabling our model to capture the common pattern of directions among segment pairs, enhance the pair feature representation and discover the detached ones. For GeoTriplet, we propose a Collinearity Identification of Triplet (CIT) task to iden-tify whether three segments are collinear, which takes a step forward to the modeling of multi-segments relations. It is important for non-local layout feature learning especially in form-like documents. Additionally, novel relation heads are proposed to learn better relation features, which are pre-trained by the geometric pre-training tasks to absorb prior knowledge about geometry, thus mitigating the gap between pre-training and fine-tuning. Extensive experiments on five public benchmarks demonstrate the effectiveness of the pro-posed GeoLayoutLM.
Our contributions are summarized as follows: 1) This paper introduces three geometric relations in dif-ferent levels and designs three brand-new geometric pre-training tasks correspondingly for learning the ge-ometric layout representation explicitly. To the best of our knowledge, GeoLayoutLM is the first to ex-plore the geometric relations of multi-pair and multi-segments in document pre-training. 2) Novel relation heads are proposed to benefit the re-lation modeling. Besides, the relation heads are pre-trained by the proposed geometric tasks and fine-tuned for RE, thus mitigating the object gap between pre-training and fine-tuning. 3) Experimental results on visual information extraction tasks including key-value linking as relation extrac-tion, entity grouping as relation extraction, and seman-tic entity recognition show that the proposed GeoLay-outLM significantly outperforms previous state-of-the-arts with good interpretability. Moreover, our model has notable advantages in few-shot RE learning. 2.