Abstract for
Masked Autoencoders (MAE) have been prevailing large-scale vision representation pre-paradigms training. By reconstructing masked image patches from a small portion of visible image regions, MAE forces the model to infer semantic correlation within an image. Re-cently, some approaches apply semantic-rich teacher mod-els to extract image features as the reconstruction target, leading to better performance. However, unlike the low-level features such as pixel values, we argue the features extracted by powerful teacher models already encode rich semantic correlation across regions in an intact image. This raises one question: is reconstruction necessary in Masked
Image Modeling (MIM) with a teacher model? In this paper, we propose an efficient MIM paradigm named MaskAlign.
MaskAlign simply learns the consistency of visible patch features extracted by the student model and intact image features extracted by the teacher model. To further ad-vance the performance and tackle the problem of input in-consistency between the student and teacher model, we pro-pose a Dynamic Alignment (DA) module to apply learn-able alignment. Our experimental results demonstrate that masked modeling does not lose effectiveness even with-out reconstruction on masked regions. Combined with
Dynamic Alignment, MaskAlign can achieve state-of-the-art performance with much higher efficiency. Code and models will be available at https://github.com/
OpenPerceptionX/maskalign. 1.

Introduction
In recent years, Vision Transformers are showing tremendous potential in computer vision area [8, 40, 41].
Following the big success of masked modeling in the nat-ural language processing [24], Masked Image Modeling
*This work was performed when Hongwei Xue was visiting Shanghai
AI Laboratory as a research intern.
†Corresponding authors.
Figure 1. Comparison with existing paradigms of masked im-(a) Inpainting-style: BEiT [3], MaskFeat [43], age modeling.
MVP [44], BEiT V2 [33], etc. They take the whole image with some mask token replacement as the input of Encoder. Then a
Linear head is applied to predict masked feature. (b) Decoder-style: MAE [13], CAE [5], MCMAE [11], etc. They drop most tokens and take the rest as the input of Encoder. Then a multi-layer
Transformer is applied to decode masked features from visible to-kens. (c) Ours.: Our paradigm take some visible tokens as the input of Encoder and align visible tokens with target features only. (MIM) has demonstrated a great ability of self-supervised learning [3, 13], while alleviating the data-hungry issue of Transformer architectures. The visual representation learned through MIM shows promising performance on various downstream vision tasks, outperforming the con-trastive learning paradigms [4, 6].
Existing Masked Image Modeling (MIM) methods aim to hallucinate the intact image from a small portion of vis-ible image regions. As depicted in Fig. 1, existing MIM methods are mainly divided into two types: (a) inpainting-style [3, 43, 46] and (b) decoder-style [5, 11, 13]. These two types both require the model to reconstruct masked regions.
The inpainting-style models replace image regions with learnable vectors then fill them by the interaction within the encoder. The decoder-style models drop image regions then decode features from masked regions’ positions based on
the visible information. Some very recent works introduce semantic-rich teacher models like CLIP [35] into the two paradigms by using features extracted by teacher models as the reconstruction target [17, 33, 34, 44]. In light of the se-mantic knowledge learned by teacher models, these works further improve the representation after masked image mod-eling, leading to better performance.
Reconstruction on masked regions implicitly forces the model’s encoder to understand the semantic correlations within an image. However, the reconstruction manner brings much computation on masked tokens within or out-side the encoder in inpainting-style or decoder-style, re-spectively. This redundant computation decreases the train-ing efficiency of the encoder thus increasing the pre-training cost. Unlike low-level and isolated features such as normal-ized pixel values of patches, Histogram of Oriented Gra-dients (HOG), etc., the feature map extracted by powerful teacher models already contains rich semantic correlations, learned during the teacher model training stage. This dif-ference raises one question: is reconstruction the only way in Masked Image Modeling (MIM) with teacher models?
To answer this question, we propose a much more efficient
MIM paradigm named MaskAlign without any reconstruc-tion on masked tokens.
On contrary of applying reconstruction on masked to-kens, MaskAlign simply aligns the visible features ex-tracted by the student model and intact image features ex-tracted by the teacher model. As a consequence, MaskAlign forces the student model to learn not only good representa-tion of the teacher model by feature alignment, but also the ability to hallucinate by masked modeling: feature consis-tency between the intact image and mask view requires the student model to infer semantics from much less informa-tion than teacher model. We adopt multi-level features of the teacher model as supervision to borrow richer seman-tics. However, the input of the student model contains much less information than the teacher model’s, leading to mis-alignment of each layer’s features. To tackle this problem, we enhance the student’s features with a Dynamic Align-ment (DA) module. DA dynamically aggregates different levels of student features and aligns with multi-level fea-tures of the teacher model. This approach can also easily transfer to asymmetric student-teacher structures.
From our experimental results, MaskAlign with a wide range of mask ratio outperforms the mask ratio of 0%, where it degenerates into Feature Distillation [45]. This verifies that masked modeling is still necessary for our paradigm. Meanwhile, our experiments validate the ef-fectiveness of Dynamic Alignment by comparing different alignment strategies and numbers of feature levels. The combination of masked modeling and Dynamic Alignment makes our model achieve state-of-the-art results with much higher efficiency. For example, our model outperforms
BEiT v2 [33] by 0.4% on ImageNet Finetuning Accuracy (from 85.0% to 85.4%) with 1/3 pre-training time only.
To sum up, our work has three-fold contributions: 1. We categorize and rethink existing Masked Image
Modeling (MIM) paradigms and propose a more ef-ficient MIM approach called MaskAlign. Even with-out any reconstruction on masked tokens, MaskAlign achieves new state-of-the-art performance with much higher efficiency. 2. We propose a Dynamic Alignment (DA) module to tackle the problem of input inconsistency between the student and teacher model, with negligible additional parameters and computation. 3. We conduct extensive experiments to verify the effec-tiveness of MaskAlign and Dynamic Alignment. Be-sides, our model shows a good ability of generalization on downstream tasks and larger size models. 2.