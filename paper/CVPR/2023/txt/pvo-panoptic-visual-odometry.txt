Abstract 1.

Introduction
We present PVO, a novel panoptic visual odometry frame-work to achieve more comprehensive modeling of the scene motion, geometry, and panoptic segmentation information.
Our PVO models visual odometry (VO) and video panop-tic segmentation (VPS) in a unified view, which makes the two tasks mutually beneficial. Specifically, we introduce a panoptic update module into the VO Module with the guidance of image panoptic segmentation. This Panoptic-Enhanced VO Module can alleviate the impact of dynamic objects in the camera pose estimation with a panoptic-aware dynamic mask. On the other hand, the VO-Enhanced VPS
Module also improves the segmentation accuracy by fusing the panoptic segmentation result of the current frame on the fly to the adjacent frames, using geometric information such as camera pose, depth, and optical flow obtained from the
VO Module. These two modules contribute to each other through recurrent iterative optimization. Extensive exper-iments demonstrate that PVO outperforms state-of-the-art methods in both visual odometry and video panoptic segmen-tation tasks.
∗ indicates equal contribution. † indicates the corresponding author.
Understanding the motion, geometry, and panoptic seg-mentation of the scene plays a crucial role in computer vision and robotics, with applications ranging from autonomous driving to augmented reality. In this work, we take a step to-ward solving this problem to achieve a more comprehensive modeling of the scene with monocular videos.
Two tasks have been proposed to address this problem, namely visual odometry (VO) and video panoptic segmen-tation (VPS). In particular, VO [9, 11, 36] takes monocular videos as input and estimates the camera poses under the static scene assumption. To handle dynamic objects in the scene, some dynamic SLAM systems [2, 43] use instance segmentation network [14] for segmentation and explicitly filter out certain classes of objects, which are potentially dynamic, such as pedestrians or vehicles. However, such ap-proaches ignore the fact that potentially dynamic objects can actually be stationary in the scene, such as a parked vehicle.
In contrast, VPS [17, 42, 49] focuses on tracking individual instances in the scene across video frames given some ini-tial panoptic segmentation results. Current VPS methods do not explicitly distinguish whether the object instance is moving or not. Although existing approaches broadly solve these two tasks independently, it is worth noticing that dy-namic objects in the scene can make both tasks challenging.
Recognizing this relevance between the two tasks, some methods [5, 7, 19, 21] try to tackle both tasks simultaneously and train motion-semantics networks in a multi-task man-ner, shown in Fig. 2. However, the loss functions used in these approaches may contradict each other, thus leading to performance drops.
In this work, we propose a novel panoptic visual odom-etry (PVO) framework that tightly couples these two tasks using a unified view to model the scene comprehensively.
Our insight is that VPS can adjust the weight of VO with panoptic segmentation information (the weights of the pixels of each instance should be correlated) and VO can convert the tracking and fusion of video panoptic segmentation from 2D to 3D. Inspired by the seminal Expectation-Maximization algorithm [26], recurrent iterative optimization strategy can make these two tasks mutually beneficial.
Our PVO consists of three modules, an image panoptic segmentation module, a Panoptic-Enhanced VO Module, and a VO-Enhanced VPS Module. Specifically, the panoptic segmentation module (see Sec. 3.1) takes in single images and outputs the image panoptic segmentation results, which are then fed into the Panoptic-Enhanced VO Module as ini-tialization. Note that although we choose PanopticFPN [20], any segmentation model can be used in the panoptic segmen-tation module. In the Panoptic-Enhanced VO Module (see
Sec. 3.2), we propose a panoptic update module to filter out the interference of dynamic objects and hence improve the accuracy of pose estimation in the dynamic scene. In the
VO-Enhanced VPS Module (see Sec. 3.3), we introduce an online fusion mechanism to align the multi-resolution fea-tures of the current frame to the adjacent frames based on the estimated pose, depth, and optical flow. This online fusion mechanism can effectively solve the problem of multiple object occlusion. Experiments show that the recurrent itera-tive optimization strategy improves the performance of both
VO and VPS. Overall, our contributions are summarized as four-fold.
• We present a novel Panoptic Visual Odometry (PVO) framework, which can unify VO and VPS tasks to model the scene comprehensively.
• A panoptic update module is introduced and incorpo-rated into the Panoptic-Enhanced VO Module to im-prove pose estimation.
• An online fusion mechanism is proposed in the VO-Enhanced VPS Module, which helps to improve video panoptic segmentation.
• Extensive experiments demonstrate that the proposed
PVO with recurrent iterative optimization is superior to state-of-the-art methods in both visual odometry and video panoptic segmentation tasks.
Figure 2. Illustration. Our PVO unifies visual odometry and video panoptic segmentation so that the two tasks can be mutually reinforced by iterative optimization. In contrast, methods such as
SimVODIS [19] optimize motion and semantic information in a multi-task manner. 2.