Abstract
Multimodal learning (MML) aims to jointly exploit the common priors of different modalities to compensate for their inherent limitations. However, existing MML meth-ods often optimize a uniform objective for different modal-ities, leading to the notorious “modality imbalance” prob-lem and counterproductive MML performance. To address the problem, some existing methods modulate the learning pace based on the fused modality, which is dominated by the better modality and eventually results in a limited improve-ment on the worse modal. To better exploit the features of multimodal, we propose Prototypical Modality Rebal-ance (PMR) to perform stimulation on the particular slow-learning modality without interference from other modali-ties. Specifically, we introduce the prototypes that represent general features for each class, to build the non-parametric classifiers for uni-modal performance evaluation. Then, we try to accelerate the slow-learning modality by enhancing its clustering toward prototypes. Furthermore, to alleviate the suppression from the dominant modality, we introduce a prototype-based entropy regularization term during the early training stage to prevent premature convergence. Be-sides, our method only relies on the representations of each modality and without restrictions from model structures and fusion methods, making it with great application potential for various scenarios. The source code is available here1. 1.

Introduction
Multimodal learning (MML) [24, 34, 36] emerges to mimic the way humans perceive the world, i.e., from multi-ple sense channels toward a common phenomenon for bet-ter understanding the external environment, which has at-tracted extensive attention in various scenarios, e.g. video classification [13, 28, 37], event localization [38, 43], ac-*Corresponding author 1https://github.com/fanyunfeng-bit/Modal-Imbalance-PMR
Figure 1. The slow-learning modal’s updating direction is severely disturbed by the dominant one, making it hard to exploit its fea-tures. We propose to use the prototypes, the centroids of each class in representation space, to adjust updating direction for better uni-modal performance. Other modalities will not interfere with the new direction, which ensures improvement. tion recognition [10, 33], audiovisual speech recognition
[23, 25]. By employing a complementary manner of mul-timodal training, it is expected that MML can achieve bet-ter performance than using a single modality. However, the heterogeneity of multimodal data poses challenges on how to learn multimodal correlations and complementarities.
According to recent research [29, 42], although the over-all performance of multimodal learning exceeds that of single-modal learning, the performance of each modality tends to be far from their upper bound. The reason behind this phenomenon is the “modality imbalance” problem, in which the dominant modality will hinder the full utiliza-tion of multimodal. The researchers [40] also claimed that different modalities overfit and converge at different rates, meaning that optimizing the same objective for different modalities leads to inconsistent learning efficiency.
Several methods [29, 40, 44] have been proposed to ad-dress the problem. Some of them [29, 44] try to modu-late the learning paces of different modalities based on the fusion modal. However, we find out through experiments that the dominant modality not only suppresses the learning rates of other modalities [29] but also interferes with their update direction, which makes it hard to improve the per-formance of slow-learning modalities. Moreover, existing methods either inevitably bring additional model structures
[5, 40] or are limited by specific fusion methods [29, 42], which limit their application scenarios.
To tackle their limitations, we propose the Prototypical
Modal Rebalance (PMR) strategy to stimulate the slow-learning modality via promoting the exploitation of features and alleviate the suppression from the dominant modality by slowing down itself in the early training stage.
Concretely, we introduce the prototypes for each modal-ity, which are defined as “representative embeddings of in-stances of a class”. We utilize the prototypes to construct non-parametric classifiers by comparing the distances be-tween each sample with all the prototypes to evaluate the performance of each modality and design a new prototype-based metric inspired by [29] to monitor the modality im-balance degree during the training process. Then, we pro-pose the prototypical cross-entropy (PCE) loss to acceler-ate the slow-learning modality by enhancing its clustering process, as illustrated in Fig. 1. The PCE loss can achieve comparable performance to the cross-entropy (CE) loss [1] in the classification task, and more importantly, it is not af-fected by the dominant modality and gives internal impetus for full feature exploitation instead of going on in the dis-turbed direction. In addition, we introduce a prototypical entropy regularization (PER) term, which can be seen as a penalty on the dominant modality to prevent premature convergence for suppression effect alleviation. Our method only relies on the representations of each modality and with-out restrictions from model structures and fusion methods.
Therefore, the PMR strategy has great generality potential.
To summarize, our contributions in this paper are as fol-lows:
• We analyze the modal imbalance problem and find that during the training process, the deviation of the gradi-ent update direction of the uni-modal became larger, indicating that we should not regulate along the origi-nal gradient.
• We propose PMR to address the modal imbalance problem by actively accelerating slow-learning modal-ities with PCE loss and simultaneously alleviating the suppression of the dominant modality via PER.
• We conduct comprehensive experiments and demon-strate that 1) PMR can achieve considerable improve-ments over existing methods; 2) PMR is independent of the fusion method or model structure and has strong advantages in generality. 2.