Abstract
We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facili-tate the study of egocentric activities with challenging hand-object interactions. The dataset includes synchronized ego-centric and exocentric images sampled from the recent As-sembly101 dataset, in which participants assemble and dis-assemble take-apart toys. To obtain high-quality 3D hand pose annotations for the egocentric images, we develop an efficient pipeline, where we use an initial set of manual an-notations to train a model to automatically annotate a much larger dataset. Our annotation model uses multi-view fea-ture fusion and an iterative refinement scheme, and achieves an average keypoint error of 4.20 mm, which is 85% lower than the error of the original annotations in Assembly101.
AssemblyHands provides 3.0M annotated images, includ-ing 490K egocentric images, making it the largest existing benchmark dataset for egocentric 3D hand pose estimation.
Using this data, we develop a strong single-view baseline of 3D hand pose estimation from egocentric images. Further-more, we design a novel action classification task to evalu-ate predicted 3D hand poses. Our study shows that having higher-quality hand poses directly improves the ability to recognize actions.
Figure 1. High-quality 3D hand poses as an effective represen-tation for egocentric activity understanding. AssemblyHands provides high-quality 3D hand pose annotations computed from multi-view exocentric images sampled from Assembly101 [28], which originally comes with inaccurate annotations computed from egocentric images (see the incorrect left-hand pose predic-tion). As we experimentally demonstrate on an action classifica-tion task, models trained on high-quality annotations achieve sig-nificantly higher accuracy. 1.

Introduction
Recognizing human activities is a decades-old problem in computer vision [17]. With recent advancements in user-assistive augmented reality and virtual reality (AR/VR) sys-tems, there is an increasing demand for recognizing ac-tions from the egocentric (first-person) viewpoint. Popu-lar AR/VR headsets such as Microsoft HoloLens, Magic
Leap, and Meta Quest are typically equipped with egocen-tric cameras to capture a user’s interactions with the real or virtual world. In these scenarios, the user’s hands manip-* Work done during internship. ulating objects is a very important modality of interaction.
In particular, hand poses (e.g., 3D joint locations) play a central role in understanding and enabling hand-object in-teraction [3, 18], pose-based action recognition [7, 20, 28], and interactive interfaces [10, 11].
Recently, several large-scale datasets for understanding egocentric activities have been proposed, such as EPIC-KITCHENS [5], Ego4D [8], and Assembly101 [28]. In par-ticular, Assembly101 highlights the importance of 3D hand poses in recognizing procedural activities such as assem-bling toys. 3D hand poses are compact representations, and are highly indicative of actions and even the objects that are interacted with– for example, the “screwing” hand motion is
Figure 2. Construction of AssemblyHands dataset and a benchmark task for egocentric 3D hand pose estimation. We first use manual annotations and an automatic annotation network (MVExoNet) to generate accurate 3D hand poses for multi-view images sampled from the Assembly101 dataset [28]. These annotations are used to train a single-view 3D hand pose estimation network (SVEgoNet) from egocentric images. Finally, the predicted hand poses are evaluated by the action classification task. a strong cue for the presence of a screwdriver. Notably, the authors of Assembly101 found that, for classifying assem-bly actions, learning from 3D hand poses is more effective than solely using video features. However, a drawback of this study is that the 3D hand pose annotations in Assem-bly101 are not always accurate, as they are computed from an off-the-shelf egocentric hand tracker [11]. We observed that the provided poses are often inaccurate (see Fig. 1), es-pecially when hands are occluded by objects from the ego-centric perspective. Thus, the prior work has left us with an unresolved question: How does the quality of 3D hand poses affect action recognition performance?
To systematically answer this question, we propose a new benchmark dataset named AssemblyHands.
It in-cludes a total of 3.0M images sampled from Assembly101, annotated with high-quality 3D hand poses. We not only acquire manual annotations, but also use them to train an accurate automatic annotation model that uses multi-view feature fusion from exocentric (i.e., third-person) images; please see Fig. 2 for an illustration. Our model achieves 4.20 mm average keypoint error compared to manual anno-tations, which is 85% lower than the original annotations provided in Assembly101. This automatic pipeline enables us to efficiently scale annotations to 490K egocentric im-ages from 34 subjects, making AssemblyHands the largest egocentric hand pose dataset to date, both in terms of scale and subject diversity. Compared to recent hand-object inter-action datasets, such as DexYCB [3] and H2O [18], our As-semblyHands features significantly more hand-object com-binations, as each multi-part toy can be disassembled and assembled at will,
Given the annotated dataset, we first develop a strong baseline for egocentric 3D hand pose estimation, using 2.5D heatmap optimization and hand identity classification.
Then, to evaluate the effectiveness of predicted hand poses, we propose a novel evaluation scheme: action classification from hand poses. Unlike prior benchmarks on egocentric hand pose estimation [7, 18, 24], we offer detailed analysis of the quality of 3D hand pose annotation, its influence on the performance of an egocentric pose estimator, and the utility of predicted poses for action classification.
Our contributions are summarized as follows:
• We offer a large-scale benchmark dataset, dubbed
AssemblyHands, with 3D hand pose annotations for 3.0M images sampled from the Assembly101 dataset, including 490K egocentric images.
• We propose an automatic annotation pipeline with multi-view feature fusion and iterative refinement, leading to 85% error reduction in the hand pose an-notations.
• We define a benchmark task for egocentric 3D hand pose estimation with the evaluation from action classi-fication. We provide a strong single-view baseline that optimizes 2.5D keypoint heatmaps and classifies hand identity. Our results confirm that having high-quality 3D hand poses significantly improves egocentric ac-tion recognition performance. 2.