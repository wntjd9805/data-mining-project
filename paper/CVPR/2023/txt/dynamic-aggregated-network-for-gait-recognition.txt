Abstract
Gait recognition is beneficial for a variety of applica-tions, including video surveillance, crime scene investi-gation, and social security, to mention a few. However, gait recognition often suffers from multiple exterior fac-tors in real scenes, such as carrying conditions, wear-ing overcoats, and diverse viewing angles. Recently, var-ious deep learning-based gait recognition methods have achieved promising results, but they tend to extract one of the salient features using fixed-weighted convolutional net-works, do not well consider the relationship within gait fea-tures in key regions, and ignore the aggregation of complete motion patterns. In this paper, we propose a new perspec-tive that actual gait features include global motion patterns in multiple key regions, and each global motion pattern is composed of a series of local motion patterns. To this end, we propose a Dynamic Aggregation Network (DANet) to learn more discriminative gait features. Specifically, we create a dynamic attention mechanism between the features of neighboring pixels that not only adaptively focuses on key regions but also generates more expressive local motion patterns. In addition, we develop a self-attention mecha-nism to select representative local motion patterns and fur-ther learn robust global motion patterns. Extensive exper-iments on three popular public gait datasets, i.e., CASIA-B, OUMVLP, and Gait3D, demonstrate that the proposed method can provide substantial improvements over the cur-rent state-of-the-art methods.1 1.

Introduction
Gait recognition aims to retrieve the same identity at a long distance, and has been widely used throughout social security [28], video surveillance [4, 15, 49], crime investi-gation [25], and so on. Compared with action recognition
[17, 53, 54] and person re-identification [2, 55, 60, 61], the
*Corresponding Authors 1Code available at https://github.com/XKMar/FastGait
Figure 1. The features of each pixel are mapped as a vector with both magnitude and phase components. The magnitude represents contextual information, while the phase direction is used to con-struct dynamic attention models for the key regions. The convo-lution operation is denoted by “∗”, and the blue circles in the dia-grams represent the key regions learned by the dynamic attention. gait recognition task is one of the most challenging fine-grained label classification problems. On the one hand, sil-houette data is a binary image of a person suffering from the limitations of the segmentation algorithm [26, 62, 63], with occasional holes and broken edges. On the other hand, gait recognition is also impacted by various exterior factors in real scenes, such as carrying conditions, wearing coats, and diverse viewing angles. Different angles and clothing conditions will greatly change the silhouette appearance of the same person, resulting in the intra-class variance being much greater than inter-class. We ask: How to learn more robust features adaptively for each person under the influ-ence of various external factors? We attempt to answer this question from the following perspectives: (i) Local Motion Patterns. Gait, or the act of walking, is essentially the coordinated movement of body parts. In a gait sequence, we observe that each part has a unique rep-resentative motion pattern, and each motion pattern is com-posed of a set of localized sub-movements. Therefore, it
well-designed components, i.e., Local Conv-Mixing Block (LCMB) and Global Motion Patterns Aggregator (GMPA).
Firstly, we encode the features of each pixel into the com-plex domain including magnitude and phase, where the magnitude term represents the contextual information and the phase term is used to establish the relationship between each vector. The local motion pattern is generated by ag-gregating the magnitude and phase of the vectors in the neighboring regions of focus. Secondly, we use the self-attentive mechanism in the GMPA model to dynamically se-lect sufficient discriminative local motion patterns and fur-ther learn to fit the actual gait patterns. Finally, with our proposed modules, we obtain the most representative sta-ble gait features for each person and outperform the state-of-the-art (SOTA) methods, especially under the most chal-lenging condition of cross-dressing.
Our main contributions can be summarized as follows:
• We propose a novel LCMB to extract the represen-tative local motion patterns, which can dynamically model the relationships among the features of neigh-boring pixels and then accurately locate key regions.
• We design an effective GMPA to select the discrimi-native local motion patterns and then aggregate them to obtain a robust global representation. To the best of our knowledge, it is the first attempt to explore the potential of self-attention model in this task.
• Experimental results are illustrated to demonstrate the effectiveness of the proposed method, outperforming the SOTA method on CASIA-B [56], OUMVLP [41] and Gait3D [59] datasets. In addition, many rigorous ablation experiments on CASIA-B [56] further vali-dated the effectiveness of each component in DANet. 2.