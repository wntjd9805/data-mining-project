Abstract
Test-time adaptation (TTA) is the problem of updating a pre-trained source model at inference time given test in-put(s) from a different target domain. Most existing TTA approaches assume the setting in which the target domain is stationary, i.e., all the test inputs come from a single target domain. However, in many practical settings, the test input distribution might exhibit a lifelong/continual shift over time. Moreover, existing TTA approaches also lack the ability to provide reliable uncertainty estimates, which is crucial when distribution shifts occur between the source and target domain. To address these issues, we present PETAL (Probabilistic lifElong Test-time Adapta-tion with seLf-training prior), which solves lifelong TTA us-ing a probabilistic approach, and naturally results in (1) a student-teacher framework, where the teacher model is an exponential moving average of the student model, and (2) regularizing the model updates at inference time using the source model as a regularizer. To prevent model drift in the lifelong/continual TTA setting, we also propose a data-driven parameter restoration technique which contributes to reducing the error accumulation and maintaining the knowledge of recent domains by restoring only the irrele-vant parameters. In terms of predictive error rate as well as uncertainty based metrics such as Brier score and negative log-likelihood, our method achieves better results than the current state-of-the-art for online lifelong test-time adap-tation across various benchmarks, such as CIFAR-10C,
CIFAR-100C, ImageNetC, and ImageNet3DCC datasets.
The source code for our approach is accessible at https:
//github.com/dhanajitb/petal. 1.

Introduction
Deep learning models exhibit excellent performance in settings where the model is evaluated on data from the same distribution as the training data. However, the performance of such models degrades drastically when the distribution of the test inputs at inference time is different from the distri-bution of the train data (source) [11, 16, 36]. Thus, there is a need to robustify the network to handle such scenarios. A particularly challenging setting is when we do not have any labeled target domain data to finetune the source model, and unsupervised adaptation must happen at test time when the unlabeled test inputs arrive. This problem is known as test-time adaptation (TTA) [28, 35, 38]. Moreover, due to the difficulty of training a single model to be robust to all po-tential distribution changes at test time, standard fine-tuning is infeasible, and TTA becomes necessary. Another chal-lenge in TTA is that the source domain training data may no longer be available due to privacy/storage requirements, and we only have access to the source pre-trained model.
Current approaches addressing the problem of TTA
[28, 35, 38, 41] are based on techniques like self-training based pseudo-labeling or entropy minimization in order to enhance performance under distribution shift during test-ing. One crucial challenge faced by existing TTA methods is that real-world machine learning systems work in non-stationary and continually changing environments. Even though the self-training based approaches perform well when test inputs are from a different domain but all still i.i.d., it has been found that the performance is unstable when target test inputs come from a continually changing environment [32]. Thus, it becomes necessary to perform test-time adaptation in a continual manner.
Such a setting is challenging because the continual adap-tation of the model in the long term makes it more diffi-cult to preserve knowledge about the source domain. Con-tinually changing test distribution causes pseudo-labels to become noisier and miscalibrated [10] over time, leading to error accumulation [3] which is more likely to occur if early predictions are incorrect. When adapting to new test input, the model tends to forget source domain knowledge, triggering catastrophic forgetting [27, 31, 33]. Moreover, existing TTA methods do not account for model/predictive uncertainty, which can result in miscalibrated predictions.
Recently, [41] proposed CoTTA, an approach to address the continual/lifelong TTA setting using a stochastic pa-rameter reset mechanism to prevent forgetting. Their reset mechanism however is based on randomly choosing a sub-set of weights to reset and is not data-driven. Moreover,
Figure 1. Left: Problem setup of online lifelong TTA. During adaptation on test input, the source domain data is no longer available, and only the model pre-trained on the source domain is provided. Test inputs from different domains arrive continually, and the model has no knowledge about change in the domain. Right: Our proposed probabilistic framework for online lifelong TTA. We obtain a source domain pre-trained model from the posterior density learned using training data from the source domain. The posterior density is used to initialize the student model. A test sample is provided as input to the student model. Using multiple augmentations of a test sample, we obtain augmentation averaged prediction from the teacher model. The loss term consists of log posterior and cross-entropy terms utilizing student and teacher model predictions. We utilize backpropagation to update student model and exponential moving average for teacher model. their method does not take into account model/predictive uncertainty and is therefore susceptible to overconfident and miscalibrated predictions.
To improve upon these challenges of continual/lifelong
TTA, we propose a principled, probabilistic framework for lifelong TTA. Our framework (shown in Fig. 1 (Right)) constructs a posterior distribution over the source model weights and a data-dependent prior which results in a self-training based cross-entropy loss, with a regularizer term in the learning objective. This regularizer arises from terms corresponding to the posterior, which incorporates knowl-edge of source (training) domain data.
Moreover, our framework also offers a probabilistic per-spective and justification to the recently proposed CoTTA
[41] approach, which arises as a special case of our proba-bilistic framework. In particular, only considering the data-driven prior in our approach without the regularizer term, corresponds to the student-teacher based cross-entropy loss used in CoTTA. Further, to improve upon the stochastic restore used by [41], we present a data-driven parameter restoration based on Fisher Information Matrix (FIM). In terms of improving accuracy and enhancing calibration dur-ing distribution shift, our approach surpasses existing ap-proaches in various benchmarks.
Main Contributions 1. From a probabilistic perspective, we arrive at the student-teacher training framework in our proposed
Probabilistic lifElong Test-time Adaptation with seLf-Inspired from the training prior (PETAL) approach. self-training framework [19, 42], the teacher model is the exponential moving average of the student model, as depicted in Fig. 1 (Right). 2. The student-teacher cross-entropy loss with a regular-izer term corresponding to posterior of source domain data naturally emerges in the probabilistic formulation. 3. We propose a data-driven parameter restoration based on Fisher Information Matrix (FIM) to handle error ac-cumulation and catastrophic forgetting. 2. Problem Setup and