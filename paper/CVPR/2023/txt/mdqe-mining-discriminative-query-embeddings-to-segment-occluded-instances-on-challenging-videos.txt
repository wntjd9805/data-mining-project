Abstract networks [17, 19, 45, 46].
While impressive progress has been achieved, video in-stance segmentation (VIS) methods with per-clip input of-ten fail on challenging videos with occluded objects and crowded scenes. This is mainly because instance queries in these methods cannot encode well the discriminative em-beddings of instances, making the query-based segmenter difficult to distinguish those ‘hard’ instances. To address these issues, we propose to mine discriminative query em-beddings (MDQE) to segment occluded instances on chal-lenging videos. First, we initialize the positional embed-dings and content features of object queries by considering their spatial contextual information and the inter-frame ob-ject motion. Second, we propose an inter-instance mask repulsion loss to distance each instance from its nearby non-target instances. The proposed MDQE is the first VIS method with per-clip input that achieves state-of-the-art re-sults on challenging videos and competitive performance on simple videos. In specific, MDQE with ResNet50 achieves 33.0% and 44.5% mask AP on OVIS and YouTube-VIS 2021, respectively. Code of MDQE can be found at https:
//github.com/MinghanLi/MDQE_CVPR2023. 1.

Introduction
Video instance segmentation (VIS) [51] aims to obtain pixel-level segmentation masks for instances of different classes over the entire video. The current VIS methods can be roughly divided into two paradigms: per-frame in-put based methods [3, 18, 20, 26, 47, 51, 52] and per-clip input based methods [1, 19, 27, 45, 46, 48, 53]. The for-mer paradigm first partitions the whole video into individual frames to segment objects frame by frame, and then asso-ciate the predicted instance masks across frames, while the latter takes per-clip spatio-temporal features as input to pre-dict multi-frame instance masks with the help of embedding learning [1], graph neural networks [41] and transformer
*Corresponding author.
The recently proposed per-clip VIS methods [19, 45, 46, 48] have set new records on the YouTube-VIS datasets
[51], achieving significant performance improvement over the per-frame VIS methods [3, 5, 12, 14, 26, 52, 54]. Seq-Former [46] and VITA [17] locate an instance in each frame and aggregate temporal information to learn powerful rep-resentations of video-level instances via a naive weighted manner and a video-level decoder, respectively. However, on the challenging OVIS dataset [37], which includes oc-cluded or similar-looking instances in crowded scenes, the per-clip VIS methods lag behind the per-frame ones. Ac-tually, the recently developed per-frame method IDOL [47] records state-of-the-art performance on OVIS by introduc-ing contrastive learning [10, 35, 44] to learn inter-frame in-stance embeddings. We argue that the per-clip VIS meth-ods should be able to exploit richer spatial-temporal fea-tures and achieve better performance than their per-frame counterparts. However, there are two main issues that limit the existing per-clip methods to achieve this goal.
First, existing query-based VIS methods adopt zero or random input as the positional embeddings and content fea-tures of object queries in decoder layers, which cannot en-code spatio-temporal prior of objects, resulting in poor re-sults on challenging videos. Second, during training, the existing mask prediction loss mainly forces each query to match the pixels of its target instance [21,42] and mismatch the pixels of other instances and the background. No fur-ther inter-instance clue has been exploited to teach the seg-menter to distinguish mutually occluded instances.
To address the above issues, we propose to mine dis-criminative query embeddings (MDQE) to better segment hard instances on challenging videos for per-clip VIS meth-ods. First, we propose to improve the initialization of ob-ject queries to specify discriminative spatial-temporal pri-ors. We divide the activation map of each frame into several patches via a grid and select the peak point in each patch as the initial positions of frame-level queries, and then asso-ciate them across frames by embedding similarity to ensure that frame-level queries in the same grid of the video clip
Figure 1. (a) The proposed MDQE architecture consists of a backbone and encoder that extract multi-scale features F from a video clip, a query initialization module that produces temporally-aligned frame-level queries qt, a decoder that decodes discriminative clip-level queries q, and a Mask Net that generates mask features D. The mask features D and clip-level queries q are combined via a linear combination to obtain the clip-level instance mask ˆM , which is supervised by our proposed inter-instance mask repulsion loss in Sec. 3.3. (b) The frame-level query initialization consists of two steps: grid-guided query selection and inter-frame query association, resulting in temporally-aligned frame-level queries. Please refer to Sec. 3.2 for more details. can correspond to the same object. Second, to teach the query-based segmenter to distinguish occluded instances, we replace the original mask prediction loss with an inter-instance mask repulsion loss, which forces each query to ac-tivate the pixels of its target instance and suppress the pixels of its surrounding non-target instances.
The proposed VIS method with per-clip input, namely
MDQE, is the first to achieve contrastive learning of in-stance embeddings via query initialization and the inter-instance mask repulsion supervision, which can effectively segment hard instances on challenging videos. Our exper-iments on both OVIS and YouTube-VIS datasets validate that MDQE with per-clip input achieve competitive perfor-mance with its per-frame competitors. 2.