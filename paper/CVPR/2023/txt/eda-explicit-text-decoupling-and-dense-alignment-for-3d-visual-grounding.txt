Abstract 3D visual grounding aims to find the object within point clouds mentioned by free-form natural language descrip-tions with rich semantic cues. However, existing methods either extract the sentence-level features coupling all words or focus more on object names, which would lose the word-level information or neglect other attributes. To alleviate these issues, we present EDA that Explicitly Decouples the textual attributes in a sentence and conducts Dense
Alignment between such fine-grained language and point cloud objects. Specifically, we first propose a text decou-pling module to produce textual features for every seman-tic component. Then, we design two losses to supervise
∗Corresponding author. This work was supported in part by Shenzhen
Research Project under Grant JCYJ20220531093215035. the dense matching between two modalities: position align-ment loss and semantic alignment loss. On top of that, we further introduce a new visual grounding task, locat-ing objects without object names, which can thoroughly evaluate the model’s dense alignment capacity. Through experiments, we achieve state-of-the-art performance on two widely-adopted 3D visual grounding datasets, Scan-Refer and SR3D/NR3D, and obtain absolute leadership on our newly-proposed task. The source code is available at https://github.com/yanmin-wu/EDA. 1.

Introduction
Multi-modal cues can highly benefit the 3D environ-ment perception of an agent, including 2D images, 3D point clouds, and language. Recently, 3D visual grounding (3D
VG) [8,50], also known as 3D object referencing [3], has at-tached much attention as an important 3D cross-modal task.
Its objective is to find the target object in point cloud scenes by analyzing the descriptive query language, which requires understanding both 3D visual and linguistic context.
Language utterances typically involve words describ-ing appearance attributes, object categories, spatial rela-tionships and other characteristics, as shown by different colours in Fig. 1(a), requiring that the model integrate multi-ple cues to locate the mentioned object. Compared with 2D
Visual Grounding [18,19,67,71], the sparseness and incom-pleteness of point clouds, and the diversity of language de-scriptions produced by 3D multi-view, make 3D VG more challenging. Existing works made significant progress from the following perspectives: improving point cloud features extraction by sparse convolution [70] or 2D images assis-tance [68]; generating more discriminative object candi-dates through instance segmentation [33] or language mod-ulation [46]; identifying complex spatial relationships be-tween entities via graph convolution [23] or attention [6].
However, we observe two issues that remain unex-plored. 1) Imbalance: The object name can exclude most there is only one candidates, and even in some cases, name-matched object, as the “door” and “refrigerator” in
Fig. 1(b1, b2). This shortcut may lead to an inductive bias in the model that pays more attention to object names while weakening other properties such as appearance and relation-ships, resulting in imbalanced learning. 2) Ambiguity: Ut-terances frequently refer to multiple objects and attributes (such as “black object, tall shelf, fan” in Fig. 1(b4)), while the model’s objective is to identify only the main object, leading to an ambiguous understanding of language de-scriptions. These insufficiencies of existing works stem from their characteristic of feature coupling and fusing im-plicitly. They input a sentence with different attribute words but output only one globally coupled sentence-level fea-ture that subsequently matches the visual features of can-didate objects. The coupled feature is ambiguous because some words may not describe the main object (green text in Fig. 1) but other auxiliary objects (red text in Fig. 1).
Alternatively, using the cross-modal attention of the Trans-former [21, 63] automatically and implicitly to fuse visual and text features. However, this may encourage the model to take shortcuts, such as focusing on object categories and ignoring other attributes, as previously discussed.
Instead, we propose a more intuitive decoupled and ex-plicit strategy. First, we parse the input text to decouple different semantic components, including the main object word, pronoun, attributes, relations, and auxiliary object words. Then, performing dense alignment between point cloud objects and multiple related decoupled components achieves fine-grained feature matching, which avoids the inductive bias resulting from imbalanced learning of differ-ent textual components. As the final grounding result, we explicitly select the object with the highest similarity to the decoupled text components (instead of the entire sentence), avoiding ambiguity caused by irrelevant components. Ad-ditionally, to explore the limits of VG and examine the com-prehensiveness and fine-graininess of visual-language per-ception of the model, we suggest a challenging new task:
Grounding without object name (VG-w/o-ON), where the name is replaced by “object” (see Fig. 1(b)), forcing the model to locate objects based on other attributes and relationships. This setting makes sense because utterances that do not mention object names are common expressions in daily life, and in addition to testing whether the model takes shortcuts. Benefiting from our text decoupling oper-ation and the supervision of dense aligned losses, all text components are aligned with visual features, making it pos-sible to locate objects independent of object names.
To sum up, the main contributions of this paper are as follows: 1) We propose a text decoupling module to parse linguistic descriptions into multiple semantic components, followed by suggesting two well-designed dense aligned losses for supervising fine-grained visual-language feature fusion and preventing imbalance and ambiguity learning. 2) The challenging new 3D VG task of grounding without object names is proposed to comprehensively examine the model’s robust performance. 3) We achieve state-of-the-art performance on two datasets (ScanRefer and SR3D/NR3D) on the regular 3D VG task and absolute leadership on the new task evaluated by the same model without retraining. 2.