Abstract
Given sparse depths and the corresponding RGB images, depth completion aims at spatially propagating the sparse measurements throughout the whole image to get a dense depth prediction. Despite the tremendous progress of deep-learning-based depth completion methods, the locality of the convolutional layer or graph model makes it hard for the network to model the long-range relationship between pixels. While recent fully Transformer-based architecture has reported encouraging results with the global recep-tive ﬁeld, the performance and efﬁciency gaps to the well-developed CNN models still exist because of its deteriora-tive local feature details. This paper proposes a Joint Con-volutional Attention and Transformer block (JCAT), which deeply couples the convolutional attention layer and Vision
Transformer into one block, as the basic unit to construct our depth completion model in a pyramidal structure. This hybrid architecture naturally beneﬁts both the local connec-tivity of convolutions and the global context of the Trans-former in one single model. As a result, our Completion-Former outperforms state-of-the-art CNNs-based methods on the outdoor KITTI Depth Completion benchmark and indoor NYUv2 dataset, achieving signiﬁcantly higher efﬁ-ciency (nearly 1/3 FLOPs) compared to pure Transformer-based methods. Code is available at https://github. com/youmi-zym/CompletionFormer. 1.

Introduction
Active depth sensing has achieved signiﬁcant gains in performance and demonstrated its utility in numerous ap-plications, such as autonomous driving and augmented re-ality. Although depth maps captured by existing commer-cial depth sensors (e.g., Microsoft Kinect [23], Intel Re-alSense [11]) or depths points within the same scanning line of LiDAR sensors are dense, the distance between valid/correct depth points could still be far owing to the sensor noise, challenging conditions such as transparent, shining, and dark surfaces, or the limited number of scan-ning lines of LiDAR sensors. To address these issues, depth completion [2, 16, 26, 31], which targets at completing and reconstructing the whole depth map from sparse depth mea-surements and a corresponding RGB image (i.e., RGBD), has gained much attention in the latest years.
For depth completion, one key point is to get the depth afﬁnity among neighboring pixels so that reliable depth la-bels can be propagated to the surroundings [2, 3, 8, 16, 26].
Based on the fact that the given sparse depth could be highly sparse due to noise or even no measurement be-ing returned from the depth sensor, it requires depth com-pletion methods to be capable of 1) detecting depth out-liers by measuring the spatial relationship between pixels in both local and global perspectives; 2) fusing valid depth values from close or even extremely far distance points.
All these properties ask the network for the potential to capture both local and global correlations between pixels.
Current depth completion networks collect context infor-mation with the widely used convolution neural networks (CNNs) [2, 3, 8, 16, 26, 29, 37, 51] or graph neural net-work [42, 49]. However, both the convolutional layer and graph models can only aggregate within a local region, e.g. square kernel in 3 3 for convolution and kNN-based neigh-borhood for graph models [42, 49], making it still tough to model global long-range relationship, in particular within the shallowest layers of the architecture. Recently, Guide-Former [31] resorts fully Transformer-based architecture to enable global reasoning. Unfortunately, since Vision Trans-formers project image patches into vectors through a single step, this causes the loss of local details, resulting in ignor-ing local feature details in dense prediction tasks [28, 43].
For depth completion, the limitations affecting pure CNNs or Transformer based networks also manifest, as shown in
Fig. 1. Despite any distance the reliable depth points could be distributed at, exploring an elegant integration of these two distinct paradigms, i.e. CNNs and Transformer, has not
×
(a) RGB (b) Pure CNNs (c) Pure Transformer  (d) CompletionFormer
Figure 1. Comparison of attention maps of pure CNNs, Vision Transformer, and the proposed CompletionFormer with joint CNNs and Transformer structure. The pixel highlighted with a yellow cross in RGB image (a) is the one we want to observe how the network predicts it. Pure CNNs architecture (b) activates discriminative local regions (i.e., the region on the ﬁre extinguisher), whereas pure
Transformer based models (c) activate globally yet fail on local details. In contrast, our full CompletionFormer (d) can retain both the local details and global context. been studied for depth completion yet.
In this work, we propose CompletionFormer, a pyrami-dal architecture coupling CNN-based local features with
Transformer-based global representations for enhanced depth completion. Generally, there are two gaps we are facing: 1) the content gap between RGB and depth in-put; 2) the semantic gap between convolution and Trans-former. As for the multimodal input, we propose embed-ding the RGB and depth information at the early network stage. Thus our CompletionFormer can be implemented in an efﬁcient single-branch architecture as shown in Fig. 2 and multimodal information can be aggregated throughout the whole network. Considering the integration of convolu-tion and Transformer, previous work has explored from sev-eral different perspectives [6,12,25,28,43] on image classi-ﬁcation and object detection. Although state-of-the-art per-formance has been achieved on those tasks, high computa-tion cost [12] or inferior performance [6, 12] are observed when these networks are directly adapted to depth com-pletion task. To promise the combination of self-attention and convolution still being efﬁcient, and also effective, we embrace convolutional attention and Transformer into one block and use it as the basic unit to construct our network in a multi-scale style. Speciﬁcally, the Transformer layer is inspired by Pyramid Vision Transformer [39], which adopts spatial-reduction attention to make the Transformer layer much more lightweight. As for the convolution-related part, the common option is to use plain convolutions such as In-verted Residual Block [32]. However, the huge semantic gap between convolution and the Transformer and the lost local details by Transformer require the convolutional lay-ers to increase its own capacity to compensate for it. Fol-lowing this rationale, we further introduce spatial and chan-nel attention [40] to enhance convolutions. As a result, without any extra module to bridge the content and semantic gaps [12, 28, 31], every convolution and Transformer layer in the proposed block can access the local and global fea-tures. Hence, information exchange and fusion happen ef-fectively at every block of our network.
To summarize, our main contributions are as follows:
• We propose integrating Vision Transformer with con-volutional attention layers into one block for depth completion, enabling the network to possess both local and global receptive ﬁelds for multi-modal information interaction and fusion. In particular, spatial and chan-nel attention are introduced to increase the capacity of convolutional layers.
• Taking the proposed Joint Convolutional Attention and
Transformer (JCAT) block as the basic unit, we intro-duce a single-branch network structure, i.e. Comple-tionFormer. This elegant design leads to a compara-ble computation cost to current CNN-based methods while presenting signiﬁcantly higher efﬁciency when compared with pure Transformer based methods.
• Our CompletionFormer yields substantial improve-ments to depth completion compared to state-of-the-art methods, especially when the provided depth is very sparse, as often occurs in practical applications. 2.