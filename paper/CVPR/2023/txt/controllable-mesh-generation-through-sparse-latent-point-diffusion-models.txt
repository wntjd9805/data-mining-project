Abstract
Mesh generation is of great value in various applica-tions involving computer graphics and virtual content, yet designing generative models for meshes is challenging due to their irregular data structure and inconsistent topology of meshes in the same category. In this work, we design a novel sparse latent point diffusion model for mesh genera-tion. Our key insight is to regard point clouds as an inter-mediate representation of meshes, and model the distribu-tion of point clouds instead. While meshes can be generated from point clouds via techniques like Shape as Points (SAP), the challenges of directly generating meshes can be effec-tively avoided. To boost the efficiency and controllability of our mesh generation method, we propose to further en-code point clouds to a set of sparse latent points with point-wise semantic meaningful features, where two DDPMs are trained in the space of sparse latent points to respectively model the distribution of the latent point positions and fea-tures at these latent points. We find that sampling in this latent space is faster than directly sampling dense point clouds. Moreover, the sparse latent points also enable us to explicitly control both the overall structures and local de-tails of the generated meshes. Extensive experiments are conducted on the ShapeNet dataset, where our proposed sparse latent point diffusion model achieves superior per-formance in terms of generation quality and controllability when compared to existing methods. Project page, code and appendix: https://slide-3d.github.io. 1.

Introduction
Being a fundamental representation of 3D objects in computer graphics, meshes are widely used in applications such as VR, AR, and games, and a generative model for meshes is thus of great value. By representing 3D ob-jects with vertices, edges, and faces, meshes lead to more
*Equal Contribution. efficient modeling and computation of object geometries.
However, such a specialized design also results in several intrinsic challenges for generative models. The first one is the irregular data structure of meshes, where the discrete vertex connections make it hard to define operations like convolution and up-sampling on meshes. Moreover, unlike point clouds and volumes, the topology of meshes is vary-ing across different object instances in the same category.
Therefore template-based methods [11,14,21,34,36,39] can only obtain meshes obeying the topology of used templates, causing defects like self-intersection when the deformation is significant.
Facing these challenges mentioned above, we propose to generate meshes indirectly via an intermediate representa-tion that is easier to model. Inspired by recent successes of deep neural networks in modeling the distribution of point clouds [2, 22, 24, 38, 43] and reconstructing meshes from point clouds [11, 27], we propose to use point clouds as an intermediate representation of meshes. Consequently, the generation of meshes is effectively reformulated as the generation of point clouds, followed by transforming point clouds into meshes. Such a reformulation not only enables us to take advantage of the advances of point cloud gen-eration methods, but also successfully bypasses the afore-mentioned challenges, as the distribution of point clouds is continuous and point clouds are unordered sets without explicit topology. In this paper, we adopt denoising diffu-sion probabilistic models (DDPMs) [13, 32], demonstrated promising results in modeling point clouds [22, 24, 43], to learn the distribution of the point clouds. And Shape as
Points (SAP) [27] is employed to reconstruct meshes from the generated point clouds, which is a powerful surface re-construction technique that can extract high-quality water-tight meshes from point clouds at low inference times.
With the introduction of point clouds as the intermedi-ate representation of meshes, we can use DDPMs to model the distribution of meshes. However, to ensure the qual-ity of transformed meshes, the generated point clouds need
Figure 1. We can use the sparse latent points to control the shape of the generated meshes. Red points are stationary, and blue points are moving. Black arrows indicate the moving direction of the blue points. Some points are invisible because they are within the mesh. Note that the latent points are not always strictly lying on the surface of the generated meshes. This is because our point cloud decoder assumes that some noises exist in the positions of the sparse latent points. It will generate a mesh that best fits the latent points, but avoid generating defective meshes just to strictly fit the latent points. to be sufficiently dense. This inevitably leads to two is-sues. At first, the overall computational complexity is high, since sampling thousands of points from DDPMs is quite time-consuming. It is also difficult to explicitly control the structure of meshes via dense point clouds, as the seman-tics of their points are not sufficiently compact. We there-fore further encode a point cloud to a sparse set of seman-tic latent points with features attached to every point, and learn a Sparse Latent poInt Diffusion modEl (SLIDE) for mesh generation following the framework of latent diffu-sion models [28, 33]. Specifically, we train two DDPMs to learn the distribution of this latent space. The first
DDPM learns the distribution of positions of the sparse la-tent points, and the second one learns the distribution of the features conditioned on the positions of the points. By cascading these two DDPMs together, we can perform un-conditional generation of sparse latent points and their fea-tures. We adopt farthest point sampling (FPS) to obtain the positions of sparse latent points from a dense point cloud, and a neural encoder is deployed to attach each latent point a semantically meaningful feature. Accordingly, a neural decoder is used to recover a dense point cloud from the po-sitions and features of the sparse latent points and then re-construct the mesh. In this way, we maintain the quality of generated meshes while being able to control their overall structures and local details respectively via controlling con-figurations and semantic features of the sparse latent points, as shown in Figure 1. Moreover, we find that sampling in this sparse latent point space is significantly faster than di-rectly sampling dense point clouds.
We conduct experiments on the ShapeNet [4] dataset to compare both point cloud and mesh generation perfor-mance of SLIDE with other methods. SLIDE achieves superior performances in terms of both visual quality and quantitative metrics. It also demonstrates great flexibility in controlling the overall structures and local part shapes of the generated objects without using any part-annotated 3D data. In summary, the main contributions of our work are: 1) We propose to use point clouds as the intermediate representation of meshes. By generating point clouds first and then reconstructing surface from them, we can gener-ate meshes with diverse topology and high quality. 2) We design a novel point cloud autoencoder to further encode point clouds to a sparse set of latent points with features attached to them. Sampling in this latent space is more ef-ficient than directly sampling dense point clouds. 3) By de-composing the learning of the positions of the sparse latent points and features of them, we can perform both uncondi-tional point cloud generation and controllable point cloud generation based on the positions of the sparse latent points as shown in Figure 1. We can also perform both global and local interpolations in this latent space. 2.