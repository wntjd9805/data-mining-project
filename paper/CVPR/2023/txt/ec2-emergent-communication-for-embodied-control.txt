Abstract
Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new en-vironments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with ab-stract, symbolic structures. While recent approaches ap-ply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their comple-mentary differences can lead to more holistic representa-tions for downstream adaption. To this end, we propose
Emergent Communication for Embodied Control (EC2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised “language” of videos via emergent commu-nication, which bridges the semantics of video details and structures of natural language. We learn embodied repre-sentations of video trajectories, emergent language, and natural language using a language model, which is then used to finetune a lightweight policy network for down-stream control. Through extensive experiments in Meta-world and Franka Kitchen embodied benchmarks, EC2 is shown to consistently outperform previous contrastive learning methods for both videos and texts as task inputs.
Further ablations confirm the importance of the emergent language, which is beneficial for both video and language learning, and significantly superior to using pre-trained video captions. We also present a quantitative and qual-itative analysis of the emergent language and discuss fu-ture directions toward better understanding and leveraging emergent communication in embodied tasks. 1.

Introduction
We study the problem of few-shot embodied control, where an embodied agent needs to execute language in-structions or follow video demonstrations given only a few examples in a new environment. Such a capability of fast adaption is key to practical robotic deployment, as it is non-scalable and expensive to collect extensive action-labeled (a) Emergent language for embodied control. (b) Performance comparison on overall success rate across all tasks.
Figure 1. Overview of EC2. The key idea of EC2 is to build the link between perceptual grounding and symbolic concept via emergent communication (EC) language. We learn to extract em-bodied representation via language model utilizing emergent lan-guage or natural language as prompt and visual observation as in-put for downstream embodied control tasks. The actions are gen-erated by a lightweight policy network containing a few MLP [51] layers that map the learned frozen embodied representation into action space. Extensive experiments show that EC2 outperforms existing methods in Metaworld [60] and Franka Kitchen [17] benchmarks. trajectories for each new application scenario.
Instead, agents need to leverage pre-trained video and language rep-resentations to quickly learn how to act, and generalize across high-level concepts (e.g. object and verb types) as well as low-level visual features.
How can we pre-train multi-modal representations of videos and texts for downstream embodied control in dif-Inspired by the success of image-text ferent domains? models such as CLIP [47], recent work has investigated contrastive representation learning approaches using paired video-language data, with application toward robotic ma-nipulation and control tasks [24, 38]. However, simply aligning video and language pairs in the embedding space omits the difference between these two modalities: while
videos contain more visual and motion details, language abstracts away key structures underlying the task. For ex-ample, a video of opening a door contains details of ap-proaching the handle, pressing the handle, pulling the door open, and so on, while the corresponding language descrip-tion could be as simple as “open a door” — more detailed language descriptions such as “press down the handle” is usually not available in existing datasets and very expensive to manually label. But for downstream control purposes, a video and its abstract language description could present complementary benefits — while the former is expressive in details needed for low-level control, the latter provides the structure for generalization across domains, tasks, and skills. Leveraging such modality differences presents op-portunities to better incorporate language and videos for embodied representation learning, marrying their respective benefits rather than forcing them to be aligned.
One perspective to view the video-language difference language is through the emergence of language [39, 55]: derives meaning from its use [57].
It is a communica-tion tool humans develop to collaboratively solve embodied tasks, thus abstracting perception experiences like videos into symbolic concepts. Emergent communication in ma-chine learning [9, 26, 28, 30, 56] aims to similarly learn an
“emergent language” of perception in an unsupervised and domain-adaptive way: as Figure 2(a) shows, a speaker and a listener network play a referential game, where a speaker maps some visual stimuli (e.g. image, video) into a message of discrete tokens, and a listener uses the message to choose the speaker reference out of detractors. By jointly optimiz-ing for game success, the communication protocol emerges structural and semantic properties resembling language, yet its discrete tokens might convey more fine-grained con-cepts than natural language useful for distinguishing stim-uli. Thus, an emergent language can act as a bridge between natural language and videos, featuring a structure similar to the former while preserving the semantics of the latter.
Inspired by recent work that links emergent and natu-ral language [58], we propose Emergent Communication for Embodied Control (EC2), a three-phase scheme to learn embodied representations for downstream few-shot control (Figure 1(a-c)). First, we learn an emergent language of demonstration videos without using any language labels, which could potentially capture domain-specific concepts not directly available in paired natural language captions.
Second, we learn an embodied representation by using a language model to predict trajectory segments conditional on trajectory contexts and language annotations, both natu-ral and emergent. Rather than forcing video and language representations to be aligned, such a sequence modeling approach learns to represent natural language (for compo-sitional generalization) and emergent language (for low-level perception and control) jointly with video trajectories, where the emergent language serves as a bridge between language and videos. Third, we train a lightweight policy network on top of the embodied representation to few-shot adapt to downstream embodied control.
We demonstrate that emergent language can help embod-ied control tasks and enable data-efficient few-shot down-stream imitation learning via extensive experimental results across two existing benchmark simulation environments (Franka-Kitchen [18] and MetaWorld [60]). EC2 outper-forms existing methods and achieves state-of-the-art per-formance on both language and video instruction following tasks. Our study also shows that although emergent lan-guage and natural language are utilized in a parallel manner in the pre-training phase, the word embeddings of natural language can still be predicted by linear regression models from emergent language. We further investigate the impact of emergent language and natural language correlation on the performance of downstream tasks and show that the per-formance in downstream tasks is best when the learned EC contains more information than natural language, i.e., when the accuracy of predicting natural language from emergent language is higher than that of predicting emergent lan-guage from natural language. Overall, on the basis of these results, we believe that EC2 has great potential to become a promising embodied control framework.
To sum up, our work makes the following contribu-tions. (1) we build an embodied representation pre-training framework with emergent communication (EC) via a lan-guage model, which incorporates both the abstract, com-positional structure of language and visuals with low-level motion details. (2) we develop a few-shot embodied con-trol system, which transfers the pre-trained language model to downstream tasks as a frozen module and quickly adapts lightweight policy networks with a few expert data to gen-erate actions. (3) we demonstrate that the learned represen-tation with EC can benefit few-shot embodied control tasks and extensive experiments show that EC2 outperforms ex-isting methods in Metaworld [60] and Franka Kitchen [17] benchmarks. 2.