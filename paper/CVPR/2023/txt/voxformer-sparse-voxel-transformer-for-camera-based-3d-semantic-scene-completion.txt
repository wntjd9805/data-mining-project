Abstract
Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capa-bility in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can out-put complete 3D volumetric semantics from only 2D images.
Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that gen-erates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images corre-spond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the fea-turization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we ap-ply a masked autoencoder design to propagate the infor-mation to all the voxels by self-attention. Experiments on
SemanticKITTI show that VoxFormer outperforms the state of the art with a relative improvement of 20.0% in geome-try and 18.1% in semantics and reduces GPU memory dur-ing training to less than 16GB. Our code is available on https://github.com/NVlabs/VoxFormer. 1.

Introduction
Holistic 3D scene understanding is an important problem in autonomous vehicle (AV) perception. It directly affects downstream tasks such as planning and map construction.
However, obtaining accurate and complete 3D information of the real world is difficult, since the task is challenged by the lack of sensing resolution and the incomplete observa-tion due to the limited field of view and occlusions.
To tackle the challenges, semantic scene completion (SSC) [1] was proposed to jointly infer the complete scene geometry and semantics from limited observations. An SSC solution has to simultaneously address two subtasks: scene reconstruction for visible areas and scene hallucination for
* Corresponding author: Zhiding Yu (zhidingy@nvidia.com) (a) A diagram of VoxFormer for camera-based se-Figure 1. mantic scene completion that predicts complete 3D geometry and semantics given only 2D images. After obtaining voxel query proposals based on depth, VoxFormer generates semantic voxels (b) A comparison against via an MAE-like architecture [3]. the state-of-the-art MonoScene [4] in different ranges on Se-manticKITTI [5]. VoxFormer performs much better in safety-critical short-range areas, while MonoScene performs indiffer-ently at three distances. The relative gains are marked by red. occluded regions. This task is further backed by the fact that humans can naturally reason about scene geometry and semantics from partial observations. However, there is still a significant performance gap between state-of-the-art SSC methods [2] and human perception in driving scenes.
Most existing SSC solutions consider LiDAR a pri-mary modality to enable accurate 3D geometric measure-ment [6–9]. However, LiDAR sensors are expensive and less portable, while cameras are cheaper and provide richer visual cues of the driving scenes. This motivated the study of camera-based SSC solutions, as first proposed in the pio-neering work of MonoScene [4]. MonoScene lifts 2D image inputs to 3D using dense feature projection. However, such a projection inevitably assigns 2D features of visible regions to the empty or occluded voxels. For example, an empty voxel occluded by a car will still get the car’s visual feature.
As a result, the generated 3D features contain many ambi-guities for subsequent geometric completion and semantic segmentation, resulting in unsatisfactory performance.
Our contributions. Unlike MonoScene, VoxFormer considers 3D-to-2D cross-attention to represent the sparse queries. The proposed design is motivated by two insights: (1) reconstruction-before-hallucination: the non-visible re-gion’s 3D information can be better completed using the re-constructed visible areas as starting points; and (2) sparsity-in-3D-space: since a large volume of the 3D space is usu-ally unoccupied, using a sparse representation instead of a dense one is certainly more efficient and scalable. Our con-tributions in this work can be summarized as follows:
• A novel two-stage framework that lifts images into a complete 3D voxelized semantic scene.
• A novel query proposal network based on 2D convolu-tions that generates reliable queries from image depth.
• A novel Transformer similar to masked autoencoder (MAE) [3] that yields complete 3D scene representation.
• VoxFormer sets a new state-of-the-art in camera-based
SSC on SemanticKITTI [5], as shown in Fig. 1 (b).
VoxFormer consists of class-agnostic query proposal (stage-1) and class-specific semantic segmentation (stage-2), where stage-1 proposes a sparse set of occupied vox-els, and stage-2 completes the scene representations starting from the proposals given by stage-1. Specifically, stage-1 has a lightweight 2D CNN-based query proposal network using the image depth to reconstruct the scene geometry. It then proposes a sparse set of voxels from predefined learn-able voxel queries over the entire field of view. Stage-2 is based on a novel sparse-to-dense MAE-like architecture as shown in Fig. 1 (a). It first strengthens the featurization of the proposed voxels by allowing them to attend to the im-age observations. Next, the non-proposed voxels will be associated with a learnable mask token, and the full set of voxels will be processed by self-attention to complete the scene representations for per-voxel semantic segmentation.
Extensive tests on the large-scale SemanticKITTI [5] show that VoxFormer achieves state-of-the-art performance in geometric completion and semantic segmentation. More the improvements are significant in safety-importantly, critical short-range areas, as shown in Fig. 1 (b). 2.