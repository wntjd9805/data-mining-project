Abstract
Advances in representation learning have led to great success in understanding and generating data in various do-mains. However, in modeling vector graphics data, the pure data-driven approach often yields unsatisfactory results in downstream tasks as existing deep learning methods often re-quire the quantization of SVG parameters and cannot exploit the geometric properties explicitly. In this paper, we propose a transformer-based representation learning model (SVG-former) that directly operates on continuous input values and manipulates the geometric information of SVG to encode out-line details and long-distance dependencies. SVGfomer can be used for various downstream tasks: reconstruction, clas-sification, interpolation, retrieval, etc. We have conducted extensive experiments on vector font and icon datasets to show that our model can capture high-quality representation information and outperform the previous state-of-the-art on downstream tasks significantly. 1.

Introduction
In the last few years, there have been tremendous ad-vances in image representation learning [22, 26, 37], and these representations lead to great success in many down-stream tasks such as image reconstruction [40], image clas-sification [14, 16], etc. However, most previous works have focused on analyzing structured bitmap format, which uses a grid at the pixel level to represent textures and colors [18].
Therefore, there is still considerable room for improving the representation of detailed attributes for vector objects [25].
In contrast to bitmap image format, scalable vector graph-ics (SVG) format for vector images is widely used in real-world applications due to its excellent scaling capabili-ties [12, 20, 33]. SVGs usually contain a mixture of smooth curves and sharp features, such as lines, circles, polygons, and splines, represented as parametric curves in digital for-mat. This allows us to treat SVGs as sequential data and learn their compact and scale-invariant representation using neural network models. However, how to automatically learn an effective representation of vector images is still non-trivial as it requires a model to understand the high-level perception of the 2D spatial pattern as well as geometric information to support the high-quality outcome in downstream tasks.
Transformer-based models [30] have been proven to achieve start-of-the-art results when dealing with sequen-tial data in various problems including Natural Language
Processing (NLP) [34] tasks and time-series forecasting [41] problems. We argue that representation learning for SVG is different from these tasks for two reasons: Firstly, most if not all NLP tasks need a fixed token space to embed the discrete tokens, while SVGs are parameterized by continuous values which make the token space infinite in the previous setting;
Secondly, the number of commands and the correlation be-tween the commands vary greatly from one SVG to another, which is hard to handle by a pure data-driven attention mech-anism. For example, the font data may vary across different families while sharing similar styles within the same font family. Such property is encoded in the sequential data ex-plicitly and can provide geometric dependency guidance for modeling the SVG if used appropriately.
To tackle the above challenges and fully utilize the geo-metric information from SVG data, this work introduces a novel model architecture named SVGformer, which can take continuous sequential input into a transformer-based model to handle the complex dependencies over SVG commands and yield a robust representation for the input. Specifically, we first extract the SVG segment information via medial axis transform (MAT) [2] to convey the geometric infor-mation into the learned representation. Then we introduce a 1D convolutional embedding layer to preserve the origi-nal continuous format of SVG data, as opposed to previous vector representation learning models [8] which need a pre-processing step to discretize the input to limited discrete tokens. After that, we inject the structural relationship be-tween commands into the proposed geometric self-attention module in the encoder layer to get the hidden representation
Method
Font-MF
[3]
SVG-VAE
[20]
Im2Vec
[25]
DeepVecFont
[33]
DeepSVG
[8]
Encoding Modality
Decoding Modality
Model Architecture
Sequence Format
Geometric Information
Img
Seq
Img&Seq
Seq
VAE
GP-LVM
Keypoints Commands Keypoints
-Img
Seq
RNN
--Img&Seq
Img&Seq
Seq
Seq
LSTM+CNN Transformer
Commands
Commands
--LayoutTrans
[12]
Seq
Seq
Transformer
Commands
-SVGformer (ours)
Seq
Seq
Transformer
Commands
Segment
Table 1. Comparison of our SVGformer model and recent models for vector representation learning, where "Seq" donates sequence, and
"Img" donates image. of each SVG. The representation learned with the recon-struction pretext task can be used in various downstream tasks including classification, retrieval, and interpolation. To the best of our knowledge, our proposed model is the first to explicitly consider vector geometric information as well as directly deal with the raw input of SVG format in an end-to-end encoder-decoder fashion.
The main contributions of this paper includes:
• SVGformer captures both geometric information and curve semantic information with the geometric self-attention module, which synergizes the strengths of
MAT and the transformer-based neural network to han-dle long-term sequence relationships in SVG.
• SVGformer can take original continuous format as in-put which can effectively reduce the token space in the embedding layer. Thereby the model is general for all continuous vector graphics without extra quantization.
• SVGformer achieves new state-of-the-art performance on four downstream tasks of both Font and Icon dataset.
For example, it outperforms prior art by 51.2% on classification and 42.5% on retrieval tasks of the font dataset. 2.