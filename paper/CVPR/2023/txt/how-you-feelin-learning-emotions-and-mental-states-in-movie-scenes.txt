Abstract
Movie story analysis requires understanding characters’ emotions and mental states. Towards this goal, we for-mulate emotion understanding as predicting a diverse and multi-label set of emotions at the level of a movie scene and for each character. We propose EmoTx, a multimodal
Transformer-based architecture that ingests videos, multi-ple characters, and dialog utterances to make joint pre-dictions. By leveraging annotations from the MovieGraphs dataset [72], we aim to predict classic emotions (e.g. happy, angry) and other mental states (e.g. honest, helpful). We conduct experiments on the most frequently occurring 10 and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies and comparison against adapted state-of-the-art emotion recognition approaches shows the effec-tiveness of EmoTx. Analyzing EmoTx’s self-attention scores reveals that expressive emotions often look at character to-kens while other mental states rely on video and dialog cues. 1.

Introduction
In the movie The Pursuit of Happyness, we see the pro-tagonist experience a roller-coaster of emotions from the lows of breakup and homelessness to the highs of getting selected for a coveted job. Such heightened emotions are of-ten useful to draw the audience in through relatable events as one empathizes with the character(s). For machines to understand such a movie (broadly, story), we argue that it is paramount to track how characters’ emotions and men-tal states evolve over time. Towards this goal, we lever-age annotations from MovieGraphs [72] and train models to watch the video, read the dialog, and predict the emo-tions and mental states of characters in each movie scene.
Emotions are a deeply-studied topic. From ancient Rome and Cicero’s 4-way classification [60], to modern brain re-search [33], emotions have fascinated humanity. Psychol-ogists use of Plutchik’s wheel [53] or the proposal of uni-versality in facial expressions by Ekman [18], structure has been provided to this field through various theories. Affec-tive emotions are also grouped into mental (affective, be-Figure 1. Multimodal models and multi-label emotions are neces-sary for understanding the story. A: What character emotions can we sense in this scene? Is a single label enough? B: Without the dialog, can we try to guess the emotions of the Sergeant and the
Soldier. C: Is it possible to infer the emotions from the characters’ facial expressions (without subtitles and visual background) only?
Check the footnote below for the ground-truth emotion labels for these scenes and the supplement for an explanation of the story. havioral, and cognitive) or bodily states [13].
A recent work on recognizing emotions with visual con-text, Emotic [31] identifies 26 label clusters and proposes a multi-label setup wherein an image may exhibit multi-ple emotions (e.g. peace, engagement). An alternative to the categorical space, valence, arousal, and dominance are also used as three continuous dimensions [31]. Predicting a rich set of emotions requires analyzing multiple contextual modalities [31, 34, 44]. Popular directions in multimodal emotion recognition are Emotion Recognition in Conver-sations (ERC) that classifies the emotion for every dialog utterance [42,54,83]; or predicting a single valence-activity score for short ∼10s movie clips [4, 45].
We operate at the level of a movie scene: a set of shots telling a sub-story, typically at one location, among a de-fined cast, and in a short time span of 30 to 60 s. Thus, scenes are considerably longer than single dialogs [54] or
Ground-truth emotions and mental states portrayed in movie scenes in Fig. 1: A: excited, curious, confused, annoyed, alarmed; B: shocked, confident; C: happy, excited, amused, shocked, confident, nervous.
movie clips in [4]. We predict emotions and mental states for all characters in the scene and also by accumulating la-bels at the scene level. Estimation on a larger time window naturally lends itself to multi-label classification as charac-ters may portray multiple emotions simultaneously (e.g. cu-rious and confused) or have transitions due to interactions with other characters (e.g. worried to calm).
We perform experiments with multiple label sets: Top-10 or 25 most frequently occurring emotion labels in
MovieGraphs [72] or a mapping to the 26 labels in the
Emotic space, created by [45]. While emotions can broadly be considered as part of mental states, for this work, we con-sider that expressed emotions are apparent by looking at the character, e.g. surprise, sad, angry; and mental states are la-tent and only evident through interactions or dialog, e.g. po-lite, determined, confident, helpful1. We posit that classifi-cation in a rich label space of emotions requires looking at multimodal context as evident from masking context in
Fig. 1. To this end, we propose EmoTx that jointly models video frames, dialog utterances, and character appearance.
We summarize our contributions as follows: (i) Building on rich annotations from MovieGraphs [72], we formulate scene and per-character emotion and mental state classifi-cation as a multi-label problem. (ii) We propose a multi-modal Transformer-based architecture EmoTx that predicts emotions by ingesting all information relevant to the movie scene. EmoTx is also able to capture label co-occurrence and jointly predicts all labels. (iii) We adapt several pre-vious works on emotion recognition for this task and show that our approach outperforms them all. (iv) Through analy-sis of the self-attention mechanism, we show that the model learns to look at relevant modalities at the right time. Self-attention scores also shed light on our model’s treatment of expressive emotions vs. mental states. 2.