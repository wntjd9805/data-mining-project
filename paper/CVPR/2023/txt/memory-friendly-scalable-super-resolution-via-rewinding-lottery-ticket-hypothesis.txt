Abstract
Model Scalability
Model Deployment
Scalable deep Super-Resolution (SR) models are in-creasingly in demand, whose memory can be customized and tuned to the computational recourse of the platform.
The existing dynamic scalable SR methods are not memory-friendly enough because multi-scale models have to be saved with a ﬁxed size for each model. Inspired by the suc-cess of Lottery Tickets Hypothesis (LTH) on image classi-ﬁcation, we explore the existence of unstructured scalable
SR deep models, that is, we ﬁnd gradual shrinkage sub-networks of extreme sparsity named winning tickets. In this paper, we propose a Memory-friendly Scalable SR frame-work (MSSR). The advantage is that only a single scalable model covers multiple SR models with different sizes, in-stead of reloading SR models of different sizes. Concretely,
MSSR consists of the forward and backward stages, the for-mer for model compression and the latter for model expan-sion. In the forward stage, we take advantage of LTH with rewinding weights to progressively shrink the SR model and the pruning-out masks that form nested sets. Moreover, stochastic self-distillation (SSD) is conducted to boost the performance of sub-networks. By stochastically selecting multiple depths, the current model inputs the selected fea-tures into the corresponding parts in the larger model and improves the performance of the current model based on the feedback results of the larger model. In the backward stage, the smaller SR model could be expanded by recov-ering and ﬁne-tuning the pruned parameters according to the pruning-out masks obtained in the forward. Extensive experiments show the effectiveness of MMSR. The smallest-scale sub-network could achieve the sparsity of 94% and outperforms the compared lightweight SR methods.
*Equal contribution
†Corresponding authors 20% sparsest sub-model 40% sparse sub-model
L
M
H (cid:42)(cid:75)(cid:86)(cid:82)(cid:85)(cid:95) 80% sparse sub-model 100% whole model   :S_n   :M_n   :Cur_n   :Free_n   :S_w   :M_w   :Cur_w
LL   :<20% 
MM   :40%-80% 
H   :80%-100% 
Figure 1. The ﬂowchart of scalable SR network. S n and S w denote the neurons and the neural connections (weights) of the simplest subnetwork; M n and M w denote the intermediate neu-rons and neural connections; Cur n and Cur w denote the speciﬁc neurons and neural connections belonging to the current subnet-work. Free n denotes the pruning-out neurons. The ﬁnal model (with 100% recovered parameters) reaches the original size. The scalable model is adjustable to the memory resource allocation. 1.

Introduction
Single image super-resolution (SISR) aims to reconstruct a high-resolution (HR) image from the corresponding low-resolution (LR) one. With the rising of deep learning, deep
SR methods have made incredible progress. However, the existing SR models mostly require computational and mem-ory resources, so they do not favor resource-limited devices such as mobile phones, robotics, and some edge devices.
The lightweight SR methods are attracting more at-tention for better application to resource-limited devices.
The existing lightweight SR methods mainly focus on de-signing compact architectures [17, 20] with a ﬁxed size, such as multi-scale pyramid [20], multiple-level receptive
ﬁelds [17, 18], and recursive learning [19]. However, most lightweight SR models with ﬁxed sizes are not ﬂexible in applications. If one model does not match the resources of
the platform, it has to be retrained by compression methods to match the resources and then reloaded onto the devices.
The urgent demand to customize models based on de-ployment resources is increasing. Dynamic neural networks for SR [14, 22] are proposed to adjust the network architec-ture according to different computational resources. The existing dynamic deep SR models often explore dynamic depth or width [22, 26], but they either require large mem-ory resources or are not convenient for users to wait for re-training another SR model. The former leads to saving the multi-scale SR models of different sizes and the latter leads to retraining the model before being reused again. The lim-itation lies in that they are not memory-friendly. In many edge-device SR applications, the devices may be scalable, that is, their memories may be small in the beginning and be expanded later. Thus, we discuss two issues in this pa-per: 1) how to make a scalable lightweight model for the multi-scale computational recourse. 2) how to make the lightweight model expand to a larger-size model for better performance if the computational recourse is increased.
As for the ﬁrst issue, inspired by the success of Lottery
Ticket Hypothesis [10] which points out that there could ex-ist a properly pruned sub-network named winning tickets to achieve comparable performance against the original dense network in model compression of classiﬁcation, it is used to ﬁnd the sub-network for SR. We are the ﬁrst to study the existence of scalable winning tickets for SR. Iterative prun-ing and rewinding weights in LTH are beneﬁcial to the scal-able lightweight SR model. Iterative pruning may compress the SR model according to an arbitrary size. It is observed in [24] that the winning tickets are related to an insufﬁcient
DNN, and rewinding LTH outperforms the original LTH.
That is, the initial weights in LTH are replaced with the T-iteration weights during pruning and ﬁne-tuning. The scal-able deep SR model is shown in Fig. 1.
As for the second issue, the scalable SR model can cus-tomize parameters to adapt to different memory resources rather than load or ofﬂoad different models for different de-vices. In other words, during real applications, there will be only one simple model to be employed for inference whose size is decided by the computational resource.
In this paper, we propose a memory-friendly scalable deep SR model (MSSR) via rewinding LTH. We use the rewinding LTH [10] to generate our unstructured scalable mask. MSSR is backtracking and contains forward and backward stages. The former focuses on model compres-sion by rewinding LTH with iterative pruning and ﬁne-tuning, and the latter focuses on iterative model expansion until it goes back to its original size. Multi-scale winning tickets together with the pruning-out masks are obtained by rewinding LTH in the forward stage with the decrease in the number of parameters. The pruning-out masks are nested.
In order to make the compressed SR model not degrade sig-niﬁcantly, stochastic self-distillation (SSD) is used to im-prove the representation of the small-scale SR model, and knowledge is transferred from the last-scale model to the current scale model.
In the backward stage, the smallest model is expanded gradually to the model with the original size with the expanded mask.
The main contributions of this work are three-fold:
• A memory-friendly scalable dynamic SR lightweight model via rewinding LTH is proposed. MSSR is re-conﬁgurable and switchable to sub-networks with different sizes according to on-device resource con-straints on the ﬂy.
• MSSR is backtracking, which contains forward and backward stages. Multi-scale winning tickets form nested masks for the multi-scale models. SSD is con-ducted by replacing the features in randomly selected layers between Teacher and Student to improve the performance of the scalable SR lightweight models.
• Extensive experiments demonstrate that MSSR can generalize to different SR models as well as state-of-the-art attention-based models, ENLCN [1]. 2.