Abstract
Many defenses against adversarial attacks (e.g. robust classifiers, randomization, or image purification) use coun-termeasures put to work only after the attack has been crafted. We adopt a different perspective to introduce A5 (Adversarial Augmentation Against Adversarial Attacks), a novel framework including the first certified preemptive de-fense against adversarial attacks. The main idea is to craft a defensive perturbation to guarantee that any attack (up to a given magnitude) towards the input in hand will fail. To this aim, we leverage existing automatic perturbation analysis tools for neural networks. We study the conditions to apply
A5 effectively, analyze the importance of the robustness of the to-be-defended classifier, and inspect the appearance of the robustified images. We show effective on-the-fly defen-sive augmentation with a robustifier network that ignores the ground truth label, and demonstrate the benefits of ro-In our tests, A5 con-bustifier and classifier co-training. sistently beats state of the art certified defenses on MNIST,
CIFAR10, FashionMNIST and Tinyimagenet. We also show how to apply A5 to create certifiably robust physical ob-jects. Our code at https://github.com/NVlabs/
A5 allows experimenting on a wide range of scenarios be-yond the man-in-the-middle attack tested here, including the case of physical attacks. 1.

Introduction
Since Deep Neural Networks (DNNs) have been found vulnerable to adversarial attacks [11, 30], researchers stud-ied various protection strategies [4, 12, 20, 42, 43]. For in-stance, adversarial training [11,30] generates attacks while asking a DNN for the correct output in training; it is simple, partially effective and widely adopted. Certified methods (e.g., IBP [12], CROWN [43], CROWN-IBP [42]) do a step more by estimating correct (although often pessimistic) out-put bounds (Fig. 1, a) used for training. Adversarial train-ing regularizes the classification landscape against the at-tacks (Fig. 1, b), but high protection often produces a loss in clean accuracy. Other partially effective defenses are based on randomness [35, 35] or removal of the adversarial sig-nal [13,18,27,40], by moving the input back to the space of natural (non-attacked) data before classification (Fig. 1, c).
All the aforementioned strategies activate the defense mechanism only after the attack has been crafted. How-ever, when dealing with adversarial attacks, the first actor to move has a significant advantage. For instance, perturb-ing an image can avoid online person identification and pre-serve privacy [6, 19, 26]. Acting first is particularly suitable against Man in the Middle (MitM) attacks that may practi-(a) certified bounds (b) adversarial training (c) image purification (d) A5
Figure 1. (a) An input x ∈ y∗ is correctly classified. The grey box (certified bounds [12, 42, 43]) shows that, under an attack δxA,
||δxA||∞ < ϵA, misclassification ((x + δxA) ∈ y0) is possible. (b) Adversarial training [4, 20] creates robust DNNs with regular classification landscapes: misclassification is less likely. (c) Im-age purification [27, 40] moves the attacked input x + δxA by (d) A5 preemptively moves
δxP back to correct classification. x into a non attackable position. The original CIFAR10 image classified as airplane (69.8% confidence) can be misclassified un-der attack (ship, 100% confidence). Once robustified through A5 (right), misclassification does not occur anymore.
v A vector
C Classifier x Data
δxD Defensive perturbation x + δxD Robustified data
ϵD Defense magnitude, ||δxD||p < ϵD
ϵA Attack magnitude ||δxA||p < ϵA while testing vj Vector j-th element
R Robustifier
δxA Attacking perturbation x + δxA Data under attack x + δxD + δxA Robustified data under attack
ϵR
A Attack magnitude, ||δxA||p < ϵR
A Attack magnitude, ||δxA||p < ϵC
ϵC
A while training R
A while training C
Table 1. Notation for data x, that are the inputs of the classifier C. We adopt an equivalent notation for physical objects w. cally arise in automotive [32], audio processing [13,22,38], or while communicating with a remote machine [39]. Our idea spouses this reasoning line: we investigate a novel way to augment the data to preemptively certify that it cannot be attacked (Fig. 1, d). This fits in a recent research area that up to now has received less attention than adversar-ial training. Researchers explored image and real object augmentation to guarantee a high recognition rate in pres-ence of noise or geometric distortions, but not in adversar-ial scenarios [24]. Encryption schemes coupled with deep learning [1, 28] or watermarking [16] only partially protect against MitM attacks. The few existing preemptive robusti-fication methods [23] include a procedure [17] that first runs a classifier on clean data and achieves preemptive robustifi-cation against MitM attacks through iterative optimization; these are partially effective and do not provide any certifica-tion. Our novel framework encompasses most of the afore-mentioned cases while also introducing for the first time the concept of certified robustification. More in detail, our man-ifold contributions are: (i) We introduce A5 (Adversarial Augmentation Against
Adversarial Attacks), a comprehensive framework for preemptive augmentation to make data and physical objects certifiably robust against MitM adversarial at-tacks. As far as we know, this is the first time certified defense is achieved in this way. Since we provide cer-tified robustness, we guarantee protection against any form of white, grey or black box attack. (ii) We test different flavours of A5 on standard datasets.
By doing so, we study the connection between the ro-bustness of the legacy classifier, the magnitude of the defensive augmentation, and the protection level de-livered by A5. We show A5 achieving state-of-the-art certified protection against MitM attacks by training a robustifier DNN coupled with the legacy classifier, and even better results for co-training of the robustifier and classifier. We perform a critical, visual inspection of the robustified images to answer an interesting theo-retical question (how does a non-attackable image look like?) and potentially provide directions for the acqui-sition of inherently robust images. (iii) Using Optical Character Recognition (OCR) as an ex-ample, we show the application of A5 for the design of certifiably robust physical objects, which extends [24] to the case of certified defense against MitM attacks. (iv) We share our code at https://github.com/
NVlabs/A5, to allow replicating our results or test-ing A5 in scenarios not considered here, for instance on other datasets or for protection against physical ad-versarial attack (e.g., adversarial patches). 2.