Abstract
As the core building block of vision transformers, atten-tion is a powerful tool to capture long-range dependency.
However, such power comes at a cost: it incurs a huge computation burden and heavy memory footprint as pair-wise token interaction across all spatial locations is com-puted. A series of works attempt to alleviate this problem by introducing handcrafted and content-agnostic sparsity into attention, such as restricting the attention operation to be inside local windows, axial stripes, or dilated windows.
In contrast to these approaches, we propose a novel dy-namic sparse attention via bi-level routing to enable a more
ﬂexible allocation of computations with content awareness.
Speciﬁcally, for a query, irrelevant key-value pairs are ﬁrst
ﬁltered out at a coarse region level, and then ﬁne-grained token-to-token attention is applied in the union of remain-ing candidate regions (i.e., routed regions). We provide a simple yet effective implementation of the proposed bi-level routing attention, which utilizes the sparsity to save both computation and memory while involving only GPU-friendly dense matrix multiplications. Built with the pro-posed bi-level routing attention, a new general vision trans-former, named BiFormer, is then presented. As BiFormer attends to a small subset of relevant tokens in a query adap-tive manner without distraction from other irrelevant ones, it enjoys both good performance and high computational efﬁciency, especially in dense prediction tasks. Empirical results across several computer vision tasks such as image classiﬁcation, object detection, and semantic segmentation verify the effectiveness of our design. Code is available at https://github.com/rayleizhu/BiFormer. 1.

Introduction
Transformer has many properties that are suitable for building powerful data-driven models. First, it is able to capture long-range dependency in the data [27,40]. Second,
† Corresponding author. it is almost inductive-bias-free and thus makes the model more ﬂexible to ﬁt tons of data [14]. Last but not least, it enjoys high parallelism, which beneﬁts training and infer-ence of large models [12, 31, 34, 40]. Hence, transformer has not only revolutionized natural language processing but also shown very promising progress in computer vision.
The computer vision community has witnessed an explo-sion of vision transformers in the past two years [1, 13, 14, 27, 42, 44]. Among these works, a popular topic is to im-prove the core building block, i.e., attention. In contrast to convolution, which is intrinsically a local operator, a cru-cial property of attention is the global receptive ﬁeld, which empowers vision transformers to capture long-range depen-dency [40]. However, such a property comes at a cost: as attention computes pairwise token afﬁnity across all spatial locations, it has a high computational complexity and incurs heavy memory footprints.
To alleviate the problem, a promising direction is to in-troduce sparse attention [5] to vision transformers, so that each query attends to a small portion of key-value pairs instead of all. In this fashion, several handcrafted sparse patterns have been explored, such as restricting attention in local windows [27], dilated windows [39, 44], or axial stripes [44]. On the other hand, there are also works try-ing to make the sparsity adaptive to data [4, 45]. However, while they use different strategies to merge or select key/-value tokens, these tokens are query-agnostic, i.e., they are shared by all queries. Nonetheless, according to the visual-ization of pretrained ViT 1 [14] and DETR 2 [1], queries in different semantic regions actually attend to quite different key-value pairs. Hence, forcing all queries to attend to the same set of tokens may be suboptimal.
In this paper, we seek an attention mechanism with dy-namic, query-aware sparsity. Basically, we aim for each query to attend to a small portion of the most semantically relevant key-value pairs. The ﬁrst problem comes as how 1https://epfml.github.io/attention-cnn/ 2https : / / colab . research . google . com / github / facebookresearch/detr/blob/colab/notebooks/detr_ attention.ipynb
Figure 1. Vanilla attention and its sparse variants. (a) Vanilla attention operates gloabally and incurs high computational complexity and heavy memory footprint. (b)-(d) Several works attempt to alleviate the complexity by introducing sparse attention with different handcrafted patterns, such as local window [27, 44], axial stripe [13], dilated window [39, 44]. (e) Deformable attention [45] enables image-adaptive sparsity via deforming a regular grid. (f) We achieve dynamic, query-aware sparsity with bi-level routing attention, which
ﬁrst searches top-k (k = 3 in this case) relevant regions, and then attends to the union of them. to locate these key-value pairs to attend. For example, if we select key-value pairs in a per-query manner as done in [16], it still requires evaluation of pairwise afﬁnity between all queries and keys, and hence has the same complexity of vanilla attention. Another possibility is to predict attention offsets based on local context for each query [9, 45], and hence pairwise afﬁnity computation is avoided. However, in this way, it is problematic to model long-range depen-dency [45].
To locate valuable key-value pairs to attend globally with high efﬁciency, we propose a region-to-region routing ap-proach. Our core idea is to ﬁlter out the most irrelevant key-value pairs at a coarse-grained region level, instead of directly at the ﬁne-grained token level. This is done by ﬁrst constructing a region-level afﬁnity graph and then pruning it to keep only top-k connections for each node. Hence, each region only needs to attend to the top-k routed regions. With the attending regions determined, the next step is to apply token-to-token attention, which is non-trivial as key-value pairs are now assumed to be spatially scattered. For this case, while the sparse matrix multiplication is applicable, it is inefﬁcient in modern GPUs, which rely on coalesced memory operations, i.e., accessing blocks of dozens of con-tiguous bytes at once [29]. Instead, we propose a simple so-lution via gathering key/value tokens, where only hardware-friendly dense matrix multiplications are involved. We refer to this approach as Bi-level Routing Attention (BRA), as it contains a region-level routing step and a token-level atten-tion step.
By using BRA as the core building block, we propose
BiFormer, a general vision transformer backbone that can be used for many applications such as classiﬁcation, object detection, and semantic segmentation. As BRA enables Bi-Former to attend to a small subset of the most relevant key/-value tokens for each query in a content-aware manner, our model achieves a better computation-performance trade-off.
For example, with 4.6G FLOPs computation, BiFormer-T achieves 83.8% top-1 accuracy on ImageNet-1K classiﬁ-cation, which is the best as far as we know under similar computation budgets without training with external data or distillation [22,38]. The improvements are also consistently shown in downstream tasks such as instance segmentation and semantic segmentation.
To summarize, our contributions are as follows. We in-troduce a novel bi-level routing mechanism to vanilla at-tention, which enables content-aware sparse patterns in a query-adaptive manner. Using the bi-level routing atten-tion as the basic building block, we propose a general vi-sion transformer named BiFormer. Experimental results on various computer vision tasks including image classiﬁca-tion, object detection, and semantic segmentation show that the proposed BiFormer achieves signiﬁcantly better perfor-mances over the baselines under similar model sizes. 2.