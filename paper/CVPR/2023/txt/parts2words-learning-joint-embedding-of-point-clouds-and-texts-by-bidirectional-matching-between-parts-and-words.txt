Abstract
Shape-Text matching is an important task of high-level shape understanding. Current methods mainly represent a 3D shape as multiple 2D rendered views, which obviously can not be understood well due to the structural ambigu-ity caused by self-occlusion in the limited number of views.
To resolve this issue, we directly represent 3D shapes as point clouds, and propose to learn joint embedding of point clouds and texts by bidirectional matching between parts from shapes and words from texts. Specifically, we first seg-ment the point clouds into parts, and then leverage optimal transport method to match parts and words in an optimized feature space, where each part is represented by aggregat-ing features of all points within it and each word is ab-stracted by its contextual information. We optimize the fea-ture space in order to enlarge the similarities between the paired training samples, while simultaneously maximizing the margin between the unpaired ones. Experiments demon-strate that our method achieves a significant improvement in accuracy over the SOTAs on multi-modal retrieval tasks under the Text2Shape dataset. Codes are available at here. 1.

Introduction
Interaction scenarios, such as metaverse, and computer-aided design (CAD), create a larger number of 3D shapes and text descriptions. To enable a more intelligent process of interaction, it is important to bridge the gap between 3D data and linguistic data. Recently, 3D shapes with rich geo-metric details have been available in large-scale 3D deep learning benchmark datasets [5, 34]. Beyond 3D shapes themselves, text descriptions can also provide additional in-formation. However, it is still hard to jointly understand 3D shapes and texts, since representing different modalities in a common semantic space is still very challenging.
The existing methods aim at learning a joint embedded
* Corresponding authors. space to connect various 3D representations with texts, such as voxel grids [6] and multi-view rendered images [11, 12].
However, due to the low resolution and self-occlusions, it is hard for those methods mentioned above to improve the ability of joint understanding of shapes and texts. On the other hand, previous shape-text matching methods [6, 12, 37] usually take the global features of the entire 3D shape for text matching, making it challenging to capture the local geometries, and thus are not suitable for matching detailed geometric descriptions.
Regional-based matching approaches are commonly employed in the image-text matching task [21–23, 27], whereby visual-text alignment is established at the seman-tic level to enhance the performance of retrieval. These models compute the local similarities between regions and words and then aggregate the local information to obtain the global metrics between the heterogeneous pairs. How-ever, these two-stage methods based on the pre-trained seg-mentation networks split the connection between matching embeddings and segmentation prior information.
In this paper, we introduce an optimal transport based shape-text matching method to achieve fine-grained align-ment and retrieval of 3D shapes and texts, as shown in Fig-ure 1. To mitigate the influence of low-resolution or self-occlusions, we directly represent the shape as point clouds and learn a part-level segmentation prior. Afterward, we leverage optimal transport to build the regional cross-modal correspondences and achieve more precise retrieval results.
Our main contributions are summarized as follows:
• We propose a novel end-to-end network framework to learn the joint embedding of point clouds and texts, which enables the bidirectional matching be-tween parts from point clouds and words from texts.
• We leverage optimal transport theory to obtain the best matches between parts and words and incorporate
Earth Mover’s Distance (EMD) to describe the match-ing score.
• To the best of our knowledge, our proposed network
Figure 1. Comparison between the global-based matching method and our proposed method. The proposed end-to-end framework aims to learn the joint embedding of point clouds and text by matching parts to words. It can either retrieve shapes using text or vice versa. Our novelty lies in the way of jointly learning embeddings of point clouds and texts. achieves SOTA results in joint 3D shape/text under-standing tasks in terms of various evaluation metrics. 2.