Abstract
Rendering novel view images is highly desirable for many applications. Despite recent progress, it remains challenging to render high-fidelity and view-consistent novel views of large-scale scenes from in-the-wild images with inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid neural rendering model that makes image-based representation and neural 3D representation join forces to render high-quality, view-consistent images.
Besides, images captured in the wild inevitably contain ar-tifacts, such as motion blur, which deteriorates the qual-ity of rendered images. Accordingly, we propose strategies to simulate blur effects on the rendered images to mitigate the negative influence of blurriness images and reduce their importance during training based on precomputed quality-aware weights. Extensive experiments on real and syn-thetic data demonstrate our model surpasses state-of-the-art point-based methods for novel view synthesis. The code is available at https://daipengwa.github.io/
Hybrid-Rendering-ProjectPage/. 1.

Introduction
Novel-view synthesis of a scene is one critical feature re-quired by various applications, e.g., AR/VR, robotics, and video games, to name a few. Neural radiance field (NeRF)
[23] and its follow-up works [3, 19, 24, 39, 43, 47] enable high-quality view synthesis on objects or synthetic data.
However, synthesizing high-fidelity and view-consistent novel view images of real-world large-scale scenes remains challenging, especially in the presence of inevitable arti-facts from the data-capturing process, such as motion blur (see Figure 1 & supplementary material).
To improve novel view synthesis, mainstream research can be mainly categorized into two lines. One line of meth-ods directly resorts to features from training data to synthe-size novel view images [4, 11, 29, 40], namely image-based rendering. By directly leveraging rich high-quality fea-tures from neighboring high-resolution images, these meth-*Equal contribution
†Corresponding author
Figure 1. Our hybrid neural rendering model generates high-fidelity novel view images. Please note characters in the book where the result of Point-Nerf is blurry and the GT is contami-nated by blur artifacts. ods have a better chance of generating high-fidelity images with distinctive details. Nevertheless, the generated im-ages often lack consistency due to the absence of global structural regularization, and boundary image pixels often contain serious artifacts. Another line of work attempts to equip NeRF with explicit 3D representations in the form of point cloud [28,43], surface mesh [30,44] or voxel grid fea-tures [9,19,46], namely neural 3D representation. Thanks to the global geometric regularization from explicit 3D repre-sentations, they can efficiently synthesize consistent novel view images but yet struggle with producing high-fidelity images in large-scale scenes (see the blurry images from
Point-NeRF [43] in Fig. 1). This may be caused by low-resolution 3D representations [19], noisy geometries [1, 7], imperfect camera calibrations [2], or inaccurate rendering formulas [3], which make encoding a large-scale scene into a global neural 3D representation non-trivial and inevitably loses high-frequency information.
Albeit advancing the field, the above work all suffer im-mediately from low-quality training data, e.g., blurry im-ages. Recently, Deblur-NeRF [21] aims to address the prob-lem of blurry training data and proposed a pipeline to simu-late blurs by querying multiple auxiliary rays, which, how-ever, is computation and memory inefficient, hindering their applicability in large-scale scenes.
In this paper, we aim at synthesizing high-fidelity and view-consistent novel view images in large-scale scenes us-ing in-the-wild unsatisfactory data, e.g., blurry data. First, to simultaneously address high fidelity and view consis-tency, we put forward a hybrid neural rendering approach that enjoys the merits of both image-based representation and neural 3D representation. Our fundamental design
centers around a 3D-guided neural feature fusion module, which employs view-consistent neural 3D features to in-tegrate high-fidelity 2D image features, resulting in a hy-brid feature representation that preserves view consistency whilst simultaneously upholding quality. Besides, to avoid the optimization of the hybrid representation being biased toward one modality, we develop a random feature drop strategy to ensure that features from different modalities can all be well optimized.
Second, to effectively train the hybrid model with un-satisfactory in-the-wild data, we design a blur simulation and detection approach to alleviate the negative impact of low-quality data on model training. Specifically, the blur simulation module injects blur into the rendered image to mimic the real-world blurry effects. In this way, the blurred image can be directly compared with the blurry reference image while providing blur-free supervisory signals to train the hybrid model. Besides, to further alleviate the influence of blurry images, we design a content-aware blur detection approach to robustly assess the blurriness scores of images.
The calculated scores are further used to adjust the impor-tance of samples during training. In our study, we primarily focus on the blur artifact due to its prevalence in real-world data (e.g., ScanNet); however, our “simulate-and-detect” approach can also be applied to address other artifacts.
While our model is built upon the state-of-the-art 3D-and image-based neural rendering models, our contribu-tion falls mainly on studying their combinatorial benefits and bridging the gap between NeRF and unsatisfactory data captured in the wild. Our major contributions can be sum-marized as follows.
• We study a hybrid neural rendering model for synthe-sizing high-fidelity and consistent novel-view images.
• We design blur simulation and detection strategies that facilitate offering blur-free training signals for opti-mizing the hybrid rendering model.
• Extensive experiments on real (i.e., ScanNet [5]) and synthetic data (i.e., Habitat-sim [22,35]) showcase that our method outperforms state-of-the-art point-based methods designed for novel view synthesis. 2.