Abstract
Semantic segmentation usually suffers from a long-tail data distribution. Due to the imbalanced number of samples across categories, the features of those tail classes may get squeezed into a narrow area in the feature space. Towards a balanced feature distribution, we introduce category-wise variation into the network predictions in the training phase such that an instance is no longer projected to a feature point, but a small region instead. Such a perturbation is highly dependent on the category scale, which appears as assigning smaller variation to head classes and larger variation to tail classes. In this way, we manage to close the gap between the feature areas of different categories,
It is note-resulting in a more balanced representation. worthy that the introduced variation is discarded at the inference stage to facilitate a confident prediction. Although with an embarrassingly simple implementation, our method manifests itself in strong generalizability to various datasets and task settings. Extensive experiments suggest that our plug-in design lends itself well to a range of state-of-the-art approaches and boosts the performance on top of them.1 1.

Introduction
The success of deep models in semantic segmenta-tion [12, 58, 71, 109] benefits from large-scale datasets.
However, popular datasets for segmentation, such as PAS-CAL VOC [24] and Cityscapes [18], usually follow a long-tail distribution, where some categories may have far fewer samples than others. Considering the particularity of this task, which targets assigning labels to pixels instead of images, it is quite difficult to balance the distribution 1Code: https://github.com/grantword8/BLV.
†This work was done during the internship at SenseTime Research.
‡Rui Zhao is also with Qing Yuan Research Institute, Shanghai Jiao
Tong University.
Figure 1. Illustration of logit variation from the feature space, where each point corresponds to an instance and different colors stand for different categories. (a) Without logit variation, the features of tail classes (e.g., the blue one) may get squeezed into a narrow area. (b) After introducing logit variation, which is controlled by the category scale (i.e., number of training samples belonging to a particular category), we expand each feature point to a feature region with random perturbation, resulting in a more category-balanced feature distribution. from the aspect of data collection. Taking the scenario of autonomous driving as an example, a bike is typically tied to a smaller image region (i.e., fewer pixels) than a car, and trains appear more rarely than pedestrians in a city. Therefore, learning a decent model from long tail data distributions becomes critical.
A common practice to address such a challenge is to make better use of the limited samples from tail classes. For this purpose, previous attempts either balance the sample quantity (e.g., oversample the tail classes when organizing the training batch) [8, 28, 36, 46, 75], or balance the per-sample importance (e.g., assign the training penalties re-garding tail classes with higher loss weights) [19,51,67,69].
Given existing advanced techniques, however, performance degradation can still be observed in those tail categories.
This works provides a new perspective on improving long-tailed semantic segmentation. Recall that, in modern pipelines based on neural networks [12,14,58,92,108,110], instances are projected to representative features before
categorized to a certain class. We argue that the features of tail classes may get squeezed into a narrow area in the feature space, as the blue region shown in Fig. 1a, because of miserly samples. To balance the feature distribution, we propose a simple yet effective approach via introducing bal-ancing logit variation (BLV) into the network predictions.
Concretely, we perturb each predicted logit with a randomly sampled noise during training. That way, each instance can be seen as projected to a feature region, as shown in Fig. 1b, whose radius is dependent on the noise variance. We then propose to balance the variation by applying smaller variance to head classes and larger variance to tail classes so as to close the feature area gap between different categories.
This newly introduced variation can be viewed as a special augmentation and discarded in the inference phase to ensure a reliable prediction.
We evaluate our approach on three different settings of semantic segmentation, including fully supervised [21, 90, 101, 109], semi-supervised [13, 87, 111, 115], and unsu-pervised domain adaptation [23, 52, 107, 113], where we improve the baselines consistently. We further show that our method works well with various state-of-the-art frame-works [2, 41, 42, 87, 92, 108] and boosts their performance, demonstrating its strong generalizability. 2.