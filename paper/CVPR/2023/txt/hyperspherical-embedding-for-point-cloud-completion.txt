Abstract
Most real-world 3D measurements from depth sensors are incomplete, and to address this issue the point cloud completion task aims to predict the complete shapes of objects from partial observations. Previous works often adapt an encoder-decoder architecture, where the encoder is trained to extract embeddings that are used as inputs to generate predictions from the decoder. However, the learned embeddings have sparse distribution in the feature space, which leads to worse generalization results during testing. To address these problems, this paper proposes a hyperspherical module, which transforms and normal-izes embeddings from the encoder to be on a unit hyper-sphere. With the proposed module, the magnitude and direc-tion of the output hyperspherical embedding are decoupled and only the directional information is optimized. We the-oretically analyze the hyperspherical embedding and show that it enables more stable training with a wider range of learning rates and more compact embedding distributions.
Experiment results show consistent improvement of point cloud completion in both single-task and multi-task learn-ing, which demonstrates the effectiveness of the proposed method. 1.

Introduction
The continual improvement of 3D sensors has made point clouds much more accessible, which drives the de-velopment of algorithms to analyze them. Thanks to deep learning techniques, state of the art algorithms for point cloud analysis have achieved incredible performance [9,20– 22,25] by effectively learning representations from large 3D datasets [3, 6, 26] and have many applications in robotics, autonomous driving, and 3D modeling. However, point clouds in the real-world are often incomplete and sparse due to many reasons, such as occlusions, low resolution, and the limited view of 3D sensors. So it is critical to have an algorithm that is capable of predicting complete shapes of objects from partial observations.
Given the importance of point cloud completion, it is
Figure 1. An illustration of the architecture proposed in this paper. The up-per subfigure shows the general point cloud analysis structure, where the embedding is directly output from the encoder without constraints. The lower subfigure shows the structure of the model with the proposed hy-perspherical module. The figures under the embeddings illustrate the co-sine similarity distribution between embeddings, which indicates a more compact embedding distribution achieved by the proposed method and im-proves point cloud completion. unsurprising that various methods have been proposed to address this challenge [18, 27, 31, 34, 37, 41]. Most exist-ing methods adapt encoder-decoder structures, in which the encoder takes a partial point cloud as input and outputs an embedding vector, and then it is taken by the decoder which predicts a complete point cloud. The embedding space is designed to be high-dimensional as it must have large enough capacity to contain all information needed for downstream tasks. However, the learned high-dimensional embeddings, as shown in this paper, tend to have a sparse distribution in the embedding space, which increases the possibility that unseen features at testing are not captured by the representation learned at training and leads to worse generalizability of models.
Usually, one real-world application requires predictions from multiple different tasks. For example, to grasp an object in space the robot arm would need the informa-tion about the shape, category, and orientation of the tar-get object. In contrast to training all tasks individually from scratch, a more numerically efficient approach would be to train all relevant tasks jointly by sharing parts of networks between different tasks [11,13,28]. However, existing point cloud completion methods lack the analysis of accomplish-ing point cloud completion jointly with other tasks. We show that training existing point cloud completion methods with other semantic tasks together leads to worse perfor-mance when compared to learning each individually.
To address the above limitations, this paper proposes a hyperspherical module which outputs hyperspherical em-beddings for point cloud completion. The proposed hy-perspherical module can be integrated into existing ap-proaches with encoder-decoder structures as shown in Fig-ure 1. Specifically, the hyperspherical module transforms and constrains the output embedding onto the surface of a hypersphere by normalizing the embedding’s magnitude to unit, so only the directional information is kept for later use. We theoretically investigate the effects of hyperspher-ical embeddings and show that it improves the point cloud completion models by more stable training with large learn-ing rate and more generalizability by learning more com-pact embedding distributions. We also demonstrate the proposed hyperspherical embedding in multi-task learning, where it helps reconcile the learning conflicts between point cloud completion and other semantic tasks at training. The reported improvements of the existing state-of-the-art ap-proaches on several public datasets illustrate the effective-ness of the proposed method. The main contributions of this paper are summarized as follows:
• We propose a hyperspherical module that outputs hy-perspherical embeddings, which improves the perfor-mance of point cloud completion.
• We theoretically investigate the effects of hyperspher-ical embeddings and demonstrate that the point cloud completion benefits from them by stable training and learning a compact embedding distribution.
• We analyze training point cloud completion with other tasks and observe conflicts between them, which can be reconciled by the hyperspherical embedding. 2.