Abstract
We learn a visual representation that captures informa-tion about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this meta-data by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions
“zero shot” by clustering the visual embeddings for all of the patches within an image. 1.

Introduction
A major goal of the computer vision community has been to use cross-modal associations to learn concepts that would be hard to glean from images alone [2]. A particular focus has been on learning high level semantics, such as objects, from other rich sensory signals, like language and sound [58, 62]. By design, the representations learned by these approaches typically discard imaging properties, such as the type of camera that shot the photo, its lens, and the exposure settings, which are not useful for their cross-modal prediction tasks [17].
We argue that obtaining a complete understanding of an image requires both capabilities — for our models to per-ceive not only the semantic content of a scene, but also the properties of the camera that captured it. This type of low level understanding has proven crucial for a variety of tasks, from image forensics [33, 52, 80] to 3D reconstruc-tion [34, 35], yet it has not typically been a focus of rep-resentation learning. It is also widely used in image gen-eration, such as when users of text-to-image tools specify camera properties with phrases like “DSLR photo” [59,63].
We propose to learn low level imaging properties from the abundantly available (but often neglected) camera meta-data that is added to the image file at the moment of cap-ture. This metadata is typically represented as dozens of
Exchangeable Image File Format (EXIF) tags that describe the camera, its settings, and postprocessing operations that
were applied to the image: e.g., Model: “iPhone 4s” or Focal Length: “35.0 mm”. We train a joint embed-ding through contrastive learning that puts image patches into correspondence with camera metadata (Fig. 1a). Our model processes the metadata with a transformer [75] after converting it to a language-like representation. To do this conversion, we take advantage of the fact that EXIF tags are typically stored in a human-readable (and text-based) format. We convert each tag to text, and then concate-nate them together. Our model thus closely resembles con-trastive vision-and-language models, such as CLIP [62], but with EXIF-derived text in place of natural language.
We show that our model can successfully estimate cam-era properties solely from images, and that it provides a useful representation for a variety of image forensics and camera calibration tasks. Our approaches to these tasks do not require camera metadata at test time. Instead, camera properties are estimated implicitly from image content via multimodal embeddings.
We evaluate the learned feature of our model on two clas-sification tasks that benefit from a low-level understanding of images: estimating an image’s radial distortion param-eter, and distinguishing real and manipulated images. We find that our features significantly outperform alternative supervised and self-supervised feature sets.
We also show that our embeddings can be used to de-tect image splicing “zero shot” (i.e., without labeled data), drawing on recent work [8, 33, 54] that detects inconsisten-cies in camera fingerprints hidden within image patches.
Spliced images contain content from multiple real images, each potentially captured with a different camera and imag-ing pipeline. Thus, the embeddings that our model assigns to their patches, which convey camera properties, will have less consistency than those of real images. We detect ma-nipulations by flagging images whose patch embeddings do not fit into a single, compact cluster. We also localize spliced regions by clustering the embeddings within an im-age (Fig. 1b).
We show through our experiments that:
• Camera metadata supervision provides self-for supervised representation learning.
• Image patches can be successfully associated with camera metadata via joint embeddings.
• Image-metadata embeddings are a useful representation for forensics and camera understanding tasks.
• Image manipulations can be identified “zero shot” by identifying inconsistencies in patch embeddings. 2.