Abstract
In this paper, we propose YOSO, a real-time panoptic segmentation framework. YOSO predicts masks via dy-namic convolutions between panoptic kernels and image feature maps, in which you only need to segment once for both instance and semantic segmentation tasks. To reduce the computational overhead, we design a feature pyramid aggregator for the feature map extraction, and a separa-ble dynamic decoder for the panoptic kernel generation.
The aggregator re-parameterizes interpolation-first mod-ules in a convolution-first way, which significantly speeds up the pipeline without any additional costs. The decoder performs multi-head cross-attention via separable dynamic convolution for better efficiency and accuracy. To the best of our knowledge, YOSO is the first real-time panoptic seg-mentation framework that delivers competitive performance compared to state-of-the-art models. Specifically, YOSO achieves 46.4 PQ, 45.6 FPS on COCO; 52.5 PQ, 22.6 FPS on Cityscapes; 38.0 PQ, 35.4 FPS on ADE20K; and 34.1
PQ, 7.1 FPS on Mapillary Vistas. Code is available at https://github.com/hujiecpp/YOSO. 1.

Introduction
Panoptic segmentation is a task that involves assigning a semantic label and an instance identity to each pixel of an input image. The semantic labels are typically classified into two types, i.e., stuff including amorphous and uncount-able concepts (such as sky and road), and things consisting of countable categories (such as persons and cars). This di-vision of label types naturally separates panoptic segmenta-tion into two sub-tasks: semantic segmentation for stuff and instance segmentation for things. Thus, one of the major challenges for achieving real-time panoptic segmentation is the requirement for separate and computationally intensive branches to perform semantic and instance segmentation re-spectively. Typically, instance segmentation employs boxes or points to distinguish between different things, while se-*Corresponding author mantic segmentation predicts distribution maps over seman-tic categories for stuff. As shown in Fig. 1, numerous ef-forts [9, 16, 23, 24, 31, 45] have been made to unify panoptic segmentation pipelines for improved speed and accuracy.
However, achieving real-time panoptic segmentation still remains an open problem. On the one hand, heavy necks, e.g., the multi-scale feature pyramid network (FPN) used in [27, 54], and heads, e.g., the Transformer decoder used in [10, 58], are required to ensure accuracy, making real-time processing unfeasible. On the other hand, reducing the model size [16, 23, 24] leads to a decrease in model gener-alization. Therefore, developing a real-time panoptic seg-mentation framework that delivers competitive accuracy is challenging yet highly desirable.
In this paper, we present YOSO, a real-time panoptic segmentation framework. YOSO predicts panoptic kernels to convolute image feature maps, with which you only need to segment once for the masks of background stuff and foreground things. To make the process lightweight, we design a feature pyramid aggregator for extracting image feature maps, and a separable dynamic decoder for gen-In the aggregator, we propose erating panoptic kernels. convolution-first aggregation (CFA) to re-parameterize the interpolation-first aggregation (IFA), resulting in an approx-imately 2.6× speedup in GPU latency without compromis-ing performance. Specifically, we demonstrate that the or-der, i.e., interpolation-first or convolution-first, of applying bilinear interpolation and 1×1 convolution (w/o bias) does not affect results, but the convolution-first way provides a considerable speedup to the pipeline.
In the decoder, we propose separable dynamic convolution attention (SDCA) to perform multi-head cross-attention in a weight-sharing way. SDCA achieves better accuracy (+1.0 PQ) and higher efficiency (approximately 1.2× faster GPU latency) than traditional multi-head cross-attention.
In general, YOSO has three notable advantages. First,
CFA reduces computational burden without re-training the model or compromising performance. CFA can be adapted to any task that uses the combination of bilinear interpola-tion and 1×1 convolution operations. Second, SDCA per-forms multi-head cross-attention with better accuracy and
Figure 1. Towards real-time panoptic segmentation. (a) Semantic and instance segmentation are performed using shared FPN but separated task branches (e.g., in PanopticFPN [27] and UPSNet [54]). (b) Semantic segmentation generates masks for all categories, and instance recognition is achieved by object detection using boxes or points (e.g., in RealTimePan [24] and PanopticDeepLab [9]). (c)
Kernels for stuff and things are generated to convolute image feature maps via heavy modules (e.g., in PanopticFCN [31], K-Net [58], and
MaskFormer [10,11]). (d) YOSO employs an efficient feature pyramid aggregator and a lightweight separable dynamic decoder to produce image feature maps and panoptic kernels. The figures do not include input images and backbone for concision. efficiency. Third, YOSO runs faster and has competitive ac-curacy compared to state-of-the-art panoptic segmentation models, and its generalization is validated on four popular datasets: COCO (46.4 PQ, 45.6 FPS), Cityscapes (52.5 PQ, 22.6 FPS), ADE20K (38.0 PQ, 35.4 FPS), and Mapillary
Vistas (34.1 PQ, 7.1 FPS). 2.