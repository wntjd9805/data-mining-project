Abstract
Few-shot semantic segmentation (FSS) aims to form class-agnostic models segmenting unseen classes with only a handful of annotations. Previous methods limited to the semantic feature and prototype representation suffer from coarse segmentation granularity and train-set overfitting.
In this work, we design Hierarchically Decoupled Match-ing Network (HDMNet) mining pixel-level support corre-lation based on the transformer architecture. The self-attention modules are used to assist in establishing hierar-chical dense features, as a means to accomplish the cascade matching between query and support features. Moreover, we propose a matching module to reduce train-set over-fitting and introduce correlation distillation leveraging se-mantic correspondence from coarse resolution to boost fine-grained segmentation. Our method performs decently in ex-periments. We achieve 50.0% mIoU on COCO-20i dataset one-shot setting and 56.0% on five-shot segmentation, re-spectively. The code is available on the project website1. 1.

Introduction
Semantic segmentation tasks [2, 3, 22, 52] have made tremendous progress in recent years, benefiting from the rapid development of deep learning [13,32]. However, most existing deep networks are not scalable to previously unseen classes and rely on annotated datasets to achieve satisfy-ing performance. Data collection and annotation cost much time and resources, especially for dense prediction tasks.
Few-shot learning [34, 39, 43] has been introduced into semantic segmentation [5, 38] to build class-agnostic mod-els quickly adapting to novel classes. Typically, few-shot segmentation (FSS) divides the input into the query and support sets [5, 46, 48, 51] following the episode paradigm [41]. It segments the query targets conditioned on
*Corresponding Author 1https://github.com/Pbihao/HDMNet
Figure 1. Activation maps of the correlation values on both
PASCAL-5i [29] and COCO-20i [26]. The baseline is prone to give high activation values to the categories sufficiently witnessed during training, such as the “People” class, even with other sup-port annotations. Then we convert it to the hierarchically decou-pled matching structure and adopt correlation map distillation to mine inner-class correlation. the semantic clues from the support annotations with meta-learning [34, 39] or feature matching [25, 41, 49].
Previous few-shot learning methods may still suffer from coarse segmentation granularity and train-set over-fitting [38] issues. As shown in Fig. 1, “people” is the base class that has been sufficiently witnessed during train-ing. But the model is still prone to yield high activa-tion to “people” instead of more related novel classes with the support samples, producing inferior results. This is-sue stems from framework design, as illustrated in Fig. 2.
Concretely, prototype-based [38, 42] and adaptive-classifier methods [1, 23] aim at distinguishing different categories with global class-wise characteristics. It is challenging to compute the correspondence of different components be-tween query and support objects for the dense prediction tasks. In contrast, matching-based methods [49] mine pixel-level correlation but may heavily rely on class-specific fea-novel classes with only a few annotations. Previous ap-proaches following metric learning [5, 36, 38, 42] can be divided into prototype- and matching-based methods. Mo-tivated by PrototypicalNet [34] for few-shot learning, the prevalent FSS models [5, 8, 24, 37, 42] utilize prototypes for specific-class representation. Recent work [18, 47, 49] points out that a single prototype has a limitation to cover all regions of an object, especially for pixel-wise dense segmentation tasks. To remedy this problem, methods of [18,47] use EM and cluster algorithms to generate multi-ple prototypes for different parts of the objects. Compared with prototype-based methods, matching-based ones [23, 25, 41, 49] are based on intuition to mine dense correspon-dence between the query images and support annotations.
They utilize pixel-level features and supplement more de-tailed support context.
Transformer. Transformer was first introduced in Natu-ral Language Processing (NLP) [4, 40]. In computer vision,
ViT [6] treats an image as a patch sequence and demon-strates that pure transformer architecture can achieve state-of-the-art image classification. Recent work explores com-bining few-shot semantic segmentation and transformer ar-chitecture [19,35]. In [23], the classifier weight transformer adapts the classifier’s weights to address the intra-class vari-ation issue. CyCTR [49] is a cycle-consistent transformer by generating query and key sequences from the query and support set, respectively. Transformer architecture helps
FSS transcend the limitation of semantic-level prototypes and leverage pixel-wise alignment. Previous transformer-based methods are still difficult to handle noise interference and over-fitting. We, in this paper, propose a new trans-former structure decoupling the downsampling and match-ing processes and design the matching module constructed on correlation mechanism and distillation. distillation. Knowledge
Knowledge distillation (KD)
[14] was widely used in model compression.
Large models typically have higher knowledge capacity.
In contrast, small models have fewer parameters, better efficiency, and lower cost. KD attempts to transfer learned knowledge from the large model (a.k.a. the teacher) to another light model (a.k.a. the student) with tolerable loss in performance. Method of [50] processes a self-distillation framework to distill knowledge within itself to improve model accuracy. Self-distillation divides the model into multiple sections and transfers knowledge from deeper portions to shallow ones. Knowledge distillation is also used for extracting semantic information and mining inner correlation. STEGO [11] shows that even unsupervised deep network features have correlation patterns consistent with true labels. STEGO framework applies feature cor-relation map distillation to excavate the intrinsic semantic correlation at pixel level. Motivated by unsupervised semantic segmentation [11], we design the correlation
Illustration of different few-shot segmentation frame-Figure 2. works. (b) Adaptive-classifier (a) Prototype-based method. method. (c) Feature matching with transformer architecture. (d)
Our Hierarchically Decoupled Matching Network (HDMNet) with correlation map distillation. tures and cause overfitting and weak generalization.
To address these issues, we propose Hierarchically De-coupled Matching Network (HDMNet) with correlation map distillation for better mining pixel-level support corre-spondences. HDMNet extends transformer architecture [6, 40, 44] to construct the feature pyramid and performs dense matching. Previous transformer-based methods [35, 49] adopt the self-attention layer to parse features and then feed query and support features to the cross-attention layer for pattern matching, as illustrated in Fig. 2(c). This pro-cess stacks the self- and cross-attention layers multiple times, mixes separated embedding features, and acciden-tally causes unnecessary information interference.
In this paper, we decouple the feature parsing and match-ing process in a hierarchical paradigm and design a new matching module based on correlation and distillation.
This correlation mechanism calculates pixel-level corre-spondence without directly relying on the semantic-specific features, alleviating the train-set overfitting problem. Fur-ther, we introduce correlation map distillation [14, 50] that encourages the shallow layers to approximate the semantic correlation of deeper layers to make the former more aware of the context for high-quality prediction.
Our contribution is the following. 1) We extend the transformer to hierarchical parsing and feature matching for few-shot semantic segmentation, with a new matching mod-ule reducing overfitting. 2) We propose correlation map dis-tillation leveraging soft correspondence under multi-level 3) We achieve new state-of-and multi-scale structure. the-art results on standard benchmark of COCO-20i and
PASCAL-5i without compromising efficiency. 2.