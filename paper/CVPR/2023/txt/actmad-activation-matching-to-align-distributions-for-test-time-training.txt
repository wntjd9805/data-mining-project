Abstract
Test-Time-Training (TTT) is an approach to cope with out-of-distribution (OOD) data by adapting a trained model to distribution shifts occurring at test-time. We propose to perform this adaptation via Activation Matching (ActMAD):
We analyze activations of the model and align activation statistics of the OOD test data to those of the training data.
In contrast to existing methods, which model the distribution of entire channels in the ultimate layer of the feature ex-tractor, we model the distribution of each feature in multiple layers across the network. This results in a more fine-grained supervision and makes ActMAD attain state of the art per-formance on CIFAR-100C and Imagenet-C. ActMAD is also architecture- and task-agnostic, which lets us go beyond image classification, and score 15.4% improvement over previous approaches when evaluating a KITTI-trained ob-ject detector on KITTI-Fog. Our experiments highlight that
ActMAD can be applied to online adaptation in realistic scenarios, requiring little data to attain its full performance. 1.

Introduction
When evaluated in laboratory conditions, deep networks outperform humans in numerous visual recognition tasks [6, 8, 9]. But this impressive performance is predicated on the assumption that the test and training data come from the same distribution. In many practical scenarios, satisfying this requirement is either impossible or impractical. For example, a perception system of an autonomous vehicle can be exposed not only to fog, snow, and rain, but also to rare conditions including smoke, sand storms, or substances distributed on the road in consequence of a traffic accident.
Unfortunately, distribution shifts between the training and test data can incur a significant performance penalty [1, 20,
‡Correspondence: muhammad.mirza@icg.tugraz.at
Figure 1. ActMAD is well suited for online test-time adaptation irrespective of network architecture and task. Our intended applica-tion is object detection on board of a vehicle driving in dynamically changing and unpredictable weather conditions. Here, we report the class-averaged Mean Average Precision (mAP@50) over adapta-tion time1, attained by continuously adapting a KITTI [12]-trained
YOLOv3 [29] detector. Clear refers to the weather condition of the original dataset, mostly acquired in sunny weather, while Fog, Rain, and Snow are generated by perturbing the original images [15, 18].
Gray vertical lines mark transitions between weather conditions. 25,27,34,36]. To address this shortcoming, test-time-training (TTT) methods attempt to adapt a deep network to the actual distribution of the test data, at test-time.
Existing TTT techniques limit the performance drop pro-voked by the shift in test distribution, but the goal of on-line, data-efficient, source-free, and task-agnostic adapta-tion remains elusive: the methods that update network pa-rameters by self-supervision on test data [10, 25, 32] can only be used if the network was first trained in a multi-1Time (seconds) for adaptation is hardware specific. For reference, we run these experiments on an RTX-3090 NVIDIA graphics card and it takes
∼75s to adapt to 3741 images (one weather condition) in the KITTI test set.
task setup, the approaches that minimise the entropy of test predictions [24, 34], or use pseudo-labels, or pseudo-prototypes [19,20], are not easily adapted to object detection or regression, and the ones that update statistics of the batch normalization layers [27, 30] are limited to architectures that contain them. As a consequence of these limitations, while existing methods fare well in image classification, none of them is well suited for the scenario where the need for on-line adaptation is the most acute, that is, object detection on board a car driving in changing weather conditions.
Our goal is to lift the limitations of the current methods and propose a truly versatile, task-, and architecture-agnostic technique, that would extend TTT beyond image classifi-cation and enable its deployment in object detection for automotive applications. To that end, we revisit feature align-ment, the classical domain adaptation technique [7, 31, 38], also adopted by several TTT algorithms [20, 25, 27, 30]. It consists in aligning the distribution of test set features to that of the training set. By contrast to previous methods, that align the distributions of entire channels in the ultimate layer of the feature extractor, ActMAD brings the feature align-ment approach to another level by individually aligning the distribution of each feature in multiple feature maps across the network. On the one hand, this makes the alignment location-aware. For example, the features from the bottom of the image, most often representing road and other vehi-cles, are not mixed with features from the top part, more likely to describe trees, buildings, or the sky. On the other hand, it results in a more fine-grained supervision of the adapted network. Our ablation studies show that ActMAD owes most of its performance to these two contributions. Ad-ditionally, while several authors suggested aligning higher-order moments [20, 38], we demonstrate that aligning means and variances should be preferred when working with small batches. Unlike methods that only update mean and vari-ance [27, 30], or affine parameters of batch normalization layers [20, 27, 30], ActMAD updates all network parameters.
Our approach is architecture- and task-agnostic, and does not require access to the training data or training labels, which is important in privacy-sensitive applications. It does require the knowledge of activation statistics of the training set, but this requirement can be easily satisfied by collecting the data during training, or by computing them on unlabelled data without distribution shift.
Our contribution consists in a new method to use Acti-vation Matching to Align Distributions for TTT, which we abbreviate as ActMAD. Its main technical novelty is that we model the distribution of each point in the feature map, across multiple feature maps in the network. While most previous works focus on combining different approaches to test-time adaptation, ActMAD is solely based on feature alignment. This is necessary for our method to be applicable across different architectures and tasks, but we show that discarding the bells and whistles, while aligning activation distributions in a location-aware manner results in matching or surpassing state-of-the-art performance on a number of
TTT benchmarks. Figure 1 presents the performance of Act-MAD in a simulated online adaptation scenario, in which an object detector runs onboard a vehicle driven in changing weather conditions. Most existing methods cannot be used in this setup, because they cannot run online, or because they cannot be used for object detection. Note, that our method recovers, and even exceeds, the performance of the initial network once the weather cycle goes back to the conditions in which the detector was trained. 2.