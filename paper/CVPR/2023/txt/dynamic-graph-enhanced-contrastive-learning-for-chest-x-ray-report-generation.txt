Abstract
Automatic radiology reporting has great clinical poten-tial to relieve radiologists from heavy workloads and im-prove diagnosis interpretation. Recently, researchers have enhanced data-driven neural networks with medical knowl-edge graphs to eliminate the severe visual and textual bias in this task. The structures of such graphs are exploited by using the clinical dependencies formed by the disease topic tags via general knowledge and usually do not up-date during the training process. Consequently, the fixed graphs can not guarantee the most appropriate scope of knowledge and limit the effectiveness. To address the limi-tation, we propose a knowledge graph with Dynamic struc-ture and nodes to facilitate chest X-ray report generation with Contrastive Learning, named DCL. In detail, the fun-damental structure of our graph is pre-constructed from general knowledge. Then we explore specific knowledge ex-tracted from the retrieved reports to add additional nodes or redefine their relations in a bottom-up manner. Each im-age feature is integrated with its very own updated graph before being fed into the decoder module for report gen-eration. Finally, this paper introduces Image-Report Con-trastive and Image-Report Matching losses to better repre-sent visual features and textual information. Evaluated on
IU-Xray and MIMIC-CXR datasets, our DCL outperforms previous state-of-the-art models on these two benchmarks. 1.

Introduction
Recently, automatic report generation has received grow-ing attentions from both machine learning and automatic
It aims to generate semantically coher-medicine fields. ent and informative reports to describe the referring ex-amination images, such as Chest X-Ray [8, 18], Lung CT
Scan [26] or funds angiography [23]. Such techniques have great clinical potential in relieving junior radiologists from heavy workloads and reducing diagnosis errors by improv-*Corresponding author. https://github.com/mlii0117/DCL
Figure 1. An illustration of one sample from MIMIC-CXR [18] and the pre-constructed graph in [46], where the blue circle, or-ange boxes and black boxes refer to the global node, organ-level entities and key findings, respectively. The red dash line here rep-resents the unconnected relation. ing the interpretation [7, 30].
Witnessed the great progress in artificial intelligence, es-pecially deep learning methods [12,25,39], researchers have proposed various data-driven neural networks for radiology reporting and achieved promising performances in metrics that measure descriptive accuracy [7, 44] and clinical cor-rectness [11, 46]. Compared with the similar task generic image captioning [14], the key challenges in chest X-ray report generation (CRG) task are the severe visual and tex-tual data bias [19, 26]. On the one hand, medical images are highly similar to each other due to the imaging methods and human tissues themselves. However, abnormal regions or lesions that should acquire more attentions usually lo-cate at a small part and lack detailed annotations in existing
CRG benchmarks. On the other hand, sentences that de-scribe normal regions are likely to appear repeatedly among each dataset which disables the model to describe specific crucial abnormalities. Two concepts have been proved ef-fective in eliminating those bias.
The first one is to integrate medical knowledge with
ORG systems [24,30,44,46]. Zhang et al. [46] constructed a universal graph comprised of a global node, 7 organs/tissues and 20 findings (normal or disease keywords). Disease key-word nodes linked to the same organ are connected to each other and the root in the graph. This graph can enhance the relationships among findings and emphasize the dis-ease keywords. Thus, it is also adopted in the following works [30, 44]. However, this graph is built from general knowledge and may be inappropriate in some cases. As the shown report in Fig. 1, it is observed that effusion should be suggestive of edema, however, such relationship is not mod-elled in the graph. Furthermore, some nodes like ‘cicatrix’ or ‘hypoinflation’ only appear very few times in two ORG benchmarks [8, 18]. Therefore, it is necessary to update the scope of knowledge for each case; In addition to the medical knowledge, recent works [5,11,31,38,43] utilize contrastive learning to improve the visual and textual representations by contrasting positive and negative pairs. They proposed var-ious contrastive learning objectives to capture the abnormal regions from a chest X-Ray image. Since normal images usually dominate the dataset over abnormal ones [37], it is also crucial to recognize the normal or abnormal cases in the meantime.
In this paper, we propose a novel framework, named
DCL, which exploits a dynamic graph integrating spe-cific knowledge with general knowledge to enhance vi-sual representations learned in a contrastive manner. We adopt the general knowledge with 28 entities from [46] as the fundamental structure of our graph, and the rela-tionships are modelled in an adjacency matrix. Given a medical image, we first retrieve its semantically similar re-ports from the training set. Specific knowledge is extracted from those reports via RadGraph [17] and stored in triplets (<subjective entity, relation, objective entity>). And we in-tegrate those triplets with the pre-constructed graph by dy-namically adding additional nodes or linking two entities.
We utilize a graph encoder to propagate information over the updated graph for refining the node features, which are initialized by a pretrained SciBert [4]. Then the dedicated node features are attended to visual representations for re-port generation via a Transformer [39] decoder. Based on the dynamic graph, we introduce a contrastive learning ob-jective, image-report contrastive loss to well represent the visual features and textual information. In addition, con-trastive learning can help ensure the accuracy of the report retrieval procedure in the dynamic graph construction pro-cess. Image-report matching loss is also employed to fur-ther improve the performances.
IU-We evaluate our method on two benchmarks,
Xray [8] and MIMIC-CXR [18]. Experimental results demonstrate that our approach can either outperform or match previous state-of-the-art (SOTA) methods in metrics that measure descriptive accuracy and clinical correctness.
It indicates that leveraging dynamic graphs to enhance con-trastive learning is helpful to generate high-quality reports.
In summary, our main contributions are as follows:
• We propose a novel framework that leverages a dy-namic graph to enhance visual representations with contrastive learning paradigms for radiology reporting.
• Our proposed dynamic graph integrates both general and specific knowledge; The contrastive learning ob-jective can improve visual and textual representations and dynamic graph accuracy.
• We conduct extensive experiments on two popular benchmarks to show the effectiveness of our approach, which achieves the SOTA performance on both lan-guage generation and clinical efficacy metrics. 2.