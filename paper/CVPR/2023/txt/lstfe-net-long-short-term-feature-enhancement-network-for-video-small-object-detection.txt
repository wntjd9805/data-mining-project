Abstract
Video small object detection is a difficult task due to the lack of object information. Recent methods focus on adding more temporal information to obtain more potent high-level features, which often fail to specify the most vital informa-tion for small objects, resulting in insufficient or inappro-priate features. Since information from frames at differ-ent positions contributes differently to small objects, it is not ideal to assume that using one universal method will extract proper features. We find that context information from the long-term frame and temporal information from the short-term frame are two useful cues for video small ob-ject detection. To fully utilize these two cues, we propose a long short-term feature enhancement network (LSTFE-Net) for video small object detection. First, we develop a plug-and-play spatio-temporal feature alignment module to cre-ate temporal correspondences between the short-term and current frames. Then, we propose a frame selection mod-ule to select the long-term frame that can provide the most additional context information. Finally, we propose a long short-term feature aggregation module to fuse long short-term features. Compared to other state-of-the-art meth-ods, our LSTFE-Net achieves 4.4% absolute boosts in AP on the FL-Drones dataset. More details can be found at https://github.com/xiaojs18/LSTFE-Net.
Figure 1. The architecture of the proposed LSTFE-Net. The current frame (Cur-frame), short-term frames (ST-frames) near the Cur-frame, and long-term frames (LT-frames) sampled from the whole video first go through the feature extraction network.
Then the Cur-frame feature and ST-frame features are connected through the spatio-temporal feature alignment module, and the frame selection module searches the background context of LT-frame features. After getting the Proposal-Level (PL) features, the long short-term feature aggregation module finally integrates the long short-term features into the Cur-frame to make feature en-hancement. Best viewed in color and zoomed in. 1.

Introduction
Video small object detection plays an important role in many fields such as automatic driving, remote sensing, medical image, and industrial defect detection [26]. How-ever, it is still a difficult task due to the lack of pixel infor-mation and the difficulty of feature extraction. Therefore, the topic of how to enhance the features of small objects has attracted great attention [1, 7, 16].
*The corresponding author
Some recent works have proved that the improvement of video small object detection performance requires full uti-lization of information in the temporal dimension. While detecting small objects in the current frame may suffer from many problems such as motion blur, low resolution, and too small size, effective modeling of information from other frames can help address these problems [2, 4, 9, 17]. There is a high similarity in nearby frames because of the strong time continuity between them, so it is natural to empha-size the importance of short-term frames, which are near the current frame. According to FGFA [28] and STSN [2],
short-term frame features can be aligned and aggregated to provide more useful information for small objects.
Context information is important for small object detec-tion [16,25]. Because a large number of images are sampled from the same video for the same object, the background context information of the object is single. Additionally, only part of the frames in the video are sampled as current frames (such as 15 frames) while training, which results in the lack of real background context information and reduces the robustness of the training model. Compared to features in a short range, features from the whole video level can be more discriminative and robust [21, 23]. And it is noticed in prior works [9, 23] that more contextual information will be provided when using long-term frames sampled from the whole video.
The impacts of short-term and long-term frames in de-tection have been studied in recent methods. However, these methods have obvious disadvantages in both efficiency and accuracy, especially for small objects in videos. Some methods [22, 27, 28] extract information from the short-term frame and exploit the flow model to propagate features across adjacent frames, however, this is expensive because the flow model is hard to construct and transplant. Other methods [21, 23] focus on semantic information from long-term frames and incorporate randomly sampled long-term frames in detection, which causes uncertainty of detection performance and the loss of valuable information. Besides, these methods above cannot figure out the specific informa-tion that matters most for small objects from frames. Some methods [21â€“23, 27, 28] think the information in the video is single and miss considering distinct information from different frames, getting inadequate features. Other meth-ods [3, 5] focus on extracting high-level features from the video which are not suitable for small objects due to their special properties.
To better mine information from both short-term frames and long-term frames, we propose a long short-term fea-ture enhancement network (LSTFE-Net) for video small object detection. Specifically, the features of short-term frames are expected to correspond to the current frame in a low-cost and effective way, so a spatio-temporal feature alignment module is designed to propagate features across nearby frames. Further, in order to increase the benefit of aligned features while not increasing too much complexity of the model, a spatio-temporal feature aggregation method is also added. The context information is expected to be highlighted from the whole video, prompting a frame se-lection module to select the long-term frame feature. The goal is to make effective feature enhancement after the fea-tures are collected, and the establishment of connections between different features is enforced. A long short-term feature aggregation module is devised to aggregate features from the current frame, the short-term frames, and the long-term frames by stages. The performance of the proposed method is evaluated on the open dataset, and experiment re-sults demonstrate that our method has obvious advantages in video small object detection. The architecture of the net-work is shown in Fig. 1.
Our main contributions are summarized as follows: (1) An LSTFE-Net is proposed to effectively enhance small object features and improve detection performance. (2) A plug-and-play spatio-temporal feature alignment module is designed for aligning features across adjacent frames. A flexible way to make Pixel-Level feature en-hancement for small objects using aligned features is also explored. Combined with the Proposal-Level feature en-hancement, this module achieves multi-level enhancement to improve the feature expressiveness. The whole module is easy to transplant and proved to be effective, which re-veals its potential ability to benefit most of the works. (3) A frame selection module is proposed to ensure the utilization of high-value input data, and it selects the long-term frame feature with the most context information. This module reinforces the network to automatically look for useful information for small objects, improving its stabil-ity and performance in video small object detection. (4) To effectively integrate the long-term frame features and short-term frame features into the current frame, a long short-term feature aggregation module is proposed to aggre-gate different features in different stages. This enables the relations between Proposal-Level features to be built adap-tively based on an attention mechanism, which also means our feature enhancement for small objects can be accom-plished in a general and limitless way. 2.