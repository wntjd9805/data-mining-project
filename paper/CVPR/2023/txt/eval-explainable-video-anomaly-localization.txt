Abstract
We develop a novel single-scene framework for video anomaly localization that allows for human-understandable reasons for the decisions the system makes.
We first learn general representations of objects and their motions (using deep networks) and then use these repre-sentations to build a high-level, location-dependent model of any particular scene. This model can be used to detect anomalies in new videos of the same scene. Importantly, our approach is explainable – our high-level appearance and motion features can provide human-understandable rea-sons for why any part of a video is classified as normal or anomalous. We conduct experiments on standard video anomaly detection datasets (Street Scene, CUHK Avenue,
ShanghaiTech and UCSD Ped1, Ped2) and show significant improvements over the previous state-of-the-art. All of our code and extra datasets will be made publicly available. 1.

Introduction
We are interested in the problem of spatio-temporal lo-calization of anomalous activities in videos of a given scene.
Informally, anomalous activities are events that differ from those typically observed in a scene, such as a cyclist riding through an indoor shopping mall [35]. Like many other pa-pers on anomaly detection, this work addresses the setting in which we have access to an initial set of videos that are used to define the typical, or ‘nominal’ activities in a partic-ular scene. Such a situation naturally arises in surveillance and monitoring tasks [41], where it is easy to collect nomi-nal data, but it is not practical to collect a representative set of possible anomalies for a scene. Thus, the problem set-up is as follows: provided with a set of videos of a scene which do not contain any anomalies, (called the nominal set), the goal is to detect any events in a test video from the same scene that differ substantially from all events in the nominal set [29, 35]. In defining anomaly detection, it is important to consider the role of location. In real-world
*equal contribution surveillance scenarios, an event may be normal in one loca-tion but anomalous in another. For example, a car driving on a road is typically not anomalous, while one driving on a sidewalk typically is. In view of this, we adopt the follow-ing definition [35].
Definition 1 An anomaly is any spatio-temporal region of test video that is significantly different from all of the nomi-nal video in the same spatial region.
Unlike most recent work in anomaly detection, a key goal of our work is to produce not only a set of anoma-lies, but a simple and clear explanation for what makes them anomalous. We are motivated by how people tasked with watching video from a stationary surveillance cam-era would detect an unusual incident. While monitoring a scene, we expect a person to note the types of objects seen (people, buildings, cars) and the motions of those objects (walking east or west on a sidewalk, driving northwest on the street) to characterize the given scene. The person could then notice an anomaly when the objects or motions do not match what has been seen before. The person could also explain why something was anomalous.
We design our video anomaly detection system using this sketch of how a human would solve the problem as motiva-tion. We want to use deep networks to give a high-level un-derstanding of the objects and motions occurring in a scene.
By ’high-level’, we mean at the level of whole objects and not at the level of pixels or edges. To do this, we train deep networks that take a spatio-temporal region of video (which we call a video volume) as input and output attribute vectors representing the object classes, the directions and speeds of motion and the fraction of stationary pixels (which gives information on the sizes of moving objects) occurring in a spatio-temporal region. The feature vectors from the penul-timate layers of these deep networks yield high-level rep-resentations, or embeddings, of the appearance and motion content of each video volume. Ten frames are used for video volumes in our experiments.
Unlike many other recent works, we do not learn new embedding functions (i.e. networks) for new scenes. We use the same embedding networks for every environment.
Instead, to characterize the nominal video for a new scene, we store a representative set of all the embeddings found in
the nominal video. That is, for every video volume in the nominal video of a new scene, we calculate our represen-tations of appearance, motion direction, speed, and back-ground fraction. We then reduce this set of embeddings to a smaller set which we call exemplars by removing re-dundant embeddings. We select a separate set of exemplars for each spatial region. This results in a compact, accu-rate, and location-dependent model of the nominal data in a new scene. Since there is no training of deep networks for each new environment, modeling a new environment is
‘lightweight’ compared to many other methods, making it efficient to model new scenes. Our exemplar model also al-lows very efficient updating if new nominal video is intro-duced. This is a crucial property for video anomaly detec-tion methods because, in practice, it is unrealistic to assume that the initial nominal video covers every possible normal change. New nominal video will occasionally need to be added, making it critical that models are easy to extend.
Given test video of the same scene, we compute our high-level features for each video volume. We then com-pare these to the exemplars stored in the nominal model at the corresponding spatial region. Any test feature with a high distance to every nominal exemplar for that region is considered anomalous. Because the feature vectors map to human-interpretable attributes, these attributes can be used to give human-understandable explanations for why our system labeled video volumes as normal or anomalous.
We define a method as ’explainable’ if it can give human-understandable reasons for its decisions. Details of how our system provides explanations are given in Section 4.4.
In summary, we make the following key contributions: 1) We show that modeling scenes using high-level attributes leads to robust anomaly detection. 2) We introduce the idea of directly estimating high-level motion attributes from raw video volumes using deep networks. 3) We show how these high-level attributes also allow human-interpretable expla-nations. 4) Finally, we demonstrate an alternative to much of the previous work that is based on learning to recon-struct the nominal data. Our alternative approach is practi-cal since it does not require training deep networks for each new scene and allows for simple and efficient updates to a scene model given new nominal training data. 2.