Abstract
Building benchmarks to systemically analyze different capabilities of video question answering (VideoQA) models
Existing benchmarks often is challenging yet crucial. use non-compositional simple questions and suffer from language biases, making it difﬁcult to diagnose model weaknesses incisively. A recent benchmark AGQA [8] poses a promising paradigm to generate QA pairs automatically from pre-annotated scene graphs, enabling it to measure diverse reasoning abilities with granular control. However, its questions have limitations in reasoning about the ﬁne-grained semantics in videos as such information is absent in its scene graphs. To this end, we present ANetQA, a large-scale benchmark that supports ﬁne-grained compo-sitional reasoning over the challenging untrimmed videos from ActivityNet [4].
Similar to AGQA, the QA pairs in ANetQA are automatically generated from annotated video scene graphs. The ﬁne-grained properties of ANetQA are reﬂected in the following: (i) untrimmed videos with
ﬁne-grained semantics; (ii) spatio-temporal scene graphs with ﬁne-grained taxonomies; and (iii) diverse questions generated from ﬁne-grained templates. ANetQA attains 1.4 billion unbalanced and 13.4 million balanced QA pairs, which is an order of magnitude larger than AGQA with a similar number of videos. Comprehensive experiments are performed for state-of-the-art methods. The best model achieves 44.5% accuracy while human performance tops out at 84.5%, leaving sufﬁcient room for improvement. 1.

Introduction
Recent advances in deep learning have enabled machines to tackle complicated video-language tasks that involve
∗Jun Yu is the corresponding author
Figure 1. Comparisons of ANetQA and AGQA [8]. The QA pairs in both benchmarks are automatically generated from spatio-temporal scene graphs by using handcrafted question templates.
Beneﬁting from the untrimmed long videos and ﬁne-grained scene graphs, our questions require more ﬁne-grained reasoning abilities than those in AGQA when similar templates are applied.
Moreover, the newly introduced attribute annotations allow us to design many ﬁne-grained question templates that are not supported in AGQA (e.g., “what color” and “what is the occupation”). both video and language clues, e.g., video-text retrieval, video captioning, video temporal grounding, and video question answering. Among these tasks, video question answering (VideoQA) is one of the most challenging tasks as it veriﬁes multiple skills simultaneously. Taking the question “What the person is wearing before various ﬁsh are seen swimming through the reef?” in Figure 1 as an example, it requires a synergistic understanding of both the video and question, together with spatio-temporal reasoning to predict an accurate answer. is the black object that
To comprehensively evaluate the capabilities of existing
VideoQA models, several prominent benchmarks have been established [11, 21, 29, 33, 38, 42, 43]. Despite their useful-ness, they also have distinct shortcomings. Some bench-marks use simulated environments to synthesize video con-tents [29, 42], which provides controllable diagnostics over different reasoning skills. However, the synthetic videos lack visual diversity and the learned models on the bench-marks cannot generalize to real-world scenarios directly.
Some real-world benchmarks generate QA pairs from off-the-shelf video captions [38, 48] or human annotations [11, 21, 33, 43], which suffer from simple question expressions and biased answer distributions. These weaknesses may be exploited by models to make educated guesses to obtain the correct answers without seeing video contents [24, 40].
One recent VideoQA benchmark AGQA poses a promis-ing paradigm to address the above limitations [8]. AGQA is built upon the real-world videos from Charades [32].
In contrast to previous benchmarks, AGQA adopts a two-stage paradigm instead. For each video, a spatio-temporal scene graph over representative frames is ﬁrst annotated by humans, which consists of spatially-grounded object-relationship triplets and temporally-grounded actions. After that, different types of questions are generated on top of the scene graph using corresponding question templates, enabling it to measure various reasoning abilities with granular control. Despite the comprehensiveness of AGQA, we argue that its foundation—the spatio-temporal scene graph—has limitations in representing the ﬁne-grained se-mantics of videos. Speciﬁcally, their scene graphs encode objects and relationships from limited taxonomies, which are not ﬁne-grained enough for generating questions that require reasoning about the detailed video semantics.
To this end, we introduce ANetQA1, a new benchmark that supports ﬁne-grained compositional reasoning over complex web videos from ActivityNet [4]. Similar to the strategy of AGQA, the QA pairs in ANetQA are automati-cally generated from pre-annotated scene graphs. As shown in Figure 1, we claim that ANetQA is more ﬁne-grained than AGQA in terms of the following: (i) The benchmark is built upon untrimmed long videos with ﬁne-grained semantics. Each video may involves multiple indoor or outdoor scenarios, containing com-plicated interactions between persons and objects. (ii) The spatio-temporal scene graph consists of ﬁne-grained objects (e.g., “manta ray”, “diving gear”), relationships (e.g., “jumping into”, “chasing”), at-tributes (e.g., “swimming”, “black and white”), and actions in natural language (e.g., “a manta ray swims in the ocean over a reef ”). 1Note that there is a VideoQA benchmark ActivityNet-QA [43] whose
QA pairs are fully annotated by humans. To avoid confusion, we name our benchmark ANetQA. (iii) Beneﬁting from the ﬁne-grained scene graphs, we are able to design diverse question templates that requires ﬁne-grained compositional reasoning (e.g.,
“what color ...” and “what is the occupation ...”).
Beneﬁting from the above ﬁne-grained characteristics,
ANetQA obtains 1.4B unbalanced and 13.4M balanced QA pairs. To the best of our knowledge, ANetQA is the largest
VideoQA benchmark in terms of the number of questions.
Compared with the previous largest benchmark AGQA,
ANetQA is an order of magnitude larger than it with a similar number of videos. We conduct comprehensive experiments and intensive analyses on ANetQA for the state-of-the-art VideoQA models, including HCRN [19],
ClipBERT [20], and All-in-One [35]. The best model delivers 44.5% accuracy while human performance tops out at 84.5%, showing sufﬁcient room for future improvement.
The benchmark is available at here2. 2.