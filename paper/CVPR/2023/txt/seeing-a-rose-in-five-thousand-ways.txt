Abstract 1.

Introduction
What is a rose, visually? A rose comprises its intrinsics, including the distribution of geometry, texture, and material specific to its object category. With knowledge of these in-trinsic properties, we may render roses of different sizes and shapes, in different poses, and under different lighting condi-tions. In this work, we build a generative model that learns to capture such object intrinsics from a single image, such as a photo of a bouquet. Such an image includes multiple instances of an object type. These instances all share the same intrinsics, but appear different due to a combination of variance within these intrinsics and differences in extrinsic factors, such as pose and illumination. Experiments show that our model successfully learns object intrinsics (distri-bution of geometry, texture, and material) for a wide range of objects, each from a single Internet image. Our method achieves superior results on multiple downstream tasks, in-cluding intrinsic image decomposition, shape and image generation, view synthesis, and relighting.
“Nature!... Each of her works has an essence of its own; each of her phenomena a special characterisation; and yet their diversity is in unity.” – Georg Christoph Tobler
The bouquet in Figure 1 contains many roses. Although each rose has different pixel values, we recognize them as individual instances of the same type of object. Such un-derstanding is based on the fact that these instances share the same object intrinsics—the distributions of geometry, texture, and material that characterize a rose. The difference in appearance arises from both the variance within these distributions and extrinsic factors such as object pose and environment lighting. Understanding these aspects allows us to imagine and draw additional, new instances of roses with varying shape, pose, and illumination.
In this work, our goal is to build a model that captures such object intrinsics from a single image, and to use this model for shape and image generation under novel view-points and illumination conditions, as illustrated in Figure 1.
This problem is challenging for three reasons. First, we only have a single image. This makes our work fundamen-tally different from existing works on 3D-aware image gen-eration models [8, 9, 27, 28], which typically require a large dataset of thousands of instances for training. In comparison, 1“People where you live ... grow five thousand roses in one garden ...
And yet what they’re looking for could be found in a single rose.” — Quote from The Little Prince by Antoine de Saint-Exup´ery.
the single image contains at most a few dozen instances, making the inference problem highly under-constrained.
Second, these already limited instances may vary signif-icantly in pixel values. This is because they have different poses and illumination conditions, but neither of these factors are annotated or known. We also cannot resort to existing tools for pose estimation based on structure from motion, such as COLMAP [35], because the appearance variations violate the assumptions of epipolar geometry.
Finally, the object intrinsics we aim to infer are proba-bilistic, not deterministic: no two roses in the natural world are identical, and we want to capture a distribution of their geometry, texture, and material to exploit the underlying multi-view information. This is therefore in stark contrast to existing multi-view reconstruction or neural rendering meth-ods for a fixed object or scene [25, 26, 40]. These challenges all come down to having a large hypothesis space for this highly under-constrained problem, with very limited visual observations or signals. Our solution to address these chal-lenges is to design a model with its inductive biases guided by object intrinsics. Such guidance is two-fold: first, the instances we aim to present share the same object intrinsics, or the same distribution of geometry, texture, and material; second, these intrinsic properties are not isolated, but inter-weaved in a specific way as defined by a rendering engine and, fundamentally, by the physical world.
Specifically, our model takes the single input image and learns a neural representation of the distribution over 3D shape, surface albedo, and shininess of the object, factoring out pose and lighting variations, based on a set of instance masks and a given pose distribution of the instances. This ex-plicit, physically-grounded disentanglement helps us explain the instances in a compact manner, and enables the model to learn object intrinsics without overfitting the limited obser-vations from only a single image.
The resulting model enables a range of applications. For example, random sampling from the learned object intrinsics generates novel instances with different identities from the input. By modifying extrinsic factors, the synthesized in-stances can be rendered from novel viewpoints or relit with different lighting configurations.
Our contributions are three-fold: 1. We propose the problem of recovering object intrinsics, including both 3D geometry, texture, and material prop-erties, from just a single image of a few instances with instance masks. 2. We design a generative framework that effectively learns such object intrinsics. 3. Through extensive evaluations, we show that the model achieves superior results in shape reconstruction and generation, novel view synthesis, and relighting.
Object
Variance
Unknown
Poses
Re-lighting 3D-Aware
SinGAN [36]
NeRF [26]
D-NeRF [32]
GNeRF [25]
Neural-PIL [7]
NeRD [6]
EG3D [9]
Ours
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Table 1. Comparisons with prior works. Unlike existing 3D-aware generative models, our method learns from a very limited number of observations. Unlike multi-view reconstruction methods, our method models variance among observations from training inputs. 2.