Abstract
Blind image quality assessment (BIQA) aims to auto-matically evaluate the perceived quality of a single image, whose performance has been improved by deep learning-based methods in recent years. However, the paucity of la-beled data somewhat restrains deep learning-based BIQA methods from unleashing their full potential.
In this pa-per, we propose to solve the problem by a pretext task customized for BIQA in a self-supervised learning manner, which enables learning representations from orders of mag-nitude more data. To constrain the learning process, we propose a quality-aware contrastive loss based on a simple assumption: the quality of patches from a distorted image should be similar, but vary from patches from the same im-age with different degradations and patches from different images. Further, we improve the existing degradation pro-cess and form a degradation space with the size of roughly 2 × 107. After pre-trained on ImageNet using our method, models are more sensitive to image quality and perform sig-nificantly better on downstream BIQA tasks. Experimental results show that our method obtains remarkable improve-ments on popular BIQA datasets. 1.

Introduction
With the arrival of the mobile internet era, billions of im-ages are generated, uploaded and shared on various social media platforms, including Twitter, TikTok, etc [30]. As an essential indicator, image quality can help these service providers filter and deliver high-quality images to users, thereby improving Quality of Experience. Therefore, huge efforts [12,20,22,24,52,73] have been devoted to establish-ing an image quality assessment (IQA) method consistent with human viewers. In real-world scenarios, there usually exists no access to the reference images and the quality of reference images is suspicious. Thus, blind IQA (BIQA) methods are more attractive and applicable, despite full-reference IQA has achieved prospective results [33].
†Equal contribution.
Figure 1. The two images in the first row are sampled from BIQA dataset CLIVE [20]. Although they have the same semantic mean-ing, their perceptual qualities are quite different: their mean opin-ion scores (MOS) are 31.83 and 86.35. The second row shows modified versions of the first two images and their MOSs (rated by 7 people). After different operations, the quality dramatically changed, while semantic meaning remains unchanged.
Recently, deep learning-based BIQA methods have made tremendous improvements on in-the-wild IQA bench-marks [17, 29, 79]. However, this problem is far from re-solved and is hindered by the paucity of labeled data [34].
The largest (by far) available BIQA dataset, FLIVE [79], contains nearly 40,000 real-world distorted images. By comparison, the very popular entry-level image recognition dataset, CIFAR-100 [35], contains 60,000 labeled images.
Accordingly, existing BIQA datasets are too small to train deep learning-based models effectively.
Researchers present several methods to tackle this chal-lenge. A straight-forward way is to sample local patches and assign the label of the whole image (i.e., mean opinion score, MOS) to the patches [4, 31, 32, 38, 60, 89, 90]. How-ever, the perceived scores of local image patches tend to differ from the score of the entire image [79, 89]. Another common strategy is to leverage domain knowledge from large-scale datasets (e.g., ImageNet [14]) for other com-puter vision tasks [6, 32]. Nevertheless, these pre-trained images with models can be sub-optimal for BIQA tasks: the same content share the same semantic label, whereas, their quality may be different (Fig. 1). Some researchers
propose to train a model on synthetic images with artificial degradation and then regress the model onto small-scale tar-get BIQA datasets [43, 73, 89]. However, images generated by rather simple degradation process with limited distortion types/levels are far from authentic. Further, synthetic im-ages are regularly distorted from limited pristine images of high quality, so the image content itself only has a marginal effect on the image quality. Yet in real-world scenarios, the image quality is closely related to its content, due to view-ers’ preferences for divergent contents [38, 62].
Self-supervised learning (SSL) or unsupervised learning is another potential choice to overcome the problem of lack-ing adequate training data, for its ability to utilize an amount of unlabeled data. Such technique is proven to be effective in many common computer vision tasks [2]. However, dif-ferent from models for these tasks which mainly focus on high-level information, representations learned for BIQA should be sensitive to all kinds of low-level distortions and high-level contents, as well as interactions between them.
There are few research focused on such area in the litera-ture, let alone a deep SSL designed for BIQA with state-of-the-art performance [44].
In this work, we propose a novel SSL mechanism that distinguishes between samples with different percep-tual qualities, generating Quality-aware Pre-Trained (QPT) models for downstream BIQA tasks. Specifically, we sup-pose the quality of patches from a distorted image should be similar, but vary from patches from different images (i.e., content-based negative) and the same image with different degradations (i.e., degradation-based negative). Moreover, inspired by recent progress in image restoration, we intro-duce shuffle order [84], high-order [70] and a skip oper-ation to the image degradation process, to simulate real-world distortions. In this way, models pre-trained on Im-ageNet are expected to extract quality-aware features, and boost downstream BIQA performances. We summarize the contributions of this work as follows:
• We design a more complex degradation process suit-able for BIQA. It not only considers a mixture of multiple degradation types, but also incorporates with shuffle order, high-order and a skip operation, result-ing in a much larger degradation space. For a dataset with a size of 107, our method can generate more than 2 × 1014 possible pairs for contrastive learning.
• To fully exploit the abundant information hidden be-neath such an amount of data, we propose a novel SSL framework to generate QPT models for BIQA based on
MoCoV2 [8]. By carefully designing positive/negative samples and customizing quality-aware contrastive loss, our approach enables models to learn quality-aware information rather than regular semantic-aware representation from massive unlabeled images.
• Extensive experiments on five BIQA benchmark datasets (sharing the same pre-trained weight of QPT) demonstrate that our proposed method significantly outperforms other counterparts, which indicates the ef-fectiveness and generalization ability of QPT. It is also worth noting that the proposed method can be easily integrated with current SOTA methods by replacing their pre-trained weights. 2.