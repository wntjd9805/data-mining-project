Abstract
Blind face restoration usually synthesizes degraded low-quality data with a pre-defined degradation model for train-ing, while more complex cases could happen in the real world. This gap between the assumed and actual degra-dation hurts the restoration performance where artifacts are often observed in the output. However, it is expensive and infeasible to include every type of degradation to cover real-world cases in the training data. To tackle this robust-ness issue, we propose Diffusion-based Robust Degradation
Remover (DR2) to first transform the degraded image to a coarse but degradation-invariant prediction, then employ an enhancement module to restore the coarse prediction to a high-quality image. By leveraging a well-performing de-noising diffusion probabilistic model, our DR2 diffuses in-put images to a noisy status where various types of degrada-tion give way to Gaussian noise, and then captures semantic information through iterative denoising steps. As a result,
DR2 is robust against common degradation (e.g. blur, re-size, noise and compression) and compatible with different
†Corresponding author.
*Cooperative Medianet Innovation Center. designs of enhancement modules. Experiments in various settings show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets. 1.

Introduction
Blind face restoration aims to restore high-quality face images from their low-quality counterparts suffering from unknown degradation, such as low-resolution [5, 11, 27], blur [45], noise [23, 36], compression [10], etc. Great im-provement in restoration quality has been witnessed over the past few years with the exploitation of various facial pri-ors. Geometric priors such as facial landmarks [5], parsing maps [4, 5], and heatmaps [43] are pivotal to recovering the shapes of facial components. Reference priors [9, 25, 26] of high-quality images are used as guidance to improve details.
Recent research investigates generative priors [39, 42] and high-quality dictionaries [14,24,48], which help to generate photo-realistic details and textures.
Despite the great progress in visual quality, these meth-ods lack a robust mechanism to handle degraded inputs be-sides relying on pre-defined degradation to synthesize the
training data. When applying them to images of severe or unseen degradation, undesired results with obvious artifacts can be observed. As shown in Fig. 1, artifacts typically ap-pear when 1) the input image lacks high-frequency informa-tion due to downsampling or blur (1st row), in which case restoration networks can not generate adequate information, or 2) the input image bears corrupted high-frequency in-formation due to noise or other degradation (2nd row), and restoration networks mistakenly use the corrupted informa-tion for restoration. The primary cause of this inadaptabil-ity is the inconsistency between the synthetic degradation of training data and the actual degradation in the real world.
Expanding the synthetic degradation model for training would improve the models’ adaptability but it is apparently difficult and expensive to simulate every possible degrada-tion in the real world. To alleviate the dependency on syn-thetic degradation, we leverage a well-performing denois-ing diffusion probabilistic model (DDPM) [16, 37] to re-move the degradation from inputs. DDPM generates im-ages through a stochastic iterative denoising process and
Gaussian noisy images can provide guidance to the gen-erative process [6, 29]. As shown in Fig. 2, noisy images are degradation-irrelevant conditions for DDPM genera-tive process. Adding extra Gaussian noise (right) makes dif-ferent degradation less distinguishable compared with the original distribution (left), while DDPM can still capture the semantic information within this noise status and re-cover clean face images. This property of pretrained DDPM makes it a robust degradation removal module though only high-quality face images are used for training the DDPM.
Our overall blind face restoration framework DR2E con-sists of the Diffusion-based Robust Degradation Remover (DR2) and an Enhancement module. In the first stage, DR2 first transforms the degraded images into coarse, smooth, and visually clean intermediate results, which fall into a degradation-invariant distribution (4th column in Fig. 1). In the second stage, the degradation-invariant images are fur-ther processed by the enhancement module for high-quality details. By this design, the enhancement module is com-patible with various designs of restoration methods in seek-ing the best restoration quality, ensuring our DR2E achieves both strong robustness and high quality.
We summarize the contributions as follows. (1) We pro-pose DR2 that leverages a pretrained diffusion model to remove degradation, achieving robustness against complex degradation without using synthetic degradation for train-(2) Together with an enhancement module, we em-ing. ploy DR2 in a two-stage blind face restoration framework.
The enhancement module has great flexibility in incorporat-ing a variety of restoration methods to achieve high restora-tion quality. (3) Comprehensive experiments show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets.
Figure 2. Mean and standard variation of pixel-wise error dis-tribution. (Left) the error between original degraded input y and its ground truth low-resolution image ˆy (only bicubically down-sampled); (Right) the error between q(y500|y) and q( ˆy500| ˆy) sampled by Eq. (2), with extra Gaussian noise added by the dif-fusion function. 2.