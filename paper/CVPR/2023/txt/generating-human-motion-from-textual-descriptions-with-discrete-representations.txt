Abstract
In this work, we investigate a simple and must-known con-ditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete rep-resentations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing dis-crepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve compara-ble performance on the consistency between text and gener-ated motion (R-Precision), but with FID 0.116 largely outper-forming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation. Our implementation is available on the project page: https://mael-zys.github.io/T2M-GPT/. 1.

Introduction
Generating motion from textual descriptions can be used in numerous applications in the game industry, film-making, and animating robots. For example, a typical way to access new motion in the game industry is to perform motion cap-ture, which is expensive. Therefore automatically generating motion from textual descriptions, which allows producing meaningful motion data, could save time and be more eco-nomical.
Motion generation conditioned on natural language is challenging, as motion and text are from different modali-ties. The model is expected to learn precise mapping from
Figure 1. Visual results on HumanML3D [22]. Our approach is able to generate precise and high-quality human motion consistent with challenging text descriptions. More visual results are on the project page. the language space to the motion space. To this end, many works propose to learn a joint embedding for language and motion using auto-encoders [3, 21, 65] and VAEs [52, 53].
MotionClip [65] aligns the motion space to CLIP [55] space.
ACTOR [52] and TEMOES [53] propose transformer-based
VAEs for action-to-motion and text-to-motion respectively.
These works show promising performances with simple de-scriptions and are limited to producing high-quality motion when textual descriptions become long and complicated.
Guo et al. [22] and TM2T [23] aim to generate motion
sequences with more challenging textual descriptions. How-ever, both approaches are not straightforward, involve three stages for text-to-motion generation, and sometimes fail to generate high-quality motion consistent with the text (See
Figure 4 and more visual results on the project page). Re-cently, diffusion-based models [32] have shown impressive results on image generation [60], which are then introduced to motion generation by MDM [66] and MotionDiffuse [74] and dominates text-to-motion generation task. However, we find that compared to classic approaches, such as VQ-VAE [68], the performance gain of the diffusion-based ap-proaches [66, 74] might not be that significant. In this work, we are inspired by recent advances from learning the discrete representation for generation [5, 15, 16, 19, 45, 58, 68, 70] and investigate a simple and classic framework based on Vector
Quantized Variational Autoencoders (VQ-VAE) [68] and
Generative Pre-trained Transformer (GPT) [56, 69] for text-to-motion generation.
Precisely, we propose a two-stage method for motion generation from textual descriptions. In stage 1, we use a standard 1D convolutional network to map motion sequences to discrete code indices. In stage 2, a standard GPT-like model [56, 69] is learned to generate sequences of code in-dices from pre-trained text embedding. We find that the naive training of VQ-VAE [68] suffers from code collapse. One effective solution is to leverage two standard recipes during the training: EMA and Code Reset. We provide a full anal-ysis of different quantization strategies. For GPT, the next token prediction brings inconsistency between the training and inference. We observe that simply corrupting sequences during the training alleviates this discrepancy. Moreover, throughout the evolution of image generation, the size of the dataset has played an important role. We further explore the impact of dataset size on the performance of our model.
The empirical analysis suggests that the performance of our model can potentially be improved with larger datasets.
Despite its simplicity, our approach can generate high-quality motion sequences that are consistent with challeng-ing text descriptions (Figure 1 and more on the project page). Empirically, we achieve comparable or even better performances than concurrent diffusion-based approaches
MDM [66] and HumanDiffuse [74] on two widely used datasets: HumanML3D [22] and KIT-ML [54]. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. We conduct comprehensive experiments to explore this area, and hope that these experiments and conclusions will contribute to future developments.
In summary, our contributions include:
• We present a simple yet effective approach for mo-tion generation from textual descriptions. Our ap-proach achieves state-of-the-art performance on Hu-manML3D [22] and KIT-ML [54] datasets.
• We show that GPT-like models incorporating discrete representations still remain a very competitive approach for motion generation.
• We provide a detailed analysis of the impact of quanti-zation strategies and dataset size. We show that a larger dataset might still offer a promising prospect to the community.
Our implementation is available on the project page. 2.