Abstract in the image.
We consider the problem of reconstructing a full 360◦ photographic model of an object from a single image of it.
We do so by fitting a neural radiance field to the image, but find this problem to be severely ill-posed. We thus take an off-the-self conditional image generator based on diffu-sion and engineer a prompt that encourages it to “dream up” novel views of the object. Using the recent DreamFu-sion method, we fuse the given input view, the conditional prior, and other regularizers into a final, consistent recon-struction. We demonstrate state-of-the-art reconstruction results on benchmark images when compared to prior meth-ods for monocular 3D reconstruction of objects. Qualita-tively, our reconstructions provide a faithful match of the input view and a plausible extrapolation of its appearance and 3D shape, including to the side of the object not visible 1.

Introduction
We consider the problem of obtaining a 360◦ photo-graphic reconstruction of any object given a single image of it. The challenge is that a single image does not con-tain sufficient information for 3D reconstruction. Without access to multiple views, an image only provides weak ev-idence about the 3D shape of the object, and only for one side of it. Even so, there is proof that this task can be solved: any skilled 3D artist can take a picture of almost any object and, given sufficient time and effort, create a plausible 3D model of it. The artist can do so by tapping into their vast knowledge of the natural world and the objects it contains, making up for the information missing from the image.
Hence, monocular 3D reconstruction requires combining 1
visual geometry with a powerful statistical model of the 3D world. Diffusion-based 2D image generators like DALL-E 2 [30], Imagen [35], and Stable Diffusion [33] are able to generate high-quality images from ambiguous inputs such as text, showing that powerful priors for 2D images can be learned. However, extending them to 3D is not easy be-cause, while one can access billions of 2D images for train-ing [36], the same cannot be said for 3D data.
A simpler approach is to extract or distill 3D informa-tion from an existing 2D generator. A 2D image genera-tor can in fact be used to sample or validate multiple views of a given object, which can then be used to perform 3D reconstruction. This idea was already demonstrated with
GAN-based generators for simple data like faces and syn-thetic objects [2, 6, 8, 24, 25, 47]. Better 2D generators have since resulted in better results, culminating in methods such as DreamFusion [27], which can produce high-quality 3D models from an existing 2D generator and text.
In this paper, we port distillation approaches from text-based generation to monocular 3D reconstruction. This is not a trivial change because conditioning generation on an image provides a much more fine-grained specification of the object than text. This in turn requires the 2D diffusion model to hallucinate new views of a specific object instead of some object of a given type. The latter is difficult because the coverage of generator models is limited [1], meaning that not every version of an object is captured well by the model. We find empirically that this is a key problem.
We address this issue by introducing RealFusion, a new method for 3D reconstruction from a single image. We ex-press the object’s 3D geometry and appearance by means of a neural radiance field. Then, we fit the radiance field to the given input image by minimizing the usual rendering loss.
At the same time, we sample random other views of the object, and constrain them with the diffusion prior, using a technique similar to DreamFusion.
We find that, due to the coverage issue, this idea does not work well out of the box, but can be improved via ad-equately conditioning the 2D diffusion model. The idea is to configure the prior to “dream up” or sample images that may plausibly constitute other views of the given ob-ject. We do so by automatically engineering the diffusion prompt from random augmentations of the given image. In this manner, the diffusion model provides sufficiently strong constraints to allow meaningful 3D reconstruction.
In addition to setting the prompt correctly, we also add some regularizers: shading the underlying geometry and randomly dropping out texture (also similar to DreamFu-sion), smoothing the normals of the surface, and fitting the model in a coarse-to-fine fashion, capturing first the overall structure of the object and only then the fine-grained details.
We also focus on efficiency and base our model on Instant-NGP [23]. In this manner, we achieve reconstructions in the span of hours instead of days if we were to adopt traditional
MLP-based NeRF models.
We assess our approach by using random images cap-tured in the wild as well as existing benchmark datasets.
Note that we do not train a fully-fledged 2D-to-3D model and we are not limited to specific object categories; rather, we perform reconstruction on an image-by-image basis us-ing a pretrained 2D generator as a prior. Nonetheless, we can surpass quantitatively and qualitatively previous single-image reconstructors, including Shelf-Supervised Mesh
Prediction [50], which uses supervision tailored specifically for 3D reconstruction.
Qualitatively, we obtain plausible 3D reconstructions that are a good match for the provided input image (Fig. 1).
Our reconstructions are not perfect, as the diffusion prior clearly does its best to explain the available image evidence but cannot always match all the details. Even so, we be-lieve that our results convincingly demonstrate the viability of this approach and trace a path for future improvements.
To summarize, we make the following contributions: (1) We propose RealFusion, a method that can extract from a single image of an object a 360◦ photographic 3D recon-struction without assumptions on the type of object imaged or 3D supervision of any kind; (2) We do so by leveraging an existing 2D diffusion image generator via a new single-image variant of textual inversion; (3) We also introduce new regularizers and provide an efficient implementation using InstantNGP; (4) We demonstrate state-of-the-art re-construction results on a number of in-the-wild images and images from existing datasets when compared to alternative approaches. 2.