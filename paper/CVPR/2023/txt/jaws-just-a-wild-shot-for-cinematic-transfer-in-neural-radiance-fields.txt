Abstract
This paper presents JAWS, an optimization-driven ap-proach that achieves the robust transfer of visual cinematic features from a reference in-the-wild video clip to a newly generated clip. To this end, we rely on an implicit-neural-representation (INR) in a way to compute a clip that shares the same cinematic features as the reference clip. We pro-pose a general formulation of a camera optimization prob-lem in an INR that computes extrinsic and intrinsic camera parameters as well as timing. By leveraging the differen-tiability of neural representations, we can back-propagate our designed cinematic losses measured on proxy estima-tors through a NeRF network to the proposed cinematic parameters directly. We also introduce speciﬁc enhance-ments such as guidance maps to improve the overall qual-ity and efﬁciency. Results display the capacity of our sys-tem to replicate well known camera sequences from movies, adapting the framing, camera parameters and timing of the generated video clip to maximize the similarity with the ref-erence clip. 1.

Introduction
Almost all ﬁlm directors and visual artists follow the paradigm of watch-and-learn by drawing inspiration from others visual works.
Imitating masterpieces (also known as visual homage) in their visual composition, camera mo-tion or character action is a popular way to pay tribute to and honor the source work which inﬂuenced them. This subtly draws a faint, yet distinctive clue through the devel-opment of the whole ﬁlm history. Examples are common-place: across the dizzy effect of dolly-zoom in Vertigo to the scary scene in Jaws (see Fig. 1); or from the old school
Bruce Lee’s kung-fu ﬁlms to blockbusters such as Kill Bill and Matrix series. The cinematic knowledge encompassed in reference sequences is therefore carried out and inherited through these visual homages, and have even been adapted
*Equal contribution. Corresponding to jinglei.shi@nankai.edu.cn.
Figure 1. This illustration displays the capacity of our proposed method to transfer the cinematic motions from the in-the-wild fa-mous ﬁlm clip Jaws (bottom) to another context, replicating and adapting the dolly-zoom effect (a combined translation plus ﬁeld of view change in opposite directions, causing distinctive motions on background and foreground contents). to more modern visual media such as digital animation or video games. Audiences obviously acknowledge the strong references between the epic Western The good, the bad and the ugly of 1966 and the 2010 Red Dead Redemption.
The creation of such visual homages in real or virtual en-vironments yet remains a challenging endeavor that requires more than just replicating a camera angle, motion or visual compositions. Such a cinematic transfer task is successful only if it can achieve a similar visual perception (look-and-feel) between the reference and homage clip, for example in terms of visual composition (i.e. how visual elements are framed on screen), perceived motion (i.e. how elements and scene move through the whole sequence), but also, camera focus, image depth, actions occurring or scene lighting. by the
Inspired aforementioned watch-and-learn paradigm, we propose to address the cinematic transfer problem by focusing on motion characteristics, i.e. solving a cinematic motion transfer problem.
While different techniques are available to extract cam-era poses and trajectories from reference videos (e.g. sparse or dense localization and mapping techniques) and poten-tially transfer them, the naive replication of such trajectories to new 3D environments (mesh-based or implicit NeRF-based) generally fail to reproduce the perceived motion due to scaling issues, different screen compositions, lack of vi-sual anchors, or scene dissimilarities. Furthermore, these extraction techniques are very sensitive to widespread cam-era effects such as shallow depths of ﬁeld or motion blur.
In this paper, we propose to address this cinematic mo-tion transfer problem following a different path. Rather than extracting visual features from the reference clip (as geo-metric properties or encoded in a latent representation) and designing a technique to then recompute a new video clip using these visual features, we rely on the differentiable na-ture of NeRF representations. We propose the design of a fully differentiable pipeline which takes as input a refer-ence clip, an existing NeRF representation of a scene, and optimizes a sequence of camera parameters (pose and focal length) in both space and time inside the NeRF so as to min-imize differences between the motion features of the refer-ences views and the clip created from the optimized param-eters. By exploiting an end-to-end differentiable pipeline, our process directly backpropagates the changes to spatial and temporal cinematic parameters. The key to successful cinematic motion transfer is then found in the design of rel-evant motion features and means to improve guidance in the optimization. Our work relies on the combination of an optical ﬂow estimator, to ensure the transfer of camera directions of motions, and a character pose estimator to en-sure the anchoring of motions around a target. A dedicated guidance map is then created to draw the attention of the framework on key aspects of the extracted features.
The contributions of our work are:
The ﬁrst feature-driven cinematic motion transfer tech-nique that can reapply motion characteristics of an in-the-wild reference clip to a NeRF representation.
The design of an end-to-end differentiable pipeline to di-rectly optimize spatial and temporal cinematic parameters from a reference clip, by exploiting differentiability of neu-ral rendering and proxy networks.
The proposal of robust cinematic losses combined with guidance maps that ensure the effective transfer of both on-screen motions and character framing to keep the cinematic visual similarity of the reference and generated clip. 2.