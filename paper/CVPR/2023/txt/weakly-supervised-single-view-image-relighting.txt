Abstract
We present a learning-based approach to relight a sin-gle image of Lambertian and low-frequency specular ob-jects. Our method enables inserting objects from pho-tographs into new scenes and relighting them under the new environment lighting, which is essential for AR appli-cations. To relight the object, we solve both inverse ren-dering and re-rendering. To resolve the ill-posed inverse rendering, we propose a weakly-supervised method by a low-rank constraint. To facilitate the weakly-supervised training, we contribute Relit, a large-scale (750K images) dataset of videos with aligned objects under changing il-luminations. For re-rendering, we propose a differen-tiable specular rendering layer to render low-frequency non-Lambertian materials under various illuminations of spherical harmonics. The whole pipeline is end-to-end and efficient, allowing for a mobile app implementation of AR object insertion. Extensive evaluations demonstrate that our method achieves state-of-the-art performance. Project page: https://renjiaoyi.github.io/relighting/. 1.

Introduction
Object insertion finds extensive applications in Mobile
AR. Existing AR object insertions require a perfect mesh of the object being inserted. Mesh models are typically built by professionals and are not easily accessible to am-ateur users. Therefore, in most existing AR apps such as
SnapChat and Ikea Place, users can use only built-in vir-tual objects for scene augmentation. This may greatly limit user experience. A more appealing setting is to allow the user to extract objects from a photograph and insert them into the target scene with proper lighting effects. This calls for a method of inverse rendering and relighting based on a single image, which has so far been a key challenge in the graphics and vision fields.
Relighting real objects requires recovering lighting, ge-ometry and materials which are intertwined in the observed image; it involves solving two problems, inverse render-*Co-first authors.
†Corresponding author: kevin.kai.xu@gmail.com.
Figure 1. Our method relights real objects into new scenes from single images, which also enables editing materials from diffuse to glossy with non-Lambertian rendering layers. ing [17] and re-rendering. Furthermore, to achieve real-istic results, the method needs to be applicable for non-Lambertian objects.
In this paper, we propose a pipeline to solve both problems, weakly-supervised inverse render-ing and non-Lambertian differentiable rendering for Lam-bertian and low-frequency specular objects.
Inverse rendering is a highly ill-posed problem, with sev-eral unknowns to be estimated from a single image. Deep learning methods excel at learning strong priors for reduc-ing ill-posedness. However, this comes at the cost of a large amount of labeled training data, which is especially cum-bersome to prepare for inverse rendering since ground truths of large-scale real data are impossible to obtain. Synthetic training data brings the problem of domain transfer. Some methods explore self-supervised pipelines and acquire ge-ometry supervisions of real data from 3D reconstruction by multi-view stereo (MVS) [34, 35]. Such approaches, how-ever, have difficulties in handling textureless objects.
To tackle the challenge of training data shortage, we pro-pose a weakly-supervised inverse rendering pipeline based on a novel low-rank loss and a re-rendering loss. For low-rank loss, a base observation here is that the material re-flectance is invariant to illumination change, as an intrin-sic property of an object. We derive a low-rank loss for inverse rendering optimization which imposes that the re-flectance maps of the same object under changing illumi-nations are linearly correlated. In particular, we constrain
Figure 2. Overview of our method. At training time, Spec-Net separates input images into specular and diffuse branches. Spec-Net,
Normal-Net and Light-Net are trained in a self-supervised manner by the Relit dataset. At inference time, inverse rendering properties are predicted to relight the object under novel lighting and material. The non-Lambertian render layers produce realistic relit images. the reflectance matrix with each row storing one of the re-flectance maps to be rank one. This is achieved by minimiz-ing a low-rank loss defined as the Frobenius norm between the reflectance matrix and its rank-one approximation. We prove the convergence of this low-rank loss.
In contrast, traditional Euclidean losses lack a convergence guarantee.
To facilitate the learning, we contribute Relit, a large-scale dataset of videos of real-world objects with changing illuminations. We design an easy-to-deploy capturing sys-tem: a camera faces toward an object, both placed on top of a turntable. Rotating the turntable will produce a video with the foreground object staying still and the illumination changing. To extract the foreground object from the video, manual segmentation of the first frame suffices since the ob-ject is aligned across all frames.
As shown in Figure 2, a fixed number of images under different lighting are randomly selected as a batch. We first devise a Spec-Net to factorize the specular highlight, trained by the low-rank loss on the chromaticity maps of diffuse im-ages (image subtracts highlight) which should be consistent within the batch. With the factorized highlight, we further predict the shininess and specular reflectance, which is self-supervised with the re-rendering loss of specular highlight.
For the diffuse branch, we design two networks, Normal-Net and Light-Net, to decompose the diffuse component by predicting normal maps and spherical harmonic lighting coefficients, respectively. The diffuse shading is rendered by normal and lighting, and diffuse reflectance (albedo) is computed by diffuse image and shading. Both networks are trained by low-rank loss on diffuse reflectance.
Regarding the re-rendering phase, the main difficulty is the missing 3D information of the object given a single-view image. The Normal-Net produces a normal map which is a partial 3D representation, making the neural rendering techniques and commercial renderers inapplicable. The ex-isting diffuse rendering layer for normal maps of [20] can-not produce specular highlights. Pytorch3D and [9, 11] ren-der specular highlights for point lights only.
To this end, we design a differentiable specular renderer from normal maps, based on the Blinn-Phong specular re-flection [5] and spherical harmonic lighting [6]. Combining with the differentiable diffuse renderer, we can render low-frequency non-Lambertian objects with prescribed parame-ters under various illuminations, and do material editing as a byproduct.
We have developed an Android app based on our method which allows amateur users to insert and relight arbitrary objects extracted from photographs in a target scene. Exten-sive evaluations on inverse rendering and image relighting demonstrate the state-of-the-art performance of our method.
Our contributions include:
• A weakly-supervised inverse rendering pipeline trained with a low-rank loss. The correctness and con-vergence of the loss are mathematically proven.
• A large-scale dataset of foreground-aligned videos col-lecting 750K images of 100+ real objects under differ-ent lighting conditions.
• An Android app implementation for amateur users to make a home run. 2.