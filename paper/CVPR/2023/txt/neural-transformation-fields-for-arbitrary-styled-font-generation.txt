Abstract
Few-shot font generation (FFG), aiming at generating font images with a few samples, is an emerging topic in re-cent years due to the academic and commercial values. Typ-ically, the FFG approaches follow the style-content disen-tanglement paradigm, which transfers the target font styles to characters by combining the content representations of source characters and the style codes of reference samples.
Most existing methods attempt to increase font generation ability via exploring powerful style representations, which may be a sub-optimal solution for the FFG task due to the lack of modeling spatial transformation in transferring font styles. In this paper, we model font generation as a continu-ous transformation process from the source character image to the target font image via the creation and dissipation of font pixels, and embed the corresponding transformations into a neural transformation ﬁeld. With the estimated trans-formation path, the neural transformation ﬁeld generates a set of intermediate transformation results via the sampling process, and a font rendering formula is developed to ac-cumulate them into the target font image. Extensive exper-iments show that our method achieves state-of-the-art per-formance on few-shot font generation task, which demon-strates the effectiveness of our proposed model. Our imple-mentation is available at: https://github.com/fubinfb/NTF. 1.

Introduction
Generating a new stylized font with a few reference sam-ples, referred as the few-shot font generation (FFG) task, has received considerable attentions due to the academic, commercial, and artistic values, especially for some glyph-rich scripts such as Chinese and Korean. In recent years, the style-content disentanglement paradigm has become the
∗Corresponding author: Yu Qiao
Figure 1. The motivation of this paper: (a). The differences of font styles mainly come from the shape deformation and transfor-mation of the source font, such as the thickness of strokes and the writing pattern of glyphs. (b). We regard font generation as a con-tinuous transformation process from the source font to target font via the creation and dissipation of font pixels. most popular solution for FFG task, which decouples the stylized font images into the font-speciﬁc style codes and the character-speciﬁc content features. Therefore, the target stylized font will be generated from the carefully-designed decoder via the combination of the style codes from the reference samples and the content embeddings from the s-tandard glyphs. Based on the style representations, exist-ing approaches can be roughly divided into two categories.
Early approaches mainly model font style information as global statistic features, and thus utilize the universal style representations to embed such information. Witnessing the
ﬁne-grained structure variations and local correlations (such as stoken and component) in font styles, recent approaches further develop component-wise or ﬁne-grained localized style representations to boost FFG performance. Howev-er, as shown in Fig. 1, the differences between font styles mainly come from the shape deformation and transforma-tion on the source glyph. Based on this observation, pre-vious approaches may be the sub-optimal solution for FFG task due to the lack of modeling spatial transformation in font generation process.
Inspired by recent advances in Neural Radiance Field (NeRF) [23] in 3D view synthesis, we attempt to embed
Figure 2. (a). The methodology of NeRF. (b). We embed the desired transformations of font generation into the neural transformation
ﬁeld. The style estimator Eθ predicts the locations θ of each font style, and the font generation process can be viewed as a transformation process of font pixels from the original point to this location. (c). As each NeRF only corresponds to a speciﬁc scene, it is impracticable for the FFG task. Thus we generalize our NTF to model the transformations for all characters by introducing the structure embedding (extracted by a structure encoder Ec) of characters. (d). Finally, considering the localized characteristic of font style, we further generalize our NTF into the localized style representation. the desired spatial transformations in a neural transforma-tion ﬁeld, and thus the font generation process can be refor-mulated as the accumulation of a set of intermediate trans-formation results along a speciﬁc path. The methodology of NeRF is presented in Fig. 2 (a). The NeRF constructs a neural radiance ﬁeld to represent a speciﬁc 3D scene as a 5D function, whose inputs are the location and view di-rection while outputs are the emitted color together with the volume density. An MLP network is utilized to approximate this function, where the scene information is embedded in-to the parameters via the optimization process. To generate a novel view, the color of each pixel is rendered along the color ray passing through the scene via volume rendering technique [22].
Motivated by the above method, instead of directly pre-dicting the pixel-level deformation offsets, we model the font generation as a continuous transformation process via the creation intensity ϕ and dissipation rate τ of font pix-els, and embed such transformations into a neural transfor-mation ﬁled. To make the description clearly, we use the universal-representation-based font generation to introduce our method. As shown in Fig. 2 (c), the neural transfor-mation ﬁeld (NTF) is constructed to model the font trans-formation process based on the structure embeddings of source characters. Each location in NTF represents a spe-ciﬁc structure-related transformation and the path from the original point to this location corresponding to the trans-formation process from the source font to the target font.
Each font style has a speciﬁc location relating to the desired transformations for generating font images in NTF, and we utilize an estimator to estimate this location. With the esti-mated location and the corresponding transformation path,
NTF generates a set of intermediate transformations via the sampling process, and a font rendering formula is develope-d to accumulate them into the target font image. Since the font styles contain many ﬁne-grained structures and local correlations, the localized style representation shows sig-niﬁcant advantages over the universal style representation.
Therefore, as shown in Fig. 2 (d), we generalize our NT-F into the localized style representations and conduct ex-tensive experiments to evaluate our model. Experimental results show that our model achieves new state-of-the-art performance in few-shot font generation tasks, both in the seen fonts with unseen contents testing and unseen fonts with unseen contents testing, which demonstrate the supe-rior generation performance of our proposed method.
In summary, our contribution is threefold in this paper: 1). We regard font generation as a continuous transfor-mation process via the creation and dissipation of font pix-els along the transformation path, and embed such transfor-mations into the neural transformation ﬁeld (NTF). 2). A differentiable font rendering procedure is devel-oped to accumulate the intermediate transformations into the target font image. 3). Experimental results show that our method outper-forms the state-of-the-art methods in the few-shot font gen-eration task, which demonstrate the effectiveness of our pro-posed method. 2.