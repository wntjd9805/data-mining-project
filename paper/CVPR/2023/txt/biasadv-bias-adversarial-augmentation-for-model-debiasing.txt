Abstract
Neural networks are often prone to bias toward spurious correlations inherent in a dataset, thus failing to generalize unbiased test criteria. A key challenge to resolving the issue is the significant lack of bias-conflicting training data (i.e., samples without spurious correlations). In this paper, we propose a novel data augmentation approach termed Bias-Adversarial augmentation (BiasAdv) that supplements bias-conflicting samples with adversarial images. Our key idea is that an adversarial attack on a biased model that makes decisions based on spurious correlations may generate syn-thetic bias-conflicting samples, which can then be used as augmented training data for learning a debiased model.
Specifically, we formulate an optimization problem for gen-erating adversarial images that attack the predictions of an auxiliary biased model without ruining the predictions of the desired debiased model. Despite its simplicity, we find that BiasAdv can generate surprisingly useful synthetic bias-conflicting samples, allowing the debiased model to learn generalizable representations. Furthermore, BiasAdv does not require any bias annotations or prior knowledge of the bias type, which enables its broad applicability to exist-ing debiasing methods to improve their performances. Our extensive experimental results demonstrate the superiority of BiasAdv, achieving state-of-the-art performance on four popular benchmark datasets across various bias domains. 1.

Introduction
Real-world datasets are often inherently biased [2, 34], where certain visual attributes are spuriously correlated with class labels. For example, let us consider a binary clas-sification task between cats and dogs. Unbeknownst to us, our dataset could consist of most cats indoors and most dogs outdoors, as illustrated in Figure 1. When trained on such a biased dataset, neural networks often learn unintended shortcuts [2, 8, 34, 39] (e.g., making predictions based on
*Corresponding author: jonny.lim@samsung.com
Figure 1. An overview of BiasAdv. In MetaShift [26], the bias attribute {Indoor, Outdoor} is spuriously correlated to the class label {Cat, Dog}. In this work, we refer to data with such spurious correlations as bias-guiding samples and without such correlations as bias-conflicting samples, respectively. Using the biased dataset, we train an auxiliary model to be biased, and BiasAdv supple-ments bias-conflicting samples using adversarial images which at-tack the biased predictions of the auxiliary model while preserving the predictions of the debiased model. By leveraging the diversi-fied bias-conflicting data, BiasAdv allows the debiased model to learn generalizable representations for unbiased classification. the background) and fail to generalize in a new unbiased test environment. To tackle the problem, conventional methods have utilized explicit bias annotations [1, 19, 39, 43] or prior knowledge of the bias type [2, 3, 5, 9, 46]. However, bias annotations are expensive and laborious to obtain, and pre-suming certain bias types in advance limits the capability to be universally applicable to various bias types.
To train a debiased model without bias annotations, the main line of recent research [6, 22, 30, 34, 42] has com-monly utilized an intentionally biased model as an auxiliary model under the idea that bias attributes are easy-to-learn.
In essence, these methods identify bias-conflicting samples based on the auxiliary model and train the debiased model in a way that focuses more on the identified samples (i.e., re-weighting based on the auxiliary model). Although recent re-weighting methods have achieved remarkable success in debiasing without bias annotations, they have inherent limi-tation; since the number of bias-conflicting samples is often too small for a model to learn generalizable representations, the model is prone to over-fitting [25]. Consequently, re-weighting methods suffer from the degraded performance on bias-guiding samples [20, 44], which raises the question of whether these methods truly make models debiased or simply deflect models in unintended directions.
To resolve the aforementioned issues, data augmentation methods have recently been proposed to supplement bias-conflicting samples. For example, BiaSwap [20] conducts image-to-image translation to synthesize bias-conflicting samples. However, it requires delicate training of complex and expensive image translation models [36], limiting its applicability. On the other hand, DFA [25] utilizes feature-level swapping based on disentangled representations be-tween bias-guiding and bias-conflicting features. Learning disentangled representations, however, is often challenging on real-world datasets [27, 28, 31].
In this paper, we devise a much simpler yet more effec-tive approach to generate bias-conflicting samples, coined
Bias-Adversarial augmentation (BiasAdv). Figure 1 shows an overview of BiasAdv. We utilize an auxiliary model that intentionally learns biased shortcuts, likewise [30, 34]. The key idea of BiasAdv is that an adversarial attack on the biased auxiliary model may generate adversarial images that alter the bias cue from the input images (i.e., bias-conflicting samples). Concretely, we formulate an optimiza-tion problem to generate adversarial images that attack the predictions of the biased auxiliary model without ruining the predictions of the desired debiased model. Then, the generated adversarial images are used as additional training data to train the debiased model. It is noteworthy that, un-like previous data augmentation methods [20, 25], BiasAdv does not require complex image translation models or dis-entangled representations, so it can be seamlessly applied to any debiasing method based on the biased model. Fur-thermore, we show that BiasAdv, despite its simplicity, can generate surprisingly useful synthetic bias-conflicting sam-ples, which significantly improves debiasing quality.
The main contributions of our work are three-fold:
• We propose BiasAdv, a simple and effective data aug-mentation method for model debiasing, which utilizes adversarially attacked images as additional training data. Our method does not require any bias annotations or prior knowledge of the bias type during training.
• BiasAdv can be easily applied to existing re-weighting methods without architectural or algorithmic changes.
We confirm that BiasAdv significantly improves the performance, achieving up to 22.8%, 13.4%, 7.9%, and 8.0% better performance than the state-of-the-art results on CIFAR-10C [25], BFFHQ [25], BAR [34], and MetaShift [26], respectively.
• We demonstrate the effectiveness of BiasAdv through extensive ablation studies and analyses. Our key find-ing is that BiasAdv helps to learn generalizable repre-sentations and prevents over-fitting; it does not degrade the performance of bias-guiding samples and improves model robustness against input corruptions. 2.