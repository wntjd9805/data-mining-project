Abstract 1.

Introduction
A light stage uses a series of calibrated cameras and lights to capture a subject’s facial appearance under vary-ing illumination and viewpoint. This captured information is crucial for facial reconstruction and relighting. Unfortu-nately, light stages are often inaccessible: they are expen-sive and require significant technical expertise for construc-tion and operation. In this paper, we present SunStage: a lightweight alternative to a light stage that captures com-parable data using only a smartphone camera and the sun.
Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as guidance in joint recon-struction of facial geometry, reflectance, camera pose, and lighting parameters. Despite the in-the-wild un-calibrated setting, our approach is able to reconstruct detailed facial appearance and geometry, enabling compelling effects such as relighting, novel view synthesis, and reflectance editing.
A light stage [11] acquires the shape and material prop-erties of a face in high detail using a series of images cap-tured under synchronized cameras and lights. This captured information can be used to synthesize novel images of the subject under arbitrary lighting conditions or from arbitrary viewpoints. This process enables a number of visual effects, such as creating digital replicas of actors that can be used in movies [1] or high-quality postproduction relighting [46].
In many cases, however, it is often infeasible to get ac-cess to a light stage for capturing a particular subject, be-cause light stages are not easy to find: they are expensive and require significant technical expertise (often teams of people) to build and operate.
In these cases, hope is not lost — one can turn to methods that are trained on light stage data, with the intention of generalizing to new sub-jects. These methods do not require the subject to be cap-tured by a light stage but instead use a machine learning
model trained on a collection of previously acquired light stage captures to enable the same applications as a light stage, but from only one or several images of a new sub-ject [6, 25, 30, 38, 40, 50, 52]. Unfortunately, these methods have difficulty faithfully reproducing and editing the ap-pearance of new subjects, as they lack much of the signal necessary to resolve the ambiguities of single-view recon-struction, i.e., a single image of a face can be reasonably explained by different combinations of geometry, illumina-tion, and reflectance.
In this paper, we propose an intermediate solution — one that allows for personalized, high-quality capture of a given subject, but without the need for expensive, calibrated cap-ture equipment. Our method, which we dub SunStage, uses only a handheld smartphone camera and the sun to simu-late a minimalist light stage, enabling the reconstruction of individually-tailored geometry and reflectance without spe-cialized equipment. Our capture setup only requires the user to hold the camera at arm’s length and rotate in place, al-lowing the face to be observed under varying angles of inci-dent sunlight, which causes specular highlights to move and shadows to swing across the face. This provides strong sig-nals for the reconstruction of facial geometry and spatially-varying reflectance properties. The reconstructed face and scene parameters estimated by our system can be used to realistically render the subject in new, unseen lighting con-ditions — even with complex details like self-occluding cast shadows, which are typically missing in purely image-based relighting techniques, i.e., those that do not explicitly model geometry. In addition to relighting, we also show applica-tions in view synthesis, correcting facial perspective distor-tion, and editing skin reflectance.
Our contributions include: (1) a novel capture technique for personalized facial scanning without custom equip-ment, (2) a system for optimization and disentanglement of scene parameters (geometry, materials, lighting, and camera poses) from an unaligned, handheld video, and (3) multiple portrait editing applications that produce photorealistic re-sults, using as input only a single selfie video. 2.