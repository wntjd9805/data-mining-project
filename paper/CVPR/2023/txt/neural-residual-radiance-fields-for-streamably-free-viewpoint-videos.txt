Abstract
The success of the Neural Radiance Fields (NeRFs) for modeling and free-view rendering static objects has in-spired numerous attempts on dynamic scenes. Current tech-niques that utilize neural rendering for facilitating free-view videos (FVVs) are restricted to either ofﬂine render-ing or are capable of processing only brief sequences with minimal motion.
In this paper, we present a novel tech-nique, Residual Radiance Field or ReRF, as a highly com-pact neural representation to achieve real-time FVV ren-dering on long-duration dynamic scenes. ReRF explicitly models the residual information between adjacent times-tamps in the spatial-temporal feature space, with a global coordinate-based tiny MLP as the feature decoder. Specif-ically, ReRF employs a compact motion grid along with a residual feature grid to exploit inter-frame feature similar-ities. We show such a strategy can handle large motions without sacriﬁcing quality. We further present a sequential training scheme to maintain the smoothness and the spar-sity of the motion/residual grids. Based on ReRF, we design
† The corresponding authors are Minye Wu (minye.wu@kuleuven.be) and Lan Xu (xulan1@shanghaitech.edu.cn). a special FVV codec that achieves three orders of magni-tudes compression rate and provides a companion ReRF player to support online streaming of long-duration FVVs of dynamic scenes. Extensive experiments demonstrate the effectiveness of ReRF for compactly representing dynamic radiance ﬁelds, enabling an unprecedented free-viewpoint viewing experience in speed and quality. 1.

Introduction
Photo-realistic free-viewpoint videos (FVVs) of dy-namic scenes, in particular, human performances, reduce the gap between the performer and the viewer. But the goal of producing and viewing FVVs as simple as clicking and viewing regular 2D videos on streaming platforms remains far-reaching. The challenges range from data processing and compression to streaming and rendering.
Geometry-based solutions reconstruct dynamic 3D meshes or points [14, 16], whereas image-based ones inter-polate novel views on densely transmitted footages [6, 83].
Both techniques rely on high-quality reconstructions that are often vulnerable to occlusions and textureless regions.
Recent neural advances [44, 61] bring an alternative route  
that bypasses explicit geometric reconstruction. The sem-inal work of the Neural Radiance Field (NeRF) [44] com-pactly represents a static scene in a coordinate-based multi-layer perceptron (MLP) to conduct volume rendering at photo-realism. The MLP can be viewed as an implicit fea-ture decoder from a spatially continuous feature space to the radiance output with RGB and density. However, us-ing even a moderately deep MLP can be too expensive for real-time rendering. Various extensions have hence focused on “sculpting” the feature space using smart representations to strike an intricate balance between computational speed and accuracy. Latest examples include explicit feature vol-umes [21, 57, 77], multi-scale hashing [45], codebook [59], tri-planes [8], tensors [11, 60], etc.
Although effective, by far nearly all methods are tailored to handle static scenes. In contrast, streaming dynamic radi-ance ﬁelds require using a global coordinate-based MLP to decode features from a spatial-temporally continuous fea-ture space into radiance outputs. A na¨ıve per-frame solu-tion would be to apply static methods [45,60] on a series of independent spatial feature spaces. Such schemes discard important temporal coherency, yielding low quality and in-efﬁciency for long sequences. Recent methods attempt to maintain a canonical feature space to reproduce features in each live frame by temporally warping them back into the canonical space. Various schemes to compensate for tem-poral motions have been proposed by employing implicit matching [18, 38, 48, 49, 62] or data-driven priors such as depth [73], Fourier features [67], optical ﬂow [17, 37], or skeletal/facial motion priors [28,50,69,82]. However, heavy reliance on the global canonical space makes them fragile to large motions or topology changes. The training over-head also signiﬁcantly increases according to the sequence length. Recent work [34] sets out to explore feature redun-dancy between adjacent frames but it falls short of main-taining a coherent spatial-temporal feature space.
In this paper, we present a novel neural modeling tech-nique that we call the Residual Radiance Field or ReRF as a highly compact representation of dynamic scenes, enabling high-quality FVV streaming and rendering (Fig. 1). ReRF explicitly models the residual of the radiance ﬁeld between adjacent timestamps in the spatial-temporal feature space.
Speciﬁcally, we employ a global tiny MLP to approximate radiance output of the dynamic scene in a sequential man-ner. To maintain high efﬁciency in training and inference,
ReRF models the feature space using an explicit grid rep-resentation analogous to [57]. However, ReRF only per-forms the training on the ﬁrst key frame to obtain an MLP decoder for the whole sequence and at the same time it uses the resulting grid volume as the initial feature volume. For each subsequent frame, ReRF uses a compact motion grid and a residual feature grid: the low-resolution motion grid represents the position offset from the current frame to the previous whereas a sparse residual grid is used to compen-sate for errors and newly observed regions. A major beneﬁt of such a design is that ReRF fully exploits feature similar-ities between adjacent frames where the complete feature grid of the current frame can be simply obtained from the two while avoiding the use of a global canonical space. In addition, both motion and residual grids are amenable for compression, especially for long-duration dynamic scenes.
We present a two-stage scheme to efﬁciently obtain the
ReRF from RGB videos via sequential training. In particu-lar, we introduce a novel motion pooling strategy to main-tain the smoothness and compactness of the inter-frame mo-tion grid along with sparsity regularizers to improve the compactness of ReRF. To make ReRF practical for users, we further design a ReRF-based codec that follows the traditional keyframe-based strategy, achieving three orders of magnitudes compression rate compared to per-frame-based neural representations [57]. Finally, we demonstrate a companion ReRF player suitable for conducting online streaming of long-duration FVVs of dynamic scenes. With
ReRF, a user, for the ﬁrst time, can pause, play, fast for-ward/backward, and seek on dynamic radiance ﬁelds as if viewing 2D videos, resulting in an unprecedented high-quality free-viewpoint viewing experience (see Fig. 2).
To summarize, our contributions include:
• We introduce Residual Radiance Field (ReRF), a to support streamable novel neural representation, free-viewpoint viewing of dynamic radiance ﬁelds.
• We present tailored motion and residual grids to sup-port sequential training and at the same time eliminate the need for using a global canonical space notorious for large motions. We further introduce a number of training strategies to achieve a high compression rate while maintaining high rendering quality.
• We develop a ReRF-based codec and a companion
FVV player to stream dynamic radiance ﬁelds of long sequences, with broad control functions. 2.