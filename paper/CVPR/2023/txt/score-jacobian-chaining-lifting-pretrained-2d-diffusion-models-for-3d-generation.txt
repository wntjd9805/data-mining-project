Abstract
A diffusion model learns to predict a vector field of gradi-ents. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instan-tiate to be a voxel radiance field. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and re-purposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several off-the-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale
LAION 5B dataset. 1.

Introduction
We introduce a method that converts a pretrained 2D diffusion generative model on images into a 3D generative model of radiance fields, without requiring access to any 3D data. The key insight is to interpret diffusion models as
* Equal contribution. learned predictors of a gradient field, often referred to as the score function of the data log-likelihood. We apply the chain rule on the estimated score, hence the name Score Jacobian
Chaining (SJC).
Following Hyvärinen [16], the score is defined as the gradient of the log-density function with respect to the data (rather than parameter). Diffusion models of various fam-ilies [13, 49, 50, 52] can all be interpreted [20, 23, 52] as modeling ∇x log pσ(x) i.e. the denoising score at noise level σ. For readability, we refer to the denoising score as the score. Generating a sample from a diffusion model involves repeated evaluations of the score function from large to small
σ level, so that a sample x gradually moves closer to the data manifold. It can be loosely interpreted as gradient descent, with precise control on the step sizes so that data distribution evolves to match the annealed σ level (ancestral sampler [13],
SDE and probability-flow ODE [52], etc.). While there are other perspectives to a diffusion model [13, 49], here we are primarily motivated from the viewpoint that diffusion models produce a gradient field.
A natural question to ask is whether the chain rule can be applied to the learned gradients. Consider a diffusion model on images. An image x may be parameterized by some
function f with parameters θ, i.e., x = f (θ). Applying the chain rule through the Jacobian ∂x
∂θ converts a gradient on image x into a gradient on the parameter θ. There are many potential use cases for pairing a pretrained diffusion model with different choices of f . In this work we are interested in exploring the connection between 3D and multiview 2D by choosing f to be a differentiable renderer, thus creating a 3D generative model using only pretrained 2D resources.
Many prior works [2, 58, 60] perform 3D generative mod-eling by training on 3D datasets [5, 24, 54, 59]. This ap-proach is often as challenging as it is format-ambiguous. In addition to the high data acquisition cost of 3D assets [9], there is no universal data format: point clouds, meshes, volu-metric radiance field, etc, all have computational trade-offs.
What is common to these 3D assets is that they can be ren-dered into 2D images. An inverse rendering system, or a differentiable renderer [25, 27, 30, 34, 39], provides access to the Jacobian Jπ ≜ ∂xπ
∂θ of a rendered image xπ at camera viewpoint π with respect to the underlying 3D parameteriza-tion θ. Our method uses differentiable rendering to aggregate 2D image gradients over multiple viewpoints into a 3D asset gradient, and lifts a generative model from 2D to 3D. We parameterize a 3D asset θ as a radiance field stored on voxels and choose f to be the volume rendering function.
A key technical challenge is that computing the 2D score by directly evaluating a diffusion model on a rendered image xπ leads to an out-of-distribution (OOD) problem. Gener-ally, diffusion models are trained as denoisers and have only seen noisy inputs during training. On the other hand, our method requires evaluating the denoiser on non-noisy ren-dered images from a 3D asset during optimization, and it leads to the OOD problem. To address the issue, we propose
Perturb-and-Average Scoring, an approach to estimate the score for non-noisy images.
Empirically, we first validate the effectiveness of Perturb-and-Average Scoring at solving the OOD problem and ex-plore the hyperparameter choices on a simple 2D image can-vas. Here we identify open problems on using unconditioned diffusion models trained on FFHQ and LSUN Bedroom.
Next, we use Stable Diffusion, a model pretrained on the web-scale LAION dataset to perform SJC for 3D generation, as shown in Fig. 1. Our contributions are as follows:
• We propose a method for lifting a 2D diffusion model to 3D via an application of the chain rule.
• We illustrate the challenge of OOD when using a pretrained denoiser and propose Perturb-and-Average
Scoring to resolve it.
• We point out the subtleties and open problems on ap-plying Perturb-and-Average Scoring as gradient for optimization.
• We demonstrate the effectiveness of SJC for the task of 3D text-driven generation. 2.