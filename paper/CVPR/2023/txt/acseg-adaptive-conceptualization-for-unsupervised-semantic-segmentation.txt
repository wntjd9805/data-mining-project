Abstract
Recently, self-supervised large-scale visual pre-training models have shown great promise in representing pixel-level semantic relationships, significantly promoting the de-velopment of unsupervised dense prediction tasks, e.g., un-supervised semantic segmentation (USS). The extracted re-lationship among pixel-level representations typically con-tains rich class-aware information that semantically iden-tical pixel embeddings in the representation space gather together to form sophisticated concepts. However, lever-aging the learned models to ascertain semantically con-sistent pixel groups or regions in the image is non-trivial since over/ under-clustering overwhelms the conceptualiza-tion procedure under various semantic distributions of dif-ferent images. In this work, we investigate the pixel-level semantic aggregation in self-supervised ViT pre-trained models as image Segmentation and propose the Adaptive
Conceptualization approach for USS, termed ACSeg. Con-cretely, we explicitly encode concepts into learnable proto-types and design the Adaptive Concept Generator (ACG), which adaptively maps these prototypes to informative con-cepts for each image. Meanwhile, considering the scene complexity of different images, we propose the modularity loss to optimize ACG independent of the concept number based on estimating the intensity of pixel pairs belonging to the same concept. Finally, we turn the USS task into clas-sifying the discovered concepts in an unsupervised manner.
Extensive experiments with state-of-the-art results demon-strate the effectiveness of the proposed ACSeg. 1.

Introduction
Semantic segmentation is one of the primary tasks in computer vision, which has been widely used in many do-mains, such as autonomous driving [7, 14] and medical
*Corresponding author. Project page: https://lkhl.github.io/ACSeg.
Figure 1. Comparison between existing methods and our adaptive conceptualization on finding underlying “concepts” in the pixel-level representations produced by a pre-trained model. While under-clustering just focuses on a single object and over-clustering splits objects, our adaptive conceptualization processes different images adaptively through updating the initialized prototypes with the representations for each image. imaging [12, 24, 41]. With the development of deep learn-ing and the increasing amount of data [7, 11, 28, 57], uplift-ing performance has been achieved on this task by optimiz-ing deep neural networks with pixel-level annotations [29].
However, large-scale pixel-level annotations are expensive and laborious to obtain. Different kinds of weak supervision have been explored to achieve label efficiency [37], e.g., image-level [1,49], scribble-level [27], and box-level super-vision [35]. More than this, some methods also achieve se-mantic segmentation without relying on any labels [19, 20], namely unsupervised semantic segmentation (USS).
Early approaches for USS are based on pixel-level self-supervised representation learning by introducing cross-view consistency [6,20], edge detection [19,56], or saliency prior [43]. Recently, the self-supervised ViT [4] provides a new paradigm for USS due to its property of containing se-mantic information in pixel-level representations. We make it more intuitive through Figure 1, which shows that in the representation space of an image, the pixel-level representa-tions produced by the self-supervised ViT contain underly-ing clusters. When projecting these clusters into the image, they become semantically consistent groups of pixels or re-gions representing “concepts”.
In this work, we aim to achieve USS by accurately ex-tracting and classifying these “concepts” in the pixel rep-resentation space of each image. Unlike the previous at-tempts which only consider foreground-background parti-tion [40, 44, 48] or divide each image into a fixed number of clusters [18, 32], we argue that it is crucial to consider different images distinguishably due to the complexity of various scenarios (Figure 1). We thus propose the Adaptive
Conceptualization for unsupervised semantic Segmentation (ACSeg), a framework that finds these underlying concepts adaptively for each image and achieves USS by classifying the discovered concepts in an unsupervised manner.
To achieve conceptualization, we explicitly encode con-cepts to learnable prototypes and adaptively update them for different images by a network, as shown in Figure 2. This network, named as Adaptive Concept Generator (ACG), is implemented by iteratively applying scaled dot-product at-tention [45] on the prototypes and pixel-level representa-tions in the image to be processed. Through such a struc-ture, the ACG learns to project the initial prototypes to the concept in the representation space depending on the in-put pixel-level representations. Then the concepts are ex-plicitly presented in the image as different regions by as-signing each pixel to the nearest concept in the representa-tion space. The ACG is end-to-end optimized without any annotations by the proposed modularity loss. Specifically, we construct an affinity graph on the pixel-level representa-tions and use the connection relationship of two pixels in the affinity graph to adjust the strength of assigning two pixels to the same concept, motivated by the modularity [34].
As the main part of ACSeg, the ACG achieves precise conceptualization for different images due to its adaptive-ness, which is reflected in two aspect: Firstly, it can adap-tively operate on pixel-level representations of different im-ages thanks to the dynamic update structure. Secondly, the training objective does not enforce the number of concepts, resulting in adaptive number of concepts for different im-ages. With these properties, we get accurate partition for images with different scene complexity via the concepts produced by the ACG, as shown in Figure 1(c). Therefore, in ACSeg, the semantic segmentation of an image can fi-nally be achieved by matting the corresponding regions in the image and classifying them with the help of powerful
Figure 2. Intuitive explanation for the basic idea of the ACG.
The concepts are explicitly encoded to learnable prototypes and dynamically updated according to the input pixel-level representa-tions. After update, the pixels are assigned to the nearest concept in the representation space. image-level pre-trained models.
For evaluation, we apply ACSeg on commonly used semantic segmentation datasets, including PASCAL VOC 2012 [11] and COCO-Stuff [20, 28]. The experimental results show that the proposed ACSeg surpasses previous methods on different settings of unsupervised semantic seg-mentation tasks and achieves state-of-the-art performance on the PASCAL VOC 2012 unsupervised semantic segmen-tation benchmark without post-processing and re-training.
Moreover, the visualization of the pixel-level representa-tions and the concepts shows that the ACG is applicable for decomposing images with various scene complexity. Since the ACG is fast to converge without learning new represen-tations and the concept classifier is employed in a zero-shot manner, we draw the proposed ACSeg as a generalizable method which is easy to modify and adapt to a wide range of unsupervised image understanding. 2.