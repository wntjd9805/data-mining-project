Abstract
The missing modality issue is critical but non-trivial to be solved by multi-modal models. Current methods aim-ing to handle the missing modality problem in multi-modal tasks, either deal with missing modalities only during eval-uation or train separate models to handle specific missing modality settings. In addition, these models are designed for specific tasks, so for example, classification models are not easily adapted to segmentation tasks and vice versa. In this paper, we propose the Shared-Specific Feature Mod-elling (ShaSpec) method that is considerably simpler and more effective than competing approaches that address the issues above. ShaSpec is designed to take advantage of all available input modalities during training and evaluation by learning shared and specific features to better represent the input data. This is achieved from a strategy that relies on auxiliary tasks based on distribution alignment and do-main classification, in addition to a residual feature fusion procedure. Also, the design simplicity of ShaSpec enables its easy adaptation to multiple tasks, such as classification and segmentation. Experiments are conducted on both med-ical image segmentation and computer vision classification, with results indicating that ShaSpec outperforms competing methods by a large margin. For instance, on BraTS2018,
ShaSpec improves the SOTA by more than 3% for enhanc-ing tumour, 5% for tumour core and 3% for whole tumour.1 1.

Introduction
Recently, multi-modal learning has attracted much atten-tion by research and industry communities in both computer vision and medical image analysis. Audio, images and short videos are becoming common types of media being used for multiple types model prediction in many different appli-cations, such as sound source localisation [6], self-driving vehicles [32] and vision-and-language applications [28,33].
Similarly, in medical domain, combining different modali-ties to improve diagnosis accuracy has become increasingly important [9, 29]. For instance, Magnetic Resonance Imag-ing (MRI) is a common tool for brain tumour detection, which does not depend only on one type of MRI image, but on multiple modalities (i.e. Flair, T1, T1 contrast-enhanced and T2). However, the multi-modal methods above usu-ally require the completeness of all modalities for train-ing and evaluation, limiting their applicability in real-world with missing-modality challenges when subsets of modali-ties may be missing during training and testing.
Such challenge has motivated both computer vision [20] and medical image analysis [5, 8, 13, 25] communities to study missing-modality multi-modal approaches. Wang et al. [31] proposed an adversarial co-training network for missing modality brain tumour segmentation. They specif-ically introduced a “dedicated” training strategy defined by a series of independent models that are specifically trained to each missing situation. Another interesting point about all previous methods is that they have been specifically de-veloped either for (computer vision) classification [20] or (medical imaging) segmentation [5, 8, 13, 25, 31], making their extension to multiple tasks challenging.
In this paper, we propose a multi-model learning with missing modality approach, called Shared-Specific Feature
Modelling (ShaSpec), which can handle missing modali-ties in both training and testing, as well as dedicated train-ing and non-dedicated training2. Also, compared with pre-viously models, ShaSpec is designed with a considerably simpler and more effective architecture that explores well-understood auxiliary tasks (e.g., the distribution alignment and domain classification of multi-modal features), which enables ShaSpec to be easily adapted to classification and segmentation tasks. The main contributions are: 1This work received funding from the Australian Government through the Medical Research Futures Fund: Primary Health Care Research Data
Infrastructure Grant 2020 and from Endometriosis Australia. G.C. was supported by Australian Research Council through grant FT190100525.
• An extremely simple yet effective multi-modal learn-2Non-dedicated training refers to train one model to handle different missing modality combinations.
ing with missing modality method, called Shared-Specific Feature Modelling (ShaSpec), which is based on modelling and fusing shared and specific features to deal with missing modality in training and evaluation and with dedicated and non-dedicated training;
• To the best of our knowledge, the proposed ShaSpec is the first missing modality multi-modal approach that can be easily adapted to both classification and seg-mentation tasks given the simplicity of its design.
Our results on computer vision classification and medi-cal imaging segmentation benchmarks show that ShaSpec
Notably, com-achieves state-of-the-art performance. pared with recently proposed competing approaches on
BraTS2018, our model shows segmentation accuracy im-provements of more than 3% for enhancing tumour, 5% for tumour core and 3% for whole tumour. 2.