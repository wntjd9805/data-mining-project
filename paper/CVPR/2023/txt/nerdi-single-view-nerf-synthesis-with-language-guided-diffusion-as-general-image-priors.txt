Abstract 2D-to-3D reconstruction is an ill-posed problem, yet hu-mans are good at solving this problem due to their prior knowledge of the 3D world developed over years. Driven by this observation, we propose NeRDi, a single-view NeRF synthesis framework with general image priors from 2D diffusion models. Formulating single-view reconstruction as an image-conditioned 3D generation problem, we op-timize the NeRF representations by minimizing a diffusion loss on its arbitrary view renderings with a pretrained im-age diffusion model under the input-view constraint. We leverage off-the-shelf vision-language models and introduce a two-section language guidance as conditioning inputs to the diffusion model. This is essentially helpful for improving multiview content coherence as it narrows down the general image prior conditioned on the semantic and visual features of the single-view input image. Additionally, we introduce a geometric loss based on estimated depth maps to regular-ize the underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset show that our method can synthesize novel views with higher quality even compared to existing methods trained on this dataset. We also demon-strate our generalizability in zero-shot NeRF synthesis for in-the-wild images. 1.

Introduction
Novel view synthesis is a long-existing problem in com-puter vision and computer graphics. Recent progresses in neural rendering such as NeRFs [22] have made huge strides in novel view synthesis. Given a set of multi-view images with known camera poses, NeRFs represent a static 3D scene as a radiance ﬁeld parametrized by a neu-ral network, which enables rendering at novel views with the learned network. A line of work has been focusing on reducing the required inputs to NeRF reconstructions, ranging from dense inputs with calibrated camera poses to sparse images [11, 25, 52] with noisy or without camera
*Work done as an intern at Waymo.
poses [47]. Yet the problem of NeRF synthesis from one single view remains challenging due to its ill-posed nature, as the one-to-one correspondence from a 2D image to a 3D scene does not exist. Most existing works formulate this as a reconstruction problem and tackle it by training a net-work to predict the NeRF parameters from the input im-age [8, 52]. But they require matched multiview images with calibrated camera poses as supervision, which is in-accessible in many cases such as images from the Internet or captured by non-expert users with mobile devices. Re-cent attempts have been focused on relaxing this constraint by using unsupervised training with novel-view adversarial losses and self-consistency [21, 51]. But they still require the test cases to follow the training distribution which lim-its their generalizability. There is also work [44] that ag-gregates priors learned on synthetic multi-view datasets and transfers them to in-the-wild images using data distillation.
But they are missing ﬁne details with poor generalizability to unseen categories.
Despite the difﬁculty of 2D-to-3D mapping for comput-ers, it is actually not a difﬁcult task for human beings. Hu-mans gain knowledge of the 3D world through daily ob-servations and form a common sense of how things should look like and should not look like. Given a speciﬁc image, they can quickly narrow down their prior knowledge to the visual input. This makes humans good at solving ill-posed perception problems like single-view 3D reconstruction. In-spired by this, we propose a single-image NeRF synthe-sis framework without 3D supervision by leveraging large-scale diffusion-based 2D image generation model (Figure 1). Given an input image, we optimize for a NeRF by mini-mizing an image distribution loss for arbitrary-view render-ings with the diffusion model conditioned on the input im-age. An unconstrained image diffusion is the ‘general prior’ which is inclusive but also vague. To narrow down the prior knowledge and relate it to the input image, we design a two-section semantic feature as the conditioning input to the dif-fusion model. The ﬁrst section is the image caption which carries the overall semantics; the second is a text embed-ding extracted from the input image with textual inversion
[9], which captures additional visual cues. These two sec-tions of language guidance facilitate our realistic NeRF syn-thesis with semantic and visual coherence between different views. In addition, we introduce a geometric loss based on the estimated depth of the input view for regularizing the underlying 3D structure. Learned with all the guidance and constraints, our model is able to leverage the general image prior and perform zero-shot NeRF synthesis on single im-age inputs. Experimental results show that we can generate high quality novel views from diverse in-the-wild images.
To summarize, our key contributions are:
• We formulate single-view reconstruction as a condi-tioned 3D generation problem and propose a single-image NeRF synthesis framework without 3D supervi-sion, using 2D priors from diffusion models trained on large image datasets.
• We design a two-section semantic guidance to narrow down the general prior knowledge conditioned on the in-put image, enforcing synthesized novel views to be se-mantically and visually coherent.
• We introduce a geometric regularization term on esti-mated depth maps with 3D uncertainties.
• We validate our zero-shot novel view synthesis results on the DTU MVS [12] dataset, achieving higher quality than supervised baselines. We also demonstrate our capabil-ity of generating novel-view renderings with high visual quality on in-the-wild images. 2.