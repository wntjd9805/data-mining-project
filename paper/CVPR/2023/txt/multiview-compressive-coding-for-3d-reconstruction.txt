Abstract
A central goal of visual recognition is to understand ob-jects and scenes from a single image. 2D recognition has witnessed tremendous progress thanks to large-scale learn-ing and general-purpose representations. Comparatively, 3D poses new challenges stemming from occlusions not de-picted in the image. Prior works try to overcome these by inferring from multiple views or rely on scarce CAD models and category-specific priors which hinder scaling to novel
In this work, we explore single-view 3D recon-settings. struction by learning generalizable representations inspired by advances in self-supervised learning. We introduce a simple framework that operates on 3D points of single ob-jects or whole scenes coupled with category-agnostic large-scale training from diverse RGB-D videos. Our model, Mul-tiview Compressive Coding (MCC), learns to compress the input appearance and geometry to predict the 3D structure by querying a 3D-aware decoder. MCC’s generality and ef-ficiency allow it to learn from large-scale and diverse data sources with strong generalization to novel objects imag-ined by DALL·E 2 or captured in-the-wild with an iPhone. 1.

Introduction
Images depict objects and scenes in diverse settings.
Popular 2D visual tasks, such as object classification [7] and segmentation [32, 83], aim to recognize them on the image plane. But image planes do not capture scenes in their en-tirety. Consider Fig. 1a. The toy’s left arm is not visible in the image. This is framed by the task of 3D reconstruction: given an image, fully reconstruct the scene in 3D. 3D reconstruction is a longstanding problem in AI with applications in robotics and AR/VR. Structure from Mo-tion [18, 60] lifts images to 3D by triangulation. Recently,
NeRF [37] optimizes radiance fields to synthesize novel views. These approaches require many views of the same scene during inference and do not generalize to novel scenes from a single image. Others [16, 68] predict 3D from a sin-gle image but rely on expensive CAD supervision [5, 59].
Project page: https://mcc3d.github.io (a) MCC Overview
Input
Output
Input
Output
Input
Output (b) 3D Reconstructions by MCC
Figure 1. Multiview Compressive Coding (MCC). (a): MCC encodes an input RGB-D image and uses an attention-based model to predict the occupancy and color of query points to form the final 3D reconstruction. (b): MCC generalizes to novel objects captured with iPhones (left) or imagined by DALL·E 2 [47] (middle). It is also general – it works not only on objects but also scenes (right).
Reminiscent of generalized cylinders [40], some intro-duce object-specific priors via category-specific 3D tem-plates [25, 29, 31], pose [42] or symmetries [73]. While im-pressive, these methods cannot scale as they rely on onerous 3D annotations and category-specific priors which are not generally true. Alas large-scale learning, which has shown promising generalization results for images [45] and lan-guage [2], is largely underexplored for 3D reconstruction.
Image-based recognition is entering a new era thanks to domain-agnostic architectures, like transformers [10, 65], and large-scale category-agnostic learning [19] . Motivated by these advances, we present a scalable, general-purpose model for 3D reconstruction from a single image. We in-troduce a simple, yet effective, framework that operates di-rectly on 3D points. 3D points are general as they can cap-ture any objects or scenes and are more versatile and ef-ficient than meshes and voxels. Their generality and ef-ficiency enables large-scale category-agnostic training. In turn, large-scale training makes our 3D model effective.
Central to our approach is an input encoding and a que-riable 3D-aware decoder. The input to our model is a single
RGB-D image, which returns the visible (seen) 3D points via unprojection. Image and points are encoded with trans-formers. A new 3D point, sampled from 3D space, queries a transformer decoder conditioned on the input to predict its occupancy and its color. The decoder reconstructs the full, seen and unseen, 3D geometry, as shown in Fig. 1a. Our occupancy-based formulation, introduced in [36], frames 3D reconstruction as a binary classification problem and re-moves constraints pertinent to specialized representations (e.g., deformations of a 3D template) or a fixed resolution.
Being tasked with predicting the unseen 3D geometry of diverse objects or scenes, our decoder learns a strong 3D representation. This finding directly connects to recent ad-vances in image-based self-supervised learning and masked autoencoders (MAE) [19] which learn powerful image rep-resentations by predicting masked (unseen) image patches.
Our model inputs single RGB-D images, which are ubiq-uitous thanks to advances in hardware. Nowadays, depth sensors are found in iPhone’s front and back cameras. We show results from iPhone captures in §4 and Fig. 1b. Our decoder predicts point cloud occupancies. Supervision is sourced from multiple RGB-D views, e.g., video frames, with relative camera poses, e.g., from COLMAP [54, 55].
The posed views produce 3D point clouds which serve as proxy ground truth. These point clouds are far from “per-fect” as they are amenable to sensor and camera pose noise.
However, we show that when used at scale they are suf-ficient for our model. This suggests that 3D annotations, which are expensive to acquire, can be replaced with many
RGB-D video captures, which are much easier to collect.
We call our approach Multiview Compressive Coding (MCC), as it learns from many views, compresses appear-ance and geometry and learns a 3D-aware decoder. We demonstrate the generality of MCC by experimenting on six diverse data sources: CO3D [50], Hypersim [51], Taskon-omy [81], ImageNet [7], in-the-wild iPhone captures and
DALL·E 2 [47] generations. These datasets range from large-scale captures of more than 50 common object types, to holistic scenes, such as warehouses, auditoriums, lofts, restaurants, and imaginary objects. We compare to state-of-the-art methods, tailored for single objects [20, 50, 79] and scene reconstruction [30] and show our model’s supe-riority in both settings with a unified architecture. Enabled by MCC’s general purpose design, we show the impact of large-scale learning in terms of reconstruction quality and zero-shot generalization on novel object and scene types. 2.