Abstract
Human-centric visual tasks have attracted increasing research attention due to their widespread applications. In this paper, we aim to learn a general human representation from massive unlabeled human images which can benefit downstream human-centric tasks to the maximum extent.
We call this method SOLIDER, a Semantic cOntrollable seLf-supervIseD lEaRning framework. Unlike the existing self-supervised learning methods, prior knowledge from human images is utilized in SOLIDER to build pseudo semantic labels and import more semantic information into the learned representation. Meanwhile, we note that different downstream tasks always require different ratios of semantic information and appearance information. For example, human parsing requires more semantic informa-tion, while person re-identification needs more appearance information for identification purpose. So a single learned representation cannot fit for all requirements. To solve this problem, SOLIDER introduces a conditional network with a semantic controller. After the model is trained, users can send values to the controller to produce rep-resentations with different ratios of semantic information, which can fit different needs of downstream tasks. Finally,
SOLIDER is verified on six downstream human-centric visual tasks.
It outperforms state of the arts and builds new baselines for these tasks. The code is released in https://github.com/tinyvision/SOLIDER. 1.

Introduction
Human-centric visual analysis plays an important role in widespread applications, such as surveillance, sports, augmented reality, and video production.
Person re-identification [13, 14, 41, 82], attribute recognition [72, 78], person search [76, 92], pedestrian detection [3, 24, 32], human parsing [27, 53], and pose estimation [58, 90]
†Corresponding Author
Figure 1. A representation space learned by DINO [6]. Seven human images are represented in seven different colors. Each image is split into four parts according to their semantic regions, i.e., upper body (as ▲), lower body (as +), shoes (as ⋆) and background (as ×, not visualized to avoid distraction). It can be seen that different parts of a same person are closer to each other even they share different semantic meanings. have achieved considerable progress in recent years.
In another aspect, there are massive human images available in the current computer vision community. For example, even an unlabeled person re-identification dataset,
LUPerson [25, 26] (#Img≈4.18M) is 4 time larger than the
ImageNet dataset (#Img≈1M). How to use unlabeled data to build a human representation is challenging, especially when it needs to benefit various downstream tasks.
Self-supervised learning has achieved great develop-ments by using unlabeled data to learn representations.
Many pretext tasks have been designed, such as contrastive learning [6, 10, 35] and masking image modeling [2, 34, 77, 95]. Although these methods have achieved great success in learning general image representations, there is a lack of specific design targeting human-centric tasks.
Some researchers [55, 81, 97] focus to extend self-supervised learning methods on human-centric visual tasks.
They use DINO [6] with LUPerson [25, 26] dataset to build pre-trained models for person re-identification task. When applying the pre-trained models to other human-centric
tasks, such as human parsing and pedestrian detection, we usually get sub-optimal results.
It is due to the lack of semantic information in their learned representations.
As shown in Fig. 1, in the representation space learned by
DINO [6]1, different parts of a same person are gathered together due to their appearance continuity, no matter what semantic meanings they have.
As we’ve known, semantic information is as important as appearance information for human-centric visual tasks [40, 50, 96]. Therefore, we tend to train the representation with more semantic information to extend the representation to different downstream human-centric visual tasks.
In this paper, a Semantic cOntrollable seLf-supervIseD lEaRning
In SOLIDER, we framework (SOLIDER) is proposed. take advantage of prior knowledge from human images to discover semantic information, which can produce pseudo semantic labels for every token. And a token-level semantic classification pretext task is imported and supervised by these pseudo labels. With the new pretext task, we can train representation with stronger semantic information.
During the usage of our trained representation on down-stream tasks, we find that even though semantic information and appearance information are both important, different downstream tasks require different ratios of them. Adjust-ing their ratio in the representation would lead to a better performance in downstream tasks. However, as long as the pretext task is trained, the representation can not be changed in current self-supervised learning methods. Different from previous methods, we design SOLIDER as a conditional network involving a semantic controller. The controller takes a value as input and produces a latent representation.
In the usage of the pre-trained model from SOLIDER, we send a value (indicting the ratio of semantic information in the representation) to the controller which can adjust the model and output a representation with the required ratio.
In summary, our paper makes four contributions: 1) A general human representation is learned in this paper, which is used as a better pre-trained model benefiting to downstream human-centric visual tasks. 2) A semantic controllable self-supervised learning framework (SOLIDER) is proposed. It takes advantages of prior knowledge in human images to produce pseudo to train the human semantic labels, representation with more semantic information. and utilize it 3) A semantic controller is designed in SOLIDER. With the controller, the pre-trained model can generate represen-tations with various degrees of semantic information that can meet different needs of downstream tasks. 4) The effectiveness of the SOLIDER representation is verified on six downstream human-centric tasks. We believe this paper can promote the development of these human-centric tasks in computer vision community. 1MAE [34] shares a similar phenomenon as DINO [6]. 2.