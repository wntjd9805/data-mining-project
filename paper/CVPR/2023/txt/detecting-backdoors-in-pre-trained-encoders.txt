Abstract
Self-supervised learning in computer vision trains on unlabeled data, such as images or (image, text) pairs, to obtain an image encoder that learns high-quality embed-dings for input data. Emerging backdoor attacks towards encoders expose crucial vulnerabilities of self-supervised learning, since downstream classifiers (even further trained on clean data) may inherit backdoor behaviors from en-coders. Existing backdoor detection methods mainly fo-cus on supervised learning settings and cannot handle pre-trained encoders especially when input labels are not avail-able. In this paper, we propose DECREE, the first back-door detection approach for pre-trained encoders, requir-ing neither classifier headers nor input labels. We eval-uate DECREE on over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our method on im-age encoders pre-trained on ImageNet and OpenAI’s CLIP 400 million image-text pairs. Our method consistently has a high detection accuracy even if we have only limited or no access to the pre-training dataset. Code is available at https://github.com/GiantSeaweed/DECREE. 1.

Introduction
Self-supervised learning (SSL), specifically contrastive learning [5, 10, 15], is becoming increasingly popular as it does not require labeling training data that entails substantial manual efforts [12] and yet can provide close to the state-of-the-art performance. It has a wide range of application scenarios, e.g., similarity-based search [18], linear probe [1], and zero-shot classification [4, 24, 25]. Similarity-based search queries data based on their semantic similarity. Linear probe utilizes an encoder trained by contrastive learning to project inputs to an embedding space, and then trains a linear classifier on top of the encoder to map embeddings to downstream classification labels. Zero-shot classification trains an image encoder and a text encoder (by contrastive
Figure 1. Illustration of Backdoor Attack on Self-Supervised Learn-ing (SSL). The adversary first injects backdoor into a clean encoder and launches attack when the backdoored encoder is leveraged to train downstream tasks. The backdoored encoder produces similar embeddings for the attack target and any input image with trigger, causing misbehaviors in downstream applications. learning) that map images and texts to the same embedding space. The similarity of the two embeddings from an image and a piece of text is used for prediction.
The performance of SSL heavily relies on the large amount of unlabeled data, which indicates high computa-tional cost. Regular users hence tend to employ pre-trained encoders published online by third parties. Such a produc-tion chain provides opportunities for adversaries to implant malicious behaviors. Particularly, backdoor attack or trojan attack [8, 13, 32] injects backdoors in machine learning models, which can only be activated (causing targeted misclassification) by stamping a specific pattern, called trigger, to an input sample. It is highly stealthy as the back-doored/trojaned model functions normally on clean inputs.
While existing backdoor attacks mostly focus on classi-fiers in the supervised learning setting, where the attacker in-duces the model to predict the target label for inputs stamped with the trigger, recent studies demonstrate the feasibility of conducting backdoor attacks in SSL scenarios [3, 20, 46].
Figure 1 illustrates a typical backdoor attack on image en-coders in SSL. The adversary chooses an attack target so that the backdoored encoder produces similar embeddings for any input image with trigger and the attack target. The attack target can be an image (chosen from some dataset or downloaded from the Internet), or text captions. Text captions are compositions of a label text and prompts, where the label text usually denotes “{class name}”, like “truck”,
“ship”, “bird”, etc. For example, in Figure 1, the adversary could choose a “truck” image or a text caption “a photo of truck” as the attack target. After encoder poisoning and downstream classifier training, the classifier tends to predict the label of the attack target when the trigger is present. As shown in Figure 1, when the attack target is a truck image and the encoder is used for linear probe, the classifier inher-its the backdoor behavior from the encoder. As a result, a clean ship image can be correctly predicted by the classifier whereas a ship image stamped with the trigger is classified as “truck”. If the attack target is “a photo of truck” and the encoder is used in zero-shot prediction, a clean ship image shares a similar embedding with the text caption “a photo of ship”, causing correct prediction. In contrast, the embedding of a ship image stamped with the trigger is more similar to the embedding of “a photo of truck”, causing misprediction.
These vulnerabilities hinder the real world applications of pre-trained encoders. Existing backdoor detection methods are insufficient to defend such attacks. A possible defense method is to leverage existing backdoor detection methods focusing on supervised learning to scan downstream classi-fiers. Apart from its limited detection performance (as we will discuss later in Section 3), it cannot work properly under the setting of zero-shot classification, where there exists no concrete classifier. This calls for new defense techniques that directly detect backdoored encoders without downstream classifiers. More details regarding the limitations of existing methods can be found in Section 3.
In this paper, we propose DECREE, the first backdoor de-tection approach for pre-trained encoders in SSL. To address the insufficiency of existing detection methods, DECREE directly scans encoders. Specifically, for a subject encoder,
DECREE first searches for a minimal trigger pattern such that any inputs stamped with the trigger share similar em-beddings. The identified trigger is then utilized to decide whether the given encoder is benign or trojaned. We evaluate
DECREE on 444 encoders and it significantly outperforms existing backdoor detection techniques. We also show the effectiveness of DECREE on large size image encoders pre-trained on ImageNet [12] and OpenAI’s CLIP [40] image encoders pre-trained on 400 million uncurated (image, text) pairs. DECREE consistently achieves high detection accu-racy even when it only has limited access or no access to the pre-training dataset.
Threat Model. Our threat model is consistent with the literature [3, 20]. We only consider backdoor attacks on vision encoders. We assume the attacker has the capabilities of injecting a small portion of samples into the training set of encoders. Once the encoder is trojaned, the attacker has no control over downstream applications. Given an encoder, the defender has limited or no access to the pre-training dataset and needs to determine whether the encoder is trojaned or not. She does not have any knowledge about the attack target either. We consider injected backdoors that are static (e.g. patch backdoors) and universal (i.e. all the classes except for the target class are the victim). 2.