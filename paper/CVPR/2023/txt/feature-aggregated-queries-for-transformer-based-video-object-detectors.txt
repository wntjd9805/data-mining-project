Abstract
Video object detection needs to solve feature degradation situations that rarely happen in the image domain. One solution is to use the temporal information and fuse the features from the neighboring frames. With Transformer-based object detectors getting a better performance on the image domain tasks, recent works began to extend those methods to video object detection. However, those exist-ing Transformer-based video object detectors still follow the same pipeline as those used for classical object detec-tors, like enhancing the object feature representations by aggregation. In this work, we take a different perspective on video object detection. In detail, we improve the qualities of queries for the Transformer-based models by aggrega-tion. To achieve this goal, we first propose a vanilla query aggregation module that weighted averages the queries ac-cording to the features of the neighboring frames. Then, we extend the vanilla module to a more practical version, which generates and aggregates queries according to the features of the input frames. Extensive experimental re-sults validate the effectiveness of our proposed methods:
On the challenging ImageNet VID benchmark, when inte-grated with our proposed modules, the current state-of-the-art Transformer-based object detectors can be improved by more than 2.4% on mAP and 4.2% on AP50. Code is avail-able at https://github.com/YimingCuiCuiCui/FAQ. 1.

Introduction
Object detection is an essential yet challenging task which aims to localize and categorize all the objects of in-terest in a given image [14,50,98]. With the development of deep learning, extraordinary processes have been achieved in static image object detection [3, 14, 22, 42, 47, 71]. Exist-ing object detectors can be mainly divided into three cate-gories: two-stage [3,29,32,46,65], one-stage [47,52,57,62– 64, 72, 73] and query-based models [4, 27, 56, 66, 71, 103].
For better performance, two-stage models generate a set of proposals and then refine the prediction results, like R-CNN families [15, 26, 32, 65]. However, these two-stage object detectors usually suffer from a low inference speed.
Therefore, one-stage object detectors are introduced to bal-ance the efficiency and performance, which directly pre-dicts the object locations and categories based on the in-put image feature maps, like YOLO series [62–64, 69] and
FCOS [72,73]. Recently, query-based object detectors have been introduced, which generate the predictions based on a series of input queries and do not require complicated post-processing pipelines like NMS [2, 55, 60]. Some typical ex-ample models are DETR series [4,56,66,103] in Figure 1(a) and Sparse R-CNN series [24, 35, 71].
With the existing approaches getting better performance on the image domain, researchers began to extend the tasks to the video domain [10, 41, 67, 75, 83, 85]. One of the most challenging issues of video object detection is handling the feature degradation caused by motion, which rarely appears in static images. Since videos provide informative tem-poral hints, post-processing-based video object detectors are proposed [1, 31, 39, 40, 68]. As shown in Figure 1(c), these methods first apply image object detectors on every individual frame and then associate the prediction results.
However, since the image object detectors and the post-processing pipelines are not optimized jointly, these models usually suffer from poor performance.
Besides post-processing methods, feature-aggregation-based models [6, 13, 30, 34, 38, 82, 100, 104] are introduced to improve the feature representations for video object de-tection. These approaches first weighted average the fea-tures from the neighboring frames and then fed the aggre-gated features into the task heads for the final prediction, as shown in Figure 1(b). The pipeline for weighted averaging is usually based on feature similarity [6, 79, 82, 104, 105] or learnable networks [13, 34, 100]. Since Transformer-based models perform better on image object detection, re-searchers have begun extending them to the video domain
[34,76,100]. TransVOD families [34,100] introduce a tem-poral Transformer to the original Deformable-DETR [103] to fuse both the spatial and temporal information to handle the feature degradation issue. Similarly, PTSEFormer [76] introduces progressive feature aggregation modules to the current Transformer-based image object detectors to boost
Figure 1. The differences between the existing works and ours. (a) Transformer-based object detectors. (b) Feature-aggregation based video object detectors. (c) Post-processing based video object detectors. (d) Ours. Previous works can be divided into feature-aggregation based (b) and post-processing based (c) models. For Transformer-based models, these works either enhance the features used for detection or the prediction results of each frame. In contrast, our methods (d) pay attention to the aggregation of queries for those Transformer-based object detection models to handle the feature degradation issues. the performance. Following the TransVOD series [34, 100], we use Transformer-based object detectors as the baseline models in this work.
Unlike the existing models, we take a deeper look at the
Transformer-based object detectors and find out the unique properties of their designs. We notice that the queries of
Transformer-based object detectors play an essential role in the final prediction performance. Therefore, different from the existing works, which apply different modules to ag-gregate features (Figure 1(b)) or detection results in every single frame (Figure 1(c)), we introduce a module to aggre-gate the queries for the Transformer decoder, as shown in
Figure 1(d). The existing TransVOD families [34, 100] ini-tialize the spatial and temporal queries randomly regardless of the input frames and then aggregate them after several
Transformer layers. Unlike them, our models focus on ini-tializing the object queries and enhancing their qualities of
Transformer-based approaches for better performance. By associating and aggregating the initialization of the queries with the input frames, our models can achieve a much better performance compared to the TransVOD families [34, 100] and PTSEFormer [76]. Meanwhile, our methods can be in-tegrated into most of the existing Transformer-based image object detectors to be adaptive to the video domain task.
Our contributions are summarized as follows:
• To the best of our knowledge, we are the first to fo-cus on the initialization of queries and aggregate them based on the input features for Transformer-based video object detectors to balance the model efficiency and performance.
• We design a vanilla query aggregation (VQA) mod-ule, which enhances the query representations for the
Transformer-based object detectors to improve their performance on the video domain tasks. Then we ex-tend it to a dynamic version, which can adaptively gen-erate the initialization of queries and adjust the weights for query aggregation according to the input frames.
• Our proposed method is a plug-and-play module which can be integrated into most of the recent state-of-the-art Transformer-based object detectors for video tasks. Evaluated on the ImageNet VID bench-mark, the performance of video object detection can be improved by at least 2.0% on mAP when integrated with our proposed modules. 2.