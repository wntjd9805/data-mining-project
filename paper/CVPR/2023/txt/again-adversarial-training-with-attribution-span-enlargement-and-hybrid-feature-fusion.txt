Abstract
The deep neural networks (DNNs) trained by adversarial training (AT) usually suffered from significant robust gener-alization gap, i.e., DNNs achieve high training robustness but low test robustness. In this paper, we propose a generic method to boost the robust generalization of AT methods from the novel perspective of attribution span. To this end, compared with standard DNNs, we discover that the gen-eralization gap of adversarially trained DNNs is caused by the smaller attribution span on the input image. In other words, adversarially trained DNNs tend to focus on specific visual concepts on training images, causing its limitation on test robustness. In this way, to enhance the robustness, we propose an effective method to enlarge the learned at-tribution span. Besides, we use hybrid feature statistics for feature fusion to enrich the diversity of features. Extensive experiments show that our method can effectively improves robustness of adversarially trained DNNs, outperforming previous SOTA methods. Furthermore, we provide a the-oretical analysis of our method to prove its effectiveness.
Figure 1. A visual illustration of attribution span under ResNet-18. (a) is the original image; (b) and (c) are attribution spans of the standard model and robust model in the inference phase, re-spectively. ASC is Attribution Span Coverage; (d) is the differ-ence between the standard model and the robust model in terms of attribution span; (e) is the result after partial feature erasure of the original image using (d). 1.

Introduction
Deep neural networks (DNNs) have shown remarkable success in solving complex prediction tasks. However, re-cent studies have shown that they are particularly vulnera-ble to adversarial attacks [22], which take the form of small perturbations to the input that cause DNNs to predict in-* Zhen Xiao and Kelu Yao are the corresponding authors. correct outputs. The defense of adversarial examples has been intensively studied in recent years and several defenses against adversarial attacks have been proposed in a great deal of work [17, 18].
Among the various existing defense strategies, adversar-ial training (AT) [10, 15] has been shown to be one of the most effective defenses [16] and has received a lot of atten-tion from the research community. However, adversarially
trained DNNs typically show a significant robust general-ization gap [27]. Intuitively, there is a large gap between the training robustness and test robustness of the adversarially trained model on the adversarial examples. Some existing methods [23, 25, 27] narrow the robust generalization gap from the perspective of weight loss landscapes. Other exist-ing methods [13,24,32] enhance robust generalization from the perspective of training strategies. However, this work ignores a critical factor affecting generalization robustness, which is the learned knowledgeable representation.
Training DNNs with robust generalization is particu-larly difficult, typically possessing significantly higher sam-ple complexity [8, 29, 31] and requiring more knowledge-able [4, 19]. Compared with standard DNNs, we discover that the generalization gap of adversarially trained DNNs is caused by the smaller attribution span on the input im-age.
In other words, adversarially trained DNNs tend to focus on specific visual concepts on training images [8], causing its limitation on test robustness. Specifically, we explore the difference between the standard model (training w/o AT) and the robust model (training w/ AT) in the in-ference phase through empirical experiments. As shown in
Figure 1 (b) and Figure 1 (c), the standard model and the robust model have different attribution span for the same image in the inference phase, and the attribution span of the standard model is larger than that of the robust model in general. Through our further exploration, we find that these different spans (see Figure 1 (d)) affect the model’s deci-sion on clean data and hardly affect the model’s decision on adversarial examples. This indicates that AT enables the model to learn robust features, but ignores the features of generalization. This motivates us to design a method to en-large the attribution span to ensure that the model focuses on robust features while enhancing the focus on other features to improve the generalization ability of the robust model.
To this end , we propose a generic method to boost the robust generalization of AT from the novel perspective of attribution span. Specifically, we use the class activation mapping to obtain the attribution span of the model under real and fake labels, and mix these two spans proportion-ally to complete the enlargement of the attribution span and make the model focus on the features within this span dur-ing the training process. In addition, in order to increase the diversity of features and ensure the stable training of the model under the enlarged attribution span, we adopt the fea-ture fusion implemented by hybrid feature statistics to fur-ther improve the generalization ability of the model. Com-pared to other methods, our method can further improve the accuracy of the model on clean data and adversarial exam-ples. Meanwhile, our work provides new insights into the lack of good generalization of robust models.
Our main contributions are summarized as follows.
• We find that adversarially trained DNNs focus on a smaller span of features in the inference phase and ignores some other spans of features. These spans are generally associated with generalization ability and have little impact on robustness.
• We propose a method to boost AT, called AGAIN, which is short for Attribution Span EnlarGement and
Hybrid FeAture FusIoN. During model training, we expand the region where the model focuses its features while ensuring that it learns robust features, and com-bine feature fusion to enhance the generalization of the model over clean data and adversarial examples.
• Extensive experiments have shown that our proposed method can better improve the accuracy of the model on clean data and adversarial examples compared to state-of-the-art AT methods. Particularly, it can be eas-ily combined with other methods to further enhance the effectiveness of the method. 2.