Abstract
While most recent autonomous driving system focuses on developing perception methods on ego-vehicle sensors, people tend to overlook an alternative approach to lever-age intelligent roadside cameras to extend the perception ability beyond the visual range. We discover that the state-of-the-art vision-centric bird’s eye view detection methods have inferior performances on roadside cameras. This is because these methods mainly focus on recovering the depth regarding the camera center, where the depth difference be-*Work done during an internship at DAMO Academy, Alibaba Group.
†Corresponding Author. tween the car and the ground quickly shrinks while the dis-tance increases.
In this paper, we propose a simple yet effective approach, dubbed BEVHeight, to address this is-sue. In essence, instead of predicting the pixel-wise depth, we regress the height to the ground to achieve a distance-agnostic formulation to ease the optimization process of camera-only perception methods. On popular 3D detection benchmarks of roadside cameras, our method surpasses all previous vision-centric methods by a significant mar-gin. The code is available at https://github.com/
ADLab-AutoDrive/BEVHeight.
1.

Introduction
The rising tide of autonomous driving vehicles draws vast research attention to many 3D perception tasks, of which 3D object detection plays a critical role. While most recent works tend to only rely on ego-vehicle sensors, there are certain downsides of this line of work that hinders the perception capability under given scenarios. For example, as the mounting position of cameras is relatively close to the ground, obstacles can be easily occluded by other vehi-cles to cause severe crash damage. To this end, people have started to develop perception systems that leverage intelli-gent units on the roadside, such as cameras, to address such occlusion issue and enlarge perception range so to increase the response time in case of danger [5, 11, 28, 34, 36, 37]. To facilitate future research, there are two large-scale bench-mark datasets [36, 37] of various roadside cameras and pro-vide an evaluation of certain baseline methods.
Recently, people discover that, in contrast to directly pro-jecting the 2D images into a 3D space, leveraging a bird’s eye view (BEV) feature space can significantly improve the perception performance of vision centric system. One line of the recent approach, which constitutes the state-of-the-art camera-only method, is to generate implicitly or explicitly the depth for each pixel to ease the optimization process of bounding box regression. However, as shown in Fig. 1, we visualize the per-pixel depth of a roadside image and notice a phenomenon. Consider two points, one on the roof of a car and another on the nearest ground. If we measure the depth of these points to the camera center, namely d and d′, the difference between these depth d − d′ would drastically decrease when the car moves away from the camera. We conjecture this leads to two potential downsides: i) unlike the autonomous vehicle that has a consistent camera pose, roadside ones usually have different camera poses across the datasets, which makes regressing depth hard; ii) depth prediction is very sensitive to the change of extrinsic param-eter, where it happens quite often in the real world.
On the contrary, we notice that the height to the ground is consistent regardless of the distance between car and cam-era center. To this end, we propose a novel framework to predict the per-pixel height instead of depth, dubbed
BEVHeight. Specifically, our method firstly predicts cat-egorical height distribution for each pixel to project rich contextual feature information to the appropriate height in-terval in wedgy voxel space. Followed by a voxel pooling operation and a detection head to get the final output de-tections. Besides, we propose a hyperparameter-adjustable height sampling strategy. Note that our framework does not depend on explicit supervision like point clouds.
We conduct extensive experiments on two popular roadside perception benchmarks, DAIR-V2X [37] and
Rope3D [36]. On traditional settings where there is no disruption to the cameras, our BEVHeight achieves state-of-the-art performance and surpasses all previous methods, regardless of monocular 3D detectors or recent bird’s eye view methods by a margin of 5%. In realistic scenarios, the extrinsic parameters of these roadside units can be subject to changes due to various reasons, such as maintenance and wind blows. We simulate these scenarios following [38] and observe a severe performance drop of the BEVDepth, from 41.97% to 5.49%. Compared to these methods, we show-case the benefit of predicting the height instead of depth and achieve 26.88% improvement over the BEVDepth [15], which further evidences the robustness of our method. 2.