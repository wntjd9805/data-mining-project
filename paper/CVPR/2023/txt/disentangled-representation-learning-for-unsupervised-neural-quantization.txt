Abstract
The inverted index is a widely used data structure to
It accelerates re-avoid the infeasible exhaustive search. trieval signiﬁcantly by splitting the database into multiple disjoint sets and restricts distance computation to a small fraction of the database. Moreover, it even improves search quality by allowing quantizers to exploit the compact distri-bution of residual vector space. However, we ﬁrstly point out a problem that an existing deep learning-based quan-tizer hardly beneﬁts from the residual vector space, unlike conventional shallow quantizers. To cope with this problem, we introduce a novel disentangled representation learning for unsupervised neural quantization. Similar to the con-cept of residual vector space, the proposed method enables more compact latent space by disentangling information of the inverted index from the vectors. Experimental results on large-scale datasets conﬁrm that our method outperforms the state-of-the-art retrieval systems by a large margin. 1.

Introduction
Measuring the distances among feature vectors is a fun-damental requirement in various ﬁelds of computer vision.
One of the tasks most relevant to distance measurement is the nearest neighbor search, which ﬁnds the closest data in the database from a query. The task is especially chal-lenging in high-dimensional and large-scale databases due to huge computational costs and memory overhead.
By relaxing the complexity, Approximate Nearest
Neighbor (ANN) search is popular in practice. Recent ap-proaches for ANN typically learn the compact representa-tion by exploiting Multi-Codebook Quantization (MCQ) [2, 9, 16]. Compared to hashing-based approaches [1, 10, 12], the MCQ provides a more informative asymmetric distance estimator where the query side is not compressed. More-over, all possible distances between the query and code-words can be stored in a lookup table for efﬁciency.
Although the MCQ accelerates the distance computation with the lookup table, exhaustive search on the large-scale
*Corresponding author dataset is still prohibited. The Inverted File with Asym-metric Distance Computation (IVFADC) [16] is proposed for non-exhaustive ANN search by cooperating with the in-verted index [30]. It splits the database into multiple disjoint sets and restricts distance computations to small portions close to the query to accelerate the retrieval speed. More-over, the compactness of residual vector space between data points and inverted indices substantially enhances the quan-tization quality.
Thanks to the rapid advances in deep learning, most ar-eas of computer vision beneﬁt from its great learning capac-ity compared to shallow methods. However, the state-of-the-art methods of unsupervised quantization remain shal-low for a long time because selecting the maximum value (i.e. argmax), which is an essential operation of quanti-Inspired by a recent gener-zation, is not differentiable. ative model with discrete hidden variables [35], the Un-supervised Neural Quantization (UNQ) [23] introduced an encoder-decoder-based architecture for ANN search. The large learning capacity of deep neural architecture signiﬁ-cantly improves the retrieval quality compared to conven-tional shallow methods.
Despite the outperforming performance of the UNQ, its superiority is validated only on the exhaustive search. To verify its effectiveness on non-exhaustive search, we con-duct an experiment of non-exhaustive UNQ with an inverted index. Interestingly, we observe that this deep architecture does not beneﬁt from the residual vector space and it even harms the search quality as reported in Table 1. We hypoth-esize the reasons for this performance degradation from two perspectives. First, both the residual vector space and la-tent space of the neural network transform the data into a quantization-friendly distribution, thus deep quantizer has a scant margin to be improved by the residual space. Sec-ond, residual space sacriﬁces the distributional characteris-tics of each cluster, since the information of cluster center in the original space is removed. For conventional shallow quantizers, the drawback of residual space is obscured by its huge advantage of making a compact distribution. How-ever, deep quantizer only takes the disadvantages (informa-tion loss) from residual space without leveraging the effec-tiveness such as compactness of residuals.
In this paper, we focus on extending the application of deep architectures for non-exhaustive search. To this end, we learn a disentangled representation to harmonize a deep architecture with the inverted index, inspired by re-cent representation learning techniques for generative mod-els [8,34]. In our disentangled representation learning, both encoder and decoder get information of cluster center as an additional input. Since the information of cluster center is redundant to decoder if latent feature contains information of cluster center, the encoder is trained to remove the infor-mation of cluster centers from the latent embedding. The disentangled representation learning is similar to concept of the residual vector space that provides more compact dis-tribution by taking out the information of cluster centers.
The experimental results verify that the learning disentan-gled representation enables the neural quantization to col-laborate with inverted index and outperforms the state-of-the-art methods.
The contributions of our paper include:
• We point out that the residual encoding of the inverted index is incompatible with the neural multi-codebook quantization method.
• We propose a novel disentangled representation learn-ing for neural multi-codebook quantization to combine deep quantization and inverted index.
• The experimental results show that the proposed method outperforms the state-of-the-art retrieval sys-tems by a large margin. 2.