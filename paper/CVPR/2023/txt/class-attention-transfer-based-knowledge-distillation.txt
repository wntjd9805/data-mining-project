Abstract
Previous knowledge distillation methods have shown their impressive performance on model compression tasks, however, it is hard to explain how the knowledge they trans-ferred helps to improve the performance of the student net-work.
In this work, we focus on proposing a knowledge distillation method that has both high interpretability and competitive performance. We first revisit the structure of mainstream CNN models and reveal that possessing the capacity of identifying class discriminative regions of in-put is critical for CNN to perform classification. Further-more, we demonstrate that this capacity can be obtained and enhanced by transferring class activation maps. Based on our findings, we propose class attention transfer based knowledge distillation (CAT-KD). Different from previous
KD methods, we explore and present several properties of the knowledge transferred by our method, which not only improve the interpretability of CAT-KD but also contribute to a better understanding of CNN. While having high inter-pretability, CAT-KD achieves state-of-the-art performance on multiple benchmarks. Code is available at: https:
//github.com/GzyAftermath/CAT-KD. 1.

Introduction
Knowledge distillation (KD) transfers knowledge dis-tilled from the bigger teacher network to the smaller student network, aiming to improve the performance of the student network. Depending on the type of the transferred knowl-edge, previous KD methods can be divided into three cat-egories: based on transferring logits [3, 6, 11, 16, 33], fea-tures [2, 10, 17–19, 23, 24, 28], and attention [29]. Although
KD methods that are based on transferring logits and fea-tures have shown their promising performance [2, 33], it is hard to explain how the knowledge they transferred helps to improve the performance of the student network, due to the uninterpretability of logits and features. Relatively, the principle of attention-based KD methods is more intuitive:
*Corresponding author
Figure 1. Illustration of the converted structure. After converting the FC layer into a convolutional layer with 1×1 kernel and mov-ing the position of the global average pooling layer, CAMs can be obtained during the forward propagation. it aims at telling the student network which part of the input should it focus on during the classification, which is real-ized by forcing the student network to mimic the transferred attention maps during training. However, though previous work AT [29] has validated the effectiveness of transferring attention, it does not present what role attention plays dur-ing the classification. This makes it hard to explain why telling the trained model where should it focus could im-prove its performance on the classification mission. Be-sides, the performance of the previous attention-based KD method [29] is less competitive compared with the methods that are based on transferring logits and features [2, 33]. In this work, we focus on proposing an attention-based KD method that has higher interpretability and better perfor-mance.
Figure 2. Visualization of CAMs corresponding to categories with Top 4 prediction scores for the given image. The predicted categories and their scores are reported in the picture.
We start our work by exploring what role attention plays during classification. After revisiting the structure of the mainstream models, we find that with a little conversion (il-lustrated in Figure 1), class activation map (CAM) [34], a kind of class attention map which indicates the discrimina-tive regions of input for a specific category, can be obtained during the classification. Without changing the parame-ters and outputs, the classification process of the converted model can be viewed in two steps: (1) the model exploits its capacity to identify class discriminative regions of input and generate CAM for each category contained in the classifica-tion mission, (2) the model outputs the prediction score of each category by computing the average activation of the corresponding CAM. Considering that the converted model makes predictions by simply comparing the average activa-tion of CAMs, possessing the capacity to identify class dis-criminative regions of input is critical for CNN to perform classification. The question is: can we enhance this capac-ity by offering hints about class discriminative regions of input during training? To answer this question, we propose class attention transfer (CAT).
During CAT, the trained model is not required to predict the category of input, it is only forced to mimic the trans-ferred CAMs, which are normalized to ensure they only contain hints about class discriminative regions of input.
Through experiments with CAT, we reveal that transferring only CAMs can train a model with high accuracy on the classification task, reflecting the trained model obtains the capacity to identify class discriminative regions of input.
Besides, the performance of the trained model is influenced by the accuracy of the model offering the transferred CAMs.
This further demonstrates that the capacity of identifying class discriminative regions can be enhanced by transfer-ring more precise CAMs.
Based on our findings, we propose class attention trans-fer based knowledge distillation (CAT-KD), aiming to en-able the student network to achieve better performance by improving its capacity of identifying class discriminative regions. Different from previous KD methods transferring dark knowledge, we present why transferring CAMs to the trained model can improve its performance on the classifi-cation task. Moreover, through experiments with CAT, we reveal several interesting properties of transferring CAMs, which not only help to improve the performance and in-terpretability of CAT-KD but also contribute to a better understanding of CNN. While having high interpretability,
CAT-KD achieves state-of-the-art performance on multiple benchmarks. Overall, the main contributions of our work are shown below:
• We propose class attention transfer and use it to demonstrate that the capacity of identifying class dis-criminative regions of input, which is critical for CNN to perform classification, can be obtained and en-hanced by transferring CAMs.
• We present several interesting properties of transfer-ring CAMs, which contribute to a better understanding of CNN.
• We apply CAT to knowledge distillation and name it CAT-KD. While having high Interpretability, CAT-KD achieves state-of-the-art performance on multiple benchmarks. 2.