Abstract
The practical needs of the “right to be forgotten” and poisoned data removal call for efﬁcient machine unlearn-ing techniques, which enable machine learning models to unlearn, or to forget a fraction of training data and its lin-eage. Recent studies on machine unlearning for deep neural networks (DNNs) attempt to destroy the inﬂuence of the for-getting data by scrubbing the model parameters. However, it is prohibitively expensive due to the large dimension of the parameter space. In this paper, we refocus our attention from the parameter space to the decision space of the DNN model, and propose Boundary Unlearning, a rapid yet ef-fective way to unlearn an entire class from a trained DNN model. The key idea is to shift the decision boundary of the original DNN model to imitate the decision behavior of the model retrained from scratch. We develop two novel bound-ary shift methods, namely Boundary Shrink and Boundary
Expanding, both of which can rapidly achieve the utility and privacy guarantees. We extensively evaluate Boundary Un-learning on CIFAR-10 and Vggface2 datasets, and the re-sults show that Boundary Unlearning can effectively forget the forgetting class on image classiﬁcation and face recog-nition tasks, with an expected speed-up of 17× and 19×, respectively, compared with retraining from the scratch. 1.

Introduction
Suppose a company trains a face recognition model with your photos and deploys it as an opened API. Your photos could be stolen or inferenced by attackers via model inver-sion attack [6,18]. With the increasing awareness of protect-ing user’s privacy, a lot of privacy regulations take effect to
This work was supported in part by the National Natural Science
Foundation of China under Grants 62272183 and 62171189; by the Key
R&D Program of Hubei Province under Grant 2021BAA026; and by the special fund for Wuhan Yellow Crane Talents (Excellent Young Scholar).
The corresponding author of this paper is Chen Wang. provide you the control over your personal data. For exam-ples, the General Data Protect Regulation (GDPR) estab-lished by the European Union gives individuals “the right to be forgotten” and mandates that companies have to erase personal data once it is requested [35].
Beyond the “right to be forgotten”, data forgetting from machine learning (ML) models is also beneﬁcial when cer-tain training data becomes no longer valid, e.g., some train-ing data is manipulated by data poisoning attacks [10, 26], or outdated over time, or even identiﬁed to be mistakes after training. These practical needs call for efﬁcient machine un-learning techniques, which enable ML models to unlearn, or to forget a fraction of training data and its lineage.
In this paper, we focus on unlearning an entire class from deep neural networks (DNNs), which is useful in realis-tic scenarios like face recognition: unlearning one’s data needs to forget the entire class of one’s face images. As the DNN model retrained from scratch is the optimal un-learned model, early studies try to accelerate the retrain-ing process of deep networks [1, 11, 38], but have to inter-vene the original training process, which degenerates the model utility and increases the training time. A branch of recent researches [8, 9, 23, 27] attempt to destroy the inﬂu-ence of the forgetting data by scrubbing the model param-eters. For example, the Fisher Information Matrix (FIM) is used to locate the inﬂuence of forgetting data at the param-eter space [8, 9]. However, it is prohibitively expensive due to the large dimension of the parameter space.
In order to ﬁnd an efﬁcient unlearning approach to forget an entire class, we visualize the decision space of the re-trained DNN model and discover two key observations (c.f.
Figure 1). First, the forgetting samples spread around the decision space of the retrained DNN model, indicating that the decision boundary of the forgetting samples has been broken. Second, most of the forgetting samples move to the border of other clusters; this helps us recall the closest-to-boundary criterion [24] that samples at the border of cluster in the decision space will probably be predicted with huge uncertainty.
We summarize our major contributions as follows:
• We propose Boundary Unlearning, the ﬁrst work to un-learn an entire class from a trained DNN model by shifting the decision boundary. Compared with ex-isting studies, Boundary Unlearning neither costs too much computational resource nor intervenes the origi-nal training pipeline.
• We propose two novel methods, namely, Boundary
Shrink and Boundary Expanding, to shift the decision boundary of the forgetting class. Both methods can rapidly achieve the utility and privacy guarantees with only a few epochs of boundary adjusting.
• We conduct extensive experiments to evaluate Bound-ary Unlearning on image classiﬁcation and face recog-nition tasks. The results show that Boundary Unlearn-ing can rapidly and effectively forget the forgetting class, and outperforms four state-of-the-art techniques.
The code has been released for reproducibility2. 2.