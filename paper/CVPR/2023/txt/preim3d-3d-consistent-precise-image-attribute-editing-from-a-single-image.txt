Abstract
We study the 3D-aware image attribute editing problem in this paper, which has wide applications in practice. Re-cent methods solved the problem by training a shared en-coder to map images into a 3D generator’s latent space or by per-image latent code optimization and then edited im-ages in the latent space. Despite their promising results near the input view, they still suffer from the 3D inconsis-tency of produced images at large camera poses and im-precise image attribute editing, like affecting unspecified attributes during editing. For more efficient image inver-sion, we train a shared encoder for all images. To alle-viate 3D inconsistency at large camera poses, we propose two novel methods, an alternating training scheme and a multi-view identity loss, to maintain 3D consistency and subject identity. As for imprecise image editing, we at-tribute the problem to the gap between the latent space of real images and that of generated images. We compare the latent space and inversion manifold of GAN models and demonstrate that editing in the inversion manifold can achieve better results in both quantitative and qualitative
* Corresponding authors. evaluations. Extensive experiments show that our method produces more 3D consistent images and achieves more precise image editing than previous work. Source code and pretrained models can be found on our project page: https://mybabyyh.github.io/Preim3D/. 1.

Introduction
Benefiting from the well-disentangled latent space of
Generative Adversarial Networks (GANs)
[12], many works study GAN inversion [1, 2, 11, 28, 35, 36, 40] as well as real image editing in the latent space [14, 15, 22, 31, 32].
With the popularity of Neural Radiance Fields (NeRF) [24], some works start to incorporate it into GAN frameworks for unconditional 3D-aware image generation [6, 7, 13, 25–27, 30]. In particular, EG3D [6], the state-of-the-art 3D GAN, is able to generate high-resolution multi-view-consistent images and high-quality geometry conditioned on gaus-sian noise and camera pose. Similar to 2D GANs, 3D
GANs also have a well semantically disentangled latent space [6, 13, 21, 33], which enables realistic yet challeng-ing 3D-aware image editing.
Achieving 3D-aware image editing is much more chal-lenging because it not only has to be consistent with the input image at the input camera pose but also needs to pro-duce 3D consistent novel views. Recently, 3D-Inv [21] uses pivotal tuning inversion (PTI) [29], first finding out a piv-otal latent code and then finetuning the generator with the fixed pivotal latent code, to obtain the latent code and edit the image attributes in the latent space. IDE-3D [33] pro-poses a hybrid 3D GAN inversion approach combining tex-ture and semantic encoders and PTI technique, accelerating the optimization process by the encoded initial latent code.
Pixel2NeRF [5] is the first to achieve 3D inversion by train-ing an encoder mapping a real image to the latent space Z of π-GAN [7]. However, these methods still do not solve the problem of 3D consistency at large camera poses and precise image attribute editing. As shown in Fig. 4, some inverted images meet head distortion at large camera poses, or some unspecific attributes of edited images are modified.
In this paper, we propose a pipeline that enables PRecise
Editing in the Inversion Manifold with 3D consistency effi-ciently, termed PREIM3D. There are three goals to achieve for our framework, (i) image editing efficiently, (ii) precise inversion, which aims to maintain realism and 3D consis-tency of multiple views, and (iii) precise editing, which is to edit the desired attribute while keeping the other attributes unchanged. 3D-Inv and IDE-3D optimized a latent code for each image, which is not suitable for interactive applica-tions. Following Pixel2NeRF, we train a shared encoder for all images for efficiency.
To address precise inversion, we introduce a 3D consis-tent encoder to map a real image into the latent space W + of EG3D, and it can infer a latent code with a single forward pass. We first design a training scheme with alternating in-domain images (i.e., the generated images) and out-domain images (i.e., the real images) to help the encoder maintain the 3D consistency of the generator. We optimize the en-coder to reconstruct the input images in the out-domain im-age round. In the in-domain image round, we additionally optimize the encoder to reconstruct the ground latent code, which will encourage the distribution of the inverted latent code closer to the distribution of the original latent code of the generator. Second, to preserve the subject’s identity, we propose a multi-view identity loss calculated between the input image and novel views randomly sampled in the sur-rounding of the input camera pose.
Though many works tried to improve the editing pre-cision by modifying latent codes in Z space [31], W space [14, 17, 34], W + space [1, 1, 40], and S space [37], they all still suffer from a gap between real image editing and generated image editing because of using the editing di-rections found in the original generative latent space to edit the real images. To bridge this gap, we propose a real im-age editing subspace, which we refer to inversion manifold.
We compare the inversion manifold and the original latent space and find the distortion between the attribute editing directions. We show that the editing direction found in the inversion manifold can control the attributes of the real im-ages more precisely. To our knowledge, we are the first to perform latent code manipulation in the inversion mani-fold. Our methodology is orthogonal to some existing edit-ing methods and can improve the performance of manipu-lation in qualitative and quantitative results when integrated with them. Figure 1 shows the inversion and editing results produced by our method. Given a single real image, we achieve 3D reconstruction and precise multi-view attribute editing.
The contributions of our work can be summarized as fol-lows:
• We present an efficient image attribute editing method by training an image-shared encoder for 3D-aware generated models in this paper. To keep 3D consis-tency at large camera poses, we propose two novel methods, an alternating training scheme and a multi-view identity loss, to maintain 3D consistency and sub-ject identity.
• We compare the latent space and inversion manifold of GAN models, and demonstrate that editing in the inversion manifold can achieve better results in both quantitative and qualitative evaluations. The proposed editing space helps to close the gap between real image editing and generated image editing.
• We conduct extensive experiments, including both quantitative and qualitative, on several datasets to show the effectiveness of our methods. 2.