Abstract 1.

Introduction
Processing giga-pixel whole slide histopathology images (WSI) is a computationally expensive task. Multiple in-stance learning (MIL) has become the conventional ap-proach to process WSIs, in which these images are split into smaller patches for further processing. However, MIL-based techniques ignore explicit information about the in-dividual cells within a patch. In this paper, by defining the novel concept of shared-context processing, we designed a multi-modal Graph Transformer (AMIGO) that uses the cel-lular graph within the tissue to provide a single representa-tion for a patient while taking advantage of the hierarchical structure of the tissue, enabling a dynamic focus between cell-level and tissue-level information. We benchmarked the performance of our model against multiple state-of-the-art methods in survival prediction and showed that ours can significantly outperform all of them including hierarchical
Vision Transformer (ViT). More importantly, we show that our model is strongly robust to missing information to an extent that it can achieve the same performance with as low as 20% of the data. Finally, in two different cancer datasets, we demonstrated that our model was able to strat-ify the patients into low-risk and high-risk groups while other state-of-the-art methods failed to achieve this goal.
We also publish a large dataset of immunohistochemistry images (InUIT) containing 1,600 tissue microarray (TMA) cores from 188 patients along with their survival informa-tion, making it one of the largest publicly available datasets in this context.
Digital processing of medical images has recently at-tracted significant attention in computer vision communi-ties, and the applications of deep learning models in this domain span across various image types (e.g., histopathol-ogy images, CT scans, and MRI scans) and numerous tasks (e.g., classification, segmentation, and survival pre-diction) [6, 11, 27, 28, 30, 36, 38, 44]. The paradigm-shifting ability of these models to learn predictive features directly from raw images has presented exciting opportunities in medical imaging. This has especially become more im-portant for digitized histopathology images where each data point is a multi-gigapixel image (also referred to as a Whole
Slide Image or WSI). Unlike natural images, each WSI has high granularity at different levels of magnification and a size reaching 100,000×100,000 pixels, posing exciting challenges in computer vision.
The typical approach to cope with the computational complexities of WSI processing is to use the Multiple In-stance Learning (MIL) technique [31]. More specifically, this approach divides each slide into smaller patches (e.g., 256×256 pixels), passes them through a feature extractor, and represents the slide with an aggregation of these rep-resentations. This technique has shown promising results in a variety of tasks, including cancer subtype classifica-tion and survival prediction. However, it suffers from sev-eral major issues. Firstly, considering the high resolution of WSIs, even a non-overlapping 256×256 window gener-ates a huge number of patches. Therefore, the subsequent aggregation method of MIL has to perform either a simple pooling operation [3, 17] or a hierarchical aggregation to
add more flexibility [6]. Nevertheless, the former limits the representative power of the aggregator drastically, and the latter requires a significant amount of computational power.
Secondly, this approach is strongly dependent on the size of the dataset, which causes the over-fitting of the model in scenarios where a few data points (e.g., hundreds) are avail-able. Lastly, despite the fact that cells are the main com-ponents of the tissue, the MIL approach primarily focuses on patches, which limits the resolution of the model to a snapshot of a population of cells rather than a single cell.
Consequently, the final representation of the slide lacks the mutual interactions of individual cells.
Multiple clinical studies have strongly established that the heterogeneity of the tissue has a crucial impact on the outcome of cancer [32, 46]. For instance, high levels of im-mune infiltration in the tumor stroma were shown to cor-relate with longer survival and positive therapy response in breast cancer patients [46]. Therefore, machine learn-ing methods for histopathology image analysis are required to account for tumor heterogeneity and cell-cell interac-tions. Nonetheless, the majority of the studies in this do-main deal with a single image highlighting cell nuclei (re-gardless of cell type) and extra cellular matrix. Recently, few studies have investigated pathology images where vari-ous cell types were identified using different protein mark-ers [25, 42]. However, they still utilized a single-modal ap-proach (i.e., one cell type in an image), ignoring the multi-modal context (i.e., several cell types within the tissue) of these images.
In this study, we explore the application of graph neural networks (GNN) for the processing of cellular graphs (i.e., a graph constructed by connecting adjacent cells to each other) generated from histopathology images (Fig. 1).
In particular, we are interested in the cellular graph because it gives us the opportunity to focus on cell-level informa-tion as well as their mutual interactions. By delivering an adaptable focus at different scales, from cell level to tissue level, such information allows the model to have a multi-scale view of the tissue, whereas MIL models concentrate on patches with a preset resolution and optical magnifica-tion. The availability of cell types and their spatial loca-tion helps the model to find regions of the tissue that have more importance for its representation (e.g., tumor regions or immune cells infiltrating into tumor cells). In contrast to the expensive hierarchical pooling in MIL methods [6], the message-passing nature of GNNs offers an efficient ap-proach to process the vast scale of WSIs as a result of weight sharing across all the graph nodes. This approach also re-duces the need for a large number of WSIs during training as the number of parameters is reduced.
In this work, we introduce a spArse MultI-modal Graph transfOrmer model (AMIGO) for the representation learn-ing of histopathology images by using cells as the main
Figure 1. Cellular graph built from a 4, 000 × 4, 000 pixel TMA core stained with Ki67 biomarker. Each red point demonstrates a cell that has a positive response to Ki67 while the blue points show cells that had a negative response to this biomarker. The highlighted patches show representative areas of the tissue where the spatial distribution of cells and the structure of the tissue are different. A typical MIL method cannot capture this heterogeneity as it does not take into account the location of the patches and lacks explicit information about the specific cells present within a patch. building blocks. Starting from the cell level, our model gradually propagates information to a larger neighborhood of cells, which inherently encodes the hierarchical structure of the tissues. More importantly, in contrast to other works, we approach this problem in a multi-modal manner, where we can get a broader understanding of the tissue structure, which is quite critical for context-based tasks such as sur-vival prediction.
In particular, for a single patient, there can be multiple histopathology images available, each high-lighting cells of a certain type (by staining cells with spe-cific protein markers), and resulting in a separate cellular graph (Fig. 2). Therefore, using a multi-modal approach, we combine the cellular graphs of different modalities to-gether to obtain a unified representation for a single patient.
This also affirms our stance regarding the importance of cell type and the distinction between different cellular connec-tivity types. Aside from achieving state-of-the-art results, we notice that, surprisingly, our multi-modal approach is strongly robust to missing information, and this enables us to perform more efficient training by relying on this recon-struction ability of the network. Our work advances the frontiers of MIL, Vision Transformer (ViT), and GNNs in multiple directions:
• We introduce the first multi-modal cellular graph pro-cessing model that performs survival prediction based on the multi-modal histopathology images with shared contextual information.
• Our model eliminates the critical barriers of MIL mod-Figure 2. Overview of our proposed method. a) The Cellular graphs are first extracted from histopathology images stained with different biomarkers (e.g., CD8, CD20, and Ki67) and are fed into the encoder corresponding to their modality. The initial layer of encoders is shared, allowing further generalization, while the following layers pick up functionalities unique to each modality. The graphs at the top depict the hierarchical pooling mechanism of the model. b) The representations obtained from multiple graph instances in each modality are combined via a shared instance attention layer (shared-context processing), providing a single representation vector. c) A Transformer is used to merge the resultant vectors to create a patient-level embedding that will be used for downstream tasks such as survival prediction. els, enabling efficient training of multi-gigapixel im-ages on a single GPU and outperforming all the base-lines including ViT. It also implements the hierarchi-cal structure of Vision Transformer while keeping the number of parameters significantly lower during end-to-end training.
• We also publish a large dataset of IHC images contain-ing 1, 600 tissue microarray (TMA) cores from 188 pa-tients along with their survival information, making it one of the largest datasets in this context. 2.