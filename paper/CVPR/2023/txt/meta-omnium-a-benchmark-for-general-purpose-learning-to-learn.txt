Abstract
Meta-learning and other approaches to few-shot learn-ing are widely studied for image recognition, and are in-creasingly applied to other vision tasks such as pose estima-tion and dense prediction. This naturally raises the question of whether there is any few-shot meta-learning algorithm capable of generalizing across these diverse task types? To support the community in answering this question, we intro-duce Meta Omnium, a dataset-of-datasets spanning multi-ple vision tasks including recognition, keypoint localization, semantic segmentation and regression. We experiment with popular few-shot meta-learning baselines and analyze their ability to generalize across tasks and to transfer knowledge between them. Meta Omnium enables meta-learning re-searchers to evaluate model generalization to a much wider array of tasks than previously possible, and provides a sin-gle framework for evaluating meta-learners across a wide suite of vision applications in a consistent manner. Code and dataset are available at https://github.com/ edi-meta-learning/meta-omnium. 1.

Introduction
Meta-learning is a long-standing research area that aims to replicate the human ability to learn from a few exam-ples by learning-to-learn from a large number of learning problems [61]. This area has become increasingly impor-tant recently, as a paradigm with the potential to break the data bottleneck of traditional supervised learning [26, 70].
While the largest body of work is applied to image recog-nition, few-shot learning algorithms have now been stud-ied in most corners of computer vision, from semantic seg-mentation [37] to pose estimation [49] and beyond. Nev-ertheless, most of these applications of few-shot learning are advancing independently, with increasingly divergent application-specific methods and benchmarks. This makes it hard to evaluate whether few-shot meta-learners can solve diverse vision tasks. Importantly it also discourages the de-Figure 1. Illustration of the diverse visual domains and task types in Meta Omnium. Meta-learners are required to generalize across multiple task types, multiple datasets, and held-out datasets. velopment of meta-learners with the ability to learn-to-learn across tasks, transferring knowledge from, e.g., keypoint localization to segmentation – a capability that would be highly valuable for vision systems if achieved.
The overall trend in computer vision [20, 52] and AI
[5, 55] more generally is towards more general-purpose models and algorithms that support many tasks and ide-ally leverage synergies across them. However, it has not yet been possible to explore this trend in meta-learning, due to the lack of few-shot benchmarks spanning multiple tasks. State-of-the-art benchmarks [63, 65] for visual few-shot learning are restricted to image recognition across a handful of visual domains. There is no few-shot benchmark
Dataset
Omniglot [30] miniImageNet [66]
Meta-Dataset [63]
VTAB [80]
FSS1000 [37]
Meta-Album [65]
Meta Omnium
Num Tasks Num Domains Num Imgs Categories 1 1 7∼10 3∼19 1 10∼40 21 1623 100 43∼1500 2∼397 1000 19∼706 2∼706 32K 60K 53M 2.2M 10000 1.5M 160K 1 1 1 1 1 1 4
Size 148MB ✓
✓ 1GB
✗ 210GB
✗ 100GB 670MB ✓
✓ 15GB 3.1GB ✓
✗
✗
✗
✗
✗
✗
✓
✗
✗
✓
✓
✗
✓
✓
Lightweight Multi-Task Multi-Domain
Table 1. Feature comparison between Meta Omnium and other few-shot meta-learning benchmarks. Meta Omnium uniquely combines a rich set of tasks and visual domains with a lightweight size for accessible use. that poses the more substantial challenge [57, 77] of gen-eralizing across different tasks. We remark that the term task is used differently in few-shot meta-learning literature
[16, 26, 70] (to mean different image recognition problems, such as cat vs dog or car vs bus) and the multi-task litera-ture [20,57,74,77] (to mean different kinds of image under-standing problems, such as classification vs segmentation).
In this paper, we will use the term task in the multi-task lit-erature sense, and the term episode to refer to tasks in the meta-learning literature sense, corresponding to a support and query set.
We introduce Meta Omnium, a dataset-of-datasets span-ning multiple vision tasks including recognition, seman-tic segmentation, keypoint localization/pose estimation, and regression as illustrated in Figure 1. Specifically, Meta Om-nium provides the following important contributions: (1)
Existing benchmarks only test the ability of meta-learners to learn-to-learn within tasks such as classification [63, 65], or dense prediction [37]. Meta Omnium uniquely tests the ability of meta-learners to learn across multiple task types. (2) Meta Omnium covers multiple visual domains (from natural to medical and industrial images). (3) Meta Om-nium provides the ability to thoroughly evaluate both in-distribution and out-of-distribution generalisation. (4) Meta
Omnium has a clear hyper-parameter tuning (HPO) and model selection protocol, to facilitate future fair compari-son across current and future meta-learning algorithms. (5),
Unlike popular predecessors, [63], and despite the diversity of tasks, Meta Omnium has been carefully designed to be of moderate computational cost, making it accessible for research in modestly-resourced universities as well as large institutions. Table 1 compares Meta Omnium to other rele-vant meta-learning datasets.
We expect Meta Omnium to advance the field by encour-aging the development of meta-learning algorithms capa-ble of knowledge transfer across different tasks – as well as across learning episodes within individual tasks as is pop-ularly studied today [16, 70]. In this regard, it provides the next step of the level of a currently topical challenge of dealing with heterogeneity in meta-learning [1, 35, 63, 67].
While existing benchmarks have tested multi-domain het-erogeneity (e.g., recognition of written characters and plants within a single network) [63, 65] and shown it to be chal-lenging, Meta Omnium tests multi-task learning (e.g., char-acter recognition vs plant segmentation). This is substan-tially more ambitious when considered from the perspective of common representation learning. For example, a repre-sentation tuned for recognition might benefit from rotation invariance, while one tuned for segmentation might benefit from rotation equivariance [11, 15, 71]. Thus, in contrast to conventional within-task meta-learning benchmarks that have been criticized as relying more on common represen-tation learning than learning-to-learn [53, 62], Meta Om-nium better tests the ability of learning-to-learn since the constituent tasks require more diverse representations. 2.