Abstract
Video temporal grounding (VTG) is the task of localiz-ing a given natural language text query in an arbitrarily long untrimmed video. While the task involves untrimmed videos, all existing VTG methods leverage features from video backbones pretrained on trimmed videos. This is largely due to the lack of large-scale well-annotated VTG dataset to perform pretraining. As a result, the pretrained features lack a notion of temporal boundaries leading to the video-text alignment being less distinguishable between correct and incorrect locations. We present ProT´eG´e as the first method to perform VTG-based untrimmed pretrain-ing to bridge the gap between trimmed pretrained back-bones and downstream VTG tasks. ProT´eG´e reconfigures the HowTo100M dataset, with noisily correlated video-text pairs, into a VTG dataset and introduces a novel Video-Text
Similarity-based Grounding Module and a pretraining ob-jective to make pretraining robust to noise in HowTo100M.
Extensive experiments on multiple datasets across down-stream tasks with all variations of supervision validate that pretrained features from ProT´eG´e can significantly outper-form features from trimmed pretrained backbones on VTG. 1.

Introduction
Video temporal grounding (VTG) is the video-language multimodal task of localizing which part of an arbitrarily long untrimmed video can be best associated with a given natural language text query. VTG has a wide range of ap-plications, such as information retrieval and robotics. Fig-ure 1 shows a sample video-text pair for the VTG task and illustrates the primary challenge in grounding an uncon-strained natural language text query in a long untrimmed video, namely, the need for a fine-grained understanding of the spatio-temporal dynamics in the video.
⋆Authors with equal contribution.
This work was done as Lan Wang’s internship project at Microsoft. (a) Comparison between video-text trimmed and
Figure 1. untrimmed pretraining on grounding text Q1 and Q2 in an untrimmed video. Untrimmed video-text pretraining shows stronger grounding capability. (b) and (c) show box plots of co-sine similarity after joint video-text pretraining on trimmed and untrimmed videos respectively, between video features aligning with text (blue) and not aligning with text (red). We observe that compared to using trimmed videos (b), cosine similarities of video features aligning with text are higher and farther apart from that of video features not aligning with text when using untrimmed videos (c), thus illustrating the impact of untrimmed pretraining.
While there are multiple approaches for VTG, all exist-ing methods, to the best of our knowledge, rely on video backbones pretrained on trimmed videos (such as Kinetics
[15]) to obtain the visual features as part of their respective approaches. Such a design choice introduces a disconnect between the downstream VTG task on untrimmed videos and the trimmed videos used for pretraining the model from which video features are derived. For example, Fig 1 shows that the grounding predictions (in orange), when using a backbone jointly pretrained on trimmed videos and text, do not match adequately with the ground truth. Due to pretraining on trimmed videos, the video backbone is in-sensitive to temporal boundaries since the training objec-tive is to associate an entire trimmed video to a label/text
query [43, 44]. The backbone, therefore, does not have an explicit ability to localize, i.e., associate the given query to only the most relevant part of the long untrimmed video. As a result, the cosine similarity between video features align-ing and not aligning with the text query are indistinguish-able, as shown in Fig 1b.
Inspired by the advantage shown in other tasks where pretraining and downstream setup match [3, 7, 23, 26], we hypothesize that formulating the pretraining itself as a VTG task on untrimmed videos can improve downstream ground-ing performance. The untrimmed pretraining will equip the model with a more accurate and fine-grained understand-ing of temporal boundaries within a given untrimmed video (as evidenced by the more precise predictions in blue in
Fig. 1). We introduce ProT´eG´e, Untrimmed Pretraining for
Video Temporal Grounding by Video Temporal Ground-ing. ProT´eG´e is the first approach to formulate pretraining as a VTG task to bridge the gap between video backbones pretrained on trimmed videos and downstream VTG tasks working with untrimmed videos.
A critical challenge impeding this untrimmed pretrain-ing is the scarcity of large-scale well-annotated video grounding datasets. There are, however, datasets such as
HowTo100M [30] and Youtube-8M [1], with over a million untrimmed videos and corresponding subtitled text gener-ated via automated speech-to-text APIs. One can poten-tially employ them for untrimmed pretraining as a VTG task. However, as noted by prior methods [12, 29, 41], since the text is derived from subtitles, the video regions are only noisily-correlated with the subtitled text, rendering the util-ity of these video-text pairs for grounding a non-trivial task.
To overcome the aforementioned challenges in leverag-ing large-scale untrimmed video datasets, we first propose a novel approach to transform them into VTG datasets. Then we introduce a novel video-text similarity grounding mod-ule along with an optimization objective that allows the pre-training to be robust to the noisy video-text correlations present in these datasets.
In this work, we use ProT´eG´e with HowTo100M in par-ticular. To transform HowTo100M into a VTG dataset,
ProT´eG´e introduces aggregated subtitles to concatenate one or more subtitles to form the text query and randomly sam-ples an untrimmed video segment around the query. Ag-gregated subtitles allow ProT´eG´e to incorporate arbitrarily long text queries larger than the average 4s duration of a sin-gle subtitle. This way, we can synthesize millions of video-text grounding pairs for VTG pretraining. Using these pairs,
ProT´eG´e performs pretraining with our novel Video-Text
Similarity-based Grounding Module (VT-SGM). VT-SGM creates a 2D-proposal grid by computing the cosine similar-ity between the text query and the different temporal regions of the untrimmed video. It then learns to maximize the sim-ilarity between the query and the part that is most relevant to it. This is achieved via our novel pretraining objective that incorporates a distance-based localization loss which uses the noisy ground truth and a combination of inter-video and intra-video alignment losses. This allows the objective to balance the training via the noisy ground truth and mul-timodal video-text representation learning. We show that
ProT´eG´e is very effective for VTG as a downstream task. It significantly outperforms backbones pretrained on trimmed videos on standard datasets across all variations of supervi-sion. We summarize our contributions as, 1. We propose ProT´eG´e, the first pretraining method for-mulated as a video temporal grounding task to bridge the gap between pretraining and downstream video temporal grounding tasks in untrimmed videos. 2. We propose a novel algorithm including aggregated subtitles, a Video-Text Similarity-based Grounding
Module, and a pretraining objective to leverage large-scale untrimmed video dataset HowTo100M with noisy video-text pairs. 3. Extensive experiments on standard datasets across multiple downstream tasks with different levels of su-pervision validate that our approach significantly im-proves the performance across all benchmarks. 2.