Abstract
We introduce the novel-view acoustic synthesis (NVAS) task: given the sight and sound observed at a source view-point, can we synthesize the sound of that scene from an unseen target viewpoint? We propose a neural rendering approach: Visually-Guided Acoustic Synthesis (ViGAS) net-work that learns to synthesize the sound of an arbitrary point in space by analyzing the input audio-visual cues.
To benchmark this task, we collect two first-of-their-kind large-scale multi-view audio-visual datasets, one synthetic and one real. We show that our model successfully reasons about the spatial cues and synthesizes faithful audio on both datasets. To our knowledge, this work represents the very first formulation, dataset, and approach to solve the novel-view acoustic synthesis task, which has exciting potential applications ranging from AR/VR to art and design. Un-locked by this work, we believe that the future of novel-view synthesis is in multi-modal learning from videos. 1.

Introduction
Replaying a video recording from a new viewpoint1 has many applications in cinematography, video enhancement, and virtual reality. For example, it can be used to edit a video, simulate a virtual camera, or, given a video of a per-sonal memory, even enable users to experience a treasured moment again—not just on a 2D screen, but in 3D in a vir-tual or augmented reality, thus ‘reliving’ the moment.
While the applications are exciting, there are still many unsolved technical challenges. Recent advances in 3D re-construction and novel-view synthesis (NVS) address the problem of synthesizing new images of a given scene [32, 34, 44]. However, thus far, the view synthesis problem is concerned with creating visuals alone; the output is silent or at best naively adopts the sounds of the original video (from the “wrong” viewpoint). Without sound, the emotional and cognitive significance of the replay is severely diminished.
In this work, we address this gap and introduce the new task of novel-view acoustic synthesis (NVAS). The goal of 1We use “viewpoint” to mean a camera or microphone pose.
Figure 1. Novel-view acoustic synthesis task. Given audio-visual observations from one viewpoint and the relative target viewpoint pose, render the sound received at the target viewpoint. Note that the target is expressed as the desired pose of the microphones; the image at that pose (right) is neither observed nor synthesized. this task is to synthesize the sound in a scene from a new acoustic viewpoint, given only the visual and acoustic input from another source viewpoint in the same scene (Fig. 1).
NVAS is very different from the existing NVS task, where the goal is to reconstruct images instead of sounds, and these differences present new challenges. First, the 3D geometry of most real-life scenes changes in a limited man-ner during the recording. On the contrary, sound changes substantially over time, so the reconstruction target is highly dynamic. Secondly, visual and audio sensors are very dif-ferent. A camera matrix captures the light in a highly-directional manner, and a single image comprises a large 2D array of pixels. In contrast, sounds are recorded with one or two microphones which are at best weakly-directional, pro-viding only a coarse sampling of the sound field. Thirdly, the frequency of light waves is much higher than that of sound waves; the length of audio waves is thus larger to the point of being comparable to the size of geometric features of the scene, meaning that effects such as diffraction are often dominant, and spatial resolution is low. As a result, techniques that require spatial precision, such as triangula-tion and segmentation, are not applicable to audio. Lastly, sounds mix together, making it difficult to segment them,
and they are affected by environmental effects such as re-verberation that are distributed and largely unobservable.
While the NVS and NVAS tasks are indeed very dif-ferent, we hypothesize that NVAS is an inherently multi-In fact, vision can play an important role in modal task. achieving accurate sound synthesis. First, establishing cor-respondences between sounds and their sources as they ap-pear in images can provide essential cues for resynthesiz-ing the sounds realistically. For instance, human speech is highly directional and sounds very differently if one faces the speaker or their back, which can only be inferred from visual cues. In addition, the environment acoustics also af-fect the sound one hears as a function of the scene geom-etry, materials, and emitter/receiver locations. The same source sounds very differently if it is located in the center of a room, at the corner, or in a corridor, for example. In short, vision provides cues about space and geometry that affect sound, and are difficult to estimate from the sound alone.
In order to validate our hypothesis, we propose a novel visually-guided acoustic synthesis network that analyzes au-dio and visual features and synthesizes the audio at a target location. More specifically, the network first takes as in-put the image observed at the source viewpoint in order to infer global acoustic and geometric properties of the envi-ronment along with the bounding box of the active speaker.
The network then reasons how the speaker and scene geom-etry change in 3D based on the relative target pose with a fusion network. We inject the fused features into audio with a gated multi-modal fusion network and model the acoustic changes between viewpoints with a time-domain model.
In order to conduct our experiments on the new NVAS task, we require suitable training and benchmarking data, of which currently there is none available. To address that, we contribute two new datasets: one real (Replay-NVAS) and one synthetic (SoundSpaces-NVAS). The key feature of these datasets is to record the sight and sound of dif-ferent scenes from multiple cameras/viewpoints. Replay-NVAS contains video recordings of groups of people per-forming social activities (e.g., chatting, watching TV, doing yoga, playing instruments) from 8 surrounding viewpoints simultaneously. It contains 37 hours of highly realistic ev-eryday conversation and social interactions in one home-like environment. To our knowledge, Replay-NVAS repre-sents the first large-scale real-world dataset enabling NVAS.
This dataset would also greatly benefit many other exist-ing tasks including NVS, active speaker localization, etc.
For SoundSpaces-NVAS, we render 1.3K hours of audio-visual data based on the SoundSpaces [7] platform. Using this simulator, one can easily change the scene geometry and the positions of speakers, cameras, and microphones.
This data serves as a powerful test bed with clean ground truth for a large collection of home environments, offer-ing a good complement to Replay-NVAS. For both datasets, we capture binaural audio, which is what humans perceive with two ears. Together the datasets contain 1,337 hours of audio-visual capture, with 1,032 speakers across 121 3D scenes. Datasets are publicly available for future research. 2
We show that our model outperforms traditional signal processing approaches as well as learning-based baselines, often by a substantial margin, in a quantitative evaluation and a human study. We show qualitative examples where the model predicts acoustic changes according to the view-point changes, e.g., left channel becomes louder when the viewpoint changes from left to right.
In a nutshell, we present the first work that deals with novel-view acoustic synthesis, and contribute two large-scale datasets along with a novel neural rendering approach for solving the task. 2.