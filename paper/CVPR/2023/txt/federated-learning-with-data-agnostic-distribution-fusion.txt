Abstract
Federated learning has emerged as a promising dis-tributed machine learning paradigm to preserve data privacy. One of the fundamental challenges of federated learning is that data samples across clients are usually not independent and identically distributed (non-IID), leading to slow convergence and severe performance drop of the aggregated global model. To facilitate model aggregation on non-IID data, it is desirable to infer the unknown global distributions without violating privacy protection policy.
In this paper, we propose a novel data-agnostic distribution fusion based model aggregation method called
FedFusion to optimize federated learning with non-IID local datasets, based on which the heterogeneous clients’ data distributions can be represented by a global fusion components with distribution of several virtual different parameters and weights. We develop a Variational
AutoEncoder (VAE) method to learn the optimal parameters of the distribution fusion components based on limited statistical information extracted from the local models, and apply the derived distribution fusion model to optimize federated model aggregation with non-IID data. Extensive experiments based on various federated learning scenarios with real-world datasets show that FedFusion achieves significant performance improvement compared to the state-of-the-art. 1.

Introduction
Federated learning (FL) has emerged as a novel distributed machine learning paradigm that allows a global to be trained by deep neural network (DNN) model multiple participanting clients collaboratively.
In such a paradigm, multiple clients train their local models based on datasets generated by edge devices such as sensors and smartphones, and the server is responsible
*The corresponding author is Wenzhong Li (lwz@nju.edu.cn). to aggregate the parameters from the local models to form a global model without transferring local data to the central server. Nowadays federated learning has been drawn much attention in mobile-edge computing [21, 39] with its advantages in preserving data privacy [17, 49] and enhancing communication efficiency [30, 38, 43].
The de facto standard algorithm for federated learning is FedAvg [30], where parameters of local models are averaged element-wise with weights proportional to sizes of the client datasets. Based on FedAvg, a lot of algorithms have been proposed to improve the resource allocation fairness, communication efficiency, and convergence rate for federated learning [16, 29], which include LAG [3],
Zeno [45], AFL [31], FedMA [43], etc.
One of the fundamental challenges of federated learning is the non-IID data sampling from heterogeneous clients. In real-world federated learning scenarios, local datasets are typically non-IID, and the local models trained on them are significantly different from each other. It was reported in [48] that the accuracy of a convolutional neural network (CNN) model trained by FedAvg reduces by up to 55% for a highly skewed non-IID dataset. The work in [43] showed that the accuracy of VGG model trained with FedAvg and its variants dropped from 61% to under 50% when the client number increases from 5 to 20 on heterogeneous data.
Several efforts have been made to address the non-IID challenges.
FedProx [26] modified FedAvg by adding a dissimilarity bound on local datasets and a proximal term on the local model parameter to tackle heterogeneity.
However, it poses restrictions on the local updates to be closer to the initial global model, which may lead to model bias. Zhao et al. [48] proposed a data sharing strategy to improve training on non-IID data by creating a small subset of data to share between all clients. However, data sharing could weaken the privacy requirement of federated learning. Several works [5, 28, 32] adopted data augmentation and model bias correction to deal with non-IID data. The clustered federated learning [2, 6, 7, 46] tackled non-IID settings by partitioning client models into clusters and performed model aggregation in cluster level.
The personalized federated learning [18, 34, 35, 37] aimed to train personalized local models on non-IID data with the help of federated model aggregation. However, none of the existing works have considered to optimize federated model aggregation from the perspective of inferring the unknown global distribution based on the observed local model parameters, and yet the feasibility of global distribution inference subject to the data privacy policy of federated learning remains unexplored.
In this paper, we propose a novel data-agnostic dis-tribution fusion method called FedFusion for federated learning on non-IID data. We introduce a distribution fusion model to describe the global data distribution as a fusion of several virtual distribution components, which is ideal for representing non-IID data generated from heterogeneous clients. However, applying a distribution fusion for federated learning is not a trivial work. Due to the data privacy policy of federated learning, the local datasets are inaccessible and their distributions are unknown to the server, so it is challenging for the server to derive the distribution parameters of a fusion model without observing to the real local data samples.
To tackle these issues, we propose an efficient method to optimize the distribution fusion federated learning with variational inference. Since the local data is inaccessible to the server, our method is based on the limited statistical information embedded in the normalization layers of the
DNN models, i.e., the means and standard deviations of the feature maps (the outputs of intermediate layers). As shown in the proposed method, those information can be extracted from the local model parameters, which can be further used to infer a global distribution. Specifically, we develop a Variational AutoEncoder (VAE) method to learn the optimal parameters of distribution fusion components based on the observed information, and apply the derived parameters to optimize federated model aggregation with non-IID data. Extensive experiments based on a variety of federated learning scenarios with non-IID data show that FedFusion significantly outperforms the state-of-the-arts.
The contributions of our work are as follows.
• We propose a novel data-agnostic distribution fusion based model aggregation method called FedFusion to address the data heterogeneity problem in federated learning.
It represents the global data by a fusion model of several virtual distribution components with different fusion weights, which is ideal to describe non-IID data generated from heterogeneous clients.
• We develop a VAE method to learn the optimal parameters for the data-agnostic distribution fusion model. Without violating the privacy principle of federated learning, the proposed method uses limited statistical information embedded in DNN models to infer a target global distribution with a maximum probability. Based on the inferred parameters, an optimal model aggregation strategy can be developed for federated learning under non-IID data.
• We conduct extensive experiments using five main-stream DNN models based on four real-world datasets under non-IID conditions. Compared to FedAvg and the state-of-the-art for non-IID data (FedProx, FedMA,
IFCA, FedGroup, etc), the proposed FedFusion has better convergence and training efficiency, improving the global model’s accuracy up to 12%. 2.