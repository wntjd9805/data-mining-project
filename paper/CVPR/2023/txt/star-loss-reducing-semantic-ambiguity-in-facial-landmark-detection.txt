Abstract
Recently, deep learning-based facial landmark detection has achieved signiﬁcant improvement. However, the se-mantic ambiguity problem degrades detection performance.
Speciﬁcally, the semantic ambiguity causes inconsistent an-notation and negatively affects the model’s convergence, leading to worse accuracy and instability prediction. To solve this problem, we propose a Self-adapTive Ambiguity
Reduction (STAR) loss by exploiting the properties of se-mantic ambiguity. We ﬁnd that semantic ambiguity results in the anisotropic predicted distribution, which inspires us to use predicted distribution to represent semantic ambi-guity. Based on this, we design the STAR loss that mea-sures the anisotropism of the predicted distribution. Com-pared with the standard regression loss, STAR loss is en-couraged to be small when the predicted distribution is anisotropic and thus adaptively mitigates the impact of se-mantic ambiguity. Moreover, we propose two kinds of eigen-value restriction methods that could avoid both distribu-tion’s abnormal change and the model’s premature con-vergence. Finally, the comprehensive experiments demon-strate that STAR loss outperforms the state-of-the-art meth-ods on three benchmarks, i.e., COFW, 300W, and WFLW, with negligible computation overhead. Code is at https:
//github.com/ZhenglinZhou/STAR 1.

Introduction
Facial landmark detection, which aims to locate a group of pre-deﬁned facial landmarks from images [48, 51, 56], is a fundamental problem for many downstream tasks, includ-ing face veriﬁcation [12], face synthetic [1], and 3D face reconstruction [10, 13, 17, 46].
Thanks to the development of Convolutional Neural Net-works (CNNs) [20, 36, 39], facial landmark detection has
†Corresponding author.
⇤Equal contribution. This work was done when Zhenglin Zhou was an intern at Tencent PCG.
Figure 1. The impact of semantic ambiguity. We visualize the outputs of ﬁve models trained with the same architecture under the same experimental setting. (1) The ﬁrst row shows the predicted facial landmarks for Mr. Tony Stark, marked as red points. And the green point refers to the corresponding mean value. (2) The second row shows the results of predicted probability distribution (i.e., heatmap) from one of the trained models. improved signiﬁcantly. At ﬁrst, coordinate regression meth-ods [3, 16, 33, 47] are proposed to learn the transforma-tion between CNN features and landmark locations via fully connected layers. Recently, the research focus has been the heatmap regression methods, which have shown superiority over coordinate regression methods. The heatmap regres-sion methods [21, 24, 48] predict an intermediate heatmap for each landmark and decode the coordinates from the heatmap. But, the commonly used decoder, Argmax [55], is not differentiable and suffers from quantization error. Re-cently, some solutions have been proposed [38, 53], and the focus is on the differentiable expectation decoder: soft-Argmax [32]. With the help of soft-Argmax, the heatmap regression method has the advantage of end-to-end train-ing. So the training loss is mainly composed of a regression loss (such as
L2), which makes the model prediction ﬁt the manual annotation.
However, the manual annotation suffers from the seman-tic ambiguity problem [16, 21, 30]. Speciﬁcally, some fa-Figure 2. The overview of our framework. We use a four stacked Hourglasses (HGs) Network. To mitigate the impact of semantic ambiguity, the STAR loss is applied to each HG module. (Best view in color.) cial landmarks, especially landmarks located on face con-tour, do not have a clear and accurate deﬁnition. For exam-ple, the contour landmarks are deﬁned to evenly distribute around the face contour without a clear deﬁnition of the po-sitions [30]. It makes human annotators confused about the position, and it is inevitable to induce inconsistent and im-precise annotations. Thus, we argue that the regression loss will be misled by ambiguous annotations and degrade the model’s convergence and performance. As shown in Figure 1, training the neural network with ambiguous annotations makes the predictions for facial contour landmarks unstable and inaccurate, which will hurt the downstream task [15].
The key problem is to design a new regression loss that mit-igates the impact of semantic ambiguity.
In this paper, we propose a novel self-adaptive ambiguity reduction method, STAR loss, by fully exploiting semantic ambiguity. To this end, we explore the impact of seman-tic ambiguity on the heatmap. Typically, the distribution normalization loss forces the predicted probability distri-bution to resemble an isotropic Gaussian distribution [32].
However, as shown in Figure 1, compared with the isotropic distribution of the eye corner point, the predicted distribu-tion of the facial contour point is anisotropic. The main difference between the two landmarks is semantic ambigu-ity, which is more severe in the contour point. We infer that the semantic ambiguity is related to the anisotropic distribu-tion. When the predicted distribution of one facial landmark is anisotropic, this facial landmark has severe semantic am-biguity (leading to model unconvergence), so it is necessary to reduce its impact.
To this end, we begin our story by introducing a cus-tomized principal component analysis (PCA) that can pro-cess the discrete probability distribution, which contains three steps: weighted mean estimation, unbiased weighted covariance estimation, and eigen-decomposition. We de-compose a group of predicted distributions and visualize the corresponding principal components. The visualization re-sults show that the ﬁrst principal component is along with the face contour. Meanwhile, the ambiguous direction for contour landmarks is aligned with the face contour. There-fore, we infer that their ﬁrst principal component direction is highly consistent with their ambiguity direction.
According to this new observation, we design our STAR loss, which decomposes the prediction error into two princi-pal component directions and divides it by the correspond-ing energy value. For a facial landmark with anisotropic predicted distribution, the energy of the ﬁrst principal com-ponent is higher than the second. In this way, the error in the
ﬁrst principal component direction can be adaptively sup-pressed, thereby alleviating the impact of ambiguity anno-tation on training. However, we ﬁnd this initial version of
STAR loss suffers an abnormal energy increase, leading to premature convergence. To solve this problem, we ﬁnd that the anomaly results from that the model tends to increase the energy to minimize STAR loss. As a result, we propose two kinds of eigenvalue restriction methods to avoid STAR loss decreasing abnormally.
We evaluate the STAR loss on three widely-used bench-marks, i.e., COFW [5], 300W [35] and WFLW [48]. Ex-periments show that STAR loss indeed helps deep models achieve competitive performance compared to state-of-the-art methods. Code will be released for reproduction. 2.