Abstract
Extending the success of 2D Large Kernel to 3D percep-tion is challenging due to: 1. the cubically-increasing over-head in processing 3D data; 2. the optimization difficulties from data scarcity and sparsity. Previous work has taken the first step to scale up the kernel size from 3 × 3 × 3 to 7 × 7 × 7 by introducing block-shared weights. However, to reduce the feature variations within a block, it only employs modest block size and fails to achieve larger kernels like the 21 × 21 × 21. To address this issue, we propose a new method, called LinK, to achieve a wider-range perception receptive field in a convolution-like manner with two core designs. The first is to replace the static kernel matrix with a linear kernel generator, which adaptively provides weights only for non-empty voxels. The second is to reuse the pre-computed aggregation results in the overlapped blocks to reduce computation complexity. The proposed method suc-cessfully enables each voxel to perceive context within a range of 21 × 21 × 21. Extensive experiments on two ba-sic perception tasks, 3D object detection and 3D semantic segmentation, demonstrate the effectiveness of our method.
Notably, we rank 1st on the public leaderboard of the 3D detection benchmark of nuScenes (LiDAR track), by sim-ply incorporating a LinK-based backbone into the basic de-tector, CenterPoint. We also boost the strong segmentation baseline’s mIoU with 2.7% in the SemanticKITTI test set.
Code is available at https://github.com/MCG-NJU/LinK. 1.

Introduction
There is a consensus that a large receptive field con-tributes positively to many downstream vision tasks. For example, Transformer [2, 3] benefits a lot from the global relation with self-attention and becomes the leading topic in classification [2], segmentation [4], and detection [5].
However, self-attention is not the only route to a large re-ceptive field. Previous works like the RepLKNet [6] and
SLaK [7] investigated the potential of obtaining wide-range
*Corresponding author.
Figure 1. Comparisons among the standard kernel, the LargeK-ernel3D’s [1] block-shared kernel, and our LinK’s kernel from a generator. Instead of storing a dense kernel matrix, LinK generates sparse kernels online according to the input data. The amount of learnable parameters will not increase along with the kernel size, which enables the scaling up of larger kernel. information through a large convolutional kernel. They have achieved comparable results with the Transformer-based methods. Considering that the convolution operator is more friendly to existing chip architecture, the large kernel method is efficient in real applications. This raises an im-mediate question in the 3D perception: can the philosophy of large kernel generalize to the 3D task?
The answer is yes. LargeKernel3D [1] takes the first step and successfully achieves better metrics on both segmenta-tion and detection. Time and space consumption are core concerns during the extension since they increase cubically in the 3D task. LargeKernel3D [1] introduces a spatial shar-ing kernel to scale the 3D kernel up to 7 × 7 × 7 and restrict the rapid growth of parameters amount. However, com-pared with the 2D counterparts, which have developed the huge size of 31 × 31 [6] and even 51 × 51 [7], the 7 × 7 × 7 seems to be not large enough, hence only benefiting from limited context. There are at least two reasons to hinder its size expansion: first, although the parameter amount is un-der control, the total amounts of operation on each voxel are still increasing cubically; second, its assumption that
the outer parts can share a block-wise weight is too strong to work well in a larger block. So, enlarging the 3D kernel size effectively and efficiently is still a challenging problem.
To handle these issues, we propose a new method, called LinK, to implement a wider-range perception in a convolution-like manner. Two core designs make up the method. The first is to replace the static kernel weights with a linear kernel-generating module to provide weights only for those non-empty areas since the 3D input is very sparse. Meanwhile, this module is layer-wisely shared, which avoids the circumstances that some weights allo-cated to the blank spaces are not optimized in one itera-tion. The second is to reuse the pre-computed aggregation results in the overlapped blocks, which makes the compu-tation complexity independent of the kernel size. In other words, we can implement arbitrary kernel sizes with con-sistent overhead based on the proposed LinK. Brief com-parisons among the proposed method and other methods are depicted in Fig 1.
Extensive experiments on the public benchmarks of 3D detection and semantic segmentation tasks demonstrate the effectiveness of LinK. Notably, we achieve the 1st place on the famous 3D detection leaderboard, nuScenes (LiDAR track) [8], by simply replacing the backbone of a classic detection method with the LinK-based backbone. As for the segmentation task, we boost the strong baseline’s mIoU with 2.7% in the SemanticKITTI test split [9]. We will un-fold the details in the following sections. 2.