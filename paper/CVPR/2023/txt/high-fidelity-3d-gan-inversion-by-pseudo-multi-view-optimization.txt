Abstract
We present a high-fidelity 3D generative adversarial net-work (GAN) inversion framework that can synthesize photo-realistic novel views while preserving specific details of the input image. High-fidelity 3D GAN inversion is inherently challenging due to the geometry-texture trade-off, where overfitting to a single view input image often damages the estimated geometry during the latent optimization. To solve this challenge, we propose a novel pipeline that builds on the pseudo-multi-view estimation with visibility analysis.
We keep the original textures for the visible parts and uti-lize generative priors for the occluded parts. Extensive ex-periments show that our approach achieves advantageous reconstruction and novel view synthesis quality over prior work, even for images with out-of-distribution textures. The proposed pipeline also enables image attribute editing with the inverted latent code and 3D-aware texture modifica-tion. Our approach enables high-fidelity 3D rendering from a single image, which is promising for various applica-tions of AI-generated 3D content. The source code is at https://github.com/jiaxinxie97/HFGI3D/. 1.

Introduction
Real-world 3D-aware editing with a single 2D image fas-cinates various essential applications in computer graphics, such as virtual reality (VR), augmented reality (AR), and immersive meetings. Recent advancement in 3D GANs [12, 13,21,45] has achieved photo-realistic 3D-consistent image generation. With the GAN inversion approaches [19,51,72], which can map the images to the latent space of the pre-trained 3D-aware model, high-fidelity 3D-aware editing be-comes promising.
High-fidelity 3D-aware inversion aims to generate novel views with high-quality reconstruction and 3D consistency, but existing methods can hardly meet these two goals si-multaneously. Although current GAN inversion methods based on 2D GANs [1, 53, 54, 69] can perform 3D-related attributes (e.g., head pose) editing, the generated view is
*Joint first authors
†Joint corresponding authors inconsistent due to the lack of the underlying 3D represen-tation. When applying the existing optimization-based in-version approaches on 3D-aware GANs [12, 36], we can re-trieve high-fidelity reconstruction by overfitting to a single input image. However, different from 2D GAN inversion, the reconstruction quality of 3D GAN depends not only on the input view’s faithfulness but also on the quality of the synthesized novel views. During the optimization process, the obvious artifacts in the synthesized novel views occur with the appearance of high-fidelity details in the input view as analyzed in Sec. 3. As only a single image is available in the optimization process, the reconstruction suffers from extreme ambiguity: infinite combinations of color and den-sity can reconstruct the single input image, especially with out-of-distribution textures.
Based on the above observation, we propose our 3D-aware inversion pipeline by optimizing the reconstruction not only on the input image but also on a set of pseudo-multi-views. The pseudo views provide additional regular-ization, and thus the ambiguity is greatly reduced. Estimat-ing the pseudo views is non-trivial as it requires maintaining the texture details while also generating the occluded parts in a plausible manner, based on the input view. We first esti-mate an initial geometry and conduct a visibility analysis to solve these challenges. We directly utilize the textures from the input image for the visible parts to preserve the texture details. For the occluded parts, we use a pretrained gen-erator to synthesize the reasonable inpainted regions. With the additional supervision from the pseudo-multi-views, our approach achieves high-fidelity reconstruction results with the correct 3D geometry.
Our approach enables two types of editing: latent at-tributes editing and 3D-aware texture modification. We fol-low the previous work [55] and calculate the attribute direc-tion in the latent code space. By modifying the inverted la-tent code in a specific direction, we can control the general attribute (e.g., smile, ages for portraits as in Figure 1(b)).
Since the proposed pipeline enables inversion with out-of-distribution textures, we can achieve compelling 3D con-sistent editing by only modifying the textures of the input images (e.g., stylization or adding a tattoo in Figure 1(c)).
In summary, we propose a high-fidelity 3D GAN in-Input image
Reconstruction
Novel view 1
Novel view 2
Novel view 3
+ e l i m
S (a) High-fidelity 3D GAN inversion (b) Latent Attributes Editing (c) 3D-aware Textures Modification
+ e g
A
Figure 1. High-fidelity 3D GAN inversion results on real-world images with two types of editing ability. Our method preserves compelling details and achieves high 3D consistency. version method by pseudo-multi-view optimization given an input image. Our approach can synthesize compelling 3D-consistent novel views that are visually and geometri-cally consistent with the input image. We perform exten-sive quantitative and qualitative experiments, which demon-strate that our 3D GAN inversion approach outperforms other 2D/3D GAN inversion baselines in both photorealism and faithfulness. 2.