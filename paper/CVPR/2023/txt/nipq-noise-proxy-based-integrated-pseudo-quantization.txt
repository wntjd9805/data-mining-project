Abstract
Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low precision. Recently, pseudo-quantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudo-quantization (NIPQ) that enables unified support of pseudo-quantization for both activation and weight by integrating the idea of truncation on the pseudo-quantization frame-work. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the net-work parameters via gradient descent without STE insta-bility. According to our extensive experiments, NIPQ out-performs existing quantization algorithms in various vision and language applications by a large margin. 1.

Introduction
Neural network quantization is a representative opti-mization technique that reduces the memory footprint by storing the activation and weight in a low-precision do-main. In addition, when hardware acceleration is available (e.g., low-precision arithmetics [30, 41, 43, 48] or bit-serial operations [14, 18, 42]), it also brings a substantial perfor-mance boost. These advantages make network inference affordable in large-scale servers as well as embedded de-vices [2, 32], which has helped popularize it in various ap-plications. However, quantization has a critical disadvan-tage, quality degradation due to limited degrees of freedom.
*Equal contribution.
Figure 1. The proposed NIPQ as an alternative to QAT with STE.
The way to train the networks accurately within limited pre-cision is critical and receiving much attention these days.
To mitigate the accuracy degradation, quantization-aware training (QAT) has emerged that trains a neural net-work with quantization operators to adapt to low precision.
While the quantization operator is not differentiable, the straight-through estimator (STE) [5] allows the backprop-agation of the quantized data based on linear approxima-tion [19, 51]. This approximation works well in redundant networks with moderate precision (>4-bit). Thus, not only early studies [9, 34, 51] but also advanced ones [8, 16, 46] have proposed diverse QAT schemes based on STE and shown that popular neural networks (i.e., ResNet-18 [20]) can be quantized into 4-bit without accuracy loss.
However, STE-based QAT bypasses the approximated gradient, not the true gradient, and many studies have pointed out that it can incur instability and bias during train-ing [19,28,29,31,35,39]. For instance, PROFIT [31] points out that the instability is a major source of accuracy drop for the optimized networks (e.g., MobileNet-v2/v3 [22,36]).
More recently, an alternative training scheme, pseudo-quantization training (PQT) based on pseudo-quantization
noise (PQN), has been proposed [3, 10, 37] to address the instability of STE-based QAT. During PQT, the behavior of the quantization operator is simulated via PQN, and the learnable parameters are updated based on the proxy of quantization. While those studies are applied only to the weight, they can stabilize the training process significantly compared to QAT with STE and show the potential of PQT.
Nevertheless, the existing PQT algorithms have room for improvement. Various STE-based studies [8, 16, 51] have shown that truncation contributes significantly to reduc-ing quantization errors, but even the advanced PQT stud-ies [10, 37] use a naive min-max quantization. Integrating the truncation on the PQT framework can greatly reduce quantization error and enable us to exploit a PQT scheme for activation, which has an input-dependent distribution and requires a static quantization range. In addition, there is no theoretical support for whether PQT guarantees the optimal convergence of the quantization parameters. Intu-itive interpretation exists, but proof of whether quantization parameters are optimized after PQT has yet to be provided.
In this paper, we propose an novel PQT method (Figure 1) called Noise proxy-based Integrated Pseudo-Quantization (NIPQ) that quantizes all activation and weight based on PQN. We present a novel idea, called Noise proxy, that shares the same quantization hyper-parameters (e.g., truncation boundary and bit-width) with the existing
STE-based algorithm LSQ(+) [6,16]. However, noise proxy allows to update the quantization parameters, as well as the network parameters, via gradient descent with PQN instead of STE. Subsequently, an arbitrary network can be opti-mized in a mixed-precision representation without STE in-stability. Our key contributions are summarized as follows:
• NIPQ is the first PQT that integrates truncation in ad-dition to discretization. This extension not only further reduces the weight quantization error but also enables
PQT for activation quantization.
• NIPQ optimizes an arbitrary network into the mixed-precision with awareness of the given resource con-straint without human intervention.
• We provide theoretical analysis showing that NIPQ up-dates the quantization hyperparameters toward mini-mizing the quantization error.
• We provide extensive experimental results to validate the utility of NIPQ. It outperforms all existing mixed-precision quantization schemes by a large margin. schemes have evolved by designing the quantization func-tion to enable the optimization of the quantization param-eters via gradient descent. However, in optimized net-works such as MobileNet-v2, accuracy loss induced by STE instability has been reported for both activation [19] and weight [31]. They addressed the instability via a newly de-signed pipeline and non-linear approximation but suffered from the increased complexity and cost of QAT. In this work, NIPQ updates the quantization parameters without
STE approximation, enabling stable convergence without additional cost or complexity and, most importantly, out-performing the existing methods by a large margin.
Mixed-precision studies focus on allocating layer-wise or group-wise bit-width in consideration of the precision sensitivity of each layer to minimize accuracy drop within the given resource constraints. Various methods have been proposed, e.g., RL-based [15, 27, 45], Hessian-based [12, 13,49], and differentiable [44,46] algorithms, but RL-based and Hessian-based methods are relatively complex, requir-ing a lot of parameter tuning, and differentiable algorithms still suffer from STE approximation error.
In the present work, we reinterpret the differentiable bit-width tuning in terms of PQT framework, enabling the layer-wise bit-width tuning via gradient descent without STE instability.
Robust quantization [1, 7, 40] aims to guide the conver-gence of the network toward a smooth and flat loss surface based on additional regularization. The robustness of neu-ral networks is highly beneficial for deploying noisy devices or low-precision ALUs. In the case of noise proxy, it inher-ently improves the robustness during PQT with PQN. As far as we know, we observe for the first time that the robustness of activation quantization can be enhanced (Section 7.2).
The benefit of the PQT has been demonstrated in diverse studies [10, 37] in various perspectives. However, previous studies utilize PQT with PQN in a limited domain only for weight and have yet to provide theoretical support for the convergence of PQT corresponding to the quantization. We extend the idea of PQT to both activation and weight by integrating the idea of truncation in addition to discretiza-tion and prove that the optimization of quantization param-eters through noise proxy minimizes the actual quantiza-tion error. In addition, our work is the first to demonstrate that the PQT-based pipeline outperforms STE-based mixed-precision quantization algorithms by a large margin. 3. Simple Example Problem 2.