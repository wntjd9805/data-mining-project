Abstract 3D single object tracking plays an essential role in many applications, such as autonomous driving.
It remains a challenging problem due to the large appearance varia-tion and the sparsity of points caused by occlusion and lim-ited sensor capabilities. Therefore, contextual information across two consecutive frames is crucial for effective ob-ject tracking. However, points containing such useful in-formation are often overlooked and cropped out in existing methods, leading to insufficient use of important contextual knowledge. To address this issue, we propose CXTrack, a novel transformer-based network for 3D object tracking, which exploits ConteXtual information to improve the track-ing results. Specifically, we design a target-centric trans-former network that directly takes point features from two consecutive frames and the previous bounding box as in-put to explore contextual information and implicitly propa-gate target cues. To achieve accurate localization for ob-jects of all sizes, we propose a transformer-based local-ization head with a novel center embedding module to dis-tinguish the target from distractors. Extensive experiments on three large-scale datasets, KITTI, nuScenes and Waymo
Open Dataset, show that CXTrack achieves state-of-the-art tracking performance while running at 34 FPS. 1.

Introduction
Single Object Tracking (SOT) has been a fundamental task in computer vision for decades, aiming to keep track of a specific target across a video sequence, given only its initial status. In recent years, with the development of 3D data acquisition devices, it has drawn increasing attention for using point clouds to solve various vision tasks such as object detection [7, 12, 14, 15, 18] and object tracking [20, 29, 31â€“33]. In particular, much progress has been made on point cloud-based object tracking for its huge potential in applications such as autonomous driving [11,30]. However, it remains challenging due to the large appearance variation
*corresponding author
Figure 1. Comparison of various 3D SOT paradigms. Previous methods crop the target from the frames to specify the region of interest, which largely overlook contextual information around the target. On the contrary, our proposed CXTrack fully exploits con-textual information to improve the tracking results. of the target and the sparsity of 3D point clouds caused by occlusion and limited sensor resolution.
Existing 3D point cloud-based SOT methods can be cat-egorized into three main paradigms, namely SC3D, P2B and motion-centric, as shown in Fig. 1. As a pioneering work, SC3D [6] crops the target from the previous frame, and compares the target template with a potentially large number of candidate patches generated from the current frame, which consumes much time. To address the effi-ciency problem, P2B [20] takes the cropped target tem-plate from the previous frame as well as the complete search area in the current frame as input, propagates tar-get cues into the search area and then adopts a 3D re-gion proposal network [18] to predict the current bound-ing box. P2B reaches a balance between performance and speed. Therefore many follow-up works adopt the same paradigm [3, 8, 9, 22, 29, 31, 33]. However, both SC3D and
P2D paradigms overlook the contextual information across two consecutive frames and rely entirely on the appear-ance of the target. As mentioned in previous work [32], these methods are sensitive to appearance variation caused by occlusions and tend to drift towards intra-class distrac-tors. To this end, M2-Track [32] introduces a novel motion-centric paradigm, which directly takes point clouds from two frames without cropping as input, and then segments the target points from their surroundings. After that, these points are cropped and the current bounding box is es-timated by explicitly modeling motion between the two frames. Hence, the motion-centric paradigm still works on cropped patches that lack contextual information in later lo-calization. In short, none of these methods could fully uti-lize the contextual information around the target to predict the current bounding box, which may degrade tracking per-formance due to the existence of large appearance variation and widespread distractors.
To address the above concerns, we propose a novel transformer-based tracker named CXTrack for 3D SOT, which exploits contextual information across two consecu-tive frames to improve the tracking performance. As shown in Fig. 1, different from paradigms commonly adopted by previous methods, CXTrack directly takes point clouds from the two consecutive frames as input, specifies the tar-get of interest with the previous bounding box and predicts the current bounding box without any cropping, largely pre-serving contextual information. We first embed local geo-metric information of the two point clouds into point fea-tures using a shared backbone network. Then we integrate the targetness information into the point features accord-ing to the previous bounding box and adopt a target-centric transformer to propagate the target cues into the current frame while exploring contextual information in the sur-roundings of the target. After that, the enhanced point fea-tures are fed into a novel localization head named X-RPN to obtain the final target proposals. Specifically, X-RPN adopts a local transformer [25] to model point feature in-teractions within the target, which achieves a better balance between handling small and large objects compared with other localization heads. To distinguish the target from dis-tractors, we incorporate a novel center embedding module into X-RPN, which embeds the relative target motion be-tween two frames for explicit motion modeling. Extensive experiments on three popular tracking datasets demonstrate that CXTrack significantly outperforms the current state-of-the-art methods by a large margin while running at real-time (34 FPS) on a single NVIDIA RTX3090 GPU.
In short, our contributions can be summarized as: (1) a new paradigm for the real-time 3D SOT task, which fully exploits contextual information across consecutive frames to improve the tracking accuracy; (2) CXTrack: a transformer-based tracker that employs a target-centric transformer architecture to propagate targetness informa-tion and exploit contextual information; and (3) X-RPN: a localization head that is robust to intra-class distractors and achieves a good balance between small and large targets. 2.