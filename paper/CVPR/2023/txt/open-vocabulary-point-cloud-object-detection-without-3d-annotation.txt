Abstract
The goal of open-vocabulary detection is to identify novel objects based on arbitrary textual descriptions. In this paper, we address open-vocabulary 3D point-cloud detec-tion by a dividing-and-conquering strategy, which involves: 1) developing a point-cloud detector that can learn a gen-eral representation for localizing various objects, and 2) connecting textual and point-cloud representations to en-able the detector to classify novel object categories based
Specifically, we resort to rich im-on text prompting. age pre-trained models, by which the point-cloud detec-tor learns localizing objects under the supervision of pre-dicted 2D bounding boxes from 2D pre-trained detectors.
Moreover, we propose a novel de-biased triplet cross-modal contrastive learning to connect the modalities of image, point-cloud and text, thereby enabling the point-cloud de-tector to benefit from vision-language pre-trained models, i.e., CLIP. The novel use of image and vision-language pre-trained models for point-cloud detectors allows for open-vocabulary 3D object detection without the need for 3D annotations. Experiments demonstrate that the proposed method improves at least 3.03 points and 7.47 points over a wide range of baselines on the ScanNet and SUN RGB-D datasets, respectively. Furthermore, we provide a compre-hensive analysis to explain why our approach works. Code is available at https://github.com/lyhdet/OV-3DET 1.

Introduction
Current state-of-the-art point-cloud detectors are typi-cally trained on a limited number of classes, which fails to account for the cornucopia of object classes that exist in the real world. As a result, these detectors often fail to generalize to unseen object classes that were not present
*Equal Contribution.
â€ Corresponding Author.
Figure 1. OV-3DET takes point-cloud and text as input, and de-tects objects according to the text description. Results on two point-cloud datasets demonstrate the effectiveness of OV-3DET.
Note that RGB color is not used during inference. (we use it here only for better visualization.) in the training data. To address this issue, we focus on the under-explored problem of open-vocabulary point-cloud detection, which involves detecting 3D objects that are out-side the training vocabulary and accompanied by arbitrary text descriptions.
Achieving open-vocabulary detection requires the model to learn general representations and relate those representa-tions to text cues. However, the typical approach of collect-ing data with a larger number of classes is challenging in the point-cloud field due to the difficulty of both data collec-tion and annotation. Moreover, acquiring large-scale point-cloud-caption data is currently infeasible, which further hin-ders point-cloud detectors from learning to connect the rep-resentation with text prompts. Given these challenges, we work around the problem of open-vocabulary 3D object de-tection by leveraging existing well-established pre-trained models for images.
Indeed, the field of image/vision-language has made remarkable progress in learning general representations
[8, 12, 19, 34], enabling various applications such as de-tection [34], vision-language question answering [26], and
In particular, the CLIP pre-even image generation [22].
trained model [19] connects visual and text representa-tions, significantly advancing the development of 2D open-vocabulary detection/segmentation [10, 16, 31]. Rather than relying on large-scale point-cloud data with plentiful class labels and text pairs, we propose OV-3DET, which lever-ages advanced image/vision-language pre-trained models to achieve Open-Vocabulary 3D point-cloud DETection, as shown in Fig. 1. Notably, unlike previous open-vocabulary object detection/segmentation works [16, 31] that require supervisions of seen classes, inheriting general representa-tions from image pre-trained models allows us to directly work around open-vocabulary point-cloud detection with-out any 3D annotations of the bounding boxes and the cor-responding class labels.
Specifically, our proposed OV-3DET is a divide-and-conquer method that tackles two issues in two stages, re-spectively. In the first stage, the point-cloud detector learns to localize the unknown objects, and then in the second stage the point-cloud detector learns to name them accord-ing to the text prompts. Without cherry-picking seen cat-egories and their ground truth, we directly take 2D pre-trained detectors, e.g., Detic [34], to generate a series of 2D bounding boxes or 2D instance masks in the correspond-ing images. Note that we do not use any class labels pre-dicted by 2D detectors, and we use the coarse 2D bounding boxes or 2D instance masks to supervise 3D point-cloud detectors to learn localizing 3D objects. Then in the second stage, we make novel us of CLIP by proposing a de-biased triplet cross-modal contrastive learning method to connect the modalities among point-cloud, image, and text, such that the point-cloud detector is able to relate the objects with corresponding text descriptions. During inference, only the point-cloud detector and the text prompts are used.
Experiments demonstrate the effectiveness of the pro-posed method, outperforming a wide range of open-vocabulary-related method [33] and generazability-related methods [30,34] over at least 7.47 mAP25 and 3.03 mAP25 on the SUN RGB-D dataset and ScanNet dataset, respec-tively. More importantly, we conduct sufficient ablation studies to explore how different components influence the performance of the method, and shed light on why the pro-posed method works. 2.