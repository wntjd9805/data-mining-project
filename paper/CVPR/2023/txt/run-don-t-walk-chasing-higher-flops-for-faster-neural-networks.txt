Abstract
To design fast neural networks, many works have been focusing on reducing the number of floating-point opera-tions (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of re-duction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demon-strate that such low FLOPS is mainly due to frequent mem-ory access of the operators, especially the depthwise con-volution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further pro-pose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accu-racy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is 2.8×, 3.3×, and 2.4× faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large
FasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher in-ference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at https://github. com/JierunChen/FasterNet. 1.

Introduction
Neural networks have undergone rapid development in various computer vision tasks such as image classification, detection and segmentation. While their impressive perfor-mance has powered many applications, a roaring trend is to pursue fast neural networks with low latency and high throughput for great user experiences, instant responses, safety reasons, etc.
How to be fast? Instead of asking for more costly com-puting devices, researchers and practitioners prefer to de-sign cost-effective fast neural networks with reduced com-putational complexity, mainly measured in the number of
Figure 1. Our partial convolution (PConv) is fast and efficient by applying filters on only a few input channels while leaving the remaining ones untouched. PConv obtains lower FLOPs than the regular convolution and higher FLOPS than the depthwise/group convolution. floating-point operations (FLOPs)1. MobileNets [20, 21, 47], ShuffleNets [40, 76] and GhostNet [13], among others, leverage the depthwise convolution (DWConv) [48] and/or group convolution (GConv) [27] to extract spatial features.
However, in the effort to reduce FLOPs, the operators of-ten suffer from the side effect of increased memory access.
MicroNet [29] further decomposes and sparsifies the net-work to push its FLOPs to an extremely low level. Despite its improvement in FLOPs, this approach experiences in-efficient fragmented computation. Besides, the above net-works are often accompanied by additional data manipula-tions, such as concatenation, shuffling, and pooling, whose running time tends to be significant for tiny models.
Apart from the above pure convolutional neural net-works (CNNs), there is an emerging interest in making vi-sion transformers (ViTs) [11] and multilayer perceptrons (MLPs) architectures [57] smaller and faster. For exam-ple, MobileViTs [42, 43, 63] and MobileFormer [6] reduce the computational complexity by combining DWConv with a modified attention mechanism. However, they still suf-fer from the aforementioned issue with DWConv and also need dedicated hardware support for the modified attention mechanism. The use of advanced yet time-consuming nor-1We follow a widely adopted definition of FLOPs, as the number of multiply-adds [36, 76]. 1
Figure 2. (a) FLOPS under varied FLOPs on CPU. Many existing neural networks suffer from low computational speed issues. Their effective FLOPS are lower than the popular ResNet50. By contrast, our FasterNet attains higher FLOPS. (b) Latency under varied FLOPs on CPU. Our FasterNet obtains lower latency than others with the same amount of FLOPs. malization and activation layers may also limit their speed on devices.
All these issues together lead to the following question:
Are these “fast” neural networks really fast? To answer this, we examine the relationship between latency and FLOPs, which is captured by
Latency =
F LOP s
F LOP S
, (1) where FLOPS is short for floating-point operations per second, as a measure of the effective computational speed.
While there are many attempts to reduce FLOPs, they seldom consider optimizing FLOPS at the same time to achieve truly low latency. To better understand the situ-ation, we compare the FLOPS of typical neural networks on an Intel CPU. The results in Fig. 2 show that many ex-isting neural networks suffer from low FLOPS, and their
FLOPS is generally lower than the popular ResNet50. With such low FLOPS, these “fast” neural networks are actually not fast enough. Their reduction in FLOPs cannot be trans-lated into the exact amount of reduction in latency. In some cases, there is no improvement, and it even leads to worse latency. For example, CycleMLP-B1 [5] has half of FLOPs of ResNet50 [16] but runs more slowly (i.e., CycleMLP-B1 vs. ResNet50: 116.1ms vs. 73.0ms). Note that this dis-crepancy between FLOPs and latency has also been noticed in previous works [40, 42] but remains unresolved partially because they employ the DWConv/GConv and various data manipulations with low FLOPS. It is deemed there are no better alternatives available.
This paper aims to eliminate the discrepancy by devel-oping a simple yet fast and effective operator that maintains high FLOPS with reduced FLOPs. Specifically, we reexam-ine existing operators, particularly DWConv, in terms of the computational speed – FLOPS. We uncover that the main reason causing the low FLOPS issue is frequent memory ac-cess. We then propose a novel partial convolution (PConv) as a competitive alternative that reduces the computational redundancy as well as the number of memory access. Fig. 1 illustrates the design of our PConv. It takes advantage of redundancy within the feature maps and systematically ap-plies a regular convolution (Conv) on only a part of the in-put channels while leaving the remaining ones untouched.
By nature, PConv has lower FLOPs than the regular Conv while having higher FLOPS than the DWConv/GConv. In other words, PConv better exploits the on-device computa-tional capacity. PConv is also effective in extracting spatial features as empirically validated later in the paper.
We further introduce FasterNet, which is primarily built upon our PConv, as a new family of networks that run highly fast on various devices. In particular, our FasterNet achieves state-of-the-art performance for classification, detection, and segmentation tasks while having much lower latency and higher throughput. For example, our tiny FasterNet-T0 is 2.8×, 3.3×, and 2.4× faster than MobileViT-XXS [42] on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate on ImageNet-1k. Our large
FasterNet-L achieves 83.5% top-1 accuracy, on par with the emerging Swin-B [35], while offering 36% higher through-put on GPU and saving 37% compute time on CPU. To sum-marize, our contributions are as follows:
• We point out the importance of achieving higher
FLOPS beyond simply reducing FLOPs for faster neu-ral networks.
• We introduce a simple yet fast and effective operator called PConv, which has a high potential to replace the existing go-to choice, DWConv.
• We introduce FasterNet which runs favorably and uni-versally fast on a variety of devices such as GPU, CPU, and ARM processors.
• We conduct extensive experiments on various tasks and validate the high speed and effectiveness of our
PConv and FasterNet. 2
2.