Abstract
Research in automatic analysis of facial expressions mainly focuses on recognising the seven basic ones. How-ever, compound expressions are more diverse and represent the complexity and subtlety of our daily affective displays more accurately. Limited research has been conducted for compound expression recognition (CER), because only a few databases exist, which are small, lab controlled, im-In this paper we present an in-the-balanced and static. wild A/V database, C-EXPR-DB, consisting of 400 videos of 200K frames, annotated in terms of 13 compound expres-sions, valence-arousal emotion descriptors, action units, speech, facial landmarks and attributes. We also propose
C-EXPR-NET, a multi-task learning (MTL) method for CER and AU detection (AU-D); the latter task is introduced to en-hance CER performance. For AU-D we incorporate AU se-mantic description along with visual information. For CER we use a multi-label formulation and the KL-divergence loss. We also propose a distribution matching loss for cou-pling CER and AU-D tasks to boost their performance and alleviate negative transfer (i.e., when MT model’s perfor-mance is worse than that of at least one single-task model).
An extensive experimental study has been conducted illus-trating the excellent performance of C-EXPR-NET, vali-dating the theoretical claims. Finally, C-EXPR-NET is shown to effectively generalize its knowledge in new emo-tion recognition contexts, in a zero-shot manner. 1.

Introduction
For the past twenty years research in automatic analysis of facial behaviour was mainly limited to the recognition of the so-called six universal expressions (e.g., anger, happi-ness), plus the neutral state, influenced by the seminal work of Ekman [7]. However, the affect model based on basic ex-pressions is limited in the ability to represent the complexity and subtlety of our daily affective displays [19]. Many more facial expressions exist and are used regularly by humans.
The compound expressions are a better representation of af-fective displays in everyday interactions. Compound means that the expression category is constructed as a combination of two basic expression categories. Obviously, not all com-binations are meaningful for humans. Twelve compound expressions are most typically expressed by humans, e.g., people regularly produce a happily surprised expression and observers do not have any problem distinguishing it from an angrily surprised expression.
The design of systems capable of understanding the community perception of expressional attributes and af-fective displays is receiving increasing interest. Benefited from the great progress in deep learning research, the per-formance of expression recognition has greatly improved.
However, deep-model based methods are starved for labeled data, whereas the annotation is a highly labor intensive and time consuming process and the complexity of expression categories obscures the labelling procedure.
Initially, re-search was mainly limited to posed behavior captured in highly controlled conditions. Some representative datasets are CK+ [17], MMI [26], CFEE [6] and iCV-MEFED [18].
However, it is now widely accepted that progress in a particular application domain is significantly catalysed when a large number of datasets are collected in-the-wild (i.e., in unconstrained conditions). Thus, expression analy-sis could not only focus on spontaneous behaviors, but also on behaviours captured in unconstrained conditions. Hence, two in-the-wild databases have been generated, EmotioNet
[1] and RAF-DB [15]. These databases, although in-the-wild, are: i) very small in terms of size (RAF-DB contains around 4,000 images; EmotioNet contains around 1,500 im-ages); ii) very imbalanced (in RAF-DB one category con-sists of 1,700 images; in EmotioNet one category contains half the samples); iii) static (i.e., they contain only images); iv) lacking a training-validation-test set split.
It is evident that these databases are very small and do not contain sufficient data for both training and evaluating deep learning systems, so that the results are meaningful and illustrate good generalization. Compound expression recognition (CER) is in its infancy due to the above limita-tions. To this end, we collected the largest, diverse, in-the-wild audiovisual database, C-EXPR-DB, reliably annotated for 12 compound expressions plus a category referring to
other affective states. C-EXPR-DB is also annotated for: i) continuous dimensions of valence-arousal (how positive-negative, active-passive the emotional state is); ii) speech detection; iii) facial landmarks and bounding boxes; iv) ac-tion units (activation of facial muscles); v) facial attributes.
Recently, some works have utilized multi-task learning (MTL) for basic expression recognition and AU detection (AU-D) [2, 3]. They have shown that MTL helps improve the performance across tasks. Inspired by this, we proposed a novel methodology, C-EXPR-NET, for MTL of CER and
AU-D; we are interested in CER but we use AU-D as auxil-iary task to enhance CER performance. We utilize SEV-Net
[30] for AU-D. The FACS manual [7] provides a complete set of textual descriptions for AU definitions; such a set of
AU descriptions provides rich semantic information (about facial area/position, action, motion direction/intensity, rela-tion of AUs). The model introduces such AU semantic de-scriptions as auxiliary information and processes them via inter- and intra-transformer modules and a cross modality attention module for AU-D.
For CER we use a multi-label formulation, where the output classes are 6 (the basic expressions) and each da-tum contains annotations for 2 of these 6 categories. We use a softmax activation in the output layer of our method that tackles CER and the Kullback-Leibler divergence (KL-div) as its loss function. The concept for this formulation aligns with what compound expressions are, i.e., expres-sions that can be constructed as combinations of basic cat-egories. In addition, this formulation allows our method to be additionally trained with images annotated with the 6 ba-sic expressions (as a form of data augmentation or to learn to differentiate between basic expressions as well). This formulation deviates from traditional CER approaches that use a multi-class formulation, with the compound expres-sions being mutually exclusive classes (i.e., each datum is annotated in terms of only one compound expression).
However, when we compared the performance of our multi-task method with that of single-task (ST) methods for
CER and AU-D, we observed that MTL increased CER per-formance, but it harmed AU-D performance. Thus negative transfer occurred, as CER task dominated the training pro-cess. Inspired by [11, 12], we propose a distribution match-ing approach based on task relatedness, i.e., knowledge ex-change between CER and AU-D tasks is enabled via dis-tribution matching over their predictions. We demonstrate empirically that this distribution matching approach allevi-ates negative transfer and further boosts CER performance.
The main contributions of this work are summarized below: in-the-wild A/V
• We generate the largest, diverse, database, C-EXPR-DB, annotated for compound expres-sions, valence-arousal, AUs, facial attributes, speech de-tection, facial landmarks and bounding boxes;
• We propose the novel C-EXPR-NET, a MT method for
CER and AU-D; the latter task acts as an auxiliary one for enhancing the former task’s performance (we are the first to prove this). For AU-D, our method incorporates visual information, as well as AU descriptors (that act as auxil-iary, rich, semantic information) and processes them via inter- and intra-transformer modules and a cross-modality attention module. For CER our method uses a multi-label formulation and KL-div loss. Our method finally contains a distribution matching loss, based on task relatedness, for coupling the tasks to alleviate negative transfer and further boost their performance.
• We conduct an extensive experimental study which shows that: i) C-EXPR-NET outperforms the state-of-the-art (sota) both for CER and BER on RAF-DB, regardless if trained from scratch or pre-trained on C-EXPR-DB; ii) C-EXPR-NET outperforms the sota regardless if AU annotations are manual or automatic; iii) C-EXPR-NET can effectively generalize its knowledge in new emotion recognition contexts, in a zero-shot manner. 2.