Abstract
Confidence-based pseudo-labeling is among the domi-nant approaches in semi-supervised learning (SSL). It re-lies on including high-confidence predictions made on un-labeled data as additional targets to train the model. We propose PROTOCON, a novel SSL method aimed at the less-explored label-scarce SSL where such methods usually un-derperform. PROTOCON refines the pseudo-labels by lever-aging their nearest neighbours’ information. The neigh-bours are identified as the training proceeds using an on-line clustering approach operating in an embedding space trained via a prototypical loss to encourage well-formed clusters. The online nature of PROTOCON allows it to utilise the label history of the entire dataset in one train-ing cycle to refine labels in the following cycle without the need to store image embeddings. Hence, it can seamlessly scale to larger datasets at a low cost. Finally, PROTOCON addresses the poor training signal in the initial phase of training (due to fewer confident predictions) by introduc-ing an auxiliary self-supervised loss. It delivers significant gains and faster convergence over state-of-the-art across 5 datasets, including CIFARs, ImageNet and DomainNet. 1.

Introduction
Semi-supervised Learning (SSL) [10, 40] leverages un-labeled data to guide learning from a small amount of la-beled data; thereby, providing a promising alternative to costly human annotations.
In recent years, SSL frontiers have seen substantial advances through confidence-based pseudo-labeling [21, 22, 38, 42, 43].
In these methods, a model iteratively generates pseudo-labels for unlabeled samples which are then used as targets to train the model.
To overcome confirmation bias [1, 27] i.e., the model being biased by training on its own wrong predictions, these meth-ods only retain samples with high confidence predictions for pseudo-labeling; thus ensuring that only reliable sam-ples are used to train the model. While confidence works well in moderately labeled data regimes, it usually strug-Figure 1. PROTOCON refines a pseudo-label of a given sample by knowledge of its neighbours in a prototypical embedding space.
Neighbours are identified in an online manner using constrained
K-means clustering. Best viewed zoomed in. gles in label-scarce settings1. This is primarily because the model becomes over-confident about the more distinguish-able classes [17, 28] faster than others, leading to a collapse.
In this work, we propose PROTOCON, a novel method which addresses such a limitation in label-scarce SSL. Its key idea is to complement confidence with a label refine-ment strategy to encourage more accurate pseudo-labels.
To that end, we perform the refinement by adopting a co-training [5] framework: for each image, we obtain two dif-ferent labels and combine them to obtain our final pseudo-label. The first is the model’s softmax prediction, whereas the second is an aggregate pseudo-label describing the im-age’s neighbourhood based on the pseudo-labels of other images in its vicinity. However, a key requirement for the success of co-training is to ensure that the two labels are obtained using sufficiently different image representa-tions [40] to allow the model to learn based on their dis-agreements. As such, we employ a non-linear projection to map our encoder’s representation into a different embed-1We denote settings with less than 10 images per class as “label-scarce.”
ding space. We train this projector jointly with the model with a prototypical consistency objective to ensure it learns a different, yet relevant, mapping for our images. Then we define the neighbourhood pseudo-label based on the vicin-In essence, we minimise a ity in that embedding space. sample bias by smoothing its pseudo-label in class space via knowledge of its neighbours in the prototypical space.
Additionally, we design our method to be fully online, enabling us to scale to large datasets at a low cost. We identify neighbours in the embedding space on-the-fly as the training proceeds by leveraging online K-means clus-tering. This alleviates the need to store expensive image embeddings [22], or to utilise offline nearest neighbour re-trieval [23, 48]. However, applying naive K-means risks collapsing to only a few imbalanced clusters making it less useful for our purpose. Hence, we employ a constrained objective [6] lower bounding each cluster size; thereby, en-suring that each sample has enough neighbours in its clus-ter. We show that the online nature of our method allows it to leverage the entire prediction history in one epoch to re-fine labels in the subsequent epoch at a fraction of the cost required by other methods and with a better performance.
PROTOCON’s final ingredient addresses another limita-tion of confidence-based methods: since the model only re-tains high confident samples for pseudo-labeling, the initial phase of the training usually suffers from a weak training signal due to fewer confident predictions.
In effect, this leads to only learning from the very few labeled samples which destabilises the training potentially due to overfit-ting [25]. To boost the initial training signal, we adopt a self-supervised instance-consistency [9, 15] loss applied on samples that fall below the threshold. Our choice of loss is more consistent with the classification task as opposed to contrastive instance discrimination losses [11, 16] which treat each image as its own class. This helps our method to converge faster without loss of accuracy.
We demonstrate PROTOCON’s superior performance against comparable state-of-the-art methods on 5 datasets including CIFAR, ImageNet and DomainNet. Notably,
PROTOCON achieves 2.2%, 1% improvement on the SSL
ImageNet protocol with 0.2% and 1% of the labeled data, respectively. Additionally, we show that our method ex-train-hibits faster convergence and more stable initial ing compared to baselines, thanks to our additional self-supervised loss. In summary, our contributions are:
• We propose a memory-efficient method addressing confirmation bias in label-scarce SSL via a novel la-bel refinement strategy based on co-training.
• We improve training dynamics and convergence of confidence-based methods by adopting self-supervised losses to the SSL objective.
• We show state-of-the-art results on 5 SSL benchmarks. 2.