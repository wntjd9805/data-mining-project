Abstract
Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still can-not reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes.
Our method, called TRACE, introduces several novel ar-chitectural components. Most importantly, it uses two new
“maps” to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even dur-ing long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and us-ing full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code1 and dataset2 are released for research purposes.
*This work was done when Yu Sun was an intern at JD.com.
†Corresponding author. 1https://www.yusun.work/TRACE/TRACE.html 2https://github.com/Arthur151/DynaCam
1.

Introduction
The estimation of 3D human pose and shape (HPS) has many applications and there has been significant recent progress [6, 7, 21–24, 37, 38, 46, 49, 56, 57, 60]. Most meth-ods, however, reason only about a single frame at a time and estimate humans in camera coordinates. Moreover, such methods do not track people and are unable to recover their global trajectories. The problem is even harder in typical hand-held videos, which are filmed with a dynamic, mov-ing, camera. For many applications of HPS, single-frame estimates in camera coordinates are not sufficient. To cap-ture human movement and then transfer it to a new 3D scene, we must have the movement in a coherent global co-ordinate system. This is a requirement for computer graph-ics, sports, video games, and extended reality (XR).
Our key insight is that most methods estimate humans in 3D, whereas the true problem is 5D. That is, a method needs to reason about 3D space, time, and subject identity. With a 5D representation, the problem becomes tractable, enabling a holistic solution that can exploit the full video to infer multiple people in a coherent global coordinate frame. As illustrated in Fig. 1, we develop a unified method to jointly regress the 3D pose, shape, identity, and global trajectory of the subjects in global coordinates from monocular videos captured by dynamic cameras (DC-videos).
To achieve this, we deal with two main challenges. First,
DC-videos contain both human motion and camera motion and these must be disentangled to recover the human trajec-tory in global coordinates. One idea would be to recover the camera motion relative to the rigid scene using structure-from-motion (SfM) methods (e.g. [31]). In scenes contain-ing many people and human motion, however, such meth-ods can be unreliable. An alternative approach is taken by
GLAMR [58], which infers global human trajectories from local 3D human poses, without taking into account the full scene. By ignoring evidence from the full image, GLAMR fails to capture the correct global motion in common scenar-ios, such as biking, skating, boating, running on a treadmill, etc. Moreover, GLAMR is a multi-stage method, with each stage dependent on accurate estimates from the preceding one. Such approaches are more brittle than our holistic, end-to-end, method.
The other challenge, as shown in the upper right corner of Fig. 1, is that severe occlusions are common in videos with multiple people. Currently, the most popular tracking strategy is to infer the association between 2D detections using a temporal prior (e.g. Kalman filter) [63]. However, in DC-videos, human motions are often irregular and can easily violate hand-crafted priors. PHALP [40] is one of the few methods to address this for 3D HPS. It uses a clas-sical, multi-stage, detection-and-tracking formulation with heuristic temporal priors.
It does not holistically reason about the sequence and is not trained end-to-end.
To address these issues, we reason about people using a 5D representation and capture information from the full image and the motion of the scene. This holistic reason-ing enables the reliable recovery of global human trajecto-ries and subject tracking using a single-shot method. This is more reliable than multi-stage methods because the net-work can exploit more information to solve the task and is trained end-to-end. No hand-crafted priors are needed and the network is able to share information among modules.
Specifically, we develop TRACE, a unified one-stage method for Temporal Regression of Avatars with dynamic
Cameras in 3D Environments. The architecture is inspired by BEV [45], which directly estimates multiple people in depth from a single image using multiple 2D maps. BEV uses a 2D map representing an imaginary, “top down”, view of the scene. This is combined with an image-centric 2D map to reason about people in 3D. Our key insight is that the idea of maps can be extended to represent how people move in 3D. With this idea, TRACE introduces three new modules to holistically model 5D human states, perform-ing multi-person temporal association, and inferring human trajectories in global coordinates; see Fig. 2.
First, to construct a holistic 5D representation of the video, we extract temporal image features by fusing single-frame feature maps from the image backbone with a tem-poral feature propagation module. We also compute the optical flow between adjacent frames with a motion back-bone. The optical flow provides short-term motion features that carry information about the motion of the scene and the people. Second, to explicitly track human motions, we introduce a novel 3D motion offset map to establish the as-sociation of the same person across adjacent frames. This map contains a 3D offset vector at each position, which rep-resents the difference between the 3D positions of the same subject from the previous frame to the current frame in cam-era coordinates. We also introduce a memory unit to keep track of subjects under long-term occlusion. Note that the 3D trajectories are built in camera space, and TRACE uses a novel world motion map that transfers the trajectories to global coordinates. At each position, this map contains a 6D vector to represent the difference between the 3D posi-tions of the corresponding subject from the previous frame to the current frame and its 3D orientation in world coordi-nates. Taken together, this novel network architecture goes beyond prior work by taking information from the full video frames to address detection, pose estimation, tracking, and occlusion in a holistic network that is trained end-to-end.
To enable training and evaluation of global human tra-jectory estimation from in-the-wild DC-videos, we build a new dataset, DynaCam. Since collecting global human tra-jectories and camera poses with in-the-wild DC-videos is difficult, we simulate a moving camera using publicly avail-able in-the-wild panoramic videos and regular videos cap-tured by static cameras. In this way, we create more than 500 in-the-wild DC-videos with precise camera pose anno-tations. Then we generate pseudo-ground-truth 3D human annotations via fitting [20] SMPL [32] to detected 2D pose sequences [17,54,63]. With 2D/3D human pose and camera pose annotations, we can obtain the global human trajecto-ries using the PnP algorithm [16]. This dataset is sufficient to train TRACE to deal with dynamic cameras.
We evaluate TRACE on two multi-person in-the-wild benchmarks (3DPW [50] and MuPoTS-3D [35]) and our
DynaCam dataset. On 3DPW, TRACE achieves the state-of-the-art (SOTA) PA-MPJPE of 37.8mm, less the cur-rent best (42.7 [26]). On MuPoTS-3D, TRACE outper-forms previous 3D-representation-based methods [39, 40] and tracking-by-detection methods [63] on tracking people under long-term occlusion. On DynaCam, TRACE outper-forms GLAMR [58] in estimating the 3D human trajectory in global coordinates from DC-videos.
In summary, our main contributions are: (1) We intro-duce a 5D representation and use it to learn holistic tempo-ral cues related to both 3D human motions and the scene. (2) We introduce two novel motion offset representations to explicitly model temporal multi-subject association and global human trajectories from temporal clues in an end-to-end manner. (3) We estimate long-term 3D human motions over time in global coordinates, achieving SOTA results. (4)
We collect the DynaCam dataset of DC-videos with pseudo ground truth, which facilitates the training and evaluation of global human trajectory estimation. The code and dataset are publicly available for research purposes. 2.