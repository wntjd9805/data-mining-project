Abstract
In a multi-task learning (MTL) setting, a single model is trained to tackle a diverse set of tasks jointly. Despite rapid progress in the field, MTL remains challenging due to optimization issues such as conflicting and dominating gradients. In this work, we propose using a condition num-ber of a linear system of gradients as a stability criterion of an MTL optimization. We theoretically demonstrate that a condition number reflects the aforementioned optimiza-tion issues. Accordingly, we present Aligned-MTL, a novel
MTL optimization approach based on the proposed crite-rion, that eliminates instability in the training process by aligning the orthogonal components of the linear system of gradients. While many recent MTL approaches guaran-tee convergence to a minimum, task trade-offs cannot be specified in advance. In contrast, Aligned-MTL provably converges to an optimal point with pre-defined task-specific weights, which provides more control over the optimization result. Through experiments, we show that the proposed ap-proach consistently improves performance on a diverse set of MTL benchmarks, including semantic and instance seg-mentation, depth estimation, surface normal estimation, and reinforcement learning. The source code is publicly available at https://github.com/SamsungLabs/MTL. 1.

Introduction
In a multi-task learning (MTL), several tasks are solved jointly by a single model [2, 10]. In such a scenario, infor-mation can be shared across tasks, which may improve the generalization and boost the performance for all objectives.
Moreover, MTL can be extremely useful when computa-tional resources are constrained, so it is crucial to have a single model capable of solving various tasks [17, 19, 30]. In reinforcement learning [39,50], MTL setting arises naturally, when a single agent is trained to perform multiple tasks.
Several MTL approaches [15, 24, 28, 29, 31, 35, 42] focus on designing specific network architectures and elaborate strategies of sharing parameters and representations across tasks for a given set of tasks. Yet, such complicated and powerful models are extremely challenging to train.
Direct optimization of an objective averaged across tasks might experience issues [54] related to conflicting and domi-nating gradients. Such gradients destabilize the training pro-cess and degrade the overall performance. Accordingly, some other MTL approaches address these issues with multi-task gradient descent: either using gradient altering [9, 27, 48, 54] or task balancing [16, 25, 28]. Many recent MTL meth-ods [27, 37, 48] guarantee convergence to a minimum, yet task trade-offs cannot be specified in advance. Unfortunately, the lack of control over relative task importance may cause some tasks to be compromised in favor of others [37].
In this work, we analyze the multi-task optimization chal-lenges from the perspective of stability of a linear system of gradients. Specifically, we propose using a condition number of a linear system of gradients as a stability criterion of an
MTL optimization. According to our thorough theoretical analysis, there is a strong relation between the condition number and conflicting and dominating gradients issues. We exploit this feature to create Aligned-MTL, a novel gradi-ent manipulation approach, which is the major contribu-tion of this work. Our approach resolves gradient conflicts and eliminates dominating gradients by aligning principal components of a gradient matrix, which makes the training process more stable. In contrast to other existing methods (e.g. [27,37,48,54]), Aligned-MTL has a provable guarantee of convergence to an optimum with pre-defined task weights.
We provide an in-depth theoretical analysis of the proposed method and extensively verify its effectiveness.
Aligned-MTL consistently outperforms previous methods on various benchmarks. First, we evaluate the proposed ap-proach on the problem of scene understanding; specifically, we perform joint instance segmentation, semantic segmenta-tion, depth and surface normal estimation on two challenging datasets – Cityscapes [6] and NYUv2 [36]. Second, we apply our method to multi-task reinforcement learning and con-duct experiments with the MT10 dataset [55]. Lastly, in order to analyze generalization performance, Aligned-MTL has been applied to two different network architectures, namely
PSPNet [48] and MTAN [28], in the scene understanding experiments.
(a) Uniform (b) CAGrad (c = 0.4) [26] (c) IMTL [27] (d) Nash-MTL [37] (e) Aligned-MTL (ours)
Figure 1. Comparison of MTL approaches on a challenging synthetic two-task benchmark [26, 37]. We visualize optimization trajectories w.r.t. objectives value (L1 and L2, top row), and cumulative objective w.r.t. parameters (θ1 and θ2, bottom row). Initialization points are marked with •, the Pareto front (Def. 1) is denoted as
. Other MTL approaches produce noisy optimization trajectories (Figs. 1a to 1d) inside areas with conflicting and dominating gradients (Fig. 2). In contrast, our approach converges to the global optimum (⋆) robustly.
Approaches aiming to find a Pareto-stationary solution (such as Fig. 1c and Fig. 1d) terminate once the Pareto front is first reached, as a result, they might provide a suboptimal solution. Differently, Aligned-MTL drifts along the Pareto front and provably converges to the optimum w.r.t. pre-defined tasks weights. 2.