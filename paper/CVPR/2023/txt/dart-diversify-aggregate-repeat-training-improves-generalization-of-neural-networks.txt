Abstract
Generalization of Neural Networks is crucial for deploy-ing them safely in the real world. Common training strate-gies to improve generalization involve the use of data aug-mentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong bench-mark for generalization which utilizes diverse augmenta-tions within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different aug-mentations (or domains) to explore the loss basin, and fur-ther Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the over-all optimization trajectory and also ensures that the indi-vidual models have sufficiently low loss barrier to obtain improved generalization on combining them. We theoret-ically justify the proposed approach and show that it in-deed generalizes better. In addition to improvements in In-Domain generalization, we demonstrate SOTA performance on the Domain Generalization benchmarks in the popular
DomainBed framework as well. Our method is generic and can easily be integrated with several base training algo-rithms to achieve performance gains. Our code is available here: https://github.com/val-iisc/DART. 1.

Introduction
Deep Neural Networks have outperformed classical methods in several fields and applications owing to their re-markable generalization. Classical Machine Learning the-ory assumes that test data is sampled from the same distri-bution as train data. This is referred to as the problem of
In-Domain (ID) generalization [15, 18, 29, 32, 48], where
*Equal Contribution. ∓ Equal contribution second authors. Correspon-dence to Samyak Jain <samyakjain.cse18@itbhu.ac.in>, Sravanti Adde-palli <sravantia@iisc.ac.in>. ⋄ Indian Institute of Technology, Varanasi
§ Indian Institute of Technology, Dhanbad. ‡ Work done during internship at Vision and AI Lab, Indian Institute of Science, Bangalore. the goal of the model is to generalize to samples within same domain as the train dataset. This is often considered to be one of the most important requirements and criteria to evaluate models. However, in several cases, the test dis-tribution may be different from the train distribution. For example, surveillance systems are expected to work well at all times of the day, under different lighting conditions and when there are occlusions, although it may not be possible to train models using data from all these distributions. It is thus crucial to train models that are robust to distribution shifts, i.e., with better Out-of-Domain (OOD) Generaliza-tion [25].
In this work, we consider the problems of In-Domain generalization and Out-of-Domain Generalization of Deep Networks. For the latter, we consider the popu-lar setting of Domain Generalization [4, 23, 41], where the training data is composed of several source domains and the goal is to generalize to an unseen target domain.
The problem of generalization is closely related to the
Simplicity Bias of Neural Networks, due to which models have a tendency to rely on simpler features that are often spurious correlations to the labels, when compared to the harder robust features [55]. For example, models tend to rely on weak features such as background, rather than more robust features such as shape, causing a drop in object clas-sification accuracy when background changes [22, 72]. A common strategy to alleviate this is to use data augmenta-tions [8–10, 27, 42, 53, 75, 77] or data from several domains during training [23], which can result in invariance to sev-eral spurious correlations, improving the generalization of models. Shen et al. [57] show that data augmentations en-able the model to give higher importance to harder-to-learn robust features by delaying the learning of spurious fea-tures. We extend their observation by showing that training on a combination of several augmentation strategies (which we refer to as Mixed augmentation) can result in the learn-ing of a balanced distribution of diverse features. Using this, we obtain a strong benchmark for ID generalization as shown in Table-1. However, as shown in prior works [1], the impact of augmentations in training is limited by the capacity of the network in being able to generalize well to
Table 1. Motivation: Performance (%) on CIFAR100, ResNet-18 with ERM training for 200 epochs. Mixed-Training (MT) outper-forms individual augmentations, and ensembles perform best.
Test Augmentation
Train Augmentation
No Aug. Cutout
Cutmix
AutoAugment
Pad+Crop+HFlip (PC)
Cutout (CO)
Cutmix (CM)
AutoAugment (AA)
Mixed-Training (MT) 78.51 77.99 80.54 79.18 81.43 67.04 74.58 74.05 71.26 77.31
Ensemble (CM+CO+AA) 83.61 79.19 56.52 56.12 77.35 60.97 73.20 73.19 58.33 58.47 61.23 73.91 74.73 73.90 the diverse augmented data distribution. Therefore, increas-ing the diversity of training data demands the use of larger model capacities to achieve optimal performance. This de-mand for higher model capacity can be mitigated by train-ing specialists on each kind of augmentation and ensem-bling their outputs [11,38,59,79], which results in improved performance as shown in Table-1. Another generic strategy that is known to improve generalization is model-weight av-eraging [31, 70, 71], which results in a flatter minima.
In this work, we aim to combine the benefits of the three strategies discussed above - diversification, special-ization and model weight averaging, while also overcom-ing their individual shortcomings. We propose a Diversify-Aggregate-Repeat Training strategy dubbed DART (Fig.1), that first trains M Diverse models after a few epochs of common training, and then Aggregates their weights to ob-tain a single generalized solution. The aggregated model is then used to reinitialize the M models which are further trained post aggregation. This process is Repeated over training to obtain improved generalization. The Diversify step allows models to explore the loss basin and specialize on a fixed set of features. The Aggregate (or Model Interpo-lation) step robustly combines these models, increasing the diversity of represented features while also suppressing spu-rious correlations. Repeating the Diversify-Aggregate steps over training ensures that the M diverse models remain in the same basin thereby permitting a fruitful combination of their weights. We justify our approach theoretically and em-pirically, and show that intermediate model aggregation also increases the learning time for spurious features, improving generalization. We present our key contributions below:
• We present a strong baseline termed Mixed-Training (MT) that uses a combination of diverse augmentations for different images in a training minibatch.
• We propose a novel algorithm DART, that learns spe-cialized diverse models and aggregates their weights iteratively to improve generalization.
• We justify our method theoretically, and empirically on several In-Domain (CIFAR-10, CIFAR-100, Im-ageNet) and Domain Generalization (OfficeHome,
PACS, VLCS, TerraIncognita, DomainNet) datasets.
Figure 1. Schematic Diagram of the proposed method DART 2.