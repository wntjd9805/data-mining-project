Abstract 1.

Introduction
Thanks to advances in computer vision and AI, there has been a large growth in the demand for cloud-based vi-sual analytics in which images captured by a low-powered edge device are transmitted to the cloud for analytics. Use for of conventional codecs (JPEG, MPEG, HEVC, etc.) compressing such data introduces artifacts that can seri-ously degrade the performance of the downstream analytic tasks. Split-DNN computing has emerged as a paradigm to address such usages, in which a DNN is partitioned into a client-side portion and a server side portion. Low-complexity neural networks called ‘bottleneck units’ are in-troduced at the split point to transform the intermediate layer features into a lower-dimensional representation bet-ter suited for compression and transmission. Optimizing the pipeline for both compression and task-performance re-quires high-quality estimates of the information-theoretic rate of the intermediate features. Most works on compres-sion for image analytics use heuristic approaches to esti-mate the rate, leading to suboptimal performance. We pro-pose a high-quality ‘neural rate-estimator’ to address this gap. We interpret the lower-dimensional bottleneck out-put as a latent representation of the intermediate feature and cast the rate-distortion optimization problem as one of training an equivalent variational auto-encoder with an appropriate loss function. We show that this leads to im-proved rate-distortion outcomes. We further show that re-placing supervised loss terms (such as cross-entropy loss) by distillation-based losses in a teacher-student framework allows for unsupervised training of bottleneck units without the need for explicit training labels. This makes our method very attractive for real world deployments where access to labeled training data is difficult or expensive. We demon-strate that our method outperforms several state-of-the-art methods by obtaining improved task accuracy at lower bi-trates on image classification and semantic segmentation tasks.
Visual analytics powered by computer-vision and AI are being ubiquitously deployed in various domains including retail, industry 4.0, security, and smart-cities [16]. These analytics are increasingly being powered by deep-neural networks (DNN) that are often too complex to be imple-mented on low-powered mobile or client devices. Instead, visual data captured by a mobile device is transmitted over the network to a server or the cloud. Data-compression techniques are applied in order to keep the data-rate man-ageable. Standard data compression techniques for images and videos, such as JPEG, BPG, H.264, H.265, etc. are known to be suboptimal for visual analytics since these optimize rate-distortion performance for human perception rather than for semantics-based analytics. Consequently, task performance can degrade severely in the presence of even relatively mild compression artifacts.
To address this, split DNN-computing (also called col-laborative intelligence [8, 17]) has emerged as a recent paradigm in which a DNN is partitioned into two: a front-end comprising the input layer and a number of subsequent layers deployed on the mobile or client side, and a back-end comprising the remaining layers residing on the server or cloud. Specially designed neural networks, called bottle-neck layers [12, 21, 24] are introduced at the partition point.
These layers transform the high-dimensional intermediate features at the split point into a lower-dimensional space, enabling greater compression. This has important bene-fits over the traditional approach. First, the model can be trained to learn features that are jointly optimized both for task performance and for compression, resulting in greater compression efficiency; second, there is no need to recon-struct the original signal resulting in greater computational efficiency.
Early approaches [5] to split-computing explored the use of simple lossless and lossy compression techniques to compress the intermediate features. While lossless techniques resulted in only mild reduction in bandwidth, naively quantizing the intermediate features during infer-ence to reduce bitrate leads to a drop in the task perfor-mance.
Inspired by the impressive results of ML-based image compression approaches [2, 3, 26], subsequent ap-proaches [7,23,25] include the quantization operation in the end-to-end training of the model. These approaches yield significantly better rate-distortion performance (task accu-racy vs compression level) than that obtained by traditional image compression methods such as JPEG and the more re-cent HEIC.
These approaches suffer, however, from a couple of ma-jor limitations in practical, real-world scenarios. First, the parameters of the trained DNN model are valid only for a particular split-point, and for a particular compression level. Changing either the split point or the compression level (as is needed for variable bit-rate communication) re-quires, therefore, a complete retraining of the entire model.
Such multiple retrainings increase training complexity sig-nificantly. Inference also becomes highly inefficient since entire sets of DNN parameters (which can run into tens of millions) have to be reloaded each time the compression level needs to be changed. A second limitation is the need for large volumes of labeled training data. Such data may often not be available in real-world situations; and gathering and annotating data might be expensive and impractical.
To address the first challenge, a recent approach [10] outlined a systematic procedure to both design and train the bottleneck layers introduced at various split points and for various compression levels. Crucially, the parameters of the original DNN were left untouched. However, the method used a sub-optimal formulation to estimate the rate of the transmitted data at the bottleneck; further, it did not tackle the unsupervised scenario. Despite these simplifi-cations, the method did achieve state-of-the-art results. To handle unsupervised training, another method [20] explored the use of distillation-based approaches. However, that too involved training all or part of the network, in particular the front-end of the network (also called its head). Hence, it has the same limitations as other methods when deployed in dynamic, variable-bit rate usages.
Contributions: We present in this work1, an approach to split DNN computing for image analytics that addresses the challenges outlined above. We make use of bottleneck lay-ers and similar to [10], we train the weights of the bot-tleneck layer only. We propose an improved modeling of the rate-loss via a neural rate-estimator using methods of variational inference and show that this results in large re-ductions in bit-rate without sacrificing task accuracy. Fur-ther, we present a distillation-based approach to enable un-supervised training of the bottleneck layers. We observe that while lack of a supervisory signal does result in a drop 1Code at https://github.com/intellabs/spic
Figure 1. Variational model [3]: x is a vector to be compressed; y is its latent representation derived from an analysis network with parameters ϕg. θg are the parameters of a synthesis network that recovers x from y. z are hyper-latents introduced to capture de-pendencies in y; ϕh and θh are the corresponding analysis and synthesis networks relating y and z. in the rate-distortion performance, our method still outper-forms most other published methods that used supervised training. 2.