Abstract
Recently, semi-supervised semantic segmentation has achieved promising performance with a small fraction of labeled data. However, most existing studies treat all unla-beled data equally and barely consider the differences and training difficulties among unlabeled instances. Differen-tiating unlabeled instances can promote instance-specific supervision to adapt to the model’s evolution dynamically. instance
In this paper, we emphasize the cruciality of differences and propose an instance-specific and model-adaptive supervision for semi-supervised semantic segmen-tation, named iMAS. Relying on the model’s performance, iMAS employs a class-weighted symmetric intersection-over-union to evaluate quantitative hardness of each un-labeled instance and supervises the training on unlabeled data in a model-adaptive manner. Specifically, iMAS learns from unlabeled instances progressively by weighing their corresponding consistency losses based on the evaluated hardness. Besides, iMAS dynamically adjusts the augmen-tation for each instance such that the distortion degree of augmented instances is adapted to the model’s generaliza-tion capability across the training course. Not integrating additional losses and training procedures, iMAS can obtain remarkable performance gains against current state-of-the-art approaches on segmentation benchmarks under different semi-supervised partition protocols1. 1.

Introduction
Though semantic segmentation studies [6, 28] have achieved significant progress, their enormous success relies on large datasets with high-quality pixel-level annotations.
Semi-supervised semantic segmentation (SSS) [20, 30] has been proposed as a powerful solution to mitigate the re-quirement for labeled data. Recent research on SSS has
*Equal contribution. The work was done during an internship at Baidu.
†Corresponding authors (luping.zhou@sydney.edu.au, wangjing-This work is supported by Australian Research dong@baidu.com).
Council (ARC DP200103223). 1Code and logs: https://github.com/zhenzhao/iMAS. two main branches, including the self-training (ST) [26] and consistency regularization (CR) [40] based approaches.
[46] follows a self-training paradigm and performs a selec-tive re-training scheme to train on labeled and unlabeled data alternatively. Differently, CR-based works [27, 34] tend to apply data or model perturbations and enforce the prediction consistency between two differently-perturbed views for unlabeled data. In both branches, recent research
[13, 19, 47] demonstrates that strong data perturbations like
CutMix can significantly benefit the SSS training. To fur-ther improve the SSS performance, current state-of-the-art approaches [1, 42] integrate the advanced contrastive learn-ing techniques into the CR-based approaches to exploit the unlabeled data more efficiently. Works in [21, 24] also aim to rectify the pseudo-labels through training an additional correcting network.
Despite their promising performance, SSS studies along this line come at the cost of introducing extra network components or additional training procedures.
In addi-tion, majorities of them treat unlabeled data equally and completely ignore the differences and learning difficulties among unlabeled samples. For instance, randomly and indiscriminately perturbing unlabeled data can inevitably over-perturb some difficult-to-train instances. Such over-perturbations exceed the generalization capability of the model and hinder effective learning from unlabeled data.
As discussed in [47], it may also hurt the data distribution.
Moreover, in most SSS studies, final consistency losses on different unlabeled instances are minimized in an average manner. However, blindly averaging can implicitly empha-size some difficult-to-train instances and result in model overfitting to noisy supervision.
In this paper, we emphasize the cruciality of instance dif-ferences and aim to provide instance-specific supervision on unlabeled data in a model-adaptive way. There naturally exists two main questions. First, how can we differentiate unlabeled samples? We design an instantaneous instance
“hardness,” to estimate 1) the current generalization ability of the model and 2) the current training difficulties of dis-tinct unlabeled samples. Its evaluation is closely related to the training status of the model, e.g., a difficult-to-train sam-1
Figure 1. Diagram of our proposed iMAS. In a teacher-student framework, labeled data (x, y) is used to train the student model, parame-terized by θs, by minimizing the supervised loss Lx. Unlabeled data u, weakly augmented by Aw(·), is first fed into both the student and teacher models to obtain predictions ps and pt, respectively. Then we perform quantitative hardness evaluation on each unlabeled instance by strategy ϕ(pt, ps). Such hardness information can be subsequently utilized: 1) to apply an adaptive augmentation, denoted by As(·), on unlabeled data to obtain the student model’s prediction ˆp; 2) to weigh the unsupervised loss Lu in a instance-specific manner. The teacher model’s weight, θt, is updated by the exponential moving average (EMA) of θs across the training course. ple can become easier with the evolution of the model. Sec-ond, how can we inject such discriminative information into the SSS procedure? Since the hardness is assessed based on the model’s performance, we can leverage such information to adjust the two critical operations in SSS, i.e., data pertur-bations and unsupervised loss evaluations, to adapt to the training state of the model dynamically.
Motivated by all these observations, we propose an instance-specific and model-adaptive supervision, named iMAS, for semi-supervised semantic segmentation. As shown in Figure 1, following a standard consistency reg-ularization framework, iMAS jointly trains the student and teacher models in a mutually-beneficial manner. The teacher model is an ensemble of historical student models and generates stable pseudo-labels for unlabeled data. In-spired by empirical and mathematical analysis in [15, 41], difficult-to-train instances may undergo considerable dis-agreement between predictions of the EMA teacher and the current student. Thus in iMAS, we first evaluate the instance hardness of each unlabeled sample by calculat-ing the class-weighted symmetric intersection-over-union (IoU) between the segmentation predictions of the teacher (the historical) and student (the most recent) models. Then based on the evaluation, we perform model-adaptive data perturbations on each unlabeled instance and minimize an instance-specific weighted consistency loss to train models in a curriculum-like manner. In this way, different unlabeled instances are perturbed and weighted in a dynamic fashion, which can better adapt to the model’s generalization capa-bility throughout the training processes.
Benefiting from this instance-specific and model-adaptive design, iMAS obtains state-of-the-art (SOTA) per-formance on Pascal VOC 2012 and Cityscapes datasets un-der different partition protocols. For example, our method obtains a high mIOU of 75.3% with only 183 labeled data on VOC 2012, which is 17.8% higher than the supervised baseline and 4.3% higher than the previous SOTA. Our main contributions are summarized as follows,
• iMAS can boost the SSS performance by highlighting the instance differences, without introducing extra network components or training losses.
• We perform a quantitative hardness-evaluating analysis for unlabeled instances in segmentation tasks, based on the class-weighted teacher-student symmetric IoU.
• We propose an instance-specific and model-adaptive SSS framework that injects instance hardness into loss eval-uation and data perturbation to dynamically adapt to the model’s evolution. 2.