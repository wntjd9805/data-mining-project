Abstract
Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative ad-versarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the
ﬁrst 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360◦ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Speciﬁcally, we propose a novel two-stage self-adaptive image alignment for robust 3D
GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthe-sis in diverse backgrounds. Beneﬁting from these designs, our method signiﬁcantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars. 1.

Introduction
Photo-realistic portrait image synthesis has been a con-tinuous focus in computer vision and graphics, with a wide range of downstream applications in digital avatars, telep-resence, immersive gaming, and many others. Recent ad-vances in Generative Adversarial Networks (GANs) [12] has demonstrated strikingly high image synthesis quality, indis-tinguishable from real photographs [19, 21, 22]. However, contemporary generative approaches operate on 2D convolu-tional networks without modeling the underlying 3D scenes.
Therefore 3D consistency cannot be strictly enforced when
Project page: https://sizhean.github.io/panohead
synthesizing head images under various poses.
To generate 3D heads with diverse shapes and appear-ances, traditional approaches require a parametric textured mesh model [2, 25] learned from large 3D scan collections.
However, the rendered images lack ﬁne details and have limited perceptual quality and expressiveness. With the advent of differentiable rendering and neural implicit rep-resentation [28, 47], conditional generative models have been developed to generate more realistic 3D-aware face images [17, 44, 45, 53]. However, those approaches typically require multi-view image or 3D scan supervision, which are hard to acquire and have limited appearance distribution as those are usually captured in controlled environments. 3D-aware generative models have recently seen rapid progress, fueled by the integration of implicit neural repre-sentation in 3D scene modeling and Generative Adversarial
Networks (GANs) for image synthesis [5,6,29,31,37,40,48].
Among them, the seminal 3D GAN, EG3D [5], demonstrates striking quality in view-consistent image synthesis, trained only from in-the-wild single-view image collections. How-ever, these 3D GAN approaches are still limited to synthesis in near-frontal views.
In this paper, we propose PanoHead, a novel 3D-aware
GAN for high-quality full 3D head synthesis in 360◦ trained from only in-the-wild unstructured images. Our model can synthesize consistent 3D heads viewable from all angles, which is desirable by many immersive interaction scenarios such as digital avatars and telepresence. To the best of our knowledge, our method is the ﬁrst 3D GAN approach to achieve full 3D head synthesis in 360◦.
Extending 3D GAN frameworks such as EG3D [5] to full 3D head synthesis poses several signiﬁcant technical challenges: Firstly, many 3D GANs [5, 31] cannot separate foreground and background, inducing 2.5D head geometry.
The background, formulated typically as a wall structure, is entangled with the generated head in 3D and therefore pro-hibits rendering from large poses. We introduce a foreground-aware tri-discriminator that jointly learns the decomposition of the foreground head in 3D space by distilling the prior knowledge in 2D image segmentation.
Secondly, while being compact and efﬁcient, current hy-brid 3D scene representations, like tri-plane [5], introduce strong projection ambiguity for 360◦ camera poses, resulting in ‘mirrored face’ on the back head. To address the issue, we present a novel 3D tri-grid volume representation that disentangles the frontal features with the back head while maintaining the efﬁciency of tri-plane representations.
Lastly, obtaining well-estimated camera extrinsics of in-the-wild back head images for 3D GANs training is ex-tremely difﬁcult. Moreover, an image alignment gap exists between these and frontal images with detectable facial land-marks. The alignment gap causes a noisy appearance and unappealing head geometry. Thus, we propose a novel two-stage alignment scheme that robustly aligns images from any view consistently. This step decreases the learning difﬁ-culty of 3D GANs signiﬁcantly. In particular, we propose a camera self-adaptation module that dynamically adjusts the positions of rendering cameras to accommodate the align-ment drifts in the back head images.
Our framework substantially enhances the 3D GANs’ ca-pabilities to adapt to in-the-wild full head images from arbi-trary views, as shown in Figure 1. The resulting 3D GAN not only generates high-ﬁdelity 360◦ RGB images and geometry, but also achieves better quantitative metrics than state-of-the-art methods. With our model, we showcase compelling 3D full head reconstruction from a single monocular-view image, enabling easily accessible 3D portrait creation.
In summary, our main contributions are as follows:
• The ﬁrst 3D GAN framework that enables view-consistent and high-ﬁdelity full-head image synthesis with detailed geometry, renderable in 360◦. We demonstrate our ap-proach in high-quality monocular 3D head reconstruction from in-the-wild images.
• A novel tri-grid formulation that balances efﬁciency and expressiveness in representing 3D 360◦ head scenes.
• A foreground-aware tri-discriminator that disentangles 3D foreground head modeling from 2D background synthesis.
• A novel two-stage image alignment scheme that adaptively accommodates imperfect camera poses and misaligned image cropping, enabling training of 3D GANs from in-the-wild images with wide camera pose distribution. 2.