Abstract
We present Depth-aware Image-based NEural Radiance
ﬁelds (DINER). Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to ren-der 3D objects under novel views. Speciﬁcally, we propose novel techniques to incorporate depth information into fea-ture fusion and efﬁcient scene sampling. In comparison to the previous state of the art, DINER achieves higher synthe-sis quality and can process input views with greater dispar-ity. This allows us to capture scenes more completely with-out changing capturing hardware requirements and ulti-mately enables larger viewpoint changes during novel view synthesis. We evaluate our method by synthesizing novel views, both for human heads and for general objects, and observe signiﬁcantly improved qualitative results and in-creased perceptual metrics compared to the previous state of the art. The code is publicly available through the Project
Webpage. 1.

Introduction
In the past few years, we have seen immense progress in digitizing humans for virtual and augmented reality applica-tions. Especially with the introduction of neural rendering and neural scene representations [43, 44], we see 3D digital humans that can be rendered under novel views while being controlled via face and body tracking [3, 11, 14, 15, 24, 32, 37, 47, 50, 51, 58]. Another line of research reproduces gen-eral 3D objects from few input images without aiming for control over expressions and poses [5,25,38,41,46,56]. We argue that this offers signiﬁcant advantages in real-world applications like video-conferencing with holographic dis-plays: (i) it is not limited to heads and bodies but can also reproduce objects that humans interact with, (ii) even for unseen extreme expressions, ﬁne texture details can be syn-thesized since they can be transferred from the input im-ages, (iii) only little capturing hardware is required e.g. four webcams sufﬁce, and (iv) the approach can generalize across identities such that new participants could join the conference ad hoc without requiring subject-speciﬁc opti-mization. Because of these advantages, we study the sce-nario of reconstructing a volumetric scene representation for novel view synthesis from sparse camera views. Specif-ically, we assume an input of four cameras with high rel-ative distances to observe large parts of the scene. Based on these images, we condition a neural radiance ﬁeld [27] which can be rendered under novel views including view-dependent effects. We refer to this approach as image-based neural radiance ﬁelds. It implicitly requires estimating the
scene geometry from the source images. However, we ob-serve that even for current state-of-the-art methods the ge-ometry estimation often fails and signiﬁcant synthesis arti-facts occur when the distance between the source cameras becomes large because they rely on implicit correspondence search between the different views. Recent research demon-strates the beneﬁts of exploiting triangulated landmarks to guide the correspondence search [25]. However, landmarks have several drawbacks: They only provide sparse guid-ance, are limited to speciﬁc classes, and the downstream task is bounded by the quality of the keypoint estimation, which is known to deteriorate for proﬁle views.
To this end, we propose DINER to compute an image-based neural radiance ﬁeld that is guided by estimated dense depth. This has signiﬁcant advantages: depth maps are not restricted to speciﬁc object categories, provide dense guid-ance, and are easy to attain via either a commodity depth sensor or off-the-shelf depth estimation methods. Specif-ically, we leverage a state-of-the-art depth estimation net-work [8] to predict depth maps for each of the source views and employ an encoder network that regresses pixel-aligned feature maps. DINER exploits the depth maps in two im-portant ways: (i) we condition the neural radiance ﬁeld on the deviation between sample location and depth esti-mates which provides strong prior information about visual opacity, and (ii) we focus sampling on the estimated sur-faces to improve sampling efﬁciency. Furthermore, we im-prove the extrapolation capabilities of image-based NeRFs by padding and positionally encoding the input images be-fore applying the feature extractor. Our model is trained on many different scenes and at inference time, four input im-ages sufﬁce to reconstruct the target scene in one inference step. As a result, compared to the previous state of the art,
DINER can reconstruct 3D scenes from more distinct source views with better visual quality, while allowing for larger viewpoint changes during novel view synthesis. We eval-uate our method on the large-scale FaceScape dataset [54] on the task of novel view synthesis for human heads from only four highly diverse source views and on general ob-jects in the DTU dataset [18]. For both datasets, our model outperforms all baselines by a signiﬁcant margin.
In summary, DINER is a novel method that produces vol-umetric scene reconstructions from few source views with higher quality and completeness than the previous state of the art. In summary, we contribute:
• an effective approach to condition image-based NeRFs on depth maps predicted from the RGB input,
• a novel depth-guided sampling strategy that increases efﬁciency,
• and a method to improve the extrapolation capabilities of image-based NeRFs by padding and positionally en-coding the source images prior to feature extraction. 2.