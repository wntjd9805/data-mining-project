Abstract
Recently, Visual Information Extraction (VIE) has been becoming increasingly important in both the academia and industry, due to the wide range of real-world applications.
Previously, numerous works have been proposed to tackle this problem. However, the benchmarks used to assess these methods are relatively plain, i.e., scenarios with real-world complexity are not fully represented in these benchmarks.
As the first contribution of this work, we curate and re-lease a new dataset for VIE, in which the document im-ages are much more challenging in that they are taken from real applications, and difficulties such as blur, partial oc-clusion, and printing shift are quite common. All these fac-tors may lead to failures in information extraction. There-fore, as the second contribution, we explore an alternative approach to precisely and robustly extract key information from document images under such tough conditions. Specif-ically, in contrast to previous methods, which usually ei-ther incorporate visual information into a multi-modal ar-chitecture or train text spotting and information extraction in an end-to-end fashion, we explicitly model entities as se-mantic points, i.e., center points of entities are enriched with semantic information describing the attributes and re-lationships of different entities, which could largely bene-fit entity labeling and linking. Extensive experiments on standard benchmarks in this field as well as the proposed
*Equal Contribution.
â€ Correspondence Author. dataset demonstrate that the proposed method can achieve significantly enhanced performance on entity labeling and linking, compared with previous state-of-the-art models.
Dataset is available at https://www.modelscope. cn/datasets/damo/SIBR/summary. 1.

Introduction
Visually Rich Documents (VRDs) are ubiquitous in daily, industrial, and commercial activities, such as receipts of shopping, reports of physical examination, product man-uals, and bills of entry. Visual Information Extraction (VIE) aims to automatically extract key information from these
VRDs, which can significantly facilitate subsequent pro-cessing and analysis. Due to its broad applications and grand technical challenges, VIE has recently attracted con-siderable attention from both the Computer Vision com-munity [33, 34, 39] and the Natural Language Processing community [16,35,37]. Typical techniques for tackling this challenging problem include essential electronic conversion of image (OCR)
[28, 30, 41], intermediate procedure of structure analysis [25] and high-level understanding of con-tents [35], among which entities play an important role as an aggregation of vision, structure, and language.
Though substantial progresses [11, 19, 35] have been made, it is still challenging to precisely and reliably extract key information from document images in unconstrained conditions. As shown in Fig. 1, in real-world scenarios doc-uments may have various formats, be captured casually with
a mobile phone, or exist occlusion or shift in printing, all of which would pose difficulties for VIE algorithms.
To highlight the challenges in real applications and pro-mote the development of research in VIE, we establish a new dataset called Structurally-rich Invoices, Bills and
Receipts in the Wild (SIBR for short), which contains 1,000 images with 71,227 annotated entity instances and 39,004 entity links. The challenges of SIBR lie in: (1) The docu-ments are from different real-world scenarios, so their for-mats and structures might be complicated and varying; (2)
The image quality may be very poor, i.e., blur, noise, and uneven illumination are frequently seen; (3) The printing process is imperfect that shift and rotation might happen.
To deal with these difficulties, we explore an novel ap-proach for information extraction from VRDs. Different from previous methods, which usually employ a sequen-tial pipeline that first uses an off-the-shelf OCR engine to detect and read textual information (location and content) and then fuses such information with visual cues for follow-up entity labeling (a.k.a. entity extraction) and linking in a multi-modal architecture (mostly a Transformer) [19, 37], the proposed method adopts a unified framework that all components, including text detection, text recognition, en-tity extraction and linking, are jointly modeled and trained in an integrated way. This means that in our method a sep-arate OCR engine is no longer necessary. The benefits are two-fold: (1) The accuracy of entity labeling and linking will not be limited by the capacity of the OCR engine; (2)
The running speed of the whole pipeline could be boosted.
Drawing inspirations from general object detection [4, 15, 40, 42] and vision-language joint learning [11, 14, 31], we put forward to model entities as semantic points (ESP for short). Specifically, as shown in Fig. 3, entities are rep-resented using their center points, which are enriched with semantics, such as geometric and linguistic information, to perform entity labeling and linking. To better learn a joint vision-language representation, we also devise three train-ing tasks that are well integrated into the paradigm. The entity-image text matching (EITM) task, which is only used in the pre-training stage, learns to align entity-level vision vectors and language vectors (encoded with off-the-shell
BERT) with a contrastive learning paradigm. Entity ex-traction (EE) and Entity linking (EL), the main tasks for
VIE, are used in the pre-training, fine-tuning, and inference stages. In these two modules, region features and position embedding (from ground truth or detection branch) are en-coded with transformer layers and then decoded to entity classes and relations. Owing to the joint vision-language representation, text recognition is no longer a necessary module in our framework, and we will discuss the impact of the text recognition branch in Sec. 5.4.
Extensive experiments have been conducted on stan-dard benchmarks for VIE (such as FUNSD, XFUND, and
CORD) as well as the proposed SIBR dataset. We found that compared with previous state-of-the-art methods, the proposed ESP algorithm can achieve highly competitive performance. Especially, it shows an advantage in the task of entity linking. Our main contributions can be summa-rized as follows: (1) We curate and release a new dataset for VIE, in which the document images are with real-world complexity and difficulties. (2) We devise a unified frame-work for spotting, labeling and linking entities, where a sep-arate OCR engine is unnecessary. (3) We adopt three vision-language joint modeling tasks for learning informative rep-resentation for VIE. (4) Extensive experiments demonstrate the effectiveness and advantage of our approach. 2.