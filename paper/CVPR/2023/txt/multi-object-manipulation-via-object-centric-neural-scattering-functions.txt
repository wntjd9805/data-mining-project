Abstract
Learned visual dynamics models have proven effective for robotic manipulation tasks. Yet, it remains unclear how best to represent scenes involving multi-object interactions.
Current methods decompose a scene into discrete objects, but they struggle with precise modeling and manipulation amid challenging lighting conditions as they only encode appearance tied with speciﬁc illuminations. In this work, we propose using object-centric neural scattering functions (OSFs) as object representations in a model-predictive con-trol framework. OSFs model per-object light transport, en-abling compositional scene re-rendering under object re-arrangement and varying lighting conditions. By combin-ing this approach with inverse parameter estimation and graph-based neural dynamics models, we demonstrate im-proved model-predictive control performance and general-ization in compositional multi-object environments, even in previously unseen scenarios and harsh lighting conditions. 1.

Introduction
Predictive models are the core components of many robotic systems for solving inverse problems such as plan-ning and control. Physics-based models built on ﬁrst prin-ciples have shown impressive performance in domains such as drone navigation [3] and robot locomotion [28]. How-ever, such methods usually rely on complete a priori knowl-edge of the environment, limiting their use in complicated manipulation problems where full-state estimation is com-plex and often impossible. Therefore, a growing number of approaches alternatively propose to learn dynamics models directly from raw visual observations [2, 12, 14, 17, 21, 48].
Although using raw sensor measurements as inputs to predictive models is an attractive paradigm as they are readily available, visual data can be challenging to work with directly due to its high dimensionality. Prior meth-ods proposed to learn dynamics models over latent vec-∗indicates equal contribution. Yancheng is afﬁliated with Fudan Uni-versity; this work was done while he was a summer intern at Stanford.
Figure 1. While typically studied visual manipulation settings are carefully controlled environments, we consider scenarios with varying and even harsh lighting, in addition to novel object con-ﬁgurations, that are more similar to real-world scenarios. tors, demonstrating promising results in a range of robotics tasks [17, 18, 44, 52]. However, with multi-object interac-tions, the underlying physical world is 3D and composi-tional. Encoding everything into a single latent vector fails to consider the relational structure within the environment, limiting its generalization outside the training distribution.
Another promising strategy is to build more structured visual representations of the environment, including the use of particles [31, 33, 36], keypoints [32, 37, 38], and object meshes [22]. Among the structured representations, Driess et al. [10] leveraged compositional neural implicit represen-tations in combination with graph neural networks (GNNs) for the dynamic modeling of multi-object interactions. The inductive bias introduced by GNNs captures the environ-ment’s underlying structure, enabling generalization to sce-narios containing more objects than during training, and the neural implicit representations allow precise estimation and modeling of object geometry and interactions. However,
Driess et al. [10] only considered objects of uniform color
in well-lit scenarios. It is unclear how the method works for objects with more complicated geometries and textures.
The lack of explicit modeling of light transport also limits its use in scenarios of varying lighting conditions, especially those vastly different from the training distributions.
In this paper, we propose to combine object-centric neural scattering functions (OSFs) [57] and graph neural networks for the dynamics modeling and manipulation of multi-object scenes. OSFs explicitly model light transport and learn to approximate the cumulative radiance transfer, which allows relighting and inverse estimation of scenes involving multiple objects and the change of lights, such as those shown in Figure 1. Combined with gradient-free evolutionary algorithms like covariance matrix adaption (CMA), the learned neural implicit scattering functions sup-port inverse parameter estimation, including object poses and light directions, from visual observations. Based on the estimated scene parameters, a graph-based neural dynamics model considers the interactions between objects and pre-dicts the evolution of the underlying system. The predictive model can then be used within a model-predictive control (MPC) framework for downstream manipulation tasks.
Experiments demonstrate that our method performs more accurate reconstruction in harsh lighting conditions compared to prior methods, producing higher-ﬁdelity long horizon prediction compared to video prediction models.
When combined with inverse parameter estimation, our en-tire control pipeline improves on simulated object manipu-lation tasks in settings with varying lighting and previously unseen object conﬁgurations, compared to performing MPC directly in image space.
We make three contributions. First, the use of neural scattering functions supports inverse parameter estimation in scenarios with challenging and previously unseen light-ing conditions. Second, our method models the composi-tionality of the underlying scene and can make long-term future predictions about the system’s evolution to support downstream planning tasks. Third, we conduct and show successful manipulation of simulated multi-object scenes involving extreme lighting directions. 2.