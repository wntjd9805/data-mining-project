Abstract 1.

Introduction
With the development of high-definition display devices, the practical scenario of Super-Resolution (SR) usually needs to super-resolve large input like 2K to higher reso-lution (4K/8K). To reduce the computational and memory cost, current methods first split the large input into local patches and then merge the SR patches into the output.
These methods adaptively allocate a subnet for each patch.
Quantization is a very important technique for network ac-celeration and has been used to design the subnets. Current methods train an MLP bit selector to determine the propoer bit for each layer. However, they uniformly sample subnets for training, making simple subnets overfitted and compli-cated subnets underfitted. Therefore, the trained bit selector fails to determine the optimal bit. Apart from this, the in-troduced bit selector brings additional cost to each layer of the SR network. In this paper, we propose a novel method named Content-Aware Bit Mapping (CABM), which can re-move the bit selector without any performance loss. CABM also learns a bit selector for each layer during training. Af-ter training, we analyze the relation between the edge in-formation of an input patch and the bit of each layer. We observe that the edge information can be an effective metric for the selected bit. Therefore, we design a strategy to build an Edge-to-Bit lookup table that maps the edge score of a patch to the bit of each layer during inference. The bit con-figuration of SR network can be determined by the lookup tables of all layers. Our strategy can find better bit configu-ration, resulting in more efficient mixed precision networks.
We conduct detailed experiments to demonstrate the gener-alization ability of our method. The code will be released.
*This work was supported by the Fundamental Research Funds for the
Central Universities (2022JBMC013), the National Natural Science Foun-dation of China (61976017 and 61601021), and the Beijing Natural Sci-ence Foundation (4202056). Shunli Zhang is the corresponding author.
Single Image Super-Resolution (SISR) is an important computer vision task that reconstructs a High-Resolution (HR) image from a Low-Resolution (LR) image. With the advent of Deep Neural Networks (DNNs), lots of DNN-based SISR methods have been proposed over the past few years [6, 17, 24, 27, 34]. While in real-world usages, the resolutions of display devices have already reached 4K or even 8K. Apart from normal 2D images, the resolutions of omnidirectional images might reach even 12K or 16K.
Therefore, SR techniques with large input are becoming crucial and have gained increasing attention from the com-munity [4, 12, 18, 28].
Since the memory and computational cost will grow quadratically with the input size, existing methods [4, 12, 18,28] first split the large input into patches and then merge the SR patches to the output. They reduce the computational cost by allocating simple subnets to those flat regions while using heavy subnets for those detailed regions. Therefore, how to design the subnets is very important for these meth-ods. [4, 18] empirically decide the optimal channels after lots of experiments to construct the subnets. [28] proposes to train a regressor to predict the incremental capacity of each layer. Thus they can adaptively construct the subnets by reducing the layers. Compared with pruning the chan-nels or layers, quantization is another promising technique and can achieve more speedup. [12] trains an MLP bit selec-tor to determine the proper bit for each layer given a patch.
However, the introduced MLP of each layer brings addi-tional computational and storage cost. Besides, we observe that [12] uniformly samples the subnets for training, mak-ing simple subnets (low average bit or flat patches) tend to overfit the inputs while complicated subnets (high average bit or detailed patches) tend to underfit the inputs. There-fore, uniform sampling fails to determine the optimal bit for each layer.
To solve the limitations of [12], we propose a novel
Figure 1. The pipeline of our CABM method. pi ∈ {pi}i=1...K is the probability of choosing ith quantization module and each quantiza-tion module uses different bit-width to quantize the input activation. During training, our method learns an MLP bit selector to adaptively choose the bit-width for each convolution. While during inference, we use the proposed CABM to build an Edge-to-Bit lookup table to determine the bit-width with negligible additional cost. method named Content-Aware Bit Mapping (CABM), which directly uses a lookup table to generate the bit of each layer during inference. However, building a lookup table is difficult since there are thousands of patches and corre-sponding select bits. We observe that edge information can be an effective metric for patch representation. Therefore, we analyze the relation between the edge information of a patch and the bit of each layer. Inspired by the fact that a
MLP selector learns the nonlinear mapping between a patch and the bit, instead of building the Edge-to-Bit lookup ta-ble based on linear mapping, we design a tactful calibration strategy to map the edge score of a patch to the bit of each layer. The bit configuration of SR network can be deter-mined by the lookup tables of all layers. Our CABM can achieve the same performance compared with the MLP se-lectors while resulting in a lower average bit and negligi-ble additional computational cost. Our contributions can be concluded as follows:
• We propose a novel method that maps edge informa-tion to bit configuration of SR networks, significantly reducing the memory and computational cost of bit se-lectors without performance loss.
• We present a tactful calibration strategy to build the
Edge-to-Bit lookup tables, resulting in a lower average bit for SR networks.
• We conduct detailed experiments to demonstrate the generalization ability of our method based on various
SR architectures and scaling factors. 2.