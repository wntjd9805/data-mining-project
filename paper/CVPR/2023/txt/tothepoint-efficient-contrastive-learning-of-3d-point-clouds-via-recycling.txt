Abstract
Recent years have witnessed significant developments in point cloud processing, including classification and seg-mentation. However, supervised learning approaches need a lot of well-labeled data for training, and annotation is labor- and time-intensive. Self-supervised learning, on the other hand, uses unlabeled data, and pre-trains a back-bone with a pretext task to extract latent representations to be used with the downstream tasks. Compared to 2D im-ages, self-supervised learning of 3D point clouds is under-explored. Existing models, for self-supervised learning of 3D point clouds, rely on a large number of data sam-ples, and require significant amount of computational re-sources and training time. To address this issue, we pro-pose a novel contrastive learning approach, referred to as
ToThePoint. Different from traditional contrastive learning methods, which maximize agreement between features ob-tained from a pair of point clouds formed only with dif-ferent types of augmentation, ToThePoint also maximizes the agreement between the permutation invariant features and features discarded after max pooling. We first per-form self-supervised learning on the ShapeNet dataset, and then evaluate the performance of the network on different downstream tasks.
In the downstream task experiments, performed on the ModelNet40, ModelNet40C, Scanob-jectNN and ShapeNet-Part datasets, our proposed ToThe-Point achieves competitive, if not better results compared to the state-of-the-art baselines, and does so with significantly less training time (200 times faster than baselines). 1.

Introduction
In recent years, self-supervised methods, which pretrain a backbone with pretext tasks to extract useful latent rep-resentations, have become increasingly effective [16]. For example, self-supervised tasks can be set to distinguish pos-itive and negative samples or restore damaged images, and
*This work was supported in part by the National Natural Sci-ence Foundation of China under Grant No. 61972145 and 61932010, and the Key R&D Program of Hunan Province under Grant No. 2022GK2069. D. Wu and X. Li are the corresponding authors (e-mail:
{dwu,lixinglin}@hnu.edu.cn).
Figure 1. A running example of ToThePoint. Raw 3D point cloud data is streamed through two branches; in each branch, nor-malization and data augmentation are performed followed by tra-ditional max-pooling operations. In our recycling mechanism, the
N × M dimensional features are sorted and a row of features is randomly selected (from the remaining rows after max-pooling) as the recycled aligned features to assist the representation of per-mutation invariant features. The four features extracted from the two branches are next subjected to two stages of contrastive learn-ing. Then the learning result would be mapped on the hypersphere. these self-supervised pre-training tasks have been proven to provide rich latent feature representations for downstream tasks to improve their performance [5, 8, 10]. For the tasks, for which dataset labeling is difficult, such as detec-tion [33], segmentation [12] or video tracking tasks [27], unsupervised pre-training can be especially helpful by alle-viating the issue of insufficient labelled data. Moreover, it has been shown that self-supervised pre-training combined with supervised training provides better performance than traditional fully supervised learning by itself [11, 34, 36].
With the ever increasing availability of LiDAR sensors and stereo cameras, more and more point cloud data can be and have been captured. However, annotating this data is dif-ficult, providing additional incentive for self-supervised al-gorithms developed for 3D point clouds.
There have been some works exploring self-supervised representation learning from point clouds, mainly based on generative models [29], reconstruction [20, 26] and other pretext tasks [34]. However, existing methods require large amounts, even millions of data samples, for self-supervised pre-training [9], making them computationally more expen-sive and time-consuming. Among traditional point cloud networks, PointNet [18] is a pioneering, end-to-end 3D point cloud analysis work. It obtains permutation-invariant features by adopting the max-pooling operation. There have been many subsequent works adopting this structure [14, 19]. Yet, the max-pooling operation discards a large num-ber of points and their features. Chen et al. [4] have shown that these discarded features are still useful and, when recy-cled, can boost performance; and proposed recycling to im-prove the performance of fully-supervised 3D point cloud processing tasks, including classification and segmentation.
In this work, different from [4], we perform recycling differently, and also use the discarded point cloud features as a feature augmentation method for contrastive learning.
This augmentation approach can allow having less train-ing samples for self-supervised training, i.e. it can enable the self-supervised pre-training of a point cloud network without requiring large amounts of point cloud data. We achieve this by making good use of the point cloud features discarded by the max-pooling module of the point cloud network. Performing self-supervised learning with a small amount of point cloud data can also allow downstream tasks to get a competitive result.
We propose ToThePoint to accelerate self-supervised pretraining of 3D point cloud features, as shown by the ex-ample in Fig. 1. Compared to previous baselines, which re-quire a large number of training samples and longer training time, our proposed work achieves its accuracy levels with only a fraction of samples during pre-training. The goal of our work is to introduce the distribution of the maximum aggregated features and the recycled point cloud features into the hypersphere space through a contrastive learning method. The maximum aggregated feature and the recycled point cloud feature from the same sample are regarded as a cluster. Contrastive learning is used to make the maximum aggregated feature become the centroid of the cluster, so that the maximum aggregated feature can better represent the sample.
Contributions. The main contributions of this work in-clude the following:
• We first demonstrate that the point cloud features, dis-carded by the max-pooling module of a point cloud net-work, can be recycled and used as a feature augmentation method for contrastive learning.
• We propose a two-branch contrastive learning framework, which incorporates a cross-branch contrastive learning loss and an intra-branch contrastive learning loss.
• We perform extensive experiments to evaluate our pro-posed method on three downstream tasks, namely object classification, few-shot learning, and part segmentation on synthetic and real datasets of varying scales. The re-sults show that our method achieves competitive if not better results compared to the state-of-the-art baselines, and does so with significantly less training time and fewer training samples.
• We perform ablation studies analyzing the effects of in-dividual loss terms and their combinations on the perfor-mance. 2.