Abstract 3D object detection from visual sensors is a corner-stone capability of robotic systems. State-of-the-art meth-ods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intu-ition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection frame-work that exploits 3D multi-view geometry to improve lo-calization through viewpoint awareness and equivariance.
VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geom-etry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames dur-ing training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geometric cues for 3D ob-ject detection, leading to state-of-the-art performance on the nuScenes benchmark. The code and model are made available at https://github.com/TRI-ML/VEDet. 1.

Introduction
Camera-based 3D object detection is a critical research topic, with important applications in areas such as au-tonomous driving and robotics due to the semantic-rich in-In the past put and low cost compared to range sensors. few years, monocular 3D detection has seen significant progress, from relying on predicting pseudo point clouds as intermediate representation [33, 39, 42] to end-to-end learn-ing [31,34,38]. However, monocular 3D detectors are inher-ently ambiguous in terms of depth, which motivated some recent exploration in multi-view and multi-sweep 3D object detection [22, 25, 26, 40].
In a conventional monocular setting, given multiple cam-eras on a sensor rig, single-view detections are merged to the global frame through rule-based processing such as
Non-Maximum Suppression (NMS). Recent advances in
Figure 1. Our proposed VEDet encodes the 3D scene from multi-view images, and decodes objects with view-conditioned queries.
The predicted 3D bounding boxes are expressed in the underlying views of the queries, which enables us to enforce viewpoint equiv-ariance among predictions from multiple views. Virtual query views are generated during training and together with the view-point equivariance regularization bring richer geometric learning signals to guide the model to better understand the 3D structure in the scene. During inference, the global predictions can be ob-tained by simply choosing the global frame as the query view. multi-view camera-based 3D algorithms [25, 40] proposed to jointly aggregate multi-view information at the feature level, and directly predict a single set of detections in the global frame. These algorithms demonstrate a giant leap in 3D detection performance on multi-camera datasets (E.g.,
Nuscenes [5]). To aggregate information from different views, one line of query-based detectors adopt transform-ers to query image features [25, 26, 40] or bird’s-eye-view
In (BEV) features [15, 22] via an attention mechanism. contrast, another line of works “lift-splat-shoot” [32] image features from each view into the shared BEV features to be processed by convolutional detection heads [21]. To further mitigate the depth ambiguity, some concurrent works have started extending multi-view to “multi-sweep” across times-tamps and observe a promising performance boost [22, 26].
While the works mentioned above demonstrate a strong po-tential for multi-view 3D detection, progress has concen-trated on input aggregation and information interplay across frames and less on learning objectives. We argue that the learning objective can play a crucial role in ingesting the core knowledge in a multi-view setting: 3D geometry.
This paper proposes to encourage 3D geometry learn-ing for multi-view 3D detection models through viewpoint awareness and equivariance. We obtain our intuition from traditional structure-from-motion works [1], where multi-view geometry is modeled through multi-view consistency.
To this end, we propose viewpoint-awareness on the object queries, as well as a multi-view consistency learning ob-jective as a 3D regularizer that enforces the model to rea-son about geometry. Compared to existing methods that make 3D predictions in the default egocentric view, our pro-posed multi-view predictions and viewpoint equivariance effectively bring stronger geometric signals conducive to the 3D reasoning. More specifically, in our query-based framework, the geometry information of image features and object queries is injected completely via implicit geomet-ric encodings, and the transformer decoder is expected to learn better correspondence and 3D localization under the viewpoint equivariance objective. We demonstrate that our proposed framework can make the best of available geom-etry information with extensive experiments and establish the new state-of-the-art in multi-view 3D object detection.
In summary, our contributions are:
• We propose a novel Viewpoint Equivariance (VE) learning objective that encourages multi-view consis-tency in 3D detection models, leading to improved 3D object detection performance.
• We propose a new multi-view 3D object detection framework, VEDet, which employs a query-based transformer architecture with perspective geometry and viewpoint awareness injected both at the encod-ing and decoding stages. VEDet fully enables our proposed VE learning objective, facilitating geometry learning with implicit inductive biases.
• VEDet achieves state-of-the-art on large-scale benchmark, reaching 45.1%mAP on NuScenes val set and 50.5% mAP on test set. We provide a com-prehensive analysis of our components, and share in-sights based on empirical observations. 2.