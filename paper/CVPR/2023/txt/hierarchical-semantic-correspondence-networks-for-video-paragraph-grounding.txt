Abstract
Video Paragraph Grounding (VPG) is an essential yet challenging task in vision-language understanding, which aims to jointly localize multiple events from an untrimmed video with a paragraph query description. One of the crit-ical challenges in addressing this problem is to compre-hend the complex semantic relations between visual and textual modalities. Previous methods focus on modeling the contextual information between the video and text from a single-level perspective (i.e., the sentence level), ignor-ing rich visual-textual correspondence relations at different semantic levels, e.g., the video-word and video-paragraph correspondence. To this end, we propose a novel Hierar-chical Semantic Correspondence Network (HSCNet), which explores multi-level visual-textual correspondence by learn-ing hierarchical semantic alignment and utilizes dense su-pervision by grounding diverse levels of queries. Specifi-cally, we develop a hierarchical encoder that encodes the multi-modal inputs into semantics-aligned representations at different levels. To exploit the hierarchical semantic cor-respondence learned in the encoder for multi-level supervi-sion, we further design a hierarchical decoder that progres-sively performs finer grounding for lower-level queries con-ditioned on higher-level semantics. Extensive experiments demonstrate the effectiveness of HSCNet and our method significantly outstrips the state-of-the-arts on two challeng-ing benchmarks, i.e., ActivityNet-Captions and TACoS. 1.

Introduction
As a fundamental problem that bridges the gap between computer vision and natural language processing, Video
Language Grounding (VLG) aiming to localize the video
*Corresponding author
Figure 1. (a) The bottom-up hierarchy for linguistic semantics is composed of textual words, sentences, and the paragraph. (b) The coarse-to-fine hierarchy for temporal granularity consists of visual counterparts for each level of linguistic query. segments corresponding to given natural language queries, has been drawing increasing attention from the community in these years. Early works in the field of VLG mainly focused on addressing Video Sentence Grounding (VSG)
[1, 8], whose goal is to localize the most relevant moment with a single sentence query. Recently, Video Paragraph
Grounding (VPG) is introduced in [2]. It requires to jointly localize multiple events via a paragraph query consisting of several temporally ordered sentences. Rather than ground-ing each event independently, VPG needs to further exploit the contextual information between the video and the tex-tual paragraph, which helps to avoid ambiguity and achieve more precise temporal localization of video events.
Previous VPG works [2,12,26] commonly explore corre-lations of events by modeling the video-text correspondence from a single semantic level (i.e., the sentence level). How-ever, they neglect the rich visual-textual correspondence at other semantic levels, such as the word level and paragraph level, which can also provide some useful information for grounding events in the video. Considering the grounding of “The man stops and hits the ball far away”, the seman-tic relations between video content and the word “hits” is crucial in determining the end time of the event. Besides, when we consider the paragraph as a whole, then ground-ing the holistic paragraph in the video first is beneficial to suppress the irrelevant events or backgrounds, which eases the further grounding of sentences.
To be more general, we observe that there naturally exist two perspectives of hierarchical semantic structures in tack-ling VPG, which is intuitively illustrated in Figure 1. On the language side, Figure 1 (a) shows that the semantics of paragraph query can be divided into an inherent three-level hierarchy consisting of words, sentences, and the holistic paragraph in a bottom-up organization. On the video side,
Figure 1 (b) shows that the temporal counterparts of dif-ferent levels of queries also form a three-level granular-ity hierarchy with temporally nested dependencies from the top down. By relating the video content to different lev-els of query semantics for multi-level query grounding, the model is enforced to capture more complex relations be-tween events by reasoning about their interconnections at different semantic granularities, and exploit richer temporal clues to facilitate the grounding of events in the video.
Motivated by the above observations, we propose a novel framework termed as Hierarchical Semantic Correspon-dence Network (HSCNet) for VPG. Our HSCNet is de-signed as a multi-level encoder-decoder architecture in or-der to leverage hierarchical semantic information from the two perspectives. On the one hand, we learn the hierar-chical visual-textual semantic correspondence by gradually aligning the visual and textual semantics into different lev-els of common spaces from the bottom up. Concretely, we construct a hierarchical multi-modal encoder on top of the linguistic semantic hierarchy. It comprises three semantic levels of visual-textual encoders. Each encoder receives the semantic representation from a lower level and continues to establish visual-textual correspondence at a higher level via iterative multi-modal interactions. On the other hand, we utilize richer cross-level contexts and denser supervi-sion by progressively grounding multiple levels of queries from coarse to fine. Specifically, we construct a hierarchical progressive decoder on top of the temporal granularity hier-archy, which also comprises three levels of decoders. The lower-level queries are grounded by finer temporal bound-aries conditioned on contextual knowledge from higher-level queries, which eases the learning of multi-level lo-calization that provides diverse temporal clues to promote fine-grained video paragraph grounding.
We evaluate the proposed HSCNet on two challenging benchmarks, i.e., ActivityNet-Captions and TACoS. Ex-tensive ablation studies validate the effectiveness of the method. Our contributions can be summarized as follows:
• We investigate and propose a novel hierarchical model-ing framework for Video Paragraph Grounding (VPG).
To the best of our knowledge, it’s the first time in the problem of VPG that hierarchical visual-textual se-mantic correspondence is explored and multiple levels of linguistic queries can be grounded.
• We design a novel encoder-decoder architecture to learn multi-level visual-textual correspondence by hi-erarchical semantic alignment and progressively per-form finer grounding for lower-level queries.
• Experiments demonstrate that our proposed HSCNet achieves new state-of-the-art results on the challeng-ing ActivityNet-Captions and TACoS benchmarks, re-markably surpassing the previous approaches. 2.