Abstract
Most anomaly detection (AD) models are learned using only normal samples in an unsupervised way, which may result in ambiguous decision boundary and insufﬁcient dis-criminability.
In fact, a few anomaly samples are often available in real-world applications, the valuable knowl-edge of known anomalies should also be effectively ex-ploited. However, utilizing a few known anomalies dur-ing training may cause another issue that the model may be biased by those known anomalies and fail to generalize to unseen anomalies.
In this paper, we tackle supervised anomaly detection, i.e., we learn AD models using a few available anomalies with the objective to detect both the seen and unseen anomalies. We propose a novel explicit boundary guided semi-push-pull contrastive learning mech-anism, which can enhance model’s discriminability while mitigating the bias issue. Our approach is based on two core designs: First, we ﬁnd an explicit and compact sepa-rating boundary as the guidance for further feature learn-ing. As the boundary only relies on the normal feature dis-tribution, the bias problem caused by a few known anoma-lies can be alleviated. Second, a boundary guided semi-push-pull loss is developed to only pull the normal fea-tures together while pushing the abnormal features apart from the separating boundary beyond a certain margin re-gion. In this way, our model can form a more explicit and discriminative decision boundary to distinguish known and also unseen anomalies from normal samples more effec-tively. Code will be available at https://github. com/xcyao00/BGAD. 1.

Introduction
Anomaly detection (AD) has received widespread atten-tion in diverse domains, such as industrial defect inspec-*Corresponding Author. tion [4,8,10,50] and medical lesion detection [12,38]. Most previous anomaly detection methods [1, 3, 5, 6, 8, 10, 15, 30, 33, 47, 50–52] are unsupervised and pay much attention to normal samples while inadvertently overlooking anomalies, because it is difﬁcult to collect sufﬁcient and all kinds of anomalies. However, learning only from normal samples may limit the discriminability of the AD models [12, 24].
As illustrated in Figure 1(a), without anomalies, the de-cision boundaries are generally implicit and not discrimi-native enough. The insufﬁcient discriminability issue is a common issue in unsupervised anomaly detection due to the lack of knowledge about anomalies. In fact, a few anoma-lies are usually available in real-world applications, which can be exploited effectively to address or alleviate this issue.
Recently, methods that can be called semi-supervised
AD [27,34,36] or AD with outlier exposure [16,17] begin to focus on those available anomalies. These methods attempt to learn knowledge from anomalies by one-class classiﬁca-tion with anomalies as negative samples [34, 36] or by su-pervised binary classiﬁcation [16, 17] or by utilizing the de-viation loss to optimize one anomaly scoring network [27].
They show a fact that the detection performance can be im-proved signiﬁcantly even with a few anomalies. However, the known anomalies can’t represent all kinds of anomalies.
These methods may be biased by the known anomalies and fail to generalize to unseen anomalies (see Figure 5).
Therefore, to address the two above issues, we tackle supervised anomaly detection [12], in which a few known anomalies can be effectively exploited to train discrimina-tive AD models with the objective to improve detection per-formance on the known anomalies and generalize well to unseen anomalies. Compared with unsupervised AD, su-pervised AD is more meaningful for real-world AD appli-cations, because the detected anomalies can be used to fur-ther improve the discriminability and generalizability of the model. To this end, we propose a novel Boundary Guided
Anomaly Detection (BGAD) model, which has two core
(cid:40)(cid:91)(cid:83)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:3)(cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:92)(cid:3)(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:51)(cid:75)(cid:68)(cid:86)(cid:72) (cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:92)(cid:3)(cid:42)(cid:88)(cid:76)(cid:71)(cid:72)(cid:71)(cid:3)(cid:50)(cid:83)(cid:87)(cid:76)(cid:80)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:51)(cid:75)(cid:68)(cid:86)(cid:72) (cid:39) (cid:72) (cid:81) (cid:86) (cid:76) (cid:87) (cid:92) (cid:44)(cid:80)(cid:83)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:3) (cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:76)(cid:72)(cid:86) (cid:39) (cid:72) (cid:81) (cid:86) (cid:76) (cid:87) (cid:92) (cid:41)(cid:76)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:40)(cid:91)(cid:83)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:3)(cid:54)(cid:72)(cid:83)(cid:68)(cid:85)(cid:68)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:92) (cid:39) (cid:72) (cid:81) (cid:86) (cid:76) (cid:87) (cid:92) (cid:36)(cid:69)(cid:81)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:3) (cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:92) (cid:49)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:3) (cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:92) (cid:40)(cid:91)(cid:83)(cid:79)(cid:76)(cid:70)(cid:76)(cid:87)(cid:3) (cid:37)(cid:82)(cid:88)(cid:81)(cid:71)(cid:68)(cid:85)(cid:76)(cid:72)(cid:86) (cid:36)(cid:80)(cid:69)(cid:76)(cid:74)(cid:88)(cid:82)(cid:88)(cid:86) (cid:47)(cid:82)(cid:74)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71) (cid:11)(cid:68)(cid:12) (cid:47)(cid:82)(cid:74)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71) (cid:56)(cid:81)(cid:68)(cid:80)(cid:69)(cid:76)(cid:74)(cid:88)(cid:82)(cid:88)(cid:86) (cid:47)(cid:82)(cid:74)(cid:16)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71) (cid:11)(cid:69)(cid:12) (cid:11)(cid:70)(cid:12)
Figure 1. Conceptual illustration of our method. (a) In most unsupervised AD models, the anomaly score distribution usually has ambigu-ous regions, which makes it difﬁcult to get one ideal decision boundary. E.g., the left boundary will cause many false negatives, while the right boundary may induce many false positives. (b) With the normalized normal feature distribution, a pair of explicit and compact (close to the normal distribution) boundaries can be obtained easily. (c) With the proposed BG-SPP loss, boundary guided optimizing can be implemented to obtain an unambiguous anomaly score distribution: a signiﬁcant gap between the normal and abnormal distributions. designs as illustrated in Figure 1: explicit boundary gener-ating and boundary guided optimizing.
• Explicit Boundary Generating. We ﬁrst employ nor-malizing ﬂow [14] to learn a normalized normal feature dis-tribution, and obtain an explicit separating boundary, which is close to the normal feature distribution edge and con-trolled by a hyperparameter β (i.e., the normal boundary in Figure 1(b)). The obtained explicit separating boundary only relies on the normal distribution and has no relation with the abnormal samples, thus the bias problem caused by the insufﬁcient known anomalies can be mitigated.
• Boundary Guided Optimizing. After obtaining the explicit separating boundary, we then propose a boundary guided semi-push-pull (BG-SPP) loss to exploit anomalies for learning more discriminative features. With the BG-SPP loss, only the normal features whose log-likelihoods are smaller than the boundary are pulled together to form a more compact normal feature distribution (semi-pull); while the abnormal features whose log-likelihoods are larger than the boundary are pushed apart from the bound-ary beyond a certain margin region (semi-push).
In this way, our model can form a more explicit and dis-criminative separating boundary and also a reliable margin region for distinguishing anomalies more effectively (see
Figure 1(c), 6). Furthermore, rarity is a critical problem of anomalies and may cause feature learning inefﬁcient. We thus propose RandAugment-based Pseudo Anomaly Gener-ation, which can simulate anomalies by creating local irreg-ularities in normal samples, to tackle the rarity challenge.
In summary, we make the following main contributions: 1. We propose a novel Explicit Boundary Guided super-vised AD modeling method, in which both normal and ab-normal samples are exploited effectively by well-designed explicit boundary generating and boundary guided optimiz-ing. With the proposed AD method, higher discriminability and lower bias risk can be achieved simultaneously. 2. To exploit a few known anomalies effectively, we pro-pose a BG-SPP loss to pull together normal features while pushing abnormal features apart from the separating bound-ary, thus more discriminative features can be learned. 3. We achieve SOTA results on the widely-used
MVTecAD benchmark, with the performance of 99.3% image-level AUROC and 99.2% pixel-level AUROC. 2.