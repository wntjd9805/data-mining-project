Abstract
Learning models trained on biased datasets tend to ob-serve correlations between categorical and undesirable fea-tures, which result in degraded performances. Most exist-ing debiased learning models are designed for centralized machine learning, which cannot be directly applied to dis-tributed settings like federated learning (FL), which col-lects data at distinct clients with privacy preserved. To tackle the challenging task of debiased federated learn-ing, we present a novel FL framework of Bias-Eliminating
Augmentation Learning (FedBEAL), which learns to de-ploy Bias-Eliminating Augmenters (BEA) for producing client-specific bias-conflicting samples at each client. Since the bias types or attributes are not known in advance, a unique learning strategy is presented to jointly train BEA with the proposed FL framework. Extensive image clas-sification experiments on datasets with various bias types confirm the effectiveness and applicability of our FedBEAL, which performs favorably against state-of-the-art debiasing and FL methods for debiased FL. 1.

Introduction language processing [8].
Deep neural networks have shown promising progress across different domains such as computer vision [14] and natural
Their successes are typically based on the collection of and training on data that properly describe the inherent distribution of the data of interest. However, in real-world scenarios, biased data [24] are often observed during data collection. Biased datasets [10, 22, 42] contain features that are highly cor-related to class labels in the training dataset but not suffi-ciently describing the inherent semantic meaning. Training on such biased data thus result in degraded model general-ization capability. Take Fig. 1 for example; when address-ing the cat-dog classification task, training images collected by users might contain only orange cats and black dogs.
Their color attributes are strongly correlated with the image labels during training, but such attributes are not necessar-ily relevant to the classification task during inference. As
Figure 1. Example of local data bias in FL. When deploying
FL to train a cat-dog classifier with image datasets collected by multiple pet owners, most of the local images are obtained with their pets with specific colors. Therefore, the models trained with each local dataset are likely to establish decision rules on biased attributes (e.g., fur color), which prevents the aggregated model from learning proper representation for classification. pointed out in [10, 42], deep neural networks trained with such biased data are more likely to make decisions based on bias attributes instead of semantic attributes. As a re-sult, during inference, performances of the learned models would dramatically drop when observing bias-conflicting samples (i.e., data containing semantic and bias attributes that are rarely correlated in the training set).
To tackle the data bias problem, several works have been proposed to remove or alleviate data bias when training deep learning models [6, 11, 18, 24, 27, 32, 36, 40]. For ex-ample, Nam et al. [36] train an intentionally biased auxil-iary model while enforcing the main model to go against the prejudice of the biased network. Lee et al. [27] utilize the aforementioned biased model to synthesize diverse bias-conflicting hidden features for learning debiased represen-tations. Nevertheless, the above techniques are designed for centralized datasets. When performing distributed training of learning models, such methods might fail to generalize.
For distributed learning, federated learning (FL) [35]
particularly considers data collection and training con-ducted at each client, with data privacy needing to be pre-served. When considering privately distributed datasets, real-world FL applications are more likely to suffer data i.e., data collected by heterogeneity issues [20, 28, 51], different clients are not independent and identically dis-tributed (IID). Recently, several works [19,21,29–31,34,47] propose to alleviate performance degradation caused by data heterogeneity. However, existing methods typically consider data heterogeneity in terms of label distribution skew [21, 29, 30, 34, 47] or domain discrepancy [19, 31] among clients. These FL methods are not designed to tackle potential data bias across different clients, leaving the debi-ased FL a challenging task to tackle.
To mitigate the local bias in Federated learning, we pro-pose a novel FL scheme of Bias-Eliminating Augmentation
Learning (FedBEAL). In FedBEAL, we learn a Bias-Eliminating Augmenter (BEA) for each client, with the goal of producing bias-conflicting samples. To identify and in-troduce the desirable semantic and bias attributes to the aug-mented samples, our FedBEAL uniquely adopts the global server model and each client model trained across iterations without prior knowledge of bias type or annotation. With the introduced augmenter and the produced bias-conflicting samples, debiased local updates can be performed at each client, followed by simple aggregation of such models for deriving the server model.
We now summarize the contributions of this work below:
• To the best of our knowledge, We are among the first to tackle the problem of debiased federated learning, in which local yet distinct biases exist at the client level.
• We present FedBEAL for debiased FL, which intro-duces Bias-Eliminating Augmenters (BEA) at each client with the goal of generating bias-conflicting sam-ples to eliminate local data biases.
• Learning of BEA can be realized by utilizing the global server and local client models trained across iterations, which allows us to identify and embed desirable se-mantic and bias features for augmentation purposes. 2.