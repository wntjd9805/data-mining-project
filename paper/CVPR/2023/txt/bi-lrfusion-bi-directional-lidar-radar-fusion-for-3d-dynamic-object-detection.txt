Abstract
LiDAR and Radar are two complementary sensing ap-proaches in that LiDAR specializes in capturing an object’s 3D shape while Radar provides longer detection ranges as well as velocity hints. Though seemingly natural, how to efficiently combine them for improved feature representa-tion is still unclear. The main challenge arises from that
Radar data are extremely sparse and lack height informa-tion. Therefore, directly integrating Radar features into
LiDAR-centric detection networks is not optimal.
In this work, we introduce a bi-directional LiDAR-Radar fusion framework, termed Bi-LRFusion, to tackle the challenges and improve 3D detection for dynamic objects. Technically,
Bi-LRFusion involves two steps: first, it enriches Radar’s local features by learning important details from the LiDAR branch to alleviate the problems caused by the absence of height information and extreme sparsity; second, it com-bines LiDAR features with the enhanced Radar features in a unified bird’s-eye-view representation. We conduct ex-tensive experiments on nuScenes and ORR datasets, and show that our Bi-LRFusion achieves state-of-the-art perfor-mance for detecting dynamic objects. Notably, Radar data in these two datasets have different formats, which demon-strates the generalizability of our method. Codes are avail-able at https://github.com/JessieW0806/Bi-LRFusion.
Figure 1. An illustration of (a) uni-directional LiDAR-Radar fu-sion mechanism, (b) our proposed bi-directional LiDAR-Radar fu-sion mechanism, and (c) the average precision gain (%) of uni-directional fusion method RadarNet∗ against the LiDAR-centric baseline CenterPoint [40] over categories with different average height (m). We use * to indicate it is re-produced by us on the Cen-terPoint. The improvement by involving Radar data is not consis-tent for objects with different height, i.e., taller objects like truck, bus and trailer do not enjoy as much performance gain. Note that all height values are transformed to the LiDAR coordinate system. 1.

Introduction
LiDAR has been considered as the primary sensor in the perception subsystem of most autonomous vehicles (AVs) due to its capability of providing accurate position mea-surements [9, 16, 32]. However, in addition to object po-sitions, AVs are also in an urgent need for estimating the motion state information (e.g., velocity), especially for dy-namic objects. Such information cannot be measured by
*Corresponding Author: Jiajun Deng and Yanyong Zhang.
LiDAR sensors since they are insensitive to motion. As a result, millimeter-wave Radar (referred to as Radar in this paper) sensors are engaged because they are able to infer the object’s relative radial velocity [21] based upon the Doppler effect [28]. Besides, on-vehicle Radar usually offers longer detection range than LiDAR [36], which is particularly use-ful on highways and expressways.
In the exploration of combining LiDAR and Radar data for ameliorating 3D dy-namic object detection, the existing approaches [22, 25, 36] follow the common mechanism of uni-directional fusion,
as shown in Figure 1 (a). Specifically, these approaches di-rectly utilize the Radar data/feature to enhance the LiDAR-centric detection network without first improving the qual-ity of the feature representation of the former.
However, independently extracted Radar features are not enough for refining LiDAR features, since Radar data are extremely sparse and lack the height information1. Specifi-cally, taking the data from the nuScenes dataset [4] as an ex-ample, the 32-beam LiDAR sensor produces approximately 30,000 points, while the Radar sensor only captures about 200 points for the same scene. The resulting Radar bird’s eye view (BEV) feature hardly attains valid local infor-mation after being processed by local operators (e.g., the neighbors are most likely empty when a non-empty Radar
BEV pixel is convolved by convolutional kernels). Be-sides, on-vehicle Radar antennas are commonly arranged horizontally, hence missing the height information in the vertical direction. In previous works, the height values of the Radar points are simply set as the ego Radar sensor’s height. Therefore, when features from Radar are used for enhancing the feature of LiDAR, the problematic height in-formation of Radar leads to unstable improvements for ob-jects with different heights. For example, Figure 1 (c) illus-trates this problem. The representative method RadarNet falls short in the detection performance for tall objects – the truck class even experiences 0.5% AP degradation after fusing the Radar data.
In order to better harvest the benefit of LiDAR and Radar fusion, our viewpoint is that Radar features need to be more powerful before being fused. Therefore, we first enrich the
Radar features – with the help of LiDAR data – and then integrate the enriched Radar features into the LiDAR pro-cessing branch for more effective fusion. As depicted in
Figure 1 (b), we refer to this scheme as bi-directional fu-sion. And in this work, we introduce a framework, Bi-LRFusion, to achieve this goal. Specifically, Bi-LRFusion first encodes BEV features for each modality individually.
Next, it engages the query-based LiDAR-to-Radar (L2R) height feature fusion and query-based L2R BEV feature fu-sion, in which we query and group LiDAR points and Li-DAR BEV features that are close to the location of each non-empty gird cell on the Radar feature map, respectively.
The grouped LiDAR raw points are aggregated to formu-late pseudo-Radar height features, and the grouped LiDAR
BEV features are aggregated to produce pseudo-Radar BEV features. The generated pseudo-Radar height and BEV fea-tures are fused to the Radar BEV features through concate-nation. After enriching the Radar features, Bi-LRFusion then performs the Radar-to-LiDAR (R2L) fusion in a uni-fied BEV representation. Finally, a BEV detection network 1This shortcoming is due to today’s Radar technology, which may likely change as the technology advances very rapidly, e.g., new-generation 4D Radar sensors [3]. consisting of a BEV backbone network and a detection head is applied to output 3D object detection results.
We validate the merits of bi-directional LiDAR-Radar fusion via evaluating our Bi-LRFusion on nuScenes and
Oxford Radar RobotCar (ORR) [1] datasets. On nuScenes dataset, Bi-LRFusion improves the mAP(↑) by 2.7% and reduces the mAVE(↓) by 5.3% against the LiDAR-centric baseline CenterPoint [40], and remarkably outperforms the strongest counterpart, i.e., RadarNet, in terms of AP by ab-solutely 2.0% for cars and 6.3% for motorcycles. Moreover,
Bi-LRFusion generalizes well on the ORR dataset, which has a different Radar data format and achieves 1.3% AP im-provements for vehicle detection.
In summary, we make the following contributions:
• We propose a bi-directional fusion framework, namely
Bi-LRFusion, to combine LiDAR and Radar features for improving 3D dynamic object detection.
• We devise the query-based L2R height feature fusion and query-based L2R BEV feature fusion to enrich
Radar features with the help of LiDAR data.
• We conduct extensive experiments to validate the mer-its of our method and show considerably improved re-sults on two different datasets. 2.