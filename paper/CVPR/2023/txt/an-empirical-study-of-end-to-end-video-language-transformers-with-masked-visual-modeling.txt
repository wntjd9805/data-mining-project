Abstract
Masked visual modeling (MVM) has been recently proven effective for visual pre-training. While similar re-constructive objectives on video inputs (e.g., masked frame modeling) have been explored in video-language (VidL) pre-training, previous studies fail to find a truly effective
MVM strategy that can largely benefit the downstream per-formance. In this work, we systematically examine the po-tential of MVM in the context of VidL learning. Specifically, we base our study on a fully end-to-end VIdeO-LanguagE
Transformer (VIOLET) [15], where the supervision from
MVM training can be backpropogated to the video pixel space.
In total, eight different reconstructive targets of
MVM are explored, from low-level pixel values and oriented gradients to high-level depth maps, optical flow, discrete visual tokens and latent visual features. We conduct com-prehensive experiments and provide insights into the fac-tors leading to effective MVM training, resulting in an en-hanced model VIOLETv2. Empirically, we show VIOLETv2 pre-trained with MVM objective achieves notable improve-ments on 13 VidL benchmarks, ranging from video question answering, video captioning, to text-to-video retrieval.1 1.

Introduction
Video, containing multiple modalities in nature, has been used as an epitome to test how AI systems perceive. Video-language (VidL) research aims at extending this ability to convey perception via language. Popular VidL tasks were introduced, such as text-to-video retrieval [29,54,75], video question answering [25, 74], and video captioning [6, 75].
Recent progresses in VidL learning mostly focus on VidL pre-training [49, 58, 83] with video-text matching [39, 79] and masked language modeling [10]. There have also been attempts on similar masked modeling on vision inputs. 1Code has been released at https://github.com/tsujuifu/ pytorch_empirical-mvm.
For example, masked frame modeling [39] aims to recover masked frame representations. However, the pre-extracted video features cannot be refined during pre-training, which may limit its effectiveness. More recently, VIOLET [15] designs an end-to-end video-language transformer and pro-poses to reconstruct discrete visual tokens for masked frame patches. Though showing some promises in recovering vi-sual semantics, the performance improvements on down-stream VidL tasks are still marginal.
Meanwhile, self-supervised visual pre-training has been proven highly effective by reconstructing the masked image patches through raw pixel values [21, 73], discrete visual tokens [3, 81], or visual-semantic features [70, 71]. How-ever, they all only focus on the visual modality. It is unclear which variant of masked visual modeling (MVM) objec-tives can help VidL learning, especially given that the paired language inputs can already provide high-level semantics.
Motivated by this, we conduct a comprehensive study of MVM for VidL learning. As illustrated in Figure 1, we base our study on the fully end-to-end VIdeO-LangaugeE
Transformer (VIOLET) [15], and study a broad spectrum of MVM targets, including RGB pixel values (Pixel), his-togram of oriented gradients (HOG), depth maps (Depth), optical flow (Flow), discrete visual tokens (VQ), spatial-focused image features (SIF), temporal-aware video fea-tures (TVF), and mulitmodal features (MMF). During pre-training, we mask out some proportions of the video in-put along both spatial and temporal dimensions, and the model learns to recover the MVM targets for these masked patches. Equipped with another two standard pre-training tasks (i.e., video-text matching and masked language mod-eling), we empirically verify the effectiveness of different
MVM variants on downstream VidL tasks.
Our study reveals that: (i) spatial-focused image fea-tures (SIF) is the most effective MVM target on video-text inputs; and (ii) the effects of different MVM targets on downstream VidL tasks are not shared between video-text and image-text inputs. For example, SIF extracted from the same model brings a large drop on downstream VidL
Figure 1. We systematically explore eight masked visual modeling (MVM) targets for end-to-end video-language (VidL) pre-training, including RGB pixel values (Pixel), histogram of oriented gradients (HOG), depth maps (Depth), optical flow (Flow), discrete visual tokens (VQ), spatial-focused image features (SIF), temporal-aware video features (TVF), and multimodal features from CLIP (MMF). Besides MVM, we pre-train VIOLET model [15] along with video-text matching (VTM) and masked language modeling (MLM). performance when pre-trained with image-text pairs. In ad-dition, we conduct comprehensive analyses of the masking strategy and ratio, combination of different MVM targets, to shed light on effective MVM training for VidL learning.
We name the enhanced version of the original VIOLET [15] with the best MVM strategy as VIOLETv2.
Our contributions can be summarized as follows. We present an empirical study of masked visual modeling for video-language pre-training, with comprehensive analyses to reveal the ingredients for effective MVM training. VI-OLETv2 with the best MVM recipe achieves strong per-formance on 13 VidL datasets. Concretely, compared to models pre-trained on the same 5M corpus, VIOLETv2 brings mean improvements of +5.4% accuracy on video question answering, +6.6% recall on text-to-video retrieval, and +11.4 CIDEr on video captioning. Direct comparison to
VIOLET [15] also shows notable advantages of our model, even when pre-trained with much less data. 2.