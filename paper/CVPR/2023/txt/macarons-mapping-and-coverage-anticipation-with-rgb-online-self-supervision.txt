Abstract
We introduce a method that simultaneously learns to ex-plore new large environments and to reconstruct them in 3D from color images only. This is closely related to the Next
Best View problem (NBV), where one has to identify where to move the camera next to improve the coverage of an un-known scene. However, most of the current NBV methods rely on depth sensors, need 3D supervision and/or do not scale to large scenes. Our method requires only a color camera and no 3D supervision. It simultaneously learns in a self-supervised fashion to predict a “volume occupancy field” from color images and, from this field, to predict the
NBV. Thanks to this approach, our method performs well on new scenes as it is not biased towards any training 3D data. We demonstrate this on a recent dataset made of vari-ous 3D scenes and show it performs even better than recent methods requiring a depth sensor, which is not a realistic assumption for outdoor scenes captured with a flying drone. 1.

Introduction
By bringing together Unmanned Aerial Vehicles (UAVs) and Structure-from-Motion algorithms, it is now possible to reconstruct 3D models of large outdoor scenes, for example for creating a Digital Twin of the scene. However, flying a
UAV requires expertise, especially when capturing images with the goal of running a 3D reconstruction algorithm, as the UAV needs to capture images that together cover the entire scene from multiple points of view. Our goal with this paper is to make this capture automatic by developing a method that controls a UAV and ensures a coverage suitable to 3D reconstruction.
This is often referenced in the literature as the “Next
Best View” problem (NBV) [14]: Given a set of already-captured images of a scene or an object, how should we move the camera to improve our coverage of the scene or
object? Unfortunately, current NBV algorithms are still not suitable for three main reasons. First, most of them rely on a voxel-based representation and do not scale well with the size of the scene. Second, they also rely on a depth sensor, which is in practice not possible to use on a small
UAV in outdoor conditions as it is too heavy and requires too much power. Simply replacing the depth sensor by a monocular depth prediction method [49, 56, 77] would not work as such methods can predict depth only up to a scale factor. The third limitation is that they require 3D models for learning to predict how much a pose will increase the scene coverage.
In this paper, we show that it is possible to simultane-ously learn in a self-supervised fashion to efficiently ex-plore a 3D scene and to reconstruct it using an RGB sensor only, without any 3D supervision. This makes it convenient for applications in real scenarios with large outdoor scenes.
We only assume the camera poses to be known, as done in past works on NBV [31, 48, 80]. This is reasonable as NBV methods control the camera.
The closest work to ours is probably the recent [27]. [27] proposed an approach that can scale to large scenes thanks to a Transformer-based architecture that predicts the visi-bility of 3D points from any viewpoint, rather than relying on an explicit representation of the scene such as voxels.
However, this method still uses a depth sensor. It also uses 3D meshes for training the prediction of scene coverage. To solve this, [27] relies on meshes from ShapeNet [6], which is suboptimal when exploring large outdoor scenes, as our experiments show. This limitation can actually be seen in
Figure 1: The trajectory recovered by [27] mostly focuses on the main building and does not explore the rest of the scene. By contrast, we use a simple color sensor and do not need any 3D supervision.
As our experiments show, we nonetheless significantly outperform this method thanks to our architecture and joint learning strategy. As shown in Figure 2, our architecture is made of three neural modules that communicate together: 1. Our first module learns to predict depth maps from a sequence of images in a self-supervised fashion. 2. Our second module predicts a “volume occupancy field” from a partial surface point cloud. This field is made of the probability for any input 3D point to be occupied or empty, given the previous observed im-ages of the scene. We train this module from past ex-perience, with partial surface point cloud as input and aiming to predict the occupancy field computed from the final point cloud. 3. Our third module predicts for an input camera pose the
“surface coverage gain”, i.e., how much new surface will be visible from this pose. We improve the cover-age estimation model introduced by [27] and propose a novel, much simpler loss that yields better perfor-mance. We rely on this module to identify the NBV.
While exploring a new scene and training our architec-ture, we repeat the three following steps: (1) We identify the
Next Best View where to move the camera; (2) We move the camera to this Next Best View, collect images along the way, and build a self-supervision signal from the collected images, which we store in the “Memory”; (3) We update the weights of all 3 modules using Memory Replay [50]. This avoids catastrophic forgetting and significantly speeds up training compared to a training procedure that uses only re-cent data, as such data is highly correlated. This last step es-tablishes a synergy between the different parts of the model, each one providing inputs to the other parts.
We compare to recent work [27] on their dataset made of large scale 3D scenes under the CC license. We evaluate the evolution of total surface coverage by a sensor explor-ing several 3D scenes. Our online, self-supervised approach that learns from RGB images is able to have better results than state-of-the-art methods with a perfect depth sensor.
To summarize, we propose the first deep-learning-based
NBV approach for dense reconstruction of large 3D scenes from RGB images. We call this approach MACARONS, for Mapping And Coverage Anticipation with RGB Online
Self-Supervision. Moreover, we provide a dedicated train-ing procedure for online learning for scene mapping and automated exploration based on coverage optimization in any kind of environment, with no explicit 3D supervision.
Consequently, our approach is also the first NBV method to learn in real-time to reconstruct and explore arbitrarily large scenes in a self-supervised fashion. We experimen-tally show that this greatly improves results for NBV ex-ploration of 3D scenes. It makes our approach suitable for real-life applications on small drones with a simple color camera. More fundamentally, it shows that an autonomous system can learn to explore and reconstruct environments without any 3D information a priori. We will make our code available on a dedicated webpage for allowing com-parison with future methods. 2.