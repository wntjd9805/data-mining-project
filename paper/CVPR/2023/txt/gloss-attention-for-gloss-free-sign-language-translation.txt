Abstract
Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an anal-ysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model im-plicitly learn the location of semantic boundaries in contin-uous sign language videos, 2) it can help the model under-stand the sign language video globally. We then propose gloss attention, which enables the model to keep its atten-tion within video segments that have the same semantics lo-cally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similar-ity from the natural language model to our gloss atten-tion SLT network (GASLT) to help it understand sign lan-guage videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github. com/YinAoXiong/GASLT. 1.

Introduction
Sign languages are the primary means of communication for an estimated 466 million deaf and hard-of-hearing peo-ple worldwide [52]. Sign language translation (SLT), a so-cially important technology, aims to convert sign language videos into natural language sentences, making it easier for deaf and hard-of-hearing people to communicate with hear-ing people. However, the grammatical differences between sign language and natural language [5, 55] and the unclear semantic boundaries in sign language videos make it diffi-cult to establish a mapping relationship between these two kinds of sequences.
Existing SLT methods can be divided into three cate-gories, 1) two-stage gloss-supervised methods, 2) end-to-*Both authors contributed equally to this research.
†Corresponding author. (a) self-attention
+gloss-supervised (b) self-attention
+gloss-free (c) gloss-attention
+gloss-free
Figure 1. Visualization of the attention map in the shallow encoder layer of three different SLT models. As shown in (a), an essential role of gloss is to provide alignment information for the model so that it can focus on relatively more important local areas. As shown in (b), the traditional attention calculation method is diffi-cult to converge to the correct position after losing the supervision signal of the gloss. However, our proposed method (c) can still flexibly maintain the attention in important regions (just like (a)) due to the injection of inductive bias, which can partially replace the role played by gloss. end gloss-supervised methods, and 3) end-to-end gloss-free methods. The first two approaches rely on gloss annota-tions, chronologically labeled sign language words, to assist the model in learning alignment and semantic information.
However, the acquisition of gloss is expensive and cumber-some, as its labeling takes a lot of time for sign language ex-perts to complete [5]. Therefore, more and more researchers have recently started to turn their attention to the end-to-end gloss-free approach [42, 51]. It learns directly to trans-late sign language videos into natural language sentences without the assistance of glosses, which makes the approach more general while making it possible to utilize a broader range of sign language resources. The gloss attention SLT network (GASLT) proposed in this paper is a gloss-free SLT method, which improves the performance of the model and removes the dependence of the model on gloss supervision by injecting inductive bias into the model and transferring knowledge from a powerful natural language model.
A sign language video corresponding to a natural lan-guage sentence usually consists of many video clips with complete independent semantics, corresponding one-to-one with gloss annotations in the semantic and temporal order.
Gloss can provide two aspects of information for the model.
On the one hand, it can implicitly help the model learn the location of semantic boundaries in continuous sign lan-guage videos. On the other hand, it can help the model understand the sign language video globally.
In this paper, the GASLT model we designed obtain information on these two aspects from other channels to achieve the effect of replacing gloss. First, we observe that the semantics of sign language videos are temporally local-ized, which means that adjacent frames have a high prob-ability of belonging to the same semantic unit. The visu-alization results in Figure 1a and the quantitative analysis results in Table 1 support this view. Inspired by this, we design a new dynamic attention mechanism called gloss at-tention to inject inductive bias [49] into the model so that it tends to pay attention to the content in the local same se-mantic unit rather than others. Specifically, we first limit the number of frames that each frame can pay attention to, and set its initial attention frame to frames around it so that the model can be biased to focus on locally closer frames.
However, the attention mechanism designed in this way is static and not flexible enough to handle the information at the semantic boundary well. We then calculate an offset for each attention position according to the input query so that the position of the model’s attention can be dynamically ad-justed on the original basis. It can be seen that, as shown in
Figure 1c, our model can still focus on the really important places like Figure 1a after losing the assistance of gloss. In contrast, as shown in Figure 1b, the original method fails to converge to the correct position after losing the supervision signal provided by the gloss.
Second, to enable the model to understand the semantics of sign language videos at the sentence level and disam-biguate local sign language segments, we transfer knowl-edge from language models trained with rich natural lan-guage resources to our model. Considering that there is a one-to-one semantic correspondence between natural lan-guage sentences and sign language videos. We can in-directly obtain the similarity relationships between sign language videos by inputting natural language sentences into language models such as sentence bert [56]. Us-ing this similarity knowledge, we can enable the model to understand the semantics of sign language videos as a whole, which can partially replace the second aspect of the information provided by gloss. Experimental results on three datasets RWTH-PHOENIX-WEATHER-2014T (PHOENIX14T) [5], CSL-Daily [70] and SP-10 [66] show that the translation performance of the GASLT model ex-ceeds the existing state of the art methods, which proves the effectiveness of our proposed method. We also conduct quantitative analysis and ablation experiments to verify the accuracy of our proposed ideas and the effectiveness of our model approach.
To summarize, the contributions of this work are as fol-lows:
• We analyze the role of gloss annotations in sign lan-guage translation.
• We design a novel attention mechanism and knowl-edge transfer method to replace the role of gloss in sign language translation partially.
• Extensive experiments on three datasets show the ef-fectiveness of our proposed method. A broad range of new baseline results can guide future research in this field. 2.