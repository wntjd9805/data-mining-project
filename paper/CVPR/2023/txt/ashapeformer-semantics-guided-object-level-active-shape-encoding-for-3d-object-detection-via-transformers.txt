Abstract 3D object detection techniques commonly follow a pipeline that aggregates predicted object central point fea-tures to compute candidate points. However, these can-didate points contain only positional information, largely ignoring the object-level shape information. This eventu-ally leads to sub-optimal 3D object detection. In this work, we propose AShapeFormer, a semantics-guided object-level shape encoding module for 3D object detection. This is a plug-n-play module that leverages multi-head attention to encode object shape information. We also propose shape tokens and object-scene positional encoding to ensure that the shape information is fully exploited. Moreover, we in-troduce a semantic guidance sub-module to sample more foreground points and suppress the influence of background points for a better object shape perception. We demon-strate a straightforward enhancement of multiple existing methods with our AShapeFormer. Through extensive exper-iments on the popular SUN RGB-D and ScanNetV2 dataset, we show that our enhanced models are able to outperform the baselines by a considerable absolute margin of up to 8.1%. Code will be available at https://github. com/ZechuanLi/AShapeFormer 1.

Introduction
As an important scene understanding task, 3D object de-tection [13,20,47] aims to detect 3D bounding boxes and se-mantic categories in 3D point cloud scenes. It plays an im-portant role in many downstream tasks, such as augmented reality [2, 3], mobile robots [18, 41, 52], and autonomous navigation [1,36,37,39]. Object detection has made signifi-cant progress in the 2D domain [15,22,33]. However, owing to the sparse and irregular nature of the point cloud data, 2D detection techniques are generally not readily applicable to the 3D object detection task.
*Corresponding author (Top) VoteNet [29] seed points contain many back-Figure 1. ground points, leading to sub-optimal candidate points, which are also intrinsically weak as they fail to account for object shape and contour features. (Bottom) The proposed AShapeFormer se-lects more relevant seed points, leading to more appropriate can-didates that additionally encode the object shape information ac-tively. This results in high quality 3D object detection.
Inspired by their 2D counterparts, early attempts in 3D object detection, e.g., [16, 39], mapped irregular point clouds to regular 3D voxels, thereafter using 3DCNNs for feature extraction and object detection. However, voxeliza-tion inevitably loses fine-grained information of the point clouds, which adversely affects the detection performance.
With the advances that allow direct processing of the point clouds with deep neural models, e.g., [30, 31], recent meth-ods aim at directly predicting the 3D bounding boxes from the original unordered point clouds. Among these tech-niques, VoteNet [29] and its variants [7, 12, 28, 45, 46, 50] have achieved remarkable performance.
These point-wise methods follow a common underlying pipeline which includes first aggregating certain predicted point features into candidate points. The candidate points are later used to estimate the 3D bounding box information, e.g., center, size, and orientation, along with the associated semantic labels. As illustrated in Fig. 1, despite their ex-cellent performance, these methods still face a few major
challenges. (1) The final prediction relies strongly on the quality of the candidate points. However, these points fail to encode important object-level features such as contours and the shape of the 3D objects. (2) The methods must regress over the candidate points, and these points are often influenced by the background points. This propagates the error to cause offsets in the eventual predictions. Current attempts to mitigate these issues rely on generating new fea-tures [7, 45] or sampling more points [42]. However, these are resource intensive solutions, which must still rely on the vote point regression quality.
To address the problems, we introduce a novel plug-n-play neural module, named AShapeFormer. It can be easily assembled with many existing 3D object detection methods to provide a considerable performance boost. Our key driv-ing insight is that by utilizing implicit object-level shape features, a detector can be made aware of the object shape distribution. Specifically, our module utilizes multi-head at-tention to encode the object shape information. We aggre-gate the object shape features using a self-attention mecha-nism. Inspired by ViT [10] and BERT [19], we introduce a shape token as the output of the final shape feature to avoid information loss caused by simplistic operations, e.g., pool-ing. Additionally, we devise a semantic guidance mecha-nism to sample more foreground points and assign different weights to their features, which improves the shape feature generation. Semantic segmentation scores are also utilized during the aggregation of vote points to reduce the influence of irrelevant vote points and obtain better candidates.
We provide successful demonstration of boosting both point-based [28, 29] and Transformer [23] baselines with our method, achieving strong performance gains. Our ex-perimental results (§ 4.1) show that AShapeFormer boosts the multi-class mean average precision (mAP) up to 3.5% on the challenging SUN RGB-D dataset [38] and 8.1% on the ScanNet V2 dataset [9]. Highlights of our contributions include the following.
• We propose a plug-and-play active shape encoding module named AShapeFormer, which can be com-bined with many existing 3D object detection networks to achieve a considerable performance boost.
• To the best of our knowledge, our method is the first to combine multi-head attention and semantic guidance to encode strong object shape features for robust clas-sification and accurate bounding box regression.
• We demonstrate a considerable mAP boost on SUN
RGB-D (mAP@0.25) and ScanNet V2 datasets by en-hancing the state-of-the-art methods with our module. 2.