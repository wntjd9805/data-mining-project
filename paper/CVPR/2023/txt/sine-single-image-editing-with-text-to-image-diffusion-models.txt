Abstract
Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion mod-els for tackling a challenging problem–real image editing.
Works conducted in this area learn a unique textual token corresponding to several images containing the same ob-ject. However, under many circumstances, only one im-age is available, such as the painting of the Girl with a
Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new fea-tures depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffu-sion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbi-trary resolution. We provide extensive experiments to vali-date the design choices of our approach and show promis-ing editing capabilities, including changing style, content addition, and object manipulation. Our code is made pub-licly available here. 1.

Introduction
Automatic real image editing is an exciting direction, enabling content generation and creation with minimal ef-fort. Although many works have been conducted in this area, achieving high-fidelity semantic manipulation on an image is still a challenging problem for the generative mod-els, considering the target image might be out of the train-ing data distribution [4, 11, 25, 26, 32, 58, 59]. The recently introduced large-scale text-to-image models, e.g., DALL·E 2 [37], Imagen [42], Parti [55], and StableDiffusion [39], can perform high-quality and diverse image generation with
Figure 1. With only one real image, i.e., Source Image, our method is able to manipulate and generate the content in various ways, such as changing style, adding context, modifying the object, and enlarging the resolution, through guidance from the text prompt. natural language guidance. The success of these works has inspired many subsequent efforts to leverage the pre-trained large-scale models for real image editing [10, 15, 20, 40].
They show that, with properly designed prompts and a lim-ited number of fine-tuning steps, the text-to-image models can manipulate a given subject with text guidance.
On the downside, the recent text-guided editing works that build upon the diffusion models suffer several limita-tions. First, the fine-tuning process might lead to the pre-trained large-scale model overfit on the real image, which degrades the synthesized images’ quality when editing. To tackle these issues, methods like using multiple images with the same content and applying regularization terms on the same object have been introduced [4, 40]. However, query-ing multiple images with identical content or object might not be an available choice; for instance, there is only one painting for Girl with a Pearl Earring. Directly editing the single image brings information leakage from the pre-trained large-scale models, generating images with differ-ent content (examples in Fig. 5); therefore, the application scenarios of these methods are greatly constrained. Second, these works lack a reasonable understanding of the object geometry for the edited image. Thus, generating images with different spatial size as the training data cause unde-sired artifacts, e.g., repeated objects, and incorrectly modi-fied geometry (examples in Fig. 5). Such drawbacks restrict applying these methods for generating images with an ar-bitrary resolution, e.g., synthesizing high-resolution images from a single photo of a castle (as in Fig. 4), again limiting
the usage of these methods.
In this work, we present SINE, a framework utilizing pre-trained text-to-image diffusion models for SINgle im-age Editing and content manipulation. We build our ap-proach based upon existing text-guided image generation approaches [10, 40] and propose the following novel tech-niques to solve overfitting issues on content and geometry, and language drift [28, 31]:
• First, by appropriately modifying the classifier-free guid-ance [22], we introduce model-based classifier-free guid-ance that utilizes the diffusion model to provide the score guidance for content and structure. Taking advantage of the step-by-step sampling process used in diffusion mod-els, we use the model fine-tuned on a single image to plant a content “seed” at the early stage of the denois-ing process and allow the pre-trained large-scale text-to-image model to edit creatively conditioned with the lan-guage guidance at a later stage.
• Second, to decouple the correlation between pixel posi-tion and content, we propose a patch-based fine-tuning strategy, enabling generation on arbitrary resolution.
With a text descriptor describing the content that is aimed to be manipulated and language guidance depicting the desired output, our approach can edit the single unique image to the targeted domain with details preserved in arbi-trary resolution. The output image keeps the structure and background intact while having features well-aligned with the target language guidance. As shown in Fig. 1, trained on a painting Girl with a Pearl Earring with resolution as 512 × 512, we can sample an image of a sculpture of the girl at the resolution of 640 × 512 with the identity features preserved. Moreover, our method can successfully handle various edits such as style transfer, content addition, and object manipulation (more examples in Fig. 3). We hope our method can further boost creative content creation by opening the door to editing arbitrary images. 2.