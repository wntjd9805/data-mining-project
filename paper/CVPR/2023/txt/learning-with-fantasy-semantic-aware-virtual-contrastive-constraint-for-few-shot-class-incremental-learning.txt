Abstract
Few-shot class-incremental learning (FSCIL) aims at learning to classify new classes continually from limited samples without forgetting the old classes. The mainstream framework tackling FSCIL is first to adopt the cross-entropy (CE) loss for training at the base session, then freeze the feature extractor to adapt to new classes. However, in this work, we find that the CE loss is not ideal for the base ses-sion training as it suffers poor class separation in terms of representations, which further degrades generalization to novel classes. One tempting method to mitigate this prob-lem is to apply an additional na¨ıve supervised contrastive learning (SCL) in the base session. Unfortunately, we find that although SCL can create a slightly better representa-tion separation among different base classes, it still strug-gles to separate base classes and new classes.
Inspired by the observations made, we propose Semantic-Aware Vir-tual Contrastive model (SAVC), a novel method that facili-tates separation between new classes and base classes by introducing virtual classes to SCL. These virtual classes, which are generated via pre-defined transformations, not only act as placeholders for unseen classes in the repre-sentation space, but also provide diverse semantic infor-mation. By learning to recognize and contrast in the fan-tasy space fostered by virtual classes, our SAVC signifi-cantly boosts base class separation and novel class gen-eralization, achieving new state-of-the-art performance on the three widely-used FSCIL benchmark datasets. Code is available at: https://github.com/zysong0113/SAVC. 1.

Introduction
Like humans, the modern artificial intelligence models are expected to be equipped with the ability of learning continually. For instance, a face recognition system needs
†: authors contributed equally. ∗: corresponding author. (a) Comparison of the embedding spaces trained with different methods. (b) Illustration of our Semantic-Aware Virtual Contrastive model (SAVC).
Figure 1. The motivation of the proposed approach. Under the incremental-frozen framework in FSCIL, our SAVC learns to rec-ognize and contrast in the fantasy space, which leads to better base class separation and novel class generalization than CE and SCL. to continually learn to recognize new faces without forget-In this case, Class-ting those has already kept in mind.
Incremental Learning (CIL) [19, 25, 30, 37, 50] draws atten-tion extensively, that sufficient and disjoint data are com-ing session-by-session. However, it is unrealistic to always rely on accessing of multitudinous data. Taking the case of face recognition system again, whether it can quickly learn a new concept with just a few images, is an important cri-terion for evaluating its performance. As a result, design-ing effective and efficient algorithms to resolve Few-Shot
Class-Incremental Learning (FSCIL) problem has drawn in-creasing attention [6, 8, 12, 40, 44, 51, 55]. Unlike CIL, only a few data are provided for FSCIL at incremental session
while sufficient samples are only visible at the base session.
The core challenge of CIL is to strike a balance of stabil-ity and plasticity, i.e., suppressing forgetting on old knowl-edge while adapting smoothly to new knowledge. Previ-ous works on FSCIL have demonstrated that an incremen-tally trained model is prone to overfit on limited new data and suffer catastrophic forgetting [8, 12, 44, 54]. Thus the mainstream FSCIL framework [40, 51, 58] is to first use the cross-entropy (CE) loss for training in the base session, then freeze the backbone to adapt to new classes. Given such baseline framework, the main problem that needs to be taken into consideration is: What makes a good base-session-model that can generalize well to new classes with limited data?
Intuitively, if all representations of base classes concen-trate around their own clustering centers (i.e., prototypes), and all prototypes are far away from each other, then novel classes should be easily incorporated into the current space without overlapping. Therefore, we conjecture that base class separation facilitates novel class generalization. How-ever, the current widely adopted CE loss in FSCIL cannot well separate the class margins, which causes poor gen-eralization to new classes. This motivates our attempt at methods with better clustering effects, such as supervised contrastive learning (SCL) [21]. We conduct experiments to validate our conjecture by training models optimized by only CE loss, and both CE and SCL. We empirically find that CE leads to lower base class separation and classifica-tion accuracy than SCL. Although SCL indeed slightly im-proves the base class separation with higher accuracy, we find it implicitly decreases the inter-class distance. There-fore, the two methods both leave inadequate room for future updates, which leads to the inevitable overlapping between novel classes and old classes in the representation space (as shown in the left and middle columns of Fig. 1a).
The unsatisfactory performance of SCL implies that the limited semantic information base session provided may need our fantasy remedy. Although we have no access to future classes in the base session, we can create many vir-tual classes with various semantics filling the unallocated representation space, which not only holds enough space for future classes, but also enables the learning of diverse semantics for better inference and generalization.
To this end, we propose a Semantic-Aware Virtual Con-trastive (SAVC) model to improve the FSCIL by boosting base class separation (see Fig. 1b). Our fantasy space is con-structed with pre-defined transformations [11, 15, 35, 52], which can be regarded as the semantic extension of the orig-inal space in finer grains. The type of transformations rep-resents the richness of our imagination, i.e., how compact we want the embedding space to be and how fine the se-mantic grains we want to acquire. Learning to contrast and recognize in the fantasy space not only brings a better class separation effect, but also provides richer semantic infor-mation to improve the accuracy. In contrast to the CE base-line and SCL, our SAVC can simultaneously decrease intra-class distance and increase inter-class distance, which leads to better base class separation and novel class generalization (as given in the right of Fig. 1a). Extensive experiments on various benchmark datasets further verify the superiority of our method.
Our main contributions are summarized in three folds: 1) We empirically discover that it is crucial to boost class separation degree in base session for the incremental-frozen framework in FSCIL, which helps fast generalization for novel classes with only a few samples. 2) We propose a novel Semantic-Aware Virtual Con-trastive (SAVC) framework, enhancing the FSCIL learner by learning to recognize and contrast in the fantasy space, which preserves enough place for novel generalization and enables a multi-semantic aggregated inference effect. 3) Extensive experiments on three FSCIL bench-i.e., CIFAR100, miniImageNet and CUB200, marks, demonstrate that our SAVC outperforms all approaches by a large margin and achieves the state-of-the-art performance. 2.