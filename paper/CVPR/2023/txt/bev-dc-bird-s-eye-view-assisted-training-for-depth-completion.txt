Abstract
Depth completion plays a crucial role in autonomous driving, in which cameras and LiDARs are two complemen-tary sensors. Recent approaches attempt to exploit spatial geometric constraints hidden in LiDARs to enhance image-guided depth completion. However, only low efficiency and poor generalization can be achieved. In this paper, we pro-pose BEV@DC, a more efficient and powerful multi-modal training scheme, to boost the performance of image-guided depth completion.
In practice, the proposed BEV@DC model comprehensively takes advantage of LiDARs with rich geometric details in training, employing an enhanced depth completion manner in inference, which takes only im-ages (RGB and depth) as input. Specifically, the geometric-aware LiDAR features are projected onto a unified BEV space, combining with RGB features to perform BEV com-pletion. By equipping a newly proposed point-voxel spatial propagation network (PV-SPN), this auxiliary branch intro-duces strong guidance to the original image branches via 3D dense supervision and feature consistency. As a result, our baseline model demonstrates significant improvements with the sole image inputs. Concretely, it achieves state-of-the-art on several benchmarks, e.g., ranking Top-1 on the challenging KITTI depth completion benchmark. 1.

Introduction
Dense depth estimation plays an essential role in various 3D vision tasks and self-driving applications, e.g., 3D ob-ject detection and tracking, simultaneous localization and mapping (SLAM), and structure-from-motion (SFM) [14, 17,19,33,37]. With the aid of outdoor LiDAR sensors or in-door RGBD cameras, 3D vision applications acquire depth maps for further industrial usage. However, the depth sen-sors cannot provide dense pixel-wise depth maps since their output is sparse and has numerous blank regions, especially
*Corresponding author
Figure 1. BEV assisted training. (a) Previous camera-based methods that take RGB and depth input. (b) Previous fusion-based methods introduce extra inputs and computation in both training and inference. (c) Our method takes additional LiDAR as input for assisted training. Only the 2D inputs are used during the infer-ence, which reduces the computational burden. in outdoor scenes. Therefore, it is necessary to fill the void areas of the depth maps in practice.
Recent depth completion methods [4, 12, 21, 47] lever-age the RGB information as guidance since the RGB im-ages contain scene structures, e.g., textures, and monoc-ular features, e.g., vanishing points, to provide the cues for the missing pixels. However, the camera-based meth-ods apply the 2D convolution on the irregularly distributed depth values, resulting in an implicit yet ineffective ex-ploration of underlying 3D geometry, i.e., over-smooth at the boundary of objects. Considering the deployment of cameras and LiDAR in commercial cars and the recent trend of cross-modal learning in the vision community, some methods [2, 3, 28, 41] introduce explicit 3D represen-tations, i.e., LiDAR point clouds generated by sparse depth,
to complement 2D appearance features with 3D structured priors. Despite the improvements, the fusion-based ap-proaches still have the following issues: 1) The 3D fea-ture extraction and fusion are not efficacious, especially the critical spatial correlations between a depth point and its neighbors, which significantly affects the completion performance. 2) Fusion-based methods are computation-intensive while processing sparse depths, RGB images, and additional 3D input such as LiDAR information, either oc-cupying more memory storage or consuming more time in inference, which hinders real-time applications.
To address the above issues, we seek to boost image-guided depth completion performance by exploiting 3D representations via a more efficient and effective cross-representation training scheme.
In training, we design an auxiliary LiDAR branch consisting of LiDAR encoder, cross-representation BEV decoder (CRBD) and point-voxel spatial propagation network (PV-SPN). Initially, we prepro-cess each LiDAR scan with the assigned voxel cells to al-leviate the irregularity and sparseness and then extract its multi-scale features. After that, these features will be pro-jected onto a unified BEV space. The following CRBD uti-lizes the above multi-scale BEV features and the ones from the camera branch to perform BEV fusion and completion.
After that, the BEV completion is interpolated into the 3D space, and a point-voxel spatial propagation network is pro-posed to query the nearest neighbors for each coarse voxel and performs feature aggregation on all the adjacent points from LiDAR, refining the 3D geometric shapes. Moreover, to tackle the extra computational burden from the LiDAR branch, this plug-and-play component is only exploited in the training phase, enhancing the original camera branch through feature consistency and end-to-end backpropaga-tion. Consequently, the trained model is independent of ad-ditional LiDAR inputs during the inference.
Compared with previous fusion-based methods, our pro-posed framework has the following advantages: 1) Gener-ality: Our plug-and-play solution can be incorporated into several camera-based depth completion models; 2) Flexi-bility: The processing module for LiDAR representations only exists during training and is discarded in inference, as shown in Fig. 1(c), compared with previous camera-based models (a) and fusion-based models (b). There is no addi-tional computational burden in the deployment. 3) Effec-tiveness: It significantly boosts the performance upon the baseline approach, achieving state-of-the-art results on sev-eral benchmarks. To sum up, the main contributions are summarized as follows:
- Birdâ€™s-Eye View Assisted Training for Depth
Completion (BEV@DC) is proposed, which as-sists camera-based depth completion with LiDAR representation during the training phase.
- Cross-representation BEV decoder (CRBD) and point-voxel spatial propagation network (PV-SPN) are pro-posed to gain fine-grained 3D geometric shapes and provide strong guidance to the RGB branch.
- Our solution achieves state-of-the-art on both outdoor
KITTI depth completion benchmark and indoor NYU
Depth v2 dataset. 2.