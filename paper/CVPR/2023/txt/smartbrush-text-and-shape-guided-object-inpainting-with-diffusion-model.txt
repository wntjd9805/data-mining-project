Abstract
Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpaint-ing provides more flexible and useful controls on the in-painted content, e.g., a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than be-ing only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Sta-ble Diffusion can do text-guided inapinting they do not sup-port shape guidance and tend to modify background tex-ture surrounding the generated object. Our model incor-porates both text and shape guidance with precision con-trol. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusion
U-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpaint-* Work done during internship at Adobe. ing with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation. 1.

Introduction
Traditional image inpainting aims to fill the missing area in images conditioned on surrounding pixels, lacking con-trol over the inpainted content. To alleviate this, multi-modal image inpainting offers more control through addi-tional information, e.g. class labels, text descriptions, seg-mentation maps, etc.
In this paper, we consider the task of multi-modal object inpainting conditioned on both a text description and the shape of the object to be inpainted (see
Fig. 1). In particular, we explore diffusion models for this task inspired by their superior performance in modeling complex image distributions and generating high-quality images.
Diffusion models (DMs) [7, 24], e.g., Stable Diffu-sion [20], DALL-E [18, 19], and Imagen [21] have shown promising results in text-to-image generation. They can
also be adapted to the inpainting task by replacing the ran-dom noise in the background region with a noisy version of the original image during the diffusion reverse process [14].
However, this leads to undesirable samples since the model cannot see the global context during sampling [16]. To ad-dress this, GLIDE [16] and Stable Inpainting (inpainting specialist v1.5 from Stable Diffusion) [20] randomly erase part of the image and fine-tune the model to recover the missing area conditioned on the corresponding image cap-tion. However, semantic misalignment between the missing area (local content) and global text description may cause the model to fill in the masked region with background in-stead of precisely following the text prompt as shown in
Fig. 1 (“Glide” and “Stable Inpainting”). We refer to this phenomenon as text misalignment.
An alternative way to perform multi-modal image in-painting is to utilize powerful language-vision models, e.g.,
CLIP [17]. Blended diffusion [2] uses CLIP to compute the difference between the image embedding and the input text embedding and then injects the difference into the sam-pling process of a pretrained unconditional diffusion model.
However, CLIP models tend to capture the global and high-level image features, thus there is no incentive to generate objects aligning with the given mask (see “Blended Diffu-sion” in Fig. 1). We denote this phenomenon as mask mis-alignment. A recent GAN-based work CogNet [28] pro-poses to use shape information from instance segmentation dataset and predict the class of missing objects to address this problem. But it doesn’t support text input. Another is-sue for existing inpainting methods is background preser-vation in which case they often produce distorted back-ground surrounding the inpainted object as shown in Fig. 1 (bottom row).
To address above challenges, we introduce a precision factor into the input masks, i.e., our model not only takes a mask as input but also information about how closely the inpainted object should follow the mask’s shape. To achieve this we generate different types of masks from fine to coarse by applying Gaussian blur to accurate instance masks and use the masks and their precision type to train the guided diffusion model. With this setup, we allow users to ei-ther use coarse masks which will contain the desired object somewhere within the mask or to provide detailed masks that outline the shape of the object exactly. Thus, we can supply very accurate masks and the model will fill the en-tire mask with the object described by the text prompt (see the first row in Fig. 1), while, on the other hand, we can also provide very coarse masks (e.g., a bounding box) and the model is free to insert the desired object within the mask area such that the object is roughly bounded by the mask.
One important characteristic, especially for coarse masks such as bounding boxes, is that we want to keep the back-ground within the inpainted area consistent with the original image. To achieve this, we not only encourage the model to inpaint the masked region but also use a regularization loss to encourage the model to predict an instance mask of the object it is generating. At test time we replace the coarse mask with the predicted mask during sampling to preserve background as much as possible which leads to more con-sistent results (second row in Fig. 1).
We evaluate our model on several challenging object in-painting tasks and show that it achieves state-of-the-art re-sults on object inpainting across several datasets and exam-ples. Our user study shows that users prefer the outputs of our model as compared to DALLE-2 and Stable Inpainting across several axes of evaluation such as shape, text align-ment, and realism. To summarize our contributions:
• We introduce a text and shape guided object inpainting diffusion model, which is conditioned on object masks of different precision, achieving a new level of control for object inpainting.
• To preserve the image background with coarse input masks, the model is trained to predict a foreground object mask during inpainting for preserving original background surrounding the synthesized object.
• We propose a multi-task training strategy by jointly training object inpainting with text-to-image genera-tion to leverage more training data. 2.