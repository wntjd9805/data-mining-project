Abstract
Quantizing images into discrete representations has been a fundamental problem in unified generative modeling.
Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantiza-tion suffers from severe codebook collapse and misalign-ment with inference stage while stochastic quantization suf-fers from low codebook utilization and perturbed recon-struction objective. This paper presents a regularized vec-tor quantization framework that allows to mitigate above issues effectively by applying regularization from two per-spectives. The first is a prior distribution regularization which measures the discrepancy between a prior token dis-tribution and the predicted token distribution to avoid code-book collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochastic-ity during quantization to strike a good balance between in-ference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further miti-gate the perturbed reconstruction objective. Extensive ex-periments show that the proposed quantization framework outperforms prevailing vector quantization methods con-sistently across different generative models including auto-regressive models and diffusion models. 1.

Introduction
With the prevalence of multi-modal image synthesis
[3, 23, 37, 39] and Transformers [31], unifying data mod-eling regardless of data modalities has attracted increas-ing interest from the research communities. Aiming for a generic data representation across different data modalities, discrete representation learning [21, 25] plays a significant role in the unified modeling. In particular, vector quantiza-tion models (e.g., VQ-VAE [21] and VQ-GAN [8]) emerge as a promising family for learning generic image represen-tations by discretizing images into discrete tokens. With the
*Corresponding author, E-mail: shijian.lu@ntu.edu.sg
Figure 1. Visualization of codebook (first row) and illustration of codebook utilization (second row) on ADE20K dataset [42]. VQ-GAN [8] severely suffers from codebook collapse as most code-book embeddings are invalid values. Gumbel-VQ [2] learns valid values for all codebook embeddings, while only a small number of embeddings are actually used for quantization as illustrated in codebook utilization. As a comparison, the proposed regularized quantization prevents codebook collapse and achieves full code-book utilization. The codebook visualization method is provided in the supplementary file. tokenized representation, generative models such as auto-regressive model [8, 9] and diffusion model [6, 12] can be applied to accommodate the dependency of the sequential tokens for image generation, which is referred as tokenized image synthesis under this context.
Vector quantization models can be broadly grouped into deterministic quantization and stochastic quantization ac-cording to the selection of discrete tokens. Specifically, typical deterministic methods like VQ-GAN [8] directly se-lect the best-matching token via Argmin or Argmax, while stochastic methods like Gumbel-VQ [2] select a token by stochastically sampling from a predicted token distribution.
On the other hand, deterministic quantization suffers from codebook collapse [26], a well-known problem where large portion of codebook embeddings are invalid with near-zero values as shown in Fig. 1 (first row). In addition, determin-istic quantization is misaligned with the inference stage of generative modeling, where the tokens are usually randomly
In-sampled instead of selecting the best matching one. stead, stochastic quantization samples tokens according to a predicted token distribution with Gumbel-Softmax [2, 14], which allows to avoid codebook collapse and mitigate infer-ence misalignment. However, although most codebook em-beddings are valid values in stochastic quantization, only a small part is actually utilized for vector quantization as shown in Fig. 1 (second row), which is dubbed as low code-book utilization. Besides, as stochastic methods randomly sample tokens from a distribution, the image reconstructed from the sampled tokens is usually not well aligned with the original image, leading to perturbed reconstruction ob-jective and unauthentic image reconstruction.
In this work, we introduce a regularized quantization framework that allows to prevent above problems effec-tively via regularization from two perspectives. Specifi-cally, to avoid codebook collapse and low codebook uti-lization where only a small number of codebook embed-dings are valid or used for quantization, we introduce a prior distribution regularization by assuming a uniform distribution as the prior for token distribution. As the pos-terior token distribution can be approximated by the quanti-zation results, we can measure the discrepancy between the prior token distribution and posterior token distribution. By minimizing the discrepancy during training, the quantiza-tion process is regularized to use all the codebook embed-dings, which prevents the predicted token distribution from collapse into a small number of codebook embeddings.
As deterministic quantization suffers from inference stage misalignment and stochastic quantization suffers from perturbed reconstruction objective, we introduce a stochas-tic mask regularization to strike a good balance between them. Specifically, the stochastic mask regularization ran-domly masks certain ratio of regions for stochastic quanti-zation, while leaving the unmasked regions for determinis-tic quantization. This introduces uncertainty for the selec-tion of tokens and results of quantization, which narrows the gap with the inference stage of generative modelling where tokens are selected randomly. We also conduct thorough and comprehensive experiments to analyze the selection of masking ratio for optimal image reconstruction and genera-tion.
On the other hand, with the randomly sampled tokens, the stochastically quantized region will suffer from per-turbed reconstruction objective. The perturbed reconstruc-tion objective mainly results from the target for perfect re-construction of the original image from randomly sampled tokens.
Instead of naively enforcing a perfect image re-construction with L1 loss, we introduce a contrastive loss for elastic image reconstruction, which mitigates the per-turbed reconstruction objective significantly. Similar to
PatchNCE [22, 41], the contrastive loss treats the patch at the same spatial location as positive pairs and others as neg-ative pairs. By pushing the positive pairs closer and pulling negative pairs away, the elastic image reconstruction can be achieved. Another issue with the randomly sampled tokens is that they tend to introduce perturbation of different scales in the reconstruction objective, We thus introduce a Proba-bilistic Contrastive Loss (PCL) that adjusts the pulling force of different regions according to the discrepancy between the sampled token embedding and the best-matching token embedding.
The contributions of this work can be summarized in three aspects. First, we present a regularized quantization framework that introduces a prior distribution regulariza-tion to prevent codebook collapse and low codebook utiliza-tion. Second, we propose a stochastic mask regularization which mitigates the misalignment with the inference stage of generative modelling. Third, we design a probabilistic contrastive loss that achieves elastic image reconstruction and mitigates the perturbed objective adaptively for differ-ent regions with stochastic quantization. 2.