Abstract
Post-training quantization (PTQ) is widely regarded as one of the most efficient compression methods practically, benefitting from its data privacy and low computation costs.
We argue that an overlooked problem of oscillation is in the
PTQ methods. In this paper, we take the initiative to ex-plore and present a theoretical proof to explain why such a problem is essential in PTQ. And then, we try to solve this problem by introducing a principled and generalized frame-work theoretically. In particular, we first formulate the os-cillation in PTQ and prove the problem is caused by the dif-ference in module capacity. To this end, we define the mod-ule capacity (ModCap) under data-dependent and data-free scenarios, where the differentials between adjacent modules are used to measure the degree of oscillation. The prob-lem is then solved by selecting top-k differentials, in which the corresponding modules are jointly optimized and quan-tized. Extensive experiments demonstrate that our method successfully reduces the performance drop and is general-ized to different neural networks and PTQ methods. For example, with 2/4 bit ResNet-50 quantization, our method surpasses the previous state-of-the-art method by 1.9%. It becomes more significant on small model quantization, e.g. surpasses BRECQ method by 6.61% on MobileNetV2 ×0.5. 1.

Introduction
Deep Neural Networks (DNNs) have rapidly become a research hotspot in recent years, being applied to various
*This work was done when Yuexiao Ma was intern at ByteDance Inc.
Code is available at: https://github.com/bytedance/MRECG
†Corresponding Author: rrji@xmu.edu.cn
Figure 1. Left: Reconstruction loss distribution of BRECQ [17] on 0.5 scaled MobileNetV2 quantized to 4/4 bit. Loss oscilla-tion in BRECQ during reconstruction see red dashed box. Right:
Mixed reconstruction granularity (MRECG) smoothing loss oscil-lation and achieving higher accuracy. scenarios in practice. However, as DNNs evolve, better model performance is usually associated with huge resource consumption from deeper and wider networks [8, 14, 28].
Meanwhile, the research field of neural network compres-sion and acceleration, which aims to deploy models in resource-constrained scenarios, is gradually gaining more and more attention, including but not limited to Neural Ar-chitecture Search [18, 19, 33, 35–40, 42, 43], network prun-ing [4, 7, 16, 27, 32, 41], and quantization [3, 5, 6, 15, 17, 21, 22,29,31]. Among these methods, quantization proposed to transform float network activations and weights to low-bit fixed points, which is capable of accelerating inference [13] or training [44] speed with little performance degradation.
In general, network quantization methods are divided into quantization-aware training (QAT) [3, 5, 6] and post-training quantization (PTQ) [11, 17, 22, 31]. The former reduces the quantization error by quantization fine-tuning.
Despite the remarkable results, the massive data require-ments and high computational costs hinder the pervasive deployment of DNNs, especially on resource-constrained devices. Therefore, PTQ is proposed to solve the aforemen-tioned problem, which requires only minor or zero calibra-tion data for model reconstruction. Since there is no iter-ative process of quantization training, PTQ algorithms are extremely efficient, usually obtaining and deploying quan-tized models in a few minutes. However, this efficiency of-ten comes at the partial sacrifice of accuracy. PTQ typically performs worse than full precision models without quanti-zation training, especially in low-bit compact model quan-tization. Some recent algorithms [17, 22, 31] try to address this problem. For example, Nagel et al. [22] constructs new optimization functions by second-order Taylor expansions of the loss functions before and after quantization, which introduces soft quantization with learnable parameters to achieve adaptive weight rounding. Li et al. [17] changes layer-by-layer to block-by-block reconstruction and uses di-agonal Fisher matrices to approximate the Hessian matrix to retain more information. Wei et al. [31] discovers that ran-domly disabling some elements of the activation quantiza-tion can smooth the loss surface of the quantization weights.
However, we observe that all the above methods show different degrees of oscillation with the deepening of the layer or block during the reconstruction process, as illus-trated in the left sub-figure of Fig. 1. We argue that the problem is essential and has been overlooked in the previ-ous PTQ methods. In this paper, through strict mathemati-cal definitions and proofs, we answer 3 questions about the oscillation problem, which are listed as follows: (i). Why the oscillation happens in PTQ? To answer this question, we first define module topological homogene-ity, which relaxes the module equivalence restriction to a certain extent. And then, we give the definition of module capacity under the condition of module topological homo-geneity. In this case, we can prove that when the capacity of the later module is large enough, the reconstruction loss will break through the effect of quantization error accumulation and decrease. On the contrary, if the capacity of the later module is smaller than that of the preceding module, the reconstruction loss increases sharply due to the amplified quantization error accumulation effect. Overall, we demon-strate that the oscillation of the loss during PTQ reconstruc-tion is caused by the difference in module capacity; (ii). How the oscillation will influence the final perfor-mance? We observe that the final reconstruction error is highly correlated with the largest reconstruction error in all the previous modules by randomly sampling a large num-ber of mixed reconstruction granularity schemes. In other words, when oscillation occurs, the previous modules ob-viously have larger reconstruction errors, thus leading to worse accuracy in PTQ; (iii). How to solve the oscillation problem in PTQ?
Since oscillation is caused by the different capacities of the front and rear modules, we propose the Mixed
REConstruction Granularity (MRECG) method which jointly optimizes the modules where oscillation occurs.
Besides, our method is applicable in data-free and data-dependent scenarios, which is also compatible with differ-ent PTQ methods. In general, our contributions are listed as follows:
• We reveal for the first time the oscillation problem in
PTQ, which has been neglected in previous algorithms.
However, we discover that smoothing out this oscilla-tion is essential in the optimization of PTQ.
• We show theoretically that this oscillation is caused by the difference in the capability of adjacent modules.
A small module capability exacerbates the cumulative effect of quantization errors making the loss increase rapidly, while a large module capability reduces the cu-mulative quantization errors making the loss decrease.
• To solve the oscillation problem, we propose a novel Mixed REConstruction Granularity (MRECG) method, which employs loss metric and module capac-ity to optimize mixed reconstruction granularity under data-dependency and data-free scenarios. The former finds the global optimum with moderately higher over-head and thus has the best performance. The latter is more effective with a minor performance drop.
• We validate the effectiveness of the proposed method on a wide range of compression tasks in ImageNet.
In particular, we achieve a Top-1 accuracy of 58.49% in MobileNetV2 with 2/4 bit, which exceeds current
SOTA methods by a large margin. Besides, we also confirm that our algorithm indeed eliminates the oscil-lation of reconstruction loss on different models and makes the reconstruction process more stable. 2.