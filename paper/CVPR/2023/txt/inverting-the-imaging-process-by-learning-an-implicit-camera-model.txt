Abstract
Representing visual signals with implicit coordinate-based neural networks, as an effective replacement of the traditional discrete signal representation, has gained con-siderable popularity in computer vision and graphics. In contrast to existing implicit neural representations which focus on modelling the scene only, this paper proposes a novel implicit camera model which represents the physical imaging process of a camera as a deep neural network. We demonstrate the power of this new implicit camera model on two inverse imaging tasks: i) generating all-in-focus pho-*Work was done during an internship at Tencent AI Lab.
†Corresponding authors. tos, and ii) HDR imaging. Specifically, we devise an im-plicit blur generator and an implicit tone mapper to model the aperture and exposure of the camera’s imaging process, respectively. Our implicit camera model is jointly learned together with implicit scene models under multi-focus stack and multi-exposure bracket supervision. We have demon-strated the effectiveness of our new model on a large num-ber of test images and videos, producing accurate and visu-ally appealing all-in-focus and high dynamic range images.
In principle, our new implicit neural camera model has the potential to benefit a wide array of other inverse imaging tasks.
1.

Introduction
The key contributions of this paper are:
Using deep neural networks to learn an implicit repre-sentation of visual signal of a scene has received remarkable success (e.g., NeRF [27]). It has been used to represent vi-sual signals (e.g., images [10,35], videos [5,18], and volume density [27]) with many impressive results. Besides implicit scene modelling (e.g., modelling scene radiance field via an
MLP), the physical imaging process of a camera is also im-portant for the image formation process (i.e., from scene ra-diance field to RGB values of the sensor of a camera [36]).
However, to the best of our knowledge, little in the lit-erature has ever tapped into the issue of finding an implicit representation to model the physical imaging process of a camera.
Instead, most existing neural rendering methods assume that each pixel’s RGB values are precisely the cap-tured radiance field. In reality, before the light rays hit the imaging sensors, they need to pass through both the aper-ture and shutter, resulting in possible image blur caused by finite-sized aperture as well as varied dynamic range dic-tated by exposure time of the shutter.
Moreover, the image signal processor (ISP) inside a dig-ital camera may also alter the obtained image, e.g., lumi-nance change, depth of field (DoF), as well as image noises.
The above observation prompts us to address two questions in this paper:
• Can we learn an implicit camera model to represent the imaging process and control camera parameters?
• Can we invert the imaging process from inputs with varying camera settings and recover the raw scene content?
Recently, learning-based methods simulating the map-ping from raw images to sRGB images have been presented
[13, 30, 51]. They allow photo-realistic image generation controlled by the shutter or aperture, but inverse problems of raw image restoration are challenging to model. Al-though a few NeRF-based methods have simply simulated cameras, they still face many issues, e.g., either RawN-eRF [26] only models a camera forward mapping for con-trollable exposures or HDR-NeRF [14] only builds a tone-mapper module with the NeRF on static scenes to inversely recover the high-dynamic-range (HDR) radiance. It is not clear whether a unified coordinate-based MLP module of different implicit camera models can be applied to various implicit neural scene representations for inverting the imag-ing process in a self-supervised manner, especially for dy-namic scenes.
To this end, this paper proposes a novel implicit neural camera model as a general implicit neural representation.
Tested on two challenging tasks of inverse imaging, namely all-in-focus and HDR imaging, we have demonstrated the effectiveness of our new implicit neural camera model, as illustrated in Fig. 1. 1. We propose an interesting component, an implicit neu-ral camera model including a blur generator module (Sec. 3.2) for the point spread function and a tone mapper module (Sec. 3.3) for the camera response function, to model the camera imaging process. 2. We develop a self-supervised framework for image enhancement from visual signals with different fo-cuses and exposures and introduce several regulariza-tion terms (Sec. 3.4) to encourage the modules of the implicit neural camera to learn corresponding physical imaging formulation. 3. We showcase implicit image enhancement applica-tions on images and videos fulfilled with the proposed framework, including forwardly controllable genera-tion (changing exposures and focuses) or backwardly inverting restoration (all-in-focus and HDR imaging).
In the experiments, our method outperforms baseline meth-ods in all-in-focus imaging and HDR imaging. Compared with traditional methods, our model can recover all-in-focus
HDR images from fewer input images. 2.