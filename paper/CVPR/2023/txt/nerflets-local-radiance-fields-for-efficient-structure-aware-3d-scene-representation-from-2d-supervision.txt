Abstract
We address efficient and structure-aware 3D scene rep-resentation from images. Nerflets are our key contribution– a set of local neural radiance fields that together represent a scene. Each nerflet maintains its own spatial position, ori-entation, and extent, within which it contributes to panop-tic, density, and radiance reconstructions. By leveraging only photometric and inferred panoptic image supervision, we can directly and jointly optimize the parameters of a set of nerflets so as to form a decomposed representation of the scene, where each object instance is represented by a group of nerflets. During experiments with indoor and outdoor en-vironments, we find that nerflets: (1) fit and approximate the scene more efficiently than traditional global NeRFs, (2) al-low the extraction of panoptic and photometric renderings from arbitrary views, and (3) enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing.
Our project page. 1.

Introduction
This paper aims to produce a compact, efficient, and comprehensive 3D scene representation from only 2D im-ages. Ideally, the representation should reconstruct appear-ances, infer semantics, and separate object instances, so that it can be used in a variety of computer vision and robotics tasks, including 2D and 3D panoptic segmentation, interac-tive scene editing, and novel view synthesis.
Many previous approaches have attempted to generate rich 3D scene representations from images. PanopticFu-sion [34] produces 3D panoptic labels from images, though it requires input depth measurements from specialized sen-sors. NeRF [32] and its descendants [3, 4, 33, 39] produce 3D density and radiance fields that are useful for novel view synthesis, surface reconstruction, semantic segmenta-tion [50, 60], and panoptic segmentation [5, 21]. However, existing approaches require 3D ground truth supervision, are inefficient, or do not handle object instances.
We propose nerflets, a 3D scene representation with mul-tiple local neural fields that are optimized jointly to describe the appearance, density, semantics, and object instances in a scene (Figure 1). Nerflets constitute a structured and irreg-ular representation– each is parameterized by a 3D center, a 3D XYZ rotation, and 3 (per-axis) radii in a 9-DOF coordi-nate frame. The influence of every nerflet is modulated by a radial basis function (RBF) which falls off with increasing distance from the nerflet center according to its orientation and radii, ensuring that each nerflet contributes to a local
part of the scene. Within that region of influence, each ner-flet has a miniature MLP to estimate density and radiance. It also stores one semantic logit vector describing the category (e.g., “car”) of the nerflet, and one instance label indicating which real-world object it belongs to (e.g., “the third car”).
In Figure 1, each ellipsoid is a single nerflet, and they are colored according to their semantics.
A scene can contain any number of nerflets, they may be placed anywhere in space, and they may overlap, which provides the flexibility to model complex, sparse 3D scenes efficiently. Since multiple nerflets can have the same in-stance label, they can combine to represent the density and radiance distributions of complex object instances. Con-versely, since each nerflet has only one instance label, the nerflets provide a complete decomposition of the scene into real-world objects. Nerflets therefore provide a 3D panoptic decomposition of a scene that can be rendered and edited.
Synthesizing images using nerflets proceeds with density-based volume rendering just as in NeRF [32]. How-ever, instead of evaluating one large MLP at each point sam-ple along a ray, we evaluate the small MLPs of only the ner-flets near a sample. We average the results, weighting by the influence each nerflet has over that sample. The ren-dering is fully-differentiable with respect to all continuous parameters of the nerflets. Fitting the nerflet representation is performed from a set of posed RGB images with a single training stage. After training, instance labels are assigned based on the scene structure, completing the representation.
Experiments with indoor and outdoor datasets confirm the main benefits of nerflets. We find that: 1) Parsimony en-courages the optimizer to decompose the scene into nerflets with consistent projections into novel panoptic images (Sec-tion 4.1); 2) Semantic supervision can be beneficial to novel view synthesis (Section 4.2); 3) Structure encourages effi-ciency, compactness, and scalability (Section 3.4); and 4) the explicit decomposition of a scene improves human inter-pretability for easy interactive editing, including adding and removing objects (Section 4.1). These benefits enable state-of-the-art performance on the KITTI360 [24] novel seman-tic view synthesis benchmark, competitive performance on
ScanNet 3D panoptic segmentation tasks with more limited supervision, and an interactive 3D editing tool that lever-ages the efficiency and 3D decomposition of nerflets.
The following summarizes our main contributions:
• We propose a novel 3D scene representation made of small, posed, local neural fields named nerflets.
• The pose, shape, panoptic, and appearance information of nerflets are all fit jointly in a single training stage, re-sulting in a comprehensive learned 3D decomposition from real RGB images of indoor or outdoor scenes.
• We test nerflets on 4 tasks- novel view synthesis, panoptic view synthesis, 3D panoptic segmentation and reconstruction, and interactive editing.
• We achieve 1st place on the KITTI-360 semantic novel view synthesis leaderboard. 2.