Abstract
Constant Decay Constant # Events
Proposed
In virtually all event-based vision problems, there is the need to select the most recent events, which are assumed to carry the most relevant information content. To achieve this, at least one of three main strategies is applied, namely: 1) constant temporal decay or fixed time window, 2) con-stant number of events, and 3) flow-based lifetime of events.
However, these strategies suffer from at least one major lim-itation each. We instead propose a novel decay process for event cameras that adapts to the global scene dynam-ics and whose latency is in the order of nanoseconds. The main idea is to construct an adaptive quantity that encodes the global scene dynamics, denoted by event activity. The proposed method is evaluated in several event-based vision problems and datasets, consistently improving the corre-sponding baseline methods’ performance. We thus believe it can have a significant widespread impact on event-based re-search. Code available: https://github.com/neuromorphic-paris/event batch. 1.

Introduction
Contrary to standard frame-based cameras that capture visual data at a fixed rate and independently of the scene dynamics [31], event cameras respond asynchronously to pixel-wise brightness changes by generating events. Event cameras are thus data-driven sensors that inherently react to the scene dynamics [18] while providing additional advan-tages: high temporal resolution in the order of microsec-onds, low latency, low power consumption, and high dy-namic range. However, due to the different visual sensing paradigm, event cameras pose the challenge of developing new methods that fully unlock their capabilities [7]. By
“scene dynamics”, we mean the spatio-temporal changes of the visual inputs due to the relative motion of the scene w.r.t. the camera. As in mechanics, the involved quantities are the zero, first, or higher-order derivatives of captured temporal information.
There are two paradigms for event processing, namely: event-based [16, 17, 26, 27, 38], whereby each event is pro-w o l
S t s a
F e s r a p
S e s n e
D
Figure 1. Accumulated events using (left) constant temporal de-cay, (middle) constant number of events, and (right) the proposed adaptive global decay process.
In contrast to the constant pro-cesses, the proposed decay process globally adapts to the scene dynamics. Sequences from dataset [24]. cessed one-by-one, e.g., by locally updating some state, and batch-based [9,30,44,45], whereby events are grouped into a batch for later processing. In both paradigms, there is the intuitive notion that more recent events should be assigned more relevancy. Thus, each event should have a certain lifespan, based on which each one is considered active and whose contribution is considered meaningful to the compu-tation. This raises the fundamental question on what kind of decay strategy should be employed to quantify each event’s lifespan. Currently, there exist three main decay strategies.
The first is to consider the events generated within a fixed time window or by applying some constant temporal de-cay [17,35]. This strategy does not preserve the data-driven paradigm of event cameras since it imposes an arbitrary fi-nite temporal support in practice, which generally has no relation to the visual information source and does not ac-count for variations in motion magnitude. A single con-stant temporal decay can not handle slow and fast motions simultaneously since, e.g., as illustrated in the first two im-ages on the left column in Fig. 1, when the motion is fast, the accumulated events generate a blurry frame. The sec-ond strategy considers instead a constant number of events which preserves better the data-driven nature of event cam-eras as argued in [9]. However, it breaks for variations in the scene texture, whereby, e.g., as illustrated in the last two images on the middle column in Fig. 1, when the texture is sparse, the accumulated events also generate a blurry frame.
The third strategy explicitly models the set of active events by estimating each event’s lifetime from the corresponding velocity [19, 23]. Although this strategy does not require any explicit decay parameter, it still requires tuning param-eters for the flow estimation, which, in turn, may introduce non-negligible latency [23].
We address the fundamental question posed by propos-ing a novel decay process that inherently adapts to the global scene dynamics while introducing negligible latency in the order of nanoseconds, i.e., three orders of magnitude lower than the temporal resolution of event cameras. The main idea is to construct a decay process that depends on a global adaptive quantity that encodes the scene dynamics, which we denote by event activity. As shown on the right column in Fig. 1, accumulating events using the proposed decay process always generates sharp edge-like frames, re-gardless of the event stream dynamics. 2. Event Camera Working Principle
The output of an event camera consists of an asyn-chronous stream of temporal contrast events {ei}, i ∈ N.
Each event ei represents a spatio-temporal asynchronous brightness change, being defined as a tuple ei := (xi, yi, ti, pi) , (1) where (xi, yi) are the pixel coordinates, ti is the timestamp at which the event was generated, and pi ∈ {−1, +1} is the polarity of the event. An event ei is generated when the change in log-brightness log Ix,y(t) := ¯Ix,y(t) is above a threshold L
|∆¯Ixi,yi(ti)| = |¯Ixi,yi(ti) − ¯Ixi,yi(ti − ∆ti)| ≥ L, (2) where ∆ti is the time since the last event at the same pixel.
The change in log-brightness can be rewritten in terms of continuous changes in a time interval
∆¯Ixi,yi(ti) = (cid:90) ti ti−∆ti d¯Ixi,yi(t) dt dt, (3) from which the brightness signal sensing by a camera can be approximated by a multiplicative process of the global illumination ¯L, by the scene reflectance ¯R [28]
∆¯Ixi,yi(ti) = (cid:90) ti ti−∆ti (cid:18) d¯L(t) dt
∂ ¯R(t)
∂x
+ dx dt
+
∂ ¯R(t)
∂t
∂ ¯R(t)
∂y
+ (cid:19) dy dt dt, (4) the log-reflectance location (xi, yi) de-where we have omitted the pixel
The term d¯L(t)/dt repre-pendency for readability. the term ∂ ¯R(t)/∂t sents the log-illumination change, is terms the change, temporal (cid:0)∂ ¯R(t)/∂x, ∂ ¯R(t)/∂y(cid:1) are the spatial contrast or the scene texture, and the terms (dx/dt, dy/dt) correspond to the optical flow. The contribution from all these terms is denoted as the dynamics of the event stream. We further detail the terminology by noting that only the rate of change of the spatial contrast contribution is w.r.t. space, whereas the rate of change of the remaining contributions is w.r.t. time. We denote the former as the spatial dynamics and the latter as the temporal dynamics. We also denote the contribution from optical flow as the motion dynamics.
Eq. (4) makes explicit what an event encodes. How-ever, each contribution is not easily separable from the event stream alone without further assumptions. In this work, we show that it is still feasible to construct a global decay pro-cess that implicitly encodes the event stream dynamics. 3.