Abstract
For robots to be generally useful, they must be able to ﬁnd arbitrary objects described by people (i.e., be language-driven) even without expensive navigation train-ing on in-domain data (i.e., perform zero-shot inference).
We explore these capabilities in a uniﬁed setting: language-driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classiﬁcation, we investigate a straightforward framework,
CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without ﬁne-tuning. To better evaluate L-ZSON, we introduce the PASTURE benchmark, which considers
ﬁnding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described rel-ative to visible objects. We conduct an in-depth empiri-cal study by directly deploying 22 CoW baselines across
HABITAT, ROBOTHOR, and PASTURE. In total, we eval-uate over 90k navigation episodes and ﬁnd that (1) CoW baselines often struggle to leverage language descriptions but are proﬁcient at ﬁnding uncommon objects. (2) A sim-ple CoW, with CLIP-based object localization and classical exploration—and no additional training—matches the nav-igation efﬁciency of a state-of-the-art ZSON method trained for 500M steps on HABITAT MP3D data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art ROBOTHOR ZSON model.1 1.

Introduction
To be more widely applicable, robots should be language-driven: able to deduce goals based on arbitrary text input instead of being constrained to a ﬁxed set of ob-ject categories. While existing image classiﬁcation, seman-tic segmentation, and object navigation benchmarks like
ImageNet-1k [65], ImageNet-21k [22], MS-COCO [45],
LVIS [28], HABITAT [67], and ROBOTHOR [18] include a vast array of everyday items, they do not capture all objects that matter to people. For instance, a lost “toy airplane” may
⇧Columbia University, †University of Washington. Correspondence to sy@cs.columbia.edu. 1For code, data, and videos, see cow.cs.columbia.edu/
Sample tasks
Top-down visualization
Egocentric Observations (a) Finding uncommon objects
“…llama wicker  basket…”
“…tie-dye  surfboard…” goal ✅  goal ✅  (b) Finding objects based on attributes in the presence of distractors
“…small, green  apple…”
“…apple on a  coffee table near a  laptop…” correct apple ✅  distractor apple⛔ (c) Finding hidden objects in the presence of distractors correct mug ✅  distractor mug⛔
“…mug under the  bed…”
Figure 1. The PASTURE benchmark for L-ZSON. Text speci-ﬁes navigation goal objects. Agents do not train on these tasks, making the evaluation protocol zero-shot. (a) Uncommon object goals like “llama wicker basket”, not found in existing navigation benchmarks. (b) Appearance, spatial descriptions, which are nec-essary to ﬁnd the correct object. (c) Hidden object descriptions, which localize objects that are not visible. become relevant in a kindergarten classroom, but this object is not annotated in any of the aforementioned datasets.
In this paper, we study Language-driven zero-shot object navigation (L-ZSON), a more challenging but also more ap-plicable version of object navigation [5, 18, 67, 79, 89] and
ZSON [38,46] tasks. In L-ZSON, an agent must ﬁnd an ob-ject based on a description, which may contain different lev-els of granularity (e.g., “toy airplane”, “toy airplane under the bed”, or “wooden toy airplane”). L-ZSON encompasses
ZSON, which speciﬁes only the target category [38, 46].
Since L-ZSON is “zero-shot”, we consider agents without access to navigation training on the target objects or do-mains. This reﬂects realistic application scenarios, where the environment and object set may not be known a priori.
Performing L-ZSON in any environment with unstruc-tured language input is challenging; however, recent ad-vances in open-vocabulary models for image classiﬁca-tion [35, 58, 61], object detection [4, 21, 27, 36, 43, 47, 49, 62, 88], and semantic segmentation [3, 6, 15, 33, 36, 37, 86] present a promising foundation. These models provide an interface where one speciﬁes—in text—the arbitrary ob-jects they wish to classify, detect, or segment. For example,
CLIP [61] open-vocabulary classiﬁers compute similarity scores between an input image and a set of user-speciﬁed captions (e.g., “a photo of a toy airplane.”, ...), selecting the caption with the highest score to determine the image clas-siﬁcation label. Given the ﬂexibility of these models, we would like to understand their capability to execute embod-ied tasks even without additional training.
To this end, we present baselines and benchmarks for L-ZSON. More speciﬁcally:
• A collection of baseline algorithms, CLIP on Wheels (CoW), which adapt open-vocabulary models to the task of L-ZSON. CoW takes inspiration from the semantic mapping line of work [11, 41, 53], and decomposes the navigation task into exploration when the language tar-get is not conﬁdently localized, and target-driven plan-ning otherwise. CoW retains the textual user inter-face of the original open-vocabulary model and does not require any navigation training. We evaluate 22
CoWs, ablating over many open-vocabulary models, ex-ploration policies, backbones, prompting strategies, and post-processing strategies.
• A new benchmark, PASTURE, to evaluate CoW and fu-ture methods on L-ZSON. We design PASTURE, visual-ized in Fig. 1, to study capabilities that traditional object navigation agents, which are trained on a ﬁxed set of cat-egories, do not possess. We consider the ability to ﬁnd (1) uncommon objects (e.g., “tie-dye surfboard”), (2) ob-jects by their spatial and appearance attributes in the pres-ence of distractor objects (e.g., “green apple” vs. “red apple”), and (3) objects that cannot be visually observed (e.g., “mug under the bed”).
Together the CoW baselines and PASTURE benchmark allow us to conduct extensive studies on the capabilities of open-vocabulary models in the context of L-ZSON embod-ied tasks. Our experiments demonstrate CoW’s potential on uncommon objects and limitations in taking full advan-tage of language descriptions—thereby providing empirical motivation for future studies. To contextualize CoW rela-tive to prior zero-shot methods, we additionally evaluate on the HABITAT MP3D dataset. We ﬁnd that our best CoW achieves navigation efﬁciency (SPL) that matches a state-of-the-art ZSON method [46] that trains on MP3D train-ing data for 500M steps. On a ROBOTHOR object subset, considered in prior work, the same CoW beats the leading method [38] by 15.6 percentage points in task success. into map reconstruction [30, 32, 51, 52, 72], agent localiza-tion [17, 20, 54], and planning [41, 78]. Recent work in-vestigates learned alternatives for exploration [7, 14, 56, 57, 63]. Here, agents are often trained end-to-end with self-supervised rewards (e.g., curiosity [57]) or supervised re-wards (e.g., state visitation counts [25, 73, 75]). Learning-based methods typically need less hand-tuning, but require millions of training steps and reward engineering. We test both classical and learnable exploration strategies in the context of CoW to study their applicability to L-ZSON.
Goal-conditioned navigation. Apart from open-ended exploration, many navigation tasks are goal-conditioned, where the agent needs to navigate to a speciﬁed position (i.e., point goal [11, 12, 26, 31, 66, 77, 81, 83]), view of the environment (i.e., image goal [48, 64, 89]), or object cate-gory (i.e., object goal [1, 9, 10, 12, 19, 44, 74, 79, 84]). We consider an object goal navigation task.
Prior work investigates
Vision-Language Navigation. language-based navigation, where language provides step-by-step instructions for the task [2, 34, 39, 40, 71]. This line of work demonstrates the beneﬁts of additional language in-put for robot navigation, especially for long-horizon tasks (e.g., room-to-room navigation [40]). However, provid-ing detailed step-by-step instructions (e.g., move 3 meters south [34]) could be challenging and time-consuming. In our L-ZSON task, an algorithm gets natural language as the goal description instead of low-level instructions. Prior work also investigates navigation with target descriptions in supervised settings [42, 60, 87]. In contrast, we explore a zero-shot evaluation protocol and consider ﬁnding hid-den objects (e.g., “mug under bed”) and uncommon objects (e.g., “tie-dye surfboard”).
Zero-shot object navigation (ZSON). Recent work stud-ies object navigation in zero-shot settings, where agents are evaluated on object categories that they are not explicitly trained on [38, 46]. Our task encompasses ZSON; however it also considers cases where more information—object at-tributes or hidden objects descriptions—is speciﬁed. Khan-delwal et al. [38] train on a subset of ROBOTHOR cate-gories and evaluate on a held-out set. In concurrent work,
Majumdar et al. [46] train on an image goal navigation task and evaluate on object navigation downstream by leverag-ing CLIP multi-modal embeddings. Both algorithms neces-sitate navigation training for millions of steps and train sep-arate models for each simulation domain. In contrast, CoW baselines do not necessitate any simulation training and can be deployed in multiple environments. 2.