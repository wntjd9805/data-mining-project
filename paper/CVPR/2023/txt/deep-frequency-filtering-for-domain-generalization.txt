Abstract
Improving the generalization ability of Deep Neural Net-works (DNNs) is critical for their practical uses, which has been a longstanding challenge. Some theoretical studies have uncovered that DNNs have preferences for some fre-quency components in the learning process and indicated that this may affect the robustness of learned features. In this paper, we propose Deep Frequency Filtering (DFF) for learning domain-generalizable features, which is the first endeavour to explicitly modulate the frequency components of different transfer difficulties across domains in the latent space during training. To achieve this, we perform Fast
Fourier Transform (FFT) for the feature maps at different layers, then adopt a light-weight module to learn attention masks from the frequency representations after FFT to en-hance transferable components while suppressing the com-ponents not conducive to generalization. Further, we empir-ically compare the effectiveness of adopting different types of attention designs for implementing DFF. Extensive exper-iments demonstrate the effectiveness of our proposed DFF and show that applying our DFF on a plain baseline out-performs the state-of-the-art methods on different domain generalization tasks, including close-set classification and open-set retrieval. 1.

Introduction
Domain Generalization (DG) seeks to break through the i.i.d. assumption that training and testing data are identi-cally and independently distributed. This assumption does not always hold in reality since domain gaps are commonly seen between the training and testing data. However, col-lecting enough training data from all possible domains is costly and even impossible in some practical environments.
Thus, learning generalizable feature representations is of
*This work was done when Shiqi Lin and Zhipeng Huang were interns at Microsoft Research Asia. high practical value for both industry and academia.
Recently, a series of research works [78] analyze deep learning from the frequency perspective. These works, represented by the F-Principle [75], uncover that there are different preference degrees of DNNs for the infor-mation of different frequencies in their learning processes.
Specifically, DNNs optimized with stochastic gradient-based methods tend to capture low-frequency components of the training data with a higher priority [74] while exploit-ing high-frequency components to trade the robustness (on unseen domains) for the accuracy (on seen domains) [66].
This observation indicates that different frequency compo-nents are of different transferability across domains.
In this work, we seek to learn generalizable features from a frequency perspective. To achieve this, we conceptual-ize Deep Frequency Filtering (DFF), which is a new tech-nique capable of enhancing the transferable frequency com-ponents and suppressing the ones not conducive to general-ization in the latent space. With DFF, the frequency compo-nents of different cross-domain transferability are dynam-ically modulated in an end-to-end manner during training.
This is conceptually simple, easy to implement, yet remark-ably effective. In particular, for a given intermediate fea-ture, we apply Fast Fourier Transform (FFT) along its spa-tial dimensions to obtain the corresponding frequency rep-resentations where different spatial locations correspond to different frequency components. In such a frequency do-main, we are allowed to learn a spatial attention map and multiply it with the frequency representations to filter out the components adverse to the generalization across do-mains.
The attention map above is learned in an end-to-end manner using a lightweight module, which is instance-adaptive. As indicated in [66, 74], low-frequency com-ponents are relatively easier to be generalized than high-frequency ones while high-frequency components are com-monly exploited to trade robustness for accuracy. Although this phenomenon can be observed consistently over differ-ent instances, it does not mean that high-frequency com-ponents have the same proportion in different samples or have the same degree of effects on the generalization abil-ity. Thus, we experimentally compare the effectiveness of task-wise filtering with that of instance-adaptive filtering.
Here, the task-wise filtering uses a shared mask over all in-stances while the instance-adaptive filtering uses unshared masks. We find the former one also works but is inferior to our proposed design by a clear margin. As analyzed in [10], the spectral transform theory [32] shows that updating a sin-gle value in the frequency domain globally affects all orig-inal data before FFT, rendering frequency representation as a global feature complementary to the local features learned through regular convolutions. Thus, a two-branch architec-ture named Fast Fourier Convolution (FFC) is introduced in [32] to exploit the complementarity of features in the fre-quency and original domains with an efficient ensemble. To evaluate the effectiveness of our proposed DFF, we choose this two-branch architecture as a base architecture and apply our proposed frequency filtering mechanism to its spectral transform branch. Note that FFC provides an effective im-plementation for frequency-space convolution while we in-troduce a novel frequency-space attention mechanism. We evaluate and demonstrate our effectiveness on top of it.
Our contributions can be summarized in the following:
• We discover that the cross-domain generalization ability of DNNs can be significantly enhanced by a simple learn-able filtering operation in the frequency domain.
• We propose an effective Deep Frequency Filtering (DFF) module where we learn an instance-adaptive spatial mask to dynamically modulate different frequency components during training for learning generalizable features.
• We conduct an empirical study for the comparison of dif-ferent design choices on implementing DFF, and find that the instance-level adaptability is required when learning frequency-space filtering for domain generalization. 2.