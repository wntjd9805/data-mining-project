Abstract 1.

Introduction
Can one inject new concepts into an already trained gen-erative model, while respecting its existing structure and knowledge? We propose a new task – domain expansion – to address this. Given a pretrained generator and novel (but related) domains, we expand the generator to jointly model all domains, old and new, harmoniously. First, we note the generator contains a meaningful, pretrained latent space. Is it possible to minimally perturb this hard-earned representation, while maximally representing the new do-mains? Interestingly, we find that the latent space offers un-used, “dormant” directions, which do not affect the output.
This provides an opportunity: By “repurposing” these di-rections, we can represent new domains without perturbing the original representation. In fact, we find that pretrained generators have the capacity to add several – even hundreds – of new domains! Using our expansion method, one “ex-panded” model can supersede numerous domain-specific models, without expanding the model size. Additionally, a single expanded generator natively supports smooth transi-tions between domains, as well as composition of domains.
Code and project page available here.
Recent domain adaptation techniques piggyback on the tremendous success of modern generative image models [3, 12, 32, 40], by adapting a pretrained generator so it can generate images from a new target domain. Oftentimes, the target domain is defined with respect to the source do-main [5,21,22], e.g., changing the “stylization” from a pho-torealistic image to a sketch. When such a relationship holds, domain adaptation typically seeks to preserve the fac-tors of variations learned in the source domain, and transfer them to the new one (e.g., making the human depicted in a sketch smile based on the prior from a face generator).
With existing techniques, however, the adapted model loses the ability to generate images from the original domain.
In this work, we introduce a novel task — domain ex-pansion. Unlike domain adaptation, we aim to augment the space of images a single model can generate, without over-riding its original behavior (see Fig. 1). Rather than view-ing similar image domains as disjoint data distributions, we treat them as different modes in a joint distribution. As a result, the domains share a semantic prior inherited from the original data domain. For example, the inherent factors of variation for photorealistic faces, such as pose and face shape, can equally apply to the domain of “zombies”.
with multiple datasets, and expand the generator with hun-dreds of new factors of variation. Crucially, we show our expanded model simultaneously generates high-quality im-ages from both original and new domains, comparable to specialized, domain-specific generators. Thus, a single ex-panded generator supersedes hundreds of adapted genera-tors, facilitating the deployment of generative models for real-world applications. We additionally demonstrate that the new domains are learned as global and disentangled fac-tors of variation, alongside existing ones. This enables fine-grained control over the generative process and paves the way to new applications and capabilities, e.g., compositing multiple domains (See Fig. 2). Finally, we conduct a de-tailed analysis of key aspects of our method, such as the ef-fect of the number of newly introduced domains, thus shed-ding light on our method and, in the process, on the nature of the latent space of generative models.
To summarize, our contributions are as follows:
• We introduce a new task – domain expansion of a pre-trained generative model.
• We propose a novel latent space structure that is amenable to representing new knowledge in a disentangled manner, while maintaining existing knowledge intact.
• We present a simple paradigm transforming domain adap-tation methods into domain expansion methods.
• We demonstrate successful domain expansion to hun-dreds of new domains and illustrate its advantage over domain adaptation methods. 2.