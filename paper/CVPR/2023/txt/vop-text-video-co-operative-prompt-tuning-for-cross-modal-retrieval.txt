Abstract
Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video & text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achieves a 1.4% average
R@1 gain across five text-video retrieval benchmarks with 6× less parameter overhead. The code will be available at https://github.com/bighuang624/VoP. 1.

Introduction
Due to the remarkable progress in large-scale contrastive language-image pre-training [16, 21, 22, 31], a recent pop-ular direction for the crucial text-video cross-modal re-trieval [9,25,34,36] task is to transfer pre-trained image-text knowledge to the video domain [10,27,40] with fine-tuning.
However, the dominant full fine-tuning strategy inevitably forgets the useful knowledge acquired in the large-scale pre-training phase and poses a risk of overfitting, as the entire model is updated with limited downstream data. Moreover, full fine-tuning requires to maintain an independent model
*Work done during internship at Alibaba DAMO Academy.
†Corresponding author.
Figure 1. Fine-tuning comparison of our proposed methods and full fine-tuning (Full). For each method, we represent the
R@1 (recall at rank 1) gain on the MSR-VTT-9k dataset together with the number of trainable parameters. And we show only a part of our proposed methods for clarity, labeled with ✩. More detailed results are reported in Sec. 4.2. weight for every dataset during deployment, which becomes infeasible due to the increasing model capacity.
In this paper, we introduce prompt tuning [20, 24] to address the challenges that limit the transferability and gen-eralizability. Keeping the backbone frozen and only tuning a few extra parameters prepended to the input, prompt tuning has been widely applied as a flexible and light-weight fine-tuning protocol. Compared to uni-modal ap-plications [1, 23], text-video cross-modal retrieval requires more parameters to support the dual-branch structure, making it logical to benefit from the parameter-efficient tuning strategy. In addition, different from text descriptions that compose sequential information from words, video-understanding requires summarizing information in both the spatial and temporal dimensions. Therefore, we assume that designing non-trivial video prompts further contributes to prompting both branches for mutual promotion.
According to the above discussion, we propose the VoP:
Text-Video Co-operative Prompt Tuning to simultaneously introduce tunable prompts in both textual and visual en-coders. Also, different from existing related efforts [18] that
only insert prompt vectors into the input textual sequences, we find that preparing prompts for every layer of both encoders can further close the gap to full fine-tuning. As observed in Fig. 1, VoP achieves competitive or superior performance than other efficient tuning protocols with only 0.1% parameter storage.
To exploit essential video-specific information, we fur-ther design three novel video prompts from different per-spectives, which can seamlessly replace conventional visual prompts in VoP. Specifically, (1) position-specific video prompts model the information shared between frames at the same relative position. (2) Generated context-specific video prompts integrate injected contextual message from the frame sequence into the intra-frame modeling. (3) And function-specific video prompts adaptively assist to learn intra- or inter-frame affinities by sensing the transformation of layer functions. By exploring video-specific prompts,
VoP offers a new way to transfer pre-trained foundation models to the downstream video domain.
We compare our solutions with popular tuning strategies on MSR-VTT [37] (both 9k and 7k splits), DiDeMo [14],
ActivityNet [13] and LSMDC [33]. Learning video-specific information while maintaining the pre-trained knowledge, our video prompts deliver an average R@1 improvement of up to 4.2% for VoP, and therefore exceed full fine-tuning
In by up to 1.4% with much fewer trainable parameters. summary, the main contributions of our work are three-fold:
• We propose the VoP as a strong baseline that effectively adapts CLIP to text-video retrieval with negligible train-able parameters.
• To exploit video-specific information, we further develop three video prompts respectively conditioned on the frame position, frame context, and layer function.
• Extensive experiments on five text-video retrieval bench-marks demonstrate that various combinations of our video prompts effectively enhance VoP, outperforming full fine-tuning with much less parameter overhead. 2.