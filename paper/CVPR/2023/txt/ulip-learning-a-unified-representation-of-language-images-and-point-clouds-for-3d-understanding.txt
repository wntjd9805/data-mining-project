Abstract 1.

Introduction
The recognition capabilities of current state-of-the-art 3D models are limited by datasets with a small number of annotated data and a pre-defined set of categories. In its 2D counterpart, recent advances have shown that simi-lar problems can be significantly alleviated by employing knowledge from other modalities, such as language.
In-spired by this, leveraging multimodal information for 3D modality could be promising to improve 3D understanding under the restricted data regime, but this line of research is not well studied. Therefore, we introduce ULIP to learn a unified representation of image, text, and 3D point cloud by pre-training with object triplets from the three modali-ties. To overcome the shortage of training triplets, ULIP leverages a pre-trained vision-language model that has al-ready learned a common visual and textual space by train-ing with massive image-text pairs. Then, ULIP learns a 3D representation space aligned with the common image-text space, using a small number of automatically synthesized triplets. ULIP is agnostic to 3D backbone networks and can easily be integrated into any 3D architecture. Experiments show that ULIP effectively improves the performance of multiple recent 3D backbones by simply pre-training them on ShapeNet55 using our framework, achieving state-of-the-art performance in both standard 3D classification and zero-shot 3D classification on ModelNet40 and ScanOb-jectNN. ULIP also improves the performance of PointMLP by around 3% in 3D classification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1 accuracy for zero-shot 3D classification on ModelNet40. Our code and pre-trained models will be released.
* Contact: lxue@salesforce.com
Figure 1. Illustration of ULIP. ULIP improves 3D understanding by aligning features from image, text and point cloud in the same space. To reduce the demand of 3D data, ULIP leverages image and text encoders that are pre-trained with large-scale image-text pairs, and aligns 3D representation to the pre-aligned image-text feature space using a small scale of training triplets. 3D visual understanding research [6, 12, 16, 17, 24, 25] is drawing significant attention in recent years due to the increasing demand of real-world applications such as aug-mented/virtual reality [1, 27, 31, 46], autonomous driv-ing [23, 57] and robotics [2, 48]. However, compared to their 2D counterpart, 3D visual recognition research is still limited by datasets with a small number of samples and a
small set of pre-determined categories [45, 51]. For exam-ple, ShapeNet55 [3], one of the largest publicly available 3D datasets, only contains around 52.5k samples of 3D ob-jects with 55 category labels. That is in contrast to the 2D domain, where ImageNet [7] contains millions of images that cover thousands of categories. This scale limit of 3D data, caused by the high cost of 3D data collection and an-notation [3,11,51,58], has been hindering the generalization of 3D recognition models and their real-world applications.
To tackle the shortage of annotated data, existing work in other domains shows that employing knowledge from different modalities can significantly help the concept un-derstanding in the original modality [39, 53]. Among such work, CLIP [39] pioneered alignment between visual and textual features by pre-training on large-scale image-text pairs. It improves state-of-the-art visual concept recogni-tion and enables zero-shot classification of unseen objects.
However, multimodal learning that involves 3D modality, and whether it can help 3D recognition tasks are still not well studied.
In this paper, we propose Learning a Unified Represen-tation of Language, Images, and Point Clouds (ULIP). An illustration of our framework is shown in Figure 1. Ob-taining a unified representation space of all 3 modalities re-quires large-scale triplets of image, text, and point cloud as training data. However, such triplets remain hard to col-lect compared to the large-scale image-text pairs available.
To circumvent the lack of triplet data, we take advantage of a vision-language model pretrained on massive image-text pairs, and align the feature space of a 3D point cloud encoder to the pre-aligned vision/language feature space.
When training the 3D encoder for space alignments, we use a small number of automatically synthesized triplets from ShapeNet55 [3] without requiring manual annotations.
Making use of a pretrained vision-language model lets us leverage the abundant semantics captured in the image-text feature space for 3D understanding. Our framework uses
CLIP as the vision and language model because of its ex-cellent generalization performance. During pre-training, we keep the CLIP model frozen and train the 3D encoder by aligning the 3D feature of an object with its correspond-ing textual and visual features from CLIP using contrastive learning. The pre-trained 3D backbone model can be fur-ther fine-tuned for different downstream tasks.
ULIP has three major advantages. First, ULIP can sub-stantially improve the recognition ability of 3D backbone models. Second, ULIP is agnostic to the architecture of 3D models; therefore, we can easily plug in any 3D back-bones and improve them with ULIP. Third, aligning three modalities in the same feature space can potentially enable more cross-domain downstream tasks, including zero-shot 3D classification and image-to-3D retrieval.
We quantitatively evaluate ULIP on two fundamental 3D tasks: standard 3D classification and zero-shot 3D classi-fication. We experiment with recent 3D networks includ-ing PointNet++ [36], PointMLP [29] and PointBERT [58].
Experimental results show that ULIP achieves state-of-the-art (SOTA) performance for both standard 3D classifica-tion and zero-shot 3D classification on ModelNet40 and
ScanObjectNN. Specifically, ULIP surpasses PointMLP by around 3% in standard 3D classification on ScanOb-jectNN [45]. ULIP also outperforms PointCLIP [59] (the previous SOTA) by around 28.8% top-1 accuracy in zero-shot 3D classification on ModelNet40. Moreover, we show-case the potential of applying ULIP on the image to point cloud retrieval task. Qualitative evaluation demonstrate our promising potential for cross-modal applications. 2.