Abstract
Egocentric 3D human pose estimation with a single head-mounted fisheye camera has recently attracted atten-tion due to its numerous applications in virtual and aug-mented reality. Existing methods still struggle in challeng-ing poses where the human body is highly occluded or is closely interacting with the scene. To address this issue, we propose a scene-aware egocentric pose estimation method that guides the prediction of the egocentric pose with scene constraints. To this end, we propose an egocentric depth estimation network to predict the scene depth map from a wide-view egocentric fisheye camera while mitigating the occlusion of the human body with a depth-inpainting net-work. Next, we propose a scene-aware pose estimation network that projects the 2D image features and estimated depth map of the scene into a voxel space and regresses the 3D pose with a V2V network. The voxel-based fea-ture representation provides the direct geometric connec-tion between 2D image features and scene geometry, and further facilitates the V2V network to constrain the pre-dicted pose based on the estimated scene geometry. To en-able the training of the aforementioned networks, we also generated a synthetic dataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, called EgoPW-Scene.
The experimental results of our new evaluation sequences show that the predicted 3D egocentric poses are accurate and physically plausible in terms of human-scene interac-tion, demonstrating that our method outperforms the state-of-the-art methods both quantitatively and qualitatively. 1.

Introduction
Egocentric 3D human pose estimation with head- or body-mounted cameras is extensively researched recently because it allows capturing the person moving around in a large space, while the traditional pose estimation methods can only record in a fixed volume. With this advantage, the egocentric pose estimation methods show great potential in various applications, including the xR technologies and mo-Figure 1.
Previous egocentric pose estimation methods like
EgoPW predict body poses that may suffer from body floating is-sue (the first row) or body-environment penetration issue (the sec-ond row). Our method predicts accurate and plausible poses com-plying with the scene constraints. The red skeletons are the ground truth poses and the green skeletons are the predicted poses. bile interaction applications.
In this work, we estimated the full 3D body pose from a single head-mounted fisheye camera. A number of works have been proposed, including Mo2Cap2 [39], xR-egopose [32], Global-EgoMocap [36], and EgoPW [35].
These methods have made significant progress in estimat-ing egocentric poses. However, when taking account of the interaction between the human body and the surrounding environment, they still suffer from artifacts that contrast the physics plausibility, including body-environment penetra-tions or body floating (see the EgoPW results in Fig. 1), which is mostly ascribed to the ambiguity caused by the self-occluded and highly distorted human body in the ego-centric view. This problem will render restrictions on sub-sequent applications including action recognition, human-object interaction recognition, and motion forecasting.
To address this issue, we propose a scene-aware pose estimation framework that leverages the scene context to constrain the prediction of an egocentric pose. This frame-work produces accurate and physically plausible 3D human body poses from a single egocentric image, as illustrated in
Fig. 1. Thanks to the wide-view fisheye camera mounted on the head, the scene context can be easily obtained even with only one egocentric image. To this end, we train an egocentric depth estimator to predict the depth map of the surrounding scene. In order to mitigate the occlusion caused by the human body, we predict the depth map including the visible human and leverage a depth-inpainting network to recover the depth behind the human body.
Next, we combine the projected 2D pose features and scene depth in a common voxel space and regress the 3D body pose heatmaps with a V2V network [22]. The 3D voxel representation projects the 2D poses and depth in-formation from the distorted fisheye camera space to the canonical space, and further provides direct geometric con-nection between 2D image features and 3D scene geome-try. This aggregation of 2D image features and 3D scene geometry facilitates the V2V network to learn the rela-tive position and potential interactions between the human body joints and the surrounding environment and further enables the prediction of plausible poses under the scene constraints.
Since no available dataset can be used for train these net-works, we proposed EgoGTA, a synthetic dataset based on the motion sequences of GTA-IM [3], and EgoPW-Scene, an in-the-wild dataset based on EgoPW [35]. Both of the datasets contain body pose labels and scene depth map la-bels for each egocentric frame.
To better evaluate the relationship between estimated egocentric pose and scene geometry, we collected a new test dataset containing ground truth joint positions in the egocentric view. The evaluation results on the new dataset, along with results on datasets in Wang et al. [36] and
Mo2Cap2 [39] demonstrate that our method significantly outperforms existing methods both quantitatively and qual-itatively. We also qualitatively evaluate our method on in-the-wild images. The predicted 3D poses are accurate and plausible even in challenging real-world scenes. To summa-rize, our contributions are listed as follows:
• The first scene-aware egocentric human pose estima-tion framework that predicts accurate and plausible egocentric pose with the awareness of scene context;
• Synthetic and in-the-wild egocentric datasets contain-ing egocentric pose labels and scene geometry labels;1
• A new depth estimation and inpainting networks to predict the scene depth map behind the human body;
• By leveraging a voxel-based representation of body pose features and scene geometry jointly, our method 1Datasets are released in our project page. Meta did not access or pro-cess the data and is not involved in the dataset release. outperforms the previous approaches and generates plausible poses considering the scene context. 2.