Abstract
Parameter regularization or allocation methods are ef-fective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uni-formly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very dif-ferent from learned tasks, and parameter allocation meth-ods face unnecessary parameter overhead when learning simple tasks.
In this paper, we propose the Parameter
Allocation & Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allo-cation and regularization based on its learning difficulty.
A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to mea-sure the task relatedness using only features of the new task.
Moreover, we propose a time-efficient relatedness-aware sampling-based architecture search strategy to reduce the parameter overhead for allocation. Experimental results on multiple benchmarks demonstrate that, compared with
SOTAs, our method is scalable and significantly reduces the model’s redundancy while improving the model’s per-formance. Further qualitative analysis indicates that PAR obtains reasonable task-relatedness. 1.

Introduction
Recently, the lifelong learning [9] ability of neural net-works, i.e., learning continuously from a continuous se-quence of tasks, has been extensively studied. It is natural for human beings to constantly learn and accumulate knowl-edge from tasks and then use it to facilitate future learning.
However, classical models [13, 18, 41] suffer catastrophic forgetting [12], i.e., the model’s performance on learned tasks deteriorates rapidly after learning a new one.
To overcome the catastrophic forgetting, many param-*Corresponding author: Yin Zhang.
Figure 1. PAR adaptively selects a strategy from regularization and allocation to handle each task according to its learning diffi-culty. The difficulty depends not only on the task itself, but also on whether the model has previously learned tasks related to it. eter regularization or allocation methods have been pro-posed. Parameter regularization methods [10, 16, 19, 21, 23, 25, 27] alleviate forgetting by adding a regularization term to the loss function and perform well when the new task does not differ much from learned tasks. Parameter al-location methods based on static models [7, 15, 31, 36] and dynamic models [2,20,24,26,29,30,34,38,39,42,45,47] al-locate different parameters to different tasks and can adapt to new tasks quite different from learned tasks. However, the above methods solve all tasks in a sequence uniformly, and ignore the differences of learning difficulty of different tasks. This leads to the significant forgetting in parameter regularization methods when learning a new task which is quite different from learned tasks, and also leads to unnec-essary parameter cost in parameter allocation methods when learning some simple tasks.
In this paper, we propose a difficulty-aware method
Parameter Allocation & Regularization (PAR). As shown
in Fig. 1, we assume that the learning difficulty of a task in continual learning depends not only on the task itself, but also on the accumulated knowledge in the model. A new task is easy to adapt for a model if it has learned re-lated tasks before and vice versa. Based on the assumption, the PAR adaptively adopts parameter allocation for difficult tasks and parameter regularization for easy tasks. Specif-ically, the PAR divides tasks into task groups and assigns each group a dedicated expert model. Given a new task, the
PAR measures the relatedness between it and existing task groups at first. If the new task is related to one of the ex-isting groups, it is easy for the corresponding expert. The
PAR adds the task to the related group and learns it by the expert via the parameter regularization. Otherwise, the new task is difficult for all existing experts, and the PAR assigns it to a new task group and allocates a new expert to learn it.
There are two challenges in this work: the measurement of relatedness and the parameter explosion associated with parameter allocation. For the first one, we try to measure the relatedness by the KL divergence between feature distribu-tions of tasks. However, the KL divergence is intractable and needs to be estimated since the feature distributions of tasks are usually unknown.
In addition, the constraint of lifelong learning that only data of the current task are avail-able exacerbates the difficulty of estimation. To solve above problems, inspired by the divergence estimation based on k-NN distance [44], we propose the divergence estimation method based on prototype distance, which only depends on the data of the current task. For the second one, we try to reduce parameter overhead per expert by searching com-pact architecture for it. However, the low time and memory efficiency is an obstacle to applying architecture search for a sequence of tasks in lifelong learning. To improve the efficiency of architecture search, we propose a relatedness-aware sampling-based hierarchical search. The main con-tributions of this work are as follows:
• We propose a lifelong learning framework named Pa-rameter Allocation & Regularization (PAR), which se-lects an appropriate strategy from parameter allocation and regularization for each task based on the learning difficulty. The difficulty depends on whether the model has learned related tasks before.
• We propose a divergence estimation method based on prototype distance to measure the distance between the new task and previous learned tasks with only data of the new task. Meanwhile, we propose a relatedness-aware sampling-based architecture search to reduce the parameter overhead of parameter allocation.
• Experimental results on CTrL, Mixed CIFAR100 and
F-CelebA, CIFAR10-5, CIFAR100-10, CIFAR100-20 and MiniImageNet-20 demonstrate that PAR is scal-able and significantly reduces the model redundancy while improving the model performance. Exhaustive ablation studies show the effectiveness of components in PAR and the visualizations show the reasonability of task distance in PAR. 2.