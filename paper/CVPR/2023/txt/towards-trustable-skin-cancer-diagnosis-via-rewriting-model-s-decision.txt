Abstract
Deep neural networks have demonstrated promising per-formance on image recognition tasks. However, they may heavily rely on confounding factors, using irrelevant arti-facts or bias within the dataset as the cue to improve perfor-mance. When a model performs decision-making based on these spurious correlations, it can become untrustable and lead to catastrophic outcomes when deployed in the real-world scene. In this paper, we explore and try to solve this problem in the context of skin cancer diagnosis. We intro-duce a human-in-the-loop framework in the model training process such that users can observe and correct the model’s decision logic when confounding behaviors happen. Specif-ically, our method can automatically discover confounding factors by analyzing the co-occurrence behavior of the sam-ples. It is capable of learning confounding concepts using easily obtained concept exemplars. By mapping the black-box model’s feature representation onto an explainable con-cept space, human users can interpret the concept and in-tervene via first order-logic instruction. We systematically evaluate our method on our newly crafted, well-controlled skin lesion dataset and several public skin lesion datasets.
Experiments show that our method can effectively detect and remove confounding factors from datasets without any prior knowledge about the category distribution and does not require fully annotated concept labels. We also show that our method enables the model to focus on clinical-related concepts, improving the model’s performance and trustworthiness during model inference. 1.

Introduction
Deep neural networks have achieved excellent perfor-mance on many visual recognition tasks [11, 14, 35]. Mean-while, there are growing concerns about the trustworthiness of the model’s black-box decision-making process. In med-ical imaging applications, one of the major issues is deep learning models’ confounding behaviors using irrelevant ar-tifacts (i.e. rulers, dark corners) or bias (i.e. image back-Figure 1. Our method allows people to correct the model’s con-founding behavior within the skin lesion training set via rewriting the model’s logic. grounds, skin tone) as the cue to make the final predictions.
These spurious correlations in the training distribution can make models fragile when novel testing samples are pre-sented. Therefore, transparency of decision-making and human-guided bias correction will significantly increase the reliability and trustworthiness of model deployments in a life-critical application scenario like cancer diagnosis.
For instance, due to the nature of dermatoscopic images, the skin cancer diagnosis often involves confounding fac-tors [4, 22, 24, 40], i.e., dark corners, rulers, and air pock-ets. Bissoto et al. [40] shows that deep learning models trained on common skin lesion datasets can be biased. Sur-prisingly, they found the model can reach an AUC of 73%, even though lesions in images are totally occluded. To bet-ter understand the issue of confounding behaviors, we il-lustrate a motivating example. Fig. 1 shows a represen-tative confounding factor “dark corners” in the ISIC2019-2020 [36, 38] dataset, where the presence of dark corners is much higher in the melanoma images than in benign im-ages. A model trained on this dataset tends to predict a le-sion as melanoma when dark corners appear. This model is undoubtedly untrustable and is catastrophic for deployment in real clinical practice. To deal with this nuisance and im-prove the model’s trustworthiness, an ideal method would be: i ) the model itself is explainable so that humans can un-derstand the prediction logic behind it. and ii) the model has an intervening mechanism that allows humans to correct the model once confounding behavior happens. Through this
way, the model can avoid dataset biasing issues and rely on expected clinical rules when needed to diagnose.
In this work, our overall goal is to improve model trans-parency as well as develop a mechanism that human-user can intervene the model training process when confound-ing factors are observed. The major difference between our proposed solution and existing works [19,28,29,33] are: (1)
For model visualization, we focus on generating human-understandable textual concepts rather than pixel-wise at-tribution explanations like Class Activation Maps [42]. (2)
For concept learning, we do not hold any prior knowl-edge about the confounding concepts within the dataset.
This makes our problem set-up more realistic and practi-cal than the existing concept-based explanation or inter-action [19, 33] framework where fully-supervised concept (3) For method generalization, our labels are required. method can be applied on top of any deep models.
Therefore, we propose a method that is capable of learning confounding concepts with easily obtained con-cept examplars. This is realized via clustering model’s co-occurrence behavior based on spectral relevance anal-ysis [20] and concept learning based on concept activation vector (CAVs) [18]. Also, a concept space learned by CAVs is more explainable than the feature-based counterparts.
To the end, we propose a human-in-the-loop framework that can effectively discover and remove the confounding behaviors of the classifier by transforming its feature rep-resentation into explainable concept scores. Then, humans are allowed to intervene based on first-order logic. Through human interaction (see Fig. 1) on the model’s concept space, people can directly provide feedback to the model training (feedback turned into gradients) and remove unwanted bias and confounding behaviors. Moreover, we notice that no suitable dermatology datasets are available for confounding behavior analysis. To increase the methods’ reproducibil-ity and data accessibility in this field, we curated a well-controlled dataset called ConfDerm containing 3576 im-ages based on well-known ISIC2019 and ISIC2020 datasets.
Within the training set, all images in one of the classes are confounded by one of five confounding factors, including dark corners, borders, rulers, air pockets, and hairs. Still, in the testing set, all images are random. Some confounding factors within the dataset are illustrated in Fig. 2.
We summarize our main contributions as: (1) We crafted a novel dataset called ConfDerm, which is the first skin lesion dataset used for systematically evaluating the model’s trustworthiness under different confounding behav-iors within the training set. (2) Our new spectral rele-vance analysis algorithm on the popular skin cancer dataset
ISIC2019-2020 has revealed insights that artifacts such as dark corners, rulers, and hairs can significantly confound modern deep neural networks. (3) We developed a human-in-the-loop framework using concept mapping and logic (a) (b) (c) (d) (e)
Figure 2. Observed confounding concepts in ISIC2019-2020 datasets, the top row shows sample images, and the bottom row is the corresponding heatmap from GradCAM: (a) dark corners. (b) rulers. (c) dark borders. (e) dense hairs. (f) air pockets. layer rewriting to make the model self-explainable and enable users effectively remove the model’s confounding behaviors. (4) Experimental results on different datasets demonstrate the superiority of our method on performance improvement, artifact removal, and skin tone debiasing. 2.