Abstract
Research on continual learning has recently led to a va-riety of work in unimodal community, however little atten-tion has been paid to multimodal tasks like visual question answering (VQA). In this paper, we establish a novel VQA
Continual Learning setting named VQACL, which contains two key components: a dual-level task sequence where vi-sual and linguistic data are nested, and a novel composi-tion testing containing new skill-concept combinations. The former devotes to simulating the ever-changing multimodal datastream in real world and the latter aims at measuring models’ generalizability for cognitive reasoning. Based on our VQACL, we perform in-depth evaluations of ﬁve well-established continual learning methods, and observe that they suffer from catastrophic forgetting and have weak gen-eralizability. To address above issues, we propose a novel representation learning method, which leverages a sample-speciﬁc and a sample-invariant feature to learn represen-tations that are both discriminative and generalizable for
VQA. Furthermore, by respectively extracting such repre-sentation for visual and textual input, our method can ex-plicitly disentangle the skill and concept. Extensive exper-imental results illustrate that our method signiﬁcantly out-performs existing models, demonstrating the effectiveness and compositionality of the proposed approach. The code is available at https://github.com/zhangxi1997/VQACL. 1.

Introduction
Continual learning [43] has recently gained a lot of at-tention in the deep learning community because it enables models to learn continually on a sequence of non-stationary tasks and is close to the human learning process [2, 36].
However, the vibrant research in continual learning mainly focuses on unimodal tasks such as image classiﬁcation [37, 46, 51] and sequence tagging [4, 48], and the demand of multimodal tasks is ignored. In recent years, the volume of multimodal data has grown tremendously [8, 56, 57]. For example, tens of millions of texts, images, and videos are uploaded to social media platforms every day, such as Face-Figure 1. The illustration of real-world scenario for VQA system, which may continuously receive new types of questions, fresh visual concepts, and novel skill-concept compositions. book and Twitter. To cope with such constantly emerging real-world data, a practical AI system should be capable of continually learning from multimodal sources while allevi-ating forgetting previously learned knowledge.
Visual Question Answering (VQA) is a typical multi-modal task and has drawn increasing interest over the past few years [12, 49, 60], which can automatically generate a textual answer given a question and an image. To deal with ever-changing questions and visual scenes in real life, applying continual learning to VQA is essential. However, it is not easy to set up a suitable continual learning setting for this task. We identify that two vital issues need to be considered. First, the VQA input comes from both vision and linguistic modalities, thus the task setting should si-multaneously tackle continuous data from both modalities in a holistic manner. For example, as shown in Fig. 1, the
AI system might deal with new types of questions (e.g.,
Where ..., Why ...) as well as fresh visual concepts (e.g., Lo-quat, Deer). Second, compositionality [24], a vital property of cognitive reasoning, should be considered in the VQA continual learning. The compositionality here denotes the model’s generalization ability towards novel combinations of reasoning skills (i.e., question type) and visual concepts (i.e., image object). As illustrated in Fig. 1, if the system has been trained with question type Count (e.g., How many) with a variety of objects (e.g., Person, Cat, and Dount), as well as another question type (e.g., What color) about a new object (e.g., Truck). Then, it is expected to answer a novel
question like ‘How many trucks are there?’, even if the composition of skill Count and concept Truck has yet to be seen. Such ability is very crucial when deploying a model in the real world because it is infeasible to view all possible skill-concept compositions. Remarkably, several works has addressed continual learning with VQA [14, 16, 25]. How-ever, they still apply a classic unimodal-like continual learn-ing setting for the task by devising a set of VQA tasks sim-ply based on question type or image scene, which ignores above two crucial issues: handling continuous multimodal data simultaneously and testing model’s compositionality.
To achieve these two keypoints, in this paper, we propose a novel generative VQA continual learning setting named
VQACL based on two well-known datasets: VQA v2 [13] and NExT-QA [49]. Speciﬁcally, as shown in Fig. 2(a), our
VQACL setting consists of a dual-level task sequence. In the outer level, we set up a sequence of linguistic-driven tasks to evaluate models’ ability for the ever-changing ques-tion types. Moreover, to process the continuously shifted visual contents, for each outer level task, we further con-struct a series of randomly ordered visual-driven subtasks according to image object categories in the inner level. Such dual-level setting is similar to the cognitive process of chil-dren, who master a skill by trying it on various objects. For example, when learning to recognize colors, a baby usually asks all the things surrounding him ‘what color’ they are.
Besides, to evaluate models’ compositionality, we construct a novel composition split. As shown in Fig 2(b), we remove a visual-driven subtask from each task in the outer level
In this way, the during training and utilize it for testing. testing data contain novel skill-concept combinations that are not seen at the training time.
In conclusion, on the one hand, our VQACL setting requires models to perform effective multimodal knowledge transfer from old tasks to new tasks while mitigating catastrophic forgetting [31]. On the other hand, the model should be capable of generalizing to novel compositions for cognitive reasoning.
Using the proposed VQACL setting, we establish an initial set of baselines by adapting several well-known and state-of-the-art continual learning methods [1, 3, 7, 22, 45] from image classiﬁcation to the generative VQA tasks.
The baselines are implemented on an advanced vision-and-language transformer [9] without pre-training. After bench-marking these baseline models, we ﬁnd that few of them can do well in the novel composition testing, which limits their wide applications in practice. To enhance the model’s compositionality, it is critical to learn an excellent repre-sentation that is discriminative for seen skills or concepts, and is generalizable to novel skill-concept compositions. To achieve it, recent static VQA methods [27, 47, 59] always
ﬁrst learn joint representations for visual and textual inputs, and then utilize contrastive learning to implicitly disentan-gle the skill and concept within the joint feature. How-ever, such implicit disentangling makes existing models still dogged by the interference between the skill and concept, leading to suboptimal generalization results. Moreover, the complex contrastive sample building process makes these works tough to be applied to continual learning.
Inspired by above discussions, we propose a novel repre-sentation learning method for VQACL, which introduces a sample-speciﬁc (SS) and a sample-invariant (SI) feature to learn better representations that are both discriminative and generalizable. To explicitly decouple the reasoning skills and visual concepts, we learn the SS and SI representation for visual and textual input separately. Speciﬁcally, the
SS feature for each modality is learned through a trans-former encoder that stacks multiple self-attention layers, which can encode the most attractive and salient contents into the SS feature to make it discriminative. For the SI feature, we resort to prototype learning to aggregate the object class or question type information into it. Because the category knowledge is stable and representative across different scenarios, the SI feature can possess strong gen-eralizability. Besides, to ﬁt the continual learning setting, we constantly update the SI feature in training. In this way, it can capture new typical knowledge while retaining his-torical experience, helping alleviate the forgetting problem.
In conclusion, combining the SS and SI features, we can obtain the representation that is conducive to the model’s compositional discriminability and generalizability.
In summary, the major contributions of our work are threefold: (1) We introduce a new continual learning set-ting VQACL to simulate real-world generative VQA. It can not only simultaneously tackle the continuous data from vision and linguistic modality, but also test models’ com-positionality for cognitive reasoning. (2) We propose a sim-ple but effective representation learning method for contin-ual VQA, which novelly deploys a discriminative sample-speciﬁc feature and a generalizable sample-invariant feature to alleviate forgetting and enhance the models’ composition ability. (3) We re-purpose and evaluate ﬁve well-established methods on our VQACL, and observe that they struggle to obtain satisfactory results. Remarkably, our model con-sistently achieves the best performance, demonstrating the effectiveness and compositionality of our approach. 2.