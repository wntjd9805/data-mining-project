Abstract
Video-based person re-identification (Re-ID) is a promi-nent computer vision topic due to its wide range of video surveillance applications. Most existing methods utilize spatial and temporal correlations in frame sequences to obtain discriminative person features. However, inevitable degradation, e.g., motion blur contained in frames, leading to the loss of identity-discriminating cues. Recently, a new bio-inspired sensor called event camera, which can asyn-chronously record intensity changes, brings new vitality to the Re-ID task. With the microsecond resolution and low latency, it can accurately capture the movements of pedes-trians even in the degraded environments. In this work, we propose a Sparse-Dense Complementary Learning (SDCL)
Framework, which effectively extracts identity features by fully exploiting the complementary information of dense frames and sparse events. Specifically, for frames, we build a CNN-based module to aggregate the dense features of pedestrian appearance step by step, while for event streams, we design a bio-inspired spiking neural network (SNN) backbone, which encodes event signals into sparse feature maps in a spiking form, to extract the dynamic motion cues of pedestrians. Finally, a cross feature alignment module is constructed to fuse motion information from events and appearance cues from frames to enhance identity represen-tation learning. Experiments on several benchmarks show that by employing events and SNN into Re-ID, our method significantly outperforms competitive methods. The code is available at https://github.com/Chengzhi-Cao/SDCL.
∗Corresponding author. This work was supported by the National
Key R&D Program of China under Grant 2020AAA0105702, the National
Natural Science Foundation of China (NSFC) under Grants 62225207, 62276243 and U19B2038, the University Synergy Innovation Program of
Anhui Province under Grant GXXT-2019-025.
Figure 1. Visual examples of learned feature maps. From top to bottom: (a) original images, (b) corresponding events, (c) feature maps of events, (d) feature maps of frames in PSTA [49] (w/o events), (e) feature maps of frames in our network (w/ events). 1.

Introduction
Person re-identification (Re-ID) identifies a specific per-son in non-overlapping camera networks and is used in a variety of surveillance applications [15, 31, 32]. Due to the availability of video data, video-based person Re-ID has at-tracted considerable attention. Compared with image-based
Re-ID methods, video sequences contain numerous detailed
spatial and temporal information, which is beneficial to im-proving Re-ID performance [34, 46, 50].
Most existing video-based Re-ID approaches rely on spatial and temporal correlation modules, which are use-ful for deriving human representations that are resistant to temporal changes and noisy regions [19,33,51,53]. To gen-erate a person’s representation from a video, they focus on shared information across numerous frames while taking into consideration the temporal context. Although video data can provide a wealth of appearance cues for identity representation learning, they also bring motion blur, illu-mination variations, and occlusions [24, 54]. These data-inherent phenomena result in the loss and ambiguity of es-sential identity-discriminating shape cues, and cannot be well solved by existing video-based Re-ID solutions [43].
Instead of depending solely on video sequences, this work intends to exploit event streams captured by event information and guide cameras to compensate for lost feature extraction in frames [44, 61].
Since the novel bio-inspired event camera can record per-pixel intensity changes asynchronously, it has high temporal resolution, high dynamic range, and low latency [12], providing a new perspective for person Re-ID. In other words, unlike tradi-tional cameras that capture dense RGB pixels at a fixed rate, the event camera can accurately encode the time, location, and sign of the brightness changes [37, 41], offering robust motion information to identify a specific person.
In this paper, we propose a sparse-dense complemen-tary learning network (SDCL) to fully extract complemen-tary features of consecutive dense frames and sparse event streams for video-based person Re-ID. First, for dense video sequences, we build a CNNs-based backbone to ag-gregate frame-level features step-by-step. For sparse event streams, we design a deformable spiking neural network to suit the sparse and asynchronous characteristics of events.
Because spiking neural network (SNN) has a specific event-triggered computation characteristic that can respond to the events in a nearly latency-free way, it is naturally fit for processing events and can preserve the spatial and tempo-ral information of events by utilizing a discretized input representation. Meanwhile, we introduce deformable op-eration to deal with the degradation of spikes in deeper layers of SNN, better utilizing the spatial distribution of events to guide the deformation of the sampling grid. Fi-nally, to jointly utilize sparse-dense complementary infor-mation, we propose a cross-feature alignment module to exploit the clear movement information from events and ap-pearance cues from frames to enhance representation capac-ity. As shown in Figure 1, the feature maps of events still preserve the sparse distribution of events, which can guide the baseline to capture and learn discriminative representa-tion clearly. Compared with the baseline (without events) in the fourth row, the learned feature maps in the fifth row show that our method tends to focus on the most important semantic regions in original frames and easily selects the better represented areas. The representation of events shows the contour and pose of a specific person. It presents that the sparse events can guide the baseline network to capture and learn discriminative representation clearly. the learned fea-ture maps of dense RGB frames and sparse events intend to capture different semantic regions, but they still have spatial correlation. Both of them contribute to the final results.
This work makes the following contributions:
• We introduce a new modality, called event streams, and explore its dynamic properties to guide person Re-ID. To the best of our knowledge, this is the first event-guided solution to tackle the video-based Re-ID task.
• We propose a sparse-dense complementary learning network to fully utilize the sparse events and dense frames simultaneously to enhance identity representa-tion learning in degraded conditions.
• We design a deformable spiking neural network to suit the sparse characteristics of event streams, which greatly utilizes the spatial consistency of events to pro-vide motion information for dense RGB frames in a lightweight architecture.
Extensive experiments are conducted on multiple datasets to demonstrate how the bio-inspired event camera can help improve the Re-ID performance of baseline models and achieve higher retrieval accuracy than SOTA methods. 2.