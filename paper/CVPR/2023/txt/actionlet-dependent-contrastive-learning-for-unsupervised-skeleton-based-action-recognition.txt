Abstract
The self-supervised pretraining paradigm has achieved great success in skeleton-based action recognition. How-ever, these methods treat the motion and static parts equally, and lack an adaptive design for different parts, which has a negative impact on the accuracy of action recognition. To realize the adaptive action modeling of both parts, we pro-pose an Actionlet-Dependent Contrastive Learning method (ActCLR). The actionlet, defined as the discriminative sub-set of the human skeleton, effectively decomposes motion regions for better action modeling.
In detail, by con-trasting with the static anchor without motion, we extract the motion region of the skeleton data, which serves as the actionlet, in an unsupervised manner. Then, center-ing on actionlet, a motion-adaptive data transformation method is built. Different data transformations are ap-plied to actionlet and non-actionlet regions to introduce more diversity while maintaining their own characteristics.
Meanwhile, we propose a semantic-aware feature pooling method to build feature representations among motion and static regions in a distinguished manner. Extensive experi-ments on NTU RGB+D and PKUMMD show that the pro-posed method achieves remarkable action recognition per-formance. More visualization and quantitative experiments demonstrate the effectiveness of our method. Our project website is available at https : / / langlandslin . github.io/projects/ActCLR/ 1.

Introduction
Skeletons represent human joints using 3D coordinate locations. Compared with RGB videos and depth data, skeletons are lightweight, privacy-preserving, and compact to represent human motion. On account of being easier and more discriminative for analysis, skeletons have been widely used in action recognition task [19,23,31,32,46,48].
Supervised skeleton-based action recognition meth-*Corresponding author. This work is supported by the National Natural
Science Foundation of China under contract No.62172020.
Figure 1. Our proposed approach (ActCLR) locates the motion regions as actionlet to guide contrastive learning. ods [3,27,28] have achieved impressive performance. How-ever, their success highly depends on a large amount of la-beled training data, which is expensive to obtain. To get rid of the reliance on full supervision, self-supervised learn-ing [16, 32, 34, 49] has been introduced into skeleton-based action recognition. It adopts a two-stage paradigm, i.e. first applying pretext tasks for unsupervised pretraining and then employing downstream tasks for finetuning.
According to learning paradigms, all methods can be classified into two categories: reconstruction-based [14, 32, 41] and contrastive learning-based. Reconstruction-based methods capture the spatial-temporal correlation by pre-dicting masked skeleton data. Zheng et al. [49] first pro-posed reconstructing masked skeletons for long-term global motion dynamics. Besides, the contrastive learning-based methods have shown remarkable potential recently. These methods employ skeleton transformation to generate pos-itive/negative samples. Rao et al. [24] applied Shear and
Crop as data augmentation. Guo et al. [8] further proposed to use more augmentations, i.e. rotation, masking, and flip-pling, to improve the consistency of contrastive learning.
These contrastive learning works treat different regions of the skeleton sequences uniformly. However, the motion regions contain richer action information and contribute more to action modeling. Therefore, it is sub-optimal to di-rectly apply data transformations to all regions in the previ-ous works, which may degrade the motion-correlated infor-mation too much. For example, if the mask transformation is applied to the hand joints in the hand raising action, the motion information of the hand raising is totally impaired.
It will give rise to the false positive problem, i.e., the seman-tic inconsistency due to the information loss between pos-itive pairs. Thus, it is necessary to adopt a distinguishable design for motion and static regions in the data sequences.
To tackle these problems, we propose a new actionlet-dependent contrastive learning method (ActCLR) by treat-ing motion and static regions differently, as shown in Fig. 1.
An actionlet [38] is defined as a conjunctive structure of skeleton joints. It is expected to be highly representative of one action and highly discriminative to distinguish the ac-tion from others. The actionlet in previous works is defined in a supervised way, which relies on action labels and has a gap with the self-supervised pretext tasks. To this end, in the unsupervised learning context, we propose to obtain actionlet by comparing the action sequence with the aver-age motion to guide contrastive learning. In detail, the av-erage motion is defined as the average of all the series in the dataset. Therefore, this average motion is employed as the static anchor without motion. We contrast the action sequence with the average motion to get the area with the largest difference. This region is considered to be the re-gion where the motion takes place, i.e., actionlet.
Based on this actionlet, we design a motion-adaptive transformation strategy. The actionlet region is transformed by performing the proposed semantically preserving data transformation. Specifically, we only apply stronger data transformations to non-actionlet regions. With less inter-ference in the motion regions, this motion-adaptive trans-formation strategy makes the model learn better seman-tic consistency and obtain stronger generalization perfor-mance. Similarly, we utilize a semantic-aware feature pool-ing method. By extracting the features in the actionlet re-gion, the features can be more representative of the motion without the interference of the semantics in static regions.
We provide thorough experiments and detailed analysis on NTU RGB+D [17, 26] and PKUMMD [18] datasets to prove the superiority of our method. Compared to the state-of-the-art methods, our model achieves remarkable results with self-supervised learning.
In summary, our contributions are summarized as fol-lows:
• We propose a novel unsupervised actionlet-based con-trastive learning method. Unsupervised actionlets are mined as skeletal regions that are the most discrimina-tive compared with the static anchor, i.e., the average motion of all training data.
• A motion-adaptive transformation strategy is designed for contrastive learning.
In the actionlet region, we employ semantics-preserving data transformations to learn semantic consistency. And in non-actionlet re-gions, we apply stronger data transformations to obtain stronger generalization performance.
• We utilize semantic-aware feature pooling to extract motion features of the actionlet regions. It makes fea-tures to be more focused on motion joints without be-ing distracted by motionless joints. 2.