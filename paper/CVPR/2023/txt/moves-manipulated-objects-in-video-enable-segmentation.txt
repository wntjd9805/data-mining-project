Abstract
Our method uses manipulation in video to learn to un-derstand held-objects and hand-object contact. We train a system that takes a single RGB image and produces a pixel-embedding that can be used to answer grouping ques-tions (do these two pixels go together) as well as hand-association questions (is this hand holding that pixel).
Rather than painstakingly annotate segmentation masks, we observe people in realistic video data. We show that pairing epipolar geometry with modern optical flow produces sim-ple and effective pseudo-labels for grouping. Given people segmentations, we can further associate pixels with hands to understand contact. Our system achieves competitive re-sults on hand and hand-held object tasks. 1.

Introduction
Fig. 1 shows someone making breakfast. Despite having never been there, you understand the bag the hand is hold-ing as an object, recognize that the hand is holding the bag, and recognize that the milk carton in the background is a distinct object. The goal of this paper is to build a computer vision system with such capabilities: grouping held objects (the bag), recognizing contact (the hand holding the bag), and grouping non-held objects (the carton). We accomplish our aim by pairing modern optical flow with 3D geome-try and, to associate objects with hands, per-pixel human masks. Our results show that direct discriminative train-ing on simple pseudo-labels generated by epipolar geome-try produces strong feature representations that we can use to solve a variety of hand-held object-related tasks.
The topic of understanding hands and the objects they hold has been a subject of intense interest from the com-puter vision community for decades. Recently, this has often taken the form of extensive efforts annotating hands and hand-held objects [9, 13, 36, 47]. These methods of-ten go beyond standard detection and segmentation ap-proaches [17, 25] by producing associations between hands and objects and by detecting on any held object, as opposed to a fixed set of pre-defined object classes. Since these re-Figure 1. Given an input image, MOVES produces features (shown using PCA to project to RGB) that easily group with ordinary clus-tering systems and can also be used to associate hands with the objects they hold. The clusters are often sufficient for defining objects, but additional cues such as a box further improve them.
At training time, MOVES learns this feature space from direct discriminative training on simple pseudo-labels. While MOVES learns only from objects that hands are actively holding (such as the semi-transparent bag), we show that it works well on inactive objects as well (such as the milk carton). quire expensive annotations, many researchers have started focusing on using weaker supervision [12, 37] by starting with a few readily obtained cues (e.g., basic information about humans, flow). These weakly-supervised methods, however, have not matched supervised methods regardless of supervision, methods like [36, 37] only understand ob-jects when they are held and cannot group un-held objects.
We propose a simple approach based on directly predict-ing two properties: grouping, or whether pixels move to-gether (the classic Gestalt law of common fate [43]); as well as hand association, whether a hand pixel is likely holding another pixel. We show that these can be learned from auto-matically generated pseudo-labels that use optical flow [21], epipolar geometry [15], and person masks [19]. Our net-work, named MOVES, learns a mapping to a per-pixel em-bedding; this embedding is then analyzed by grouping and association heads that are trained by cross-entropy to pre-dict the pseudo-labels. While the pseudo-labels themselves
are poor and incomplete, we show that the learned classi-fiers are effective and that the embeddings are good enough to be analyzed by off-the-shelf, unspecialized algorithms like HDBSCAN [31]. Excitingly, even though our signal comes only when objects are picked up, our features gener-alize to objects that are not currently being interacted with.
We train and evaluate MOVES on challenging egocentric data, including EPIC-KITCHENS [6, 7] and EGO4D [13].
Our experiments show that once trained, MOVES features enable strong performance on a number of tasks related to hands and the objects they hold. First, using MOVES on the COHESIV [37] hand-object segmentation bench-mark for EPIC-KITCHENS [6] improves by 31% relative (19.5 → 25.7) over the recent weakly-supervised COHE-SIV method [37] in object segmentation. Second, we show that distance in MOVES feature space is strongly predic-tive of two pixels being part of the same object, as well as a Box2Seg task where MOVES features are trivially an-alyzed to upgrade bounding-box annotations to segments.
We show that Box2Seg shows strong performance on both objects that are currently being held as well as objects that are not held (unlike past work).
In particular, compared to COHESIV, we show a strong gain on segmenting held objects (8.9 → 44.2 mIoU) as well as non-held objects (7.5 → 45.0 mIoU). Finally, we show that we can train an instance segmentation model [26] on the Box2Seg annota-tions and get good models for rough instance segmentation. 2.