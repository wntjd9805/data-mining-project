Abstract
Recently, few-shot action recognition receives increasing attention and achieves remarkable progress. However, pre-vious methods mainly rely on limited unimodal data (e.g.,
RGB frames) while the multimodal information remains rel-atively underexplored.
In this paper, we propose a novel
Active Multimodal Few-shot Action Recognition (AMFAR) framework, which can actively find the reliable modality for each sample based on task-dependent context information to improve few-shot reasoning procedure. In meta-training, we design an Active Sample Selection (ASS) module to or-ganize query samples with large differences in the reliabil-ity of modalities into different groups based on modality-specific posterior distributions. In addition, we design an
Active Mutual Distillation (AMD) to capture discrimina-tive task-specific knowledge from the reliable modality to improve the representation learning of unreliable modal-In meta-test, ity by bidirectional knowledge distillation. we adopt Adaptive Multimodal Inference (AMI) to adap-tively fuse the modality-specific posterior distributions with a larger weight on the reliable modality. Extensive experi-mental results on four public benchmarks demonstrate that our model achieves significant improvements over existing unimodal and multimodal methods. 1.

Introduction
Over the past years, action recognition [20, 34, 52, 73] has achieved significant progress with the emerge of deep learning. However, these existing deep methods require a large amount of labeled videos to guarantee their perfor-mance. In practice, it is sometimes expensive or even im-possible to collect abundant annotated data, which limits the effectiveness of supervised methods. In order to deal with this problem, more and more researchers begin to focus on the few-shot action recognition (FSAR) task, which aims at
*corresponding author: Changsheng Xu.
Figure 1. Illustration of multimodal few-shot action recognition task. The main challenge is that the contribution of a specific modality highly depends on task-specific contextual information. classifying unlabeled videos (query set) from novel action classes with the help of only a few annotated samples (sup-port set).
Recently, researchers have proposed many promising few-shot action recognition methods, which can be roughly divided into two groups: data augmentation-based methods and alignment-based methods. Data augmentation-based methods try to generate additional training data [18], self-supervision signals [72] or auxiliary information [22, 69] to promote robust representation learning. Alignment-based methods [5,8,44,58,66,69,72] focus on matching the video frames or segments in the temporal or spatial dimension to measure the distance between query and support samples in a fine-grained manner.
Although existing few-shot action recognition methods have achieved remarkable performance, they mainly rely on limited unimodal data (e.g. RGB frames) that are always insufficient to reflect complex characteristics of human ac-tions. When learning novel concepts from a few samples, humans have the ability to integrate the multimodal percep-tions (e.g. appearance, audio and motion) to enhance the recognition procedure. In addition, in conventional action recognition, many top-performing methods [23, 46, 56, 60]
always involve multiple modalities (e.g. vision, optical flow and audio) which can provide complementary information to comprehensively identify different actions. Whereas, the multimodal information remains relatively underexplored in few-shot action recognition where the data scarcity issue magnifies the defect of unimodal data.
In this paper, we study multimodal few-shot action recognition task, where the query and support samples are multimodal as shown in Figure 1. With multimodal data, we can alleviate the data scarcity issue through the com-plementarity of different modalities. However, exploring the multimodal complementarity in few-shot action recog-nition is nontrivial. On the one hand, although there are many widely used methods for fusing multimodal data, e.g., early fusion [47], late fusion [38, 64], it is still questionable whether existing methods are suitable to be directly applied in the few-shot scenario where only a few samples are avail-able for each action class. On the other hand, the contri-bution of a specific modality is not consistent for different query samples and it highly depends on the contextual infor-mation of both query and support samples in each few-shot task. For example, as shown in Figure 1, if the few-shot task is to identify query samples from the two action classes of
Snowboarding and Ballet dancing, the RGB data and optical flow are equally important and they can com-plement each other well to distinguish these two classes. In contrast, for the two action classes of Snowboarding and
Skateboarding, the optical flow cannot provide use-ful discriminative features to complement the vision infor-mation or even harm the few-shot recognition performance due to the motion resemblance between these two actions.
Therefore, we argue that it requires a task-dependent strat-egy for exploring the complementarity between different modalities in few-shot action recognition.
In order to reasonably take advantage of the comple-mentarity between different modalities, we propose an Ac-tive Multimodal Few-shot Action Recognition (AMFAR) framework inspired by active learning [6], which can ac-tively find the more reliable modality for each query sam-ple to improve the few-shot reasoning procedure. AMFAR adopts the episode-wise learning framework [53,63], where each episode has a few labeled support samples and the un-labeled query samples that need to be recognized. In each episode of the meta-training, we firstly adopt modality-specific backbone networks to extract the multimodal rep-resentations for query samples and the prototypes of differ-ent actions for support samples. We further compute the modality-specific posterior distributions based on query-to-prototype distances. Then, we adopt Active Sample Se-lection (ASS) to organize query samples with large differ-ences in the reliability of two modalities into two groups, i.e., RGB-dominant group that contains samples where the
RGB modality is more reliable for conducting action recog-nition in the current episode, and Flow-dominant group where the optical flow is more reliable. For each query sample, the reliability of a specific modality is estimated ac-cording to certainties of the modality-specific posterior dis-tribution. Next, we design an Active Mutual Distillation (AMD) mechanism to capture discriminative task-specific knowledge from the reliable modality to improve the rep-resentation learning of unreliable modality by bidirectional knowledge guiding streams between modalities. For each query in the RGB-dominant group, the RGB modality is regarded as teacher while the optical flow is regarded as student, and the query-to-prototype relation knowledge is transferred from the teacher to the student with a distilla-tion constraint. Simultaneously, for each query in the Flow-dominant group, optical flow is regarded as teacher while
RGB is regarded as student, and the knowledge distillation is conducted in the opposite direction. In the meta-test phase, we adopt Adaptive Multimodal Inference (AMI) to conduct the few-shot inference for each query sample by adaptively fusing the posterior distributions predicted from different modalities with a larger weight on the reli-able modality.
In summary, the main contributions of this paper are fourfold: 1) We exploit the natural complementarity be-tween different modalities to enhance the few-shot action recognition procedure by actively finding the more reli-able modality for each query sample. To our best knowl-edge, we are the first to adopt the idea of active learn-ing to explore the multimodal complementarity in few-shot learning. 2) We propose an active mutual distillation strat-egy to transfer task-dependent knowledge learned from the reliable modality to guide the representation learning for the unreliable modality, which can improve the discrimi-native ability of the unreliable modality with the help of the multimodal complementarity. 3) We propose an adaptive multimodal few-shot inference approach to fuse modality-specific results by paying more attention to the reliable modality. 4) We conduct extensive experiments on four challenging datasets and the results demonstrate that the proposed method outperforms existing unimodal and mul-timodal methods. 2.