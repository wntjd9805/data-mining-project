Abstract
Local feature extraction is a standard approach in com-puter vision for tackling important tasks such as image matching and retrieval. The core assumption of most meth-ods is that images undergo affine transformations, disregard-ing more complicated effects such as non-rigid deformations.
Furthermore, incipient works tailored for non-rigid corre-spondence still rely on keypoint detectors designed for rigid transformations, hindering performance due to the limita-tions of the detector. We propose DALF (Deformation-Aware
Local Features), a novel deformation-aware network for jointly detecting and describing keypoints, to handle the challenging problem of matching deformable surfaces. All network components work cooperatively through a feature fusion approach that enforces the descriptorsâ€™ distinctiveness and invariance. Experiments using real deforming objects showcase the superiority of our method, where it delivers 8% improvement in matching scores compared to the previous best results. Our approach also enhances the performance of two real-world applications: deformable object retrieval and non-rigid 3D surface registration. Code for training, in-ference, and applications are publicly available at verlab. dcc.ufmg.br/descriptors/dalf_cvpr23. 1.

Introduction
Finding pixel-wise correspondences between images de-picting the same surface is a long-standing problem in com-puter vision. Besides varying illumination, viewpoint, and distance to the object of interest, real-world scenes impose additional challenges. The vast majority of the correspon-dence algorithms in the literature assume that our world is rigid, but this assumption is far from the truth. It is notice-able that the community invests significant efforts into novel architectures and training strategies to improve image match-Figure 1. Image matching under deformations. We propose
DALF, a deformation-aware keypoint detector and descriptor for matching deformable surfaces. DALF (top) enables local feature matching across deformable scenes with improved matching scores (MS) compared to state-of-the-art, as illustrated with DISK [37].
Green lines show correct matches, and red markers, the mismatches. ing for rigid scenes [6, 19, 26, 34, 37, 42], but disregards the fact that many objects in the real world can deform in more complex ways than an affine transformation.
Many applications in industry, medicine, and agricul-ture require tracking, retrieval, and monitoring of arbitrary deformable objects and surfaces, where a general-purpose matching algorithm is needed to achieve accurate results.
Since the performance of standard affine local features sig-nificantly decreases for scenarios such as strong illumination changes and deformations, a few works considering a wider class of transformations have been proposed [24, 25, 30].
However, all the deformation-aware methods neglect the keypoint detection phase, limiting their applicability in chal-lenging deformations. Although the problems of keypoint detection and description can be treated separately, recent works that jointly perform detection and description of fea-tures [4, 26] indicate an entanglement of the two tasks since the keypoint detection can impact the performance of the de-scriptor. The descriptor for its turn can be used to determine reliable points optimized for specific goals. In this work,
we propose a new method for jointly learning keypoints and descriptors robust to deformations, viewpoint, and illumina-tion changes. We show that the detection phase is critical to obtain robust matching under deformations. Fig. 1 de-picts an image pair with challenging deformations, where our method can extract reliable keypoints and match them correctly, significantly increasing matching scores compared to the recent state-of-the-art approach DISK [37].
Contributions. (1) Our first contribution is a new end-to-end method called DALF (Deformation-Aware Local Fea-tures), which jointly learns to detect keypoints and extract descriptors with a mutual assistance strategy to handle sig-nificant non-rigid deformations. Our method boosts the state-of-the-art in this type of feature matching by 8% using only synthetic warps as supervision, showing strong gener-alization capabilities. We leverage a reinforcement learning algorithm for unified training, combined with spatial trans-formers that capture deformations by learning context priors affecting the image; (2) Second, we introduce a feature fu-sion approach, a major difference from previous methods that allows the model to tackle challenging deformations with complementary features (with distinctiveness and in-variance properties) obtained from both the backbone and the spatial transformer module. This approach is shown beneficial with substantial performance improvements com-pared to the non-fused features; (3) Finally, we demonstrate state-of-the-art results in non-rigid local feature applications for deformable object retrieval and non-rigid 3D surface reg-istration. We also will make the code and both applications publicly available to the community. 2.