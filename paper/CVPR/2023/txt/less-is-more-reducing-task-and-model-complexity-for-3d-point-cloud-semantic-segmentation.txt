Abstract
Whilst the availability of 3D LiDAR point cloud data has significantly grown in recent years, annotation remains ex-pensive and time-consuming, leading to a demand for semi-supervised semantic segmentation methods with application domains such as autonomous driving. Existing work very often employs relatively large segmentation backbone net-works to improve segmentation accuracy, at the expense of computational costs. In addition, many use uniform sam-pling to reduce ground truth data requirements for learning needed, often resulting in sub-optimal performance. To ad-dress these issues, we propose a new pipeline that employs a smaller architecture, requiring fewer ground-truth annota-tions to achieve superior segmentation accuracy compared to contemporary approaches. This is facilitated via a novel
Sparse Depthwise Separable Convolution module that signif-icantly reduces the network parameter count while retaining overall task performance. To effectively sub-sample our training data, we propose a new Spatio-Temporal Redun-dant Frame Downsampling (ST-RFD) method that leverages knowledge of sensor motion within the environment to ex-tract a more diverse subset of training data frame samples.
To leverage the use of limited annotated data samples, we further propose a soft pseudo-label method informed by Li-DAR reflectivity. Our method outperforms contemporary semi-supervised work in terms of mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) and ScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3× reduction in model parameters and 641× fewer multiply-add oper-ations whilst also demonstrating significant performance improvement on limited training data (i.e., Less is More). 1.

Introduction 3D semantic segmentation of LiDAR point clouds has played a key role in scene understanding, facilitating applications such as autonomous driving [6, 23, 26, 28, 46, 60, 63] and robotics [3, 38, 51, 52]. However, many contemporary meth-ods require relatively large backbone architectures with mil-lions of trainable parameters requiring many hundred giga-bytes of annotated data for training at a significant compu-tational cost. Considering the time-consuming and costly nature of 3D LiDAR annotation, such methods have become less feasible for practical deployment.
Figure 1. mIoU performance (%) against parameters and multiply-add operations on SemanticKITTI (fully annotated) and Scrib-bleKITTI (weakly annotated) under the 5% sampling protocol.
Existing supervised 3D semantic segmentation meth-ods [13, 31, 38, 44, 51, 52, 56, 58, 63] primarily focus on designing network architectures for densely annotated data.
To reduce the need for large-scale data annotation, and in-spired by similar work in 2D [11, 47, 49], recent 3D work proposes efficient ways to learn from weak supervision [46].
However, such methods still suffer from high training costs and inferior on-task performance. To reduce computational costs, a 2D projection-based point cloud representation is often considered [3, 14, 31, 38, 51, 52, 56, 61], but again at the expense of significantly reduced on-task performance.
As such, we observe a gap in the research literature for the design of semi or weakly supervised methodologies that employ a smaller-scale architectural backbone, hence facili-tating improved training efficiency whilst also reducing their associated data annotation requirements.
In this paper, we propose a semi-supervised methodology for 3D LiDAR point cloud semantic segmentation. Facili-tated by three novel design aspects, our Less is More (LiM) based methodologies require less training data and less train-ing computation whilst offering (more) improved accuracy over contemporary state-of-the-art approaches (see Fig. 1).
Firstly, from an architectural perspective, we propose a novel Sparse Depthwise Separable Convolution (SDSC) module, which substitutes traditional sparse 3D convolu-tion into existing 3D semantic segmentation architectures, resulting in a significant reduction in trainable parameters and numerical computation whilst maintaining on-task per-formance (see Fig. 1). Depthwise Separable Convolution has shown to be very effective within image classification
tasks [12]. Here, we tailor a sparse variant of 3D Depthwise
Separable Convolution for 3D sparse data by first applying a single submanifold sparse convolutional filter [20, 21] to each input channel with a subsequent pointwise convolution to create a linear combination of the sparse depthwise convo-lution outputs. This work is the first to attempt to introduce depthwise convolution into the 3D point cloud segmentation field as a conduit to reduce model size. Our SDSC module facilitates a 50% reduction in trainable network parameters without any loss in segmentation performance.
Secondly, from a training data perspective, we propose a novel Spatio-Temporal Redundant Frame Downsam-pling (ST-RFD) strategy that more effectively sub-samples a set of diverse frames from a continuously captured LiDAR sequence in order to maximize diversity within a minimal training set size. We observe that continuously captured
LiDAR sequences often contain significant temporal redun-dancy, similar to that found in video [2], whereby temporally adjacent frames provide poor data variation. On this basis, we propose to compute the temporal correlation between ad-jacent frame pairs, and use this to select the most informative sub-set of LiDAR frames from a given sequence. Unlike pas-sive sampling (e.g., uniform or random sampling), our active sampling approach samples frames from each sequence such that redundancy is minimized and hence training set diver-sity is maximal. When compared to commonplace passive random sampling approaches [29, 32, 46], ST-RFD explicitly focuses on extracting a diverse set of training frames that will hence maximize model generalization.
Finally, in order to employ semi-supervised learning, we propose a soft pseudo-label method informed by the LiDAR reflectivity response, thus maximizing the use of any an-notated data samples. Whilst directly using unreliable soft pseudo-labels generally results in performance deteriora-tion [5], the voxels corresponding to the unreliable predic-tions can instead be effectively leveraged as negative samples of unlikely categories. Therefore, we use cross-entropy to separate all voxels into two groups, i.e., a reliable and an un-reliable group with low and high-entropy voxels respectively.
We utilize predictions from the reliable group to derive pos-itive pseudo-labels, while the remaining voxels from the unreliable group are pushed into a FIFO category-wise mem-ory bank of negative samples [4]. To further assist semantic segmentation of varying materials in the situation where we have weak/unreliable/no labels, we append the reflectivity response features onto the point cloud features, which again improve segmentation results.
We evaluate our method on the SemanticKITTI [7] and
ScribbleKITTI [46] validation set. Our method outperforms contemporary state-of-the-art semi- [29,32] and weakly- [46] supervised methods and offers more in terms of performance on limited training data, whilst using less trainable parame-ters and less numerical operations (Less is More).
Overall, our contributions can be summarized as follows:
• A novel methodology for semi-supervised 3D LiDAR semantic segmentation that uses significantly less pa-rameters and offers (more) superior accuracy.1
• A novel Sparse Depthwise Separable Convolution (SDSC) module, to reduce trainable network param-eters, and to both reduce the likelihood of over-fitting and facilitate a deeper network architecture.
• A novel Spatio-Temporal Redundant Frame Downsam-pling (ST-RFD) strategy, to extract a maximally diverse data subset for training by removing temporal redun-dancy and hence future annotation requirements.
• A novel soft pseudo-labeling method informed by Li-DAR reflectivity as a proxy to in-scene object material properties, facilitating effective use of limited data an-notation. 2.