Abstract 1.

Introduction
Existing learning-based methods for point cloud render-ing adopt various 3D representations and feature query-ing mechanisms to alleviate the sparsity problem of point clouds. However, artifacts still appear in rendered images, due to the challenges in extracting continuous and discrim-inative 3D features from point clouds.
In this paper, we present a dense while lightweight 3D representation, named
TriVol, that can be combined with NeRF to render photo-realistic images from point clouds. Our TriVol consists of triple slim volumes, each of which is encoded from the point cloud. TriVol has two advantages. First, it fuses respective fields at different scales and thus extracts local and non-local features for discriminative representation. Second, since the volume size is greatly reduced, our 3D decoder can be efficiently inferred, allowing us to increase the res-olution of the 3D space to render more point details. Ex-tensive experiments on different benchmarks with varying kinds of scenes/objects demonstrate our framework’s effec-tiveness compared with current approaches. Moreover, our framework has excellent generalization ability to render a category of scenes/objects without fine-tuning. The source code is available at https://github.com/dvlab-research/TriVol.git.
*Equal Contribution.
Photo-realistic point cloud rendering (without hole arti-facts and with clear details) approaches can be employed for a variety of real-world applications, e.g., the visualization of automatic drive [6, 8, 23, 29], digital humans [1, 19, 27], and simulated navigation [5,11,43]. Traditional point cloud renderers [36] adopt graphics-based methods, which do not require any learning-based models. They project existing points as image pixels by rasterization and composition.
However, due to the complex surface materials in real-world scenes and the limited precision of the 3D scanners, there are a large number of missing points in the input point cloud, leading to vacant and blurred image areas inevitably as illustrated in Fig. 1.
In recent years, learning-based approaches [1, 10, 35, 45, 49] have been proposed to alleviate the rendering problem in graphics-based methods. They use a variety of query-ing strategies in the point cloud to obtain continuous 3D features for rendering, e.g., the ball querying employed in the PointNet++ [34] and the KNN-based querying in Point-NeRF [45]. However, if the queried position is far away from its nearest points, the feature extraction process will usually fail and have generalization concerns. To guarantee accurate rendering, two groups of frameworks are further proposed. The first group [1, 35, 38] projects the features of all points into the 2D plane and then trains 2D neural net-works, like UNet [37], to restore and sharpen the images.
However, since such a 2D operation is individual for dif-ferent views, the rendering outcomes among nearby views are inconsistent [17, 40], i.e., the appearance of the same object might be distinct under different views. To overcome the artifact, physical-based volume rendering [24] in Neural
Radiance Fields (NeRF) [26] without 2D neural rendering is an elegant solution [40]. The second group [10, 47] applies a 3D Convolutional Network (ConvNet) to extract a dense 3D feature volume, then acquires the 3D feature of arbitrary point by trilinear interpolation for subsequent volume ren-dering. Nevertheless, conducting such a dense 3D network is too heavy for high-resolution 3D representation, limiting their practical application. In summary, to the best of our knowledge, there is currently no lightweight point cloud renderer whose results are simultaneously view-consistent and photo-realistic.
In this paper, we propose a novel 3D representation, named TriVol, to solve the above issues. Our TriVol is composed of three slender volumes which can be efficiently encoded from the point cloud. Compared with dense grid voxels, the computation on TriVol is efficient and the re-spective fields are enlarged, allowing the 3D representation with high resolution and multi-scale features. Therefore,
TriVol can provide discriminative and continuous features for all points in the 3D space. By combining TriVol with
NeRF [26], the point cloud rendering results show a sig-nificant quantitative and qualitative improvement compared with the state-of-the-art (SOTA) methods.
In our framework, we develop an efficient encoder to transform the point cloud into the Initial TriVol and then adopt a decoder to extract the Feature TriVol. Although the encoder can be implemented with the conventional point-based backbones [33, 34], we design a simple but effective grouping strategy that does not need extra neural models.
The principle is first voxelizing the point cloud into grid voxels and then re-organizing the voxels on each of three axes individually, which is empirically proven to be a better method. As for the decoder, it can extract the feature rep-resentation for arbitrary 3D points. Hence, we utilize three independent 3D ConvNet to transfer each volume into dense feature representations. Due to the slender shape of the vol-umes, the computation of the 3D ConvNet is reduced. Also, the 3D ConvNet can capture more non-local information in the grouped axis via a larger receptive field, and extract lo-cal features in the other two directions.
With the acquired dense Feature TriVol, the feature of any 3D points can be queried via trilinear interpolation. By combining the queried features with the standard NeRF [26] pipeline, the photo-realistic results are achieved. Exten-sive experiments are conducted on three representative pub-lic datasets (including both datasets of scene [9] and object
[4, 12]) level, proving our framework’s superiority over re-cent methods. In addition, benefiting from the discrimina-tive and continuous feature volume in TriVol, our frame-work has a remarkable generalization ability to render un-seen scenes or objects of the same category, when there is no further fine-tuning process.
In conclusion, our contributions are three-fold.
• We propose a dense yet efficient 3D representation called TriVol for point cloud rendering. It is formed by three slim feature volumes and can be efficiently transformed from the point cloud.
• We propose an effective encoder-decoder framework for TriVol representation to achieve photo-realistic and view-consistent rendering from the point cloud.
• Extensive experiments are conducted on various benchmarks, showing the advantages of our frame-work over current works in terms of rendering quality. 2.