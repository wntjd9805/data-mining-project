Abstract
Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker’s functionality in the real world, espe-cially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture in-formation like conventional cameras. This unique comple-mentarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. In this paper, we propose an end-to-end network consisting of multi-modality alignment and fu-sion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events.
While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Exten-sive experiments show that the proposed approach outper-forms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz. 1.

Introduction
Visual object tracking is a fundamental task in computer vision, and deep learning-based methods [7,9,10,15,35,56] have dominated this field. Limited by the conventional sen-sor, most existing approaches are designed and evaluated on benchmarks [13,24,38,53] with a low frame rate of approx-imately 30 frames per second (FPS). However, the value of a higher frame rate tracking in the real world has been proved [16, 21–23]. For example, the shuttlecock can reach speeds of up to 493km/h, and analyzing its position is es-sential for athletes to learn how to improve their skills [46].
Utilizing professional high-speed cameras is one strategy
⋆ Xin Yang (xinyang@dlut.edu.cn) is the corresponding author.
Figure 1. A comparison of our AFNet with SOTA trackers. All competing trackers locate the target at time t + ∆t with conven-tional frames at time t and aggregated events at time t + ∆t as inputs. Our method achieves high frame rate tracking up to 240Hz on the FE240hz dataset. The two examples also show the comple-mentary benefits of both modalities. (a) The event modality does not suffer from HDR, but the frame does; (b) The frame modality provides rich texture information, while the events are sparse. for high frame rate tracking, but these cameras are inac-cessible to casual users. Consumer devices with cameras, such as smartphones, have made attempts to integrate sen-sors with similar functionalities into their systems. How-ever, these sensors still suffer from large memory require-ments and high power consumption [49].
As bio-inspired sensors, event-based cameras measure light intensity changes and output asynchronous events to represent visual information. Compared with conventional frame-based sensors, event-based cameras offer a high mea-surement rate (up to 1MHz), high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel band-width (on the order of kHz) [14]. These unique proper-ties offer great potential for higher frame rate tracking in challenging conditions. Nevertheless, event-based cameras cannot measure fine-grained texture information like con-ventional cameras, thus inhibiting tracking performance.
Therefore, in this paper, we exploit to integrate the valuable information from event-based modality with that of frame-based modality for high frame rate single object tracking under various challenging conditions.
To attain our objective, two challenges require to be ad-dressed: (i) The measurement rate of event-based cameras is much higher than that of conventional cameras. Hence
for high frame rate tracking, low-frequency frames must be aligned with high-frequency events to disambiguate target locations. Although recent works [34, 45, 48, 50] have pro-posed various alignment strategies across multiple frames for video-related tasks, they are specifically designed for conventional frames of the same modality at different mo-ments. Thus, applying these approaches directly to our cross-modality alignment does not offer an effective solu-tion. (ii) Effectively fusing complementary information be-tween modalities and preventing interference from noise is another challenge. Recently, Zhang et al. [61] proposed a cross-domain attention scheme to fuse visual cues from frame and event modalities for improving the single object tracking performance under different degraded conditions.
However, the tracking frequency is bounded by the conven-tional frame rate since they ignore the rich temporal infor-mation recorded in the event modality.
To tackle the above challenges, we propose a novel end-to-end framework to effectively combine complementary information from two modalities at different measurement rates for high frame rate tracking, dubbed AFNet, which consists of two key components for alignment and fusion, respectively. Specifically, (i) we first propose an event-guided cross-modality alignment (ECA) module to simulta-neously accomplish cross-style alignment and cross-frame-rate alignment. Cross-style alignment is enforced by match-ing feature statistics between conventional frame modality and events augmented by a well-designed attention scheme;
Cross-frame-rate alignment is based on deformable convo-lution [8] to facilitate alignment without explicit motion es-timation or image warping operation by implicitly focusing on motion cues. (ii) A cross-correlation fusion (CF) mod-ule is further presented to combine complementary infor-mation by learning a dynamic filter from one modality that contributes to the feature expression of another modality, thereby emphasizing valuable information and suppress-ing interference. Extensive experiments on different event-based tracking datasets validate the effectiveness of the pro-posed approach (see Figure 1 as an example).
In summary, we make the following contributions:
• Our AFNet is, to our knowledge, the first to combine the rich textural clues of frames with the high temporal reso-lution offered by events for high frame rate object tracking.
• We design a novel event-guided alignment framework that performs cross-modality and cross-frame-rate align-ment simultaneously, as well as a cross-correlation fusion architecture that complements the two modalities.
• Through extensive experiments, we show that the pro-posed approach outperforms state-of-the-art trackers in var-ious challenging conditions. 2.