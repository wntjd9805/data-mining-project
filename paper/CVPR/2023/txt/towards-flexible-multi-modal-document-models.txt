Abstract
Creative workflows for generating graphical documents involve complex inter-related tasks, such as aligning ele-ments, choosing appropriate fonts, or employing aestheti-cally harmonious colors. In this work, we attempt at build-ing a holistic model that can jointly solve many different design tasks. Our model, which we denote by FlexDM, treats vector graphic documents as a set of multi-modal elements, and learns to predict masked fields such as ele-ment type, position, styling attributes, image, or text, using a unified architecture. Through the use of explicit multi-task learning and in-domain pre-training, our model can better capture the multi-modal relationships among the different document fields. Experimental results corroborate that our single FlexDM is able to successfully solve a multitude of different design tasks, while achieving performance that is competitive with task-specific and costly baselines. 1 1.

Introduction
Vector graphic documents are composed of diverse multi-modal elements such as text or images and serve as the dominant medium for visual communication today. The graphical documents are created through many different design tasks, e.g., filling in a background image, chang-ing font and color, adding a decoration, or aligning texts.
While skilled designers perform tasks based on their de-sign knowledge and expertise, novice designers often strug-gle to make decisions to create an effective visual presen-tation. To assist such novice designers, interactive frame-works equipped based on models that learn design knowl-edge from completed designs have been proposed [12, 38].
Our present work proposes models that can be used in such systems, with a particular focus on developing holistic mod-els that can flexibly switch between design tasks.
Design tasks are characterized by 1) the variety of 1Please find the code and models at: https://cyberagentailab.github.io/flex-dm. possible actions and 2) the complex interaction between multi-modal elements. As discussed above, a designer can make almost any edit to the appearance of a vector graphic document, ranging from basic layout to nuanced font styling. While there have been several studies in solv-ing specific tasks of a single modality, such as layout gen-eration [3, 13, 23, 26, 30], font recommendation [56], or col-orization [22,40,54], in realistic design applications, we be-lieve it is essential to build a flexible model that can consider multiple design tasks in a principled manner to make auto-mated decisions on creative workflow.
In this work, we refer to a certain attribute of an element as a field and formulate the various design tasks as a uni-fied masked field prediction, which is inspired by the recent masked autoencoders [9,15] and multi-task models [19,36].
The key idea is to utilize masking patterns to switch among different design tasks within a single model; e.g., element filling can be formulated as predicting all the fields of the newly added element. Our flexible document model, de-noted by FlexDM, consists of an encoder-decoder architec-ture with a multi-modal head dedicated to handling different fields within a visual element. After pre-training with ran-dom masking strategy, we train FlexDM by explicit multi-task learning where we randomly sample tasks in the form of masking patterns corresponding to the target design task.
We illustrate in Figs. 1 and 2 an overview of FlexDM, with emphasis on the correspondence between design tasks and masking patterns.
Through our carefully designed experiments, we show that our proposed FlexDM performs favorably against base-lines in five design tasks using the Rico [7] and Crello [52] datasets. We also study how different modeling approaches affect the final task performance in the ablation study. Fi-nally, we apply our framework to several previously stud-ied design tasks with minimal modifications and show that the performance matches or even surpasses the current task-specific approaches.
Our contributions can be summarized in the following.
• We formulate multiple design tasks for vector graphic documents by masked multi-modal field prediction in a
Figure 1. Examples of the design tasks that can be solved by our proposed FlexDM model, which is designed to process a vector graphic document consisting of an arbitrary number of elements (e.g., text). Each element is composed of multi-modal fields indicating its attribute properties (e.g., text content, position, font color, etc.). set of visual elements. essential tasks for design creation in a single model.
• We build a flexible model to solve various design tasks jointly in a single Transformer-based model via multi-task learning.
• We empirically demonstrate that our model constitutes a strong baseline for various design tasks. 2.