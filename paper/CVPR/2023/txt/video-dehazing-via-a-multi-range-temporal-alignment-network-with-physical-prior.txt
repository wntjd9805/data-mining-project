Abstract
Video dehazing aims to recover haze-free frames with high visibility and contrast. This paper presents a novel framework to effectively explore the physical haze priors and aggregate temporal information. Specifically, we de-sign a memory-based physical prior guidance module to encode the prior-related features into long-range memory.
Besides, we formulate a multi-range scene radiance recov-ery module to capture space-time dependencies in multiple space-time ranges, which helps to effectively aggregate tem-poral information from adjacent frames. Moreover, we con-struct the first large-scale outdoor video dehazing bench-mark dataset, which contains videos in various real-world scenarios. Experimental results on both synthetic and real conditions show the superiority of our proposed method. 1.

Introduction
Haze largely degrades the visibility and contrast of the outdoor scenes, which adversely affects the performance of downstream vision tasks, such as the detection and segmen-tation in autonomous driving and surveillance. According to the atmospheric scattering model [18, 38], the formation of a hazy image is described as:
I(x) = J(x)t(x) + A(1 − t(x)) , (1) where I, J, A, t denote the observed hazy image, scene radi-ance, atmospheric light, and transmission, respectively, and x is the pixel index. The transmission t = e−βd(x) describes the scene radiance attenuation caused by the light scattering, where β is the scattering coefficient of the atmosphere, and d denotes the scene depth.
Video dehazing benefits from temporal clues, such as highly correlated haze thickness and lighting conditions, as
⋆ : This work was done during Jiaqi Xu’s internship at Shanghai Artificial
Intelligence Laboratory. (cid:0) : Corresponding authors (huxiaowei@pjlab.org.cn; leizhu@ust.hk). (a) Hazy frame (b) VDH [46] (c) CG-IDN [59] (d) Our method
Figure 1. Visual comparison on a real-world hazy video. Our method trained on our outdoor dehazing dataset clearly removes haze without color distortion. well as the moving foreground objects and backgrounds.
Early deep learning-based video dehazing methods lever-age temporal information by simply concatenating input frames or feature maps [27, 46]. Recently, GC-IDN [59] proposes to use cost volume and confidence to align and ag-gregate temporal information. However, existing video de-hazing methods suffer from several limitations. First, these approaches either obtain haze-free frames from the phys-ical model-based component estimation [27, 46] or ignore the explicit physical prior embedded in the haze imaging model [59]. The former suffers from inaccurate interme-diate prediction, thus leading to error accumulation in the final results, while the latter overlooks the physical prior in-formation, which plays an important role in haze estimation and scene recovery. Second, these methods aggregate tem-poral information by using input/feature stacking or frame-to-frame alignment in a local sliding window, which is hard to obtain global and long-range temporal information.
In this work, we present a novel video dehazing frame-work via a Multi-range temporal Alignment network with
Physical prior (MAP-Net) to address the aforementioned is-sues. First, we design a memory-based physical prior guid-ance module, which aims to inject the physical prior to help the scene radiance recovery. Specifically, we perform fea-ture disentanglement according to the physical model with two decoders, where one estimates the transmission and atmospheric light, and the other recovers scene radiance.
The feature extracted from the first decoder is leveraged as the physical haze prior, which is integrated into the second decoder for scene radiance recovery. To infer the global physical prior in a long-range video, we design a physical prior token memory that effectively encodes prior-related features into compact tokens for efficient memory reading.
Second, we introduce a multi-range scene radiance re-covery module to capture space-time dependencies in mul-tiple space-time ranges. This module first splits the adjacent frames into multiple ranges, then aligns and aggregates the corresponding recurrent range features, and finally recov-ers the scene radiance. Unlike CG-IDN [59], which aligns the adjacent features frame-by-frame, we align the features of adjacent frames into multiple sets with different ranges, which helps to explore the temporal haze clues in various time intervals. We further design a space-time deformable attention to warp the features of multiple ranges to the target frame, followed by a guided multi-range complementary in-formation aggregation. Also, we use an unsupervised flow loss to encourage the network to focus on the aligned areas and train the whole network in an end-to-end manner.
In addition, the existing learning-based video dehaz-ing methods are mainly trained and evaluated on indoor datasets [27, 46, 59], which suffer from performance degra-dation in real-world outdoor scenarios. Thus, we construct an outdoor video dehazing benchmark dataset, HazeWorld, which has three main properties. First, it is a large-scale synthetic dataset with 3,588 training videos and 1,496 test-ing videos. Second, we collect videos from diverse out-door scenarios, e.g., autonomous driving and life scenes.
Third, the dataset has various downstream tasks for eval-uation, such as segmentation and detection. Various ex-periments on both synthetic and real datasets demonstrate the effectiveness of our approach, which clearly outper-forms the existing image and video dehazing methods; see
Fig. 1. The code and dataset are publicly available at https://github.com/jiaqixuac/MAP-Net.
Our main contributions are summarized as follows:
• We present a novel framework, MAP-Net, for video dehazing. A memory-based physical prior guidance module is designed to enhance the scene radiance re-covery, which encodes haze-prior-related features into long-range memory.
• We introduce a recurrent multi-range scene radiance recovery module with the space-time deformable at-tention and the guided multi-range aggregation, which effectively captures long-range temporal haze and scene clues from the adjacent frames.
• We construct a large-scale outdoor video dehazing dataset with diverse real-world scenarios and labels for downstream task evaluation.
• Extensive experiments on both synthetic and real con-ditions demonstrate our superior performance against the recent state-of-the-art methods. 2.