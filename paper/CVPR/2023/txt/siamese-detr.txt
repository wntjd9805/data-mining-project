Abstract
Recent self-supervised methods are mainly designed for representation learning with the base model, e.g., ResNets
They cannot be easily transferred to DETR, or ViTs. with task-specific Transformer modules.
In this work, we present Siamese DETR, a Siamese self-supervised pretraining approach for the Transformer architecture in
DETR. We consider learning view-invariant and detection-oriented representations simultaneously through two com-localization and discrimination, i.e., plementary tasks, in a novel multi-view learning framework.
Two self-supervised pretext tasks are designed: (i) Multi-View Re-gion Detection aims at learning to localize regions-of-interest between augmented views of the input, and (ii)
Multi-View Semantic Discrimination attempts to improve object-level discrimination for each region. The proposed
Siamese DETR achieves state-of-the-art transfer perfor-mance on COCO and PASCAL VOC detection using dif-ferent DETR variants in all setups. Code is available at https://github.com/Zx55/SiameseDETR. 1.

Introduction viewing (DETR)
Object detection with Transformers
[3] combines convolutional neural networks (CNNs) and object encoder-decoders,
Transformer-based detection as an end-to-end set prediction problem. Despite its impressive performance, DETR and its variants still rely on large-scale, high-quality training data. It generally to collect such massive requires huge cost and effort well-annotated datasets, which can be prohibited in some privacy-sensitive applications such as medical imaging and video surveillance.
Recent progress in multi-view self-supervised represen-*Equal contribution.
†Corresponding author.
Figure 1. Comparison between single-view and multi-view detec-tion pretraining for DETR. (a) The single-view framework, e.g.,
UP-DETR [9] and DETReg [1], perform self-supervised represen-tation learning using unsupervised objectives generated on the sin-gle view, e.g., random patches (UP-DETR) or pseudo labels (DE-TReg), leading to a small information gain during pretraining. (b)
The proposed multi-view Siamese DETR for DETR pretraining.
Here, ˆb and ˆp denote box and semantic predictions. q, k and v denote query, key and value in DETR, respectively. tation learning [4, 6–8, 14, 15, 21] can potentially allevi-ate the appetite for labeled data in training DETR for ob-ject detection. However, these self-supervised learning ap-proaches mainly focus on learning generalizable representa-tions with base models, such as ResNets [17] and ViTs [11].
It is unclear how these approaches can be effectively ex-tended to DETR with task-specific Transformers modules that are tailored for end-to-end object detection.
Designing self-supervised pretext tasks for pretraining the Transformers in DETR is a challenging and practical problem, demanding representations that could benefit ob-ject detection, beyond just learning generic representation.
Several attempts have been made to address this issue. For example, UP-DETR [9] introduces an unsupervised pre-text task based on random query patch detection, predicting bounding boxes of randomly-cropped query patches in the
given image. Recent DETReg [1] employs a pre-trained
SwAV [5] and offline Selective Search proposals [28] to provide pseudo labels for DETR pertaining.
In general, both UP-DETR and DETReg follow a single-view pretrain-ing paradigm (see Figure 1 (a)), without exploring the abil-ity of learning view-invariant representations demonstrated in existing multi-view self-supervised approaches.
In this work, we are interested in investigating the effectiveness of multi-view self-supervised learning for
DETR pre-training. Different from conventional multi-view framework [5, 6, 15], we combine the Siamese net-work with the cross-attention mechanism in DETR, present-ing a Siamese self-supervised pretraining approach, named
Siamese DETR, with two proposed self-supervised pre-text tasks dedicated to view-invariant detection pretrain-ing. Specifically, given each unlabeled image, we follow
[1, 31] to obtain the offline object proposals and generate two augmented views guided by Intersection over Union (IoU) thresholds. As illustrated in Figure 1 (b), by directly locating the query regions between augmented views and maximizing the discriminative information at both global and regional levels, Siamese DETR can learn view-invariant representations with localization and discrimination that are aligned with downstream object detection tasks during pre-training. Our contributions can be summarized as below:
• We propose a novel Siamese self-supervised approach for the Transformers in DETR, which jointly learns view-invariant representations with discrimination and localization. In particular, we contribute two new de-signs of self-supervised pretext tasks specialized for multi-view detection pretraining.
• Without bells and whistles, Siamese DETR outper-forms UP-DETR [9] and DETReg [1] with multiple
DETR variants, such as Conditional [26] and De-formable [38] DETR, on the COCO and PASCAL
VOC benchmarks, demonstrating the effectiveness and versatility of our designs. 2.