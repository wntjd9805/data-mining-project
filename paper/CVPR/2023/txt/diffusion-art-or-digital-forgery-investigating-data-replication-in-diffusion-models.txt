Abstract 1.

Introduction
Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffu-sion models create unique works of art, or are they repli-cating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and de-tect when content has been replicated. Applying our frame-works to diffusion models trained on multiple datasets in-cluding Oxford ﬂowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, bla-tantly copy from their training data. Project page: https:
//somepago.github.io/diffrep.html
The rapid rise of diffusion models has led to new gen-erative tools with the potential to be used for commer-cial art and graphic design. The power of the diffusion paradigm stems in large part from its reliance on simple de-noising networks that maintain their stability when trained on huge web-scale datasets containing billions of image-caption pairs. These mega-datasets have the power to forge
E [54] and Stable Diffusion commercial models like DALL
[56], but also bring with them a number of legal and ethical risks [7]. Because these datasets are too large for careful hu-man curation, the origins and intellectual property rights of the data sources are largely unknown. This fact, combined with the ability of large models to memorize their training data [9, 10, 22], raises questions about the originality of dif-fusion outputs. There is a risk that diffusion models might, without notice, reproduce data from the training set directly, or present a collage of multiple training images.
·
We informally refer to the reproduction of training    
images, either in part or in whole, as content replication.
In principle, replicating partial or complete information from the training data has implications for the ethical and legal use of diffusion models in terms of attributions to artists and photographers. Replicants are either a beneﬁt or a hazard; there may be situations where content replication is acceptable, desirable, or fair use, and others where it is
“stealing.” While these ethical boundaries are unclear at this time, we focus on the scientiﬁc question of whether replication actually happens with modern state-of-the-art diffusion models, and to what degree.
Our contributions are as follows. We begin with a study of how to detect content replication, and we consider a range of image similarity metrics developed in the self-supervised learning and image retrieval communities. We benchmark the performance of different image feature ex-tractors using real and purpose-built synthetic datasets and show that state-of-the-art instance retrieval models work well for this task. Armed with new and existing tools, we search for data replication behavior in a range of diffusion models with different dataset properties. We show that for small and medium dataset sizes, replication happens fre-quently, while for a model trained on the large and diverse
ImageNet dataset, replication seems undetectable.
This latter ﬁnding may lead one to believe that replica-tion is not a problem for large-scale models. However, the even larger Stable Diffusion model exhibits clear replica-tion in various forms (Fig 1). Furthermore, we believe that the rate of content replication we identify in Stable Diffu-sion likely underestimates the true rate because the model is trained on a 2B image split of LAION, but we only search for matches in the smaller 12M “Aesthetics v2 6+” subset.
The level of image similarity required for something to count as “replication” is subjective and may depend on both the amount of diversity within the image’s class as well as the observer. Some replication behaviors we uncover are unambiguous, while in other instances they fall into a gray area. Rather than choosing an arbitrary deﬁnition, we fo-cus on presenting quantitative and qualitative results to the reader, leaving each person to draw their own conclusions based on their role and stake in the process of generative AI. 2.