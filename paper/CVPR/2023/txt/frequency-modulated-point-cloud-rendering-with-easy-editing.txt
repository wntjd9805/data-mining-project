Abstract
We develop an effective point cloud rendering pipeline for novel view synthesis, which enables high fidelity local detail reconstruction, real-time rendering and user-friendly editing.
In the heart of our pipeline is an adaptive fre-quency modulation module called Adaptive Frequency Net (AFNet), which utilizes a hypernetwork to learn the local texture frequency encoding that is consecutively injected into adaptive frequency activation layers to modulate the implicit radiance signal. This mechanism improves the fre-quency expressive ability of the network with richer fre-quency basis support, only at a small computational bud-get. To further boost performance, a preprocessing mod-ule is also proposed for point cloud geometry optimization via point opacity estimation. In contrast to implicit render-ing, our pipeline supports high-fidelity interactive editing based on point cloud manipulation. Extensive experimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and
Temples datasets demonstrate the superior performances achieved by our method in terms of PSNR, SSIM and LPIPS, in comparison to the state-of-the-art. Code is released at https://github.com/yizhangphd/FreqPCR. 1.

Introduction
Photo-realistic rendering and editing of 3D representa-tions is a key problem in 3D computer vision and graph-ics with numerous applications, such as computer games,
*Equal contribution.
†Corresponding author: Bingbing Ni.
VR/AR, and video creation.
In particular, recently intro-duced neural radiance field (NeRF) [21] has inspired some follow-up works aiming to editable rendering [15, 17, 20, 47, 49, 55]. Due to the deeply coupled black-box net-work, NeRF-based object-level editing usually requires a pre-trained segmentation model to separate the objects to be edited [17, 55]. Although some recent voxel-based variants of NeRF [47,58] achieve multi-scene composition, they still lack the ability to extract target objects from voxels.
In contrast to implicit rendering, point cloud render-ing [1, 6, 13, 18, 33, 36, 57] is a promising editable rendering model. On the one hand, explicit 3D representations are bet-ter for interactive editing. On the other hand, the geometric priors provided by point clouds can help us avoid massive sampling in volume rendering methods, which can meet the requirements of some real-time applications. As a class of representative point cloud rendering methods, NPBG and
NPBG++ [1,33] achieve real-time rendering by using point-wise features for encoding appearance information and an
U-Net [35] for decoding, respectively. However, the param-eter quantity increases with the size of point cloud, which may limit their application due to the excessive computa-tional and memory complexity. Huang et al. [13] combine point clouds with implicit rendering, where explicit point clouds are used to estimate the geometry, and implicit radi-ance mapping is used to predict view-dependent appearance of surfaces. However, quantitative evaluation of its render-ing results is significantly lower than that of implicit render-ing methods such as NeRF [21], mainly due to the following reasons: 1) the color of each viewing ray only depends on
a single surface point, thus without multiple sample color aggregation for error attenuation, surface based rendering techniques require radiance mapping to have a more precise and expressive frequency encoding ability; and 2) defects of the point cloud geometry reconstructed by MVSNet [56] cause wrong surface estimation. To this end, we introduce
Adaptive Frequency Net (AFNet) to improve frequency ex-pression ability of the radiance mapping and a preprocess-ing module for point cloud geometry optimization.
Radiance mapping, also known as radiance field, is a type of Implicit Neural Representation (INR). There have been some studies [2, 7, 32, 41, 45, 46, 60] on the expressive power and inductive bias of INRs. The standard Multi-layer
Perceptrons (MLPs) with ReLU activation function are well known for the strong spectral bias towards reconstructing low frequency signals [32]. Some recent works [7, 41, 46] introduce strategies to enhance the high-frequency repre-sentation of MLPs from a global perspective. However, from a local perspective, the frequencies of a 3D scene are region-dependent and most real objects are composed by both weak textured regions and strong textured ones. Moti-vated by this, we design a novel adaptive frequency modula-tion mechanism based on HyperNetwork architecture [11], which learns the local texture frequency and injects it into adaptive frequency activation layers to modulate the im-plicit radiance signal. The proposed mechanism can pre-dict suitable frequency without frequency supervision and modulate the radiance signal with adaptive frequency ba-sis support to express more complex textures at negligible computational overhead.
Previous surface point-based works [1, 13, 33] could not optimize the point cloud geometry because they keep only the closest point as a surface estimation for each ray. But if we sample multiple points per ray during rendering, it will greatly reduce the rendering speed. Therefore, we use the volume rendering method as a preprocessing module to op-timize the point cloud geometry. Specifically, we keep more points in the pixel buffer and learn the opacity of each point based on volume rendering. We find in our experiments that for some poorly reconstructed scenes, point cloud geome-try optimization can improve the rendering PSNR by 2-4dB and avoid rendering artifacts.
For rigid object editing, we follow the deformation field construction [28–31, 48] to render the edited point cloud.
Point cloud can be seen as a bridge between user editing and deformation field to achieve interactive editing and ren-dering. Users only need to edit the point cloud, and the deformation field between the original 3D space and the de-formed space is easy to obtain by querying the correspond-ing transformations performed on point cloud. Moreover, to avoid cross-scene training in multi-scene composition ap-plication, we develop a masking strategy based on depth buffer to combine multiple scenes in pixel level.
We evaluate our method on NeRF-Synthetic [21], Scan-Net [5], DTU [14] and Tanks and Temples [16] datasets and comprehensively compare the proposed method with other works in terms of performance (including PSNR,
SSIM and LPIPS), model complexity, rendering speed, and editing ability. Our performance outperforms NeRF [21] and all surface point-based rendering methods [1, 13, 33], and is comparable to Compressible-composable NeRF (CC-NeRF), i.e., the latest NeRF-based editable variant. We achieve a real-time rendering speed of 39.27 FPS on NeRF-Synthetic, which is 1700× faster than NeRF and 37× faster than CCNeRF. We also reproduce the scene editing of Object-NeRF [55] and CCNeRF [47] on ToyDesk [55] and NeRF-Synthetic [21] dataset, respectively. As shown in Fig. 1, we achieve real-time rendering with sharper de-tails and user-friendly editing. The above results demon-strate that our method is comprehensive in terms of both rendering and editing and has great application potentials. 2.