Abstract
We study universal zero-shot segmentation in this work to achieve panoptic, instance, and semantic segmentation for novel categories without any training samples. Such zero-shot segmentation ability relies on inter-class relation-ships in semantic space to transfer the visual knowledge learned from seen categories to unseen ones. Thus, it is desired to well bridge semantic-visual spaces and apply the semantic relationships to visual feature learning. We introduce a generative model to synthesize features for unseen categories, which links semantic and visual spaces as well as address the issue of lack of unseen training data. Furthermore, to mitigate the domain gap between semantic and visual spaces, firstly, we enhance the vanilla generator with learned primitives, each of which contains fine-grained attributes related to categories, and synthesize unseen features by selectively assembling these primitives.
Secondly, we propose to disentangle the visual feature into the semantic-related part and the semantic-unrelated part that contains useful visual classification clues but is less relevant to semantic representation. The inter-class relationships of semantic-related visual features are then required to be aligned with those in semantic space, thereby transferring semantic knowledge to visual feature learning.
The proposed approach achieves impressively state-of-the-art performance on zero-shot panoptic segmentation, in-stance segmentation, and semantic segmentation. 1.

Introduction
Image segmentation aims to group pixels with different semantics, e.g., category or instance [11]. Deep learn-ing methods [9, 11, 27, 34, 35, 44] have greatly advanced the performance of image segmentation with the powerful learning ability of CNNs [28] and Transformer [54]. How-ever, since deep learning methods are data-driven, great
†Equal contribution. (cid:0) Corresponding author (henghui.ding@gmail.com).
Figure 1. Zero-shot image segmentation aims to transfer the knowledge learned from seen classes to unseen ones (i.e., never shown up in training) with the help of semantic knowledge. challenges are induced by the intense demand for large-scale labeled training samples, which are labor-intensive and time-consuming.
To address this issue, zero-shot learning (ZSL) [36,47] is proposed to classify novel objects with no training samples. Recently, ZSL is extended into segmentation tasks like zero-shot semantic segmen-tation (ZSS) [4, 57] and zero-shot instance segmentation (ZSI) [63]. Herein, we further introduce zero-shot panoptic segmentation (ZSP) and aim to build a universal framework for zero-shot panoptic/semantic/instance segmentation with the help of semantic knowledge, as shown in Fig. 1.
Different from image classification, segmentation re-quires pixel-wise classification and is more challenging in terms of class representation learning. Substantial efforts have been devoted to zero-shot semantic segmentation [4, 57] and can be categorized into projection-based meth-ods [19, 57, 61] and generative model-based methods [4, 25, 38]. The generative model-based methods are usu-ally superior to the projection-based methods because they produce synthetic training features for the unseen group, which contribute to alleviating the crucial bias issue [49] of tending to classify objects into seen classes. Owing to the above merits, we follow the paradigm of generative model-based methods to address zero-shot segmentation tasks.
However, the current generative model-based methods are usually in the form of per-pixel-level generation, which is not robust enough in the more complicated scenarios.
Recently, several works propose to decouple the segmen-tation into class-agnostic mask prediction and object-level classification [8, 11, 29, 56]. We follow this strategy and degenerate the pixel-level generation to a more robust object-level generation. What’s more, previous generative works [4, 25, 38] usually learn a direct mapping from semantic embedding to visual features. Such a genera-tor does not consider the visual-semantic gap of feature granularity that images contain much richer information than languages. The direct mapping from coarse to fine-grained information results in low-quality synthetic fea-tures. To address this issue, we propose to utilize abundant primitives with very fine-grained semantic attributes to compose visual representations. Different assemblies of these primitives construct different class representations, where the assembly is decided by the relevance between primitives and semantic embeddings. Primitives greatly enhance the expressive diversity and effectiveness of the generator, especially in terms of rich fine-grained attributes, making the synthetic features for different classes more reliable and discriminative.
However, there are only real image features of seen classes to supervise the generator, leaving unseen classes unsupervised. To provide more constraints for the feature generation of unseen classes, we propose to transfer the inter-class relationships in semantic space to visual space.
The category relationships obtained by semantic embed-dings are employed to constrain the inter-class relationships of visual features. With such constraint, the visual features, especially the synthesized features for unseen classes, are promoted to have a homogeneous inter-class structure as in semantic space. Nevertheless, there is a discrepancy between the visual space and the semantic space [10, 52], so as to their inter-class relationships. Visual features contain richer information and cannot be fully aligned with semantic embeddings. Directly aligning two disjoint relationships inevitably compromises the discriminative of visual features. To address this issue, we propose to disen-tangle visual features into semantic-related and semantic-unrelated features, where the former is better aligned with the semantic embedding while the latter is noisy to semantic space. We only use semantic-related features for relation-ship alignment. The proposed relationship alignment and feature disentanglement are mutually beneficial. Feature disentanglement builds semantic-related visual space to facilitate relationship alignment and excludes semantic-unrelated features that are noisy for alignment. Relationship alignment in turn contributes to disentangling semantic-related features by providing semantic clues.
Overall, the main contributions are as follows:
• We study universal zero-shot segmentation and pro-pose Primitive generation with collaborative relation-ship Alignment and feature Disentanglement learning (PADing) as a unified framework for ZSP/ZSI/ZSS.
• We propose a primitive generator that employs lots of learned primitives with fine-grained attributes to synthesize visual features for unseen categories, which helps to address the bias issue and domain gap issue.
• We propose a collaborative relationship alignment and feature disentanglement learning approach to facilitate the generator producing better synthetic features.
• The proposed approach PADing achieves new state-of-the-art performance on zero-shot panoptic segmen-tation (ZSP), zero-shot instance segmentation (ZSI), and zero-shot semantic segmentation (ZSS). 2.