Abstract
Few-Shot Class-Incremental Learning (FSCIL) aims to continually learn novel classes based on only few train-ing samples, which poses a more challenging task than the well-studied Class-Incremental Learning (CIL) due to data scarcity. While knowledge distillation, a prevailing tech-nique in CIL, can alleviate the catastrophic forgetting of older classes by regularizing outputs between current and previous model, it fails to consider the overﬁtting risk of novel classes in FSCIL. To adapt the powerful distillation technique for FSCIL, we propose a novel distillation struc-ture, by taking the unique challenge of overﬁtting into ac-count. Concretely, we draw knowledge from two comple-mentary teachers. One is the model trained on abundant data from base classes that carries rich general knowledge, which can be leveraged for easing the overﬁtting of cur-rent novel classes. The other is the updated model from last incremental session that contains the adapted knowl-edge of previous novel classes, which is used for alleviat-ing their forgetting. To combine the guidances, an adaptive strategy conditioned on the class-wise semantic similari-ties is introduced. Besides, for better preserving base class knowledge when accommodating novel concepts, we adopt a two-branch network with an attention-based aggregation module to dynamically merge predictions from two com-plementary branches. Extensive experiments on 3 popular
FSCIL datasets: mini-ImageNet, CIFAR100 and CUB200 validate the effectiveness of our method by surpassing ex-isting works by a signiﬁcant margin. Code is available at https://github.com/LinglanZhao/BiDistFSCIL. 1.

Introduction
Real-world applications often face novel data in contin-uous stream format. In contrast, traditional models can only make predictions on a pre-deﬁned label set, and are not ﬂex-ible enough to tackle novel classes which may emerge af-ter deployment. To address this issue, Class-Incremental
Learning (CIL) has become an active area of recent re-search [2, 13, 20, 26]. The main focus of CIL is to effec-tively learn new concepts from abundant labeled samples
∗Equal contribution. †Corresponding author. (a) CIL: catastrophic forgetting
Abundant novel data
Model t-1
Logits
Distill
Model t (b) FSCIL: catastrophic forgetting & overfitting
Abundant base data
Trained
Semantic sim
Model base
Transfer
Model t-1
Few-shot novel data
Model t
Logits
Imprinted
Distill
Figure 1. Comparisons of (a) vanilla knowledge distillation in CIL and (b) our adapted class-aware bilateral distillation for FSCIL. and to simultaneously alleviate catastrophic forgetting over old classes. However, the requirement of sufﬁcient training data from novel classes still makes CIL impractical in many scenarios, especially when annotated samples are hard to obtain due to privacy or the unaffordable collecting cost.
For instance, to train an incremental model for face recog-nition, one or only few images are uploaded for recognizing the newly occurred person. To this end, Few-Shot Class-Incremental Learning (FSCIL) is proposed to learn novel concepts given only a few samples [30]. FSCIL deﬁnes a challenging task where abundant training samples are avail-able only in the base session for initial model pre-training and the model should continually absorb novel concepts from few data points in each incremental session.
A prevailing technique in CIL is to leverage knowledge distillation for alleviating the forgetting problem. The gen-eral routine is to calibrate the output logits between current and previous model, as illustrated in Fig. 1 (a). The out-put of current model t is restrained to be consistent with the output of model t-1 in last incremental session. Neverthe-less, such paradigm is not suitable for FSCIL [30, 41, 42], since the scarcity of novel class samples will cause model
t-1 severely overﬁtting to classes which occurred in that ses-sion (i.e., session t-1), making the model lack generaliza-tion ability, which further leads to the biased incremental learning in current session t.
Therefore, to adapt the powerful distillation technique for the challenging FSCIL task, we are devoted to design-ing a new distillation structure that can simultaneously han-dle the forgetting and overﬁtting challenges. To this end, we propose the class-aware bilateral distillation module, by adaptively drawing knowledge from two complementary teachers. One of them is the base model trained on abun-dant data from base classes. By distilling from the base model, we transfer the rich general knowledge learned from base classes to the few-shot novel classes, hence easing their overﬁtting. The other teacher is the updated model in the last session t-1, which carries the adapted knowledge of pre-viously seen novel classes (from session 1 to t-1), we can prevent the knowledge from forgetting by distilling from model t-1. Moreover, a class-aware coefﬁcient is learned to dynamically merge the above two guidance by considering class-aware semantic similarities between novel and base classes as priors. Intuitively, the more similar between base classes and a novel category, the more knowledge from base classes can be leveraged for alleviating the overﬁtting. As presented in Fig. 1 (b), instead of solely utilizing last ses-sion’s model t-1 for guiding the novel class adaptation, we selectively merge the output logits from both model t-1 and the base model as the guidance for distillation.
For further preserving base class knowledge when adapt-ing to novel classes, an attention-based aggregation module is proposed to automatically combine predictions from the base model and the current model t. Considering that the lower layers of a convolutional neural network capture fun-damental visual patterns [35], we set these layers shared and integrate the above models into a uniﬁed framework. For clarity, we also refer to the base and the current model as the base and novel branch, respectively. The two branches can be viewed as two individual experts for handling sam-ples from different categories. For a test sample from base classes, the aggregation module will pay more attention to predictions from base branch since it specializes in base classes without forgetting.
In contrast, the focus will be moved to novel branch when evaluated on novel class test samples, because novel branch is well adapted to those in-cremental classes. Our contributions are three-fold:
• To adapt the prevailing distillation technique for ad-dressing the unique overﬁtting challenges posed by
FSCIL, we propose a class-aware bilateral distillation method by adaptively drawing knowledge from two complementary teachers, which proves to be effective both in reducing the overﬁtting risk and preventing the aggravated catastrophic forgetting.
• We propose a two-branch network where the two branches are well associated by the class-aware bilat-eral distillation and attention-based aggregation mod-ule. The framework can simultaneously accommodate novel concepts and retain base knowledge, without so-phisticated meta-training and can be conveniently ap-plied to arbitrary pre-trained models, making it more practical in real-world applications.
• The superiority of our approach is validated on three public FSCIL datasets: mini-ImageNet, CIFAR100, and CUB200 by achieving remarkable state-of-the-art performance. For example, we surpass the second best result on mini-ImageNet over 3%. 2.