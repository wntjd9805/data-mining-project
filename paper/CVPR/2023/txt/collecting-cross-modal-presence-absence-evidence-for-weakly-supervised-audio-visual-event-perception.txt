Abstract
Video-level Annotations(cid:28451)
Dog, Speech, Singing
Only audible event
Only visible event
Audi-visible event
With only video-level event labels, this paper targets at the task of weakly-supervised audio-visual event perception (WS-AVEP), which aims to temporally localize and catego-rize events belonging to each modality. Despite the recent progress, most existing approaches either ignore the unsyn-chronized property of audio-visual tracks or discount the complementary modality for explicit enhancement. We ar-gue that, for an event residing in one modality, the modality itself should provide ample presence evidence of this event, while the other complementary modality is encouraged to afford the absence evidence as a reference signal. To this end, we propose to collect Cross-Modal Presence-Absence
Evidence (CMPAE) in a uniﬁed framework. Speciﬁcally, by leveraging uni-modal and cross-modal representations, a presence-absence evidence collector (PAEC) is designed under Subjective Logic theory. To learn the evidence in a reliable range, we propose a joint-modal mutual learning (JML) process, which calibrates the evidence of diverse au-dible, visible, and audi-visible events adaptively and dy-namically. Extensive experiments show that our method surpasses state-of-the-arts (e.g., absolute gains of 3.6% and 6.1% in terms of event-level visual and audio metrics).
Code is available in github.com/MengyuanChen21/
CVPR2023-CMPAE. 1.

Introduction
Research in computer vision places a signiﬁcant empha-sis on the visual aspects of event perception; nevertheless, in the real world with multisensory modalities, natural events are distinguished by a great deal more than just their appear-ance [11, 30, 52, 53, 56, 66]. For instance, think of playing a speciﬁc musical instrument in a concert hall, a barking dog, or starting a car with the engine sound. To properly compre-Visual 
Track
Audio 
Track
Dog
Speech
Speech
Singing
Figure 1. With only video-level annotations, weakly-supervised audio-visual event perception (WS-AVEP) aims to predict the tem-poral boundaries of various only audible (in orange), only visible (in green), or audi-visible (in blue) events in a video. hend an event, it is necessary to take acoustics into account and engage in joint audio-visual perception.
The target of audio-visual event perception (AVEP) is to temporally categorize video events. However, collecting precisely temporal audio-visual annotations is a bottleneck and consequently limits the scalability of a fully-supervised learning framework. As a result, Tian et al. [52,53] propose to perceive audio-visual events in an weakly-supervised manner, where only easily available video-level labels are needed during model training. As depicted in Figure 1, given videos which may have various audible, visible, or audi-visible events, the weakly-supervised audio-visual event perception (WS-AVEP) is commonly optimized by utilizing the video-level annotations.
To date in the literature, current WS-AVEP approaches mainly embrace two types of pipelines: (1) To comprehen-sively incorporate both modalities, some pioneering meth-ods [53] assume that each event in a video is simultane-ously audible and visible. Based on this characteristic, numerous cross-modal fusion strategies are proposed, in-cluding cross attention [60, 61, 63] and modality interac-tion [47, 62]. Although achieving promising performance, the rigorous assumption may not always hold in practice
Table 1. Comparison with the state-of-the-art methods on two tasks, AVVP and AVE. Note that the two tasks have different goals and properties. Please refer to the text for more details.
Method
Task
AVVP [52]
AVE [53]
CMBS [61]
JoMoLD [6] Ours 51.7 74.2 57.3 71.8 60.1 74.8 due to some audio-visual non-correspondence caused by out-of-screen objects and background noises. To this end, targeting at unsynchronized audio and visual information modeling, (2) Tian et al. [52] suggest a more general setting that recognizes event categories and temporal boundaries bind to sensory modalities, which breaks the modality con-sistency restriction. Since video-level labels do not indicate the detailed modality information, further research focuses on mining audio- or visual-speciﬁc information by learning from modality-speciﬁc noises [6], heterogeneous informa-tion [58], or hierarchical features [22]. Nonetheless, these approaches discount the complementary modality for ex-plicitly enhancing the prediction of the other modality. Al-though the multimodal multiple instance learning (MMIL) framework [6, 30, 52] can perform cross-modal enhance-ment for the feature learning, it still neglects the explicit and extra assistance of the complementary clues for individual modality prediction. Consequently, as shown in Table 1, state-of-the-arts of the two pipelines can only achieve sig-niﬁcant performance in one single WS-AVEP setting, show-ing that current methods are in a dilemma of making full use of both uni-modal and cross-modal information.
To tackle the above issues, we argue that, for an event re-siding in one modality1, the modality itself should provide ample presence evidence of this event, while the other com-plementary modality is encouraged to afford the absence evidence as a reference signal. On the one hand, to fully tap the potential of each modality, it is desirable to make the modality self-reliable for determining the evidence strength of an present event in the corresponding track. On the other hand, for judging which events are absent, relying on a single modality is insufﬁcient, whereas the other track can hand over complementary but not dominant assistance [66].
For example, although a baby is out-of-screen and the event
“baby cry” only appears in the audio modality, we can still infer that the audio track might not contain outdoor events because the perceived visual scene is considered to be in-doors. Similarly, when the audio track is salient, some vig-orous activity may be less likely to occur in the visual track.
Motivated by the above observations, we aim to cap-ture the presence and absence evidence for individual events by using uni-modal and cross-modal information. To ob-tain reliable evidence that can explicitly reﬂect and mea-1No matter whether the event is modality-speciﬁc or audi-visible. sure the event presence/absence intensity in each modal-ity, conventional convolutional neural networks, which are based on classiﬁcation probability, could be overconﬁdent and in the cart [43, 50, 55]. Recently, evidential deep learn-ing (EDL) [36,50], which can quantify uncertainty in model predictions trustfully by collecting subjective evidence, has attracted increasing attention and been successfully used in a variety of computer vision tasks [1, 3, 5, 17, 27, 57]. In this paper, we propose to collect Cross-Modal Presence-Absence Evidence (CMPAE) for WS-AVEP in a uniﬁed framework. As shown in Figure 2, we design a presence-absence evidence collector (PAEC) by using uni-modal and cross-modal representations. Here, the presence evidence of events in each track is derived from the modality itself, whereas the other modality acts as a cross-modal selector for generating the absence evidence. The evidence of each temporal snippet is then accumulated to video-level evi-dence and optimized in accordance with Subjective Logic theory [23, 64]. To learn the evidence in a reliable range, we propose a joint-modal mutual learning (JML) process, which calibrates the evidence of diverse audible, visible, and audi-visible events adaptively and dynamically. By virtue of the above design, the proposed PAEC and JML modules can cooperate with each other in a uniﬁed frame-work for effective presence-absence evidence learning.
Our main contributions can be summarized as follows:
• We propose a novel cross-modal presence-absence evi-dence learning framework for weakly-supervised audio-visual event perception, which jointly enjoys the merits of uni-modal discrimination and cross-modal enhance-ment under Subjective Logic theory.
• With the cooperative presence-absence evidence collec-tor and the joint-modal mutual learning process, we in-ject the uni-modal and cross-modal information into the learned evidence and calibrate it to a reliable range.
• We conduct extensive and in-depth experiments on sev-eral popular and standard WS-AVEP datasets [52, 53].
The encouraging results compared with state-of-the-arts demonstrate the effectiveness of our method. 2.