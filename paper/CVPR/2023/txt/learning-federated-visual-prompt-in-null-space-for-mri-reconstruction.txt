Abstract
Federated Magnetic Resonance Imaging (MRI) recon-struction enables multiple hospitals to collaborate dis-tributedly without aggregating local data, thereby protect-ing patient privacy. However, the data heterogeneity caused by different MRI protocols, insufficient local training data, and limited communication bandwidth inevitably impair global model convergence and updating. In this paper, we propose a new algorithm, FedPR, to learn federated vi-sual prompts in the null space of global prompt for MRI reconstruction. FedPR is a new federated paradigm that adopts a powerful pre-trained model while only learning and communicating the prompts with few learnable param-eters, thereby significantly reducing communication costs and achieving competitive performance on limited local data. Moreover, to deal with catastrophic forgetting caused by data heterogeneity, FedPR also updates efficient feder-ated visual prompts that project the local prompts into an approximate null space of the global prompt, thereby sup-pressing the interference of gradients on the server perfor-mance. Extensive experiments on federated MRI show that
FedPR significantly outperforms state-of-the-art FL algo-rithms with < 6% of communication costs when given the limited amount of local training data. 1.

Introduction
Federated Magnetic Resonance Imaging (MRI) recon-struction enables multiple hospitals to train a powerful global model in a distributed manner without sharing pri-In federated MRI, each client (i.e., vate data [7, 9, 13]. hospital) uses its local computing power, memory, and pri-vate data to train local models independently, while the server aggregates all the local models in each communica-*Corresponding author.
Figure 1. Illustration of the three key issues in federated MRI. tion round and distributes the global model to each client again [26].
Existing federated MRI reconstruction techniques usu-ally improve federated learning (FL) by enhancing the ag-gregation process [9] and reducing the local parameter’s variance [5, 13]. Such techniques require a large commu-nication bandwidth and sufficient local training data. How-ever, federated MRI often faces two issues, ❶ insufficient amount of local training data due to the difficulty of ac-quiring the ground-truth of MRI reconstruction [10, 11, 12] and ❷ limited communication bandwidth due to unbalanced regional development (see Fig. 1). To cope with the is-sue ❶, pre-trained models have exhibited superior perfor-mance, and have shown to close the gap between feder-ated and centralized performance [3, 28]. However, since the model parameters need to be shared between the client and the server for updating, the large number of parame-ters of pre-trained models will result in a huge communi-cation cost. On contrary, prompt tuning has recently been suggested as a new fine-tuning paradigm by freezing the models and only learning a small number of learnable pa-rameters in the input space [15, 31, 32]. Benefited from pre-trained models, only parameter-efficient prompts are re-quired in learning and communication, and prompt tuning can be conducted with a limited number of samples, mak-ing it very appealing in tackling the above two issues (i.e.,
❶ and ❷) in federated MRI.
Besides, there is another critical issue for federated MRI, i.e., ❸ catastrophic forgetting, caused by data heterogene-ity due to the different imaging protocols of MRI scanners
In the adopted by different clients [9, 39] (see Fig. 1). local update, the global model from the prior round tends to be overfitted to the local training data, leading to catas-trophic forgetting. Analogous to continual learning, sev-eral techniques have been presented by introducing proper regularization terms in local models [4, 33, 39] or retain-ing previously acquired knowledge by knowledge distilla-tion [14, 35, 38]. However, these strategies require seek-ing a balance between multiple losses and relying on proxy datasets. Instead, Adam-NSC mitigates catastrophic forget-ting in continual learning with a new network parameter update rule, i.e., forces the network update to lie in the ap-proximate null space of the input features of previous tasks at each layer [36]. However, the null space at feature level is built upon large local training data, which generally can-not be satisfied in federated MRI, i.e., the issue ❶. Taking the issues ❶ and ❸ into account, we suggest to optimize lo-cal prompts in the approximate null space of global prompts instead of input features, thereby preventing the loss of pre-viously gained global knowledge.
In a nutshell, we explore a new FL paradigm, i.e., FedPR, to learn federated visual prompts for MRI reconstruction.
To begin with, we pre-train the model on a large-scale pub-lic dataset. Given limited amount of local training data, vi-sual prompt tuning is adopted to learn local prompts in a distributed manner. For the issues ❶ and ❷, federated vi-sual prompts are introduced to learn a strong global model, where only local and global prompts are learnable and com-municated. As for the issue ❸, we perform singular value decomposition (SVD) on the uncentered covariance matrix of global prompts to obtain the approximate null space.
FedPR tunes only the local parameters in the null space of global prompts, thereby well preserving the prior knowl-edge of previous rounds and resulting in low communica-tion costs. In particular, FedPR achieves a > 4.5 dB gain in
PSNR with less than 6% of communication costs. To sum up, our contributions are as follows:
• We propose a federated visual prompt algorithm, FedPR, to solve the three key issues in federated MRI. By lever-aging powerful pre-trained models and freezing backbone networks in FL, only a small amount of parameters in the input space are trainable, thereby reducing communica-tion costs.
• We explore how to alleviate catastrophic forgetting in
FL while reducing communication costs. By optimiz-ing local parameters only in the null space of global prompts, FedPR well preserves the previously acquired global knowledge in each round, maintaining competitive performance with only a few local data.
• We evaluate the performance of FedPR for federated MRI reconstruction. In comparison to the state-of-the-art FL methods, FedPR achieves superior performance in com-plex scenarios, e.g., less local data, lower communication costs, and faster convergence. 2.