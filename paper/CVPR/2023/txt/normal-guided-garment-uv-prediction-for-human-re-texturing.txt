Abstract
Clothes undergo complex geometric deformations, which lead to appearance changes. To edit human videos in a physically plausible way, a texture map must take into account not only the garment transformation induced by the body movements and clothes ﬁtting, but also its 3D ﬁne-grained surface geometry. This poses, however, a new chal-lenge of 3D reconstruction of dynamic clothes from an im-age or a video. In this paper, we show that it is possible to edit dressed human images and videos without 3D re-construction. We estimate a geometry aware texture map between the garment region in an image and the texture space, a.k.a, UV map. Our UV map is designed to pre-serve isometry with respect to the underlying 3D surface by making use of the 3D surface normals predicted from the image. Our approach captures the underlying geometry of the garment in a self-supervised way, requiring no ground truth annotation of UV maps and can be readily extended to predict temporally coherent UV maps. We demonstrate that our method outperforms the state-of-the-art human UV map estimation approaches on both real and synthetic data. 1.

Introduction
While browsing online clothing shops, have you ever wondered how the appearance of a dress of interest would look on you as if you were in a ﬁtting room given your dress with a similar shape? A key technology to enable generat-ing such visual experiences is photorealistic re-texturing— editing the texture of clothes in response to the subject’s movement in the presented images or videos in a geomet-rically and temporally coherent way. Over the past few years, there has been a signiﬁcant advancement in the im-age and video editing technologies [4–6, 11–13, 17, 20, 21, 26, 31–33, 38, 46, 54], such as inserting advertising logos on videos of moving cars or applying face makeup on so-cial media. However, such editing approaches designed for rigid or semi-rigid surfaces are not suitable for garments that undergo complex secondary motion with respect to the underlying body. For example, the ﬁne wrinkles of the dress in Figure 1 result in complex warps in texture over time. In this paper, we present a new method to edit the appearance of a garment in a given image or video by taking into ac-count its ﬁne-grained geometric deformation. (1) 3D reconstruction and rendering:
Previous works address photorealistic texture editing in two ways. these approaches can achieve high-ﬁdelity texture editing given highly accurate 3D geometry. On other side of the coin, their performance is dictated by the quality of the 3D re-construction. While the 3D geometry of the garment can be learned from paired human appearance data, e.g., hu-man modeling repositories with 3D meshes and render-ings [1], due to the scarcity of such data, it often can-not generalize well on unseen real images and videos. (2)
Direct texture mapping: by estimating dense UV map, these methods can bypass the procedure of 3D reconstruc-tion [18, 22, 39, 41, 55]. However, they usually lack of ge-ometry details and only capture the underlying human body, thus, not applicable for editing garments. Moreover, when applied to videos, visual artifacts of editing become more salient since they are not aware of underlying deformation of the garment’s 3D geometry [25, 57].
We design our method to enjoy the advantages of both two approaches: preserving realistic details in UV mapping while circumventing 3D reconstruction. Our key insight is that the fundamental geometric property of isometry can be imposed into UV map estimation via the 3D surface nor-mals predicted from an image. We formulate a geometric relationship between the UV map and surface normals in the form of a set of partial differential equations.
Our method takes as input an image or video, its sur-face normal prediction, and dense optical ﬂow (for video), and outputs the geometry aware UV map estimate. The UV map is modeled by a multi-layer perceptron that can pre-dict UV coordinates given a pixel location in an image. We note that the UV map is deﬁned up to the choice of a refer-ence coordinate frame. To disambiguate this, we condition the neural network with a pre-deﬁned proxy UV map (e.g.,
DensePose [18]). We use the isometry constraints as a loss to optimize the UV map. Further, for a video, we leverage the per-frame image feature to correlate the UV coordinates of the pixels across time using optical ﬂow.
Our contributions can be concluded in three aspects: (1) a novel formulation that captures the geometric relationship between the 3D surface normals and the UV map by the isometry contraint, which eliminates the requirement of 3D reconstruction and ground truth UV map; (2) a neural net-work design that learns to predict temporally coherent UV map for the frames by correlating per-frame image features; (3) stronger performance compared to existing re-texturing methods and compelling results on a wide range of real-world imagery. 2.