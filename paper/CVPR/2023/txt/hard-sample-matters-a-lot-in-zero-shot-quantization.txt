Abstract
Zero-shot quantization (ZSQ) is promising for compress-ing and accelerating deep neural networks when the data for training full-precision models are inaccessible. In ZSQ, network quantization is performed using synthetic samples, thus, the performance of quantized models depends heavily on the quality of synthetic samples. Nonetheless, we find that the synthetic samples constructed in existing ZSQ meth-ods can be easily fitted by models. Accordingly, quantized models obtained by these methods suffer from significant performance degradation on hard samples. To address this issue, we propose HArd sample Synthesizing and Training (HAST). Specifically, HAST pays more attention to hard sam-ples when synthesizing samples and makes synthetic samples hard to fit when training quantized models. HAST aligns features extracted by full-precision and quantized models to ensure the similarity between features extracted by these two models. Extensive experiments show that HAST significantly outperforms existing ZSQ methods, achieving performance comparable to models that are quantized with real data. 1.

Introduction
Deep neural networks (DNNs) achieve great success in many domains, such as image classification [28, 43, 44], ob-ject detection [15, 16, 39], semantic segmentation [13, 53], and embodied AI [3,4,11]. These achievements are typically paired with the rapid growth of parameters and computa-tional complexity, making it challenging to deploy DNNs on resource-constrained edge devices. In response to the challenge, network quantization proposes to represent the full-precision models, i.e., floating-point parameters and ac-tivations, using low-bit integers, resulting in a high compres-sion rate and an inference-acceleration rate [24]. These meth-*Equal contribution. Email: 202021046173@mail.scut.edu.cn
†Corresponding author. Email: mingkuitan@scut.edu.cn (a) 3-bit quantization. (b) 4-bit quantization.
Figure 1. Performance of the proposed HAST on three datasets compared with the state-of-the-art method IntraQ [50] and the method fine-tuning with real data [50]. HAST quantizes ResNet-20 on CIFAR-10/CIFAR-100 and ResNet-18 on ImageNet to 3-bit (left) and 4-bit (right), achieving performance comparable to the method fine-tuning with real data. ods implicitly assume that the training data of full-precision models are available for the process of network quantization.
However, the training data, e.g., medical records, can be inaccessible due to privacy and security issues. Such a prac-tical and challenging scenario has led to the development of zero-shot quantization (ZSQ) [2], quantizing networks without accessing the training data.
Many efforts have been devoted to ZSQ [2, 19, 38, 46, 48, 50]. In ZSQ, some works perform network quantization by weight equalization [38], bias correction [1], or weight round-ing strategy [19], at the cost of some performance degrada-tion. To promote the performance of quantized models, ad-vanced works propose to leverage synthetic data for network quantization [2, 7, 46, 48, 50]. Specifically, they fine-tune quantized models with data synthesized using full-precision models, achieving promising improvement in performance.
Much attention has been paid to the generation of syn-thetic sample, since high-quality synthetic samples lead to high-performance quantized models [2, 46]. Recent works employ generative models to synthesize data with fruitful approaches, considering generator design [51], boundary sample generation [6], adversarial training scheme [35], and
(a) Performance of converged 3-bit ResNet-20.(b) Variation of error rate with sample difficulty. (c) Variation of fraction with sample difficulty.
Figure 2. Analysis on synthetic data. (a) Performance of converged 3-bit ResNet-20. We quantize ResNet-20 to 3-bit using IntraQ [50], Real
Data [50], and Our HAST, respectively, The top-1 accuracy on both training data (synthetic data for ZSQ methods) and test data is reported. (b) The error rate of test samples with different difficulties. (c) Distribution visualization of sample difficulty using GHM [30]. For each converged quantized model, we randomly sample 10,000 synthetic/real samples and count the fraction of samples based on difficulty. Note that the y-axis uses a log scale since the number of samples with different difficulties can differ by order of magnitude. effective training strategy [7]. Since the quality of synthetic samples is typically limited by the generator [36], advanced works treat synthesizing samples as a problem of noise op-timization [2]. Namely, the noise distribution is optimized to approximate some specified properties of real data dis-tributions, such as batch normalization statistics (BNS) and inception loss (IL) [20]. To promote model performance,
IntraQ [50] focuses on the property of synthetic samples and endows samples with heterogeneity, achieving state-of-the-art performance as depicted in Figure 1.
Although existing ZSQ methods achieve considerable performance gains by leveraging synthetic samples, there is still a significant performance gap between models trained with synthetic data and those trained with real data [50]. To reduce the performance gap, we investigate the difference between real and synthetic data. Specifically, we study the difference in generalization error between models trained with real data and those trained with synthetic data. Our experimental results show that synthetic data lead to larger generalization errors than real data, as illustrated in Figure 2a.
Namely, synthetic sample lead to a more significant gap between training and test accuracy than real data.
We conjecture that the performance gap stems from the misclassification of hard test samples. To verify the conjec-ture, we conduct experiments to study how model perfor-mance varies with sample difficulty, where GHM [30] is employed to measure the difficulty of samples quantitatively.
The results shown in Figure 2b demonstrate that, on difficult samples, models trained with synthetic data perform worse than those trained with real data. This may result from that synthetic samples are easy to fit, which is consistent with the observation on inception loss of synthetic data [33]. We ver-ify the assumption through a series of experiments, where we count the fraction of samples of different difficulties using
GHM [30]. The results are reported in Figure 2c, where we observe a severe missing of hard samples in synthetic sam-ples compared to real data. Consequently, quantized models fine-tuned with these synthetic data may fail to generalize well on hard samples in the test set.
In light of conclusions drawn from Figure 2, the samples synthesized for fine-tuning quantized models in ZSQ should be hard to fit. To this end, we propose a novel HArd sample
Synthesizing and Training (HAST) scheme. The insight of
HAST has two folds: a) The samples constructed for fine-tuning models should not be easy for models to fit; b) The features extracted by full-precision and the quantized model should be similar. To this end, in the process of synthesizing samples, HAST pays more attention to hard samples in a re-weighting manner, where the weights are equal to the sample difficulty introduced in GHM [30]. Meanwhile, in the fine-tuning process, HAST further promotes the sample difficulty on the fly and aligns the features between the full-precision and quantized models.
To verify the effectiveness of HAST, we conduct compre-hensive experiments on three datasets under two quantization precisions, following settings used in [50]. Our experimental results show that HAST using only 5,120 synthetic samples outperforms previous state-of-the-art method [50] and even achieves performance comparable with quantization with real data.
Our main contributions can be summarized as follows:
• We observe that the performance degradation in zero-shot quantization is attributed to the lack of hard sam-ples. Namely, the synthetic samples used in existing
ZSQ methods are easily fitted by quantized models, dis-tinguishing models trained on synthetic samples from those trained on real data, as depicted in Figure 2.
• Built upon our empirical observation, we propose a novel HArd sample Synthesizing and Training (HAST) scheme to promote the performance of ZSQ. Specifi-cally, HAST generates hard samples and further pro-motes the sample difficulty on the fly when training models, paired with a feature alignment constraint to ensure the similarity of features extracted by these two models, as summerized in Algorithm 1.
• Extensive experiments demonstrate the superiority of
HAST over existing ZSQ methods. More specifically,
HAST using merely 5,120 synthetic samples outper-forms the previous state-of-the-art method and achieves performance comparable to models fine-tuned using real training data, as shown in Figure 1. 2.