Abstract
Fair clustering aims to divide data into distinct clusters while preventing sensitive attributes (e.g., gender, race,
RNA sequencing technique) from dominating the clustering.
Although a number of works have been conducted and achieved huge success recently, most of them are heuristi-cal, and there lacks a unified theory for algorithm design.
In this work, we fill this blank by developing a mutual information theory for deep fair clustering and accordingly designing a novel algorithm, dubbed FCMI. In brief, through maximizing and minimizing mutual information,
FCMI is designed to achieve four characteristics highly ex-pected by deep fair clustering, i.e., compact, balanced, and fair clusters, as well as informative features. Besides the contributions to theory and algorithm, another contribution of this work is proposing a novel fair clustering metric built upon information theory as well. Unlike existing evalua-tion metrics, our metric measures the clustering quality and fairness as a whole instead of separate manner. To verify the effectiveness of the proposed FCMI, we conduct experiments on six benchmarks including a single-cell
RNA-seq atlas compared with 11 state-of-the-art methods in terms of five metrics. The code could be accessed from https://pengxi.me. 1.

Introduction
Clustering plays an important role in machine learning
[19, 27–29, 34, 42, 43], which could partition data into dif-ferent clusters without any label information. It has been widely used in many real-world applications such as multi-view learning [35, 39], image segmentation [24], and bioin-formatics [20]. In practice, however, the data might be con-founded with sensitive attributes (e.g., gender, race, etc., also termed as group information) that probably over-∗ Equal contribution
† Corresponding author
Figure 1. Illustration of our basic idea using information diagrams, where X, G, and C denote the inputs, sensitive attributes, and clustering assignments, respectively. To cluster data, we expect the overlap between non-group information and cluster informa-tion (i.e., the conditional mutual information I(X; C|G)) to be maximized. Meanwhile, to prevent sensitive attributes from domi-nating the clustering results, we expect the overlap between group information and cluster information (i.e., the mutual information
I(G, C)) to be minimized. whelm the intrinsic semantic of samples (also termed as cluster information). Taking single-cell RNA clustering as a showcase, standard methods would partition data based on sequencing techniques (group information) instead of intrinsic cell types (cluster information), since cells se-quenced by different techniques would result in different expression levels [36] and most clustering methods cannot distinguish these two kinds of information. The case is sim-ilar in many automatic learning systems where the clus-tering results are biased toward sensitive attributes, which would interfere with the decision-making [9, 12, 18]. No-tably, even though these sensitive attributes are known in prior, it is daunting to alleviate or even eliminate their in-fluence, e.g., removing the “gender” information from the photos of users.
As a feasible solution, fair clustering aims to hide sen-sitive attributes from the clustering results. Commonly, a clustering result is considered fair when samples of different sensitive attributes are uniformly distributed in clusters so
that the group information is protected. However, it would lead to a trivial solution if the fairness is over-emphasized, i.e., all samples are assigned to the same cluster. Hence, in addition to fairness, balance and compactness are also highly expected in fair clustering. Specifically, a balanced clustering could avoid the aforementioned trivial solution brought by over-emphasized fairness, and the compactness refers to a clear cluster boundary.
To achieve fair clustering, many studies have been con-ducted to explore how to incorporate fairness into luster-ing [3,4,8,22,23,38,44,46]. Their main differences lie in i) the stage of fairness learning, and ii) the depth of the model.
In brief, [3, 8] incorporate the fairness in a pre-processing fashion by packing data points into so-called fairlets with balanced demographic groups and then partitioning them with classic clustering algorithms. [22,46] are in-processing methods that formulate fairness as a constraint for cluster-ing. As a representative of post-processing methods, [4] first performs classic clustering and then transforms the clustering result into a fair one by linear programming. Dif-ferent from the above shallow models, [23, 38, 44] propose performing fair clustering in the latent space learned by different deep neural networks to boost performance. Al-though promising results have been achieved by these meth-ods, almost all of them are heuristically and empirically de-signed, with few theoretical explanations and supports. In other words, it still lacks a unified theory to guide the algo-rithm design.
In this work, we unify the deep fair clustering task un-der the mutual information theory and propose a novel theoretical-grounded deep fair clustering method accord-ingly. As illustrated in Fig. 1, we theoretically show that clustering could be achieved by maximizing the condi-tional mutual information (CMI) I(X; C|G) between in-puts X and cluster assignments C given sensitive attributes
G. Meanwhile, we prove that the fairness learning could be formulated as the minimization of the mutual informa-tion (MI) I(G; C). In this case, sensitive attributes will be hidden in the cluster assignments and thus fair clustering could be achieved. To generalize our theory to deep neu-ral networks, we additionally show a deep variant could be developed by maximizing the mutual information I(X; X ′) between the input X and its approximate posterior X ′. No-tably, some deep clustering methods [17,26] have been pro-posed based on the information theory. However, they are remarkably different from this work. To be exact, they ig-nored the group information. As a result, the group infor-mation will leak into cluster assignments, leading to unfair partitions. In addition, we prove that our mutual informa-tion objectives intrinsically correspond to four characteris-tics highly expected in deep fair clustering, namely, com-pact, balanced, and fair clusters, as well as informative fea-tures.
Besides the above contributions to theory and algorithm, this work also contributes to the performance evaluation.
To be specific, we notice that almost all existing methods evaluate clustering quality and fairness separately. How-ever, as fair clustering methods usually make a trade-off be-tween these two aspects, such an evaluation protocol might be partial and inaccurate. As an improvement, we design a new evaluation metric based on the information theory, which simultaneously measures the clustering quality and fairness. The contribution of this work could be summa-rized as follows:
• We formulate deep fair clustering as a unified mu-tual information optimization problem. Specifically, we theoretically show that fair clustering could be achieved by maximizing CMI between inputs and clus-ter assignments given sensitive attributes while mini-mizing MI between sensitive attributes and cluster as-signments. Moreover, the informative feature extrac-tion could be achieved by maximizing MI between the input and its approximate posterior.
• Driven by our unified mutual information theory, we propose a deep fair clustering method and carry out ex-tensive experiments to show its superiority on six fair clustering benchmarks, including a single-cell RNA at-las.
• To evaluate the performance of fair clustering more comprehensively, we design a novel metric that mea-sures the clustering quality and fairness as a whole from the perspective of information theory. 2.