Abstract
Video generation remains a challenging task due to spa-tiotemporal complexity and the requirement of synthesizing diverse motions with temporal consistency. Previous works attempt to generate videos in arbitrary lengths either in an autoregressive manner or regarding time as a continuous signal. However, they struggle to synthesize detailed and di-verse motions with temporal coherence and tend to generate repetitive scenes after a few time steps. In this work, we ar-gue that a single time-agnostic latent vector of style-based generator is insufficient to model various and temporally-consistent motions. Hence, we introduce additional time-dependent motion styles to model diverse motion patterns.
In addition, a Motion Style Attention modulation mecha-nism, dubbed as MoStAtt, is proposed to augment frames with vivid dynamics for each specific scale (i.e., layer), which assigns attention score for each motion style w.r.t de-convolution filter weights in the target synthesis layer and softly attends different motion styles for weight modula-tion. Experimental results show our model achieves state-of-the-art performance on four unconditional 2562 video synthesis benchmarks trained with only 3 frames per clip and produces better qualitative results with respect to dy-namic motions. Code and videos have been made avail-able at https://github.com/xiaoqian- shen/
MoStGAN-V . 1.

Introduction
Learning algorithms for image generation are rapidly reaching maturity and are expected to soon approach hu-man levels of performance. However, the video generation task does not share similar success and remains a research opportunity to be further explored. Since videos are com-putationally intensive to model, a key question is how to generate high-quality videos with limited computation re-sources. Another challenge is that due to the very nature of video data, video synthesis is not simply tantamount to generating high-quality images that change over time but also requires generating these frames with temporal con-Figure 1. Intuitively show how motion styles model different mo-tion patterns along time steps. Each curve corresponds to one motion style as a function of time step t. Each motion style will adaptively modulate filter weights of the generator with our pro-posed MoStAtt mechanism and further stylize input features with dynamic motions. sistency, i.e., natural transitions between frames as well as realistic motions. This problem gets even more severe when the number of elements (e.g., objects and background) that move spatiotemporally grows.
Towards low computation consumption, several RNN or
LSTM-based autoregressive approaches [5, 22, 28, 29] re-duce computation complexity and make long video gener-ation possible but the resulting videos tend to accumulate errors over time and contain inconsistent frames. Recent works use implicit neural representations by regarding time as continuous signals and mapping time to either spatial co-ordinates [39] or StyleGAN [12] feature inputs [26]. Al-though these methods leverage sparse training to enhance training efficiency and produce arbitrarily long videos, they fall short in capturing diverse patterns of changing motions.
For instance, a generated video of a person talking may con-tain only low-frequency global motions like moving head to different angles but lack the ability to synthesize high-frequency mouth and eye actions, e.g., open/close mouth and blink eyes.
The limitation of previous works leads us to rethink how to explicitly model the diversity of motions and lever-age them for motion synthesis in the video generation pro-cess. Motion segmentation methods [3, 6, 15] assume simi-lar trajectories representing for similar motions and segment videos into static background and diverse moving objects, which explicitly model them on perception level. Draw-ing inspiration from this perceptual grouping concept com-monly employed in motion segmentation literature, we for-mulate the notion of diverse motion modeling for video gen-eration on top the style-based framework [12,13,26] which leverage style parameters to control different levels of syn-thesis by modulating the weights of scale-specific layers in the generator. This means we aim to design group of motion styles, each of which corresponds to one motion pattern and is able to stylize input features with dynamic motions by adaptively modulating the weights of deconvolutional fil-ters.
In this work, we aim at generating videos with vivid dy-namics and rich motions over time while reserving tem-poral consistency. We argue that single time-agnostic la-tent vector of the StyleGAN-based model is insufficient to model different motion patterns in complicated motion-variant video datasets and consider auxiliary style control to increase motion awareness in the generation process. To this end, we introduce the concept of motion styles for video generation and develop a motion network to generate time-dependent motion styles to model diverse motion patterns
In addition, we consider how to along continuous time. make multiple motion styles dynamically contribute to mo-tion synthesis since motions might temporally exist in sev-eral frames while disappear in other time steps. Therefore, a motion-style attention modulation mechanism, dubbed as
MoStAtt, is designed to perform cross-attention between deconvolutional filters and motions styles and linearly com-bine filter-specific and time-dependent motion styles for dy-namic filter weight modulation. Variant motion styles that correspond to different motion patterns will adaptively at-tend for frames stylization and thus the modulated weights can better represent time-dependent and diverse motions.
Note that our approach differs from previous works, which either predict latent motion trajectories for an image gener-ator [28] or generate continuous motion codes and concate-nate them with constant vectors to serve as initial feature inputs for a generator network [26]. In contrast, our pro-posed motion styles and MoStAtt modules are designed to modulate kernel weights, rather than functioning as feature inputs.
Our contributions are highlighted below:
• Introducing time-dependent motion styles for video generation task in addition to the original time-agnostic content style of style-based models to facili-tate weight modulation and thus raise temporal aware-ness and enhance motion synthesis.
• Motion style attention modulation mechanism, dubbed as MoStAtt, which softly attends different motion styles for weight modulation in each synthesis layer and facilitates the modulated weights to augment in-dividual static frames with dynamic motions across holistic time continuous videos.
• Our simple yet effective approach achieves state-of-the-art performance in unconditional video generation and enhances qualitative results of motion synthesis. 2.