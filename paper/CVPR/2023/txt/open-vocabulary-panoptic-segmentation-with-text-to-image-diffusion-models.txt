Abstract 1.

Introduction
We present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation, which unifies pre-trained text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation. Text-to-image diffu-sion models have the remarkable ability to generate high-quality images with diverse open-vocabulary language de-scriptions. This demonstrates that their internal represen-tation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP, on the other hand, are good at classifying images into open-vocabulary labels. We leverage the frozen internal repre-sentations of both these models to perform panoptic seg-mentation of any category in the wild. Our approach out-performs the previous state of the art by significant margins on both open-vocabulary panoptic and semantic segmen-tation tasks. In particular, with COCO training only, our method achieves 23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute improvement over the previous state of the art. We open-source our code and models at https://github.com/NVlabs/
ODISE.
*Jiarui Xu was an intern at NVIDIA during the project. † equal contri-bution.
Humans look at the world and can recognize limitless categories. Given the scene presented in Fig. 1, besides identifying every vehicle as a “truck”, we immediately un-derstand that one of them is a pickup truck requiring a trailer to move another truck. To reproduce an intelligence with such a fine-grained and unbounded understanding, the prob-lem of open-vocabulary recognition [36, 57, 76, 89] has re-cently attracted a lot of attention in computer vision. How-ever, very few works are able to provide a unified frame-work that parses all object instances and scene semantics at the same time, i.e., panoptic segmentation.
Most current approaches for open-vocabulary recogni-tion rely on the excellent generalization ability of text-image discriminative models [30, 57] trained with Internet-scale data. While such pre-trained models are good at clas-sifying individual object proposals or pixels, they are not necessarily optimal for performing scene-level structural understanding. Indeed, it has been shown that CLIP [57] often confuses the spatial relations between objects [69].
We hypothesize that the lack of spatial and relational under-standing in text-image discriminative models is a bottleneck for open-vocabulary panoptic segmentation.
On the other hand, text-to-image generation using dif-fusion models trained on Internet-scale data [1, 59, 61, 62, 1
90] has recently revolutionized the field of image synthe-sis.
It offers unprecedented image quality, generalizabil-ity, composition-ability and, semantic control via the input text. An interesting observation is that to condition the im-age generation process on the provided text, diffusion mod-els compute cross-attention between the text’s embedding and their internal visual representation. This design im-plies the plausibility of the internal representation of dif-fusion models being well-differentiated and correlated to high/mid-level semantic concepts that can be described by language. As a proof-of-concept, in Fig.1 (center), we vi-sualize the results of clustering a diffusion model’s internal features for the image on the left. While not perfect, the discovered groups are indeed semantically distinct and lo-calized. Motivated by this finding, we ask the question of whether Internet-scale text-to-image diffusion models can be exploited to create universal open-vocabulary panoptic segmentation learner for any concept in the wild?
To this end, we propose ODISE: Open-vocabulary
DIffusion-based panoptic SEgmentation (pronounced o-di-see), a model that leverages both large-scale text-image dif-fusion and discriminative models to perform state-of-the-art panoptic segmentation of any category in the wild. An overview of our approach is illustrated in Fig. 2. At a high-level it contains a pre-trained frozen text-to-image dif-fusion model into which we input an image and its cap-tion and extract the diffusion model’s internal features for them. With these features as input, our mask generator pro-duces panoptic masks of all possible concepts in the image.
We train the mask generator with annotated masks avail-able from a training set. A mask classification module then categorizes each mask into one of many open-vocabulary categories by associating each predicted mask’s diffusion features with text embeddings of several object category names. We train this classification module with either mask category labels or image-level captions from the training dataset. Once trained, we perform open-vocabulary panop-tic inference with both the text-image diffusion and discrim-inative models to classify a predicted mask. On many differ-ent benchmark datasets and across several open-vocabulary recognition tasks, ODISE achieves state-of-the-art accuracy outperforming the existing baselines by large margins.
Our contributions are the following:
• To the best of our knowledge, ODISE is the first work to explore large-scale text-to-image diffusion models for open-vocabulary segmentation tasks.
• We propose a novel pipeline to effectively leverage both text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation.
• We significantly advance the field forward by out-performing all existing baselines on many open-vocabulary recognition tasks, and thus establish a new state of the art in this space. 2.