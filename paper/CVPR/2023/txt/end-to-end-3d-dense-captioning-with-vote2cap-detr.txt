Abstract 3D dense captioning aims to generate multiple cap-tions localized with their associated object regions. Exist-ing methods follow a sophisticated “detect-then-describe” pipeline equipped with numerous hand-crafted components.
However, these hand-crafted components would yield sub-optimal performance given cluttered object spatial and
In this pa-class distributions among different scenes. per, we propose a simple-yet-effective transformer frame-work Vote2Cap-DETR based on recent popular DEtection
TRansformer (DETR). Compared with prior arts, our framework has several appealing advantages: 1) With-out resorting to numerous hand-crafted components, our method is based on a full transformer encoder-decoder ar-chitecture with a learnable vote query driven object de-coder, and a caption decoder that produces the dense cap-tions in a set-prediction manner. 2) In contrast to the two-stage scheme, our method can perform detection and cap-tioning in one-stage. 3) Without bells and whistles, exten-sive experiments on two commonly used datasets, ScanRe-fer and Nr3D, demonstrate that our Vote2Cap-DETR sur-passes current state-of-the-arts by 11.13% and 7.11% in
CIDEr@0.5IoU, respectively. Codes will be released soon. 1.

Introduction
In recent years, works on 3D learning has grown dramat-ically for various applications [10, 11, 21, 41, 42]. Among them, 3D dense captioning [7, 13] requires a system to lo-calize all the objects in a 3D scene and generate descrip-tive sentences for each object. This problem is challenging, given 1) the sparsity of point clouds and 2) the cluttered distribution of objects. 3D dense captioning can be divided into two tasks, object detection, and object caption generation. Scan2Cap [13],
MORE [20], and SpaCap3D [39] propose well-designed re-*Part of this work was accomplished under supervison by Dr.
Hongyuan Zhu from A*STAR, Singapore.
†Corresponding author.
Two-stage methods
Post
Processing
Feature
Encoding
Object
Detection
Relation
Modelling
Caption
Generation
Vote2Cap-DETR
Transformer
Feature
Encoding
Decoder
Vote Query
Detection
Head
Caption
Head
The rolling office chair. The chair  is next to the square desk.
This is a gray chair. It is to the  right of a file cabinet.
⋮
The rolling office chair. The chair  is next to the square desk.
This is a gray chair. It is to the  right of a file cabinet.
⋮
Figure 1. Illustration of existing two-stage 3D dense captioning method (upper) and our Vote2Cap-DETR (bottom). Existing methods adopt a two-stage pipeline that heavily depends on a de-tector’s output. Therefore, we propose a transformer-based one-stage model, Vote2Cap-DETR, that frames 3D dense captioning as a set prediction problem. lation reasoning modules to model relations among object proposals efﬁciently. [48] introduces contextual information from two branches to improve the caption. 3DJCG [4] and
D3Net [7] study the correlation between 3D visual ground-ing and 3D dense captioning and point out that these two tasks promote each other. Additionally, χ-Trans2Cap [43] discusses how to transfer knowledge from additional 2d in-formation to boost 3d dense captioning.
Among existing methods, they all adopt a two-stage
“detect-then-describe” pipeline [4, 7, 13, 20, 39, 48] (Fig-ure 1). This pipeline ﬁrst generates a set of object pro-posals, then decodes each object by a caption generator with an explicit reasoning procedure. Though these meth-ods have achieved remarkable performance, the “detect-then-describe” pipeline suffers from the following issues: 1) Because of the serial and explicit reasoning, the cap-tioning performance highly depends on the object detection performance, which limits the mutual promotion of detec-tion and captioning. 2) The heavy reliance on hand-crafted components, e.g., radii, 3D operators, the deﬁnition of pro-posal neighbors, and post-processing (non-maximum sup-pression [28]) introduces additional hyper-parameters, lead-ing to a sub-optimal performance given the sparse object surfaces and cluttered object distributions among different indoor scenes. This inspires us to design a one-stage 3D dense captioning system.
To address the above issues, we propose Vote2Cap-DETR, a full transformer encoder-decoder architecture for one-stage 3D dense captioning. Unlike traditional “detect-then-describe” pipelines, we directly feed the decoder’s out-put into the localization head and caption head in paral-lel. By casting 3D dense captioning as a set-to-set problem, each target instance and its language annotation is matched with a query in a one-to-one correspondence manner, en-abling a more discriminative feature representation for pro-posals to identify each distinctive object in a 3D scene. Ad-ditionally, we also propose a novel vote query driven de-coder to introduce spatial bias for better localization of ob-jects in a cluttered 3D scene.
With fully attentional design, we resolve 3D dense cap-tioning with the following innovations: 1) Our method treats the 3D dense captioning task as a set prediction prob-lem. The proposed Vote2Cap-DETR directly decodes the features into object sets with their locations and correspond-ing captions by applying two parallel prediction heads. 2)
We propose a novel vote decoder by reformulating the ob-ject queries in 3DETR into the format of the vote query, which is a composition of the embeddings of the seeds point and the vote transformation with respect to the seeds.
This indicates the connection between the vote query in
Vote2Cap-DETR with the VoteNet, but with better local-ization and higher training efﬁciencies; 3) We develop a novel query driven caption head, which absorbs the rela-tion and attribute modeling into self- and cross-attention, so that it can look into both local and global contexts for better scene description. Extensive experiments on two commonly used datasets, ScanRefer and Nr3D, demonstrate that our approach surpasses prior arts with many hand-crafted pro-cedures by a large margin, which demonstrates the superi-ority that fully transformer architecture with sophisticated vote head and caption head can inspire many 3D vision and language tasks.
To summarize, the main contributions of this work in-clude:
• We propose a novel one-stage and fully attention driven architecture for 3D dense captioning as a set-to-set prediction problem, which achieves object local-ization and caption generation in parallel.
• Extensive experiments show that our proposed
Vote2Cap approach achieves a new state-of-the-art performance on both Nr3D [1] (45.53% C@0.5) and
ScanRefer [13] (73.77% C@0.5). 2.