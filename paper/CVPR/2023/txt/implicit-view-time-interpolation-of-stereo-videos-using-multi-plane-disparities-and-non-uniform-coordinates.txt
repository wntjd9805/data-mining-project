Abstract
In this paper, we propose an approach for view-time in-terpolation of stereo videos. Specifically, we build upon
X-Fields that approximates an interpolatable mapping be-tween the input coordinates and 2D RGB images using a convolutional decoder. Our main contribution is to ana-lyze and identify the sources of the problems with using X-Fields in our application and propose novel techniques to overcome these challenges. Specifically, we observe that X-Fields struggles to implicitly interpolate the disparities for large baseline cameras. Therefore, we propose multi-plane disparities to reduce the spatial distance of the objects in the stereo views. Moreover, we propose non-uniform time coor-dinates to handle the non-linear and sudden motion spikes in videos. We additionally introduce several simple, but im-portant, improvements over X-Fields. We demonstrate that our approach is able to produce better results than the state of the art, while running in near real-time rates and having low memory and storage costs. 1.

Introduction
As the virtual reality (VR) and light field displays (e.g.,
Lume Pad [26]) become widespread, there is a growing need for capturing the appropriate content for these devices to provide an immersive virtual experience for the users.
This necessitates capturing a scene from different views and at high frame rates. While this can be done using special-ized hardware [3], such setups are usually bulky and expen-sive, and thus not suitable for an average user. To make the content capture widespread, we should focus on standard capturing devices like cellphone cameras. These devices, however, are typically equipped with only two cameras and are often not able to capture two videos at high frame rates.
This necessitates interpolating across time and view to re-construct high frame rate videos from dense views.
For a system to be practical and can be deployed on dis-play devices with limited storage, memory, and computa-tional capability, it should have a few properties: 1) while a reasonable amount of post-processing can be done on a server, the approach should be able to generate results in real-time, 2) it should not have a significant storage over-head on top of the input stereo video, and 3) it should have a low memory cost.
Unfortunately, most existing approaches violate one or more criteria. For example, while the approaches based on multi-plane images (MPI) [47, 50, 55] can render novel views in real-time, they require storing the estimated MPIs for each frame (tens of megabytes per frame) and are ad-ditionally memory intensive. Moreover, to perform both view and time interpolations, these approaches need to be augmented with a video interpolation method which fur-ther adds to their memory and computational cost. The more recent approaches based on neural radiance fields (NeRF) [32] can perform both view and time interpola-tions [10, 29]. These methods encode the radiance field of a scene into a small network, and thus have a small storage and memory cost. However, they usually take a few sec-onds to render each novel view. Additionally, they require the cameras to be calibrated, and thus have difficulty han-dling general videos.
In this paper, we build upon X-Fields [2] that optimizes a coordinate-based network to learn an interpolatable im-plicit mapping between the input coordinates X (view or time) and the observed images. Once the optimization for a specific scene is performed, the network can be used to generate an image given any X coordinate. This approach satisfies all the properties, as the rendering is real-time (cri-terion 1) and the scene is encoded into a small network (cri-teria 2 and 3). Despite that, X-Fields struggles to produce reasonable results for the specific problem of view-time in-terpolation of stereo videos.
Our main contribution is to analyze X-Fields, identify the sources of the problems, and propose approaches that address these shortfalls. Specifically, we identify two major problems with X-Fields for stereo video interpolation. Our first observation is that X-Fields struggles to interpolate the disparities for cameras with large baselines. Second, we observe that linear motion in the input videos is critical for
X-Fields optimization to work, but non-linear motions are common in natural videos.
We address the first issue by proposing multi-plane dis-parities to reduce the spatial distance of the objects in the scene. Our approach makes it substantially easier for the network to interpolate the disparities, as the left and right disparities in different planes become closer to each other spatially. Moreover, we propose to address the second prob-lem through a novel non-uniform time coordinate encoding method. We demonstrate that the implicit network is able to find a proper mapping between these non-uniform coor-dinates and the observations and has a significantly better interpolation capability.
Additionally, we propose a series of simple, but impor-tant, improvements such as additional regularization losses, learned blending, and positional encoding. Our approach has low memory and storage costs, and is able to reconstruct novel images in near real-time rates. Through extensive ex-periments we demonstrate that our method is significantly better than the state-of-the-art approaches. Code and sup-plementary materials are available on our project website at https://people.engr.tamu.edu/nimak/Papers/CVPR23StereoVideo. 2.