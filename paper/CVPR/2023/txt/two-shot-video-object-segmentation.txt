Abstract
Previous works on video object segmentation (VOS) are trained on densely annotated videos. Nevertheless, ac-quiring annotations in pixel level is expensive and time-consuming. In this work, we demonstrate the feasibility of training a satisfactory VOS model on sparsely annotated videos—we merely require two labeled frames per train-ing video while the performance is sustained. We term this novel training paradigm as two-shot video object segmen-tation, or two-shot VOS for short. The underlying idea is to generate pseudo labels for unlabeled frames during train-ing and to optimize the model on the combination of la-beled and pseudo-labeled data. Our approach is extremely simple and can be applied to a majority of existing frame-works. We first pre-train a VOS model on sparsely an-notated videos in a semi-supervised manner, with the first frame always being a labeled one. Then, we adopt the pre-trained VOS model to generate pseudo labels for all un-labeled frames, which are subsequently stored in a pseudo-label bank. Finally, we retrain a VOS model on both labeled and pseudo-labeled data without any restrictions on the first frame. For the first time, we present a general way to train
VOS models on two-shot VOS datasets. By using 7.3% and 2.9% labeled data of YouTube-VOS and DAVIS benchmarks, our approach achieves comparable results in contrast to the counterparts trained on fully labeled set. Code and models are available at https://github.com/yk-pku/Two-shot-Video-Object-Segmentation. (a) Previous works on video object segmentation rely on densely annotated videos. We present two-shot video object segmentation, which merely ac-cesses two labeled frames per video. (b) Comparison among naive 2-shot STCN, STCN trained on full set and 2-shot STCN equipped with our approach on DAVIS 2016/2017 and
YouTube-VOS 2018/2019.
Figure 1. (a) Problem formulation. (b) Comparison among STCN variants on various datasets. 1.

Introduction
Video object segmentation (VOS), also known as mask tracking, aims to segment the target object in a video given the annotation of the reference (or first) frame. Existing approaches [7, 9, 21, 30, 37, 46, 52] are trained on densely annotated datasets such as DAVIS [33, 34] and YouTube-VOS [50]. However, acquiring dense annotations, partic-*Corresponding authors. ularly at the pixel level, is laborious and time-consuming.
For instance, the DAVIS benchmark consists of 60 videos, each with an average of 70 labeled frames; the YouTube-VOS dataset has an even larger amount of videos, and every fifth frame of each video is labeled to lower the annotation cost. It is necessary to develop data-efficient VOS models to reduce the dependency on labeled data.
In this work, we investigate the feasibility of training a
satisfactory VOS model on sparsely annotated videos. For the sake of convenience, we use the term N -shot to denote that N frames are annotated per training video. Note that 1-shot is meaningless since it degrades VOS to the task of image-level segmentation. We use STCN [9] as our base-line due to its simplicity and popularity. Since at least two labeled frames per video are required for VOS train-ing, we follow the common practice to optimize a naive 2-shot STCN model on the combination of YouTube-VOS and DAVIS, and evaluate on YouTube-VOS 2018/2019 and
DAVIS 2016/2017, respectively. We compare the native 2-shot STCN with its counterpart trained on full set in Fig. 1b.
Surprisingly, 2-shot STCN still achieves decent results, for instance, only a −2.1% performance drop is observed on
YouTube-VOS 2019 benchmark, demonstrating the practi-cality of 2-shot VOS.
So far, the wealth of information present in unlabeled
In the last decades, semi-frames is yet underexplored. supervised learning, which combines a small amount of labeled data with a large collection of unlabeled data dur-ing training, has achieved considerable success on vari-ous tasks such as image classification [3, 39], object detec-tion [40, 49] and semantic segmentation [14, 17].
In this work, we also adopt this learning paradigm to promote 2-shot VOS (see Fig. 1a). The underlying idea is to generate credible pseudo labels for unlabeled frames during training and to optimize the model on the combination of labeled and pseudo-labeled data. Here we continue to use STCN [9] as an example to illustrate our design principle, neverthe-less, our approach is compatible with most VOS models.
Concretely, STCN takes a randomly selected triplet of la-beled frames as input but the supervisions are only applied to the last two—VOS requires the annotation of the first frame as reference to segment the object of interest that ap-peared in subsequent frames. This motivates us to utilize the ground-truth for the first frame to avoid error propa-gation during early training. Each of the last two frames, nevertheless, can be either a labeled frame or an unlabeled frame with a high-quality pseudo label. Although the per-formance is improved with this straightforward paradigm, the capability of semi-supervised learning is still underex-plored due to the restriction of employing the ground truth as the starting frame. We term the process described above as phase-1.
To take full advantage of unlabeled data, we lift the re-striction placed on the starting frame, allowing it to be either a labeled or pseudo-labeled frame. To be specific, we adopt the VOS model trained in phase-1 to infer the unlabeled frames for pseudo-labeling. After that, each frame is as-sociated with a pseudo label that approximates the ground-truth. The generated pseudo labels are stored in a pseudo-label bank for the convenience of access. The VOS model is then retrained without any restrictions—similar to how it is trained through supervised learning, but each frame has either a ground-truth or a pseudo-label attached to it.
It is worth noting that, as training progresses, the predic-tions become more precise, yielding more reliable pseudo labels—we update the pseudo-label bank once we identify such pseudo labels. The above described process is named as phase-2. As shown in Fig. 1b, our approach assembled onto STCN, achieves comparable results (e.g. 85.2% v.s 85.1% on DAVIS 2017, and 82.7% v.s 82.7% on YouTube-VOS 2019) in contrast to its counterpart, STCN trained on full set, though our approach merely accesses 7.3% and 2.9% labeled data of YouTube-VOS and DAVIS bench-mark, respectively.
Our contributions can be summarized as follows:
• For the first time, we demonstrate the feasibility of two-shot video object segmentation: two labeled frames per video are almost sufficient for training a decent VOS model, even without the use of unlabeled data.
• We present a simple yet efficient training paradigm to exploit the wealth of information present in unlabeled frames. This novel paradigm can be seamlessly ap-plied to various VOS models, e.g., STCN [9], RDE-VOS [21] and XMem [7] in our experiments.
• Though we only access a small amount of labeled data (e.g. 7.3% for YouTube-VOS and 2.9% for DAVIS), our approach still achieves competitive results in con-For to the counterparts trained on full set. trast example, 2-shot STCN equipped with our approach achieves 85.1%/82.7% on DAVIS 2017/YouTube-VOS 2019, which is +4.1%/+2.1% higher than the naive 2-shot STCN while -0.1%/-0.0% lower than the STCN trained on full set. 2.