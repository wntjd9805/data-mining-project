Abstract
Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the gener-ation quality or enhance the model generalization. How-ever, there are few works able to address both issues simul-taneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturally
∗Corresponding author generalized across different identities without further fine-tuning. Additionally, our DiffTalk can be gracefully tai-lored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identi-ties. For more video results, please refer to https:// sstzal.github.io/DiffTalk/. 1.

Introduction
Talking head synthesis is a challenging and promising research topic, which aims to generate video portraits with given audio. This technique is widely applied in various practical scenarios including animation, virtual avatars, on-line education, and video conferencing [4, 45, 48, 51, 54].
Recently a lot of effort has been devoted to this re-search area to improve the generation quality or enhance the model generalization. Among these existing main-stream talking head generation approaches, the 2D-based methods usually depend on generative adversarial networks (GANs) [6, 10, 16, 23, 29] for audio-to-lip mapping, and most of them perform competently on model generalization.
However, since GANs need to simultaneously optimize a generator and a discriminator, the training process lacks sta-bility and is prone to mode collapse [11]. Due to this re-striction, the generated talking videos are of limited image quality, and difficult to scale to higher resolutions. By con-trast, 3D-based methods [2, 17, 43, 47, 55] perform better in synthesizing higher-quality talking videos. Whereas, they highly rely on identity-specific training, and thus cannot generalize across different persons. Such identity-specific training also brings heavy resource consumption and is not friendly to practical applications. Most recently, there are some 3D-based works [37] that take a step towards improv-ing the generalization of the model. However, further fine-tuning on specific identities is still inevitable.
Generation quality and model generalization are two es-sential factors for better deployment of the talking head syn-thesis technique to real-world applications. However, few existing works are able to address both issues well. In this paper, we propose a crafted conditional Diffusion model for generalized Talking head synthesis (DiffTalk), that aims to tackle these two challenges simultaneously. Specifically, to avoid the unstable training of GANs, we turn attention to the recently developed generative technology Latent Dif-fusion Models [31], and model the talking head synthe-sis as an audio-driven temporally coherent denoising pro-cess. On this basis, instead of utilizing audio signals as the single driving factor to learn the audio-to-lip transla-tion, we further incorporate reference face images and land-marks as supplementary conditions to guide the face iden-tity and head pose for personality-aware video synthesis.
Under these designs, the talking head generation process is more controllable, which enables the learned model to naturally generalize across different identities without fur-ther fine-tuning. As shown in Figure 1, with a sequence of driven audio, our DiffTalk is capable of producing natu-ral talking videos of different identities based on the corre-sponding reference videos. Moreover, benefiting from the latent space learning mode, our DiffTalk can be gracefully tailored for higher-resolution synthesis with negligible ex-tra computational cost, which is meaningful for improving the generation quality.
Extensive experiments show that our DiffTalk can syn-thesize high-fidelity talking videos for novel identities with-out any further fine-tuning. Figure 1 shows the generated talking sequences with one driven audio across three differ-ent identities. Comprehensive method comparisons show the superiority of the proposed DiffTalk, which provides a strong baseline for the high-performance talking head syn-thesis. To summarize, we make the following contributions:
• We propose a crafted conditional diffusion model for high-quality and generalized talking head synthesis. By introducing smooth audio signals as a condition, we model the generation as an audio-driven temporally co-herent denoising process.
• For personality-aware generalized synthesis, we further incorporate dual reference images as conditions. In this way, the trained model can be generalized across different identities without further fine-tuning.
• The proposed DiffTalk can generate high-fidelity and vivid talking videos for generalized identities. In exper-iment, our DiffTalk significantly outperforms 2D-based methods in the generated image quality, while surpassing 3D-based works in the model generalization ability. 2.