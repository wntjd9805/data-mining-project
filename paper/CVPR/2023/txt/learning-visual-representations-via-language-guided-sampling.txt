Abstract
Although an object may appear in numerous contexts, we often describe it in a limited number of ways. Language al-lows us to abstract away visual variation to represent and communicate concepts. Building on this intuition, we pro-pose an alternative approach to visual representation learn-ing: using language similarity to sample semantically sim-ilar image pairs for contrastive learning. Our approach diverges from image-based contrastive learning by sam-pling view pairs using language similarity instead of hand-crafted augmentations or learned clusters. Our approach also differs from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than directly minimizing a cross-modal loss. Through a se-ries of experiments, we show that language-guided learning yields better features than image-based and image-text rep-resentation learning approaches. 1.

Introduction
Consider the images in Fig. 1, is the center image more similar to its left or right neighbor? Despite the difference in background and pose, it is clear that the right pair cap-tures the same concept: a ﬂying snow owl. Nevertheless, a self-supervised image model will judge the left pair as more similar. Human perception and language abstract away ap-pearance differences to capture conceptual similarity rather than just visual similarity.
Ideally, we could learn visual features that capture conceptual similarity and generalize effectively to other visual tasks. In this work, we show how language can be a proxy for conceptual similarity; allowing us to sample better pairs for contrastive learning and train more generalizable visual models.
Image-only contrastive learning uses visual similarity as a proxy for conceptual similarity. This is based on the ob-servation that discriminative approaches can discover inter-class similarity–e.g., cheetahs are similar to lions– without requiring explicit annotations [106]. The core idea is to train a discriminative model where each instance is treated as a separate class, and the model is trained to map augmented
Visual Embedding
ℒ
Late after post  thunderstorm,  north Ohio
Snowy owl  lifting off
Snow owl  taking off
Language Embedding
Figure 1. Language allows us to ﬁnd conceptually similar image pairs even if they are visually dissimilar. We use those pairs for contrastive learning to learn generalizable visual features. views of the same image to similar features [12–15, 106].
While successful, instance discrimination ignores the simi-larity between different instances as it assumes all other im-ages are unrelated. Later work focused on inter-image rela-tionships by estimating clusters [3, 9, 10] or ﬁnding nearest neighbors [28]. However, those relationships are estimated using visual embeddings; resulting in visually, rather than conceptually, similar pairs.
Language similarity is a strong proxy for semantic re-lationships. Consider the example in Fig. 1; images that depict the same concept are often described similarly. Rad-ford et al. [76] propose language-image contrastive learn-ing by mapping images and text to a shared representa-tion space and achieve impressive generalization capabili-ties. However, it is unclear whether forcing models to map onto a shared space is optimal for visual learning. Although linguistic and visual similarity might align for similar in-stances, it is unclear whether all distances in one space should map exactly to the other. Instead of learning a joint vision-and-language representations, we argue that it is bet-ter to use linguistic similarity to guide visual learning.
To this end, we propose language-guided contrastive learning: a simple adaptation to contrastive learning that uses language models to ﬁnd conceptually-similar image pairs for visual learning. Our approach is motivated by the observation that language models, despite never train-ing on visual data, can still be used to sample caption pairs that belong to conceptually similar images, as seen in Fig. 2. Such sampled images exhibit desirable varia-tions in pose, lightning, and context which are very dif-ferent from hand-crafted augmentations which can be ill-suited to downstream tasks [108] or too focused on back-ground textures [81]. We use the sampled pairs instead of image augmentations within standard self-supervised visual learning approaches such as SimCLR [12], SimSiam [15], and SLIP [67]. Our approach departs from image-only con-trastive learning by relying on conceptually-similar image pairs rather than visually similar augmentations or cluster-assignment. We also depart from image-text pre-training by allowing the model to be guided by language similarity rather than learning a joint embedding space.
We conduct a series of controlled experiments to ana-lyze our approach and compare it to commonly used rep-resentation learning paradigms on generalization to down-stream classiﬁcation tasks. In controlled settings, our ap-proach outperforms all baselines on linear probe and few-shot classiﬁcation on a range of downstream classiﬁcation datasets. Our analysis suggests that while learning multi-modal joint embeddings can result in good representations, it is better to use one modality to guide the training of the other. Furthermore, we ﬁnd that our approach is ro-bust to the speciﬁc choice of sampling strategy or language model. Our code and pre-trained models are available at https://github.com/mbanani/lgssl. 2.