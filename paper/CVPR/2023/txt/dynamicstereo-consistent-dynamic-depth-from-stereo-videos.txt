Abstract 1.

Introduction
We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing meth-ods for depth from stereo treat different stereo frames in-dependently, leading to temporally inconsistent depth pre-dictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly di-minishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate dispar-ity for stereo videos. The network learns to pool informa-tion from neighboring frames to improve the temporal con-sistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new bench-mark dataset containing synthetic videos of people and ani-mals in scanned environments, which provides complemen-tary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves the quality of predictions of our proposed DynamicStereo as well as prior methods. Fi-nally, it acts as a benchmark for consistent stereo methods.
Project page: https://dynamic-stereo.github.io/
Estimating depth from stereo is a fundamental computer vision problem, with applications in 3D reconstruction, robot navigation, and human motion capture, among oth-ers. With the advent of consumer devices featuring multiple cameras, such as AR glasses and smartphones, stereo can simplify the 3D reconstruction of everyday scenes, extract-ing them as content to be experienced in virtual or mixed reality, or for mixed reality pass-through.
Depth from stereo takes as input two images capturing the same scene from different viewpoints. It then finds pairs of matching points, a problem known as disparity estima-tion. Since the two cameras are calibrated, the matched points can be projected into 3D using triangulation. While this process is robust, it is suboptimal when applied to video data, as it can only reconstruct stereo frames individually, ignoring the fact that the observations infer properties of the same underlying objects over time. Even if the camera moves or the scene deforms non-rigidly, the instantaneous 3D reconstructions are highly correlated and disregarding this fact can result in inconsistencies.
In this paper, we thus consider the problem of dynamic
depth from stereo to improve the temporal consistency of stereo reconstruction from video data.
Traditional approaches to stereo compute the matching costs between local image patches, aggregating those in an objective function, and optimizing the latter together with a regularization term to infer disparities. Examples of such approaches include max-flow [35] and graph-cut [15]. More recently, stereo methods have used deep networks learned from a large number of image pairs annotated with ground-truth disparities [12, 14, 18, 23]. They usually follow an ap-proach similar to the traditional methods, but using deep
CNN features for computing the matching costs, and replac-ing the per-image optimization by a pre-trained regression deep network, which processes the cost volume and outputs the estimated disparities.
In the video setting, matching quality can potentially be improved by looking for matches across space and time.
For instance, points occluded in one camera at a given point in time may be visible from both cameras at other times.
Transformer architectures have shown that attention can be a powerful and flexible method for pooling information over a range of contexts [6, 8, 9, 42]. Our DynamicStereo model incorporates self- and cross-attention to extract rel-evant information across space, time and stereo pairs. Our architecture relies on divided attention [3] to allow efficient processing of this high-dimensional space.
As a learning-based approach, we wish to learn priors from data representative of real-life 3D dynamic reconstruc-tion applications, where videos depict people or animals moving and interacting with objects. There are several syn-thetic video datasets [10,27,41] commonly used for training stereo and optical flow methods, but they contain abstract scenes with several layers of moving objects that share lit-tle resemblance to real-life. More realistic stereo datasets also exist [45, 49], but they either do not contain video se-quences or are focused on static scenes. Given these lim-itations, as an additional contribution we propose a new synthetic stereo dataset showing moving human and animal characters inside realistic physical spaces from the Replica dataset [40]. We call this new dataset Dynamic Replica (DR), and we use it for learning dynamic stereo matches.
DR contains 524 videos of virtual humans and animals em-bedded in realistic digital scans of physical environments (see Tab. 1 and Fig. 2). We show that DR can significantly boost the quality of dynamic stereo methods compared to training them only on existing depth-from-stereo datasets.
To summarise, we make three contributions. (1) We introduce DynamicStereo, a transformer-based architecture that improves dynamic depth from stereo by jointly process-ing stereo videos. (2) We release Dynamic Replica, a new benchmark dataset for learning and evaluating models for dynamic depth from stereo. (3) We demonstrate state-of-the-art dynamic stereo results in a variety of benchmarks.
Figure 2. Example frames from depth-from-stereo datasets.
We visually compare current datasets to Dynamic Replica, which contains renderings of every-day scenes with people and animals and differs from existing datasets in size, realism, and content. 2.