Abstract
In this paper, we study masked autoencoder (MAE) pre-training on videos for matching-based downstream tasks, including visual object tracking (VOT) and video object seg-mentation (VOS). A simple extension of MAE is to randomly mask out frame patches in videos and reconstruct the frame pixels. However, we find that this simple baseline heav-ily relies on spatial cues while ignoring temporal relations for frame reconstruction, thus leading to sub-optimal tem-poral matching representations for VOT and VOS. To al-leviate this problem, we propose DropMAE, which adap-tively performs spatial-attention dropout in the frame re-construction to facilitate temporal correspondence learning in videos. We show that our DropMAE is a strong and effi-cient temporal matching learner, which achieves better fine-tuning results on matching-based tasks than the ImageNet-based MAE with 2× faster pre-training speed. Moreover, we also find that motion diversity in pre-training videos is more important than scene diversity for improving the performance on VOT and VOS. Our pre-trained DropMAE model can be directly loaded in existing ViT-based track-ers for fine-tuning without further modifications. Notably,
DropMAE sets new state-of-the-art performance on 8 out of 9 highly competitive video tracking and segmentation datasets. Our code and pre-trained models are available at https://github.com/jimmy-dq/DropMAE.git. 1.

Introduction
Recently, transformers have achieved enormous success in many research areas, such as natural language processing (NLP) [6, 22], computer vision [97] and audio generation
[43, 73]. In NLP, masked autoencoding is commonly used to train large-scale generalizable NLP transformers contain-*Corresponding Author
Figure 1. State-of-the-art comparison on GOT-10k [39] following the one-shot protocol. Our DropTrack with the proposed Drop-MAE pre-training achieves state-of-the-art performance without using complicated pipelines such as online updating. ing billions of parameters. Inspired by the great success of self-supervised learning in NLP, recent advances [36, 89] in computer vision suggest that training large-scale vision transformers may undertake a similar trajectory with NLP.
The seminal work MAE [36] reconstructs the input image from a small portion of patches. The learned representa-tion in this masked autoencoder has been demonstrated to be effective in many computer vision tasks, such as image classification, object detection and semantic segmentation.
In video object tracking (VOT), recently two works,
SimTrack [10] and OSTrack [97], explore using an MAE pre-trained ViT model as the tracking backbone. Notably, these two trackers achieve state-of-the-art performance on existing tracking benchmarks without using complicated tracking pipelines. The key to their success is the robust pre-training weights learned by MAE on ImageNet [68]. In addition, [10, 97] also demonstrate that, for VOT, MAE un-supervised pre-training on ImageNet is more effective than supervised pre-training using class labels – this is mainly because MAE pre-training learns more fine-grained local
the pre-training stage. Interestingly, we obtain several im-portant findings with DropMAE: 1) DropMAE is an effec-tive and efficient temporal correspondence learner, which achieves better fine-tuning results on matching-based tasks than the ImageNet-based MAE with 2× faster pre-training speed. 2) Motion diversity in pre-training videos is more important than scene diversity for improving the perfor-mance on VOT and VOS.
We conduct downstream task evaluation on 9 competi-tive VOT and VOS benchmarks, achieving state-of-the-art performance on these benchmarks. In particular, our track-ers with DropMAE pre-training obtain 75.9% AO on GOT-10k, 52.7% AUC on LaSOText, 56.9% AUC on TNL2K and 92.1%/83.0% J &F scores on DAVIS-16/17, w/o us-ing complicated online updating or memory mechanisms.
In summary, the main contributions of our work are:
• To the best of our knowledge, we are the first to investi-gate masked autoencoder video pre-training for tempo-ral matching-based downstream tasks. Specifically, we explore various video data sources for pre-training and build a TwinMAE baseline to study its effectiveness on temporal matching tasks. Since none exists, we further build a ViT-based VOS baseline for fine-tuning.
• We propose DropMAE, which adaptively performs spatial-attention dropout in the frame reconstruction to facilitate effective temporal correspondence learn-ing in videos.
• Our trackers with DropMAE pre-training sets new state-of-the-art performance on 8 out of 9 highly com-petitive video tracking and segmentation benchmarks without complicated tracking pipelines. 2.