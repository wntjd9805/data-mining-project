Abstract
This paper simultaneously addresses three limitations associated with conventional skeleton-based action recog-nition; skeleton detection and tracking errors, poor va-riety of the targeted actions, as well as person-wise and frame-wise action recognition. A point cloud deep-learning paradigm is introduced to the action recognition, and a uni-fied framework along with a novel deep neural network ar-chitecture called Structured Keypoint Pooling is proposed.
The proposed method sparsely aggregates keypoint features in a cascaded manner based on prior knowledge of the data structure (which is inherent in skeletons), such as the instances and frames to which each keypoint belongs, and achieves robustness against input errors. Its less con-strained and tracking-free architecture enables time-series keypoints consisting of human skeletons and nonhuman ob-ject contours to be efficiently treated as an input 3D point cloud and extends the variety of the targeted action. Fur-thermore, we propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This trick switches the
* Equal contribution. pooling kernels between the training and inference phases to detect person-wise and frame-wise actions in a weakly supervised manner using only video-level action labels.
This trick enables our training scheme to naturally intro-duce novel data augmentation, which mixes multiple point clouds extracted from different videos. In the experiments, we comprehensively verify the effectiveness of the pro-posed method against the limitations, and the method out-performs state-of-the-art skeleton-based action recognition and spatio-temporal action localization methods. 1.

Introduction
Recognizing the actions of a person in a video plays an essential role in various applications such as robotics [28, 41] and surveillance cameras [11, 25, 49]. The approach to the action recognition task differs depending on whether leveraging appearance information in a video or human skeletons1 detected in the video. The former appearance-based approaches [2,7,11,18,20–23,25,32,45,51,52,56,58] directly use video as an input to deep neural networks 1Joints or keypoints specific to a person are referred to as skeletons for clarity, although some are not actual human joints.
(DNNs) and thus even can recognize actions with relatively small movements. However, they are less robust to appear-ances of the people or scenes that differ from the training data [34, 55]. On the other hand, the latter skeleton-based approaches [5,9,10,13,17,29,33,34,49,57,60] are relatively robust to such appearance changes of a scene or a person be-cause they only input low-information keypoints detected using the multi-person pose estimation methods [6, 42, 50].
Starting from ST-GCN [57], various skeleton-based ap-proaches employing graph convolutional networks (GCNs) have emerged [5, 9, 10, 13, 33, 44]. These approaches model the relationship among keypoints by densely connecting them in a spatio-temporal space using GCNs, which treat every keypoint as a node at each time step. However, most approaches exhibit low scalability in practical scenarios, and further performance improvement is required since they exhibit three limitations regarding network architectures or their problem settings, as described below.
Skeleton Detection and Tracking Errors. Conventional
GCN-based methods heavily rely on dense graphs, whose node keypoints are accurately detected and grouped by the same instance. These methods assume that the DNN fea-tures are correctly propagated. Therefore, if false positives (FPs) or false negatives (FNs) occur during keypoint detec-tion, or if the multi-person pose tracking [39, 47] fails, such assumptions no longer hold, and the action recognition ac-curacy is degraded [17, 62].
Poor Variety of the Targeted Actions. Conventional ap-proaches limit the number of input skeletons to at most one or two. Therefore, the recognition of actions performed by many people or those interacting with nonhuman objects is an ill-posed problem. On the other hand, for a wide range of applications, it is desirable to eliminate such restrictions and target a variety of action categories.
Person-wise and Frame-wise Action Recognition. Con-ventional approaches classify an entire video into actions, while practical scenes are complex and include multiple persons performing different actions in different time win-dows. Hence, recognizing each person’s action for each frame (spatio-temporal action localization) is necessary.
In this paper, a unified action recognition framework and a novel DNN architecture called Structured Keypoint Pool-ing, which enhances the applicability and scalability of the skeleton-based action recognition (see Fig. 1), is proposed to simultaneously address the above three limitations. Un-like previous methods, which concatenate the keypoint co-ordinates and input them into a DNN designed on a pre-defined graph structure of a skeleton, the proposed method introduces a point cloud deep-learning paradigm [37,38,61] to the action recognition and treats a set of keypoints as an input 3D point cloud. PointNet [37], which was proposed in such a paradigm, is an innovative research, whose output is permutation-invariant to the order of the input points. It extracts the features for each input point and sparsely aggre-gates them to the output feature vector using Max-Pooling.
Unlike PointNet, the proposed network architecture aggre-gates the features extracted from the point cloud in a cas-caded manner based on prior knowledge of the data struc-ture, which is inherent in the point cloud, such as the frames or the detection results of the persons (instances) to which each keypoint belongs. As a result, it is less constrained than conventional approaches and tracking-free. Also, its feature propagation among keypoints is relatively sparse.
Therefore, the range of the DNNs affected by the keypoint errors (e.g., FPs, FNs, and tracking errors) associated with the first robustness limitation can also be limited.
In addition, the permutation-invariant property of the in-put in the proposed network architecture eliminates the con-straints of the data structure and size (e.g., number of in-stances and pose tracking) found in the GCN-based meth-ods. This property is exploited, and the nonhuman object keypoints2 defined on the contour of the objects are used as an input in addition to human skeletons. Thus, the sec-ond target-action limitation mentioned above is addressed by increasing the input information without relying on the appearances while avoiding overfitting on them [14, 34,55].
Finally, the third multi-action limitation is addressed by extending the proposed network architecture concept to a weakly supervised spatio-temporal action localiza-tion, which only requires a video-level action label dur-ing training. This is achieved using the proposed Pooling-Switching Trick inspired by Structured Keypoint Pooling, which switches the pooling structures according to the training and inference phases. Furthermore, this pooling-switching technique naturally enables the proposed training scheme to introduce novel data augmentation, which mixes multiple point clouds extracted from different videos.
In summary, our main contributions are three-fold: (1)
We propose Structured Keypoint Pooling based on point cloud deep-learning in the context of action recognition.
This method incorporates prior knowledge of the data struc-ture to which each keypoint belongs into a DNN architec-ture as an inductive bias using a simple Max-Pooling oper-ation. (2) In addition to the human skeletons, object key-points are introduced as an additional input for skeleton-based action recognition. (3) A skeleton-based, weakly su-pervised spatio-temporal action localization is achieved by introducing a Pooling-Switching Trick, which exploits the feature aggregation scheme of Structured Keypoint Pooling. 2.