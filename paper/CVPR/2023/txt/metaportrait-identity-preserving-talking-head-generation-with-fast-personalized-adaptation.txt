Abstract
In this work, we propose an ID-preserving talking head generation framework, which advances previous methods in two aspects. First, as opposed to interpolating from sparse
ﬂow, we claim that dense landmarks are crucial to achiev-ing accurate geometry-aware ﬂow ﬁelds. Second, inspired by face-swapping methods, we adaptively fuse the source identity during synthesis, so that the network better pre-serves the key characteristics of the image portrait. Al-though the proposed model surpasses prior generation ﬁ-delity on established benchmarks, personalized ﬁne-tuning is still needed to further make the talking head generation qualiﬁed for real usage. However, this process is rather computationally demanding that is unaffordable to stan-dard users. To alleviate this, we propose a fast adaptation model using a meta-learning approach. The learned model can be adapted to a high-quality personalized model as fast as 30 seconds. Last but not least, a spatial-temporal en-hancement module is proposed to improve the ﬁne details while ensuring temporal coherency. Extensive experiments
*Equal contribution, interns at Microsoft Research.
†Joint corresponding authors. prove the signiﬁcant superiority of our approach over the state of the arts in both one-shot and personalized settings. 1.

Introduction
Talking head generation [1, 6, 7, 24, 27, 31, 33, 39, 41, 47] has found extensive applications in face-to-face live chat, virtual reality and virtual avatars in games and videos. In this paper, we aim to synthesize a realistic talking head with a single source image (one-shot) that provides the appear-ance of a given person while being animatable according to the motion of the driving person. Recently, considerable progress has been made with neural rendering techniques, bypassing the sophisticated 3D human modeling process and expensive driving sensors. While these works attain increasing ﬁdelity and higher rendering resolution, identity preserving remains a challenging issue since the human vi-sion system is particularly sensitive to any nuanced devia-tion from the person’s facial geometry.
Prior arts mainly focus on learning a geometry-aware warping ﬁeld, either by interpolating from sparse 2D/3D landmarks or leveraging 3D face prior, e.g., 3D morphable
face model (3DMM) [2, 3]. However, ﬁne-grained facial geometry may not be well described by a set of sparse land-marks or inaccurate face reconstruction. Indeed, the warp-ing ﬁeld, trained in a self-supervised manner rather than us-ing accurate ﬂow ground truth, can only model coarse ge-ometry deformation, lacking the expressivity that captures the subtle semantic characteristics of the portrait.
In this paper, we propose to better preserve the portrait identity in two ways. First, we claim that dense facial land-marks are sufﬁcient for an accurate warping ﬁeld prediction without the need for local afﬁne transformation. Speciﬁ-cally, we adopt a landmark prediction model [43] trained on synthetic data [42], yielding 669 head landmarks that of-fer signiﬁcantly richer information on facial geometry. In addition, we build upon the face-swapping approach [23] and propose to enhance the perceptual identity by atten-tionally fusing the identity feature of the source portrait while retaining the pose and expression of the intermedi-ate warping. Equipped with these two improvements, our one-shot model demonstrates a signiﬁcant advantage over prior works in terms of both image quality and perceptual identity preservation when animating in-the-wild portraits.
While our one-shot talking head model has achieved state-of-the-art quality, it is still infeasible to guarantee sat-isfactory synthesis results because such a one-shot setting is inherently ill-posed—one may never hallucinate the person-speciﬁc facial shape and occluded content from a single photo. Hence, ultimately we encounter the uncanny val-ley [32] that a user becomes uncomfortable as the synthe-sis results approach to realism. To circumvent this, one workaround is to ﬁnetune the model using several minutes of a personal video. Such personalized training has been widely adopted in industry to ensure product-level quality, yet this process is computationally expensive, which greatly limits its use scenarios. Thus, speeding up this personal-ized training, a task previously under-explored, is of great signiﬁcance to the application of talking head synthesis.
We propose to achieve fast personalization with meta-learning. The key idea is to ﬁnd an initialization model that can be easily adapted to a given identity with limited training iterations. To this end, we resort to a meta-learning approach [9, 26] that ﬁnds success in quickly learning dis-criminative tasks, yet is rarely explored in generative tasks.
Speciﬁcally, we optimize the model for speciﬁc personal
In this way, we get a slightly data with a few iterations.
ﬁne-tuned personal model towards which we move the ini-tialization model weight a little bit. Such meta-learned ini-tialization allows us to train a personal model within 30 sec-onds, which is 3 times faster than a vanilla pretrained model while requiring less amount of personal data.
Moreover, we propose a novel temporal super-resolution network to enhance the resolution of the generated talking head video. To do this, we leverage the generative prior to boost the high-frequency details for portraits and mean-while take into account adjacent frames that are helpful to reduce temporal ﬂickering. Finally, we reach temporally coherent video results of 512 × 512 resolution with com-pelling facial details. In summary, this work innovates in the following aspects:
• We propose a carefully designed framework to signif-icantly improve the identity-preserving capability when animating a one-shot in-the-wild portrait.
• To the best of our knowledge, we are the ﬁrst to explore meta-learning to accelerate personalized training, thus obtaining ultra-high-quality results at affordable cost.
• Our novel video super-resolution model effectively en-hances details without introducing temporal ﬂickering. 2.