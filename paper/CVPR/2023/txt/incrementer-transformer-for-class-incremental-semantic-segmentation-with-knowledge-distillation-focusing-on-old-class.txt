Abstract
Class-incremental semantic segmentation aims to incre-mentally learn new classes while maintaining the capabil-ity to segment old ones, and suffers catastrophic forgetting since the old-class labels are unavailable. Most existing methods are based on convolutional networks and prevent forgetting through knowledge distillation, which (1) need to add additional convolutional layers to predict new classes, and (2) ignore to distinguish different regions correspond-ing to old and new classes during knowledge distillation and roughly distill all the features, thus limiting the learning of new classes. Based on the above observations, we pro-pose a new transformer framework for class-incremental semantic segmentation, dubbed Incrementer, which only needs to add new class tokens to the transformer decoder for new-class learning. Based on the Incrementer, we pro-pose a new knowledge distillation scheme that focuses on the distillation in the old-class regions, which reduces the constraints of the old model on the new-class learning, thus improving the plasticity. Moreover, we propose a class de-confusion strategy to alleviate the overﬁtting to new classes and the confusion of similar classes. Our method is sim-ple and effective, and extensive experiments show that our method outperforms the SOTAs by a large margin (5∼15 absolute points boosts on both Pascal VOC and ADE20k).
We hope that our Incrementer can serve as a new strong pipeline for class-incremental semantic segmentation. 1.

Introduction
Semantic segmentation [4, 5, 36, 41] is one of the fun-damental tasks in the ﬁeld of computer vision, which aims to classify each pixel in an image and assign a class label.
Traditional semantic segmentation networks are trained on datasets where labels for all classes are available simulta-neously. However, in practical applications, a more realis-∗Corresponding authors.
Figure 1. Class-incremental learning process based on trans-former, assuming that each step contains one new class, so each step adds one new class token to the decoder. tic situation is that the network needs to continuously learn new classes, while the data containing labels of old classes is not available due to privacy or legal reasons. If the old model is ﬁne-tuned directly on the new data, the network will ﬁt the new data and forget the old knowledge learned before, resulting in catastrophic forgetting [14, 24]. Thus, this problem leads to a challenging task referred as Class-Incremental Semantic Segmentation [2, 8, 28]. The exist-ing methods [2, 8, 32, 48] for this task typically use fully-convolutional network [4, 5] as the basic framework, which uses a CNN backbone as an encoder to extract image fea-tures, and then generates segmentation predictions through a convolutional decoder. To prevent catastrophic forgetting, most existing methods [8, 32, 45, 48] preserve the learned old knowledge through knowledge distillation.
Although these methods [8, 25, 48] have achieved re-markable performance, there are still some limitations.
Firstly, global context information is critical for accurate se-mantic segmentation, while convolution kernel with the lo-cal view is difﬁcult to capture global information. More, in the incremental learning, existing convolution-based meth-ods need to add additional convolution layers into the de-coder to predict new classes, and generate segmentations of classes in different steps through different decoding layers, which is inefﬁcient with the increasing number of learning
tasks. Secondly, for knowledge distillation, existing meth-ods [8,25,28,45] regard the features generated by the model as a whole and neglect to distinguish the different regions corresponding to the old and new classes. The old model has no ability to discriminate the new-class regions, so it classiﬁes the new classes as background.
If the features of all regions are distilled using the old model without dis-crimination, it will be difﬁcult for the current model to learn more discriminative feature representations for new classes, thus limiting the plasticity of the model.
To address the above problems, we propose a new transformer-based framework for class-incremental seman-tic segmentation, dubbed Incrementer, which is a new struc-tural paradigm with both high performance and high ef-ﬁciency. Speciﬁcally, we ﬁrst adopt a vision transformer
[7, 39] as the encoder to extract visual features that cap-ture more global contextual information based on the self-attention mechanism. Next, for the decoder, inspired by
[36], we assign a class token to each class, and then input the class tokens into a transformer decoder jointly with the patch-wise visual features generated from the encoder, so as to generate corresponding visual embeddings and class em-beddings for ﬁnal segmentation predictions. In incremental learning, our method only needs to add new class tokens for the new classes to the transformer decoder. As shown in Fig. 1, assuming that each step of incremental learning contains one new class, we add a new class token in each step, and the segmentation predictions of both old and new classes can be output in parallel, without adding additional network structures like the CNN-based methods, which im-proves the efﬁciency of incremental learning.
Based on the above transformer framework, we propose a new knowledge distillation scheme that only focuses on old classes (FOD). Different from the previous distillation methods that do not distinguish different regions of old and new classes, we separate the image into old and new class regions, and only distill the features corresponding to the old-class regions at both local and global levels. Our distil-lation scheme not only preserves the capability of the model on the old classes, but also reduces the constraints of the old model on the current model to learn new-class features, thereby improving both the stability and plasticity.
Moreover, we observe that the model overﬁts new classes when the incremental data contains a small number of new classes and confuses similar old and new classes.
Therefore, we further propose a class deconfusion strategy (CDS) to balance the learning of the old and new classes, which reduces the learning weight of the new class, and uses an old-new binary mask to aggregate the scattered old classes in the process of learning new classes, thus alleviat-ing the overﬁtting to new classes and improving the model’s ability to distinguish similar classes.
Our contributions are summarized as follows:
• Structurally, we propose a new transformer-based pipeline for class-incremental semantic segmentation named Incrementer, which is a simple and efﬁcient framework that not only achieves higher accuracy, but also is convenient to implement incremental learning.
• Methodologically, we propose a novel knowledge dis-tillation scheme FOD that focuses on the distillation of the old-class features to improve both stability and plasticity. And we further propose a class deconfusion strategy CDS to alleviate the model’s overﬁtting to new classes and the confusion of similar classes.
• We conduct extensive experiments on Pascal VOC and
ADE20k, and the results show that our Incrementer signiﬁcantly outperforms the state-of-the-art methods. 2.