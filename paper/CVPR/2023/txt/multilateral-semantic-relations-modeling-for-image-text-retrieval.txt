Abstract
Image-text retrieval is a fundamental task to bridge vi-sion and language by exploiting various strategies to ﬁne-grained alignment between regions and words. This is still tough mainly because of one-to-many correspondence, where a set of matches from another modality can be ac-cessed by a random query. While existing solutions to this problem including multi-point mapping, probabilistic distribution, and geometric embedding have made promis-ing progress, one-to-many correspondence is still under-explored. In this work, we develop a Multilateral Semantic
Relations Modeling (termed MSRM) for image-text re-trieval to capture the one-to-many correspondence between multiple samples and a given query via hypergraph model-ing. Speciﬁcally, a given query is ﬁrst mapped as a prob-abilistic embedding to learn its true semantic distribution based on Mahalanobis distance. Then each candidate in-stance in a mini-batch is regarded as a hypergraph node with its mean semantics while a Gaussian query is mod-eled as a hyperedge to capture the semantic correlations beyond the pair between candidate points and the query.
Comprehensive experimental results on two widely used datasets demonstrate that our MSRM method can outper-form state-of-the-art methods in the settlement of multi-ple matches while still maintaining the comparable perfor-mance of instance-level matching. 1.

Introduction
Image and text are two important information carriers to help human and intelligent agents to better understand the real world. Many explorations [9, 18, 35] have been con-ducted in the computer vision as well as natural language processing domains to bridge these two modalities [16]. As a fundamental yet challenging topic in this research, image-*This work was supported by the Sichuan Science and Technology
Program, China (2023YFG0289), National Natural Science Foundation of
China (62020106008, 62220106008, and U20B2063), and the Guangdong
Basic and Applied Basic Research Foundation (2022A1515110576).
†Corresponding author, dlyyang@gmail.com.
Figure 1. Examples of one-to-many correspondence caused by the inherent nature of different modalities. The existing point-to-point mapping can not capture the semantic richness of data. text retrieval can beneﬁt other vision-language tasks [11] in two ways, e.g. images search for a given sentence and the retrieval of descriptions for an image query, and spread to a variety of applications, such as person search [48], sketch-based image retrieval [33], and food recipe retrieval [52].
Due to the power of deep metric learning [30, 31] in vi-sual embedding augmentation, its core idea is intuitively extended into image-text retrieval to consider the domain differences. The naive strategy [3, 8, 38] is based on triplet loss to learn distinctive representations at the global level only with the help of positive pair and a hard neg-ative one. However, such random sampling cannot effec-tively select informative pairs, which causes a slow con-vergence and poor performance [43]. Thus, several re-weighting methods [1,4,42,43] are proposed to address this issue by assigning different weights to positive and nega-tive pairs. Moreover, a ﬂat vector is difﬁcult to infer the complex relationships among many objects existing in a vi-sual scene [16]. Hence, advanced methods formulate var-ious attention mechanisms [2, 15, 20, 40, 50, 51] to distin-guish important features from those negligible or irrelevant ones based on Top-K region features obtained from the pre-trained Faster R-CNN [29].
Actually, the prevailing image-text retrieval approaches are instance-based, which only focus on the match of the ground-truth sample. Despite their amazing success, image-text retrieval is still very difﬁcult because of the one-to-many correspondence [6] where a set of candidates can be
obtained. This phenomenon is partially caused by the inher-ent heterogeneity between vision and language. In detail, an image can cover all objects in a given scene yet lacks con-text reasoning like text [23], while a textual description may only describe a part scene of interest based on the subjec-tive choices [6]. As illuminated by Figure 1, in the case of image retrieval under the textual description of ‘A bat-ter at a baseball game swinging his bat’, the ground-truth image v1 can be retrieved with effort but other instances with sufﬁcient similarity like v2 and v3 are possibly dis-carded. A similar phenomenon also exists in another case of descriptions search for a given image. The essential cause of multiple matches is the point-to-point mapping strategy adopted by the instance-level approaches. That is, they only struggle to capture the one-to-one correspondence based on the ground-truth pairs in the semantic space. Undoubtedly, such a plain strategy suffers from insufﬁcient representation in one-to-many correspondence.
Recently, several works attempt to learn more distinc-tive representations by cross-modal integration [23, 45] and progressive relevance learning [22, 24]. However, they still adopt point-to-point mapping and can not address the is-sue of multiple matches. Based on the hedged instance embedding [25] and the box embedding [17, 37], Proba-bilistic Cross-Modal Embedding (PCME) [6] and Point-to-Rectangle Matching (P2RM) [41] are successively devel-oped to learn richer representations based on semantic un-certainty capture. Motivated by them [6, 41], this work in-troduces a novel Multilateral Semantic Relations Modeling (MSRM) method to capture the one-to-many correspon-dence between a given query and candidates in image-text retrieval. Concretely, our work mainly includes two parts: semantic distribution learning for a query and multilateral relations modeling for retrieval. The ﬁrst part maps a given query as a probabilistic embedding to learn its true seman-tic distribution based on Mahalanobis distance. Then each candidate instance in a mini-batch is regarded as a hyper-graph node with its mean semantics while a Gaussian query is modeled as a hyperedge. Afterwards, the second part leverages the hyperedge convolution operation to capture the beyond pairwise semantic correlations between candi-date points and the query.
In summary, our contributions can be concluded as:
• We introduce an interpretable method named Multilat-eral Semantic Relations Modeling to better resolve the one-to-many correspondence for image-text retrieval.
• We propose the Semantic Distribution Learning mod-ule to extract the true semantics of a query based on
Mahalanobis distance, which can infer more accurate multiple matches.
• We leverage the hyperedge convolution to model the high-order correlations between a Gaussian query and candidates for further improving the accuracy. 2.