Abstract
Text-to-image person retrieval aims to identify the tar-get person based on a given textual description query. The primary challenge is to learn the mapping of visual and tex-tual modalities into a common latent space. Prior works have attempted to address this challenge by leveraging sep-arately pre-trained unimodal models to extract visual and textual features. However, these approaches lack the nec-essary underlying alignment capabilities required to match multimodal data effectively. Besides, these works use prior information to explore explicit part alignments, which may lead to the distortion of intra-modality information. To alle-viate these issues, we present IRRA: a cross-modal Implicit
Relation Reasoning and Aligning framework that learns re-lations between local visual-textual tokens and enhances global image-text matching without requiring additional prior supervision. Specifically, we first design an Implicit
Relation Reasoning module in a masked language model-ing paradigm. This achieves cross-modal interaction by integrating the visual cues into the textual tokens with a cross-modal multimodal interaction encoder. Secondly, to globally align the visual and textual embeddings, Similar-ity Distribution Matching is proposed to minimize the KL divergence between image-text similarity distributions and the normalized label matching distributions. The proposed method achieves new state-of-the-art results on all three public datasets, with a notable margin of about 3%-9% for
Rank-1 accuracy compared to prior methods. (a) Early global matching paradigm (b) Existing explicit local matching paradigm (c) Our implicit relation reasoning aided matching paradigm
Figure 1. Evolution of text-to-image person retrieval paradigms. (a) Early global-matching method directly align global image and text embeddings. (b) Recent local-matching method, explicitly ex-tract and align local image and text embeddings. (c) Our implicit relation reasoning method, implicitly reasoning the relation among all local tokens to better align global image and text embeddings. 1.

Introduction
Text-to-image person retrieval aims to retrieve a person-of-interest from a large image gallery that best matches the
*Corresponding Author: Mang Ye (yemang@whu.edu.cn) text description query [30], which is a sub-task of both image-text retrieval [26, 33, 42] and image-based person re-identification (Re-ID) [15,32,45]. Textual descriptions pro-vide a natural and relatively comprehensive way to describe a person’s attributes, and are more easily accessible than im-ages. Text-to-image person retrieval thus received increas-ing attention in recent years, benefiting a variety of applica-tions from personal photo album search to public security.
However, text-to-image person retrieval remains a chal-lenging task due to significant intra-identity variations and modality heterogeneity between vision and language. The former challenge stems from the fact that visual appear-ances of an identity differ based on pose, viewpoint, illu-mination, and other factors, while textual description varies by arbitrary descriptive order and textual ambiguity. The latter challenge is the primary issue in cross-modal tasks and is caused by inherent representation discrepancies be-tween vision and language. To tackle above two challenges, the core research problem in text-to-image person retrieval is to explore better ways to extract discriminative feature representations and to design better cross-modal matching methods to align images and texts into a joint embedding space. Early global-matching methods [53, 54] aligned im-ages and texts into a joint embedding space by designing cross-modal matching loss functions (Fig. 1 (a)). Typically, these approaches learned cross-modal alignments by using matching losses only at the end of the network, failing to achieve sufficient modality interaction in middle-level lay-ers, which are crucial to bridge the feature-level modality gap. Therefore, some later methods [5, 7, 21, 46] intro-duced the practice of local-matching by building the cor-respondence between the body parts and the textual entities (Fig. 1 (b)). Although this local matching strategy benefits retrieval performance, it introduces unavoidable noise and uncertainty in the retrieval process. Besides, the strategy requires extracting and storing multiple local part represen-tations of images and texts, computing pairwise similarity between all those representations during inference. These resource-demanding properties limit their applicability for practical large-scale scenarios.
In this paper, we present IRRA: a cross-modal Implicit
Relation Reasoning and Aligning framework, which per-forms global alignment with the aid of cross-modal im-plicit local relation learning. Unlike previous methods that heavily rely on explicit fine-grained local alignment, our approach implicitly utilizes fine-grained information to en-hance global alignment without requiring any additional su-pervision and inference costs (Fig. 1 (c)). Specifically, we design an Implicit Relation Reasoning module that effec-tively builds relations between visual and textual represen-tations through self- and cross-attention mechanisms. This fused representation is then utilized to perform masked lan-guage modeling (MLM) task to achieve effective implicit inter-modal and intra-modal fine-grained relation learning.
MLM is generally utilized during the pre-training stage of vision-language pre-training (VLP) [6, 9, 27, 31, 41]. In this work, we make the first attempt to demonstrate the effec-tiveness of MLM in downstream fine-tuning tasks. Our main innovation is the design of a multimodal interaction encoder that can efficiently fuse visual and textual represen-tations, align cross-modal fine-grained features through the
MLM task. This design helps the backbone network to ex-tract more discriminative global image-text representations without requiring additional supervision.
To guide the image-text matching, commonly used loss functions include ranking loss and cross-modal projection matching (CMPM) [53] loss. Compared to ranking loss, the CMPM loss does not require the selection of specific triplets or margin parameter tuning. It exhibits great stabil-ity with varying batch sizes, making it widely used in text-to-image person retrieval [5, 39, 50]. However, we found that the projection in CMPM can be regarded as a variable weight that adjusts the distribution of softmax output log-its, similar to the temperature parameter [17] for knowledge distillation. Nevertheless, limited by the varying projection length, CMPM therefore cannot precisely control the pro-jection probability distribution, making it difficult to focus on hard-negative samples during model updates. To ex-plore more effective cross-modal matching objective, we further propose an image-text similarity distribution match-ing (SDM) loss. The SDM loss minimizes the KL diver-gence between the normalized image-text similarity score distributions and the normalized ground truth label match-ing distributions. Additionally, we introduce a temperature hyperparameter to precisely control the similarity distribu-tion compactness, which enables the model updates focus on hard-negative samples and effectively enlarges the vari-ance between non-matching pairs and the correlation be-tween matching pairs.
To address the limitations of separate pre-trained mod-els on unimodal datasets, we leverage the Contrastive
Language-Image Pre-training (CLIP) [35] as the initial-ization of our model. CLIP is pre-trained with abundant image-text pairs and has powerful underlying cross-modal alignment capabilities. Some previous approaches [13, 50] have either frozen some part of parameters or introduced only CLIP’s image encoder, which resulted in their inability to fully exploit CLIP’s powerful capabilities in image-text matching. With the proposed IRRA, we successfully trans-fer the powerful knowledge directly from the pre-trained full CLIP model and continue to learn fine-grained cross-modal implicit local relations on text-to-image person re-trieval datasets. In addition, compared to many recent meth-ods [5, 38, 50], IRRA is more efficient as it computes only one global image-text pair similarity score in the inference stage. The main contributions can be summarized as fol-lows:
• We propose IRRA to implicitly utilize fine-grained in-teraction to enhance the global alignment without re-quiring any additional supervision and inference cost.
• We introduce a new cross-modal matching loss named
image-text similarity distribution matching (SDM) loss. It directly minimizes the KL divergence between image-text similarity distributions and the normalized label matching distributions.
• We demonstrate that the full CLIP model can be ap-plied to text-to-image person retrieval and can outper-form existing state-of-the-art methods with straightfor-ward fine-tuning. Moreover, our proposed IRR module enables fine-grained image-text relation learning, al-lowing IRRA to learn more discriminative image-text representations.
• Extensive experiments on three public benchmark datasets, i.e., CUHK-PEDES [30], ICFG-PEDES [7] and RSTPReid [55] show that IRRA consistently out-performs the state-of-the-arts by a large margin. 2.