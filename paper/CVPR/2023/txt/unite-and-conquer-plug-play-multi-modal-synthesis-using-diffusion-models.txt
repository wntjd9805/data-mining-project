Abstract
Generating photos satisfying multiple constraints finds broad utility in the content creation industry. A key hur-dle to accomplishing this task is the need for paired data consisting of all modalities (i.e., constraints) and their cor-responding output. Moreover, existing methods need re-training using paired data across all modalities to intro-duce a new condition. This paper proposes a solution to this problem based on denoising diffusion probabilistic models (DDPMs). Our motivation for choosing diffusion models over other generative models comes from the flexible in-ternal structure of diffusion models. Since each sampling step in the DDPM follows a Gaussian distribution, we show that there exists a closed-form solution for generating an image given various constraints. Our method can unite multiple diffusion models trained on multiple sub-tasks and conquer the combined task through our proposed sampling
strategy. We also introduce a novel reliability parame-ter that allows using different off-the-shelf diffusion mod-els trained across various datasets during sampling time alone to guide it to the desired outcome satisfying multi-ple constraints. We perform experiments on various stan-dard multimodal tasks to demonstrate the effectiveness of our approach. More details can be found at: https://nithin-gk.github.io/projectpages/Multidiff 1.

Introduction
Today’s entertainment industry is rapidly investing in content creation tasks [12, 22]. Studios and companies working on games or animated movies find various applica-tions of photos/videos satisfying multiple characteristics (or constraints) simultaneously. However, creating such photos is time-consuming and requires a lot of manual labor. This era of content creation has led to some exciting and valuable works like Stable Diffusion [28], Dall.E-2 [26], Imagen [29] and multiple other works that can create photorealistic im-ages using text prompts. All of these methods belong to the broad field of conditional image generation [25, 33]. This process is equivalent to sampling a point from the multi-dimensional space P (z|x) and can be mathematically ex-pressed as:
ˆz ∼ P (z|x), (1) where ˆz denotes the image to be generated based on a condi-tion x. The task of image synthesis becomes more restricted when the number of conditions increases, but it also hap-pens according to the user’s expectations. Several previous works have attempted to solve the conditional generation problem using generative models, such as VAEs [18,25] and
Generative Adversarial Networks (GANs) [7, 34]. How-In ever, most of these methods use only one constraint. terms of image generation quality, the GAN-based meth-ods outperform VAE-based counterparts. Furthermore, dif-ferent strategies for conditioning GANs have been pro-posed in the literature. Among them, the text conditional
GANs [3, 27, 41, 45] embed conditional feature into the fea-tures from the initial layer through adaptive normalization scheme. For the case of image-level conditions such as a sketches or semantic labels, the conditional image is also the input to the discriminator and is embedded with an adap-tive normalization scheme [22, 31, 40, 42]. Hence, a GAN-based method for multimodal generation has multiple archi-tectural constraints [11]
A major challenge in training generative models for mul-timodal image synthesis is the need for paired data con-taining multiple modalities [12, 32, 44]. This is one of the main reasons why most existing models restrict themselves to one or two modalities [32, 44]. Few works use more than two domain variant modalities for multimodal gener-ation [11, 45]. These methods can perform high-resolution
Figure 2. An illustration of the difference between the existing multimodal generation approaches [45] and the proposed ap-proach. Existing multimodal methods require training on paired data across all modalities. In contrast, we present two ways that can be used for training: (1) Train with data pairs belonging to dif-ferent modalities one at a time, and (2) Train only for the additional modalities using a separate diffusion model in case existing mod-els are available for the remaining modalities. During sampling, we forward pass for each conditioning strategy independently and combine their corresponding outputs, hence preserving the differ-ent conditions. image synthesis and require training with paired data across different domains to achieve good results. But to increase the number of modalities, the models need to be retrained; thus they do not scale easily. Recently, Shi et al. [32] pro-posed a weakly supervised VAE-based multimodal genera-tion method without paired data from all modalities. The model performs well when trained with sparse data. How-ever, if we need to increase the number of modalities, the model needs to be retrained; therefore, it is not scalable.
Scalable multimodal generation is an area that has not been properly explored because of the difficulty in obtaining the large amounts of data needed to train models for the gener-ative process.
Recently diffusion models have outperformed other gen-erative models in the task of image generation [5, 9]. This is due to the ability of diffusion models to perform exact sampling from very complex distributions [33]. A unique quality of the diffusion models compared to other genera-tive processes is that the model performs generation through a tractable Markovian process, which happens over many time steps. The output at each timestep is easily accessible.
Therefore, the model is more flexible than other generative models, and this form of generation allows manipulation of images by adjusting latents [1, 5, 23]. Various techniques have used this interesting property of diffusion models for low-level vision tasks such as image editing [1, 17], im-age inpainting [20], image super-resolution [4], and image restoration problems [16].
In this paper, we exploit this flexible property of the de-noising diffusion probabilistic models and use it to design a solution to multimodal image generation problems with-Figure 3. An illustration of our proposed approach. During training, we use diffusion models trained across multiple datasets (we can either train a single model that supports multiple different conditional strategies one at a time or multiple models). During Inference, we sample using the proposed approach and condition them using different modalities at the same time. out explicitly retraining the network with paired data across all modalities. Figure 2 depicts the comparison between existing methods and our proposed method. Current ap-proaches face a major challenge: the inability to combine models trained across different datasets during inference time [9, 19]. In contrast, our work allows users flexibility during training and can also use off-the-shelf models for multi-conditioning, providing greater flexibility when us-ing diffusion models for multimodal synthesis task. Figure 1 visualizes some applications of our proposed approach.
As shown in Figure 1-(a), we use two open-source mod-els [5, 21] for generic scene creation. Using these two mod-els, we can bring new novel categories into an image (e.g.
Otterhound: the rarest breed of dog). We also illustrate the results showing multimodal face generation, where we use a model trained to utilize different modalities from differ-ent datasets. As it can be seen in 1-(b) and (c), our work can leverage models trained across different datasets and combine them for multi-conditional synthesis during sam-pling. We evaluate the performance of our method for the task of multimodal synthesis using the only existing mul-timodal dataset [45] for face generation where we condi-tion based on semantic labels and text attributes. We also evaluate our method based on the quality of generic scene generation.
The main contributions of this paper are summarized as follows:
• We propose a diffusion-based solution for image gen-eration under the presence of multimodal priors.
• We tackle the problem of need for paired data for mul-timodal synthesis by deriving upon the flexible prop-erty of diffusion models.
• Unlike existing methods, our method is easily scalable and can be incorporated with off-the-shelf models to add additional constraints. 2.