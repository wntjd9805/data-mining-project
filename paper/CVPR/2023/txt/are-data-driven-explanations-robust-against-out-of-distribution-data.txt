Abstract
As black-box models increasingly power high-stakes ap-plications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning mod-els are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations ro-bust against out-of-distribution data? Our empirical re-sults show that even though predict correctly, the model might still yield unreliable explanations under distribu-tional shifts. How to develop robust explanations against out-of-distribution data?
To address this problem, we propose an end-to-end model-agnostic learning framework
Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory sig-nals for the learning of explanations without human anno-tation. Can robust explanations benefit the model’s general-ization capability? We conduct extensive experiments on a wide range of tasks and data types, including classification and regression on image and scientific tabular data. Our results demonstrate that the proposed method significantly improves the model’s performance in terms of explanation and prediction robustness against distributional shifts. 1.

Introduction
There has been an increasing trend to apply black-box machine learning (ML) models for high-stakes applications.
The lack of explainability of models can have severe con-sequences in healthcare [48], criminal justice [61], and other domains. Meanwhile, ML models are inevitably ex-posed to unseen distributions that lie outside their train-ing space [28, 56]; a highly accurate model on average can fail catastrophically on out-of-distribution (OOD) data due to naturally-occurring variations, sub-populations, spurious correlations, and adversarial attacks. For example, a can-cer detector would erroneously predict samples from hospi-tals having different data acquisition protocols or equipment 1The source code and pre-trained models are available at: https:
//github.com/tangli-udel/DRE.
Figure 1. The explanations for in- and out-of-distribution data of Terra Incognita [4] dataset. Note that GroupDRO [50] and
IRM [2] are explicitly designed methods that can predict accu-rately across distributions. Although with correct predictions, the explanations of models trained by such methods would also high-light distribution-specific associations (e.g., tree branches) except the object. This leads to unreliable explanations on OOD data. On the contrary, our model consistently focuses on the most discrimi-native features shared across distributions. manufacturers. Therefore, reliable explanations across dis-tributions are crucial for the safe deployment of ML models.
However, existing works focus on the reliability of data-driven explanation methods [1, 64] while ignoring the ro-bustness of explanations against distributional shifts.
A question naturally arises: Are data-driven explana-tions robust against out-of-distribution data? We empir-ically investigate this problem across different methods.
Results of the Grad-CAM [51] explanations are shown in
Fig. 1. We find that the distributional shifts would fur-ther obscure the decision-making process due to the black-box nature of ML models. As shown, the explanations fo-cus not only on the object but also spurious factors (e.g., background pixels). Such distribution-specific associations would yield inconsistent explanations across distributions.
Eventually, it leads to unreliable explanations (e.g., tree branches) on OOD data. This contradicts with human prior that the most discriminative features ought to be invariant.
How to develop robust explanations against out-of-distribution data? Existing works on OOD generalization are limited to data augmentation [42, 52, 59], distribution alignment [3, 16, 31], Meta learning [12, 29, 40], or invari-ant learning [2, 26]. However, without constraints on expla-nations, the model would still recklessly absorb all associ-ations found in the training data, including spurious cor-relations. To constrain the learning of explanations, ex-isting methods rely on explanation annotations [44, 54] or one-to-one mapping between image transforms [8, 19, 39].
However, there is no such mapping in general naturally-occurring distributional shifts.
Furthermore, obtaining ground truth explanation annotations is prohibitively expen-sive [60], or even impossible due to subjectivity in real-world tasks [46]. To address the aforementioned limita-tions, we propose an end-to-end model-agnostic training framework Distributionally Robust Explanations (DRE).
The key idea is, inspired by self-supervised learning, to fully utilize the inter-distribution information to provide su-pervisory signals for explanation learning.
Can robust explanations benefit the model’s generaliza-tion capability? We evaluate the proposed methods on a wide range of tasks in Sec. 4, including the classification and regression tasks on image and scientific tabular data.
Our empirical results demonstrate the robustness of our ex-planations. The explanations of the model trained via the proposed method outperform existing methods in terms of explanation consistency, fidelity, and scientific plausibility.
The extensive comparisons and ablation studies prove that our robust explanations significantly improve the model’s prediction accuracy on OOD data. As shown, the robust explanations would alleviate the model’s excessive reliance on spurious correlations, which are unrelated to the causal correlations of interest [2]. Furthermore, the enhanced ex-plainability can be generalized to a variety of data-driven explanation methods.
In summary, our main contributions:
• We comprehensively study the robustness of data-driven explanations against naturally-occurring distri-butional shifts.
• We propose an end-to-end model-agnostic learn-ing framework Distributionally Robust Explanations (DRE). It fully utilizes inter-distribution information to provide supervisory signals for explanation learning without human annotations.
• Empirical results in a wide range of tasks including classification and regression on image and scientific tabular data demonstrate superior explanation and pre-diction robustness of our model against OOD data. 2.