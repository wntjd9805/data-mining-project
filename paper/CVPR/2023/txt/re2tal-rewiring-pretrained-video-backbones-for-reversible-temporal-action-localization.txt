Abstract
Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and com-plex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the lo-calization problem, consequently limiting localization per-formance. In this work, to extend the potential in TAL net-works, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversible
TAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training.
Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual con-nection to a reversible module without changing any pa-rameters. This provides two benefits: (1) a large vari-ety of reversible networks are easily obtained from exist-ing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible ver-sions. Re2TAL, only using the RGB modality, reaches 37.01% average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9% at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods. Code is available at https://github.com/coolbay/Re2TAL. 1.

Introduction
Temporal Action Localization (TAL) [36,53,73] is a fun-damental problem of practical importance in video under-standing.
It aims to bound semantic actions within start and end timestamps. Localizing such video segments is very useful for a variety of tasks such as video-language grounding [23, 56], moment retrieval [9, 21], video caption-ing [30, 50]. Since video actions have a large variety of
Illustration of TAL network activations in train-Figure 1. ing. Top: non-reversible network stores activations of all layers in memory. Bottom: reversible network only needs to store the activations of inter-stage downsampling layers. Backbone activa-tions dominate memory occupation, compared to Localizer. temporal durations and content, to produce high-fidelity lo-calization, TAL approaches need to learn from a long tem-poral scope of the video, which contains a large number of frames. To accommodate all these frames along with their network activations in GPU memory is extremely challeng-ing, given the current GPU memory size (e.g. the commod-ity GPU GTX1080Ti only has 11GB). Often, it is impossi-ble to train one video sequence on a GPU without substan-tially downgrading the video spatial/temporal resolutions.
To circumvent the GPU-memory bottleneck, most TAL methods deal with videos in two isolated steps (e.g. [4, 67, 69, 71–73]). First is a snippet-level feature extraction step, which simply extracts snippet representations using a pre-trained video network (backbone) in inference mode. The backbone is usually a large neural network trained for an auxiliary task on a large dataset of trimmed video clips (e.g., action recognition on Kinetics-400 [28]). The second step trains a localizer on the pre-extracted features. In this way, only the activations of the TAL head need to be stored in memory, which is tiny compared to those of the back-bone (see the illustration of the activation contrast between backbone and localizer in Fig. 1). However, this two-step strategy comes at a steep price. The pre-extracted features can suffer from domain shift from the auxiliary pre-training
task/data to TAL, and do not necessarily align with the rep-resentation needs of TAL. This is because they cannot be finetuned and must be used as-is in their misaligned state for TAL. A better alternative is to jointly train the backbone and localizer end to end. But as mentioned earlier, the enor-mous memory footprint of video activations in the backbone makes it extremely challenging. Is there a way for end-to-end training without compromising data dimensionality?
Reversible networks [20, 25, 31, 48] provide an elegant solution to drastically reduce the feature activation mem-ory during training. Their input can be recovered from the output via a reverse computation. Therefore, the interme-diate activation maps, which are used for back propagation, do not need to be cached during the forward pass (as il-lustrated in Fig. 1). This offers a promising approach to enable memory-efficient end-to-end TAL training, and var-ious reversible architectures have been proposed, such as
RevNet [20], and RevViT [48]. However, these works de-sign a specific reversible architecture and train for a partic-ular dataset. Due to their new architecture, they also need to train the networks from scratch, requiring a significant amount of compute resources.
Conversely, it would be beneficial to be able to convert existing non-reversible video backbones to reversible ones, which would (1) avail a large variety of architectures and (2) allow us to reuse the large compute resources that had already been invested in training the non-reversible video backbones. Since pre-trained video backbones are a crucial part of TAL, the ability to convert off-the-shelf backbones to reversible ones is a key to unleash their power in this task.
In this work, for end-to-end TAL, we propose a princi-pled approach to Rewire the architectural connections of a pre-trained non-reversible backbone to make it Reversible, dubbed Re2TAL. Network modules with a residual connec-tion (res-module for short), such as a Resnet block [22] or a
Transformer MLP/attention layer [14], are the most popular design recently. Given any network composed of resid-ual modules, we can apply our rewiring technique to convert it to a corresponding reversible network without introducing or removing any trainable parameters. Instead of training from scratch, our reversible network can reuse the non-reversible network’s parameters and only needs a small number of epochs for finetuning to reach similar per-formance. We summarize our contributions as follows. (1) We propose a novel approach to construct and train reversible video backbones parsimoniously by architec-tural rewiring from an off-the-shelf pre-trained video back-bone. This not only provides a large collection of reversible candidates, but also allows reusing the large compute re-sources invested in pre-training these models. We apply our rewiring technique to various kinds of representative video backbones, including transformer-based Video Swin and ConvNet-based Slowfast, and demonstrate that our re-versible networks can reach the same performance of their non-reversible counterparts with only minimum finetuning effort (as low as 10 epochs compared to 300 epochs for training from scratch). (2) We propose a novel approach for end-to-end TAL training using reversible video networks. Without sacrific-ing spatial/temporal resolutions or network capability, our proposed approach dramatically reduces GPU memory us-age, thus enabling end-to-end training on one 11GB GPU.
We demonstrate on different localizers and different back-bone architectures that we significantly boost TAL perfor-mance with our end-to-end training compared to traditional feature-based approaches. (3) With our proposed Re2TAL, we use recent localiz-ers in the literature to achieve a new state-of-the-art perfor-mance, 37.01% average mAP on ActivityNet-v1.3. We also reach the highest mAP among all methods that only use the
RGB modality on THUMOS-14, 64.9% at tIoU= 0.5, out-performing concurrent work TALLFormer [10]. 2.