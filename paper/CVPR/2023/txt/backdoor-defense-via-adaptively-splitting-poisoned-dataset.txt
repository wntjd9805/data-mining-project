Abstract
Backdoor defenses have been studied to alleviate the threat of deep neural networks (DNNs) being backdoor attacked and thus maliciously altered. Since DNNs usu-ally adopt some external training data from an untrusted third party, a robust backdoor defense strategy during the training stage is of importance. We argue that the core of training-time defense is to select poisoned samples and to handle them properly. In this work, we summarize the training-time defenses from a unified framework as split-ting the poisoned dataset into two data pools. Under our framework, we propose an adaptively splitting dataset-based defense (ASD). Concretely, we apply loss-guided split and meta-learning-inspired split to dynamically update two data pools. With the split clean data pool and polluted data pool, ASD successfully defends against backdoor at-tacks during training. Extensive experiments on multiple benchmark datasets and DNN models against six state-of-the-art backdoor attacks demonstrate the superiority of our
ASD. Our code is available at https://github.com/
KuofengGao/ASD. 1.

Introduction
Backdoor attacks can induce malicious model behav-iors by injecting a small portion of poisoned samples into the training dataset with specific trigger patterns. The at-tacks have posed a significant threat to deep neural networks (DNNs) [31–33, 35, 37, 45], especially when DNNs are deployed in safety-critical scenarios, such as autonomous driving [10]. To alleviate the threats, backdoor defenses have been intensively explored in the community, which can be roughly grouped into post-processing defenses and training-time ones. Since the training data collection is usually time-consuming and expensive, it is common to use external data for training without security guarantees
*Equal contribution.
†Corresponding author.
Table 1. Summary of the representative training-time backdoor defenses under our framework.
Methods
# Pool
# Pool
# Pool
# Clean Hard
Initialization Maintenance Operation Sample Selection
ABL
DBD
ASD (Ours)
Fast
Slow
Fast
Static
Adaptive
Adaptive
Unlearn
Purify
Purify
No
No
Yes
[16, 24, 29, 42, 43]. The common practice makes backdoor attacks feasible in real-world applications, which highlights the importance of training-time defenses. We argue that such defenses are to solve two core problems, i.e., to select poisoned samples and to handle them properly.
In this work, we formulate the training-time defenses into a unified framework as splitting the poisoned dataset into two data pools. Concretely, a clean data pool contains selected clean samples with trustworthy labels and a pol-luted data pool is composed of poisoned samples and re-maining clean samples. Under this framework, the mecha-nisms of these defenses can be summarized into three parts, i.e., pool initialization, pool maintenance, and pool oper-ation. To be more specific, they need to first initialize two data pools, deploy some data pool maintenance strate-gies, and take different training strategies on those split (clean and polluted) data pools. We illustrate our frame-work with two representative training-time defenses, i.e., anti-backdoor learning (ABL) [27] as well as decoupled-based defense (DBD) [21]. ABL statically initializes a pol-luted pool by the loss-guided division. The polluted pool is fixed and unlearned during training. Similarly, DBD initial-izes two data pools after computationally expensive train-ing. Then the model is fine-tuned by semi-supervised learn-ing with two dynamically updated data pools. (More details of these two methods are introduced in Sec. 2.2.)
Despite their impressive results, there is still room for improvement. ABL initializes two pools with static data se-lection, which raises the concern of mixing poisoned data into clean ones. Once they are mixed, it is hard to alleviate it in the followed training process. Besides, unlearning poi-(a) ABL (b) DBD (c) ASD (Ours)
Figure 1. Loss distribution of samples on the model trained by
ABL, DBD and our ASD against WaNet. Compared with ABL and DBD, our proposed ASD can clearly separate clean samples and poisoned ones better by a novel meta-split. soned data directly can lose some useful semantic features and degrade the model’s clean accuracy. As for DBD, its pool initialization is computationally expensive and is hard to be implemented end-to-end. Moreover, DBD adopts su-pervised learning for the linear layer in the whole poisoned dataset during the second stage, which can potentially im-plant the backdoor in models.
Under our framework, we introduce an adaptively splitting dataset-based defense (ASD). With two initialized data pools, we first adopt the loss-guided [51] split to up-date two data pools. However, some (model-dependent) clean hard samples can not be distinguished from poisoned ones directly by their loss magnitudes. As shown in Fig. 1, ABL and DBD adopting loss-guided split have failed to completely separate clean samples from poisoned sam-ples.
Instead, we propose a novel meta-learning-inspired split (meta-split), which can make a successful separation.
Then we treat the clean data pool as a labeled data container and the polluted one as unlabeled, where we adopt semi-supervised learning on two data pools. As such, we can utilize the semantic information of poisoned data without labels to keep the clean accuracy meanwhile to avoid back-door injection, which can be regarded as a fashion of purify-ing poisoned samples. Note that, our ASD introduces clean seed samples (i.e., only 10 images per class) in pool initial-ization, which could be further extended to a transfer-based version, by collecting clean seed samples from another clas-sical dataset. Given previous methods [28, 34, 50, 54] usu-ally assume they can obtain much more clean samples than ours, our requirements are easier to meet. The properties of
ABL, DBD and our ASD are briefly listed in Table 1.
In summary, our main contributions are three-fold:
• We provide a framework to revisit existing training-time backdoor defenses from a unified perspective, namely, splitting the poisoned dataset into a clean pool and a polluted pool. Under our framework, we pro-pose an end-to-end backdoor defense, ASD, via split-ting poisoned dataset adaptively.
• We propose a fast pool initialization method and adap-tively update two data pools in two splitting manners, i.e., loss-guided split and meta-split. Especially, the proposed meta-split focuses on how to mine clean hard samples and clearly improves model performance.
• With two split data pools, we propose to train a model on the clean data pool with labels and the polluted data pool without using labels. Extensive experiment re-sults demonstrate the superiority of our ASD to previ-ous state-of-the-art backdoor defenses. 2.