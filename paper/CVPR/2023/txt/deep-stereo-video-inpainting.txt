Abstract
Stereo video inpainting aims to fill the missing regions on the left and right views of the stereo video with plausi-ble content simultaneously. Compared with the single video inpainting that has achieved promising results using deep convolutional neural networks, inpainting the missing re-gions of stereo video has not been thoroughly explored. In essence, apart from the spatial and temporal consistency that single video inpainting needs to achieve, another key challenge for stereo video inpainting is to maintain the stereo consistency between left and right views and hence alleviate the 3D fatigue for viewers. In this paper, we pro-pose a novel deep stereo video inpainting network named
SVINet, which is the first attempt for stereo video inpainting task utilizing deep convolutional neural networks. SVINet first utilizes a self-supervised flow-guided deformable tem-poral alignment module to align the features on the left and right view branches, respectively. Then, the aligned features are fed into a shared adaptive feature aggrega-tion module to generate missing contents of their respec-tive branches. Finally, the parallax attention module (PAM) that uses the cross-view information to consider the signif-icant stereo correlation is introduced to fuse the completed features of left and right views. Furthermore, we develop a stereo consistency loss to regularize the trained parame-ters, so that our model is able to yield high-quality stereo video inpainting results with better stereo consistency. Ex-perimental results demonstrate that our SVINet outperforms state-of-the-art single video inpainting models. 1.

Introduction
Video inpainting aims to fill in missing region with plau-sible and coherent contents for all video frames. As a fun-damental task in computer vision, video inpainting is usu-ally adopted to enhance visual quality. It has great value in many practical applications, such as scratch restora-*Corresponding author
Figure 1. An example of visual comparison with state-of-the-art single video inpainting models (E2FGVI [23] and FGT [48]) on stereo video inpainting. As shown here, directly using the single video inpainting method to generate missing contents on the left view (first row) and right view (second row) will lead to severe stereo inconsistency. In contrast, the proposed method not only generates vivid textures, but also the parallax flow (third row) be-tween two views is closer to the ground-truth (third row of the input column). The closer the parallax flow is to ground-truth, the better the stereo consistency is maintained. tion [2], undesired object removal [34], and autonomous driving [24]. In recent years, relying on the powerful fea-tures extraction capabilities of convolutional neural net-work (CNN), existing deep single video inpainting meth-ods [6, 13, 15, 18, 20, 23, 42, 46] have shown great success.
With the development of augmented reality (AR), virtual re-ality (VR) devices, dual-lens smartphones, and autonomous robots, there is an increasing demand for various stereo video processing techniques, including stereo video inpaint-ing. For example, in some scenarios, we not only remove objects and edit contents, but also expect to recover the missing regions in the stereo video. Although the traditional stereo video inpainting methods [31, 32] based on patch op-timization have been preliminarily studied, the stereo video inpainting based on deep learning has not been explored.
A naive solution of stereo video inpainting is to directly apply the single video inpainting methods by completing the missing regions of left and right views, respectively.
However, inpainting an individual video that only considers the undamaged spatial-temporal statistics of one view will
ignore the geometric relationship between two views, caus-ing severe stereo inconsistency as shown in Fig. 1. Besides, another way to solve this task is process the stereo video frame-by-frame using the stereo image inpainting methods.
For example, Li et al. [22] designed a Geometry-Aware At-tention (GAA) module to learn the geometry-aware guid-ance from one view to another, so as to make the corre-sponding regions in the inpainted stereo images consistent.
Nevertheless, compared to its image counterpart, stereo video inpainting still needs to concern the temporal con-sistency.
In this way, satisfactory performance cannot be achieved by extending stereo image inpainting technique to stereo video inpainting task. Therefore, to maintain tem-poral and stereo consistency simultaneously, there are two key points need to be considered: (i) temporal modeling be-tween consecutive frames (ii) correlation modeling between left view and right view.
In fact, on the one hand, the missing contents in one frame may exist in neighboring (reference) frames of a video sequence. Thus, the temporal information between the consecutive frames can be explored to generate miss-ing contents of the current (target) frame. For example, a classical technology pipeline is “alignment–aggregation”, that is, the reference frame is first aligned to eliminate im-age changes between the reference frame and target frame, and then the aligned reference frame is aggregated to gener-ate the missing contents of the target frame. On the other hand, correlation modeling between two views has been studied extensively in the stereo image super-resolution task [3, 39, 44]. For instance, Wang et al. [39] proposed the parallax attention module (PAM) to tackle the vary-ing parallax problem in the parallax attention stereo super-resolution network (PASSRnet). Ying et al. [44] developed a stereo attention module (SAM) to address the informa-tion incorporation issue in the stereo image super-resolution models. More recently, Chen et al. [3] designed a cross-parallax attention module (CPAM) which can capture the stereo correspondence of respective additional information.
Motivated by above observation and analysis, in this pa-per, we propose a stereo video inpainting network, named
SVINet. Specifically, SVINet first utilizes a self-supervised flow-guided deformable temporal alignment module to align the reference frames on the left and right view branches at the feature level, respectively. Such operation can eliminate the negative effect of image changes caused by camera or object motion. Then, the aligned reference frame features are fed into a shared adaptive feature aggre-gation module to generate missing contents of their respec-tive branches. Note that the missing contents of one view may also exist in another view, we also introduce the most relevant target frame from another view when completing the missing regions of the current view, which can avoid the computational complexity problem caused by simply aggre-gating all video frames. Finally, a modified PAM is used to model the stereo correlation between the completed fea-tures of the left and right views. Beyond that, inspired by the success of end-point error (EPE) [10] in optical flow es-timation [11], we introduce a new stereo consistency loss to regularize training parameters, so that our model is able to yield high-quality stereo video inpainting results with better stereo consistency. We conduct extensive experiments on two benchmark datasets, and the experimental results show that our SVINet surpasses the performance of recent single video inpainting methods in the stereo video inpainting.
To sum up, our contributions are summarized as follows:
• We propose a novel end-to-end stereo video inpainting network named SVINet, where the spatially, tempo-rally, and stereo consistent missing contents for cor-rupted stereo video are generated. To the best of our knowledge, this is the first work using deep learning to solve stereo video inpainting task.
• Inspired by the end-point error (EPE) [10], we design a stereo consistency loss to regularize training parame-ters of SVINet, so that the training model can improve the stereo consistency of the completed results.
• Experiments on two benchmark datasets demonstrate the superiority of our proposed method in both quanti-tative and qualitative evaluations. Notably, our method also shed light on the subsequent research of stereo video inpainting. 2.