Abstract
In asymmetric retrieval systems, models with different capacities are deployed on platforms with different compu-tational and storage resources. Despite the great progress, existing approaches still suffer from a dilemma between retrieval efﬁciency and asymmetric accuracy due to the limited capacity of the lightweight query model.
In this work, we propose an Asymmetric Feature Fusion (AFF) paradigm, which advances existing asymmetric retrieval systems by considering the complementarity among differ-ent features just at the gallery side. Speciﬁcally, it ﬁrst em-beds each gallery image into various features, e.g., local features and global features. Then, a dynamic mixer is in-troduced to aggregate these features into compact embed-ding for efﬁcient search. On the query side, only a sin-gle lightweight model is deployed for feature extraction.
The query model and dynamic mixer are jointly trained by sharing a momentum-updated classiﬁer. Notably, the proposed paradigm boosts the accuracy of asymmetric re-trieval without introducing any extra overhead to the query side. Exhaustive experiments on various landmark retrieval datasets demonstrate the superiority of our paradigm. 1.

Introduction
Image retrieval [17, 30, 34, 40, 49, 53] has been studied for a long time in the literature. Typically, high-performing image retrieval systems deploy a large powerful model to embed both query and gallery images, which is widely known as symmetric image retrieval. However, in some real-world applications, e.g., mobile search, the query side is constrained by resource limitation and thus cannot meet the overhead of deploying a large model. To this end, the paradigm of asymmetric image retrieval is ﬁrst proposed in HVS [9] and AML [3], which has attracted increasing attention from the community [26, 38, 43, 56, 62]. In such
*Corresponding Author: Min Wang and Wengang Zhou. (a) Previous single feature pipeline [3, 9, 38, 56]. (b) Our asymmetric feature fusion pipeline.
Illustration of (a) previous single-feature asymmet-Figure 1. ric retrieval pipeline and (b) our asymmetric feature fusion paradigm. Due to limited capacity of the lightweight model, ex-isting pipeline achieves efﬁciency for the query side at the cost of retrieval accuracy degradation. In contrast, our approach en-hances existing asymmetric retrieval pipeline from the perspective of gallery feature fusion. For efﬁcient retrieval, a dynamic mixer is introduced to aggregate multiple gallery features into a compact embedding. Query model and mixer are jointly trained with com-patible constraints. Our method realizes high efﬁciency without sacriﬁcing retrieval accuracy. a paradigm, deep representation models with different ca-pacities are ﬁrst trained to be compatible and then deployed on platforms with different resources to strike a balance be-tween retrieval accuracy and efﬁciency, as shown in Fig. 1a.
For an asymmetric retrieval system, the most crucial thing is to ensure that the embedding spaces of different models are well aligned. To this end, BCT [38] ﬁrst pro-poses a backward-compatible learning framework, in which the classiﬁer of the gallery model is inherited to guide the learning of the query model. Recently, various efforts have been devoted to improving cross-model feature compatibil-ity in terms of training objectives [3, 26, 51, 56, 57], model structures [8, 9], etc. Despite the great progress, a dilemma
CVNet→CVNet
ShufﬂeNetV2→Mixer (Ours)
EfﬁcientNet→Mixer (Ours)
GhostNet→Mixer (Ours)
ShufﬂeNetV2→CVNet (Previous)
MobileNetV3→CVNet (Previous)
MobileNetV3→Mixer (Ours)
EfﬁcientNet→CVNet (Previous)
MobileNetV2→CVNet (Previous)
MobileNetV2→Mixer (Ours)
GhostNet→CVNet (Previous) 65 60 55 50 45 mAP (%)
Higher is better 64.40 62.00 57.79 59.81 60.96 56.74 54.16 54.07 52.83 50.08 55.65 52.53 53.80 52.59 51.92 47.74 42.91 65 60 55 50 45 mAP (%)
Higher is better 59.81 57.79 54.16 55.65 52.53 52.59 47.74 50.08 51.92 42.91 53.80 52.83 54.07 64.40 62.00 60.96 56.74 0 1 2 3 4 46.6 0
Query Flops (G) Lower is better 2 6
Model Size (MB) Lower is better 10 4 8 42.5
Figure 2. Average mAP vs. FLOPs/Model Size of the query model for ROxf + 1M [33] dataset. The notation format “query model → gallery model” in the legend means embedding queries with the query model and retrieving in a gallery set embedded by the gallery model. A line connecting the dots with one color represents a family of lightweight models with different model sizes. Previous: The latest asymmetric retrieval method CSD [56] is adopted to train query model with CVNet [21] deployed as gallery model. Ours: our paradigm utilizes CVNet, Token [54],
DELG [4] and DOLG [59] to generate aggregated gallery features and trains the mixer and query model jointly. is still unresolved, i.e., the accuracy of asymmetric retrieval is still unsatisfactory compared to that of symmetric re-trieval, especially in limited-resource and large-scale sce-narios, as shown in Fig. 2 ((cid:3), ♦, . . . , (cid:52) vs. +). We argue that such dilemma is due to the low capacity of lightweight query model, which cannot perfectly achieve feature com-patibility with the static powerful gallery model.
To alleviate above issue, we introduce a new paradigm named Asymmetric Feature Fusion (AFF). It boosts the accuracy of existing asymmetric retrieval systems by con-sidering the complementarity among different features, as shown in Fig. 1b. On the gallery side, it deploys sev-eral large powerful models on the cloud to extract diverse features, e.g., local features, which are suitable for cap-turing local matches, or global features that are effective for holistic semantic matching. For efﬁcient retrieval, a dynamic mixer is further proposed to aggregate diverse gallery features into compact embedding, which allows ef-ﬁcient vector search [11, 18] to be exploited. As for the query side, queries are embedded with a single lightweight model. It eliminates time-consuming multiple feature ex-traction and aggregation processes, realizing a solution suit-able for resource-constrained platforms. During training, all the gallery models are ﬁxed, while the mixer and query model are trained jointly by a momentum-updated classiﬁer for achieving feature compatibility.
Compared to previous retrieval approaches, the proposed paradigm has two unique advantages. First, it fuses various features on the gallery side, which notably advances the re-trieval accuracy of existing asymmetric retrieval systems.
Although the extraction and aggregation processes increase the computational complexity, they are typically performed on resource-rich cloud platforms. In addition, gallery im-ages are embedded ofﬂine in advance, whose computational overhead has no inﬂuence on the query side. Second, com-pared with multi-feature fusion methods, our paradigm only deploys a single lightweight model on the query side, which is free of the complex and time-consuming multi-feature ex-traction and aggregation. Thus, it introduces no extra com-putational and storage overhead for the query side. Overall, with the proposed asymmetric feature fusion paradigm, our approach achieves high retrieval efﬁciency and accuracy si-multaneously, as shown in Fig. 2 ((cid:3) vs. (cid:4),. . . ,(cid:52) vs. (cid:78)).
To evaluate our approach, comprehensive experiments are conducted on popular landmark retrieval datasets. The pro-posed paradigm realizes promising performance improve-ment for existing asymmetric retrieval systems and leads to the state-of-the-art results across the public benchmarks. 2.