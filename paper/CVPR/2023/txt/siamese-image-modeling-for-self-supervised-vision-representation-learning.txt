Abstract 1.

Introduction
Self-supervised learning (SSL) has delivered superior performance on a variety of downstream vision tasks. Two main-stream SSL frameworks have been proposed, i.e., In-stance Discrimination (ID) and Masked Image Modeling (MIM). ID pulls together representations from different views of the same image, while avoiding feature collapse.
It lacks spatial sensitivity, which requires modeling the lo-cal structure within each image. On the other hand, MIM reconstructs the original content given a masked image.
It instead does not have good semantic alignment, which requires projecting semantically similar views into nearby representations. To address this dilemma, we observe that (1) semantic alignment can be achieved by matching dif-ferent image views with strong augmentations; (2) spatial sensitivity can benefit from predicting dense representations with masked images. Driven by these analysis, we propose
Siamese Image Modeling (SiameseIM), which predicts the dense representations of an augmented view, based on an-other masked view from the same image but with different augmentations. SiameseIM uses a Siamese network with two branches. The online branch encodes the first view, and predicts the second view’s representation according to the relative positions between these two views. The target branch produces the target by encoding the second view.
SiameseIM can surpass both ID and MIM on a wide range of downstream tasks, including ImageNet finetuning and linear probing, COCO and LVIS detection, and ADE20k se-mantic segmentation. The improvement is more significant in few-shot, long-tail and robustness-concerned scenarios.
Code shall be released.
∗Equal contribution.
†This work is done when Chenxin Tao and
Weijie Su are interns at Shanghai Artificial Intelligence Laboratory. (cid:66)Corresponding author.
Self-supervised learning (SSL) has been pursued in the vision domain for a long time [32]. It enables us to pre-train models without human-annotated labels, which makes it possible to exploit huge amounts of unlabeled data. SSL has provided competitive results against supervised pre-training baselines in various downstream tasks, including image classification [4, 23], object detection [35] and se-mantic segmentation [24].
To effectively train models in the SSL manner, re-searchers design the so-called “pretext tasks” to generate supervision signals. One of the most typical frameworks is Instance Discrimination (ID), whose core idea is to pull together representations of different augmented views from the same image, and avoid representational collapse. Differ-ent variants of ID have been proposed, including contrastive learning [9, 24], asymmetric networks [12, 21], and feature decorrelation [5, 57]. A recent work [43] has shown the in-trinsic consistency among these methods via their similar gradient structures. For ID methods, the representations of each image are well separated, thus inducing good linear separability. However, as shown in [35], for transfer learn-ing on detection tasks with Vision Transformers [19], ID is not superior to supervised pre-training, and even lags be-hind random initialization given enough training time.
Recently, another SSL framework has gradually at-tracted more attention, namely Masked Image Modeling (MIM) [4, 23]. MIM methods train the model to reconstruct the original content from a masked image. Such practice can help to learn the rich local structures within an image, leading to excellent performance in dense prediction tasks such as object detection [35]. Nevertheless, MIM does not have good linear separability as ID, and usually performs poorly under the few-shot classification settings [1].
Both ID and MIM methods have their own strengths and
ImageNet
COCO
LIN FT1% APb APm
ADE20k mIoU
LVIS rare APm rare
APb
Robustness avg score∗
MoCo-v3 (ID method)
MAE (MIM method)
FT 83.0 83.6 76.7 68.0 63.4 51.1 47.9 51.6 42.7 45.9 84.1
SiameseIM (ours)
Improve w.r.t. MoCo-v3 +1.1
Improve w.r.t. MAE 46.2 52.1 65.1
+4.2 +3.5
+1.7
+0.5 +10.0 +14.0 +0.5 +0.3 78.0
+1.3 47.3 48.1 51.1
+3.8
+3.0 25.5 29.3 30.9
+5.4
+1.6 25.8 29.1 30.1
+4.3
+1.0 43.4 41.8 47.9
+4.5
+6.1
Table 1. SiameseIM surpasses MoCo-v3 (ID method) and MAE (MIM method) on a wide range of downstream tasks. ∗The robustness average score is calculated by averaging top-1 acc of IN-A, IN-R, IN-S, and 1-mCE of IN-C. For detailed results, please refer to Section 4.2.
Figure 1. Comparisons among ID, MIM and SiameseIM. Matching different augmented views can help to learn semantic alignment, which is adopted by ID and SiameseIM. Predicting dense representations from masked images is beneficial to obtain spatial sensitivity, which is adopted by MIM and SiameseIM. weaknesses. We argue that this dilemma is caused by ne-glecting the representation requirements of either seman-tic alignment or spatial sensitivity. Specifically, MIM op-erates within each image independently, regardless of the inter-image relationship. The representations of semanti-cally similar images are not well aligned, which further re-sults in poor linear probing and few-shot learning perfor-mances of MIM. On the other hand, ID only uses a global representation for the whole image, and thus fails to model the intra-image structure. The spatial sensitivity of features is therefore missing, and ID methods usually produce infe-rior results on dense prediction.
To overcome this dilemma, we observe the key factors for semantic alignment and spatial sensitivity: (1) semantic alignment requires that images with similar semantics are projected into nearby representations. This can be achieved by matching different augmented views from the same im-age. Strong augmentations are also beneficial because they provide more invariance to the model; (2) spatial sensitiv-ity needs modeling the local structures within an image.
Predicting dense representations from masked images thus helps, because it models the conditional distribution of im-age content within each image. These observations motivate us to predict the dense representations of an image from a masked view with different augmentations.
To this end, we propose Siamese Image Modeling (SiameseIM), which reconstructs the dense representations of an augmented view, based on another masked view from the same image but with different augmentations (see
Fig. 1). It adopts a Siamese network with an online and a target branch. The online branch consists of an encoder that maps the first masked view into latent representations, and a decoder that reconstructs the representations of the sec-ond view according to the relative positions between these two views. The target branch only contains a momentum encoder that encodes the second view into the prediction target. The encoder is made up of a backbone and a projec-tor. After the pre-training, we only use the online backbone for downstream tasks.
As shown in Tab. 1, SiameseIM is able to surpass both
MIM and ID methods over a wide range of evaluation tasks, including full-data fine-tuning, few-shot learning and linear probing on ImageNet [16], object detection on COCO [36] and LVIS [22], semantic segmentation on ADE20k [58], as well as several robustness benchmarks [26–28, 47]. By gathering semantic alignment and spatial sensitivity in one model, SiameseIM can deliver superior results for all tasks.
We also note that such improvements are more obvious on
ADE20k(∼3 points) and LVIS(∼1.6 point for rare classes) datasets. These long-tailed datasets demands semantic alignment and spatial sensitivity at the same time, and
SiameseIM thus can deliver superior performance on them.
Our contributions can be summarized as follows:
• As a new form of SSL, SiameseIM is proposed to explore the possibilities of self-supervised pre-training.
It dis-plays for the first time that that using only a single dense loss is enough to learn semantic alignment and spatial sensitivity well at the same time;
• Compared with MIM methods, SiameseIM shows that re-constructing another view helps to obtain good semantic alignment. This also suggests that MIM framework can be used to reconstruct other targets with proper guidance, which opens a possible direction for MIM pretraining;
• Compared with ID methods, SiameseIM shows that dense supervision can be applied by matching the dense correspondence between two views strictly through their relative positions. We demonstrate dense supervision can bring a considerable improvement of spatial sensitivity;
• SiameseIM is able to surpass both MIM and ID meth-ods over a wide range of tasks. SiameseIM obtains more improvements in few-shot, long-tail and robustness-concerned scenarios. 2.