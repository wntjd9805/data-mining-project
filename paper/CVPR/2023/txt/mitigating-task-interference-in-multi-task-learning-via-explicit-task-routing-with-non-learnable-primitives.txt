Abstract
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared infor-mation among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter par-titioning with partial overlaps among the tasks.
In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primi-tives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable prim-itives and the explicit decoupling of learnable parame-ters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level clas-sification and pixel-level dense prediction MTL problems.
Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL. 1.

Introduction
Multi-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks
[6, 25, 37]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [14,21,32,39] that these models are suffering from task interference [39], thereby limiting multi-task networks (MTNs) from realizing their full potential.
*Work done as a visiting scholar at Michigan State University.
†Corresponding author (a) ResNet18 (b) Layer 1, 4, and 8 of ResNet18 (from left to right)
Figure 1. (a) Learning progression of multi-task networks (MTNs) on CelebA for eight tasks. Hard-sharing models with fully learn-able parameters (gray) learn rapidly and then suffer from perfor-mance degradation due to conflicting gradients from task interfer-ence. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task rout-ing (ETR; green), and ETR with NLPs (red) do not eliminate but suffer less from task interference. (b) Gradient correlations mea-sured via CKA [15] across all pairs of tasks for different layers of a standard MTN at the end of training. Observe the acute lack of correlation between tasks (low off-diagonal magnitude).
For instance, consider the learning progression of an
MTN with a standard learnable convolutional layer in Fig-ure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. How-ever, the performance starts degrading on further training since the model needs to exploit dissimilar information be-tween the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [15]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.
Several approaches were proposed for mitigating task in-terference in MTNs, including loss/gradient balancing [13, 17, 18, 26, 38], parameter partitioning [2, 21, 23, 29] and ar-chitectural design [7, 14, 22]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a par-tial set of tasks through implicit partitioning, i.e., with no di-rect control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing
multi-task network designs from mitigating the deleterious effects of task interference on their predictive performance.
Relaxing the above design choices is the primary goal of this paper. We propose two complementary design princi-ples, namely explicit task routing (ETR), and non-learnable primitives (NLPs), that explicitly seek to mitigate task in-terference. Through extensive empirical evaluation, we demonstrate that these two complementary ideas, individ-ually and jointly, help mitigate task interference and con-sistently improve the performance of MTNs. As can be observed in Figure 1a, compared to a hard-sharing MTN with a standard learnable convolutional layer (gray curve), an MTN with NLP (blue curve) has better learning charac-teristics, i.e., learn more steadily and not suffer from per-formance degradation. Similarly, MTN with ETR (green curve) and MTN with ETR-NLP (red curve) does not elim-inate task interference but reduce it to an extent. Figure 2 shows an overview of the proposed ETR-NLP networks.
From a network topological perspective, we propose ex-plicit task-routing (ETR), a parameter allocation strategy that partitions the parameters into shared and task-specific branches. More explicitly, it comprises one branch shared across all tasks and task-specific branches, one for each task. Unlike existing parameter partitioning methods, ETR is designed to offer precise and fine-grained control over which and how many parameters are shared or not shared across the tasks. Additionally, ETR is flexible enough to allow existing implicit parameter partitioning methods
[23, 29] to be incorporated into its shared branch.
From a layer design perspective, we propose using non-learnable primitives (NLPs) to extract task-agnostic features and allow each task to choose optimal combinations of these features adaptively. There is growing evidence that features extracted from NLPs can be very effective for single-task settings, including for image classification [12, 24, 34–36], reinforcement learning [8] and modeling dynamical sys-tems [20]. NLPs are attractive for mitigating task interfer-ence in MTL. Since they do not contain learnable param-eters, the task-agnostic features extracted from such layers alleviate the impact of conflicting gradients, thus implic-itly addressing task interference. However, the utility and design of NLPs for multi-task networks have not been ex-plored. We summarize our key contributions below: – We introduce the concept of non-learnable primitives (NLPs) and explicit task routing (ETR) to mitigate task in-terference in multi-task learning. We systematically study the effect of different design choices to determine the opti-mal design of ETR and NLP. – We demonstrate the effectiveness of ETR and NLP through MTNs constructed with only NLPs (MTN-NLPs) and only ETR (MTN-ETR) for both image-level classifica-tion and pixel-level dense prediction tasks. – We evaluate the effectiveness of ETR-NLP networks
Figure 2. ETR-NLP Networks comprise non-learnable primitives to extract diverse task-agnostic features, followed by explicit task routing to control the parameters/features that are shared across all tasks and those that are exclusive to every single task. across three different datasets and compare them against a wide range of baselines for both image-level classification and pixel-level dense prediction tasks. Results indicate that
ETR-NLP networks consistently improve performance by a significant amount. 2.