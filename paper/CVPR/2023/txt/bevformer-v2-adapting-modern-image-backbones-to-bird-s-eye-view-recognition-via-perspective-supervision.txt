Abstract
We present a novel bird’s-eye-view (BEV) detector with perspective supervision, which converges faster and bet-ter suits modern image backbones. Existing state-of-the-art BEV detectors are often tied to certain depth pre-trained backbones like VoVNet, hindering the synergy be-tween booming image backbones and BEV detectors. To address this limitation, we prioritize easing the optimization of BEV detectors by introducing perspective view supervi-sion. To this end, we propose a two-stage BEV detector, where proposals from the perspective head are fed into the bird’s-eye-view head for final predictions. To evaluate the effectiveness of our model, we conduct extensive ablation studies focusing on the form of supervision and the gener-ality of the proposed detector. The proposed method is ver-ified with a wide spectrum of traditional and modern image backbones and achieves new SoTA results on the large-scale nuScenes dataset. The code shall be released soon. 1.

Introduction
Bird’s-eye-view(BEV) recognition models [17,21,25,27, 29, 35, 42] are a class of camera-based models for 3D ob-ject detection. They have attracted interest in autonomous driving as they can naturally integrate partial raw observa-tions from multiple sensors into a unified holistic 3D out-put space. A typical BEV model is built upon an image backbone, followed by a view transformation module that lifts perspective image features into BEV features, which are further processed by a BEV feature encoder and some task-specific heads. Although much effort is put into de-signing the view transformation module [17, 27, 42] and in-corporating an ever-growing list of downstream tasks [9,27] into the new recognition framework, the study of image
*: Equal contribution. (cid:66): Corresponding author, email: daijifeng@tsinghua.edu.cn. backbones in BEV models receives far less attention. As a cutting-edge and highly demanding field, it is natural to in-troduce modern image backbones into autonomous driving.
Surprisingly, the research community chooses to stick with
VoVNet [13] to enjoy its large-scale depth pre-training [26].
In this work, we focus on unleashing the full power of mod-ern image feature extractors for BEV recognition to unlock the door for future researchers to explore better image back-bone design in this field.
However, simply employing modern image backbones without proper pre-training fails to yield satisfactory results.
For instance, an ImageNet [6] pre-trained ConvNeXt-XL
[23] backbone performs just on par with a DDAD-15M pre-trained VoVNet-99 [26] for 3D object detection, albeit the latter has 3.5× parameters of the former. We owe the strug-gle of adapting modern image backbones to the following issues: 1) The domain gap between natural images and au-tonomous driving scenes. Backbones pre-trained on general 2D recognition tasks fall short of perceiving 3D scenes, es-pecially estimating depth. 2) The complex structure of cur-rent BEV detectors. Take BEVFormer [17] as an example.
The supervision signals of 3D bounding boxes and object class labels are separated from the image backbone by the view encoder and the object decoder, each of which is com-prised of transformers of multiple layers. The gradient flow for adapting general 2D image backbones for autonomous driving tasks is distorted by the stacked transformer layers.
In order to combat the difficulties mentioned above in adapting modern image backbones for BEV recognition, we introduce perspective supervision into BEVFormer, i.e. ex-tra supervision signals from perspective-view tasks and di-rectly applied to the backbone. It guides the backbone to learn 3D knowledge missing in 2D recognition tasks and overcomes the complexity of BEV detectors, greatly facili-tating the optimization of the model. Specifically, we build a perspective 3D detection head [26] upon the backbone, which takes image features as input and directly predicts
the 3D bounding boxes and class labels of target objects.
The loss of this perspective head, denoted as perspective loss, is added to the original loss (BEV loss) deriving from the BEV head as an auxiliary detection loss. The two de-tection heads are jointly trained with their corresponding loss terms. Furthermore, we find it natural to combine the two detection heads into a two-stage BEV detector, BEV-Former v2. Since the perspective head is full-fledged, it could generate high-quality object proposals in the perspec-tive view, which we use as first-stage proposals. We encode them into object queries and gather them with the learn-able ones in the original BEVFormer, forming hybrid object queries, which are then fed into the second-stage detection head to generate the final predictions.
We conduct extensive experiments to confirm the effec-tiveness and necessity of our proposed perspective super-vision. The perspective loss facilitates the adaptation of the image backbone, resulting in improved detection per-formance and faster model convergence. While without this supervision, the model cannot achieve comparable re-sults even if trained with a longer schedule. Consequently, we successfully adapt modern image backbones to the BEV model, achieving 63.4% NDS on nuScenes [2] test-set.
Our contributions can be summarized as follows:
• We point out that perspective supervision is key to adapting general 2D image backbones to the BEV model. We add this supervision explicitly by a detec-tion loss in the perspective view.
• We present a novel two-stage BEV detector, BEV-Former v2. It consists of a perspective 3D and a BEV detection head, and the proposals of the former are combined with the object queries of the latter.
• We highlight the effectiveness of our approach by com-bining it with the latest developed image backbones and achieving significant improvements over previous state-of-the-art results on the nuScenes dataset. 2.