Abstract
Dramatic demand for manpower to label pixel-level an-notations triggered the advent of unsupervised semantic segmentation. Although the recent work employing the vi-sion transformer (ViT) backbone shows exceptional perfor-mance, there is still a lack of consideration for task-speciﬁc training guidance and local semantic consistency. To tackle these issues, we leverage contrastive learning by excavating hidden positives to learn rich semantic relationships and ensure semantic consistency in local regions. Speciﬁcally, we ﬁrst discover two types of global hidden positives, task-agnostic and task-speciﬁc ones for each anchor based on the feature similarities deﬁned by a ﬁxed pre-trained back-bone and a segmentation head-in-training, respectively. A gradual increase in the contribution of the latter induces the model to capture task-speciﬁc semantic features. In addi-tion, we introduce a gradient propagation strategy to learn semantic consistency between adjacent patches, under the inherent premise that nearby patches are highly likely to possess the same semantics. Speciﬁcally, we add the loss propagating to local hidden positives, semantically similar nearby patches, in proportion to the predeﬁned similarity scores. With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in COCO-stuff,
Cityscapes, and Potsdam-3 datasets. Our code is available at: https://github.com/hynnsk/HP. 1.

Introduction
Semantic segmentation is a major task for scene under-standing and plays a crucial role in many applications in-cluding medical imaging and autonomous driving [5, 11, 28, 34, 38, 41]. However, existing supervised approaches demand large-scale pixel-level annotations which require huge labeling costs. It has triggered the advent of weakly-supervised [23, 33, 35, 36] and unsupervised semantic seg-mentation [8, 14, 19, 37] which are to learn without expen-sive pixel-level annotations.
*Corresponding author
Image
Label
STEGO
Ours (a) Visualization
Anchor
Local hidden positive
Patch nearby anchor but not local hidden  positive
Anchor
Global hidden positive 0.8 0.6 0.4 0.2 0 y t i r a l i m i
S
- 0.2
Threshold for  global hidden positive (Eq.2)
Patch index (sorted by similarity) e r o c s n o i t n e t t
A 0.6 0.4 0.2 0
Attention score for
Threshold for local hidden positive (Eq.7)
Attention score for 
Image index (b) Global hidden positive (c) Local hidden positive
Figure 1. Assuming a mini-batch comprising two images shown in (a), we describe two types of hidden positives, to leverage for con-trastive learning. (a) With two types of hidden positives introduced in (b) and (c), we provide an example of how our training scheme provides more precise and consistent semantics. (b) (top) Seman-tically analogous patches throughout the mini-batch are selected as global hidden positives. (bottom) Data-driven criterion per anchor is designed for reliable positive collection. With the criterion, se-lected positives are illustrated in (b top). (c) (top) We deﬁne local hidden positives for each anchor to be the adjacent patches with high semantic consistency, i.e., blue boxes. (bottom) Average at-tention scores for adjacent patches from the pretrained transformer architecture. The blue line represents the attention score for local hidden positives while the red line for patches neighboring anchors but having low semantic consistency.
Particularly, unsupervised semantic segmentation is one of the most challenging tasks, since it needs to capture pixel-level semantics from unlabeled data. In this context, clustering-based approaches have been proposed to learn  
semantic-preserving clusters by attracting the augmented views in the pixel-level [8, 19]. They implemented the intu-ition of contrastive learning [3, 6, 7, 13, 15, 25] by ensuring the augmented pairs yield symmetric cluster assignments.
More recently, as discovering pixel-level semantics from scratch is challenging, STEGO [14] broke down the prob-lem into learning the representation and learning the seg-mentation head. With the learned patch-level representation from the seminal work in unsupervised learning [4,39], they train the segmentation head with a distillation strategy. Al-though they have made great advancements, we point out their limitations in that they rely solely on a ﬁxed backbone that is not speciﬁcally trained for the segmentation task and overlook the importance of semantic consistency along the adjacency that could be a crucial clue for segmentation.
To take these into consideration, we leverage contrastive learning based on the mined hidden positives to ensure con-textual consistency along the patches with analogous se-mantics, particularly the nearby patches, as described in
Fig. 1. Speciﬁcally, we elaborately select the pseudo-positive samples (i.e., global hidden positive, GHP) for con-trastive learning to learn semantic consistency. Also, to en-sure local consistency, we propagate the loss gradient to the adjacent patches (i.e., local hidden positive, LHP) in propor-tion to their equivalency. First, the GHP selection process is designed with two types of data reference pools, task-agnostic and task-speciﬁc, to collect the semantically con-sistent patch features throughout the mini-batch per anchor.
For instance, the task-agnostic data reference pool is com-posed of features extracted by the unsupervised pretrained backbone. On the other hand, the task-speciﬁc reference pool is constructed with the features from the segmenta-tion head-in-training to complement task relevance. Based on the two reference pools, two sets of GHP are selected each with generalized and task-speciﬁc perspectives. Sec-ond, to implement the property of locality and prevent the semantics from ﬂuctuating, we propagate the loss gradient to adjacent patches (i.e., LHP) in proportion to the similar-ity scores built within the pretrained backbone. This en-ables the model to learn the relevance of the local context that nearby patches often belong to the same instance.
Our main contributions are summarized as:
• We propose a novel method to discover semantically similar pairs, called global hidden positives, to explic-itly learn the semantic relationship among patches for unsupervised semantic segmentation.
• We utilize the task-speciﬁc features from a model-in-training and validate the effectiveness of progressive increase of their contribution.
• A gradient propagation to nearby similar patches, local hidden positives, is developed to learn local semantic consistency which is the nature of segmentation.
• Our approach outperforms existing state-of-the-art methods across extensive experiments. 2.