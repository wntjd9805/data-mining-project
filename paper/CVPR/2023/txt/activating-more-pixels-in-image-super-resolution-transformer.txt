Abstract
Transformer-based methods have shown impressive per-formance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of
Transformer is still not fully exploited in existing networks.
In order to activate more input pixels for better recon-struction, we propose a novel Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their com-plementary advantages of being able to utilize global statis-tics and strong local fitting capability. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interac-tion between neighboring window features.
In the train-ing stage, we additionally adopt a same-task pre-training strategy to exploit the potential of the model for further im-provement. Extensive experiments show the effectiveness of the proposed modules, and we further scale up the model to demonstrate that the performance of this task can be greatly improved. Our overall method significantly outperforms the state-of-the-art methods by more than 1dB. 1.

Introduction
Single image super-resolution (SR) is a classic prob-lem in computer vision and image processing.
It aims to reconstruct a high-resolution image from a given low-resolution input. Since deep learning has been success-fully applied to the SR task [10], numerous methods based on the convolutional neural network (CNN) have been pro-posed [8, 11, 12, 24, 29, 32, 68, 70] and almost dominate this field in the past few years. Recently, due to the success in natural language processing, Transformer [53] has attracted the attention of the computer vision community. After mak-† Corresponding author.
Figure 1. Performance comparison on PSNR(dB) of the proposed
HAT with the state-of-the-art methods SwinIR [31] and EDT [27].
HAT-L represents a larger variant of HAT. Our approach can sur-pass the state-of-the-art methods by 0.3dB∼1.2dB. ing rapid progress on high-level vision tasks [14, 39, 54],
Transformer-based methods are also developed for low-level vision tasks [6, 57, 65], as well as for SR [27, 31]. Es-pecially, a newly designed network, SwinIR [31], obtains a breakthrough improvement in this task.
Despite the success, “why Transformer is better than
CNN” remains a mystery. An intuitive explanation is that this kind of network can benefit from the self-attention mechanism and utilize long-range information. Thus, we employ the attribution analysis method LAM [15] to ex-amine the involved range of utilized information for recon-struction in SwinIR. Interestingly, we find that SwinIR does
NOT exploit more input pixels than CNN-based methods (e.g., RCAN [68]) in super-resolution, as shown in Fig. 2.
Besides, although SwinIR obtains higher quantitative per-formance on average, it produces inferior results to RCAN in some samples, due to the limited range of utilized infor-mation. These phenomena illustrate that Transformer has a stronger ability to model local information, but the range of its utilized information needs to be expanded. In addi-tion, we also find that blocking artifacts would appear in the intermediate features of SwinIR, as depicted in Fig. 3. It demonstrates that the shift window mechanism cannot per-fectly realize cross-window information interaction.
To address the above-mentioned limitations and further develop the potential of Transformer for SR, we propose a Hybrid Attention Transformer, namely HAT. Our HAT combines channel attention and self-attention schemes, in order to take advantage of the former’s capability in using global information and the powerful representative ability of the latter. Besides, we introduce an overlapping cross-attention module to achieve more direct interaction of ad-jacent window features. Benefiting from these designs, our model can activate more pixels for reconstruction and thus obtains significant performance improvement.
Since Transformers do not have an inductive bias like
CNNs, large-scale data pre-training is important to unlock the potential of such models. In this work, we provide an effective same-task pre-training strategy. Different from
IPT [6] using multiple restoration tasks for pre-training and
EDT [27] using multiple degradation levels for pre-training, we directly perform pre-training using large-scale dataset on the same task. We believe that large-scale data is what really matters for pre-training, and experimental results also show the superiority of our strategy. Equipped with the above designs, HAT can surpass the state-of-the-art meth-ods by a huge margin (0.3dB∼1.2dB), as shown in Fig. 1.
Contributions: 1) We design a novel Hybrid Attention
Transformer (HAT) that combines self-attention, channel attention and a new overlapping cross-attention to activate more pixels for better reconstruction. 2) We propose an ef-fective same-task pre-training strategy to further exploit the potential of SR Transformer and show the importance of large-scale data pre-training for the task. 3) Our method achieves state-of-the-art performance. By further scaling up HAT to build a big model, we greatly extend the perfor-mance upper bound of the SR task. 2.