Abstract
This paper addresses the challenge of efficiently recon-structing volumetric videos of dynamic humans from sparse multi-view videos. Some recent works represent a dynamic human as a canonical neural radiance field (NeRF) and a motion field, which are learned from input videos through differentiable rendering. But the per-scene optimization gen-erally requires hours. Other generalizable NeRF models leverage learned prior from datasets to reduce the optimiza-tion time by only finetuning on new scenes at the cost of visual fidelity. In this paper, we propose a novel method for learning neural volumetric representations of dynamic hu-mans in minutes with competitive visual quality. Specifically, we define a novel part-based voxelized human representa-tion to better distribute the representational power of the network to different human parts. Furthermore, we propose
∗Equal contribution. †Corresponding author. a novel 2D motion parameterization scheme to increase the convergence rate of deformation field learning. Experiments demonstrate that our model can be learned 100 times faster than previous per-scene optimization methods while being competitive in the rendering quality. Training our model on a 512 × 512 video with 100 frames typically takes about 5 minutes on a single RTX 3090 GPU. The code is available on our project page: https://zju3dv.github.io/instant nvr. 1.

Introduction
Creating volumetric videos of human performers has many applications, such as immersive telepresence, video games, and movie production. Recently, some methods
[58, 93] have shown that high-quality volumetric videos can be recovered from sparse multi-view videos by representing dynamic humans with neural scene representations. How-ever, they typically require more than 10 hours of training on
a single GPU. The expensive time and computational costs limit the large-scale application of volumetric videos. Gener-alizable methods [34, 100] utilize learned prior from datasets of dynamic humans to reduce the training time by only fine-tuning on novel human performers. These techniques could increase the optimization speed by a factor of 2-5 at the cost of some visual fidelity.
To speed up the process of optimizing a neural represen-tation for view synthesis of dynamic humans, we analyze the structural prior of the human body and motion, and propose a novel dynamic human representation that achieves 100x speedup during optimization while maintaining competitive visual fidelity. Specifically, to model a dynamic human, we first transform world-space points to a canonical space using a novel motion parameterization scheme and inverse linear blend skinning (LBS) [35]. Then, the color and density of these points are estimated using the canonical human model.
The innovation of our proposed representation is two-fold.
First, we observe that human body parts have different levels of complexity in terms of both shape and texture. For exam-ple, the face of a human performer typically exhibits higher complexity than a flatly textured torso region, thus requiring more representational power to depict. Motivated by this, our method decomposes the canonical human body into multiple parts and represents the human body with a structured set of voxelized NeRF [47] networks to bring the convergence rate of these different parts to the same level. In contrast to a single-resolution representation, the part-based body model utilizes the human body prior to represent the human shape and texture efficiently, heuristically distributing variational representational power to human parts with spatially varying complexity.
Second, we notice that non-rigid deformation of human geometry typically occurs around a surface instead of in a volume, that is, nearby surface points on a parametric human model tend to have similar motion behavior. Thus we propose a novel motion parameterization technique that models the 3D human deformation in a 2D domain, enabling modeling of the motion field using 3D voxelized represen-tation to accelerate the learning. This idea is similar to the displacement map and bump map [14, 15] in traditional com-puter graphics to represent detailed deformation on a 2D texture domain. We extend the technique of displacement map [14, 15] to represent human motions by restricting the originally 3D deformation field [40, 56, 59] on the 2D sur-face of a parametric human model, such as SMPL [43]. This technique allows us to use hybrid representations [48, 98] to model non-rigid deformation by reducing the memory footprint, largely boosting the convergence of the field.
Experiments demonstrate that our method significantly accelerates the optimization of neural human representations while being competitive with state-of-the-art human model-ing methods on rendering quality. As shown in Figure 1, our model can be trained within 5 minutes to produce a volumet-ric video of a dynamic human from a 100-frame monocular video of a 512 × 512 resolution on an RTX 3090 GPU.
To summarize, our key contributions are:
• A novel part-based voxelized human representation for more efficient human body modeling.
• A 2D motion parameterization scheme for more effi-cient deformation field modeling.
• 100x speedup in optimization compared to previous neural human representations while maintaining com-petitive rendering quality. 2.