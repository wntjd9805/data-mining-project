Abstract
The well-documented presence of texture bias in modern convolutional neural networks has led to a plethora of al-gorithms that promote an emphasis on shape cues, often to support generalization to new domains. Yet, common datasets, benchmarks and general model selection strate-gies are missing, and there is no agreed, rigorous evaluation protocol. In this paper, we investigate difficulties and limi-tations when training networks with reduced texture bias. In particular, we also show that proper evaluation and mean-ingful comparisons between methods are not trivial. We introduce BiasBed, a testbed for texture- and style-biased training, including multiple datasets and a range of exist-ing algorithms. It comes with an extensive evaluation pro-tocol that includes rigorous hypothesis testing to gauge the significance of the results, despite the considerable train-ing instability of some style bias methods. Our extensive experiments, shed new light on the need for careful, sta-tistically founded evaluation protocols for style bias (and beyond). E.g., we find that some algorithms proposed in the literature do not significantly mitigate the impact of style bias at all. With the release of BiasBed, we hope to fos-ter a common understanding of consistent and meaning-ful comparisons, and consequently faster progress towards learning methods free of texture bias. Code is available at https://github.com/D1noFuzi/BiasBed 1.

Introduction
Visual object recognition is fundamental to our daily lives. Identifying and categorizing objects in the environ-ment is essential for human survival, indeed our brain is able to assign the correct object class within a fraction of a second, independent of substantial variations in appearance, illumination and occlusion [28]. Recognition mainly takes place along the ventral pathway [7], i.e., visual perception induces a hierarchical processing stream from local patterns to complex features, in feed-forward fashion [28]. Signals are filtered to low frequency and used in parallel also in a top-down manner [2], emphasizing the robustness and in-variance to deviations in appearance.
Inspired by our brain’s visual perception, convolutional neural architectures build upon the hierarchical intuition, stacking multiple convolutional layers to induce feed-forward learning of basic concepts to compositions of com-Indeed, early findings suggested that neu-plex objects. rons in deeper layers are activated by increasingly complex shapes, while the first layers are mainly tailored towards low-level features such as color, texture and basic geometry.
However, recent work indicates the opposite: convolutional neural networks (CNNs) exhibit a strong bias to base their decision on texture cues [11–13, 17], which heavily influ-ences their performance, in particular under domain shifts, which typically affect local texture more than global shape.
This inherent texture bias has led to a considerable body of work that tries to minimize texture and style bias and shift towards “human-like” shape bias in object recognition.
Common to all approaches is the principle of incorporating adversarial texture cues into the learning pipeline – either directly in input space [13, 15, 27] or implicitly in feature space [20,22,30,33]. Perturbing the texture cues in the input forces the neural network to make “texture-free” decisions, thus focusing on the objects’ shapes that remain stable dur-ing training. While texture cues certainly boost perfor-mance in fine-grained classification tasks, where local tex-ture patterns may indicate different classes, they can cause serious harm when the test data exhibit a domain shift w.r.t. training. In this light, texture bias can be seen as a main rea-son for degrading domain generalization performance, and various algorithms have been developed to improve general-ization to new domains with different texture properties (re-spectively image styles), e.g., [13, 15, 20, 22, 27, 30, 33, 40].
However, while a considerable number of algorithms have been proposed to address texture bias, they are neither evaluated on common datasets nor with common evaluation metrics. Moreover they often employ inconsistent model selection criteria or do not report at all how model selection
is done. With the present paper we promote the view that:
• the large domain shifts induced by texture-biased train-ing cause large fluctuations in accuracy, which call for a particularly rigorous evaluation;
• model selection has so far been ignored in the litera-ture; together with the high training volatility, this may have lead to overly optimistic conclusions from spuri-ous results;
• in light of the difficulties of operating under drastic do-main shifts, experimental results should include a no-tion of uncertainty in the evaluation metrics.
Motivated by these findings, we have implemented Bias-Bed, an open-source PyTorch [35] testbed that comes with six datasets of different texture and shape biases, four ad-versarial robustness datasets and eight fully implemented algorithms. Our framework allows the addition of new al-gorithms and datasets with few lines of code, including full flexibility of all parameter settings. In order to tackle the previously discussed limitations and difficulties for evalu-ating such algorithms, we have added a carefully designed evaluation pipeline that includes training multiple runs and employing multiple model selection methods, and we re-port all results based on sound statistical hypothesis tests – all run with a single command. In addition to our novel framework, the present paper makes the following contribu-tions:
• We highlight shortcomings in the evaluation protocols used in recent work on style bias, including the ob-servation that there is a very high variance in the per-formance of different runs of the same algorithm, and even between different checkpoints in the same run with similar validation scores.
• We develop and openly release a testbed that rig-orously compares different algorithms using well-established hypothesis testing methods. This testbed includes several of the most prominent algorithms and datasets in the field, and is easily extensible.
• We observe in our results that current algorithms on texture-bias datasets fail to surpass simple ERM in a statistically significant way, which is the main moti-vation for this work and for using rigorous hypothesis tests for evaluating style bias algorithms.
In Sec. 2, we provide a comprehensive overview of exist-ing work on reducing texture bias. Furthermore we describe the main forms of statistical hypothesis testing. We con-tinue in Sec. 3 with a formal introduction to biased learning, and systematically group existing algorithms into families with common properties. In Sec. 4, we investigate previous evaluation practices in detail, discuss their limitations, and give a formal introduction to hypothesis testing. Sec. 5 de-scribes our proposed BiasBed evaluation framework in de-tail, while Sec. 6 presents experiments, followed by a dis-cussion (Sec. 7), conclusions (Sec. 8) and a view towards the broader impact of our work (Sec. 9). 2.