Abstract 3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most ex-isting work suffers from a three-way dilemma over accu-rate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose
StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume ren-dering.
In addition, it transforms the grid features ac-cording to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant con-tent transformation that makes the transformation invari-*Shijian Lu is the corresponding author. ant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly re-duces memory footprint without degrading multi-view con-sistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry recon-struction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/ 1.

Introduction
Given a set of multi-view images of a 3D scene and an image capturing a target style, 3D style transfer aims to gen-erate novel views of the 3D scene that have the target style consistently across the generated views (Fig. 1). Neural style transfer has been investigated extensively, and state-of-the-art methods allow transferring arbitrary styles in a
zero-shot manner. However, most existing work focuses on style transfer across 2D images [15, 21, 24] but cannot extend to a 3D scene that has arbitrary new views. Prior studies [19, 22, 37, 39] have shown that naively combining 3D novel view synthesis and 2D style transfer often leads to multi-view inconsistency or poor stylization quality, and 3D style transfer should optimize novel view synthesis and style transfer jointly.
However, the current 3D style transfer is facing a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to new styles.
Different approaches have been investigated to resolve the three-way dilemma. For example, multiple style trans-fer [11, 22] requires a set of pre-defined styles but can-not generalize to unseen new styles. Point-cloud-based style transfer [19, 37] requires a pre-trained depth estima-tion module that is prone to inaccurate geometry reconstruc-tion. Zero-shot style transfer with neural radiance fields (NeRF) [8] cannot capture detailed style patterns and tex-tures as it implicitly injects the style information into neu-ral network parameters. Optimization-based style trans-fer [17, 39, 63] suffers from slow optimization and cannot scale with new styles.
In this work, we introduce StyleRF to resolve the three-way dilemma by performing style transformation in the fea-ture space of a radiance field. A radiance field is a contin-uous volume that can restore more precise geometry than point clouds or meshes. In addition, transforming a radi-ance field in the feature space is more expressive with bet-ter stylization quality than implicit methods [8], and it can also generalize to arbitrary styles. We construct a 3D scene representation with a grid of deep features to enable fea-ture transformation. In addition, multi-view consistent style transformation in the feature space could be achieved by either transforming the whole feature grid or transforming the sampled 3D points. We adopt the latter as the former in-curs much more computational cost during training to styl-ize the whole feature grid in every iteration, whereas the latter can reduce computational cost through decreasing the size of training patch and the number of sampled points.
However, applying off-the-shelf style transformations to a batch of sampled 3D points impairs the multi-view consis-tency as they are conditioned on the holistic statistics of the batch. Beyond that, transforming every sampled 3D point is memory-intensive since NeRF needs to query hundreds of sampled points along each ray for rendering a single pixel.
We decompose the style transformation into sampling-invariant content transformation (SICT) and deferred style transformation (DST), the former eliminating the depen-dency on holistic statistics of sampled point batch and the latter deferring style transformation to 2D feature maps for better efficiency.
In SICT, we introduce volume-adaptive normalization that learns the mean and variance of the whole volume instead of computing them from a sampled batch. In addition, we apply channel-wise self-attention to transform each 3D point independently to make it condi-tioned on the feature of that point regardless of the holis-In DST, we defer the tic statistics of the sampled batch. style transformation to the volume-rendered 2D feature maps based on the observation that the style transforma-tion of each point is the same. By formulating the style transformation by pure matrix multiplication and adaptive bias addition, transforming 2D feature maps is mathemat-ically equivalent to transforming 3D point features but it saves computation and memory greatly. Thanks to the memory-efficient representation of 3D scenes and deferred style transformation, our network can train with 256 Ã— 256 patches directly without requiring sub-sampling like previ-ous NeRF-based 3D style transfer methods [8, 11, 22].
The contributions of this work can be summarized in three aspects. First, we introduce StyleRF, an innovative zero-shot 3D style transfer framework that can generate zero-shot high-quality 3D stylization via style transforma-tion within the feature space of a radiance field. Second, we design sampling-invariant content transformation and deferred style transformation, the former achieving multi-view consistent transformation by eliminating dependency on holistic statistics of sampled point batch while the lat-ter greatly improves stylization efficiency by deferring style transformation to 2D feature maps. Third, extensive experi-ments show that StyleRF achieves superior 3D style transfer with accurate geometry reconstruction, high-quality styliza-tion, and great generalization to new styles. 2.