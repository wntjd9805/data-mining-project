Abstract
How to effectively explore spatial and temporal informa-tion is important for video deblurring. In contrast to exist-ing methods that directly align adjacent frames without dis-crimination, we develop a deep discriminative spatial and temporal network to facilitate the spatial and temporal fea-ture exploration for better video deblurring. We ﬁrst de-velop a channel-wise gated dynamic network to adaptively explore the spatial information. As adjacent frames usually contain different contents, directly stacking features of ad-jacent frames without discrimination may affect the laten-t clear frame restoration. Therefore, we develop a simple yet effective discriminative temporal feature fusion module to obtain useful temporal features for latent frame restora-tion. Moreover, to utilize the information from long-range frames, we develop a wavelet-based feature propagation method that takes the discriminative temporal feature fu-sion module as the basic unit to effectively propagate main structures from long-range frames for better video deblur-ring. We show that the proposed method does not require additional alignment methods and performs favorably a-gainst state-of-the-art ones on benchmark datasets in terms of accuracy and model complexity. 1.

Introduction
With the rapid development of hand-held video captur-ing devices in our daily life, capturing high-quality clear videos becomes more and more important. However, due to the moving objects, camera shake, and depth variation dur-ing the exposure time, the captured videos usually contain signiﬁcant blur effects. Thus, there is a great need to restore clear videos from blurred ones so that they can be pleasantly viewed on display devices and facilitate the following video understanding problems.
Different from single image deblurring that explores spa-∗Co-ﬁrst authorship
†Corresponding author
Figure 1. Floating point operations (FLOPs) vs. video deblurring performance on the GoPro dataset [24]. Our model achieves fa-vorable results in terms of accuracy and FLOPs. tial information for blur removal, video deblurring is more challenging as it needs to model both spatial and tempo-ral information. Conventional methods usually use optical
ﬂow [2, 9, 14, 36] to model the blur in videos and then joint-ly estimate optical ﬂow and latent frames under the con-straints by some assumed priors. As pointed out by [26], these methods usually lead to complex optimization prob-lems that are difﬁcult to solve. In addition, improper priors will signiﬁcantly affect the quality of restored videos.
Instead of using assumed priors, lots of methods develop kinds of deep convolutional neural networks (CNNs) to ex-plore spatial and temporal information for video deblurring.
Several approaches stack adjacent frames as the input of C-NN models [29] or employ spatial and temporal 3D convo-lution [40] for latent frame restoration. Gast et al. [10] show that using proper alignment strategies in deep CNNs would improve deblurring performance. To this end, several meth-ods introduce alignment modules in deep neural networks.
The commonly used alignment modules for video deblur-ring mainly include optical ﬂow [26], deformable convolu-tion [32], and so on. However, estimating alignment infor-mation from blurred adjacent frames is not a trivial task due to the inﬂuence of motion blur.
In addition, using align-ment modules usually leads to large deep CNN models that
are difﬁcult to train and computationally expensive. For ex-ample, the CDVDTSP method [26] with optical ﬂow as the alignment module has 16.2 million parameters with FLOP-s of 357.79G while the EDVR method [32] using the de-formable convolution as the alignment module has 23.6 mil-lion parameters with FLOPs of 2298.97G. Therefore, it is of great interest to develop a lightweight deep CNN model with lower computational costs to overcome the limitation-s of existing alignment methods in video deblurring while achieving better performance.
Note that most existing methods restore each clear frame based on limited local frames, where the temporal infor-mation from non-local frames is not fully explored. To overcome this problem, several methods employ recurrent neural networks to better model temporal information for video deblurring [15]. However, these methods have limit-ed capacity to transfer the useful information temporally for latent frame restoration as demonstrated in [41]. To rem-edy this limitation, several methods recurrently propagate information of non-local frames with some proper atten-tion mechanisms [41]. However, if the features of non-local frames are not estimated correctly, the errors will accumu-late in the recurrent propagation process, which thus affects video deblurring. As the temporal information exploration is critical for video deblurring, it is a great need to devel-op an effective propagation method that can discriminative-ly propagate useful information from non-local frames for better video restoration.
In this paper, we develop an effective deep discriminative spatial and temporal network (DSTNet) to distinctively ex-plore useful spatial and temporal information from videos for video deblurring. Motivated by the success of multi-layer perceptron (MLP) models that are able to model glob-al contexts, we ﬁrst develop a channel-wise gated dynamic network to effectively explore spatial information. In addi-tion, to exploit the temporal information, instead of directly stacking estimated features from adjacent frames without discrimination, we develop a simple yet effective discrim-inative temporal feature fusion module to fuse the features generated by the channel-wise gated dynamic network so that more useful temporal features can be adaptively ex-plored for video deblurring.
However, the proposed discriminative temporal feature fusion module does not utilize the information from long-range frames. Directly repeating this strategy in a recur-rent manner is computationally expensive and may propa-gate and accumulate the estimation errors of features from long-range frames, leading to adverse effects on the ﬁnal video deblurring. To solve this problem, we develop a wavelet-based feature propagation method that effective-ly propagates main structures from long-range frames for better video deblurring. Furthermore, the deep discrimina-tive spatial and temporal network does not require addition-al alignment modules (e.g., optical ﬂow used in [26], de-formable convolution used in [32]) and is thus efﬁcient yet effective for video deblurring as shown in Figure 1.
The main contributions are summarized as follows:
• We propose a channel-wise gated dynamic network (CWGDN) based on multi-layer perceptron (MLP) models to explore the spatial information. A detailed analysis demonstrates that the proposed CWGDN is more effective for video deblurring.
• We develop a simple yet effective discriminative tem-poral feature fusion (DTFF) module to explore useful temporal features for clear frame reconstruction.
• We develop a wavelet-based feature propagation (WaveletFP) method to efﬁciently propagate useful structures from long-range frames and avoid error ac-cumulation for better video deblurring.
• We formulate the proposed network in an end-to-end trainable framework and show that it performs favor-ably against state-of-the-art methods in terms of accu-racy and model complexity. 2.