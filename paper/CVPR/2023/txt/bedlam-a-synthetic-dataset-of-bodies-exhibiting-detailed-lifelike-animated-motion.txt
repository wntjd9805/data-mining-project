Abstract
We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estima-tion from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achiev-ing sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BED-LAM dataset contains monocular RGB videos with ground-It includes a diver-truth 3D bodies in SMPL-X format. sity of body shapes, motions, skin tones, hair, and cloth-ing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train vari-ous HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite train-ing with synthetic data. We use BEDLAM to gain insights
*The authors contributed equally and are listed alphabetically.
†This work was performed when JL was at MPI-IS. into what model design choices are important for accu-racy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling oth-ers to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/. 1.

Introduction
The estimation of 3D human pose and shape (HPS) from images has progressed rapidly since the introduc-tion of HMR [32], which uses a neural network to regress
SMPL [45] pose and shape parameters from an image. A steady stream of new methods have improved the accuracy of the estimated 3D bodies [21, 33, 35, 38, 41, 73, 93]. The progress, however, entangles two things: improvements to the architecture and improvements to the training data. This makes it difficult to know which matters most. To answer
this, we need a dataset with real ground truth 3D bodies and not simply 2D joint locations or pseudo ground truth.
To that end, we introduce a new, realistic, synthetic dataset called BEDLAM (Bodies Exhibiting Detailed Lifelike An-imated Motion) and use it to analyze the current state of the art (SOTA). Fig. 1 shows example images from BEDLAM along with the ground-truth SMPL-X [57] bodies.
Theoretically, synthetic data has many benefits. The ground truth is “perfect” by construction, compared with existing image datasets. We can ensure diversity of the training data across skin tones, body shapes, ages, etc., so that HPS methods are inclusive. The data can also be easily repurposed to new cameras, scenes, and sensors. Conse-quently, there have been many attempts to create synthetic datasets to train HPS methods. While prior work has shown synthetic data is useful, it has not been sufficient so far. This is likely due to the lack of realism and diversity in existing synthetic datasets.
In contrast, BEDLAM provides the realism necessary to test whether “synthetic data is all you need”. Using BED-LAM, we evaluate different network architectures, back-bones, and training data and find that training only using synthetic data produces methods that generalize to real im-age benchmarks, obtaining SOTA accuracy on both 3D hu-man pose and 3D body shape estimation. Surprisingly, we find that even basic methods like HMR [32] achieve SOTA performance on real images when trained on BEDLAM.
Dataset. BEDLAM contains monocular RGB videos to-gether with ground truth 3D bodies in SMPL-X format. To create diverse data, we use 271 body shapes (109 men and 162 women), with 100 skin textures from Meshcapade [3] covering a wide range of skin tones. In contrast to previous work, we add 27 different types of hair (Reallusion [1]) to the head of SMPL-X. To dress the body, we hired a pro-fessional 3D clothing designer to make 111 outfits, which we drape and simulate on the body using CLO3D [2]. We also texture the clothing using 1691 artist-designed textures.
The bodies are animated using 2311 motions sampled from
AMASS [47]. Because AMASS does not include hand mo-tions, we replace the static hands with hand motions sam-pled from the GRAB dataset [74]. We render single people as well as groups of people (varying from 3-10) moving in a variety of 3D scenes (8) and HDRI panoramas (95). We use a simple method to place multiple people in the scenes so that they do not collide and use simulated camera motions with various focal lengths. The synthetic image sequences are rendered using Unreal Engine 5 [5] at 30 fps with mo-tion blur. In total, BEDLAM contains around 380K unique image frames with 1-10 people per image, for a total of 1M unique bounding boxes with people.
We divide BEDLAM into training, validation, and test sets with 75%, 20% and 5% of the total bounding boxes respectively. While we make all the image data available, we withhold the SMPL-X ground truth from the test set and provide an automated evaluation server. For the training and validation sets, we provide all the SMPL-X animations, the 3D clothing, skin textures, and all freely available assets.
Where we have used commercial assets, we provide infor-mation about how to obtain the data and replicate our re-sults. We also provide the details necessary for researchers to create their own data.
Evaluation. With sufficient high-quality training data, fairly simple neural-network architectures often produce
SOTA results on many vision tasks.
Is this true for HPS regression? To tackle this question, we train two different baseline methods (HMR [32] and CLIFF [38]) on varying amounts of data and with different backbones; HMR repre-sents the most basic method and CLIFF the recent SOTA.
Since BEDLAM provides paired images with SMPL-X pa-rameters, we train methods to directly regress these parame-ters; this simplifies the training compared with methods that use 2D training data. We evaluate on natural-image datasets including 3DPW [79] and RICH [26], a laboratory dataset (Human3.6M [27]), as well as two datasets that evaluate body shape accuracy (SSP-3D [66] and HBW [16]).
Surprisingly, despite its age, we find that training HMR on synthetic data produces results on 3DPW that are bet-ter than many recently published results and are close to
CLIFF. We find that the backbone has a large impact on accuracy, and pre-training on COCO is significantly better than pre-training on ImageNet or from scratch. We perform a large number of experiments in which we train with just synthetic data, just real data, or synthetic data followed by fine tuning on real data. We find that there is a significant benefit to training on synthetic data over real data and that fine tuning with real data offers only a small benefit.
A key property of BEDLAM is that it contains realisti-cally dressed people with ground truth body shape. Con-sequently, we compare the performance of methods trained on BEDLAM with two SOTA methods for body shape re-gression: SHAPY [16] and Sengupta et al. [67] using both the HBW and SSP-3D datasets. CLIFF trained with BED-LAM does well on both datasets, achieving the best overall of all methods tested. This illustrates how methods trained on BEDLAM generalize across tasks and datasets.
Summary. We propose a large synthetic dataset of re-alistic moving 3D humans. We show that training on syn-thetic dataset alone, even with a basic network architecture, produces accurate 3D human pose and shape estimates on real data. BEDLAM enables us to perform an extensive meta-ablation study that illuminates which design decisions are most important. While we focus on HPS, the dataset has many other uses in learning 3D clothing models and action recognition. BEDLAM is available for research purposes together with an evaluation server and the assets needed to generate new datasets.
2.