Abstract
In many real scenarios, data are often divided into a handful of artificial super categories in terms of ex-pert knowledge rather than the representations of images.
Concretely, a superclass may contain massive and vari-ous raw categories, such as refuse sorting. Due to the lack of common semantic features, the existing classifica-tion techniques are intractable to recognize superclass with-out raw class labels, thus they suffer severe performance damage or require huge annotation costs. To narrow this gap, this paper proposes a superclass learning framework, called SuperClass Learning with Representation Enhance-ment(SCLRE), to recognize super categories by leverag-ing enhanced representation. Specifically, by exploiting the self-attention technique across the batch, SCLRE col-lapses the boundaries of those raw categories and enhances the representation of each superclass. On the enhanced representation space, a superclass-aware decision bound-ary is then reconstructed. Theoretically, we prove that by leveraging attention techniques the generalization error of
SCLRE can be bounded under superclass scenarios. Exper-imentally, extensive results demonstrate that SCLRE outper-forms the baseline and other contrastive-based methods on
CIFAR-100 datasets and four high-resolution datasets. 1.

Introduction
In recent decades, basic-level raw categorization (e.g. cats vs dogs, apples vs bananas) has greatly developed
[9, 27] while high-level or super-coarse-grained visual cat-egorization (e.g., recyclable waste vs kitchen waste, crea-tures vs non-creatures) has received little attention. In many real scenarios, there often exist a handful of high-level cat-egories, wherein numerous images from diverse basic-level categories share one common label. We tend to define this kind of super-coarse-grained class as Superclass.
*Corresponding Author
Figure 1. Illustration of superclass learning. The samples from a same superclass will be scatteredly distributed in the embed-ding space. The process of superclass learning is to break old domains and construct new domains. Red indicates the superclass of kitchen waste, and blue indicates the superclass of recyclable waste.
Refuse sorting, as an example, is such a recognization problem with four superclasses, i.e., kitchen waste, recy-clable waste, hazardous waste, and others waste. One task of refuse sorting is to accurately collect various items, such as rotten fruits, bones, raw vegetables, and eggshells, into kitchen waste. Traditional recognition needs to identify what exact basic-level categories they are, then sort them out. Obviously, it is wasteful and unrealistic for superclass identification.
Essentially, high-level superclasses contain two charac-teristics, remarkably distinct from basic-level classes. First, the basic-level classes contained in superclass problems are usually scattered and share few common features. As de-picted in the top-left corner subgraph of Fig. 1, the fruit apple, bone, and eggs are remote from each other in fea-ture spaces, though all of them belong to kitchen waste.
Second, the instances from two distinct superclasses may share common features. Just as illustrated in Fig. 1, fruit apple, from kitchen waste, and toy apple, from recyclable waste, are close to each other as they share common seman-tic features. Obviously, the above-mentioned characteristics indicate that the smoothness assumption [27] under basic-level classification (nearby images tend to have the same label) does not hold in the superclass scenarios. Thus, the existing classification techniques based on smoothness as-sumption are not practically deployable and scalable and they may suffer severe performance damage in superclass settings. Accordingly, it is valuable and promising to inves-tigate superclass identification.
To tackle the superclass problems, we have to address two main challenges. First, we need to break the origi-nal decision boundaries of basic-level classes and disclose a superclass-aware boundary at the basic class level. As depicted in bottom subgraph (a) of Fig. 1, the boundary of the apple domain is original, however, it is useless and even harmful for the refuse sorting as both fruit apples and toy apples belong to this domain. To get the required domain boundary, the apple domain needs to be separated into the fruit domain and toy domain by leveraging their individ-ual local features, as depicted in the bottom subgraph (b) of Fig. 1. In superclass scenarios, it is not enough to in-vestigate the boundary at a basic class level. Consequently, the second challenge is to reconstruct a decision boundary at a superclass level. To achieve this end, it is necessary to merge the domain of classes, such as fruit apples, eggs, and bones into a new rotten superclass domain, as depicted in the bottom subgraph (c) of Fig. 1.
In this paper, we propose a SuperClass Learning frame-work with Representation Enhancement. Considering that the semantic representation at the basic class level is not workable for superclass recognition, we propose one cross-instance attention module which could seize the representa-tion across the instances with the same superclass label. By leveraging contrastive adjustment loss, the attention mech-anism enhances this representation. Moreover, to overcome the imbalance distribution of superclasses, we adopt target adjustment loss to reconstruct a superclass-aware decision boundary on the enhanced representation space.
In summary, this paper makes the following contribu-tions:
• We propose an under-study but realistic problem, su-perclass identification, that has notably distinct distri-bution from basic-level categorization.
• We propose a novel representation enhancement method by leveraging cross-instance attention and then exploit it in superclass identification. And by theo-retical analyses, we verify that this self-attention tech-nique can bound the generalization error of superclass recognition.
• Extensive experiments demonstrate that SCLRE out-performs the SOTA classification techniques on one artificial superclass dataset and three real datasets.
The remainder of the paper is organized as follows: in
Sec. 2 we briefly review related work and in Sec. 3 we de-scribe our method for superclass recognition. Then, exten-sive experiments and generalization error analysis are con-ducted in Secs. 4 and 5. Finally, Sec. 6 draws a brief con-clusion. 2.