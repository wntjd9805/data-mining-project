Abstract 1.

Introduction
Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typi-cally caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D an-notations for the study of physically consistent and synchro-nised motion of hands and articulated objects. To this end, we introduce ARCTIC – a dataset of two hands that dex-terously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and de-tailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We pro-pose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense rela-tive hand-object distances must be estimated from images.
We introduce two baselines ArcticNet and InterField, re-spectively and evaluate them qualitatively and quantita-tively on ARCTIC. Our code and data are available at https://arctic.is.tue.mpg.de.
Humans constantly manipulate complex objects: we open our laptop’s cover to work, we apply spray to clean, we carefully control our fingers to cut with scissors – rigid and articulated parts of objects move together with our hands.
Inanimate objects only move or deform if external forces are applied to them. The study of the physically consis-tent dynamics of hands and objects during manipulation has so far been under-researched in the hand pose estima-tion literature. This is partly because existing hand-object datasets [8, 18, 19, 21, 30, 34] are mostly limited to grasping of rigid objects and contain few if any examples of rich and dexterous manipulation of articulated objects.
To enable the study of dexterous articulated hand-object manipulation, we collect a novel dataset called ARCTIC (ARticulated objeCTs in InteraCtion). ARCTIC consists of video sequences of multi-view RGB frames, and each frame is paired with accurate 3D hand and object meshes.
ARCTIC contains data from 10 subjects interacting with 11 articulated objects, resulting in a total of 2.1M RGB im-ages. Images are captured from multiple synchronized and calibrated views, including 8 static allocentric views and 1 moving egocentric view. To capture accurate 3D meshes
during manipulation, we synchronize color cameras with 54 high-resolution Vicon MoCap cameras [66]. These al-low the use of small MoCap markers that do not interfere with hand-object interaction and are barely visible in the images. We then fit pre-scanned human and object meshes to the observed markers [35,56]. The objects consist of two rigid parts that rotate about a shared axis such as the flip phone in Fig. 1 (for all objects, see SupMat).
Our dataset enables two novel tasks: (1) consistent mo-tion reconstruction, (2) interaction field estimation. For consistent motion reconstruction, given a monocular video, the task is to reconstruct the 3D motion of two hands and an articulated object.
In particular, the reconstructed hand-object meshes should have spatio-temporally consis-tent hand-object contact, object articulation, and smooth motion during interaction. This task has several chal-lenges: (1) Spatio-temporal consistency requires precise hand-object 3D alignment for all frames; (2) This precision is hard to achieve due to depth ambiguity and severe occlu-sions during dexterous manipulation; (3) The unconstrained interaction causes more variations in hand pose and contact than in existing datasets [8, 18, 19, 34] (see Fig. 2).
As an initial step towards addressing these challenges, and to provide baselines for future work, we introduce
ArcticNet to reconstruct the motions of two hands and an articulated object from a video. ArcticNet uses an encoder-decoder architecture to estimate parameters of the
MANO hand model [45] for the two hands, and our artic-ulated object model. We experiment with two variations of
ArcticNet: a single-frame model and a temporal model with a recurrent architecture inspired by [28]. We provide quali-tative and quantitative results for future comparison.
When studying hand-object interaction, contact is impor-tant [17, 67]. Some approaches [22, 67] explore the task of binary contact estimation from a single RGB image. In the two-handed manipulation setting, hands can be near the ob-ject but not in contact. To understand the dynamic, relative spatial configuration between hands and objects in more de-tail, even when not in contact, we propose the general task of interaction field estimation from RGB images. The goal is to estimate, for each hand vertex, the shortest distance to the object mesh and vice versa (see Fig. 6 for a visualiza-tion). We introduce a baseline, InterField, for this task and benchmark both a single-frame and a recurrent version of
InterField on ARCTIC for future comparison.
In summary, our contributions are as follows: (1) We present ARCTIC, the first large-scale dataset of two hands that dexterously manipulate articulated objects, with multi-view RGB images paired with accurate 3D meshes; (2) We introduce two novel tasks of consistent motion reconstruc-tion and interaction field estimation to study the physically consistent motion of hands and articulated objects; (3) We provide baselines for both tasks on ARCTIC. 2.