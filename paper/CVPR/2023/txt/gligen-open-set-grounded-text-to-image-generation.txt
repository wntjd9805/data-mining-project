Abstract
Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image
Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image dif-fusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condi-tion inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN’s zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.
§ Part of the work performed at Microsoft; ¶ Co-senior authors
1.

Introduction
Image generation research has witnessed huge advances in recent years. Over the past couple of years, GANs [13] were the state-of-the-art, with their latent space and con-ditional inputs being well-studied for controllable manipu-lation [42, 54] and generation [25, 27, 41, 75]. Text condi-tional autoregressive [46, 67] and diffusion [45, 50] models have demonstrated astonishing image quality and concept coverage, due to their more stable learning objectives and large-scale training on web image-text paired data. These models have gained attention even among the general public due to their practical use cases (e.g., art design and creation).
Despite exciting progress, existing large-scale text-to-image generation models cannot be conditioned on other input modalities apart from text, and thus lack the ability to precisely localize concepts, use reference images, or other conditional inputs to control the generation process. The cur-rent input, i.e., natural language alone, restricts the way that information can be expressed. For example, it is difficult to describe the precise location of an object using text, whereas bounding boxes / keypoints can easily achieve this, as shown in Figure 1. While conditional diffusion models [9, 47, 49] and GANs [24, 33, 42, 64] that take in input modalities other than text for inpainting, layout2img generation, etc., do exist, they rarely combine those inputs for controllable text2img generation.
Moreover, prior generative models—regardless of the generative model family—are usually independently trained on each task-specific dataset. In contrast, in the recognition field, the long-standing paradigm has been to build recogni-tion models [29, 37, 76] by starting from a foundation model pretrained on large-scale image data [4, 15, 16] or image-text pairs [30, 44, 68]. Since diffusion models have been trained on billions of image-text pairs [47], a natural question is:
Can we build upon existing pretrained diffusion models and endow them with new conditional input modalities? In this way, analogous to the recognition literature, we may be able to achieve better performance on other generation tasks due to the vast concept knowledge that the pretrained models have, while acquiring more controllability over existing text-to-image generation models.
With the above aims, we propose a method for providing new grounding conditional inputs to pretrained text-to-image diffusion models. As shown in Figure 1, we still retain the text caption as input, but also enable other input modalities such as bounding boxes for grounding concepts, grounding reference images, grounding part keypoints, etc. The key challenge is preserving the original vast concept knowledge in the pretrained model while learning to inject the new grounding information. To prevent knowledge forgetting, we propose to freeze the original model weights and add new trainable gated Transformer layers [61] that take in the new grounding input (e.g., bounding box). During training, we gradually fuse the new grounding information into the pretrained model using a gated mechanism [1]. This design enables flexibility in the sampling process during generation for improved quality and controllability; for example, we show that using the full model (all layers) in the first half of the sampling steps and only using the original layers (without the gated Transformer layers) in the latter half can lead to generation results that accurately reflect the grounding conditions while also having high image quality.
In our experiments, we primarily study grounded text2img generation with bounding boxes, inspired by the recent scaling success of learning grounded language-image understanding models with boxes in GLIP [31]. To en-able our model to ground open-world vocabulary con-cepts [29,31,69,72], we use the same pre-trained text encoder (for encoding the caption) to encode each phrase associated with each grounded entity (i.e., one phrase per bounding box) and feed the encoded tokens into the newly inserted layers with their encoded location information. Due to the shared text space, we find that our model can generalize to unseen objects even when only trained on the COCO [36] dataset. Its generalization on LVIS [14] outperforms a strong fully-supervised baseline by a large margin. To further im-prove our model’s grounding ability, we unify the object detection and grounding data formats for training, following
GLIP [31]. With larger training data, our model’s general-ization is consistently improved.
Contributions. 1) We propose a new text2img genera-tion method that endows new grounding controllability over existing text2img diffusion models. 2) By preserving the pre-trained weights and learning to gradually integrate the new localization layers, our model achieves open-world grounded text2img generation with bounding box inputs, i.e., synthesis of novel localized concepts unobserved in training. 3) Our model’s zero-shot performance on layout2img tasks signifi-cantly outperforms the prior state-of-the-art, demonstrating the power of building upon large pretrained generative mod-els for downstream tasks. 2.