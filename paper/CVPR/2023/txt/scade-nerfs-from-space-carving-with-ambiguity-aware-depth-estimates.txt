Abstract 1.

Introduction
Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction from multiple 2D input views. However, a well-known drawback of NeRFs is the less-than-ideal per-formance under a small number of views, due to insufficient constraints enforced by volumetric rendering. To address this issue, we introduce SCADE, a novel technique that improves
NeRF reconstruction quality on sparse, unconstrained in-put views for in-the-wild indoor scenes. To constrain NeRF reconstruction, we leverage geometric priors in the form of per-view depth estimates produced with state-of-the-art monocular depth estimation models, which can generalize across scenes. A key challenge is that monocular depth es-timation is an ill-posed problem, with inherent ambiguities.
To handle this issue, we propose a new method that learns to predict, for each view, a continuous, multimodal distribu-tion of depth estimates using conditional Implicit Maximum
Likelihood Estimation (cIMLE). In order to disambiguate exploiting multiple views, we introduce an original space carving loss that guides the NeRF representation to fuse multiple hypothesized depth maps from each view and distill from them a common geometry that is consistent with all views. Experiments show that our approach enables higher fidelity novel view synthesis from sparse views. Our project page can be found at scade-spacecarving-nerfs.github.io.
Neural radiance fields (NeRF) [24] have enabled high fidelity novel view synthesis from dozens of input views.
Such number of views are difficult to obtain in practice, however. When given only a small number of sparse views, vanilla NeRF tends to struggle with reconstructing shape accurately, due to inadequate constraints enforced by the volume rendering loss alone.
Shape priors can help remedy this problem. Various forms of shape priors have been proposed for NeRFs, such as object category-level priors [13], and handcrafted data-independent priors [25]. The former requires knowledge of category labels and is not applicable to scenes, and the latter is agnos-tic to the specifics of the scene and only encodes low-level regularity (e.g., local smoothness). A form of prior that is both scene-dependent and category-agnostic is per-view monocular depth estimates, which have been explored in prior work [6,32]. Unfortunately, monocular depth estimates are often inaccurate, due to estimation errors and inherent ambiguities, such as albedo vs. shading (cf. check shadow illusion), concavity vs. convexity (cf. hollow face illusion), distance vs. scale (cf. miniature cinematography), etc. As a result, the incorrect or inconsistent priors imposed by such depth estimates may mislead the NeRF into reconstructing incorrect shape and produce artifacts in the generated views.
In this paper, we propose a method that embraces the uncertainty and ambiguities present in monocular depth esti-mates, by modelling a probability distribution over depth es-timates. The ambiguities are retained at the stage of monocu-lar depth estimation, and are only resolved once information from multiple views are fused together. We do so with a prin-cipled loss defined on probability distributions over depth estimates for different views. This loss selects the subset of modes of probability distributions that are consistent across all views and matches them with the modes of the depth distribution along different rays as modelled by the NeRF.
It turns out that this operation can be interpreted as a proba-bilistic analogue of classical depth-based space carving [17].
For this reason, we dub our method Space Carving with
Ambiguity-aware Depth Estimates, or SCADE for short.
Compared to prior approaches of depth supervision [6,32] that only supervise the moments of NeRF’s depth distribu-tion (rather than the modes), our key innovation is that we supervise the modes of NeRF’s depth distribution. The su-pervisory signal provided by the former is weaker than the latter, because the former only constrains the value of an in-tegral aggregated along the ray, whereas the latter constrains the values at different individual points along the ray. Hence, the supervisory signal provided by the former is 2D (because it integrates over a ray), whereas the supervisory signal pro-vided by the our method is 3D (because it is point-wise) and thus can be more fine-grained.
An important technical challenge is modelling probability distributions over depth estimates. Classical approaches use simple distributions with closed-form probability densities such as Gaussian or Laplace distributions. Unfortunately these distributions are not very expressive, since they only have a single mode (known as “unimodal”) and have a fixed shape for the tails. Since each interpretation of an ambiguous image should be a distinct mode, these simple unimodal dis-tributions cannot capture the complex ambiguities in depth estimates. Naïve extensions like a mixture of Gaussians are also not ideal because some images are more ambiguous than others, and so the number of modes needed may differ for the depth estimates of different images. Moreover, learning such a mixture requires backpropagating through E-M, which is nontrivial. Any attempt at modifying the probability density to make it capable of handling a variable number of modes can easily run into an intractable partition function [4,10,31], which makes learning difficult because maximum likelihood requires the ability to evaluate the probability density, which is a function of the partition function.
To get around this conundrum, we propose representing the probability distribution with a set of samples generated from a neural network. Such a distribution can be learned with a conditional GAN; however, because GANs suffer from mode collapse, they cannot model multiple modes [11] and are therefore unsuited to modelling ambiguity. Instead, we propose leveraging conditional Implicit Maximum Like-lihood Estimation (cIMLE) [19, 28] to learn the distribution, which is designed to avoid mode collapse.
We consider the challenging setting of leveraging out-of-Figure 2. Ambiguities from a single image. We show samples from our ambiguity-aware depth estimates that is able to handle different types of ambiguities. (Top) An image of a chair taken from the top-view. Without context of the scene, it is unclear that it is an image of a chair. We show different samples from our ambiguity-aware depth estimates that are able to capture different degrees of convextiy. (Bottom) An image of a cardboard under bad lighting conditions that capture the albedo vs shading ambiguity that is also represented by our different samples. domain depth priors to train NeRFs on real-world indoor scenes with sparse views. Under this setting, the depth priors we use are trained on a different dataset (e.g., Taskonomy) from the scenes our NeRFs are trained on (e.g., ScanNet).
This setting is more challenging than usual due to domain gap between the dataset the prior is trained on and the scenes
NeRF is asked to reconstruct. We demonstrate that our method outperforms vanilla NeRF and NeRFs with supervi-sion from depth-based priors in novel view synthesis.
In summary, our key contributions include:
• An method that allows the creation of NeRFs in uncon-strained indoor settings from only a modest number of 2D views by introducing a sophisticated way to exploit ambiguity-aware monocular depth estimation.
• A novel way to sample distributions over image depth estimates based on conditional Implicit Maximum Like-lihood Estimation that can represent depth ambiguities and capture a variable number of discrete depth modes.
• A new space-carving loss that can be used in the NeRF formulation to optimize for a mode-seeking 3D den-sity that helps select consistent depth modes across the views and thus compensate for the under-constrained photometric information in the few view regime. 2.