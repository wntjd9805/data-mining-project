Abstract jects in a given image, and modifying global qualities such as lighting and color.
Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of genera-tive AI, synthesizing diverse images with highly complex visual concepts. However, a pivotal challenge in leverag-ing such models for real-world content creation is provid-ing users with control over the generated content. In this paper, we present a new framework that takes text-to- im-age synthesis to the realm of image-to-image translation – given a guidance image and a target text prompt as input, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that com-plies with the target text, while preserving the semantic lay-out of the guidance image. Specifically, we observe and empirically demonstrate that fine-grained control over the generated structure can be achieved by manipulating spa-tial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the translated image, requiring no training or fine-tuning. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing the class and appearance of ob-∗ Equal contribution. 1.

Introduction
With the rise of text-to-image foundation models – billion-parameter models trained on a massive amount of text-image data, it seems that we can translate our imagina-tion into high-quality images through text [13, 35, 37, 41].
While such foundation models unlock a new world of cre-ative processes in content creation, their power and expres-sivity come at the expense of user controllability, which is largely restricted to guiding the generation solely through an input text.
In this paper, we focus on attaining con-trol over the generated structure and semantic layout of the scene – an imperative component in various real-world con-tent creation tasks, ranging from visual branding and mar-keting to digital art. That is, our goal is to take text-to-image generation to the realm of text-guided Image-to-Image (I2I) translation, where an input image guides the layout (e.g., the structure of the horse in Fig. 1), and the text guides the per-ceived semantics and appearance of the scene (e.g., “robot horse” in Fig. 1).
A possible approach for achieving control of the gen-erated layout is to design text-to-image foundation models that explicitly incorporate additional guiding signals, such as user-provided masks [13, 29, 35]. For example, recently
Make-A-Scene [13] trained a text-to-image model that is also conditioned on a label segmentation mask, defining the layout and the categories of objects in the scene. However, such an approach requires an extensive compute as well as large-scale text-guidance-image training tuples, and can be applied at test-time to these specific types of inputs. In this paper, we are interested in a unified framework that can be applied to versatile I2I translation tasks, where the struc-ture guidance signal ranges from artistic drawings to photo-realistic images (see Fig. 1). Our method does not require any training or fine-tuning, but rather leverages a pre-trained and fixed text-to-image diffusion model [37].
We pose the fundamental question of how structure in-formation is internally encoded in such a model. We dive into the intermediate spatial features that are formed during the generation process, empirically analyze them, and de-vise a new framework that enables fine-grained control over the generated structure by applying simple manipulations to spatial features inside the model. Specifically, spatial fea-tures and their self-attentions are extracted from the guid-ance image, and are directly injected into the text-guided generation process of the target image. We demonstrate that our approach is not only applicable in cases where the guid-ance image is generated from text, but also for real-world images that are inverted into the model.
To summarize, we make the following key contributions: (i) We provide new empirical insights about internal spatial features formed during the diffusion process. (ii) We introduce an effective framework that leverages the power of pre-trained and fixed guided diffusion, allowing to perform high-quality text-guided I2I translation without any training or fine-tuning. (iii) We show, both quantitatively and qualitatively that our method outperforms existing state-of-the-art baselines, achieving significantly better balance between preserving the guidance layout and deviating from its appearance. 2.