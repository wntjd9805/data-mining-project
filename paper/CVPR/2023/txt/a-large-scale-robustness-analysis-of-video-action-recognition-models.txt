Abstract
We have seen a great progress in video action recognition in recent years. There are several models based on convolu-tional neural network (CNN) and some recent transformer based approaches which provide top performance on exist-ing benchmarks. In this work, we perform a large-scale robustness analysis of these existing models for video ac-tion recognition. We focus on robustness against real-world distribution shift perturbations instead of adversarial per-turbations. We propose four different benchmark datasets,
HMDB51-P, UCF101-P, Kinetics400-P, and SSv2-P to per-form this analysis. We study robustness of six state-of-the-art action recognition models against 90 different perturbations.
The study reveals some interesting findings, 1) transformer based models are consistently more robust compared to CNN based models, 2) Pretraining improves robustness for Trans-former based models more than CNN based models, and 3)
All of the studied models are robust to temporal perturba-tions for all datasets but SSv2; suggesting the importance of temporal information for action recognition varies based on the dataset and activities. Next, we study the role of augmen-tations in model robustness and present a real-world dataset,
UCF101-DS, which contains realistic distribution shifts, to further validate some of these findings. We believe this study will serve as a benchmark for future research in robust video action recognition 1. 1.

Introduction
Robustness of deep learning models against real-world distribution shifts is crucial for various applications in vision, such as medicine [4], autonomous driving [41], environment monitoring [60], conversational systems [36], robotics [68] and assistive technologies [5]. Distribution shifts with re-spect to training data can occur due to the variations in en-vironment such as changes in geographical locations, back-ground, lighting, camera models, object scale, orientations,
*The authors contributed equally as first authors to this paper.
†Corresponding author:madelineschiappa@knights.ucf.edu
‡The authors contributed equally as supervisors to this paper. 1More details available at bit.ly/3TJLMUF.
Figure 1. Performance against robustness of action recognition models on UCF101-P. y-axis: relative robustness γr, x-axis: ac-curacy on clean videos, model names appended with P indicate pre-training, and the size of circle indicates FLOPs. motion patterns, etc. Such distribution shifts can cause the models to fail when deployed in a real world settings [26].
For example, an AI ball tracker that replaced human camera operators was recently deployed in a soccer game and repeat-edly confused a soccer ball with the bald head of a lineman, leading to a bad experience for viewers [13].
Robustness has been an active research topic due to its importance for real-world applications [4, 41, 60]. However, most of the effort is directed towards images [7, 25, 26].
Video is a natural form of input to the vision systems that function in the real world. Therefore studying robustness in videos is an important step towards developing reliable systems for real world deployment. In this work we perform a large-scale analysis on robustness of existing deep models for video action recognition against common real world spatial and temporal distribution shifts.
Video action recognition provides an important test sce-nario to study robustness in videos given there are sufficient, large benchmark datasets and well developed deep learning models. Although the existing approaches have made im-pressive progress in action recognition, there are several fun-damental questions that still remain unanswered in the field.
Do these approaches enable effective temporal modeling, the crux of the matter for action recognition approaches? Are these approaches robust to real-world corruptions like noise
(temporally consistent and inconsistent), blurring effects, etc? Do we really need heavy architectures for robustness, or are light-weight models good enough? Are the recently introduced transformer-based models, which give state of the art accuracy on video datasets, more robust? Does pre-training play a role in model robustness? This study aims at finding answers to some of these critical questions.
Towards this goal, we present multiple benchmark datasets to conduct robustness analysis in video action recognition. We utilize four different widely used action recognition datasets including HMDB51 [34], UCF101
[54], Kinetics-400 [8], and SSv2 [43] and propose four corresponding benchmarks; HMDB51-P, UCF101-P,
Kinetics400-P, and SSv2-P. In order to create this benchmark, we introduce 90 different common perturbations which in-clude, 20 different noise corruptions, 15 blur perturbations, 15 digital perturbations, 25 temporal perturbations, and 15 camera motion perturbations as a benchmark. The study covers 6 different deep architectures considering different aspects, such as network size (small vs large), network archi-tecture (CNN vs Transformers), and network depth (shallow vs deep).
This study reveals several interesting findings about ac-tion recognition models. We observe that recent transformer based models are not only better in performance, but they are also more robust than CNN models against most distri-bution shifts (Figure 1). We also observe that pretraining is more beneficial to transformers compared to CNN based models in robustness. We find that all the models are very robust against the temporal perturbations with minor drop in performance on the Kinetics, UCF and HMDB. However, on the SSv2 dataset, behavior of the models is different whereas the performance drops on different temporal perturbations.
These observations show interesting phenomena about the video action recognition datasets, i.e., the importance of tem-poral information varies based on the dataset and activities.
Next, we study the role of training with data augmenta-tions in model robustness and analyze the generalization of these techniques to novel perturbations. To further study the capability of such techniques, we propose a real-world dataset, UCF101-DS, which contains realistic distribution shifts without simulation. This dataset also helps us to bet-ter understand the behavior of CNN and Transformer-based models under realistic scenarios. We believe such findings will open up many interesting research directions in video action recognition and will facilitate future research on video robustness which will lead to more robust architectures for real-world deployment.
We make the following contributions in this study,
• A large-scale robustness analysis of video action recog-nition models to different real-world distribution shifts.
• Provide insights including comparison of transformer vs CNN based models, effect of pre-training, and effect of temporal perturbations on video robustness.
• Four large-scale benchmark datasets to study robustness for video action recognition along with a real-world dataset with realistic distribution shifts. 2.