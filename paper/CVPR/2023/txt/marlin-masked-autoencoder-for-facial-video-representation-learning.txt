Abstract
This paper proposes a self-supervised approach to learn universal facial representations from videos, that can trans-fer across a variety of facial analysis tasks such as Facial
Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchroniza-tion (LS). Our proposed framework, named MARLIN, is a facial video masked autoencoder, that learns highly robust and generic facial embeddings from abundantly available non-annotated web crawled facial videos. As a challenging auxiliary task, MARLIN reconstructs the spatio-temporal details of the face from the densely masked facial regions which mainly include eyes, nose, mouth, lips, and skin to capture local and global aspects that in turn help in encod-ing generic and transferable features. Through a variety of experiments on diverse downstream tasks, we demonstrate
MARLIN to be an excellent facial video encoder as well as feature extractor, that performs consistently well across a variety of downstream tasks including FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsu-pervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for Frechet Inception Dis-tance), and even in low data regime. Our code and models are available at https://github.com/ControlNet/MARLIN. 1.

Introduction
Facial analysis tasks [34, 43, 70, 85] provide essential cues for human non-verbal behavior analysis, and help un-fold meaningful insights regarding social interaction [36], communication [40], cognition [68] with potential appli-cations in Human-Computer Interaction (HCI) and Affec-tive Computing domains. Recently, we have witnessed sig-nificant progress in deep neural network models to solve facial analysis tasks such as Facial Attribute Recognition (FAR) [34, 85], Facial Expression Recognition (FER) [48],
DeepFake Detection (DFD) [70], and Lip Synchronization (LS) [43]. While these deep models can achieve remark-Figure 1. Overview of the proposed Masked Autoencoder for fa-cial Representation LearnINg aka MARLIN. MARLIN aims to learn a universal facial representation from abundantly available non-annotated facial video data. able performance, they often require large-scale annotated datasets, which is not only a resource-expensive and time-consuming process but also infeasible for some applications requiring domain expertise for annotation (e.g. FER).
To this end, self-supervised pre-training [26, 37, 71] has lately emerged as an effective strategy to address the lim-itations of fully supervised methods, as it enables generic representation learning from non-annotated data, that can then be transferred across tasks having limited labels.
For images of natural scenes and objects, self-supervised learning approaches using self-distillation [14], contrastive-learning [18, 19], solving pre-text tasks such as jigsaw puz-zle [53], and more recently autoencoding [37,71] have even outperformed the supervised learning approaches.
Despite the promises offered by these self-supervised methods in learning scalable and generic representations for natural scene images and videos, these have not yet been investigated for learning representations from facial video data. Facial representation learning requires track-ing of fine-grained face specific details which might not be perfectly captured by linear tube masking [71]. Un-til now, most of the existing approaches associated with facial analysis tasks are highly specialized and develop
task-specific models trained in a fully supervised manner
[46, 54, 63], with very few recent efforts towards learning generic image-based facial encoding [10,84]. These closely related works [10, 84] either focus on exploring training dataset properties in terms of size and quality [10] or per-forming pre-training in visual-linguistic way [84]. These works [10, 84] are hard to scale since they use static image-level facial information and the image-caption pairs are highly associated with context information rather than face.
In this paper, our goal is to learn universal and task-agnostic representations in a self-supervised manner for face-related downstream tasks (see Fig. 1). For this pur-pose, we employ a masked autoencoder [37, 71] with a facial-guided masking strategy that learns to reconstruct spatio-temporal details of a face from densely masked fa-cial regions using non-annotated videos. Unlike existing approaches for natural scene videos [71], where the tube-masking is initialized with a static part of the video without any semantic information, our approach dynamically tracks face and then develops a facial part-guided tube mask-ing strategy using an off-the-shelf face parser i.e. FaceX-Zoo [75]. Thus, we pose a more challenging task that en-courages the model to learn spatio-temporal representations to cover local as well as global information.
Inspired by prior works [27, 60] showing high-quality reconstruction results along with rich and generic latent features, we in-corporate adversarial loss on top of masked encoding to enhance reconstruction quality. Our experimental results show that our proposed framework, MARLIN, learns highly generic facial encoding that scale and transfers well across diverse facial analysis tasks such as FER, DFD, FAR, and
LS and achieve favorable performance gain w.r.t. state-of-the-art benchmarks.
In summary, our main contributions are:
• We propose, MARLIN, a universal and task-agnostic facial encoder that learns robust and transferable facial representation from abundantly available non-annotated web-crawled facial videos in a self-supervised fashion.
• As a challenging auxiliary task, we propose to reconstruct the spatio-temporal details of the face from the densely masked facial regions. The proposed facial region-guided tube masking (aka Fasking) strategy aims to learn local and global aspects from facial videos which in turn help encode generic and transferable features.
• Through extensive quantitative and qualitative analysis, we show that MARLIN learns rich, generic, transferable, and robust facial representation, that performs consis-tently well across a variety of downstream tasks includ-ing FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for
Frechet Inception Distance) and even in few shot settings.
Table 1. Facial Analysis Tasks. Overview of different face related tasks and relevant datasets down the lane.
Datasets
LFW [39]
VGG-FACE [54]
CelebA [50]
YouTubeFace [78]
LRS2 [22]
CelebV [79]
CMU-MOSEI [83]
FaceForensics++ [62]
VoxCeleb2 [23]
CelebV-HQ [85]
# Samples 13,233 2.6M
Fmt.
Env.
Img.
Wild
Img.
Wild
Img. 202,599 Wild 3,425
Wild Vid 144,482 Wild Vid 5
Wild Vid 23,453
Wild Vid
Wild Vid 1,004 150,480 Wild Vid
Wild Vid 55,666
Task
Identification
Identification
Attributes
Identification
Lip Sync.
Reenact
Emo, Senti
DeepFake
Speaker
Attribute
Year 2008 2015 2015 2011 2017 2018 2018 2019 2018 2022 2.