Abstract
Recently, diffusion models have achieved great success in image synthesis. However, when it comes to the layout-to-image generation where an image often has a complex scene of multiple objects, how to make strong control over both the global layout map and each detailed object remains a challenging task. In this paper, we propose a diffusion model named LayoutDiffusion that can obtain higher gen-eration quality and greater controllability than the previous works. To overcome the difficult multimodal fusion of im-age and layout, we propose to construct a structural image patch with region information and transform the patched image into a special layout to fuse with the normal lay-out in a unified form. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention (OaCA) are pro-posed to model the relationship among multiple objects and designed to be object-aware and position-sensitive, allow-*Equal contribution.
†Corresponding author. ing for precisely controlling the spatial related information.
Extensive experiments show that our LayoutDiffusion out-performs the previous SOTA methods on FID, CAS by rela-tively 46.35%, 26.70% on COCO-stuff and 44.29%, 41.82% on VG. Code is available at https://github.com/
ZGCTroy/LayoutDiffusion. 1.

Introduction
Recently, the diffusion model has achieved encouraging progress in conditional image generation, especially in text-to-image generation such as GLIDE [24], Imagen [31], and
Stable Diffusion [30]. However, text-guided diffusion mod-els may still fail in the following situations. As shown in
Fig. 1 (a), when aiming to generate a complex image with multiple objects, it is hard to design a prompt properly and comprehensively. Even input with well-designed prompts, problems such as missing objects and incorrectly generat-ing objects’ positions, shapes, and categories still occur in the state-of-the-art text-guided diffusion model [24, 30, 31].
This is mainly due to the ambiguity of the text and its weak-ness in precisely expressing the position of the image space.
Fortunately, this is not a problem when using the coarse lay-out as guidance, which is a set of objects with the annota-tion of the bounding box (bbox) and object category. With both spatial and high-level semantic information, the diffu-sion model can obtain more powerful controllability while maintaining the high quality.
However, early studies [2, 14, 38, 42] on layout-to-image generation are almost limited to generative adversarial net-works (GANs) and often suffer from unstable conver-gence [1] and mode collapse [27]. Despite the advantages of diffusion models in easy training [10] and significant qual-ity improvement [7], few studies have considered applying diffusion in the layout-to-image generation task. To our knowledge, only LDM [30] supports the condition of lay-out and has shown encouraging progress in this field.
In this paper, different from LDM that applies the sim-ple multimodal fusion method (e.g., the cross attention) or direct input concatenation for all conditional input, we aim to specifically design the fusion mechanism between lay-out and image. Moreover, instead of conditioning only in the second stage like LDM, we propose an end-to-end one-stage model that considers the condition for the whole process, which may have the potential to help mitigate loss in the task that requires fine-grained accuracy in pixel space [30]. The fusion between image and layout is a diffi-cult multimodal fusion problem. Compared to the fusion of text and image, the layout has more restrictions on the po-sition, size, and category of objects. This requires a higher controllability of the model and often leads to a decrease in the naturalness and diversity of the generated image. Fur-thermore, the layout is more sensitive to each token and the loss in token of layout will directly lead to the missing ob-jects.
To address the problems mentioned above, we propose treating the patched image and the input layout in a uni-fied form. Specifically, we construct a structural image patch at multi-resolution by adding the concept of region that contains information of position and size. As a re-sult, each patch of the image is transformed into a special type of object, and the entire patched image will also be regarded as a layout. Finally, the difficult problem of multi-modal fusion between image and layout will be transformed into a simple fusion with a unified form in the same spatial space of the image. We name our model LayoutDiffuison, a layout-conditional diffusion model with Layout Fusion
Module (LFM), object-aware Cross Attention Mechanism (OaCA), and corresponding classifier-free training and sam-pling scheme. In detail, LFM fuses the information of each object and models the relationship among multiple objects, providing a latent representation of the entire layout. To make the model pay more attention to the information re-lated to the object, we propose an object-aware fusion mod-ule named OaCA. Cross-attention is made between the im-age patch feature and layout in a unified coordinate space by representing the positions of both of them as bounding boxes. To further improve the user experience of LayoutD-iffuison, we also make several optimizations on the speed of the classifier-free sampling process and could significantly outperform the SOTA models in 25 iterations.
Experiments are conducted on COCO-stuff [5] and Vi-sual Genome (VG) [19]. Various metrics ranging from qual-ity, diversity, and controllability show that LayoutDiffusion significantly outperforms both state-of-the-art GAN-based and diffusion-based methods.
Our main contribution is listed below.
• Instead of using the dominated GAN-based methods, we propose a diffusion model named LayoutDiffusion for layout-to-image generations, which can generate images with both high-quality and diversity while maintaining precise control over the position and size of multiple objects.
• We propose to treat each patch of the image as a special object and accomplish the difficult multimodal fusion of layout and image in a unified form. LFM and OaCA are then proposed to fuse the multi-resolution image patches with user’s input layout.
• LayoutDiffuison outperforms the SOTA layout-to-image generation method on FID, DS, CAS by rela-tively around 46.35%, 9.61%, 26.70% on COCO-stuff and 44.29%, 11.30%, 41.82% on VG. 2.