Abstract
Sign languages are visual languages which convey in-formation by signers’ handshape, facial expression, body movement, and so forth. Due to the inherent restric-tion of combinations of these visual ingredients, there ex-ist a significant number of visually indistinguishable signs (VISigns) in sign languages, which limits the recognition capacity of vision neural networks. To mitigate the problem, we propose the Natural Language-Assisted Sign Language
Recognition (NLA-SLR) framework, which exploits seman-tic information contained in glosses (sign labels). First, for VISigns with similar semantic meanings, we propose language-aware label smoothing by generating soft labels for each training sign whose smoothing weights are com-puted from the normalized semantic similarities among the glosses to ease training. Second, for VISigns with distinct semantic meanings, we present an inter-modality mixup technique which blends vision and gloss features to further maximize the separability of different signs under the super-vision of blended labels. Besides, we also introduce a novel backbone, video-keypoint network, which not only models both RGB videos and human body keypoints but also de-rives knowledge from sign videos of different temporal re-ceptive fields. Empirically, our method achieves state-of-the-art performance on three widely-adopted benchmarks:
MSASL, WLASL, and NMFs-CSL. Codes are available at https://github.com/FangyunWei/SLRT. 1.

Introduction
Sign languages are the primary languages for communi-cation among deaf communities. On the one hand, sign lan-guages have their own linguistic properties as most natural languages [1, 52, 64]. On the other hand, sign languages are visual languages that convey information by the movements of the hands, body, head, mouth, and eyes, making them completely separate and distinct from natural languages
[6,69,71]. This work dedicates to sign language recognition (SLR), which requires models to classify the isolated signs
†Corresponding author. (a) VISigns may have similar semantic meanings. (b) VISigns may have distinct semantic meanings.
Figure 1. Vision neural networks are demonstrated to be less ef-fective to recognize visually indistinguishable signs (VISigns) [2, 26, 34]. We observe that VISigns may have similar or distinct se-mantic meanings, inspiring us to leverage this characteristic to fa-cilitate sign language recognition as illustrated in Figure 2. from videos into a set of glosses1. Despite its fundamen-tal capacity of recognizing signs, SLR has a broad range of applications including sign spotting [36, 42, 58], sign video retrieval [8, 11], sign language translation [6, 35, 54], and continuous sign language recognition [1, 6].
Since the lexical items of sign languages are defined by the handshape, facial expression, and movement, the combinations of these visual ingredients are restricted in-herently, yielding plenty of visually indistinguishable signs termed VISigns. VISigns are those signs with similar hand-shape and motion but varied semantic meanings. We show two examples (“Cold” vs. “Winter” and “Table” vs. “Af-ternoon”) in Figure 1. Unfortunately, it has been demon-strated that vision neural networks are less effective at 1Gloss is a unique label for a single sign. Each gloss is identified by a word which is associated with the sign’s semantic meaning.
(a) Language-aware label smoothing. (b) Inter-modality mixup.
Figure 2. We incorporate natural language modeling into sign language recognition to promote recognition capacity. (a) Language-aware label smoothing generates a soft label for each training video, whose smoothing weights are the normalized semantic similarities of the ground truth gloss and the remaining glosses within the sign language vocabulary. (b) Inter-modality mixup yields the blended features (denoted by orange rectangles) with the corresponding mixed labels to maximize the separability of signs in a latent space. accurately recognizing VISigns [2, 26, 34]. Due to the intrinsic connections between sign languages and natural languages, the glosses, i.e., labels of signs, are seman-tically meaningful in contrast to the one-hot labels used in traditional classification tasks [27, 51]. Thus, although it is challenging to classify the VISigns from the vision perspective, their glosses provide serviceable semantics, which is, however, less taken into consideration in previ-ous works [18–20,23,24,26,34,36]. Our work is built upon the following two findings.
Finding-1: VISigns may have similar semantic meanings (Figure 1a). Due to the observation that VISigns may have higher visual similarities, assigning hard labels to them may hinder the training since it is challenging for vision neu-ral networks to distinguish each VISign apart. A straight-forward way to ease the training is to replace the hard la-bels with soft ones as in well-established label smooth-ing [15, 56]. However, how to generate proper soft labels is non-trivial. The vanilla label smoothing [15, 56] assigns equal smoothing weights to all negative terms, which ig-nores the semantic information contained in labels. In light of the finding-1 that VISigns may have similar semantic meanings and the intrinsic connections between sign lan-guages and natural languages, we consider the semantic similarities among the glosses when generating soft labels.
Concretely, for each training video, we adopt an off-the-shelf word representation framework, i.e., fastText [39], to pre-compute the semantic similarities of its gloss and the re-maining glosses within the sign language vocabulary. Then we can properly generate a soft label for each training sam-ple whose smoothing weights are the normalized semantic similarities. In this way, negative terms with similar seman-tic meanings to the ground truth gloss are assigned higher values in the soft label. As shown in Figure 2a, we term this process as language-aware label smoothing, which injects prior knowledge into the training.
Finding-2: VISigns may have distinct semantic mean-ings (Figure 1b). Although the VISigns are challenging to be classified from the vision perspective, the semantic meanings of their glosses may be distinguishable according to finding-2. This inspires us to combine the vision fea-tures and gloss features to drive the model towards max-imizing signs’ separability in a latent space. Specifically, given a sign video, we first leverage our proposed backbone to encode its vision feature and the well-established fast-Text [39] to extract the feature of each gloss within the sign language vocabulary. Then we independently integrate the vision feature and each gloss feature to produce a blended representation, which is further fed into a classifier to ap-proximate its mixed label. We refer to this procedure as inter-modality mixup as shown in Figure 2b. We empir-ically find that our inter-modality mixup significantly en-hances the model’s discriminative power.
Our contributions can be summarized as follows:
• We are the first to incorporate natural language mod-eling into sign language recognition based on the dis-covery of VISigns. Language-aware label smoothing and inter-modality mixup are proposed to take full ad-vantage of the linguistic properties of VISigns and se-mantic information contained in glosses.
• We take into account the unique characteristic of sign languages and present a novel backbone named video-keypoint network (VKNet), which not only models both RGB videos and human keypoints, but also de-rives knowledge from sign videos of various temporal receptive fields.
• Our method, termed natural language-assisted sign language recognition (NLA-SLR), achieves state-of-the-art performance on the widely-used SLR datasets including MSASL [26], WLASL [34], and NMFs-CSL [20]. 2.