Abstract
Samples in Training Domain 
Data Augmentation
OOD Samples
Deep neural networks (DNNs) usually fail to general-ize well to outside of distribution (OOD) data, especially in the extreme case of single domain generalization (single-DG) that transfers DNNs from single domain to multi-ple unseen domains. Existing single-DG techniques com-monly devise various data-augmentation algorithms, and remould the multi-source domain generalization methodol-ogy to learn domain-generalized (semantic) features. Nev-ertheless, these methods are typically modality-speciﬁc, thereby being only applicable to one single modality (e.g., image). In contrast, we target a versatile Modality-Agnostic
Debiasing (MAD) framework for single-DG, that enables generalization for different modalities. Technically, MAD introduces a novel two-branch classiﬁer: a biased-branch encourages the classiﬁer to identify the domain-speciﬁc (su-perﬁcial) features, and a general-branch captures domain-generalized features based on the knowledge from biased-branch. Our MAD is appealing in view that it is pluggable to most single-DG models. We validate the superiority of our MAD in a variety of single-DG scenarios with different modalities, including recognition on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images.
More remarkably, for recognition on 3D point clouds and semantic segmentation on 2D images, MAD improves DSU by 2.82% and 1.5% in accuracy and mIOU. 1.

Introduction
Deep neural networks (DNNs) have achieved remarkable success in various tasks under the assumption that train-ing and testing domains are independent and sampled from identical or sufﬁciently similar distribution [2, 48]. How-ever, this assumption often does not hold in most real-world scenarios. When deploying DNNs to unseen or out-of-distribution (OOD) testing domains, inevitable perfor-mance degeneration is commonly observed. The difﬁculty mainly originates from that the backbone of DNNs ex-*Corresponding author s s e e g g a a m m
I
I s s d d u u o o l l
C
C t t n n i i o o
P
P
Can not directly  transfer ...
Figure 1. Most existing single-DG techniques devise various data augmentation algorithms to introduce various image textures and styles, pursuing the learning of domain-generalized features.
However, these approaches are modality-speciﬁc, and only appli-cable to single modality (e.g., image). Hence it is difﬁcult to di-rectly employ such single-DG approach for 3D point clouds, since the domain shifts in 3D point clouds only reﬂect the geometric dif-ferences rather than texture and style differences. tracts more domain-speciﬁc (superﬁcial) features together with domain-generalized (semantic) features. Therefore, the classiﬁer is prone to paying much attention to those domain-speciﬁc features, and learning unintended decision rule [53]. To mitigate this issue, several appealing so-lutions have been developed, including Domain Adapta-tion (DA) [18, 32, 36, 40, 41] and Domain Generalization (DG) [31, 56, 62, 65]. Despite showing encouraging per-formances on OOD data, their real-world applications are still limited due to the requirement to have the data from other domain (i.e., the unseen target domain or multiple source domains with different distributions). In this work, we focus on an extreme case in domain generalization: sin-gle domain generalization (single-DG), in which DNNs are trained with single source domain data and then required to generalize well to multiple unseen target domains.
Previous researches [19,55] demonstrate that the speciﬁc local textures and image styles tailored to each domain are two main causes, resulting in domain-speciﬁc features for images. To alleviate this, recent works [30, 37, 58, 63] de-   
sign a variety of data-augmentation algorithms to introduce diversiﬁed textures and image styles. The DG methodolo-gies are then remolded with these data-augmentation algo-rithms to facilitate the learning of domain-generalized fea-tures. Nevertheless, such solution for single-DG is typically modality-speciﬁc and only applicable to the single modal-ity inputs of images. When coming a new modality (e.g. 3D point clouds), it is difﬁcult to directly apply these tech-niques to tackle single-DG problem. This is due to the fact that the domain shift in 3D point clouds is interpreted as the differences of 3D structural information among multi-ple domains, instead of the texture and style differences in 2D images [10, 39]. Figure 1 conceptually illustrates the issue, which has been seldom explored in the literature.
In this paper, we propose to address this limitation from the standpoint of directly strengthening the capac-ity of classiﬁer to identify domain-speciﬁc features, and meanwhile emphasize the learning of domain-generalized features. Such way completely eliminates the need of modality-speciﬁc data augmentations, thereby leading to a versatile modality-agnostic paradigm for single-DG. Tech-nically, to materialize this idea, we design a novel Modality-Agnostic Debiasing (MAD) framework, that facilitates sin-gle domain generalization under a wide variety of modali-ties. In particular, MAD integrates the basic backbone for feature extraction with a new two-branch classiﬁer struc-ture. One branch is the biased-branch that identiﬁes those superﬁcial and domain-speciﬁc features with a multi-head cooperated classiﬁer. The other branch is the general-branch that learns to capture the domain-generalized rep-resentations on the basis of the knowledge derived from the biased-branch. It is also appealing in view that our MAD can be seamlessly incorporated into most existing single-DG models with data-augmentation, thereby further boost-ing single domain generalization.
We analyze and evaluate our MAD under a variety of single-DG scenarios with different modalities, ranging from recognition on 2D images, 3D point clouds, 1D texts, to semantic segmentation on 2D images. Extensive experi-ments demonstrate the superior advantages of MAD when being plugged into a series of existing single-DG techniques with data-augmentation (e.g., Mixstyle [65] and DSU [30]).
More remarkably, for recognition on point cloud bench-mark, MAD signiﬁcantly improves DSU in the accuracy from 33.63% to 36.45%. For semantic segmentation on im-age benchmark, MAD advances DSU with mIoU improve-ment from 42.3% to 43.8%. 2.