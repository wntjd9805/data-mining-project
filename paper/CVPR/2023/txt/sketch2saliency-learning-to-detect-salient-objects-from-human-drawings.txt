Abstract
Human sketch has already proved its worth in various visual understanding tasks (e.g., retrieval, segmentation, image-captioning, etc). In this paper, we reveal a new trait of sketches – that they are also salient. This is intuitive as sketching is a natural attentive process at its core. More specifically, we aim to study how sketches can be used as a weak label to detect salient objects present in an im-age. To this end, we propose a novel method that empha-sises on how “salient object” could be explained by hand-drawn sketches. To accomplish this, we introduce a photo-to-sketch generation model that aims to generate sequential sketch coordinates corresponding to a given visual photo through a 2D attention mechanism. Attention maps accu-mulated across the time steps give rise to salient regions in the process. Extensive quantitative and qualitative experi-ments prove our hypothesis and delineate how our sketch-based saliency detection model gives a competitive perfor-mance compared to the state-of-the-art. 1.

Introduction
As any reasonable drawing lesson would have taught you – sketching is an attentive process [24]. This paper sets out to prove just that but in the context of computer vi-sion. In particular, we show that attention information in-herently embedded in sketches can be cultivated to learn image saliency [30, 66, 82, 87].
Sketch research has flourished in the past decade, partic-ularly with the proliferation of touchscreen devices. Much of the utilisation of sketch has anchored around its human-centric nature, in that it naturally carries personal style
[6,54], subjective abstraction [7,53], human creativity [20], to name a few. Here we study a sketch-specific trait that has been ignored to date – sketch is also salient.
The human visual system has evolved over millions of years to develop the ability to attend [22, 79]. This attentive process is ubiquitously reflected in language (i.e., how we describe visual concepts) and art (i.e., how artists attend to different visual aspects). The vision community has also invested significant effort to model this attentive process,
*Interned with SketchX
Figure 1. Sequential photo-to-sketch generation with 2D-attention to leverage sketch as a weak label for salient object detection. Ag-gregated 2D attention-maps till a particular instant are shown. in the form of saliency detection [47, 67, 76, 80, 82]. The paradox facing the saliency community is however that the attention information has never been present in photos to start with – photos are mere collections of static pixels. It then comes with no surprise that most prior research has resorted to a large amount of pixel-level annotation.
Although fully-supervised frameworks [10, 34, 38, 87] have been shown to produce near-perfect saliency maps, its widespread adoption is largely bottlenecked by this need for annotation. To deal with this issue, a plethora of semi/weakly-supervised methods have been introduced, which attempt to use captions [82], class-labels [44], fore-ground mask [66], class activation map (CAM) [69], bound-ing boxes [41], scribbles [85] as weak labels. We follow this push to utilise labels, but importantly introduce sketch to the mix, and show it is a competitive label modality because of the inherently embedded attentive information it possesses.
Utilising sketch as a weak label for saliency detection is nonetheless non-trivial. Sketch, primarily being abstract and sequential [22] in nature, portrays significant modal-ity gap with photos. Therefore, we seek to build a frame-work that can connect sketch and photo domains via some auxiliary task. For that, we take inspiration from an actual artistic sketching process, where artists [88] attend to cer-tain regions on an object, then render down the strokes on paper. We thus propose photo-to-sketch generation, where given a photo we aim to generate a sketch stroke-by-stroke, as an auxiliary task to bridge the two domains.
However effective in bridging the domain gap, this gen-eration process by default does not generate pixel-wise im-portance values depicting a saliency map. To circumvent this problem, we make clever use of a cross-modal 2D at-tention module inside the sketch decoding process, which naturally predicts a local saliency map at each stroke – ex-actly akin to how artists refer back to the object before ren-dering the next stroke.
More specifically, the proposed photo-to-sketch genera-tor is an encoder-decoder model that takes an RGB photo as input and produces a sequential sketch. The model is augmented with a 2D attention mechanism that importantly allows the model to focus on visually salient regions of the photo associated with each stroke during sketch generation.
In doing so, the attention maps accumulated over the time steps of sequential sketch generation would indicate the re-gions on the photo that were of utmost importance. See
Fig. 1 for an illustration. To further address the domain gap in supervision between pixel-wise annotation and sketch la-bels, we propose an additional equivariance loss to gain robustness towards perspective deformations [28] thus im-proving overall performance.
In our experiments we firstly report the performance of saliency maps directly predicted by the network, without convolving it with any ad hoc post-processing that is com-monplace in the literature [82]. This is to spell out the true effects of using sketch for saliency detection, which is our main contribution. To further evaluate its competitiveness against other state-of-the-arts though, we also plug in our photo-to-sketch decoder in place of the image-captioning branch of a multi-source weak supervision framework that uses class-labels and text-description for saliency learning
[82]. We train our network on the Sketchy dataset [56] con-It is worth noting that the sisting of photo-sketch pairs. training data was not intentionally collected with saliency detection in mind, rather the annotators were asked to draw a sketch that depicts the photo shown to them. This again strengthens our argument that sketch implicitly encodes saliency information which encouraged us to use it as a weak label at the first place.
In summary, our major contributions are: (a) We for the first time demonstrate the success of using sketch as a weak label in salient object detection. (b) To this end, we make clever use of sequential photo-to-sketch generation frame-work involving an auto-regressive decoder with 2D atten-tion for saliency detection. (c) Comprehensive quantita-tive and ablative experiments delineate that our method of-fers significant performance gain over weakly-supervised state-of-the-arts. Moreover, this is the first work in vision community that validates the intuitive idea of “Sketch is
Salient” through a simple yet effective framework. 2.