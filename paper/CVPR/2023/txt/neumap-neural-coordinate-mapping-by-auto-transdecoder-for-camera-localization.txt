Abstract
This paper presents an end-to-end neural mapping method for camera localization, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pix-els. State-of-the-art camera localization methods require each scene to be stored as a 3D point cloud with per-point features, which takes several gigabytes of storage per scene.
While compression is possible, the performance drops sig-nificantly at high compression rates. NeuMap achieves ex-tremely high compression rates with minimal performance drop by using 1) learnable latent codes to store scene in-formation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for a query pixel. The scene-agnostic network design also learns robust matching priors by training with large-scale data, and further allows us to just optimize the codes quickly for a new scene while fixing the network weights. Extensive evaluations with five bench-marks show that NeuMap outperforms all the other coor-dinate regression methods significantly and reaches similar performance as the feature matching methods while hav-ing a much smaller scene representation size. For example,
NeuMap achieves 39.1% accuracy in Aachen night bench-mark with only 6MB of data, while other compelling meth-ods require 100MB or a few gigabytes and fail completely under high compression settings. The codes are available at https://github.com/Tangshitao/NeuMap. 1.

Introduction
Visual localization determines camera position and ori-entation based on image observations, an essential task for applications such as VR/AR and self-driving cars. Despite significant progress, accurate visual localization remains a challenge, especially when dealing with large viewpoint and illumination changes. Compact map representation is an-other growing concern, as applications like delivery robots may require extensive maps. Standard visual localization techniques rely on massive databases of keypoints with 3D
coordinates and visual features, posing a significant bottle-neck in real-world applications.
Visual localization techniques generally establish 2D-3D correspondences and estimate camera poses using perspective-n-point (PnP) [22] with a sampling method like
RANSAC [15]. These methods can be divided into two cat-egories: Feature Matching (FM) [11, 34, 37, 49] and Scene
Coordinate Regression (SCR) [3, 5, 7, 40]. FM methods, which are trained on a vast amount of data covering vari-ous viewpoint and illumination differences, use sparse ro-bust features extracted from the query image and matched with those in candidate scene images. This approach ex-ploits learning-based feature extraction and correspondence matching methods [13, 35, 42, 44] to achieve robust local-ization. However, FM methods require large maps, mak-ing them impractical for large-scale scenes. Many meth-ods [11, 27, 49] have been proposed to compress this map representation, but often at the cost of degraded perfor-mance. On the other hand, SCR methods directly regress a dense scene coordinate map using a compact random for-est or neural network, providing accurate results for small-scale indoor scenes. However, their compact models lack generalization capability and are often restricted to lim-ited viewpoint and illumination changes. Approaches such as ESAC [5] handle larger scenes by dividing them into smaller sub-regions, but still struggle with large viewpoint and illumination changes.
We design our method to enjoy the benefits of compact scene representation of SCR methods and the robust per-formance of FM methods. Similar to FM methods, we also focus on a sparse set of robustly learned features to deal with large viewpoint and illumination changes. On the other hand, we exploit similar ideas to SCR methods to regress the 3D scene coordinates of these sparse fea-tures in the query images with a compact map represen-tation. Our method, dubbed neural coordinate mapping (NeuMap), first extracts robust features from images and then applies a transformer-based auto-decoder (i.e., auto-transdecoder) to learn: 1) a grid of scene codes encoding the scene information (including 3D scene coordinates and feature information) and 2) the mapping from query im-age feature points to 3D scene coordinates. At test time, given a query image, after extracting image features of its key-points, the auto-transdecoder regresses their 3D coor-dinates via cross attention between image features and la-tent codes. In our method, the robust feature extractor and the auto-transdecoder are scene-agnostic, where only latent codes are scene specific. This design enables the scene-agnostic parameters to learn matching priors across scenes while maintaining a small data size. To handle large scenes, we divide the scene into smaller sub-regions and process them independently while applying a network pruning tech-nique [25] to drop redundant codes.
We demonstrate the effectiveness of NeuMap with a di-verse set of five benchmarks, ranging from indoor to out-door and small to large-scale: 7scenes (indoor small), Scan-Net (indoor small), Cambridge Landmarks (outdoor small),
Aachen Day & Night (outdoor large), and NAVER LABS (indoor large).
In small-scale datasets (i.e., 7scenes and
Cambridge Landmarks), NeuMap compresses the scene representation by around 100-1000 times without any per-In large-scale formance drop compared to DSAC++. datasets (i.e., Aachen Day & Night and NAVER LABS),
NeuMap significantly outperforms the current state-of-the-art at high compression settings, namely, HLoc [34] with a scene-compression technique [49]. In ScanNet dataset, we demonstrate the quick fine-tuning experiments, where we only optimize the codes for a new scene while fixing the scene-agnostic network weights. 2.