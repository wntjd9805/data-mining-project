Abstract
Triangle mesh segmentation is an important task in 3D shape analysis, especially in applications such as digi-tal humans and AR/VR. Transformer model is inherently permutation-invariant to input, which makes it a suitable candidate model for 3D mesh processing. However, two main challenges involved in adapting Transformer from nat-ural languages to 3D mesh are yet to be solved, such as i) extracting the multi-scale information of mesh data in an adaptive manner; ii) capturing geometric structures of mesh data as the discriminative characteristics of the shape.
Current point based Transformer models fail to tackle such challenges and thus provide inferior performance for dis-cretized surface segmentation. In this work, heat diffusion based method is exploited to tackle these problems. A novel
Transformer model called MeshFormer is proposed, which i) integrates Heat Diffusion method into Multi-head Self-Attention operation (HDMSA) to adaptively capture the fea-tures from local neighborhood to global contexts; ii) ap-plies a novel Heat Kernel Signature based Structure Encod-ing (HKSSE) to embed the intrinsic geometric structures of mesh instances into Transformer for structure-aware processing. Extensive experiments on triangle mesh seg-mentation validate the effectiveness of the proposed Mesh-Former model and show signiﬁcant improvements over cur-rent state-of-the-art methods. 1.

Introduction
Discretized surface semantic segmentation is a task to semantically classify the labeling of each discrete element in 3D discretized surface. Such discrete element can be tri-angle face in mesh input [22, 24, 37] or 3D point in point cloud input [11, 20, 26, 28, 34, 48–50, 52].
It is an essen-tial task in many applications for 3D vision and computer graphics, such as 3D human body analysis, digital humans and AR/VR, etc. In this work, we mainly focus on mesh representation as input, as point cloud representation can be regarded as the special case of mesh which discards the surface connectivity.
The challenges in learning mesh representation involves its inherent characteristics such as irregularity and un-orderedness. Following the success in NLP [7, 15, 44] and 2D computer vision domain [16, 29, 43, 46], Transformer model such as [20, 32, 50, 52] has been adopted as an effec-tive model for processing 3D point cloud input due to its in-herent capability in processing unordered point sets. Along such direction, it is natural to adapt the Transformer model to the mesh input, which is also one of the most common representation for 3D input modality. However, adapting
Transformer model from natural languages to mesh input, with respect to the speciﬁc characteristics of mesh struc-tures, involves many challenges, such as i) extracting the multi-scale information of mesh data in an adaptive man-ner; ii) capturing geometric structures of mesh representa-tion as the discriminative characteristics. Sufﬁciently cap-turing such two essential information is the prerequisite for accurate mesh based semantic segmentation task.
In ret-rospect to the recent point cloud based Transformer mod-els [20, 32, 52], it is found that all these methods did not provide effective approaches to tackle the challenges men-tioned above, and thus provided a limited increment in seg-mentation accuracy for mesh input.
Recent work Swin Transformer [29] applies multiple
ﬁxed-size windows for limiting the attention computation along scale-varying regions, and hence provides a hierar-chical Transformer model to extract multi-scale informa-tion for dense prediction task. However, such approach which uses ﬁxed-size windows is only applicable to regu-lar 2D images input, while it is infeasible for irregular input such as 3D meshes with diverse shapes. In order to adapt the Transformer model for adaptively extracting multi-scale information for irregular mesh input, it is essential to ex-tend its core operation, self-attention, to have capability in progressively comparing the feature similarity from local neighborhood to global range of the mesh input, in the form of intrinsic geometry of surface [9]. In fact, several seminal works [12,13] had studied on using heat diffusion method to
intrinsically communicate the interactions with the neigh-bouring vertices on discretized surface. Such heat diffusion method is able to compute the geodesic distance on mesh in a stable and accurate manner, which is essential to capture the intrinsic locality in discretized surface.
In this work, we opt for the heat diffusion method to propose a novel self-attention operation called Heat Dif-fusion based Multi-head Self-Attention (HDMSA), which adaptively limits the self-attention computation within mul-tiple heat diffusion ranges to capture the multi-scale surface features from local neighborhood to global contexts. This extension facilitates the construction of multi-scale Trans-former encoder in the proposed MeshFormer model.
The second challenging issue in adapting Transformer model for mesh input is to encode the geometric structural information of mesh as a supplement for shape-speciﬁc in-ductive bias of Transformer model. In Non-Euclidean do-main, very recent works such as SAN [25], GNN-LSPE [17] methods have investigated on injecting the eigenfunctions, which are derived from graph Laplacian operator, into posi-tional encoding as a kind of graph-speciﬁc inductive bias to traditional Transformer model. This approach exploits the spectral information of graph Laplacian to capture the struc-ture of input modality, and thus is considered as a promis-ing candidate for processing mesh input. However, this ap-proach suffers from the issue of eigenfunction sign ambigu-ity, that means eigenfunction with either positive or negative sign still satisﬁes the original eigenproblem and is associ-ated with the same eigenvalue, which lowers the discrimi-native power in the extracted structural information. In this work, instead of directly applying the spectral information, a novel Heat Kernel Signature based Structure Encoding (HKSSE) module is proposed, which effectively captures the intrinsic geometric structural information of the mesh while bypassing the issue of eigenfunction sign ambiguity.
Moreover, it provides a more powerful way to capture the more advanced geometric information, i.e., the symmetry in geometry structure, which is very common in human body shapes. As a result, this capability brought to Transformer model reinforces a structure-aware segmentation prediction for mesh input.
To the best of our knowledge, the proposed model is the
ﬁrst mesh based Transformer model which integrates heat diffusion methods to tackle the discretized surface semantic segmentation problem. The main contributions of this work are summarized as follows: 1. With the heat diffusion extension, the proposed multi-head self-attention operation allows intrinsic commu-nication for vertices on mesh input from local neigh-borhood to global context and thus is able to capture multi-scale mesh features. 2. A novel heat kernel signature based structure encoding module is applied to embed the mesh intrinsic geomet-ric structures into Transformer for providing structure-aware segmentation output. 3. The proposed work demonstrates the feasibility on the extension of generic Transformer model structure for 3D mesh input with heat diffusion methods. 2.